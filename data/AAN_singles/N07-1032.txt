Proceedings of NAACL HLT 2007, pages 252?259,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsCross-Instance Tuning of Unsupervised Document Clustering Algorithms?Damianos Karakos, Jason Eisnerand Sanjeev KhudanpurCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218{damianos,eisner,khudanpur}@jhu.eduCarey E. PriebeDept.
of Applied Mathematicsand StatisticsJohns Hopkins UniversityBaltimore, MD 21218cep@jhu.eduAbstractIn unsupervised learning, where no train-ing takes place, one simply hopes thatthe unsupervised learner will work wellon any unlabeled test collection.
How-ever, when the variability in the data islarge, such hope may be unrealistic; atuning of the unsupervised algorithm maythen be necessary in order to perform wellon new test collections.
In this paper,we show how to perform such a tuningin the context of unsupervised documentclustering, by (i) introducing a degree offreedom, ?, into two leading information-theoretic clustering algorithms, throughthe use of generalized mutual informa-tion quantities; and (ii) selecting the valueof ?
based on clusterings of similar, butsupervised document collections (cross-instance tuning).
One option is to performa tuning that directly minimizes the erroron the supervised data sets; another optionis to use ?strapping?
(Eisner and Karakos,2005), which builds a classifier that learnsto distinguish good from bad clusterings,and then selects the ?
with the best pre-dicted clustering on the test set.
Experi-ments from the ?20 Newsgroups?
corpusshow that, although both techniques im-prove the performance of the baseline al-gorithms, ?strapping?
is clearly a betterchoice for cross-instance tuning.
?This work was partially supported by the DARPA GALEprogram (Contract No?HR0011-06-2-0001) and by the JHUWSE/APL Partnership Fund.1 IntroductionThe problem of combining labeled and unlabeledexamples in a learning task (semi-supervised learn-ing) has been studied in the literature under variousguises.
A variety of algorithms (e.g., bootstrapping(Yarowsky, 1995), co-training (Blum and Mitchell,1998), alternating structure optimization (Ando andZhang, 2005), etc.)
have been developed in order toimprove the performance of supervised algorithms,by automatically extracting knowledge from lots ofunlabeled examples.
Of special interest is the workof Ando and Zhang (2005), where the goal is to buildmany supervised auxiliary tasks from the unsuper-vised data, by creating artificial labels; this proce-dure helps learn a transformation of the input spacethat captures the relatedness of the auxiliary prob-lems to the task at hand.
In essence, Ando and Zhang(2005) transform the semi-supervised learning prob-lem to a multi-task learning problem; in multi-tasklearning, a (usually large) set of supervised tasks isavailable for training, and the goal is to build mod-els which can simultaneously do well on all of them(Caruana, 1997; Ben-David and Schuller, 2003; Ev-geniou and Pontil, 2004).Little work, however, has been devoted to studythe situation where lots of labeled examples, of onekind, are used to build a model which is tested onunlabeled data of a ?different?
kind.
This problem,which is the topic of this paper, cannot be cast as amulti-task learning problem (since there are labeledexamples of only one kind), neither can be cast as asemi-supervised problem (since there are no traininglabels for the test task).
Note that we are interestedin the case where the hidden test labels may haveno semantic relationship with the training labels; in252some cases, there may not even be any informa-tion about the test labels?what they represent, howmany they are, or at what granularity they describethe data.
This situation can arise in the case of un-supervised clustering of documents from a large anddiverse corpus: it may not be known in what way theresulting clusters split the corpus (is it in terms oftopic?
genre?
style?
authorship?
a combination ofthe above?
), unless one inspects each resulting clus-ter to determine its ?meaning.
?At this point, we would like to differentiate be-tween two concepts: a target task refers to a classof problems that have a common, high-level de-scription (e.g., the text document clustering task, thespeech recognition task, etc.).
On the other hand,a task instance refers to a particular example fromthe class.
For instance, if the task is ?documentclustering,?
a task instance could be ?clustering ofa set of scientific documents into particular fields?
;or, if the task is ?parsing,?
a task instance could be?parsing of English sentences from the Wall StreetJournal corpus?.
For the purposes of this paper, wefurther assume that there are task instances whichare unrelated, in the sense that that there are nocommon labels between them.
For example, if thetask is ?clustering from the 20 Newsgroups corpus,?then ?clustering of the computer-related documentsinto PC-related and Mac-related?
and ?clusteringof the politics-related documents into Middle-East-related and non-Middle-East-related?
are two dis-tinct, unrelated instances.
In more mathematicalterms, if task instances T1, T2 take sets of observa-tionsX1,X2 as input, and try to predict labels fromsets S1, S2, respectively, then they are called unre-lated if X1 ?X2 = ?
and S1 ?
S2 = ?.The focus of this paper is to study the problemof cross-instance tuning of unsupervised algorithms:how one can tune an algorithm, which is used tosolve a particular task instance, using knowledgefrom an unrelated task instance.
To the best of ourknowledge, this cross-instance learning problem hasonly been tackled in (Eisner and Karakos, 2005),whose ?strapping?
procedure learns a meta-classifierfor distinguishing good from bad clusterings.In this paper, we introduce a scalar parameter ?
(a new degree of freedom) into two basic unsuper-vised clustering algorithms.
We can tune ?
to max-imize unsupervised clustering performance on dif-ferent task instances where the correct clustering isknown.
The hope is that tuning the parameter learnssomething about the task in general, which trans-fers from the supervised task instances to the un-supervised one.
Alternatively, we can tune a meta-classifier so as to select good values of ?
on the su-pervised task instances, and then use the same meta-classifier to select a good (possibly different) valueof ?
in the unsupervised case.The paper is organized as follows: Section 2 givesa background on text categorization, and briefly de-scribes the algorithms that we use in our experi-ments.
Section 3 describes our parameterization ofthe clustering algorithms using Jensen-Re?nyi diver-gence and Csisza?r?s mutual information.
Experi-mental results from the ?20 Newsgroups?
data setare shown in Section 4, along with two techniquesfor cross-instance learning: (i) ?strapping,?
which, attest time, picks a parameter based on various ?good-ness?
cues that were learned from the labeled dataset, and (ii) learning the parameter from a superviseddata set which is chosen to statistically match the testset.
Finally, concluding remarks appear in Section 5.2 Document CategorizationDocument categorization is the task of decidingwhether a piece of text belongs to any of a set ofprespecified categories.
It is a generic text process-ing task useful in indexing documents for later re-trieval, as a stage in natural language processingsystems, for content analysis, and in many otherroles (Lewis and Hayes, 1994).
Here, we dealwith the unsupervised version of document cate-gorization, in which we are interested in cluster-ing together documents which (hopefully) belong tothe same topic, without having any training exam-ples.1 Supervised information-theoretic clusteringapproaches (Torkkola, 2002; Dhillon et al, 2003)have been shown to be very effective, even with asmall amount of labeled data, while unsupervisedmethods (which are of particular interest to us) havebeen shown to be competitive, matching the classifi-cation accuracy of supervised methods.Our focus in this paper is on document catego-rization algorithms which use information-theoretic1By this, we mean that training examples having the samecategory labels as the test examples are not available.253criteria, since there are natural ways of generalizingthese criteria through the introduction of tunable pa-rameters.
We use two such algorithms in our exper-iments, the sequential Information Bottleneck (sIB)and Iterative Denoising Trees (IDTs); details aboutthese algorithms appear below.A note on mathematical notation: We assumethat we have a collection A = {X(1), .
.
.
, X(N)}of N documents.
Each document X(i) is essentiallya ?bag of words?, and induces an empirical distri-bution P?X(i) on the vocabulary X .
Given a sub-set (cluster) C of documents, the conditional dis-tribution on X , given the cluster, is just the cen-troid: P?X|C = 1|C|?X(i)?C P?X(i).
If a subcollec-tion S ?
A of documents is partitioned into clustersC1, .
.
.
, Cm, and each document X(i) ?
S is as-signed to a cluster CZ(i), where Z(i) ?
{1, .
.
.
,m}is the cluster index, then the mutual information be-tween words and corresponding clusters is given byI(X;Z|S) =?z?
{1,...,m}P (z|S)D(P?X|Cz?P?X|S),where P (z|S) , |Cz|/|S| is the ?prior?
distributionon the clusters and D(???)
is the Kullback-Leiblerdivergence (Cover and Thomas, 1991).2.1 The Information Bottleneck MethodThe Information Bottleneck (IB) method (Tishby etal., 1999; Slonim and Tishby, 2000; Slonim et al,2002) is one popular approach to unsupervised cat-egorization.
The goal of the IB (with ?hard?
clus-tering) is to find clusters such that the mutual in-formation I(X;Z) between words and clusters is aslarge as possible, under a constraint on the numberof clusters.
The procedure for finding the maximiz-ing clustering in (Slonim and Tishby, 2000) is ag-glomerative clustering, while in (Slonim et al, 2002)it is based on many random clusterings, combinedwith a sequential update algorithm, similar to K-means.
The update algorithm re-assigns each datapoint (document) d to its most ?similar?
cluster C,in order to minimize I(X;Z|C ?
{d}), i.e.,?D(P?X|{d}?P?X|{d}?C)+(1??
)D(P?X|C?P?X|{d}?C),where ?
= 1|C|+1 .
This latter procedure is calledthe sequential Information Bottleneck (sIB) method,and is considered the state-of-the-art in unsuper-vised document categorization.2.2 Iterative Denoising TreesDecision trees are a powerful technique for equiva-lence classification, accomplished through a recur-sive successive refinement (Jelinek, 1997).
In thecontext of unsupervised classification, the goal ofdecision trees is to cluster empirical distributions(bags of words) into a given number of classes, witheach class corresponding to a leaf in the tree.
Theyare built top-down (as opposed to the bottom-upconstruction in IB) using maximization of mutualinformation between words and clusters I(X;Z|t)to drive the splitting of each node t; the hope is thateach leaf will contain data points which belong toonly one latent category.Iterative Denoising Trees (also called IntegratedSensing and Processing Decision Trees) were intro-duced in (Priebe et al, 2004a), as an extension ofregular decision trees.
Their main feature is thatthey transform the data at each node, before split-ting, by projecting into a low-dimensional space.This transformation corresponds to feature extrac-tion; different features are suppressed (or ampli-fied) by each transformation, depending on whatdata points fall into each node (corpus-dependent-feature-extraction property (Priebe et al, 2004b)).Thus, dimensionality reduction and clustering arechosen so that they jointly optimize the local objec-tive.In (Karakos et al, 2005), IDTs were used for anunsupervised hyperspectral image segmentation ap-plication.
The objective at each node t was to maxi-mize the mutual information between spectral com-ponents and clusters given the pixels at node t, com-puted from the projected empirical distributions.
Ateach step of the tree-growing procedure, the nodewhich yielded the highest increase in the average,per-node mutual information, was selected for split-ting (until a desired number of leaves was reached).In (Karakos et al, 2007b), the mutual informationobjective was replaced with a parameterized form ofmutual information, namely the Jensen-Re?nyi diver-gence (Hero et al, 2001; Hamza and Krim, 2003), ofwhich more details are provided in the next section.3 Parameterizing Unsupervised ClusteringAs mentioned above, the algorithms considered inthis paper (sIB and IDTs) are unsupervised, in the254sense that they can be applied to test data with-out any need for tuning.
Our procedure of adapt-ing them, based on some supervision on a differenttask instance, is by introducing a parameter into theunsupervised algorithm.
At least for simple cross-instance tuning, this parameter represents the infor-mation which is passed between the supervised andthe unsupervised instances.The parameterizations that we focused on haveto do with the information-theoretic objectives inthe two unsupervised algorithms.
Specifically, fol-lowing (Karakos et al, 2007b), we replace the mu-tual information quantities in IDTs as well as sIBwith the parameterized mutual information mea-sures mentioned above.
These two quantities pro-vide estimates of the dependence between the ran-dom quantities in their arguments, just as the usualmutual information does, but also have a scalar pa-rameter ?
?
(0, 1] that controls the sensitivity of thecomputed dependence on the details of the joint dis-tribution of X and Z.
As a result, the effect of datasparseness on estimation of the joint distribution canbe mitigated when computing these measures.3.1 Jensen-Re?nyi DivergenceThe Jensen-Re?nyi divergence was used in (Hero etal., 2001; Hamza and Krim, 2003) as a measure ofsimilarity for image classification and retrieval.
Fortwo discrete random variables X,Z with distribu-tions PX , PZ and conditional PX|Z , it is defined asI?
(X;Z) = H?(PX)??zPZ(z)H?
(PX|Z(?|z)),(1)where H?(?)
is the Re?nyi entropy, given byH?
(P ) =11?
?log(?x?XP (x)?
), ?
6= 1.
(2)If ?
?
(0, 1), H?
is a concave function, and henceI?
(X;Z) is non-negative (and it is equal to zero ifand only if X and Z are independent).
In the limitas ?
?
1, H?(?)
approaches the Shannon entropy(not an obvious fact), so I?(?)
reduces to the regularmutual information.
Similarly, we defineI?
(X;Z|W ) =?wPW (w)I?
(X;Z|W = w),where I?
(X;Z|W = w) is computed via (1) usingthe conditional distribution of X and Z given W .Except in trivial cases, H?(?)
is strictly largerthan H(?)
when 0 < ?
< 1; this means that the ef-fects of extreme sparsity (few words per document,or too few occurrences of non-frequent words) onthe estimation of entropy and mutual informationcan be dampened with an appropriate choice of ?.This happens because extreme sparsity in the datayields empirical distributions which lie at, or closeto, the boundary of the probability simplex.
Theentropy of such distributions is usually underesti-mated, compared to the smooth distributions whichgenerate the data.
Re?nyi?s entropy is larger thanShannon?s entropy, especially in those regions closeto the boundary, and can thus provide an estimatewhich is closer to the true entropy.3.2 Csisza?r?s Mutual InformationCsisza?r defined the mutual information of order ?
asIC?
(X;Z) = minQ?zPZ(z)D?(PX|Z(?|z)?Q(?
)),(3)where D?(???)
is the Re?nyi divergence (Csisza?r,1995).
It was shown that IC?
(X;Z) retains mostof the properties of I(X;Z)?it is a non-negative,continuous, and concave function of PX , it is con-vex in PX|Z for ?
< 1, and converges to I(X;Z) as?
?
1.Notably, IC?
(X;Z) ?
I(X;Z) for 0 < ?
< 1;this means, as above, that ?
regulates the overesti-mation of mutual information that may result fromdata sparseness.There is no analytic form for the minimizer of theright-hand-side of (3) (Csisza?r, 1995), but it may becomputed via an alternating minimization algorithm(Karakos et al, 2007a).4 Experimental Methods and ResultsWe demonstrate the feasibility of cross-instance tun-ing with experiments on unsupervised document cat-egorization from the 20 Newsgroups corpus (Lang,1995); this corpus consists of roughly 20,000 newsarticles, evenly divided among 20 Usenet groups.Random samples of 500 articles each were chosenby (Slonim et al, 2002) to create multiple test col-lections: 250 each from 2 arbitrarily chosen Usenet255groups for the Binary test collection, 100 articleseach from 5 groups for the Multi5 test collection,and 50 each from 10 groups for the Multi10 test col-lection.
Three independent test collections of eachkind (Binary, Multi5 and Multi10) were created, fora total of 9 collections.
The sIB method was used toseparately cluster each collection, given the correctnumber of clusters.A comparison of sIB and IDTs on the same 9 testcollections was reported in (Karakos et al, 2007b;Karakos et al, 2007a).
Matlab code from (Slonim,2003) was used for the sIB experiments, while theparameterized mutual information measures of Sec-tion 3 were used for the IDTs.
A comparison wasalso made with the EM-based Gaussian mixturesclustering tool mclust (Fraley and Raftery, 1999),and with a simple K-means algorithm.
Since thetwo latter techniques gave uniformly worse cluster-ings than those of sIB and IDTs, we omit them fromthe following discussion.To show that our methods work beyond the 9 par-ticular 500-document collections described above,in this paper we instead use five different randomlysampled test collections for each of the Binary,Multi5 and Multi10 cases, making for a total of 15new test collections in this paper.
For diversity, weensure that none of the five test collections (in eachcase) contain any documents used in the three col-lections of (Slonim et al, 2002) (for the same case).We pre-process the documents of each test col-lection using the procedure2 mentioned in (Karakoset al, 2007b).
The 15 test collections are thenconverted to feature matrices?term-document fre-quency matrices for sIB, and discounted tf/idf ma-trices (according to the Okapi formula (Gatford etal., 1995)) for IDTs?with each row of a matrix rep-resenting one document in that test collection.2Excluding the subject line, the header of each abstract isremoved.
Stop-words such as a, the, is, etc.
are removed, andstemming is performed (e.g., common suffixes such as -ing, -er, -ed, etc., are removed).
Also, all numbers are collapsedto one symbol, and non-alphanumeric sequences are convertedto whitespace.
Moreover, as suggested in (Yang and Pedersen,1997) as an effective method for reducing the dimensionality ofthe feature space (number of distinct words), all words whichoccur fewer than t times in the corpus are removed.
For thesIB experiments, we use t = 2 (as was done in (Slonim et al,2002)), while for the IDT experiments we use t = 3; thesechoices result in the best performance for each method, respec-tively, on another dataset.4.1 Selecting ?
with ?Strapping?In order to pick the value of the parameter ?
foreach of the sIB and IDT test experiments, we use?strapping?
(Eisner and Karakos, 2005), which, aswe mentioned earlier, is a technique for training ameta-classifier that chooses among possible cluster-ings.
The training is based on unrelated instances ofthe same clustering task.
The final choice of cluster-ing is still unsupervised, since no labels (or groundtruth, in general) for the instance of interest are used.Here, our collection of possible clusterings foreach test collection is generated by varying the ?
pa-rameter.
Strapping does not care, however, how thecollection was generated.
(In the original strappingpaper, for example, Eisner and Karakos (2005) gen-erated their collection by bootstrapping word-senseclassifiers from 200 different seeds.
)Here is how we choose a particular unsupervised?-clustering to output for a given test collection:?
We cluster the test collection (e.g., the first Multi5collection) with various values of ?, namely ?
=0.1, 0.2, .
.
.
, 1.0.?
We compute a feature vector from each of theclusterings.
Note that the features are computedfrom only the clusterings and the data points,since no labels are available.?
Based on the feature vectors, we predict the?goodness?
of each clustering, and return the?best?
one.How do we predict the ?goodness?
of a cluster-ing?
By first learning to distinguish good cluster-ings from bad ones, by using unrelated instances ofthe task on which we know the true labels:?
We cluster some unrelated datasets with variousvalues of ?, just as we will do in the test condi-tion.?
We evaluate each of the resulting clusterings us-ing the true labels on its dataset.3?
We train a ?meta-classifier?
that predicts the truerank (or accuracy) of each clustering based on thefeature vector of the clustering.3To evaluate a clustering, one only really needs the true la-bels on a sample of the dataset, although in our experiments wedid have true labels on the entire dataset.256Specifically, for each task (Binary, Multi5, andMulti10) and each clustering method (sIB and IDT),a meta-classifier is learned thus:?
We obtain 10 clusterings (?
= 0.1, 0.2, .
.
.
, 1.0)for each of 5 unrelated task instances (datasetswhose construction is described below).?
For each of these 50 clusterings, we compute thefollowing 14 features: (i) One minus the aver-age cosine of the angle (in tf/idf space) betweeneach example and the centroid of the cluster towhich it belongs.
(ii) The average Re?nyi diver-gence, computed for parameters 1.0, 0.5, 0.1, be-tween the empirical distribution of each exampleand the centroid of the cluster to which it belongs.
(iii) We create 10 more features, one per ?.
Forthe ?
used in this clustering, the feature value isequal to e?0.1r?, where r?
is the average rank of theclustering (i.e., the average of the 4 ranks result-ing from sorting all 10 clusterings (per trainingexample) according to one of the 4 features in (i)and (ii)).
For all other ?
?s, the feature is set tozero.
Thus, only ?
?s which yield relatively goodrankings can have non-zero features in the model.?
We normalize each group of 10 feature vectors,translating and scaling each of the 14 dimensionsto make it range from 0 to 1.
(We will do the sameat test time.)?
We train ranking SVMs (Joachims, 2002), witha Gaussian kernel, to learn how to rank these 50clusterings given their respective normalized fea-ture vectors.
The values of c, ?
(which controlregularization and the Gaussian kernel) were op-timized through leave-one-out cross validation inorder to maximize the average accuracy of thetop-ranked clustering, over the 5 training sets.Once a local maximum of the average accuracywas obtained, further tuning of c, ?
to maximizethe Spearman rank correlation between the pre-dicted and true ranks was performed.A model trained in this way knows somethingabout the task, and may work well for many new,unseen instances of the task.
However, we pre-sume that it will work best on a given test instanceif trained on similar instances.
The ideal would beto match the test collection in every aspect: (i) thenumber of training labels should be equal to thenumber of desired clusters of the test collection; (ii)the training clusters should be topically similar tothe desired test clusters.In our scenario, we enjoy the luxury of plentyof labeled data that can be used to create similarinstances.
Thus, given a test collection A to beclustered into L clusters, we create similar train-ing sets by identifying the L training newsgroupswhose centroids in tf/idf space (using the Okapi for-mula mentioned earlier) have the smallest angle tothe centroid of A.4 (Of course, we exclude news-groups that appear in A.)
We then form a supervised500-document training set A?
by randomly choosing500/L documents from each of these L newsgroups;we do this 5 times to obtain 5 supervised trainingsets.Table 1 shows averaged classification errors re-sulting from strapping (?str?
rows) for the Jensen-Re?nyi divergence and Csisza?r?s mutual information,used within IDTs and sIB, respectively.
(We alsotried the reverse, using Jensen-Re?nyi in sIB andCsisza?r?s in IDTs, but the results were uniformlyworse in the former case and no better in the lattercase.)
The ?MI?
rows show the classification errorsof the untuned algorithms (?
= 1), which, in almostall cases, are worse than the tuned ones.4.2 Tuning ?
on Statistically Similar ExamplesWe now show that strapping outperforms a simplerand more obvious method for cross-instance tun-ing.
To cluster a test collection A, we could simplytune the clustering algorithm by choosing the ?
thatworks best on a related task instance.We again take care to construct a training instanceA?
that is closely related to the test instance A. Infact, we take even greater care this time.
Given A,4For each of the Binary collections, the closest trainingnewsgroups in our experiments were talk.politics.guns,talk.religion.misc; for each of the Multi5 collectionsthe closest newsgroups were sci.electronics, rec.autos,sci.med, talk.politics.misc, talk.religion.misc, and forthe Multi10 collections they were talk.politics.misc,rec.motorcycles, talk.religion.misc, comp.graphics,comp.sys.ibm.pc.hardware, rec.sport.baseball, comp.os.ms-windows.misc, comp.windows.x, soc.religion.christian,talk.politics.mideast.
Note that each of the Binary testcollections happens to be closest to the same two trainingnewsgroups; a similar behavior was observed for the Multi5and Multi10 newsgroups.257PPPPPPPPMethodSet Binary Multi5 Multi10IDTsMI 11.3% 9.9% 42.2%I?
(str) 10.4% 9.2% 39.0%I?
(rls) 10.1% 10.4% 42.7%sIBMI 12.0% 6.8% 38.5%IC?
(str) 11.2% 6.9% 35.8%IC?
(rls) 11.1% 7.4% 37.4%Table 1: Average classification errors for IDTs andsIB, using strapping (?str?
rows) and regularizedleast squares (?rls?
rows) to pick ?
in Jensen-Re?nyidivergence and Csisza?r?s mutual information.
Rows?MI?
show the errors resulting from the untuned al-gorithms, which use the regular mutual informationobjective (?
= 1).
Results which are better than thecorresponding ?MI?
results are shown in bold.we identify the same set of L closest newsgroups asdescribed above.
This time, however, we carefullyselect |A|/L documents from each newsgroup ratherthan randomly choosing 500/L of them.
Specifi-cally, for each test example (document) X ?
A, weadd a similar training example X ?
into A?, chosen asfollows:We associate each test example X to the mostsimilar of the L training newsgroups, under a con-straint that only |A|/L training examples may be as-sociated to each newsgroup.
To do this, we iteratethrough all pairs (X,G) where X is a test exampleand G is a training newsgroup, in increasing orderby the angle between X and G. If X is not yet asso-ciated and G is not yet ?full,?
then we associate Xwith G, and choose X ?
to be the document in G withthe smallest angle to X .We cluster A?
10 times, for ?
= 0.1, .
.
.
, 1.0,and we collect supervised error results E(?
), ?
?
{0.1, .
.
.
, 1.0}.
Now, instead of using the single best??
= argmin?
E(?)
to cluster A (which may re-sult in overfitting) we use regularized least-squares(RLS) (Hastie et al, 2001), where we try to approx-imate the probability that an ?
is the best.
The esti-mated probabilities are given byp?
= K(?I+K)?1p,where I is the unit matrix, p is the training prob-ability of the best ?
(i.e., it is 1 at the position of??
and zero elsewhere), and K is the kernel matrix,where K(i, j) = exp(?
(E(?i) ?
E(?j))2/?2) isthe value of the kernel which expresses the ?sim-ilarity?
between two clusterings of the same train-ing dataset, in terms of their errors.
The parame-ters ?, ?
are set to 0.5, 0.1, respectively, after per-forming a (local) maximization of the Spearman cor-relation between training accuracies and predictedprobabilities p?, for all 15 training instances.
Af-ter performing a linear normalization of p?
to makeit a probability vector, the average predicted valueof ?, i.e., ??
=?10i=1 p?i ?i, (rounded-off to one of{0.1, .
.
.
, 1.0}) is used to cluster A.Table 1 shows the average classification error re-sults using RLS (?rls?
rows).
We can see that, onaverage over the 15 test instances, the error rate ofthe tuned IDTs and sIB algorithms is lower than thatof the untuned algorithms, so cross-instance tuningwas effective.
On the other hand, the errors aregenerally higher than that of the strapping method,which examines the results of using different ?
val-ues on A.5 Concluding RemarksWe have considered the problem of cross-instancetuning of two unsupervised document clustering al-gorithms, through the introduction of a degree offreedom into their mutual information objective.This degree of freedom is tuned using labeled doc-ument collections (which are unrelated to the testcollections); we explored two approaches for per-forming the tuning: (i) through a judicious samplingof training data, to match the marginal statistics ofthe test data, and (ii) via ?strapping?, which trains ameta-classifier to distinguish between good and badclusterings.
Our unsupervised categorization exper-iments from the ?20 Newsgroups?
corpus indicatethat, although both approaches improve the base-line algorithms, ?strapping?
is clearly a better choicefor knowledge transfer between unrelated task in-stances.ReferencesR.
K. Ando and T. Zhang.
2005.
A framework forlearning predictive structures from multiple tasksand unlabeled data.
Journal of Machine LearningResearch, 6:1817?1853, Nov.258S.
Ben-David and R. Schuller.
2003.
Exploiting taskrelatedness for multiple task learning.
In Proc.
ofthe Sixteenth Annual Conference on Learning Theory(COLT-03).A.
Blum and T. Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In Proceedingsof the Workshop on Computational Learning Theory(COLT-98), pages 92?100.R.
Caruana.
1997.
Multitask learning.
Machine Learn-ing, 28(1):41?75.T.
Cover and J. Thomas.
1991.
Elements of InformationTheory.
John Wiley and Sons.I.
Csisza?r.
1995.
Generalized cutoff rates and Re?nyi?sinformation measures.
IEEE Trans.
on InformationTheory, 41(1):26?34, January.I.
Dhillon, S. Mallela, and R. Kumar.
2003.
A divisiveinformation-theoretic feature clustering algorithmfor text classification.
Journal of Machine LearningResearch (JMLR), Special Issue on Variable andFeature Selection, pages 1265?1287, March.J.
Eisner and D. Karakos.
2005.
Bootstrapping withoutthe boot.
In Proc.
2005 Conference on HumanLanguage Technology / Empirical Methods in NaturalLanguage Processing (HLT/EMNLP 2005), October.T.
Evgeniou and M. Pontil.
2004.
Regularized multi-tasklearning.
In Proc.
Knowledge Discovery and DataMining.C.
Fraley and A. E. Raftery.
1999.
Mclust: Software formodel-based cluster analysis.
Journal on Classifica-tion, 16:297?306.M.
Gatford, M. M. Hancock-Beaulieu, S. Jones,S.
Walker, and S. E. Robertson.
1995.
Okapi atTREC-3.
In The Third Text Retrieval Conference(TREC-3), pages 109?126.A.
Ben Hamza and H. Krim.
2003.
Jensen-Re?nyidivergence measure: Theoretical and computationalperspectives.
In Proc.
IEEE Int.
Symp.
on InformationTheory, Yokohama, Japan, June.T.
Hastie, R. Tibshirani, and J. Friedman.
2001.
TheElements of Statistical Learning.
Springer-Verlag.A.
O.
Hero, B. Ma, O. Michel, and J. Gorman.
2001.Alpha-divergence for classification, indexing andretrieval.
Technical Report CSPL-328, University ofMichigan Ann Arbor, Communications and SignalProcessing Laboratory, May.F.
Jelinek.
1997.
Statistical Methods for Speech Recog-nition.
MIT Press.T.
Joachims.
2002.
Optimizing search engines usingclickthrough data.
In ACM Conf.
on KnowledgeDiscovery and Data Mining (KDD).D.
Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.2005.
Unsupervised classification via decision trees:An information-theoretic perspective.
In Proc.
2005International Conference on Acoustics, Speech andSignal Processing (ICASSP 2005), March.D.
Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.2007a.
Information-theoretic aspects of iterativedenoising.
Submitted to the Journal of MachineLearning Research, February.D.
Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.2007b.
Iterative denoising using Jensen-Re?nyi diver-gences with an application to unsupervised documentcategorization.
In Proc.
2007 International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP 2007), April.K.
Lang.
1995.
Learning to filter netnews.
In Proc.
13thInt.
Conf.
on Machine Learning, pages 331?339.David D. Lewis and Philip J. Hayes.
1994.
Guesteditorial.
ACM Transactions on Information Systems,12(3):231, July.C.
E. Priebe, D. J. Marchette, and D. M. Healy.2004a.
Integrated sensing and processing decisiontrees.
IEEE Trans.
on Pat.
Anal.
and Mach.
Intel.,26(6):699?708, June.C.
E. Priebe, D. J. Marchette, Y.
Park, E. Wegman,J.
Solka, D. Socolinsky, D. Karakos, K. Church,R.
Guglielmi, R. Coifman, D. Lin, D. Healy, M. Ja-cobs, and A. Tsao.
2004b.
Iterative denoising forcross-corpus discovery.
In Proc.
2004 InternationalSymposium on Computational Statistics (COMPSTAT2004), August.N.
Slonim and N. Tishby.
2000.
Document clusteringusing word clusters via the information bottleneckmethod.
In Research and Development in InformationRetrieval, pages 208?215.N.
Slonim, N. Friedman, and N. Tishby.
2002.
Un-supervised document classification using sequentialinformation maximization.
In Proc.
SIGIR?02, 25thACM Int.
Conf.
on Research and Development ofInform.
Retrieval.N.
Slonim.
2003.
IBA 1.0: Matlab code for informationbottleneck clustering algorithms.
Available fromhttp://www.princeton.edu/?nslonim/IB Release1.0/IB Release1 0.tar.N.
Tishby, F. Pereira, and W. Bialek.
1999.
The informa-tion bottleneck method.
In 37th Allerton Conferenceon Communication and Computation.K.
Torkkola.
2002.
On feature extraction by mutual in-formation maximization.
In Proc.
IEEE Int.
Conf.
onAcoustics, Speech and Signal Proc.
(ICASSP-2002),May.Y.
Yang and J. Pedersen.
1997.
A comparative study onfeature selection in text categorization.
In Intl.
Conf.on Machine Learning (ICML-97), pages 412?420.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proc.
33rdAnnual Meeting of the Association for ComputationalLinguistics, pages 189?196, Cambridge, MA.259
