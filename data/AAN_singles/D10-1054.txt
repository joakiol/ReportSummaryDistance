Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 555?563,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsMaximum Entropy Based Phrase Reorderingfor Hierarchical Phrase-based TranslationZhongjun He Yao Meng Hao YuFujitsu R&D Center CO., LTD.15/F, Tower A, Ocean International Center, 56 Dongsihuan Zhong Rd.Chaoyang District, Beijing, 100025, China{hezhongjun, mengyao, yu}@cn.fujitsu.comAbstractHierarchical phrase-based (HPB) translationprovides a powerful mechanism to captureboth short and long distance phrase reorder-ings.
However, the phrase reorderings lack ofcontextual information in conventional HPBsystems.
This paper proposes a context-dependent phrase reordering approach thatuses the maximum entropy (MaxEnt) modelto help the HPB decoder select appropriate re-ordering patterns.
We classify translation rulesinto several reordering patterns, and build aMaxEnt model for each pattern based on var-ious contextual features.
We integrate theMaxEnt models into the HPB model.
Ex-perimental results show that our approachachieves significant improvements over a stan-dard HPB system on large-scale translationtasks.
On Chinese-to-English translation,the absolute improvements in BLEU (case-insensitive) range from 1.2 to 2.1.1 IntroductionThe hierarchical phrase-based (HPB) model (Chi-ang, 2005; Chiang, 2007) has been widely adoptedin statistical machine translation (SMT).
It utilizessynchronous context free grammar (SCFG) rulesto perform translation.
Typically, there are threetypes of rules (see Table 1): phrasal rule, a phrasepair consisting of consecutive words; hierarchicalrule, a hierarchical phrase pair consisting of bothwords and variables; and glue rule, which is used tomerge phrases serially.
Phrasal rule captures shortdistance reorderings within phrases, while hierar-chical rule captures long distance reorderings be-Type Constituent ExamplesWord VariablePR?- X ?
??
?, one of?HR?
?X ?
?X, ofX?GR -?S ?
?SX, SX?Table 1: A classification of grammar rules for the HPBmodel.
PR = phrasal rule, HR = hierarchical rule, GR =glue rule.tween phrases.
Therefore, the HPB model outper-forms conventional phrase-based models on phrasereorderings.However, HPB translation suffers from a limita-tion, in that the phrase reorderings lack of contex-tual information, such as the surrounding words ofa phrase and the content of sub-phrases that rep-resented by variables.
Consider the following twohierarchical rules in translating a Chinese sentenceinto English:X ?
?X1  X2, X1 ?s X2?
(1)X ?
?X1  X2, X2X1?
(2)?
?d  !with Russia ?s talkstalks with RussiaBoth pattern-match the source sentence, but pro-duce quite different phrase reorderings.
The firstrule generates a monotone translation, while the sec-ond rule swaps the source phrases covered by X1and X2 on the target side.
During decoding, the first555rule is more likely to be used, as it occurs more fre-quently in a training corpus.
However, the exam-ple is not a noun possessive case because the sub-phrase covered by X1 is not a noun but a preposi-tional phrase.
Thus, without considering informa-tion of sub-phrases, the decoder may make errors onphrase reordering.Contextual information has been widely used toimprove translation performance.
It is helpful to re-duce ambiguity, thus guide the decoder to choosecorrect translation for a source text.
Several re-searchers observed that word sense disambiguationimproves translation quality on lexical translation(Carpuat and Wu, 2007; Chan et al, 2007).
Thesemethods utilized contextual features to determinethe correct meaning of a source word, thus help anSMT system choose an appropriate target transla-tion.Zens and Ney (2006) and Xiong et al (2006)utilized contextual information to improve phrasereordering.
They addressed phrase reordering asa two-class classification problem that translatingneighboring phrases serially or inversely.
They builta maximum entropy (MaxEnt) classifier based onboundary words to predict the order of neighboringphrases.He et al (2008) presented a lexicalized rule selec-tion model to improve both lexical translation andphrase reordering for HPB translation.
They builta MaxEnt model for each ambiguous source sidebased on contextual features.
The method was alsosuccessfully applied to improve syntax-based SMTtranslation (Liu et al, 2008), using more sophisti-cated syntactical features.
Shen et al (2008) inte-grated various contextual and linguistic features intoan HPB system, using surrounding words and de-pendency information for building context and de-pendency language models, respectively.In this paper, we focus on improving phrase re-ordering for HPB translation.
We classify SCFGrules into several reordering patterns consisting oftwo variables X and F (or E) 1, such as X1FX2and X2EX1.
We treat phrase reordering as a classi-fication problem and build a MaxEnt model for eachsource reordering pattern based on various contex-1We use F and E to represent source and target words, re-spectively.tual features.
We propose a method to integrate theMaxEnt models into an HPB system.
Specifically:?
For hierarchical rules, we classify the source-side and the target-side into 7 and 17 reorderingpatterns, respectively.
Target reordering pat-terns are treated as possible labels.
We thenbuild a classifier for each source pattern to pre-dict phrase reorderings.
This is different fromHe et al (2008), in which they built a clas-sifier for each ambiguous hierarchical source-side.
Therefore, the training examples for eachMaxEnt model is small and the model maybeunstable.
Here, we classify source hierarchicalphrases into 7 reordering patterns according tothe arrangement of words and variables.
Wecan obtain sufficient samples for each MaxEntmodel from large-scale bilingual corpus.?
For glue rules, we extend the HPB model byusing bracketing transduction grammar (BTG)(Wu, 1996) instead of the monotone glue rule.By doing this, there are two options for the de-coder to merge phrases: serial or inverse.
Wethen build a classifier for glue rules to predictreorderings of neighboring phrases, analogousto Xiong et al (2006).?
We integrate the MaxEnt based phrase reorder-ing models as features into the HPB model(Chiang, 2005).
The feature weights can betuned together with other feature functions byMERT algorithm (Och, 2003).Experimental results show that the presented methodachieves significant improvement over the baseline.On Chinese-to-English translation tasks of NISTevluation, improvements in BLEU (case-insensitive)are 1.2 on MT06 GALE set, 1.8 on MT06 NIST set,and 2.1 on MT08.The rest of the paper is structured as follows: Sec-tion 2 describes the MaxEnt based phrase reorder-ing method.
Section 3 integrates the MaxEnt mod-els into the translation model.
In Section 4, we re-port experimental results.
We analyze the presentedmethod and experimental results in Section 5 andconclude in Section 6.556Source phrase Target phraseX andX ?
with Xbetween X andFigure 1: A source hierarchical phrase and its corre-sponding target translation.2 MaxEnt based Phrase ReorderingWe regard phrase reordering as a pattern classifica-tion problem.
A reordering pattern indicates an ar-rangement of words and variables.
Although thereare a large amount of hierarchical rules may be ex-tracted from bilingual corpus, these rules can beclassified into several reordering patterns (Section2.1).
In addition, we extend the HPB model withBTG, that adding an inverted glue rule to mergephrases inversely (Section 2.2).
Therefore, the gluerules are classified into two patterns: serial or in-verse.
We then build a MaxEnt phrase reordering(MEPR) classifier for each source reordering pattern(Section 2.3).
In Section 2.4, we describe contextualfeatures.2.1 Reordering Pattern Classification forHierarchical RuleHierarchical rule, consisting of both words and vari-ables, is of great importance for the HPB model.During decoding, words are used for lexical trans-lation, and variables capture phrase reordering.
Wemay learn millions of hierarchical rules from a bilin-gual corpus.
Although these rules are different fromeach other, they can be classified into several re-ordering patterns according to the arrangement ofvariables and words.In this paper, we follow the constraint as de-scribed in (Chiang, 2005) that a hierarchical rulecan have at most two variables and they cannot beadjacent on the source side.
We use ?X?
to rep-resent the variable, and ?F ?
and ?E?
to representword strings in source and target language, respec-tively.
Therefore, in a hierarchical rule, E is the lex-ical translation of F , while the order of X and Econtains phrase reordering information.For the hierarchical rule that contains one vari-able (see Figure 1 for example), both the source andthe target phrases can be classified into three pat-Source pattern Target patternXF XEFX EXFXF EXETable 2: A classification of the source side and the targetside for the hierarchical rule that contains one variable.Source pattern Target patternX1EX2X2EX1X1X2EX2X1EEX1X2X1FX2 EX2X1X1FX2F X1EX2EFX1FX2 X2EX1EFX1FX2F EX1X2EEX2X1EEX1EX2EX2EX1EX1EX2EEX2EX1ETable 3: A classification of the source side and the targetside for the hierarchical rule that contains two variables.terns (Table 2).
To reduce the complexity of clas-sification, we do not distinguish the order of wordstrings.
For example, we consider ?e1Xe2?
and?e2Xe1?
as the same pattern ?EXE?, because thetarget words are determined by lexical translation ofsource words.
Our focus is the order between X andE.
During decoding the phrases covered by X aredynamically changed and the contextual informationof these phrases is ignored for pattern-matching ofhierarchical rules.Analogously, for the hierarchical rule that con-tains two variables, the source phrases are classifiedinto 4 patterns, while the target phrases are classifiedinto 14 patterns, as shown in Table 3.
The patternnumber on the source side is less than that on thetarget side, because on the source side, ?X1?
alwaysappears before ?X2?, and they cannot be adjacent.5572.2 Reordering Pattern Classification for GlueRuleThe HPB model used glue rule to combine phrasesserially.
The reason is that in some cases, there areno valid translation rules that cover a source span.Therefore, the glue rule provides a default monotonecombination of phrases in order to complete a trans-lation.
This is not sufficient because in certain cases,the order of phrases may be inverted on the target-side.In this paper, we extend the glue rule with BTG(Wu, 1996), which consists of three types of rules:X ?
?f?
, e??
(3)X ?
?X1X2, X1X2?
(4)X ?
?X1X2, X2X1?
(5)Rule 3 is a phrasal rule that translates a sourcephrase f?
into a target phrase e?.
Rule 4 merges twoconsecutive phrases in monotone order, while Rule5 merges them in inverted order.
During decod-ing, the decoder first uses Rule 3 to produce phrasetranslation, and then iteratively uses Rule 4 and 5 tomerge two neighboring phrases into a larger phraseuntil the whole sentence is covered.We replace the original glue rules in the HPBmodel with BTG rules (see Table 4).
We believethat the extended HPB model can benefit from BTGin the following aspects:?
In the HPB model, as we mentioned, hierarchi-cal rules are constrained in that nonterminalscannot be adjacent on the source side, i.e., thesource side cannot contain ?X1X2?.
One rea-son is that it will heavily increase the rule tablesize.
The other reason is that it can cause a spu-rious ambiguity problem (Chiang, 2005).
Theinverted glue rule in BTG, however, can solvethis problem.?
In the HPB model, only a monotone glue ruleis provided to merge phrases serially.
In the ex-tended HPB model, the combination of phrasesis classified into two types: monotone and in-verse.Analogous to Xiong et al (2006), to performcontext-dependent phrase reordering, we build aGlue Rule Extended Glue RuleS ?
?X,X?
S ?
?X,X?S ?
?SX, SX?
X ?
?X1X2, X1X2?- X ?
?X1X2, X2X1?Table 4: Extending the glue rules in the HPB model withBTG.MaxEnt based classifier for glue rules to predict theorder of two neighboring phrases.
In this paper, weutilize more contextual features.2.3 The MaxEnt based Phrase ReorderingClassifierAs described above, we classified phrase reorderingsinto several patterns.
Therefore, phrase reorderingcan be regarded as a classification problem: for eachsource reordering pattern, we treat the correspond-ing target reordering patterns as labels.We build a general classification model within theMaxEnt framework:Pme(T?
|T?, ?, ?)
=exp(?i ?ihi(?, ?, f(X), e(X))?T?
exp(?i ?ihi(?, ?, f(X), e(X))(6)where, ?
and ?
are the source and target side, re-spectively.
T?/T?
is the reordering pattern of ?/?.f(X) and e(X) are the phrases that covered by Xone the source and target side, respectively.
Givena source phrase, the model predicts a target reorder-ing pattern, considering various contextual features(Section 2.4).According to the classification of reordering pat-terns, there are 3 kinds of classifiers:?
P hr1me includes 3 classifiers for the hierarchicalrules that contain 1 variable.
Each of the clas-sifier has 3 labels;?
P hr2me includes 4 classifiers for the hierarchicalrules that contain 2 variables.
Each of the clas-sifier has 14 labels;?
P grme includes 1 classifier for the glue rules.
Theclassifier has 2 labels that predict a monotoneor inverse order for two neighboring phrases.This classifier is analogous to (Xiong et al,2006).558There are 8 classifiers in total.
This is much fewerthan the classifiers in He et al (2008), in which aclassifier was built for each ambiguous hierarchicalsource side.
In this way, a classifier may face therisk that there are not enough samples for training astable MaxEnt model.
While our approach is moregeneric, rather than training a MaxEnt model for aspecific hierarchical source side, we train a modelfor a source reordering pattern.
Thus, we reduce thenumber of classifiers and can extract large trainingexamples for each classifier.2.4 Feature definitionFor a reordering pattern pair ?T?, T?
?, we designthree feature functions for phrase reordering classi-fiers:?
Source lexical feature, including boundarywords and neighboring words.
Boundarywords are the left and right word of the sourcephrases covered by f(X), while neighboringwords are the words that immediately to the leftand right of a source phrase f(?);?
Part-of-Speech (POS) feature, POS tags of theboundary and neighboring words on the sourceside.?
Target lexical feature, the boundary words ofthe target phrases covered by e(X).These features can be extracted together withtranslation rules from bilingual corpus.
However,since the hierarchical rule does not allow for adja-cent variables on the source side, we extract featuresfor P grme by using the method described in Xiong etal.
(2006).
We train the classifiers with a MaxEnttrainer (Zhang, 2004).3 Integrating the MEPR Classifier into theHPB ModelThe HPB model is built within the standard log-linear framework (Och and Ney, 2002):Pr(e|f) ?
?i?ihi(?, ?)
(7)where hi(?, ?)
is a feature function and ?i is theweight of hi.
The HPB model has the following fea-tures: translation probabilities p(?|?)
and p(?|?
),lexical weights pw(?|?)
and pw(?|?
), word penalty,phrase penalty, glue rule penalty, and a target n-gram language model.To integrate the MEPR classifiers into the transla-tion model, the features of the log-linear model arechanged as follows:?
We add the MEPR classifier as a feature func-tion to predict reordering pattern:hme(T?
|T?)
=?Pme(T?
|T?, ?, ?)
(8)During decoding, we first classify each sourcephrase into one of the 8 source reordering pat-terns and then use the corresponding MEPRclassifier to predict the possible target reorder-ing pattern.
Therefore, the contextual informa-tion guides the decoder to perform phrase re-ordering.?
We split the ?glue rule penalty?
into two fea-tures: monotone glue rule number and invertedglue rule number.
These features reflect pref-erence of the decoder for using monotone orinverted glue rules.The advantage of our extension method is that theweights of the new features can be tuned togetherwith the other features by MERT algorithm (Och,2003).We utilize a standard CKY algorithm for decod-ing.
Given a source sentence, the decoder searchesthe best derivation from the bottom to top.
For asource span [j1, j2], the decoder uses three kinds ofrules: translation rules produce lexical translationand phrase reordering (for hierarchical rules), mono-tone rule merges any neighboring sub-spans [j1, k]and [k + 1, j2] serially, and inverted rule swap them.Note that when the decoder uses the monotone andinverted glue rule to combine sub-spans, it mergesphrases that do not contain variables.
Because theCKY algorithm guarantees that the sub spans [j1, k]and [k + 1, j2] have been translated before [j1, j2].5594 ExperimentsWe carried out experiments on four systems:?
HPB: replication of the Hiero system (Chiang,2005);?
HPB+MEHR: HPB with MaxEnt based classi-fier for hierarchical rules, as described in Sec-tion 2.1;?
HPB+MEGR: HPB with MaxEnt based classi-fier for glue rules, as described in Section 2.2;?
HPB+MER: HPB with MaxEnt based classifierfor both hierarchical and glue rules.All systems were tuned on NIST MT03 and testedon MT06 and MT08.
The evaluation metric wasBLEU (Papineni et al, 2002) with case-insensitivematching of n-grams, where n = 4.We evaluated our approach on Chinese-to-English translation.
The training data contained77M Chinese words and 81M English words.These data come from 17 corpora: LDC2002E18,LDC2002L27, LDC2002T01, LDC2003E07,LDC2003E14, LDC2004T07, LDC2005E83,LDC2005T06, LDC2005T10, LDC2005T34,LDC2006E24, LDC2006E26, LDC2006E34,LDC2006E86, LDC2006E92, LDC2006E93,LDC2004T08 (HK News, HK Hansards).To obtain word alignments, we first ran GIZA++(Och and Ney, 2000) in both translation directionsand then refined the results using the ?grow-diag-final?
method (Koehn et al, 2003).
For the lan-guage model, we used the SRI Language ModelingToolkit (Stolcke, 2002) to train two 4-gram modelson the Xinhua portion of the GigaWord corpus andthe English side of the training corpus.4.1 Statistical Information of RulesHierarchical RulesWe extracted 162M translation rules from the train-ing corpus.
Among them, there were 127M hi-erarchical rules, which contained 85M hierarchicalsource phrases.
We classified these source phrasesinto 7 patterns as described in Section 2.1.
Table5 shows the statistical information.
We observedthat the most frequent source pattern is ?FXF ?,Source Pattern Percentage (%)XF 9.7FX 9.7FXF 46.1X1FX2 3.7X1FX2F 11.9FX1FX2 11.8FX1FX2F 7.1Table 5: Statistical information of reordering pattern clas-sification for hierarchical source phrases.# SourceTarget (%) FX XF FXFEX 82.8 7 4.6XE 6.4 82.4 2.9EXE 10.8 10.6 92.5Table 6: Percentage of target reordering pattern for eachsource pattern containing one variable.which accounted for 46.1% of the total.
Interest-ingly, ?X1FX2?, accounting for 3.7%, was the leastfrequent pattern.
Table 6 and Table 7 show thedistributions of reordering patterns for hierarchicalsource phrases that contain one and two variables,respectively.
From both the tables, we observedthat for Chinese-to-English translation, the most fre-quent ?reordering?
pattern for a source phrase ismonotone translation (bold font in the tables).Glue RulesTo train a MaxEnt classifier for glue rules, we ex-tracted 65.8M reordering (monotone and inverse)instances from the training data, using the algo-rithm described in Xiong et al (2006).
There were63M monotone instances, accounting for 95.7%.
Al-though instances of inverse reordering accounted for4.3%, they are important for phrase reordering.4.2 ResultsTable 8 shows the BLEU scores and decoding speedof the four systems on MT06 (GALE set and NISTset) and MT08.
From the table, we made the follow-ing observations:560# SourceTarget (%) FX1FX2 FX1FX2F X1FX2 X1FX2FEX1EX2 78.1 3.6 4.6 1.2EX1EX2E 2.1 75.9 0.1 1.6EX1X2 6.8 0.1 2.8 0.1EX1X2E 1.8 11.2 0.1 2EX2EX1 2.8 1.4 2 1.2EX2EX1E 1.4 2.3 0.7 1.1EX2X1 0.9 0.1 2.2 0.2EX2X1E 1 1.1 0.9 1.0X1EX2 1.9 0.1 71.2 3.3X1EX2E 0.7 2.1 6 78.4X1X2E 0.1 0.1 2.8 5.9X2EX1 0.9 0.4 1.6 0.7X2EX1E 1.5 1.5 2.6 2.4X2X1E 0.1 0.04 2.2 0.8Table 7: Percentage of target reordering pattern for each source pattern containing two variables.System Test Data Speed06G 06N 08HPB 14.19 33.93 25.85 8.7HPB+MEHR 14.76 34.95 26.56 3.2HPB+MEGR 15.09 35.72 27.34 2.7HPB+MER 15.42 35.80 27.94 1.7Table 8: BLEU percentage scores and translation speed (words/second) on test data.
G=GALE set, N=NIST set.
Allimprovements are statistically significant (p < 0.01).
Note that MT06G has one reference for each source sentence,while the MT06N and MT08 have four references.?
The HPB+MEHR system achieved significantimprovements on all test sets compared to theHPB system.
The absolute increases in BLEUscores ranging from 0.6 (on 06G) to 1.0 (on06N) percentage points.
This indicates that theME based reordering for hierarchical rules im-proves translation performance.?
The HPB+MEGR system achieved significantimprovements over the HPB system.
The ab-solute increases in BLEU scores ranging from0.9 (on 06G) to 1.8 (on 06N) percentage points.The HPB+MEGR system overcomes the short-coming of the HPB system by using bothmonotone glue rule and inverted glue rule,which merging phrases serially and inversely,respectively.
Furthermore, the HPB+MEGRsystem outperformed the HPB+MEHR system.?
The HPB+MER system achieved the best per-formances on all test sets, with absolute in-creases of BLEU scores ranging from 1.2 (on06G) to 2.1 (on 08).
The system combin-ing with ME based reordering for both hier-archical and glue rules, outperformed both theHPB+MEHR and HPB+MEGR systems.?
In addition, we found that the decoder takesmore time after adding the MEPR models (thespeed column of Table 8).
The average transla-tion speed of HPB+MER (1.7 words/second) isabout 5 times slower than the HPB system (8.7words/second).
One reason is that the MEPRmodels utilized contextual information to com-pute classification scores.
Another reason isthat adding inverted glue rules increases searchspace.5615 AnalysisExperiments showed that the presented approachachieved significant gains on BLEU scores.
Further-more, we sought to explore what would happen af-ter integrating the MEPR classifiers into the transla-tion model.
We compared the outputs of HPB andHPB+MER and observed that the translation perfor-mance are improved on phrase reordering.
For ex-ample, the translations of a source sentence in MT08are as follows 2:?
Src: ?I1 ?2 ?
?3 .4 m?5 ?
?6?7 ?m8 J?9 4010 ?11 ?
?12 13 ?14 Oy15?
Ref: At the end4 of last3 month3, theSouth1 Korean1 government2 began5 a plan15to provide9 400,00010 tonnes11 of rice12 asaid14 to North8 Korea8?
HPB: South Korean government late lastmonth to start with 400,000 tons of rice aid tothe DPRK?
HPB+MER: Start at the end of last month,South Korean government plans to provide400,000 tons of rice in aid to the DPRKThe most obvious error that the baseline systemmakes is the order of the time expression ??
?., the end of last month?, which should be eitherat the beginning or the end on target side.
However,the baseline produced a monotone translation by us-ing the rule ?
?I ?
X1, South Korean govern-ment X1?.
The HPB+MER system, however, movedthe time expression to the beginning of the sentenceby using the rule ?
?I ?
X1, X1 South Ko-rean government?.
The reason is that the MaxEntphrase reordering classifier uses the contextual fea-tures (e.g.
the boundary words) of the phrase cov-ered by X1 to predict the phrase reordering as X1Efor the source phrase FX1.2The co-indexes of the words in the source and referencesentence indicate word alignments.6 Conclusions and Future WorkIn this paper, we have proposed a MaxEnt basedphrase reordering approach to help the HPB decoderselect reordering patterns.
We classified hierarchicalrules into 7 reordering patterns on the source sideand 17 reordering patterns on the target side.
In ad-dition, we introduced BTG to enhance the reorder-ing of neighboring phrases and classified the gluerules into two patterns.
We trained a MaxEnt clas-sifier for each reordering pattern and integrated itinto a standard HPB system.
Experimental resultsshowed that the proposed approach achieved signif-icant improvements over the baseline.
The absoluteimprovements in BLEU range from 1.2 to 2.1.MaxEnt based phrase reordering provides a mech-anism to incorporate various features into the trans-lation model.
In this paper, we only use a few fea-ture sets based on standard contextual word and POStags.
We believe that additional features will fur-ther improve translation performance.
Such featurescould include syntactical features (Chiang et al,2009).
In the future, we will carry out experimentson deeper features and evaluate the effects of differ-ent feature sets.ReferencesMarine Carpuat and Dekai Wu.
2007.
Improving statisti-cal machine translation using word sense disambigua-tion.
In Proceedings of EMNLP-CoNLL 2007, pages61?72.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Linguis-tics, pages 33?40.David Chiang, Wei Wang, and Kevin Knight.
2009.11,001 new features for statistical machine transla-tion.
In Proceedings of the North American Chapterof the Association for Computational Linguistics - Hu-man Language Technologies 2009 Conference, page218?226.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics, pages 263?270.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, pages 33(2):201?228.Zhongjun He, Qun Liu, and Shouxun Lin.
2008.
Im-proving statistical machine translation using lexical-562ized rule selection.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics,pages 321?328.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofthe 2003 Conference of the North American Chapter ofthe Association for Computational Linguistics on Hu-man Language Technology, pages 48?54.Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.2008.
Maximum entropy based rule selection modelfor syntax-based statistical machine translation.
InProceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, page89?97.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics, pages 440?447.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 295?302.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics,pages 311?318.Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2008.
Effective use of linguis-tic and contextual information for statistical machinetranslation.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 72?80.Andreas Stolcke.
2002.
SRILM ?
An extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken language Processing,volume 2, pages 901?904.Dekai Wu.
1996.
A polynomial-time algorithm for sta-tistical machine translation.
In Proceedings of theThirty-Fourth Annual Meeting of the Association forComputational Linguistics.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Max-imum entropy based phrase reordering model for sta-tistical machine translation.
In Proceedings of the 44thAnnual Meeting of the Association for ComputationalLinguistics, pages 521?528.Richard Zens and Hermann Ney.
2006.
Discriminativereordering models for statistical machine translation.In Proceedings of the Workshop on Statistical MachineTranslation, pages 55?63.Le Zhang.
2004.
Maximum entropy model-ing toolkit for python and c++.
available athttp://homepages.inf.ed.ac.uk/s0450736/maxent too-lkit.html.563
