Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 118?126,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUnsupervised Syntactic Alignment with Inversion Transduction GrammarsAdam Pauls Dan KleinComputer Science DivisionUniversity of California at Berkeley{adpauls,klein}@cs.berkeley.eduDavid Chiang Kevin KnightInformation Sciences InstituteUniversity of Southern California{chiang,knight}@isi.eduAbstractSyntactic machine translation systems cur-rently use word alignments to infer syntacticcorrespondences between the source and tar-get languages.
Instead, we propose an un-supervised ITG alignment model that directlyaligns syntactic structures.
Our model alignsspans in a source sentence to nodes in a targetparse tree.
We show that our model producessyntactically consistent analyses where possi-ble, while being robust in the face of syntacticdivergence.
Alignment quality and end-to-endtranslation experiments demonstrate that thisconsistency yields higher quality alignmentsthan our baseline.1 IntroductionSyntactic machine translation has advanced signif-icantly in recent years, and multiple variants cur-rently achieve state-of-the-art translation quality.Many of these systems exploit linguistically-derivedsyntactic information either on the target side (Gal-ley et al, 2006), the source side (Huang et al, 2006),or both (Liu et al, 2009).
Still others induce theirsyntax from the data (Chiang, 2005).
Despite differ-ences in detail, the vast majority of syntactic meth-ods share a critical dependence on word alignments.In particular, they infer syntactic correspondencesbetween the source and target languages throughword alignment patterns, sometimes in combinationwith constraints from parser outputs.However, word alignments are not perfect indi-cators of syntactic alignment, and syntactic systemsare very sensitive to word alignment behavior.
Evena single spurious word alignment can invalidate alarge number of otherwise extractable rules, whileunaligned words can result in an exponentially largeset of extractable rules to choose from.
Researchershave worked to incorporate syntactic informationinto word alignments, resulting in improvements toboth alignment quality (Cherry and Lin, 2006; DeN-ero and Klein, 2007), and translation quality (Mayand Knight, 2007; Fossum et al, 2008).In this paper, we remove the dependence on wordalignments and instead directly model the syntacticcorrespondences in the data, in a manner broadlysimilar to Yamada and Knight (2001).
In particu-lar, we propose an unsupervised model that alignsnodes of a parse tree (or forest) in one language tospans of a sentence in another.
Our model is an in-stance of the inversion transduction grammar (ITG)formalism (Wu, 1997), constrained in such a waythat one side of the synchronous derivation respectsa syntactic parse.
Our model is best suited to sys-tems which use source- or target-side trees only.The design of our model is such that, for divergentstructures, a structurally integrated backoff to flatterword-level (or null) analyses is available.
There-fore, our model is empirically robust to the casewhere syntactic divergence between languages pre-vents syntactically accurate ITG derivations.We show that, with appropriate pruning, ourmodel can be efficiently trained on large parallel cor-pora.
When compared to standard word-alignment-backed baselines, our model produces more con-sistent analyses of parallel sentences, leading tohigh-count, high-quality transfer rules.
End-to-end translation experiments demonstrate that thesehigher quality rules improve translation quality by1.0 BLEU over a word-alignment-backed baseline.2 Syntactic Rule ExtractionOur model is intended for use in syntactic transla-tion models which make use of syntactic parses oneither the target (Galley et al, 2006) or source side(Huang et al, 2006; Liu et al, 2006).
Our model?s118SNPDT* NN NNVPVBZADVPRB VBNthe trade surplus has drastically fallen?????????
?tradesurplusdrasticallyfall(past)Figure 1: A single incorrect alignment removes an ex-tractable node, and hence several desirable rules.
Werepresent correct extractable nodes in bold, spurious ex-tractable nodes with a *, and incorrectly blocked ex-tractable nodes in bold strikethrough.chief purpose is to align nodes in the syntactic parsein one language to spans in the other ?
an alignmentwe will refer to as a ?syntactic?
alignment.
Thesealignments are employed by standard syntactic ruleextraction algorithms, for example, the GHKM al-gorithm of Galley et al (2004).
Following that work,we will assume parses are present in the target lan-guage, though our model applies in either direction.Currently, although syntactic systems make use ofsyntactic alignments, these alignments must be in-duced indirectly from word-level alignments.
Pre-vious work has discussed at length the poor interac-tion of word-alignments with syntactic rule extrac-tion (DeNero and Klein, 2007; Fossum et al, 2008).For completeness, we provide a brief example of thisinteraction, but for a more detailed discussion we re-fer the reader to these presentations.2.1 Interaction with Word AlignmentsSyntactic systems begin rule extraction by first iden-tifying, for each node in the target parse tree, aspan of the foreign sentence which (1) contains ev-ery source word that aligns to a target word in theyield of the node and (2) contains no source wordsthat align outside that yield.
Only nodes for whicha non-empty span satisfying (1) and (2) exists mayform the root or leaf of a translation rule; for thatreason, we will refer to these nodes as extractablenodes.Since extractable nodes are inferred based onword alignments, spurious word alignments can ruleout otherwise desirable extraction points.
For exam-ple, consider the alignment in Figure 1.
This align-ment, produced by GIZA++ (Och and Ney, 2003),contains 4 correct alignments (the filled circles),but incorrectly aligns the to the Chinese past tensemarker ?
(the hollow circle).
This mistaken align-ment produces the incorrect rule (DT ?
the ; ?
),and also blocks the extraction of (VBN ?
fallen ;???
).More high-level syntactic transfer rules are alsoruled out, for example, the ?the insertion rule?
(NP?
the NN1 NN2 ; NN1 NN2) and the high-level (S?
NP1 VP2 ; NP1 VP2).3 A Syntactic Alignment ModelThe most common approach to avoiding these prob-lems is to inject knowledge about syntactic con-straints into a word alignment model (Cherry andLin, 2006; DeNero and Klein, 2007; Fossum et al,2008).1 While syntactically aware, these models re-main limited by the word alignment models that un-derly them.Here, we describe a model which directly infersalignments of nodes in the target-language parse treeto spans of the source sentence.
Formally, our modelis an instance of a Synchronous Context-Free Gram-mar (see Chiang (2004) for a review), or SCFG,which generates an English (target) parse tree T andforeign (source) sentence f given a target sentence e.The generative process underlying this model pro-duces a derivation d of SCFG rules, from which Tand f can be read off; because we condition on e,the derivations produce e with probability 1.
Thismodel places a distribution over T and f given byp(T, f | e) =?dp(d | e) =?d?r?dp(r | e)where the sum is over derivations d which yield Tand f .
The SCFG rules r come from one of 4 types,pictured in Table 1.
In general, because our modelcan generate English trees, it permits inference overforests.
Although we will restrict ourselves to a sin-gle parse tree for our experiments, in this section, wediscuss the more general case.1One notable exception is May and Knight (2007), who pro-duces syntactic alignments using syntactic rules derived fromword-aligned data.119Rule Type Root English Foreign Example InstantiationTERMINAL E e ft FOUR ?
four ;?UNARY A B fl B fr CD ?
FOUR ;  FOUR ?BINARYMONO A B C fl B fm C fr NP ?
NN NN ;  NN ?
NN BINARYINV A B C fl C fm B fr PP ?
IN NP ;?
NP  IN Table 1: Types of rules present in the SCFG describing our model, along with some sample instantiations of each type.Empty word sequences f have been explicitly marked with an .The first rule type is the TERMINAL production,which rewrites a terminal symbol2 E as its En-glish word e and a (possibly empty) sequence offoreign words ft. Generally speaking, the majorityof foreign words are generated using this rule.
Itis only when a straightforward word-to-word corre-spondence cannot be found that our model resorts togenerating foreign words elsewhere.We can also rewrite a non-terminal symbol A us-ing a UNARY production, which on the English sideproduces a single symbol B, and on the foreign sideproduces the symbol B, with sequences of words flto its left and fr to its right.Finally, there are two binary productions: BINA-RYMONO rewrites A with two non-terminals B andC on the English side, and the same non-terminalsB and C in monotonic order on the foreign side,with sequences of words fl, fr, and fm to the left,right, and the middle.
BINARYINV inverts the or-der in which the non-terminals B and C are writtenon the source side, allowing our model to capture alarge subset of possible reorderings (Wu, 1997).Derivations from this model have two key prop-erties: first, the English side of a derivation is con-strained to form a valid constituency parse, as is re-quired in a syntax system with target-side syntax;and second, for each parse node in the English pro-jection, there is exactly one (possibly empty) con-tiguous span of the foreign side which was gener-ated from that non-terminal or one of its descen-dants.
Identifying extractable nodes from a deriva-tion is thus trivial: any node aligned to a non-emptyforeign span is extractable.In Figure 2, we show a sample sentence pair frag-2For notational convenience, we imagine that for each par-ticular English word e, there is a special preterminal symbol Ewhich produces it.
These symbols E act like any other non-terminal in the grammar with respect to the parameterization inSection 3.1.
To denote standard non-terminals, we will use A,B, and C.PP[0,4]IN[3,4]NP[1,3]DT[1,1]NNS[1,3]the[1,1]elections[1,3]?
????
?at parliament electionbeforebefore[3,4]PPNP INNNSDT0 1 2 3 4?PP ?
IN NP ; ?
NP INNP ?
DT NNS ; DT NNSIN ?
before ; beforebefore ?
before ; ?
?DT ?
the ; thethe ?
the ; !NNS ?
elections ; electionselections ?
elections ; ??
?
?Figure 2: Top: A synchronous derivation of a small sen-tence pair fragment under our model.
The English pro-jection of the derivation represents a valid constituencyparse, while the foreign projection is less constrained.We connect each foreign terminal with a dashed line tothe node in the English side of the synchronous deriva-tion at which it is generated.
The foreign span assignedto each English node is indicated with indices.
All nodeswith non-empty spans, shown in boldface, are extractablenodes.
Bottom: The SCFG rules used in the derivation.ment as generated by our model.
Our model cor-rectly identifies that the English the aligns to nothingon the foreign side.
Our model also effectively cap-tures the one-to-many alignment3 of elections to ?3While our model does not explicitly produce many-to-onealignments, many-to-one rules can be discovered via rule com-position (Galley et al, 2006).120?
??.
Finally, our model correctly analyzes theChinese circumposition ?
.
.
.??
(before .
.
.
).
Inthis construction, ??
carries the meaning of ?be-fore?, and thus correctly aligns to before, while ?functions as a generic preposition, which our modelhandles by attaching it to the PP.
This analysis per-mits the extraction of the general rule (PP ?
IN1NP2 ;?
NP2 IN1), and the more lexicalized (PP?before NP ;?
NP??
).3.1 ParameterizationIn principle, our model could have one parameter foreach instantiation r of a rule type.
This model wouldhave an unmanageable number of parameters, pro-ducing both computational and modeling issues ?
itis well known that unsupervised models with largenumbers of parameters are prone to degenerate anal-yses of the data (DeNero et al, 2006).
One solutionmight be to apply an informed prior with a compu-tationally tractable inference procedure (e.g.
Cohnand Blunsom (2009) or Liu and Gildea (2009)).
Weopt here for the simpler, statistically more robust so-lution of making independence assumptions to keepthe number of parameters at a reasonable level.Concretely, we define the probability of the BI-NARYMONO rule,4p(r = A?
B C; fl B fm C fr|A, eA)which conditions on the root of the rule A and theEnglish yield eA, aspg(A?
B C | A, eA) ?
pinv(I | B,C)?pleft(fl | A, eA)?pmid(fm | A, eA)?pright(fr | A, eA)In words, we assume that the rule probability de-composes into a monolingual PCFG grammar prob-ability pg, an inversion probability pinv, and a proba-bility of left, middle, and right word sequences pleft,pmid, and pright.5 Because we condition on e, themonolingual grammar probability pg must form adistribution which produces e with probability 1.64In the text, we only describe the factorization for the BI-NARYMONO rule.
For a parameterization of all rules, we referthe reader to Table 2.5All parameters in our model are multinomial distributions.6A simple case of such a distribution is one which places allof its mass on a single tree.
More complex distributions can beobtained by conditioning an arbitrary PCFG on e (Goodman,1998).We further assume that the probability of produc-ing a foreign word sequence fl decomposes as:pleft(fl | A, eA) = pl(|fl| = m | A)m?j=1p(fj | A, eA)where m is the length of the sequence fl.
The pa-rameter pl is a left length distribution.
The prob-abilities pmid, pright, decompose in the same way,except substituting a separate length distribution pmand pr for pl.
For the TERMINAL rule, we emit ftwith a similarly decomposed distribution pterm us-ing length distribution pw.We define the probability of generating a foreignword fj asp(fj | A, eA) =?i?eA1| eA |pt(fj | ei)with i ?
eA denoting an index ranging over the in-dices of the English words contained in eA.
Thereader may recognize the above expressions as theprobability assigned by IBM Model 1 (Brown et al,1993) of generating the words fl given the words eA,with one important difference ?
the length m of theforeign sentence is often not modeled, so the termpl(|fl| = m | A) is set to a constant and ignored.Parameterizing this length allows our model to ef-fectively control the number of words produced atdifferent levels of the derivation.It is worth noting how each parameter affects themodel?s behavior.
The pt distribution is a standard?translation?
table, familiar from the IBM Models.The pinv distribution is a ?distortion?
parameter, andmodels the likelihood of inverting non-terminals Band C. This parameter can capture, for example,the high likelihood that prepositions IN and nounphrases NP often invert in Chinese due to its useof postpositions.
The non-terminal length distribu-tions pl, pm, and pr model the probability of ?back-ing off?
and emitting foreign words at non-terminalswhen a more refined analysis cannot be found.
Ifthese parameters place high mass on 0 length wordsequences, this heavily penalizes this backoff be-haviour.
For the TERMINAL rule, the length distri-bution pw parameterizes the number of words pro-duced for a particular English word e, functioningsimilarly to the ?fertilities?
employed by IBM Mod-els 3 and 4 (Brown et al, 1993).
This allows us121to model, for example, the tendency of English de-terminers the and a translate to nothing in the Chi-nese, and of English names to align to multiple Chi-nese words.
In general, we expect an English wordto usually align to one Chinese word, and so weplace a weak Dirichlet prior on on the pe distributionwhich puts extra mass on 1-length word sequences.This is helpful for avoiding the ?garbage collection?
(Moore, 2004) problem for rare words.3.2 Exploiting Non-Terminal LabelsThere are often foreign words that do not correspondwell to any English word, which our model mustalso handle.
We elected for a simple augmentationto our model to account for these words.
When gen-erating foreign word sequences f at a non-terminal(i.e.
via the UNARY or BINARY productions), wealso allow for the production of foreign words fromthe non-terminal symbol A.
We modify p(fj | eA)from the previous section to allow production of fjdirectly from the non-terminal7 A:p(fj | eA) = pnt ?
p(fj | A)+ (1?
pnt) ?
?i?eA1|eA|pt(fj | ei)where pnt is a global binomial parameter which con-trols how often such alignments are made.This necessitates the inclusion of parameters likept(?
| NP) into our translation table.
Generally,these parameters do not contain much information,but rather function like a traditional NULL rootedat some position in the tree.
However, in somecases, the particular annotation used by the PennTreebank (Marcus et al, 1993) (and hence mostparsers) allows for some interesting parameters tobe learned.
For example, we found that our aligneroften matched the Chinese word ?, which marksthe past tense (among other things), to the preter-minals VBD and VBN, which denote the Englishsimple past and perfect tense.
Additionally, Chinesemeasure words like ?
and ?
often align to the CD(numeral) preterminal.
These generalizations can bequite useful ?
where a particular number might pre-dict a measure word quite poorly, the generalizationthat measure words co-occur with the CD tag is veryrobust.7For terminal symbols E, this production is not possible.3.3 Membership in ITGThe generative process which describes our modelcontains a class of grammars larger than the com-putationally efficient class of ITG grammars.
For-tunately, the parameterization described above notonly reduces the number of parameters to a man-ageable level, but also introduces independence as-sumptions which permit synchronous binarization(Zhang et al, 2006) of our grammar.
Any SCFG thatcan be synchronously binarized is an ITG, meaningthat our parameterization permits efficient inferencealgorithms which we will make use of in the nextsection.
Although several binarizations are possi-ble, we give one such binarization and its associatedprobabilities in Table 2.3.4 Robustness to Syntactic DivergenceGenerally speaking, ITG grammars have provenmore useful without the monolingual syntactic con-straints imposed by a target parse tree.
When deriva-tions are restricted to respect a target-side parse tree,many desirable alignments are ruled out when thesyntax of the two languages diverges, and align-ment quality drops precipitously (Zhang and Gildea,2004), though attempts have been made to addressthis issue (Gildea, 2003).Our model is designed to degrade gracefully inthe case of syntactic divergence.
Because it can pro-duce foreign words at any level of the derivation,our model can effectively back off to a variant ofModel 1 in the case where an ITG derivation thatboth respects the target parse tree and the desiredword-level alignments cannot be found.For example, consider the sentence pair fragmentin Figure 3.
It is not possible to produce an ITGderivation of this fragment that both respects theEnglish tree and also aligns all foreign words totheir obvious English counterparts.
Our model han-dles this case by attaching the troublesome ??
atthe uppermost VP.
This analysis captures 3 of the4 word-level correspondences, and also permits ex-traction of abstract rules like (S?
NP VP ; NP VP)and (NP?
the NN ; NN).Unfortunately, this analysis leaves the Englishword tomorrow with an empty foreign span, permit-ting extraction of the incorrect translation (VP ?announced tomorrow ; ??
), among others.
Our122Rule Type Root English side Foreign side ProbabilityTERMINAL E e wt pterm(wt | E)UNARY A Bu wl Bu pg(A ?
B | A)pleft(wl | A, eA)Bu B B wr pright(wr | A, eA)BINARY A A1 wl A1 pleft(wl | A, eA)A1 B C1 B C1 pg(A ?
B C | A)pinv(I=false | B,C)A1 B C1 C1 B pg(A ?
B C | A)pinv(I=true | B,C)C1 C2 fm C2 pmid(fm | A, eA)C2 C C fr pright(fr | A, eA)Table 2: A synchronous binarization of the SCFG describing our model.S[0,4]NP[3,4]DT[3,3] NN[3,4]VP[0,3]VB[2,2]VP[2,3]VBN[2,3]NN[3,3]VP[2,3]MD[1,2]??
?
??
?
?listannouncewilltomorrow0 1 2 3 4the[3,3] list[3,4]be[2,2]announced[2,3] tomorrow[3,3]will[1,2](a)Figure 3: The graceful degradation of our model in theface of syntactic divergence.
It is not possible to alignall foreign words with their obvious English counterpartswith an ITG derivation.
Instead, our model analyzes asmuch as possible, but must resort to emitting ??
highin the tree.point here is not that our model?s analysis is ?cor-rect?, but ?good enough?
without resorting to morecomputationally complicated models.
In general,our model follows an ?extract as much as possi-ble?
approach.
We hypothesize that this approachwill capture important syntactic generalizations, butit also risks including low-quality rules.
It is an em-pirical question whether this approach is effective,and we investigate this issue further in Section 5.3.There are possibilities for improving our model?streatment of syntactic divergence.
One option isto allow the model to select trees which are moreconsistent with the alignment (Burkett et al, 2010),which our model can do since it permits efficient in-ference over forests.
The second is to modify thegenerative process slightly, perhaps by including the?clone?
operator of Gildea (2003).4 Learning and Inference4.1 Parameter EstimationThe parameters of our model can be efficientlyestimated in an unsupervised fashion using theExpectation-Maximization (EM) algorithm.
The E-step requires the computation of expected counts un-der our model for each multinomial parameter.
Weomit the details of obtaining expected counts foreach distribution, since they can be obtained usingsimple arithmetic from a single quantity, namely, theexpected count of a particular instantiation of a syn-chronous rule r. This expectation is a standard quan-tity that can be computed in O(n6) time using thebitext Inside-Outside dynamic program (Wu, 1997).4.2 Dynamic Program PruningWhile our model permits O(n6) inference over aforest of English trees, inference over a full forestwould be very slow, and so we fix a single n-ary En-glish tree obtained from a monolingual parser.
How-ever, it is worth noting that the English side of theITG derivation is not completely fixed.
Where ourEnglish trees are more than binary branching, wepermit any binarization in our dynamic program.For efficiency, we also ruled out span alignmentsthat are extremely lopsided, for example, a 1-wordEnglish span aligned to a 20-word foreign span.Specifically, we pruned any span alignment in whichone side is more than 5 times larger than the other.Finally, we employ pruning based on high-precision alignments from simpler models (Cherryand Lin, 2007; Haghighi et al, 2009).
We com-pute word-to-word alignments by finding all wordpairs which have a posterior of at least 0.7 accordingto both forward and reverse IBM Model 1 parame-ters, and prune any span pairs which invalidate morethan 3 of these alignments.
In total, this pruning re-123Span P R F1Syntactic Alignment 50.9 83.0 63.1GIZA++ 56.1 67.3 61.2Rule P R F1Syntactic Alignment 39.6 40.3 39.9GIZA++ 46.2 34.7 39.6Table 3: Alignment quality results for our syntacticaligner and our GIZA++ baseline.duced computation from approximately 1.5 secondsper sentence to about 0.3 seconds per sentence, aspeed-up of a factor of 5.4.3 DecodingGiven a trained model, we extract a tree-to-stringalignment as follows: we compute, for each nodein the English tree, the posterior probability of aparticular foreign span assignment using the samedynamic program needed for EM.
We then com-pute the set of span assignments which maximizesthe sum of these posteriors, constrained such thatthe foreign span assignments nest in the obviousway.
This algorithm is a natural synchronous gener-alization of the monolingual Maximum ConstituentsParse algorithm of Goodman (1996).5 Experiments5.1 Alignment QualityWe first evaluated our alignments against gold stan-dard annotations.
Our training data consisted of the2261 manually aligned and translated sentences ofthe Chinese Treebank (Bies et al, 2007) and approx-imately half a million unlabeled sentences of parallelChinese-English newswire.
The unlabeled data wassubsampled (Li et al, 2009) from a larger corpus byselecting sentences which have good tune and testset coverage, and limited to sentences of length atmost 40.
We parsed the English side of the train-ing data with the Berkeley parser.8 For our baselinealignments, we used GIZA++, trained in the stan-dard way.9 We used the grow-diag-final alignmentheuristic, as we found it outperformed union in earlyexperiments.We trained our unsupervised syntactic aligner onthe concatenation of the labelled and unlabelled8http://code.google.com/p/berkeleyparser/95 iterations of model 1, 5 iterations of HMM, 3 iterationsof Model 3, and 3 iterations of Model 4.data.
As is standard in unsupervised alignment mod-els, we initialized the translation parameters pt byfirst training 5 iterations of IBM Model 1 using thejoint training algorithm of Liang et al (2006), andthen trained our model for 5 EM iterations.
Weextracted syntactic rules using a re-implementationof the Galley et al (2006) algorithm from both oursyntactic alignments and the GIZA++ alignments.We handle null-aligned words by extracting everyconsistent derivation, and extracted composed rulesconsisting of at most 3 minimal rules.We evaluate our alignments against the gold stan-dard in two ways.
We calculated Span F-score,which compares the set of extractable nodes pairedwith a foreign span, and Rule F-score (Fossum et al,2008) over minimal rules.
The results are shown inTable 3.
By both measures, our syntactic aligner ef-fectively trades recall for precision when comparedto our baseline, slightly increasing overall F-score.5.2 Translation QualityFor our translation system, we used a re-implementation of the syntactic system of Galley etal.
(2006).
For the translation rules extracted fromour data, we computed standard features based onrelative frequency counts, and tuned their weightsusing MERT (Och, 2003).
We also included alanguage model feature, using a 5-gram languagemodel trained on 220 million words of English textusing the SRILM Toolkit (Stolcke, 2002).For tuning and test data, we used a subset of theNIST MT04 and MT05 with sentences of length atmost 40.
We used the first 1000 sentences of this setfor tuning and the remaining 642 sentences as testdata.
We used the decoder described in DeNero etal.
(2009) during both tuning and testing.We provide final tune and test set results in Ta-ble 4.
Our alignments produce a 1.0 BLEU improve-ment over the baseline.
Our reported syntactic re-sults were obtained when rules were thresholded bycount; we discuss this in the next section.5.3 AnalysisAs discussed in Section 3.4, our aligner is designedto extract many rules, which risks inadvertently ex-tracting low-quality rules.
To quantify this, wefirst examined the number of rules extracted by ouraligner as compared with GIZA++.
After relativiz-124Tune TestSyntactic Alignment 29.78 29.83GIZA++ 28.76 28.84GIZA++ high count 25.51 25.38Table 4: Final tune and test set results for our grammarsextracted using the baseline GIZA++ alignments and oursyntactic aligner.
When we filter the GIZA++ grammarswith the same count thresholds used for our aligner (?highcount?
), BLEU score drops substantially.ing to the tune and test set, we extracted approx-imately 32 million unique rules using our aligner,but only 3 million with GIZA++.
To check thatwe were not just extracting extra low-count, low-quality rules, we plotted the number of rules witha particular count in Figure 4.
We found that whileour aligner certainly extracts many more low-countrules, it also extracts many more high-count rules.Of course, high-count rules are not guaranteedto be high quality.
To verify that frequent ruleswere better for translation, we experimented withvarious methods of thresholding to remove ruleswith low count extracted from using aligner.
Wefound in early development found that removinglow-count rules improved translation performancesubstantially.
In particular, we settled on the follow-ing scheme: we kept all rules with a single foreignterminal on the right-hand side.
For entirely lexical(gapless) rules, we kept all rules occurring at least3 times.
For unlexicalized rules, we kept all rulesoccurring at least 20 times per gap.
For rules whichmixed gaps and lexical items, we kept all rules oc-curring at least 10 times per gap.
This left us witha grammar about 600 000 rules, the same grammarwhich gave us our final results reported in Table 4.In contrast to our syntactic aligner, rules extractedusing GIZA++ could not be so aggressively pruned.When pruned using the same count thresholds, ac-curacy dropped by more than 3.0 BLEU on the tuneset, and similarly on the test set (see Table 4).
Toobtain the accuracy shown in our final results (ourbest results with GIZA++), we had to adjust thecount threshold to include all lexicalized rules, allunlexicalized rules, and mixed rules occurring atleast twice per gap.
With these count thresholds, theGIZA++ grammar contained about 580 000 rules,roughly the same number as our syntactic grammar.We also manually searched the grammars forrules that had high count in the syntactically-0 200 400 600 800 10001e+001e+021e+041e+06CountNumber of rules withcount SyntacticGIZA++Figure 4: Number of extracted translation rules with aparticular count.
Grammars extracted from our syntacticaligner produce not only more low-count rules, but alsomore high-count rules than GIZA++.extracted grammar and low (or 0) count in theGIZA++ grammar.
Of course, we can alwayscherry-pick such examples, but a few rules were il-luminating.
For example, for the ?
.
.
.??
con-struction discussed earlier, our aligner permits ex-traction of the general rule (PP?
IN1 NP2 ;?
NP2IN1) 3087 times, and the lexicalized rule (PP?
be-fore NP ; ?
NP ??)
118 times.
In constrast, theGIZA++ grammar extracts the latter only 23 timesand the former not at all.
The more complex rule(NP?
NP2 , who S1 , ; S1 ?
NP2), which capturesa common appositive construction, was absent fromthe GIZA++ grammar but occurred 63 in ours.6 ConclusionWe have described a syntactic alignment modelwhich explicitly aligns nodes of a syntactic parse inone language to spans in another, making it suitablefor use in many syntactic translation systems.
Ourmodel is unsupervised and can be efficiently trainedwith a straightforward application of EM.
We havedemonstrated that our model can accurately capturemany syntactic correspondences, and is robust in theface of syntactic divergence between language pairs.Our aligner permits the extraction of more reliable,high-count rules when compared to a standard word-alignment baseline.
These high-count rules also pro-duce improvements in BLEU score.AcknowledgementsThis project is funded in part by the NSF under grant 0643742;by BBN under DARPA contract HR0011-06-C-0022; and anNSERC Postgraduate Fellowship.
The authors would like tothank Michael Auli for his input.125ReferencesAnn Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007.English chinese translation treebank v 1.0. web download.In LDC2007T02.Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,and Robert L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
ComputationalLinguistics, 19:263?311.David Burkett, John Blitzer, and Dan Klein.
2010.
Joint pars-ing and alignment with weakly synchronized grammar.
InProceedings of the North American Association for Compu-tational Linguistics.Colin Cherry and Dekang Lin.
2006.
Soft syntactic constraintsfor word alignment through discriminative training.
In Pro-ceedings of the Association of Computational Linguistics.Colin Cherry and Dekang Lin.
2007.
Inversion transductiongrammar for joint phrasal translation modeling.
In Workshopon Syntax and Structure in Statistical Translation.David Chiang.
2004.
Evaluating grammar formalisms for ap-plications to natural language processing and biological se-quence analysis.
Ph.D. thesis, University of Pennsylvania.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In The Annual Conference ofthe Association for Computational Linguistics.Trevor Cohn and Phil Blunsom.
2009.
A Bayesian model ofsyntax-directed tree to string grammar induction.
In Pro-ceedings of the Conference on Emprical Methods for NaturalLanguage Processing.John DeNero and Dan Klein.
2007.
Tailoring word alignmentsto syntactic machine translation.
In The Annual Conferenceof the Association for Computational Linguistics.John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006.Why generative phrase models underperform surface heuris-tics.
In Workshop on Statistical Machine Translation atNAACL.John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.2009.
Efficient parsing for transducer grammars.
In Pro-ceedings of NAACL.Victoria Fossum, Kevin Knight, and Steven Abney.
2008.
Us-ing syntax to improve word alignment precision for syntax-based machine translation.
In Proceedings of the ThirdWorkshop on Statistical Machine Translation.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proceed-ings of the North American Chapter of the Association forComputational Linguistics.Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006.
Scal-able inference and training of context-rich syntactic transla-tion models.
In Proceedings of the Association for Compu-tational Linguistics.Daniel Gildea.
2003.
Loosely tree-based alignment for ma-chine translation.
In Proceedings of the Association forComputational Linguistics.Joshua Goodman.
1996.
Parsing algorithms and metrics.
InProceedings of the Association for Computational Linguis-tics.Joshua Goodman.
1998.
Parsing Inside-Out.
Ph.D. thesis,Harvard University.Aria Haghighi, John Blitzer, John Denero, and Dan Klein.2009.
Better word alignments with supervised itg models.In Proceedings of the Association for Computational Lin-guistics.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.
Asyntax-directed translator with extended domain of locality.In Proceedings of CHSLP.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch,Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton,Jonathan Weese, and Omar F. Zaidan.
2009.
Joshua: anopen source toolkit for parsing-based machine translation.In Proceedings of the Fourth Workshop on Statistical Ma-chine Translation.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Alignment byagreement.
In Proceedings of the North American Chapterof the Association for Computational Linguistics.Ding Liu and Daniel Gildea.
2009.
Bayesian learning ofphrasal tree-to-string templates.
In Proceedings of EMNLP.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-stringalignment template for statistical machine translation.
InProceedings of the Association for Computational Linguis-tics.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improving tree-to-tree translation with packed forests.
In Proceedings of ACL.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.
Build-ing a large annotated corpus of English: The Penn Treebank.In Computational Linguistics.Jonathan May and Kevin Knight.
2007.
Syntactic re-alignmentmodels for machine translation.
In Proceedings of the Con-ference on Emprical Methods for Natural Language Pro-cessing.Robert C. Moore.
2004.
Improving ibm word alignment model1.
In The Annual Conference of the Association for Compu-tational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29:19?51.Franz Josef Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proceedings of the Associationfor Computational Linguistics.Andreas Stolcke.
2002.
SRILM: An extensible language mod-eling toolkit.
In ICSLP 2002.Dekai Wu.
1997.
Stochastic inversion transduction grammarsand bilingual parsing of parallel corpora.
ComputationalLinguistics, 23:377?404.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statis-tical translation model.
In Proceedings of the Association ofComputational Linguistics.Hao Zhang and Daniel Gildea.
2004.
Syntax-based alignment:supervised or unsupervised?
In Proceedings of the Confer-ence on Computational Linguistics.Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.2006.
Synchronous binarization for machine translation.
InProceedings of the North American Chapter of the Associa-tion for Computational Linguistics.126
