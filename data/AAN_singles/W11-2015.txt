Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 120?129,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsPredicting the Micro-Timing of User Input for an IncrementalSpoken Dialogue System that Completes a User?s Ongoing TurnTimo BaumannDepartment of LinguisticsPotsdam UniversityGermanytimo@ling.uni-potsdam.deDavid SchlangenFaculty of Linguistics and LiteratureBielefeld UniversityGermanydavid.schlangen@uni-bielefeld.deAbstractWe present the novel task of predicting tem-poral features of continuations of user input,while that input is still ongoing.
We show thatthe remaining duration of an ongoing word, aswell as the duration of the next can be predictedreasonably well, and we put this information touse in a system that synchronously completesa user?s speech.
While we focus on collabo-rative completions, the techniques presentedhere may also be useful for the alignment ofback-channels and immediate turn-taking in anincremental SDS, or to synchronously monitorthe user?s speech fluency for other reasons.1 IntroductionTurn completion, that is, finishing a user?s ongoing ut-terance, can be considered an ideal test-case of incre-mental spoken language processing, as it requires thatall levels of language understanding and productionare carried out in real time, without any noticeablelags and with proper timing and even with the abilityto predict what will come.
Spoken dialogue systems,especially incremental ones, have come a long waytowards reducing lags at turn changes (e. g. (Raux andEskenazi, 2009; Skantze and Schlangen, 2009)), oreven predicting upcoming turn changes (Schlangen,2006; Baumann, 2008; Ward et al, 2010).
Com-pared to regular turn changes, where short pauses oroverlaps occur frequently (Weilhammer and Rabold,2003), turn completions in natural dialogues are typ-ically precisely aligned and prosodically highly in-tegrated with the turn that is being completed (Lo-cal, 2007).
With ever more incremental (and hencequicker) spoken dialogue systems, the phenomenonof completion comes into reach for SDSs, and hencequestions of micro-timing become important.While completing someone else?s turn ?
especiallyfor a computer ?
may be considered impolite or evenannoying, being able to do so can be a useful capa-bility.
Some tasks where it might be helpful are?
negotiation training to induce stress in a humantrainee as presented by DeVault et al (2009), or?
pronunciation aids for language learners, inwhich hard to pronounce words could be spokensimultaneously by the system.A system should certainly not try to complete allor even many user turns, but having the capabilityto do so means that the system has a very efficientinteractional device at its disposal.Furthermore, monitoring the user?s timing, as isrequired for the temporal prediction of turn continua-tions, can also be used for other conversational taskssuch as producing back-channels that are preciselyaligned to the user?s back-channel inviting cues, toenable micro-alignment of turn-onsets, or to quicklyreact to deviations in the user?s fluency.In this paper, we concentrate on the temporal as-pects of turn completion, that is, the prediction ofthe precise temporal alignment of a turn continuationand the technical realization of this timing.
We as-sume the task of predicting the completion itself tobe handled by some other system component.
Suchcomponents are indeed under development (see Sec-tion 2).
However, previous work has left out thequestion of how the precise timing of turn comple-tions can be accomplished, which is what we try toanswer here.The remainder of this paper is structured as fol-lows: In Section 2 we review literature on turn com-pletion and related work in spoken dialogue systems,120before we explain what exactly our task is in Sec-tion 3.
In Section 4 we present our system?s overallarchitecture and the duration modelling techniquethat we use, before describing the corpus that we usein Section 5.
In Section 6 we first analyse whetherenough time to output a completion is available suffi-ciently often, before turning to the question for theactual sub-tasks of when and how to complete.
Wewrap up with concluding remarks and ideas for futurework.2 Related WorkThe general phenomenon of turn completion canbe broken down into cases where the completionis spoken simultaneously with the original speaker(turn sharing, (Lerner, 2002)) and where the floorchanges in mid-utterance (collaborative turn se-quences (Lerner, 2004) or split utterances (Purveret al, 2009)).
In this paper, a differentiation be-tween the two cases is not important, as we onlydeal with the question of when to start speaking(for the previously non-speaking system) and not thequestion of whether the current turn owner will stopspeaking.
Moreover, whether the other speaker willstop is beyond the system?s control.
Lerner (2004)distinguishes turn co-optation, in which a listenerjoins in to come first and win the floor, and turn co-completion, in which the completion is produced inchorus.
Both of these phenomena relate to the cur-rent speaker?s speech: either to match it, or to beatit.
While we focus on matching in this work, themethods described similarly apply to co-optation.As Lerner (2002) notes, attributing this view toSacks et al (1974), simultaneous speech in conver-sation is often treated exclusively as a turn takingproblem in need of repair.
This is exactly the pointof view taken by current spoken dialogue systems,which avoid overlap and interpret al simultaneousspeech as barge-in, regardless of content.
However,Lerner (2002) also notes that simultaneous speechsystematically occurs without being perceived as aproblem, e. g. in greetings, or when saying good bye,which are relevant sub-tasks in deployed SDSs.Two corpus studies are available which investi-gate split utterances and their frequency: Skuplik(1999) looked at sentence cooperations in a corpusof task-oriented German (Poesio and Rieser, 2010)and found 3.4 % of such utterances.
Purver et al(2009) find 2.8 % of utterance boundaries in the BNC(as annotated by Ferna?ndez and Ginzburg (2002))to meet their definition of utterances split betweenspeakers.
Thus, while the absolute frequency mayseem low, the phenomenon does seem to occur con-sistently across different languages and corpora.Local (2007) describes phonetic characteristics atutterance splits (he calls the phenomenon turn co-construction) which distinguish them from regularturn handovers, namely temporal alignment and closeprosodic integration with the previous speaker?s utter-ance.
In this paper, we focus on the temporal aspects(both alignment and speech rate) when realizing turncompletions, leaving pitch integration to future work.Cummins (2009) analyses speech read aloud bytwo subjects at the same time (which he calls syn-chronous speech): Synchrony is slightly better in alive setting than with a subject synchronizing to arecording of speech which was itself spoken in syn-chrony and this is easier than to a recording of uncon-strained speech.
Cummins (2009) also experimentswith reduced stimuli: eliminating f0-contour had nosignificant impact on synchrony, while a carrier with-out segmental information (but including f0-contour)fared significantly better than speaking to an uninfor-mative hiss.
(The first sentence of each recording wasalways left unmodified, allowing subjects to estimatespeech rate even in the HISS condition.)
Thus, pitchinformation does not seem necessary for the task butmay help in the absence of segmental information.On a more technical level and as mentioned above,much work has been put into speeding up end-of-turn detection and reducing processing lags at turnchanges (Raux and Eskenazi, 2009) and more re-cently into end-of-turn prediction: Ward et al (2010)present a model of turn-taking which estimates theremaining duration of a currently ongoing turn.
Weextend the task to predicting the remaining durationof any currently ongoing word in the turn.
Of course,for this to be possible, words must be recognizedwhile they are still being uttered.
We have previ-ously shown (Baumann et al, 2009) that this can beachieved with incremental ASR for the vast major-ity of words and with an average of 102 ms betweenwhen a word is first recognized and the word?s end.As mentioned above, our work relies on other in-cremental components to form a meaningful, turn121completing application and such components are be-ing developed: Incremental understanding is well un-derway (Sagae et al, 2009; Heintze et al, 2010), as isdecision making on whether full understanding of anutterance has been reached (DeVault et al, 2009), andPurver et al (2011) present an incremental semanticscomponent aimed explicitly at split utterances.
Infact, DeVault et al (2009) provide exactly the coun-terpart to our work, describing a method that, giventhe words of an ongoing utterance, decides when thepoint of maximum understanding has been reachedand with what words this utterance is likely to end.However, in their system demonstration, Sagae et al(2010) use short silence time-outs to trigger systemresponses.
Our work eliminates the need for suchtime-outs.Hirasawa et al (1999) present a study where im-mediate, overlapping back-channel feedback fromthe system was found to be inferior to acknowledg-ing information only after the user?s turn.
However,they disregarded the back-channels?
micro-temporalalignment as explored in this study (presumably pro-ducing back-channels as early as possible), so theirnegative results cannot be taken as demonstrating ageneral shortcoming of the interactional strategy.3 The TaskThe general task that our timing component tacklesis illustrated in Figure 1.
The component is triggeredinto action when an understanding module signalsthat (and with what words) a turn should be com-pleted.
At this decision point, our component mustestimate (a) when the current word ends and (b) howthe user will speak the predicted continuation.
Ide-ally, the system will start speaking the continuationprecisely when the next word starts and match theuser?s speech as best as possible.
Thus, our compo-nent must estimate the time between decision pointand ideal onset (which we call holding time) and theuser?s speech rate during the following words.In order for the system to be able to produce acontinuation (?five six seven?
in Figure 1) in time,of course the decision point must come sufficientlyearly (i. e. during ?four?)
to allow for a completionto be output in due time.
This important preconditionmust be met by-and-large by the employed ASR.However, it is not a strict requirement: If ASR resultsFigure 1: The task: When notified that the ongoing utter-ance should be completed with ?five six seven?
after theword ?four?, the first three words are used to (a) estimatethe remaining duration of ?four?
and to (b) estimate thespeech rate for the completion.are lagging behind, the timing component?s estimatedholding time should turn negative.
Depending on theestimated lag, a completion can be suppressed or,if it is small, fairly good completions can still berealized by shortening the first (few) phonemes ofthe completion to be synthesized.We will now present our overall system beforedescribing two strategies we developed for solvingthe task just described, and further on present theexperiments we conducted with the system and theirresults in Sections 5 and 6.4 System DescriptionOur system is based on the InproTK toolkit for in-cremental spoken dialogue systems (Schlangen etal., 2010) which uses Sphinx-4 (Walker et al, 2004)and MaryTTS (Schro?der and Trouvain, 2003) as un-derlying ASR and TTS engines, respectively.
Thecore of our system is a component that incrementallyreceives rich speech recognition input (words, theirdurations and a pitch track) from an incremental ASRand computes the timing of completions.When receiving a new word from ASR, our com-ponent queries an understanding component whethera completion can be predicted, and if so, whethersuch a completion should be performed.
In order tonot duplicate the work of DeVault et al (2009), weuse a mock implementation of an understanding mod-ule, which actually knows what words are going to bespoken (from a transcript file) and aims to completeafter every word spoken.We have implemented two strategies for the timingmodule, which we will describe in turn, after firstdiscussing a simple baseline approach.122Baseline: Speak Immediately A first, very simpleapproach for our timing component is to never waitbetween the decision point and outputting a comple-tion right away.
We believe that this was the strategytaken by Hirasawa et al (1999) and we will show inour evaluation in Section 6.2 that it is not very good.Strategy 1: Estimating ASR Lookahead In ourASR-based strategy (illustrated in Figure 2, top) thesystem estimates what we call its lookahead rate,i.
e. the average time between when a word is firstrecognized by ASR and the word?s end in the signal.This lookahead is known for the words that have beenrecognized so far and the average lookahead can thenbe used as an estimate of the remaining durationof the word that is currently being detected (i. e. itsholding time).
Once the currently spoken word isexpected to end, the system should start to speak.The strategy just described, as well as the baselinestrategy, only solve half of the task, namely, when thecontinuation should be started, but not the questionof how to speak, which we will turn to now.
Bothsub-tasks can be solved simultaneously by estimatingthe speech rate of the current speaker, based on whatshe already said so far, and considering this speechrate when synthesizing a completion.
Speech rateestimation using some kind of duration model thusforms the second strategy?s main component.
For thepurpose of this work, we focus on duration modelsin the context of TTS, where they are used to assigndurations to the phones to be uttered.
Rule-basedapproaches (Klatt, 1979) as well as methods usingmachine learning have been used (primarily CART(Breiman et al, 1984)); for HMM-based speech syn-thesis, durations can be generated from Gaussianprobability density functions (PDFs) (Yoshimura etal., 1998).
We are not aware of any work that usesduration models to predict the remaining time of anongoing word or utterance.In our task, we need the duration model to makeestimations based on limited input (instead of pro-viding plausibility ratings as in most ASR-relatedapplications).
As it turns out, a TTS system in itselfis an excellent duration model because it potentiallyponders all kinds of syntactic, lexical, post-lexical,phonological and prosodical context when assigningdurations to words and their phones.
Also, our taskalready involves a TTS system to synthesize the turnFigure 2: Our strategies to estimate holding time (when tospeak), and speech rate (how to speak; only Strategy 2).completion ?
in our case MaryTTS (Schro?der andTrouvain, 2003).
The durations can be accessed insymbolic form in MaryTTS, and the system allowsto manipulate this information prior to acoustic syn-thesis.
Depending on which voice is used, MaryTTSuses machine-learned duration models (CART orPDFs) or an optimized version of Klatt?s (1979) ruleswhich have been shown to perform only marginallyworse than the CART-based approach (Brinckmannand Trouvain, 2003).Strategy 2: Analysis-by-Synthesis As just de-scribed, we hence employ the TTS?
duration modelin an analysis-by-synthesis approach in this secondstrategy, as illustrated in Figure 2 (bottom): Whentriggered to complete an ongoing utterance, we querythe TTS for the durations it would assign to a produc-tion of the predicted full utterance, i. e. the prefix thatwas heard plus the predicted continuation of the turn.In that way, the TTS can take the full utterance intoaccount when assigning prosodic patterns which mayinfluence durations.
We then compute the factor thatis needed to scale the TTS?s duration of the wordsalready finished by the user (in the example: ?onetwo three?)
to the duration of the actual utteranceand apply this scaling factor to the remaining wordsin the synthesized completion.
We can then read offthe expected duration of the currently spoken wordfrom the scaled TTS output and, by subtracting thetime that this word is already going on, find out theholding time.
Similarly, the completion of the turnwhich is now scaled to match the user?s speech ratecan be fed back to the synthesis system in order togenerate the acoustic waveform which is to be outputto the speakers once the system should start to speak.1235 Corpus and Experiment SetupIn order to evaluate the accuracy of the individualcomponents involved in the specific subtasks, weconducted a controlled offline experiment.
We havenot yet evaluated how actual users of our systemwould judge its performance at outputting collabora-tive completions.As evaluation corpus we use recordings of theGerman version of the story The North Wind andthe Sun (IPA, 1999) from the Kiel Corpus of ReadSpeech (IPDS, 1994).
The story (including title)consists of 111 words and is read by 16 speakers,giving a total of 1776 words in 255 inter-pausal-units(IPUs), altogether resulting in about 12 minutes ofspeech.
(In the following, we will equate ?turns?
withIPUs, as our corpus of read speech does not containtrue turns.)
Words and phones in our corpus havea mean/median/std dev duration of 319/290/171 msand 78/69/40 ms, respectively.We assume that every word can be a possible com-pletion point in a real system, hence we evaluate theperformance of our timing component for all wordsin the corpus.
(This generalization may have an in-fluence on our results: real collaborative completionsare sometimes invited by the speaker, probably bygiving cues that might simplify co-completion; if thatis true, the version tackled here is actually harder thanthe real task.
)Good turn completions (and good timings) canprobably only be expected in the light of high ASRperformance.
We trained a domain-specific languagemodel (based on the test corpus) and used an acous-tic model trained for conversational speech whichwas not specifically tuned for the task.
The resultingWER is 4.2 %.
While our results could hence be con-sidered too optimistic, Baumann et al (2009) showedthat incremental metrics remained stable in the lightof varying ASR performance.
We expect that lowerASR performance would not radically change pre-diction quality itself; rather, it would have an impacton how often continuations could be predicted, sincethat is based on correct understanding of the prefixof the utterance, limiting the amount of data pointsfor our statistics.Even though we simulated the understanding andprediction module, we built in some constraints thatare meant to be representative of real implementa-tions of such a module: it can only find the rightcompletion if the previous two words are recognizedcorrectly and the overall WER is lower than 10 %.
(Coming back to Figure 1, if the system had falselyrecognized ?on two three?, no completion wouldtake place: Even though the last two words ?twothree?
were recognized correctly, the WER between?on two three?
and ?one two three?
is too high.)
Un-der this constraint, the timing component generateddata for 1100 IPU-internal and 223 IPU-final wordsin our corpus.The main focus of this paper is turn completion andcompletions can only take place if there is somethingleft to complete (i. e. after turn-internal words).
Itis still useful to be able to predict the duration ofturn-final words, though, as this is a prerequisite forthe related task of timing speaker changes.
For thisreason, we include both turn-internal and turn-finalwords in the analyses in Section 6.2.In the evaluation, we use the ASR?s word align-ments from recognition as gold standard (instead ofe.
g. hand-labelled timings), which are essentiallyequal to output from forced alignment.
However,when evaluating how well our timing component pre-dicts the following word?s duration, we need thatword to also be correctly recognized by ASR.
Thisholds for 1045 words in our corpus, for which wereport results in Section 6.3.6 ResultsWe evaluate the timing of our system with regards towhether completions are possible in general, when acompletion should be produced, and what the speechrate of the completion should be in the subsectionsbelow.6.1 Availability of Time to Make a DecisionWhile it is strictly speaking not part of the timingcomponent, a precondition to being able to speakjust-in-time is to ponder this decision sufficientlyearly as outlined above.Figure 3 shows a statistic of when our ASR firsthypothesizes a correct word relative to the word?send (which can be determined post-hoc from thefinal recognition result) on the corpus.
Most wordsare hypothesized before their actual endings, with amean of 134 ms (median: 110 ms) ahead.
This leaves12405101520?-0.48-0.40-0.32-0.24-0.16-0.080 ?0.08%decision point relative to end of word (in seconds)binned histogrammedian (-0.11)Figure 3: Statistics of when decisions can be first takenrelative to the word?s end (determined post-hoc).enough lookahead to synthesize a completion andfor some delays that must be taken into account forinput and output buffering in the sound card, whichtogether take around 50 ms in our system.Interestingly, lookahead differs widely for thespeakers in our corpus with means between 97 and237 ms. As can be seen in Figure 3, some words areonly hypothesized after the fact, or at least too lateto account for the inevitable lags, which renders im-possible successful turn-completions following thesewords.
However, the timing component should knowwhen it is too late ?
the holding time should be nega-tive ?
and could either not output the completion atthis point or e. g. back off to setting in one or morephones or syllables later (actually, back off until theholding time turns positive).6.2 When to Start SpeakingWe evaluate the strategies from Section 4 by com-paring the predicted holding times with the idealholding time, i. e. the time necessary to match theASR?s lookahead.Figure 3 can also be taken as depicting the errordistribution of our baseline strategy to find out whento start a completion: on average, the completionwill be early by 134 ms if it is uttered immediatelyand the distribution is somewhat skewed.
An unbi-ased baseline strategy is obtained by subtracting theglobal mean from the holding times.
This however re-quires the mean to be known in advance and is henceinflexible: the global mean may very well be differ-ent for other data sets as it already differs betweenmodelerror distribution metrics (in ms)mean median std dev MAEbaseline: all -134 -110 107 110baseline ??
0 23 107 63ASR-based : all -2 19 105 60IPU-internal 26 33 82 51IPU-final -148 -143 87 142TTS-based : all -3 4 85 45IPU-internal 12 11 77 41IPU-final -78 -76 83 79Table 1: Descriptive statistics of the error distributionsover estimated onset times for different duration models.speakers in our corpus.
The two other strategies?
er-ror distributions are less skewed, so we just reportthe distributions?
mean, median, and standard devi-ation,1 as well as the median absolute error (MAE)for the ASR-based, the TTS-based and the baselinestrategies in Table 1.As can be seen in Table 1, both strategies aresimilarly effective in predicting the average remain-ing time of a currently uttered word, reducing themean error close to zero, a significant improvementover starting a completion or next turn immediately.
(ANOVA with post-hoc Tukey?s honest significancedifferences test.)
While our two approaches performsimilarly when comparing the performance for allwords, there actually are differences when lookingseparately at IPU-internal and IPU-final words.
Inboth cases the TTS-based approach has a significantlylower bias (paired Student?s t-tests, p < 0.01).The bias of both strategies differs depending onwhether the current word is IPU-internal or -final.We believe this to be due to final lengthening: phonesare about 40 % longer in IPU-final words.
This is notcaptured by the ASR-based strategy and the length-ening may be stronger than what is predicted by thepronunciation model of the TTS we use.A low standard deviation of the error distributionis probably even more important than a low meanerror, as it is variability, or jitter, that makes a systemunpredictable to the user.
While there is no signifi-cant improvement of the ASR-based approach overthe baseline, the TTS-based approach significantlyoutperforms the other approaches with a 20 % re-1We prefer to report mean and std dev for bias and jitterseparately; notice that RMSE=?
?2 + ?2.125taskerror distribution metric (in ms)mean median std dev MAETTS-based : duration -5 4 75 45+ ASR-based : onset 26 33 82 51= end of word 25 30 100 81+ TTS-based : onset 12 11 77 41= end of word 7 10 94 74Table 2: Descriptive statistics of the error distributions forthe first spoken word of a completion.duction of jitter down to about the average phone?slength (Browne-Forsythe?s modified Levene?s test,p < 0.001).Regarding human performance in synchronousspeech, Cummins (2002) reports an MAE of 30 ms forthe synchronous condition.
However, MAE increasedto 56 ms when synchronizing to an (unsynchronouslyread) recording, a value which is in the range of ourresults (and with our system relying on similar input).6.3 How to SpeakAs explained in the task description, knowing whento speak is only one side of the medal, as a turncompletion itself must be integrated with the previ-ous speech in terms of duration, prosodic shape andloudness.Only our TTS-based strategy is capable of out-putting predictions for a future word; our ASR-basedapproach does not provide this information.
How-ever, both duration and onset estimation (the nextonset is identical to the end of the current word asestimated in Section 6.2) together determine the errorat the word?s end.
Hence, we report the error at thenext word?s end for the TTS strategy?s duration esti-mate combined with both strategies?
onset estimatesin Table 2.Duration prediction for the next word with theTTS-based strategy works similarly well as for on-going words (as in Section 6.2), with an MAE of45 ms (which is again in the range of human perfor-mance).
However, for the next word?s end to occurwhen the speaker?s word ends, correct onset estima-tion is just as important.
When we combine onsetestimation with duration prediction, errors add upand hence the error for the next word?s end is some-what higher than for either of the tasks alone, with astandard deviation of 94 ms and an MAE of 74 ms forthe TTS-based model, which again outperforms theASR-based model.So far, we have not evaluated the matching ofprosodic characteristics such as loudness and intona-tion (nor implemented their prediction).
We believethat simple matching (as we implemented for onsetand speech rate) is not as good a starting point forthese as they are more complex.
Instead, we believethese phenomena to mostly depend on communica-tive function, e. g. a co-optation having a wide pitch-range and relatively high loudness regardless of thecurrent speaker?s speech.
Additionally, pitch-rangewould have to be incrementally speaker-normalizedwhich results in some implementation difficulties.27 Demo Application: ShadowingTo get a feeling for the complete system and todemonstrate that our timing component works onlive input, we implemented a shadowing applicationwhich completes ?
or rather shadows ?
a user utter-ance word-by-word.
Given the prediction for the nextword?s onset time and duration it prepares the outputof that next word while the user is still speaking thepreceding word.
As the application expects to knowwhat the user is going to speak, the user is currentlylimited to telling the story of North Wind and theSun.Two examples of shadowings are shown in Ap-pendix A.3 As can be seen in the screenshots, thedecision points for all words are sufficiently earlybefore the next word, allowing for the next word?soutput generation to take place.
Overall, shadowingquality is good, with the exception of the second ?die?in the second example.
However, there is an ASRerror directly following (?aus?
instead of ?luft?)
andthe ASR?s alignment quality for ?sonne die?
is al-ready sub-optimal.
Also, notice that the two wordsfollowing the ASR error are not shadowed as per ourerror recovery strategy outlined in Section 5.2Edlund and Heldner (2007) report that for a reliable pitch-range estimation 10 to 20 seconds of voiced speech and hence ?in our view ?
twice the amount of audio is necessary.
This wouldhave reduced our corpus size by too much.3Audio files of the examples are available at http://www.ling.uni-potsdam.de/?timo/pub/shadowing/.1268 Discussion and Future WorkWe described the task of micro-timing, or micro-aligning a system response (in our case a turn com-pletion and shadowing a speaker) to the user?s speechbased on incremental ASR output and with both ASRand symbolic TTS output as duration models to pre-dict when and how a completion should be uttered.We have shown first of all, that a completion is pos-sible after most words, as an incremental ASR in asmall-enough domain can have a sufficient lookahead.Additionally, we have shown that the TTS-based du-ration model is better than both the baseline and theASR-based model.
Both the next word?s onset andduration can be predicted relatively well (?
= 77 msand ?
= 75 ms, respectively), and within the mar-gin of human performance in synchronously readingspeech.
It is interesting to note here that synchronousspeech is simplified in prosodic characteristics (Cum-mins, 2002), which presumably facilitates the task.Errors in speech rate estimation add up, so that thedeviation at the next word?s end is somewhat higher(?
= 94 ms).
Deviation will likely increase for longercompletions, underlining the need for an incremen-tal speech synthesis system which should allow toinstantly adapt output to changes in speech rate, con-tent, and possibly sentiment of the other speaker.Clearly, our duration modelling is rather simplisticand could likely be improved by combining ASR andTTS knowledge, more advanced (than a purely lin-ear) mapping when calculating relative speech rate,integration of phonetic and prosodic features fromthe ASR, and possibly more.
As currently imple-mented, improvements to the underlying TTS sys-tem (e. g. more ?conversational?
synthesis) shouldautomatically improve our model.
The TTS-basedapproach integrates additional, non-ASR knowledge,and hence it should be possible to single out thosedecision points after which a completion would be es-pecially error-prone, trading coverage against qualityof results.
Initial experiments support this idea andwe would like to extend it to a full error estimationcapability.We have focused the analysis of incrementallycomparing expected to actual speech rate to the taskof micro-aligning a turn-completion and shadowing aspeaker.
However, we believe that this capability canbe used in a broad range of tasks, e. g. in combinationwith word-based end-of-turn detection (Atterer et al,2008) to allow for swift turn taking.4 In fact, precisemicro-alignment of turn handovers could be used forcontrolled testing of linguistic/prosodic theory suchas the oscillator model of the timing of turn-taking(Wilson and Wilson, 2005).Finally, duration modelling can be used to quicklydetect deviations in speech rate (which may indicatehesitations or planning problems of the user) as theyhappen (rather than post-hoc), allowing to take thespeaker?s fluency into account in understanding andturn-taking coordination as outlined by Clark (2002).AcknowledgmentsThis work was funded by a DFG grant in the EmmyNoether programme.
We wish to thank the anony-mous reviewers for their very helpful comments.ReferencesMichaela Atterer, Timo Baumann, and David Schlangen.2008.
Towards incremental end-of-utterance detectionin dialogue systems.
In Proceedings of Coling, Manch-ester, UK.Timo Baumann, Michaela Atterer, and David Schlangen.2009.
Assessing and improving the performance ofspeech recognition for incremental systems.
In Pro-ceedings of NAACL, Boulder, USA.Timo Baumann.
2008.
Simulating spoken dialogue witha focus on realistic turn-taking.
In Proceedings of the13th ESSLLI Student Session, Hamburg, Germany.Leo Breiman, Jerome H. Friedman, Richard A. Olshen,and Charles J.
Stone.
1984.
Classification and regres-sion trees.
Wadsworth, Monterey.Caren Brinckmann and Ju?rgen Trouvain.
2003.
The roleof duration models and symbolic representation fortiming in synthetic speech.
International Journal ofSpeech Technology, 6(1):21?31.Herbert H. Clark.
2002.
Speaking in time.
Speech Com-munication, 36(1):5?13.Fred Cummins.
2002.
On synchronous speech.
AcousticResearch Letters Online, 3(1):7?11.Fred Cummins.
2009.
Rhythm as entrainment: The caseof synchronous speech.
Journal of Phonetics, 37(1):16?28.4Additionally, both our models consistently under-estimatethe duration of IPU-final words.
It should be possible to turn thisinto a feature by monitoring whether a word actually has endedwhen it was predicted to end.
If it is still ongoing, this may bean additional indicator that the word is turn-final.127David DeVault, Kenji Sagae, and David Traum.
2009.Can I finish?
Learning when to respond to incrementalinterpretation results in interactive dialogue.
In Pro-ceedings of SIGDIAL, London, UK.Jens Edlund and Mattias Heldner.
2007.
Underpinning/nailon/: Automatic Estimation of Pitch Range andSpeaker Relative Pitch.
In Speaker Classification II,volume 4441 of LNCS, pages 229?242.
Springer.Raquel Ferna?ndez and Jonathan Ginzburg.
2002.
Non-sentential utterances: A corpus-based study.
Traitementautomatique des languages, 43(2):13?42.Silvan Heintze, Timo Baumann, and David Schlangen.2010.
Comparing local and sequential models for sta-tistical incremental natural language understanding.
InProceedings of SIGDIAL, Tokyo, Japan.Jun-ichi Hirasawa, Mikio Nakano, Takeshi Kawabata, andKiyoaki Aikawa.
1999.
Effects of system barge-inresponses on user impressions.
In Proceedings of Eu-rospeech, Budapest, Hungary.International Phonetic Association, IPA.
1999.
Handbookof the International Phonetic Association.
CambridgeUniversity Press.Institut fu?r Phonetik und digitale Sprachverarbeitung,IPDS.
1994.
The Kiel corpus of read speech.
CD-ROM.Dennis H. Klatt.
1979.
Synthesis by rule of segmentaldurations in English sentences.
Frontiers of SpeechCommunication Research, pages 287?299.Gene H. Lerner.
2002.
Turn sharing: The choral co-production of talk in interaction.
In C. Ford, B. Fox,and S. Thompson, editors, The Language of Turn andSequence, chapter 9.
Oxford University Press.Gene H. Lerner.
2004.
Collaborative turn sequences.
InGene H. Lerner, editor, Conversation Analysis: Studiesfrom the First Generation, Pragmatics & Beyond, pages225?256.
John Benjamins, Amsterdam.John Local.
2007.
Phonetic detail and the organisation oftalk-in-interaction.
In Proceedings of the 16th ICPhS,Saarbru?cken, Germany.Massimo Poesio and Hannes Rieser.
2010.
Completions,coordination, and alignment in dialogue.
Dialogue andDiscourse, 1(1):1?89.Matthew Purver, Christine Howes, Patrick G. T. Healey,and Eleni Gregoromichelaki.
2009.
Split utterances indialogue: a corpus study.
In Proceedings of SIGDIAL,London, UK.Matthew Purver, Arash Eshghi, and Julian Hough.
2011.Incremental semantic construction in a dialogue system.In Proceedings of the 9th IWCS, Oxford, UK.Antoine Raux and Maxine Eskenazi.
2009.
A finite-state turn-taking model for spoken dialog systems.
InProceedings of NAACL, Boulder, USA.Harvey Sacks, Emanuel A. Schegloff, and Gail A. Jeffer-son.
1974.
A simplest systematic for the organizationof turn-taking in conversation.
Language, 50:735?996.Kenji Sagae, Gwen Christian, David DeVault, and DavidTraum.
2009.
Towards natural language understandingof partial speech recognition results in dialogue systems.In Proceedings of NAACL, Boulder, USA.Kenji Sagae, David DeVault, and David Traum.
2010.Interpretation of partial utterances in virtual humandialogue systems.
In Proceedings of NAACL.David Schlangen, Timo Baumann, Hendrik Buschmeier,Okko Bu?, Stefan Kopp, Gabriel Skantze, and RaminYaghoubzadeh.
2010.
Middleware for incrementalprocessing in conversational agents.
In Proceedings ofSIGDIAL, Tokyo, Japan.David Schlangen.
2006.
From reaction to prediction:Experiments with computational models of turn-taking.In Proceedings of Interspeech, Pittsburgh, USA.Marc Schro?der and Ju?rgen Trouvain.
2003.
The Ger-man text-to-speech synthesis system MARY: A toolfor research, development and teaching.
InternationalJournal of Speech Technology, 6(3):365?377.Gabriel Skantze and David Schlangen.
2009.
Incrementaldialogue processing in a micro-domain.
In Proceedingsof EACL, Athens, Greece.Kristina Skuplik.
1999.
Satzkooperationen.
Defini-tion und empirische Untersuchung.
Technical Report1999/03, SFB 360, Universita?t Bielefeld.Willie Walker, Paul Lamere, Philip Kwok, Bhiksha Raj,Rita Singh, Evandro Gouvea, Peter Wolf, and JoeWoelfel.
2004.
Sphinx-4: A flexible open sourceframework for speech recognition.
Technical ReportSMLI TR2004-0811, Sun Microsystems Inc.Nigel Ward, Olac Fuentes, and Alejandro Vega.
2010.Dialog prediction for a general model of turn-taking.In Proceedings of Interspeech, Tokyo, Japan.Karl Weilhammer and Susen Rabold.
2003.
Durational as-pects in turn taking.
In Proceedings of the 15th ICPhS,Barcelona, Spain.Margaret Wilson and Thomas P. Wilson.
2005.
An oscil-lator model of the timing of turn-taking.
PsychonomicBulletin & Review, 12(6):957?968.Takayoshi Yoshimura, Keiichi Tokuda, Takashi Masuko,Takao Kobayashi, and Tadashi Kitamura.
1998.
Du-ration modeling for HMM-based speech synthesis.
InProceedings of the 5th ICSLP, Sydney, Australia.128Appendix A Examples of ShadowingFigure 4: Example of shadowing for a file in our corpus (k73nord2).
The first line of labels shows the final ASR output,the second line shows the decision points for each word and the third and fourth lines show the system?s output (plannedoutput may overlap, hence two lines; in the system, an overlapped portion of a word is replaced by the following word?saudio).Figure 5: Example of shadowing with live input (verena2nord2).
Notice that ?Luft?
is predicted and synthesizedalthough it is (later) misunderstood by ASR as ?aus?, resulting in a missing shadowing of ?mit?
and ?ihren?.
In orderto not disturb the speaker, the system?s audio output was muted.129
