IntmmnmnInnmmmI IImmlmmmmURobust Parsing Using a Hidden Markov ModelWide R. Hogenhout Yuji MatsumotoNara Institute of Science and TechnologyAbstract .
Recent approaches to statistical parsing include those that estimate an approxi-mation of a stochastic, lexicalised gr~mm~x directly from a treebank and others that rebuildtrees with a number of trse-coustructing operators, which are applied in order accordingto a stochastic model when parsing a sentence.
In this paper we take an ehtirely di~erentapproach to statistical parsing, as we propose a method for parsing using a Hidden MarkovModel.
We describe the stochastic model and the tree construction procedure, and we reportresults on the Wall Street Journal Corpus.1 IntroductionRecent approaches to statistical parsing include those that estimate an approximation of astochastic, lexicalized ~rammar directly from a treebank (Colll-.q~ 1997; Charuiak, 1997) andothers that rebuild trees with a number of tree-construction perators, which are appliedin order according to a stochastic model when parsing a sentence (Magerman, I995; Ratna-parldd, 1997).
The results have been around 86% in labeled precision and recall on the WallStreet Journal treebank (Marcus, Santorini, and Marcinldewicz, 1994).In this paper we take an entirely different approach to statistical parsing.
We propose amethod for left to fight parsing using a Hidden Markov Model (HMM).
The results weobtain are not as good as the more general approaches mentioned above, which consider thewhole sentence rather then working in an incremental fashion, but the method does give anumber of interesting new perspectives.
In particular, it can be applied in an environmentthat requires left to right processing, such as a speech recognition system, it can easilyprocess text that has not been separated into sentences (for example when punctuation ismi~ing or when~processing ungrammatical, spoken text), and it can give a shallow parse(i.e., leaving out long distance dependencies) asit is focused On local context.
It also makesthe parsing process closer to the way humans process language, although we do not explorethis psychological spect in this paper.In the next three sections we will discuss the way we decide the syntactic ontext of a word(%raversal strings"), how this can be used for parsing and how a tree can be constructed~om them.
The following four sections discuss the HMM model used to predict a syntacticcontext for every word.
The last two sections discuss the results, conclusions and futureperspectives.2 Traversal StringsTake the sentence "I am singing in the rain."
This can be analyzed as indicated in figure I,where the first line of symbols above the text indicates parts of speech as used in the WallStreet Journal Corpus (for example, VBG stands for "verb -gerund or present participle").The abbreviations used for nonterminals are self-explanatory.We would like to characterize every word separately instead of having one intertwined struc-ture that models everything.
This is possible by tracing the path from every word throughthe tree up to the top of the tree.
This results in table 1.37mss$PP VBP VBG IN DT NNI am .~in~n~ in the rainF igure  1.
Example of Syntactic Analysis.We will call these strings (excluding the word and its tag) traversal strings.
It will be obviousthat this representation is very redundant, and it is exactly this property that we will exploit.But first we note it is also possible to carry out the inverse action of reconstructing thetree from a set of traversal strings.
Later we will describe a robust, heuristic algorithm forreconstructing the tree.The basic concept of parsing with traver?~l strings is that after seeing a word, one considersa number of possible tree contexts in which that word normally occurs.
The most likely onesare selected both by considering the likelihood of a context occun'ing with a word and thelikelihood of a context following another context.
As for the last relation, it is here that theredundance becomes me~nlngful; since neighboring traversal strings are often partially orcompletely equal.Oflazer (1996) used a similar structure called "vertex lists" which he defined as the path froma leaf to the root of the tree but, different from our definition, including the tag and the word.Oflazer uses vertex lists for error-tolerant tree-matching.
In some cases trees can be said tomatch approximately, and using vertex lists to quantify the amount of difference betweentrees, Oflazer shows how trees similar to a given tree can be retrieved from a database(treebank).
As will be clear from what follows, we use traversal strings in a completelydifferent way, and to the best of our knowledge these strings have not been used for parsingbefore.The work of Jcehi and Srinivas (1994) actually comes closer to our work.
While we attachtraversed strings to word-tag pairs, they do the same with elementary structures in a Lexi-calized Tree-Adjolnlng Grammar, called Supertags.
This is a partial tree that only containsone lexical word, as well as part of its context.
Joshi and Srinlvas show how, by statisti-cally choosing such structures for each word in a sentence, they are able to disambiguatesyntactical structure.
Note however that the results they give are not comparable to ours asthey only tested on short sentences, and report on the accuracy of Supertags rather thanbracket-accuracy or recall.I/PP -> NP -> Sam/VBP -> VP -> VP -> Ssinging/VBG -> VP -> VP -> Sin/ IN -> PP -> VP -> Sthe/DT -> NP -> PP -> VP -> Srain/NN -> NP -> PP -> VP -> S. / .
->  S?
ab le  1.
Example of Traversai Strings38mmm3 Parsing with Traversal StringsFigure 2 shows the system design.
When presented with a sentence, the first componentfinds the most likely set of part of speech tags (not shown) and traversal strings matchingthe words in the sentence.
After this a second component assembles a tree from the traversalstrings.l predict s p /P ~ \[ combinel am singing.., traversal Iamsm.~...J strings into 1.. Iamsmghlg...-\[ strings 1 tree !Figure 2.
System DesignThe prediction of traversal strings presents us with a problem, since traversal strings ofarbitrary length are too numerous to be predicted accurately.
To see our answer to thisproblem it is instructive to look at the analysis of a longer sentence, please see figure 3.
Inparticular, notice the shaded areas that show how traversal strings of neighboring words areoften equal or partially equal.
The most common relation that is seen is a 'shift', where thevertex at position n for word wi becomes vertex n + 1 for word wi+l.
At the bottom of thetraversal string one vertex is added, and nothing else changes.This illustrates the next step we will take.
Even after cutting off traversal strings at a fixedmaximum length, it is still possible to reconstruct the tree.
The dotted line in figure 3 showshow traversal strings are cut off at a maximum length of 5 vertices.
Having part of thetraversal string still leaves it possible to see that a particular word is likely to be in the samecontext as his neighbor.
More generally, we look at what subtrees are likely to share partof their context with other, neighboring subtrees.
We will show how doing this iterativelymakes it possible to restore the tree with a high degree of accuracy.4 Transforming Traversal Strings into TreesWe use a heuristic algorithm for reconstructing a tree from traversal strings.
This includes'~artial" traversal strings, but we simply refer to them as traversal strings since we willalways be working with partial traversal strings anyway.
This is a brief, informal descriptionof the algorithm.
A complete technical description is given in (Hogenhout, 1998).The algorithm is based on the heuristic that best matches hould go first.
The best match isdecided by checking neighboring strings (or, later, subtrees) for equal nonterminals starting atthe top of either side.
The pair with the most matching nonterminals i  merged as displayedin figure 4.
This process is repeated until one tree remains, or when there are no morematching neighbors.For example, the choicein/IN -> PP -> YP -> Sthe/DT -> NP -> PP -> VP -> Srain/NN -> NP -> PP -> VP -> S39Figure 3.
Similarity between Traversal Strings and cut at maximum length 5Wi W2 W3 W4 W5 W6 W7 Ws W9 WloFigure 4.
"\[~raversal Strings merged into (sub)treeswould initially be decided in favor of the and rain, and.after they are merged completely, thetop three nonterminals would be merged to those of i~There is an easy way of testing this algorithm.
One can take trees from a treebank, convertthe trees to traversal strings, then use the algorithm to reconstruct the trees.
Figure 5 showsthe labeled accuracy and recall of these reconstructed trees when compared to the originaltreebank trees, for various maximum traversal string lengths.The accuracy is calculated asnumber of identical brackets with identical nonterminalaccuracy -- number of brackets in system parse (1)and the recall asnumber of identical brackets with identical nonterminalrecall = number of brackets in treebank parse (2)which we will refer tO as "labeled accuracy" and "labeled recall" as opposed to the "unla-beled" versions of these measures that ignore nonterminals.40Even for long traversal strings the original tree is not reconstructed completely.
This happens,for example, when two identical nonterminals are siblings, as in the sentence "She ga~e \[NPthe man\] [NP a book\]."
It is of course possible to solve such problems with a post-processorthat tries to recognize such situations and correct hem whenever they arise.
But as can beseen it only involves a small percentage (about 2%) of all brackets and for this reason it isnot very significant at this stage.lOO9590% correct858075recall ~i | i ~, J .
.
.
.8 7 6 5 4 3 2travemal stdng lengthF igure  5.
Upper Bound Imposed by Tree Construction AlgorithmThe graph shows that if we are capable of predicting up to 5 or more vertices, the algorithmwill be able to do very well.
If we can only predict up to 4 vertices we still have a high upperbound, but it is slightly lower~ Predicting up to 3 or less vertices however will not produceuseful results.It must however be stressed that this .is only an upper bound and does not reflect theperformance of a useful system in any way.
The upper bound only helps to pin down theborder line of 4-5 vertices, and what really counts in practice is how the algorithm will dowhen the traversal strings that are predicted contain errors--as they undoubtedly will.5 Guessing Traversal StringsWe will now look at the question of how to predict traversal strings.
As will become clearwhen inspecting the equation this bears similarity to part-of-speech tagging.
But there isone factor that makes a big difference: we do not test on the correct raversal string, but onthe result of the tree that is reconstructed at the end.
In many cases the traversal string thatis guessed is not correct, but similar to the correct raversal string, and a Similar traversalstring will render much better esults at tree reconstruction than a completely different one.As usual our approach is maximizing the likelihood of the training dal~a.
We will use a HiddenMarkov Model which has traversal string-tag combinations as states and which produceswords as output.
We do not re~stimate probabilities using the Bantu-Welch algorithm (Bantu,1972) but we use smoothed Maximum Likelihood estimates from treebank data.Let us say we have a string of words wl...wn, and we are interested in guessing tags 7" --- t l  ...tnand traversal strings S = sl...s,~.
We also use s0 ----- to -- too = s~+l ~- t~+l = dummy as ashort-hand to signal the beginning and end of the sentence.41IIIIIIWe take the probability of a sentence to bep(Wl--.~.Ort) = ~'p(wl...w~lT,,q) (3)7",8~ ~pC~, Is,, t,)p(~,+,., t,+~ I,~.
t,), (4)7",8 in0corresponding to the transition and output probabilities of a hidden markov model.In practice the probabilities p(wi\[si, ti) and p(s~+l, ti+l\]s,, t,) can not be estimated irectlyusing Maximum Likelihood because of sparse data.
For this reason we smooth the estimateswith our version of lower-order models as follows:~(w~lsl, ti) = A,it~p(wils,, ti) + (1 - ;~,m)p(w, lt0 (5)where the interpolation factor sit, is ad jus ted  for different values of si and t, as suggested in(Bahl, Jelinek, and Mercer, 1983).
We also divide the si-ti pa~r values over different bucketsso that all pairs in the same bucket have the same ,~ parameter.
It should be noted that wehave a special word which stands for "nnlcnown word," to take care of words that were notseen in the training data.We do something similar for p(si+l, ti+llsi, tO, namely~(s~+lt,+lls.
tO = 6~,pCs~+~t~+iIs.
td + 6~mpCsi+lt~+lltd + 6~mpCs,+lt,+l) (fi)where of course 6~,ti + ~it, + 6~ti = 1.
The interpolation factors axe bucketed in the samew~y.Using the obtained model we choose T and ,q by maximizing the probability of the sentencethat we wish to analyse:(~r, s)" = arsma~p((7", S)lw2...w.) (7)(r,s)nargmax ITp(w~ls~, tdp(si+2, ti+l Is.
to (8)?r,s) ~owhich can be resolved using the Viterbi-algorithm (Viterbi, 1967).6 Select ion of Part  of  Speech TagsThe process outlined above still has one problem that will be central in the rest of thediscussion.
The number of traversal strings is easily a few thousand, and the number of partof speech tag-traversal string pairs is even larger.
Clearly, the computational complexity ofthe algorithm is in calculating (8).
But, given a word and the history up to that word, mosttags and traversal strings can be ruled out immediately.
We will therefore only consider afraction of the possible part of speech tags and traversal strings.
This section will discusshow we select part of speech tags.The equation we use for selecting a tag is similar to the standard tagging HMM based model.We pretend for the time being that we are dealing with another stochastic process, namelyone that only generates tags.
We assume thatp(wl...w~) = y~p(wl...w~, 7") (9)Tn~.
~_~ l'Ip(w~lti)p(ti+llti) (10)T i=Obut we do not really use this model, we only' use the idea behind it to approximate theprobability of a tag.
We find the most likely tags after seeing word i using the following42mm\[\]\[\]mmmnmmmmmmmmmmmmapproximation:1;~' = argm.axp(t: i  = t:\['wl...'w.i) (11)t= argmax ~ p(t, = tit,-1 = ~-l)p(w, lt,)atC/- I, ~-1) (12)t= argmax ~ p(t, = tlt,-~ = ~-~)p(w, lt,)tUi--1O~C.,,,)4i - 1, (s ,u) )6(u,  ua - l )  (13)O,,'.,.
)(E .ei- 1where s is a traversal string, the symbol Bi - I  indicates the set of tag-traversal string pairsthat is being considered for word wi-l, and ~ indicates the "forward probability" accordingto the HMM.
As usual 64u, u~-l) -- 1 i fu = u~_~ and 0 otherwise.
We will discuss later howthe set B~-I is chosen, but this of course depends on the tags selected for the word wi-1.We distinguish between ~, (tagging model) and ~(~,~) (traversal string model).We take two significant assumptions at this point.
First, we do not really use the HMMindicated in 410), but in equation (12) we restrict ourselves to the forward probability.
Thesecond assumption we take is (13), i.e., we estimate the probability of the previous tag bythe tag-traversal string pairs that were selected for the previous word.
Using this method wedo not need to implement the markov model for tags, we only need the tables for p(t, lti_l)and p(wdti ).
As we already need the last one for the traversal string model, we only needthe (small) table p(ti\[t~-l) especially for tagging.We must emphasize that the tagging described here is only a first estimate.
We consider themost ~lcely one, two or three tags according to this model and discard the rest.
Once theyare selected, these probabilities are discarded and we return to the regular model.
The nextsection will describe how the tags are selected in the next phase.7 Se lec t ion  o f  Traversa l  S t r ings  : F i rs t  PhaseThe next problem is how to select a few traversal strings given a word and a few tags, one ofwhich is likely to be correct.
The model we use for this pre~selection is actually more simple;as we ignore the selected traversal strings for previous words.
1 From the corpus we directlyestimate in Maximum Likelihood fashionP4w,, si, ti) (14)and select the most likely travexsal strings si from this table.
If there are too few samplesfor a particular word wi, the list is completed with the more general distributionP(s,,t,), (15)again maximizing over si.
We will have to consider that we do not have a single tag butseveral options, but we will first pretend that we do have one single tag.Figure 6 shows the results of this first phase, in case the maximum length of traversal stringsis set to 5.
If the best 50 candidates are selected according to 414), supplemented withselection according to (15) if necessary, we have the correct candidate between them about80% of the time.
That means that for 20% of the words, we can only hope that a similartraver-~al string will be available for them.
If we use the best 300 candidates, we will miss thecorrect candidate for about one word per sentence.
We must however emphas!ze two points:1.
The question is not only if we can select the correct candidate.
It is crucial that, whena wrong candidate is chosen, this is at least similar to the correct candidate.2.
Figure 6 indicates the percentage for traversal strings cut of at length 5.
If traversalstings of a different maximum length are used, this will change (the higher the maximumlength, the lower the percentage of hits).430.950.9% correct 0.85traversalstring hit0.80.75  I I I i5O IO0 150 2O0 25O 3O0number of candidates phase 1Figure 6.
Hit percentage for first phaseNow we return to the tagging problem; after all we do not have the right tag available to us.We solve this, heuristically, as follows.
Let a be the most likely tag, b the second most likelyand c the third.- If p(a)/p(b) > 50, select 300 candidates for tag a and ignore other tags.- If 50 > p(a)/p(b) > 4 we select 300 candidates for tag a and more 100 candidates fortag b.- If 4 >_ p(a)/p(b) we select 300 candidates for tag a, 200 candidates for tag b and 100candidates for tag c.This scheme gives more candidates for more ambiguous words, but as about 80% of all wordsfall in the first category and only 9% in the last category, this is not so bad.
This list willcontain the correct raversal string about 95% of the time.8 Selection of Traversal Strings : Second PhaseThe previous ection explained how initial candidates can be selected quickly from all possiblesets.
After these initial candidates were selected, the transition and output probabilities arecalculated.
Let again B~- 1 be the set of candidates considered for word w~_ 1.
Then we needto calculate (regrouping the product as compared to (8)) the quantity,~(8,t,) = ~ ,~(8,t)p{,~,ls,,t,)p(8,,~,ls,0 (16)(~,t)~Bi_1where we set1 if s0 =dummy and Co =dummya(s0to) -- 0 otherwise (17)and Bo = {(dummy, dummy)}.
The sum in (16) reflects almost all of the time that thecalculation process takes up.
But equation (16) gives a much more accurate estimate oflikelihood than the rather primitive word-based selection (14), so once this sum is calculatedwe have a much better idea of the likelihood of candidates.
For this reason we use two criteria:Note that using a technique similar to that for part of speech tags is not an option as this is exactlywhat we are trying to avoid doing for all possible traversai strings.44- In the first phase we use equation (14) and select he best p candidates.
(As explained,depending on tagging confidence we vary the number of candidates, o # should bethought of as an average.
)- In the second phase we use equation (16) and select he best 7 candidates.It will be clear that we can choose 7 ((/z.
We have illustrated this in figure 7, which displaysthe percentage ofcorrect candidates for various values for 7, again using a maximum traversalstring length.of 5.
Note that the computational complexity of the Viterbi algorithm will beO(p~n) where n is sentence length.9O8O7O6O% correctcandidate 504030f f  .
.
.
.~ a t e  available after phase 2 - -' correct candidate ?ho6en by Viterbl algodthmr i i i i5 10 15 20 25 30number of candidates phase 2F igure  7.
Hit percentage for second phaseFigure 7gives the percentage of cases in which the correct candidate is available (the upperline) and also the percentage of cases in which the correct candidate is chosen by the Viterbialgorithm.
A remarkable fact arises from this figure: the percentage of traversal strings thatare chosen correctly stabilizes at about 7 = 4.
From that point the percentage is about 50%and while increasing 7 increases the chance that the correct one will be available, choosingit becomes more diiBcult and these two effects cancel each other out.
Nevertheless the resultcontinues to improve for higher 7, as better alternatives become available.
We will put 7 to15 as a higher number contributes little more to the final scores.9 ParsingWe have now dealt with all parts of the parsing process.
Whenever a new word is seen, a fewtags are selected according to (13).
After this a set of about 300 (depending on the confidencein the tags) traversal strings is selected according to (14) or (15).
The forward probabilityof these candidates i calculated (16) and this is used to further reduce the candidates to 15tag-traversal string pairs.
This set is saved with their forward probabilities, and when theend-of-sentence signal is received the best series is given by the Viterbi algorithm.
A tree isthen produced according to the algorithm described in section 4.Until now we h~ve set the maximum traversal string length to 5 but now we can show howvariation in the maximum length affects the result.
The experiments we present here werecarried out with data from the Wall Street Journal Treebank.
Parameters were estimated withthe first 22 sections (over 40,000 sentences), section 24 was used for smoothing (interpolation)45go85oo75% corredbracket 7O65603\] Ij /4 5 6 7mmdmmm ~nmm~  ~Figure 8.
Precision and recall plotted again.qt maximum traversal string lengthand section 23 (2,416 sentences) was used exclusively for testing.
Figure 8 shows the labeledaccuracy and recall for various maximum lengths that result from this data.This shows that the optimal length is about 4, 5 or perhaps 6.
This picture is slightly favoringthe shorter lengths, since # and ~ are fixed while the longer lengths have more candidatesto choose from.
But on the other handl keeping p and ~7 fixed corresponds to giving thealgorithm a certain time and letting it do its best in the given time?
The longer lengths alsohave a disadvantage in that they lead to larger tables, using more memory.The differences between 4, 5 and 6 axe minor, and the performance d grades eriously at 3or 7.
This shows that a maximum of 5 is a sensible choice.
The first colnm, of table 2 givesdetailed information about the final performance.
It is also possible to restrict he parserto lower level structures, t~iclng only those parts which are the most safe, namely low-levelbrackets that do not depend on long distance dependencies.
We carried this out by removingbrackets covering more than three words and some particular nonterminals that often resultin errors, such as SBAR~ These results axe indicated in the ~Shallow Parsing z col-re,Table 2.
Parsing ResultsMeasure Regular Score Shallow Parsinglabeled precision 75.6% 87.4%labeled recall 72.9% 37.9%unlabeled precision 79.5% 89.2%unlabeled recall 76.6% 38.9%crossing brackets per sentence 2.31 0.44tagging accuracy 94.4%speed on Spaxc Station 20 6.5 words/second10 DiscussionThe method we propose analyses language indirectly as a regular language.
This makes itimpossible to use long distance dependencies, but nevertheless the experiment shows that it46|mmU\[\]n\[\]mmmUUmUmm\[\]\[\]mmmmperforms quite reasonable and is very robust.The score is less than the scores obtained by systems that consider the entire sentence with,in particular, the headwords of phrases.
But the method creates new possibilities uch asprocessing ungrammatical text and processing unpunctuated text.
Shallow parsing is also apossible application.As far as future directions are concerned, we would like to mention that our parsing strategyis not limited, to regular languages and HMM models.
It is possible to switch to a history-based approach, where the choice of si depends on both the words wl...w~ and all earliertags and traversal strings chosen by the system.
In that case a statistical decision tree or amarkov field can be used to model the optimal choice for s~ after seeing word wi.11 AcknowledgementsWe would like to thank the anonymous reviewers for their useful comments.ReferencesBald, Lalit R., Frederick Jelinek, and Robert L. Mercer.
1983.
A maximum likelihoodapproach to continuous peech recognition.
IEEE Transactions on Pattern Analysis andMachine Intelligence, PAMI-5(2):l?9-190.Baum, L.E.
1972.
An inequality and associated maximization technique in statisticalestimation for probabilistic functions of Markov processes.
Inequalities, 3:1-8.Charuiak, E. 1997.
Statistical parsing with a context-f~ee grammar and word statistics.
InProceedings of the Fourteenth National Conference on Artificial Intelligence, AAAI, pages598-603.Collins, M. J.
1997.
Three generative, lexicalised models for statistical parsing.
In Proceed-ings of the 85th Annual Meeting of the Association for Computational Linguistics and 8thConference of the European Chapter of the Association for Computational Linguistics, pages16-23.Hogsnhout, Wide 1t.
1998.
Supervised Learning of Syntactic Structure.
Ph.D. thesis, NaraInstitute of Science and Technology.Joshi, Aravind K. and B. Srinivas.
1994.
Disambiguation of super parts of speech (or su-pertags): Almost parsing.
In Proceedings ofthe 15th International Conference on Computa-tional Linguistics (COLING-g4), pages 154-160.Magerman, D. M. 1995.
Statistical declsion-tree models for parsing.
In Proceedings ofthe33d Annual Meeting of the Association for Computational Linguistics, pages 276-283.Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz.
1994.
Build-ing a large annotated corpus of English: the Penn Treebank.
Computational Linguistics,19(2):313--330.Oflazer, Kemal.
1996.
Error-tolerant tree matching.
In Proceedings ofthe 16th InternationalConference on Computational Linguistics (COLING-96), pages 860--864.Ratnaparkhi, Adwait.
1997.
A linear observed time statistical parser based on maximumentropy models.
In Proceedings of the Second Conference on Empirical Methods in NaturalLanguage Processing.Viterbi, A.J.
1967.
Error bounds for convolutional codes and an asymptotically optimaldecoding algorithm.
IEEE Transactions on Information Theory, 13:260-269.47
