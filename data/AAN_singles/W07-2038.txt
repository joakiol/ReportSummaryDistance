Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 183?186,Prague, June 2007. c?2007 Association for Computational LinguisticsILK2: Semantic Role Labelling for Catalan and Spanish using TiMBLRoser Morante, Bertjan BusserILK, Dept.
of Language and Information SciencesTilburg University, P.O.Box 90153NL-5000 LE Tilburg, The Netherlands{R.Morante,G.J.Busser}@uvt.nlAbstractIn this paper we present a semantic role la-beling system submitted to the task Multi-level Semantic Annotation of Catalan andSpanish in the context of SemEval?2007.The core of the system is a memory?basedclassifier that makes use of full syntactic in-formation.
Building on standard features,we train two classifiers to predict separatelythe semantic class of the verb and the seman-tic roles.1 IntroductionSemantic role labelling (SRL) has been addressedin the CoNLL?2004 and CoNLL?2005 SharedTasks (Carreras and Ma`rquez, 2004; Carreras andMa`rquez, 2005) for English.
In the task MultilevelSemantic Annotation of Catalan and Spanish of theSemEval competition 2007, the target are two differ-ent languages.
The general SRL task consists of twotasks: prediction of semantic roles (SR) and predic-tion of the semantic class of the verb (SC).The data provided in the task (Ma`rquez et al,2007) are sentences annotated with lemma, POStags, syntactic information, semantic roles, and thesemantic classes of the verb.
A training corpus forCatalan (ca.3LB) and another for Spanish (sp.3LB)are provided.
Although the setting is similar tothe CoNLL?Shared Task 2005, three relevant differ-ences are that the corpora are significantly smaller,that the syntactic information is based on a manu-ally corrected treebank, which contains also syntac-tic functions (i.e.
direct object, indirect object, etc.
),and that the set of semantic roles is larger, especiallyfor core arguments.Our goal is to check whether simple individualsystems could produce competitive results in bothsubtasks, and whether they would be robust enoughwhen applied to two languages and to the held?outtest sets provided.2 System descriptionWe approach the SRL task as two classificationproblems: prediction of SR and prediction of SC.We hypothesize that the two problems can be solvedin the same way for both languages.
We build twovery similar systems that differ only in some of thefeatures used, as we explain below.The task is solved in three phases: 1) A pre?processing phase that is very similar to the sequen-tialization in (Ma`rquez et al, 2005).
We call it focusselection.
It consists of identifying the potential can-didates to be assigned a semantic role or a semanticverb class.
2) The classification.
3) Some limitedpostprocessing.2.1 Focus selectionThe system starts by finding the target verb (whichis marked in the corpus as such).
Then, it findsthe complete form of the verb (that in the corpus istagged as verb group, infinitive, gerund, etc.)
andthe clause boundaries in order to look for the siblingsof the verb that are under the same clause.
Our as-sumption is that all siblings of the verb are potentialcandidates for semantic roles.
The focus selectionprocess produces two groups of focus tokens: on theone hand, the verbs and, on the other, the siblings of183the verbs.
These tokens will be the instances in eachtraining set.
Table 1 shows the number of trainingand test instances for each subtask.Training 3LB Test 3LB Test CESSCa.
Sp.
Ca.
Sp.
Ca.
Sp.SR 23202 24668 1335 1451 1241 1186SC 8932 9707 510 615 463 465Table 1: Number of instances per corpus for each task (?Ca?stands for Catalan, ?Sp?
stands for Spanish).2.2 ClassificationIn both systems we approach the classification taskin one step, predicting directly the SR and the SCclass.
This means that in the SR task we do notperform a previous classification to select the tokensthat might be assigned a role.
We assume that allverbs belong to a class.
As for the SR, we assumethat most siblings of the verb will have a class, ex-cept for those that have syntactic functions AO, ET,MOD, NEG, IMPERS, PASS, and VOC.
The sib-lings that do not have a semantic role are assignedthe NONE tag.
Because the corpus is small and be-cause the amount of instances with a NONE class isproportionally low, we do not consider it necessaryto filter these cases.Regarding the learning algorithm, we use theIB1 classifier as implemented in TiMBL (version5.1) (Daelemans et al, 2004), a supervised induc-tive algorithm for learning classification tasks basedon the k nearest neighbor (k-nn) algorithm.
In IB1,similarity is defined by a feature?level distance met-ric between a test instance and a memorized traininginstance.
The metric combines a per?feature value?based distance metric with global feature weightsthat account for relative differences in importanceof the features.The TiMBL parameters used in the systems arethe IB1 algorithm, the Jeffrey Divergence as featuremetric, MVDM threshold at level 1, weighting us-ing GainRatio, k=11, and weighting neighbors asfunction of their Inverse Linear Distance (for detailswe refer the reader to the TiMBL reference guide(Daelemans et al, 2004)).As for the features, we started by using the samefeature set for both classifiers and then, after someexperimentation, we decided to use slightly differ-ent feature sets for the two sub-tasks.
Most of thefeatures we designed are features that have becomestandard for the SRL task (Gildea and Jurafsky,2002; Xue and Palmer, 2004; Carreras and Ma`rquez,2004; Carreras and Ma`rquez, 2005).
In our system,the features relate to the verb, the verb siblings, whatwe take to be the content word of the siblings, theclause, and the relation verb?arguments.
Addition-ally, we added lexical features extracted from theverb lexicon provided for the task, and from Word-Net.After experimenting with 323 features, we se-lected 98 for the SR task and 77 for the SC subclass.In order to select the features, we started with a basicsystem, the results of which were used as a baseline.Every new feature that was added to the basic systemwas evaluated in terms of average accuracy in 10-fold cross-validation experiments; if it improved theperformance on held-out data, it was added to the se-lection.
One problem with this hill-climbing methodis that the selection of features is determined by theorder in which the features have been introduced.We also performed experiments applying the featureselection process reported in (Tjong Kim Sang et al,2005), a bi-directional hill climbing process.
How-ever, experiments with this advanced method did notproduce a better selection of features.The features for the SR prediction subtask are thefollowing:?
Features on the verb (6).
They are shared by allthe instances that represent phrases belonging to thesame clause:VForm; VLemma; VCau: binary feature that indicate if theverb is in a causative construction with hacer, fer or if the mainverb is causar; VPron, VImp, VPass: binary features that indi-cate if the verb is pronominal, impersonal, and in passive formrespectively.?
Features on the sibling in focus (12):SibSynCat: syntactic category; SibSynFunc: syntacticfunction; SibPrep: preposition; SibLemW1, SibPOSW1,SibLemW2, SibPOSW2, SibLemW3, SibPOSW3: lemmaand POS tag of the first, second and third words of the sibling;SibRelPos: position of the sibling in relation to the verb (PREor POST); Sib+1RelPos: position of the sibling next to the cur-rent phrase in relation to the verb (PRE or POST); SibAbsPos:absolute position of the sibling in the clause.?
Features that describe the properties of the contentword (CW) of the focus sibling (13): in the case ofprepositional phrases the CW is the head of the firstnoun phrase; in cases of coordination, we only takethe first element of the coordination.184CWord; CWLemma; CWPOS: we take only the first char-acter of the POS tags provided; CWPOSType: the type ofPOS, second character of the POS tags provided; CWGender;CWne: binary feature that indicates if the CW is a named en-tity; CWtmp, CWloc: binary features that indicate if the CWis a temporal or a locative adverb respectively; CW+2POS,CW+3POS: POS of the second and third words after CW.CWwnsc1, CWwnsc2, CWwnsc3: additionally, if the CWis a noun, we extract information from WordNet (Fellbaum,1998) about the first, second, and third more frequent seman-tic classes of the CW in WordNet.
We cannot decide on a sin-gle one because the corpus is not disambiguated.
The seman-tic class corresponds to the lexicographer files in WN3.0.
Fornouns there are 25 file numbers.?
Features on the clause (24):CCtot: total number of siblings with function CC (cir-cumstancial complement); SUJRelPos, CAGRelPos, CDRel-Pos, CIRelPos, ATRRelPos, CPREDRelPos, CREGRelPos:relative positions of siblings with functions SUJ, CAG, CD,CI,ATR, CPRED, and CREG in relation to verb (PRE orPOST); SEsib: binary feature that indicates if the clause con-tains a verbal se; SIBtot: total number of verb siblings inthe clause; SynFuncSib8, SynCatSib8, PrepSib8,W1Sib8,W2Sib8, W3Sib8, W4Sib8, SynFuncSib9, SynCatSib9,PrepSib9, W1Sib9, W2Sib9, W3Sib9, W4Sib9: syntacticfunction, syntactic category, preposition, and first to fourthword of siblings 8 and 9.?
Features extracted from the lexicon of verbalframes (43) that the task organizers provided.
Weaccess the lexicon to check if it is possible for a verbto have a certain semantic role.
We check it for allsemantic role classes, except for ArgX-Ag, ArgX-Cau, ArgX-Pat, ArgX-Tem because they proved notto be informative.
The features are binary.For the SC prediction task the features are similar,but not exactly the same.
Both systems contain somefeatures about all candidate arguments.
We point outthe differences:?
Features that are in the SR system and that are notin the SC system:Verb form (VForm), verb lemma (VLemma), absolute po-sition of the sibling in the clause (SibAbsPos), function ofthe sibling (SibSynFunc), preposition of the sibling (SibPrep),POS tag of the second and third words after CW (CW+2POS,CW+3POS), information about the WN classes of the CW(CWwnsc1, CWwnsc2, CWwnsc3), feature about the CW be-ing a named entity (CWne, SIBtot), syntactic function, syn-tactic category, preposition and first to fourth word of sib-lings 8 and 9 (SynFuncSib8, SynCatSib8, PrepSib8,W1Sib8,W2Sib8, W3Sib8, W4Sib8, SynFuncSib9, SynCatSib9,PrepSib9, W1Sib9, W2Sib9, W3Sib9, W4Sib9).?
Features that are only in the SC system:AllCats: vector of the syntactic categories of the siblings inthe order that they appear in the clause; AllFuncs: vector of thefunctions of the siblings in the order that they appear; AllFuncs-Bin vector with eight binary values that represent if a siblingwith that function is present or not; Sib+1Prep, Sib+2Prep:prepositions of the two siblings after the verb.2.3 PostprocessingAs for the postprocessing phase, it consists of sixsimple rules to correct some basic errors in predict-ing some types of ArgM arguments.
It only appliesto the SR task.
The rules are the following ones:1.
If prediction = ArgM?LOC, ArgM?MNR or ArgM?ADV,and either {SibPrep = ?durante?
or ?durant?
}, or {SibSynCat =sn and one of the WN semantic classes = 28}, then prediction =ArgM-TMP.2.
If prediction = ArgM?LOC, ArgM?MNR or ArgM?ADV,and CWLemma is a temporal adverb, then prediction = ArgM?TMP.3.
If prediction = ArgM?TMP and one of the WN classes = 15,then prediction = ArgM?LOC.4.
If prediction = ArgM?TMP, ArgM-MNR or ArgM-ADV, andCWLemma = locative adverb, then prediction = ArgM-LOC.5.
If prediction = ArgM-TMP or ArgM-ADV, and CWwnsc1 =15, and SibPrep = ?en?
or ?desde?
or ?hacia?
or ?a?
or ?des de?or ?cap a?, then prediction = ArgM?LOC.6.
If prediction = ArgM?ADV and CWLemma = causal con-junction, then prediction = ArgM?CAU.We are aware of the fact that these are very simplerules and that more elaborate postprocessing tech-niques can be applied, like the ones used in (TjongKim Sang et al, 2005) in order to make sure that thesame role was not predicted more than once in thesame clause.SR TASK Perf.Props Precision Recall F?=1Test ca.3LB 73.35% 86.59% 85.91% 86.25Test ca.CESS 60.55% 82.60% 78.03% 80.25Overall ca 67.24% 84.72% 82.12% 83.40Test sp.3LB 68.07% 83.05% 82.54% 82.80Test sp.CESS 73.76% 85.88% 85.80% 85.84Overall sp 70.52% 84.30% 83.98% 84.14Overall SR 68.96% 84.50% 83.07% 83.78SC TASK Perf.Props Precision Recall F?=1Test ca.3LB 90.86% 90.30% 88.72% 89.50Test ca.CESS 90.41% 90.20% 88.27% 89.22Overall ca 90.64% 90.25% 88.50% 89.37Test sp.3LB 84.12% 80.00% 78.44% 79.21Test sp.CESS 90.54% 89.89% 89.89% 89.89Overall sp 86.88% 84.30% 83.36% 83.83Overall SC 88.67% 87.12% 85.81% 86.46SRL TASK Perf.Props Precision Recall F?=1Overall ca ?
86.44% 84.08 % 85.24Overall sp ?
84.30% 83.78 % 84.04Overall SRL ?
85.32% 83.93 % 84.62Table 2: Overall results in the SR (above), SC (middle),and general SRL tasks (?Perf.Props?
: perfect propositions; ?ca?
:Catalan; ?sp?
: Spanish).1853 ResultsThe overall official results of the system are shownin Table 2.
The SC system performs better (over-all F1 = 86.46) than the SR system (overall F1 =83.78).
In global, the systems perform better forCatalan (overall F1 = 85.24) than for Spanish (over-all F1 = 84.04), although the SC system performsbetter for Catalan (89.37 vs. 86.46), and the SR sys-tem performs better for Spanish (84.14 vs 83.40).Striking results are that the SR system gets signif-icantly better results with the held?out test for Span-ish, and that both of the complete SRL systems getsignificantly better results with the held?out test forSpanish.
This might be due to differences in the pro-cess of gathering and annotation of the corpus.SP?CESS F Precision Recall F?=1Overall 85.88% 85.80% 85.84Arg0?AGT 16.19% 92.83% 92.41% 92.62Arg0?CAU 1.23% 100% 50% 66.67Arg1 1.79% 88.46% 82.14% 85.19Arg1?LOC 0.11% 0.00% 0.00% 0.00Arg1?PAT 20.09% 93.82% 94.19% 94.00Arg1?TEM 14.08% 86.54% 91.84% 89.11Arg2 2.05% 68.00% 77.27% 72.34Arg2?ATR 9.88% 91.67% 90.41% 91.03Arg2?BEN 2.40% 96.30% 100.00% 98.11Arg2?EFI 0.19% 0.00% 0.00% 0.00Arg2?EXT 0.19% 0.00% 0.00% 0.00Arg2?LOC 1.13% 0.00% 0.00% 0.00Arg2?PAT 0.01% 0.00% 0.00% 0.00Arg3?ATR 0.05% 0.00% 0.00% 0.00Arg3?BEN 0.16% 100.00% 100.00% 100.00Arg3?EIN 0.08% 0.00% 0.00% 0.00Arg3?FIN 0.04% 100.00% 33.33% 50.00Arg3?ORI 0.29% 0.00% 0.00% 0.00Arg4?DES 0.60% 83.33% 83.33% 83.33ArgL 0.71% 16.67% 20.00% 18.18ArgM?ADV 10.67% 68.12% 68.12% 68.12ArgM?CAU 1.50% 55.56% 45.45% 50.00ArgM?FIN 1.30% 64.71% 84.62% 73.33ArgM?LOC 4.94% 78.21% 77.22% 77.71ArgM?MNR 2.28% 36.36% 57.14% 44.44ArgM?TMP 7.19% 88.75% 81.61% 85.03V ?
100.00% 100.00% 100.00Table 3: Detailed results on the Spanish CESS?ECE test cor-pus for the SR subtask.
F: frequency of the semantic roles inthe training corpus, without counting V.Table 3 shows detailed results on the SpanishCESS?ECE corpus for the SR task.
Low scores aregenerally related to low frequency of the SR in thetraining corpus, and high scores are related to highfrequency or to overt marking of the SR.4 ConclusionsWe have presented two memory?based SRL systemsthat make use of full syntactic information and ap-proach the tasks in three steps.
Results show thatrather simple individual systems can produce com-petitive results in both tasks, and that they are ro-bust enough to be applied to two languages and tothe held?out test sets provided.
Improvements of thesystems would consist in improving the focus selec-tion step, and applying more elaborate techniquesfor feature selection and postprocessing.AcknowledgementsThis research has been funded by the postdoctoral grantEX2005?1145 awarded by the Ministerio de Educacio?n y Cien-cia of Spain to the project Te?cnicas semiautoma?ticas para el eti-quetado de roles sema?nticos en corpus del espan?ol.
We wouldlike to thank Martin Reynaert, Caroline Sporleder, Antal vanden Bosch, and the anonymous reviewers for their commentsand suggestions.ReferencesX.
Carreras and Ll.
Ma`rquez.
2004.
Introduction to theCoNLL-2004 shared task: Semantic role labeling.
In Pro-ceedings of CoNLL?2004, Boston MA, USA.X.
Carreras and Ll.
Ma`rquez.
2005.
Introduction to theCoNLL-2005 shared task: Semantic role labeling.
In Pro-ceedings of CoNLL?2005, Ann Arbor, Michigan, June.W.
Daelemans, J. Zavrel, K. Van der Sloot, and A.
Van denBosch.
2004.
TiMBL: Tilburg memory based learner, ver-sion 5.1, reference guide.
Technical Report Series 04-02,ILK, Tilburg, The Netherlands.Christiane Fellbaum, editor.
1998.
WordNet: An ElectronicDatabase.
MIT Press, Cambridge, MA.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling of seman-tic roles.
Computational Linguistics, 28(3):245?288.LL.
Ma`rquez, P. Comas, J. Gime?nez, and N. Catala`.
2005.
Se-mantic role labeling as sequential tagging.
In Proceedings ofCoNLL?2005, Ann Arbor, Michigan.Ll.
Ma`rquez, M.A.
Mart?
?, M.
Taule?, and L. Villarejo.
2007.SemEval-2007 Task 09: Multilevel semantic annotation ofcatalan and spanish.
In Proceedings of SemEval-2007, the4th Workshop on Semantic Evaluations, Prague, Czech Re-public.E.
Tjong Kim Sang, S. Canisius, A. van den Bosch, andT.
Bogers.
2005.
Applying spelling error correction tech-niques for improving semantic role labelling.
In Proceed-ings of CoNLL-2005, pages 229?232, Ann Arbor, Michigan.N.
Xue and M. Palmer.
2004.
Calibrating features for semanticrole labeling.
In Proceedings of 2004 Conference on Empir-ical Methods in Natural Language Processing, Barcelona,Spain.186
