Class-Based n-gram Models of NaturalLanguagePeter F. Brown"Peter V. deSouza*Robert  L. Mercer*IBM T. J. Watson Research CenterVincent J. Della Pietra*Jenifer C. Lai*We address the problem of predicting a word from previous words in a sample of text.
In particular,we discuss n-gram models based on classes of words.
We also discuss everal statistical algorithmsfor assigning words to classes based on the frequency of their co-occurrence with other words.
Wefind that we are able to extract classes that have the flavor of either syntactically based groupingsor semantically based groupings, depending on the nature of the underlying statistics.1.
IntroductionIn a number of natural language processing tasks, we face the problem of recovering astring of English words after it has been garbled by passage through a noisy channel.To tackle this problem successfully, we must be able to estimate the probability withwhich any particular string of English words will be presented as input to the noisychannel.
In this paper, we discuss a method for making such estimates.
We also discussthe related topic of assigning words to classes according to statistical behavior in alarge body of text.In the next section, we review the concept of a language model and give a defini-tion of n-gram models.
In Section 3, we look at the subset of n-gram models in whichthe words are divided into classes.
We show that for n = 2 the maximum likelihoodassignment of words to classes is equivalent to the assignment for which the averagemutual information of adjacent classes is greatest.
Finding an optimal assignment ofwords to classes is computationally hard, but we describe two algorithms for finding asuboptimal ssignment.
In Section 4, we apply mutual information to two other formsof word clustering.
First, we use it to find pairs of words that function together as asingle lexical entity.
Then, by examining the probability that two words will appearwithin a reasonable distance of one another, we use it to find classes that have someloose semantic oherence.In describing our work, we draw freely on terminology and notation from themathematical theory of communication.
The reader who is unfamiliar with this fieldor who has allowed his or her facility with some of its concepts to fall into disrepairmay profit from a brief perusal of Feller (1950) and Gallagher (1968).
In the first ofthese, the reader should focus on conditional probabilities and on Markov chains; inthe second, on entropy and mutual information.
* IBM T. J. Watson Research Center, Yorktown Heights, New York 10598.
(~) 1992 Association for Computational LinguisticsComputational Linguistics Volume 18, Number 4SourceLanguageModelW ChannelModelYP r (W)  x P r (Y IW)  = Pr (W,Y)Figure 1Source-channel setup.2.
Language ModelsFigure I shows a model that has long been used in automatic speech recognition (Bahl,Jelinek, and Mercer 1983) and has recently been proposed for machine translation(Brown et al 1990) and for automatic spelling correction (Mays, Demerau, and Mercer1990).
In automatic speech recognition, y is an acoustic signal; in machine translation, yis a sequence of words in another language; and in spelling correction, y is a sequenceof characters produced by a possibly imperfect typist.
In all three applications, givena signal y, we seek to determine the string of English words, w, which gave rise toit.
In general, many different word strings can give rise to the same signal and sowe cannot hope to recover w successfully in all cases.
We can, however, minimizeour probability of error by choosing as our estimate of w that string @ for which thea posteriori probability of @ given y is greatest.
For a fixed choice of y, this probabilityis proportional to the joint probability of @ and y which, as shown in Figure 1, is theproduct of two terms: the a priori probability of @ and the probability that y willappear at the output of the channel when @ is placed at the input.
The a prioriprobability of @, Pr (@), is the probability that the string @ will arise in English.
Wedo not attempt a formal definition of English or of the concept of arising in English.Rather, we blithely assume that the production of English text can be characterized bya set of conditional probabilities, Pr(Wk I W~1-1), in terms of which the probability of astring of words, w~l, can be expressed as a product:Pr (wkl) = Pr(wl)Pr (w2 Iwl)'"Pr(Wk Iw~-l).
(1)Here, W~ -1 represents the string WlW2" ' 'Wk_  1.
In the conditional probability Pr(Wk Iw~-l), we call Wl k-1 the history and Wk the prediction.
We refer to a computationalmechanism for obtaining these conditional probabilities as a language model.Often we must choose which of two different language models is the better one.The performance of a language model in a complete system depends on a delicateinterplay between the language model and other components of the system.
One lan-guage model may surpass another as part of a speech recognition system but performless well in a translation system.
However, because it is expensive to evaluate a lan-guage model in the context of a complete system, we are led to seek an intrinsicmeasure of the quality of a language model.
We might, for example, use each lan-468Peter E Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural Languageguage model to compute the joint probability of some collection of strings and judgeas better the language model that yields the greater probability.
The perplexity of alanguage model with respect o a sample of text, S, is the reciprocal of the geomet-ric average of the probabilities of the predictions in S. If S has I S \] words, then theperplexity is Pr (S) -1/Isj.
Thus, the language model with the smaller perplexity will bethe one that assigns the larger probability to S. Because the perplexity depends notonly on the language model but also on the text with respect to which it is measured,'it is important that the text be representative of that for which the language modelis intended.
Because perplexity is subject o sampling error, making fine distinctionsbetween language models may require that the perplexity be measured with respectto a large sample.In an n-gram language model, we treat two histories as equivalent if they end inthe same n - 1 words, i.e., we assume that for k > n, Pr (Wk I wl k-l) is equal towk--1 Pr (Wk \[ k-n+1)" For a vocabulary of size V, a 1-gram model has V - 1 independentparameters, one for each word minus one for the constraint that all of the probabilitiesadd up to 1.
A 2-gram model has V(V - 1) independent parameters of the formPr (w2 \[ Wl) and V - 1 of the form Pr (w) for a total of V 2 - 1 independent parameters.In general, an n-gram model has V n - 1 independent parameters: V n-1 (V - 1) of theform Pr (Wn \] w~-l), which we call the order-n parameters, plus the V n - l -  1 parametersof an (n - 1)-gram model.We estimate the parameters of an n-gram model by examining a sample of text,t~, which we call the training text, in a process called training.
If C(w) is the numberof times that the string w occurs in the string t~, then for a 1-gram language modelthe maximum likelihood estimate for the parameter Pr (w) is C(w)/T.
To estimate theparameters ofan n-gram model, we estimate the parameters of the (n - 1)-gram modelthat it contains and then choose the order-n parameters soas to maximize Pr (tn T \[ t~-').Thus, the order-n parameters areC(w -'wn) (2)Pr  (wn \ ]w / -1 )  -~ n -1  " Ew C(wl w)We call this method of parameter estimation sequential maximum likelihood estimation.We can think of the order-n parameters of an n-gram model as constituting thetransition matrix of a Markov model the states of which are sequences of n - 1 words.Thus, the probability of a transition between the state wlw2..  "Wn-1 and the statew2w3.., w n is Pr (Wn \ ]wlw2""  Wn-1) ?
The steady-state distribution for this transitionmatrix assigns a probability to each (n - 1)-gram, which we denote S(w~-l).
We saythat an n-gram language model is consistent if, for each string w ln-1, the probability thatn-1 is S(w~ -1).
Sequential maximum likelihood estimation does the model assigns to w 1not, in general, lead to a consistent model, although for large values of T, the modelwill be very nearly consistent.
Maximum likelihood estimation of the parameters of aconsistent n-gram language model is an interesting topic, but is beyond the scope ofthis paper.The vocabulary of English is very large and so, even for small values of n, thenumber of parameters in an n-gram model is enormous.
The IBM Tangora speechrecognition system has a vocabulary of about 20,000 words and employs a 3-gramlanguage model with over eight trillion parameters (Averbuch et al 1987).
We canillustrate the problems attendant toparameter stimation for a 3-gram language modelwith the data in Table 1.
Here, we show the number of 1-, 2-, and 3-grams appearingwith various frequencies in a sample of 365,893,263 words of English text from avariety of sources.
The vocabulary consists of the 260,740 different words plus a special469Computational Linguistics Volume 18, Number 4Table 1Count 1 -grams 2-grams 3-grams1 36,789 8,045,024 53,737,3502 20,269 2,065,469 9,229,9583 13,123 970,434 3,653,791> 3 135,335 3,413,290 8,728,789> 0 205,516 14,494,217 ,75,349,888> 0 260,741 6.799 x 101?
1.773 X 1016Number of n-grams with various frequencies in 365,893,263 words of running text.unknown word into which all other words are mapped.
Of the 6.799 x 10 l?
2-grams thatmight have occurred in the data, only 14,494,217 actually did occur and of these,8,045,024 occurred only once each.
Similarly, of the 1.773 x 1016 3-grams that mighthave occurred, only 75,349,888 actually did occur and of these, 53,737,350 occurredonly once each.
From these data and Turing's formula (Good 1953), we can expectthat maximum likelihood estimates will be 0 for 14.7 percent of the 3-grams and for2.2 percent of the 2-grams in a new sample of English text.
We can be confident thatany 3-gram that does not appear in our sample is, in fact, rare, but there are so manyof them that their aggregate probability is substantial.As n increases, the accuracy of an n-gram model increases, but the reliability of ourparameter estimates, drawn as they must be from a limited training text, decreases.Jelinek and Mercer (1980) describe a technique called interpolated estimation that com-bines the estimates of several language models so as to use the estimates of the moreaccurate models where they are reliable and, where they are unreliable, to fall back onthe more reliable estimates of less accurate models.
If Prq) (wi I i-1 w I ) is the conditionalprobability as determined by the jth language model, then the interpolated estimate,Pr(wi \[ i-1 w 1 ), is given byPr Iwi \[W~--I / ~ ~ &j(w~-l)Pr(J)(wi \[ w~-l).J(3)Given values for prq)(.
), the /~j(W~ -1) are chosen, with the help of the EM algorithm,so as to maximize the probability of some additional sample of text called the held-outdata (Baum 1972; Dempster, Laird, and Rubin 1977; Jelinek and Mercer 1980).
When weuse interpolated estimation to combine the estimates from 1-, 2-, and 3-gram models,we choose the )~s to depend on the history, W 1i-1, only through the count of the 2-gram, wi_2wi_ 1.
We expect hat where the count of the 2-gram is high, the 3-gramestimates will be reliable, and, where the count is low, the estimates will be unreliable.We have constructed an interpolated 3-gram model in which we have divided the ;~sinto 1,782 different sets according to the 2-gram counts.
We estimated these ),s from aheld-out sample of 4,630,934 words.
We measure the performance ofour model on theBrown corpus, which contains a variety of English text and is not included in eitherour training or held-out data (Ku~era nd Francis 1967).
The Brown corpus contains1,014,312 words and has a perplexity of 244 with respect o our interpolated model.3.
Word ClassesClearly, some words are similar to other words in their meaning and syntactic function.We would not be surprised to learn that the probability distribution of words in thevicinity of Thursday is very much like that for words in the vicinity of Friday.
Of470Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural Languagecourse, they will not be identical: we rarely hear someone say Thank God it's Thursday!or worry about Thursday the 13 th.
If we can successfully assign words to classes, itmay be possible to make more reasonable predictions for histories that we have notpreviously seen by assuming that they are similar to other histories that we have seen.Suppose that we partition a vocabulary of V words into C classes using a function,7r, which maps a word, wi, into its class, ci.
We say that a language model is an n-gram class model if it is an n-gram language model and if, in addition, for 1 < k < n,Pr (Wk I W~ -1) = Pr (Wk \[ Ck)Pr (Ck \[~-1).
An n-gram class model has C" - 1 + V - Cindependent parameters: V - C of the form Pr (wi \] ci), plus the C" - 1 independentparameters of an n-gram language model for a vocabulary of size C. Thus, except inthe trivial cases in which C = V or n = 1, an n-gram class language model always hasfewer independent parameters than a general n-gram language model.Given training text, t T, the maximum likelihood estimates of the parameters of a1-gram class model arePr (w \[ c) - C(w) C(c) ' (4)andPr(c) = C(c) -T-' (5)where by C(c) we mean the number of words in tl r for which the class is c. From theseequations, we see that, since c = 7r(w), Pr (w) = Pr (w I c) Pr (c) -- C(w)/T.
For a 1-gramclass model, the choice of the mapping rr has no effect.
For a 2-gram class model,the sequential maximum likelihood estimates of the order-2 parameters maximizePr (t T \] tl) or, equivalently, log Pr(t2 r I h) and are given byC(clc2)Pr (c2 I Cl) -= y-~cC(ClC ) ?
(6)By definition, Pr (clc2) = Pr (Cl)Pr (c2 \] Cl), and so, for sequential maximum likelihoodestimation, we havec(cl) Pr (CLC2) - C(CLC2) x (7)T ~,c C(clc)Since C(Cl) and ~c C(ClC) are the numbers of words for which the class is Cl in thestrings tT and t r-1 respectively, the final term in this equation tends to 1 as T tends toinfinity.
Thus, Pr (CLC2) tends to the relative frequency of ClC2 as consecutive classes inthe training text.Let L(rr) = (T - 1) -1 logPr (t r \[ h).
ThenC(WlW2) L(Tr) = y~ ~---~ logPr(c2ICl)Pr(w2\[c2)Wl W2Pr (c2 \] Cl) ~W C(~w2)- -ClC2 W2logPr (w2 I c2) Pr(c2).
(8)YPr(w2)Therefore, since Y~w C(ww2)/(T-1) tends to the relative frequency of w2 in the trainingtext, and hence to Pr (w2), we must have, in the limit,L(Tr) = ZPr (w) logPr (w)+ZPr (C lCa) log  Pr (c2 Icl)Pr (c2)W CiC 2= -H(w) ?
I(Cl, C2), (9)471Computational Linguistics Volume 18, Number 4where H(w) is the entropy of the 1-gram word distribution and I(cl, C2) is the averagemutual information of adjacent classes.
Because L(1r) depends on ~r only through thisaverage mutual information, the partition that maximizes L(~r) is, in the limit, also thepartition that maximizes the average mutual information of adjacent classes.We know of no practical method for finding one of the partitions that maximize theaverage mutual information.
Indeed, given such a partition, we know of no practicalmethod for demonstrating that it does, in fact, maximize the average mutual informa-tion.
We have, however, obtained interesting results using a greedy algorithm.
Initially,we assign each word to a distinct class and compute the average mutual informationbetween adjacent classes.
We then merge that pair of classes for which the loss inaverage mutual information is least.
After V - C of these merges, C classes remain.Often, we find that for classes obtained in this way the average mutual informationcan be made larger by moving some words from one class to another.
Therefore, afterhaving derived a set of classes from successive merges, we cycle through the vocabu-lary moving each word to the class for which the resulting partition has the greatestaverage mutual information.
Eventually no potential reassignment of a word leads toa partition with greater average mutual information.
At this point, we stop.
It maybe possible to find a partition with higher average mutual information by simultane-ously reassigning two or more words, but we regard such a search as too costly to befeasible.To make even this suboptimal lgorithm practical one must exercise a certain carein implementation.
There are approximately (V-i)2/2 merges that we must investigateto carry out the /th step.
The average mutual information remaining after any one ofthem is the sum of (V -/)2 terms, each of which involves a logarithm.
Since altogetherwe must make V - C merges, this straightforward approach to the computation is oforder V s. We cannot seriously contemplate such a calculation except for very smallvalues of V. A more frugal organization of the computation must take advantage ofthe redundancy in this straightforward calculation.
As we shall see, we can make thecomputation of the average mutual information remaining after a merge in constanttime, independent of V.Suppose that we have already made V-  k merges, resulting in classes Ck (1), Ck (2),..., Ck(k) and that we now wish to investigate the merge of Ck(i) with Ck(j) for 1 _<i < j <_ k. Let pk(l, m) = Pr (Ck(1), Ck(m)), i.e., the probability that a word in class Ck(m)follows a word in class Ck(1).
Letplk(l) = ~ pk(l, m), (10)mletand letprk(m) = ~ pk(l, m), (11)1qk(l,m) = pk(l,m)" pk(l,m) log p l~m ) "The average mutual information remaining after V - k merges is(12)Ik = ~ qk(l, m).
(13)l,mWe use the notation i + j  to represent the cluster obtained by merging Ck(i) and Ck(j).472Peter E Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural LanguageThus, for example, pk(i +j ,m)  = pk(i,m) + pk(j,m) andpk(i + j, m)qk(i + j, m) = pk(i + j, m)log plk(i + j)prk(m)" (14)The average mutual information remaining after we merge Ck(i) and Ck(j) is thenIk(i,j) Ik -- Sk(i) -- Sk(j) + qk(i,j) + qk(j, i) + qk(i +j,  i +j)+ Y2 qk(l,i + j) + ~ qk(i + j ,m),l#i d m#i,j(15)wheresk(i) = y~ qk(l, i) + ~ qk(i, m) -- qk(i, i).I m(16)If we know Ik, Sk(i), and Sk(J'), then the majority of the time involved in computingIk(i,j) is devoted to computing the sums on the second line of equation (15).
Each ofthese sums has approximately V - k terms and so we have reduced the problem ofevaluating Ik(i,j) from one of order V 2 to one of order V. We can improve this furtherby keeping track of those pairs I, m for which pk(l, m) is different from 0.
We recallfrom Table 1, for example, that of the 6.799 x 101?
2-grams that might have occurredin the training data, only 14,494,217 actually did occur.
Thus, in this case, the sumsrequired in equation (15) have, on average, only about 56 non-zero terms instead of260,741, as we might expect from the size of the vocabulary.By examining all pairs, we can find that pair, i ( j, for which the loss in averagemutual information, Lk(i,j) =-- Ik -- Ik(i,j), is least.
We complete the step by mergingCk(i) and Ck(j) to form a new cluster Ck-l(/).
I f j  # k, we rename Ck(k) as Ck-l(j) andfor I  i,j, we set Ck_l(l) to Ck(1).
Obviously, lk-1 = Ik(i,j).
The values of pk-1, plk-1,prk-1, and qk-1 can be obtained easily from pk, plk, prk, and qk.
If 1 and m both denoteindices neither of which is equal to either i or j, then it is easy to establish thatSk-l(l)sk_l(j)Lk-1 (1, m)= Sk(l) -- qk(l, i) -- qk(i, 1) -- qk(l,j) -- qk(J', l) + qk-~ (I, i) + qk-l(i, I)= Sk(k) -- qk(k, i) -- qk(i, k) - qk(k,j) - qk(j, k) + qk-l(j, i) + qk-l(i,j)= Lk ( l ,m) - -qk( l+m, i ) - -qk ( i , l+m)- -qk( l+m, j ) - -qk ( j , l+m)+qk-1 (I + m, i) + qk-1 (i, 1 + m)Lk-1 (l,j) = Lk(l, k) - qk(l + k, i) - qk(i, 1 + k) - qk(l + k,j) - qk(J', l + k)+qk-l(l + j, i) + qk-1 (i, I + j)Lk-l(j,l) ---- Lk-l(l, j) (17)Finally, we must evaluate Sk-l(i) and Lk_l(l,i) from equations 15 and 16.
Thus, theentire update process requires something on the order of V 2 computations in thecourse of which we will determine the next pair of clusters to merge.
The algorithm,then, is of order V 3.Although we have described this algorithm as one for finding clusters, we actuallydetermine much more.
If we continue the algorithm for V - 1 merges, then we willhave a single cluster which, of course, will be the entire vocabulary.
The order in whichclusters are merged, however, determines a binary tree the root of which corresponds473Computational Linguistics Volume 18, Number 4planletterrequestmemocasequestion -'-7charge----I ~__statement L - \ ]draft ~F-.evaluationassessment ~"analysis ,.,understandingopinion 1 IconversationdiscussiondayyearweekmonthquarterhalfiIaccountspeoplecustomersindividualsemployeesstudents \]reps i~  representativesrepresentativerepFigure 2Sample subtrees from a 1,000-word mutual information tree.iL Ito this single cluster and the leaves of which correspond to the words in the vocabulary.Intermediate nodes of the tree correspond to groupings of words intermediate betweensingle words and the entire vocabulary.
Words that are statistically similar with respectto their immediate neighbors in running text will be close together in the tree.
Wehave applied this tree-building algorithm to vocabularies of up to 5,000 words.
Figure2 shows some of the substructures in a tree constructed in this manner for the 1,000most frequent words in a collection of office correspondence.Beyond 5,000 words this algorithm also fails of practicality.
To obtain clusters forlarger vocabularies, we proceed as follows.
We arrange the words in the vocabularyin order of frequency with the most frequent words first and assign each of the firstC words to its own, distinct class.
At the first step of the algorithm, we assign the(C Jr 1) st most probable word to a new class and merge that pair among the resultingC + 1 classes for which the loss in average mutual information is least.
At the k th stepof the algorithm, we assign the (C + k) th most probable word to a new class.
Thisrestores the number of classes to C + 1, and we again merge that pair for which theloss in average mutual information is least.
After V - C steps, each of the words inthe vocabulary will have been assigned to one of C classes.We have used this algorithm to divide the 260,741-word vocabulary of Table I into1,000 classes.
Table 2 contains examples of classes that we find particularly interesting.Table 3 contains examples that were selected at random.
Each of the lines in the tablescontains members of a different class.
The average class has 260 words and so tomake the table manageable, we include only words that occur at least ten times and474Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural LanguageFriday Monday Thursday Wednesday Tuesday Saturday Sunday weekends Sundays SaturdaysJune March July April January December October November September Augustpeople guys folks fellows CEOs chaps doubters commies unfortunates blokesdown backwards ashore sideways outhward northward overboard aloft downwards adriftwater gas coal liquid acid sand carbon steam shale irongreat big vast sudden mere sheer gigantic lifelong scant colossalman woman boy girl lawyer doctor guy farmer teacher citizenAmerican Indian European Japanese German African Catholic Israeli Italian Arabpressure temperature p rmeability density porosity stress velocity viscosity gravity tensionmother wife father son husband brother daughter sister boss unclemachine device controller processor CPU printer spindle subsystem compiler plotterJohn George James Bob Robert Paul William Jim David Mikeanyone someone anybody somebodyfeet miles pounds degrees inches barrels tons acres meters bytesdirector chief professor commissioner commander treasurer founder superintendent dean cus-todianliberal conservative parliamentary oyal progressive Tory provisional separatist federalist PQhad hadn't hath would've could've should've must've might'veasking telling wondering instructing informing kidding reminding bc)thering thanking deposingthat tha theathead body hands eyes voice arm seat eye hair mouthTable 2Classes from a 260,741-word vocabulary.we include no more than the ten most frequent words of any class (the other twomonths would appear with the class of months if we extended this limit to twelve).The degree to which the classes capture both syntactic and semantic aspects of Englishis quite surprising iven that they were constructed from nothing more than countsof bigrams.
The class {that ha theat} is interesting because although tha and theat arenot English words, the computer has discovered that in our data each of them is mostoften a mistyped that.Table 4 shows the number of class 1-, 2-, and 3-grams occurring in the text withvarious frequencies.
We can expect from these data that maximum likelihood estimateswill assign a probability of 0 to about 3.8 percent of the class 3-grams and to about.02 percent of the class 2-grams in a new sample of English text.
This is a substantialimprovement over the corresponding numbers for a 3-gram language model, whichare 14.7 percent for word 3-grams and 2.2 percent for word 2-grams, but we haveachieved this at the expense of precision in the model.
With a class model, we distin-guish between two different words of the same class only according to their relativefrequencies in the text as a whole.
Looking at the classes in Tables 2 and 3, we feel that475Computational Linguistics Volume 18, Number 4little prima moment's trifle tad Litle minute's tinker's hornet's teammate's6ask remind instruct urge interrupt invite congratulate commend warn applaudobject apologize apologise avow whishcost expense risk profitability deferral earmarks capstone cardinality mintage resellerB dept.
AA Whitey CL pi Namerow PA Mgr.
LaRose# Rel rel.
#S ShreeS Gens nai Matsuzawa ow Kageyama Nishida Sumit Zollner Mallikresearch training education science advertising arts medicine machinery Art AIDSrise focus depend rely concentrate dwell capitalize mbark intrude typewritingMinister mover Sydneys Minster Miniter3running moving playing setting holding carrying passing cutting driving fightingcourt judge jury slam Edelstein magistrate marshal Abella Scalia larcenyannual regular monthly daily weekly quarterly periodic Good yearly convertibleaware unaware unsure cognizant apprised mindful partakersforce ethic stoppage force's conditioner stoppages conditioners waybill forwarder Atonabeesystems magnetics loggers products' coupler Econ databanks Centre inscriber correctorsindustry producers makers fishery Arabia growers addiction medalist inhalation addictbrought moved opened picked caught ied gathered cleared hung liftedTable 3Randomly selected word classes.Table 4Count 1-gram 2-grams 3-grams1 0 81 ,171 13,873,1922 0 57,056 4,109,9983 0 43,752 2,012,394> 3 1,000 658,564 6,917,746> 0 1,000 840,543 26,913,3300 1,000 1,000,000 1.000 x 109Number of class n-grams with various frequencies in 365,893,263 words of running text.this is reasonable for pairs like John and George or liberal and conservative but perhapsless so for pairs like little and prima or Minister and mover.We used these classes to construct an interpolated 3-gram class model using thesame training text and held-out data as we used for the word-based language modelwe discussed above.
We measured the perplexity of the Brown corpus with respect othis model and found it to be 271.
We then interpolated the class-based estimators withthe word-based estimators and found the perplexity of the test data to be 236, whichis a small improvement over the perplexity of 244 we obtained with the word-basedmodel.476Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural Language4.
Sticky Pairs and Semantic ClassesIn the previous section, we discussed some methods for grouping words togetheraccording to the statistical similarity of their surroundings.
Here, we discuss two ad-ditional types of relations between words that can be discovered by examining variousco-occurrence statistics.The mutual information of the pair wl and w2 as adjacent words isPr (wl w2) (18)log Pr (wl) Pr (w2).If w2 follows Wl less often than we would expect on the basis of their independentfrequencies, then the mutual information is negative.
If w 2 follows Wl more often thanwe would expect, then the mutual information is positive.
We say that the pair WlW2 issticky if the mutual information for the pair is substantially greater than 0.
In Table 5,we list the 20 stickiest pairs of words found in a 59,537,595-word sample of text fromthe Canadian parliament.
The mutual information for each pair is given in bits, whichcorresponds to using 2 as the base of the logarithm in equation 18.
Most of the pairsare proper names such as Pontius Pilate or foreign phrases that have been adopted intoEnglish such as mutatis mutandis and avant garde.
The mutual information for HumptyDumpty, 22.5 bits, means that the pair occurs roughly 6,000,000 times more than onewould expect from the individual frequencies of Humpty and Dumpty.
Notice that theproperty of being a sticky pair is not symmetric and so, while Humpty Dumpty formsa sticky pair, Dumpty Humpty does not.Table 5Sticky word pairs.Word pair Mutual InformationHumpty Dumpty 22.5Klux Klan 22.2Ku Klux 22.2Chah Nulth 22.2Lao Bao 22.2Nuu Chah 22.1Tse Tung 22.1avant garde 22.1Carena Bancorp 22.0gizzard shad 22.0Bobby Orr 22.0Warnock Hersey 22.0mutatis mutandis 21.9Taj Mahal 21.8Pontius Pilate 21.7ammonium nitrate 21.7jiggery pokery 21.6Pitney Bowes 21.6Lubor Zink 21.5anciens combattants 21.5Abu Dhabi 21.4Aldo Moro 21.4fuddle duddle 21.4helter skelter 21.4mumbo jumbo 21.4477Computational Linguistics Volume 18, Number 4Table 6Semantic lusters.we our us ourselves oursquestion questions asking answer answers answeringperformance performed perform performs performingtie jacket suitwrite writes writing written wrote penmorning noon evening night nights midnight bedattorney counsel trial court judgeproblems problem solution solve analyzed solved solvingletter addressed enclosed letters correspondencelarge size small larger smalleroperations operations operating operate operatedschool classroom teaching rade mathstreet block avenue corner blockstable tables dining chairs platepublished publication author publish writer titledwall ceiling walls enclosure roofsell buy selling buying soldInstead of seeking pairs of words that occur next to one another more than wewould expect, we can seek pairs of words that simply occur near one another morethan we would expect.
We avoid finding sticky pairs again by not considering pairs ofwords that occur too close to one another.
To be precise, let Prnear (WlW2) be the proba-bility that a word chosen at random from the text is Wl and that a second word, chosenat random from a window of 1,001 words centered on wl but excluding the words ina window of 5 centered on wl, is w2.
We say that Wl and w2 are semantically sticky ifPrnear (WlW2) is much larger than Pr (wO Pr (W2).
Unlike stickiness, semantic stickinessis symmetric so that if Wl sticks semantically to w2, then w2 sticks semantically to Wl.In Table 6, we show some interesting classes that we constructed, using Prnear (WlW2),in a manner similar to that described in the preceding section.
Some classes group to-gether words having the same morphological stem, such as performance, performed,perform, performs, and performing.
Other classes contain words that are semanticallyrelated but have different stems, such as attorney, counsel, trial, court, and judge.5.
D iscuss ionWe have described several methods here that we feel clearly demonstrate he value ofsimple statistical techniques as allies in the struggle to tease from words their linguisticsecrets.
However, we have not as yet demonstrated the full value of the secrets thusgleaned.
At the expense of a slightly greater perplexity, the 3-gram model with wordclasses requires only about one-third as much storage as the 3-gram language modelin which each word is treated as a unique individual (see Tables 1 and 4).
Even whenwe combine the two models, we are not able to achieve much improvement in theperplexity.
Nonetheless, we are confident hat we will eventually be able to makesignificant improvements o 3-gram language models with the help of classes of thekind that we have described here.478Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural LanguageAcknowledgmentThe authors would like to thank JohnLafferty for his assistance in constructingword classes described in this paper.ReferencesAverbuch, A.; Bahl, L.; Bakis, R.; Brown, P.;Cole, A.; Daggett, G.; Das, S.; Davies, K.;Gennaro, S.
De.
; de Souza, P.; Epstein, E.;Fraleigh, D.; Jelinek, E; Moorhead, J.;Lewis, B.; Mercer, R.; Nadas, A.;Nahamoo, D.; Picheny, M.; Shichman, G.;Spinelli, P.; Van Compernolle, D.; andWilkens, H. (1987).
"Experiments with theTangora 20,000 word speech recognizer.
"In Proceedings, IEEE International Conferenceon Acoustics, Speech and Signal Processing.Dallas, Texas, 701-704.Bahl, L. R.; Jelinek, E; and Mercer, R. L.(1983).
"A maximum likelihood approachto continuous speech recognition."
IEEETransactions on Pattern Analysis and MachineIntelligence, PAMI-5(2), 179-190.Baum, L. (1972).
"An inequality andassociated maximization technique instatistical estimation of probabilisticfunctions of a Markov process.
"Inequalities, 3, 1-8.Brown, P. E; Cocke, J.; DellaPietra, S. A.;DellaPietra, V. J.; Jelinek, E; Lafferty, J. D.;Mercer, R. L.; and Roossin, P. S. (1990).
"Astatistical approach to machinetranslation."
Computational Linguistics,16(2), 79-85.Dempster, A.; Laird, N.; and Rubin, D.(1977).
"Maximum likelihood fromincomplete data via the EM algorithm.
"Journal of the Royal Statistical Society, 39(B),1-38.Feller, W. (1950).
An Introduction toProbability Theory and its Applications,Volume I. John Wiley & Sons, Inc.Gallagher, R. G. (1968).
Information Theoryand Reliable Communication.
John Wiley &Sons, Inc.Good, I.
(1953).
"The population frequenciesof species and the estimation ofpopulation parameters."
Biometrika,40(3-4), 237-264.Jelinek, E, and Mercer, R. L.
(1980).
"Interpolated estimation of Markovsource parameters from sparse data."
InProceedings, Workshop on Pattern Recognitionin Practice, Amsterdam, The Netherlands,381-397.Ku~era, H., and Francis, W. (1967).Computational Analysis of Present DayAmerican English.
Brown University Press.Mays, E.; Damerau, E J.; and Mercer, R. L.(1990).
"Context-based spellingcorrection."
In Proceedings, IBM NaturalLanguage ITL.
Paris, France, 517-522.479
