c?
2003 Association for Computational LinguisticsAnaphora and Discourse StructureBonnie Webber?
Matthew Stone?Edinburgh University Rutgers UniversityAravind Joshi?
Alistair Knott?University of Pennsylvania University of OtagoWe argue in this article that many common adverbial phrases generally taken to signal a discourserelation between syntactically connected units within discourse structure instead work anaphor-ically to contribute relational meaning, with only indirect dependence on discourse structure.This allows a simpler discourse structure to provide scaffolding for compositional semantics andreveals multiple ways in which the relational meaning conveyed by adverbial connectives caninteract with that associated with discourse structure.
We conclude by sketching out a lexicalizedgrammar for discourse that facilitates discourse interpretation as a product of compositional rules,anaphor resolution, and inference.1.
IntroductionIt is a truism that a text means more than the sum of its component sentences.
Onesource of additional meaning are relations taken to hold between adjacent sentences?syntactically?
connected within a larger discourse structure.
It has been very difficult,however, to say what discourse relations there are, either theoretically (Mann andThompson 1988; Kehler 2002; Asher and Lascarides 2003) or empirically (Knott 1996).Knott?s empirical attempt to identify and characterize cue phrases as evidencefor discourse relations illustrates some of the difficulties.
Knott used the followingtheory-neutral test to identify cue phrases: For a potential cue phrase ?
in naturallyoccurring text, consider in isolation the clause in which it appears.
If the clause ap-pears incomplete without an adjacent left context, whereas it appears complete if ?
isremoved, then ?
is a cue phrase.
Knott?s test produced a nonexhaustive list of abouttwo hundred different phrases from 226 pages of text.
He then attempted to charac-terize the discourse relation(s) conveyed by each phrase by identifying when (always,sometimes, never) one phrase could substitute for another in a way that preservedmeaning.
He showed how these substitution patterns could be a consequence of a setof semantic features and their values.
Roughly speaking, one cue phrase could alwayssubstitute for another if it had the same set of features and values, sometimes do so ifit was less specific than the other in terms of its feature values, and never do so if theirvalues conflicted for one or more features.?
School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK.
E-mail:bonnie@inf.ed.ac.uk.?
Department of Computer Science, Rutgers Universtiy, 110 Frelinghuysen Road, Piscataway, NJ08854-8019.
E-mail: mdstone@cs.rutgers.edu.?
Department of Computer & Information Science, University of Pennsylvania, 200 South 33rd Street,Philadelphia, PA 19104-6389.
E-mail: joshi@linc.cis.upenn.edu.?
Department of Computer Science, University of Otago, P.O.
Box 56, DUNEDIN 9015, New Zealand.E-mail: alik@cs.otago.ac.nz.546Computational Linguistics Volume 29, Number 4By assuming that cue phrases contribute meaning in a uniform way, Knott wasled to a set of surprisingly complex directed acyclic graphs relating cue phrases interms of features and their values, each graph loosely corresponding to some family ofdiscourse relations.
But what if the relational meaning conveyed by cue phrases couldin fact interact with discourse meaning in multiple ways?
Then Knott?s substitutionpatterns among cue phrases may have reflected these complex interactions, as well asthe meanings of individual cue phrases themselves.This article argues that cue phrases do depend on another mechanism for convey-ing extrasentential meaning?specifically, anaphora.
One early hint that adverbial cuephrases (called here discourse connectives) might be anaphoric can be found in anACL workshop paper in which Janyce Wiebe (1993) used the following example toquestion the adequacy of tree structures for discourse:(1) a.
The car was finally coming toward him.b.
He [Chee] finished his diagnostic tests,c.
feeling relief.d.
But then the car started to turn right.The problem Wiebe noted was that the discourse connectives but and then appear tolink clause (1d) to two different things: then to clause (1b) in a sequence relation (i.e.,the car?s starting to turn right being the next relevant event after Chee?s finishing histests) and but to a grouping of clauses (1a) and (1c) (i.e., reporting a contrast between,on the one hand, Chee?s attitude toward the car coming toward him and his feelingof relief and, on the other hand, his seeing the car turning right).
(Wiebe doesn?t givea name to the relation she posits between (1d) and the grouping of (1a) and (1c), butit appears to be some form of contrast.
)If these relations are taken to be the basis for discourse structure, some possiblediscourse structures for this example are given in Figure 1.
Such structures might seemadvantageous in allowing the semantics of the example to be computed directly bycompositional rules and defeasible inference.
However, both structures are directedacyclic graphs (DAGs), with acyclicity the only constraint on what nodes can be con-nected.
Viewed syntactically, arbitrary DAGs are completely unconstrained systems.They substantially complicate interpretive rules for discourse, in order for those rulesto account for the relative scope of unrelated operators and the contribution of syn-tactic nodes with arbitrarily many parents.1We are not committed to trees as the limiting case of discourse structure.
Forexample, we agree, by and large, with the analysis that Bateman (1999) gives of(2) (vi) The first to do that were the German jewellers, (vii) in particular KlausBurie.
(viii) And Morris followed very quickly after, (ix) using a lacquetrytechnique to make the brooch, (x) and using acrylics, (xi) and exploringthe use of colour, (xii) and colour is another thing that was new at thattime.1 A reviewer has suggested an alternative analysis of (1) in which clause (1a) is elaborated by clause(1b), which is in turn elaborated by (1c), and clause (1d) stands in both a sequence relation and acontrast relation to the segment as a whole.
Although this might address Wiebe?s problem, the result isstill a DAG, and such a fix will not address the additional examples we present in section 2, in which apurely structural account still requires DAGs with crossing arcs.547Webber et al Anaphora and Discourse Structurebseqdcontrastccbelaborationelaborationa seqa dseq contrast(i)(ii)Figure 1Possible discourse structure for example (1).
Each root and internal node is labeled by the typeof relation that Wiebe takes to hold between the daughters of that node.
(i) uses an n-arybranching sequence relation, whereas in (ii), sequence is binary branching.
(ix)(vi)succession manner(viii)Figure 2Simple multiparent structure.in which clause (ix) stands in a manner relation with clause (viii), which in turn standsin a succession (i.e., sequence) relation with clause (vi).
This is illustrated in Figure 2,which shows a DAG (rather than a tree), but without crossing dependencies.So it is the cost of moving to arbitrary DAGs for discourse structure that we feel istoo great to be taken lightly.
This is what has led us to look for another explanation forthese and other examples of apparent complex and crossing dependencies in discourse.The position we argue for in this article, is that whereas adjacency and explicitconjunction (coordinating conjunctions such as and, or, so, and but; subordinating con-junctions such as although, whereas, and when) imply discourse relations between (theinterpretation of) adjacent or conjoined discourse units, discourse adverbials such asthen, otherwise, nevertheless, and instead are anaphors, signaling a relation between theinterpretation of their matrix clause and an entity in or derived from the discoursecontext.
This position has four advantages:1.
Understanding discourse adverbials as anaphors recognizes theirbehavioral similarity to the pronouns and definite noun phrases (NPs)that are the bread and butter of previous work on anaphora.
This isdiscussed in section 2.2.
By understanding and exploring the full range of phenomena for whichan anaphoric account is appropriate, we can better characterize anaphorsand devise more accurate algorithms for resolving them.
This is exploredin section 3.3.
Any theory of discourse must still provide an account of how a sequenceof adjacent discourse units (clauses, sentences, and the larger units thatthey can comprise) means more than just the sum of its component548Computational Linguistics Volume 29, Number 4units.
This is a goal that researchers have been pursuing for some time,using both compositional rules and defeasible inference to determinethese additional aspects of meaning (Asher and Lascarides 1999; Gardent1997; Hobbs et al 1993; Kehler 2002; Polanyi and van den Berg 1996;Scha and Polanyi 1988; Schilder 1997a, 1997b; van den Berg 1996) If thatportion of discourse semantics that can be handled by mechanismsalready needed for resolving other forms of anaphora and deixis isfactored out, there is less need to stretch and possibly distortcompositional rules and defeasible inference to handle everything.2Moreover, recognizing the possibility of two separate relations (onederived anaphorically and one associated with adjacency and/or astructural connective) admits additional richness to discourse semantics.Both points are discussed further in section 4.4.
Understanding discourse adverbials as anaphors allows us to see moreclearly how a lexicalized approach to the computation of clausal syntaxand semantics extends naturally to the computation of discourse syntaxand semantics, providing a single syntactic and semantic matrix withwhich to associate speaker intentions and other aspects of pragmatics(section 5.
)The account we provide here is meant to be compatible with current approachesto discourse semantics such as DRT (Kamp and Reyle 1993; van Eijck and Kamp1997), dynamic semantics (Stokhof and Groenendijk 1999), and even SDRT (Asher1993; Asher and Lascarides 2003), understood as a representational scheme ratherthan an interpretive mechanism.
It is also meant to be compatible with more detailedanalyses of the meaning and use of individual discourse adverbials, such as Jayesand Rossari (1998a, 1998b) and Traugott, (1995, 1997).
It provides what we believe tobe a more coherent account of how discourse meaning is computed, rather than analternative account of what that meaning is or what speaker intentions it is being usedto achieve.2.
Discourse Adverbials as Anaphors2.1 Discourse Adverbials Do Not Behave like Structural ConnectivesWe take the building blocks of the most basic level of discourse structure to be explicitstructural connectives between adjacent discourse units (i.e., coordinating and subor-dinating conjunctions and ?paired?
conjunctions such as not only .
.
.
but also, and on theone hand .
.
.
on the other (hand) and inferred relations between adjacent discourse units(in the absense of an explicit structural connective).
Here, adjacency is what triggersthe inference.
Consider the following example:(3) You shouldn?t trust John.
He never returns what he borrows.Adjacency leads the hearer to hypothesize that a discourse relation of something likeexplanation holds between the two clauses.
Placing the subordinate conjunction (struc-tural connective) because between the two clauses provides more evidence for this rela-2 There is an analogous situation at the sentence level, where the relationship between syntactic structureand compositional semantics is simplified by factoring away intersentential anaphoric relations.
Herethe factorization is so obvious that one does not even think about any other possibility.549Webber et al Anaphora and Discourse Structuretion.
Our goal in this section is to convince the reader that many discourse adverbials,including then, also, otherwise, nevertheless, and instead, do not behave in this way.Structural connectives and discourse adverbials do have one thing in common:Like verbs, they can both be seen as heading a predicate-argument construction; unlikeverbs, their arguments are independent clauses.
For example, both the subordinateconjunction after and the adverbial then (in its temporal sense) can be seen as binarypredicates (e.g., sequence) whose arguments are clausally derived events, with theearlier event in first position and the succeeding event in second.But that is the only thing that discourse adverbials and structural connectives havein common.
As we have pointed out in earlier papers (Webber, Knott, and Joshi 2001;Webber et al, 1999a, 1999b), structural connectives have two relevant properties: (1)they admit stretching of predicate-argument dependencies; and (2) they do not admitcrossing of those dependencies.
This is most obvious in the case of preposed subor-dinate conjunctions (example (4)) or ?paired?
coordinate conjunctions (example (5)).With such connectives, the initial predicate signals that its two arguments will follow.
(4) Although John is generous, he is hard to find.
(5) On the one hand, Fred likes beans.
On the other hand, he?s allergic to them.Like verbs, structural connectives allow the distance between the predicate and itsarguments to be ?stretched?
over embedded material, without loss of the dependencybetween them.
For the verb like and an object argument apples, such stretching withoutloss of dependency is illustrated in example (6b).
(6) a. Apples John likes.b.
Apples Bill thinks he heard Fred say John likes.That this also happens with structural connectives and their arguments is illustratedin example (7) (in which the first clause of example (4) is elaborated by another pre-posed subordinate-main clause construction embedded within it) and in example (8)(in which the first conjunct of example (5) is elaborated by another paired-conjunctionconstruction embedded within it).
Possible discourse structures for these examples aregiven in Figure 3.
(7) a.
Although John is very generous?b.
if you need some money,c.
you only have to ask him for it?d.
he?s very hard to find.
(8) a.
On the one hand, Fred likes beans.b.
Not only does he eat them for dinner.c.
But he also eats them for breakfast and snacks.d.
On the other hand, he?s allergic to them.But, as already noted, structural connectives do not admit crossing of predicate-argument dependencies.
If we admit crossing dependencies in examples (7) and (8),we get(9) a.
Although John is very generous?b.
if you need some money?550Computational Linguistics Volume 29, Number 4(i) (ii)abdcontrast[one/other]elaborationcomparison[not only/but also]abdelaborationconcession[although]cccondition[if]Figure 3Discourse structures associated with (i) example (7) and (ii) Example (8).a cbconcession[although] condition[if]elaborationa belaborationcontrast[one/other] comparison[not only...](i) (ii)dcdFigure 4(Impossible) discourse structures that would have to be associated with (i) Example (9) and(ii) example (10).c.
he?s very hard to find?d.
you only have to ask him for it.
(10) a.
On the one hand, Fred likes beans.b.
Not only does he eat them for dinner.c.
On the other hand, he?s allergic to them.d.
But he also eats them for breakfast and snacks.Possible discourse structures for these (impossible) discourses are given in Figure 4.Even if the reader finds no problem with these crossed versions, they clearly do notmean the same thing as their uncrossed counterparts: In (10), but now appears to link(10d) with (10c), conveying that despite being allergic to beans, Fred eats them forbreakfast and snacks.
And although this might be inferred from (8), it is certainly notconveyed directly.
As a consequence, we stipulate that structural connectives do notadmit crossing of their predicate-argument dependencies.3That is not all.
Since we take the basic level of discourse structure to be a conse-quence of (1) relations associated with explicit structural connectives and (2) relations3 A reviewer has asked how much stretching is possible in discourse without losing its thread or havingto rephrase later material in light of the intervening material.
One could ask a similar question aboutthe apparently unbounded dependencies of sentence-level syntax, which inattentive speakers are proneto lose track of and ?fracture.?
Neither question seems answerable on theoretical grounds alone, withboth demanding substantial amounts of empirical data from both written and spoken discourse.
Thepoint we are trying to make is simply that there is a difference in discourse between any amount ofstretching and even the smallest amount of crossing.551Webber et al Anaphora and Discourse Structurewhose defeasible inference is triggered by adjacency, we stipulate that discourse struc-ture itself does not admit crossing structural dependencies.
(In this sense, discourse structuremay be truly simpler than sentence structure.
To verify this, one might examine thediscourse structure of languages such as Dutch that allow crossing dependencies insentence-level syntax.
Initial cursory examination does not give any evidence of cross-ing dependencies in Dutch discourse.
)If we now consider the corresponding properties of discourse adverbials, we seethat they do admit crossing of predicate-argument dependencies, as shown in exam-ples (11)?(13).
(11) a. John loves Barolo.b.
So he ordered three cases of the ?97.c.
But he had to cancel the orderd.
because then he discovered he was broke.
(12) a.
High heels are fine for going to the theater.b.
But wear comfortable shoesc.
if instead you plan to go to the zoo.
(13) a.
Because Fred is illb.
you will have to stay homec.
whereas otherwise the two of you could have gone to the zoo.Consider first the discourse adverbial then in clause (11d).
For it to get its firstargument from (11b) (i.e., the event that the discovery in (d) is ?after?
), it must crossthe structural connection between clauses (c) and (d) associated with because).
Thiscrossing dependency is illustrated in Figure 5(i).
Now consider the discourse adverbialinstead) in clause (12c).
For it to get its first argument from (12a) (i.e., going to the zoo isan alternative to going to the theater), it must cross the structural connection betweenclauses (12b) and (12c) associated with if.
This crossing dependency is illustrated inFigure 5(ii).
Example (13) is its mirror image: For the discourse adverbial otherwise in(13c) to get its first argument from (13a) (i.e., alternatives to the state/condition ofFred being ill), it must cross the structural connection associated with because.
This isillustrated in Figure 5(iii).Crossing dependencies are not unusual in discourse when one considers anaphora(e.g., pronouns and definite NPs), as for example inbconseq[so]contrast[but]dcseq[then]bexplanation[because]conditional[if]acontrast[but]alt[instead]ac bacontrast[whereas]cexplanation?
[because]alt[otherwise](i) (ii) (iii)Figure 5Discourse structures for examples (11)?(13).
Structural dependencies are indicated by solidlines and dependencies associate with discourse adverbials are indicated by broken lines.(explanation?
is the inverse of explanation?i.e., with its arguments in reverse order.
Suchrelations are used to maintain the given linear order of clauses.
)552Computational Linguistics Volume 29, Number 4(14) Every mani tells every womanj hei meets that shej reminds himi of hisimother.
(15) Suei drives an Alfa Romeo.
Shei drives too fast.
Maryj races heri on week-ends.
Shej often beats heri.
(Strube 1998)This suggests that in examples (11)?
(13), the relationship between the discourse ad-verbial and its (initial) argument from the previous discourse might usefully be takento be anaphoric as well.42.2 Discourse Adverbials Do Behave like AnaphorsThere is additional evidence to suggest that otherwise, then, and other discourse adver-bials are anaphors.
First, anaphors in the form of definite and demonstrative NPs cantake implicit material as their referents.
For example, in(16) Stack five blocks on top of one another.
Now close your eyes and tryknocking {the tower, this tower} over with your nose.both NPs refer to the structure which is the implicit result of the block stacking.
(Further discussion of such examples can be found in Isard [1975]; Dale [1992]; andWebber and Baldwin [1992].)
The same is true of discourse adverbials.
In(17) Do you want an apple?
Otherwise you can have a pear.the situation in which you can have a pear is one in which you don?t want an apple?that is, one in which your answer to the question is ?no.?
But this answer isn?t therestructurally: It is only inferred.
Although it appears natural to resolve an anaphor to aninferred entity, it would be much more difficult to establish such links through purelystructural connections: To do so would require complex transformations that introduceinvisible elements into discourse syntax with no deeper motivation.
For example, in(17), we would need a rule that takes a discourse unit consisting solely of a yes/noquestion P?
and replaces it with a complex segment consisting of P?
and the clauseit is possible that P, with the two related by something like elaboration.
Then and onlythen could we account for the interpretation of the subsequent otherwise structurally,by a syntactic link to the covert material (i.e., to the possibility that P holds, whichotherwise introduces an alterative to).Second, discourse adverbials have a wider range of options with respect to theirinitial argument than do structural connectives (i.e., coordinating and subordinatingconjunctions).
The latter are constrained to linking a discourse unit on the right frontierof the evolving discourse (i.e., the clause, sentence and larger discourse units to itsimmediate left).
Discourse adverbials are not so constrained.
To see this, consider thefollowing example:4 We are aware that crossing examples such as (11)?
(13) are rare in naturally occurring discourse.
Webelieve that this is because they are only possible when, as here, strong constraints from the discourseadverbial and from context prevent the adverbial from relating to the closest (leftmost) eventuality oran eventuality coerced from that one.
But rarity doesn?t necessarily mean ill-formedness or marginality,as readers can see for themselves if they use Google to search the Web for strings such as because then, ifinstead, and whereas otherwise, and consider (1) whether the hundreds, even thousands, of texts in whichthese strings occur are ill-formed, and (2) what then, instead, and otherwise are relating in these texts.One must look at rare events if one is studying complex linguistic phenomena in detail.
Thus it is notthe case that only common things in language are real or worth further study.553Webber et al Anaphora and Discourse Structure(18) If the light is red, stop.
Otherwise you?ll get a ticket.
(If you do something other than stop, you?ll get a ticket.
)This can be paraphrased using the conjunction or:If the light is red, stop, or you?ll get a ticket.Here or links its right argument to a unit on the right frontier of the evolving discourse?in this case, the clause stop on its immediate left.
Now consider the related example(19) If the light is red, stop.
Otherwise go straight on.
(If the light is not red, go straight on.
)This cannot be paraphrased with or, as in(20) If the light is red, stop, or go straight on.even though both stop and If the light is red, stop are on the right frontier of the evolvingdiscourse structure.
This is because otherwise is accessing something else, so that (20)means something quite different from either (18) or (19).
What otherwise is accessing,which or cannot, is the interpretation of the condition alone.5 Thus discourse adver-bials, like other anaphors, have access to material that is not available to structuralconnectives.Finally, discourse adverbials, like other anaphors, may require semantic represen-tations in which their arguments are bound variables ranging over discourse entities.That is, whereas it might be possible to represent Although P, Q using a binary modaloperator(21) although(p, q)where formulas p and q translate the sentences P and Q that although combines, wecannot represent P .
.
.Nevertheless, Q this way.
We need something more like(22) p ?
nevertheless(e, q)The motivation for the variable e in this representation is that discourse adverbials,like pronouns, can appear intrasententially in an analog of donkey sentences.
Donkeysentences such as example (23) are a special kind of bound-variable reading:(23) Every farmer who owns a donkey feeds it rutabagas.In donkey sentences, anaphors are interpreted as covarying with their antecedents:The it that is being fed in (23) varies with the farmer who feeds it.
These anaphors,however, appear in a structural and interpretive environment in which a direct syn-tactic relationship between anaphor and antecedent is normally impossible, so theycannot be a reflex of true binding in the syntax-semantics interface.
Rather, donkeysentences show that discourse semantics has to provide variables to translate pronouns,5 This was independently pointed out by several people when this work was presented at ESSLLI?01 inHelsinki in August 2001.
The authors would like to thank Natalia Modjeska, Lauri Karttunen, MarkSteedman, Robin Cooper, and David Traum for bringing it to their attention.554Computational Linguistics Volume 29, Number 4and that discourse mechanisms must interpret these variables as bound?even thoughthe pronouns appear ?free?
by syntactic criteria.Thus, it is significant that discourse adverbials can appear in their own version ofdonkey sentences, as in(24) a.
Anyone who has developed innovative new software has then had tohire a lawyer to protect his/her interests.
(i.e., after developing innovativenew software)b.
Several people who have developed innovative new software havenevertheless failed to profit from it.
(i.e., despite having developed innovativenew software)c. Every person selling ?The Big Issue?
might otherwise be asking forspare change.
(i.e., if he/she weren?t selling ?The Big Issue?
)The examples in (24) involve binding in the interpretation of discourse adverbials.In (24a), the temporal use of then locates each hiring event after the correspondingsoftware development.
Likewise in (24b), the adversative use of nevertheless signalseach developer?s unexpected failure to turn the corresponding profit.
And in (24c),otherwise envisions each person?s begging if that person weren?t selling ?The Big Issue?.Such bound interpretations require variables in the semantic representations andalternative values for them in some model?hence the representation given in (22).Indeed, it is clear that the binding here has to be the discourse kind, not the syntac-tic kind, for the same reason as in (23), although we cannot imagine anyone arguingotherwise, since discourse adverbials have always been treated as elements of dis-course interpretation.
So the variables must be the discourse variables usually used totranslate other kinds of discourse anaphors.6These arguments have been directed at the behavioral similarity between discourseadverbials and what we normally take to be discourse anaphors.
But this isn?t the onlyreason to recognize discourse adverbials as anaphors: In the next section, we suggesta framework for anaphora that is broad enough to include discourse adverbials aswell as definite and demonstrative pronouns and NPs, along with other discoursephenomena that have been argued to be anaphoric, such as VP ellipsis (Hardt 1992,1999; Kehler 2002), tense (Partee 1984; Webber 1988) and modality (Kibble 1995; Frankand Kamp 1997; Stone and Hardt 1999).3.
A Framework for AnaphoraHere we show how only a single extension to a general framework for discourseanaphora is needed to cover discourse adverbials.
The general framework is presentedin Section 3.1, and the extension in Section 3.2.3.1 Discourse Referents and Anaphor InterpretationThe simplest discourse anaphors are coreferential: definite pronouns and definite NPsthat denote one (or more) discourse referents in focus within the current discourse6 Although rhetorical structure theory (RST) (Mann and Thompson 1998) was developed as an accountof the relation between adjacent units within a text, Marcu?s guide to RST annotation (Marcu 1999) hasadded an ?embedded?
version of each RST relation in order to handle examples such as (24).
Althoughthis importantly recognizes that material in an embedded clause can bear a semantic relation to itsmatrix clause, it does not contribute to understanding the nature of the phenomenon.555Webber et al Anaphora and Discourse Structurecontext.
(Under coreference we include split reference, in which a plural anaphor suchas the companies denotes all the separately mentioned companies in focus within thediscourse context.)
Much has been written about the factors affecting what discoursereferents are taken to be in focus.
For a recent review by Andrew Kehler, see chap-ter 18 of Jurafsky and Martin (2000).
For the effect of different types of quantifiers ondiscourse referents and focus, see Kibble (1995).Somewhat more complex than coreference is indirect anaphora (Hellman andFraurud 1996) (also called partial anaphora [Luperfoy 1992], textual ellipsis [Hahn,Markert, and Strube 1996], associative anaphora [Cosse 1996] bridging anaphora[Clark 1975; Clark and Marshall 1981; Not, Tovena, and Zancanaro 1999], and in-ferrables [Prince 1992]), in which the anaphor (usually a definite NP) denotes a dis-course referent associated with one (or more) discourse referents in the current discoursecontext; for example,(25) Myra darted to a phone and picked up the receiver.Here the receiver denotes the receiver associated with (by virtue of being part of) thealready-mentioned phone Myra darted to.Coreference and indirect anaphora can be uniformly modeled by saying that thediscourse referent e?
denoted by an anaphoric expression ?
is either equal to or asso-ciated with an existing discourse referent er, that is, e?=er or e?
?assoc(er).
But coref-erence and associative anaphora do not exhaust the space of constructs that deriveall or part of their sense from the discourse context and are thus anaphoric.
Consider?other NPs?
(Bierner 2001a; Bierner and Webber 2000; Modjeska 2001, 2002), as in:(26) Sue grabbed one phone, as Tom darted to the other phone.Although ?other NPs?
are clearly anaphoric, should the referent of the other phone(e?
)?the phone other than the one Sue grabbed (er)?simply be considered a case ofe?
?
assoc(er)?
Here are two reasons why they should not.First, in all cases of associative anaphora discussed in the literature, possible as-sociations have depended only on the antecedent er and not on the anaphor.
Forexample, only antecedents that have parts participate in whole-part associations (e.g.,phone ?
receiver).
Only antecedents with functional schemata participate in schema-based associations (e.g., lock ?
key).
In (26), the relationship between e?, the referentof the other phone, and its antecedent, er, depends in part on the anaphor, and not juston the antecedent?in particular, on the presence of the word other.Second, we also have examples such as(27) Sue lifted the receiver as Tom darted to the other phone.7in which the referent of the other phone (e?)
is the phone other than the phone associatedwith the receiver that Sue lifted.
Together, these two points argue for a third possibility,in which an anaphoric element can convey a specific function f?
that is idiosyncraticto the anaphor, which may be applied to either er or an associate of er.
The result ofthat application is e?.
For want of a better name, we will call these lexically specifiedanaphors.Other lexically specified anaphors include noun phrases headed by other (exam-ple (28)), NPs with such but no postmodifying as phrase (example (29)), comparative7 Modjeska (2001) discovered such examples in the British National Corpus.556Computational Linguistics Volume 29, Number 4NPs with no postmodifying than phrase (example (30)), and the pronoun elsewhere(example (31)) (Bierner 2001b)(28) Some dogs are constantly on the move.
Others lie around until you callthem.
(29) I saw a 2kg lobster in the fish store yesterday.
The fishmonger said it takesabout five years to grow to such a size.
(30) Terriers are very nervous.
Larger dogs tend to have calmer dispositions.
(31) I don?t like sitting in this room.
Can we move elsewhere?To summarize the situation with anaphors so far, we have coreference when e?=er,indirect anaphora when e?
?assoc(er), and lexically specified anaphora when e?=f?
(ei)where ei = er or ei ?assoc(er).3.2 Discourse Adverbials as Lexical AnaphorsThere is nothing in this generalized approach to discourse anaphora that requires thatthe source of er be an NP, or that the anaphor be a pronoun or NP.
For example, theantecedent er of a singular demonstrative pronoun (in English, this or that) is oftenan eventuality that derives from a clause, a sentence, or a larger unit in the recentdiscourse (Asher 1993; Byron 2002; Eckert and Strube 2000; Webber 1991).
We willshow that this is the case with discourse adverbials as well.The extension we make to the general framework presented above in order toinclude discourse adverbials as discourse anaphors is to allow more general functionsf?
to be associated with lexically specified anaphors.
In particular, for the discourseadverbials considered in this article, the function associated with an adverbial maps itsanaphoric argument?an eventuality derived from the current discourse context?toa function that applies to the interpretation of the adverbial?s matrix clause (itself aneventuality).
The result is a binary relation that holds between the two eventualitiesand is added to the discourse context.
For example, in(32) John loves Barolo.
So he ordered three cases of the ?97.
But he had tocancel the order because he then discovered he was broke.then, roughly speaking, contributes the fact that its matrix clause event (John?s find-ing he was broke) is after the anaphorically derived event of his ordering the wine.8Similarly, in(33) John didn?t have enough money to buy a mango.
Instead, he bought aguava.instead contributes the fact that its matrix clause event (buying a guava) is an alternativeto the anaphorically derived event of buying a mango.
The relation between the twosentences is something like result, as in So instead, he bought a guava.8 Words and phrases that function as discourse adverbials usually have other roles as well: For example,otherwise also serves as an adjectival modifier, as in I was otherwise occupied with grading exams.
Thisoverloading of closed-class lexico-syntactic items is not unusual in English, and any ambiguities thatarise must be handled as part of the normal ambiguity resolution process.557Webber et al Anaphora and Discourse StructureNote that our only concern here is with the compositional and anaphoric mech-anisms by which adverbials contribute meaning.
For detailed analysis of the lexicalsemantics of adverbials (but no attention to mechanism), the reader is referred to Jayesand Rossari (1998a, 1998b, Lagerwerf (1998), Traugott (1995, 1997), and others.Formally, we represent the function that a discourse adverbial ?
contributes as a?-expression involving a binary relation R?
that is idiosyncratic to ?, one of whosearguments (represented here by the variable EV) is resolved anaphorically:?x .
R?
(x, EV)R?
gets its other argument compositionally, when this ?-expression is applied to ?
?smatrix clause S interpreted as an eventuality ?, that is,[?x .
R?
(x, EV)]?
?
R?
(?, EV)The result of both function application and resolving EV to some eventuality ei derivedfrom the discourse context either directly or by association is the proposition R?
(?, ei),one of whose arguments (ei) has been supplied by the discourse context and the other(?)
compositionally from syntax.Note that this is a formal model, meant to have no implications for how pro-cessing takes place.
We have not tried at this stage to instantiate our view of howdiscourse adverbials are resolved in the context of (simultaneous) sentence-level anddiscourse-level processing.
Our basic view is that resolution is initiated when the dis-course adverbial (?)
is encountered.
As ?
?s matrix clause S is incrementally parsed andinterpreted, producing eventuality ?, the resolution process polls the discourse contextand either finds an appropriate eventuality ei (or creates one by a bridging inference,as illustrated in the next section) such that R?
(?, ei) makes sense with respect to thediscourse so far.
As is the case with resolving a discourse deictic (Asher 1993; Byron2002; Eckert and Strube 2000; Webber 1991) this resolution process would use syn-tactic and semantic constraints that it accumulates as the incremental sentence-levelparser/interpreter processes S. As with discourse deixis, this is best seen as a con-straint satisfaction problem that involves finding or deriving an eventuality from thecurrent discourse context that meets the constraints of the adverbial with respect to theeventuality interpretation of the matrix clause.
(Examples of this are given throughoutthe rest of the article.
)3.3 A Logical Form for EventualitiesBefore using this generalized view of anaphora to show what discourse adverbialscontribute to discourse and how they interact with discourse relations that arise fromadjacency or explicit discourse connectives, we briefly describe how we representclausal interpretations in logical form (LF).Essentially, we follow Hobbs (1985) in using a rich ontology and a representationscheme that makes explicit all the individuals and abstract objects (i.e., propositions,facts/beliefs, and eventualities) (Asher 1993) involved in the LF interpretation of anutterance.
We do so because we want to make intuitions about individuals, eventual-ities, lexical meaning, and anaphora as clear as possible.
But certainly, other forms ofrepresentation are possible.In this LF representation scheme, each clause and each relation between clausesis indexed by the label of its associated abstract object.
So, for example, the LF inter-pretation of the sentence(34) John left because Mary left.558Computational Linguistics Volume 29, Number 4would be writtene1:left(j) ?
john(j) ?
e2:left(m) ?
mary(m) ?
e3:because(e1,e2)where the first argument of the asymmetric binary predicate because is the consequentand the second is the eventuality leading to this consequent.
Thus when because occurssentence-medially, as in the above example, the eventuality arguments are in the sameorder as their corresponding clauses occur in the text.
When because occurs sentence-initially (as in Because Mary left, John did), the interpretation of the second clause (John[left]) will appear as the first argument and the interpretation of the first clause (Maryleft) will appear as the second.9The set of available discourse referents includes both individuals like j and m, andalso abstract objects like e1 and e2.
We then represent resolved anaphors by reusingthese discourse referents.
So, for example, the LF interpretation of the follow-on sen-tence(35) This upset Sue.would be writtene4:upset(DPRO, s) ?
sue(s)where DPRO is the anaphoric variable contributed by the demonstrative pronoun this.Since the subject of upset could be either the eventuality of John?s leaving or the factthat he left because Mary left, DPRO could be resolved to either e1 or e3, that is,a.
e4:upset(e1, s) ?
sue(s)b. e4:upset(e3, s) ?
sue(s)depending on whether one took Sue to have been upset by (1) John?s leaving or (2)that he left because Mary left.3.4 The Contribution of Discourse Adverbials to Discourse SemanticsHere we step through some examples of discourse adverbials and demonstrate howthey make their semantic contribution to the discourse context.
We start with exam-ple (32), repeated here as (36):(36) a. John loves Barolo.b.
So he ordered three cases of the ?97.c.
But he had to cancel the orderd.
because he then discovered he was broke.9 We are not claiming to give a detailed semantics of discourse connectives except insofar as they mayaffect how discourse adverbials are resolved.
Thus, for example, we are not bothering to distinguishamong different senses of because (epistemic vs. nonepistemic), while (temporal vs. concessive), since(temporal vs. causal), etc.
Of course, these distinctions are important to discourse interpretation, butthey are independent of and orthogonal to the points made in this article.
Similarly, Asher (1993)argues that a simple ontology of eventualities is too coarse-grained, and that discourse representationsneed to distinguish different kinds of abstract objects, including actions, propositions, and facts as wellas eventualities.
Different discourse connectives will require different kinds of abstract objects asarguments.
This distinction is also orthogonal to the points made in this article, because we canunderstand these abstract referents to be associates of the corresponding Hobbsian eventualities andleave the appropriate choice to the lexical semantics of discourse connectives.
Byron (2002) advocates asimilar approach to resolving discourse anaphora.559Webber et al Anaphora and Discourse StructureUsing the above LF representation scheme and our notation from Section 3.2, namely,?
?
= the anaphoric expression (here, the discourse adverbial)?
R?
= the relation name linked with ??
S = the matrix clause/sentence containing ??
?
= the interpretation of S as an abstract objectand ignoring, for now, the conjunction because (discussed in section 4), the relevantelements of (36d) can be represented as:?
= thenR?
= afterS = he [John] discovered he was broke?
= e4:find(j,e5), where e5:broke(j)This means that the unresolved interpretation of (36d) is[?x .
R?(x,EV)]?
?
[?x .
after(x,EV)]e4 ?
after(e4, EV)The anaphoric argument EV is resolved to the eventuality e2, derived from (36b)?e2:order(j, c1).after(e4,EV) ?
after(e4,e2)That is, the eventuality of John?s finding he was broke is after that of John?s orderingthree cases of the ?97 Barolo.
The resulting proposition after(e4,e2) would be given itsown index, e6, and added to the discourse context.When then is understood temporally, as it is above, as opposed to logically, itrequires a culminated eventuality from the discourse context as its first argument(which Vendler (1967) calls an achievement or an accomplishment).
The orderingevent in (36b) is such a Vendlerian accomplishment.
In example (37), though, thereis no culminated eventuality in the discourse context for then), to take as its firstargument.
(37) a.
Go west on Lancaster Avenue.b.
Then turn right on County Line.How does (37b) get its interpretation?As with (36d), the relevant elements of (37b) can be represented as?
= thenR?
= afterS = turn right on County Line?
= e3:turn-right(you, county line)and the unresolved interpretation of (37b) is thus[?
x .
after(x, EV)]e3 ?
after(e3, EV)560Computational Linguistics Volume 29, Number 4As for resolving EV, in a well-known article, Moens and Steedman (1988) discussseveral ways in which an eventuality of one type (e.g., a process) can be coerced intoan eventuality of another type (e.g., an accomplishment, which Moens and Steedmancall a culminated process).
In this case, the matrix argument of then (the eventuality ofturning right on County Line) can be used to coerce the process eventuality in (37b) intoa culminated process of going west on Lancaster Avenue until County Line.
We treat thiscoercion as a type of associative or bridging inference, as in the examples discussedin section 3.1.
That is,e2 = culmination(e1)?assoc(e1), where e1:go-west(you, lancaster ave)Taking this e2 as the anaphoric argument EV of then yields the propositionafter(e3, e2)That is, the eventuality of turning right onto County Line is after that of going weston Lancaster Avenue to County Line.
This proposition would be indexed and addedto the discourse context.It is important to stress here that the level of representation we are concernedwith is essentially an LF for discourse.
Any reasoning that might then have to be doneon the content of LFs might then require making explicit the different modal andtemporal contexts involved, their accessibility relations, the status of abstract objectsas facts, propositions or eventualities, and so on.
But as our goal here is primarilyto capture the mechanism by means of which discourse adverbials are involved indiscourse structure and discourse semantics, we will continue to assume for as longas possible that an LF representation will suffice.Now it may appear as if there is no difference between treating adverbials asanaphors and treating them as structural connectives, especially in cases like (37) inwhich the antecedent comes from the immediately left-adjacent context, and in whichthe only obvious semantic relation between the adjacent sentences appears to be theone expressed by the discourse adverbial.
(Of course, there may also be a separateintentional relation between the two sentences [Moore and Pollack 1992], independentof the relation conveyed by the discourse adverbial.
)One must distinguish, however, between whether a theory allows a distinctionto be made and whether that distinction needs to be made in a particular case.
Itis clear that there are many examples in which the two approaches (i.e., a purelystructural treatment of all connectives, versus one that treats adverbials as linking intothe discourse context anaphorically) appear to make the same prediction.
We havealready, however, demonstrated cases in which a purely structural account makes thewrong prediction, and in the next section, we will demonstrate the additional powerof an account that allows for two relations between an adverbial?s matrix clause orsentence and the previous discourse: one arising from the anaphoric connection andthe other inferred from adjacency or conveyed explicitly by a structural connective.Before closing this section, we want to step through examples (19)?
(20), repeatedhere as examples (38)?(39).
(38) If the light is red, stop.
Otherwise you?ll get a ticket.
(39) If the light is red, stop.
Otherwise go straight on.561Webber et al Anaphora and Discourse StructureRoughly speaking, otherwise conveys that the complement of its anaphorically derivedargument serves as the condition under which the interpretation of its structural ar-gument holds.
(This complement must be with respect to some contextually relevantset.
)10If we represent a conditional relation between two eventualities with the asym-metric relation if(e1,e2), where e1 is derived from the antecedent and e2 from the conse-quent, and we approximate a single contextually relevant alternative e2 to an eventu-ality e1 using a symmetric complement relation, complement(e1, e2), then we can representthe interpretation of otherwise as?
x .
if(VE, x), where complement(VE, EV)where variable EV is resolved anaphorically to an eventuality in the current discoursecontext that admits a complement.
That is, otherwise requires a contextually relevantcomplement to its antecedent and asserts that if that complement holds, the argumentto the ?-expression will as well.
The resulting ?-expression applies to the interpretationof the matrix clause of otherwise, resulting in the conditional?s being added to thediscourse context:[?x .
if(VE,x)] ?
?
if(VE,?
), where complement(VE,EV)Here the relevant elements of (38b) and (39b) can be represented as?
= otherwiseR?
= ifS38 = you get a ticket?38 = e3, where e3:get ticket(you)S39 = go straight on?39 = e3?
, where e3?
:go straight(you)The unresolved interpretations of (38b) and (39b) are thus:[?x .
if(VE38,x)] e3 ?
if(VE38,e3), where complement(VE38,EV38)[?x .
if(VE39,x)] e3?
?
if(VE39,e3?
), where complement(VE39,EV39)As we showed in section 2.2, different ways of resolving the anaphoric argument leadto different interpretations.
In (38), the anaphoric argument is resolved to e2:stop(you),10 Kruijff-Korbayova?
and Webber (2001a) demonstrate that the information structure of sentences in theprevious discourse (theme-rheme partitioning, as well as focus within theme and within rheme[Steedman 2000a]) can influence what eventualities er are available for resolving the anaphoricallyderived argument of otherwise.
This then correctly predicts different interpretations for ?otherwise?
in(i) and (ii):(i) Q.
How should I transport the dog?A.
You should carry the dog.
Otherwise you might get hurt.
(ii) Q.
What should I carry?A.
You should carry the dog.
Otherwise you might get hurt.In both (i) and (ii), the questions constrain the theme/rheme partition of the answer.
Small capitalsrepresent focus within the rheme.
In (i), the otherwise clause will be interpreted as warning the hearer(H) that H might get hurt if he/she transports the dog in some way other than carrying it (e.g., H mightget tangled up in its lead).
In (ii), the otherwise clause warns H that he/she might get hurt if what sheis carrying is not the dog (e.g., H might be walking past fanatical members of the Royal Kennel Club).562Computational Linguistics Volume 29, Number 4whereas in (39), it is resolved to e1:red(light1).
Thus the resulting interpretations of(38b) and (39b) are, respectively,if(e4,e3), where complement(e2,e4) and e2:stop(you)(If you do something other than stop, you?ll get a ticket.)if(e4?
, e3?
), where complement(e1,e4? )
and e1:red(light)(If the light is not red, go straight on.
)We have not been specific about how the anaphoric argument of otherwise (orof any other discourse adverbial) is resolved, other than having it treated as a con-straint satisfaction problem.
This is the subject of current and future work, exploringthe empirical properties of resolution algorithms with data drawn from appropriatelyannotated corpora and from psycholinguistic studies of human discourse interpreta-tion.
To this end, Creswell et al (2002) report on a preliminary annotation study ofdiscourse adverbials and the location and type of their antecedents.
This initial ef-fort involves nine discourse adverbials?three each from the classes of concessive,result, and reinforcing (additive) conjuncts given in Quirk et al (1972).
Meanwhile,Venditti et al (2002) present a preliminary report on the use of a constraint satisfac-tion model of interpretation, crucially combining anaphoric and structural reasoningabout discourse relations, to predict subjects?
on-line interpretation of discourses in-volving stressed pronouns.
In addition, two proposals have recently been submitted toconstruct a larger and more extensively annotated corpus, covering more adverbials,based on what we have learned from this initial effort.
This more extensive studywould be an adequate basis for developing resolution algorithms.113.5 SummaryIn this section, we have presented a general framework for anaphora with the follow-ing features:?
Anaphors can access one or more discourse referents or entitiesassociated with them through bridging inferences.
These are sufficientfor interpreting anaphoric pronouns, definite NPs and demonstrativeNPs, allowing entities to be evoked by NPs or by clauses.
In the case ofclauses, this may be on an as-needed basis, as in Eckert and Strube(2000).?
A type of anaphor ?
that we call lexically specified can also contributeadditional meaning through a function f?
that is idiosyncratic to ?, thatcan be applied to either an existing discourse referent or an entityassociated with it through a bridging inference.
In the case of thepremodifier other, f?
applied to its argument produces contextually11 With respect to how many discourse adverbials there are, Quirk et al (1972) discuss 60 conjunctionsand discourse adverbials under the overall heading time relations and 123 under the overall headingconjuncts.
Some entries appear under several headings, so that the total number of conjunctions anddiscourse adverbials they present is closer to 160.
In another enumeration of discourse adverbials,Forbes and Webber (2002) start with all annotations of sentence-level adverbials in the Penn Treebank,then filter them systematically to determine which draw part of their meaning from the precedingdiscourse and how they do so.
What we understand from both of these studies is that there are fewerthan 200 adverbials to be considered, many of which are minor variations of one another (in contrast, bycontrast, by way of contrast, in comparison, by comparison, by way of comparison that are unlikely to differ intheir anaphoric properties, and some of which, such as contrariwise, hitherto, and to cap it all, will occuronly rarely in a corpus of modern English.563Webber et al Anaphora and Discourse Structurerelevant alternatives to that argument.
In the case of the premodifiersuch, it yields a set of entities that are similar to its argument in acontextually relevant way.?
Discourse adverbials are lexically specified anaphors whose meaningfunction f?
is a ?-expression involving a binary relation R?
that isidiosyncratic to ?, one of whose arguments is resolved anaphorically andthe other is provided compositionally, when the ?-expression is appliedto ?
?s matrix clause interpreted as an eventuality ?.In the next section, we move on to consider how the presence of both a semantic rela-tion associated with a discourse adverbial and a semantic relation associated with theadjacency of two clauses or a structural connective between them allows for interestinginteractions between the two.4.
Patterns of Anaphoric Relations and Structural/Inferred RelationsPrior to the current work, researchers have treated both explicit structural connec-tives (coordinating and subordinating conjunctions, and ?paired?
conjunctions) anddiscourse adverbials simply as evidence for a particular structural relation holdingbetween adjacent units.
For example, Kehler (2002) takes but as evidence of a contrastrelation between adjacent units, in general as evidence of a generalization relation,in other words as evidence of an elaboration relation, therefore as evidence of a resultrelation, because as evidence of an explanation relation, and even though as evidenceof a denial of preventer relation (Kehler 2002, Section 2.1).
Here Kehler has probablycorrectly identified the type of relation that holds between elements, but not whichelements it holds between.In one respect, we follow previous researchers, in that we accept that when clauses,sentences, or larger discourse units are placed adjacent to one another, listeners infer arelation between the two, and that the structural connective (coordinate or subordinateconjunction) gives evidence for the relation that is intended to hold between them.Because we take discourse adverbials to contribute meaning through an anaphoricconnection with the previous discourse, however, this means that there may be tworelations on offer and opens up the possibility that the relation contributed by thediscourse adverbial can interact in more than one way with the relation conveyedby a structural connective or inferred through adjacency.
Below we show that thisprediction is correct.We start from the idea that, in the absence of an explicit structural connective, de-feasible inference correlates with structural attachment of adjacent discourse segmentsin discourse structure, relating their interpretations.
The most basic relation is that thefollowing segment in some way describes the same object or eventuality as the one itabuts (elaboration).
But evidence in the segments can lead (via defeasible inference) toa more specific relation, such as one of the resemblance relations (e.g., parallel, contrast,exemplification, generalisation), or cause-effect relations (result, explanation, violated expecta-tion), or contiguity relations (narration) described in Hobbs (1990) and Kehler (2002).
Ifnothing more specific can be inferred, the relation will remain simply elaboration.
Whatexplicit structural connectives can do is convey relations that are not easy to conveyby defeasible inference (e.g., if, conveying condition, and or, conveying disjunction) orprovide nondefeasible evidence for an inferrable relation (e.g., yet, so, and because).Discourse adverbials can interact with structural connectives, with adjacency-triggered defeasible inference, and with each other.
To describe the ways in which we564Computational Linguistics Volume 29, Number 4have so far observed discourse adverbials to interact with relations conveyed struc-turally, we extend the notation used in the previous section:?
?
= discourse adverbial?
R?
= the name of the relation associated with ??
S = the matrix clause/sentence of ??
?
= the logical form (LF) interpretation of Sadding the following:?
D = the discourse unit that is left-adjacent to S, to which a relationshipholds by either inference or a structural connective?
?
= the LF interpretation of D?
R = the name of the relation that holds with ?Although ?
is one argument of R, we show below that the other argument of R maybe one of at least two different abstract objects.Case 1: ?
separately serves as an argument to both R?
and R. This is the case thatholds in example (36) (repeated here):(36) a. John loves Barolo.b.
So he ordered three cases of the ?97.c.
But he had to cancel the orderd.
because he then discovered he was broke.We have already seen that the interpretation of the clause in (36d) following becauseinvolvesR?
= after?
= e4:discover(j,e5), where e5:broke(j)[?x .
after(x,EV)]e4 ?
after(e4, EV)where EV is resolved to e2:order(j, c1), and the proposition after(e4, e2) is added to thediscourse context?that is, John?s discovering he was broke is after his ordering thewine.Now consider the explanation relation R associated with because in (36d).
It relatese4, John?s finding he was broke, to the intepretation of (36c), e3:cancel(j,o1)?that is,explanation(e4,e3).
Clause 36d thus adds both explanation(e4,e3) and after(e4, e2) to thediscourse.
Although these two propositions share an argument (e4), they are neverthe-less distinct.1212 Because eventuality e4, John?s finding he was broke, both explains the canceling and follows the ordering, itfollows that the canceling is after the ordering.565Webber et al Anaphora and Discourse StructureCase 2: R?
(?, ei) is an argument of R. In case 1, it is the interpretation of the ad-verbial?s matrix clause ?
that serves as one argument to the discourse relation R. Incontrast, in case 2, that argument is filled by the relation contributed by the discourseadverbial (itself an abstract object available for subsequent reference).
In both cases,the other argument to R is ?.One configuration in which case 2 holds is with the discourse adverbial otherwise.Recall from section 3.4 that the interpretation of otherwise involves a conditional relationbetween the complement of its anaphoric argument and the interpretation ?
of itsmatrix clause:[?x .
if(VE,x)] ?
?
if(VE,?
), where complement(VE,EV)With variable EV resolved to an eventuality in the discourse context, it is the resultingrelation (viewed as an abstract object) that serves as one argument to R, with ?
servingas the other.
We can see this most clearly by considering variants of examples (38) and(39) that contain an explicit connective between the clauses.
In (38), the conjunctionbecause is made explicit (example (40)), and in (39), the connective is simply and or but(example (41)).
(40) If the light is red, stop, because otherwise you?ll get a ticket.R?
= if?38 = e3:get ticket(you)(41) If the light is red, stop, and/but otherwise go straight on.R?
= if?39 = e3?
:go straight(you)In the case of (40), resolving otherwise contributes the relatione6: if(e4,e3), where complement(e4,e2) and e2:stop(you)(If you do something other than stop, you?ll get a ticket.
)At the level of LF, the abstract object e6 that is associated with the conditional relationserves as one argument to the explanation relation contributed by because, with e2 beingthe other.
That is, because and otherwise together end up contributing explanation(e2,e6)(i.e., your needing to stop is explained by the fact that if you do something other thanstop, you?ll get a ticket).In the case of (41), resolving otherwise contributes the relatione6?
:if(e4?
, e3?
), where complement(e4?
,e1) and e1:red(light)(If the light is not red, go straight on.
)What is the discourse relation to which otherwise contributes this abstract object e6?
?Whether the connective is and or but, both its conjuncts describe (elaborate) alternativespecializations of the same situation e0 introduced earlier in the discourse (e.g., e0 couldbe associated with the first sentence of Go another mile and you?ll get to a bridge.
If thelight is red, stop.
Otherwise go straight on.)
If the connective is and, what is added tocontext might simply be elaboration(e6?
,e0).
(Note that without otherwise, the relationelaboration(e5,e0) would have been added to context, where e5 is the abstract objectassociated with the interpretation of If the light is red, stop.)
If the connective is but, thenone might also possibly add contrast(e6?
,e5)?that is, the situation that (if the light is566Computational Linguistics Volume 29, Number 4red) you should stop is in contrast to the situation that if the light is not red, youshould go straight on.13As is clear from the original pair of examples (38) and (39), interpretations canarise through adjacency-triggered inference that are similar to those that arise with anexplicit connective.
In either case, the above treatment demonstrates that there is noneed for a separate otherwise relation, as proposed in rhetorical structure theory (Mannand Thompson 1988).
We are not, however, entirely clear at this point when case 1holds and when case 2 does.
A more careful analysis is clearly required.Case 3: R?
is parasitic on R. Case 3 appears to apply with discourse adverbials suchas for example and for instance.
The interpretation of such adverbials appears to beparasitic on the relation associated with a structural connective or discourse adverbialto their left, or on an inferred relation triggered by adjacency.
The way to understandthis is to first consider intraclausal for example, where it follows the verb, as in(42) Q.
What does this box contain?A.
It contains, for example, some hematite.The interpretation of for example here involves abstracting the meaning of its matrixstructure with respect to the material to its right, then making an assertion with respectto this abstraction.
That is, if the LF contributed by the matrix clause of (42A) is,roughly,i.
contain(box1,hematite1)then the LF resulting from the addition of for example can be written either with setnotation (as in (ii)), taking an entity to exemplify a set, or with ?-notation (as in (iii)),taking an entity to exemplify a property:ii.
exemplify(hematite1, {X | contain(box1,X)})iii.
exemplify(hematite1, ?X .
contain(box1,X))Both express the fact that hematite is an example of what is contained in the box.14 Sinceone can derive (i) logically from either (ii) or (iii), one might choose to retain only (ii) or(iii) and derive (i) if and when it is needed.
In the remainder of the article, we use the?
notation given in (iii).
Note that from the perspective of compositional semantics, forexample resembles a quantifier, in that the scope of its interpretation is not isomorphicto its syntactic position.
Thus producing an interpretation for for example will requiretechniques similar to those that have long been used in interpreting quantifiers (Woods,1978; Barwise and Cooper 1981).
We take this up again in section 5.If we look at the comparable situation in discourse, such as (43)?
(44), where forexample occurs to the right of a discourse connective, it can also be seen as abstracting13 A much finer-grained treatment of the semantics of otherwise in terms of context-update potential isgiven in Kruijff-Korbayova?
and Webber (2001b).
Here we are just concerned with its interaction withstructural connectives and adjacency-triggered relations.14 The material to the right of for example can be any kind of constituent, including such strange ones asJohn gave, for example, a flower to a nurse.Here, a flower to a nurse would be an example of the set of object-recipient pairs within John?s givings.Such nonstandard constituents are also found with coordination, which was one motivation forcombinatory categorial grammar (Steedman 1996).
This just illustrates another case in which suchnonstandard constituents are needed.567Webber et al Anaphora and Discourse Structurethe interpretation of its discourse-level matrix structure, with respect to the materialto its right:(43) John just broke his arm.
So, for example, he can?t cycle to work now.
(44) You shouldn?t trust John because, for example, he never returns what heborrows.In (43), the connective so leads toresult(?,?
)being added to the discourse, where ?
is the interpretation of John can?t cycle to worknow, and ?
is the interpretation of John just broke his arm.
For example then abstracts thisrelation with respect to the material to its right (i.e., ?
), thereby contributingexemplify(?, ?X .
result(X, ?
))That is, John can?t cycle to work is an example of what results from John?s breaking hisarm.
Similarly, because in (44) leads toexplanation(?,?
)being added to the discourse, where ?
is the interpretation of he never returns what heborrows, ?
is the interpretation of you shouldn?t trust John, and for example addsexemplify(?, ?X .
explanation(X,?
))that is, that ?
is an example of the reasons for not trusting John.For example interacts with discourse adverbials in the same way:(45) Shall we go to the Lincoln Memorial?
Then, for example, we can go to theWhite House.
(46) As a money manager and a grass-roots environmentalist, I was very dis-appointed to read in the premiere issue of Garbage that The Wall StreetJournal uses 220,000 metric tons of newsprint each year, but that only 1.4%of it comes from recycled paper.
By contrast, the Los Angeles Times, for ex-ample, uses 83% recycled paper.
[WSJ, from Penn Treebank /02/wsj-0269]In example (45), the resolved discourse adverbial then leads to after(?,?)
being addedto the discourse context, where ?
is the interpretation of we can go to the White House, ?is the interpretation of we shall go to the Lincoln Memorial, and for example addsexemplify(?, ?X .
after(X,?
))that is, that ?
is an example of the events that [can] follow going to the LincolnMemorial.
(As already noted, we are being fairly fast and loose regarding tense andmodality, in the interests of focusing on the types of interactions.
)568Computational Linguistics Volume 29, Number 4In example (46), the resolved discourse anaphor by contrast contributes contrast(?,?
),where ?
is the interpretation of the Los Angeles Times?s using 83% recycled paper and ?
isthe intepretation of only 1.4% of it [newsprint used by the WSJ] comes from recycled paper.For example then contributesexemplify(?, ?X .
contrast(X,?
))that is, that ?
is one example of contrasts with the WSJ?s minimal use of recycledpaper.What occurs with discourse connectives and adverbials can also occur with rela-tions added through adjacency-triggered defeasible inference, as in(47) You shouldn?t trust John.
For example, he never returns what he borrows.explanation(?,?
)exemplify(?, ?X .
explanation(?,X))Here, as in (44), the relation provided by adjacency-triggered inference is R = explana-tion, which is then used by for example.But what about the many cases in which only exemplify seems present, as in(48) In some respects they [hypertext books] are clearly superior to normalbooks, for example they have database cross-referencing facilities ordinaryvolumes lack.
(British National Corpus, CBX 1087)(49) He [James Bellows] and his successor, Mary Anne Dolan, restored respectfor the editorial product, and though in recent years the paper had beenlimping along on limited resources, its accomplishments were notable.
Forexample, the Herald consistently beat its much-larger rival on disclosuresabout Los Angeles Mayor Tom Bradley?s financial dealings.There are at least two explanations: One is that for example simply provides directnondefeasible evidence for exemplify, which is the only relation that holds.
The otherexplanation follows the same pattern as the examples given above, but with no furtherrelation than elaboration(?,?).
That is, we understand in (48) that having database cross-referencing facilities elaborates the respects in which hypertext books are superior tonormal books, whereas in (49), we understand that the Herald?s [newspaper] consistentlybeating its much-larger rival elaborates the claim that its accomplishments were notable.
Thiselaboration relation is then abstracted (in response to for example) to produce:exemplify(?, ?X .
elaboration(X, ?
))that is, that this is one example of many possible elaborations.
Because this is morespecific than elaboration and seems to mean the same as exemplify(?,?
), one mightsimply take it to be the only relation that holds.
Given that so many naturally occuringinstances of for example occur with elaboration, it is probably useful to persist with theabove shorthand.
But it shouldn?t obscure the regular pattern that appears to hold.Before going on to case 4, we should comment on an ambiguity associated with forexample.
When for example occurs after an NP, a PP, or a clause that can be interpretedas a general concept or a set, it can contribute a relation between the general conceptor set and an instance, rather than being parasitic on another relation.
For example,in:569Webber et al Anaphora and Discourse Structure(50) In the case of the managed funds they will be denominated in a leadingcurrency, for example US dollar, .
.
.
(BNC CBX 1590)for example relates the general concept denoted by a leading currency to a specific in-stance, U.S. dollars.
(In British English, the BNC shows that most such examples occurwith such as?i.e., in the construction such as for example.
This paraphrase does not workwith the predicate-abstracting for example that is of primary concern here, as in exam-ple (42).
)But for example occurring after an NP, a PP, or a clause can, alternatively, contributea more subtle parasitic relationship to the previous clause, as in(51) All the children are ill, so Andrew, for example, can?t help out in the shop.This differs from both (43) and (50).
That is, one cannot paraphrase (51) as (52) as in(43) where for example follows so:(52) All the children are ill, so for example Andrew can?t help out in the shop.Example (52) simply specifies an example consequence of all the children being ill, asdoes(53) All the children are ill, so for example one of us has to be at home at alltimes.In contrast, (51) specifies an example consequence for Andrew, as one of the children.Support for this comes from the fact that in (52), Andrew doesn?t have to be one ofthe children: He could be their nanny or child minder, now stuck with dealing with alot of sick kids.
But (51) is not felicitous if Andrew is not one of the children.We suspect here the involvement of information structure (Steedman 2000a):Whereas the interpretation conveyed by for example is parasitic on the adjacency rela-tion (result in example (51)), its position after the NP Andrew in (51) may indicate acontrastive theme with respect to the previous clause, according to which Andrew incontrast to the other children suffers this particular consequence.
But more work needsto be done on this to gain a full understanding of what is going on.Case 4: R?
is a defeasible rule that incorporates R. Case 4 occurs with discourseadverbials that carry the same presupposition as the discourse connectives althoughand the concessive sense of while (Lagerwerf 1998).
Case 4 shares one feature withcase 1, in that the discourse relation R conveyed by a structural connective or inferredfrom adjacency holds between ?
(the interpretation of the adverbial?s matrix clause)and ?
(the interpretation of the left-adjacent discourse unit).
Where it differs is thatthe result is then incorporated into the presupposition of the discourse adverbial.
Thispresupposition, according to Lagerwerf (1998), has the nature of a presupposed (orconventionally implicated) defeasible rule that fails to hold in the current situation.He gives as an example(54) Although Greta Garbo was called the yardstick of beauty, she never mar-ried.This asserts both that Greta Garbo was called the yardstick of beauty and that shenever married.
The first implies that Greta Garbo was beautiful.
The example also570Computational Linguistics Volume 29, Number 4presupposes that, in general, if a woman is beautiful, she will marry.
If such a pre-supposition can be accommodated, it will simply be added to the discourse context.If not, the hearer will find the utterance confusing or possibly even insulting.We argue here that the same thing happens with the discourse adverbials never-theless and though.
The difference is that, with discourse adverbials, the antecedent tothe rule derives anaphorically from the previous discourse, whereas the consequentderives from the adverbial?s matrix clause.
(With the conjunctions although and con-cessive while, both arguments are provided structurally.
)We first illustrate case 4 with two examples in which nevertheless occurs in themain clause of a sentence containing a preposed subordinate clause.
The subordinateconjunction helps clarify the relation between the clauses that forms the basis for thepresupposed defeasible rule.
After these, we give a further example in which therelation between the adjacent clauses comes through inference.
(55) While John is discussing politics, he is nevertheless thinking about his fish.In (55), the conjunction while conveys a temporal relation R between the two clausesit connects:during(e2, e1), where e1:discuss(john,politics) and e2:think about(john,fish)What nevertheless contributes to (55) is a defeasible rule based on this relation, whichwe will write informally asduring(X,E) ?
E:discuss(Y,politics)) > ?X:think about(Y,fish))Normally, whatever one does during the time one is discussing politics, it isnot thinking about one?s fish.This rule uses Asher and Morreau?s (1991) defeasible implication operator (>) andabstracts over the individual (John), which seems appropriate for the general statementconveyed by the present tense of the utterance.Similarly, in(56) Even after John has had three glasses of wine, he is nevertheless able tosolve difficult math problems.the conjunction after contributes a relation between the two clauses it connects:after(e2, e1), where e1:drink(john,wine) and e2:solve(john,hard problems)What nevertheless contributes to this example is a defeasible rule that we will againwrite informally asafter(X,E) ?
E:drink(Y,wine)) > ?X:solve(Y,hard problems))Normally, whatever one is able to do after one has had three glasses of wine, itis not solving difficult algebra problems.571Webber et al Anaphora and Discourse StructureAgain, we have abstracted over the individual, as the presupposed defeasible ruleassociated with the present-tense sentence appears to be more general than a statementabout a particular individual.15On the other hand, in the following example illustrating a presupposed defeasi-ble rule and a discourse relation associated with adjacency, it seems possible for thepresupposed defeasible rule to be about John himself:(57) John is discussing politics.
Nevertheless, he is thinking about his fish.Here the discourse relation between the two clauses, each of which denotes a specificevent, isduring(e2, e1), where e1:discuss(john,politics) and e2:think about(john,fish)(Note that our LF representation isn?t sufficiently rich to express the difference between(55) and (57).)
What nevertheless contributes here is the presupposed defeasible ruleduring(X,e1) > ?X = e2Normally what occurs during John?s discussing politics is not John?s thinkingabout his fish.Lagerwerf (1998) does not discuss how specific or general will be the presup-posed defeasible rule that is accommodated or what factors affect the choice.
Kruijff-Korbayova?
and Webber (2001a) also punt on the question, when considering the effectof information structure on what presupposed defeasible rule is associated with al-though.
Again, this seems to be a topic for future work.SummaryWe have indicated four ways in which we have found the relation associated with adiscourse adverbial to interact with a relation R triggered by adjacency or conveyedby structural connectives or, in some cases, by another relational anaphor:1. ?
separately serves as an argument to both R?
and R.2.
R?
(?, ei) is an argument of R.3.
R?
is parasitic on R.4.
R?
is a defeasible rule that incorporates R.We do not know whether this list is exhaustive or whether a discourse adverbialalways behaves the same way vis-a`-vis other relations.
Moreover, in the process ofsetting down the four cases we discuss, we have identified several problems that wehave not addressed, on which further work is needed.
Still, we hope that we haveconvinced the reader of our main thesis: that by recognizing discourse adverbialsas doing something different from simply signaling the discourse relation betweenadjacent discourse units and by considering their contribution as relations in their ownright, one can begin to characterize different ways in which anaphoric and structuralrelations may themselves interact.15 We speculate that the reason examples such as (55) and (56) sound more natural with the focus particleeven applied to the subordinate clause is that even conveys an even greater likelihood that thedefeasible rule holds, so nevertheless emphasizes its failure to do so.572Computational Linguistics Volume 29, Number 45.
Lexicalized Grammar for Discourse Syntax and SemanticsThe question we consider in this section is how the treatment we have presented ofdiscourse adverbials and structural connectives can be incorporated into a generalapproach to discourse interpretation.
There are three possible ways.The first possibility is simply to incorporate our treatment of adverbials and con-nectives into a sentence-level grammar, since such grammars already cover the syntaxof sentence-level conjunction (both coordinate and subordinate) and the syntax ofadverbials of all types.
The problem with this approach is that sentence-level gram-mars, whether phrasal or lexicalized, stop at explicit sentence-level conjunction and donot provide any mechanism for forming the meaning of multiclausal units that crosssentence-level punctuation.
Moreover, as we have already shown in section 3, theinterpretation of discourse adverbials can interact with the implicit relation betweenadjacent sentences, as well as with an explicitly signaled relation, so that a syntax andcompositional semantics that stops at the sentence will not provide all the structuresand associated semantics needed to build the structures and interpretations of interest.The second possibility is to have a completely different approach to discourse-level syntax and semantics than to sentence-level syntax and semantics, combining(for example) a definite clause grammar with rhetorical structure theory.
But as weand others have already noted, this requires discourse semantics reaching further andfurther into sentence-level syntax and semantics to handle relations between main andembedded clauses, and between embedded clauses themselves, as in example (58).
(58) If they?re drunk and they?re meant to be on parade and you go to theirroom and they?re lying in a pool of piss, then you lock them up for a day.
(The Independent, June 17, 1997)Thus it becomes harder and harder to distinguish the scope of discourse-level syntaxand semantics from that at the sentence-level.The third possibility is to recognize the overlapping scope and similar mechanismsand simply extend a sentence-level grammar and its associated semantic mechanismsto discourse.
The additional responsibilities of the grammer would be to account forthe formation of larger units of discourse from smaller units; the projection of theinterpretation of smaller discourse units onto the interpretation of the larger discourseunits they participate in; and the effect of discourse unit interpretation on the evolv-ing discourse model.
There are two styles of grammar one could use for this: (1) aphrase structure grammar (PSG) extended to discourse, as in Figure 6, or (2) a lexi-calized grammar that extends to discourse, a sentence-level lexicalized grammar suchas tree-adjoining grammar (Joshi, 1987; XTAG-Group 2001) or combinatory categorialgrammar (CCG) (Steedman 1996, 2000b).Whereas Polanyi and van den Berg (1996) extend a PSG to discourse, we arguefor extending a lexicalized grammar, even though TAG and CCG are weakly context-sensitive (CS) and the power needed for a discourse grammar with no crossing de-pendencies is only context-free (section 2.1).
Our argument is based on our desire touse a discourse grammar in natural language generation (NLG).
It is well-known thatcontext-free PSGs (CF PSGs) set up a complex search space for NLG.
A discoursegrammar specified in terms of phrase structure rules such as those shown in Figure 6doesn?t provide sufficient guidance when reversed for use in generating discourse.For example, one might end up having to guess randomly how many sentences andconnectives one had, in what order, before being able to fill in the sentences and con-nectives with any content.
More generally, trying to generate exactly a given semantics573Webber et al Anaphora and Discourse StructureSeg := SPunct Seg | Seg SPunct | SPunct |on the one hand Seg on the other hand Seg |not only Seg but also SegSPunct := S PunctuationPunctuation := .
| ; | : | ?
| !S := S Coord S | S Subord S | Subord S S | Sadv S |NP Sadv VP | S Sadv | .
.
.Coord := and | or | but | soSubord := although | after | because | before | ...Sadv := DAdv | SimpleAdvDAdv := instead | otherwise | for example | meanwhile | ...SimpleAdv := yesterday | today | surprisingly | hopefully | ...Figure 6PS rules for a discourse grammar.when semantics underspecifies syntactic dependency (as discourse semantics must, onour account) is known to be intractable (Koller and Striegnitz 2002).
An effective so-lution is to generate semantics and syntax simultaneously, which is straightforwardwith a lexicalized grammar (Stone et al 2001).Given the importance of various types of inference in discourse understanding,there is a second argument for using a lexicalized discourse grammar that derives fromthe role of implicature in discourse.
Gricean reasoning about implicatures requires ahearer be able to infer the meaningful alternatives that a speaker had in composing asentence.
With lexicalization, these alternatives can be given by a grammar, allowingthe hearer, for example, to ask sensible questions like ?Why did the speaker say ?in-stead?
here instead of nothing at all??
and draw implicatures from this.
A CF PSG, onthe other hand, might suggest questions like ?Why did the speaker say two sentencesrather than one here??
which seem empirically not to lead to any real implicatures.
(On the contrast between choices, which seem to lead to implicatures, and mere alter-native linguistic formulations, which do not seem to, see, for example, Dale and Reiter[1995] and Levison [2000].
)In several previous papers (Webber, Knott, and Joshi, 2001; Webber et al, 1999a,1999b), we described how our approach fits into the framework of tree-adjoining gram-mar.
This led to the initial version of a discourse parser (Forbes et al 2001) in whichthe same parser that builds trees for individual clauses using clause-level LTAG treesthen combines them using discourse-level LTAG trees.
Here we simply outline thegrammar, called DLTAG (section 5.1), then show how it supports the approach tostructural and anaphoric discourse connectives presented earlier (section 5.2).
(Of course, one still needs to account for how speakers realize their intentionsthrough text and how what is achieved through a single unit of text contributes towhat a speaker hopes to achieve through any larger unit in which it is embedded.Preliminary accounts are given in Grosz and Sidner [1990] and Moser and Moore[1996].
Given the complex relation between individual sentences and speaker inten-tions, however, it is unlikely that the relation between multisentence discourse andspeaker intentions can be modeled in a straightforward way similar to the basicallymonotonic compositional process that we have discussed in this article for discoursesemantics.
)574Computational Linguistics Volume 29, Number 4DcDcDcsubconj(a)DcDc Dcsubconj(b)?
:subconj_mid ?
: subconj_preFigure 7Initial trees for a subordinate conjunction: (a) postposed; (b) preposed.
Dc stands for discourseclause, ?
indicates a substitution site, and subconj stands for the particular subordinateconjunction that anchors the tree.5.1 DLTAG and Discourse SyntaxA lexicalized TAG begins with the notion of a lexical anchor, which can have oneor more associated tree structures.
For example, the verb likes anchors one tree corre-sponding to John likes apples, another corresponding to the topicalized Apples John likes,a third corresponding to the passive Apples are liked by John, and others as well.
Thatis, there is a tree for each minimal syntactic construction in which likes can appear, allsharing the same predicate-argument structure.
This syntactic/semantic encapsulationis possible because of the extended domain of locality of LTAG.A lexicalized TAG contains two kinds of elementary trees: initial trees that reflectbasic functor-argument dependencies and auxiliary trees that introduce recursion andallow elementary trees to be modified and/or elaborated.
Unlike the wide variety oftrees needed at the clause level, we have found that extending a lexicalized TAG todiscourse requires only a few elementary tree structures, possibly because clause-levelsyntax exploits structural variation in ways that discourse doesn?t.5.1.1 Initial Trees.
DLTAG has initial trees associated with subordinate conjunctions,with parallel constructions, and with some coordinate conjuctions.
We describe eachin turn.In the large LTAG developed by the XTAG project (XTAG-Group 2001) subordi-nate clauses are seen as adjuncts to sentences or verb phrases (i.e., as auxiliary trees)because they are outside the domain of locality of the verb.
In DLTAG, however, itis predicates on clausal arguments (such as coordinate and subordinate conjunctions)that define the domain of locality.
Thus, at this level, these predicates anchor initialtrees into which clauses substitute as arguments.
Figure 7 shows the initial trees for (a)postposed subordinate clauses and (b) preposed subordinate clauses.16 At both leavesand root is a discourse clause (Dc): a clause or a structure composed of discourseclauses.One reason for taking something to be an initial tree is that its local dependenciescan be stretched long distance.
At the sentence level, the dependency between applesand likes in Apples John likes is localized in all the trees for likes.
This dependency canbe stretched long distance, as in Apples, Bill thinks John may like.
In discourse, as wenoted in section 2, local dependencies can be stretched long distance as well, as in(59) a.
Although John is generous, he?s hard to find.16 Although in an earlier paper (Webber and Joshi 1998), we discuss reasons for taking the lexical anchorsof the initial trees in Figures 7 and 8 to be feature structures, following the analysis in Knott (1996) andKnott and Mellish (1996), here we just take them to be specific lexical items.575Webber et al Anaphora and Discourse StructureDcOn theone handOn theotherDc Dc?
:contrastFigure 8An initial tree for parallel constructions.
This particular tree is for a contrastive constructionanchored by on the one hand and on the other hand.b.
Although John is generous?for example, he gives money to anyonewho asks him for it?he?s hard to find.
(60) a.
On the one hand, John is generous.
On the other hand, he?s hard tofind.b.
On the one hand, John is generous.
For example, suppose you neededsome money: You?d only have to ask him for it.
On the other hand,he?s hard to find.Thus DLTAG also contains initial trees for parallel constructions as in (60).
Such aninitial tree is shown in Figure 8.
Like some initial trees in XTAG (XTAG-Group 2001),such trees can have a pair of anchors.
Since there are different ways in which dis-course units can be parallel, we assume a different initial tree for contrast (on the onehand.
.
.
on the other (hand).
.
.
), disjunction (either.
.
.
or.
.
.
), addition (not only.
.
.
but also.
.
.
),and concession (admittedly.
.
.
but.
.
.
).Finally, there are initial trees for structural connectives between adjacent sentencesor clauses that convey a particular relation between the connected units.
One clearexample is so, conveying result.
Its initial tree is shown in Figure 9.
We will have abetter sense of what other connectives to treat as structural as a result of annotationefforts of the sort described in Creswell et al (2002).175.1.2 Auxiliary Trees.
DLTAG uses auxiliary trees in two ways: (1) for discourse unitsthat continue a description in some way, and (2) for discourse adverbials.
Again wedescribe each in turn.17 For example, one might also have initial trees for marked uses of and and or that have a specificmeaning beyond simple conjunction or disjunction, as in(61) a.
Throw another spit ball and you?ll regret it.b.
Eat your spinach or you won?t get dessert.These differ from the more frequent, simple coordinate uses of and and or in that the second conjunctin these marked cases bears a discourse relation to the first conjunct (result in both (61a) and (61b)).With simple coordinate uses of and and or, all conjuncts (disjuncts) bear the same relation to the sameimmediately left-adjacent discourse unit.
For example, in (62), each conjunct is a separate explanationfor not trusting John, wheras in (63), each disjunct conveys an alternative result of John?s good fortune:(62) You shouldn?t trust John.
He never returns what he borrows, and he bad-mouths his associatesbehind their backs.
(63) John just won the lottery.
So he will quit his job, or he will at least stop working overtime.For simple coordinate uses of and and or, we have auxiliary trees (section 5.1.2).576Computational Linguistics Volume 29, Number 4DcDcDc?
:sosoFigure 9Initial tree for coordinate conjunction so.DcDc Dc?.DcDc Dc?
and ?SSthen(a) (b) (c)?
: punct1 ?
: and ?
: thenFigure 10Auxiliary trees for basic elaboration.
These particular trees are anchored by (a) thepunctuation mark ?period?
and (b) and.
The symbol ?
indicates the foot node of the auxiliarytree, which has the same label as its root.
(c) Auxiliary tree for the discourse adverbial then.?
: punct1?
: punct13?2*.T1T2T1 T2.
?10Figure 11TAG derivation of example (64).First, auxiliary trees anchored by punctuation (e.g., period, comma, semicolon.)
(Fig-ure 10a) or by simple coordination (Figure 10b) are used to provide further descriptionof a situation or of one or more entities (objects, events, situations, states, etc.)
withinthe situation.18 The additional information is conveyed by the discourse clause that fillsits substitution site.
Such auxiliary trees are used in the derivation of simple discoursessuch as(64) a. John went to the zoo.b.
He took his cell phone with him.Figure 11 shows the DLTAG derivation of example (64), starting from LTAG deriva-tions of the individual sentences.19 To the left of the horizontal arrow are the elemen-tary trees to be combined: T1 stands for the LTAG tree for clause (64a), T2 for clause18 The latter use of an auxiliary tree is related to dominant topic chaining in Scha and Polanyi (1988) andentity chains in Knott et al (2001).19 We comment on left-to-right incremental construction of DLTAG structures in parallel withsentence-level LTAG structures at the end of Section 5.2.577Webber et al Anaphora and Discourse Structure(64b), and ?
:punct1 for the auxiliary tree assocated with the period after (64a).
In thederivation, the foot node of ?
:punct1 is adjoined to the root of T1 and its substitutionsite filled by T2, resulting in the tree to the right of the horizontal arrow.
(A standardway of indicating TAG derivations is shown under the horizontal arrow, where bro-ken lines indicate adjunction and solid lines, substitution.
Each line is labeled with theaddress of the argument at which the operation occurs.
?1 is the derivation tree forT1 and ?2, the derivation tree for T2.
)The other auxiliary trees used in the lexicalized discourse grammar are those fordiscourse adverbials, which are simply auxiliary trees in a sentence-level LTAG (XTAG-Group 2001), but with an interpretation that projects up to the discourse level.
Anexample is shown in Figure 10c.
Adjoining such an adverbial to a clausal/sententialstructure contributes to how information conveyed by that structure relates to theprevious discourse.There is some lexical ambiguity in this grammar, but no more than serious con-sideration of adverbials and conjunctions demands.
First, as already noted, discourseadverbials have other uses that may not be anaphoric (65a?b) and may not be clausal(65a?c):(65) a. John ate an apple instead of a pear.b.
In contrast with Sue, Fred was tired.c.
Mary was otherwise occupied.Second, many of the adverbials found in second position in parallel constructions(e.g., on the other hand, at the same time, nevertheless) can also serve as simple adverbialdiscourse connectives on their own.
In the first case, they will be one of the two anchorsof an initial tree (Figure 8), and in the second, they will anchor a simple auxiliary tree(Figure 10c).
These lexical ambiguities correlate with structural ambiguity.5.2 Example DerivationsIt should be clear by now that our approach aims to explain discourse semantics interms of a product of the same three interpretive mechanisms that operate withinclause-level semantics:?
compositional rules on syntactic structure (here, discourse structure)?
anaphor resolution?
inference triggered by adjacency and structural connectionFor the compositional part of semantics in DLTAG (in particular, computing interpre-tations on derivation trees), we follow Joshi and Vijay-Shanker (2001).
Roughly, theycompute interpretations on the derivation tree using a bottom-up procedure.
At eachlevel, function application is used to assemble the interpretation of the tree from theinterpretation of its root node and its subtrees.
Where multiple subtrees have functiontypes, the interpretation procedure is potentially nondeterministic: The resulting am-biguities in interpretation may be admitted as genuine, or they may be eliminated by alexical specification.
Multicomponent TAG tree sets are used to provide an appropriatecompositional treatment for quantifiers, which we borrow for interpreting for example(examples (66c?d)).In showing how DLTAG and an interpretative process on its derivations operate,we must, of necessity, gloss over how inference triggered by adjacency or associatedwith a structural connective provides the intended relation between adjacent discourse578Computational Linguistics Volume 29, Number 4units: It may be a matter simply of statistical inference, as in Marcu and Echihabi(2002), or of more complex inference, as in Hobbs et al (1993).
As we noted, our viewis that there are three mechanisms at work in discourse semantics, just as there are inclause-level semantics: Inference isn?t the only process involved.
Thus the focus of ourpresentation here is on how compositional rules and anaphor resolution (which itselfoften appears to require inference) operate together with inference to yield discoursesemantics.We start with previous examples (44) (here (66c)) and (47) (here (66d)) and twosomewhat simpler variants (66a?b):(66) a.
You shouldn?t trust John because he never returns what he borrows.b.
You shouldn?t trust John.
He never returns what he borrows.c.
You shouldn?t trust John because, for example, he never returns whathe borrows.d.
You shouldn?t trust John.
For example, he never returns what he bor-rows.This allows us to show how (66a?b) and (66c?d) receive similar interpretations, despitehaving somewhat different derivations, and how the discourse adverbial for examplecontributes both syntactically and semantically to those interpretations.We let T1 stand for the LTAG parse tree for you shouldn?t trust John, ?1, for itsderivation tree, and interp(T1), for the eventuality associated with its interpretation.Similarly, we let T2 stand for the LTAG parse tree for he never returns what he bor-rows, ?2, for its derivation tree, and interp(T2), for the eventuality associated with itsinterpretation.Example (66a) involves an initial tree (?
:because-mid) anchored by because (Fig-ure 12).
Its derived tree comes from T1 substituting at the left-hand substitutionsite of ?
:because-mid (index 1) and T2 at its right-hand substitution site (index 3).Compositional interpretation of the resulting derivation tree yields explanation(interp(T2),interp(T1)).
(A more precise interpretation would distinguish between the directand epistemic causality senses of because, but the derivation would proceed in thesame way.
)In contrast with (66a), example (66b) employs an auxiliary tree (?
:punct1) anchoredby a period (Figure 13).
Its derived tree comes from T2 substituting at the right-handsubstitution site (index 3) of ?
:punct1, and ?
:punct1 adjoining at the root of T1 (index 0).Compositional interpretation of the derivation tree yields merely that T2 continues thedescription of the situation associated with T1, that is, elaboration(interp(T2),interp(T1)).Further inference triggered by adjacency and structural connection leads to a con-?:because_mid?
:because_mid31T2T1?1 ?2becausebecauseT1 T2Figure 12Derivation of example (66a).
The derivation tree is shown below the arrow, and the derivedtree, to its right.
(Node labels Dc have been omitted for simplicity.
)579Webber et al Anaphora and Discourse Structure?
: punct1?
: punct1*T2 .
.T1T1 T23?2?10Figure 13Derivation of example (66b).DD  *cD{ cc}for-ex2?
: for-ex103?:?10because_mid?2?:1T2T1?
: for-ex1because_mid?
:T1T2 because?
: for-ex2becausefor example?for exampleFigure 14Derivation of example (66c).clusion of causality between them, that is, explanation(interp(T2),interp(T1)), but thisconclusion is defeasible because it can be denied without a contradiction: for example,(67) You shouldn?t trust John.
He never returns what he borrows.
But that?snot why you shouldn?t trust him.Example (66c) differs from (66a) in containing for example in its second clause.
Asnoted earlier, for example resembles a quantifier with respect to its semantics, as itsinterpretation takes wider scope than would be explained by its syntactic position.
Wehandle this in the same way that quantifiers are handled in Joshi and Vijay-Shanker(2001) by associating with for example a two-element TAG tree set (Figure 14).
Bothtrees in the tree set participate in the derivation: The auxiliary tree ?
:for ex1 adjoinsat the root of T2, whereas the auxiliary tree ?
:for ex2 adjoins at the root of the higherdiscourse unit.
Since we saw from example (66a) that the interpretation of this higherdiscourse unit is explanation(interp(T2),interp(T1)), the interpretation associated withthe adjoined ?
:for ex2 node both embeds and abstracts this interpretation, yieldingexemplification(interp(T2), ?X .
explanation(X,interp(T1))That is, John?s never returning what he borrows is one instance of a set of explanations.Similarly, example (66d) differs from (66b) in containing for example in its secondsentence (Figure 15).
As in example (66b), an inferred relation is triggered betweenthe interpretations of T2 and T1, namely, explanation(interp(T2),interp(T1)).
Then, asa result of ?
:for ex1 adjoining at T2 and ?
:for ex2 adjoining at the root of the higher580Computational Linguistics Volume 29, Number 4DD* ..D* }{ for-ex1?
: for-ex20punct1?230?:?1?:0T1T2?
: for-ex2?
:punct1for exampleT2T1for example ?for-ex1?
:Figure 15Derivation of example (66d).discourse unit, for example again contributes the interpretationexemplification(interp(T2), ?X .
explanation(X,interp(T1))Thus (66c) and (66d) differ only in the derivation of the interpretation that for examplethen abstracts over.The next example we will walk through is example (11) (repeated here as exam-ple (68)):(68) John loves Barolo.
So he ordered three cases of the ?97.
But he had tocancel the order because then he discovered he was broke.As shown in Figure 16, this example involves two initial trees (?
:so, ?
:because mid) forthe structural connectives so and because; an auxiliary tree for the structural connectivebut (?
:but), since but functions as a simple conjunction to continue the description ofthe situation under discussion; an auxiliary tree (?
:then) for the discourse adverbialthen; and initial trees for the four individual clauses T1?T4.
As can be seen from thederivation tree, T1 and T2 substitute into ?
:so as its first and third arguments, and ?
:butroot-adjoins to the result.
The substitution argument of ?
:but is filled by ?
:because mid,with T3 and T4 substituted in as its first and third arguments, and ?
:then is root-adjoined to T4.
The interpretation contributed by then, after its anaphoric argument isresolved to interp(T2), is?4: after(interp(T4), interp(T2))The interpretations derived compositionally from the structural connectives so, because,and but are?1: result(interp(T2), interp(T1))?2: explanation(interp(T4), interp(S3))?3: elaboration(?2,?1)Further inference may then refine elaboration to contrast, based on how but is beingused.Finally, we want to point out one more way in which texts that seem to be closeparaphrases get their interpretations in different ways.
Consider the two texts in ex-ample (69):581Webber et al Anaphora and Discourse Structure*so?:then*thenbecause?
:because_mid?3T1 ?1?4T3T2T4?
: but?
: so?
: but?:because_mid?
:?231 30?3 ?4then?1 ?231 0becausethenT3T4T2soT1but?
:butsoFigure 16Derivation of example (68).
(69) a.
You should eliminate part 2 before part 3 because part 2 is more sus-ceptible to damage.b.
You should eliminate part 2 before part 3.
This is because part 2 ismore susceptible to damage.Example (69b) is a simpler version of an example in Moser and Moore (1995), in whichThis is because is treated as an unanalyzed cue phrase, no different from because in (69a).We show here that this isn?t necessary: One can analyze (69b) using compositionalsemantics and anaphor resolution and achieve the same results.First consider (69a).
Given the interpretations of its two component clauses, itsoverall interpretation follows in the same way as (66a), shown in Figure 12.
Nowconsider (69b) and the derivation shown in Figure 17.
Here the initial tree ?
:because midT1T2TB?:because_midbecause?
: punct1*..becauseT2 TB?
: punct1?
:because_mid31?2 ??
?103T1Figure 17Derivation of example (69b).582Computational Linguistics Volume 29, Number 4has its two arguments filled by T2, the TAG analysis of this is and TB, the TAG analysisof part 2 is more susceptible to damage.
The overall derived tree for (69b) comes from?
:punct1 root-adjoining to T1 (the TAG analysis of You should eliminate part 2 beforepart 3), with the subsitution site of ?
:punct1 filled by the ?
:because mid derivation.The compositional interpretation of the derivation tree yields the interpretation of the?
:because mid tree (i1) as an elaboration of the interpretation of T1:i1: explanation(interp(TB),interp(T2))i2: elaboration(i1,interp(T1))But this is not all.
The pronoun this in T2 is resolved anaphorically to the nearest con-sistent eventuality (Eckert and Strube 2000; Byron 2002) which in this case is interp(T1).Taking this as the interpretation of T2 and substituting, we geti1: explanation(interp(TB),interp(T1))i2: elaboration(i1,interp(T1))Notice that i1 is also the interpretation of (69a).
To this, i2 adds the somewhat redun-dant information that i1 serves to elaborate the advice in T1.
Thus (69a) and (69b)receive similar interpretations but by different means.
This treatment has the addedadvantage that one does not have to treat This is not because as a separate cue phrase.Rather, negation simply producesi1: ?explanation(interp(TB),interp(T1))i2: elaboration(i1,interp(T1))That is, T1 is elaborated by a denial of a (possible) explanation.
Presumably, the textwould go on to provide the actual explanation.Finally, we want to comment on the Holy Grail of discourse parsing: a realisticmodel that is computed in parallel with incremental sentence-level parsing.
Neitherthe analyses given in this section nor the discourse parsing described in Forbes etal.
(2001) is done in a left-to-right incremental fashion, in parallel with incrementalleft-to-right sentence-level parsing.What would an integrated incremental method of sentence-discourse processingrequire?
At minimum, we believe it would involve:?
A left-to-right parser that would simultaneously compute increments tosentence-level syntactic structure, sentence-level semantics,discourse-level syntactic structure, and discourse-level semantics.Increments to the latter two would occur only at clause boundaries andwith discourse adverbials and structural connectives.?
An incremental anaphor resolution mechanism, similar to that in Strube(1998), but extended both to deictic pronouns, as in Eckert and Strube(2000) and Byron (2002) and to the anaphoric argument of discourseadverbials.?
Incremental computation of discourse structure in terms of elaborationrelations and further nondefeasible reasoning to more specific relations,where possible.A left-to-right parser that simultaneously produces sentence-level syntactic andsemantic analyses already exists for combinatory categorial grammar (Steedman 1996,583Webber et al Anaphora and Discourse Structure2000b; Hockenmaier, Bierner, and Baldridge, forthcoming), and it would seem straight-forward to extend such a parser to computing discourse-level syntax and semanticsas well.
Similarly, it seems straightforward to produce an incremental version of anyof the current generation of anaphor resolution mechanisms, extended to deictic pro-nouns, although current approaches attempt to resolve this and that only with theinterpretation of a single clause, not with that of any larger discourse unit.
As theseapproaches are also not very accurate as yet, incremental anaphor resolution awaitsimprovements to anaphor resolution in general.
Moreover, as we better understandthe specific anaphoric properties of discourse adverbials through empirical analysissuch as Creswell et al (2000), such anaphor resolution mechanisms can be extendedto include them as well.As for building discourse structure incrementally in parallel with syntactic struc-ture, there is no working prototype yet that will do what is needed.
But we have nodoubt that as psycholinguistics and computation together develop a better understand-ing of incremental semantic processing, researchers?
desire for a working prototypewill eventually result in the development of one.6.
ConclusionIn this article, we have argued that discourse adverbials make an anaphoric, rather thana structural, connection with the previous discourse (section 2), and we have provideda general view of anaphora in which it makes sense to talk of discourse adverbials asbeing anaphoric (section 3).
We have then shown that this view of discourse adverbialsallows us to characterize a range of ways in which the relation contributed by adiscourse adverbial can interact with the relation conveyed by a structural connectiveor inferred through adjacency (section 4), and then illustrated how discourse syntaxand semantics can be treated as an extension of sentence-level syntax and semantics,using a lexicalized discourse grammar (section 5).We are clearly not the first to have proposed a grammatical treatment of low-levelaspects of discourse semantics (Asher and Lascarides 1999; Gardent 1997; Polanyi andvan den Berg 1996; Scha and Polanyi 1988; Schilder 1997a, 1997b; van den Berg 1996),but we are the first to have recognized that the key to avoiding problems of maintain-ing a compositional semantics for discourse lies in recognizing discourse adverbials asanaphors and not trying to shoehorn everything into a single class of discourse connec-tives.
Although we are not yet able to propose a solution to the problem of correctlyresolving discourse adverbials or a way of achieving the Holy Grail of computingdiscourse syntax and semantics in parallel with incremental sentence processing, theproposed approach does simplify issues of discourse structure and discourse semanticsin ways that have not before been possible.AcknowledgmentsThe authors would like to thank KateForbes, Katja Markert, Natalia Modjeska,Rashmi Prasad, Eleni Miltsakaki, CassandraCreswell, Mark Steedman, members of theUniversity of Edinburgh Dialogue SystemsGroup, and participants at ESSLLI?01 forhelpful criticism as the ideas in the articlewere being developed.
We would also liketo thank our three anonymous reviewers.We believe that in addressing theircriticisms and suggestions, both the article?sarguments and its presentation havebecome clearer.
This work has been fundedin part by EPSRC grant GR/M75129(Webber), NSF grant CISE CDA 9818322(Stone), and NSF grants NSF-STC SBR8920230 and NSF-EIA02-24417 (Joshi).ReferencesAsher, Nicholas.
1993.
Reference to AbstractObjects in Discourse.
Kluwer, Boston.Asher, Nicholas and Alex Lascarides.
1999.The semantics and pragmatics of584Computational Linguistics Volume 29, Number 4presupposition.
Journal of Semantics,15(3):239?300.Asher, Nicholas and Alex Lascarides.
2003.Logics of Conversation.
CambridgeUniversity Press, Cambridge, England.Asher, Nicholas and Michael Morreau.
1991.Commonsense entailment.
In Proceedingsof the Ninth International Joint Conference onArtificial Intelligence IJCAI?91, pages387?392, Sydney, Australia.Barwise, Jon and Robin Cooper.
1981.Generalized quantifiers and naturallanguage.
Linguistics and Philosophy,4:159?219.Bateman, John.
1999.
The dynamics of?surfacing?
: An initial exploration.
InProceedings of International Workshop onLevels of Representation in Discourse(LORID?99), pages 127?133, Edinburgh.Bierner, Gann.
2001a.
Alternative phrasesand natural language informationretrieval.
In Proceedings of the 39th AnnualConference of the Association forComputational Linguistics, Toulouse,France, July.Bierner, Gann.
2001b.
Alternative Phrases:Theoretical Analysis and Practical Application.Ph.D.
thesis, University of Edinburgh.Bierner, Gann and Bonnie Webber.
2000.Inference through alternative setsemantics.
Journal of Language andComputation, 1(2):259?274.Byron, Donna.
2002.
Resolving pronominalreference to abstract entities.
InProceedings of the 40th Annual Meeting,Association for Computational Linguistics,pages 80?87, University of Pennsylvania.Clark, Herbert.
1975.
Bridging.
InProceedings of Theoretical Issues in NaturalLanguage Processing (TINLAP-1), pages169?174, Cambridge, MA.Clark, Herbert and Catherine Marshall.1981.
Definite reference and mutualknowledge.
In Aravind Joshi, BonnieWebber, and Ivan Sag, editors, Elements ofDiscourse Understanding.
CambridgeUniversity Press, Cambridge, England,pages 10?63.Cosse, Michel.
1996.
Indefinite associativeanaphora in French.
In Proceedings of theIndiAna Workshop on Indirect Anaphora,University of Lancaster, Lancaster,England.Creswell, Cassandre, Kate Forbes, EleniMiltsakaki, Rashmi Prasad, Aravind Joshi,and Bonnie Webber.
2002.
The discourseanaphoric properties of connectives.
InProceedings of the Discourse Anaphora andAnaphor Resolution Colloquium, Lisbon,Portugal.Dale, Robert.
1992.
Generating ReferringExpressions.
MIT Press, Cambridge, MA.Dale, Robert and Ehud Reiter.
1995.Computational interpretations of theGricean maxims in the generation ofreferring expressions.
Cognitive Science,18:233?263.Eckert, Miriam and Michael Strube.
2000.Synchronising units and anaphoraresolution.
Journal of Semantics, 17:51?89.Forbes, Katherine, Eleni Miltsakaki, RashmiPrasad, Anoop Sarkar, Aravind Joshi, andBonnie Webber.
2001.
D-LTAG system:Discourse parsing with a lexicalizedtree-adjoining grammar.
In ESSLLI?2001Workshop on Information Structure, DiscourseStructure and Discourse Semantics, Helsinki,Finland.Forbes, Kate and Bonnie Webber.
2002.
Asemantic account of adverbials asdiscourse connectives.
In Proceedings ofThird SIGDial Workshop, pages 27?36,Philadelphia, PA.Frank, Anette and Hans Kamp.
1997.
Oncontext dependence in modalconstructions.
In SALT-97, Stanford, CA.Gardent, Claire.
1997.
Discourse treeadjoining grammars.
Claus Report no.
89,University of the Saarland, Saarbru?cken,Germany.Grosz, Barbara and Candace Sidner.
1990.Plans for discourse.
In Philip Cohen, JerryMorgan, and Martha Pollack, editors,Intentions in Communication.
MIT Press,Cambridge, MA, pages 417?444.Hahn, Udo, Katja Markert, and MichaelStrube.
1996.
A conceptual reasoningapproach to textual ellipsis.
In Proceedingsof the 12th European Conference on ArtificialIntelligence, pages 572?576, Budapest,Hungary.Hardt, Dan.
1992.
VP ellipsis and contextualinterpretation.
In Proceedings ofInternational Conference on ComputationalLinguistics(COLING-92), pages 303?309,Nantes.Hardt, Dan.
1999.
Dynamic interpretation ofverb phrase ellipsis.
Linguistics andPhilosophy, 22:187?221.Hellman, Christina and Kari Fraurud.
1996.Proceedings of the IndiAna Workshop onIndirect Anaphora.
University of Lancaster,Lancaster, England.Hobbs, Jerry.
1985.
Ontological promiscuity.In Proceedings of the 23rd Annual Meeting ofthe Association for Computational Linguistics,pages 61?69, Palo Alto, CA.
MorganKaufmann.Hobbs, Jerry.
1990.
Literature and Cognition.Volume 21 of CSLI Lecture Notes.
Centerfor the Study of Language andInformation, Stanford, CA.585Webber et al Anaphora and Discourse StructureHobbs, Jerry, Mark Stickel, Paul Martin, andDouglas Edwards.
1993.
Interpretation asabduction.
Artificial Intelligence,63(1?2):69?142.Hockenmaier, Julia, Gann Bierner, and JasonBaldridge.
Forthcoming.
Providingrobustness for a CCG system.
Journal ofLanguage and Computation.Isard, Stephen.
1975.
Changing the context.In Edward Keenan, editor, FormalSemantics of Natural Language.
CambridgeUniversity Press, Cambridge, England,pages 287?296.Jayez, Jacques and Corinne Rossari.
1998a.Pragmatic connectives as predicates.
InPatrick Saint-Dizier, editor, PredicativeStructures in Natural Language and LexicalKnowledge Bases.
Kluwer Academic,Dordrecht, the Netherlands, pages306?340.Jayez, Jacques and Corinne Rossari.
1998b.The semantics of pragmatic connectivesin TAG: The French donc example.
InAnne Abeille?
and Owen Rambow, editors,Proceedings of the TAG+4 Conference.
CSLIPublications, Stanford, CA.Joshi, Aravind.
1987.
An introduction to treeadjoining grammar.
In AlexisManaster-Ramer, editor, Mathematics ofLanguage.
John Benjamins, Amsterdam,pages 87?114.Joshi, Aravind and K. Vijay-Shanker.
2001.Compositional semantics with lexicalizedtree-adjoining grammar (LTAG): Howmuch underspecification is necessary?
InHarry Bunt, Reinhard Muskens, and EliasThijsse, editors, Computing Meaning,Volume 2, Kluwer, Dordrecht, theNetherlands, pages 147?163.Jurafsky, Dan and James Martin.
2000.Speech and Language Processing.Prentice-Hall, Englewood Cliffs, NJ.Kamp, Hans and Uwe Reyle.
1993.
FromDiscourse to Logic.
Kluwer, Dordrecht, theNetherlands.Kehler, Andrew.
2002.
Coherence, Referenceand the Theory of Grammar.
CSLIPublications, Stanford, CA.Kibble, Rodger.
1995.
Modalinsubordination.
In Empirical Issues inFormal Syntax and Semantics, Selected Papersfrom the Colloque de Syntaxe et de Se?mantiquede Paris, pages 317?332.Knott, Alistair.
1996.
A Data-DrivenMethodology for Motivating a Set of CoherenceRelations.
Ph.D. thesis, Department ofArtificial Intelligence, University ofEdinburgh.Knott, Alistair and Chris Mellish.
1996.
Afeature-based account of the relationssignalled by sentence and clauseconnectives.
Language and Speech,39(2?3):143?183.Knott, Alistair, Jon Oberlander, MickO?Donnell, and Chris Mellish.
2001.Beyond elaboration: The interaction ofrelations and focus in coherent text.
InT.
Sanders, J. Schilperoord, andW.
Spooren, editors, Text Representation:Linguistic and Psycholinguistic Aspects.
JohnBenjamins, Amsterdam, pages 181?196.Koller, Alexander and Kristina Striegnitz.2002.
Generation as dependency parsing.In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics,pages 17?24, Philadelphia, PA.Kruijff-Korbayova?, Ivana and BonnieWebber.
2001a.
Concession, implicatureand alternative sets.
In Fourth InternationalWorkshop on Computational Semantics,Tilburg, the Netherlands.Kruijff-Korbayova?, Ivana and BonnieWebber.
2001b.
Information structure andthe semantics of ?otherwise.?
InESSLLI?2001 Workshop on InformationStructure, Discourse Structure and DiscourseSemantics, pages 61?78, Helsinki, Finland.Lagerwerf, Luuk.
1998.
Causal ConnectivesHave Presuppositions.
Holland AcademicGraphics, The Hague, the Netherlands.Levinson, Stephen.
2000.
PresumptiveMeanings: The Theory of GeneralizedConversational Implicature.
MIT Press,Cambridge, MA.Luperfoy, Susann.
1992.
The representationof multimodal user interface dialoguesusing discourse pegs.
In Proceedings of the30th Annual Meeting of the Association forComputational Linguistics (ACL), pages22?31, University of Delaware, Newark.Mann, William and Sandra Thompson.1988.
Rhetorical structure theory: Towarda functional theory of text organization.Text, 8(3):243?281.Marcu, Daniel.
1999.
Instructions formanually annotating the discoursestructure of texts.
Available fromhttp://www.isi.edu/?marcu.Marcu, Daniel and Abdessamad Echihabi.2002.
An unsupervised approach torecognizing discourse relations.
InProceedings of the 40th Annual Meeting,Association for Computational Linguistics,pages 368?375, University ofPennsylvania, Philadelphia.Modjeska, Natalia Nygren.
2001.
Towards aresolution of comparative anaphora: Acorpus study of ??other.?
In PAPACOL,Italy.Modjeska, Natalia Nygren.
2002.
Lexicaland grammatical role constraints inresolving other-anaphora.
In Proceedings of586Computational Linguistics Volume 29, Number 4the Discourse Anaphora and AnaphorResolution Colloquium, Lisbon, Portugal.Moens, Marc and Mark Steedman.
1988.Temporal ontology and temporalreference.
Computational Linguistics,14(1):15?28.Moore, Johanna and Martha Pollack.
1992.A problem for RST: The need formulti-level discouse analysis.Computational Linguistics, 18(4):537?544.Moser, Megan and Johanna Moore.
1995.Investigating cue selection and placementin tutorial discourse.
In Proceedings of the33rd Annual Meeting, Association forComputational Linguistics, pages 130?135,MIT, Cambridge, MA.Moser, Megan and Johanna Moore.
1996.Toward a synthesis of two accounts ofdiscourse structure.
ComputationalLinguistics, 22(3):409?419.Not, Elena, Lucia Tovena, and MassimoZancanaro.
1999.
Positing and resolvingbridging anaphora in deverbal NPs.
InACL?99 Workshop on the Relationship betweenDiscourse/Dialogue Structure and Reference,College Park, MD.Partee, Barbara.
1984.
Nominal andtemporal anaphora.
Linguistics andPhilosophy, 7(3):287?324.Polanyi, Livia and Martin H. van den Berg.1996.
Discourse structure and discourseinterpretation.
In P. Dekker andM.
Stokhof, editors, Proceedings of the TenthAmsterdam Colloquium, pages 113?131,University of Amsterdam.Prince, Ellen.
1992.
The ZPG letter: Subjects,definiteness and information-status.
InSusan Thompson and William Mann,editors, Discourse Description: DiverseAnalyses of a Fundraising Text.
JohnBenjamins, Amsterdam, pages 295?325.Quirk, Randolph, Sidney Greenbaum,Geoffrey Leech, and Jan Svartik.
1972.
AGrammar of Contemporary English.Longman, Harlow, England.Scha, Remko and Livia Polanyi.
1988.
Anaugmented context free grammar fordiscourse.
In Proceedings of the 12thInternational Conference on ComputationalLinguistics (COLING?88), pages 573?577,Budapest, Hungary, August.Schilder, Frank.
1997a.
Towards a theory ofdiscourse processing: Flashback sequencesdescribed by D-trees.
In Proceedings of theFormal Grammar Conference (ESSLLI?97),Aix-en-Provence, France, August.Schilder, Frank.
1997b.
Tree discoursegrammar, or how to get attached to adiscourse.
In Proceedings of the SecondInternational Workshop on ComputationalSemantics, Tilburg, the Netherlands,January.Steedman, Mark.
1996.
Surface Structure andInterpretation.
Volume 30 of LinguisticInquiry Monograph, 5, MIT Press,Cambridge, MA.Steedman, Mark.
2000a.
Informationstructure and the syntax-phonologyinterface.
Linguistic Inquiry, 34:649?689.Steedman, Mark.
2000b.
The SyntacticProcess.
MIT Press, Cambridge, MA.Stokhof, Martin and Jeroen Groenendijk.1999.
Dynamic semantics.
In RobertWilson and Frank Keil, editors, MITEncyclopedia of Cognitive Science.
MIT Press.Cambridge, MA, pages 247?249Stone, Matthew, Christine Doran, BonnieWebber, Tonia Bleam, and Martha Palmer.2001.
Microplanning from communicativeintentions: Sentence planning usingdescriptions (SPUD).
Technical Report no.RUCCS TR68, Department of CognitiveScience, Rutgers University, NewBrunswick, NJ.Stone, Matthew and Daniel Hardt.
1999.Dynamic discourse referents for tense andmodals.
In Harry Bunt, editor,Computational Semantics.
Kluwer,Dordrecht, the Netherlands, pages287?299.Strube, Michael.
1998.
Never look back: Analternative to centering.
In Proceedings,COLING/ACL?98, pages 1251?1257,Montreal, Quebec, Canada.Traugott, Elizabeth.
1995.
The role of thedevelopment of discourse markers in atheory of grammaticalization.
Paperpresented at ICHL XII, Manchester,England.
Revised version of (1997)available at http://www.stanford.edu/traugott/ect-papersonline.html.Traugott, Elizabeth.
1997.
The discourseconnective after all: A historicalpragmatic account.
Paper presented atICL, Paris.
Available at http://www.stanford.edu/traugott/ect-papersonline.html.van den Berg, Martin H. 1996.
Discoursegrammar and dynamic logic.
In P. Dekkerand M. Stokhof, editors, Proceedings of theTenth Amsterdam Colloquium, pages 93?111,ILLC/Department of Philosophy,University of Amsterdam.van Eijck, Jan and Hans Kamp.
1997.Representing discourse in context.
In Janvan Benthem and Alice ter Meulen,editors, Handbook of Logic and Language.Elsevier Science B.V., Amsterdam, pages181?237.Venditti, Jennifer J., Matthew Stone,Preetham Nanda, and Paul Tepper.
2002.Discourse constraints on the587Webber et al Anaphora and Discourse Structureinterpretation of nuclear-accentedpronouns.
In Proceedings of Symposium onSpeech Prosody, Aix-en-Provence, France.Available at http://www.lpl.univ-aix.fr/sp2002/papers.htm.Vendler, Zeno.
1967.
Linguistics in Philosophy.Cornell University Press, Ithaca, NY.Webber, Bonnie.
1988.
Tense as discourseanaphor.
Computational Linguistics,14(2):61?73.Webber, Bonnie.
1991.
Structure andostension in the interpretation ofdiscourse deixis.
Language and CognitiveProcesses, 6(2):107?135.Webber, Bonnie and Breck Baldwin.
1992.Accommodating context change.
InProceedings of the 30th Annual Meeting of theAssociation for Computational Linguistics(ACL), pages 96?103, University ofDelaware, Newark.Webber, Bonnie and Aravind Joshi.
1998.Anchoring a lexicalized tree-adjoininggrammar for discourse.
In COLING/ACLWorkshop on Discourse Relations andDiscourse Markers, pages 86?92, Montreal,Quebec, Canada.Webber, Bonnie, Alistair Knott, and AravindJoshi.
2001.
Multiple discourseconnectives in a lexicalized grammar fordiscourse.
In Harry Bunt, ReinhardMuskens, and Elias Thijsse, editors,Computing Meaning, volume 2.
Kluwer,Dordrecht, the Netherlands, pages229?249.Webber, Bonnie, Alistair Knott, MatthewStone, and Aravind Joshi.
1999a.Discourse relations: A structural andpresuppositional account using lexicalisedTAG.
In Proceedings of the 36th AnnualMeeting of the Association for ComputationalLinguistics, pages 41?48, College Park,MD.Webber, Bonnie, Alistair Knott, MatthewStone, and Aravind Joshi.
1999b.
Whatare little trees made of: A structural andpresuppositional account using lexicalisedTAG.
In Proceedings of InternationalWorkshop on Levels of Representation inDiscourse (LORID?99), pages 151?156,Edinburgh.Wiebe, Janyce.
1993.
Issues in linguisticsegmentation.
In Workshop on Intentionalityand Structure in Discourse Relations,Association for Computational Linguistics,pages 148?151, Ohio State University.Woods, William.
1978.
Semantics andquantification in natural languagequestion answering.
In Marshall C. Yovits,editor, Advances in Computers, volume 17.Academic Press, New York, pages 1?87.XTAG-Group.
2001.
A lexicalized treeadjoining grammar for English.
TechnicalReport no.
IRCS 01-03, Universityof Pennsylvania, Philadelphia.
Available atftp://ftp.cis.upenn.edu/pub/ircs/technical-reports/01-03.
