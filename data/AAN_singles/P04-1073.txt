Question Answering using Constraint Satisfaction:QA-by-Dossier-with-ConstraintsJohn PragerT.J.
Watson Research Ctr.Yorktown HeightsN.Y.
10598jprager@us.ibm.comJennifer Chu-CarrollT.J.
Watson Research Ctr.Yorktown HeightsN.Y.
10598jencc@us.ibm.comKrzysztof CzubaT.J.
Watson Research Ctr.Yorktown HeightsN.Y.
10598kczuba@us.ibm.comAbstractQA-by-Dossier-with-Constraints is a new ap-proach to Question Answering whereby candi-date answers?
confidences are adjusted byasking auxiliary questions whose answers con-strain the original answers.
These constraintsemerge naturally from the domain of interest,and enable application of real-world knowledgeto QA.
We show that our approach signifi-cantly improves system performance (75% rela-tive improvement in F-measure on selectquestion types) and can create a ?dossier?
of in-formation about the subject matter in the origi-nal question.1 IntroductionTraditionally, Question Answering (QA) hasdrawn on the fields of Information Retrieval, NaturalLanguage Processing (NLP), Ontologies, Data Basesand Logical Inference, although it is at heart a prob-lem of NLP.
These fields have been used to supplythe technology with which QA components havebeen built.
We present here a new methodologywhich attempts to use QA holistically, along withconstraint satisfaction, to better answer questions,without requiring any advances in the underlyingfields.Because NLP is still very much an error-proneprocess, QA systems make many mistakes; accord-ingly, a variety of methods have been developed toboost the accuracy of their answers.
Such methodsinclude redundancy (getting the same answer frommultiple documents, sources, or algorithms), deepparsing of questions and texts (hence improving theaccuracy of confidence measures), inferencing(proving the answer from information in texts plusbackground knowledge) and sanity-checking (veri-fying that answers are consistent with known facts).To our knowledge, however, no QA system deliber-ately asks additional questions in order to deriveconstraints on the answers to the original questions.We have found empirically that when our ownQA system?s (Prager et al, 2000; Chu-Carroll et al,2003) top answer is wrong, the correct answer isoften present later in the ranked answer list.
In otherwords, the correct answer is in the passages re-trieved by the search engine, but the system was un-able to sufficiently promote the correct answerand/or deprecate the incorrect ones.
Our new ap-proach of QA-by-Dossier-with-Constraints (QDC)uses the answers to additional questions to providemore information that can be used in ranking candi-date answers to the original question.
These auxil-iary questions are selected such that naturalconstraints exist among the set of correct answers.After issuing both the original question and auxiliaryquestions, the system evaluates all possible combi-nations of the candidate answers and scores them bya simple function of both the answers?
intrinsic con-fidences, and how well the combination satisfies theaforementioned constraints.
Thus we hope to im-prove the accuracy of an essentially NLP task bymaking an end-run around some of the more diffi-cult problems in the field.We describe QDC and experiments to evaluate itseffectiveness.
Our results show that on our test set,substantial improvement is achieved by using con-straints, compared with our baseline system, usingstandard evaluation metrics.2 Related WorkLogic and inferencing have been a part of Ques-tion-Answering since its earliest days.
The firstsuch systems employed natural-language interfacesto expert systems, e.g.
SHRDLU (Winograd, 1972),or to databases e.g.
LUNAR (Woods, 1973) andLIFER/LADDER (Hendrix et al 1977).
CHAT-80(Warren & Pereira, 1982) was a DCG-based NL-query system about world geography, entirely inProlog.
In these systems, the NL question is trans-formed into a semantic form, which is then proc-essed further; the overall architecture and systemoperation is very different from today?s systems,however, primarily in that there is no text corpus toprocess.Inferencing is used in at least two of the morevisible systems of the present day.
The LCC system(Moldovan & Rus, 2001) uses a Logic Prover toestablish the connection between a candidate answerpassage and the question.
Text terms are convertedto logical forms, and the question is treated as a goalwhich is ?proven?, with real-world knowledge beingprovided by Extended WordNet.
The IBM systemPIQUANT (Chu-Carroll et al, 2003) uses Cyc (Le-nat, 1995) in answer verification.
Cyc can in somecases confirm or reject candidate answers based onits own store of instance information; in other cases,primarily of a numerical nature, Cyc can confirmwhether candidates are within a reasonable rangeestablished for their subtype.At a more abstract level, the use of constraintsdiscussed in this paper can be viewed as simply anexample of finding support (or lack of it) for candi-date answers.
Many current systems (see, e.g.
(Clarke et al, 2001), (Prager et al, 2004)) employredundancy as a significant feature of operation:  ifthe same answer appears multiple times in an inter-nal top-n list, whether from multiple sources or mul-tiple algorithms/agents, it is given a confidenceboost, which will affect whether and how it gets re-turned to the end-user.Finally, our approach is somewhat reminiscent ofthe scripts introduced by Schank (Schank et al,1975, and see also Lehnert, 1978).
In order to gener-ate meaningful auxiliary questions and constraints,we need a model (?script?)
of the situation the ques-tion is about.
Among others, we have identified onesuch script modeling the human life cycle that seemscommon to different question types regarding peo-ple.3 Introducing QDCQA-by-Dossier-with-Constraints is an extensionof on-going work of ours called QA-by-Dossier(QbD) (Prager et al, 2004).
In the latter, defini-tional questions of the form ?Who/What is X?
areanswered by asking a set of specific factoid ques-tions about properties of X.
So if X is a person, forexample, these auxiliary questions may be aboutimportant dates and events in the person?s life-cycle,as well as his/her achievement.
Likewise, questionsets can be developed for other entities such as or-ganizations, places and things.QbD employs the notion of follow-on questions.Given an answer to a first-round question, the sys-tem can ask more specific questions based on thatknowledge.
For example, on discovering a person?sprofession, it can ask occupation-specific follow-onquestions: if it finds that people are musicians, it canask what they have composed, if it finds they areexplorers, then what they have discovered, and soon.QA-by-Dossier-with-Constraints extends this ap-proach by capitalizing on the fact that a set of an-swers about a subject must be mutually consistent,with respect to constraints such as time and geogra-phy.
The essence of the QDC approach is to ini-tially return instead of the best answer toappropriately selected factoid questions, the top nanswers (we use n=5), and to choose out of this topset the highest confidence answer combination thatsatisfies consistency constraints.We illustrate this idea by way of the example,?When did Leonardo da Vinci paint the MonaLisa??.
Table 1 shows our system?s top answers tothis question, with associated scores in the range0-1.Score Painting Date1 .64 20002 .43 19883 .34 19114 .31 15035 .30 1490Table 1.
Answers for ?When did Leonardo daVinci paint the Mona Lisa?
?The correct answer is ?1503?, which is in 4thplace, with a low confidence score.
Using QA-by-Dossier, we ask two related questions ?When wasLeonardo da Vinci born??
and ?When did Leonardoda Vinci die??
The answers to these auxiliary ques-tions are shown in Table 2.Given common knowledge about a person?s lifeexpectancy and that a painting must be producedwhile its author is alive, we observe that the bestdates proposed in Table 2 consistent with one an-other are that Leonardo da Vinci was born in 1452,died in 1519, and painted the Mona Lisa in 1503.
[The painting date of 1490 also satisfies the con-straints, but with a lower confidence.]
We will ex-amine the exact constraints used a little later.
Thisexample illustrates how the use of auxiliary ques-tions helps constrain answers to the original ques-tion, and promotes correct answers with initial lowconfidence scores.
As a side-effect, a short dossieris produced.Score Born  Score Died1 .66 1452  .99 15192 .12 1519  .98 19893 .04 1920  .96 14524 .04 1987  .60 19885 .04 1501  .60 1990Table 2.
Answers for auxiliary questions ?Whenwas Leonardo da Vinci born??
and ?When did Leo-nardo da Vinci die?
?.3.1 Reciprocal QuestionsQDC also employs the notion of reciprocal ques-tions.
These are a type of follow-on question usedsolely to provide constraints, and do not add to thedossier.
The idea is simply to double-check the an-swer to a question by inverting it, substituting thefirst-round answer and hoping to get the originalsubject back.
For example, to double-check ?Sac-ramento?
as the answer to ?What is the capital ofCalifornia??
we would ask ?Of what state is Sacra-mento the capital??.
The reciprocal question wouldbe asked of all of the candidate answers, and theconfidences of the answers to the reciprocal ques-tions would contribute to the selection of the opti-mum answer.
We will discuss later how thisreciprocation may be done automatically.
In a sepa-rate study of reciprocal questions (Prager et al,2004), we demonstrated an increase in precisionfrom .43 to .95, with only a 30% drop in recall.Although the reciprocal questions seem to besymmetrical and thus redundant, their power stemsfrom the differences in the search for answers inher-ent in our system.
The search is primarily based onthe expected answer type (STATE vs. CAPITAL inthe above example).
This results in different docu-ment sets being passed to the answer selection mod-ule.
Subsequently, the answer selection moduleworks with a different set of syntactic and semanticrelationships, and the process of asking a reciprocalquestion ends up looking more like the process ofasking an independent one.
The only difference be-tween this and the ?regular?
QDC case is in the typeof constraint applied to resolve the resulting answerset.3.2 Applying QDCIn order to automatically apply QDC during ques-tion answering, several problems need to be ad-dressed.
First, criteria must be developed todetermine when this process should be invoked.Second, we must identify the set of question typesthat would potentially benefit from such an ap-proach, and, for each question type, develop a set ofauxiliary questions and appropriate constraintsamong the answers.
Third, for each question type,we must determine how the results of applying con-straints should be utilized.3.2.1 When to apply QDCTo address these questions we must distinguishbetween ?planned?
and ?ad-hoc?
uses of QDC.
Foranswering definitional questions (?Who/what isX??)
of the sort used in TREC2003, in which collec-tions of facts can be gathered by QA-by-Dossier, wecan assume that QDC is always appropriate.
Bydefining broad enough classes of entities for whichthese questions might be asked (e.g.
people, places,organizations and things, or major subclasses ofthese), we can for each of these classes manuallyestablish once and for all a set of auxiliary questionsfor QbD and constraints for QDC.
This is the ap-proach we have taken in the experiments reportedhere.
We are currently working on automaticallylearning effective auxiliary questions for some ofthese classes.In a more ad-hoc situation, we might imagine thata simple variety of QDC will be invoked usingsolely reciprocal questions whenever the differencebetween the scores of the first and second answer isbelow a certain threshold.3.2.2 How to apply QDCWe will posit three methods of generating auxil-iary question sets:o By hando Through a structured repository, such as aknowledge-base of real-world informationo Through statistical techniques tied to a machine-learning algorithm, and a text corpus.We think that all three methods are appropriate,but we initially concentrate on the first for practicalreasons.
Most TREC-style factoid questions areabout people, places, organizations, and things, andwe can generate generic auxiliary question sets foreach of these classes.
Moreover, the purpose of thispaper is to explain the QDC methodology and toinvestigate its value.3.2.3 Constraint NetworksThe constraints that apply to a given situation canbe naturally represented in a network, and we find ituseful for visualization purposes to depict the con-straints graphically.
In such a graph the entities andvalues are represented as nodes, and the constraintsand questions as edges.It is not clear how possible, or desirable, it is toautomatically develop such constraint networks(other than the simple one for reciprocal questions),since so much real-world knowledge seems to berequired.
To illustrate, let us look at the constraintsrequired for the earlier example.
A more complexconstraint system is used in our experiments de-scribed later.
For our Leonardo da Vinci example,the set of constraints applied can be expressed asfollows1:Date(Died) <= Date(Born) + 100Date(Painting) >=  Date(Born) + 7Date(Painting) <=  Date(Died)The corresponding graphical representation is inFigure 1.
Although the numerical constants in theseconstraints betray a certain arbitrariness, we found ita useful practice to find a middle ground betweenabsolute minima or maxima that the values canachieve and their likely values.
Furthermore, al-though these constraints are manually derived forour prototype system, they are fairly general for thehuman life-cycle and can be easily reused for other,similar questions, or for more complex dossiers, asdescribed below.Figure 1.
Constraint Network for Leonardo ex-ample.
Dashed lines represent question-answerpairs, solid lines constraints between the answers.We also note that even though a constraint net-work might have been inspired by and centeredaround a particular question, once the network isestablished, any question employed in it could be theend-user question that triggers it.There exists the (general) problem of when morethan one set of answers satisfies our constraints.Our approach is to combine the first-round scores ofthe individual answers to provide a score for thedossier as a whole.
There are several ways to dothis, and we found experimentally that it does notappear critical exactly how this is done.
In the ex-ample in the evaluation we mention one particularcombination algorithm.3.2.4 Kinds of constraint networkThere are an unlimited number of possible con-straint networks that can be constructed.
We haveexperimented with the following:Timelines.
People and even artifacts have life-cycles.
The examples in this paper exploit these.1 Painting is only an example of an activity in these constraints.Any other achievement that is usually associated with adulthoodcan be used.Geographic (?Where is X?).
Neighboring entitiesare in the same part of the world.Kinship (?Who is married to X?).
Most kinshiprelationships have named reciprocals e.g.
husband-wife, parent-child, and cousin-cousin.
Even thoughthese are not in practice one-one relationships, wecan take advantage of sufficiency even if necessity isnot entailed.Definitional (?What is X?
?, ?What does XYZ standfor??)
For good definitions, a term and its defini-tion are interchangeable.Part-whole.
Sizes of parts are no bigger than sizesof wholes.
This fact can be used for populations,areas, etc.3.2.5 QDC potentialWe performed a manual examination of the 500TREC2002 questions2 to see for how many of thesequestions the QDC framework would apply.
Beinga manual process, these numbers provide an upperbound on how well we might expect a future auto-matic process to work.We noted that for 92 questions (18%) a non-trivial constraint network of the above kinds wouldapply.
For a total of 454 questions (91%), a simplereciprocal constraint could be generated.
However,for 61 of those, the reciprocal question was suffi-ciently non-specific that the sought reciprocal an-swer was unlikely to be found in a reasonably-sizedhit-list.
For example, the reciprocal question to?How did Mickey Mantle die??
would be ?Who diedof cancer??
However, we can imagine using otherfacts in the dossier to craft the question, giving us?What famous baseball player (or Yankees player)died of cancer?
?, giving us a much better chance ofsuccess.
For the simple reciprocation, though, sub-tracting these doubtful instances leaves 79% of thequestions appearing to be good candidates for QDC.4 Experimental Setup4.1 Test set generationTo evaluate QDC, we had our system developdossiers of people in the creative arts, unseen in pre-vious TREC questions.
However, we wanted to usethe personalities in past TREC questions as inde-pendent indicators of appropriate subject matter.Therefore we collected all of the ?creative?
peoplein the TREC9 question set, and divided them up intoclasses by profession, so we had, for example, malesingers Bob Marley, Ray Charles, Billy Joel andAlice Cooper; poets William Wordsworth andLangston Hughes; painters Picasso, Jackson Pollock2 This set did not contain definition questions, which, by ourinspection, lend themselves readily to reciprocation.BirthdateDeathdateLeonardo Paintingand Vincent Van Gogh, etc.
?
twelve such groupingsin all.
For each set, we entered the individuals in the?Google Sets?
interface(http://labs.google.com/sets), which finds ?similar?entities to the ones entered.
For example, from ourset of male singers it found: Elton John, Sting, GarthBrooks, James Taylor, Phil Collins, MelissaEtheridge, Alanis Morissette, Annie Lennox, Jack-son Browne, Bryan Adams, Frank Sinatra and Whit-ney Houston.Altogether, we gathered 276 names of creativeindividuals this way, after removing duplicates,items that were not names of individuals, and namesthat did not occur in our test corpus (the AQUAINTcorpus).
We then used our system manually to helpus develop ?ground truth?
for a randomly selectedsubset of 109 names.
This ground truth served bothas training material and as an evaluation key.
Wesplit the 109 names randomly into a set of 52 fortraining and 57 for testing.
The training processused a hill-climbing method to find optimal valuesfor three internal rejection thresholds.
In developingthe ground truth we might have missed some in-stances of assertions we were looking for, so thereported recall (and hence F-measure) figures shouldbe considered to be upper bounds, but we believe thecalculated figures are not far from the truth.4.2 QDC OperationThe system first asked three questions for eachsubject X:In what year was X born?In what year did X die?What compositions did X have?The third of these triggers our named-entity typeCOMPOSITION that is used for all kinds of titledworks ?
books, films, poems, music, plays and soon, and also quotations.
Our named-entity recog-nizer has rules to detect works of art by phrases thatare in apposition to ?the film ?
?
or the ?the book?
?
etc., and also captures any short phrase in quotesbeginning with a capital letter.
The particular ques-tion phrasing we used does not commit us to anyspecific creative verb.
This is of particular impor-tance since it very frequently happens in text thattitled works are associated with their creators bymeans of a possessive or parenthetical construction,rather than subject-verb-object.The top five answers, with confidences, are re-turned for the born and died questions (subject toalso passing a confidence threshold test).
The com-positions question is treated as a list question, mean-ing that all answers that pass a certain threshold arereturned.
For each such returned work Wi, two addi-tional questions are asked:What year did X have Wi?Who had Wi?The top 5 answers to each of these are returned,again as long as they pass a confidence threshold.We added a sixth answer ?NIL?
to each of the datesets, with a confidence equal to the rejection thresh-old.
(NIL is the code used in TREC ever sinceTREC10 to indicate the assertion that there is noanswer in the corpus.)
We used a two stage con-straint-satisfaction process:Stage 1:  For each work Wi for subject X, weadded together its original confidence to the confi-dence of the answer X in the answer set of the recip-rocal question (if it existed ?
otherwise we addedzero).
If the total did not exceed a learned threshold(.50) the work was rejected.Stage 2.
For each subject, with the remainingcandidate works we generated all possible combina-tions of the date answers.
We rejected any combina-tion that did not satisfy the following constraints:DIED >= BORN + 7DIED <= BORN + 100WORK >= BORN + 7WORK <= BORN + 100WORK <= DIEDDIED <= WORK + 100The apparent redundancy here is because of thepotential NIL answers for some of the date slots.We also rejected combinations of works whoseyears spanned more than 100 years (in case therewere no BORN or DIED dates).
In performing theseconstraint calculations, NIL satisfied every test byfiat.
The constraint network we used is depicted inFigure 2.Figure 2.
Constraint Network for evaluation ex-ample.
Dashed lines represent question-answerpairs, solid lines constraints between the answers.We used as a test corpus the AQUAINT corpusused in TREC-QA since 2002.
Since this was notthe same corpus from which the test questions weregenerated (the Web), we acknowledged that theremight be some difference in the most common spell-ing of certain names, but we made no attempt to cor-rect for this.
Neither did we attempt to normalize,translate or aggregate names of the titled works thatwere returned, so that, for example, ?Well-Birthdate of XDeathdate of XWork WiAuthor X Date of WiXi = Author of WiTempered Klavier?
and ?Well-Tempered Clavier?were treated as different.
Since only individualswere used in the question set, we did not have in-stances of problems we saw in training, such aswhere an ensemble (such as The Beatles) created acertain piece, which in turn via the reciprocal ques-tion was found to have been written by a single per-son (Paul McCartney).
The reverse situation wasstill possible, but we did not handle it.
We foresee afuture version of our system having knowledge ofensembles and their composition, thus removing thisrestriction.
In general, a variety of ontological rela-tionships could occur between the original individ-ual and the discovered performer(s) of the work.We generated answer keys by reading the pas-sages that the system had retrieved and from whichthe answers were generated, to determine ?truth?.
Incases of absent information in these passages, wedid our own corpus searches.
This of course madethe issue of evaluation of recall only relative, sincewe were not able to guarantee we had found all ex-isting instances.We encountered some grey areas, e.g., if a paint-ing appeared in an exhibition or if a celebrity en-dorsed a product, then should the exhibition?s orproduct?s name be considered an appropriate ?work?of the artist?
The general perspective adopted wasthat we were not establishing or validating the natureof the relationship between an individual and a crea-tive work, but rather its existence.
We answered?yes?
if we subjectively felt the association to beboth very strong and with the individual?s participa-tion ?
for example, Pamela Anderson and Playboy.However, books/plays about a person or dates ofperformances of one?s work were considered incor-rect.
As we shall see, these decisions would nothave a big impact on the outcome.4.3 Effect of ConstraintsThe answers collected from these two rounds ofquestions can be regarded as assertions about thesubject X.
By applying constraints, two possibleeffects can occur to these assertions:1.
Some works can get thrown out.2.
An asserted date (which was the top candidatefrom its associated question) can get replaced bya candidate date originally in positions 2-6(where sixth place is NIL)Effect #1 is expected to increase precision at therisk of worsening recall; effect #2 can go either way.We note that NIL, which is only used for dates, canbe the correct answer if the desired date assertion isabsent from the corpus; NIL is considered a ?value?in this evaluation.By inspection, performances and other indirectworks (discussed in the previous section) were usu-ally associated with the correct artist, so our decisionto remove them from consideration resulted in a de-crease in both the numerator and denominator of theprecision and recall calculations, resulting in aminimal effect.The results of applying QDC to the 57 test indi-viduals are summarized in Table 3.
The baselineassertions for individual X were:o Top-ranking birthdate/NILo Top-ranking deathdate/NILo Set of works Wi that passed thresholdo Top-ranking date for Wi /NILThe sets of baseline assertions (by individual) arein effect the results of QA-by-Dossier WITHOUTConstraints (QbD).Assertions Micro-Average Macro-AverageTotal Cor-rectTru-thPrec Rec F Prec Rec FBase-line1671 517 933 .309 .554 .396 .331 .520 .386QDC 1417 813 933 .573 .871 .691 .603 .865 .690Table 3.
Results of Performance Evaluation.Two calculations of P/R/F are made, depending onwhether the averaging is done over the whole set, orfirst by individual; the results are very similar.The QDC assertions were the same as those forQbD, but reflecting the following effects:o Some {Wi, date} pairs were thrown out (3 out of14 on average)o Some dates in positions 2-6 moved up (applica-ble to birth, death and work dates)The results show improvement in both precisionand recall, in turn determining a 75-80% relativeincrease in F-measure.5 DiscussionThis exposition of QA-by-Dossier-with-Constraints is very short and undoubtedly leavesmay questions unanswered.
We have not presenteda precise method for computing the QDC scores.One way to formalize this process would be to treatit as evidence gathering and interpret the results in aBayesian-like fashion.
The original system confi-dences would represent prior probabilities reflectingthe system?s belief that the answers are correct.
Asmore evidence is found, the confidences would beupdated to reflect the changed likelihood that an an-swer is correct.We do not know a priori how much ?slop?
shouldbe allowed in enforcing the constraints, since auxil-iary questions are as likely to be answered incor-rectly as the original ones.
A further problem is todetermine the best metric for evaluating such ap-proaches, which is a question for QA in general.The task of generating auxiliary questions andconstraint sets is a matter of active research.
Evenfor simple questions like the ones considered here,the auxiliary questions and constraints we looked atwere different and manually chosen.
Hand-crafting alarge number of such sets might not be feasible, butit is certainly possible to build a few for commonsituations, such as a person?s life-cycle.
More gener-ally, QDC could be applied to situations in which acertain structure is induced by natural temporal (ourLeonardo example) and/or spatial constraints, or byproperties of the relation mentioned in the question(evaluation example).
Temporal and spatial con-straints appear general to all relevant question types,and include relations of precedence, inclusion, etc.For certain relationships, there are naturally-occurring reciprocals (if X is married to Y, then Y ismarried to X; if X is a child of Y then Y is a parentof X; compound-term to acronym and vice versa).Transitive relationships (e.g.
greater-than, located-in, etc.)
offer the immediate possibility of con-straints, but this avenue has not yet been explored.5.1 Automatic Generation of Reciprocal Ques-tionsWhile not done in the work reported here, we arelooking at generating reciprocal questions automati-cally.
Consider the following transformations:?What is the capital of California??
-> ?Of whatstate is <candidate> the capital??
?What is Frank Sinatra?s nickname??
->?Whose (or what person?s) nickname is <can-didate>??
?How deep is Crater Lake??
-> ?What (or whatlake) is <candidate> deep??
?Who won the Oscar for best actor in 1970?
?-> ?In what year did <candidate> win theOscar for best actor??
(and/or ?What awarddid <candidate> win in 1970??
)These are precisely the transformations necessaryto generate the auxiliary reciprocal questions fromthe given original questions and candidate answersto them.
Such a process requires identifying an en-tity in the question that belongs to a known class,and substituting the class name for the entity.
Thisentity is made the subject of the question, the previ-ous subject (or trace) being replaced by the candi-date answer.
We are looking at parse-tree ratherthan string transformations to achieve this.
Thiswork will be reported in a future paper.5.2 Final ThoughtsDespite these open questions, initial trials withQA-by-Dossier-with-Constraints have been veryencouraging, whether it is by correctly answeringpreviously missed questions, or by improving confi-dences of correct answers.
An interesting questionis when it is appropriate to apply QDC.
Clearly, ifthe base QA system is too poor, then the answers tothe auxiliary questions will be useless; if the basesystem is highly accurate, the increase in accuracywill be negligible.
Thus our approach seems mostbeneficial to middle-performance levels, which, byinspection of TREC results for the last 5 years, iswhere the leading systems currently lie.We had initially thought that use of constraintswould obviate the need for much of the complexityinherent in NLP.
As mentioned earlier, with thecase of ?The Beatles?
being the reciprocal answer tothe auxiliary composition question to ?Who is PaulMcCartney?
?, we see that structured, ontologicalinformation would benefit QDC.
Identifying alter-nate spellings and representations of the same name(e.g.
Clavier/Klavier, but also taking care of varia-tions in punctuation and completeness) is also nec-essary.
When we asked ?Who is Ian Anderson?
?,having in mind the singer-flautist for the Jethro Tullrock band, we found that he is not only that, but alsothe community investment manager of the Englishconglomerate Whitbread, the executive director ofthe U.S.
Figure Skating Association, a writer forNew Scientist, an Australian medical advisor to theWHO, and the general sales manager of Houseman,a supplier of water treatment systems.
Thus theproblem of word sense disambiguation has returnedin a particularly nasty form.
To be fully effective,QDC must be configured not just to find a consistentset of properties, but a number of independent setsthat together cover the highest-confidence returnedanswers3.
Altogether, we see that some of the veryproblems we aimed to skirt are still present and needto be addressed.
However, we have shown that evendisregarding these issues, QDC was able to providesubstantial improvement in accuracy.6 SummaryWe have presented a method to improve the accu-racy of a QA system by asking auxiliary questionsfor which natural constraints exist.
Using these con-straints, sets of mutually consistent answers can begenerated.
We have explored questions in the bio-graphical areas, and identified other areas of appli-cability.
We have found that our methodologyexhibits a double advantage:  not only can it im-3 Possibly the smallest number of sets that provide such cover-age.prove QA accuracy, but it can return a set of mutu-ally-supporting assertions about the topic of theoriginal question.
We have identified many openquestions and areas of future work, but despite thesegaps, we have shown an example scenario whereQA-by-Dossier-with-Constraints can improve the F-measure by over 75%.7 AcknowledgementsWe wish to thank Dave Ferrucci, Elena Filatovaand Sasha Blair-Goldensohn for helpful discussions.This work was supported in part by the AdvancedResearch and Development Activity (ARDA)'s Ad-vanced Question Answering for Intelligence(AQUAINT) Program under contract numberMDA904-01-C-0988.ReferencesChu-Carroll, J., J. Prager, C. Welty, K. Czuba andD.
Ferrucci.
?A Multi-Strategy and Multi-SourceApproach to Question Answering?, Proceedingsof the 11th TREC,  2003.Clarke, C., Cormack, G., Kisman, D.. and Lynam, T.?Question answering by passage selection(Multitext experiments for TREC-9)?
in Proceed-ings of the 9th TREC, pp.
673-683, 2001.Hendrix, G., E. Sacerdoti, D. Sagalowicz, J. Slocum:Developing a Natural Language Interface to Com-plex Data.
VLDB 1977: 292Lehnert, W.  The Process of Question Answering.
AComputer Simulation of Cognition.
LawrenceErlbaum Associates, Publishers, 1978.Lenat, D. 1995.
"Cyc: A Large-Scale Investment inKnowledge Infrastructure."
Communications ofthe ACM 38, no.
11.Moldovan, D. and V. Rus, ?Logic Form Transfor-mation of WordNet and its Applicability to Ques-tion Answering?, Proceedings of the ACL, 2001.Prager, J., E. Brown, A. Coden, and D. Radev.
2000.
"Question-Answering by Predictive Annotation?.In Proceedings of SIGIR 2000, pp.
184-191.Prager, J., J. Chu-Carroll and K. Czuba, "A Multi-Agent Approach to using Redundancy and Rein-forcement in Question Answering" in New Direc-tions in Question-Answering, Maybury, M.
(Ed.
),to appear in 2004.Schank, R. and R. Abelson.
?Scripts, Plans andKnowledge?, Proceedings of IJCAI?75.Voorhees, E. ?Overview of the TREC 2002 Ques-tion Answering Track?, Proceedings of the 11thTREC, 2003.Warren, D., and F. Pereira "An efficient easilyadaptable system for interpreting natural languagequeries," Computational Linguistics, 8:3-4, 110-122, 1982.Winograd, T. Procedures as a representation for datain a computer program for under-standing naturallanguage.
Cognitive Psychology, 3(1), 1972.Woods, W. Progress in natural language understand-ing --- an application in lunar geology.
Proceed-ings of the 1973 National Computer Conference,AFIPS Conference Proceedings, Vol.
42, 441--450, 1973.
