Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 2?7,Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational LinguisticsAutomatic extraction of NV expressions in Basque: basic issues oncooccurrence techniquesAntton GurrutxagaElhuyar Foundationa.gurrutxaga@elhuyar.comIn?aki AlegriaIXA group/Univ.
of the Basque Countryi.alegria@ehu.esAbstractTaking as a starting-point the developmenton cooccurrence techniques for several lan-guages, we focus on the aspects that shouldbe considered in a NV extraction task forBasque.
In Basque, NV expressions are con-sidered those combinations in which a noun,inflected or not, is co-occurring with a verb, aserabakia hartu (?to make a decision?
), kontuanhartu (?to take into account?)
and buruz jakin(?to know by heart?).
A basic extraction sys-tem has been developed and evaluated againsttwo references: a) a reference which includesNV entries from several lexicographic works;and b) a manual evaluation by three experts ofa random sample from the n-best lists.1 IntroductionThe last decade has witnessed great advances in theautomatic identification and processing of MWEs.In the case of Basque, advances are limited to termi-nology extraction and the tagging in corpora of theMWEs represented in lexical databases.Furthermore, the work on both theoretical andpractical phraseology in Basque has been mainly fo-cused on idiomatic expressions, leaving aside col-locations (Pe?rez Gaztelu et al, 2004).
As a con-sequence, Basque NLP and lexicography have notbenefited from the approach that emphasized the im-portance of such units, and very important areas areunderdeveloped.With the aim of taking steps to turn this situa-tion, we undertake the task of extracting NV com-binations from corpora.
As a preliminary step, wemust face the morphosyntactic aspects of Basquethat might influence the efficiency of the process.2 MWE: basic definition and extractiontechniquesAs a basis for our work, we take idiomaticity asthe key feature for the definition and classifica-tion of MWE.
Idiomaticity could be described as anon-discrete magnitude, whose ?value?, accordingto recent investigations (Baldwin and Kim, 2010;Fazly and Stevenson, 2007; Granger and Paquot,2008), has turned to depend on a complex combi-nation of features such as institutionalization, non-compositionality and lexico-syntactic fixedness.The idiomaticity of MWEs appears rather as acontinuum than as a scale of discrete values (Sin-clair, 1996; Wulff, 2010).
Thus, the classifica-tion of MWEs into discrete categories is a difficulttask.
Taking Cowie?s classification as an initial basis(Cowie, 1998), our work is focused on phrase-likeunits, aiming, at this stage, to differentiate MWEs(idioms and collocations) from free combinations.Specifically, NV combinations with the followingcharacteristics are considered as MWEs:?
Idioms: non-compositional combinations, asopaque idioms (adarra jo: ?to pull somebody?sleg?
; lit: ?to play the horn?)
and figurative id-ioms (burua hautsi: ?to rack one?s brain?
; lit:?to break one?s head?).?
Collocations:?
Semicompositional combinations, inwhich the noun keeps its literal meaning,2whereas the verb acts as a support verb(lan egin: ?to work?
; lit.
?to do work?
),or has a meaning which is specific to thatcombination (atentzioa eman: ?to catchsomeone?s eye?
; lit.
?to give attention?
(sth to sb)); legea urratu: ?to break thelaw?
; lit.
?to tear the law?).?
Compositional combinations with lexicalrestriction, in which it is not possible tosubstitute the verb with its synonyms, orthat present a clear statistical idiosyncrasyin favor of a given synonym choice (elka-rtasuna adierazi: ?to express solidarity?
;konpromisoa berretsi: ?to confirm a com-mitment?
).Among the different techniques that have beenproposed to extract and characterize MWEs, thecooccurrence of the components is the most usedheuristic of institutionalization, and the use of asso-ciation measures (AM) goes back to early researchon this field (Church and Hanks, 1990; Smadja,1993).
In recent years, the comparative analysis ofAMs has aroused considerable interest, as well asthe possibility of obtaining better results by com-bining them (Pearce, 2002; Pecina, 2005).
Cooc-currence techniques are usually used in combinationwith linguistic techniques, which allow the use oflemmatized and POS-tagged corpora, or even syn-tactic dependencies (Seretan, 2008).3 Special features of Basque NVcombinationsThese are some characteristics of the NV combina-tions in Basque to be considered in order to designthe extraction process efficiently:?
Basque being an agglutinative language, MWEextraction must work on tagged texts, in orderto identify different surface forms with theircorresponding lemma.
Thus, pure statisticalmethods working with raw text are not ex-pected to yield acceptable results.?
Some combinations with a noun as first lemmado not correspond to NV combinations in thesense that is usually understood in English.
Forexample, the expression kontuan hartu can betranslated as take into account, where kontu isa noun in the inessive case.
We are interested inall types of combinations that a noun can formwith verbs.?
Representing NV combinations as lemma-lemma pairs is by no means satisfactory; wewould not be able to differentiate the aforemen-tioned kontuan hartu from kontu hartu (?to askfor an explanation?).
So it is necessary to dealwith the form or type of every noun.?
In order to propose canonical forms for NVcombinations, we need case and number an-notations for nouns in bigram data.
The nextexamples are different forms of the canoni-cal erabakia hartu (?to make a decision?
): ezzuen erabakirik hartu (?he did not make anydecision?
), zenbait erabaki hartu behar ditugu(?we have to make some decisions?).
Canonicalforms can be formulated by bigram normaliza-tion (see section 4.5 for details).4 Experimental setup4.1 Corpora resourcesIn our experiments, we use a journalistic cor-pus from two sources: (1) Issues published be-tween 2001-2002 by the newspaper EuskaldunonEgunkaria (28 Mw); and (2) Issues published be-tween 2006-2010 by the newspaper Berria (47 Mw).So, the overall size of the corpus is 75 Mw.4.2 Corpus-processingFor linguistic tagging, we use EUSTAGGER by theIXA group of the University of the Basque Country(Aduriz et al, 1996).
After linguistic processing, weobtain information about the lemma, part-of-speech,subcategory, case, number and other morphosyntac-tic features.We used EUSTAGGER without the module to de-tect and annotate MWEs in order to evaluate the au-tomatic extraction, regardless of wheter the candi-dates are in the lexical database.4.3 Preparing tagged corpora for bigramgenerationFor bigram generation, we use the Ngram StatisticsPackage-NSP (Banerjee and Pedersen, 2010).
In3order to retain in the text sent to NSP the linguis-tic information needed according to section 3, weadd different types of linguistic information to thetokens, depending on the POS of the components ofthe combination we are dealing with.
In the case ofNV combinations, the nouns are represented in thefollowing form:token lemma POS subcategory case numberIn the case of verbs, only lemma and POS areused, as verb inflection has no influence on thecanonical form of the expression.
In future work,verb inflection will be one of the parameters to mea-sure syntactical flexibility.
All other types of tokensare discarded and considered as ?non-token?
for NSPprocessing.Before this step, some surface-grammar rules aredefined to detect and filter the participle forms thatare not part of a NV combination, but must be ana-lyzed as adjectives or nouns (eg.
herrialde aurrerat-uak ?developed countries?, and gobernuaren aliat-uak, ?government?s allies?
).4.4 Bigram generationWe generated bigram sets for two different windowspans: ?1 and ?5.
In both sets, the frequency cri-terion for a bigram to be generated is f > 30.
Also,the following punctuation marks are interpreted asa boundary for bigram generation: period, colon,semicolon, and question and exclamation marks.Then, all counts of bigrams in NV and VN order arecombined using NSP, and reordered in NV order.Additionally, a heuristic is used to filter somecombinations.
The first member of many ?com-pound verbs?
like nahi izan (?to want?
), is a noun,and some of them combine usually with a verb, inVN order: ikusi nahi (zuen) (?he wanted to see?).
Inorder to reduce this noise, the combinations occur-ring mostly in VN order are removed.
The combi-nations generated from passive constructions (hartu-tako erabakien ondorioak, ?the consequences of thedecisions made?)
are not affected by this filtering.4.5 Bigram normalizationIn order to get more representative statistics, andto get information that would enable us to proposea canonical form for each MWE candidate, differ-ent inflection forms of the same case in nouns arenormalized to the most frequent form, and bigramcounts are recalculated.
I.e.
[ erabakia / erabakiak/ erabakiok / erabakirik / erabaki ] hartu are col-lapsed to erabakia hartu (?to make a decision?
), be-cause all the mentioned forms of the lemma erabakiappear in the absolutive case.
In contrast, the com-binations kontu hartu (?to ask for an explanation?
)and kontuan hartu (?take into account?)
are not nor-malized, as their noun forms correspond to differ-ent cases, namely, absolutive (kontu) and inessive(kontuan).
A Perl script detects in the dataset thebigrams to be normalized, using the combined keynoun lemma/noun case+verb lemma, creates a sin-gle bigram with the most frequent form, and sumsthe frequencies of bigrams and those of the noun un-igrams.As an example, this is normalization data forkalean ibili (?to walk on the street?
):kalean kale IZE ARR INE NUMS<>ibili ADI<>223 3354 10880kaleetan kale IZE ARR INE NUMP<>ibili ADI<>119 243 10880?kalean kale IZE ARR INE NUMS<>ibili ADI<>342 3597 10880Besides, ergative-singular ?
absolutive-pluralnormalization is carried out when the ratio is greaterthan 1:5.
This heuristic is used in order to repairsome mistakes from the tagger.
Finally, partitivecase (PAR) is assimilated to absolutive (ABS) for bi-gram normalization; partitive is a case used in neg-ative, interrogative and conditional sentences withsubjects of intransitive verbs and objects of transi-tive verbs.
I.e.
ez zuen erabakirik hartu (?he did notmake any decision?
).Thus, this is the normalization of erabakia hartu:erabakia erabaki IZE ARR ABS NUMS<>hartu ADI<>2658 6329 88447erabakiak erabaki IZE ARR ABS NUMP<>hartu ADI<>1632 2397 88447erabakiak erabaki IZE ARR ERG NUMP<>hartu ADI<>88 141 88447erabakirik erabaki IZE ARR PAR MG<>hartu ADI<>211 211 88447?erabakia erabaki IZE ARR ABS NUMS<>hartu ADI<>4589 9361 884474.6 AM calculationThe statistical analysis of cooccurrence data is car-ried out using Stefan Evert?s UCS toolkit (Evert,2005).
The most common association measures arecalculated for each bigram: f , t-score (also t-test),log-likelihood ratio, MI, MI3, and chi-square (?2).4.7 EvaluationIn order to evaluate the results of the bigram extrac-tion process, we use as a reference a collection of4NV expressions published in five Basque resources:a) The Unified Basque Dictionary, b) Euskal Hizte-gia (Sarasola, 1996); c) Elhuyar Hiztegia; d) Intzaproject; and e) EDBL (Aldezabal et al, 2001).The total number for NV expressions is 3,742.Despite the small size of the reference, we believethat it may be valid for a comparison of the perfor-mance of different AMs.
Nevertheless, even a su-perficial analysis reveals that the reference is mostlymade up of two kinds of combinations, idioms andtypical ?compound verbs?1.Every evaluation against a dictionary dependslargely on its recall and quality, and we envisage,as recommended by Krenn (1999), to build a hand-made gold standard.
To this end, we extract an eval-uation sample merging the 2,000-best candidates ofeach AM ranking from the w = ?1 extraction set.There are 4,334 different bigrams in this set.
Thismanual evaluation is an ongoing work by a group ofthree experts (one of them is an author of this paper).Annotators were provided with an evaluation man-ual, with explanatory information about the evalua-tion task and the guidelines that must be followed todifferentiate MWEs from free combinations, basedon the criteria mentioned in section 2.
Illustrativeexamples are included.At present, a random sample of 600 has been eval-uated (13.8%), with a Fleiss kappa of 0.46.
Eventhough some authors have reported lower agree-ments on this task (Street et al, 2010), this level ofagreement is comparatively low (Fazly and Steven-son, 2007; Krenn et al, 2004), and by no means sat-isfactory.
It is necessary to make further efforts toimprove the discriminatory criteria, and achieve abetter ?tuning?
between the annotators.5 ResultsFigure 1 shows the precision curves obtained foreach AM in the automatic evaluation.
Frequencyyields the best precision, followed by t-score, log-likelihood and MI3.
MI and ?2 have a very lowperformance, even below the baseline2.
These re-1Support verbs with syntactic idiosyncrasy (anomalous useof the indefinite noun), as lan egin (?to work?)
and min hartu(?to get hurt?
).2Following Evert (2005), our baseline corresponds to theprecision yielded by a random ranking of the n candidates fromthedata set?
; and our topline is ?the precision achieved by ansults are consistent with those reported by Krenn andEvert (2001) for support-verbs (FVG).
Accordingly,this is the type of combination which is very muchpresent in our dictionary reference.Figure 1: Precision results for the extraction set with w =?1 and f > 30.Figure 2 offers an evaluation of the influence ofwindow span and bigram normalization.
The bestresults are obtained by the f ranking with a narrowwindow and without bigram normalization.
Regard-ing bigram normalization, it could be concluded, atfirst sight, that the canonical forms included in thedictionary are not the most frequent forms of theircorresponding MWEs.
Thus, the frequency criteriaused to normalize different forms of the same caseand assign canonical forms must be reviewed.
As forwindow span, the hypothesis that, since Basque islargely a free-word-order language, a wider windowwould yield more significant cooccurrence statistics,is not confirmed at the moment.
Further analysis isneeded to interpret these results from a deeper lin-guistic point of view.Even though the manually evaluated random sam-ple is small (600 combinations), some provisionalconclusions can be drawn from the results.
Theamount of candidates validated by at least two of thethree evaluators is 153, whereas only 29 of them areincluded in the dictionary reference.
Even thoughMWE classification has not yet been undertaken bythe annotator?s team, a first analysis by the authorsshows that most of the manually validated combina-?ideal?
measure that ranks all TPs at the top of the list?.5Figure 2: Precision results of f and t-score for three dif-ferent extraction sets (f > 30): a) w = ?1 with bigramnormalization; b) w = ?1 without bigram normalization;and c) w = ?5 with bigram normalization.tions not included in the dictionary (108 out of 124)are restricted collocations (mainly support-verb con-structions that are not ?compound verbs?)
or statis-tically idiosyncratic units.
This is the first clue thatconfirms our suspicions about the limited coverageand representativeness of the reference.
At the sametime, it could be one of the possible explanations forthe low inter-annotator agreement achieved, as far asthose types of MWEs are the most difficult to differ-entiate from free combinations.Figure 3 presents the precision curves for thecomplete evaluation set estimated from the manu-ally evaluated random sample using the techniqueproposed by Evert and Krenn (2005).
As expected,precision results increase compared with the evalu-ation against the dictionary.
Frequency and t-scoreoutperform the other AMs, but frequency is not thebest measure in the whole range, as it is overtakenby t-score in the first 1,200 candidates.6 Conclusions and Future workThe first results for the extraction of NV expressionsin Basque are similar to the figures in Krenn andEvert (2001).
Frequency and t-score are good mea-sures and it seems difficult to improve upon them.Nevertheless, in light of the results, it is essential tocomplete the manual evaluation and build a repre-sentative gold standard in order to have a more pre-cise idea of the coverage of the reference, and getFigure 3: Precision results estimated from a 13.8% ran-don sample manually evaluated (600 conbinations).a more accurate view of the behaviour of AMs infunction of several factors such as the type of combi-nation, corpus size, frequency range, window span,etc.
Bigram normalization is, in principle, a reason-able procedure to formulate representative canoni-cal forms, but requires a deeper analysis of the si-lence that it seems to generate in the results.
Finally,the first evaluation using a small gold-standard is en-couraging, because it suggests that using AMs it ispossible to find new expressions that are not pub-lished in Basque dictionaries.In the near future, we want to carry out a morecomprehensive evaluation of the AMs, and studyhow to combine them in order to improve the re-sults (Pecina and Schlesinger, 2006).
In addition ofthis, we want to detect lexical, syntactic and seman-tic features of the expressions, and use this informa-tion to characterize them (Fazly et al, 2009).AcknowledgmentsThis research was supported in part by the Span-ish Ministry of Education and Science (OpenMT-2,TIN2009-14675-C03-01) and by the Basque Gov-ernment (Berbatek: Tools and Technologies to pro-mote Language Industry.
Etortek - IE09-262).
Ourcolleagues Ainara Estarrona and Larraitz Uria arekindly acknowledged for providing their expertise aslinguists in the manual evaluation process.6ReferencesAduriz, I., I. Aldezabal, I. Alegria, X. Artola,N.
Ezeiza, and R. Urizar (1996).
EUSLEM:A lemmatiser/tagger for Basque.
Proc.
of EU-RALEX?96, 17?26.Aldezabal, I., O. Ansa, B. Arrieta, X. Artola,A.
Ezeiza, G. Herna?ndez, and M. Lersundi(2001).
EDBL: A general lexical basis for the au-tomatic processing of Basque.
In IRCS Workshopon linguistic databases, pp.
1?10.Baldwin, T. and S. Kim (2010).
Multiword expres-sions.
Handbook of Natural Language Process-ing, second edition.
Morgan and Claypool.Banerjee, S. and T. Pedersen (2010).
The design,implementation, and use of the Ngram StatisticsPackage.
Computational Linguistics and Intelli-gent Text Processing, 370?381.Church, K. and P. Hanks (1990).
Word associa-tion norms, mutual information, and lexicogra-phy.
Computational linguistics 16(1), 22?29.Cowie, A.
(1998).
Phraseology: Theory, analysis,and applications.
Oxford University Press, USA.Evert, S. (2005).
The statistics of word cooccur-rences: Word pairs and collocations.
Ph.
D. the-sis, University of Stuttgart.Evert, S. and B. Krenn (2005).
Using small ran-dom samples for the manual evaluation of statis-tical association measures.
Computer Speech &Language 19(4), 450?466.Fazly, A., P. Cook, and S. Stevenson (2009).
Un-supervised type and token identification of id-iomatic expressions.
Computational Linguis-tics 35(1), 61?103.Fazly, A. and S. Stevenson (2007).
Distinguish-ing subtypes of multiword expressions usinglinguistically-motivated statistical measures.
InProceedings of the Workshop on A Broader Per-spective on Multiword Expressions, pp.
9?16.
As-sociation for Computational Linguistics.Granger, S. and M. Paquot (2008).
Disentanglingthe phraseological web.
Phraseology.
An inter-disciplinary perspective, 27?50.Krenn, B.
(1999).
The usual suspects: Data-oriented models for identification and represen-tation of lexical collocations.
German ResearchCenter for Artificial Intelligence.Krenn, B. and S. Evert (2001).
Can we do better thanfrequency?
A case study on extracting PP-verbcollocations.
In Proceedings of the ACL Work-shop on Collocations, pp.
39?46.Krenn, B., S. Evert, and H. Zinsmeister (2004).
De-termining intercoder agreement for a collocationidentification task.
In Proceedings of KONVENS,pp.
89?96.Pearce, D. (2002).
A comparative evaluation of col-location extraction techniques.
In Proc.
of LREC2002, pp.
1530?1536.Pecina, P. (2005).
An extensive empirical study ofcollocation extraction methods.
In Proceedings ofthe ACL Student Research Workshop, pp.
13?18.Association for Computational Linguistics.Pecina, P. and P. Schlesinger (2006).
Combining as-sociation measures for collocation extraction.
pp.651?658.Pe?rez Gaztelu, E., I. Zabala, and L. Gra?cia (2004).Las fronteras de la composicio?n en lenguasroma?nicas y en vasco.
San Sebastia?n: Universi-dad de Deusto.Sarasola, I.
(1996).
Euskal Hiztegia.
Kutxa Fun-dazioa / Fundacio?n Kutxa.Seretan, V. (2008).
Collocation extraction based onsyntactic parsing.
Ph.
D. thesis, University ofGeneva.Sinclair, J.
(1996).
The search for units of meaning.Textus 9(1), 75?106.Smadja, F. (1993).
Retrieving collocations fromtext: Xtract.
Computational linguistics 19(1),143?177.Street, L., N. Michalov, R. Silverstein, M. Reynolds,L.
Ruela, F. Flowers, A. Talucci, P. Pereira,G.
Morgon, S. Siegel, M. Barousse, A. Anderson,T.
Carroll, and A. Feldman (2010).
Like finding aneedle in a haystack: Annotating the american na-tional corpus for idiomatic expressions.
In Proc.of LREC 2010, Valletta, Malta.Wulff, S. (2010).
Rethinking Idiomaticity.
Corpusand Discourse.
New York: Continuum Interna-tional Publishing Group Ltd.7
