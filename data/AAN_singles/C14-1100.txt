Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1059?1070, Dublin, Ireland, August 23-29 2014.Automatic Classification of Communicative Functions of DefinitenessArchna Bhatia?,?Chu-Cheng Lin?Nathan Schneider?Yulia Tsvetkov?Fatima Talib Al-Raisi?Laleh Roostapour?Jordan Bender?Abhimanu Kumar?Lori Levin?Mandy Simons?Chris Dyer?
?Carnegie Mellon University?University of PittsburghPittsburgh, PA 15213 Pittsburgh, PA 15260?archnab@cs.cmu.eduAbstractDefiniteness expresses a constellation of semantic, pragmatic, and discourse properties?thecommunicative functions?of an NP.
We present a supervised classifier for English NPs thatuses lexical, morphological, and syntactic features to predict an NP?s communicative function interms of a language-universal classification scheme.
Our classifiers establish strong baselines forfuture work in this neglected area of computational semantic analysis.
In addition, analysis ofthe features and learned parameters in the model provides insight into the grammaticalization ofdefiniteness in English, not all of which is obvious a priori.1 IntroductionDefiniteness is a morphosyntactic property of noun phrases (NPs) associated with semantic and pragmaticcharacteristics of entities and their discourse status.
Lyons (1999), for example, argues that definitemarkers prototypically reflect identifiability (whether a referent for the NP can be identified by thediscourse participants or not); other aspects identified in the literature include uniqueness of the entityin the world and whether the hearer is already familiar with the entity given the context and precedingdiscourse (Roberts, 2003; Abbott, 2006).
While some morphosyntactic forms of definiteness are employedby all languages?namely, demonstratives, personal pronouns, and possessives?languages display a vastrange of variation with respect to the form and meaning of definiteness.
For example, while languageslike English make use of definite and indefinite articles to distinguish between the discourse status ofvarious entities (the car vs. a car vs. cars), many other languages?including Czech, Indonesian, andRussian?do not have articles (although they do have demonstrative determiners).
Sometimes definitenessis marked with affixes or clitics, as in Arabic.
Sometimes it is expressed with other constructions, as inChinese (a language without articles), where the existential construction can be used to express indefinitesubjects and the ba- construction can be used to express definite direct objects (Chen, 2004).Aside from this variation in the form of (in)definite NPs within and across languages, there is also vari-ability in the mapping between semantic, pragmatic, and discourse functions of NPs and the (in)definitesexpressing these functions.
We refer to these as communicative functions of definiteness, followingBhatia et al.
(2014).
Croft (2003, pp.
6?7) shows that even when two languages have access to thesame morphosyntactic forms of definiteness, the conditions under which an NP is marked as definiteor indefinite (or not at all) are language-specific.
He illustrates this by contrasting English and Frenchtranslations (both languages use definite as well as indefinite articles) such as:(1) He showed extreme care.
(unmarked)Il montra un soin extr?me.
(indef.
)(2) I love artichokes and asparagus.
(unmarked)J?aime les artichauts et les asperges.
(def.
)(3) His brother became a soldier.
(indef.
)Son fr?re est devenu soldat.
(unmarked)This work is licensed under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organisers.
License details: http://creativecommons.org/licenses/by/4.0/1059?
NONANAPHORA [?A,?B] 999?
UNIQUE [+U] 287*UNIQUE_HEARER_OLD [+F,?G,+S] 251?
UNIQUE_PHYSICAL_COPRESENCE [+R] 13?
UNIQUE_LARGER_SITUATION [+R] 237?
UNIQUE_PREDICATIVE_IDENTITY [+P] 1*UNIQUE_HEARER_NEW [?F] 36?
NONUNIQUE [?U] 581*NONUNIQUE_HEARER_OLD [+F] 169?
NONUNIQUE_PHYSICAL_COPRESENCE [?G,+R,+S] 39?
NONUNIQUE_LARGER_SITUATION [?G,+R,+S] 117?
NONUNIQUE_PREDICATIVE_IDENTITY [+P] 13*NONUNIQUE_HEARER_NEW_SPEC [?F,?G,+R,+S] 231*NONUNIQUE_NONSPEC [?G,?S] 181?
GENERIC [+G,?R] 131*GENERIC_KIND_LEVEL 0*GENERIC_INDIVIDUAL_LEVEL 131?
ANAPHORA [+A] 1574?
BASIC_ANAPHORA [?B,+F] 795*SAME_HEAD 556*DIFFERENT_HEAD 329?
EXTENDED_ANAPHORA [+B] 779*BRIDGING_NOMINAL [?G,+R,+S] 43*BRIDGING_EVENT [+R,+S] 10*BRIDGING_RESTRICTIVE_MODIFIER [?G,+S] 614*BRIDGING_SUBTYPE_INSTANCE [?G] 0*BRIDGING_OTHER_CONTEXT [+F] 112?
MISCELLANEOUS [?R] 732?
PLEONASTIC [?B,?P] 53?
QUANTIFIED 248?
PREDICATIVE_EQUATIVE_ROLE [?B,+P] 58?
PART_OF_NONCOMPOSITIONAL_MWE 100?
MEASURE_NONREFERENTIAL 125?
OTHER_NONREFERENTIAL 148+ ?
0 + ?
0 + ?
0 + ?
0Anaphoric 1574 999 732 Generic 131 1476 1698 Predicative 72 53 3180 Specific 1305 181 1819Bridging 779 1905 621 Familiar 1327 267 1711 Referential 690 863 1752 Unique 287 581 2437Figure 1: CFD (Communicative Functions of Definiteness) annotation scheme, with frequencies in thecorpus.
Internal (non-leaf) labels are in bold; these are not annotated or predicted.
+/?
values are shownfor ternary attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, andUnique; these are inherited from supercategories, but otherwise default to 0.
Thus, for example, thefull attribute specification for UNIQUE_PHYSICAL_COPRESENCE is [?A,?B,+F,?G,0P,+R,+S,+U].Counts for these attributes are shown in the table at bottom.A cross-linguistic classification of communicative functions should be able to characterize the aspectsof meaning that account for the different patterns of definiteness marking exhibited in (1?3): e.g., that(2) concerns a generic class of entities while (3) concerns a role filled by an individual.
For more oncommunicative functions, see ?2.This paper develops supervised classifiers to predict communicative function labels for English NPsusing lexical, morphological, and syntactic features.
The contribution of our work is in both the output ofthe classifiers and the models themselves (features and weights).
Each classifier predicts communicativefunction labels that capture aspects of discourse-newness, uniqueness, specificity, and so forth.
Suchfunctions are useful in a variety of language processing applications.
For example, they should usually bepreserved in translation, even when the grammatical mechanisms for expressing them are different.
Thecommunicative function labels also represent the discourse status of entities, making them relevant forentity tracking, knowledge base construction, and information extraction.Our log-linear model is a form-meaning mapping that relates syntactic, lexical, and morphologicalfeatures to properties of communicative functions.
The learned weights of this model can, e.g., gener-ate plausible hypotheses regarding the form-meaning relationship which can then be tested rigorouslythrough controlled experiments.
This hypothesis generation is linguistically significant as it indicates newgrammatical mechanisms beyond the obvious a and the articles that are used for expressing definitenessin English.To build our models, we leverage a cross-lingual definiteness annotation scheme (?2) and annotatedEnglish corpus (?3) developed in prior work (Bhatia et al., 2014).
The classifiers, ?4, are supervisedmodels with features that combine lexical and morphosyntactic information and the prespecified attributesor groupings of the communicative function labels (such as Anaphoric, Bridging, Specific in fig.
1) topredict leaf labels (the non-bold faced labels in fig.
1); the evaluation measures (?5) include one thatexploits these label groupings to award partial credit according to relatedness.
?6 presents experimentscomparing several models and discussing their strengths and weaknesses; computational work andapplications related to definiteness are addressed in ?7.10602 Annotation schemeThe literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoric-ity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundelet al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell,1905, inter alia) as being related to definiteness.
Reductionist approaches to definiteness try to defineit in terms of one or two of the aforementioned communicative functions.
For example, Roberts (2003)proposes that the combination of uniqueness and a presupposition of familiarity underlie all definitedescriptions.
However, possessive definite descriptions (John?s daughter) and the weak definites (the sonof Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listener before theyare spoken.
In contrast to the reductionist approaches are approaches to grammaticalization (Hopper andTraugott, 2003) in which grammar develops over time in such a way that each grammatical constructionhas some prototypical communicative functions, but may also have many non-prototypical communica-tive functions.
The scheme we are adopting for this work?the annotation scheme for CommunicativeFunctions of Definiteness (CFD) as described in Bhatia et al.
(2014)?assumes that there may be multiplefunctions to definiteness.
CFD is based on a combination of these functions and is summarized in fig.
1.
Itwas developed by annotating texts in two languages (English and Hindi) for four different genres?namelyTED talks, a presidential inaugural speech, news articles, and fictional narratives?keeping in mind thecommunicative functions that have been associated with definiteness in the linguistic literature.CFD is hierarchically organized.
This hierarchical organization serves to reduce the number of decisionsthat an annotator needs to make for speed and consistency.
We now highlight some of the major distinctionsin the hierarchy.At the highest level, the distinction is made between Anaphora, Nonanaphora, and Miscellaneousfunctions of an NP (the annotatable unit).
Anaphora and Nonanaphora respectively describe whetheran entity is old or new in the discourse; the Miscellaneous function is mainly assigned to various kinds ofnonreferential NPs.The Anaphora category has two subcategories: Basic_Anaphora and Extended_Anaphora.
Ba-sic_Anaphora applies to NPs referring to entities that have been mentioned before.
Extended_Anaphoraapplies to any NP whose referent has not been mentioned itself, but is evoked by a previously mentionedentity.
For example, after mentioning a wedding, the bride, the groom, and the cake are considered to beExtended_Anaphora.Within the Nonanaphora category, a first distinction is made between Unique, Nonunique, andGeneric.
The Unique function applies to NPs whose referent becomes unique in a context for any ofseveral reasons.
For example, Obama can safely be considered unique in contemporary political discoursein the United States.
The function Nonunique applies to NPs that start out with multiple possible referentsand that may or may not become identifiable in a speech situation.
For example, a little riding hood ofred velvet in fig.
2 could be annotated with the label Nonunique.
Finally, Generic NPs refer to classesor types of entities rather than specific entities.
For example, Dinosaurs in Dinosaurs are extinct.
is aGeneric NP.Another important distinction CFD makes is between Hearer_Old for references to entities that arefamiliar to the hearer (e.g., if they are physically present in the speech situation), versus Hearer_Newfor nonfamiliar references.
This distinction cuts across the two subparts of the hierarchy, Anaphoraand Nonanaphora; thus, labels marking Hearer_Old or Hearer_New also encode other distinctions(e.g., Unique_Hearer_Old, Unique_Hearer_New, Nonunique_Hearer_Old).
For further details onthe annotation scheme, see fig.
1 and Bhatia et al.
(2014).Because the ordering of distinctions determines the tree structure of the hierarchy, the same commu-nicative functions could have been organized in a superficially different way.
In fact, Komen (2013) hasproposed a hierarchy with similar leaf nodes, but different internal structure.
Since it is possible thatsome natural groupings of labels are not reflected in the hierarchy we used, we also decompose eachlabel into fundamental communicative functions, which we call attributes.
Each label type is associatedwith values for attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, andUnique.
These attributes can have values of +, ?, or 0, as shown in fig.
1.
For instance, with the Anaphoric1061Once upon a time there was a dear little girl who was loved by everyone who looked at her, but most of all by her grandmother,and there was nothing that she would not have given to the child.Once sheSAME_HEADgave herDIFFERENT_HEADa little riding hood of red velvetOTHER_NONREFERENTIALNONUNIQUE_HEARER_NEW_SPEC, which suited herSAME_HEADso well thatsheSAME_HEADwould never wear anything elseQUANTIFIED; so sheSAME_HEADwas always called ?Little Red Riding HoodUNIQUE_HEARER_NEW.
?Figure 2: An annotated sentence from ?Little Red Riding Hood.?
The previous sentence is shown forcontext.attribute, a value of + applies to labels that can never mark NPs new to the discourse, ?
applies to labelsthat can only apply if the NP is new in the discourse, and 0 applies to labels such as Pleonastic (whereanaphoricity is not applicable because there is no discourse referent).3 DataWe use the English definiteness corpus of Bhatia et al.
(2014), which consists of texts from multiple genresannotated with the scheme described in ?2.1The 17 documents consist of prepared speeches (TED talksand a presidential address), published news articles, and fictional narratives.
The TED data predominates(75% of the corpus);2the presidential speech represents about 16%, fictional narratives 5%, and newsarticles 4%.
All told, the corpus contains 13,860 words (868 sentences), with 3,422 NPs (the annotatableunits).
Bhatia et al.
(2014) report high inter-annotator agreement, estimating Cohen?s ?
= 0.89 within theTED genre as well as for all genres.Figure 2 is an excerpt from the ?Little Red Riding Hood?
annotated with the CFD scheme.4 Classification frameworkTo model the relationship between the grammar of definiteness and its communicative functions in adata-driven fashion, we work within the supervised framework of feature-rich discriminative classification,treating the functional categories from ?2 as output labels y and various lexical, morphological, andsyntactic characteristics of the language as features of the input x.
Specifically, we learn two kindsof probabilistic models.
The first is a log-linear model similar to multiclass logistic regression, butdeviating in that logistic regression treats each output label (response) as atomic, whereas we decomposeeach into attributes based on their linguistic definitions, enabling commonalities between related labelsto be recognized.
Each weight in the model corresponds to a feature that mediates between percepts(characteristics of the input NP) and attributes (characteristics of the label).
This is aimed at attainingbetter predictive accuracy as well as feature weights that better describe the form?function interactions weare interested in recovering.
We also train a random forest model on the hypothesis that it would allow usto sacrifice interpretability of the learned parameters for predictive accuracy.Our setup is formalized below, where we discuss the mathematical models and linguistically motivatedfeatures.4.1 ModelsWe experiment with two classification methods: a log-linear model and a nonlinear tree-based ensemblemodel.
Due to their consistency and interpretability, linear models are a valuable tool for quantifying andanalyzing the effects of individual features.
Non-linear models, while less interpretable, often outperformlogistic regression (Perlich et al., 2003), and thus could be desirable when the predictions are needed for adownstream task.1The data can be obtained from http://www.cs.cmu.edu/~ytsvetko/definiteness_corpus.2The TED talks are from a large parallel corpus obtained from http://www.ted.com/talks/.10624.1.1 Log-linear modelAt test time, we model the probability of communicative function label y conditional on an NP x asfollows:p?
(y?x) = logexp??f(x,y)?y?
?Y exp??f(x,y?
)(1)where ?
?Rd is a vector of parameters (feature weights), and f ?X ?Y ?Rd is the feature function overinput?label pairs.
The feature function is defined as follows:f(x,y) = ?
(x)?
??
(y) (2)where the percept function ?
?X ?Rc produces a vector of real-valued characteristics of the input, andthe attribute function??
?Y ?
{0,1}a encodes characteristics of each label.
There is a feature for everypercept?attribute pairing: so d = c ?a and f(i?1)a+ j(x,y) = ?i(x) ??
j(y),1 ?
i ?
c,1 ?
j ?
a.3The contents ofthe percept and attribute functions are detailed in ?4.2 and ?4.3 respectively.For prediction, having learned weights??
we use the Bayes-optimal decision rule for minimizingmisclassification error, selecting the y that maximizes this probability:y??
argmaxy?Yp??
(y?x) (3)Training optimizes??
so as to maximize a convex L2-regularized4learning objective over the training dataD:??
= argmax???
???
?
?22+ ??x,y??Dlogexp??f(x,y)?y?
?Y exp(??f(x,y?))(4)With??
(y) = the identity of the label, this reduces to standard logistic regression.4.1.2 Non-linear modelWe employ a random forest classifier (Breiman, 2001), an ensemble of decision tree classifiers learnedfrom many independent subsamples of the training data.
Given an input, each tree classifier assigns aprobability to each label; those probabilities are averaged to compute the probability distribution acrossthe ensemble.An important property of the random forests, in addition to being an effective tool in prediction, istheir immunity to overfitting: as the number of trees increases, they produce a limiting value of thegeneralization error.5Thus, no hyperparameter tuning is required.
Random forests are known to berobust to sparse data and to label imbalance (Chen et al., 2004), both of which are challenges with thedefiniteness dataset.4.2 PerceptsThe characteristics of the input that are incorporated in the model, which we call percepts to distinguishthem from model features linking inputs to outputs, see ?4.1, are intended to capture the aspects of Englishmorphosyntax that may be relevant to the communicative functions of definiteness.After preprocessing the text with a dependency parser and coreference resolver, which is described in?6.1, we extract several kinds of percepts for each NP.4.2.1 BasicWords of interest.
These are the head within the NP, all of its dependents, and its governor (external tothe NP).
We are also interested in the attached verb, which is the first verb one encounters when traversingthe dependency path upward from the head.
For each of these words, we have separate percepts capturing:the token, the part-of-speech (POS) tag, the lemma, the dependency relation, and (for the head only) a3Chahuneau et al.
(2013) use a similar parametrization for their model of morphological inflection.4As is standard practice with these models, bias parameters (which capture the overall frequency of percepts/attributes) areexcluded from regularization.5See Theorem 1.2 in Breiman (2001) for details.1063binary indicator of plurality (determined from the POS tag).
As there may be multiple dependents, wehave additional features specific to the first and the last one.
Moreover, to better capture tense, aspectand modality, we collect the attached verb?s auxiliaries.
We also make note of the negative particle (withdependency label neg) if it is a dependent of the verb.Structural.
The structural percepts are: the path length from the head up to the root, and to the attachedverb.
We also have percepts for the number of dependents, and the number of dependency relations thatlink non-neighbors.
Integer values were binarized with thresholding.Positional.
These percepts are the token length of the NP, the NP?s location in the sentence (first orsecond half), and the attached verb?s position relative to the head (left or right).
12 additional percepttemplates record the POS and lemma of the left and right neighbors of the head, governor, and attachedverb.4.2.2 Contextual NPsWhen extracting features for a given NP (call it the ?target?
), we also consider NPs in the followingrelationship with the target NP: its immediate parent, which is the smallest NP whose span fully subsumesthat of the target; the immediate child, which is the largest NP subsumed within the target; the immediateprecedent and immediate successor within the sentence; and the nearest preceding coreferent mention.For each of these related NPs, we include all of their basic percepts conjoined with the nature of therelation to the target.4.3 AttributesAs noted above, though CFD labels are organized into a tree hierarchy, there are actually several dimensionsof commonality that suggest different groupings.
These attributes are encoded as ternary characteristics;for each label (including internal labels), every one of the 8 attributes is assigned a value of +, ?, or 0(refer to fig.
1).
In light of sparse data, we design features to exploit these similarities via the attributevector function?
(y) = [y,A(y),B(y),F(y),G(y),P(y),R(y),S(y),U(y)]?
(5)where A ?Y ?
{+,?,0} returns the value for Anaphoric, B(y) for Bridging, etc.
The identity of the labelis also included in the vector so that different labels are always recognized as different by the attributefunction.
The categorical components of this vector are then binarized to form??
(y); however, insteadof a binary component that fires for the 0 value of each ternary attribute, there is a component that firesfor any value of the attribute?a sort of bias term.
The weights assigned to features incorporating + or ?attribute values, then, are easily interpreted as deviations relative to the bias.5 EvaluationThe following measures are used to evaluate our predictor against the gold standard for the held-outevaluation (dev or test) set E :?
Exact Match: This accuracy measure gives credit only where the predicted and gold labels are identical.?
By leaf label: We also compute precision and recall of each leaf label to determine which categoriesare reliably predicted.?
Soft Match: This accuracy measure gives partial credit where the predicted and gold labels arerelated.
It is computed as the proportion of attributes-plus-full-label whose (categorical) values match:??(y)??(y?
)?/9.6 Experiments6.1 Experimental SetupData splits.
The annotated corpus of Bhatia et al.
(2014) (?3) contains 17 documents in 3 genres:13 prepared speeches (mostly TED talks),62 newspaper articles, and 2 fictional narratives.
We arbitrarilychoose some documents to hold out from each genre; the resulting test set consists of 2 TED talks6We have combined the TED talks and presidential speech genres since both involved prepared speeches.1064Condition ??
?
?
Exact Match Acc.
Soft Match Acc.Majority baseline ?
?
12.1 47.8Log-linear classifier, attributes only 473,064 100 38.7 77.1Log-linear classifier, labels only 413,931 100 40.8 73.6Full log-linear classifier (labels + attributes) 926,417 100 43.7 78.2Random forest classifier 20,363 ?
49.7 77.5Table 1: Classifiers and baseline, as measured on the test set.
The first two columns give the number ofparameters and the tuned regularization hyperparameter, respectively; the third and fourth columns giveaccuracies as percentages.
The best in each column is bolded.
(?Alisa_News?, ?RobertHammond_park?
), 1 newspaper article (?crime1_iPad_E?
), and 1 narrative(?Little Red Riding Hood?).
The test set then contains 19,28 tokens (111 sentences), in which there are511 annotated NPs; while the training set contains 2,911 NPs among 11,932 tokens (757 sentences).Preprocessing.
Automatic dependency parses and coreference information were obtained with theparser and coreference resolution system in Stanford CoreNLP v. 3.3.0 (Socher et al., 2013; Recasenset al., 2013) for use in features (?4.2).
Syntactic features were extracted from the Basic dependenciesoutput by the parser.
To evaluate the performance of Stanford system on our data, we manually inspectedthe dependencies and coreference information for a subset of sentences from our corpus (using textsfrom TED talks and fictional narratives genres) and recorded the errors.
We found that about 70% of thesentences had all correct dependencies, and only about 0.04% of the total dependencies were incorrectfor our data.
However, only 62.5% of the coreference links were correctly identified by the coreferenceresolver.
The rest of them were either missing or incorrectly identified.
We believe this may have caused aportion of the classifier errors while predicting the Ananphoic labels.Throughout our experiments (training as well as testing), we use the gold NP boundaries identified bythe human annotators.
The automatic dependency parses are used to extract percepts for each gold NP.If there is a conflict between the gold NP boundaries and the parsed NP boundaries, to avoid extractingmisleading percepts, we assign a default value.Learning.
The log-linear model variants are trained with an in-house implementation of supervisedlearning with L2-regularized AdaGrad (Duchi et al., 2011).
Hyperparameters are tuned on a developmentset formed by holding out every tenth instance from the training set (test set experiments use the fulltraining set): the power of 10 giving the highest Soft Match accuracy was chosen for ?
.7The Pythonscikit-learn toolkit (Pedregosa et al., 2011) was used for the random forest classifier.86.2 ResultsMeasurements of overall classification performance appear in table 1.
While far from perfect, ourclassifiers achieve promising accuracy levels given the small size of the training data and the number oflabels in the annotation scheme.
The random forest classifier is the most accurate in Exact Match, likelydue to the robustness of that technique under conditions where the data are small and the frequenciesof individual labels are imbalanced.
By the Soft Match measure, our attribute-aware log-linear modelsperform very well.
The most successful of the log-linear models is the richest model, which combines thefine-grained communicative function labels with higher-level attributes of those labels.
But notably theattribute-only model, which decomposes the semantic labels into attributes without directly consideringthe full label, performs almost as well as the random forest classifier in Soft Match.
This is encouragingbecause it suggests that the model has correctly exploited known linguistic generalizations to account forthe grammaticalization of definiteness in English.Table 2 reports the precision and recall of each leaf label predicted.
Certain leaf labels are foundto be easier for the classifier to predict: e.g., the communicative function label Pleonastic has a highF1score.
This is expected as the Ploenastic CFD for English is quite regular and captured by the EX7Preliminary experiments with cross-validation on the training data showed that the value of ?
was stable across folds.8Because it is a randomized algorithm, the results may vary slightly between runs; however, a cross-validation experiment onthe training data found very little variance in accuracy.1065Leaf label N P R F1Leaf label N P R F1Pleonastic 44 100 78 88 Part_of_Noncompositional_MWE 88 20 17 18Bridging_Restrictive_Modifier 552 58 84 68 Bridging_Nominal 33 33 10 15Quantified 213 57 57 57 Generic_Individual_Level 113 14 11 13Unique_Larger_Situation 97 52 58 55 Nonunique_Nonspec 173 9 25 13Same_Head 452 41 41 41 Bridging_Other_Context 96 33 6 11Measure_Nonreferential 98 88 26 40 Bridging_Event 9 ?
0 ?Nonunique_Hearer_New_Spec 190 36 46 40 Nonunique_Physical_Copresence 36 0 0 ?Other_Nonreferential 134 39 36 37 Nonunique_Predicative_Identity 10 ?
0 ?Different_Head 271 32 33 32 Predicative_Nonidentity 57 0 0 ?Nonunique_Larger_Situation 97 29 25 27 Unique_Hearer_New 26 ?
0 ?Table 2: Number of training set instances and precision, recall, and F1percentages for leaf labels.part-of-speech tag.
The classifier finds predictions of certain CFD labels, such as Bridging_Event,Bridging_Nominal and Nonunique_Nonspecific, to be more difficult due to data sparseness: it appearsthat there were not enough training instances for the classifier to learn the generalizations correspondingto these CFDs.
Bridging_Other_Context was hard to predict as this was a category which referred notto the entities previously mentioned but to the whole speech event from the past.
There seem to be noclear morphosyntactic cues associated with this CFD, so to train a classifier to predict this category label,we would need to model more complex semantic and discourse information.
This also applies to theclassifier confusion between the Same_Head and Different_Head, since both of these labels share allthe semantic attributes used in this study.An advantage of log-linear models is that inspecting the learned feature weights can provide usefulinsights into the model?s behavior.
Figure 3 lists 10 features that received the highest positive weightsin the full model for the + and ?
values of the Specific attribute.
These confirm some known propertiesof English definites and indefinites.
The definite article, possessives (PRP$), proper nouns (NNP), and thesecond person pronoun are all associated with specific NPs, while the indefinite article is associated withnonspecific NPs.
The model also seems to have picked up on the less obvious but well-attested tendencyof objects to be nonspecific (Aissen, 2003).In addition to confirming known grammaticalization patterns of definiteness, we can mine the highly-weighted features for new hypotheses: e.g., in figs.
3 and 4, the model thinks that objects of ?from?
areespecially likely to be Specific, and that NPs with comparative adjectives (JJR) are especially likely to benonspecific (fig.
3).
From fig.
3, we also know that Num.
of dependents, dependent?s POS: 1,PRP$ hasa higher weight than, say, Num.
of dependents, dependent?s POS: 2,PRP$.
This observation suggests ahypothesis that in English the NPs which have possessive pronouns immediately preceding the head aremore likely to be specific than the NPs which have intervening words between the possessive pronounand the head.
Similarly, looking at another example in fig.
4, the following two percepts get high weightsfor the NP the United States of America to be Specific: last dependent?s POS: NNP and first dependent?slemma: the.
Since frequency and other factors affect the feature weights learned by the classifier, thesedifferences in weights may or may not reflect an inherent association with Specificity.
Whether theseare general trends, or just an artifact of the sentences that happened to be in the training data and ourstatistical learning procedure, will require further investigation, ideally with additional datasets and morerigorous hypothesis testing.Finally, we can remove features to test their impact on predictive performance.
Notably, in experimentsablating features indicating articles?the most obvious exponents of definiteness in English?we seea decrease in performance, but not a drastic one.
This suggests that the expression of communicativefunctions of definiteness is in fact much richer than morphological definiteness.Errors.
Several labels are unattested or virtually unattested in the training data, so the models unsurpris-ingly fail to predict them correctly at test time.
Same_Head and Different_Head, though both common,are confused quite frequently.
Whether the previous coreferent mention has the same or different head is asimple distinction for humans; low model accuracy is likely due to errors propagated from coreferenceresolution.
This problem is so frequent that merging these two categories and retraining the randomforest model improves Exact Match accuracy by 8% absolute and Soft Match accuracy by 5% absolute.1066Percepts+Specific ?SpecificFirst dependent?s POS PRP$ First dependent?s lemma aHead?s left neighbor?s POS PRP$ Last dependent?s lemma aLast dependent?s lemma you Num.
of dependents, dependent?s lemma 1,aNum.
of dependents, dependent?s lemma 1,you Head?s left neighbor?s POS JJRNum.
of dependents, dependent?s POS 1,PRP$ Last dependent?s POS JJRGovernor?s right neighbor?s POS PRP$ Num.
of dependents, dependent?s lemma 2,aLast dependent?s POS NNP First dependent?s lemma newLast dependent?s POS PRP$ Last dependent?s lemma newFirst dependent?s lemma the Num.
of dependents, dependent?s POS 2,JJRGovernor?s lemma from Governor?s left neighbor?s POS VBFigure 3: Percepts receiving highest positive weights in association with values of the Specific attribute.Example Relevant percepts from fig.
3 CFD annotationThis is just for the United States of America.
Last dependent?s POS: NNPFirst dependent?s lemma: theUnique_Larger_SituationWe were driving from our home in Nashvilleto a little farm we have 50 miles east ofNashville ?
driving ourselves.First dependent?s POS: PRP$Head?s left neighbor?s POS: PRP$Governor?s right neighbor?s POS: PRP$Governor?s lemma: fromBridging_Restrictive_ModifierFigure 4: Sentences from our corpus illustrating percepts fired for gold NPs and their CFD annotations.Another common confusion is between the highly frequent category Unique_Larger_Situation and therarer category Unique_Hearer_New; the latter is supposed to occur only for the first occurrence of aproper name referring to a entity that is not already part of the knowledge of the larger community.
Inother words, this distinction requires world knowledge about well-known entities, which could perhaps bemined from the Web or other sources.7 Related WorkBecause semantic/pragmatic analysis of referring expressions is important for many NLP tasks, a compu-tational model of the communicative functions of definiteness has the potential to leverage diverse lexicaland grammatical cues to facilitate deeper inferences about the meaning of linguistic input.
We have useda coreference resolution system to extract features for modeling definiteness, but an alternative would beto predict definiteness functions as input to (or jointly with) the coreference task.
Applications such asinformation extraction and dialogue processing could be expected to benefit not only from coreferenceinformation, but also from some of the semantic distinctions made in our framework, including specificityand genericity.Better computational processing of definiteness in different languages stands to help machine translationsystems.
It has been noted that machine translation systems face problems when the source and the targetlanguage use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkovet al., 2013).
Previous work on machine translation has attempted to deal with this in terms of either(a) preprocessing the source language to make it look more like the target language (Collins et al., 2005;Habash, 2007; Nie?en and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machinetranslation output to match the target language, (e.g., Popovi?c et al., 2006).
Attempts have also been madeto use syntax on the source and/or the target sides to capture the syntactic differences between languages(Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007).
Automated prediction of (in)definitearticles has been found beneficial in a variety of applications, including postediting of MT output (Knightand Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correctionof ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010).
More recently, Tsvetkov et al.
(2013)trained a classifier to predict where English articles might plausibly be added or removed in a phrase, andused this classifier to improve the quality of statistical machine translation.While definiteness morpheme prediction has been thoroughly studied in computational linguistics,1067studies on additional, more complex aspects of definiteness are limited.
Reiter and Frank (2010) exploitlinguistically-motivated features in a supervised approach to distinguish between generic and specificNPs.
Hendrickx et al.
(2011) investigated the extent to which a coreference resolution system can resolvethe bridging relations.
Also in the context of coreference resolution, Ng and Cardie (2002) and Konget al.
(2010) have examined anaphoricity detection.
To the best of our knowledge, no studies have beenconducted on automatic prediction of semantic and pragmatic communicative functions of definitenessmore broadly.Our work is related to research in linguistics on the modeling of syntactic constructions such as dativeshift and the expression of possession with ?of?
or ??s?.
Bresnan and Ford (2010) used logistic regressionwith semantic features to predict syntactic constructions.
Although we are doing the opposite (usingsyntactic features to predict semantic categories), we share the assumption that reductionist approaches (asmentioned earlier) are not able to capture all the nuances of a linguistic phenomenon.
Following Hopperand Traugott (2003) we observe that grammaticalization is accompanied by function drift, resulting inmultiple communicative functions for each grammatical construction.
Other attempts have also been madeto capture, using classifiers, (propositional as well as non propositional) aspects of meaning that havebeen grammaticalized: see, for instance, Reichart and Rappoport (2010) for tense sense disambiguation,Prabhakaran et al.
(2012) for modality tagging, and Srikumar and Roth (2013) for semantics expressed byprepositions.8 ConclusionWe have presented a data-driven approach to modeling the relationship between universal communicativefunctions associated with (in)definiteness and their lexical/grammatical realization in a particular language.Our feature-rich classifiers can give insights into this relationship as well as predict communicativefunctions for the benefit of NLP systems.
Exploiting the higher-level semantic attributes, our log-linearclassifier compares favorably to the random forest classifier in Soft Match accuracy.
Further improvementsto the classifier may come from additional features or better preprocessing.
This work has focused onEnglish, but in future work we plan to build similar models for other languages?including languageswithout articles, under the hypothesis that such languages will rely on other, subtler devices to encodemany of the functions of definiteness.AcknowledgmentsThis work was sponsored by the U. S. Army Research Laboratory and the U. S. Army Research Officeunder contract/grant number W911NF-10-1-0533.
We thank the reviewers for their useful comments.ReferencesBarbara Abbott.
2006.
Definite and indefinite.
In Keith Brown, editor, Encyclopedia of Language and Linguistics,pages 3?392.
Elsevier.Judith Aissen.
2003.
Differential object marking: iconicity vs. economy.
Natural Language & Linguistic Theory,21(3):435?483.Archna Bhatia, Mandy Simons, Lori Levin, Yulia Tsvetkov, Chris Dyer, and Jordan Bender.
2014.
A unified anno-tation scheme for the semantic/pragmatic components of definiteness.
In Proc.
of LREC.
Reykjav?k, Iceland.Betty Birner and Gregory Ward.
1994.
Uniqueness, familiarity and the definite article in English.
In Proc.
of theTwentieth Annual Meeting of the Berkeley Linguistics Society, pages 93?102.Leo Breiman.
2001.
Random forests.
Machine Learning, 45(1):5?32.Joan Bresnan and Marilyn Ford.
2010.
Predicting syntax: Processing dative constructions in American and Aus-tralian varieties of English.
Language, 86(1):168?213.Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer.
2013.
Translating into morphologically richlanguages with synthetic phrases.
In Proc.
of EMNLP, pages 1677?1687.
Seattle, Washington, USA.1068Chao Chen, Andy Liaw, and Leo Breiman.
2004.
Using random forest to learn imbalanced data.
University ofCalifornia, Berkeley.Ping Chen.
2004.
Identifiability and definiteness in Chinese.
Linguistics, 42:1129?1184.Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005.
Clause restructuring for statistical machine translation.In Proc.
of ACL, pages 531?540.
Ann Arbor, Michigan.Cleo Condoravdi.
1992.
Strong and weak novelty and familiarity.
In Proc.
of SALT II, pages 17?37.William Croft.
2003.
Typology and Universals.
Cambridge University Press.John Duchi, Elad Hazan, and Yoram Singer.
2011.
Adaptive subgradient methods for online learning and stochasticoptimization.
Journal of Machine Learning Research, 12(Jul):2121?2159.Michael Elhadad.
1993.
Generating argumentative judgment determiners.
In Proc.
of AAAI, pages 344?349.Gareth Evans.
1977.
Pronouns, quantifiers and relative clauses.
Canadian Journal of Philosophy, 7(3):46.Gareth Evans.
1980.
Pronouns.
Linguistic Inquiry, 11.Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.
1988.
The generation and interpretation of demonstrativeexpressions.
In Proc.
of XIIth International Conference on Computational Linguistics, pages 216?221.Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.
1993.
Cognitive status and the form of referring expres-sions in discourse.
Language, 69:274?307.Nizar Habash.
2007.
Syntactic preprocessing for statistical machine translation.
In MT Summit XI, pages 215?222.Copenhagen.Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006.
Detecting errors in english article usage by non-nativespeakers.
Natural Language Engineering, 12:115?129.Irene Heim.
1990.
E-type pronouns and donkey anaphora.
Linguistics and Philosophy, 13:137?177.Iris Hendrickx, Orph?e De Clercq, and V?ronique Hoste.
2011.
Analysis and reference resolution of bridgeanaphora across different text genres.
In Iris Hendrickx, Sobha Lalitha Devi, Antonio Horta Branco, and RuslanMitkov, editors, DAARC, volume 7099 of Lecture Notes in Computer Science, pages 1?11.
Springer.Paul J. Hopper and Elizabeth Closs Traugott.
2003.
Grammaticalization.
Cambridge University Press.Nirit Kadmon.
1987.
On unique and non-unique reference and asymmetric quantification.
Ph.D. thesis, Universityof Massachusetts.Nirit Kadmon.
1990.
Uniqueness.
Linguistics and Philosophy, 13:273?324.Kevin Knight and Ishwar Chander.
1994.
Automated postediting of documents.
In Proc.
of the National Conferenceon Artificial Intelligence, pages 779?779.
Seattle, WA.Erwin Ronald Komen.
2013.
Finding focus: a study of the historical development of focus in English.
LOT,Utrecht.Fang Kong, Guodong Zhou, Longhua Qian, and Qiaoming Zhu.
2010.
Dependency-driven anaphoricity determi-nation for coreference resolution.
In Proc.
of COLING, pages 599?607.
Beijing, China.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine translation.In Proc.
of COLING/ACL, pages 609?616.
Sydney, Australia.Christopher Lyons.
1999.
Definiteness.
Cambridge University Press.Guido Minnen, Francis Bond, and Ann Copestake.
2000.
Memory-based learning for article generation.
In Proc.
of1069the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural LanguageLearning, pages 43?48.Vincent Ng and Claire Cardie.
2002.
Identifying anaphoric and non-anaphoric noun phrases to improve coreferenceresolution.
In Proc.
of COLING.
Taipei, Taiwan.Sonja Nie?en and Hermann Ney.
2000.
Improving SMT quality with morpho-syntactic analysis.
In Proc.
ofCOLING, pages 1081?1085.Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-napeau, Matthieu Brucher, M. Perrot, and Edouard Duchesnay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Claudia Perlich, Foster Provost, and Jeffrey S. Simonoff.
2003.
Tree induction vs. logistic regression: a learning-curve analysis.
Journal of Machine Learning Research, 4:211?255.Maja Popovi?c, Daniel Stein, and Hermann Ney.
2006.
Statistical machine translation of German compound words.In Advances in Natural Language Processing, pages 616?624.
Springer.Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, OwenRambow, and Benjamin Van Durme.
2012.
Statistical modality tagging from rule-based annotations and crowd-sourcing.
In Proc.
of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,ExProM ?12, pages 57?64.Ellen F. Prince.
1992.
The ZPG letter: Subjects, definiteness and information status.
In S. Thompson and W. Mann,editors, Discourse description: diverse analyses of a fund raising text, pages 295?325.
John Benjamins.Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts.
2013.
The life and death of discourseentities: identifying singleton mentions.
In Proc.
of NAACL-HLT, pages 627?633.
Atlanta, Georgia, USA.Roi Reichart and Ari Rappoport.
2010.
Tense sense disambiguation: A new syntactic polysemy task.
In Proc.
ofEMNLP, EMNLP ?10, pages 325?334.Nils Reiter and Anette Frank.
2010.
Identifying generic noun phrases.
In Proc.
of ACL, pages 40?49.
Uppsala,Sweden.Craig Roberts.
2003.
Uniqueness in definite noun phrases.
Linguistics and Philosophy, 26:287?350.Alla Rozovskaya and Dan Roth.
2010.
Training paradigms for correcting errors in grammar and usage.
In Proc.of NAACL-HLT, pages 154?162.Bertrand Russell.
1905.
On denoting.
Mind, New Series, 14:479?493.Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng.
2013.
Parsing with compositional vectorgrammars.
In Proc.
of ACL, pages 455?465.
Sofia, Bulgaria.Vivek Srikumar and Dan Roth.
2013.
An inventory of preposition relations.
CoRR, abs/1305.5785.Sara Stymne.
2009.
Definite noun phrases in statistical machine translation into Danish.
In Proc.
of Workshop onExtracting and Using Constructions in NLP, pages 4?9.Yulia Tsvetkov, Chris Dyer, Lori Levi, and Archna Bhatia.
2013.
Generating English determiners in phrase-basedtranslation with synthetic translation options.
In Proc.
of WMT.Kenji Yamada and Kevin Knight.
2002.
A decoder for syntax-based statistical MT.
In Proc.
of ACL, pages 303?310.Philadelphia, Pennsylvania, USA.Yuqi Zhang, Richard Zens, and Hermann Ney.
2007.
Improved chunk-level reordering for statistical machinetranslation.
In IWSLT 2007: International Workshop on Spoken Language Translation, pages 21?28.1070
