Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 884?893,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDensity Maximization in Context-Sense Metric Spacefor All-words WSDKoichi Tanigaki??
Mitsuteru Shiba?
Tatsuji Munaka?
Yoshinori Sagisaka??
Information Technology R&D Center, Mitsubishi Electric Corporation5-1-1 Ofuna, Kamakura, Kanagawa 247-8501, Japan?
Global Information and Telecommunication Institute, Waseda University1-3-10 Nishi-Waseda, Shinjuku-ku, Tokyo 169-0051, JapanAbstractThis paper proposes a novel smoothingmodel with a combinatorial optimizationscheme for all-words word sense disam-biguation from untagged corpora.
By gen-eralizing discrete senses to a continuum,we introduce a smoothing in context-sensespace to cope with data-sparsity result-ing from a large variety of linguistic con-text and sense, as well as to exploit sense-interdependency among the words in thesame text string.
Through the smoothing,all the optimal senses are obtained at onetime under maximum marginal likelihoodcriterion, by competitive probabilistic ker-nels made to reinforce one another amongnearby words, and to suppress conflictingsense hypotheses within the same word.Experimental results confirmed the superi-ority of the proposed method over conven-tional ones by showing the better perfor-mances beyond most-frequent-sense base-line performance where none of SemEval-2 unsupervised systems reached.1 IntroductionWord Sense Disambiguation (WSD) is a task toidentify the intended sense of a word based on itscontext.
All-words WSD is its variant, where allthe unrestricted running words in text are expectedto be disambiguated.
In the all-words task, allthe senses in a dictionary are potentially the targetdestination of classification, and purely supervisedapproaches inherently suffer from data-sparsityproblem.
The all-words task is also character-ized by sense-interdependency of target words.
Asthe target words are typically taken from the sametext string, they are naturally expected to be inter-related.
Disambiguation of a word should affectother words as an important clue.From such characteristics of the task,knowledge-based unsupervised approacheshave been extensively studied.
They computedictionary-based sense similarity to find the mostrelated senses among the words within a certainrange of text.
(For reviews, see (Agirre andEdmonds, 2006; Navigli, 2009).)
In recent years,graph-based methods have attracted considerableattentions (Mihalcea, 2005; Navigli and Lapata,2007; Agirre and Soroa, 2009).
On the graphstructure of lexical knowledge base (LKB),random-walk or other well-known graph-basedtechniques have been applied to find mutuallyrelated senses among target words.
Unlikeearlier studies disambiguating word-by-word, thegraph-based methods obtain sense-interdependentsolution for target words.
However, thosemethods mainly focus on modeling sense dis-tribution and have less attention to contextualsmoothing/generalization beyond immediatecontext.There exist several studies that enrich immedi-ate context with large corpus statistics.
McCarthyet al (2004) proposed a method to combine sensesimilarity with distributional similarity and config-ured predominant sense score.
Distributional sim-ilarity was used to weight the influence of contextwords, based on large-scale statistics.
The methodachieved successful WSD accuracy.
Agirre et al(2009) used a k-nearest words on distributionalsimilarity as context words.
They apply a LKBgraph-based WSD to a target word together withthe distributional context words, and showed thatit yields better results on a domain dataset thanjust using immediate context words.
Though these884studies are word-by-word WSD for target words,they demonstrated the effectiveness to enrich im-mediate context by corpus statistics.This paper proposes a smoothing model that in-tegrates dictionary-based semantic similarity andcorpus-based context statistics, where a combina-torial optimization scheme is employed to dealwith sense interdependency of the all-words WSDtask.
The rest of this paper is structured as fol-lows.
We first describe our smoothing model in thefollowing section.
The combinatorial optimizationmethod with the model is described in Section 3.Section 4 describes a specific implementation forevaluation.
The evaluation is performed with theSemEval-2 English all-words dataset.
We presentthe performance in Section 5.
In Section 6 we dis-cuss whether the intended context-to-sense map-ping and the sense-interdependency are properlymodeled.
Finally we review related studies in Sec-tion 7 and conclude in Section 8.2 Smoothing ModelLet us introduce in this section the basic idea formodeling context-to-sense mapping.
The distance(or similarity) metrics are assumed to be given forcontext and for sense.
A specific implementationof these metrics is described later in this paper, fornow the context metric is generalized with a dis-tance function dx(?, ?)
and the sense metric withds(?, ?).
Actually these functions may be arbitraryones that accept two elements and return a positivereal number.Now suppose we are given a dataset concern-ing N number of target words.
This dataset isdenoted by X = {xi}Ni=1, where xi correspondsto the context of the i-th word but not the wordby itself.
For each xi, the intended sense of theword is to be found in a set of sense candidatesSi = {sij}Mij=1 ?
S, where Mi is the number ofsense candidates for the i-th word, S is the wholeset of sense inventories in a dictionary.
Let thetwo-tuple hij = (xi, sij) be the hypothesis thatthe intended sense in xi is sij .
The hypothesis isan element of the direct product H = X ?
S. As(X, dx) and (S, ds) each composes a metric space,H is also a metric space, provided a proper dis-tance definition with dx and ds.Here, we treat the space H as a continuous one,which means that we assume the relationship be-tween context and sense can be generalized in con-tinuous fashion.
In natural language processing,continuity has been sometimes assumed for lin-guistic phenomena including word context for cor-pus based WSD.
As for classes or senses, it maynot be a common assumption.
However, whenthe classes for all-words WSD are enormous, fine-grained, and can be associated with distance, wecan rather naturally assume the continuity also forsenses.
According to the nature of continuity, oncegiven a hypothesis hij for a certain word, we canextrapolate the hypothesis for another word of an-other sense hi?j?
= (xi?
, si?j?)
sufficiently close tohij .
Using a Gaussian kernel (Parzen, 1962) as asmoothing model, the probability density extrapo-lated at hi?j?
given hij is defined by their distanceas follows:K(hij , hi?j?)
(1)?
12pi?x?sexp[?
dx2(xi, xi?)2?x2?
ds2(sij , si?j?
)2?s2],where ?x and ?s are parameters of positive realnumber ?x, ?s ?
R+ called kernel bandwidths.They control the smoothing intensity in contextand in sense, respectively.Our objective is to determine the optimal sensefor all the target words simultaneously.
It is es-sentially a 0-1 integer programing problem, andis not computationally tractable.
We relax theinteger constraints by introducing a sense prob-ability parameter piij corresponding to each hij .piij denotes the probability by which hij is true.As piij is a probability, it satisfies the constraints?i ?j piij = 1 and ?i, j 0 ?
piij ?
1.
The proba-bility density extrapolated at hi?j?
by a probabilis-tic hypothesis hij is given as follows:Qij(hi?j?)
?
piij K(hij , hi?j?).
(2)The proposed model is illustrated in Figure 1.Due to the limitation of drawing, both the contextmetric space and the sense metric space are drawnschematically as 1-dimensional spaces (axes), ac-tually arbitrary metric spaces similarity-based orfeature-based are applicable.
The product metricspace of the context metric space and the sensemetric space composes a hypothesis space.
In thehypothesis space, n sense hypotheses for a cer-tain word is represented as n points on the hyper-plane that spreads across the sense metric space.The two small circles in the middle of the fig-ure represent the two sense hypotheses for a sin-gle word.
The position of a hypothesis representswhich sense is assigned to the current word in885decoyfloraH.B.
Tree(actor)tree diagramtreeSense ProbabilityHypothesisContext MetricSpaceContext (Input)Sense MetricSpaceExtrapolatedDensitySense (Class)"Invasive, exotic plants cause particular problems for wildlife.
""Exotic tree"Figure 1: Proposed probability distribution modelfor context-to-sense mapping space.what context.
The upward arrow on a hypothesisrepresents the magnitude of its probability.Centered on each hypotheses, a Gaussian ker-nel is placed as a smoothing model.
It extrapo-lates the hypotheses of other words around it.
Inaccordance with geometric intuition, intensity ofextrapolation is affected by the distance from a hy-pothesis, and by the probability of the hypothesisby itself.
Extrapolated probability density is rep-resented by shadow thickness and surface height.If there is another word in nearby context, the ker-nels can validate the sense of that word.
In thefigure, there are two kernels in the context ?Inva-sive, exotic ...?.
They are two competing hypothe-sis for the senses decoy and flora of the wordplants.
These kernels affect the senses of anotherambiguous word tree in nearby context ?Exotic...?, and extrapolate the most at the sense treenearby flora.
The extrapolation has non-lineareffect.
It affects little to the word far away in con-text or in sense as is the case for the backgroundword in the figure.
Strength of smoothing is deter-mined by kernel bandwidths.
Wider bandwidthsbring stronger effect of generalization to furtherhypotheses, but too wide bandwidths smooth outdetailed structure.
The bandwidths are the key fordisambiguation, therefore they are to be optimizedon a dataset together with sense probabilities.3 Simultaneous Optimization ofAll-words WSDGiven the smoothing model to extrapolate thesenses of other words, we now make its in-stances interact to obtain the optimal combinationof senses for all the words.3.1 Likelihood DefinitionLet us first define the likelihood of model param-eters for a given dataset.
The parameters con-sist of a context bandwidth ?x, a sense bandwidth?s, and sense probabilities piij for all i and j.For convenience of description, the sense proba-bilities are all together denoted as a vector pi =(.
.
.
, piij , .
.
.
)?, in which actual order is not thematter.Now remind that our dataset X = {xi}Ni=1 iscomposed of N instances of unlabeled word con-text.
We consider all the mappings from contextto sense are latent, and find the optimal parametersby maximizing marginal pseudo likelihood basedon probability density.
The likelihood is definedas follows:L(pi, ?x, ?s;X) ?
ln?i?jpiijQ(hij), (3)where ?i denotes the product over xi ?
X ,?jdenotes the summation over all possible sensessij ?
Si for the current i-th context.
Q(hij)denotes the probability density at hij .
We com-pute Q(hij) using leave-one-out cross-validation(LOOCV), so as to prevent kernels from over-fitting to themselves, as follows:Q(hij) (4)?
1N ?Ni?i?:wi?
?=wi?j?pii?j?K(hij , hi?j?
),where Ni denotes the number of occurrences of aword type wi in X , and ?i?:wi?
?=wi denotes thesummation over xi?
?
X except the case that theword type wi?
equals to wi.
?j?
denotes the sum-mation over si?j?
?
Si?
.
We take as the unit ofLOOCV not a word instance but a word type, be-cause the instances of the same word type invari-ably have the same sense candidates, which stillcause over-fitting when optimizing the sense band-width.3.2 Parameter OptimizationWe are now ready to calculate the optimal senses.The optimal parameterspi?, ?
?x, ?
?s are obtained bymaximizing the likelihood L subject to the con-straints on pi, that is ?i ?j piij = 1 1.
Using theLagrange multipliers {?i}Ni=1 for every i-th con-straint, the solution for the constrained maximiza-1It is guaranteed that the other constraints ?i, j 0 ?
piij ?1 are satisfied according to Equation (7).886tion of L is obtained as the solution for the equiv-alent unconstrained maximization of L?
as follows:pi?, ?
?x, ?
?s = argmaxpi, ?x, ?sL?, (5)whereL?
?
L+?i?i(?jpiij ?
1).
(6)When we optimize the parameters, the first termof Equation (6) in the right-hand side acts to re-inforce nearby hypotheses among different words,whereas the second term acts to suppress conflict-ing hypotheses of the same word.Taking ?L?
= 0, erasing ?i, and rearranging,we obtain the optimal parameters as follows:piij =?i?, j?wi?
?=wiR iji?j?
+?i?, j?wi?
?=wiR i?j?ij1 + ?j?i?, j?wi?
?=wiR i?j?ij(7)?x2 =1N?i, i?, j, j?wi?
?=wiR iji?j?
dx2(xi, xi?)
(8)?s2 =1N?i, i?, j, j?wi?
?=wiR iji?j?
ds2(sij , si?j?
), (9)where R iji?j?
denotes the responsibility of hi?j?
tohij : the ratio of total expected density at hij , takenup by the expected density extrapolated by hi?j?
,normalized to the total for xi be 1.
It is defined asR iji?j?
?piijQi?j?
(hij)?j piijQ(hij).
(10)Qi?j?
(hij) denotes the probability density at hijextrapolated by hi?j?
alone, defined as follows:Qi?j?
(hij) ?1N ?Nipii?j?K(hij , hi?j?).
(11)Intuitively, Equations (7)-(9) are interpreted asfollows.
As for Equation (7), the right-hand sideof the equation can be divided as the left term andthe right term both in the numerator and in thedenominator.
The left term requires piij to agreewith the ratio of responsibility of the whole to hij .The right term requires piij to agree with the ra-tio of responsibility of hij to the whole.
As forEquation (8), (9), the optimal solution is the meansquared distance in context, and in sense, weightedby responsibility.To obtain the actual values of the optimal pa-rameters, EM algorithm (Dempster et al, 1977)is applied.
This is because Equations (7)-(9) arecircular definitions, which include the objectiveparameters implicitly in the right hand side, thusthe solution is not obtained analytically.
EM al-gorithm is an iterative method for finding maxi-mum likelihood estimates of parameters in statis-tical models, where the model depends on unob-served latent variables.
Applying the EM algo-rithm to our model, we obtain the following steps:Step 1.
Initialization: Set initial values to pi,?x, and ?s.
As for sense probabilities,we set the uniform probability in accor-dance with the number of sense candidates,thereby piij ?
|Si|?1, where |Si| denotesthe size of Si.
As for bandwidths, we setthe mean squared distance in each metric;thereby ?x2 ?
N?1?i, i?
dx2(xi, xi?
)for context bandwidth, and ?s2 ?
(?i|Si|)?1?i, i?
?j, j?
ds2(sij , si?j?)
forsense bandwidth.Step 2.
Expectation: Using the current parame-ters pi, ?x, and ?s, calculate the responsibili-tiesR iji?j?
according to Equation (10).Step 3.
Maximization: Using the current respon-sibility R iji?j?
, update the parameters pi, ?x,and ?s, according to Equation (7)-(9).Step 4.
Convergence test: Compute the likeli-hood.
If its ratio to the previous iteration issufficiently small, or predetermined numberof iterations has been reached, then terminatethe iteration.
Otherwise go back to Step 2.To visualize how it works, we applied the aboveEM algorithm to pseudo 2-dimensional data.
Theresults are shown in Figure 2.
It simulates WSDfor an N = 5 words dataset, whose contexts aredepicted by five lines.
The sense hypotheses aredepicted by twelve upward arrows.
At the base ofeach arrow, there is a Gaussian kernel.
Shadowthickness and surface height represents the com-posite probability distribution of all the twelvekernels.
Through the iterative parameter update,sense probabilities and kernel bandwidths wereoptimized to the dataset.
Figure 2(a) illustrates theinitial status, where all the sense hypothesis areequivalently probable, thus they are in the mostambiguous status.
Initial bandwidths are set to themean squared distance of all the hypotheses pairs,887Context(Input)Sense(Class)(a) Initial status.Context(Input)Sense(Class)(b) Status after the 7th iteration.Context(Input)Sense(Class)(c) Converged status after 25 iterations.Figure 2: Pseudo 2D data simulation to visualize the dynamics of the proposed simultaneous all-wordsWSD with ambiguous five words and twelve sense hypotheses.
(There are twelve Gaussian kernels atthe base of each arrow, though the figure shows just their composite distribution.
Those kernels reinforceand compete one another while being fitted their affecting range, and finally settle down to the mostconsistent interpretation for the words with appropriate generalization.
For the dynamics with an actualdataset, see Figure 5.
)which is rather broad and makes kernels stronglysmoothed, thus the model captures general struc-ture of space.
Figure 2(b) shows the status after the7th iteration.
Bandwidths are shrinking especiallyin context, and two context clusters, so to speak,two usages, are found.
Figure 2(c) shows the sta-tus of convergence after 25 iterations.
All the ar-row lengths incline to either 1 or 0 along with theirneighbors, thus all the five words are now disam-biguated.Note that this is not the conventional cluster-ing of observed data.
If, for instance, the Gaus-sian mixture clustering of 2-mixtures is appliedto the positions of these hypotheses, it will findthe clusters just like Figure 2(b) and will stop.The cluster centers are located at the means of hy-potheses including miscellaneous alternatives notintended, thus the estimated probability distribu-tion is, roughly speaking, offset toward the centerof WordNet, which is not what we want.
In con-trast, the proposed method proceeds to Figure 2(c)and finds clusters in the data after conflicting datais erased.
This is because our method is aim-ing at modeling not the disambiguation of cluster-memberships but the disambiguation of senses foreach word.4 Metric Space ImplementationSo far, we have dealt with general metrics for con-text and for sense.
This section describes a spe-cific implementation of those metrics employed inthe evaluation.
We followed the previous studyby McCarthy et al (2004), (2007), and imple-mented a type-based WSD.
The context of wordinstances are tied to the distributional context ofthe word type in a large corpus.
To calculate sensesimilarities, we used the WordNet similarity pack-age by Pedersen et al (2004), version 2.05.
Twomeasures proposed by Jiang and Conrath (1997)and Lesk (1986) were examined, which performedbest in the previous study (McCarthy et al, 2004).Distributional similarity (Lin, 1998) wascomputed among target words, based on thestatistics of the test set and the background textprovided as the official dataset of the SemEval-2English all-words task (Agirre et al, 2010).
Thosetexts were parsed using RASP parser (Briscoeet al, 2006) version 3.1, to obtain grammaticalrelations for the distributional similarity, as wellas to obtain lemmata and part-of-speech (POS)tags which are required to look up the senseinventory of WordNet.
Based on the distributionalsimilarity, we just used k-nearest neighbor wordsas the context of each target word.
Although it isan approximation, we can expect reliability im-provement often seen by ignoring the lower part.In addition, this limitation of interactions highlyreduces computational cost in particular whenapplying to larger-scale problems.
To do this, theexhaustive sum ?i, i?
:wi ?=wi?
in Equation (7)-(9)is altered by the local sum ?i, i?
: (wi,wi?
)?PkNN ,where PkNN denotes the set of word pairs ofwhich either is a k-nearest neighbors of theother.
The normalizing factors 1, N , and N ?Niin Equation (7), (8)-(9), and (11) are alteredby the actual sum of responsibilities withinthose neighbors as ?i?, j, j?
: (wi,wi?
)?PkNNRiji?j?
,888?i, i?, j, j?
: (wi,wi?
)?PkNNR iji?j?
, and?
?, i?, j, j?
: (w?,wi?
)?PkNN ?
?
?=iR ?ji?j?
, respectively.To treat the above similarity functions of con-text and of sense as distance functions, we use theconversion: d(?, ?)
?
??
ln(f(?, ?
)/fmax), whered denotes the objective distance function, i.e., dxfor context and ds for sense, while f and fmax de-note the original similarity function and its max-imum, respectively.
?
is a standardization co-efficient, which is determined so that the meansquared distance be 1 in a dataset.
According tothis standardization, initial values of ?x2, ?s2 arealways 1.5 EvaluationTo confirm the effect of the proposed smoothingmodel and its combinatorial optimization scheme,we conducted WSD evaluations.
The primaryevaluations compare our method with conven-tional ones, in Section 5.2.
Supplementary eval-uations are described in the subsequent sectionsthat include the comparison with SemEval-2 par-ticipating systems, and the analysis of model dy-namics with the experimental data.5.1 Evaluation SchemeTo make the evaluation comparable to state-of-the-art systems, we used the official dataset of theSemEval-2 English all-words WSD task (Agirreet al, 2010), which is currently the latest pub-lic dataset available with published results.
Thedataset consists of test data and background doc-uments of the same environment domain.
Thetest data consists of 1,398 target words (1,032nouns and 366 verbs) in 5.3K running words.
Thebackground documents consists of 2.7M runningwords, which was used to compute distributionalsimilarity.Precisions and recalls were all computed us-ing the official evaluation tool scorer2 in fine-grained measure.
The tool accepts answers eitherin probabilistic format (senses with probabilitiesfor each target word) or in deterministic format(most likely senses, with no score information).As the proposed method is a probability model, weevaluated in the probabilistic way unless explicitlynoted otherwise.
For this reason, we evaluated allthe sense probabilities as they were.
Disambigua-tions were executed in separate runs for nouns andverbs, because no interaction takes place acrossPOS in this metric implementation.
The two runs?results were combined later to a single answer tobe input to scorer2.The context metric space was composed by k-nearest neighbor words of distributional similarity(Lin, 1998), as is described in Section 4.
The valueof k was evaluated for {2, 3, 5, 10, 20, 30, 50, 100,200, 300}.
As for sense metric space, we evalu-ated two measures i.e., (Jiang and Conrath, 1997)denoted as JCN, and (Lesk, 1986) denoted as Lesk.In every condition, stopping criterion of iterationis always the number of iteration (500 times), irre-spective of the convergence in likelihood.Primary evaluations compared our method withtwo conventional methods.
Those methods differto ours only in scoring schemes.
The first oneis the method by McCarthy et al (2004), whichdetermines the word sense based on sense simi-larity and distributional similarity to the k-nearestneighbor words of a target word by distributionalsimilarity.
Our major advantage is the combina-torial optimization framework, while the conven-tional one employs word-by-word scheme.
Thesecond one is based on the method by Patwardhanet al (2007), which determines the word sense bymaximizing the sum of sense similarity to the kimmediate neighbor words of a target word.
The kwords were forced to be selected from other targetwords of the same POS to the word of interest, soas to make information resource equivalent to theother comparable two methods.
It is also a word-by-word method.
It exploits no distributional simi-larity.
Our major advantages are the combinatorialoptimization scheme and the smoothing model tointegrate distributional similarity.
In the followingsection, these comparative methods are referred toas Mc2004 and Pat2007, respectively.5.2 Comparison with Conventional MethodsLet us first confirm our advantages compared tothe conventional methods ofMc2004 and Pat2007.The comparative results are shown in Figure 3 inrecall measure.
Precisions are simply omitted be-cause the difference to the recalls are always thenumber of failures on referring to WordNet bymislabeling of lemmata or POSs, which is alwaysthe same for the three methods.
Vertical range de-picts 95% confidence intervals.
The graphs alsoindicate the most-frequent-sense (MFS) baselineestimated from out-of-domain corpora, whose re-call is 0.505 (Agirre et al, 2010).As we can see in Figure 3(a) and 3(b), higher889Context k-NN Context k-NN0.40.51  10  100  10000.40.51  10  100  1000MFSRecall(a) JCN (b) LeskRecallMFSProposedMc2004Pat2007ProposedMc2004Pat2007Figure 3: Comparison to the conventional methodsthat differ to our method only in scoring schemes.Table 1: Comparison with the top-5 knowledge-based systems in SemEval-2 (JCN/k=5).Rank Participants R P Rn Rv- Proposed (best) 50.8 51.0 52.5 46.2- MFS Baseline 50.5 50.5 52.7 44.31 Kulkarni et al (2010) 49.5 51.2 51.6 43.42 Tran et al (2010) 49.3 50.6 51.6 42.63 Tran et al (2010) 49.1 50.4 51.5 42.54 Soroa et al (2010) 48.1 48.1 48.7 46.25 Tran et al (2010) 47.9 49.2 49.4 43.4... ... ... ... ... ...- Random Baseline 23.2 23.2 25.3 17.2recalls are obtained in the order of the proposedmethod, Mc2004, and Pat2007 on the whole.Comparing JCN and Lesk, difference among thethree is smaller in Lesk.
It is possibly becauseLesk is a score not normalized for different wordpairs, which makes the effect of distributional sim-ilarity unsteady especially when combining manyk-nearest words.
Therefore the recalls are ex-pected to improve if proper normalization is ap-plied to the proposed method and Mc2004.
InJCN, the recalls of the proposed method signif-icantly improve compared to Pat2007.
Our bestrecall is 0.508 with JCN and k = 5.
Thus wecan conclude that, though significance depends onmetrics, our smoothing model and the optimiza-tion scheme are effective to improve accuracies.5.3 Comparison with SemEval-2 SystemsWe compared our best results with the participat-ing systems of the task.
Table 1 compares thedetails to the top-5 systems, which only includesunsupervised/knowledge-based ones and excludessupervised/weakly-supervised ones.
Those values0.3  0.4  0.5RecallMFSProposed (best)RankFigure 4: Comparison with the all 20 knowledge-based systems in SemEval-2 (JCN/k=5).0.510  100  200  300  400  500 1.081.09SenseProbability010  100  200  300  400  500IterationContext BandwidthSense Bandwidth?x2?
s 2?
ijFigure 5: Model dynamics through iteration withSemEval-2 nouns (JCN/k=5).are transcribed from the official report (Agirre etal., 2010).
?R?
and ?P?
denote the recall and theprecision for the whole dataset, while ?Rn?
and?Rv?
denote the recall for nouns and verbs, re-spectively.
The results are ranked by ?R?, in ac-cordance with the original report.
As shown in thetable, our best results outperform all of the systemsand the MFS baseline.Overall rankings are depicted in Figure 4.
Itmaps our best results in the distribution of allthe 20 unsupervised/knowledge-based participat-ing systems.
The ranges spreading left and rightare 95% confidence intervals.
As is seen fromthe figure, our best results are located above thetop group, which are outside the confidence inter-vals of the other participants ranked intermediateor lower.5.4 Analysis on Model DynamicsThis section examines the model dynamics withthe SemEval-2 data, which has been illustrated8900.40.50  100  200  300  400  500RecallIterationJCNLeskProbabilisticDeterministicProbabilisticDeterministicFigure 6: Recall improvement via iteration withSemEval-2 all POSs (JCN/k=30, Lesk/k=10).with pseudo data in Section 3.2.
Let us start bylooking at the upper half of Figure 5, which showsthe change of sense probabilities through itera-tion.
At the initial status (iteration 0), the prob-abilities were all 1/2 for bi-semous words, all 1/3for tri-semous words, and so forth.
As iterationproceeded, the probabilities gradually spread outto either side of 1 or 0, and finally at iteration500, we can observe that almost all the words wereclearly disambiguated.
The lower half of Figure 5shows the dynamics in bandwidths.
Vertical axison the left is for the sense bandwidth, and on theright is for the context bandwidth.
We can ob-serve those bandwidths became narrower as iter-ation proceeded.
Intensity of smoothing was dy-namically adjusted by the whole disambiguationstatus.
These behaviors confirm that even with anactual dataset, it works as is expected, just as illus-trated in Figure 2.6 DiscussionThis section discusses the validity of the proposedmethod as to i) sense-interdependent disambigua-tion and ii) reliability of data smoothing.
We hereanalyze the second peak conditions at k = 30(JCN) and k = 10 (Lesk) instead of the first peakat k = 5, because we can observe tendency thebetter with the larger number of word interactions.6.1 Effects of Sense-interdependentDisambiguationLet us first examine the effect of our sense-interdependent disambiguation.
We would liketo confirm that how the progressive disambigua-tion is carried out.
Figure 6 shows the changeof recall through iteration for JCN (k = 30) andLesk (k = 10).
Those recalls were obtained byevaluating the status after each iteration.
The re-calls were here evaluated both in probabilistic for-mat and in deterministic format.
From the fig-ure we can observe that the deterministic recallsalso increased as well as the probabilistic recalls.This means that the ranks of sense candidates foreach word were frequently altered through itera-tion, which further means that some new infor-mation not obtained earlier was delivered one af-ter another to sense disambiguation for each word.From these results, we could confirm the expectedsense-interdependency effect that a sense disam-biguation of certain word affected to other words.6.2 Reliability of Smoothing as SupervisionLet us now discuss the reliability of our smoothingmodel.
In our method, sense disambiguation of aword is guided by its nearby words?
extrapolation(smoothing).
Sense accuracy fully depends on thereliability of the extrapolation.
Generally speak-ing, statistical reliability increases as the numberof random sampling increases.
If we take suffi-cient number of random words as nearby words,the sense distribution comes close to the true dis-tribution, and then we expect the statistically truesense distribution should find out the true sense ofthe target word, according to the distributional hy-potheses (Harris, 1954).
On the contrary, if wetake nearby words that are biased to particularwords, the sense distribution also becomes biased,and the extrapolation becomes less reliable.We can compute the randomness of words thataffect for sense disambiguation, by word per-plexity.
Let the word of interest be w ?
V .The word perplexity is calculated as 2H|w , whereH|w denotes the entropy defined as H|w ???w?
?V \{w} p(w?|w) log2 p(w?|w).
The con-ditional probability p(w?|w) denotes the proba-bility with which a certain word w?
?
V \{w} determines the sense of w, which canbe defined as the density ratio: p(w?|w) ??i:wi=w?i?:wi?=w??j,j?
Qi?j?
(hij).The relation between word perplexity and prob-ability change for ground-truth senses of nouns(JCN/k = 30) is shown in Figure 7.
The upper his-togram shows the change in iteration 1-100, andthe lower shows that of iteration 101-500.
We di-vide the analysis at iteration 100, because roughlyuntil the 100th iteration, the change in bandwidthsconverged, and the number of words to interactsettled, as can be seen in Figure 5.
The bars that891-15-10-50510150  20  40  60  80  100-500501001500  20  40  60  80  100X Iteration 1 to 100Iteration 101 to 500Perplexity of Extrapolator WordsProb.ofGround-truthSenseCorrectWrongCorrectWrongFigure 7: Correlation between reliability and per-plexity with SemEval-2 nouns (JCN/k=30).extend upward represent the sum of the amountraised (correct change), and the bars that extenddownward represent the sum of the amount re-duced (wrong change).
From these figures, weobserve that when perplexity is sufficiently large(?
30), change occurred largely (79%) to the cor-rect direction.
In contrast, at the lower left of thefigure, where perplexity is small (< 30) and band-widths has been narrowed at iteration 101-500,correct change occupied only 32% of the whole.Therefore, we can conclude that if sufficiently ran-dom samples of nearby words are provided, oursmoothing model is reliable, though it is trained inan unsupervised fashion.7 Related WorkAs described in Section 1, graph-based WSD hasbeen extensively studied, since graphs are favor-able structure to deal with interactions of data onvertices.
Conventional studies typically consideras vertices the instances of input or target class,e.g.
knowledge-based approaches typically regardsenses as vertices (see Section 1), and corpus-based approaches such as (Ve?ronis, 2004) regardwords as vertices or (Niu et al, 2005) regards con-text as vertices.
Our method can be viewed as oneof graph-based methods, but it regards input-to-class mapping as vertices, and the edges representthe relations both together in context and in sense.Mihalcea (2005) proposed graph-based methods,whose vertices are sense label hypotheses on wordsequence.
Our method generalize context repre-sentation.In the evaluation, our method was comparedto SemEval-2 systems.
The main subject of theSemEval-2 task was domain adaptation, thereforethose systems each exploited their own adaptationtechniques.
Kulkarni et al (2010) used a Word-Net pre-pruning.
Disambiguation is performed byconsidering only those candidate synsets that be-long to the top-k largest connected componentsof the WordNet on domain corpus.
Tran et al(2010) used over 3TB domain documents acquiredby Web search.
They parsed those documentsand extracted the statistics on dependency relationfor disambiguation.
Soroa et al (2010) used themethod by Agirre et al (2009) described in Sec-tion 1.
They disambiguated each target word us-ing its distributionally similar words instead of itsimmediate context words.The proposed method is an extension of densityestimation (Parzen, 1962), which is a construc-tion of an estimate based on observed data.
Ourmethod naturally extends the density estimation intwo points, which make it applicable to unsuper-vised knowledge-based WSD.
First, we introducestochastic treatment of data, which is no longer ob-servations but hypotheses having ambiguity.
Thisextension makes the hypotheses possible to cross-validate the plausibility each other.
Second, weextend the definition of density from Euclideandistance to general metric, which makes the pro-posed method applicable to a wide variety ofcorpus-based context similarities and dictionary-based sense similarities.8 ConclusionsWe proposed a novel smoothing model with acombinatorial optimization scheme for all-wordsWSD from untagged corpora.
Experimental re-sults showed that our method significantly im-proves the accuracy of conventional methods byexceeding most-frequent-sense baseline perfor-mance where none of SemEval-2 unsupervisedsystems reached.
Detailed inspection of dynam-ics clearly show that the proposed optimizationmethod effectively exploit the sense-dependencyof all-words.
Moreover, our smoothing model,though unsupervised, provides reliable supervi-sion when sufficiently random samples of wordsare available as nearby words.
Thus it was con-firmed that this method is valid for finding the op-timal combination of word senses with large un-tagged corpora.
We hope this study would elicitfurther investigation in this important area.892ReferencesEneko Agirre and Philip Edmonds.
2006.
Word sensedisambiguation: Algorithms and applications, vol-ume 33.
Springer Science+ Business Media.Eneko Agirre and Aitor Soroa.
2009.
Personalizingpagerank for word sense disambiguation.
In Pro-ceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 33?41.Eneko Agirre, Oier Lopez De Lacalle, Aitor Soroa,and Informatika Fakultatea.
2009.
Knowledge-based wsd on specific domains: performing betterthan generic supervised wsd.
In Proceedings of the21st international jont conference on Artifical intel-ligence, pages 1501?1506.Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-baum, Shu-Kai Hsieh, Maurizio Tesconi, Mon-ica Monachini, Piek Vossen, and Roxanne Segers.2010.
Semeval-2010 task 17: All-words word sensedisambiguation on a specific domain.
In Proceed-ings of the 5th International Workshop on SemanticEvaluation, pages 75?80.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the rasp system.
In Proceed-ings of the COLING/ACL on Interactive presenta-tion sessions, pages 77?80.Arthur Pentland Dempster, Nan McKenzie Laird, andDonald Bruce Rubin.
1977.
Maximum likelihoodfrom incomplete data via the em algorithm.
Journalof the Royal Statistical Society.
Series B (Method-ological), pages 1?38.Zellig Sabbetai Harris.
1954.
Distributional structure.Word.Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical tax-onomy.
arXiv preprint cmp-lg/9709008.Anup Kulkarni, Mitesh M. Khapra, Saurabh Sohoney,and Pushpak Bhattacharyya.
2010.
CFILT: Re-source conscious approaches for all-words domainspecific.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 421?426.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell apine cone from an ice cream cone.
In Proceedings ofthe 5th annual international conference on Systemsdocumentation, pages 24?26.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 17th inter-national conference on Computational linguistics-Volume 2, pages 768?774.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In Proceedings of the 42nd AnnualMeeting on Association for Computational Linguis-tics, pages 279?286.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2007.
Unsupervised acquisition of pre-dominant word senses.
Computational Linguistics,33(4):553?590.Rada Mihalcea.
2005.
Unsupervised large-vocabularyword sense disambiguation with graph-based algo-rithms for sequence data labeling.
In Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing, pages 411?418.Roberto Navigli and Mirella Lapata.
2007.
Graphconnectivity measures for unsupervised word sensedisambiguation.
In Proceedings of the 20th inter-national joint conference on Artifical intelligence,pages 1683?1688.Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
ACM Computing Surveys (CSUR), 41(2):10.Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan.2005.
Word sense disambiguation using label prop-agation based semi-supervised learning.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 395?402.Emanuel Parzen.
1962.
On estimation of a probabilitydensity function and mode.
The annals of mathe-matical statistics, 33(3):1065?1076.Siddharth Patwardhan, Satanjeev Banerjee, and TedPedersen.
2007.
UMND1: Unsupervised wordsense disambiguation using contextual semantic re-latedness.
In proceedings of the 4th InternationalWorkshop on Semantic Evaluations, pages 390?393.Ted Pedersen, Siddharth Patwardhan, and JasonMiche-lizzi.
2004.
WordNet::Similarity: measuring the re-latedness of concepts.
In Demonstration Papers atHLT-NAACL 2004, pages 38?41.Aitor Soroa, Eneko Agirre, Oier Lopez de Lacalle,Monica Monachini, Jessie Lo, Shu-Kai Hsieh,Wauter Bosma, and Piek Vossen.
2010.
Kyoto: Anintegrated system for specific domain WSD.
In Pro-ceedings of the 5th International Workshop on Se-mantic Evaluation, pages 417?420.Andrew Tran, Chris Bowes, David Brown, Ping Chen,Max Choly, and Wei Ding.
2010.
TreeMatch: Afully unsupervised WSD system using dependencyknowledge on a specific domain.
In Proceedings ofthe 5th International Workshop on Semantic Evalu-ation, pages 396?401.Jean Ve?ronis.
2004.
HyperLex: lexical cartographyfor information retrieval.
Computer Speech & Lan-guage, 18(3):223?252.893
