Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 19?26,Avignon, France, April 23 2012. c?2012 Association for Computational LinguisticsHybrid Combination of Constituency and Dependency Trees into anEnsemble Dependency ParserNathan David Green and Zdene?k Z?abokrtsky?Charles University in PragueInstitute of Formal and Applied LinguisticsFaculty of Mathematics and PhysicsPrague, Czech Republic{green,zabokrtsky}@ufal.mff.cuni.czAbstractDependency parsing has made many ad-vancements in recent years, in particu-lar for English.
There are a few de-pendency parsers that achieve compara-ble accuracy scores with each other butwith very different types of errors.
Thispaper examines creating a new depen-dency structure through ensemble learn-ing using a hybrid of the outputs of var-ious parsers.
We combine all tree out-puts into a weighted edge graph, using 4weighting mechanisms.
The weighted edgegraph is the input into our ensemble sys-tem and is a hybrid of very different parsingtechniques (constituent parsers, transition-based dependency parsers, and a graph-based parser).
From this graph we take amaximum spanning tree.
We examine thenew dependency structure in terms of accu-racy and errors on individual part-of-speechvalues.The results indicate that using a greaternumber of more varied parsers will improveaccuracy results.
The combined ensemblesystem, using 5 parsers based on 3 differentparsing techniques, achieves an accuracyscore of 92.58%, beating all single parserson the Wall Street Journal section 23 testset.
Additionally, the ensemble system re-duces the average relative error on selectedPOS tags by 9.82%.1 IntroductionDependency parsing has made many advance-ments in recent years.
A prime reason for thequick advancement has been the CoNLL sharedtask competitions.
These competitions gave thecommunity a common training/testing frameworkalong with many open source systems.
These sys-tems have, for certain languages, achieved fairlyhigh accuracy.
Many of the top systems havecomparable accuracy but vary on the types oferrors they make.
The approaches used in theshared task vary from graph-based techniques totransition-based techniques to the conversion ofconstituent trees produced by state-of-the-art con-stituent parsers.
This varied error distributionmakes dependency parsing a prime area for theapplication of new hybrid and ensemble algo-rithms.Increasing accuracy of dependency parsing of-ten is in the realm of feature tweaking and opti-mization.
The idea behind ensemble learning is totake the best of each parser as it currently is andallow the ensemble system to combine the outputsto form a better overall parse using prior knowl-edge of each individual parser.
This is often doneby different weighting or voting schemes.2 Related WorkEnsemble learning (Dietterich, 2000) has beenused for a variety of machine learning tasks andrecently has been applied to dependency pars-ing in various ways and with different levels ofsuccess.
(Surdeanu and Manning, 2010; Haf-fari et al, 2011) showed a successful combina-tion of parse trees through a linear combinationof trees with various weighting formulations.
Tokeep their tree constraint, they applied Eisner?s al-gorithm for reparsing (Eisner, 1996).Parser combination with dependency trees hasbeen examined in terms of accuracy (Sagae andLavie, 2006; Sagae and Tsujii, 2007; Zeman andZ?abokrtsky?, 2005).
However, the various tech-niques have generally examined similar parsers19or parsers which have generated various differentmodels.
To the best of our knowledge, our ex-periments are the first to look at the accuracy andpart of speech error distribution when combiningtogether constituent and dependency parsers thatuse many different techniques.
However, POStags were used in parser combination in (Hall etal., 2007) for combining a set of Malt Parser mod-els with success.Other methods of parser combinations haveshown to be successful such as using one parserto generate features for another parser.
This wasshown in (Nivre and McDonald, 2008), in whichMalt Parser was used as a feature to MST Parser.The result was a successful combination of atransition-based and graph-based parser, but didnot address adding other types of parsers into theframework.3 MethodologyThe following sections describe the process flow,choice of parsers, and datasets needed for oth-ers to recreate the results listed in this paper.Although we describe the specific parsers anddatasets used in this paper, this process flowshould work for any number of hybrid combina-tions of parsers and datasets.3.1 Process FlowTo generate a single ensemble parse tree, our sys-tem takes N parse trees as input.
The inputs arefrom a variety of parsers as described in 3.2.All edges in these parse trees are combined intoa graph structure.
This graph structure acceptsweighted edges.
So if more than one parse treecontains the same tree edge, the graph is weightedappropriately according to a chosen weighting al-gorithm.
The weighting algorithms used in ourexperiments are described in 3.5.Once the system has a weighted graph, it thenuses an algorithm to find a corresponding treestructure so there are no cycles.
In this set of ex-periments, we constructed a tree by finding themaximum spanning tree using ChuLiu/Edmonds?algorithm, which is a standard choice for MSTtasks.
Figure 1 graphically shows the decisionsone needs to make in this framework to create anensemble parse.Figure 1: General flow to create an ensemble parsetree.3.2 ParsersTo get a complete representation of parsers inour ensemble learning framework we use 5 ofthe most commonly used parsers.
They rangefrom graph-based approaches to transition-basedapproaches to constituent parsers.
Constituencyoutput is converted to dependency structures us-ing a converter (Johansson and Nugues, 2007).All parsers are integrated into the Treex frame-work (Z?abokrtsky?
et al, 2008; Popel et al, 2011)using the publicly released parsers from the re-spective authors but with Perl wrappers to allowthem to work on a common tree structure.?
Graph-Based: A dependency tree is a spe-cial case of a weighted edge graph thatspawns from an artificial root and is acyclic.Because of this we can look at a large historyof work in graph theory to address findingthe best spanning tree for each dependencygraph.
In this paper we use MST Parser(McDonald et al, 2005) as an input to ourensemble parser.?
Transition-Based: Transition-based parsingcreates a dependency structure that is pa-rameterized over the transitions used to cre-ate a dependency tree.
This is closely re-lated to shift-reduce constituency parsing al-gorithms.
The benefit of transition-basedparsing is the use of greedy algorithms whichhave a linear time complexity.
However, dueto the greedy algorithms, longer arc parsescan cause error propagation across each tran-sition (Ku?bler et al, 2009).
We make use20of Malt Parser (Nivre et al, 2007b), whichin the shared tasks was often tied with thebest performing systems.
Additionally weuse Zpar (Zhang and Clark, 2011) which isbased on Malt Parser but with a different setof non-local features.?
Constituent Transformation While not atrue dependency parser, one technique of-ten applied is to take a state-of-the-art con-stituent parser and transform its phrase basedoutput into dependency relations.
This hasbeen shown to also be state-of-the-art in ac-curacy for dependency parsing in English.
Inthis paper we transformed the constituencystructure into dependencies using the PennConverter conversion tool (Johansson andNugues, 2007).
A version of this converterwas used in the CoNLL shared task to createdependency treebanks as well.
For the fol-lowing ensemble experiments we make useof both (Charniak and Johnson, 2005) andStanford?s (Klein and Manning, 2003) con-stituent parsers.In addition to these 5 parsers, we also reportthe accuracy of an Oracle Parser.
This parser issimply the best possible parse of all the edges ofthe combined dependency trees.
If the reference,gold standard, tree has an edge that any of the 5parsers contain, we include that edge in the Or-acle parse.
Initially all nodes of the tree are at-tached to an artificial root in order to maintainconnectedness.
Since only edges that exist in areference tree are added, the Oracle Parser main-tains the acyclic constraint.
This can be viewedas the maximum accuracy that a hybrid approachcould achieve with this set of parsers and with thegiven data sets.3.3 DatasetsMuch of the current progress in dependency pars-ing has been a result of the availability of commondata sets in a variety of languages, made avail-able through the CoNLL shared task (Nivre et al,2007a).
This data is in 13 languages and 7 lan-guage families.
Later shared tasks also releaseddata in other genres to allow for domain adap-tation.
The availability of standard competition,gold level, data has been an important factor independency based research.For this study we use the English CoNLL data.This data comes from the Wall Street Journal(WSJ) section of the Penn treebank (Marcus et al,1993).
All parsers are trained on sections 02-21 ofthe WSJ except for the Stanford parser which usessections 01-21.
Charniak, Stanford and Zpar usepre-trained models ec50spfinal, wsjPCFG.ser.gz,english.tar.gz respectively.
For testing we use sec-tion 23 of the WSJ for comparability reasons withother papers.
This test data contains 56,684 to-kens.
For tuning we use section 22.
This data isused for determining some of the weighting fea-tures.3.4 EvaluationAs an artifact of the CoNLL shared taskscompetition, two standard metrics for com-paring dependency parsing systems emerged.Labeled attachment score (LAS) and unlabeledattachment score (UAS).
UAS studies the struc-ture of a dependency tree and assesses whether theoutput has the correct head and dependency arcs.In addition to the structure score in UAS, LASalso measures the accuracy of the dependency la-bels on each arc.
A third, but less common met-ric, is used to judge the percentage of sentencesthat are completely correct in regards to their LASscore.
For this paper since we are primarily con-cerned with the merging of tree structures we onlyevaluate UAS (Buchholz and Marsi, 2006).3.5 WeightingCurrently we are applying four weighting algo-rithms to the graph structure.
First we give eachparser the same uniform weight.
Second we ex-amine weighting each parser output by the UASscore of the individual parser taken from our tun-ing data.
Third we use plural voting weights(De Pauw et al, 2006) based on parser ranks fromour tuning data.
Due to the success of Plural vot-ing, we try to exaggerate the differences in theparsers by using UAS10 weighting.
All four ofthese are simple weighting techniques but even intheir simplicity we can see the benefit of this typeof combination in an ensemble parser.?
Uniform Weights: an edge in the graph getsincremented +1 weight for each matchingedge in each parser.
If an edge occurs in 4parsers, the weight is 4.?
UAS Weighted: Each edge in the graph gets21incremented by the value of it?s parsers in-dividual accuracy.
So in the UAS resultsin Table 1 an edge in Charniak?s tree gets.92 added while MST gets .86 added to ev-ery edge they share with the resulting graph.This weighting should allow us to add poorparsers with very little harm to the overallscore.?
Plural Voting Weights: In Plural Votingthe parsers are rated according to their rankin our tuning data and each gets a ?vote?based on their quality.
With N parsers thebest parser gets N votes while the last placeparser gets 1 vote.
In this paper, Charniakreceived 5 votes, Stanford received 4 votes,MST Parser received 3 votes, Malt Parserreceived 2 votes, and Zpar received 1 vote.Votes in this case are added to each edge asa weight.?
UAS10: For this weighting scheme we tookeach UAS value to the 10th power.
This gaveus the desired affect of making the differ-ences in accuracy more apparent and givingmore distance from the best to worse parser.This exponent was empirically selected fromresults with our tuning data set.4 ResultsTable 1 contains the results of different parsercombinations of the 5 parsers and Table 2 showsthe baseline scores of the respective individualparsers.
The results indicate that using twoparsers will result in an ?average?
score, and nocombination of 2 parsers gave an improvementover the individual parsers, these were left outof the table.
Ensemble learning seems to start tohave a benefit when using 3 or more parsers with afew combinations having a better UAS score thanany of the baseline parsers, these cases are in boldthroughout the table.
When we add a 4th parserto the mix almost all configurations lead to animproved score when the edges are not weighteduniformly.
The only case in which this does notoccur is when Stanford?s Parser is not used.Uniform voting gives us an improved score in afew of the model combinations but in most casesdoes not produce an output that beats the best in-dividual system.
UAS weighting is not the bestoverall but it does give improved performance inthe majority of model combinations.
Problemati-cally UAS weighted trees do not give an improvedaccuracy when all 5 parsers are used.
Given theslight differences in UAS scores of the baselinemodels in Table 2 this is not surprising as thebest graph edge can be outvoted as the numberof N parsers increases.
The slight differences inweight do not seem to change the MST parse dra-matically when all 5 parsers are used over Uni-form weighting.
Based on the UAS scores learnedin our tuning data set, we next looked to amplifythe weight differences using Plural Voting.
Forthe majority of model combinations in Plural vot-ing we achieve improved results over the individ-ual systems.
When all 5 parsers are used togetherwith Plural Voting, the ensemble parser improvesover the highest individual parser?s UAS score.With the success of Plural voting we looked toamplify the UAS score differences in a more sys-tematic way.
We looked at using UASx wherex was found experimentally in our tuning data.UAS10 matched Plural voting in the amount ofsystem combinations that improved over their in-dividual components.
The top overall score iswhen we use UAS10 weighting with all parsers.For parser combinations that do not feature Char-niak?s parser, we also find an increase in over-all accuracy score compared to each individualparser, although never beating Charniak?s individ-ual score.To see the maximum accuracy a hybrid combi-nation can achieve we include an Oracle Ensem-ble Parser in Table 1.
The Oracle Parser takesthe edges from all dependency trees and only addseach edge to the Oracle Tree if the correspondingedge is in the reference tree.
This gives us a ceil-ing on what ensemble learning can achieve.
Aswe can see in Table 1, the ceiling of ensemblelearning is 97.41% accuracy.
Because of this highvalue with only 5 parsers, ensemble learning andother hybrid approaches should be a very prosper-ous area for dependency parsing research.In (Ku?bler et al, 2009) the authors confirm thattwo parsers, MST Parser and Malt Parser, givesimilar accuracy results but with very differenterrors.
MST parser, a maximum spanning treegraph-based algorithm, has evenly distributed er-rors while Malt Parser, a transition based parser,has errors on mainly longer sentences.
This re-22System Uniform UAS Plural UAS10 OracleWeighting Weighted Voting Weighted UASCharniak-Stanford-Mst 91.86 92.27 92.28 92.25 96.48Charniak-Stanford-Malt 91.77 92.28 92.3 92.08 96.49Charniak-Stanford-Zpar 91.22 91.99 92.02 92.08 95.94Charniak-Mst-Malt 88.80 89.55 90.77 92.08 96.3Charniak-Mst-Zpar 90.44 91.59 92.08 92.08 96.16Charniak-Malt-Zpar 88.61 91.3 92.08 92.08 96.21Stanford-Mst-Malt 87.84 88.28 88.26 88.28 95.62Stanford-Mst-Zpar 89.12 89.88 88.84 89.91 95.57Stanford-Malt-Zpar 88.61 89.57 87.88 87.88 95.47Mst-Malt-Zpar 86.99 87.34 86.82 86.49 93.79Charniak-Stanford-Mst-Malt 90.45 92.09 92.34 92.56 97.09Charniak-Stanford-Mst-Zpar 91.57 92.24 92.27 92.26 96.97Charniak-Stanford-Malt-Zpar 91.31 92.14 92.4 92.42 97.03Charniak-Mst-Malt-Zpar 89.60 89.48 91.71 92.08 96.79Stanford-Mst-Malt-Zpar 88.76 88.45 88.95 88.44 96.36All 91.43 91.77 92.44 92.58 97.41Table 1: Results of the maximum spanning tree algorithm on a combined edge graph.
Scores are in bold whenthe ensemble system increased the UAS score over all individual systems.Parser UASCharniak 92.08Stanford 87.88MST 86.49Malt 84.51Zpar 76.06Table 2: Our baseline parsers and corresponding UASused in our ensemble experimentssult comes from the approaches themselves.
MSTparser is globally trained so the best mean solu-tion should be found.
This is why errors on thelonger sentences are about the same as the shortersentences.
Malt Parser on the other hand uses agreedy algorithm with a classifier that chooses aparticular transition at each vertex.
This leads tothe possibility of the propagation of errors furtherin a sentence.
Along with this line of research,we look at the error distribution for all 5 parsersalong with our best ensemble parser configura-tion.
Much like the previous work, we expect dif-ferent types of errors, given that our parsers arefrom 3 different parsing techniques.
To examineif the ensemble parser is substantially changingthe parse tree or is just taking the best parse treeand substituting a few edges, we examine the partof speech accuracies and relative error reductionin Table 3.As we can see the range of POS errors variesdramatically depending on which parser we ex-amine.
For instance for CC, Charniak has 83.54%accuracy while MST has only 71.16% accuracy.The performance for certain POS tags is almostuniversally low such as the left parenthesis (.Given the large difference in POS errors, weight-ing an ensemble system by POS would seem likea logical choice in future work.
As we can seein Figure 2, the varying POS accuracies indicatethat the parsing techniques we have incorporatedinto our ensemble parser, are significantly differ-ent.
In almost every case in Table 3, our ensembleparser achieves the best accuracy for each POS,while reducing the average relative error rate by9.82%.The current weighting systems do not simplydefault to the best parser or to an average of all er-rors.
In the majority of cases our ensemble parserobtains the top accuracy.
The ability of the en-semble system to use maximum spanning tree ona graph allows the ensemble parser to connectnodes which might have been unconnected in asubset of the parsers for an overall gain, whichis preferable to techniques which only select thebest model for a particular tree.
In all cases,our ensemble parser is never the worst parser.
In23POS Charniak Stanford MST Malt Zpar Best Relative ErrorEnsemble ReductionCC 83.54 74.73 71.16 65.84 20.39 84.63 6.62NNP 94.59 92.16 88.04 87.17 73.67 95.02 7.95VBN 91.72 89.81 90.35 89.17 88.26 93.81 25.24CD 94.91 92.67 85.19 84.46 82.64 94.96 0.98RP 96.15 95.05 97.25 95.60 94.51 97.80 42.86JJ 95.41 92.99 94.47 93.90 89.45 95.85 9.59PRP 97.82 96.21 96.68 95.64 95.45 98.39 26.15TO 94.52 89.44 91.29 90.73 88.63 94.35 -3.10WRB 63.91 60.90 68.42 73.68 4.51 63.91 0.00RB 86.26 79.88 81.49 81.44 80.61 87.19 6.77WDT 97.14 95.36 96.43 95.00 9.29 97.50 12.59VBZ 91.97 87.35 83.86 80.78 57.91 92.46 6.10( 73.61 75.00 54.17 58.33 15.28 73.61 0.00POS 98.18 96.54 98.54 98.72 0.18 98.36 9.89VB 93.04 88.48 91.33 90.95 84.37 94.24 17.24MD 89.55 82.02 83.05 78.77 51.54 89.90 3.35NNS 93.10 89.51 90.68 88.65 78.93 93.67 8.26NN 93.62 90.29 88.45 86.98 83.84 94.00 5.96VBD 93.25 87.20 86.27 82.73 64.32 93.52 4.00DT 97.61 96.47 97.30 97.01 92.19 97.97 15.06RBS 90.00 76.67 93.33 93.33 86.67 90.00 0.00IN 87.80 78.66 83.45 80.78 73.08 87.48 -2.66) 70.83 77.78 96.46 55.56 12.50 72.22 4.77VBG 85.19 82.13 82.74 82.25 81.27 89.35 28.09Average 9.82Table 3: POS accuracies for each of our systems that are used in the ensemble system.
We use these accuraciesto obtain the POS error distribution for our best ensemble system, which is the combination of all parsers usingUAS10 weighting.
Relative error reduction is calculated between our best ensemble system against the CharniakParser which had the best individual scores.24Figure 2: POS errors of all 5 parsers and the best en-semble systemcases where the POS is less frequent, our ensem-ble parser appears to average out the error distri-bution.5 ConclusionWe have shown the benefits of using a maxi-mum spanning tree algorithm in ensemble learn-ing for dependency parsing, especially for thehybrid combination of constituent parsers withother dependency parsing techniques.
This en-semble method shows improvements over the cur-rent state of the art for each individual parser.
Wealso show a theoretical maximum oracle parserwhich indicates that much more work in this fieldcan take place to improve dependency parsing ac-curacy toward the oracle score of 97.41%.We demonstrated that using parsers of differ-ent techniques, especially including transformedconstituent parsers, can lead to the best accuracywithin this ensemble framework.
The improve-ments in accuracy are not simply due to a fewedge changes but can be seen to improve the ac-curacy of the majority of POS tags over all indi-vidual systems.While we have only shown this for English,we expect the results to be similar for other lan-guages since our methodology is language in-dependent.
Future work will contain differentweighting mechanisms as well as application toother languages which are included in CoNLLdata sets.6 AcknowledgmentsThis research has received funding from theEuropean Commission?s 7th Framework Pro-gram (FP7) under grant agreement n?
238405(CLARA)ReferencesSabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning, CoNLL-X ?06,pages 149?164, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,ACL ?05, pages 173?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Guy De Pauw, Gilles-Maurice de Schryver, and PeterWagacha.
2006.
Data-driven part-of-speech tag-ging of kiswahili.
In Petr Sojka, Ivan Kopecek, andKarel Pala, editors, Text, Speech and Dialogue, vol-ume 4188 of Lecture Notes in Computer Science,pages 197?204.
Springer Berlin / Heidelberg.Thomas G. Dietterich.
2000.
Ensemble methods inmachine learning.
In Proceedings of the First In-ternational Workshop on Multiple Classifier Sys-tems, MCS ?00, pages 1?15, London, UK.
Springer-Verlag.Jason Eisner.
1996.
Three new probabilistic mod-els for dependency parsing: An exploration.
InProceedings of the 16th International Conferenceon Computational Linguistics (COLING-96), pages340?345, Copenhagen, August.Gholamreza Haffari, Marzieh Razavi, and AnoopSarkar.
2011.
An ensemble model that combinessyntactic and semantic clustering for discriminativedependency parsing.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages710?714, Portland, Oregon, USA, June.
Associa-tion for Computational Linguistics.Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsenEryigit, Bea?ta Megyesi, Mattias Nilsson, andMarkus Saers.
2007.
Single malt or blended?a study in multilingual parser optimization.
InProceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007, pages 933?939.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion for25English.
In Proceedings of NODALIDA 2007,pages 105?112, Tartu, Estonia, May 25-26.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.S.
Ku?bler, R. McDonald, and J. Nivre.
2009.
Depen-dency parsing.
Synthesis lectures on human lan-guage technologies.
Morgan & Claypool, US.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: the Penn Treebank.
Com-put.
Linguist., 19:313?330, June.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceed-ings of Human Language Technology Conferenceand Conference on Empirical Methods in NaturalLanguage Processing, pages 523?530, Vancouver,British Columbia, Canada, October.
Association forComputational Linguistics.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of ACL-08: HLT, pages950?958, Columbus, Ohio, June.
Association forComputational Linguistics.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007a.
The CoNLL 2007 shared taskon dependency parsing.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL2007, pages 915?932, Prague, Czech Republic,June.
Association for Computational Linguistics.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, Gulsen Eryigit, Sandra Ku?bler, SvetoslavMarinov, and Erwin Marsi.
2007b.
MaltParser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Martin Popel, David Marec?ek, Nathan Green, andZdene?k Z?abokrtsky?.
2011.
Influence of parserchoice on dependency-based mt.
In Proceedings ofthe Sixth Workshop on Statistical Machine Trans-lation, pages 433?439, Edinburgh, Scotland, July.Association for Computational Linguistics.Kenji Sagae and Alon Lavie.
2006.
Parser combi-nation by reparsing.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,Companion Volume: Short Papers, pages 129?132,New York City, USA, June.
Association for Com-putational Linguistics.Kenji Sagae and Jun?ichi Tsujii.
2007.
Depen-dency parsing and domain adaptation with LR mod-els and parser ensembles.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL2007, pages 1044?1050, Prague, Czech Republic,June.
Association for Computational Linguistics.Mihai Surdeanu and Christopher D. Manning.
2010.Ensemble models for dependency parsing: cheapand good?
In Human Language Technologies:The 2010 Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, HLT ?10, pages 649?652, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas.
2008.TectoMT: Highly Modular MT System with Tec-togrammatics Used as Transfer Layer.
In Proceed-ings of the 3rd Workshop on Statistical MachineTranslation, ACL, pages 167?170.Daniel Zeman and Zdene?k Z?abokrtsky?.
2005.
Im-proving parsing accuracy by combining diverse de-pendency parsers.
In In: Proceedings of the 9th In-ternational Workshop on Parsing Technologies.Yue Zhang and Stephen Clark.
2011.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational Linguistics, 37(1):105?151.26
