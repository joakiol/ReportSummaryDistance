PROGRESS IN INFORMATION EXTRACTIONRalph Weischedel, Sean Boisen, Daniel Bikel, Robert Bobrow, Michael Crystal, WilliamFerguson, Allan Wechsler and the PLUM Research GroupBBN Systems and Techno log ies70 Fawcett  StreetCambr idge,  MA 02138weischedel  @bbn.com617-873-34960.
ABSTRACTThis paper provides a quick summary of thefollowing topics: enhancements to the PLUMinformation extraction engine, what we learned fromMUC-6 (the Sixth Message Understanding Conference),the results of an experiment on merging templates fromtwo different information extraction engines, a learningtechnique for named entity recognition, and towardsinformation extraction from speech.1.
ENHANCEMENTS TO THE PLUMINFORMATION EXTRACTIONENGINE1.1 What's NewRecently BBN has ported part or all of the PLUMsystem to new languages (Chinese and Spanish) andnew domains (name finding, heterogeneous newswiresources, and labor negotiations).
Coupled with thisthere have been several improvements which aredescribed next.1.1.1 InfrastructureThe message reader was improved.
As before, it takesdeclarative finite state descriptions (regular expressions)defining document structures.
We added a general parserfor SGML to simplify text descriptions of documentsthat contain SGML markup.
The overall impact hasbeen more robust handling of documents with SGMLand less effort to specify this format.In MUC-5 and MUC-6 the output of informationextraction has been a multi-level object-oriented datastructure.
Although appropriate for an object-orienteddata base, the structures frequently were notstraightforwardly mappable from linguistic structures.This was one of the reasons that prior to MUC-6,semantic inferencing was added to PLUM's discourseprocessor.
The discourse processor, when invoked on asentence, first adds all semantic predicates associatedwith a sentence to the semantic database.
Referenceresolution is performed, and the relevant semanticobjects are unified in the database as a result.
At thispoint, a set of inference rules are applied, and any thatsucceed will add more predicates to the database.The semantic database is used as the repository ofsemantic information for all the objects mentioned in amessage.
It is queried frequently by the discourseprocessor.
The semantic inference component performstwo primary functions:?
Perform inferences to normalize different butequivalent semantic representations present in thedatabase, where the differences may have stemmedfrom syntactic variations, or from incompleteknowledge at the time they were generated; and?
Generate more complex representations (closer tothe template structure) from simpler, "flatter"semantics (closer to the linguistic structure).In the ST task of MUC-6, the inference module wasused, for example, to try to infer the corporationsinvolved in a succession event.
We represented eachinstance where a job position is mentioned as a JOB-SITUATION semantic object.
JOB-SITUATIONs are aflattened version of the SUCCESSION and IN-AND-OUT objects.
An inference rule queried the semanticdatabase to determine whether the JOB-SITUATIONPERSON was involved in multiple JOB-SITUATIONs,if so, it would add predicates for the JOB-SITUATION'sOTHER-ORG and REL-OTHER-ORG.1.1.2 Capitalizing on lightweightprocessesWe have begun making a distinction betweenlightweight techniques and heavyweight processing.Lightweight techniques are those that rely only on localprocessing, do not involve deep understanding, and canbe optimized.
Example lightweight echniques are forSGML recognition, hidden Markov models, finite statepattern recognition, text classifiers, text indexing andretrieval and SGML output.
BBN's IdentiFinder TM(Figure 1) is made up solely of lightweight algorithms,and was entereed in the MUC-6 Named Entity task(recognizing named organizations, named persons,named locations, monetary amounts, dates, times, etc.
).127MessageMessage Reader \]Morphological Analyzer ILexical Pattern MatcherSGML/Annotation Generator \]OuStFormat & SGML HandlingIdentification ofEntitiesOutput EntitiesFigure 1: IdentiFinder System Architecture: Rectangles represent domain-independent, language-independent algorithms; ovals represent knowledge basesHeavyweight processing includes procedures thatdepend on global evidence, involve deeperunderstanding, and are research oriented.
Full parsers,semantic inference, and co-rreference algorithms areexamples.
PLUM (Figure 2) employs both lightweightand heavyweight techniques.
PLUM was employed inboth MUC-6 template tasks (TE and ST).Two new heavyweight algorithms were developed inthe last year.
One is a full parser of English, using astatistically learned ecision procedure; SPATTER hasachieved the highest scores yet reported on parsingEnglish text (Magerman, 1995).
Because themeasurable improvement in parsing is so great(compared to manually constructed parsers), it appearsto offer a qualitatively better parser.
We are looking ormat De sc r ip t ion~MessageMessage Reader IM orphologieal Analyzer \]Lexieai Pattern Matcher \]Fast Partial Parser \]Semantic Interpreter \]entene -Level Pattern MatcherDiscourse \[Format & S GML HandlingInitial Iden tification of EntitiesGrouping Words into Meaningful PhrasesEstablish Relationships (within sentences)Establish Relationships Overall ----~Template/Annotation Generator I Output Entities and Relationships |OutputFigure 2: PLUM System Architecture: Rectangles represent domain-independent, language-independentalgorithms; ovals represent knowledge bases.128forward to trying it on information extraction problems.The second is the semantic inference procedure discussedin the previous ection.1.1.3 Domain-independent systemsThe NE configuration of IdentiFinder is stand-aloneand domain-independent; no information regardingsuccession of corporate officers is employed.
Similarly,the MUC-6 TE (properties of organizations and persons)configuration of PLUM uses no information regardingsuccession of corporate officers, and therefore can beused on other domains.
We believe that development ofother broadly applicable information extractionfunctionality such as NE and TE will be a win,maximizing the value of defining reusable knowledgebases for information extraction.1.2 Tools for Porting and Maintainingthe PLUM Information ExtractionSystemCustomizing extraction systems to a givenapplication domain is a difficult process.
It is timeconsuming and requires highly specialized skills.
BBNhas developed the NLU Shell, which provides tools thatsimplify many of the tasks associated with portinginformation extraction to a new domain, and supportmaintenance once the port is complete.
Such a tool kitwould be unimportant if we did not anticipate the needfor creating many extraction applications.
In fact,however, message processing systems typically mustdeal with a large number of distinct applications.The NLU Shell software is a collection ofindependent editors, some tools for managing a set offactbases, and a compiler for compiling the factbasesinto an NLU Application.
Its major components are:?
Top Level Executive (TLX): Primarily a humaninterface tool for invoking the other NLU Shellcomponents.?
Text Description Editor (TDE): Maintains the TextDescription Factbase, a data file containingknowledge about the structure of messages, i.e.where word, sentence, and paragraph boundariesfall.?
Output Record Editor (ORE): Maintains theOutput Record Factbase, a data file containingknowledge about how the NLU Application Outputshould be formatted.?
Semantic Model Editor (SME): Maintains theSemantic Model Factbase, a data file containing theconcepts which the NLU Application uses torepresent the information contained withinmessages.Part of Speech Editor (POSE): Maintains the partof speech models that are used to determine wordcategories in the face of ambiguity (e.g.
is "hit" averb or a noun).
Generates a new word list oftagged words not found in the lexicon.New Word Editor (NWE): Updates the dictionaryof words (lexicon) to be recognized by the NLUApplication.
Enters new words found by POSEinto the lexicon.Lexicon Editor (LE): Maintains the dictionary ofwords (lexicon) to be recognized by the NLUApplication.
Adds and edits word meanings basedon concepts and their attributes in the semanticmodel.Case Frame Editor (CFE): Maintains the lexicalCase Frame Factbase, a data file of how to inferinformation by analyzing how verbs relate tological sentence lements (e.g.
subjects or objects)and to the objects of their prepositions.Fill Rule Editor (FRE): Maintains the TriggerRule Factbase, a data file containing the knowledgedescribing when output records should be createdgiven the existence of semantic oncepts.
Also,maintains the Fill Rule Factbase, a data filecontaining knowledge describing how to fill fieldsof output records when they are generated by atrigger in the Trigger Rule Factbase.Consistency Checkers ( CC' s): Reportinconsistencies within and between factbases and,also, report states of the factbases that suggest areasfor further work (e.g.
output rrecords that can neverbe created).
There is one consistency checker forevery factbase.Application Generator (AG): Compiles thedifferent factbases into an NLU Application.1.2.1 Evaluation procedureA programmer with no computational linguisticsbackground performed a usability evaluation of the NLUShell toolkit.
The evaluation exercised only three ofthe system's eight editors.
First, the Fill Rule Editorwas used to attempt o improve performance on anexisting application.
Second, the New Word Editor andthe Lexicon Editor were used to add a list of new wordsto the application's dictionary.
The results of theevaluation follow.1.2.2 System strengthsThe NLU Shell provides a way for non-programmersto build and maintain language processing applications.The use of a GUI and a database in place of files ofsource code and data (which must be edited as text)represents a fundamental advance in making natural129language technology widely available.
While the NLUShell users need detailed familiarity with the processesand knowledge needed for extracting formatted data fromnatural anguage, they do not need to be programmers.This is not only because the need to know a specificprogramming language is eliminated; the greater benefitof using NLU Shell is that the deep understanding ofnatural anguage processing algorithms and heuristicsthat went into the creation of the NLU Shell is notrequired for its use.
This constitutes a qualitativechange in the construction and use of languageprocessing software.On a more mundane level the NLU Shell GUI isstructured as a set of cascading diagrams depicting theunderlying data extraction process.
Thus, the user isguided through the application building process.
Theuse of a hierarchy of access screens keeps each onesimple.There is design and implementation leveldocumentation for the system as well as acomprehensive user's manual.1.2.3 Areas for ImprovementDuring the evaluation several areas needingimprovement in the NLU system emerged.
These aredivided into three categories: user interface, system ?speed and missing features.
The interface is fromcascading menus, which allowed the initial screen to besimple, at times awkward to use when navigatingamong the tools.
It was often necessary to move fromone tool to another to complete an operation (e.g.
usingthe semantic model editor during word definition to pickout the right meaning for a word).
The need to pop upadditional windows repeatedly made editing multipleentries time consuming.
Common processes can befurther streamlined in the interface.
?Compiling the complete NLU Shell application inorder to test changes is time consuming.
Addingincremental compilation functionality would improvethis.A handful of missing features were identified, such asthe need to "find" words in the lexicon rather thenscrolling.
An available filtering mechanism is valuablebut does not replace a "find" function.1.2.4 ConclusionTo the extent covered by this evaluation, the NLUShell is a successful first prototype of a GUI-basednatural language processing application builder.
Itenables aspects of the maintenance and construction ofsuch systems to be performed without knowledge ofspecific programming languages and environments.Further work would make it even more effective.In the test conducted here, vocabulary wassuccessfully added and slot filling performanceimproved, using only the NLU Shell tools, however theprocess could be improved.
With the improvementsmentioned above, plus a more extensive review of thesystem for other enhancements, the NLU Shell couldsignificantly reduce the time and effort needed to build anatural anguage processing application and make thisprocess available to knowledge ngineers who are notprogrammers a well.2.
LESSONS FROM MUC-62.1 Participation in MUC-6For MUC-6, there were three different applicationdimensions that one could choose to participate in, asfollows:Named Entity (NE), recognition of namedorganizations, named persons, dates and times,monetary amounts, and percentages.
The task inpnnciple is domain-independent, though in MUC-6 it was evaluated only on documents obtained bya query for documents about change in corporateofficers.Template Element (TE), extraction oforganizations, persons, and properties of them.
Forinstance, organizations have full names, aliases,descriptions, locations, and types (company,government, or other).
Persons have full names,aliases, and titles.
Like NE, this really is domain-independent, though in MUC-6 it was evaluatedonly on documents obtained by a query fordocuments about change in corporate officers.Scenario Template (ST), a domain-specific, fulltemplate xtraction task.
For MUC-6, the task waschange in corporate officers.
This included threelevels of objects; the lowest level of objects isidentical to the TE objects.
Information extractedabove that level included the individual, theposition they were taking on (if any), thecorporation in which the position is held, theposition they were leaving (if reported), thecorporation of the former position, the reason forthe change (e.g., retirement), whether the holderwas acting in that position or not, etc.One approach to the three tasks is to have a fullinformation extraction system apply all its knowledgeto all three tasks, simply providing three different levelsof output.
Certainly that would appear to offer thehighest scores, since even the domain-independent taskswere evaluated only on domain-dependent data.
Manygroups chose exactly that route.130Figure 3: Relation of BBN Systemsin MUC-6However, we chose a path represented in Figure 3below so that there would be domain-independentversions of the software for efforts other than MUC,such as the TIPSTER program.
The more complexsystems are built on top of the simpler systems in order"to minimize duplication of effort and maximizeknowledge transfer.
The NE task is the simplest taskand makes use of only lightweight processes, the firstthree modules of the PLUM system (the message reader,the morphological analyzer, and the lexical patternmatcher, which together form IdentiFinderrU).
This NEsystem is represented at the core of Figure 3, since itwas the core for all three tasks.The TE task takes the entity names found by the NEsystem, and merges multiple references to the sameentity using syntactic and semantic information.
Sincethe components are the same in the TE and ST systemconfigurations, and since the knowledge bases of TE areinherited by ST, we have represented this in Figure 3via making TE the next innermost circle.
By thisorganization, the knowledge bases of TE do not includedomain-specific knowledge, and domain-specificknowledge is localized only in ST.As a consequence, we have a domain-independentlow-level template xtraction application in TE.2.2 Lessons LearnedThe most exciting lesson we learned is that nearhuman performance in named entity recognition iswithin the state of the art for mixed case English.Several systems performed at 90% or above.
SinceMUC-6 we have improved IdentiFinder's prediction ofaliases once a name has been seen and added rules forlow frequency cases, e.g., for names that are quiteunlike Western European names.
Our NE and TEsystems employ no domain-specific knowledge bydesign.
They should therefore work well on other text,not specific to change in corporate officers.Compared to PLUM's previous performance inMUC-3, -4, and -5, our progress was much more rapidand our official score was higher than in any previoustemplate fill task.
Figure 4 shows progress in portingPLUM to ST, evaluating periodically on blind testmaterial.Progress  on Blind Test Data For STO60 T0 uU.1 7 12 16 18 23Work DaysFigure 4: Measured Progress on the NewDomain.We make two general observations.
First, the stateof the art has progressed greatly in portability in the lastfour years.
For MUC-3, some high performing roupsinvested a small number of person years.
By contrast,several groups this year achieved an F in the 50s in 30calendar days.We suspect there are other basic objects that would bebroadly applicable (as TE seems to be) and that theremay be many generically useful event types andrelationships that can be defined.
This should broadenthe utility of information extraction technology and cutits cost.In retrospect, lightweight processes carried most ofthe burden in our TE system.
We believe that with alittle more effort, we could achieve an F score of 80 inTE with only lightweight techniques.
It would bevaluable to see how far only lightweight echniques cango, not just on TE, but on other tasks of extractingbasic objects and direct relationships between them.Yet, we believe that we are only beginning tounderstand techniques for learning domain-independentknowledge and domain-dependent k owledge.
Far more131can be achieved.
BBN particularly would like toinvestigate how statistical algorithms over largeunmarked corpora can effectively extrapolate from a fewtraining examples, such as in ST in MUC-6, to prrovidegreater coverage.
For example, statistical techniquesmay have suggested the importance of "hire," a verbwhich many groups did not happen to define.Second, since there has been a marked improvementin the quality of full parsers, now achieving an F in thehigh 80s (Magerman, 1995), we believe it is nowfeasible to consider using full parsers again.
Therationale is straightforward: for full templates (e.g., ST)scores have been mired with an F in the 50s ever sinceMUC-3 in 1991.
Pattern matching has given us veryrobust, very portable technology, but has not brokenthe performance barrier all systems have run up against.Recent (statistical) full parsers (e.g., BBN's, IBM's, andUPenn's) have such quantitatively better performancethat they are qualitatively better.
We believe this offersthe opportunity to again try heavyweight techniques toattempt deeper understanding.
Pattern matchingtechniques will still have a crucial role for domain-specific details, but we believe overall improvementwill be achieved by deeper understanding.3.
MERGING EXTRACTIONOUTPUT FROM TWO SYSTEMSThe parallel structure of the MUC competition, withmany different systems attempting to extract he samedata from the same material, creates an opportunity toachieve even greater performance by combining theresults of two or more systems.
A combined systemcould use the outputs of several different systems toproduce performance l vels potentially superior to thoseof any one component alone.
BBN has produced anarchitecture for system output combination.The success of a data extraction system is judged byhow well its results fit a hand-coded target set (the key)of structured template objects.
For our target stories weselected the MUC-5 story set - -  one hundred Englishnewspaper articles dealing with joint ventures.Our combined system was based on the output of ourown BBN PLUM system, configured for EJV, and theoutput of the Shogun system developed at GE, andmade available to us through Lockheed-Martin.Combining the outputs of the PLUM and Shogunsystems appeared promising because the two systemshad different performance profiles in terms of precisionand recall, but similar F-scores.
Shogun had the betterrecall score - -  it extracted a higher proportion of theinformation contained in the key templates.
PLUM hadthe better precision score - -  it extracted less spuriousand incorrect information.
Overall, Shogun had thehigher F-score because it was optimized for maximal Fwhile PLUM had been optimized for a mix of high Fand low errors.
We hoped that by combining the twosystems we could produce a system with a high enoughrecall and precision to yield a higher F-score than eithersystem could achieve alone.In the hope that our results could be applied tocombining any two template generating systems, weignored the methods and mechanisms employed by thetwo extraction programs, and used only their output.For each story, this output consists of a set ofinterconnected frames pecifying the content of the jointventures as described in the source text.
Each systemproduces an overall structure for each input story.
Thistemplate organizes a set of joint-venture frames calledTie-Up-Relationships (or TURs), one for each jointventure mentioned in the story.
These TUR frames, inturn, organize many smaller frames that describe theorganizations, people, places, and activities involved inthe joint venture.3.1 EffortOur first step was to massage the data from thePLUM and Shogun systems into a form that could beread into a merging program.
In parallel with this dataconversion we built an initial version of a data-combining program.
This initial system matched thewhole templates produced by the two systems for eachstory.
It aligned the joint-venture frames and otherframes from each system and then used variousheuristics to combine the outputs frame by frame.
Thisinitial approach proved unwieldy as we had tosimultaneously deal with the problems of aligningoutput from the two systems and merging that output.Additionally, we realized that we needed to gather moredata to begin to learn about what each system was doingwell compared to the other.Useful dissimilarities in the detailed performance ofthe two systems proved hard to identify.
We examinedthe score reports for PLUM and Shogun and found thattheir performance in almost every category matchedtheir overall performance.
PLUM had higher precision,while Shogun had better ecall.
Shogun's F-score wasalways slightly higher.
This similarity rules outstrategies that take advantage of contexts in which onesystem is superior to the other, lWe decided to simplify the combining task by usingthe scoring program (which had been used to test thePLUM and Shogun systems before and during theMUC-5 competition) to match the PLUM and Shogunframes.1 This similarity in performance profiles may indicate asimilarity in the underlying methodology of the twosystems.132In order to get a sense of how well we couldpotentially perform by treating the overall system-combining task as one of choosing templates, weperformed an experiment as follows: we combined allthe frames from both systems and scored the combinedresult against he key.
We then took only the framesfrom this combined result that matched frames in theanswer key, and we scored these selected frames as ifthey were the result of some ideal combining system.This score represents an upper bound on how well wecould possibly do if our combining strategies allowedus to choose exactly the best frames provided by eachsystem.
This experiment showed a large improvement,since the no spurious objects would be produced, onlythe occasional spurious lot.Our first step was to score the PLUM system againstthe Shogun system as if the Shogun system were thekey.
The mappings produced by this scoring were thenused by filtering heuristics.
Since the scorer's mappingwas intended to maximize overall F-score, thealignment i produced was well suited to our purposes.Our first heuristic (and one of the best we found) was totake only the frames from the system with the bestrecall (Shogun) that also matched frames from thesystem with the best precision (PLUM).
This subset ofShogun frames had a higher precision than Shogun'soutput alone, but its recall was low enough that its F-score was also worse than that of Shogun alone.
The"opposite" strategy, taking all the Shogun frames andadding those PLUM frames that did not match Shogunframes, improved recall somewhat but lost on precisionand resulted in a decrease in overall f-score.We tried many variations on this theme for matchingPLUM/Shogun frames, as well as combining variousmatching approaches with the use of simple statisticalmethods on individual frames to judge their likelihoodof matching the key.
While we uncovered many filtersthat had some predictive value, none of the tests wedevised was of high enough quality to allow us to raiseF-scores for the combined system over those of Shogunalone.Our results indicated that the matching process usedby the scorer was sensitive to perturbations of theinternal structure of the joint-venture frames.
Thematching processes for related frames wereinterdependent, so that the removal of a "good" frameoften caused other frames which pointed to it to fail tomatch.
This occurred because these other frames reliedon the match provided by the good frame in order to bematched to the key themselves.
Thus, even though wefound fairly good strategies for eliminating bad frames,the damage done by eliminating even a few good framesmore than outweighed the benefit of eliminating manybad frames.
To circumvent this difficulty, we decided totry using strategies that would select among wholejoint-venture structures rather than selecting individualframes.We tried another experiment in which we selected theShogun TURs (entire joint-venture descriptions) havingthe highest percentages of individual frames thatmatched the key.
Scoring just these templates produceda combined result with a better F-score than that ofShogun alone, though not nearly so good as the scorefor choosing just the right individual frames in theprevious experiment.
Thus, it would be possible toget an improved F-score by selecting TURs if we had agood enough selection heuristic or filter.
The best filterwe found was simply to select hose Shogun templatesthat had the highest percentage of frame matches to theframes produced by EJV.
By varying the acceptancethreshold, we hoped to find a level at which we wouldget enough increase in precision to offset he decrease inrecall.
The results are graphed below in Figure 5.i I:='==:n I I ERRSyst~rn \[=~1~ r Filtear e FilgterFigure 5: Trade-off in undergeneration andovergeneration in the combined system.This graph refers to three measures: 1) under-generation (UG), 2) over-generation (OG) and 3) error(ERR).
These are the values produced by the MUC-5scoring system.
All three are expressed as percentages.UG varies inversely with recall; OG varies inverselywith precision; and ERR varies inversely with F-score.These inverse relationships are not linear, but this willnot matter to the arguments presented here.As the graph shows, the  lowest error for thecombined system occurs when the filter is loosened allthe way and all Shogun frames are used.Undergeneration rises at the other points, but at eachpoint it more than offsets the gain in overgenerafion.As a result, ERR rises (usually) slightly in each case.3.2 Conc lus ionsThe template merging experiment providedsubstantial range in recall versus precision, i.e.,undergeneration versus overgeneration.
Thisnoteworthy itself.ainis133Nevertheless, we did not achieve a breakthrough inoverall F-score, i.e., in ERR.
There are severalpotential contributing factors:The merging occurred after output was produced;perhaps results would have been better bycombining results earlier in linguistic processing.?
The systems in our experiment perhaps produce toosimilar results.The scoring mechanism suffers from the linchpinphenomenon.
In flatter template structure withoutthe linchpin phenomenon, the penalty for a mistakein merging templates would be less severe.One other possibility to investigate is to use the twosystems in parallel (take anything that either produces)or in series (take only what both accept).
The parallelsystem increases F-score only if the two systems havemuch better precision than recall, while the series caseyields improvement only if the two systems have muchbetter ecall than precision.
These two systems fell intoneither category.4.
A LEARNING TECHNIQUE FORNAMED ENT ITY  RECOGNIT IONLike several other groups, we are pioneering researchin automatically learning to extract information basedon examples.
Unlike other groups which have focusedon case-based reasoning or on binary decision trees, weare focusing on statistical learning techniques.4.1 The Appl icationThe first extraction problem that we are tackling islearning to identify entities.
Entity recognition andidentification has been recognized by the community asa core problem and was evaluated in MUC-6 inNovember, 1995 for English and was also evaluated forChinese, Japanese, and Spanish in the recently heldMulti-lingual Entity Task (MET).
An example forEnglish is shown in Figure 6.Other OrgauizationstThe delegation, which included the commander of the U.N. troops inI ~ Bosnia r Lt. Gcn.
SirMichael Rose, t went o the Serb stronghold of ~near Sa~jevo~ for talks with Bosnian Serb leader Radovan Karadzie.1~: l,o~t ons PerstnsFigure 6: Named Entity Task.
An example ofthe named entity task for English.Nothing about our approach is restricted to the namedentity task; other kinds of data could be spotted withsimilar techniques, such as product names, addresses,core noun phrases, verb groups and military units.Previous approaches uch as our own IdentiFindersystem described earlier in this paper and evaluated inMUC-6, have used manually constructed finite statepatterns.
For every new language and every new classof new information to spot, one has to write a new setof rules to cover the new language and to cover the newclass of information.
Though the technique has clearlyresulted in systems with very high performance andvery high speed and has also led to commercialproducts, we believe that the technology based onlearning is highly desirable for the following reasons:1.
Freeing a group's best people from manuallywriting such rules and maintaining them is a betteruse of the time of highly gifted people.2.
A learned system may be brought up with far lesseffort, since both manually constructed rules andlearned systems assume a substantial collection ofannotated textual examples..
In the ideal, the only knowledge of the languagerequired for the learned system would be theexamples of correct output.
Therefore, once alinguist had defined the guidelines for correctoutput, potentially less sophisticated, less trainedspeakers of the language could develop the answerkeys.4.2 ApproachOur approach is represented in Figure 7.
As thediagram shows, there are four requirements to ourapproach.1..A set of training data must be provided; trainingdata consists of sentences and annotations thatrepresent correct output, i.e., an answer key.A model must be defined that states the mappingfrom words to the annotations.. A training algorithm or module must estimate theparameters of the probability model to be learnedfrom the example data, that is, from the sentenceswith their correct annotations.4.
A recognition algorithm or module must apply theprobabilistic, learned model to new sentences toprovide their annotations.Under separate DARPA Jhnding, we have applied thisapproach to the NE problem for English and forSpanish.
Government supplied data in MUC-6 and inMET serve as the training data.
The probabilisticmodel employed is in the general class of hidden134Markov models (HMM), though the HMM usedcurrently is more complex than those traditionally usedin speech recognition and in part-of-speech tagging.Since the SGML-marked text can be straightforwardlyaligned with the answer keys, the training algorithmsimply counts the frequency of events and normalizeswith respect to the event class to estimate theparameters of the model.
For the recognitionalgorithm, the Viterbi algorithm, which is typicallyused in hidden Markov models, is applied.Figure 7: Components of a LearningApproachThe details of the probabilistic model will bedocumented separately.4.3 ExperimentsThe training module and recognition module werefirst tested on English in early March.
Scores on testmaterial in preliminary testing have ranged between 87and 89.
Using the same language model and nothingspecific to Spanish other than Spanish trainingexamples, we are achieving scores even higher than inEnglish.These preliminary results are quite encouraging, sincethey are better than any previously reported scores for alearned system and since they are approaching the scoresof the state-of-the-art for manually built, rule basedsystems.
We would like to test the techniques on quitedifferent languages, uch as Chinese where performancefor manually based systems is lagging behind systemsin English and Spanish.
We would also like to trylanguages where new word formation poses problems,such as in German.The current version assumes three language-specificfeatures: (1) that the system is provided with words asinput (i.e., we have not tackled word segmentation), (2)that sentence boundaries have been found and (3) thatsimple, deterministic computations on words todetermine word-features can be performed, to distinguishdifferent sorts of numbers--which are largely language-independent--and different sorts of capitalization, in thecase of Roman languages.
As to segmentation, othergroups such as the University of Massachusetts haveexplored probabilistic techniques for segmentingChinese; Xerox has reported good results on learning topredict sentence boundaries from example data.5.
TOWARDS INFORMATIONEXTRACTION FROM SPEECHA pioneering effort started under DARPA funding(though not part of the TIPSTER program) is the goalof information extraction from speech.
A goal is toidentify objectives appropriate for future research anddevelopment.
For example, can simple templateinformation (e.g., who did what to whom, where, andwhen) be reliably exlracted?
Is name finding includingrecognizing unknown names feasible?
Can a systemreliably identify the topic of a speech segment?
Asecond goal is to explore the architecture for such asystem.5.1 The ChallengeSchematically, the architecture being investigated isgiven in Figure 8 below; a system with this architecturehas already been demonstrated.
An audio signal isreceived from radio, television, or telephone.
State-of-the-art large vocabulary continuous peech recognition(LVCSR) technology automatically transcribes speech.The output of an LVCSR may be a singletranscription (1-best), the n highest scoringtranscriptions (n-best), or a chart of high scoringalternatives at each point.
To date, we have onlyinvestigated 1-best.As a preliminary experiment, we provided the singlebest transcription to PLUM configured for ST (theMUC-6 domain on succession of corporate officers) inorder to determine the kinds of problems that speechinput would pose.
Consider the example below, wherethe source text is presented first.
The second text is theresult of automatic transcription.
The followingproblems are evident:?
Lack of punctuation?
Lack of reliable mixed case to signal names?
Transcription errors when input is outside the45,000 word vocabulary, which is problematic forinfrequent ames.Text135Diane Creel, recently promoted to chiefexecutive officer, said the return to profitabilityreflects the success of the company's "back-to-basics" strategy focusing on its coregovernment and commercial hazardous-wasteconsulting businesses.Automatic Transcription of SpeechDIANE COELHO RECENTLY PROMOTEDTO CHIEF EXECUTIVE OFFICER SAIDTHE RETURN TO PROFITABILITYREFLECTS THE SUCCESS OF THECOMPANY'S BACK TO BASICSSTRATEGY FOCUSING ON ESCORTGOVERNMENT AND COMMERCIALHAZARDOUS WASTE CONSULTINGBUSINESSES .The effect on information extraction is most notableon names.
Whenever apart of a name (e.g., a person'slast name) is outside the vocabulary, current LVCSRtechnology will find the closest ranscription within thevocabulary, causing one to misrecognize the name, andproviding errorful input to information extraction.
Anexample appears below: Text input is first; theautomatic transcription of a spoken version appearsnext.
Note how the company name is heard correctly(each word is in the vocabulary), but the person's lastname "Barbakow" is misheard as "BARR NOW".TextAs a managing director of Donaldson, Lufkin& Jenrette, Mr. Barbakow also was aninvestment banker for National Medical.Automatic Transcription of SpeechAS A MANAGING DIRECTOR OFDONALDSON LUFKIN AND JENRETI'EMR.
BARR NOW ALSO WAS ANINVESTMENT BANKER FOR NATIONALMEDICAL5.2 Quantifying the ChallengeUsing the evaluation methodology developed underthe DARPA-sponsored Message UnderstandingConferences (MUC), we measured informationextraction performance on the MUC-6 template lement(TE) task.
TE requires recognition of organizations,persons, and properties of them.
Like NE, this really isdomain-independent.To simulate the challenge of speech input, onespeaker read Wall Street Journal texts from the MUC-6corpora and automatically transcribed those texts viaBYBLOS.
Reading these texts in an office environmenthad a 20% error rate, which is better than the currentbest error rate of 27% on broadcast news.
Assummarized in Table 1 and in Figure 9, theeffectiveness of information extraction (on trainingmaterial) dropped from the high 80s to the mid 50s.Cond i t ion  FText: Mixed Case 86 .76Text: Upper Case 80 .86Speech: with Commas 76 .57Speech: 0% Word Error 71Speech: 20% Word Error 53 .28Table 1: Result of prototype experimenton information extraction from speech.Part of the problem is the speech error rate.
This cant! '
' ?
/Figure 8: Architecture of Information Extraction from Speech136be seen in the drop in performance from 0% word errorrate (perfect ranscription) to 20% word error rate.
Asmall part of the problem is the lack of capitalization(upper case only).
The remainder of the degradation iperformance compared to text input is due to otherproblems, such as the lack of punctuation; note howimportant commas are in information extraction.5.3  Hear ing  New NamesThere are at least three ways one could improveLVCSR for names.
The one we believe most likely tosucceed is vastly increasing the vocabulary size.
Insteadof only a 45,000 word vocabulary, which was derivedby including all words occurring at least 50 times inseven years of The Wall Street Journal, suppose we addroughly 200,000 vocabulary items, focusing on lastnames, rare first names, and rare words in organizationlists, e.g., companies listed by Dun & Bradstreet.Based on previous experiments (Asadi, 1991), theimpact should be that?
About half of the occurrences of the additional200,000 newly added words will be correctlyrecognized as the top choice in context.?
The computation for BYBLOS will increase.?
The overall word error rate will increase slightly.That is quite promising, since it is relativelystraightforward, might halve the error rate on names,and has only modest effect on speed and overallaccuracy.A second alternative is to allow the LVSCR systemto enter into the transcription a symbol meaning"something outside the vocabulary".
Recognizing thatsomething is outside the vocabulary is not itself areliable procedure, e.g., perhaps detecting 50% of suchsegments correctly with a 2% false alarm rate.However, even in the 50% correct detections, one stilldoes not know the transcription; at present, a personwould have to transcribe it.
This seems far lesspromising than the first alternative above.A third alternative would be to give a phonetictranscription of out-of-vocabulary items.
While highlyattractive, this seems like a long term research agenda.LVSCR transcription places some demands oninformation extraction.
First, the technology shouldallow for detection of important information even whenfewer cues are present.
In ST, one could produce asuccession relation even if the person name ororganization ame is absent.
Systems must rely farless on punctuation and on mixed case than theconfigurations in MUC-6.
PLUM and otherinformation extraction systems eem well poised to dealwith such problems.6.
CONCLUSIONSThis paper briefly described a diverse collection ofresearch activities at BBN on information extraction.We have concluded the following:Our information extraction engines for the MUC-6Named Entity task and Template Element taskemploy no domain-specific information.
Webelieve that development of other broadlySPEECH: 20% Word ErrorSPEECH: 0% Word ErrorSPEECH: with CommasTEXT: Upper CaseTEXT: Mixed Case0 10 20 30 40 50 60 70 80 90 100Figure 9: Challenge of Speech Input.
Several factors each cause the quality of informationextracted to decrease on speech input.137applicable information extraction functionality suchas NE and TE will be a win, maximizing the valueof defining reusable knowledge bases forinformation extraction.?
We developed a full parser of English, using astatistically learned decision procedure; SPATTERhas achieved the highest scores yet report onparsing English text (Magerman, 1995).
The factthat its recall and precision are both in the high 80srepresents not just a quantitative improvement inparser performance, but also a qualitativeimprovement.?
The NLU Shell provides a way for non-programmers to build and maintain informationextraction systems based on PLUM.
The use of aGUI and a database in place of files of source codeand data represents a fundamental advance inmaking natural language technology widelyavailable.
While the NLU Shell users need detailedfamiliarity with extracting formatted ata fromnatural language, they do not need to beprogrammers.?
Compared to PLUM's previous performance inMUC-3, -4, and -5, our progress in MUC-6 wasmuch more rapid and our official score was higherthan in any previous template fill task.Furthermore, PLUM's performance was higher thanin any of the previous full template MUC tasks.?
The template merging experiment provided asubstantial range in recall versus precision, i.e., inundergeneration versus overgeneration.Nevertheless, we did not achieve a breakthrough inoverall F-score, i.e., in ERR.?
Our preliminary results in learning the NamedEntity Extraction task in English and Spanish arequite encouraging, since they are better than anypreviously reported scores for a learned system andsince they are approaching the scores of the state-of-the-art for manually built, rule based systems.?
A preliminary experiment in information extractionfrom speech has shown that there are verysignificant challenges for TIPSTER text extractiontechnology, including the current 20-30% worderror rate of transcription systems, the lack ofpunctuation within sentences, the lack ofcapitalization, and the error ate on names.7.
ACKNOWLEDGMENTSThe work reported here was supported in part by theDefense Advanced Research Projects Agency; technicalagents for part of the work were Rome Laboratory undercontract number F30602-95-C-0111 and FortHuachucha under contract number DABT63-94-C-0062.Part of the work was supported by Rome Laboratoryunder contract number F30602-92-C-0078.
The viewsand conclusions contained in this document are those ofthe authors and should not be interpreted as necessarilyrepresenting the official policies, either expressed orimplied, of the Defense Advanced Research ProjectsAgency or the United States Government.Significant contributions to this paper were made byRichard Schwartz.8.
REFERENCESAsadi, A.
"Automatic Detection and Modeling of NewWords in a Large-Vocabulary Continuous SpeechRecognition System", Doctoral Thesis, NortheasternUniversity, 1991.Ayuso, D.M., Boisen, S., Fox, H., Ingria, R., andWeischedel, R. "BBN: Description of the PLUMSystem as Used for MUC-4", MUC-4 Proceedings,1992.Iwanska, et.al., "Computational Aspects of Discoursein the Context of MUC-3", Proceedings of the ThirdMessage Understanding Conference (MUC-3), 1991.Magerman, D., "Statistical Decision-Tree Modeling forParsing", Proceedings of the 33rd Annual Meeting ofthe Association for Computational Linguistics, 1995.Matsukawa, T., "Hypothesizing Word Association fromUntagged Text", Proceedings of the ARPA Workshopon Human Language Technology, 1993.Matsukawa, T., Miller, S., and Weischedel, R."Example-Based Correction of Word Segmentation andPart of Speech Labelling", Proceedings of the ARPAWorkshop on Human Language Technology, 1993.Weischedel, R., Ayuso, D.M., Boisen, S., Fox, H.,Ingfia, R., and Palmucci, J., "Partial Parsing, A Reporton Work in Progress", Proceedings of the FourthARPA Workshop on Speech and Natural Language,1991.Weischedel, R., Ayuso, D.M., Bobrow, R., Boisen, S.,Fox, H., Matsukawa, T., MacLaughlin, D.,Papageorgiou, C., Sakai, T., Abe, J., Hoshi, H.,Miyamoto, Y., and Miller, S., "BBN's PLUMProbabilistic Language Understanding System",Proceedings of the TIPSTER Text Program (Phase 1),1993.Weischedel, R., Meteer, M., Schwartz, R., Ramshaw,L., and Palmucci, J.
"Coping with Ambiguity andUnknown Words through Probabilistic Models",Computational Linguistics (Special Issue on UsingLarge Corpora: II) 19, 359-382, 1993.138
