Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 397?402,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsThe VerbCorner Project: Findings from Phase 1 of Crowd-Sourcing aSemantic Decomposition of VerbsJoshua K. HartshorneDepartment of Brain and Cognitive SciencesMassachusetts Institute of Technology77 Massachusetts AvenueCambridge, MA 02139, USAjkhartshorne@gmail.comClaire Bonial, Martha PalmerDepartment of LinguisticsUniversity of Colorado at BoulderHellems 290, 295 UCBBoulder, CO 80309, USA{CBonial, MPalmer}@colorado.eduAbstractAny given verb can appear in some syntac-tic frames (Sally broke the vase, The vasebroke) but not others (*Sally broke at thevase, *Sally broke the vase to John).
Thereis now considerable evidence that the syn-tactic behaviors of some verbs can be pre-dicted by their meanings, and many cur-rent theories posit that this is true for mostif not all verbs.
If true, this fact wouldhave striking implications for theories andmodels of language acquisition, as well asnumerous applications in natural languageprocessing.
However, empirical investiga-tions to date have focused on a small num-ber of verbs.
We report on early resultsfrom VerbCorner, a crowd-sourced projectextending this work to a large, representa-tive sample of English verbs.1 IntroductionVerbs vary in terms of which syntactic frames theycan appear in (Table 1).
In principle, this could bean unpredictable fact about the verb that must beacquired, much like the phonological form of theverb.However, most theorists posit that there is a sys-tematic relationship between the semantics of averb and the syntactic frames in which it can ap-pear (Levin and Hovav, 2005).
For instance, itis argued that verbs like break, which describe aFrame hit like breakNP V NP x x xNP V - - xNP that S - x -NP V at NP x - -Table 1: Some of the syntactic frames available forhit, like, and break.caused change of state, can appear in both the NPV NP form (Sally broke the vase) and the NPV form (The vase broke).
Verbs such as hit andlike do not describe a change of state and so can-not appear in both forms.1Similarly, only verbsthat describe propositional attitudes, such as like,can take a that complement (John liked that Sallybroke the vase).1.1 The Semantic Consistency HypothesisThis account has a natural consequence, which wedub the Semantic Consistency Hypothesis: Thereis some set of semantic features such that verbsthat share the same syntactic behavior are identi-cal along those semantic features.2Note that oncertain accounts, this is a strong tendency ratherthan a strict necessity (e.g., Goldberg, 1995).It is widely recognized that a principled re-lationship between syntax and semantics wouldhave broad implications.
It is frequently invokedin theories of language acquisition.
For instance,Pinker (1984, 1989) has described how this cor-respondence could solve long-standing puzzlesabout how children learn syntax in the first place.Conversely, Gleitman (1990) has shown such asyntax-semantics relationship could solve signif-icant problems in vocabulary acquisition.
In fact,both researchers argue that a principled relation-ship between syntax and semantics is necessaryfor language to be learnable at all.In computational linguistics and natural lan-guage processing, some form of the SemanticConsistency Hypothesis is often included in lin-guistic resources and utilized in applications.
We1Note that this is a simplification in that there are non-causal verbs that appear in both the NP V NP frame and theNP V frame.
For details, see (Levin, 1993).2There is a long tradition of partitioning semantics intothose aspects of meaning which are ?grammatically relevant?and those which are not.
We refer the interested reader toPinker (1989), Jackendoff (1990), and Levin & RappaportHovav (2005).397describe in detail one such resource, VerbNet,which is highly relevant to our investigation.1.2 VerbNetVerbNet (Kipper et al, 2008; based on Levin,1993) lists over 6,000 verbs, categorized into 280classes according to the syntactic frames they canappear in.
That is, all verbs in the same class ap-pear in the same set of syntactic frames.
Impor-tantly, in addition to characterizing the syntacticframes associated with each class, VerbNet alocharacterizes the semantics of each class.For instance, class 9.7, which comprises acouple dozen verbs, allows 7 different syntacticframes.
The entry for one frame is shown below:Syntactic Frame NP V NP PP.DESTINATIONExample Jessica sprayed the wall.Syntax AGENT V THEME {+LOC|+DEST CONF}DESTINATIONSemantics MOTION(DURING(E), THEME)NOT(PREP(START(E), THEME, DESTINATION))PREP(END(E), THEME, DESTINATION)CAUSE(AGENT, E)Importantly, the semantics listed here is not justfor the verb spray but applies to all verbs from theSpray Class whenever they appear in that syntac-tic frame ?
that is, VerbNet assumes the SemanticConsistency Hypothesis.VerbNet and its semantic features have beenused in a variety of NLP applications, such as se-mantic role labeling (Swier and Stevenson, 2004),inferencing (Zaenen et al, 2008), verb classifica-tion (Joanis et al, 2008), and information extrac-tion (Maynard et al, 2009).
It has also been em-ployed in models of language acquisition (Parisienand Stevenson, 2011; Barak et al, 2012).
In gen-eral, there has been interest in the NLP literaturein using these syntactially-relevant semantic fea-tures for shallow semantic parsing (e.g., Giugleaand Moschitti, 2006).2 Empirical Status of the SemanticConsistency HypothesisGiven the prominence of the Semantic Consis-tency Hypothesis in both theory and practice, onemight expect that it was on firm empirical foot-ing.
That is, ideally there would be some databaseof semantic judgments for a comprehensive setof verbs from each syntactic class.
In princi-ple, these judgments would come from naive an-notators, since researchers?
intuitions about sub-tle judgments may be unconsciously clouded bytheoretical commitments (Gibson and Fedorenko,2013).
The Semantic Consistency Hypothesiswould be supported if, within that database, predi-cates with the same syntactic properties were sys-tematically related semantically.No such database exists, whether consisting ofthe judgments of linguists or naive annotators.Most theoretical studies report researcher judg-ments for only a handful of examples; how manyadditional examples were considered by the re-searcher goes unreported.
In any case, to ourknowledge, of the 280 syntactic verb classes listedby VerbNet, only a handful have been studied inany detail.The strongest evidence comes from experimen-tal work on several so-called alternations (the pas-sive, causative, locative, and dative alternations).Here, there does appear to be a systematic seman-tic distinction between the two syntactic frames ineach alternation, at least most of the time.
Thishas been tested with a reasonable sample of therelevant verbs and also in both children and adults(Ambridge et al, 2013; Pinker, 1989).
However,the relevant verbs make up a tiny fraction of allEnglish verbs, and even for these verbs, the syn-tactic frames in question represent only a fractionof the syntactic frames available to those verbs.This is not an accidental oversight.
The limit-ing factor is scale: with many thousands of verbsand over a hundred commonly-discussed seman-tic features and syntactic frames, it is not feasi-ble for a single researcher, or even team of re-searchers, to check which verbs appear in whichsyntactic frames and carry which semantic en-tailments.
Collecting data from naive subjects iseven more laborious, particularly since the aver-age Man on the Street is not necessarily equippedwith metalinguistic concepts like caused change ofstate and propositional attitude.
The VerbCornerProject is aimed at filling that empirical gap.3 VerbCornerThe VerbCorner Project3is devoted to collectingsemantic judgments for a comprehensive set ofverbs along a comprehensive set of theoretically-relevant semantic dimension.
These data can beused to test the Semantic Consistency Hypothesis.3http://gameswithwords.org/VerbCorner/398Independent of the validity of that hypothesis, thesemantic judgments themselves should prove use-ful for any study of linguistic meaning or relatedapplication.We address the issue of scale through crowd-sourcing: Recruiting large numbers of volunteers,each of whom may provide only a few annota-tions.
Several previous projects have success-fully crowd-sourced linguistic annotations, suchas Phrase Detectives, where volunteers have con-tributed 2.5 million judgments on anaphoric rela-tions (Poesio et al, 2012).3.1 Integration with VerbNetOne significant challenge for any such project isfirst classifying verbs according to the syntacticframes they can appear in.
Thus, at least initially,we are focusing on the 6,000+ verbs already cata-loged in VerbNet.
As such, the VerbCorner Projectis also verifying and validating the semantics cur-rently encoded in VerbNet.
VerbNet will be editedas necessary based on the empirical results.Integration with VerbNet has additional bene-fits, since VerbNet itself is integrated with a vari-ety of linguistic resources, such as PropBank andPenn TreeBank.
This amplifies the impact of anyVerbCorner-inspired changes to VerbNet.3.2 The TasksWe selected semantic features of interest based onthose most commonly cited in the linguistics lit-erature, with a particular focus on those that ?
ac-cording to VerbNet ?
apply to many predicates.Previous research has shown that humans findit easier to reason about real-world scenarios thanmake abstract judgments (Cosmides and Tooby,1992).
Thus, for each feature (e.g., MOVEMENT),we converted the metalinguistic judgment (?Doesthis verb entail movement on the part of some en-tity??)
into a real-world problem.For example, in ?Simon Says Freeze,?
a taskdesigned to elicit judgments about movement, theGalactic Overlord (Simon) decrees ?Galactic StayWhere You Are Day,?
during which nobody is al-lowed to move from their current location.
Par-ticipants read descriptions of events and decidewhether anyone violated the rule.In ?Explode on Contact,?
designed to elicitjudgments about physical contact, objects andpeople explode when they touch one another.
Theparticipant reads descriptions of events and de-cides whether anything has exploded.Note that each task is designed to elicit judg-ments about entailments ?
things that must be truerather than are merely likely to be true.
If Johngreeted Bill, they might have come into contact(e.g., by shaking hands), but perhaps they did not.Previous work suggests that it is the semantic en-tailments that matter, particularly for explainingthe syntactic behavior of verbs (Levin, 1993).3.3 The ItemsThe exact semantics associated with a verb maydepend on its syntactic frame.
Thus Sally rolledthe ball entails that somebody applied force to theball (namely: Sally), whereas The ball rolled doesnot.
Thus, we investigate the semantics of eachverb in each syntactic frame available to it (as de-scribed by VerbNet).
Below, the term item is theunit of annotation: a verb in a frame.In order to minimize unwanted effects of worldknowledge, the verb?s arguments are replaced withnonsense words or randomly chosen proper names(Sally sprayed the dax onto the blicket).
The useof novel words is explained by the story for eachtask.3.4 The PhasesGiven the sheer scale of the project, data-collection is expected to take several years at least.Thus, data-collection has been broken up into a se-ries of phases.
Each phase focuses on a small num-ber of classes and/or semantic entailments.
Thisensures that there are meaningful intermediate re-sults that can be disseminated prior to the comple-tion of the entire project.
This manuscript reportsthe results of Phase 1.4 ResultsThe full data and annotations will be released inthe near future and may be available now by re-quest.
Below, we summarize the main findingsthus far.4.1 Description of Phase 1In Phase 1 of the project, we focused on 11 verbclasses (Table 3) comprising 641 verbs and sevendifferent semantic entailments (Table 2).
Whilesix of these entailments were chosen from amongthose features widely believed to be relevant forsyntax, one was not: A Good World, which inves-tigated evaluation (Is the event described by theverb positive or negative?).
Although evaluation399Task Semantic Feature Anns.
Anns./Item Mode ConsistencyEntropy PHYSICAL CHANGE 23,875 7 86% 95%Equilibrium APPLICATION OF FORCE 27,128 8 79% 95%Explode on Contact PHYSICAL CONTACT 23,590 7 93% 95%Fickle Folk CHANGE OF MENTAL STATE 16,466 5 81% 96%Philosophical Zombie Hunter MENTAL STATE 24,592 7 80% 89%Simon Says Freeze LOCATION CHANGE 24,245 7 83% 88%A Good World EVALUATION 22,668 7 72% 74%Table 2: Respectively: Task, semantic feature tested, number of annotations, mean number of annotationsper item, mean percentage of participants choosing the modal response, consistency within class.of events is an important component of humanpsychology, to our knowledge no researcher hassuggested that it is relevant for syntax.
As such,this task provides a lower bound for how much se-mantic consistency one might expect within a syn-tactic verb class.In all, we collected 162,564 judgments from1,983 volunteers (Table 2).4.2 Inter-annotator AgreementEach task had been iteratively piloted and re-designed until inter-annotator reliability was ac-ceptable, as described in a previous publication.However, these pilot studies involved a small num-ber of items which were coded by all annota-tors.
How good was the reliability in the crowd-sourcing context?Because we recruited large numbers of an-notators, most of whom annotated only a fewitems, typical measures of inter-annotator agree-ment such as Cohen?s kappa are not easily calcu-lated.
Instead, for each item, we calculated themost common (modal) response.
We then con-sidered what proportion of all annotations wereaccounted for by the modal response: a mean of100% would indicate that there was no disagree-ment among annotators for any item.As can be seen in Table 2, for every task, themodal response covered the bulk responses, rang-ing from a low of 72% for EVALUATION to a highof 93% for PHYSICAL CONTACT.
Since therewere typically 4 or more possible answers peritem, inter-annotator agreement was well abovechance.
This represents good performance giventhat the annotators were entirely untrained.In many cases, annotator disagreement seemsto be driven by syntactic constructions that areonly marginally grammatical.
For instance, inter-annotator agreement was typically low for class63.
VerbNet suggests two syntactic frames forclass 63, one of which (NP V THAT S) appears tobe marginal (?I control that Mary eats).
In fact,annotators frequently flagged these items as un-grammatical, which is a valuable result in itself forimproving VerbNet.Class Examples PChange Force Contact MChange Mental LChange12 yank, press - x d - - d18.1 hit, squash d x d - - d29.5 believe, conjecture - - - - d -31.1 amuse, frighten - - - x d -31.2 like, fear - - - - x -45.1 break, crack x d d - - d51.3.1 bounce, roll - d d - - d51.3.2 run, slink - d - - - d51.6 chase, follow - - - - - d61 attempt, try - - - - - -63 control, enforce - - - - - -Table 3: VerbNet classes investigated in Phase 1, with presence of semantic entailments as indicated bydata.
x = feature present; - = feature absent; d = depends on syntactic frame.4004.3 Testing the Semantic ConsistencyHypothesis4.3.1 Calculating consistencyWe next investigated whether our results supportthe Semantic Consistency Hypothesis.
As notedabove, the question is not whether all verbs in thesame syntactic class share the same semantic en-tailments.
Even a single verb may have differentsemantic entailments when placed in different syn-tactic frames.
Thus, calculating consistency of aclass must take differing frames into account.There are many sophisticated rubrics for calcu-lating consistency.
However, for expository pur-poses here, we use one that is intuitive and easyto interpret.
First, we determined the annotationfor each item (i.e., each verb/frame combination)by majority vote.
We then considered how manyverbs in each class had the same annotation in anygiven syntactic frame.For example, suppose a class had 10 verbs and2 frames.
In the first frame, 8 verbs received thesame annotation and 2 received others.
The con-sistency for this class/frame combination is 80%.In the second frame, 6 verbs received the sameannotation and 4 verbs received others.
The con-sistency for this class/frame combination is 60%.The consistency for the class as a whole is the av-erage across frames: 70%.4.3.2 ResultsMean consistency averaged across classes isshown for each task in Table 2.
As expected,consistency was lowest for EVALUATION, whichis not expected to necessarily correlate with syn-tax.
Interestingly, consistency for EVALUATIONwas nonetheless well above floor.
This is per-haps not surprising: two sentences that have thesame values for PHYSICAL CHANGE, APPLICA-TION OF FORCE, PHYSICAL CONTACT, CHANGEOF MENTAL STATE, MENTAL STATE, and LO-CATION CHANGE are, on average, also likely tobe both good or both bad.Consistency was much higher for the othertasks, and in fact was close to ceiling for most ofthem.
It remains to be seen whether the items thatdeviate from the mode represent true differences insemantics or reflect merely noise.
One way of ad-dressing this question is to collect additional anno-tations for those items that deviate from the mode.4.4 Verb semanticsFor each syntactic frame in each class, we deter-mined the most common annotation.
This is sum-marized in Table 3.
The semantic annotation de-pended on syntactic frame nearly 1/4 of the time.4These frequently matched VerbNet?s seman-tics, though not always.
For instance, annota-tors judged that class 18.1 verbs in the NP V NPPP.INSTRUMENT entailed movement on the partof the instrument (Sally hit the ball with the stick)?
something not reflected in VerbNet.5 Conclusion and Future WorkResults of Phase 1 provide support for the Seman-tic Consistency Hypothesis, at least as a strongbias.
More work will be needed to determine thestrength of that bias.
The findings are largely con-sistent with VerbNet?s semantics, but changes areindicated in some cases.We find that inter-annotator agreement is suf-ficiently high that annotation can be done effec-tively using the modal response with an averageof 6-7 responses per item.
We are currently in-vestigating whether we can achieve better reliabil-ity with fewer responses per item by taking intoaccount an individual annotator?s history acrossitems, as recent work suggests is possible (Passon-neau and Carpenter, 2013; Rzhetsky et al, 2009;Whitehill et al, 2009).Thus, crowd-sourcing VerbNet semantic entail-ments appears to be both feasible and productive.Data-collection continues.
Phase 2, which addedover 10 new verb classes, is complete.
Phase 3,which includes both new classes and new entail-ments, has been launched.AcknowledgmentsWe gratefully acknowledge the support of theNational Science Foundation Grant NSF-IIS-1116782, DARPA Machine Reading FA8750-09-C-0179, and funding from the Ruth L. KirschsteinNational Research Service Award.
Any opinions,findings, and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of the NationalScience Foundation.4Note that this table was calculated based on whether thesemantic feature was present or not.
In many cases, the datawas significantly richer.
For instance, for APPLICATION OFFORCE, annotators determined which participant in the eventwas applying the force.401ReferencesBen Ambridge, Julian Pine, Caroline Rowland,Franklin Chang, and Amy Bidgood.
2013.
The re-treat from overgeneralization in child language ac-quisition: word learning, morphology and verb ar-gument structure.
Wiley Interdisciplinary Reviews:Cognitive Science, 4(1):47?62.Libby Barak, Afsaneh Fazly, and Suzanne Steven-son.
2012.
Modeling the acquisition of mentalstate verbs.
In Proceedings of the 3rd Workshop onCognitive Modeling and Computational Linguistics,pages 1?10.
Association for Computational Linguis-tics.Leda Cosmides and John Tooby.
1992.
Cognitiveadaptations for social exchange.
The Adapted Mind,pages 163?228.Edward Gibson and Evelina Fedorenko.
2013.
Theneed for quantitative methods in syntax and seman-tics research.
Language and Cognitive Processes,28(1-2):88?124.Ana-Maria Giuglea and Alessandro Moschitti.
2006.Shallow semantic parsing based on framenet, verb-net and propbank.
In Proceedings of the 217thEuropean Conference on Artificial Intelligence,pages 563?567, Amsterdam, The Netherlands, TheNetherlands.
IOS Press.Lila Gleitman.
1990.
The structural sources of verbmeanings.
Language Acquisition, 1(1):3?55.Adele E. Goldberg.
1995.
Constructions: A Construc-tion Grammar approach to argument structure.
Uni-versity of Chicago Press.Eric Joanis, Suzanne Stevenson, and David James.2008.
A general feature space for automaticverb classification.
Natural Language Engineering,14(3):337?367.Beth Levin and Malka Rappaport Hovav.
2005.
Argu-ment Realization.
Cambridge University Press.Beth Levin.
1993.
English Verb Classes and Alter-nations: A preliminary Investigation.
University ofChicago press.Diana Maynard, Adam Funk, and Wim Peters.
2009.Using lexico-syntactic ontology design patterns forontology creation and population.
In Proc.
of theWorkshop on Ontology Patterns.Christopher Parisien and Suzanne Stevenson.
2011.Generalizing between form and meaning usinglearned verb classes.
In Proceedings of the 33rd An-nual Meeting of the Cognitive Science Society.
Cite-seer.Rebecca J Passonneau and Bob Carpenter.
2013.
Thebenefits of a model of annotation.
In Proceedings ofthe 7th Linguistic Annotation Workshop and Inter-operability with Discourse, pages 187?195.Steven Pinker.
1984.
Language Learnability and Lan-guage Development.
Harvard University Press.Steven Pinker.
1989.
Learnability and Cognition: TheAcquisition of Argument Structure.
MIT Press.Massimo Poesio, Jon Chamberlain, Udo Kruschwitz,Livio Robaldo, and Luca Ducceschi.
2012.
Thephrase detective multilingual corpus, release 0.1.
InCollaborative Resource Development and DeliveryWorkshop Programme, page 34.Andrey Rzhetsky, Hagit Shatkay, and W John Wilbur.2009.
How to get the most out of your curation ef-fort.
PLoS Computational Biology, 5(5):1?13.Robert S Swier and Suzanne Stevenson.
2004.
Un-supervised semantic role labeling.
In Proceedingsof the Generative Lexicon Conference, volume 95,page 102.Jacob Whitehill, Paul Ruvolo, Tingfan Wu, JacobBergsma, and Javier R Movellan.
2009.
Whose voteshould count more: Optimal integration of labelsfrom labelers of unknown expertise.
In Advances inNeural Information Processing Systems, volume 22,pages 2035?2043.Annie Zaenen, Daniel G Bobrow, and Cleo Condo-ravdi.
2008.
The encoding of lexical implications inverbnet: Predicates of change of locations.
In Lan-guage Resources Evaluation Conference.402
