Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1035?1045,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsExtracting Opinion Targets in a Single- and Cross-Domain Settingwith Conditional Random FieldsNiklas JakobTechnische Universita?t DarmstadtHochschulstra?e 1064289 Darmstadt, GermanyIryna GurevychTechnische Universita?t DarmstadtHochschulstra?e 1064289 Darmstadt, Germanyhttp://www.ukp.tu-darmstadt.de/peopleAbstractIn this paper, we focus on the opinion tar-get extraction as part of the opinion min-ing task.
We model the problem as an in-formation extraction task, which we addressbased on Conditional Random Fields (CRF).As a baseline we employ the supervised al-gorithm by Zhuang et al (2006), which rep-resents the state-of-the-art on the employeddata.
We evaluate the algorithms comprehen-sively on datasets from four different domainsannotated with individual opinion target in-stances on a sentence level.
Furthermore, weinvestigate the performance of our CRF-basedapproach and the baseline in a single- andcross-domain opinion target extraction setting.Our CRF-based approach improves the perfor-mance by 0.077, 0.126, 0.071 and 0.178 re-garding F-Measure in the single-domain ex-traction in the four domains.
In the cross-domain setting our approach improves the per-formance by 0.409, 0.242, 0.294 and 0.343 re-garding F-Measure over the baseline.1 IntroductionThe automatic extraction and analysis of opinionshas been approached on several levels of granular-ity throughout the last years.
As opinion mining istypically an enabling technology for another task,this overlaying system defines requirements regard-ing the level of granularity.
Some tasks only requirean analysis of the opinions on a document or sen-tence level, while others require an extraction andanalysis on a term or phrase level.
Amongst thetasks which require the finest level of granularityare: a) Opinion question answering - i.e.
with ques-tions regarding an entity as in ?What do the peoplelike / dislike about X??.
b) Recommender systems- i.e.
if the system shall only recommend entitieswhich have received good reviews regarding a cer-tain aspect.
c) Opinion summarization - i.e.
if onewants to create an overview of all positive / negativeopinions regarding aspect Y of entity X and clusterthem accordingly.
All of these tasks have in com-mon that in order to fulfill them, the opinion min-ing system must be capable of identifying what theopinions in the individual sentences are about, henceextract the opinion targets.Our goal in this work is to extract opinion tar-gets from user-generated discourse, a discourse typewhich is quite frequently encountered today, due tothe explosive growth of Web 2.0 community web-sites.
Typical sentences which we encounter in thisdiscourse type are shown in the following examples.The opinion targets which we aim to extract are un-derlined in the sentences, the corresponding opinionexpressions are shown in italics.
(1) While none of the features areearth-shattering, eCircles does provide a greatplace to keep in touch.
(2) Hyundai?s more-than-modest refresh haslargely addressed all the original car?sweaknesses while maintaining its pricecompetitiveness.The extraction of opinion targets can be consid-ered as an instance of an information extraction(IE) task (Cowie and Lehnert, 1996).
Conditional1035Random Fields (CRF) (Lafferty et al, 2001) havebeen successfully applied to several IE tasks inthe past (Peng and McCallum, 2006).
A recur-ring problem, which arises when working with su-pervised approaches, concerns the domain portabil-ity.
In the opinion mining context this question hasbeen prominently investigated with respect to opin-ion polarity analysis (sentiment analysis) in previ-ous research (Aue and Gamon, 2005; Blitzer et al,2007).
Terms as ?unpredictable?
can express a pos-itive opinion when uttered about the storyline of amovie but a negative opinion when the handling ofa car is described.
Hence the effects of training andtesting a machine learning algorithm for sentimentanalysis on data from different domains have beenanalyzed in previous research.
However to the bestof our knowledge, these effects have not been inves-tigated regarding the extraction of opinion targets.The contribution of this paper is a CRF-based ap-proach for opinion targets extraction which tacklesthe problem of domain portability.
We first evalu-ate our approach in three different domains againsta state-of-the art baseline system and then evaluatethe performance of both systems in a cross-domainsetting.
We show that our CRF-based approach out-performs the baseline in both settings, and how thediffrerent combinations of features we introduce in-fluence the results of our CRF-based approach.
Theremainder of this paper is structured as follows: InSection 2 we discuss the related work, and in Sec-tion 3 we describe our CRF-based approach.
Sec-tion 4 comprises our experimental setup includingthe description of the dataset we employ in our ex-periments in Section 4.1 and the baseline system inSection 4.2.
The results of our experiments and theirdiscussion follow in Section 5.
Finally we draw ourconclusions in Section 6.2 Related WorkIn the following we will discuss the related work re-garding opinion target extraction and domain adap-tation in opinion mining.
The discussion of the re-lated work on opinion target extraction is separatedin supervised and unsupervised approaches.
Weconclude with a discussion of the related work ondomain adaptation in opinion mining.2.1 Unsupervised Opinion Target ExtractionThe first work on opinion target extraction was doneon customer reviews of consumer electronics.
Huand Liu (2004) introduce the task of feature basedsummarization, which aims at creating an overviewof the product features commented on in the re-views.
Their approach relies on a statistical analysisof the review terms based on association mining.
Adataset of customer reviews from five domains wasannotated by the authors regarding mentioned prod-uct features with respective opinion polarities.
Theassociation mining based algorithm yields a preci-sion of 0.72 and a recall of 0.80 in the extractionof a manually selected subset of product features.The same dataset of product reviews was used in thework of Yi et al (2003).
They present and evalu-ate a complete system for opinion extraction whichis based on a statistical analysis based on the Like-lihood Ratio Test for opinion target extraction.
TheLikelihood Ratio Test yields a precision of 0.97 and1.00 in the task of opinion target (product feature)extraction, recall values are not reported.Popescu and Etzioni (2005) present the OPINEsystem for opinion mining on product reviews.Their algorithm is based on an information extrac-tion system, which uses the pointwise mutual infor-mation based on the hitcounts of a web-search en-gine as an input.
They evaluate the opinion targetextraction separately on the dataset by Hu and Liu(2004).
OPINE?s precision is on average 22% higherthan the association mining based approach, whilehaving an average 3% lower recall.Bloom et al (2007) manually create taxonomiesof opinion targets for two datasets.
With a hand-crafted set of dependency tree paths their algorithmidentifies related opinion expressions and targets.Due to the lack of a dataset annotated with opinionexpressions and targets, they just evaluate the accu-racy of several aspects of their algorithm by man-ually assessing an output sample.
Their algorithmyields an accuracy of 0.75 in the identification ofopinion targets.Kim and Hovy (2006) aim at extracting opinionholders and opinion targets in newswire with se-mantic role labeling.
They define a mapping of thesemantic roles identified with FrameNet to the re-spective opinion elements.
As a baseline, they im-1036plement an approach based on a dependency parser,which identifies the targets following the dependen-cies of opinion expressions.
They measure the over-lap between two human annotators and their algo-rithm as well as the baseline system.
The algorithmbased on semantic role labeling yields an F-Measureof 0.315 with annotator1 and 0.127 with annotator2,while the baseline yields an F-Measure of 0.107 and0.109 regarding opinion target extraction2.2 Supervised Opinion Target ExtractionZhuang et al (2006) present a supervised algorithmfor the extraction of opinion expression - opiniontarget pairs.
Their algorithm learns the opinion tar-get candidates and a combination of dependency andpart-of-speech paths connecting such pairs from anannotated dataset.
They evaluate their system in across validation setup on a dataset of user-generatedmovie reviews and compare it to the results of the Huand Liu (2004) system as a baseline.
Thereby, thesystem by Zhuang et al (2006) yields an F-Measureof 0.529 and outperforms the baseline which yieldsan F-Measure of 0.488 in the task of extracting opin-ion target - opinion expression pairs.Kessler and Nicolov (2009) solely focus on iden-tifying which opinion expression is linked to whichopinion target in a sentence.
They present a datasetof car and camera reviews in which opinion expres-sions and opinion targets are annotated.
Startingwith this information, they train a machine learn-ing classifier for identifying related opinion expres-sions and targets.
Their algorithm receives the opin-ion expression and opinion target annotations as in-put during runtime.
The classifier is evaluated us-ing the algorithm by Bloom et al (2007) as a base-line.
The support vector machine based approachby Kessler and Nicolov (2009) yields an F-Measureof 0.698, outperforming the baseline which yields anF-Measure of 0.445.2.3 Domain Adaptation in Opinion MiningThe task of creating a supervised algorithm, whichwhen trained on data from domain A, also performswell on data from another domain B, is a domainadaptation problem (Daume?
III and Marcu, 2006;Jiang and Zhai, 2007).
Aue and Gamon (2005) haveinvestigated this challenge very early in the task ofdocument level sentiment classification (positive /negative).
They observe that increasing the amountof training data raises the classification accuracy, butonly if the training data is from one source domain.Increasing the training data by mixing domains doesnot yield any consistent improvements.
Blitzer etal.
(2007) introduce an extension to a structural cor-respondence learning algorithm, which was specifi-cally designed to address the task of domain adap-tation.
Their enhancement aims at identifying pivotfeatures, which are stable across domains.
In a seriesof experiments in document level sentiment classi-fication they show that their extension outperformsthe original structural correspondence learning ap-proach.
In their error analysis, the authors observethe best results were reached when the training - test-ing combinations were Books - DVDs or Electronics- Kitchen appliances.
They conclude that the topi-cal relatedness of the domains is an important factor.Furthermore they observe that training the algorithmon a smaller amount of data from a similar domain ismore effective than increasing the amount of train-ing data by mixing domains.3 CRF-based Approach for OpinionTarget ExtractionIn the following we will describe the features weemploy as input for our CRF-based approach.
As thedevelopment data, we used 29 documents from themovies dataset, 23 documents from the web-servicesdataset and 15 documents from the cars & camerasdatasets.TokenThis feature represents the string of the current tokenas a feature.
Even though this feature is rather ob-vious, it can have considerable impact on the targetextraction performance.
If the vocabulary of targetsis rather compact for a certain domain (correspond-ing to a low target type / target ratio), the trainingdata is likely to contain the majority of the targettypes, which should hence be a good indicator.
Wewill refer to this feature as tk in our result tables.POSThis feature represents the part-of-speech tag of thecurrent token as identified by the Stanford POS Tag-ger1.
It can provide some means of lexical disam-1http://nlp.stanford.edu/software/tagger.shtml1037biguation, e.g.
indicate that the token ?sounds?
isa noun and not a verb in a certain context.
At thesame time, the CRF algorithm is provided with ad-ditional information to extract opinion targets whichare multiword expressions, i.e.
noun combinations.We will refer to this feature as pos in our result ta-bles.Short Dependency PathPrevious research has successfully employed pathsin the dependency parse tree to link opinion expres-sions and the corresponding targets (Zhuang et al,2006; Kessler and Nicolov, 2009).
Both works iden-tify direct dependency relations such as ?amod?
and?nsubj?
as the most frequent and at the same timehighly accurate connections between a target and anopinion expression.
We hence label all tokens whichhave a direct dependency relation to an opinion ex-pression in a sentence.
The Stanford Parser2 is em-ployed for the constituent and dependency parsing.We will refer to this feature as dLn in our result ta-bles.Word DistanceFrom the work of Zhuang et al (2006) we can inferthat opinion expressions and their target(s) are notalways connected via short paths in the dependencyparse tree.
Since we cannot capture such paths withthe abovementioned feature we introduce anotherfeature which acts as heuristic for identifying thetarget to a given opinion expression.
Hu and Liu(2004) and Yi et al (2003) have shown that (base)noun phrases are good candidates for opinion targetsin the datasets of product reviews.
We therefore la-bel the token(s) in the closest noun phrase regardingword distance to each opinion expression in a sen-tence.
We will refer to this feature as wrdDist in ourresult tables.Opinion SentenceWith this feature, we simply label all tokens occur-ring in a sentence containing an opinion expression.This feature shall enable the CRF algorithm todistinguish between the occurence of a certaintoken in a sentence which contains an opinion vs. asentence without an opinion.
We will refer to thisfeature as sSn in our result tables.2http://nlp.stanford.edu/software/lex-parser.shtmlOur goal is to extract individual instances of opiniontargets from sentences which contain an opinionexpression.
This can be modeled as a sequencesegmentation and labeling task.
The CRF algorithmreceives a sequence of tokens t1...tn for whichit has to predict a sequence of labels l1...ln.
Werepresent the possible labels following the IOBscheme: B-Target, identifying the beginning of anopinion target, I-Target identifying the continuationof a target, and O for other (non-target) tokens.
Wemodel the sentences as a linear chain CRF, whichis based on an undirected graph.
In the graph, eachnode corresponds to a token in the sentence andedges connect the adjacent tokens as they appear inthe sentence.
In our experiments, we use the CRFimplementation from the Mallet toolkit3.4 Experimental Setup4.1 DatasetsIn our experiments, we employ datasets from threedifferent sources, which span four domains in total(see Table 1).
All of them consist of reviews col-lected from Web 2.0 sites.
The first dataset con-sists of reviews for 20 different movies collectedfrom the Internet Movie Database.
It was presentedin Zhuang et al (2006) and annotated regardingopinion target - opinion expression pairs.
The sec-ond dataset consists of 234 reviews for two differentweb-services collected from epinions.com, as de-scribed in Toprak et al (2010).
The third dataset isan extended version of the data presented in Kesslerand Nicolov (2009).
The authors have provided uswith additional documents, which have been anno-tated in the meantime.
The version of the datasetused in our experiments consists of 179 blog post-ings regarding different digital cameras and 336 re-views of different cars.
In the description of theirannotation guidelines, Kessler and Nicolov (2009)refer to opinion targets as mentions.
Mentions areall aspects of the review topic, which can be targetsof expressed opinions.
However, not only mentionswhich occur as opinion targets were originally anno-tated, but also mentions which occur in non-opinionsentences.
In our experiments, we only use the men-tions which occur as targets of opinion expressions.3http://mallet.cs.umass.edu/1038All three datasets contain annotations regardingthe antecedents of anaphoric opinion targets.
In ourexperimental setup, we do not require the algorithmsto also correctly resolve the antecedent of an opin-ion target representy by a pronoun, as we are solelyinterested in evaluating the opinion target extractionnot any anaphora resolution.As shown in rows 4 and 5 of Table 1, the docu-ments from the cars and the cameras datasets exhibita much higher density of opinions per document.53.5% of the sentences from the cars dataset containan opinion and in the cameras dataset even 56.1%of the sentences contain an opinion, while in themovies and the web-services reviews just 22.1% and22.4% of the sentences contain an opinion.
Further-more in the cars and the cameras datasets the lexicalvariability regarding the opinion targets is substan-tially larger than in the other two datasets: We calcu-late target types by counting the number of distinctopinion targets in a dataset.
We divide this by thesum of all opinion target instances in the dataset.
Forthe cars dataset this ratio is 0.440 and for the cam-eras dataset it is 0.433, while for the web-servicesdataset it is 0.306 and for the movies dataset only0.122.
In terms of reviews this means, that in themovie reviews the same movie aspects are repeat-edly commented on, while in the cars and the cam-eras datasets many different aspects of these entitiesare discussed, which in turn each occur infrequently.Table 1: Dataset Statisticsmovies web- cars camerasservicesDocuments 1829 234 336 179Sentences 24555 6091 10969 5261Tokens /20.3 17.5 20.3 20.4sentenceSentences with21.4% 22.4% 51.1% 54.0%target(s)Sentences with21.4% 22.4% 53.5% 56.1%opinion(s)Targets 7045 1875 8451 4369Target types 865 574 3722 1893Tokens / target 1.21 1.35 1.29 1.42Avg.
targets /1.33 1.37 1.51 1.53opinion sent.4.2 Baseline SystemIn the task of opinion target extraction the super-vised algorithm by Zhuang et al (2006) representsthe state-of-the-art on the movies dataset we alsoemploy in our experiments.
We therefore use it asa baseline.
The algorithm learns two aspects fromthe labeled training data:1.
A set of opinion target candidates2.
A set of paths in a dependency tree which iden-tify valid opinion target - opinion expressionpairsIn our experiments, we learn the full set of opin-ion targets from the labeled training data in the firststep.
This is slightly different from the approachin (Zhuang et al, 2006), but we expect that this mod-ification should be beneficial for the overall perfor-mance in terms of recall, as we do not remove anylearned opinion targets from the candidate list.
Inthe second step, the annotated sentences are parsedand a graph containing the words of a sentence iscreated, which are connected by the dependency re-lations between them.
For each opinion target -opinion expression pair from the gold standard, theshortest path connecting them is extracted from thedependency graph.
A path consists of the part-of-speech tags of the nodes and the dependency typesof the edges.
Example 3 shows a typical dependencypath.
(3) NN - nsubj - NP - amod - JJDuring runtime, the algorithm identifies opinion tar-gets from the candidate list in the training data.
Theopinion expressions are directly taken from the goldstandard, as we focus on the opinion target extrac-tion aspect in this work.
The sentences are thenparsed and if a valid path between a target andan opinion expression is found in the list of possi-ble paths, then the pair is extracted.
Since the de-pendency paths only identify pairs of single wordtarget and opinion expression candidates, we em-ploy a merging step.
Extracted target candidates aremerged into a multiword target if they are adjacentin a sentence.
Thereby, the baseline system is alsocapable of extracting multiword opinion targets.10394.3 MetricsWe employ the following requirements in our eval-uation of the opinion target extraction: An opin-ion target must be extracted with exactly the spanboundaries as annotated in the gold standard.
Thisis especially important regarding multiword tar-gets.
Extracted targets which partially overlap withthe annotated gold standard are counted as errors.Hence a target extracted by the algorithm whichdoes not exactly match the boundaries of a targetin the gold standard is counted as a false positive(FP), e.g.
if ?battery life?
is annotated as the tar-get in the gold standard, only ?battery?
or ?life?extracted as targets will be counted as FPs.
Exactmatches between the targets extracted by the algo-rithm and the gold standard are true positives (TP).We refer to the number of annotated targets in thegold standard as TGS .
Precision is calculated asPrecision = TPTP+FP , and recall is calculated asRecall = TPTGS .
F-Measure is the harmonic mean ofprecision and recall.5 Results and DiscussionWe investigate the performance of the baseline andthe CRF-based approach for opinion target extrac-tion in a single- and cross-domain setting.
Thesingle-domain approach assumes that there is a setof training data available for the same domain asthe domain the algorithm is being tested on.
In thissetup, we will both run the baseline and our CRFbased system in a 10-fold cross-validation and reportresults macro averaged over all runs.
In the cross-domain approach, we will investigate how the algo-rithm performs if given training data from domain Awhile being tested on another domain B.
In this set-ting, we will train the algorithm on the entire datasetA, and test it on the entire dataset B, we hence reportone micro averaged result set.
In Subsection 5.1 wepresent the results of both the baseline system andour CRF-based approach in the single-domain set-ting, in Subsection 5.2 we present the results of thetwo systems in the cross-domain opinion target ex-traction.Table 2: Single-Domain Extraction with Zhuang BaselineDataset Precision Recall F-Measuremovies 0.663 0.592 0.625web-services 0.624 0.394 0.483cars 0.259 0.426 0.322cameras 0.423 0.431 0.4265.1 Single-Domain Results5.1.1 Zhuang BaselineAs shown in Table 2, the state-of-the-art algo-rithm of Zhuang et al (2006) performs best on themovie review dataset and worst on the cars dataset.The results on the movie dataset are higher thanoriginally reported in (Zhuang et al, 2006) (Preci-sion 0.483, Recall 0.585, F-Measure 0.529).
We as-sume that this is due to two reasons: 1.
In our task,the algorithm uses the opinion expression annotationfrom the gold standard.
2.
We do not remove anylearned opinion target candidates from the trainingdata (See Section 4.2).During training we observed that for each datasetthe lists of possible dependency paths (see Exam-ple 3) contained several hundred entries, many ofthem only occurring once.
We assume that the re-call of the algorithm is limited by a large varietyof possible dependency paths between opinion tar-gets and opinion expressions, since the algorithmcannot link targets and opinion expressions in thetesting data if there is no valid candidate depen-dency path.
Furthermore, we observe that for thecars dataset the size of the dependency path candi-date list (6642 entries) was approximately five timeslarger than the dependency graph candidate list forthe web-services dataset (1237 entries), which has acomparable size regarding documents.
At the sametime, the list of target candidates of the cars datasetwas approximately eight times larger than the tar-get candidate list for the web-services dataset.
Weassume that a large number of both the target can-didates as well as the dependency path candidatesintroduces many false positives during the target ex-traction, hence lowering the precision of the algo-rithm on the cars dataset considerably.1040Table 3: Single-Domain Extraction with our CRF-based Approachmovies web-services cars camerasFeatures Prec Rec F-Me Prec Rec F-Me Prec Rec F-Me Prec Rec F-Metk, pos 0.639 0.133 0.220 0.500 0.051 0.093 0.438 0.110 0.175 0.300 0.085 0.127tk, pos, wDs 0.542 0.181 0.271 0.451 0.272 0.339 0.570 0.354 0.436 0.549 0.375 0.446tk, pos, dLn 0.777 0.481 0.595 0.634 0.380 0.475 0.603 0.372 0.460 0.569 0.376 0.453tk, pos, sSn 0.673 0.637 0.653 0.604 0.397 0.476 0.453 0.180 0.257 0.398 0.172 0.238tk, pos, dLn, wDs 0.792 0.481 0.598 0.620 0.354 0.450 0.603 0.389 0.473 0.596 0.425 0.496tk, pos, sSn, wDs 0.662 0.656 0.659 0.664 0.461 0.544 0.564 0.370 0.446 0.544 0.381 0.447tk, pos, sSn, dLn 0.791 0.477 0.594 0.654 0.501 0.568 0.598 0.384 0.467 0.586 0.391 0.468tk, pos, sSn, dLn, wDs 0.749 0.661 0.702 0.722 0.526 0.609 0.622 0.414 0.497 0.614 0.423 0.500pos, sSn, dLn, wDs 0.672 0.441 0.532 0.612 0.322 0.422 0.612 0.369 0.460 0.674 0.398 0.5005.1.2 Our CRF-based ApproachTable 3 shows the results of the opinion target ex-traction using the CRF algorithm.
Row 8 containsthe results of the feature configuration, which yieldsthe best performance regarding F-Measure across alldatasets.
We observe that our aproach outperformsthe Zhuang baseline on all datasets.
The gain in F-Measure is between 0.077 in the movies domain and0.175 in the cars domain.
Although the CRF-basedapproach clearly outperforms the baseline systemon all four datasets, we also observe the same gen-eral trend regarding the individual results: The CRFyields the best results on the movies dataset and theworst results on the cars & cameras dataset.As shown in the first row, the results when usingjust the token string and part-of-speech tags as fea-tures are very low, especially regarding recall.
Weobserve that the higher the lexical variability of theopinion targets is in a dataset, the lower the resultsare.
If we add the feature based on word distance(row 2), the recall is improved on all datasets, whilethe precision is slightly lowered on the movies andweb-services datasets.
The dependency path basedfeature performs better compared to the word dis-tance heuristic as shown in row 3.
The precision isconsiderably increased on all datasets, on the moviesand cars & cameras datasets even reaching the over-all highest value.
At the same time, we observean increase of recall on all datasets.
The obser-vation made in previous research that short pathsin the dependency graph are a high precision indi-cator of related opinion expressions - opinion tar-gets (Kessler and Nicolov, 2009) is confirmed on alldatasets.
Adding the information regarding opinionsentences to the basic features of the token string andthe part-of-speech tag (row 4) yields the biggest im-provements regarding F-Measure on the movies andweb-services dataset (+0.433 / +0.383).
On the cars& cameras dataset the recall is relatively low again.We assume that this is again due to the high lexicalvariability, so that the CRF algorithm will encountermany actual opinion targets in the testing data whichhave not occurred in the training data and will hencenot be extracted.As shown in row 5, if we combine the dependencygraph based feature with the word distance heuris-tic, the results regarding F-Measure are consistentlyhigher than the results of these features in isolation(rows 2 - 4) on all datasets.
We conclude that thesetwo features are complementary, as they apparentlyindicate different kinds of opinion targets which arethen correctly extracted by the CRF.
If we combineeach of the opinion expression related features withthe label which identifies opinion sentences in gen-eral (rows 6 & 7), we observe that this feature isalso complementary to the others.
On all datasets theresults regarding F-Measure are consistently highercompared to the features in isolation (rows 2 - 4).Row 8 shows the results of all features in combina-tion.
Again, we observe the complementarity of thefeatures, as the results of this feature combinationare the best regarding F-Measure across all datasets.In row 9 of the results, we exclude the tokenstring as a feature.
In comparison to the full fea-ture combination of row 8 we observe a significantdecrease of F-Measure on the movies and the web-services dataset.
On the cars dataset we only observea slight decrease of recall.
Interestingly on the cam-eras dataset we even observe a slight increase of pre-cision which compensates a slight decrease of recall,1041in turn resulting in stable F-Measure of 0.500 as inthe full feature set of row 8.We have run some additional experiments inwhich we did not rely on the annotated opinion ex-pressions, but employed a general pupose subjectiv-ity lexicon4.
Already in the single-domain extrac-tion, we observed that the results declined substan-tially (e.g.
web-services F-Measure: 0.243, moviesF-Measure: 0.309, cars F-Measure: 0.192 and cam-eras F-Measure: 0.198).We performed a quantitative error analysis on theresults of the CRF-based approach in the single-domain setting.
In doing so, we focused on misclas-sifications of B-Target and I-Target instances, as therecall is consistently lower than the precision acrossall datasets.
We observe that most of the recall errorsresult from one-word opinion targets or the begin-ning of opinion targets (B-Targets) being missclassi-fied as non-targets (movies 83%, web-services 73%,cars 68%, cameras 64%).
For the majority of thesemissclassifications neither the dLn nor the wDs fea-tures were present (movies 82%, web-services 56%,cars 64%, cameras 61%).
We assume that our fea-tures cannot capture the structure of more complexsentences very well.
Our results indicate that thedLn and wDs features are complementary, but appar-ently there are quite a few cases in which the opin-ion target is neither directly related to the opinionexpression in the dependency graph nor close to itin the sentence.
One of these sentences, in this casefrom a camera review, in shown in Example 4.
(4) A lens cap and a strap may not sound veryimportant, but it makes a huge difference in thespeed and usability of the camera.In this sentence, the dLn and wDs features both la-beled ?speed?
which was incorrectly extracted as thetarget of the opinion.
None of the actual targets ?lenscap?, ?strap?
and ?camera?
have a short dependencypath to the opinion expression and ?speed?
is sim-ply the closest noun to it.
Note that although both?speed?
and ?usability?
are attributes of a camera,the opinion in this sentence is about the ?lens cap?and ?strap?, hence only these attributes are anno-tated as targets.4http://www.cs.pitt.edu/mpqa/5.2 Cross-Domain Results5.2.1 Zhuang BaselineTable 4 shows the results of the opinion target ex-traction with the state-of-the-art system in the cross-domain setting.
We observe that the results on alldomain combinations are very low.
A quantitativeerror analysis has revealed that there is hardly anyoverlap in the opinion target candidates between do-mains, as reflected by the low recall in all config-urations.
The vocabularies of the opinion targetsare too different, hence the performance of the algo-rithm by Zhuang et al (2006) is so low.
The overlapregarding the dependency paths between domainswas however higher.
Especially identical short pathscould be found across domains which at the sametime typically occured quite often.
For future workit might be interesting to investigate how the algo-rithm by Zhuang et al (2006) performs in the cross-domain setting if the target candidate learning is per-formed differently, e.g.
with a statistical approach asoutlined in Section 2.1.5.2.2 CRF-based ApproachThe results of the cross-domain target extractionwith the CRF-based algorithm are shown in Table 5.Due to the increase of system configurations intro-duced by the training - testing data combinations,we had to limit results of the feature combinationsreported in the Table.
The feature combination pos,sSn, wDs, dLn yielded the best results regarding F-Measure.
Hence, we report its result as the basic fea-ture set.
When comparing the results of the best per-forming feature / training data combination of ourCRF-based approach with the baseline, we observethat our approach outperforms the baseline on allfour domains.
The gain in F-Measure is 0.409 in themovies domain, 0.242 in the web-services domain,0.294 in the cars domain and 0.343 in the camerasdomain.Effects of FeaturesInterestingly with the best performing feature com-bination from the single-domain extraction, the re-sults regarding recall in the cross-domain extractionare very low5.
This is due to the fact that the CRF at-tributed a relatively large weight to the token string5Not shown in any result table due to limited space.1042Table 4: Cross-Domain Extraction with Zhuang BaselineTraining Testing Precision Recall F-Measureweb-services movies 0.194 0.032 0.055cars movies 0.032 0.034 0.033cameras movies 0.155 0.084 0.109cars + cameras movies 0.071 0.104 0.084web-services + cars + cameras movies 0.070 0.103 0.083movies web-services 0.311 0.073 0.118cars web-services 0.086 0.091 0.089cameras web-services 0.164 0.081 0.108cars + cameras web-services 0.086 0.104 0.094movies + cars + cameras web-services 0.074 0.100 0.080movies cars 0.182 0.014 0.026web-services cars 0.218 0.028 0.049cameras cars 0.250 0.121 0.163cameras + web-services cars 0.247 0.131 0.171movies + web-services cars 0.246 0.045 0.076movies cameras 0.108 0.012 0.022web-services cameras 0.268 0.048 0.082cars cameras 0.125 0.160 0.140cars + web-services cameras 0.119 0.157 0.136movies + web-services cameras 0.245 0.063 0.100feature.
As we also observed in the analysis of thebaseline results, the overlap of the opinion target vo-cabularies between domains is low, which resultedin a very small number of targets extracted by theCRF.
As shown in Table 5 the results are promisingregarding F-Measure if we just leave the token fea-ture out of the configuration.Effects of Training DataWhen analyzing the results of the different training- testing domain configurations we observe the fol-lowing: In isolation training data from the camerasdomain consistently yields the best results regardingF-Measure when the algorithm is run on the datasetsfrom the other three domains.
This is particularlyinteresting since the cameras dataset is the smallestof the four (see Table 1).
We investigated whetherthe CRF algorithm was overfitting to the trainingdatasets by reducing their size to the size of the cam-eras dataset.
However, the reduction of the train-ing data sizes never improved the extraction resultsregarding F-Measure for the movies, web-serviecsand cars datasets.
The good results when trainingon the cameras dataset are in line with our obser-vations from Section 5.1.2.
We noticed that on thecameras dataset the results regarding F-Measure re-mained stable if the token feature is not used in thetraining.In isolation, training only on the cars data yieldsthe second highest results on the movies and web-services datasets and the highest results regardingF-Measure on the cameras data.
However, the re-sults of the cars + cameras training data combinationindicate that the cameras data does not contributeany additional information during the learning, sincethe results on both the movies and the web-servicesdatasets are lower than when training only on thecameras data.Our results also confirm the insights gainedby Blitzer et al (2007), who observed that in cross-domain polarity analysis adding more training datais not always beneficial.
Apparently even the small-est training dataset (cameras) contain enough featureinstances to learn a model which performs well onthe testing data.We observe that the results of the cross-domainextraction regarding F-Measure come relativelyclose to the results of the single-domain setting, es-pecially if the token string feature is removed there(see Table 3 row 9).
On the cars and the camerasdataset the cross-domain results are even closer tothe single-domain results.
The features we employseem to scale well across domains and compensatethe difference between training and testing data andthe lack of information regarding the target vocabu-1043Table 5: Cross-Domain Extraction with our CRF-based ApproachTestingweb-services moviesPre Rec F-Me Pre Rec F-MeTrainingweb-services - - - 0.560 0.339 0.422movies 0.565 0.219 0.316 - - -cars 0.538 0.248 0.340 0.642 0.382 0.479cameras 0.529 0.256 0.345 0.642 0.408 0.499movies + cars 0.554 0.249 0.344 - - -movies + cameras 0.530 0.273 0.360 - - -movies + cars + cameras 0.562 0.250 0.346 - - -cars + cameras 0.538 0.254 0.345 0.641 0.395 0.489web-services + cars - - - 0.651 0.396 0.492web-services + cameras - - - 0.642 0.435 0.518web-services + cars + cameras - - - 0.639 0.405 0.496cars camerasPre Rec F-Me Pre Rec F-Meweb-services 0.391 0.277 0.324 0.505 0.330 0.399movies 0.512 0.307 0.384 0.550 0.303 0.391cars - - - 0.665 0.369 0.475cameras 0.589 0.384 0.465 - - -cameras + movies 0.567 0.394 0.465 - - -cameras + web-services 0.572 0.381 0.457 - - -movies + web-services 0.489 0.327 0.392 0.553 0.339 0.421movies + cars - - - 0.634 0.376 0.472web-services + cars - - - 0.678 0.376 0.483web-services + movies + cars - - - 0.635 0.378 0.474movies + web-services + cameras 0.549 0.381 0.450 - - -lary.6 ConclusionsIn this paper, we have shown how a CRF-basedapproach for opinion target extraction performs ina single- and cross-domain setting.
We have pre-sented a comparative evaluation of our approachon datasets from four different domains.
In thesingle-domain setting, our CRF-based approach out-performs a supervised baseline on all four datasets.Our error analysis indicates that additional features,which can capture opinions in more complex sen-tences, are required to improve the performance ofthe opinion target extraction.
Our CRF-based ap-proach also yields promising results in the cross-domain setting.
The features we employ scale wellacross domains, given that the opinion target vocab-ularies are substantially different.
For future work,we might investigate how machine learning algo-rithms, which are specifically designed for the prob-lem of domain adaptation (Blitzer et al, 2007; Jiangand Zhai, 2007), perform in comparison to our ap-proach.
Since three of the features we employed inour CRF-based approach are based on the respec-tive opinion expressions, it is to investigate how tomitigate the possible negative effects introduced byerrors in the opinion expression identification if theyare not annotated in the gold standard.
We observesimilar challenges as Choi et al (2005) regarding theanalysis of complex sentences.
Although our data isuser-generated from Web 2.0 communities, a man-ual inspection has shown that the documents wereof relatively high textual quality.
It is to investigateto which extent the approaches taken in the analysisof newswire, such as identifying targets with coref-erence resolution, can also be applied to our task onuser-generated discourse.AcknowledgmentsThe project was funded by means of the German Fed-eral Ministry of Economy and Technology under thepromotional reference ?01MQ07012?.
The authors takethe responsibility for the contents.
This work has beensupported by the Volkswagen Foundation as part ofthe Lichtenberg-Professorship Program under grant No.I/82806.1044ReferencesAnthony Aue and Michael Gamon.
2005.
Customizingsentiment classifiers to new domains: A case study.In Proceedings of the 5th International Conferenceon Recent Advances in Natural Language Processing,Borovets, Bulgaria, September.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 440?447,Prague, Czech Republic, June.Kenneth Bloom, Navendu Garg, and Shlomo Argamon.2007.
Extracting appraisal expressions.
In Proceed-ings of Human Language Technologies 2007: TheConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 308?315, Rochester, New York, USA, April.Yejin Choi, Claire Cardie, Ellen Riloff, and SiddharthPatwardhan.
2005.
Identifying sources of opinionswith conditional random fields and extraction patterns.In Proceedings of Human Language Technology Con-ference and Conference on Empirical Methods in Nat-ural Language Processing, pages 355?362, Vancou-ver, Canada, October.James R. Cowie and Wendy G. Lehnert.
1996.
In-formation extraction.
Communications of the ACM,39(1):80?91.Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research (JAIR), 26:101?126.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 168?177,Seattle, Washington, USA, August.Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in nlp.
In Proceedings of the45th Annual Meeting of the Association of Computa-tional Linguistics, pages 264?271, Prague, Czech Re-public, June.
Association for Computational Linguis-tics.Jason Kessler and Nicolas Nicolov.
2009.
Targeting sen-timent expressions through supervised ranking of lin-guistic configurations.
In Proceedings of the Third In-ternational AAAI Conference on Weblogs and SocialMedia, pages 90?97, San Jose, California, USA, May.Soo-Min Kim and Eduard Hovy.
2006.
Extracting opin-ions, opinion holders, and topics expressed in onlinenews media text.
In Proceedings of the ACL Workshopon Sentiment and Subjectivity in Text, pages 1?8, Syd-ney, Australia, July.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proceedings of the 18th International Conferenceon Machine Learning, pages 282?289, Williamstown,MA, USA, June.Fuchun Peng and Andrew McCallum.
2006.
Informationextraction from research papers using conditional ran-dom fields.
Information Processing and Management,42(4):963?979, July.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InProceedings of Human Language Technology Confer-ence and Conference on Empirical Methods in Natu-ral Language Processing, pages 339?346, Vancouver,Canada, October.Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.2010.
Sentence and expression level annotation ofopinions in user-generated discourse.
In Proceedingsof the 48th Annual Meeting of the Association forComputational Linguistics, pages 575?584, Uppsala,Sweden, July.Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, andWayne Niblack.
2003.
Sentiment analyzer: Extract-ing sentiments about a given topic using natural lan-guage processing techniques.
In Proceedings of the3rd IEEE International Conference on Data Mining,pages 427?434, Melbourne, Florida, USA, December.Li Zhuang, Feng Jing, and Xiao-Yan Zhu.
2006.
Moviereview mining and summarization.
In Proceedings ofthe ACM 15th Conference on Information and Knowl-edge Management, pages 43?50, Arlington, Virginia,USA, November.1045
