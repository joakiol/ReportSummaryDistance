MULTIPLE APPROACHES TO ROBUST SPEECH RECOGNITIONRichard M. Stern, Fu-Hua Liu, Yoshiaki Ohshima, Thomas M. Sullivan, Alejandro Acero*Department of  Electrical and Computer EngineeringSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213ABSTRACTThis paper compares everal different approaches to robustspeech recognition.
We review CMU's ongoing research in theuse of acoustical pre-proeessing to achieve robust speech recog-nition, and we present the results of the first evaluation of pre-processing in the context of the DARPA standard ATIS domainfor spoken language systems.
We also describe and compare theeffectiveness of three complementary methods of signal process-ing for robust speech recognition: acoustical pre-procossing,microphone array processing, and the use of physiologically-motivated models of peripheral signal processing.
Recognitionerror ates are presented using these three approaches in isolationand in combination with each other for the speaker-independentcontinuous alphanumeric census peech recognition task.1.
INTRODUCTIONThe need for speech recognition systems and spoken lan-guage systems to be robust with respect to their acousticalenvironment has become more widely appreciated inrecent years (e.g.
\[1\]).Results of several studies have demonstrated that evenautomatic speech recognition systems that are designed tobe speaker independent can perform very poorly when theyare tested using a different ype of microphone or acous-tical environment from the one with which they weretrained (e.g.
\[2, 3\]), even in a relatively quiet office en-vironment.
Applications uch as speech recognition overtelephones, in automobiles, on a factory floor, or outdoorsdemand an even greater degree of environmental robust-ness.The CMU speech group is committed to the developmentof speech recognition systems that are robust with respectto environmental variation, just as it has been an earlyproponent of speaker-independent r cognition.
While mostof our work presented to date has described new acousticalpre-processing algorithms (e.g.
\[2, 4, 5\], we have alwaysregarded pre-processing asone of several approaches thatmust be developed in concert to achieve robust recog-nition.The purpose of this paper is twofold.
First, we describeour results for the DARPA benchmark evaluation forrobust speech recognition for the ATIS task, discussing theeffectiveness of our methods of acoustical pre-preprocessing in the context of this task.
Second, wedescribe and compare the effectiveness of three com-plementary methods of signal processing for robust speechrecognition: acoustical pre-processing, microphone arrayprocessing, and the use of physiologically-motivatedmodels of peripheral signal processing.2.
ACOUSTICAL  PRE-PROCESSINGWe have found that two major factors degrading the per-formance of speech recognition systems using desktopmicrophones in normal office environments are additivenoise and unknown linear filtering.
We showed in \[2\] thatsimultaneous joint compensation for the effects of additivenoise and linear filtering is needed to achieve maximalrobustness with respect o acoustical differences betweenthe training and testing environments of a speech recog-nition system.
We described in \[2\] two algorithms that canperform such joint compensation, based on additive correc-tions to the cepstral coefficients of the speech waveform.The first compensation algorithm, SNR-DependentCepstral Normalization (SDCN), applies an additive cor-rection in the cepstral domain that depends exclusively onthe instantaneous SNR of the signal.
This correction vec-tor equals the average difference in cepstra between simul-taneous "stereo" recordings of speech samples from boththe training and testing environments at each SNR ofspeech in the testing environment.
At high SNRs, thiscorrection vector primarily compensates for differences inspectral flit between the training and testing environments(in a manner similar to the blind deconvolution procedurefirst proposed by Stockham et al \[6\]), while at low SNRsthe vector provides a form of noise subtraction (in a man-ner similar to the spectral subtraction algorithm firstproposed by Boll \[7\]).
The SDCN algorithm is simple andeffective, but it requires environment-specific training.The second compensation algorithm, Codeword-Dependent Cepstral Normalization (CDCN), uses EMtechniques to compute ML estimates of the parameterscharacterizing the contributions of additive noise andlinear filtering that when applied in inverse fashion to the*Present address: Telef6nica Invesfigaci6n y Desarrollo, Emilio Vargas 6, Madrid 28043, Spain274cepstra of an incoming utterance produce an ensemble ofcepstral coefficients that best match (in the ML sense) thecepstral coefficients of the incoming speech in the testingenvironment tothe locations of VQ codewords in the train-ing environment.
The CDCN algorithm has the advantagethat it does not require a pr io r i  knowledge of the testingenvironment (in the form of stereo training data in thetraining and testing environments), but it is much morecomputationally demanding than the SDCN algorithm.Compared to the SDCN algorithm, the CDCN algorithmuses a greater amount of structural knowledge about thenature of the degradations to the speech signal in order toachieve good recognition accuracy.
The SDCN algorithm,on the other hand, derives its compensation vectors en-tirely from empirical observations of differences betweendata obtained from the training and testing environments.1006O4020O.Train CLSTLK, Tu l  PZM6FS%"0.... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.~..Q\ [ \ ]  \ [ \ ]  .
.
.
.
.
.
.
.
.
.
\ [ \ ]Train and Toe!
CLSTLK !
!Baseline Norm Sub CDCNFigure 1: Comparison of error rates obtained on the cen-sus task with no processing, spectral subtraction, spectralnormalization, and the CDCN algorithm.
SPHINX wastrained on the CLSTLK microphone and tested using ei-ther the CLSTLK microphone (solid curve) or thePZM6FS microphone (broken curve).Figure 1 compares the error rate obtained when the SPHINXsystem is trained using the DARPA standard HMD-414closetalking microphone (CLSTLK), and tested using ei-ther the CLSTLK microphone or the omnidirectionaldesktop Crown PZM-6FS microphone (PZM6FS).
Thecensus database was used, which contains simultaneousrecoredings of speech from the CLSTLK and PZM6FSmicrophones in the context of a speaker-independentcontinuous-speech alphanumeric task with perplexity 65\[2\].
These results demonstrate the value of the joint com-pensation provided by the CDCN algorithm in contrast tothe independent compensation using either spectral sub-traction or spectral normalization.
The horizontal dottedlines indicate the recognition accuracy obtained when thesystem is tested on the microphone with which it wastrained, with no processing.
The intersection of the uppercurve with the upper horizontal line indicates that withCDCN compensation, SPHINX Can recognize speech usingthe PZM6FS microphone just as well when trained on theCLSTLK microphone as when trained using the PZM6FS.More recently we have been attempting to develop newalgorithms which combine the computational simplicity ofSDCN with the environmental independence of CDCN.One such algorithm, Bl ind  SNR-Dependent  Cepst ra lNormal i za t ion  (BSDCN) avoids the need for environment-specific training by establishing a correspondence b tweenALGO- ENVIRN.
COM- ERRRITHM SPEC?
PLEXITY RATENONE NO NONE 68.6%SDCN YES MINIMAL 27.6%CDCN NO GREATER 24.3%BSDCN NO MINIMAL 30.0%Table 1: Comparison of recognition accuracy of SPHINXwith no processing and the CDCN, SDCN, and BSDCNalgorithms.
The system was trained using the CLSTLKmicrophone and tested using the PZM6FS microphone.Training and testing on the CLSTLK produces a recog-nition accuracy of 86.9%, while training and testing on thePZM6FS produces 76.2%SNRs in the training and testing environments by use oftraditional nonlinear warping techniques \[8\] on histogramsof SNRs from each of the two environments \[5\].
Table 1compares the environmental specificity, computationalcomplexity, and recognition accuracy of these algorithmswhen evaluated on the alphanumeric database described in\[2\].
Recognition accuracy is somewhat different from thefigures reported in Fig.
1 because the version of SPHINXused to produce these data was different.
All of these al-gorithms are similar in function to other currently-popularcompensation strategies (e.g.
\[3, 9\]).The DARPA ATIS robust speech evaluation.
Theoriginal CDCN algorithm described in \[2\] was used for theFebruary, 1992, ATIS-domain robust-speech evaluation.For this evaluation, the SPHINX-II system was trained usingthe CLSTLK microphone, and tested using both theCLSTLK microphone and the unidirectional CrownPCC-160 microphone (PCC160).
All incoming speech inthis evaluation was processed by the CDCN algorithm,regardless of whether the testing environment was actuallythe CLSTLK or PCC160 microphone, and the CDCN algo-rithm was not provided with explicit knowledge of theidentity of the environment within which it is operating.As described elsewhere in these Proceedings \[10\], the sys-tem used for the official robust-speech evaluations was nottrained as thoroughly as the baseline system was lrained.Specifically, the official evaluations were performed afteronly a single iteration through training data that wasprocessed with the CDCN algorithm, and without thebenefit of general English sentences in the trainingdatabase.In Fig.
2 we show the results of an unofficial evaluation ofthe SPHINX-II system that was performed immediatelyafter the official evaluation was complete.
The purpose ofthis second evaluation was to evaluate the improvementprovided by an additional round of training with speechprocessed by CDCN, in order to be able to directly com-pare error rates on the ATIS task with CDCN with thoseproduced by a comparably-trained system on the samedata, but without CDCN.
As Fig.
2 shows, using the275CDCN algorithm causes the error rate to increase from15.1% to only 20.4% as the testing microphone is changedfrom the CLSTLK to the PCC160 microphone.
In contrast,the error rate increases from 12.2% to 38.8% when oneswitches from the CLSTLK to the PCC160 microphonewithout CDCN.,250a302010G ..~) TeBt PCC.t~O~J~ Tesl CLSTLK!
!Basollno CDCNFigure 2: Comparison of error rates obtained on theDARPA ATIS task with no processing, spectral subtrac-tion, spectral normalization, and the CDCN algorithm.SPmNX-II was trained on the CLSTLK microphone in allcases, and tested using either the CLSTLK microphone(solid curve) or the cardiod desktop Crown PCC160microphone (broken curve).Only two sites submitted data for the present robust speechevaluation.
CMU's percentage degradation i error rate inchanging from the CLSTLK to the PCC160 environment,as well as the absolute error rate obtained using thePCC160 microphone, were the better of the results fromthese two sites.3.
M ICROPHONE ARRAYS ANDACOUSTICAL  PRE-PROCESSINGDespite the encouraging results that we have achievedusing acoustical pre-processing, we believe that further im-provements in recognition accuracy can be obtained in dif-ficult environments by combining acoustical pre-processing with other complementary t pes of signalprocessing.
The use of microphone arrays is motivated bya desire to improve the effective SNR of speech as it isinput to the recognition system.
For example, the headset-mounted CLSTLK microphone produces a higher SNRthan the PZM6FS microphone under normal circumstancesbecause it picks up a relatively small amount of additivenoise, and the incoming signal is not degraded by rever-berated components of the original speech.To estimate the potential significance of the reduced SNRprovided by the PZM6FS microphone in the office en-vironment, we manually examined all utterances in the testset of the census task that were recognized correctly whentraining and testing with the CLSTLK microphone but thatwere recognized incorrectly when training and testingusing the PZM6FS.
We found that 54.7 percent of theseerrors were caused by the confusion of silence or noisesegments with weak phonetic events, (20 percent of theerrors were caused by cross-talk from other noise sourcesin the room, and the remaining errors could not be at-tributed to a particular cause.)
Microphone arrays can, inprinciple, produce directionally-sensitive gain patterns thatcan be adjusted to produce maximal sensitivity in thedirection of the speaker and reduced sensitivity in thedirection of competing sound sources.
To the extent hatsuch processing could improve the effective SNR at theinput to a speech recognition system, the error rate wouldbe likely to be substantially decreased, because the numberof confusions between weak phonetic events and noisewould be sharply reduced.Several different types of array-processing strategies havebeen applied to automatic speech recognition.
Thesimplest approach is that of the delay-and-sum beam-former, in which delays are inserted in each channel tocompensate for differences in travel time between thedesired sound source and the various sensors (e.g.\[11, 12\]).
A second option is to use an adaptation algo-rithm based on minimizing mean square nergy such as theFrost or Gfiffiths-Jim algorithm \[13\].
These algorithmsprovide the opportunity to develop nulls in the direction ofnoise sources as well as more sharply focused beam pat-terns, but they assume that he desired signal is statisticallyindependent of all sources of degradation.
Consequently,these algorithms can provide good improvement in SNRwhen signal degradations are caused by additive independ-ent noise sources, but these algorithms do not perform wellin reverberant environments when the distortion is at leastin part a delayed version of the desired speech signal\[14, 15\].
(This problem can be avoided by only adaptingduring non-speech segments \[16\]).
A third type of ap-proach to microphone arraay processing is to use a cross-correlation-based algorithm that isolates inter-sensor dif-ferences in arrival time of the signals directly (e.g.
\[17\]).These algorithms are appealing because they are based onhuman binaural hearing, and cross-correlation is an ef-ficient way to identify the direction of a strong signalsource.
Nevertheless, the nonlinear nature of the cross-correlation operation renders it inappropriate as a means todirectly process waveforms.
We believe that signalprocessing techniques based on human binaural perceptionare worth pursuing, but their effectiveness for automaticspeech recognition remains to be conclusivelydemonstrated.Pilot evaluation of the Flanagan array.
In order to ob-tain a better understanding of the ability of array process-ing to provide further improvements in recognition ac-curacy we conducted a pilot evaluation of the 23-microphone array developed by Flanagan and his col-leagues at AT&T Bell Laboratories.
The Flanagan array,which is described in detail in\[l , 12\], is a one-dimensional delay-and-sum beamformer which uses 23microphones that are unevenly spaced in order to provide abeamwidth at is approximately constant over the range offrequencies of interest.
The array uses first-order gradientmicrophones, which develop anull response in the verticalplane.
We wished to compare the recognition accuracy onthe census task obtained using the Flanagan array with theaccuracy observed using the CLSTLK and PZM6FS276microphones.
We were especially interested in determin-ing the extent o which array processing provides an im-provement in recognition accuracy that is complementaryto the improvement in accuracy provided by acousticalpre-processing algorithms uch as the CDCN algorithm..28040200.,99,8O4O200= = 3 Meters!
| !
m !PZM6FS Array Array CLSTLKCLSTLK4.CDCN +CDCNMicrophone Type= = 3 Meters = '"2"rI I I I tPZM6FS PZM6FS Array CLSTLK CLSTLK+CDCN +CDCN +CDCNMicrophone TypeFigure 3: Comparison of recognition accuracy obtainedon a portion of the census task using the omnidirectionalCrown PZM-6FS, the 23-microphone array developed byFlanagan, and the Senneheiser microphone, each withand without CDCN.
Data were obtained from simul-taneous recordings using the three microphones at dis-tances of 1 and 3 meters (for the PZM-6FS and the array).14 utterances from the census database were obtained fromeach of five male speakers in a sparsely-furnishedlaboratory at the Rutgers CAIP Center with hard walls andfloors.
The reverberation time of this room was informallyestimated to be between 500 and 750 ms. Simultaneousrecordings were made of each utterance using threemicrophones: the Sennheiser HMD-414 (CLSTLK)microphone, the Crown PZM6FS, and the Flanagan arraywith input lowpass-filtered at 8 kHz.
Recordings weremade with the speaker seated at distances of 1, 2, and 3meters from the PZM6FS and Flanagan arraymicrophones, wearing the CLSTLK microphone in theusual fashion at all times.Figure 3 summarizes the error rates obtained from thesespeech samples at two distances, 1and 3 meters, with andwithout he CDCN algorithm applied to the output of themicrophone army.
Error rates using the CLSTLKmicrophone differed somewhat for the two distances be-cause different speech samples were obtained at each dis-tance and because the sample size is small.
The SPHINXsystem had been previously trained on speech obtainedusing the CLSTLK microphone.
As expected, the worstresults were obtained using the PZM6FS microphone,while the lowest error raate was obtained for speechrecorded using the CLSTLK.
More interestingly, theresults in Fig.
3 show that both the Flanagan array and theCDCN algorithm are effective in reducing the error rate,and that in fact the error rate at each distance obtained withthe combination of the two is very close to the error rateobtained with the CLSTLK microphone and no acousticalpre-processing.
The complementary nature of the im-provement of the Flanagan array and the CDCN algorithmis indicated by the fact that adding CDCN to the arrayimproves the error rate (upper panel of Fig.
3), and thatconverting to the array even when CDCN is alreadyemployed also improves performance (lower panel).4.
PHYSIOLOGICALLY-MOTIVATEDFRONT ENDS ANDACOUSTICAL PRE-PROCESSINGIn recent years there has also been an increased interest'inthe use of peripheral signal processing schemes that aremotivated by human auditory physiology and perception,and a number of such schemes have been proposed (e.g.\[18, 19, 20, 21\]).
Recent evaluations indicate that with"clean" speech, such approaches tend to provide recog-nition accuracy that is comparable to that obtained withconventional LPC-based or DFT-based signal processingschemes, but that these auditory models can providegreater obustness with respect o enviromental changeswhen the quality of the incoming speech (or the extent owhich it resembles peech used in training the system)decreases \[22, 23\].
Despite the apparent utility of suchprocessing schemes, no one has a deep-level understandingof why they work as well as they do, and in fact differentresearchers choose to emphasize rather different aspects ofthe peripheral auditory system's response to sound in theirwork.
Most auditory models include a set of linearbandpass filters with bandwidth that increases nonlinearlywith center frequency, a nonlinear rectification stage thatfrequently includes short-term adaptation and lateral sup-pression, and, in some cases, a more central display basedon short-term temporal information.
We estimate that thenumber of arithmetic operations of some of the currently-popular auditory models ranges from 35 to 600 times thenumber of operations required for the LPC-based process-ing used in SPmNX-II.Pilot evalution of the Seneff auditory model.
Werecently completed a series of pilot evaluations using animplementation f the Seneff auditory model \[21\] on thecensus databse.
Since almost all evaluations ofphysiologically-motivated front ends to date have beenperformed using artificaUy-added white Gaussian noise,we have been interested in the extent o which auditorymodels can provide useful improvements in recognitionaccuracy for speech that has been degraded by reverbera-tion or other types of linear filtering.
As in the case ofmicrophone arrays, we are also especially interested indetermining the extent o which improvements in robust-277ness provided by auditory modelling complement thosethat we already enjoy by the use of acoustical pre-processing algorithms such as CDCN.We compared error rates obtained using the standard 12LPC-based cepstral coefficents normally input to theSPHINX system, with those obtained using an implemen-tation of the 40-channel mean-rate output of the Seneffmodel \[21\], and with the 40-channel outputs of Seneff'sGeneralized Synchrony Detectors (GSDs).
The systemwas evaluated using the original testing database from thecensus task with the CLSTLK and PZM6FS microphones,and also with white Ganssian noise artificially added atsig.nal-to-noise ratios of +10, +20, and +30 dB, measuredusing the global SNR method escribed in \[19\].100 Z~... 80 J."
"~ LPCI~ ~ ,  ~"'~' LPC+CDCN60 ,me Mean Rate%% : : GSD4o2O0 lO'dB 20'dB 30'dB CI;anSNR.~, 100@ so4O k- ~ LPC20 : = Mean Rate- - GSD' ' 30'dB ' 10 dB 20 dB CleanSNRFigure 4: Pilot data comparing error rates obtained on thecensus task using the conventional LPC-based processingof SPHINX with results obtained using the mean rate andsynchrony outputs of the Seneff auditory model.
SPHINXwas trained on the CLSTLK microphone in all cases, andtested using either the CLSTLK microphone (upper panel)or the Crown PZM6FS microphone (lower panel).
Whitenoise was artificially added to the speech signals and dataare plotted as a function of global SNR.Figure 4 summarizes the results of these comparisons, witherror rate plotted as a function of SNR using each of thethree peripheral signal processing schemes.
The upperpanel describes recognition error rates obtained with thesystem both trained and tested using the CLSTLKmicrophone, and the lower panel describes error rates ob-tained with the system trained with the CLSTLKmicrophone but tested with the PZM6FS microphone.When the system is trained and tested using the CLSTLKmicrophone, best performance is obtained using conven-tional LPC-based signal processing for "clean" speech.
Asthe SNR is decreased, however, error rates obtained usingeither the mean rate or GSD outputs of the Seneff modeldegrade more gradually confirming similar findings fromprevious tudies.
The results in the lower panel of Fig.
4,demonstrate hat the mean rate and GSD outputs of theSeneff model provide lower error rates than conventionalLPC cepstra when the system is trained using the CLSTLKmicrophone and tested using the PZM6FS.
Nevertheless,the level of performance achieved by the present im-plementation of the auditory model is not as good as thatachieved by conventional LPC cepstra combined with theCDCN algorithm on the same data (Fig.
1).
Furthermore,the combination of conventional LPC-based processingand the CDCN algorithm produced performance thatequaled or bettered the best performance obtained with theauditory model for each test condition.
Because theauditory model is nonlinear and not easy to port from onesite to another, these comparisons should all be regarded aspreliminary.
It is quite possible that performance using theauditory model could further improve if greater attentionwere paid to tuning it to more closely match the charac-teristics of SPHINX.We also attempted todetermine the extent o which a com-bination of auditory processing and the CDCN algorithmcould provide greater ecognition accuracy than eitherprocessing scheme used in isolation.
In these experimentswe combined the effects of CDCN and auditory processingby resynthesizing the speech waveform from cepstral coef-ficients that were produced by the original LPC front endand then modified by the CDCN algorithm.
The resyn-thesized speech, which was totally intelligible, was thenpassed through the Seneff auditory model in the usualfashion.
Unfortunately, it was found that this particularcombination of CDCN and the auditory model did not im-prove the recognition error raate beyond the level achievedby CDCN alone.
A subsequent error analysis revealed thatthis concatenation of cepstral processing and the CDCNalgorithm, followed by resynthesis and processing by theoriginal SPHINX front end, degraded the error rates even inthe absence of the auditory processing, although analysisand resynthesis without the CDCN algorithm did notproduce much degradation.
This indicates that useful in-formation for speech recognition is lost when the resyn-thesis process is performed after the CDCN algorithm isrun.
Hence we regard this experiment as inconclusive, andwe intend to explore other types of combinations ofacous-tical pre-processing with auditory modelling in the future.5.
SUMMARY AND CONCLUSIONSIn this paper we describe our current research in acousticalpre-processing for robust speech recognition, as well asour first attempts to integrate pre-processing with otherapproaches torobust speech recognition.
The CDCN algo-rithm was also applied to the ATIS task for the first time,and provided the best recognition scores for speech col-lected using the unidirectional desktop PCC160microphone.
We demonstrated that the CDCN algorithmand the Flanagan delay-and-sum icrophone array canprovide complementary benefits to speech recognition inreverberant environments.
We also found that the Seneffauditory model improves recognition accuracy of the CMUspeech system in reverberant as well as noisy environ-278ments, but preliminary efforts to combine the auditorymodel with the CDCN algorithm were inconclusive.ACKNOWLEDGMENTSThis research is sponsored by the Defense Advanced ResearchProjects Agency, DoD, through ARPA Order 7239, and monitoredby the Space and Naval Warfare Systems Command under con-tract N00039-91-C-0158.
Views and conclusions contained inthis document are those of the authors and should not be inter-preted as representing official policies, either expressed or im-plied, of the Defense Advanced Research Projects Agency or ofthe United States Government.
We thank Hsiao-Wuen Hon,Xuedong Huang, Kai-Fu Lee, Raj Reddy, Eric Thayer, BobWeide, and the rest of the speech group for their contributions tothis work.
We also thank Jim Flanagan, Joe French, andA.
C. Surendran for their assistance inobtaining the experimentaldata using the array microphone, and Stephanie Seneff forproviding source code for her auditory model.
The graduatestudies of Tom Sullivan and Yoshlaki Ohshima have been sup-ported by Motorola and IBM Japan, respectively.1.2.3.4.5.6.7.8.9.REFERENCESJuang, B. H., "Speech Recognition in Adverse Envoron-ments", Comp.
Speech and Lang., Vol.
5, 1991, pp.275 -294.Acero, A. and Stern, R. M., "Environmental Robusmessin Automatic Speech Recognition", ICASSP-90, April1990, pp.
849-852.Erell, A. and Weintraub, M., "Estimation Using Log-Spectral-Distance Criterion for Noise-Robust SpeechRecognition", ICASSP-90, April 1990, pp.
853-856.Acero, A. and Stem, R. M., "Robust Speech Recognitionby Normalization of the Acoustic Space", ICASSP-91,May 1991, pp.
893-896.Liu, F.-H., Acero, A., and Stern, R. M., "Efficient JointCompensation ofSpeech for the Effects of Additive Noiseand Linear Filtering", IEEE International Conference onAcoustics, Speech, and Signal Processing, March 1992.Stockham, T. G., Cannon, T. M,.
and lngebretsen, R. B.,"Blind Deconvolution Through Digital Signal Process-ing", Proc.
IEEE, Vol.
63, 1975, pp.
678-692.Boll, S. F. , "Suppression of Acoustic Noise in SpeechUsing Spectral Subtraction", ASSP, Vol.
27, 1979, pp.113-120.Sakoe, H., Chiba, S., "Dynamic Programming AlgorithmOptimization for Spoken Word Recognition", IEEETransactions on Acoustics, Speech, and SignalProcessing, Vol.
26, 1978, pp.
43-49.Hermansky, H., Morgan, N., Bayya, A., Kohn, P., "Com-pensation for the Effect of the Communication Channel inAuditory-Like Analysis of Speech (RASTA-PLP)", Proc.of the Second European Confi on Speech Comm.
andTech., September 1991.10.
Ward, W., Issar, S., Huang, X., Hon, H.-W., Hwang,M.-Y., Young, S., Matessa, M., Liu, F.-H., Stem, R.,"Speech Understanding in Open Tasks", Proc.
of theDARPA Speech and Natural Language Workshop,February 1992.11.
Flanagan, J. L., Johnston, J. D., Zahn, R., and Elko, G.W.,"Computer-steered Microphone Arrays for SoundTransduction in Large Rooms", The Journal of theAcoustical Society of America, Vol.
78, Nov. 1985, pp.1508-1518.12.
Flanagan, J. L., Berkeley, D. A., Elko, G. W., West, J. E.,and Sondhi, M. M., "Autodirective microphone sys-terns", Acustica, Vol.
73, February 1991, pp.
58-71.13.
Widrow, B., and Stearns, S. D., Adaptive SignalProcessing, Prentice-Hall, Englewood Cliffs, NJ, 1985.14.
Peterson, P. M., "Adaptive Array Processing for MultipleMicrophone Hearing Aids".
RLE TR No.
541, Res.
Lab.of Electronics, MIT, Cambridge, MA15.
Alvarado, V. M., Silverman, H. F., "ExperimentalResults Showing the Effects of Optimal Spacing BetweenElements of a Linear Microphone Array", ICASSP-90,April 1990, pp.
837-840.16.
Van Compernolle, D., "Switching Adaptive Filters forEnhancing Noisy and Reverberant Speech fromMicrophone Array Recordings", IEEE International Con-ference on Acoustics, Speech, and Signal Processing,April 1990, pp.
833-836.17.
Lyon, R. F., "A Computational Model of BinauralLocalization and Separation", IEEE International Con-ference on Acoustics, Speech, and Signal Processing,1983, pp.
1148-1151.18.
Cohen, J. R., "Application of an Auditory Model toSpeech Recognition", The Journal of the AcousticalSociety of America, Vol.
85, No.
6, June 1989, pp.2623 -2629.19.
Ghitza, O., "Auditory Nerve Representation as a Front-End for Speech Recognition in a Noisy Environment",Corap.
Speech and Lang., Vol.
1, 1986, pp.
109-130.20.
Lyon, R. F., "A Computational Model of Filtering,Detection, and Compression i the Cochlea", IEEE Inter-national Conference on Acoustics, Speech, and SignalProcessing, May 1982, pp.
1282-1285.21.
Seneff, S., "A Joint Synchrony\]Mean-Rate Model ofAuditory Speech Processing", Journal of Phonetics, Vol.16, No.
1, January 1988, pp.
55-76.22.
Hunt, M., "A Comparison of Several Acoustic Represen-tations for Speech Recognition with Degraded and Un-degraded Speech", IEEE International Conference onAcoustics, Speech, and Signal Processing, May 1989.23.
Meng, H., and Zue, V. W., "A Comparative Study ofAcoustic Representations of Speech for Vowel Classifica-tion Using Multi-Layer Perceptrons", Int.
Conf.
onSpoken Lang.
Processing, November 1990, pp.1053-1056.279
