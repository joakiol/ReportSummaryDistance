Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 793?800Manchester, August 2008Metric Learning for Synonym AcquisitionNobuyuki ShimizuInformation Technology CenterUniversity of Tokyoshimizu@r.dl.itc.u-tokyo.ac.jpMasato HagiwaraGraduate School of Information ScienceNagoya Universityhagiwara@kl.i.is.nagoya-u.ac.jpYasuhiro Ogawa and Katsuhiko ToyamaGraduate School of Information ScienceNagoya University{yasuhiro,toyama}@kl.i.is.nagoya-u.ac.jpHiroshi NakagawaInformation Technology CenterUniversity of Tokyon3@dl.itc.u-tokyo.ac.jpAbstractThe distance or similarity metric plays animportant role in many natural languageprocessing (NLP) tasks.
Previous stud-ies have demonstrated the effectiveness ofa number of metrics such as the Jaccardcoefficient, especially in synonym acqui-sition.
While the existing metrics per-form quite well, to further improve perfor-mance, we propose the use of a supervisedmachine learning algorithm that fine-tunesthem.
Given the known instances of sim-ilar or dissimilar words, we estimated theparameters of the Mahalanobis distance.We compared a number of metrics in ourexperiments, and the results show that theproposed metric has a higher mean averageprecision than other metrics.1 IntroductionAccurately estimating the semantic distance be-tween words in context has applications formachine translation, information retrieval (IR),speech recognition, and text categorization (Bu-danitsky and Hirst, 2006), and it is becomingclear that a combination of corpus statistics can beused with a dictionary, thesaurus, or other knowl-edge source such as WordNet or Wikipedia, to in-crease the accuracy of semantic distance estima-tion (Mohammad and Hirst, 2006).
Although com-piling such resources is labor intensive and achiev-ing wide coverage is difficult, these resources tosome extent explicitly capture semantic structuresc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.of concepts and words.
In contrast, corpus statis-tics achieve wide coverage, but the semantic struc-ture of a concept is only implicitly represented inthe context.
Assuming that two words are semanti-cally closer if they occur in similar contexts, statis-tics on the contexts of words can be gathered andcompared for similarity, by using a metric such asthe Jaccard coefficient.Our proposal is to extend and fine-tune the latterapproach with the training data obtained from theformer.
We apply metric learning to this task.
Al-though still in their infancy, distance metric learn-ing methods have undergone rapid development inthe field of machine learning.
In a setting simi-lar to semi-supervised clustering, where known in-stances of similar or dissimilar objects are given,a metric such as the Mahalanobis distance can belearned from a few data points and tailored to fit aparticular purpose.
Although classification meth-ods such as logistic regression now play impor-tant roles in natural language processing, the useof metric learning has yet to be explored.Since popular current methods for synonym ac-quisition require no statistical learning, it seemsthat supervised machine learning should easilyoutperform them.
Unfortunately, there are obsta-cles to overcome.
Since metric learning algorithmsusually learn the parameters of a Mahalanobis dis-tance, the number of parameters is quadratic to thenumber of features.
They learn how two featuresshould interact to produce the final metric.
Whiletraditional metrics forgo examining of the interac-tions entirely, in applying metrics such as Jaccardcoefficient, it is not uncommon nowadays to usemore than 10,000 features, a number that a typicalmetric learner is incapable of processing.
Thus wehave two options: one is to find the most impor-tant features and model the interactions between793them, and the other is simply to use a large numberof features.
We experimentally examined the twooptions and found that metric learning is useful insynonym acquisition, despite it utilizing fewer fea-tures than traditional methods.The remainder of this paper is organized as fol-lows: in section 2, we review prior work on syn-onym acquisition and metric learning.
In section3, we introduce the Mahalanobis distance metricand a learning algorithm based on this metric.
Insection 4 and 5, we explain the experimental set-tings and propose the use of normalization to makethe Mahalanobis distances work in practice, andthen in section 6, we discuss issues we encounteredwhen applying this metric to synonym acquisition.We conclude in section 7.2 Prior WorkAs this paper is based on two different lines of re-search, we first review the work in synonym acqui-sition, and then review the work in generic metriclearning.
To the best of the authors?
knowledge,none of the metric learning algorithms have beenapplied to automatic synonym acquisition.Synonym relation is important lexical knowl-edge for many natural language processingtasks including automatic thesaurus construction(Croach and Yang, 1992; Grefenstette, 1994) andIR (Jing and Croft, 1994).
Various methods (Hin-dle, 1990; Lin, 1998) of automatically acquiringsynonyms have been proposed.
They are usu-ally based on the distributional hypothesis (Har-ris, 1985), which states that semantically simi-lar words share similar contexts, and they can beroughly viewed as the combinations of two steps:context extraction and similarity calculation.
Theformer extracts useful features from the contexts ofwords, such as surrounding words or dependencystructure.
The latter calculates how semanticallysimilar two given words are based on similarity ordistance metrics.Many studies (Lee, 1999; Curran and Moens,2002; Weeds et al, 2004) have investigatedsimilarity calculation, and a variety of dis-tance/similarity measures have already been com-pared and discussed.
Weeds et al?s work is espe-cially useful because it investigated the character-istics of metrics based on a few criteria such asthe relative frequency of acquired synonyms andclarified the correlation between word frequency,distributional generality, and semantic generality.However, all of the existing research conductedonly a posteriori comparison, and as Weeds et alpointed out, there is no one best measure for all ap-plications.
Therefore, the metrics must be tailoredto applications, even to corpora and other settings.We next review the prior work in generic metriclearning.
Most previous metric learning methodslearn the parameters of the Mahalanobis distance.Although the algorithms proposed in earlier work(Xing et al, 2002; Weinberger et al, 2005; Glober-son and Roweis, 2005) were shown to yield excel-lent classification performance, these algorithmsall have worse than cubic computational complex-ity in the dimensionality of the data.
Because ofthe high dimensionality of our objects, we optedfor information-theoretic metric learning proposedby (Davis et al, 2007).
This algorithm only usesan operation quadratic in the dimensionality of thedata.Other work on learning Mahalanobis metrics in-cludes online metric learning (Shalev-Shwartz etal., 2004), locally-adaptive discriminative methods(Hastie and Tibshirani, 1996), and learning fromrelative comparisons (Schutz and Joahims, 2003).Non-Mahalanobis-based metric learning methodshave also been proposed, though they seem to suf-fer from suboptimal performance, non-convexity,or computational complexity.
Examples includeneighborhood component analysis (Goldberger etal., 2004).3 Metric Learning3.1 Problem FormulationTo set the context for metric learning, we first de-scribe the objects whose distances from one an-other we would like to know.
As noted above re-garding the distributional hypothesis, our object isthe context of a target word.
To represent the con-text, we use a sparse vector in Rd.
Each dimensionof an input vector represents a feature of the con-text, and its value corresponds to the strength ofthe association.
The vectors of two target wordsrepresent their contexts as points in multidimen-sional feature-space.
A suitable metric (for exam-ple, Euclidean) defines the distance between thetwo points, thereby estimating the semantic dis-tance between the target words.Given points xi, xj?
Rd, the (squared) Ma-halanobis distance between them is parameter-ized by a positive definite matrix A as followsdA(xi, xj) = (xi?
xj)?A(xi?
xj).
The Ma-794halanobis distance is a straightforward extensionof the standard Euclidean distance.
If we let Abe the identity matrix, the Mahalanobis distancereduces to the Euclidean distance.
Our objectiveis to obtain the positive definite matrix A that pa-rameterizes the Mahalanobis distance, so that thedistance between the vectors of two synonymouswords is small, and the distance between the vec-tors of two dissimilar words is large.
Stated moreformally, the Mahalanobis distance between twosimilar points must be smaller than a given upperbound, i.e., dA(xi, xj) ?
u for a relatively smallvalue of u.
Similarly, two points are dissimilar ifdA(xi, xj) ?
l for sufficiently large l.As we discuss below, we were able to usethe Euclidean distance to acquire synonyms quitewell.
Therefore, we would like the positive definitematrix A of the Mahalanobis distance to be close tothe identity matrix I .
This keeps the Mahalanobisdistance similar to the Euclidean distance, whichwould help to prevent overfitting the data.
To op-timize the matrix, we follow the information theo-retic metric learning approach described in (Daviset al, 2007).
We summarize the problem formula-tion advocated by this approach in this section andthe learning algorithm in the next section.To define the closeness between A and I , weuse a simple bijection (up to a scaling function)from the set of Mahalanobis distances to the setof equal mean multivariate Gaussian distributions.Without loss of generalization, let the equal meanbe ?.
Then given a Mahalanobis distance pa-rameterized by A, the corresponding Gaussian isp(x;A) =1Zexp(?12dA(x, ?))
where Z is thenormalizing factor.
This enables us to measurethe distance between two Mahalanobis distanceswith the Kullback-Leibler (KL) divergence of twoGaussians:KL(p(x; I)||p(x;A)) =?p(x, I) log(p(x; I)p(x;A))dx.Given pairs of similar points S and pairs of dis-similar points D, the optimization problem is:minAKL(p(x; I)||p(x;A))subject to dA(xi, xj) ?
u (i, j) ?
SdA(xi, xj) ?
l (i, j) ?
D3.2 Learning Algorithm(Davis and Dhillon, 2006) has shown that theKL divergence between two multivariate Gaus-sians can be expressed as the convex combinationof a Mahalanobis distance between mean vectorsand the LogDet divergence between the covariancematrices.
The LogDet divergence equalsDld(A,A0) = tr(AA?10)?
log det(AA?10)?
nfor n by n matrices A,A0.
If we assume the meansof the Gaussians to be the same, we haveKL(p(x;A0||p(x,A)) =12Dld(A,A0)The optimization problem can be restated asminA0Dld(A, I)s.t.
tr(A(xi?
xj)(xi?
xj)?)
?
u (i, j) ?
Str(A(xi?
xj)(xi?
xj)?)
?
l (i, j) ?
DWe then incorporate slack variables into the for-mulation to guarantee the existence of a feasiblesolution for A.
The optimization problem be-comes:minA0Dld(A, I) + ?Dld(diag(?
), diag(?0))s.t.
tr(A(xi?
xj)(xi?
xj)?)
?
?c(i,j)(i, j) ?
Str(A(xi?
xj)(xi?
xj)?)
?
?c(i,j)(i, j) ?
Dwhere c(i, j) is the index of the (i, j)-th constraintand ?
is a vector of slack variables whose compo-nents are initialized to u for similarity constraintsand l for dissimilarity constraints.
The tradeoffbetween satisfying the constraints and minimiz-ing Dld(A, I) is controlled by the parameter ?.To solve this optimization problem, the algorithmshown in Algorithm 3.1 repeatedly projects thecurrent solution onto a single constraint.This completes the summary of (Davis et al,2007).4 Experimental SettingsIn this section, we describe the experimental set-tings including the preprocessing of data and fea-tures, creation of the query word sets, and settingsof the cross validation.4.1 FeaturesWe used a dependency structure as the context forwords because it is the most widely used and oneof the best performing contextual information inthe past studies (Ruge, 1997; Lin, 1998).
As theextraction of an accurate and comprehensive de-pendency structure is in itself a complicated task,the sophisticated parser RASP Toolkit 2 (Briscoeet al, 2006) was utilized to extract this kind ofword relation.Let N(w, c) be the raw cooccurrence count ofword w and context c, the grammatical relation795Algorithm3.1: INFORMATION THEORETIC METRIC LEARNINGInput :X(d by n matrix), I(identity matrix)S(set of similar pairs),D(set of dissimilar pairs)?
(slack parameter), c(constraint index function)u, l(distance thresholds)Output :A(Mahalanobis matrix)A := I?ij:= 0?c(i,j):= u for (i, j) ?
S; otherwise, ?c(i,j):= lrepeatPick a constraint (i, j) ?
S or (i, j) ?
Dp := (xi?
xj)?A(xi?
xj)?
:= 1 if (i, j) ?
S,?1 otherwise.?
:= min(?ij,?2(1p???c(i,j)))?
:= ??/(1?
??
?c(i,j))?c(i,j):= ??c(i,j)/(?
+ ??
?c(i,j))?ij:= ?ij?
?A := A + ?A(xi?
xj)(xi?
xj)?Auntil convergencereturn (A)in which w occurs.
These raw counts were ob-tained from New York Times articles (July 1994)extracted from English Gigaword 1.
The sectionconsists of 7,593 documents and approx.
5 millionwords.
As discussed below, we limited the vocab-ulary to the nouns in the Longman Defining Vo-cabulary (LDV) 2.
The features were constructedby weighting them using pointwise mutual infor-mation: wgt(w, c) = PMI(w, c) = log P (w,c)P (w)P (c).Co-occurrence data constructed this way canyield more than 10,000 context types, renderingmetric learning impractical.
As the applicationsof feature selection reduce the performance of thebaseline metrics, we tested them in two differentsettings: with and without feature selection.
Tomitigate this problem, we applied a feature selec-tion technique to reduce the feature dimensional-ity.
We selected features using two approaches.The first approach is a simple frequency cutoff, ap-plied as a pre-processing to filter out words andcontexts with low frequency and to reduce com-putational cost.
Specifically, all words w suchthat?cN(w, c) < ?fand contexts c such that?wN(w, c) < ?f, with ?f= 5, are removed fromthe co-occurrence data.The second approach is feature selection by con-1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2003T052http://www.cs.utexas.edu/users/kbarker/working notes/ldoce-vocab.htmltext importance (Hagiwara et al, 2008).
First, thecontext importance score for each context type iscalculated, and then the least important contexttypes are eliminated, until a desired numbers ofthem remains.
To measure the context importancescore, we used the number of unique words thecontext co-occurs with: df(c) = |{w|N(w, c) >0}|.
We adopted this context selection criterionon the assumption that the contexts shared bymany words should be informative, and the syn-onym acquisition performance based on normaldistributional similarity calculation retains its orig-inal level of performance until up to almost 90%of context types are eliminated (Hagiwara et al,2008).
In our experiment, we selected featuresrather aggressively, finally using only 10% of theoriginal contexts.
These feature reduction oper-ations reduced the dimensionality to a figure assmall as 1,281, while keeping the performance lossat a minimum.4.2 Similarity and Distance FunctionsWe compared seven similarity/distance functionsin our experiments: cosine similarity, Euclideandistance, Manhattan distance, Jaccard coeffi-cient, vector-based Jaccard coefficient (Jaccardv),Jensen-Shannon Divergence (JS) and skew diver-gence (SD99).
We first define some notations.
LetC(w) be the set of context types that co-occur withword w, i.e., C(w) = {c|N(w, c) > 0}, and wibethe feature vector corresponding to word w, i.e.,wi= [wgt(wi, c1) ... wgt(wi, cM)]?.
The firstthree, the cosine, Euclidean and Manhattan dis-tance, are vector-based metrics.cosine similarityw1?w2||w1|| ?
||w2||Euclidean distance?
?c?C(w1)?C(w2)(wgt(w1, c)?
wgt(w2, c))2Manhattan distance?c?C(w1)?C(w2)|wgt(w1, c)?
wgt(w2, c)|Jaccard coefficient?c?C(w1)?C(w2)min(wgt(w1, c),wgt(w2, c))?c?C(w1)?C(w2)max(wgt(w1, c),wgt(w2, c)),796vector-based Jaccard coefficient (Jaccardv)wi?wj||wi|| + ||wj|| ?wi?wj.Jensen-Shannon divergence (JS)12{KL(p1||m) + KL(p2||m)}, m = p1+ p2.JS and SD99 are based on the KL divergence, sothe vectors must be normalized to form a probabil-ity distribution.
For notational convenience, we letpibe the probability distribution representation offeature vector wi, i.e., pi(c) = N(wi, c)/N(wi).While the KL divergence suffers from the so-calledzero-frequency problem, a symmetric version ofthe KL divergence called the Jensen-Shannon di-vergence naturally avoids it.skew divergence (SD99)KL(p1||?p2+ (1?
?
)p1).As proposed by (Lee, 2001), the skew diver-gence also avoids the zero-frequency problem bymixing the original distribution with the target dis-tribution.
Parameter ?
is set to 0.99.4.3 Query Word Set and Cross ValidationTo formalize the experiments, we must prepare aset of query words for which synonyms are knownin advance.
We chose the Longman DefiningVocabulary (LDV) as the candidate set of querywords.
For each word in the LDV, we consultedthree existing thesauri: Roget?s Thesaurus (Ro-get, 1995), Collins COBUILD Thesaurus (Collins,2002), and WordNet (Fellbaum, 1998).
Each LDVword was looked up as a noun to obtain the unionof synonyms.
After removing words marked ?id-iom?, ?informal?
or ?slang?
and phrases com-prised of two or more words, this union was usedas the reference set of query words.
LDV words forwhich no noun synonyms were found in any of thereference thesauri were omitted.
From the remain-ing 771 LDV words, there were 231 words that hadfive or more synonyms in the combined thesaurus.We selected these 231 words to be the query wordsand distributed them into five partitions so as toconduct five-fold cross validation.
Four partitionswere used in training, and the remaining partitionwas used in testing.
For each fold, we createdthe training set from four partitions as follows; foreach query word in the partitions, we randomly se-lected five synonymous words and added the pairsof query words and synonymous words to S, theset of similar pairs.
Similarly, five pairs of querywords and dissimilar words were randomly addedto D, the set of dissimilar pairs.
The training setfor each fold consisted of S and D. Since a learnertrained on an imbalanced dataset may not learnto discriminate enough between classes, we sam-pled dissimilar pairs to create an evenly distributedtraining dataset.To make the evaluation realistic, we used a dif-ferent method to create the test set: we paired eachquery word with each of the 771 remaining wordsto form the test set.
Thus, in each fold, the trainingset had an equal number of positive and negativepairs, while in the test set, negative pairs outnum-bered the positive pairs.
While this is not a typicalsetting for cross validation, it renders the evalua-tion more realistic since an automatic synonym ac-quisition system in operation must be able to picka few synonyms from a large number of dissimilarwords.The meta-parameters of the metric learningmodel were simply set u = 1, l = 2 and ?
= 1.Each training set consisted of 1,850 pairs, and thetest set consisted of 34,684 pairs.
Since we con-ducted five-fold cross validation, the reported per-formance in this paper is actually a summary overdifferent folds.4.4 Evaluation MeasuresWe used an evaluation program for KDD Cup2004 (Caruana et al, 2004) called Perf to measurethe effectiveness of the metrics in acquiring syn-onyms.
To use the program, we used the followingformula to convert each distance metric to a simi-larity metric.
s(xi, xj) = 1/(1 + exp(d(xi, xj))).Below, we summarize the three measures weused: Mean Average Precision, TOP1, and Aver-age Rank of Last Synonym.Mean Average Precision (APR)Perf implements a definition of average preci-sion sometimes called ?expected precision?.
Perfcalculates the precision at every recall where it isdefined.
For each of these recall values, Perf findsthe threshold that produces the maximum preci-sion, and takes the average over all of the recallvalues greater than 0.
Average precision is mea-sured on each query, and then the mean of eachquery?s average precision is used as the final met-ric.
A mean average precision of 1.0 indicates per-fect prediction.
The lowest possible mean average797precision is 0.0.Average Rank of Last Synonym (RKL)As in other evaluation measures, synonym can-didates are sorted by predicted similarity, and thismetric measures how far down the sorted cases wemust go to find the last true synonym.
A rank of1 indicates that the last synonym is placed in thetop position.
Given a query word, the highest ob-tainable rank is N if there are N synonyms in thecorpus.
The lower this measure is the better.
Aver-age ranks near 771 indicate poor performance.TOP1In each query, synonym candidates are sorted bypredicted similarity.
If the word that ranks at thetop (highest similarity to the query word) is a truesynonym of the query word, Perf scores a 1 forthat query, and 0 otherwise.
If there are ties, Perfscores 0 unless all of the tied cases are synonyms.TOP1 score ranges from 1.0 to 0.0.
To achieve 1.0,perfect TOP1 prediction, a similarity metric mustplace a true synonym at the top of the sorted listin every query.
In the next section, we report themean of each query?s TOP1.5 ResultsThe evaluations of the metrics are listed in Table1.
The figure on the left side of ?
represents theperformance with 1,281 features, and that on theright side with 12,812 features.
Of all the met-rics in Table 1, only the Mahalanobis L2 is trainedwith the previously presented metric learning al-gorithm.
Thus, the values for the MahalanobisL2 are produced by the five-fold cross validation,while the rest are given by the straight applicationof the metrics discussed in Section 4.2 to the samedataset.
Strictly speaking, this is not a fair com-parison, since we ought to compare a supervisedlearning with a supervised learning.
However, ourbaseline is not the simple Euclidean distance; itis the Jaccard coefficient and cosine similarity, ahandcrafted, best performing metric for synonymacquisition, with 10 times as many features.The computational resources required to obtainthe Mahalanobis L2 results were as follows: in thetraining phase, each fold of cross validation tookabout 80 iterations (less than one week) to con-verge on a Xeon 5160 3.0GHz.
The time requiredto use the learned distance was a few hours at most.At first, we were unable to perform competi-tively with the Euclidean distance.
As seen in Ta-ble 1, the TOP1 measure of the Euclidean distanceis only 1.732%.
This indicates that the likelihoodof finding the first item on the ranked list to be atrue synonym is 1.732%.
The vector-based Jac-card coefficient performs much better than the Eu-clidean distance, placing a true synonym at the topof the list 30.736% of the time.Table 2 shows the Top 10 Words for Query?branch?.
The results for the Euclidean distancerank ?hut?
and other dissimilar words highly.
Thisis because the norm of such vectors is small, and ina high dimensional space, the sparse vectors nearthe origin are relatively close to many other sparsevectors.
To overcome this problem, we normal-ized the input vectors by the L2 norm x?
= x/||x||This normalization enables the Euclidean distanceto perform very much like the cosine similarity,since the Euclidean distance between points on asphere acts like the angle between the vectors.
Sur-prisingly, normalization by L2 did not affect othermetrics all that much; while the performances ofsome metrics improved slightly, the L2 normaliza-tion lowered that of the Jaccardv metric.Once we learned the normalization trick, thelearned Mahalanobis distance consistently outper-formed all other metrics, including the ones with10 times more features, in all three evaluationmeasures, achieving an APR of 18.66%, RKL of545.09 and TOP1 of 45.455%.6 DiscussionExamining the learned Mahalanobis matrix re-vealed interesting features.
The matrix essentiallyshows the covariance between features.
While itwas not as heavily weighted as the diagonal ele-ments, we found that its positive non-diagonal el-ements were quite interesting.
They indicate thatsome of the useful features for finding synonymsare correlated and somewhat interchangeable.
Theexample includes a pair of features, (dobj begin*) and (dobj end *).
It was a pleasant surprise tosee that one implies the other.
Among the diag-onal elements of the matrix, one of the heaviestfeatures was being the direct object of ?by?.
Thisindicates that being the object of the preposition?by?
is a good indicator that two words are simi-lar.
A closer inspection of the NYT corpus showedthat this preposition overwhelmingly takes a per-son or organization as its object, indicating thatwords with this feature belong to the same classof a person or organization.
Similarly, the class798Metric APR RKL TOP1Cosine 0.1184 ?
0.1324 580.27 ?
579.00 0.2987 ?
0.3160Euclidean 0.0229 ?
0.0173 662.74 ?
695.71 0.0173 ?
0.0000Euclidean L2 0.1182 ?
0.1324 580.30 ?
578.99 0.2943 ?
0.3160Jaccard 0.1120 ?
0.1264 580.76 ?
579.51 0.2684 ?
0.2943Jaccard L2 0.1113 ?
0.1324 580.29 ?
570.88 0.2640 ?
0.2987Jaccardv 0.1189 ?
0.1318 580.50 ?
580.19 0.3073 ?
0.3030Jaccardv L2 0.1184 ?
0.1254 580.27 ?
570.00 0.2987 ?
0.3160JS 0.0199 ?
0.0170 681.97 ?
700.53 0.0129 ?
0.0000JS L2 0.0229 ?
0.0173 679.21 ?
699.00 0.0303 ?
0.0086Manhattan 0.0181 ?
0.0168 687.73 ?
701.47 0.0043 ?
0.0000Manhattan L2 0.0185 ?
0.0170 686.56 ?
701.11 0.0043 ?
0.0086SD99 0.0324 ?
0.1039 640.71 ?
588.16 0.0173 ?
0.2640SD99 L2 0.0334 ?
0.1117 633.32 ?
586.78 0.0216 ?
0.2900Mahalanobis L2 0.1866 545.09 0.4545Table 1: Evaluation of Various Metrics, as Number of Features Increase from 1,281 to 12,812Cosine Euclidean Euclidean L2 Jaccard Jaccardv Mahalanobis L21 (*) office hut (*) office (*) office (*) office (*) division2 area wild area border area group3 (*) division polish (*) division area (*) division (*) office4 border thirst border plant border line5 group hollow group (*) division group period6 organization shout organization mouth organization organization7 store fold store store store (*) department8 mouth dear mouth circle mouth charge9 plant hate plant stop plant world10 home wake home track home body(*) = a true synonymTable 2: Top 10 Words for Query ?branch?of words that ?to?
and ?within?, take as an objectswere clear from the corpus: ?to?
takes a personor place, ?within?
takes duration of time 3.
Otherheavy features includes being the object of ?write?or ?about?.
While not obvious, we postulate thathaving these words as a part of the context indi-cates that a word is an event of some type.7 ConclusionWe applied metric learning to automatic synonymacquisition for the first time, and our experimentsshowed that the learned metric significantly out-performs existing similarity metrics.
This outcomeindicates that while we must resort to feature se-lection to apply metric learning, the performancegain from the supervised learning is enough to off-set the disadvantage and justify its usage in someapplications.
This leads us to think that a com-bination of the learned metric with unsupervisedmetrics with even more features may produces thebest results.
We also discussed interesting featuresfound in the learned Mahalanobis matrix.
Since3Interestingly, we note that not all prepositions were asheavy: ?beyond?
and ?without?
were relatively light amongthe diagonal elements.
In the NYT corpus, the class of wordsthey take was not as clear as, for example, ?by?.metric learning is known to boost clustering per-formance in a semi-supervised clustering setting,we believe these automatically identified featureswould be helpful in assigning a target word to aword class.ReferencesT.
Briscoe, J. Carroll and R. Watson.
2006.
The Sec-ond Release of the RASP System.
Proc.
of the COL-ING/ACL 2006 Interactive Presentation Sessions,77?80.T.
Briscoe, J. Carroll, J. Graham and A. Copestake,2002.
Relational evaluation schemes.
Proc.
of theBeyond PARSEVAL Workshop at the Third Interna-tional Conference on Language Resources and Eval-uation, 4?8.A.
Budanitsky and G. Hirst.
2006.
Evaluat-ing WordNet-based measures of semantic distance.Computational Linguistics, 32(1):13?47.R.
Caruana, T. Jachims and L. Backstrom.
2004.
KDD-Cup 2004: results and analysis ACM SIGKDD Ex-plorations Newslatter, 6(2):95?108.C.
J. Croach and B. Yang.
1992.
Experiments in au-tomatic statistical thesaurus construction.
the 15th799Annual International ACM SIGIR Conference on Re-search and Development in Information Retrieval,77?88.J.
R. Curran and M. Moens.
2002.
Improvements inautomatic thesaurus extraction.
In Workshop on Un-supervised Lexical Acquisition.
Proc.
of the ACLSIGLEX, 231?238.J.
V. Davis and I. S. Dhillon.
2006.
Differential En-tropic Clustering of Multivariate Gaussians.
Ad-vances in Neural Information Processing Systems(NIPS).J.
V. Davis, B. Kulis, P. Jain, S. Sra and I. S. Dhillon.2007.
Information Theoretic Metric Learning.
Proc.of the International Conference on Machine Learn-ing (ICML).A.
Globerson and S. Roweis.
2005.
Metric Learning byCollapsing Classes.
Advances in Neural InformationProcessing Systems (NIPS).J.
Goldberger, S. Roweis, G. Hinton and R. Salakhut-dinov.
2004.
Neighbourhood Component Analysis.Advances in Neural Information Processing Systems(NIPS).G.
Grefenstette.
1994.
Explorations in Automatic The-suarus Discovery.
Kluwer Academic Publisher.M.
Hagiwara, Y. Ogawa, and K. Toyama.
2008.
Con-text Feature Selection for Distributional Similarity.Proc.
of IJCNLP-08, 553?560.Z.
Harris.
1985.
Distributional Structure.
JerroldJ.
Katz (ed.)
The Philosophy of Linguistics.
OxfordUniversity Press.
26?47.T.
Hastie and R. Tibshirani.
1996.
Discriminant adap-tive nearest neighbor classification.
Pattern Analysisand Machine Intelligence, 18, 607?616.D.
Hindle.
1990.
Noun classification from predicate-argument structures.
Proc.
of the ACL, 268?275.J.
J. Jiang and D. W. Conrath.
1997.
Semantic sim-ilarity based on corpus statistics and lexical taxon-omy.
Proceedings of International Conference onResearch on Computational Linguistics (ROCLINGX), Taiwan.Y.
Jing and B. Croft.
1994.
An Association The-saurus for Information Retrieval.
Proc.
of Recherched?Informations Assiste?e par Ordinateur (RIAO),146?160.D.
Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
Proc.
of COLING/ACL 1998, 786?774.L.
Lee.
1999.
Measures of distributional similarity.Proc.
of the ACL, 23?32L.
Lee.
2001.
On the Effectiveness of the Skew Diver-gence for Statistical Language Analysis.
ArtificialIntelligence and Statistics 2001, 65?72.S.
Mohammad and G. Hirst.
2006.
Distributional mea-sures of concept-distance: A task-oriented evalua-tion.
Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP),Sydney, Australia.P.
Resnik.
1995.
Using information content to evaluatesemantic similarity.
Proceedings of the 14th Inter-national Joint Conference on Artificial Intelligence(IJCAI-95), 448?453, Montreal, Canada.G.
Ruge.
1997.
Automatic detection of thesaurus re-lations for information retrieval applications.
Foun-dations of Computer Science: Potential - Theory -Cognition, LNCS, Volume 1337, 499?506, SpringerVerlag, Berlin, Germany.S.
Shalev-Shwartz, Y.
Singer and A. Y. Ng.
2004.
On-line and Batch Learning of Pseudo-Metrics.
Proc.
ofthe International Conference on Machine Learning(ICML).M.
Schutz and T. Joachims.
2003.
Learning a Dis-tance Metric from Relative Comparisons.
Advancesin Neural Information Processing Systems (NIPS)..J. Weeds, D. Weir and D. McCarthy.
2004.
Character-ising Measures of Lexical Distributional Similarity.Proc.
of COLING 2004, 1015?1021.K.
Q. Weinberger, J. Blitzer and L. K. Saul.
2005.Distance Metric Learning for Large Margin NearestNeighbor Classification.
Advances in Neural Infor-mation Processing Systems (NIPS).E.
P. Xing, A. Y. Ng, M. Jordan and S. Russell 2002.Distance metric learning with application to cluster-ing with sideinformation.
Advances in Neural Infor-mation Processing Systems (NIPS).Y.
Yang and J. O. Pedersen.
1997.
A ComparativeStudy on Feature Selection in Text Categorization.Proc.
of the International Conference on MachineLearning (ICML), 412?420.800
