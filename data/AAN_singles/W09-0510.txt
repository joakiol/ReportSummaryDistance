Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 74?81,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsIncrementality, Speaker-Hearer Switchingand the Disambiguation ChallengeRuth Kempson, Eleni GregoromichelakiKing?s College London{ruth.kempson, eleni.gregor}@kcl.ac.ukYo SatoUniversity of Hertfordshirey.sato@herts.ac.ukAbstractTaking so-called split utterances as ourpoint of departure, we argue that a newperspective on the major challenge of dis-ambiguation becomes available, given aframework in which both parsing and gen-eration incrementally involve the samemechanisms for constructing trees reflect-ing interpretation (Dynamic Syntax: (Cannet al, 2005; Kempson et al, 2001)).
Withall dependencies, syntactic, semantic andpragmatic, defined in terms of incrementalprogressive tree growth, the phenomenonof speaker/hearer role-switch emerges asan immediate consequence, with the po-tential for clarification, acknowledgement,correction, all available incrementally atany sub-sentential point in the interpreta-tion process.
Accordingly, at all interme-diate points where interpretation of an ut-terance subpart is not fully determined forthe hearer in context, uncertainty can beresolved immediately by suitable clarifica-tion/correction/repair/extension as an ex-change between interlocutors.
The resultis a major check on the combinatorial ex-plosion of alternative structures and inter-pretations at each choice point, and the ba-sis for a model of how interpretation incontext can be established without eitherparty having to make assumptions aboutwhat information they and their interlocu-tor share in resolving ambiguities.1 IntroductionA major characteristic of dialogue is effortlessswitching between the roles of hearer and speaker.Dialogue participants seamlessly shift betweenparsing and generation bi-directionally across anysyntactic dependency, without any indication ofthere being any problem associated with suchshifts (examples from Howes et al (in prep)):(1) Conversation from A and B, to C:A: We?re goingB: to Bristol, where Jo lives.
(2) A smelling smoke comes into the kitchen:A: Have you burntB the buns.
Very thoroughly.A: But did you burnB: Myself?
No.
Luckily.
(3) A: Are you left orB: Right-handed.Furthermore, in no case is there any guarantee thatthe way the shared utterance evolves is what ei-ther party had in mind to say at the outset, indeedobviously not, as otherwise the exchange risks be-ing otiose.
This flexibility provides a vehicle forongoing clarification, acknowledgement, correc-tions, repairs etc.
((6)-(7) from (Mills, 2007)):(4) A: I?m seeing Bill.B: The builder?A: Yeah, who lives with Monica.
(5) A: I saw DonB: John?A: Don, the guy from Bristol.
(6) A: I?m on the second switchB: Switch?A: Yeah, the grey thing(7) A: I?m on the second row third on the left.B: What?A: on the leftThe fragmental utterances that constitute such in-cremental, joint contributions have been analysedas falling into discrete structural types accordingto their function, in all cases resolved to propo-sitional types by combining with appropriate ab-stractions from context (Ferna?ndez, 2006; Purver,2004).
However, any such fragment and theirresolution may occur as mid-turn interruptions,well before any emergent propositional structureis completed:74(8) A: They X-rayed me, and took a urinesample, took a blood sample.Er, the doctor ...B: Chorlton?A: Chorlton, mhm, he examined me, erm,he, he said now they were on about a slight[shadow] on my heart.
[BNC: KPY1005-1008]The advantage of such ongoing, incremental, jointconversational contributions is the effective nar-rowing down of the search space out of whichhearers select (a) interpretations to yield somecommonly shared understanding, e.g.
choiceof referents for NPs, and, (b) restricted struc-tural frames which allow (grammatical) context-dependent fragment resolution, i.e.
exact speci-fications of what contextually available structuresresolve elliptical elements.
This seems to pro-vide an answer as to why such fragments are sofrequent and undemanding elements of dialogue,forming the basis for the observed coordinationbetween participants: successive resolution at sub-sentential stages yields a progressively jointly es-tablished common ground, that can thereafter betaken as a secure, albeit individual, basis for filter-ing out interpretations inconsistent with such con-firmed knowledge-base (see (Poesio and Rieser,2008; Ginzburg, forthcmg) etc).
All such dialoguephenomena, illustrated in (1)-(8), jointly and in-crementally achieved, we address with the generalterm split utterances.However, such exchanges are hard to modelwithin orthodox grammatical frameworks, giventhat usually it is the sentence/proposition that istaken as the unit of syntactic/semantic analysis;and they have not been addressed in detail withinsuch frameworks, being set aside as deviant, giventhat such grammars in principle do not specifya concept of grammaticality that relies on a de-scription of the context of occurrence of a certainstructure (however, see Poesio and Rieser (2008)for German completions).
In so far as fragmentutterances are now being addressed, the pressureof compatibility with sentence-based grammarsis at least partly responsible for analyses of e.g.clarificatory-request fragments as sentential in na-ture (Ginzburg and Cooper, 2004).
But such anal-yses fail to provide a basis for incrementally re-solved clarification requests such as the interrup-tion in (8) where no sentential basis is yet avail-able over which to define the required abstractionof contextually provided content.In the psycholinguistic literature, on the otherhand, there is broad agreement that incrementalityis a crucial feature of parsing with semantic inter-pretation taking place as early as possible at thesub-sentential level (see e.g.
(Sturt and Crocker,1996)).
Nonetheless, this does not, in and of it-self, provide a basis for explaining the ease andfrequency of split utterances in dialogue: the inter-active coordination between the parsing and pro-duction activities, one feeding the other, remainsas a challenge.In NLP modelling, parsing and generation algo-rithms are generally dissociated from the descrip-tion of linguistic entities and rules, i.e.
the gram-mar formalisms, which are considered either to beindependent of processing (?process-neutral?)
orto require some additional generation- or parsing-specific mechanisms to be incorporated.
However,this point of view creates obstacles for a success-ful account of data as in (1)-(8).
Modelling thosewould require that, for the current speaker, the ini-tiated generation mechanism has to be displacedmid-production without the propositional genera-tion task having been completed.
Then the parsingmechanism, despite being independent of, indeedin some sense the reverse of, the generation com-ponent, has to take over mid-sentence as though, insome sense there had been parsing involved up tothe point of switchover.
Conversely, for the hearer-turned-speaker, it would be necessary to somehowconnect their parse with what they are now aboutto produce in order to compose the meaning of thecombined sentence.
Moreover, in both directionsof switch, as (2) shows, this is not a phenomenonof both interlocutors intending to say the samesentence: as (3) shows, even the function of theutterance (e.g.
question/answer) can alter in theswitch of roles and such fragments can play tworoles (e.g.
question/completion) at the same time(e.g.
(2)).
Hence the grammatical integration ofsuch joint contributions must be flexible enoughto allow such switches which means that suchfragment resolutions must occur before the com-putation of intentions at the pragmatic level.
Sothe ability of language users to successfully pro-cess such utterances, even at sub-sentential levels,means that modelling their grammar requires fine-grained grammaticality definitions able to char-acterise and integrate sub-sentential fragments inturns jointly constructed by speaker and hearer.75This can be achieved straightforwardly if fea-tures like incrementality and context-dependentprocessing are built into the grammar architectureitself.
The modelling of split utterances then be-comes straightforward as each successive process-ing step exploits solely the grammatical apparatusto succeed or fail.
Such a view notably does not in-voke high-level decisions about speaker/hearer in-tentions as part of the mechanism itself.
That thisis the right view to take is enhanced by the fact thatas all of (1)-(8) show, neither party in such role-exchanges can definitively know in advance whatwill emerge as the eventual joint proposition.
If,to the contrary, generation decisions are modelledas involving intentions for whole utterances, therewill be no the basis for modelling how such in-complete strings can be integrated in suitable con-texts, with joint propositional structures emergingbefore such joint intentions have been established.An additional puzzle, equally related to boththe challenges of disambiguation and the statusof modelling speaker?s intentions as part of themechanism whereby utterance interpretation takesplace, is the common occurrence of hearers NOTbeing constrained by any check on consistencywith speaker intentions in determining a putativeinterpretation, failing to make use of well estab-lished shared knowledge:(9) A: I?m going to cook salmon, as John?scoming.B: What?
John?s a vegetarian.A: Not my brother.
John Smith.
(10) A: Why don?t you have cheese and noodles?B: Beef?
You KNOW I?m a vegetarianSuch examples are problematic for any accountthat proposes that interpretation mechanisms forutterance understanding solely depend on selec-tion of interpretations which either the speakercould have intended (Sperber and Wilson, 1986;Carston, 2002), or ones which are compati-ble with checking consistency with the com-mon ground/plans established between speakerand hearer (Poesio and Rieser, 2008; Ginzburg,forthcmg), mutual knowledge, etc.
(Clark, 1996;Brennan and Clark, 1996).
To the contrary, thedata in (9)-(10) tend to show that the full rangeof interpretations computable by the grammar hasin principle to be available at all choice points forconstrual, without any filter based on plausibilitymeasures, thus leaving the disambiguation chal-lenge still unresolved.In this paper we show how with speaker andhearer in principle using the same mechanisms forconstrual, equally incrementally applied, such dis-ambiguation issues can be resolved in a timelymanner which in turn reduces the multiplicationof structural/interpretive options.
As we shall see,what connects our diverse examples, and indeedunderpins the smooth shift in the joint endeav-our of conversation, lies in incremental, context-dependent processing and bidirectionality, essen-tial ingredients of the Dynamic Syntax (Cann et al,2005) dialogue model.2 Incrementality in Dynamic SyntaxDynamic Syntax (DS) is a procedure-orientedframework, involving incremental processing, i.e.strictly sequential, word-by-word interpretation oflinguistic strings.
The notion of incrementalityin DS is closely related to another of its features,the goal-directedness of BOTH parsing and gener-ation.
At each stage of processing, structural pre-dictions are triggered that could fulfill the goalscompatible with the input, in an underspecifiedmanner.
For example, when a proper name likeBob is encountered sentence-initially in English,a semantic predicate node is predicted to follow(?Ty(e ?
t)), amongst other possibilities.By way of introducing the reader to the DSdevices, let us look at some formal details withan example, Bob saw Mary.
The ?complete?
se-mantic representation tree resulting after the com-plete processing of this sentence is shown in Fig-ure 2 below.
A DS tree is formally encoded withthe tree logic LOFT (Blackburn and Meyer-Viol(1994)), we omit these details here) and is gen-erally binary configurational, with annotations atevery node.
Important annotations here, see the(simplified) tree below, are those which representsemantic formulae along with their type informa-tion (e.g.
?Ty(x)?)
based on a combination of theepsilon and lambda calculi1.Such complete trees are constructed, startingfrom a radically underspecified annotation, the ax-iom, the leftmost minimal tree in Figure 2, andgoing through monotonic updates of partial, orstructurally underspecified, trees.
The outline ofthis process is illustrated schematically in Figure2.
Crucial for expressing the goal-directednessare requirements, i.e.
unrealised but expected1These are the adopted semantic representation languagesin DS but the computational formalism is compatible withother semantic-representation formats760?Ty(t),?7?1?Ty(t)?Ty(e),?
?Ty(e?
t)7?2?Ty(t)Ty(e),Bob?
?Ty(e?
t),?7?3?Ty(t)Ty(e),Bob?
?Ty(e?
t)?Ty(e),?Ty(e?
(e?
t)),See?7?0(gen)/4Ty(t),?See?(Mary?)(Bob?)Ty(e),Bob?Ty(e?
t),See?(Mary?)Ty(e),Mary?Ty(e?
(e?
t)),See?Figure 2: Monotonic tree growth in DSTy(t),See?(Mary?)(Bob?)Ty(e),Bob?Ty(e?
t),See?(Mary?)Ty(e),Mary?Ty(e?
(e?
t)),See?Figure 1: A DS complete treenode/tree specifications, indicated by ???
in frontof annotations.
The axiom says that a proposition(of type t, Ty(t)) is expected to be constructed.Furthermore, the pointer, notated with ???
indi-cates the ?current?
node in processing, namely theone to be processed next, and governs word order.Updates are carried out by means of applyingactions, which are divided into two types.
Compu-tational actions govern general tree-constructionalprocesses, such as moving the pointer, introducingand updating nodes, as well as compiling interpre-tation for all non-terminal nodes in the tree.
In ourexample, the update of (1) to (2) is executed viacomputational actions specific to English, expand-ing the axiom to the subject and predicate nodes,requiring the former to be processed next by theposition of the ?.
Construction of only weaklyspecified tree relations (unfixed nodes) can also beinduced, characterised only as dominance by somecurrent node, with subsequent update required.
In-dividual lexical items also provide procedures forbuilding structure in the form of lexical actions,inducing both nodes and annotations.
For exam-ple, in the update from (2) to (3), the set of lexicalactions for the word see is applied, yielding thepredicate subtree and its annotations.
Thus partialtrees grow incrementally, driven by procedures as-sociated with particular words as they are encoun-tered.Requirements embody structural predictions asmentioned earlier.
Thus unlike the conven-tional bottom-up parsing,2 the DS model takesthe parser/generator to entertain some predictedgoal(s) to be reached eventually at any stage ofprocessing, and this is precisely what makes theformalism incremental.
This is the characteri-sation of incrementality adopted by some psy-cholinguists under the appellation of connected-ness (Sturt and Crocker, 1996; Costa et al, 2002):an encountered word always gets ?connected?
to alarger, predicted, tree.Individual DS trees consist of predicates andtheir arguments.
Complex structures are obtainedvia a general tree-adjunction operation licensingthe construction of so-called LINKed trees, pairsof trees where sharing of information occurs.
Inits simplest form this mechanism is the same onewhich provides the potential for compiling in-2The examples in (1)-(8) also suggest the implausibilityof purely bottom-up or head-driven parsing being adopted di-rectly, because such strategies involve waiting until all thedaughters are gathered before moving on to their projection.In fact, the parsing strategy adopted by DS is somewhat sim-ilar to mixed parsing strategies like the left-corner or Earleyalgorithm to a degree.
These parsing strategic issues are morefully discussed in Sato (forthcmg).77A consultant, a friend of Jo?s, is retiring: Ty(t), Retire?
((?, x, Consultant?
(x) ?
Friend?(Jo?
)(x)))Ty(e), (?, x, Consultant?
(x) ?
Friend?(Jo?
)(x)) Ty(e?
t), Retire?Ty(e), (?, x, Friend?(Jo?
)(x))Ty(cn), (x, Friend?(Jo?
)(x))x Friend?(Jo?)Jo?
Friend?Ty(cn?
e), ?P.
?, PFigure 3: Apposition in DSterpretation for apposition constructions as canbe seen in Figure (3)3.
The assumption in theconstruction of such LINKed structures is that atany arbitrary stage of development, some type-complete subtree may constitute the context forthe subsequent parsing of the following string asan adjunct structure candidate for incorporationinto the primary tree, hence the obligatory sharingof information in the resulting semantic represen-tation.More generally, context in DS is defined as thestorage of parse states, i.e., the storing of par-tial tree, word sequence parsed to date, plus theactions used in building up the partial tree.
For-mally, a parse state P is defined as a set of triples?T, W, A?, where: T is a (possibly partial) tree;W is the associated sequence of words; A is theassociated sequence of lexical and computationalactions.
At any point in the parsing process, thecontext C for a particular partial tree T in the setP can be taken to consist of: a set of triples P ?
={.
.
.
, ?Ti, Wi, Ai?, .
.
.}
resulting from the previ-ous sentence(s); and the triple ?T, W, A?
itself,the subtree currently being processed.
Anaphoraand ellipsis construal generally involve re-use offormulae, structures, and actions from the set C.Grammaticality of a string of words is then de-fined relative to its context C, a string being well-formed iff there is a mapping from string ontocompleted tree with no outstanding requirementsgiven the monotonic processing of that string rela-tive to context.
All fragments illustrated above areprocessed by means of either extending the current3Epsilon terms, like ?, x, Consultant?
(x), stand for wit-nesses of existentially quantified formulae in the epsilon cal-culus and represent the semantic content of indefinites in DS.Defined relative to the equivalence ?
(?, x, ?
(x)) = ?x?
(x),their defining property is their reflection of their contain-ing environment, and accordingly they are particularly well-suited to expressing the growth of terms secured by such ap-positional devices.tree, or constructing LINKed structures and trans-fer of information among them so that one treeprovides the context for another, and are licensedas wellformed relative to that context.
In particu-lar, fragments like the doctor in (8) are licensed bythe grammar because they occur at a stage in pro-cessing at which the context contains an appropri-ate structure within which they can be integrated.The definite NP is taken as an anaphoric device,relying on a substitution process from the contextof the partial tree to which the node it decorates isLINKed to achieve the appropriate construal andtree-update:(11) The?parse?
tree licensing production of thedoctor: LINK adjunction?Ty(t)Chorlton?
?Ty(e?
t)(Doctor?(Chorlton?
)),?3 Bidirectionality in DSCrucially, for our current concern, this architec-ture allows a dialogue model in which generationand parsing function in parallel, following exactlythe same procedure in the same order.
See Fig (2)for a (simplified) display of the transitions manip-ulated by a parse of Bob saw Mary, as each wordis processed and integrated to reach the completetree.
Generation of this utterance from a completetree follows precisely the same actions and treesfrom left to right, although the complete tree isavailable from the start (this is why the completetree is marked ?0?
for generation): in this case theeventual message is known by the speaker, thoughof course not by the hearer.
What generation in-volves in addition to the parse steps is reference78to this complete tree to check whether each pu-tative step is consistent with it in order not to bedeviated from the legitimate course of action, thatis, a subsumption check.
The trees (1-3) are li-censed because each of these subsumes (4).
Eachtime then the generator applies a lexical action, itis licensed to produce the word that carries that ac-tion under successful subsumption check: at Step(3), for example, the generator processes the lex-ical action which results in the annotation ?See?
?,and upon success and subsumption of (4) licenseto generate the word see at that point ensues.For split utterances, two more assumptions arepertinent.
On the one hand, speakers may haveinitially only a partial structure to convey: this isunproblematic, as all that is required by the for-malism is monotonicity of tree growth, the checkbeing one of subsumption which can be carriedout on partial trees as well.
On the other hand,the utterance plan may change, even within a sin-gle speaker.
Extensions and clarifications in DScan be straightforwardly generated by appendinga LINKed structure projecting the added materialto be conveyed (preserving the monotonicity con-straint)4.
(12) I?m going home, with my brother, maybewith his wife.Such a model under which the speaker andhearer essentially follow the same sets of actions,updating incrementally their semantic representa-tions, allows the hearer to ?mirror?
the same seriesof partial trees, albeit not knowing in advance whatthe content of the unspecified nodes will be.4 Parser/generator implementationThe process-integral nature of DS emphasisedthus far lends itself to the straightforward imple-mentation of a parsing/generating system, sincethe ?actions?
defined in the grammar directly pro-vide a major part of its implementation.
By now itshould also be clear that the DS formalism is fullybi-directional, not only in the sense that the samegrammar can be used for generation and parsing,but also because the two sets of activities, conven-tionally treated as ?reverse?
processes, are mod-elled to run in parallel.
Therefore, not only can thesame sets of actions be used for both processes,4Revisions however will involve shifting to a previouspartial tree as the newly selected context: I?m going home,to my brother, sorry my mother.but also a large part of the parsing and generationalgorithms can be shared.This design architecture and a prototype im-plementation are outlined in (Purver and Otsuka,2003), and the effort is under way to scale up theDS parsing/generating system incorporating theresults in (Gargett et al, 2008; Gregoromichelakiet al, to appear).5 The parser starts from the axiom(step 0 in Fig.2), which ?predicts?
a proposition tobe built, and follows the applicable actions, lexi-cal or general, to develop a complete tree.
Now,as has been described in this paper, the genera-tor follows exactly the same steps: the axiom isdeveloped through successive updates into a com-plete tree.
The only material difference from ?or rather in addition to?
parsing is the completetree (Step 0(gen)/4), given from the very start ofthe generation task, which is then referred to ateach tree update for subsumption check.
The mainpoint is that despite the obvious difference in theirpurposes ?outputting a string from a meaning ver-sus outputting a meaning from a string?
parsingand generation indeed share the direction of pro-cessing in DS.
Moreover, as no intervening levelof syntactic structure over the string is ever com-puted, the parsing/generation tasks are more effi-ciently incremental in that semantic interpretationis directly imposed at each stage of lexical integra-tion, irrespective of whether some given partiallydeveloped constituent is complete.To clarify, see the pseudocode in the Prologformat below, which is a close analogue of theimplemented function that both does parsing andgeneration of a word (context manipulation isignored here for reasons of space).
The plusand minus signs attached to a variable indicate itmust/needn?t be instantiated, respectively.
In ef-fect, the former corresponds to the input, the latterto the output.
(13) parse gen word(+OldMeaning,?Word,?NewMeaning):-apply lexical actions(+OldMeaning, ?Word,+LexActions, ?IntermediateMeaning ),apply computational actions(+IntermediateMeaning, +CompActions,?NewMeaning )OldMeaning is an obligatory input item, whichcorresponds to the semantic structure con-structed so far (which might be just structuraltree information initially before any lexical5The preliminary results are described in (Sato,forthcmg).79input has been processed thus advocating astrong predictive element even compared to(Sturt and Crocker, 1996).
Now notice thatthe other two variables ?corresponding to theword and the new (post-word) meaning?
mayfunction either as the input or output.
Moreprecisely, this is intended to be a shorthandfor either (+OldMeaning,+Word,?NewMeaning)i.e.
Word as input and NewMeaning as out-put, or (+OldMeaning,?Word,+NewMeaning), i.e.NewMeaning as input and Word as output, to repeat,the former corresponding to parsing and the latterto generation.In either case, the same set of two sub-procedures, the two kinds of actions described in(13), are applied sequentially to process the inputto produce the output.
These procedures corre-spond to an incremental ?update?
from one par-tial tree to another, through a word.
The wholefunction is then recursively applied to exhaust thewords in the string, from left to right, either inparsing or generation.
Thus there is no differ-ence between the two in the order of proceduresto be applied, or words to be processed.
Thus it isa mere switch of input/output that shifts betweenparsing and generation.64.1 Split utterances in Dynamic SyntaxSplit utterances follow as an immediate conse-quence of these assumptions.
For the dialogues in(1)-(8), therefore, while A reaches a partial tree ofwhat she has uttered through successive updatesas described above, B as the hearer, will followthe same updates to reach the same representationof what he has heard.
This provides him with theability at any stage to become the speaker, inter-rupting to continue A?s utterance, repair, ask forclarification, reformulate, or provide a correction,as and when necessary7.
According to our modelof dialogue, repeating or extending a constituentof A?s utterance by B is licensed only if B, thehearer turned now speaker, entertains a message6Thus the parsing procedure is dictated by the grammar toa large extent, but importantly, not completely.
More specif-ically, the grammar formalism specifies the state paths them-selves, but not how the paths should be searched.
The DS ac-tions are defined in conditional terms, i.e.
what to do as andwhen a certain condition holds.
If a number of actions can beapplied at some point during a parse, i.e.
locally ambiguityis encountered, then it is up to a particular implementationof the parser to decide which should be traversed first.
Thecurrent implementation includes suggestions of search strate-gies.7The account extends the implementation reported in(Purver et al, 2006)to be conveyed that matches or extends the parsetree of what he has heard in a monotonic fashion.In DS, this message is a semantic representationin tree format and its presence allows B to only ut-ter the relevant subpart of A?s intended utterance.Indeed, this update is what B is seeking to clarify,extend or acknowledge.
In DS, B can reuse thealready constructed (partial) parse tree in his con-text, rather than having to rebuild an entire propo-sitional tree or subtree.The fact that the parsing formalism integratesa strong element of predictivity, i.e.
the parseris always one step ahead from the lexical in-put, allows a straightforward switch from pars-ing to generation thus resulting in an explana-tion of the facility with which split utterances oc-cur (even without explicit reasoning processes).Moreover, on the one hand, because of incremen-tality, the issue of interpretation-selection can befaced at any point in the process, with correc-tions/acknowledgements etc.
able to be providedat any point; this results in the potential exponen-tial explosion of interpretations being kept firmlyin check.
And, structurally, such fragments canbe integrated in the current partial tree represen-tation only (given the position of the pointer) sothere is no structural ambiguity multiplication.
Onthe other hand, for any one of these intermedi-ate check points, bidirectionality entails that con-sistency checking remains internal to the individ-ual interlocutors?
system, the fact of their mir-roring each other resulting at their being at thesame point of tree growth.
This is sufficient to en-sure that any inconsistency with their own parserecognised by one party as grounds for correc-tion/repair can be processed AS a correction/repairby the other party without requiring any additionalmetarepresentation of their interlocutors?
informa-tion state (at least for these purposes).
This allowsthe possibility of building up apparently complexassumptions of shared content, without any neces-sity of constructing hypotheses of what is enter-tained by the other, since all context-based selec-tions are based on the context of the interlocutorthemselves.
This, in its turn, opens up the possi-bility of hearers constructing interpretations basedon selections made that transparently violate whatis knowledge shared by both parties, for no pre-sumption of common ground is essential as inputto the interpretation process (see, e.g.
(9)-(10)).805 ConclusionIt is notable that, from this perspective, no pre-sumption of common ground or hypothesis as towhat the speaker could have intended is necessaryto determine how the hearer selects interpretation.All that is required is a concept of system-internalconsistency checking, the potential for clarifica-tion in cases of uncertainty, and reliance at suchpoints on disambiguation/correction/repair by theother party.
The advantage of such a proposal, wesuggest, is the provision of a fully mechanistic ac-count for disambiguation (cf.
(Pickering and Gar-rod, 2004)).
The consequence of such an analysisis that language use is essentially interactive (seealso (Ginzburg, forthcmg; Clark, 1996)): the onlyconstraint as to whether some hypothesised in-terpretation assigned by either party is confirmedturns on whether it is acknowledged or corrected(see also (Healey, 2008)).AcknowledgementsThis work was supported by grants ESRC RES-062-23-0962,the EU ITALK project (FP7-214668) and Leverhulme F07-04OU.
We are grateful for comments to: Robin Cooper, AlexDavies, Arash Eshghi, Jonathan Ginzburg, Pat Healey, GregJames Mills.
Normal disclaimers apply.ReferencesPatrick Blackburn and Wilfried Meyer-Viol.
1994.Linguistics, logic and finite trees.
Bulletin of theIGPL, 2:3?31.Susan E. Brennan and Herbert H. Clark.
1996.
Con-ceptual pacts and lexical choice in conversation.Journal of Experimental Psychology: Learning,Memory and Cognition, 22:482?1493.Ronnie Cann, Ruth Kempson, and Lutz Marten.
2005.The Dynamics of Language.
Elsevier, Oxford.Robyn Carston.
2002.
Thoughts and Utterances: ThePragmatics of Explicit Communication.
Blackwell.Herbert H. Clark.
1996.
Using Language.
CambridgeUniversity Press.Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo,Patrick Sturt, and Giovanni Soda.
2002.
Enhanc-ing first-pass attachment prediction.
In ECAI 2002:508-512.Raquel Ferna?ndez.
2006.
Non-Sentential Utterancesin Dialogue: Classification, Resolution and Use.Ph.D.
thesis, King?s College London, University ofLondon.Andrew Gargett, Eleni Gregoromichelaki, ChrisHowes, and Yo Sato.
2008.
Dialogue-grammar cor-respondence in dynamic syntax.
In Proceedings ofthe 12th SEMDIAL (LONDIAL).Jonathan Ginzburg and Robin Cooper.
2004.
Clarifi-cation, ellipsis, and the nature of contextual updatesin dialogue.
Linguistics and Philosophy, 27(3):297?365.Jonathan Ginzburg.
forthcmg.
Semantics for Conver-sation.
CSLI.Eleni Gregoromichelaki, Yo Sato, Ruth Kempson, An-drew Gargett, and Christine Howes.
to appear.
Dia-logue modelling and the remit of core grammar.
InProceedings of IWCS 2009.Patrick Healey.
2008.
Interactive misalignment: Therole of repair in the development of group sub-languages.
In R. Cooper and R. Kempson, editors,Language in Flux.
College Publications.Christine Howes, Patrick G. T. Healey, and GregoryMills.
in prep.
a: An experimental investigationinto.
.
.
b: .
.
.
split utterances.Ruth Kempson, Wilfried Meyer-Viol, and Dov Gabbay.2001.
Dynamic Syntax: The Flow of Language Un-derstanding.
Blackwell.Gregory J.
Mills.
2007.
Semantic co-ordination in di-alogue: the role of direct interaction.
Ph.D. thesis,Queen Mary University of London.Martin Pickering and Simon Garrod.
2004.
Towarda mechanistic psychology of dialogue.
Behavioraland Brain Sciences.Massimo Poesio and Hannes Rieser.
2008.
Comple-tions, coordination, and alignment in dialogue.
Ms.Matthew Purver and Masayuki Otsuka.
2003.
Incre-mental generation by incremental parsing: Tacticalgeneration in Dynamic Syntax.
In Proceedings ofthe 9th European Workshop in Natural LanguageGeneration (ENLG), pages 79?86.Matthew Purver, Ronnie Cann, and Ruth Kempson.2006.
Grammars as parsers: Meeting the dialoguechallenge.
Research on Language and Computa-tion, 4(2-3):289?326.Matthew Purver.
2004.
The Theory and Use of Clari-fication Requests in Dialogue.
Ph.D. thesis, Univer-sity of London, forthcoming.Yo Sato.
forthcmg.
Local ambiguity, search strate-gies and parsing in dynamic syntax.
In Eleni Gre-goromichelaki and Ruth Kempson, editors, DynamicSyntax: Collected Papers.
CSLI.Dan Sperber and Deirdre Wilson.
1986.
Relevance:Communication and Cognition.
Blackwell.Patrick Sturt and Matthew Crocker.
1996.
Monotonicsyntactic processing: a cross-linguistic study of at-tachment and reanalysis.
Language and CognitiveProcesses, 11:448?494.81
