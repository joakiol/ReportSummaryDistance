Arabic Dialect IdentificationOmar F. Zaidan?Microsoft ResearchChris Callison-Burch?
?University of PennsylvaniaThe written form of the Arabic language, Modern Standard Arabic (MSA), differs in a non-trivial manner from the various spoken regional dialects of Arabic?the true ?native languages?of Arabic speakers.
Those dialects, in turn, differ quite a bit from each other.
However, due toMSA?s prevalence in written form, almost all Arabic data sets have predominantly MSA content.In this article, we describe the creation of a novel Arabic resource with dialect annotations.
Wehave created a large monolingual data set rich in dialectal Arabic content called the Arabic On-line Commentary Data set (Zaidan and Callison-Burch 2011).
We describe our annotationeffort to identify the dialect level (and dialect itself) in each of more than 100,000 sentences fromthe data set by crowdsourcing the annotation task, and delve into interesting annotator behaviors(like over-identification of one?s own dialect).
Using this new annotated data set, we considerthe task of Arabic dialect identification: Given the word sequence forming an Arabic sentence,determine the variety of Arabic in which it is written.
We use the data to train and evaluateautomatic classifiers for dialect identification, and establish that classifiers using dialectal datasignificantly and dramatically outperform baselines that use MSA-only data, achieving near-human classification accuracy.
Finally, we apply our classifiers to discover dialectical data froma large Web crawl consisting of 3.5 million pages mined from on-line Arabic newspapers.1.
IntroductionThe Arabic language is a loose term that refers to the many existing varieties of Arabic.Those varieties include one ?written?
form, Modern Standard Arabic (MSA), and many?spoken?
forms, each of which is a regional dialect.
MSA is the only variety thatis standardized, regulated, and taught in schools, necessitated by its use in writtencommunication and formal venues.
The regional dialects, used primarily for day-to-day dealings and spoken communication, remain somewhat absent from written com-munication compared with MSA.
That said, it is certainly possible to produce dialectalArabic text, by using the same letters used in MSA and the same (mostly phonetic)spelling rules of MSA.?
E-mail: ozaidan@gmail.com.??
Computer and Information Science Department University of Pennsylvania, Levine Hall, room 506,3330 Walnut Street, Philadelphia, PA 19104.
E-mail: ccb@cis.upenn.edu.Submission received: 12 March 2012; revised version received: 14 March 2012; accepted for publication:17 April 2013.doi:10.1162/COLI a 00169?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 1One domain of written communication in which both MSA and dialectal Arabic arecommonly used is the on-line domain: Dialectal Arabic has a strong presence in blogs,forums, chatrooms, and user/reader commentary.
Harvesting data from such sourcesis a viable option for computational linguists to create large data sets to be used instatistical learning setups.
However, because all Arabic varieties use the same characterset, and furthermore much of the vocabulary is shared among different varieties, it isnot a trivial matter to distinguish and separate the dialects from each other.In this article, we focus on the problem of Arabic dialect identification.
We describea large data set that we created by harvesting a large amount of reader commentaryon on-line newspaper content, and describe our annotation effort on a subset of theharvested data.
We crowdsourced an annotation task to obtain sentence-level labelsindicating what proportion of the sentence is dialectal, and which dialect the sentenceis written in.
Analysis of the collected labels reveals interesting annotator behaviorpatterns and biases, and the data are used to train and evaluate automatic classifiers fordialect detection and identification.
Our approach, which relies on training languagemodels for the different Arabic varieties, greatly outperforms baselines that use (muchmore) MSA-only data: On one of the classification tasks we considered, where humanannotators achieve 88.0% classification accuracy, our approach achieves 85.7% accuracy,compared with only 66.6% accuracy by a system using MSA-only data.The article is structured as follows.
In Section 2, we provide an introduction tothe various Arabic varieties and corresponding data resources.
In Section 3, we intro-duce the dialect identification problem for Arabic, discussing what makes it a difficultproblem, and what applications would benefit from it.
Section 4 provides details aboutour annotation set-up, which relied on crowdsourcing the annotation to workers onAmazon?s Mechanical Turk.
By examining the collected labels and their distribution,we characterize annotator behavior and observe several types of human annotatorbiases.
We introduce our technique for automatic dialect identification in Section 5.The technique relies on training separate language models for the different Arabicvarieties, and scoring sentences using these models.
In Section 6, we report on a large-scale Web crawl that we performed to gather a large amount of Arabic text from on-linenewspapers, and apply our classifier on the gathered data.
Before concluding, we givean overview of related work in Section 7.2.
Background: The MSA/Dialect Distinction in ArabicAlthough the Arabic language has an official status in over 20 countries and is spokenby more than 250 million people, the term itself is used rather loosely and refers todifferent varieties of the language.
Arabic is characterized by an interesting linguisticdichotomy: the written form of the language, MSA, differs in a non-trivial fashion fromthe various spoken varieties of Arabic, each of which is a regional dialect (or a lahjah,lit.
?accent?
; also darjah, lit.
?modern?).
MSA is the only variety that is standardized,regulated, and taught in schools.
This is necessitated because of its use in writtencommunication in formal venues.1 The regional dialects, used primarily for day-to-daydealings and spoken communication, are not taught formally in schools, and remainsomewhat absent from traditional, and certainly official, written communication.1 The term MSA is used primarily by linguists and in educational settings.
For example, constitutionsof countries where Arabic is an official language simply refer to The Arabic Language, the reference tothe standard form of Arabic being implicit.172Zaidan and Callison-Burch Arabic Dialect IdentificationUnlike MSA, a regional dialect does not have an explicit written set of grammarrules regulated by an authoritative organization, but there is certainly a concept ofgrammatical and ungrammatical.2 Furthermore, even though they are ?spoken?
varieties,it is certainly possible to produce dialectal Arabic text, by spelling out words using thesame spelling rules used in MSA, which are mostly phonetic.3There is a reasonable level of mutual intelligibility across the dialects, but theextent to which a particular individual is able to understand other dialects dependsheavily on that person?s own dialect and their exposure to Arab culture and literaturefrom outside of their own country.
For example, the typical Arabic speaker has littletrouble understanding the Egyptian dialect, thanks in no small part to Egypt?s historyin movie-making and television show production, and their popularity across the Arabworld.
On the other hand, the Moroccan dialect, especially in its spoken form, is quitedifficult to understand by a Levantine speaker.
Therefore, from a scientific point ofview, the dialects can be considered separate languages in their own right, much likeNorth Germanic languages (Norwegian/Swedish/Danish) and West Slavic languages(Czech/Slovak/Polish).42.1 The Dialectal Varieties of ArabicOne possible breakdown of regional dialects into main groups is as follows (seeFigure 1): Egyptian: The most widely understood dialect, due to a thriving Egyptiantelevision and movie industry, and Egypt?s highly influential role in theregion for much of the 20th century (Haeri 2003). Levantine: A set of dialects that differ somewhat in pronunciation andintonation, but are largely equivalent in written form; closely related toAramaic (Bassiouney 2009). Gulf: Folk wisdom holds that Gulf is the closest of the regional dialect toMSA, perhaps because the current form of MSA evolved from an Arabicvariety originating in the Gulf region.
Although there are major differencesbetween Gulf and MSA, Gulf has notably preserved more of MSA?s verbconjugation than other varieties have (Versteegh 2001).2 There exist resources that describe grammars and dictionaries of many Arabic dialects (e.g.,Abdel-Massih, Abdel-Malek, and Badawi 1979; Badawi and Hinds 1986; Cowell 1964; Erwin 1963;Ingham 1994; Holes 2004), but these are compiled by individual linguists as one-off efforts, ratherthan updated regularly by central regulatory organizations, as is the case with MSA and manyother world languages.3 Arabic speakers writing in dialectal Arabic mostly follow MSA spelling rules in cases where MSAis not strictly phonetic as well (e.g., the pronunciation of the definite article Al).
Habash, Diab, andRambow (2012) have proposed CODA, a Conventional Orthography for Dialectal Arabic, tostandardize the spelling of Arabic dialect computational models.4 Note that such a view is not widely accepted by Arabic speakers, who hold MSA in high regard.
Theyconsider dialects, including their own, to be simply imperfect, even ?corrupted,?
versions of MSA,rather than separate languages (Suleiman 1994).
One exception might be the Egyptian dialect, where anationalistic movement gave rise to such phenomena as the Egyptian Wikipedia, with articles writtenexclusively in Egyptian, and little, if any, MSA.
Another notable exception is the Lebanese poet Said Akl,who spearheaded an effort to recognize Lebanese as an independent language, and even proposed aLatin-based Lebanese alphabet.173Computational Linguistics Volume 40, Number 1Figure 1One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine,Gulf, and Iraqi.
Habash (2010) and Versteegh (2001) give a breakdown along mostly the samelines.
Note that this is a relatively coarse breakdown, and further division of the dialect groupsis possible, especially in large regions such as the Maghreb. Iraqi: Sometimes considered to be one of the Gulf dialects, though it hasdistinctive features of its own in terms of prepositions, verb conjugation,and pronunciation (Mitchell 1990). Maghrebi: Heavily influenced by the French and Berber languages.
TheWestern-most varieties could be unintelligible by speakers from otherregions in the Middle East, especially in spoken form.
The Maghreb is alarge region with more variation than is seen in other regions such as theLevant and the Gulf, and could be subdivided further (Mohand 1999).There are a large number of linguistic differences between MSA and the regionaldialects.
Some of those differences do not appear in written form if they are on the levelof short vowels, which are omitted in Arabic text anyway.
That said, many differencesmanifest themselves textually as well: MSA?s morphology is richer than dialects?
along some dimensions suchas case and mood.
For instance, MSA has a dual form in addition to thesingular and plural forms, whereas the dialects mostly lack the dualform.
Also, MSA has two plural forms, one masculine and one feminine,whereas many (though not all) dialects often make no such gendereddistinction.5 On the other hand, dialects have a more complex cliticizationsystem than MSA, allowing for circumfix negation, and for attachedpronouns to act as indirect objects. Dialects lack grammatical case, whereas MSA has a complex case system.In MSA, most cases are expressed with diacritics that are rarely explicitlywritten, with the accusative case being a notable exception, as it isexpressed using a suffix (+A) in addition to a diacritic (e.g., on objectsand adverbs).5 Dialects may preserve the dual form for nouns, but often lack it in verb conjugation and pronouns, usingplural forms instead.
The same is true for the gendered plural forms, which exist for many nouns (e.g.,?teachers?
is either m?lmyn [male] or m?lmAt [female]), but not used otherwise as frequently as in MSA.174Zaidan and Callison-Burch Arabic Dialect Identification There are lexical choice differences in the vocabulary itself.
Table 1gives several examples.
Note that these differences go beyond a lackof orthography standardization. Differences in verb conjugation, even when the triliteral root is preserved.See the lower part of Table 1 for some conjugations of the root s?-r-b(to drink).This list, and Table 1, deal with differences that are expressed at the inidividual-word level.
It is important to note that Arabic varieties differ markedly in style andsentence composition as well.
For instance, all varieties of Arabic, MSA, and otherwise,allow both SVO and VSO word orders, but MSA has a higher incidence of VSO sen-tences than dialects do (Aoun, Benmamoun, and Sportiche 1994; Shlonsky 1997).2.2 Existing Arabic Data SourcesDespite the fact that speakers are usually less comfortable communicating in MSA thanin their own dialect, MSA content significantly dominates dialectal content, as MSAis the variant of choice for formal and official communication.
Relatively little printedmaterial exists in local dialects, such as folkloric literature and some modern poetry,but the vast majority of published Arabic is in MSA.
As a result, MSA?s dominance isalso apparent in data sets available for linguistic research.
The problem is somewhatmitigated in the speech domain, since dialectal data exists in the form of phone conver-sations and television program recordings, but, in general, dialectal Arabic data sets arehard to come by.Table 1A few examples illustrating similarities and differences across MSA and three Arabic dialects:Levantine, Gulf, and Egyptian.
Even when a word is spelled the same across two or morevarieties, the pronunciation might differ due to differences in short vowels (which are notspelled out).
Also, due to the lack of orthography standardization, and variance in pronunciationeven within a single dialect, some dialectal words could have more than one spelling (e.g.,Egyptian ?I drink?
could be bAs?rb, Levantine ?He drinks?
could be bys?rb).
(We use theHabash-Soudi-Buckwalter transliteration scheme to represent Arabic orthography, which mapseach Arabic letter to a single, distinct character.
We provide a table with the character mappingin Appendix A.
)English MSA LEV GLF EGYBook ktAb ktAb ktAb ktAbYear sn sn sn snMoney nqwd mSAry flws flwsCome on!
hyA!
ylA!
ylA!
ylA!I want Aryd bdy Ab?y?
?AyzNow AlA?n hlq AlHyn dlwqtWhen?
mty??
Aymty??
mty??
Amty??What?
mAA?
Ays??
ws??
Ayh?I drink A?s?rb bs?rb As?rb bs?rbHe drinks ys?rb bs?rb ys?rb bys?rbWe drink ns?rb bns?rb ns?rb bns?rb175Computational Linguistics Volume 40, Number 1Figure 2Two roughly equivalent Arabic sentences, one in MSA and one in Levantine Arabic, translatedby the same MT system (Google Translate) into English.
An acceptable translation would beWhen will we see this group of criminals undergo trial (or tried)?.
The MSA variant is handled well,whereas the dialectal variant is mostly transliterated.Figure 3Two roughly equivalent Arabic sentences, one in MSA and one in Egyptian Arabic, translatedby the same MT system (Google Translate) into English.
An acceptable translation would beWhat is this that is happening?
What is this that I?m seeing?.
As in Figure 2, the dialectal variantis handled quite poorly.The abundance of MSA data has greatly aided research on computational meth-ods applied to Arabic, but only the MSA variant of it.
For example, a state-of-the-artArabic-to-English machine translation system performs quite well when translatingMSA source sentences, but often produces incomprehensible output when the inputis dialectal.
For example, most words of the dialectal sentence shown in Figure 2 aretransliterated, whereas an equivalent MSA sentence is handled quite well.
The hightransliteration rate is somewhat alarming, as the first two words of the dialectal sentenceare relatively frequent function words: Aymty?
means ?when?
and rH corresponds to themodal ?will?.Figure 3 shows another dialectal sentence, this time in Egyptian, which again causesthe system to produce a poor translation even for frequent words.
Case in point, thesystem is unable to consistently handle any of Ayh (?what?
), Ally (the conjunction ?that?
),or dh (?this?).
Granted, it is conceivable that processing dialectal content is more difficultthan MSA, but the main problem is the lack of dialectal training data.6This is an important point to take into consideration, because the dialects differ toa large enough extent to warrant treating them as more or less different languages.
Thebehavior of machine translation systems translating dialectal Arabic when the system6 In the context of machine translation in particular, additional factors make translating dialectal contentdifficult, such as a general mismatch between available training data and the topics that are usuallydiscussed dialectally.176Zaidan and Callison-Burch Arabic Dialect IdentificationFigure 4The output of a Spanish-to-English system when given a Portuguese sentence as input,compared with the output of a Portuguese-to-English system, which performs well.The behavior is very similar to that in Figures 2 and 3, namely, the failure to translateout-of-vocabulary words when there is a language mismatch.has been trained exclusively on MSA data is similar to the behavior of a Spanish-to-English MT system when a user inputs a Portuguese sentence.
Figure 4 illustrateshow MT systems behave (the analogy is not intended to draw a parallel between thelinguistic differences MSA-dialect and Spanish-Portuguese).
The MT system?s behavioris similar to the Arabic example, in that words that are shared in common betweenSpanish and Portuguese are translated, while the Portuguese words that were neverobserved in the Spanish training data are left untranslated.This example illustrates the need for dialectal data to train MT systems to handledialectal content properly.
A similar scenario would arise with many other NLP tasks,such as parsing or speech recognition, where dialectal content would be needed in largequantities for adequate training.
A robust dialect identifier could sift through immensevolumes of Arabic text, and separate out dialectal content from MSA content.2.3 Harvesting Dialect Data from On-line Social MediaOne domain of written communication in which MSA and dialectal Arabic are bothcommonly used is the on-line domain, because it is more individual-driven and lessinstitutionalized than other venues.
This makes a dialect much more likely to be theuser?s language of choice, and dialectal Arabic has a strong presence in blogs, fo-rums, chatrooms, and user/reader commentary.
Therefore, on-line data is a valuableresource of dialectal Arabic text, and harvesting this data is a viable option for com-putational linguists for purposes of creating large data sets to be used in statisticallearning.We created the Arabic On-line Commentary Data Set (AOC) (Zaidan and Callison-Burch 2011) a 52M-word monolingual data set by harvesting reader commentary fromthe on-line versions of three Arabic newspapers (Table 2).
The data is characterizedby the prevalence of dialectal Arabic, alongside MSA, mainly in Levantine, Gulf, andEgyptian.
These correspond to the countries in which the three newspapers are pub-lished: Al-Ghad is from Jordan, Al-Riyadh is from Saudi Arabia, and Al-Youm Al-Sabe?
isfrom Egypt.7Although a significant portion of the AOC?s content is dialectal, there is still a verylarge portion of it that is in MSA.
(Later analysis in Section 4.2.1 shows dialectal contentis roughly 40%.)
In order to take full advantage of the AOC (and other Arabic data sets7 URLs: www.alghad.com, www.alriyadh.com, and www.youm7.com.177Computational Linguistics Volume 40, Number 1Table 2A summary of the different components of the AOC data set.
Overall, 1.4M comments wereharvested from 86.1K articles, corresponding to 52.1M words.News Source Al-Ghad Al-Riyadh Al-Youm Al-Sabe?
ALL# articles 6.30K 34.2K 45.7K 86.1K# comments 26.6K 805K 565K 1.4M# sentences 63.3K 1,686K 1,384K 3.1M# words 1.24M 18.8M 32.1M 52.1Mcomments/article 4.23 23.56 12.37 16.21sentences/comment 2.38 2.09 2.45 2.24words/sentence 19.51 11.14 23.22 16.65with at least some dialectal content), it is desirable to separate dialectal content fromnon-dialectal content automatically.
The task of dialect identification (and its automa-tion) is the focus for the remainder of this article.
We next present the task of Arabicdialect identification, and discuss our effort to create a data set of Arabic sentences withtheir dialectal labels.
Our annotation effort relied on crowdsourcing the annotation taskto Arabic-speakers on Amazon?s Mechanical Turk service (Section 3).3.
Arabic Dialect IdentificationThe discussion of the varieties of Arabic and the differences between them gives riseto the task of automatic dialect identification (DID).
In its simplest form, the task is tobuild a learner that can, given an Arabic sentence S, determine whether or not S containsdialectal content.
Another form of the task would be to determine in which dialect S waswritten, which requires identification at a more fine-grained level.In many ways, DID is equivalent to language identification.
Although languageidentification is often considered to be a ?solved problem,?
DID is most similar to aparticularly difficult case of language ID, where it is applied to a group of closely relatedlanguages that share a common character set.
Given the parallels between DID andlanguage identification, we investigate standard statistical methods to establish howdifficult the task is.
We discuss prior efforts for Arabic DID in Section 7.3.1 The Difficulty of Arabic DIDDespite the differences illustrated in the previous section, in which we justify treatingthe different dialects as separate languages, it is not a trivial matter to automaticallydistinguish and separate the dialects from each other.
Because all Arabic varietiesuse the same character set, and because much of the vocabulary is shared amongdifferent varieties, identifying dialect in a sentence is not simply a matter of, say,compiling a dialectal dictionary and detecting whether or not a given sentence containsdialectal words.This word-level source ambiguity is caused by several factors: A dialectal sentence might consist entirely of words that are used across allArabic varieties, including MSA.
Each of the sentences in Figure 5 consists178Zaidan and Callison-Burch Arabic Dialect Identificationof words that are used both in MSA and dialectally, and an MSA-baseddictionary would not (and should not) recognize those words asout of vocabulary (OOV).
Nevertheless, the sentences are heavilydialectal. Some words are used across the varieties with different functions.
Forexample, Tyb is used dialectally as an interjection, but is an adjectivein MSA.
(This is similar to the English usage of okay.
) Primarily due to the omission of short vowels, a dialectal word mighthave the same spelling as an MSA word with an entirely differentmeaning, forming pairs of heteronyms.
This includes strongly dialectalwords such as dwl and nby: dwl is either Egyptian for these (pronounceddowl) or the MSA for countries (pronounced duwal); nby is eitherthe Gulf for we want (pronounced nibi) or the MSA for prophet(pronounced nabi).It might not be clear for a non-Arabic speaker what makes certain sentences, suchas those of Figure 5, dialectal, even when none of the individual words are.
The answerlies in the structure of such sentences and the particular word order within them, ratherthan the individual words themselves taken in isolation.
Figure 6 shows MSA sentencesthat express the same meaning as the dialectal sentences from Figure 5.
As one couldsee, the two versions of any given sentence could share much of the vocabulary, butin ways that are noticeably different to an Arabic speaker.
Furthermore, the differenceswould be starker still if the MSA sentences were composed from scratch, rather thanby modifying the dialectal sentences, since the tone might differ substantially whencomposing sentences in MSA.Figure 5Three sentences that were identified by our annotators as dialectical, even thought they donot contain individually dialectal words.
A word-based OOV-detection approach wouldfail to classify these sentences as being dialectal, because all these words could appearin an MSA corpus.
One might argue that a distinction should be drawn between informaluses of MSA versus dialectical sentences, but annotators consistently classify these sentencesas dialect.179Computational Linguistics Volume 40, Number 1Figure 6The dialectal sentences of Figure 5, with MSA equivalents.3.2 Applications of Dialect IdentificationBeing able to perform automatic DID is interesting from a purely linguistic and experi-mental point of view.
In addition, automatic DID has several useful applications: Distinguishing dialectal data from non-dialectal data would aid in creatinga large monolingual dialectal data set, exactly as we would hope to dowith the AOC data set.
Such a data set would aid many NLP systems thatdeal with dialectal content, for instance, to train a language model foran Arabic dialect speech recognition system (Novotney, Schwartz, andKhudanpur 2011).
Identifying dialectal content can also aid in creatingparallel data sets for machine translation, with a dialectal source side. A user might be interested in content of a specific dialect, or, conversely,in strictly non-dialectal content.
This would be particularly relevantin fine-tuning and personalizing search engine results, and couldallow for better user-targeted advertising.
In the same vein, beingable to recognize dialectal content in user-generated text could aid incharacterizing communicants and their biographic attributes (Gareraand Yarowsky 2009).180Zaidan and Callison-Burch Arabic Dialect Identification In the context of an application such as machine translation (MT),identifying dialectal content could be quite helpful.
Most MT systems,when faced with OOV words, either discard the words or make an effortto transliterate them.
If a segment is identified as being dialectal first, theMT system might instead attempt to find equivalent MSA words, whichare presumably easier to process correctly (e.g., as in Salloum and Habash[2011] and, to some degree, Habash [2008]).
Even for non-OOV words,identifying dialectal content before translating could be critical to resolvethe heteronym ambiguity of the kind mentioned in Section 3.1.4.
Crowdsourcing Arabic Dialect AnnotationIn this section, we discuss crowdsourcing Arabic dialect annotation.
We discuss howwe built a data set of Arabic sentences, each of which is labeled with whether or notit contains dialectal content.
The labels include additional details about the level ofdialectal content (i.e., how much dialect there is), and of which type of dialect it is.
Thesentences themselves are sampled from the AOC data set, and we observe that about40% of sentences contain dialectal content, with that percentage varying between 37%and 48%, depending on the news source.Collecting annotated data for speech and language applications requires carefulquality control (Callison-Burch and Dredze 2010).
We present the annotation interfaceand discuss an effective way for quality control that can detect spamming behavior.
Wethen examine the collected data itself, analyzing annotator behavior, measuring agree-ment among annotators, and identifying interesting biases exhibited by the annotators.In Section 5, we use the collected data to train and evaluate statistical models for severaldialect identification tasks.4.1 Annotation InterfaceThe annotation interface displayed a group of Arabic sentences, randomly selected fromthe AOC.
For each sentence, the annotator was instructed to examine the sentence andmake two judgments about its dialectal content: the level of dialectal content, and itstype, if any.
The instructions were kept short and simple:This task is for Arabic speakers who understand the different local Arabic dialects,and can distinguish them from Fusha8 Arabic.Below, you will see several Arabic sentences.
For each sentence:1.
Tell us how much dialect is in the sentence, and then2.
Tell us which Arabic dialect the writer intends.The instructions were accompanied by the map of Figure 1, to visually illustratethe dialect breakdown.
Figure 7 shows the annotator interface populated with someactual examples, with labeling in progress.
We also collected self-reported informationsuch as native Arabic dialect and age (or number of years speaking Arabic for non-native speakers).
The interface also had built-in functionality to detect each annotator?sgeographic location based on their IP address.8 Fusha is the Arabic word for MSA, pronounced foss-ha.181Computational Linguistics Volume 40, Number 1Figure 7The interface for the dialect identification task.
This example, and the full interface, can beviewed at the http://bit.ly/eUtiO3.Of the 3.1M sentences in the AOC, we randomly9 selected a ?small?
subset of about110,000 sentences to be annotated for dialect.For each sentence shown in the interface, we asked annotators to label which dialectthe segment is written in and the level of dialect in the segment.
The dialect labels wereEgyptian, Gulf, Iraqi, Levantine, Maghrebi, other dialect, general dialect (for segmentsthat could be classified as multiple dialects), dialect but unfamiliar (for sentences thatare clearly dialect, but are written in a dialect that the annotator is not familiar with), nodialect (for MSA), or not Arabic (for segments written in English or other languages).Options for the level of dialect included no dialect (for MSA), a small amount of dialect,an even mix of dialect and MSA, mostly dialect, and not Arabic.
For this article weuse only the dialect labels, and not the level of dialect.
Zaidan (2012) incorporatesfiner-grained labels into an ?annotator rationales?
model (Zaidan, Eisner, and Piatko2007).The sentences were randomly grouped into sets of 10 sentences each, and whenWorkers performed our task, they were shown the 10 sentences of a randomly selectedset on a single HTML page.
As a result, each screen contained a mix of sentences acrossthe three newspapers presented in random order.
As control items, each screen had twoadditional sentences that were randomly sampled from the article bodies.
Such sentencesare almost always in MSA Arabic, and so their expected label is MSA.
Any workerwho frequently mislabeled the control sentences with a non-MSA label was considereda spammer, and their work was rejected.
Hence, each screen had twelve sentences intotal.9 There are far fewer sentences available from Al-Ghad commentary than the other two sources over anygiven period of time (third line of Table 2).
We have taken this imbalance into account and heavilyoversampled Al-Ghad sentences when choosing sentences to be labeled, to obtain a subset that is morebalanced across the three sources.182Zaidan and Callison-Burch Arabic Dialect IdentificationWe offered a reward of $0.05 per screen (later raised to $0.10), and had each setredundantly completed by three distinct Workers.
The data collection lasted about4.5 months, during which 33,093 Human Intelligence Task (HIT) Assignments werecompleted, corresponding to 330,930 collected labels (excluding control items).
Thetotal cost of annotation was $3,050.52 ($2,773.20 for rewards, and $277.32 for Amazon?scommission).4.2 Annotator BehaviorWith the aid of the embedded control segments (taken from article bodies) and expecteddialect label distribution, it was possible to spot spamming behavior and reject it.
Table 3shows three examples of workers whose work was rejected on this basis, having clearlydemonstrated they are unable or unwilling to perform the task faithfully.
In total,11.4% of the assignments were rejected on this basis.
In the approved assignments,the embedded MSA control sentence was annotated with the MSA label 94.4% ofthe time.
In the remainder of this article, we analyze only data from the approvedassignments.We note here that we only rejected assignments where the annotator?s behaviorwas clearly problematic, opting to approve assignments from workers mentioned later inSection 4.2.3, who exhibit systematic biases in their labels.
Although these annotators?behavior is non-ideal, we cannot assume that they are not working faithfully, andtherefore rejecting their work might not be fully justified.
Furthermore, such behaviormight be quite common, and it is worth investigating these biases to benefit futureresearch.Table 3Some statistics over the labels provided by three spammers.
Compared with the typical worker(right-most column), all workers perform terribly on the MSA control items, and also usually failto recognize dialectal content in commentary sentences.
Other red flags, such as geographiclocation and ?identifying?
unrepresented dialects, are further proof of the spammy behavior.A29V7OGM2C6205A3SZLM2NK8NUOGA8EF1I6CO7TCUTypicalMSA in control items 0% 14% 33% >90%LEV in Al-Ghad 0% 0% 15% 25%GLF in Al-Riyadh 8% 0% 14% 20%EGY in Al-Youm Al-Sabe?
5% 0% 27% 33%Other dialects 56% 0% 28% <1%Incomplete answers 13% 6% 1% <2%Worker location Romania Philippines Jordan Middle EastClaimed native dialect Gulf ?Other?
Unanswered (Various)183Computational Linguistics Volume 40, Number 14.2.1 Label Distribution.
Overall, 454 annotators participated in the task, 138 of whomcompleted at least 10 HITs.
Upon examination of the provided labels for the com-mentary sentences, 40.7% of them indicate some level of dialect, and 57.1% indicateno dialectal content (Figure 8a).
Note that 2.14% of the labels identify a sentence asbeing non-Arabic, non-textual, or as being left unanswered.
The label breakdown is astrong confirmation of our initial motivation, which is that a large portion of readercommentary contains dialectal content.10Figure 8 also illustrates the following: The most common dialectal label within a given news source matchesthe dialect of the country of publication.
This is not surprising, since thereadership for any newspaper is likely to mostly consist of the localpopulation of that country.
Also, given the newspapers?
countries ofpublication, there is almost no content that is in a dialect other thanLevantine, Gulf, or Egyptian.
For this reason, other dialects such asIraqi and Maghrebi, all combined, correspond to less than 0.01% of ourdata, and we mostly drop them from further discussion. The three news sources vary in the prevalence of dialectal content.
TheEgyptian newspaper has a markedly larger percentage of dialectal content(46.6% of labels) compared with the Saudi newspaper (40.1%) and theJordanian newspaper (36.8%). A nontrivial amount of labels (5?8%) indicate General dialectal content.The General label was meant to indicate a sentence that is dialectal butlacks a strong indication of a particular dialect.
Although many of theprovided General labels seem to reflect an intent to express this fact,there is evidence that some annotators used this category in cases wherechoosing the label Not sure would have been more appropriate butwas ignored (see Section 4.2.3). Non-Arabic content, although infrequent, is not a rare occurrence in theJordanian and Egyptian newspapers, at around 3%.
The percentage ismuch lower in the Saudi newspaper, at 0.8%.
This might reflect the deeperpenetration of the English language (and English-only keyboards) inJordan and Egypt compared with Saudi Arabia.We can associate a label with each segment based on the majority vote over the threeprovided labels for that segment.
If a sentence has at least two annotators choosinga dialectal label, we label it as dialect.
If it has at least two annotators choosingthe MSA label, we label it as MSA.11 In the remainder of the article, we will reportclassification accuracy rates that assume the presence of gold-standard class labels.Unless otherwise noted, this majority-vote label set is used as the gold-standard in suchexperiments.10 Later analysis in Section 4.2.3 shows that a non-trivial portion of the labels were provided by MSA-biasedannotators, indicating that dialectal content could be even more prevalent than what is initially suggestedby the MSA/dialect label breakdown.11 A very small percentage of sentences (2%) do not have such agreement; upon inspection these aretypically found to be sentences that are in English, e-mail addresses, romanized Arabic, or simplyrandom symbols.184Zaidan and Callison-Burch Arabic Dialect IdentificationFigure 8The distribution of labels provided by the workers for the dialect identification task, over allthree news sources (a) and over each individual news source (b?d).
Al-Ghad is published inJordan, Al-Riyadh in Saudi Arabia, and Al-Youm Al-Sabe?
in Egypt.
Their local readerships arereflected in the higher proportion of corresponding dialects.
Note that this is not a breakdownon the sentence level, and does not reflect any kind of majority voting.
For example, most ofthe LEV labels on sentences from the Saudi newspaper are trumped by GLF labels when takinga majority vote, making the proportion of LEV-majority sentences smaller than what might bededuced by looking at the label distribution in (c).In experiments where the dialectal label set is more fine-grained (i.e., LEV, GLF, andEGY instead of simply dialect), we assign to the dialectal sentence the label corre-sponding to the news source?s country of publication.
That is, dialectal sentences in theJordanian (respectively, Saudi, Egyptian) are given the label LEV (respectively, GLF, EGY).We could have used dialect labels provided by the annotators, but chose to overridethose using the likely dialect of the newspaper instead.
It turns out that sentences withan EGY majority, for instance, are extremely unlikely to appear in either the Jordanian orSaudi newspaper?only around 1% of those sentences have an EGY majority.
In the caseof the Saudi newspaper, 9% of all dialectal sentences were originally annotated as LEVbut were transformed to GLF.
Our rationales for performing the transformation is thatno context was given for the sentences when they were annotated, and annotators had abias towards their own dialect.
We provide the original annotations for other researchersto re-analyze if they wish.185Computational Linguistics Volume 40, Number 1Table 4The specific-dialect label distribution (given that a dialect label was provided), shown for eachspeaker group.Group size % LEV % GLF % EGY % GNRL % Other dialectsAll speakers 454 26.1 27.1 28.8 15.4 2.6Levantine speakers 181 35.9 28.4 21.2 12.9 1.6Gulf speakers 32 21.7 29.4 25.6 21.8 1.4Egyptian speakers 121 25.9 19.1 38.0 10.9 6.1Iraqi speakers 16 18.9 29.0 23.9 18.2 10.1Maghrebi speakers 67 20.5 28.0 34.5 12.7 4.3Other/Unknown 37 17.9 18.8 27.8 31.4 4.1Even when a sentence would receive a majority-vote label that differs fromthe news source?s primary dialect, inspection of such sentences reveals that theclassification was usually unjustified, and reflected a bias towards the annotator?snative dialect.
Case in point: Gulf-speaking annotators were in relatively short supply,whereas a plurality of annotators spoke Levantine (see Table 4).
Later in Section 4.2.3,we point out that annotators have a native-dialect bias, whereby they are likely tolabel a sentence with their native dialect even when the sentence has no evidence ofbeing written in that particular dialect.
This explains why a non-trivial number of LEVlabels were given by annotators to sentences from the Saudi newspaper (Figure 8).
Inreality, most of these labels were given by Levantine speakers over-identifying theirown dialect.
Even if we were to assign dialect labels based on the (Levantine-biased)majority votes, Levantine would only cover 3.6% of the sentences from the Saudinewspaper.12Therefore, for simplicity, we assume that a dialectal sentence is written in thedialect corresponding to the sentence?s news source, without having to inspect thespecific dialect labels provided by the annotators.
This not only serves to simplify ourexperimental set-up, but also contributes to partially reversing the native dialect biasthat we observed.4.2.2 Annotator Agreement and Performance.
The annotators exhibit a decent level ofagreement with regard to whether a segment is dialectal or not, with full agreement (i.e.,across all three annotators) on 72.2% of the segments regarding this binary dialect/MSAdecision.
This corresponds to a kappa value of 0.619 (using the definition of Fleiss[1971] for multi-rater scenarios), indicating very high agreement.13 The full-agreementpercentage decreases to 56.2% when expanding the classification from a binary decisionto a fine-grained scale that includes individual dialect labels as well.
This is still quitea reasonable result, since the criterion is somewhat strict: It does not include a segmentlabeled, say, {Levantine, Levantine, General}, though there is good reason to considerthat annotators are in ?agreement?
in such a case.12 Note that the distributions in Figure 8 are on the label level, not on the sentence level.13 Although it is difficult to determine the significance of a given kappa value, Landis and Koch (1977)characterize kappa values above 0.6 to indicate ?substantial agreement?
between annotators.186Zaidan and Callison-Burch Arabic Dialect IdentificationFigure 9A bubble chart showing workers?
MSA and dialect recall.
Each data point (or bubble) inthe graph represents one annotator, with the bubble size corresponding to the number ofassignments completed by that annotator.So how good are humans at the classification task?
We examine their classifica-tion accuracy, dialect recall, and MSA recall.
The classification accuracy is measuredover all sentences, both MSA and dialectal.
We define dialect (MSA) recall to be thenumber of sentences labeled as being dialectal (MSA), over the total number of sen-tences that have dialectal (MSA) labels based on the majority vote.
Overall, humanannotators have a classification accuracy of 90.3%, with dialect recall at 89.0%, andMSA recall at 91.5%.
Those recall rates do vary across annotators, as shown in Fig-ure 9, causing some accuracy rates to drop as low as 80% or 75%.
Of the annota-tors performing at least five HITs, 89.4% have accuracy rates greater than or equalto 80%.Most annotators have both high MSA recall and high dialect recall, with about 70%of them achieving at least 80% in both MSA and dialect recall.
Combined with thegeneral agreement rate measure, this is indicative that the task is well-defined?it isunlikely that many people would agree on something that is incorrect.We note here that the accuracy rate (90.3%) is a slight overestimate of the humanannotators?
accuracy rate, by virtue of the construction of the gold labels.
Because thecorrect labels are based on a majority vote of the annotators?
labels themselves, the twosets are not independent, and an annotator is inherently likely to be correct.
A moreinformative accuracy rate disregards the case where only two of the three annotatorsagreed and the annotator whose accuracy was being evaluated contributed one of thosetwo votes.
In other words, an annotator?s label would be judged against a majority votethat is independent of that annotator?s label.
Under this evaluation set-up, the humanaccuracy rate slightly decreases, to 88.0%.4.2.3 Annotator Bias Types.
Examining the submitted labels of individual workers revealsinteresting annotation patterns, and indicates that annotators are quite diverse in their187Computational Linguistics Volume 40, Number 1Table 5Two annotators with a General label bias, one who uses the label liberally, and one who is moreconservative.
Note that in both cases, there is a noticeably smaller percentage of General labelsin the Egyptian newspaper than in the Jordanian and Saudi newspapers.AllworkersA1M50UV37AMBZ3A2ZNK1PZOVIECD% General 6.3 12.0 2.3% General in Al-Ghad 5.2 14.2 3.1% General in Al-Riyadh 7.7 13.1 2.6% General in Al-Youm Al-Sabe?
4.9 7.6 1.0Native dialect (Various) Maghrebi Egyptianbehavior.
An annotator can be observed to have one or more of the following biastypes:14 MSA bias/dialect bias: Figure 9 shows that annotators vary in how willingthey are to label a sentence as being dialectal.
Whereas most workers (topright) exhibit both high MSA and high dialect recall, other annotators haveeither a MSA bias (top left) or a dialect bias (bottom right). Dialect-specific bias: Many annotators over-identify a particular dialect,usually their native one.
If we group the annotators by their native dialectand examine their label breakdown (Table 4), we find that Levantinespeakers over-identify sentences as being Levantine, Gulf speakersover-identify Gulf, and Egyptian speakers over-identify Egyptian.
Thisholds for speakers of other dialects as well, as they over-identify otherdialects more often than most speakers.
Another telling observationis that Iraqi speakers have a bias for the Gulf dialect, which is quitesimilar to Iraqi.
Maghrebi speakers have a bias for Egyptian, reflectingtheir unfamiliarity with the geographically distant Levantine andGulf dialects. The General bias: The General label is meant to signify sentences thatcannot be definitively classified as one dialect over another.
This is thecase when enough evidence exists that the sentence is not in MSA, butcontains no evidence for a specific dialect.
In practice, some annotatorsmake very little use of this label, even though many sentences warrantits use, whereas other annotators make extensive use of this label (see,for example, Table 5).
One interesting case is that of annotators whoseGeneral label seems to mean they are unable to identify the dialect,14 These biases should be differentiated from spammy behavior, which we already can deal with quiteeffectively, as explained in Section 4.2.188Zaidan and Callison-Burch Arabic Dialect Identificationand a label like Not sure might have been more appropriate.
Take thecase of the Maghrebi worker in Table 5, whose General bias is muchmore pronounced in the Jordanian and Saudi newspapers.
This isan indication she might have been having difficulty distinguishingLevantine and Gulf from each other, but that she is familiar with theEgyptian dialect.5.
Automatic Dialect IdentificationFrom a computational point of view, we can think of dialect identification as languageidentification, though with finer-grained distinctions that make it more difficult thantypical language ID.
Even languages that share a common character set can be distin-guished from each other at high accuracy rates using methods as simple as examiningcharacter histograms (Cavnar and Trenkle 1994; Dunning 1994; Souter et al.
1994), and,as a largely solved problem, the one challenge becomes whether languages can beidentified for very short segments.Due to the nature and characteristics and high overlap across Arabic dialects, rely-ing on character histograms alone is ineffective (see Section 5.3.1), and more contextis needed.
We will explore higher-order letter models as well as word models, anddetermine what factors determine which model is best.5.1 Smoothed n-Gram ModelsGiven a sentence S to classify into one of k classes C1, C2, .
.
.
, Ck, we will choose the classwith the maximum conditional probability:C?
= argmaxCiP(Ci|S) = argmaxCiP(S|Ci) ?
P(Ci) (1)Note that the decision process takes into account the prior distribution of theclasses, which is estimated from the training set.
The training set is also used to trainprobabilistic models to estimate the probability of S given a particular class.
We relyon training n-gram language models to compute such probabilities, and apply Kneser-Ney smoothing to these probabilities and also use that technique to assign probabilitymass to unseen or OOV items (Chen and Goodman 1998).
In language model scoring, asentence is typically split into words.
We will also consider letter-based models, wherethe sentence is split into sequences of characters.
Note that letter-based models wouldbe able to take advantage of clues in the sentence that are not complete words, such asprefixes or suffixes.
This would be useful if the amount of training data is very small,or if we expect a large domain shift between training and testing, in which case contentwords indicative of MSA or dialect might not still be valuable in the new domain.Although our classification method is based only on language model scoring, andis thus relatively simple, it is nevertheless very effective.
Experimental results in Sec-tion 5.3 (e.g., Figure 10) indicate that this method yields accuracy rates above 85%,only slightly behind the human accuracy rate of 88.0% reported in Section 4.2.2.5.2 BaselinesTo properly evaluate classification performance trained on dialectal data, we comparethe language-model classifiers to two baselines that do not use the newly collected data.189Computational Linguistics Volume 40, Number 1Figure 10Learning curves for the general MSA vs. dialect task, with all three news sources pooledtogether.
Learning curves for the individual news sources can be found in Figure 11.The 83% line has no significance, and is provided to ease comparison with Figure 11.Rather, they use available MSA-only data and attempt to determine how MSA-like asentence is.The first baseline is based on the assumption that a dialectal sentence would containa higher percentage of ?non-MSA?
words that cannot be found in a large MSA corpus.To this end, we extracted a vocabulary list from the Arabic Gigaword Corpus, producinga list of 2.9M word types.
Each sentence is given a score that equals the OOV percentage,and if this percentage exceeds a certain threshold, the sentence is classified as beingdialectal.
For each of the cross validation runs in Section 5.3.1, we use the thresholdthat yields the optimal accuracy rate on the test set (hence giving this baseline asmuch a boost as possible).
In our experiments, we found this threshold to be usuallyaround 10%.The second approach uses a more fine-grained approach.
We train a language modelusing MSA-only data, and use it to score a test sentence.
Again, if the perplexity exceedsa certain threshold, the sentence is classified as being dialectal.
To take advantage ofdomain knowledge, we train this MSA model on the sentences extracted from the articlebodies of the AOC, which corresponds to 43M words of highly relevant content.5.3 Experimental ResultsIn this section, we explore using the collected labels to train word- and letter-basedDID systems, and show that they outperform other baselines that do not utilize theannotated data.5.3.1 Two-Way, MSA vs. Dialect Classification.
We measure classification accuracy at vari-ous training set sizes, using 10-fold cross validation, for several classification tasks.
Weexamine the task both as a general MSA vs. dialect task, as well as when restrictedwithin a particular news source.
We train unigram, bigram, and trigram (word-based)models, as well as unigraph, trigraph, and 5-graph (letter-based) models.
Table 6 sum-marizes the accuracy rates for these models, and includes rates for the baselines that donot utilize the dialect-annotated data.Generally, we find that a unigram word model performs best, with a 5-graph modelslightly behind (Figure 11).
Bigram and trigram word models seem to suffer from thesparseness of the data and lag behind, given the large number of parameters they190Zaidan and Callison-Burch Arabic Dialect IdentificationFigure 11Learning curves for the MSA vs. dialect task, for each of the three news sources.
The 83% linehas no significance, and is provided to ease comparison across the three components, andwith Figure 10.would need to estimate (and instead resort to smoothing heavily).
The letter-basedmodels, with a significantly smaller vocabulary size, do not suffer from this problem,and perform well.
This is a double-edged sword though, especially for the trigraphmodel, as it means the model is less expressive and converges faster.Overall though, the experiments show a clear superiority of a supervised method,be it word- or letter-based, over baselines that use existing MSA-only data.
Whichevermodel we choose (with the exception of the unigraph model), the obtained accuracyrates show a significant dominance over the baselines.It is worth noting that a classification error becomes less likely to occur as the lengthof the sentence increases (Figure 12).
This is not surprising given prior work on thelanguage identification problem (R?ehu?r?ek and Kolkus 2009; Verma, Lee, and Zakos2009), which points out that the only ?interesting?
aspect of the problem is performanceon short segments.
The same is true in the case of dialect identification: a short sentence191Computational Linguistics Volume 40, Number 1Table 6Accuracy rates (%) on several two-way classification tasks (MSA vs. dialect) for various models.Models in the top part of the table do not utilize the dialect-annotated data, whereas models inthe bottom part do.
(For the latter kind of models, the accuracy rates reported are based on atraining set size of 90% of the available data.
)Model MSAvs.dialectAl-GhadMSAvs.dialect(Levantine)Al-RiyadhMSAvs.dialect(Gulf)Al-YoumAl-Sabe?MSAvs.dialect(Egyptian)Majority Class 58.8 62.5 60.0 51.9OOV % vs. Gigaword 65.5 65.1 65.3 66.7MSA LM-scoring 66.6 67.8 66.8 65.2Letter-based, 1-graph 68.1 69.9 68.0 70.4Letter-based, 3-graph 83.5 85.1 81.9 86.0Letter-based, 5-graph 85.0 85.7 81.4 87.0Word-based, 1-gram 85.7 87.2 83.3 87.9Word-based, 2-gram 82.8 84.1 80.6 85.9Word-based, 3-gram 82.5 83.7 80.4 85.6that contains even a single misleading feature is prone to misclassification, whereas along sentence is likely to have other features that help identify the correct class label.15One could also observe that distinguishing MSA from dialect is a more difficulttask in the Saudi newspaper than in the Jordanian paper, which in turn is harder thanin the Egyptian newspaper.
This might be considered evidence that the Gulf dialectis the closest of the dialects to MSA, and Egyptian is the farthest, in agreement withthe conventional wisdom.
Note also that this is not due to the fact that the Saudisentences tend to be significantly shorter?the ease of distinguishing Egyptian holdseven at higher sentence lengths, as shown by Figure 12.5.3.2 Multi-Way, Fine-Grained Classification.
The experiments reported earlier focusedon distinguishing MSA from dialect when the news source is known, making itstraightforward to determine which of the Arabic dialects a sentence is written in (once15 The accuracy curve for the Egyptian newspaper has an outlier for sentence lengths 10?12.
Uponinspection, we found that over 10% of the sentences in that particular length subset were actuallyrepetitions of a single 12-word sentence.
(A disgruntled reader, angry about perceived referee corruption,essentially bombarded the reader commentary section of several articles with that single sentence.
)This created an artificial overlap between the training and test sets, hence increasing the accuracyrate beyond what would be reasonably expected due to increased sentence length alone.192Zaidan and Callison-Burch Arabic Dialect IdentificationFigure 12Accuracy rates vs. sentence length in the general MSA vs. dialect task.
Accuracy rates shownare for the unigram word model trained on 90% of the data.the sentence is determined to be dialectal).
If the news source is not known, we do nothave the luxury of such a strong prior on the specific Arabic dialect.
It is thereforeimportant to evaluate our approach in a multi-way classifiation scenario, where theclass set is expanded from {MSA, dialect} to {MSA, LEV, GLF, EGY}.Under this classification set-up, the classification accuracy decreases from 85.7% to81.0%.16 The drop in performance is not at all surprising, since four-way classification isinherently more difficult than two-way classification.
(Note that the classifier is trainedon exactly the same training data in both scenarios, but with more fine-grained dialectallabels in the four-way set-up.
)Table 7 is the classifier?s confusion matrix for this four-way set-up, illustratingwhen the classifier tends to make mistakes.
We note here that most classification errorson dialectal sentences occur when these sentences are mislabeled as being MSA, notwhen they are misidentified as being in some other incorrect dialect.
In other words,dialect?dialect confusion constitutes a smaller proportion of errors than dialect?MSAconfusion.
Indeed, if we consider a three-way classification setup on dialectal sentencesalone (LEV vs. GLF vs. EGY), the classifier?s accuracy rate shoots up to 88.4%.
This is ahigher accuracy rate than for the general two-way MSA vs. dialect classification (85.7%),despite involving more classes (three instead of two), and being trained on less data(0.77M words instead of 1.78M words).
This indicates that the dialects deviate fromMSA in various ways, and therefore distinguishing dialects from each other can be doneeven more effectively than distinguishing dialect from MSA.5.3.3 Word and Letter Dialectness.
Examining the letter and word distribution in thecorpus provides valuable insight into what features of a sentence are most dialectal.Let DF(w) denote the dialectness factor of a word w, defined as:DF(w) def=f (w|D)f (w|MSA) =countD(w)/countD(.)countMSA(w)/countMSA(.
)(2)16 For clarity, we report accuracy rates only for the unigram classifier.
The patterns from Section 5.3.1 mostlyhold here as well, in terms of how the different n-gram models perform relative to each other.193Computational Linguistics Volume 40, Number 1Table 7Confusion matrix in the four-way classification setup.
Rows correspond to actual labels, andcolumns correspond to predicted labels.
For instance, 6.7% of MSA sentences were given a GLFlabel (first row, third column).
Note that entries within a single row sum to 100%.Class label MSA LEV GLF EGYMSA Sentences 86.5% 4.2% 6.7% 2.6%LEV Sentences 20.6% 69.1% 8.6% 1.8%GLF Sentences 24.2% 2.4% 72.0% 1.4%EGY Sentences 14.4% 2.2% 4.6% 78.8%where countD(w) (respectively, countMSA(w)) is the number of times w appeared in thedialectal (respectively, MSA) sentences, and countD(.)
is the total number of words inthose sentences.
Hence, DF(w) is simply a ratio measuring how much more likely w is toappear in a dialectal sentence than in an MSA sentence.
Note that the dialectness factorcan be easily computed for letters as well, and can be computed for bigrams/bigraphs,trigrams/trigraphs, and so forth.Figure 13 lists, for each news source, the word types with the highest and lowestdialectness factor.
The most dialectal words tend to be function words, and they alsotend to be strong indicators of dialect, judging by their very high DF.
On the other hand,the MSA word group contains several content words, relating mainly to politics andreligion.One must also take into account the actual frequency of a word, as DF only capturesrelative frequencies of dialect/MSA, but does not capture how often the word occurs inthe first place.
Figure 14 plots both measures for the words of Al-Ghad newspaper.
TheFigure 13Words with the highest and lowest dialectness factor values in each of the three news sources.194Zaidan and Callison-Burch Arabic Dialect IdentificationFigure 14A plot of the most common words in the Al-Ghad sentences, showing each word?s DF andcorpus frequency.
The right- and left-most words here also appear in Figure 13.
Not everyword from that list appears here though, since some words have counts below 100.
For clarity,not all points display the words they represent.plot illustrates which words are most important to the classifier: the words that arefarthest away from the point of origin, along both dimensions.As for letter-based features, many of the longer ones (e.g., 5-graph features) areessentially the same words important to the unigram word model.
The letter-basedmodels are, however, able to capture some linguistic phenomenon that the word modelis unable to: the suffixes +s?
(not in Levantine) and +wn (plural conjugation in Gulf),and the prefixes H+ (will in Egyptian), bt+ (present tense conjugation in Levantine andEgyptian), and y+ (present tense conjugation in Gulf).Figure 15 sheds some light on why even the unigraph model outperforms thebaselines.
It picks up on subtle properties of the MSA writing style that are lackingFigure 15A plot of the most common letters in the Al-Ghad sentences, showing each letter?s DF andcorpus frequency.195Computational Linguistics Volume 40, Number 1when using dialect.
Namely, there is closer attention to following hamza rules (distin-guishing A, A?, and A?
from each other, rather than mapping them all to A), and betteradherence to (properly) using + instead of +h at the end of many words.
There is alsoa higher tendency to use words containing the letters that are most susceptible to beingtransformed when pronounced dialectally: ?
(usually pronounced as z), D?
(pronouncedas D), and ?
(pronounced as t).On the topic of spelling variation, one might wonder if nomalizing the Arabictext before training language models might enhance coverage and therefore improveperformance.
For instance, would it help to map all forms of the alef hamza to a singleletter, and all instances of  to h, and so on?
Our pilot experiments indicated that suchnormalization tends to slightly but consistently hurt performance, so we opted to leavethe Arabic text as is.
The only type of preprocessing we performed was more on the?cleanup?
side of things rather than computationally motivated normalization, such asproper conversion of HTML entities (e.g., &quot; to ") and mapping Eastern Arabicnumerals to their European equivalents.6.
Applying DID to a Large-Scale Arabic Web CrawlWe conducted a large-scale Web crawl to gather Arabic text from the on-line versions ofnewspapers from various Arabic-speaking countries.
The first batch contained 319 on-line Arabic-language newspapers published in 24 countries.
This list was compiled fromhttp://newspapermap.com/ and http://www.onlinenewspapers.com/, which are Websites that show the location and language of newspapers published around the world.The list contained 55 newspapers from Lebanon, 42 from Egypt, 40 from Saudi Arabia,26 from Yemen, 26 from Iraq, 18 from Kuwait, 17 from Morocco, 15 from Algeria, 12from Jordan, and 10 from Syria.
The data were gathered from July?Sept 2011.We mirrored the 319 Web sites using wget, resulting in 20 million individual filesand directories.
We identified 3,485,241 files that were likely to contain text by selectingthe extensions htm, html, cmff, asp, pdf, rtf, doc, and docx.
We converted these filesto text using xpdf?s pdftotext for PDFs and Apple?s textutil for HTML and Doc files.When concatenated together, the text files contained 438,940,861 lines (3,452,404,197words).
We performed de-duplication to remove identical lines, after which 18,219,348lines (1,393,010,506 words) remained.We used the dialect-annotated data to train a language model for each of the fourArabic varieties (MSA, LEV, GLF, EGY), as described in the previous section.
We used thesemodels to classify the crawled data, assigning a given sentence the label correspondingto the language model under which that sentence received the highest score.
Table 8gives the resulting label breakdown.
We see that the overwhelming majority of thesentences are classified as MSA, which comes as no surprise, given the prevalence ofMSA in the newspaper genre.
Figure 16 shows some sentences that were given non-MSA labels by our classifier.7.
Related WorkHabash et al.
(2008) presented annotation guidelines for the identification of dialec-tal content in Arabic content, paying particular attention to cases of code switching.They present pilot annotation results on a small set of around 1,600 Arabic sentences(19k words), with both sentence- and word-level dialectness annotations.196Zaidan and Callison-Burch Arabic Dialect IdentificationTable 8Predicted label breakdown for the crawled data, over the four varieties of Arabic.
All varietieswere given equal priors.Variety Sentence Count PercentageMSA 13,102,427 71.9%LEV 3,636,525 20.0%GLF 630,726 3.5%EGY 849,670 4.7%ALL 18,219,348 100.0%The Cross Lingual Arabic Blog Alerts (COLABA) project (Diab et al.
2010) is anotherlarge-scale effort to create dialectal Arabic resources (and tools).
They too focus on on-line sources such as blogs and forums, and use information retrieval tasks to measuretheir ability to properly process dialectal Arabic content.
The COLABA project demon-strates the importance of using dialectal content when training and designing tools thatdeal with dialectal Arabic, and deal quite extensively with resource creation and dataharvesting for dialectal Arabic.Figure 16Example sentences from the crawled data set that were predicted to be dialectal, two in each ofthe three Arabic dialects.197Computational Linguistics Volume 40, Number 1Chiang et al.
(2006) investigate building a parser for Levantine Arabic, withoutusing any significant amount of dialectal data.
They utilize an available Levantine?MSAlexicon, but no parses of Levantine sentences.
Their work illustrates the difficulty ofadapting MSA resources for use in a dialectal domain.Zbib et al.
(2012) show that incorporating dialect training data into a statisticalmachine translation system vastly improves the quality of the translation of dialectsentences when compared to a system trained solely on an MSA-English parallel cor-pus.
When translating Egyptian and Levantine test sets, a dialect Arabic MT systemoutperforms a Modern Standard Arabic MT system trained on a 150 million wordArabic?English parallel corpus?over 100 times the amount of data as their dialectparallel corpus.As far as we can tell, no prior dialect identification work exists that is applied to Ara-bic text.
However, Lei and Hansen (2011) and Biadsy, Hirschberg, and Habash (2009) in-vestigate Arabic dialect identification in the speech domain.
Lei and Hansen (2011) buildGaussian mixture models to identify the same three dialects we consider, and are ableto achieve an accuracy rate of 71.7% using about 10 hours of speech data for training.Biadsy, Hirschberg, and Habash (2009) utilize a much larger data set (170 hours ofspeech data) and take a phone recognition and language modeling approach (Zissman1996).
In a four-way classification task (with Iraqi as a fourth dialect), they achieve a78.5% accuracy rate.
It must be noted that both works use speech data, and that dialectidentification is done on the speaker level, not the sentence level as we do.8.
ConclusionSocial media, like reader commentary on on-line newspapers, is a rich source of dialectalArabic that has previously not been studied in detail.
We have harvested this type ofresource to create a large data set of informal Arabic that is rich in dialectal content.
Weselected a large subset of this data set, and had the sentences in it manually annotatedfor dialect.
We used the collected labels to train and evaluate automatic classifiers fordialect identification, and observed interesting linguistic aspects about the task andannotators?
behavior.
Using an approach based on language model scoring, we developclassifiers that significantly outperform baselines that use large amounts of MSA data,and we approach the accuracy rates exhibited by human annotators.In addition to n-gram features, one could imagine benefiting from morphologicalfeatures of the Arabic text, by incorporating analyses given by automatic analyzers suchas BAMA (Buckwalter 2004), MAGEAD (Habash and Rambow 2006), ADAM (Salloumand Habash 2011), or CALIMA (Habash, Eskander, and Hawwari 2012).
Although thedifference between our presented approach and human annotators was found to berelatively small, incorporating additional linguistically motivated features might bepivotal in bridging that final gap.In future annotation efforts, we hope to solicit more detailed labels about dialectalcontent, such as specific annotation for why a certain sentence is dialectal and not MSA:Is it due to structural differences, dialectal terms, and so forth?
We also hope to expandbeyond the three dialects discussed in this article, by including sources from a largernumber of countries.Given the recent political unrest in the Middle East (2011), another rich source ofdialectal Arabic are Twitter posts (e.g., with the #Egypt tag) and discussions on variouspolitical Facebook groups.
Here again, given the topic at hand and the individualisticnature of the posts, they are very likely to contain a high degree of dialectal data.198Zaidan and Callison-Burch Arabic Dialect IdentificationAppendix AThe Arabic transliteration scheme used in the article is the Habash-Soudi-Buckwaltertransliteration (HSBT) mapping (Habash, Soudi, and Buckwalter 2007), which extendsthe scheme designed by Buckwalter in the 1990s (Buckwalter 2002).
Buckwalter?s origi-nal scheme represents Arabic orthography by designating a single, distinct ASCII char-acter for each Arabic letter.
HSBT uses some non-ASCII characters for better readibility,but maintains the distinct 1-to-1 mapping.Figure 17 lists the character mapping used in HSBT.
We divide the list into foursections: vowels, forms of the hamzah (glottal stop), consonants, and pharyngealizedFigure 17The character mapping used in the HBST scheme.
Most mappings are straightforward; afew non-obvious mappings are highlighted with an arrow (?)
next to them.
For brevity, themappings for short vowels and other diacritics are omitted.
Note that we take the view that?
is a pharyngealized glottal stop, which is supported by Gairdner (1925), Al-Ani (1970),Ka?stner (1981), Thelwall and Sa?Adeddin (1990), and Newman (2002).
For completeness,we indicate its IPA name as well.199Computational Linguistics Volume 40, Number 1consonants.
Pharyngealized consonants are ?thickened?
versions of other, more familiarconsonants, voiced such that the pharynx or epiglottis is constricted during the articula-tion of the sound.
Those consonants are present in very few languages and are thereforelikely to be unfamiliar to most readers, which is why we place them in a separatesection?there is no real distinction in Arabic between them and other consonants.HSBT also allows for the expression of short vowels and other Arabic diacritics, butbecause those diacritics are only rarely expressed in written (and typed) form, we omitthem for brevity.AcknowledgmentsThis research was supported in part bythe DARPA GALE program under contractno.
HR0011-06-2-0001, the DARPA BOLTprogram contract no.
HR0011-12-C-0014,the EuroMatrixPlus project funded by theEuropean Commission (7th FrameworkProgramme), the Human LanguageTechnology Center of Excellence, and bygifts from Google and Microsoft.
The viewsand findings are the authors?
alone.
Theydo not reflect the official policies or positionsof the Department of Defense or theU.S.
Government.
The authors would liketo thank the anonymous reviewers for theirextremely valuable comments on earlierdrafts of this article, and for suggestingfuture work ideas.ReferencesAbdel-Massih, Ernest T., Zaki N.Abdel-Malek, and El-Said M. Badawi.1979.
A Reference Grammar of EgyptianArabic.
Georgetown University Press.Al-Ani, Salman H. 1970.
Arabic Phonology:An Acoustical and Physiological Investigation.Mouton.Aoun, Joseph, Elabbas Benmamoun, andDominique Sportiche.
1994.
Agreement,word order, and conjunction in somevarieties of Arabic.
Linguistic Inquiry,25(2):195?220.Badawi, El-Said and Martin Hinds.
1986.A Dictionary of Egyptian Arabic.
Librairiedu Liban.Bassiouney, Reem.
2009.
ArabicSociolinguistics.
EdinburghUniversity Press.Biadsy, Fadi, Julia Hirschberg, andNizar Habash.
2009.
Spoken Arabic dialectidentification using phonotactic modeling.In Proceedings of the EACL Workshop onComputational Approaches to SemiticLanguages, pages 53?61, Athens.Buckwalter, Tim.
2002.
Buckwalter Arabictransliteration.
http://www.qamus.org/transliteration.htm.Buckwalter, Tim.
2004.
Buckwalter Arabicmorphological analyzer version 2.0.Linguistic Data Consortium,Philadelphia, PA.Callison-Burch, Chris and Mark Dredze.2010.
Creating speech and languagedata with Amazon?s Mechanical Turk.In Proceedings of the NAACL HLT 2010Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk,pages 1?12, Los Angeles, CA.Cavnar, William B. and John M. Trenkle.1994.
N-gram-based text categorization.In Proceedings of SDAIR-94, pages 161?175,Vilnius.Chen, Stanley F. and Joshua T. Goodman.1998.
An empirical study of smoothingtechniques for language modeling.Technical Report TR-10-98, ComputerScience Group, Harvard University.Chiang, David, Mona Diab, Nizar Habash,Owen Rambow, and Safiullah Shareef.2006.
Parsing Arabic dialects.In Proceedings of EACL, pages 369?376,Trento.Cowell, Mark W. 1964.
A ReferenceGrammar of Syrian Arabic.
GeorgetownUniversity Press.Diab, Mona, Nizar Habash, Owen Rambow,Mohamed Altantawy, and YassineBenajiba.
2010.
COLABA: Arabic dialectannotation and processing.
In Proceedingsof the LREC Workshop on Semitic LanguageProcessing, pages 66?74.Dunning, T. 1994.
Statistical identification oflanguage.
Technical Report MCCS 94-273,New Mexico State University.Erwin, Wallace.
1963.
A Short ReferenceGrammar of Iraqi Arabic.
GeorgetownUniversity Press.Fleiss, Joseph L. 1971.
Measuring nominalscale agreement among many raters.Psychological Bulletin, 76(5):378?382.Gairdner, William Henry Temple.
1925.The Phonetics of Arabic.
OxfordUniversity Press.Garera, Nikesh and David Yarowsky.
2009.Modeling latent biographic attributes in200Zaidan and Callison-Burch Arabic Dialect Identificationconversational genres.
In Proceedings ofACL, pages 710?718, Singapore.Habash, Nizar.
2008.
Four techniques foronline handling of out-of-vocabularywords in Arabic-English statisticalmachine translation.
In Proceedingsof ACL, Short Papers, pages 57?60,Columbus, OH.Habash, Nizar, Mona Diab, and OwenRambow.
2012.
Conventional orthographyfor dialectal Arabic.
In Proceedings ofthe Language Resources and EvaluationConference (LREC), pages 711?718,Istanbul.Habash, Nizar, Ramy Eskander, and AbdelatiHawwari.
2012.
A morphological analyzerfor Egyptian Arabic.
In Proceedings of theTwelfth Meeting of the Special Interest Groupon Computational Morphology and Phonology,pages 1?9, Montre?al.Habash, Nizar and Owen Rambow.
2006.MAGEAD: A morphological analyzerand generator for the Arabic dialects.In Proceedings of the 21st InternationalConference on Computational Linguistics and44th Annual Meeting of the Association forComputational Linguistics, pages 681?688,Sydney.Habash, Nizar, Owen Rambow, Mona Diab,and Reem Kanjawi-Faraj.
2008.
Guidelinesfor annotation of Arabic dialectness.In Proceedings of the LREC Workshop onHLT & NLP within the Arabic World,pages 49?53, Marrakech.Habash, Nizar, Abdelhadi Soudi, andTim Buckwalter.
2007.
On Arabictransliteration.
In Antal van den Bosch,Abdelhadi Soudi, and Gu?nter Neumann,editors, Arabic Computational Morphology:Knowledge-based and Empirical Methods.Kluwer/Springer Publications, chapter 2.Habash, Nizar Y.
2010.
Introduction toArabic Natural Language Processing.Morgan & Claypool.Haeri, Niloofar.
2003.
Sacred Language,Ordinary People: Dilemmas of Culture andPolitics in Egypt.
Palgrave Macmillan.Holes, Clive.
2004.
Modern Arabic: Structures,Functions, and Varieties.
GeorgetownClassics in Arabic Language andLinguistics.
Georgetown University Press.Ingham, Bruce.
1994.
Najdi Arabic: CentralArabian.
John Benjamins.Ka?stner, Hartmut.
1981.
Phonetik undPhonologie des modernen Hocharabisch.Verlag Enzyklopa?die.Landis, J. Richard and Gary G. Koch.
1977.The measurement of observer agreementfor categorical data.
Biometrics, 33:159?174.Lei, Yun and John H. L. Hansen.
2011.
Dialectclassification via text-independent trainingand testing for Arabic, Spanish, andChinese.
IEEE Transactions on Audio, Speech,and Language Processing, 19(1):85?96.Mitchell, Terence Frederick.
1990.Pronouncing Arabic.
Clarendon Press.Mohand, Tilmatine.
1999.
Substrat etconvergences: Le berbe?re et l?arabenord-africain.
Estudios de Dialectologia?Norteaafricana y andalus?
?, 4:99?119.Newman, Daniel L. 2002.
The phonetic statusof Arabic within the world?s languages.Antwerp Papers in Linguistics, 100:63?75.Novotney, Scott, Rich Schwartz, and SanjeevKhudanpur.
2011.
Unsupervised Arabicdialect adaptation with self-training.In Interspeech, pages 541?544, Florence.R?ehu?r?ek, Radim and Milan Kolkus.
2009.Language Identification on the Web:Extending the Dictionary Method,volume 5449 of Lecture Notes in ComputerScience, pages 357?368.
SpringerLink.Salloum, Wael and Nizar Habash.
2011.Dialectal to standard Arabic paraphrasingto improve Arabic-English statisticalmachine translation.
In Proceedingsof the EMNLP Workshop on Algorithmsand Resources for Modelling of Dialectsand Language Varieties, pages 10?21,Edinburgh.Shlonsky, Ur.
1997.
Clause Structure andWord Order in Hebrew and Arabic:An Essay in Comparative Semitic Syntax.Oxford University Press.Souter, Clive, Gavin Churcher, Judith Hayes,John Hughes, and Stephen Johnson.
1994.Natural language identification usingcorpus-based models.
Hermes Journal ofLinguistics, 13:183?203.Suleiman, Yasir.
1994.
Nationalism andthe Arabic language: A historicaloverview.
In Yasir Suleiman, editor,Arabic Sociolinguistics.
Curzon Press.Thelwall, Robin and M. Akram Sa?Adeddin.1990.
Arabic.
Journal of the InternationalPhonetic Association, 20(2):37?39.Verma, Brijesh, Hong Lee, and John Zakos.2009.
An Automatic Intelligent LanguageClassifier, volume 5507 of Lecture Notesin Computer Science, pages 639?646.SpringerLink.Versteegh, Kees.
2001.
The Arabic Language.Edinburgh University Press.Zaidan, Omar, Jason Eisner, and ChristinePiatko.
2007.
Using ?annotator rationales?to improve machine learning for textcategorization.
In Human LanguageTechnologies 2007: The Conference of the201Computational Linguistics Volume 40, Number 1North American Chapter of the Associationfor Computational Linguistics; Proceedingsof the Main Conference, pages 260?267,Rochester, NY.Zaidan, Omar F. 2012.
CrowdsourcingAnnotation for Machine Learning in NaturalLanguage Processing Tasks.
Ph.D. thesis,Johns Hopkins University, Baltimore, MD.Zaidan, Omar F. and Chris Callison-Burch.
2011.The Arabic Online Commentary Dataset:An annotated dataset of informal Arabicwith high dialectal content.
In Proceedingsof ACL, pages 37?41, Portland, OR.Zbib, Rabih, Erika Malchiodi, Jacob Devlin,David Stallard, Spyros Matsoukas, RichardSchwartz, John Makhoul, Omar F. Zaidan,and Chris Callison-Burch.
2012.
Machinetranslation of Arabic dialects.
In the 2012Conference of the North American Chapter ofthe Association for Computational Linguistics,pages 49?59, Montreal.Zissman, Marc A.
1996.
Comparison offour approaches to automatic languageidentification of telephone speech.
IEEETransactions on Speech and Audio Processing,4(1):31?44.202
