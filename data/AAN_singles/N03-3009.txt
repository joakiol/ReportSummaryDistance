Spoken and Written News Story Segmentation using Lexical ChainsNicola Stokes.Department of Computer Science,University College Dublin, Ireland.Nicola.Stokes@ucd.ieAbstractIn this paper we describe a novel approach tolexical chain based segmentation of broadcastnews stories.
Our segmentation systemSeLeCT is evaluated with respect to two otherlexical cohesion based segmenters TextTilingand C99.
Using the Pk and WindowDiffevaluation metrics we show that SeLeCToutperforms both systems on spoken newstranscripts (CNN) while the C99 algorithmperforms best on the written newswirecollection (Reuters).
We also examine thedifferences between spoken and written newsstyles and how these differences can affectsegmentation accuracy.1 IntroductionText segmentation can be defined as the automatic iden-tification of boundaries between distinct textual units(segments) in a textual document.
The aim of earlysegmentation research was to model the discourse struc-ture of a text, thus focusing on the detection of fine-grained topic shifts, at a clausal, sentence or pas-sage/subtopic level (Hearst 1997).
More recently withthe introduction of the TDT initiative (Allan et al 1998)segmentation research has concentrated on the detectionof coarse-grained topic shifts resulting in the identifica-tion of story boundaries in news feeds.
In particular, un-segmented broadcast news streams represent a challeng-ing real-world application for text segmentation ap-proaches, since the success of other tasks such as topictracking or first story detection depend heavily on thecorrect identification of distinct and non-overlappingnews stories.
Most approaches to story segmentation useeither Information Extraction techniques (cue phraseextraction), techniques based on lexical cohesion analy-sis or a combination of both (Reynar 1998; Beefermanet al 1999).
More recently promising results have alsobeen achieved though the use of Hidden Markov model-ing techniques, which are commonly used in speechrecognition applications (Mulbregt et al 1999).In this paper we focus on lexical cohesion basedapproaches to story segmentation.
Lexical cohesion isone element of a broader linguistic device called cohe-sion which is describe as the textual quality responsiblefor making the elements of a text appear unified or con-nected.
More specifically, lexical cohesion ?is the cohe-sion that arises from semantic relationships betweenwords?
(Morris, Hirst 1991).
With respect to segmenta-tion, an analysis of lexical cohesion can be used to indi-cate portions of text that represent single topical units orsegments i.e.
they contain a high number of semanti-cally related words.
Almost all approaches to lexicalcohesion based segmentation examine patterns of syn-tactic repetition in the text e.g.
(Reynar 1998; Hearst1997; Choi 2000).
However, there are four additionaltypes of lexical cohesion present in text: synonymy(car, automobile), specialization/generalization (horse,stallion), part-whole/whole-part (politicians, govern-ment) and statistical co-occurrences (Osama bin Laden,World Trade Center).
Lexical chaining based ap-proaches to text segmentation, on the other hand, ana-lyse all aspects of lexical cohesion in text.
Lexicalchains are defined as groups of semantically relatedwords that represent the lexical cohesive structure of atext e.g.
{flower, petal, rose, garden, tree}.
In our lexi-cal chaining implementation, words are clustered basedon the existence of statistical relationships and lexico-graphical associations (provided by the WordNet onlinethesaurus) between terms in a text.There have been three previous attempts to tackletext segmentation using lexical chains.
The first byOkumara and Honda (1994) involved an evaluationbased on five Japanese texts, the second by Stairmand(1997) used twelve general interest magazine articlesand the third by Kan et al (1998) used fifteen WallStreet Journal and five Economist articles.
All of theseattempts focus on sub-topic rather than story segmenta-tion.
In contrast, this paper investigates the usefulness oflexical chains as a technique for determining story seg-ments in spoken and written broadcast news streams.
InSection 2, we explain how this technique can be refinedEdmonton, May-June 2003Student Research Workshop , pp.
49-54Proceedings of HLT-NAACL 2003to address story segmentation.
In Section 3, we comparethe segmentation performance of our lexical chainingalgorithm with two other well known lexical cohesionbased approaches to segmentation; namely TextTiling(Hearst 1997) and C99 (Choi 2000).
Finally we examinethe grammatical differences between written and spokennews media and show how these differences can be util-ized to improve spoken transcript segmentation accu-racy.2 SeLeCT: Segmentation using LexicalChains on TextIn this section we present our topic segmenter SeLeCT.This system takes a concatenated stream of text andreturns a segmented stream of distinct news reports.
Thesystem consists of three components a ?Tokeniser?, a?Chainer?
which creates lexical chains, and a ?Detector?that uses these chains to determine news story bounda-ries.
More detailed descriptions of the ?Tokeniser?
and?Chainer?
components are reported in Stokes et al(2003).2.1  The TokeniserThe objective of the chain formation process is to builda set of lexical chains that capture the cohesive structureof the input stream.
Before work can begin on lexicalchain identification, each sample text is processed by apart-of-speech tagger.
Morphological analysis is thenperformed on these tagged texts; all plural nouns aretransformed into their singular form, adjectives pertain-ing to nouns are nominalized and all sequences of wordsthat match grammatical structures of compound nounphrases are extracted.
This idea is based on a simpleheuristic proposed by Justeson and Katz (Justeson, Katz1995), which involves scanning part-of-speech taggedtexts for patterns of adjacent tags that commonly matchproper noun phrases like ?White House aid?, ?PLOleader Yasir Arafat?, and WordNet noun phrases like?red wine?
or  ?act of god?.
Since the likelihood of find-ing exact syntactic matches of these phrases elsewherein a story is low, we include a fuzzy string matchingfunction in the lexical chainer to identify related phraseslike George_Bush  President_Bush.2.2 The Lexical ChainerThe aim of the Chainer is to find relationships betweentokens (nouns, proper nouns, compound nouns, nomi-nalized adjectives) in the data set using the WordNetthesaurus and a set of statistical word associations, andto then create lexical chains from these relationshipswith respect to a set of chain membership rules.
Thechaining procedure is based on a single-pass clusteringalgorithm, where the first token in the input stream be-comes the head of the first lexical chain.
Each subse-quent token is then added to the most recently updatedchain that it shares the strongest semantic relationship1with.
This process is continued until all tokens in thetext have been chained.
Our chaining algorithm is simi-lar to one proposed by St Onge (1995) for the detectionof malapropisms in text, however statistical word asso-ciations and proper nouns were not considered in hisoriginal implementation.2.3  Boundary DetectionThe final step in the segmentation process is to partitionthe text into its individual news stories based on thepatterns of lexical cohesion identified by the Chainer inthe previous step.
Our boundary detection algorithm is avariation on one devised by Okumara and Honda(Okumara, Honda 1994) and is based on the followingobservation:?Since lexical chain spans (i.e.
start and end points)represent semantically related units in a text, a highconcentration of chain begin and end points betweentwo adjacent textual units is a good indication of aboundary point between two distinct news stories?We define boundary strength w(n, n+1) between eachpair of adjacent textual unit in our test set, as the sum ofthe number of lexical chains whose span ends at para-graph n and the number of chains that begin their spanat paragraph n+1.
When all boundary strengths betweenadjacent paragraphs have been calculated we then get themean of all the non-zero cohesive strength scores.
Thismean value then acts as the minimum allowable boundarystrength that must be exceeded if the end of textual unit nis to be classified as the boundary point between two newsstories.Finally these boundary strength scores are ?cleaned?using an error reduction filter which removes all bound-ary points which are separated by less than x number oftextual units from a higher scoring boundary, where x istoo small to be a ?reasonable?
story length.
This filterhas the effect of smoothing out local maxima in theboundary score distribution, thus increasing segmenta-tion precision.
Different occurrences of this error areillustrated in Figure 1.
Regions A and C represent clustersof adjacent boundary points.
In this situation only theboundary with the highest score in the cluster is retainedas the true story boundary.
Therefore the boundary whichscores 6 is retained in region A while in region C bothpoints have the same score so in this case we consider thelast point in region C to be the correct boundary position.Finally, the story boundary in region B is also eliminatedbecause it is situated too close to the boundary points in1Repetition is the strongest cohesive relationship, followed bysynonymy, and then statistical associations, generaliza-tion/specialization and part-whole/whole-part relationships.C B Aregion C and it has a lower score than either of thoseboundaries.0 046 50 0 0 0 0 0 030 0 05 50 0Figure 1.
Diagram shows different types of segmentationerror; numbers greater than zero are possible boundarypositions, while zero scores represent no story boundarypoint between these two textual units.3 Segmentation EvaluationIn this section we give details of two news story seg-mentation test sets, some evaluation metrics used todetermine segmentation accuracy, and the performanceresults of the SeLeCT, C99 and TextTiling algorithms.3.1 News Segmentation Test CollectionsBoth the CNN and Reuters test collections referred to inthis paper contain 1000 randomly selected news storiestaken from the TDT1 corpus.
These test collectionswere then reorganized into 40 files each consisting of 25concatenated news stories.
Consequently, allexperimental results in Section 3.3 are averaged scoresgenerated from the individual results calculated for eachof the 40 samples.
By definition a segment in thiscontext refers to a distinct news story, thus eliminatingthe need for a set of human-judged topic shifts forassessing system accuracy.3.2 Evaluation MetricsThere has been much debate in the segmentation litera-ture regarding appropriate evaluation metrics for esti-mating segmentation accuracy.
Earlier experimentsfavored an IR style evaluation that measures perform-ance in terms of recall and precision.
However thesemetrics were deemed insufficiently sensitive when try-ing to determine system parameters that yield optimalperformance.
The most widely used evaluation metric isBeeferman et al?s (1999) probabilistic error metricPk, which calculates segmentation accuracy with respectto three different types of segmentation error: false posi-tives (falsely detected segments), false negatives(missed segments) and near-misses (very close but notexact boundaries).
However, in a recent publicationPevzner and Hearst (2002) highlight several faults withthe Pk metric.
Most notable they criticize Pk for its un-fair penalization of false negatives over false positivesand its over-penalization of near-misses.
In their paper,the authors proposed an alternative error metric calledWindowDiff which rectifies these problems.3.3 Story Segmentation ResultsIn this section we present performance results for eachsegmenter on both the CNN and Reuters test sets withrespect to the aforementioned evaluation metrics.
Asexplained in Section 3, we determine the effectivenessof our SeLeCT system with respect to two other lexicalcohesion based approaches to segmentation, namely theTextTiling (Hearst 1997) and C99 algorithms (Choi2000)2.
We also include average results from a randomsegmenter that returned 25 random boundary positionsfor each of the 40 files in both test sets.
These resultsrepresent a lower bound on segmentation performance.All results in this section are calculated using para-graphs as the basic unit of text.
Since both our test setsare in SGML format, we consider the beginning of aparagraph in this context to be indicated by a speakerchange tag in the CNN transcripts and a paragraph tagin the case of the Reuters news stories.Table 1: Pk and WD (WindowDiff) values for segmenta-tion systems on CNN and Reuters Collections.Table 1 summarizes the results of the CNN data setfor each segmentation system evaluated with respect tothe four metrics.
All values for these metrics range from0 to 1 inclusively, where 0 represents the lowest possi-ble measure of system error.
From these results we ob-serve that the accuracy of our SeLeCT segmentationalgorithm is greater than the accuracy of C99,TextTiling or the Random segmenter for both evalua-tion metrics on the CNN ?spoken?
data set.
As for theReuters segmentation performance, the C99 algorithmsignificantly outperforms both the SeLeCT andTextTiling systems.
We also observe that the Win-dowDiff metric penalizes systems more than Pk, how-ever the overall ranking of the systems with respect tothese error metrics remains the same.
With regard to theSeLeCT system, optimal performance was achievedwhen only patterns of lexical repetition were examinedduring the boundary detection phase, thus eliminatingthe need for an examination of lexicographical and sta-tistical relationships between tokens in the text.2We use Choi?s java implementations of TextTiling and C99available for free download at www.cs.man.ac.uk/~choif.
In(Choi 2000) boundaries are hypothesized using sentences asthe basic unit of text; however both C99 and TextTiling cantake advantage of paragraph information when the input con-sists of one paragraph per line.CNN  Reuters  SystemPk WD Pk WDSeLeCT 0.25 0.253 0.191 0.207TextTiling 0.259 0.299 0.221 0.244C99 0.294 0.351 0.128 0.148Random 0.421 0.48 0.490 0.514A similar conclusion was reported by Hearst (1997) andMin-Yen et al (1998); however neither of these ap-proaches included statistical word associations in theirchaining process.4 Written and Spoken Text SegmentationIt is evident from the results of our segmentation ex-periments on the CNN and Reuters test collections thatsystem performance is dependant on the type of newssource being segmented i.e.
spoken texts are more diffi-cult to segment.
This disagreement between result setsis a largely unsurprising outcome as it is well docu-mented by the linguistic community that written andspoken language modes differ greatly in the way inwhich they convey information.
At a first glance, it isobvious that written texts tend to use more formal andverbose language than their spoken equivalents.
How-ever, although CNN transcripts share certain spoken textcharacteristics (see Section 4.1), they lie somewherenearer written documents on a spectrum of linguisticforms of expression, since they contain a mixture ofspeech styles ranging from formal prepared speechesfrom anchor people, politicians, and correspondents, toinformal interviews/comments from ordinary membersof the public.
Furthermore, spoken language is alsocharacterized by false starts, hesitations, back-trackings,and interjections; however information regarding pro-sodic features and these characteristics are not repre-sented in CNN transcripts.
In the next section we look atsome grammatical differences between spoken and writ-ten text that are actually evident in CNN transcripts.
Inparticular, we look at the effect that these differenceshave on parts of speech distributions and how these im-pact segmentation performance.4.1 Lexical DensityOne method of measuring the grammatical intricacy ofspeech compared to written text, is to calculate the lexi-cal density of the language being used.
The simplestmeasure of lexical density, as defined by Halliday(1995), is the ?the number of lexical items (contentwords) as a portion of the number of running words(grammatical words)?.
Halliday states that written textsare more lexically dense while spoken texts are morelexically sparse.
In accordance with this, we observebased on part-of-speech tag information that the CNNtest set contains 8.58% less lexical items than theReuters news collection.33Lexical items included all nouns, adjectives and verbs, ex-cept for function verbs like modals and auxiliary verbs.
In-stead these verbs form part of the grammatical item lexiconwith all remaining parts of speech.
Our CNN and Reuters datasets consisted of 43.68% and 52.26% lexical items respec-tively.Halliday explains that this difference in lexical den-sity between the two modes of expression can be attrib-uted to the following observation:?Written language represents phenomena as products,while spoken language represents phenomena as proc-esses.
?In real terms this means that written text tends to con-veys most of its meaning though nouns (NN) and adjec-tives (ADJ), while spoken text conveys it though ad-verbs (ADV) and verbs (VB).
To illustrate this pointconsider the following written and spoken paraphrase ofthe same information:Written: Improvements/NN in American zooshave resulted in better living/ADJ conditionsfor their animal residents/NN.Spoken: Since/RB American zoos have beenimproved/VB the animals residing/VB in themare now/RB living/VB in better conditions.Although this example is a little contrived, it showsthat in spite of changes to the grammar, by and large thevocabulary has remained the same.
More specifically,these paraphrases illustrate how the products in the writ-ten version, improvements, resident, and living, are con-veyed as processes in spoken language though the useof verbs.
The spoken variant also contains more ad-verbs; a grammatical necessity that provides cohesion totext when processes are being described in verb clauses.As explained in Section 2.2 the SeLeCT lexicalchainer only looks at cohesive relationships betweennouns and nominalized adjectives in a text.
This ac-counts partly for SeLeCT?s lower performance on theCNN test set, since the extra information conveyedthough verbs in spoken texts is ignored by the lexicalchainer.
However since C99 and TextTiling use all partsof speech in their analysis of the text, the replacement ofproducts with processes is not the reason for a similardeterioration in their performance.
More specifically,both C99 and TextTiling rely on stopword lists to iden-tifying spurious inter-segment links between functionwords that by their nature do not indicate common topi-cality.
For the purpose of their original implementationtheir stopwords lists contained mostly pronouns, deter-miners, adverbs, and function verbs such as auxiliaryand modal verbs.
However, we have observed that thestandard set of textual function verbs is not enough forspeech text processing tasks and that their lists shouldbe extended to include other common ?low information?verbs.
These types of verbs are not necessarily charac-terized by large frequency counts in the spoken newscollection like the domain specific phrases to report orto comment.
Instead these verbs tend to have no?equivalent?
nominal form, like the verbs ?to let?
?tohear?
?to look?
or ?to try?.To test this observation we re-ran C99 andTextTiling experiments on the Reuters and CNNC B Acollections, using only nouns, adjectives, nominalizedverbs (provided by the NOMLEX (Meyers et al 1998)),and nominalized adjectives as input.
Our results showthat there is a significant decrease in WindowDiff errorfor the C99 system on both the CNN collection (a de-crease from 0.351 to 0.268) and the Reuters collection(a decrease from 0.148 to 0.121).
Similarly, we observean improvement in the WindowDiff based performanceof the TextTiling system on the CNN data set (a de-crease from 0.299 to 0.274).
However, we observe amarginal fall in performance on the Reuters data set (anincrease from 0.244 to 0.247).
These results illustratethe increased dominance of verbs in spoken text and theimportance of function verb removal by our verb nomi-nalization process for CNN segmentation performance.4.2 Reference and Conjunction in Spoken TextA picture paints a thousand words, they say, and sincenews programme transcripts are accompanied by visualand audio cues in the news stream, there will always bea loss in communicative value when transcripts are in-terpreted independently.
As stated in Section 4.1, it iswell known that conversational speech is accompaniedby prosodic and paralinguistic contributions, facial ex-pressions, gestures, intonation etc., which are rarelyconveyed in spoken transcripts.
However there are alsoexplicit (exophoric) references in the transcript to eventsoccurring outside the lexical system itself.
These exo-phoric references in CNN transcripts relate specificallyto audio references like speaker change, musical inter-ludes, background noise; and visual references likeevent, location and people shots in the video stream.We believe that this property of transcribed news is an-other reason for the deterioration in segmentation per-formance on the CNN test collection.Solving endophoric (anaphora and cataphora) andexophoric reference has long been recognized as a verydifficult problem, which requires pragmatic, semanticand syntactic knowledge in order to be solved.
Howeverthere are simple heuristics commonly used by text seg-mentation algorithms that in our case can be used totake advantage of the increased presence of reference inspoken text.
One such heuristic is based on the observa-tion that when common referents like personal and pos-sessive pronouns, and possessive determiners appear atthe beginning of a sentence, this indicates that thesereferents are linked in some way to the previous textualunit (in our case the previous paragraph).
The resolutionof these references is not of interest to our algorithm butthe fact that two textual units are linked in this waygives the boundary detection process an added advan-tage when determining story segments in the text.
Ananalysis of conjunction (another form of textual cohe-sion) can also be used to provide the detection processwith useful evidence of related paragraphs, since para-graphs that begin with conjunctions (because, and, or,however, nevertheless) and conjunctive phrases (in themean time, in addition, on the other hand) are particu-larly useful in identify cohesive links between units inconversational/interview sequences in the transcript.4.3 Refining SeLeCT Boundary DetectionIn Section 2.3 we describe in detail how the boundarydetection phrase uses lexical chaining information todetermine story segments in a text.
One approach tointegrating referential and conjunctive information withthe lexical cohesion analysis provided by the chains isto remove all paragraphs from the system output thatcontain a reference or conjunctive relationship with theparagraph immediately following it in the text.
Theproblem with this approach is that Pk and WindowDifferrors will increase if ?incorrect?
segment end points areremoved that represented near system misses rather than?pure?
false positives.
Hence, we take a more measuredapproach to integration that uses conjunctive and refer-ential evidence in the final filtering step of the detectionphrase, to eliminate boundaries in boundary clusters(Section 2.3) that cannot be story end points in the newsstream.
Figure 2 illustrates how this technique can beused to refine the filtering step.
Originally, the boundarywith score six in region A would have been consideredthe correct boundary point.
However since a conjunctivephrase links the adjacent paragraphs at this boundaryposition in the text, the boundary which scores five isdeemed the correct boundary point by the algorithm.0 046 50 0 0 0 0 0 030 0 05 50 0Figure 2 Illustrates how cohesion information can helpSeLeCT?s boundary detector resolve clusters of possiblestory boundaries.Using this technique and the verb nominalization proc-ess described in section 4.1 on both news media collec-tions, we observed an improvement in SeLeCT systemperformance on the CNN data set (a decrease in errorfrom 0.253 to 0.225), but no such improvement on theReuters collection.
Again the ineffectiveness of thistechnique on the Reuters results can be attributed todifferences between the two modes of language expres-sion, where conjunctive and referential relationshipsresolve 51.66% of the total possible set of boundarypoints between stories in the CNN collection and only22.04% in the Reuters collection.
In addition, these ref-erences in the Reuters articles mostly occur betweensentences in a paragraph rather than between paragraphsin the text thus provide no additional cohesiveinformation.
A summary of the improved results dis-cussed in this section is shown in Table 2.CNN WD Score Reuters WD Score SystemBefore After Before AfterSeLeCT 0.253 0.225 0.207 0.209C99 0.351 0.268 0.148 0.121TextTiling 0.299 0.274 0.244 0.247Table 2: Improvements in system performance as a resultof system modifications discuss in Sections 4.1 and 4.3.5 ConclusionsIn this paper we have presented a lexical chaining basedapproach to coarse-grained segmentation of CNN newstranscripts and concatenated Reuters newswire articles.We have shown that the performance of our SeLeCTsystem exceeds that of the TextTiling and C99 systemswhen detecting topic shifts in CNN transcripts.
How-ever the results of a similar experiment on Reuters newsstories showed that the C99 system outperformed allother systems on a written news collection.
Overall,lower CNN segmentation results were attributed to theinformation loss caused by prosodic and paralinguisticcharacteristics of speech and grammatical differencesbetween written and spoken modes of expression.
Fur-ther experiments showed that by limiting the input of allthe segmentation systems to nouns, adjectives, andnominalized verbs and adjectives, the effect of thesegrammatical differences on CNN segmentation per-formance was significantly reduced.
Additional SeLeCTperformance improvements were also achieved by usingreferential and conjunctive relationships as additionalevidence of cohesion in the boundary detection step.
Infuture experiments we plan to compare SeLeCT?s per-formance on written and spoken news texts with tworecently proposed systems, U00 (Utiyama 2001) andCWM (Choi 2001), which have marginally outper-formed the C99 algorithm on Choi?s (2000) test corpus.AcknowledgementsThe support of Enterprise Ireland is gratefully acknowledged.Also I wish to thank Marti Hearst for providing us with a ver-sion of the WindowDiff evaluation software and Joe Carthy forinvaluable comments.ReferencesAllan J., J. Carbonell, G. Doddington, J. Yamron, Y.Yang.
Topic Detection and Tracking Pilot Study Fi-nal Report.
In the proceedings of the DARPA Broad-casting News Workshop, pp.
194-218, 1998.Beeferman D., A. Berger, and J. Lafferty.
Statisticalmodels for text segmentation.
Machine Learning,(34):177-210.
1999.Choi F., Advances in domain independent linear textsegmentation.
In Proceedings of NAACL?00.
2000.Choi F., P. Wiemer-Hastings, J. Moore.
Latent semanticanalysis for Text Segmentation.
In proceedingsEMNLP 2001, pp.109-117, 2001.Halliday M.A.K., Spoken and Written Language.OxfordUniversity Press, 1985.Hearst M., TextTiling: Segmenting Text into Multi-Paragraph Subtopic Passages, Computational Lin-guistics, 23 (1):33-64, 1997.Justeson, J. S., S.M.
Katz., Technical terminology: somelinguistic properties and an algorithm for identifica-tion in text.
Natural Language Engineering (11): 9-27, 1995.Kan Min-Yen, J. L. Klavans, K. R. McKeown.
LinearSegmentation and Segment Relevance.
In the pro-ceedings of WVLC-6, pp.
197-205, 1998.Kozima H., Text segmentation based on similarity be-tween words.
In Proceedings of ACL-93, pp.
286-288, 1993.Meyers A., et al Using NOMLEX to produce nominali-zation patterns for information extraction.
In Pro-ceedings of the COLING-ACL Workshop on Com-putational Treatment of Nominals, 1998.Morris J., G. Hirst, Lexical Cohesion by ThesauralRelations as an Indicator of the Structure of Text,Computational Linguistics 17(1), 1991.Okumura M., T. Honda, Word sense disambiguationand text segmentation based on lexical cohesion.
Inproceedings of COLING-94, pp.
755-761, 1994.Pevzner, L., and M. Hearst, A Critique and Improve-ment of an Evaluation Metric for Text Segmentation,Computational Linguistics, 28 (1):19-36, 2002.Reynar J., Topic Segmentation: Algorithms andApplications, Ph.D. thesis, Dept.
Computer andInformation Science, UPenn, 1998.Stairmand M.A, A Computational Analysis of LexicalCohesion with Applications in IR, PhD Thesis, Dept.of Language Engineering, UMIST.
1996.St-Onge D., Detecting and Correcting Malapropismswith Lexical Chains, Dept.
of Computer Science,University of Toronto, M.Sc.
Thesis, 1995.Stokes N., J. Carthy, A.F.
Smeaton.
SeLeCT: A LexicalCohesion Based News Story Segmentation System.Technical Report CS02-03, Dept.
of Computer Sci-ence, University College Dublin, 2003.Utiyama M., H. Isahara.
A statistical model for domain-independent text segmentation.
In proceedings ofACL-2001, pp.491-498, 2001.van Mulbregt P., I. Carp, L. Gillick, S. A. Lowe, J. P.Yamron.
Segmentation of Automatically TranscribedBroadcast News Text, In Proceedings of the DARPABroadcast News Workshop, 1999.
