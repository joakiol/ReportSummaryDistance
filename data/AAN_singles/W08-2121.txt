CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 159?177Manchester, August 2008The CoNLL-2008 Shared Task onJoint Parsing of Syntactic and Semantic DependenciesMihai Surdeanu?,?Richard Johansson?Adam Meyers?Llu?
?s M`arquez?
?Joakim Nivre??,???
: Barcelona Media Innovation Center, mihai.surdeanu@barcelonamedia.org?
: Yahoo!
Research Barcelona, mihais@yahoo-inc.com?
: Lund University, richard@cs.lth.se?
: New York University, meyers@cs.nyu.edu??
: Technical University of Catalonia, lluism@lsi.upc.edu??
: V?axj?o University, joakim.nivre@vxu.se??
: Uppsala University, joakim.nivre@lingfil.uu.seAbstractThe Conference on Computational Natu-ral Language Learning is accompanied ev-ery year by a shared task whose purposeis to promote natural language processingapplications and evaluate them in a stan-dard setting.
In 2008 the shared task wasdedicated to the joint parsing of syntacticand semantic dependencies.
This sharedtask not only unifies the shared tasks ofthe previous four years under a uniquedependency-based formalism, but also ex-tends them significantly: this year?s syn-tactic dependencies include more informa-tion such as named-entity boundaries; thesemantic dependencies model roles of bothverbal and nominal predicates.
In this pa-per, we define the shared task and describehow the data sets were created.
Further-more, we report and analyze the results anddescribe the approaches of the participat-ing systems.1 IntroductionIn 2004 and 2005 the shared tasks of the Confer-ence on Computational Natural Language Learn-ing (CoNLL) were dedicated to semantic role la-beling (SRL), in a monolingual setting (English).In 2006 and 2007 the shared tasks were devoted tothe parsing of syntactic dependencies, using cor-pora from up to 13 languages.
The CoNLL-2008shared task1proposes a unified dependency-basedc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1http://www.yr-bcn.es/conll2008formalism, which models both syntactic depen-dencies and semantic roles.
Using this formalism,this shared task merges both the task of syntacticdependency parsing and the task of identifying se-mantic arguments and labeling them with semanticroles.
Conceptually, the 2008 shared task can bedivided into three subtasks: (i) parsing of syntacticdependencies, (ii) identification and disambigua-tion of semantic predicates, and (iii) identificationof arguments and assignment of semantic roles foreach predicate.
Several objectives were addressedin this shared task:?
SRL is performed and evaluated using adependency-based representation for bothsyntactic and semantic dependencies.
WhileSRL on top of a dependency treebank hasbeen addressed before (Hacioglu, 2004),our approach has several novelties: (i) ourconstituent-to-dependency conversion strat-egy transforms all annotated semantic argu-ments in PropBank and NomBank not just asubset; (ii) we address propositions centeredaround both verbal (PropBank) and nominal(NomBank) predicates.?
Based on the observation that a richer setof syntactic dependencies improves seman-tic processing (Johansson and Nugues, 2007),the syntactic dependencies modeled are morecomplex than the ones used in the previousCoNLL shared tasks.
For example, we nowinclude apposition links, dependencies de-rived from named entity (NE) structures, andbetter modeling of long-distance grammaticalrelations.?
A practical framework is provided for thejoint learning of syntactic and semantic de-pendencies.159Given the complexity of this shared task, welimited the evaluation to a monolingual, English-only setting.
The evaluation is separated into twodifferent challenges: a closed challenge, wheresystems have to be trained strictly with informa-tion contained in the given training corpus, and anopen challenge, where systems can be developedmaking use of any kind of external tools and re-sources.
The participants could submit results ineither one or both challenges.This paper is organized as follows.
Section 2defines the task, including the format of the data,the evaluation metrics, and the two challenges.Section 3 introduces the corpora used and ourconstituent-to-dependency conversion procedure.Section 4 summarizes the results of the submit-ted systems.
Section 5 discusses the approachesimplemented by participants.
Section 6 analyzesthe results using additional non-official evaluationmeasures.
Section 7 concludes the paper.2 Task DefinitionIn this section we provide the definition of theshared task, starting with the format of the sharedtask data, followed by a description of the eval-uation metrics used and a discussion of the twoshared task challenges, i.e., closed and open.2.1 Data FormatThe data format used in this shared task was highlyinfluenced by the formats used in the 2004?2007shared tasks.
The data follows these general rules:?
The files contain sentences separated by ablank line.?
A sentence consists of one or more tokens andthe information for each token is representedon a separate line.?
A token consists of at least 11 fields.
Thefields are separated by one or more whites-pace characters (spaces or tabs).
Whitespacecharacters are not allowed within fields.Table 1 describes the fields stored for each tokenin the closed-track data sets.
Columns 1?3 and5?8 are available at both training and test time.Column 4, which contains gold-standard part-of-speech (POS) tags, is not given at test time.
Thesame holds for columns 9 and above, which con-tain the syntactic and semantic dependency struc-tures that the systems should predict.The PPOS and PPOSS fields were automati-cally predicted using the SVMTool POS tagger(Gim?enez, 2004).
To predict the tags in the train-ing set, a 5-fold cross-validation procedure wasused.
The LEMMA and SPLIT LEMMA fieldswere predicted using the built-in lemmatizer inWordNet (Fellbaum, 1998) based on the most fre-quent sense for the form and part-of-speech tag.Since NomBank uses a sub-word anal-ysis in some hyphenated words (such as[finger]ARG-[pointing]PRED), the data for-mat represents the parts in hyphenated words asseparate tokens (columns 6?8).
However, theformat also represents how the parts originally fittogether before splitting (columns 2?5).
Paddingcharacters (?
?)
are used in columns 2?5 toensure the same number of rows for all columnscorresponding to one sentence.
All syntactic andsemantic dependencies are annotated relative tothe split word forms (columns 6?8).Table 2 shows the columns available to the sys-tems participating in the open challenge: named-entity labels as in the CoNLL-2003 Shared Task(Tjong Kim San and De Meulder, 2003) andfrom the BBN Wall Street Journal Entity Corpus,2WordNet supersense tags, and the output of an off-the-shelf dependency parser (Nivre et al, 2007b).Columns 1?3 were predicted using the tagger ofCiaramita and Altun (2006).
Because the BBNcorpus shares lexical content with the Penn Tree-bank, we generated the BBN tags using a 2-foldcross-validation procedure.2.2 Evaluation MeasuresWe separate the evaluation measures into twogroups: (i) official measures, which were used forthe ranking of participating systems, and (ii) addi-tional unofficial measures, which provide furtherinsight into the performance of the participatingsystems.2.2.1 Official Evaluation MeasuresThe official evaluation measures consist of threedifferent scores: (i) syntactic dependencies arescored using the labeled attachment score (LAS),(ii) semantic dependencies are evaluated using alabeled F1score, and (iii) the overall task is scoredwith a macro average of the two previous scores.We describe all these scoring measures next.The LAS score is defined similarly as in the pre-vious two shared tasks, as the percentage of to-2LDC catalog number LDC2005T33.160Number Name Description1 ID Token counter, starting at 1 for each new sentence.2 FORM Unsplit word form or punctuation symbol.3 LEMMA Predicted lemma of FORM.4 GPOS Gold part-of-speech tag from the Treebank (empty at test time).5 PPOS Predicted POS tag.6 SPLIT FORM Tokens split at hyphens and slashes.7 SPLIT LEMMA Predicted lemma of SPLIT FORM.8 PPOSS Predicted POS tags of the split forms.9 HEAD Syntactic head of the current token, which is either a value of ID or zero (0).10 DEPREL Syntactic dependency relation to the HEAD.11 PRED Rolesets of the semantic predicates in this sentence.12.
.
.
ARG Columns with argument labels for each semantic predicate following textual order.Table 1: Column format in the closed-track data.
The columns in the lower part of the table are unseenat test time and are to be predicted by systems.Number Name Description1 CONLL2003 Named entity labels using the tag set from the CoNLL-2003 shared task.2 BBN NE labels using the tag set from the BBN Wall Street Journal Entity Corpus.3 WNSS WordNet super senses.4 MALT HEAD Head of the syntactic dependencies generated by MaltParser.5 MALT DEPREL Label of syntactic dependencies generated by MaltParser.Table 2: Column format in the open-track data.kens for which a system has predicted the correctHEAD and DEPREL columns (see Table 1).
Sameas before, our scorer also computes the unlabeledattachment score (UAS), i.e., the percentage of to-kens with correct HEAD, and label accuracy, i.e.,the percentage of tokens with correct DEPREL.The semantic propositions are evaluated by con-verting them to semantic dependencies, i.e., wecreate a semantic dependency from every predicateto all its individual arguments.
These dependen-cies are labeled with the labels of the correspond-ing arguments.
Additionally, we create a seman-tic dependency from each predicate to a virtualROOT node.
The latter dependencies are labeledwith the predicate senses.
This approach guaran-tees that the semantic dependency structure con-ceptually forms a single-rooted, connected (but notnecessarily acyclic) graph.
More importantly, thisscoring strategy implies that if a system assignsthe incorrect predicate sense, it still receives somepoints for the arguments correctly assigned.
Forexample, for the correct proposition:verb.01: ARG0, ARG1, ARGM-TMPthe system that generates the following output forthe same argument tokens:verb.02: ARG0, ARG1, ARGM-LOCreceives a labeled precision score of 2/4 becausetwo out of four semantic dependencies are incor-rect: the dependency to ROOT is labeled 02 in-stead of 01 and the dependency to the ARGM-TMPis incorrectly labeled ARGM-LOC.
Using this strat-egy we compute precision, recall, and F1scoresfor both labeled and unlabeled semantic dependen-cies.Finally, we combine the syntactic and semanticmeasures into one global measure using macro av-eraging.
We compute macro precision and recallscores by averaging the labeled precision and re-call for semantic dependencies with the LAS forsyntactic dependencies:3LMP = Wsem?
LPsem+ (1?Wsem) ?
LAS (1)LMR = Wsem?
LRsem+ (1?Wsem) ?
LAS (2)where LMP is the labeled macro precision andLPsemis the labeled precision for semantic depen-dencies.
Similarly, LMR is the labeled macro re-call and LRsemis the labeled recall for semanticdependencies.
Wsemis the weight assigned to thesemantic task.4The macro labeled F1score, whichwas used for the ranking of the participating sys-tems, is computed as the harmonic mean of LMPand LMR.3We can do this because the LAS for syntactic dependen-cies is a special case of precision and recall, where the pre-dicted number of dependencies is equal to the number of golddependencies.4We assign equal weight to the two tasks, i.e., Wsem=0.5.1612.2.2 Additional Evaluation MeasuresWe used several additional evaluation measuresto further analyze the performance of the partici-pating systems.The first additional measure used is ExactMatch, which reports the percentage of sentencesthat are completely correct, i.e., all the generatedsyntactic dependencies are correct and all the se-mantic propositions are present and correct.
Whilethis score is significantly lower than any of the of-ficial scores, it will award systems that performedjoint learning or optimization for all subtasks.In the same spirit but focusing on the seman-tic subtasks, we report the Perfect Proposition F1score, where we score entire semantic frames orpropositions.
This measure is similar to the PPropsaccuracy score from the 2005 shared task (Carrerasand M`arquez, 2005), with the caveat that this yearthis score is implemented as an F1measure, be-cause predicates are not provided in the test data.Hence, propositions may be over or under gener-ated at prediction time.Lastly, we analyze systems based on the ratiobetween labeled F1score for semantic dependen-cies and the LAS for syntactic dependencies.
Inother words, this measure normalizes the seman-tic scores relative to the performance of the pars-ing component.
This measure estimates the trueoverall performance of the semantic subtasks, in-dependent of the syntactic parser.5For example,this score addresses the situations where the se-mantic labeled F1score of one system is artificiallylow because the corresponding syntactic compo-nent does not perform well.2.3 Closed and Open ChallengesSimilarly to the CoNLL-2005 shared task, thisshared task evaluation is separated into two chal-lenges:Closed Challenge - systems have to be builtstrictly with information contained in the giventraining corpus, and tuned with the developmentsection.
In addition, the PropBank and NomBanklexical frames can be used.
These restrictionsmean that constituent-based parsers or SRL sys-tems can not be used in this challenge because theconstituent-based annotations are not provided inour training set.
The aim of this challenge is to5A correct evaluation of the stand-alone SRL systemswould require the usage of gold syntactic dependencies, butthese were not provided for the testing corpora.compare the performance of the participating sys-tems in a fair environment.Open Challenge - systems can be developed mak-ing use of any kind of external tools and resources.The only condition is that such tools or resourcesmust not have been developed with the annota-tions of the test set, both for the input and out-put annotations of the data.
In this challenge,we are interested in learning methods which makeuse of any tools or resources that might improvethe performance.
For example, we encourage theuse of semantic information, as provided by NErecognition or word-sense disambiguation (WSD)systems (such state-of-the-art annotations are pro-vided by the organizers, see Table 2).
Also, inthis challenge participants are encouraged to useconstituent-based parsers and SRL systems, aslong as these systems were trained only with thesections of Penn Treebank used in the shared tasktraining corpus.
To encourage the participation ofthe groups that are only interested in SRL, the or-ganizers provide also the output of a state-of-the-art dependency parser as input in this challenge.The comparison of different systems in this settingmay not be fair, and thus ranking of systems is notnecessarily important.3 DataThe corpora used in the shared task evaluationwere generated through a process that mergesseveral input corpora and converts them fromthe constituent-based formalism to dependencies.This section starts with an introduction of the in-put corpora used, followed by a description ofthe constituent-to-dependency conversion process.The section concludes with an overview of theshared task corpora.3.1 Input CorporaInput to our merging procedures includes the PennTreebank, BBN?s named entity corpus, PropBankand NomBank.
In this section, we will pro-vide brief descriptions of these annotations interms of both form and content.
All annotationsare currently being distributed by the LinguisticData Consortium, with the exception of NomBank,which is freely downloadable.66http://nlp.cs.nyu.edu/meyers/NomBank.html1623.1.1 Penn Treebank 3The Penn Treebank 3 corpus (Marcus et al,1993) consists of hand-coded parses of the WallStreet Journal (test, development and training) anda small subset of the Brown corpus (W. N. Fran-cis and H. Ku?cera, 1964) (test only).
These handparses are notated in-line and sometimes involvechanging the strings of the input data.
For ex-ample, in file wsj 0309, the token fearlast in thetext corresponds to the two tokens fear and lastin the annotated data.
In a similar way, cannotis regularly split to can and not.
It is significantthat the other annotations assume the tokeniza-tion of the Penn Treebank, as this makes it easierfor us to merge the annotation.
The Penn Tree-bank syntactic annotation includes phrases, partsof speech, empty category representations of vari-ous filler/gap constructions and other phenomena,based on a theoretical perspective similar to thatof Government and Binding Theory (Chomsky,1981).3.1.2 BBN Pronoun Coreference and EntityType CorpusBBN?s NE annotation of the Wall Street Journalcorpus (Weischedel and Brunstein, 2005) takes theform of SGML inline markup of text, tokenizedto be completely compatible with the Penn Tree-bank annotation, e.g., fearlast and cannot are splitin the same ways.
Named entity categories in-clude: Person, Organization, Location, GPE, Fa-cility, Money, Percent, Time and Date, based onthe definitions of these categories in MUC (Chin-chor and Robinson, 1998) and ACE7tasks.
Sub-categories are included as well.
Note however thatfrom this corpus we only use NE boundaries toderive NAME dependencies between NE tokens,e.g., we create a NAME dependency from Mary toSmith given the NE mention Mary Smith.3.1.3 Proposition Bank I (PropBank)The PropBank annotation (Palmer et al, 2005)classifies the arguments of all the main verbs in thePenn Treebank corpus, other than be.
Argumentsare numbered (ARG0, ARG1, .
.
.)
based on lexicalentries or frame files.
Different sets of argumentsare assumed for different rolesets.
Dependent con-stituents that fall into categories independent ofthe lexical entries are classified as various types7http://projects.ldc.upenn.edu/ace/of ARGM (TMP, ADV, etc.
).8Rather than us-ing PropBank directly, we used the version createdfor the CoNLL-2005 shared task (Carreras andM`arquez, 2005).
PropBank?s pointers to subtreesare converted into the list of leaves of those sub-trees, minus the empty categories.
On occasion,arguments of verbs end up being two non-adjacentsubstrings.
For example, the argument of claims inthe following sentence is indicated in bold: Thissentence, Mary claims, is self-referential.
TheCoNLL-2005 format handles this by marking bothstrings A1 (This sentence and is self-referential),but adding a C- prefix to the argument tag on thesecond argument.
Another difference between thePropBank annotation and the CoNLL-2005 ver-sion of it is their treatments of filler gap construc-tions involving empty categories.
PropBank an-notation includes the whole chain of empty cate-gories, as well as the antecedent of the empty cate-gory (the filler of the gap).
In contrast, the CoNLL-2005 version only includes the filler of the gap andif there is no filler, the argument is omitted, e.g.,no ARG0 (subject) for leave would be included inI said to leave because the subject of leave is un-specified.3.1.4 NomBankNomBank annotation (Meyers et al, 2004) usesessentially the same framework as PropBank to an-notate arguments of nouns.
Differences betweenPropBank and NomBank stem from differencesbetween noun and verb argument structure; differ-ences in treatment of nouns and verbs in the PennTreebank; and differences in the sophistication ofprevious research about noun and verb argumentstructure.
Only the subset of nouns that take ar-guments are annotated in NomBank and only asubset of the non-argument siblings of nouns aremarked as ARGM.
These limitations were nec-essary to make the NomBank task consistent andtractable.
In addition, long distance dependenciesof nouns, e.g., the relation between Mary and walkin Mary took dozens of walks is handled as fol-lows: Mary is marked as the ARG0 of walk andtook + dozens + of is marked as a support chainin NomBank.
In contrast, verbal long distance de-pendencies can be handled by means of empty cat-egories in the Penn Treebank, e.g., the relation be-8PropBank I is used here.
Later versions of PropBankmark instances of be in addition to other verbs.
PropBank?suse of the terms roleset and ARGM correspond approximatelyto sense and adjunct in common usage.163tween John and walked in John seemed t to walk.Support chains are needed because nominal longdistance dependencies are not captured under thePenn Treebank?s system of empty categories.3.2 Conversion to Dependencies3.2.1 Syntactic DependenciesThere exists no large-scale dependency tree-bank for English, and we thus had to construct adependency-annotated corpus automatically fromthe Penn Treebank (Marcus et al, 1993).
Sincedependency syntax represents grammatical struc-ture by means of labeled binary head?dependentrelations rather than phrases, the task of the con-version procedure is to identify and label thehead?dependent pairs.
The idea underpinningconstituent-to-dependency conversion algorithms(Magerman, 1994; Collins, 1999; Yamada andMatsumoto, 2003) is that head?dependent pairs arecreated from constituents by selecting one word ineach phrase as the head and setting all other as itsdependents.
The dependency labels are then in-ferred from the phrase?subphrase or phrase?wordrelations.Our conversion procedure (Johansson andNugues, 2007) differs from this basic approach byexploiting the rich structure of the constituent for-mat used in Penn Treebank 3:?
Grammatical function labels that often can bedirectly used in the dependency framework.?
Long-distance grammatical relations repre-sented by means of empty categories and sec-ondary edges, which can be used to create (of-ten nonprojective) dependency links.Of the grammatical function tags available in theTreebank, we removed the HLN, NOM, TPC, andTTL tags since they represent structural propertiesof single phrases rather than binary relations.
Forcompatibility between the WSJ and Brown cor-pora, we removed the ETC, UNF, and IMP tagsfrom Brown and the CLR tag from WSJ.Algorithms 1 and 2 show the constituent-to-dependency conversion algorithm and function la-beling, respectively.
The first steps apply structuraltransformations to the constituent trees.
Next, ahead word is assigned to each constituent.
Afterthis, grammatical functions are inferred, allowinga dependency tree to be created.To find head children (used inassign-heads), a system of rules is usedAlgorithm 1: Pseudocode for constituent-to-dependency conversion.procedure constituents-to-dependencies(T )import-glarf(T )reattach-traces(T )split-small-clauses(T )assign-heads(T.root)assign-functions(T )return create-dependency-tree(T )procedure import-glarf(T )Import a GLARF surface dependency graph Gfor each multi-word name N in Gfor each token d in NSet the function tag of d to NAMEfor each dependency link h ?Ld in Gif L ?
{ APPOSITE, A-POS, N-POS, POST-HON, Q-POS,RED-RELATIVE, SUFFIX, T-POS, TITLE }or if h and d are inside a split wordSet the function tag of d to L in Tif h and d are part of a larger constituentAdd an NX constituent to T that brackets h and dprocedure reattach-traces(T )for each empty category t in Tif t is linked to a constituent C via a secondary edge label Land L ?
{*ICH*,*T*,*RNR*}disconnect Cdisconnect the secondary edgeattach C to the parent of tprocedure split-small-clauses(T )for each verb phrase C in Tif C has a child S and the phrase label of S is Sand S is not preceded by a ??
or , tagand S has a subject child sdisconnect sattach s to Cset the function tag of s to OBJset the function tag of S to OPRDprocedure assign-heads(N)for each child C of Nassign-heads(C)if is-coordinated(N)e ?
index of first CC or CONJP or , or :elsee ?
index of last child of Nfind head child H between 1 and e according to head rules (Table 3)N.head ?
H.headprocedure is-coordinated(N)if N has the label UCP return Trueif N has a CC or CONJP child which is not leftmost return Trueif N has a , or : child c, and c is not leftmost or rightmost orcrossed by an apposition link, return Trueelse return Falseprocedure create-dependency-tree(T )D ?
{}for each token t in Tlet C be the highest constituent that t is the head oflet P be the parent of Clet L be the function tag of CD ?
D ?
P.head ?Ltreturn D(Table 3).
The first column in the table indicatesthe phrase type, the second is the search direction,and the third is a priority list of phrase types tolook for.
For instance, to find the head of an Sphrase, we look from right to left for a VP.
Ifno VP is found, look for anything with a PRDfunction tag, and so on.Moreover, since the grammatical structure in-164ADJP ?
NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DTFW RBR RBS SBAR RBADVP ?
RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NNCONJP ?
CC RB INFRAG ?
(NN*| NP) W*SBAR (PP | IN) (ADJP | JJ) ADVPRBINTJ ?
*LST ?
LS :NAC, NP, NX, WHNP ?
(NN*| NX) NP-?
JJR CD JJ JJS RB QP NPPP, WHPP ?
IN TO VBG VBN RP FWPRN ?
S*N*W*PP|IN ADJP|JJ*ADVP|RB*PRT ?
RPQP ?
$ IN NNS NN JJ RB DT CD NCD QP JJR JJSRRC ?
VP NP ADVP ADJP PPS ?
VP*-PRD S SBAR ADJP UCP NPSBAR ?
S SQ SINV SBAR FRAG IN DTSBARQ ?
SQ S SINV SBARQ FRAGSINV ?
VBZ VBD VBP VB MD VP*-PRD S SINV ADJP NPSQ ?
VBZ VBD VBP VB MD*-PRD VP SQUCP ?
*VP ?
VBD VBN MD VBZ VB VBG VBP VP*-PRD ADJP NN NNSNPWHADJP ?
CC WRB JJ ADJPWHADVP ?
CC WRBX ?
*Table 3: Head rules.Algorithm 2: Pseudocode for the function la-beling procedure.procedure assign-functions(T )for each constituent C in Tif C has no function tag from Penn or GLARFL ?
infer-function(C)Set the function tag of C to Lprocedure infer-function(C)let c be the head of C, P the parent of C, and p the head of Pif C is an object return OBJif C is PRN return PRNif h is punctuation return Pif C is coordinated with P return COORDif C is PP, ADVP, or SBAR and P is VP return ADVif C is PRT and P is VP return PRTif C is VP and P is VP, SQ, or SINV return VCif C is TO and P is VP return IMif P is SBAR and p is IN return SUBif P is VP, S, SBAR, SBARQ, SINV, or SQ and C is RB return ADVif P is NP, NX, NAC, or WHNP return NMODif P is ADJP, ADVP, WHADJP, or WHADVP return AMODif P is PP or WHPP return PMODelse return DEPside noun phrases (NP) is under-specified in thePenn Treebank, we imported dependencies in-side NPs and hyphenated words from a versionof the Penn Treebank mapped into GLARF, theGrammatical and Logical Argument Representa-tion Framework (Meyers et al, 2007).The parts of GLARF?s NP analysis that are mostrelevant to this task include: (i) identifying ap-posites (APPO, e.g., that book depends on gift inMary?s gift, a book about cheese; (ii) the iden-tification of name boundaries taken from BBN?sNE annotation, e.g., identifying that Smith de-pends on Mary which depends on appointmentin the Mary Smith appointment; (iii) identifyingTITLE and POSTHON dependencies, e.g., deter-mining that Ms. and III depend on Mary in Ms.Mary Smith III.
These identifications were car-ried out by hand-coded rules that have been finetuned as part of GLARF, over the past severalyears.
For example, identifying apposition con-structions requires identifying that both the headand the apposite can stand alone ?
proper nouns(John Smith), plural nouns (books), and singularcommon nouns with determiners (the book) arestand-alone cases, whereas singular nouns withoutdeterminers (green book) do not qualify.We split Treebank tokens at a hyphen (-) or aforward slash (/) if the segments on either side ofthese delimiters are: (a) a word in a dictionary(COMLEX Syntax or any of the dictionaries avail-able on the NOMLEX website); (b) part of a mark-able Named Entity;9or (c) a prefix from the list:co, pre, post, un, anti, ante, ex, extra, fore, non,over, pro, re, super, sub, tri, bi, uni, ultra.
For ex-ample, York-based was split into 3 segments: (1)York, (2) - and (3) based.9The CoNLL-2008 website contains a Named Entity To-ken gazetteer to aid in this segmentation.1653.2.2 Semantic DependenciesWhen encoding the semantic dependencies, itwas necessary to convert the underlying con-stituent analysis of PropBank and NomBank intoa dependency analysis.
Because semantic predi-cates are already assigned to individual tokens inboth PropBank (the version used for the CoNLL-2005 shared task) and NomBank, constituent-to-dependency conversion is thus necessary only forsemantic arguments.
Conceptually, this conver-sion can be handled using similar heuristics as de-scribed in Section 3.2.1.
However, in order toavoid replicating this effort and to ensure compat-ibility between syntactic and semantic dependen-cies, we decided to generate semantic dependen-cies using only argument boundaries and the syn-tactic dependencies generated in Section 3.2.1, i.e.,ignoring syntactic constituents.
Given this input,we identify the head of a semantic argument usingthe following heuristic:The head of a semantic argument is as-signed to the token inside the argumentboundaries whose head is a token out-side the argument boundaries.This heuristic works remarkably well: over 99%of the PropBank arguments in the training corpushave a single token whose head is located outsideof the argument boundaries.
As a simple example,consider the following annotated text: [sold]PRED[1214 cars]ARG1[in the U.S.]ARGM-LOC.
Us-ing the above heuristic, the head of the ARG1 ar-gument is set to cars, because it has an OBJ de-pendency to sold, and the head of the ARGM-LOC argument is set to in, because it modifies soldthrough a LOC dependency.While this heuristic processes the vast majorityof arguments, there are several cases that requirespecial treatment.
We discuss these situations inthe remainder of this section.Arguments with several syntactic headsFor 0.7% of the semantic arguments, the aboveheuristic detects several syntactic heads for thegiven boundary.
For example, in the text [it]ARG0[expects]PRED[its U.S. sales to remain steadyat about 1200 cars]ARG1, the above heuris-tic assigns two syntactic heads to ARG1: sales,which modifies expects through an OBJ depen-dency, and to, which modifies expects through aPRD dependency.
These situations are causedby the constituent-to-dependency conversion pro-cess described in Section 3.2.1, which in somecases interprets syntax differently than the orig-inal Treebank annotation, e.g., the raising phe-nomenon for the PRD dependency in the aboveexample.
In such cases, we split the original argu-ment into a sequence of discontinuous arguments,e.g., the ARG1 in the above example becomes [itsU.S.
sales]ARG1[to remain steady at about 1200cars]C-ARG1.Merging discontinuous argumentsWhile in the above case we split arguments, thereare situations where we can merge arguments thatwere initially discontinuous in PropBank or Nom-Bank.
This typically happens when the Prop-Bank/NomBank predicate is infixed inside one ofits arguments.
For example, in the text [Million-dollar conferences]ARG1were [held]PRED[tochew on subjects such as... ]C-ARG1, PropBanklists multiple constituents as aggregately filling theARG1 slot of held.
These cases are detected au-tomatically because the least common ancestor ofthe argument pieces is actually one of the argumentsegments.
In the above example, to chew on sub-jects such as... depends on Million-dollar confer-ences because to modifies conferences through aNMOD dependency.
In these situations, we treatthe least common ancestor, e.g., conferences in theabove text, as the true argument.
This heuristic al-lowed us to merge 1665 (or 0.6% of total) argu-ments that were initially discontinuous in the Prop-Bank training corpus.Empty categoriesPropBank and NomBank both encode chains ofempty categories.
As with the 2005 shared task(Carreras and M`arquez, 2005), we used the headof the antecedent of empty categories as argumentsrather than empty categories.
Furthermore, emptycategory arguments with no antecedents were ig-nored.10For example, given The man wanted t tomake a speech, we assume that the A0 of make andspeech is man, rather than the chain consisting ofthe empty category represented as t and man.Annotation disagreementsNomBank and Penn Treebank annotators some-times disagree about constituent structure.
Nom-10Under our approach to filler gap constructions, the filleris a shared argument (as in Relational Grammar, most FeatureStructure and Dependency Grammar frameworks), in con-trast with the Penn Treebank?s empty category antecedent ap-proach (more closely resembling the various Chomskian ap-proaches).166Label Freq.
DescriptionNMOD 324834 Modifier of nominalP 135260 PunctuationPMOD 115988 Modifier of prepositionSBJ 89371 SubjectOBJ 66677 ObjectROOT 49178 RootADV 47379 General adverbialNAME 41138 Name-internal linkVC 35250 Verb chainCOORD 31140 CoordinationDEP 29456 UnclassifiedTMP 26305 Temporal adverbial or nominal modifierCONJ 24522 Second conjunct (dependent on conjunction)LOC 18500 Locative adverbial or nominal modifierAMOD 17868 Modifier of adjective or adverbialPRD 16265 Predicative complementAPPO 16163 AppositionIM 16071 Infinitive verb (dependent on infinitive marker to)HYPH 14073 Token part of a hyphenated word (dependent on a preceding part of the hyphenated word)HMOD 13885 Token inside a hyphenated word (dependent on the head of the hyphenated word)SUB 12995 Subordinated clause (dependent on subordinating conjuction)OPRD 11707 Predicative complement of raising/control verbSUFFIX 10548 Possessive suffix (dependent on possessor)DIR 6145 Adverbial of directionTITLE 5917 Title (dependent on name)MNR 4753 Adverbial of mannerPOSTHON 4377 Posthonorific modifier of nominalPRP 4013 Adverbial of purpose or reasonPRT 3235 Particle (dependent on verb)LGS 3115 Logical subject of a passive verbEXT 2374 Adverbial of extentPRN 2176 ParentheticalEXTR 658 Extraposed element in cleftDTV 496 Dative complement (to) in dative shiftPUT 271 Complement of the verb putBNF 44 Benefactor complement (for) in dative shiftVOC 24 VocativeTable 4: Statistics for atomic syntactic labels.Bank annotators are in effect assuming that theconstituents provided form a phrase.
In this case,the constituents are adjacent to each other.
For ex-ample, consider the NP the human rights discus-sion.
In this case, the Penn Treebank would treateach of the four words the, human, rights, discus-sion as daughters of a single NP node.
However,NomBank would treat human rights as a singleARG1 of discussion.
Since noun noun modifica-tion constructions are head final, we can easily de-termine (via GLARF) that rights is the markabledependent of discussion.Support chainsFinally, NomBank?s encoding of support chains ishandled as chains of dependencies in the data (al-though these are not scored).
For example, givenMary took dozens of walks, where Mary is theARG0 of walks, the support chain took + dozens +of is represented as a sequence of dependencies: ofdepends on Mary, dozens depends on of and tookdepends on dozens.
Each of these dependencies islabeled SU.3.3 Overview of CorporaThe syntactic dependency types are divided intoatomic types that consist of a single label, and non-atomic types consisting of more than one label.There are 38 atomic and 70 non-atomic labels inthe corpus.
There are three types of non-atomiclabels: those consisting of a PRD or OPRD con-catenated with an adverbial label such as LOC orTMP; gapping labels such as GAP-SBJ; and com-bined adverbial tags such as LOC-TMP.Table 4 shows statistics for the atomic syntac-tic dependencies: label type, the frequency of thelabel in the complete corpus, and a description ofthe label.
Table 5 shows the corresponding statis-tics for non-atomic dependencies, excluding gap-ping dependencies.
The non-atomic labels are rare,which made it difficult to learn these relations ef-167Label FrequencyLOC-PRD 798PRD-TMP 51PRD-PRP 45LOC-OPRD 31DIR-PRD 4MNR-PRD 3LOC-TMP 2MNR-TMP 1LOC-MNR 1DIR-OPRD 1Table 5: Statistics for non-atomic syntactic labelsexcluding gapping labels.Label FrequencyGAP-SBJ 116GAP-OBJ 102DEP-GAP 83GAP-TMP 69GAP-PRD 66GAP-LGS 44GAP-LOC 42DIR-GAP 37GAP-PMOD 22GAP-VC 20EXT-GAP 16ADV-GAP 15GAP-NMOD 13GAP-LOC-PRD 6DTV-GAP 6AMOD-GAP 6GAP-MNR 5GAP-PRP 4EXTR-GAP 3GAP-SUB 1GAP-PUT 1GAP-OPRD 1Table 6: Statistics for non-atomic labels containinga gapping label.fectively.
Table 6 shows the table for non-atomiclabels containing a gapping label.A dependency link wi?
wjis said to be pro-jective if all words occurring between wiand wjinthe surface word order are dominated by wi(wheredominance is the transitive closure of the directlink relation).
Nonprojective links are impossibleto handle for the search procedures in many typesof dependency parsers.
It has been previously ob-served that the majority of dependencies in all lan-guages are projective, and this is particularly truefor English ?
in the complete corpus, only 4118links (0.4%) are nonprojective.
3312 sentences, or7.6%, contain at least one nonprojective link.Table 7 shows statistics for different types ofnonprojective links: nonprojectivity caused bywh-movement, such as in Where are you going?or What have you done?
; split clauses such asType Frequencywh-movement 1709Split clause 734Split noun phrase 590Other 1085Table 7: Statistics for nonprojective links.POS FrequencyNN 68477NNS 30048VBD 24106VB 23650VBN 19339VBG 14245VBZ 10883VBP 6330Other 83Table 8: Statistics for predicates, by POS tags.Even to make love, he says, you need experience;split noun phrases such as hold a hearing tomor-row on the topic; and all other types of nonprojec-tive links.Lastly, Tables 8 and 9 summarizes statistics forsemantic predicates and roles.
Table 8 shows thenumber of non-support predicates with a givenPOS tag in the whole corpus (we used GPOS orPPOSS for predicates inside hyphenated words).The last line shows the number of predicates witha POS tag that does not start with NN or VB.
Thislast table entry is generated by POS tagger mis-takes when producing the PPOSS tags, or by errorsin our NomBank/PropBank conversion software.11Nevertheless, the overall picture given by the tableindicates that predicates are almost perfectly dis-tributed between nouns and verbs: there are 98525nominal and 98553 verbal predicates.Table 9 shows the number of arguments with agiven role label.
For brevity we list only labels thatare instantiated at least 10 times in the whole cor-pus.
The total number of arguments labeled with arole label with frequency lower than 10 is listedin the last line in the table.
The table indicatesthat, while the top three most common role labelsare ?core?
labels (A1, A0, A2), modifier arguments(AM-*) account for approximately 20% of the totalnumber of arguments.
On the other hand, discon-tinuous arguments are not common: only 0.7% ofthe total number of arguments have a continuationlabel (C-*).11In very few situations, we select incorrect head tokens formulti-word predicates.168Label FrequencyA1 161409A0 109437A2 51197AM-TMP 25913AM-MNR 13080AM-LOC 11409A3 10269AM-MOD 9986AM-ADV 9496AM-DIS 5369R-A0 4432AM-NEG 4097A4 3281C-A1 3118R-A1 2565AM-PNC 2445AM-EXT 1428AM-CAU 1346AM-DIR 1318R-AM-TMP 797R-A2 307R-AM-LOC 246R-AM-MNR 155A5 91AM-PRD 78C-A0 70C-A2 65R-AM-CAU 50C-A3 37R-A3 29C-AM-MNR 24C-AM-ADV 20AM-REC 16AA 14R-AM-PNC 12C-AM-EXT 11C-AM-TMP 11C-A4 11Frequency < 10 70Table 9: Statistics for semantic roles.4 Submissions and ResultsNineteen groups submitted test runs in the closedchallenge and five groups participated in the openchallenge.
Three of the latter groups participatedonly in the open challenge, and two of these sub-mitted results only for the semantic subtask.
Theseresults are summarized in Tables 10 and 11.Table 10 summarizes the official results ?
i.e.,results at evaluation deadline ?
for the closed chal-lenge.
Note that several teams corrected bugsand/or improved their systems and they submit-ted post-evaluation scores (accounted in the sharedtask website).
The table indicates that most of thetop results cluster together: three systems had alabeled macro F1score on the WSJ+Brown cor-pus around 82 points (che, ciaramita, and zhao);five systems scored around 79 labeled macro F1points (yuret, samuelsson, zhang, henderson, andwatanabe).
Remarkably, the top-scoring system(johansson) is in a class of its own, with scores2?3 points higher than the next system.
This ismost likely caused by the fact that Johansson andNugues (2008) implemented a thorough systemthat addressed all facets of the task with state-of-the-art methods: second-order parsing model, ar-gument identification/classification models sepa-rately tuned for PropBank and NomBank, rerank-ing inference for the SRL task, and, finally, jointoptimization of the complete task using meta-learning (more details in Section 5).Table 11 lists the official results in the open chal-lenge.
The results in this challenge are lower thanin the closed challenge, but this was somewhatto be expected considering that there were fewerparticipants in this challenge and none of the topfive groups in the closed challenge submitted re-sults in the open challenge.
Only one of the sys-tems that participated in both challenges (zhang)improved the results submitted in the closed chal-lenge.
Zhang et al (2008) achieved this by ex-tracting features for their semantic subtask mod-els both from the parser used in the closed chal-lenge and a secondary parser that was trained ona different corpus.
The improvements measuredwere relatively small for the in-domain WSJ cor-pus (0.2 labeled macro F1points) but larger for theout-of-domain Brown corpus (approximately 1 la-beled macro F1point).Tables 10 and 11 indicate that in both chal-lenges the results on the out-of-domain corpus(Brown) are much lower than the results measuredin-domain (WSJ).
The difference is around 7?8LAS points for the syntactic subtask and 12?14 la-beled F1points for semantic dependencies.
Over-all, this yields a drop of approximately 10 labeledmacro F1points for most systems.
This perfor-mance decrease on out-of-domain corpora is con-sistent with the results reported in CoNLL-2005on SRL (using the same Brown corpus).
Theseresults indicate that domain adaptation is a prob-lem that is far from being solved for both syntacticand semantic analysis of text.
Furthermore, as thescores on the syntactic and semantic subtasks in-dicate, domain adaptation becomes even harder asthe task to be solved gets more complex.We describe the participating systems in the nextsection.
Then, in Section 6, we revert to resultanalysis using different evaluation measures anddifferent views of the data.169Labeled Macro F1Labeled Attachment Score Labeled F1(complete task) (syntactic dependencies) (semantic dependencies)WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brownjohansson 84.86 (1) 85.95 75.95 89.32 (1) 90.13 82.81 80.37 (1) 81.75 69.06che 82.66 (2) 83.78 73.57 86.75 (5) 87.51 80.73 78.52 (2) 80.00 66.37ciaramita 82.06 (3) 83.25 72.46 86.60 (11) 87.47 79.67 77.50 (3) 79.00 65.24zhao 81.44 (4) 82.62 71.78 86.66 (8) 87.52 79.83 76.16 (4) 77.67 63.69yuret 79.84 (5) 80.97 70.55 86.62 (10) 87.39 80.46 73.06 (5) 74.54 60.62samuelsson 79.79 (6) 80.92 70.49 86.63 (9) 87.36 80.77 72.94 (6) 74.47 60.18zhang 79.32 (7) 80.41 70.48 87.32 (2) 88.14 80.80 71.31 (7) 72.67 60.16henderson 79.11 (8) 80.19 70.34 86.91 (4) 87.78 80.01 70.97 (8) 72.26 60.38watanabe 79.10 (9) 80.30 69.29 87.18 (3) 88.06 80.17 70.84 (9) 72.37 58.21morante 78.43 (10) 79.52 69.55 86.07 (12) 86.88 79.58 70.51 (10) 71.88 59.23li 78.35 (11) 79.38 70.01 86.69 (6) 87.42 80.8 69.95 (11) 71.27 59.17baldridge 77.49 (12) 78.57 68.53 86.67 (7) 87.42 80.64 67.92 (14) 69.35 55.95chen 77.00 (13) 77.95 69.23 84.47 (16) 85.20 78.58 69.45 (12) 70.62 59.81lee 76.90 (14) 77.96 68.34 84.82 (15) 85.69 77.83 68.71 (13) 69.95 58.63sun 76.28 (15) 77.10 69.58 85.75 (13) 86.37 80.75 66.61 (15) 67.62 58.26choi 71.23 (16) 72.22 63.44 77.56 (17) 78.58 69.46 64.78 (16) 65.72 57.4trandabat 63.45 (17) 64.21 57.41 85.21 (14) 85.96 79.24 40.63 (17) 41.36 34.75lluis 63.29 (18) 63.74 59.65 71.95 (18) 72.30 69.14 54.52 (18) 55.09 49.95neumann 19.93 (19) 20.13 18.14 16.25 (19) 16.22 16.47 22.36 (19) 22.86 17.94Table 10: Official results in the closed challenge (post-evaluation scores are available on the sharedtask website).
Teams are denoted by the last name of the first author of the corresponding paper inthe proceedings or the last name of the person who registered the team if no paper was submitted.Italics indicate that there is no corresponding paper in the proceedings.
Results are sorted in descendingorder of the labeled macro F1score on the WSJ+Brown corpus.
The number in parentheses next to theWSJ+Brown scores indicates the system rank in the corresponding task.Labeled Macro F1Labeled Attachment Score Labeled F1(complete task) (syntactic dependencies) (semantic dependencies)WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brownvickrey ?
?
?
?
?
?
76.17 (1) 77.38 66.23riedel ?
?
?
?
?
?
74.59 (2) 75.72 65.38zhang 79.61 (1) 80.61 71.45 87.32 (1) 88.14 80.80 71.89 (3) 73.08 62.11li 77.84 (2) 78.87 69.51 86.69 (2) 87.42 80.80 68.99 (4) 70.32 58.22wang 76.19 (3) 78.39 59.89 84.56 (3) 85.50 77.06 67.12 (5) 70.41 42.67Table 11: Official results in the open challenge (post-evaluation scores are available on the shared taskwebsite).
Teams are denoted by the last name of the first author of the corresponding paper in theproceedings or the last name of the person who registered the team if no paper was submitted.
Italicsindicate that there is no corresponding paper in the proceedings.
Results are sorted in descending order ofthe labeled F1score for semantic dependencies on the WSJ+Brown corpus.
The number in parenthesesnext to the WSJ+Brown scores indicates the system rank in the corresponding task.5 ApproachesTable 5 summarizes the properties of the sys-tems that participated in the closed the open chal-lenges.
The second column of the table high-lights the overall architectures.
We used + to in-dicate that the components are sequentially con-nected.
The lack of a + sign indicates that the cor-responding tasks are performed jointly.
For exam-ple, Riedel and Meza-Ruiz (2008) perform pred-icate and argument identification and classifica-tion jointly, whereas Ciaramita et al (2008) im-plemented a pipeline architecture of three compo-nents.
We use the || to indicate that several differ-ent architectures that span multiple subtasks weredeployed in parallel.This summary of system architectures indicatesthat it is common that systems combine sev-eral components in the semantic or syntactic sub-tasks ?
e.g., nine systems jointly performed pred-icate/argument identification and classification ?but only four systems combined components be-tween the syntactic and semantic subtasks: Hen-derson et al (2008), who implemented a generativehistory-based model (Incremental Sigmoid BeliefNetworks with vectors of latent variables) wheresyntactic and semantic structures are separately170generated but using a synchronized derivation (se-quence of actions); Samuelsson et al (2008),who, within an ensemble-based architecture, im-plemented a joint syntactic-semantic model usingMaltParser with labels enriched with semantic in-formation; Llu?
?s and M`arquez, who used a modi-fied version of the Eisner algorithm to jointly pre-dict syntactic and semantic dependencies; and fi-nally, Sun et al (2008), who integrated depen-dency label classification and argument identifi-cation using a maximum-entropy Markov model.Additionally, Johansson and Nugues (2008), whohad the highest ranked system in the closed chal-lenge, integrate syntactic and semantic analysis ina final reranking step, which maximizes the jointsyntactic-semantic score in the top k solutions.
Inthe same spirit, Chen et al (2008) search in thetop k solutions for the one that maximizes a globalmeasure, in this case the joint probability of thecomplete problem.
These joint learning strategiesare summarized in the Joint Learning/Opt.
col-umn in the table.
The system of Riedel and Meza-Ruiz (2008) deserves a special mention: eventhough Riedel and Meza-Ruiz did not implementa syntactic parser, they are the only group that per-formed the complete SRL subtask ?
i.e., predicateidentification and classification, argument identifi-cation and classification ?
jointly, simultaneouslyfor all the predicates in a sentence.
They imple-mented a joint SRL model using Markov LogicNetworks and they selected the overall best solu-tion using inference based on the cutting-plane al-gorithm.Although some of the systems that implementedjoint approaches obtained good results, the topfive systems in the closed challenge are essen-tially systems with pipeline architectures.
Further-more, Johansson and Nugues (2008) and Riedeland Meza-Ruiz (2008) showed that joint learn-ing/optimization improves the overall results, butthe improvement is not large.
These initial ef-forts indicate at least that the joint modeling of thisproblem is not a trivial task.The D Arch.
and D Inference columns summa-rize the parsing architectures and the correspond-ing inference strategies.
Similar to last year?sshared task (Nivre et al, 2007), the vast majority ofparsing models fall in two classes: transition-based(?trans?
in the table) or graph-based (?graph?)models.
By and large, transition-based models usea greedy inference strategy, whereas graph-basedmodels used different Maximum Spanning Tree(MST) algorithms: Carreras (2007) ?
MSTC, Eis-ner (2000) ?
MSTE, or Chu-Liu/Edmonds (Mc-Donald et al, 2005; Chu and Liu, 1965; Edmonds,1967) ?
MSTCL/E.
More interestingly, most ofthe best systems used some strategy to mitigateparsing errors.
In the top three systems in theclosed challenge, two (che and ciaramita) usedparser combination through voting and/or stackingof different models (see the D Comb.
column).Samuelsson et al (2008) perform a MST infer-ence with the bag of all dependencies output bythe individual MALT parser variants.
Johanssonand Nugues (2008) use a single parsing model, butthis model is extended with second-order features.The PA Arch.
and PA Inference columns sum-marize the architectures and inference strategiesused for the identification and classification ofpredicates and arguments.
The columns indicatethat most systems modeled the SRL problem as atoken-by-token classification problem (?class?
inthe table) with a corresponding greedy inferencestrategy.
Some systems (e.g., yuret, samuelsson,henderson, lluis) incorporate SRL within parsing,in which case we report the corresponding parsingarchitecture and inference approach.
Vickrey andKoller (2008) simplify the sentences to be labeledusing a set of hand-crafted rules before deployinga classification model on top of a constituent-basedrepresentation.
Unlike in the case of parsing, fewsystems (yuret, samuelssson, and morante) com-bine several PA models and the combination is lim-ited to simple voting strategies (see the PA Comb.column).Finally, the ML Methods column lists the Ma-chine Learning (ML) methods used.
The columnindicates that maximum entropy (ME) was themost popular method (12 distinct systems reliedon it).
Support Vector Machines (SVM) (eight sys-tems) and the Perceptron algorithm (three systems)were also popular ML methods.6 AnalysisSection 4 summarized the results in the closedand open challenges using the official evaluationmeasures.
In this section, we analyze the sub-mitted runs using different evaluation measures,e.g., Exact Match or Perfect Proposition F1scores,and different views of the data, e.g., only non-projective dependencies or NomBank versus Prop-Bank frames.171OverallDDDPAPAPAJointMLclosedArch.Arch.Comb.InferenceArch.Comb.InferenceLearning/Opt.MethodsjohanssonD+PI+PC+AI+ACgraphnoMSTCclassnorerankrerankPerceptron,MEcheD+PI+PC+AICgraphstackingMSTCL/EclassnoILPnoMEciaramitaD+PIC+AICtransvoting,greedyclassnoreranknoSVM,ME,stackingPerceptronzhaoD+AIC+PI+PCtransnogreedyclassnogreedynoMEyuretD+(PIC+AI+AC||graphnoMSTEclass,votinggreedynoMLE,PIC+AIC)generativeMBLsamuelssonD+PI+transMSTCL/Egreedyclass,votinggreedyunifiedSVM(AI+AC||DAIC)+PCblendingtranslabelszhangD+PI+AI+AC+PCgraph,meta-MSTCL/E,classnogreedynoSVM,MEtranslearninggreedyhendersonDPAIC+Dgenerative,nobeamtransnobeamsynchronizedISBNtranssearchsearchderivationwatanabeDI+DC+PI+PC+AI+ACrelativepreferencenogreedytournamentclassnononoSVM,modelmodel,ViterbiCRF,MBLmoranteD+PI+AI+ACtransnogreedyclassvotinggreedynoSVM,MBLliD+PIC+AICgraphnoMSTCL/EclassvotinggreedynoMEchenD+PI+PC+AICtransnoprobclassnoprobglobalprobabilityMEoptimizationleeD+PI+AIC+PCtransnogreedyclassnoprobnoSVM,MEsunDI+PI+DCAI+ACgraphnoMSTE,graphnoViterbi,MEMM,MEViterbiILPViterbilluisD+PI+DAIC+PCgraphnoMSTEgraphnoMSTEMSTEPerceptron,SVMneumannD+PI+PC+AI+ACtransnogreedyclassnononoMEopenvickreyAI+AC+PI+PC???sentencenogreedy?MEsimplification,classriedelPAIC??
?MarkovnoCutting?MIRALogicPlaneNetworkwangPI+AICtrans,nogreedy,classnoprobnoSVM,MEgraphMSTCL/EMIRATable12:Summaryofsystemarchitecturesthatparticipatedintheclosedandopenchallenges.Theclosed-challengesystemsaresortedbymacrolabeledF1scoreontheWSJ+Browncorpus.Becausesomeopen-challengesystemsdidnotimplementsyntacticparsing,thesesystemsaresortedbylabeledF1scoreofthesemanticdependenciesontheWSJ+Browncorpus.Onlythesystemsthathaveacorrespondingpaperintheproceedingsareincluded.Systemsthatparticipatedinbothchallengesarelistedonlyintheclosedchallenge.Acronymsused:D-syntacticdependencies,P-predicate,A-argument,I-identification,C-classification.Overallarch.standsforthecompletesystemarchitecture;DArch.standsforthearchitectureofthesyntacticparser;DComb.indicatesifthefinalparseroutputwasgeneratedusingparsercombination;DInferencestandsforthetypeofinferenceusedforsyntacticparsing;PAArch.standsthetypeofarchitectureusedforPAIC;PAComb.indicatesifthePAoutputwasgeneratedthroughsystemcombination;PAInferencestandsforthethetypeofinferenceusedforPAIC;JointLearning/Opt.indicatesifsomeformofjointlearningoroptimizationwasimplementedforthesyntactic+semanticglobaltask;MLmethodsliststheMLmethodsusedthroughoutthecompletesystem.172Exact Match Perfect Proposition F1(complete task) (semantic dependencies)closed WSJ+Brown WSJ Brown WSJ+Brown WSJ Brownjohansson 12.46 (1) 12.46 12.68 54.12 (1) 56.12 36.90che 10.37 (2) 10.21 11.50 48.05 (2) 50.15 30.90ciaramita 9.27 (3) 9.04 10.80 46.05 (3) 48.05 28.61zhao 9.20 (4) 9.00 10.56 43.19 (4) 45.23 26.14henderson 8.11 (5) 7.75 10.33 39.24 (5) 40.64 27.51watanabe 7.79 (6) 7.54 9.39 36.44 (6) 38.09 22.72yuret 7.65 (7) 7.33 9.62 34.61 (9) 36.13 21.78zhang 7.40 (8) 7.46 7.28 34.96 (8) 36.25 24.22li 7.12 (9) 6.71 9.62 32.08 (10) 33.45 20.62samuelsson 6.94 (10) 6.62 8.92 35.20 (7) 36.96 20.22chen 6.83 (11) 6.46 9.15 31.02 (12) 32.08 22.14lee 6.69 (12) 6.29 9.15 31.40 (11) 32.52 22.18morante 6.44 (13) 6.04 8.92 30.41 (14) 31.97 17.49sun 5.38 (14) 4.96 7.98 30.43 (13) 31.51 21.40baldridge 5.24 (15) 4.92 7.28 25.35 (15) 26.57 15.26choi 3.33 (16) 3.50 2.58 24.77 (16) 25.71 17.37trandabat 3.26 (17) 3.08 4.46 6.59 (18) 6.81 4.76lluis 2.55 (18) 1.96 6.10 16.07 (17) 16.46 13.00neumann 0.11 (19) 0.12 0.23 0.30 (19) 0.31 0.20openvickrey ?
?
?
44.94 (1) 46.68 30.28riedel ?
?
?
42.77 (2) 44.18 31.15zhang 8.14 (1) 8.04 8.92 35.46 (3) 36.74 24.84li 6.90 (2) 6.46 9.62 29.91 (4) 31.30 18.41wang 5.17 (3) 5.12 5.63 18.63 (5) 20.31 7.09Table 13: Exact Match and Perfect Proposition F1scores for runs submitted in the closed and openchallenges.
The closed-challenge systems are sorted in descending order of Exact Match scores onthe WSJ+Brown corpus.
Open-challenge submissions are sorted in descending order of the PerfectProposition F1score.
The number in parentheses next to the WSJ+Brown scores indicates the systemrank according to the corresponding scoring measure.6.1 Exact Match and Perfect PropositionsTable 13 lists the Exact Match and Perfect Propo-sition F1scores for test runs submitted in bothchallenges.
Both these scores measure the capac-ity of a system to correctly parse structures withgranularity much larger than a simple dependency,i.e., entire sentences for Exact Match and completepropositions for Perfect Proposition F1(see Sec-tion 2.2.2 for a formal definition of these evalua-tion measures).
The table indicates that these val-ues are much smaller than the scores previouslyreported, e.g., labeled macro F1.
This is to beexpected: the probability of an incorrectly parsedunit (sentence or proposition) is much larger givenits granularity.
However, the main purpose of thisanalysis is to investigate if systems that focusedon joint learning or optimization performed bet-ter than others with respect to these global mea-sures.
This indeed seems to be the case for atleast two systems.
The system of Johansson andNugues (2008), which jointly optimizes the la-beled F1score (for semantic dependencies) andthen the labeled macro F1score (for the completetask), increases its distance from the next rankedsystem: its Perfect Proposition F1score is over6 points higher than the score of the second sys-tem in Table 13.
The system of Henderson etal.
(2008), which was designed for joint learningof the complete task, improves its rank from eighthto fifth compared to the official results (Table 10).6.2 NonprojectivityTable 14 shows the unlabeled F1 scores for pre-diction of nonprojective syntactic dependencies.Since nonprojectivity is quite rare, many teamschose to ignore this issue.
The table shows onlythose systems that submitted well-formed depen-dency trees, and whose output contained at leastone nonprojective link.
The small number of non-projective links in the training set makes it hard tolearn to predict such links, and this is also reflectedin the figures.
In general, the figures for nonpro-jective wh-movements and split clauses are higher,and they are also the most common types.
Also,they are detectable by fairly simple patterns, suchas the presence of a wh-word or a pair of commas.173System All wh-mov.
SpCl SpNPchoi 25.43 49.49 45.47 8.72lee 46.26 50.30 64.84 20.69nugues 46.15 58.96 59.26 11.32samuelsson 24.47 38.15 0 9.83titov 42.32 50.56 48.71 0zhang 13.39 5.71 12.33 7.3Table 14: Unlabeled F1-measures for nonprojec-tive links.
Results are given for all links, wh-movements, split clauses, and split noun phrases.6.3 Normalized SRL PerformanceTable 6.3 lists the scores for the semantic sub-task measured as the ratio of the labeled F1scoreand LAS.
As previously mentioned, this score es-timates the performance of the SRL componentindependent of the performance of the syntacticparser.
This analysis is not a substitute for theactual experiment where the SRL components areevaluated using correct syntactic information but,nevertheless, it indicates several interesting facts.First, the ranking of the top three systems in Ta-ble 10 changes: the system of Che et al (2008)is now ranked first, and the system of Johanssonand Nugues (2008) is second.
This shows that Cheet al have a relatively stronger SRL component,whereas Johansson and Nugues developed a bet-ter parser.
Second, several other systems improvedtheir ranking compared to Table 10: e.g., chenfrom position thirteenth to ninth and choi from six-teenth to eighth.
This indicates that these systemswere penalized in the official ranking mainly dueto the relative poor performance of their parsers.Note that this experiment is relevant only forsystems that implemented pipeline architectures,where the semantic components are in fact sep-arated from the syntactic ones; this excludes thesystems that blended syntax with SRL: henderson,sun, and lluis.
Furthermore, systems that had sig-nificantly lower scores in syntax will receive an un-reasonable boost in ranking according to this mea-sure.
Fortunately, there was only one such outlierin this evaluation (neumann), shown in gray in thetable.6.4 PropBank versus NomBankTable 16 lists the labeled F1scores for semanticdependencies for two different views of the test-ing data sets: for propositions centered around ver-bal predicates, i.e., from PropBank, and for propo-sitions centered around nominal predicates, i.e.,from NomBank.Labeled F1/ LASclosed WSJ+Brown WSJ Brownneumann 137.60 (1) 140.94 108.93che 90.51 (2) 91.42 82.21johansson 89.98 (3) 90.70 83.40ciaramita 89.49 (4) 90.32 81.89zhao 87.88 (5) 88.75 79.78yuret 84.35 (6) 85.30 75.34samuelsson 84.20 (7) 85.24 74.51choi 83.52 (8) 83.63 82.64chen 82.22 (9) 82.89 76.11morante 81.92 (10) 82.73 74.43zhang 81.67 (11) 82.45 74.46henderson 81.66 (12) 82.32 75.47watanabe 81.26 (13) 82.18 72.61lee 81.01 (14) 81.63 75.33li 80.69 (15) 81.53 73.23baldridge 78.37 (16) 79.33 69.38sun 77.68 (17) 78.29 72.15lluis 75.77 (18) 76.20 72.24trandabat 47.68 (19) 48.12 43.85openzhang 82.33 82.91 76.87li 79.58 80.44 72.05wang 79.38 82.35 55.37Table 15: Ratio of the labeled F1score for seman-tic dependencies and LAS for syntactic dependen-cies.
Systems are sorted in descending order of thisratio score on the WSJ+Brown corpus.
We onlyshow systems that participated in both the syntac-tic and semantic subtasks.The table indicates that, generally, systems per-formed much worse on nominal predicates thanon verbal predicates.
This is to be expected con-sidering that there is significant body of previ-ous work that analyzes the SRL problem on Prop-Bank, but minimal work for NomBank.
On aver-age, the difference between the labeled F1scoresfor verbal predicates and nominal predicates on theWSJ+Brown corpus is 7.84 points.
Furthermore,the average difference between labeled F1scoreson the Brown corpus alone is 12.36 points.
This in-dicates that the problem of SRL for nominal predi-cates is more sensitive to domain changes than theequivalent problem for verbal predicates.
Our con-jecture is that, because there is very little syntac-tic structure between nominal predicates and theirarguments, SRL models for nominal predicates se-lect mainly lexical features, which are more brittlethan syntactic or other non-lexicalized features.Remarkably, there is one system (baldridge)which performed better on the WSJ+Brown fornominal predicates than verbal predicates.
Un-fortunately, this group did not submit a system-description paper so it is not clear what was theirapproach.174Labeled F1Labeled F1(verbal predicates) (nominal predicates)closed WSJ+Brown WSJ Brown WSJ+Brown WSJ Brownjohansson 84.45 (1) 86.37 71.87 74.32 (2) 75.42 60.13che 80.46 (2) 82.17 69.33 75.18 (1) 76.64 56.87ciaramita 80.15 (3) 82.09 67.62 73.17 (4) 74.42 57.69zhao 77.67 (4) 79.40 66.38 73.28 (3) 74.69 54.81samuelsson 76.17 (5) 78.03 64.00 68.13 (7) 69.58 49.24yuret 75.91 (6) 77.88 63.02 68.81 (5) 69.98 53.58zhang 74.82 (7) 76.62 63.15 65.61 (11) 66.82 50.18li 74.36 (8) 76.14 62.92 62.61 (14) 63.76 47.09henderson 73.80 (9) 75.40 63.36 66.26 (10) 67.44 50.73watanabe 73.06 (10) 75.02 60.34 67.15 (8) 68.37 50.92sun 72.97 (11) 74.45 63.50 58.68 (15) 59.73 45.75morante 72.81 (12) 74.36 62.72 66.50 (9) 67.92 47.97lee 72.34 (13) 74.15 60.49 62.83 (13) 63.66 52.18chen 72.02 (14) 73.49 62.46 65.02 (12) 66.14 50.48choi 70.00 (15) 71.28 61.71 56.16 (16) 57.19 44.05baldridge 67.02 (16) 68.64 56.50 68.57 (6) 69.78 52.96lluis 62.42 (17) 63.49 55.49 42.15 (17) 42.81 34.22trandabat 42.88 (18) 43.79 37.06 37.14 (18) 37.89 27.50neumann 22.87 (19) 23.53 18.24 21.7 (19) 22.04 17.14openvickrey 78.41 (1) 79.75 69.57 71.86 (1) 73.29 53.25riedel 77.13 (2) 78.72 66.75 70.25 (2) 71.03 60.17zhang 75.00 (3) 76.62 64.44 66.76 (3) 67.79 53.76li 73.74 (4) 75.57 62.05 61.24 (5) 62.38 46.36wang 67.50 (5) 70.34 49.72 66.53 (4) 69.83 28.96Table 16: Labeled F1scores for frames centered around verbal and nominal predicates.
The number inparentheses next to the WSJ+Brown scores indicates the system rank in the corresponding data set.Systems can mitigate the inherent differencesbetween verbal and nominal predicates with dif-ferent models for the two sub-problems.
This wasindeed the approach taken by two out of the topthree systems (johansson and che).
Johansson andNugues (2008) developed different models for ver-bal and nominal predicates and implemented sep-arate feature selection processes for each model.Che et al (2008) followed the same method butthey also implemented separate domain constraintsfor inference for the two models.7 ConclusionThe previous four CoNLL shared tasks popular-ized and, without a doubt, boosted research in se-mantic role labeling and dependency parsing.
Thisyear?s shared task introduces a new task that es-sentially unifies the problems addressed in the pastfour years under a unique, dependency-based for-malism.
This novel task is attractive both froma research perspective and an application-orientedperspective:?
We believe that the proposed dependency-based representation is a better fit for manyapplications (e.g., Information Retrieval, In-formation Extraction) where it is often suffi-cient to identify the dependency between thepredicate and the head of the argument con-stituent rather than extracting the complete ar-gument constituent.?
It was shown that the extraction of syntac-tic and semantic dependencies can be per-formed with state-of-the-art performance inlinear time (Ciaramita et al, 2008).
This cangive a significant boost to the adoption of thistechnology in real-world applications.?
We hope that this shared task will motivateseveral important research directions.
For ex-ample, is the dependency-based representa-tion better for SRL than the constituent-basedformalism?
Does joint learning improve syn-tactic and semantic analysis??
Surface (string related patterns, syntax, etc.
)linguistic features can often be detected withgreater reliability than deep (semantic) fea-tures.
In contrast, deep features can covermore ground because they regularize acrossdifferences in surface strings.
Machine learn-ing systems can be more effective by usingevidence from both deep and surface featuresjointly (Zhao, 2005).175Even though this shared task was more complexthan the previous shared tasks, 22 different teamssubmitted results in at least one of the challenges.Building on this success, we hope to expand thiseffort in the future with evaluations on multiplelanguages and on larger out-of-domain corpora.AcknowledgmentsWe want to thank the following people who helpedus with the generation of the data sets: Jes?usGim?enez, for generating the predicted POS tagswith his SVMTool POS tagger, and MassimilianoCiaramita, for generating columns 1, 2 and 3 in theopen-challenge corpus with his semantic tagger.We also thank the following people who helpedus with the organization of the shared task: PaolaMerlo and James Henderson for the idea and theimplementation of the Exact Match measure, Se-bastian Riedel for his dependency visualizationsoftware,12Hai Zhao, for the the idea of the F1ratio score, and Carlos Castillo, for help with theshared task website.
Last but not least, we thankthe organizers of the previous four shared tasks:Sabine Buchholz, Xavier Carreras, Ryan McDon-ald, Amit Dubey, Johan Hall, Yuval Krymolowski,Sandra K?ubler, Erwin Marsi, Jens Nilsson, Sebas-tian Riedel, and Deniz Yuret.
This shared taskwould not have been possible without their previ-ous effort.Mihai Surdeanu is a research fellow in theRam?on y Cajal program of the Spanish Ministry ofScience and Technology.
Richard Johansson wasfunded by the Swedish National Graduate Schoolof Language Technology (GSLT).
Adam Meyers?participation was supported by the National Sci-ence Foundation, award CNS-0551615 (Towardsa Comprehensive Linguistic Annotation of Lan-guage) and IIS-0534700 (Collaborative Research:Structure Alignment-based Machine Translation).Llu?
?s M`arquez?s participation was supported bythe Spanish Ministry of Education and Science,through research projects Trangram (TIN2004-07925-C03-02) and OpenMT (TIN2006-15307-C03-02).ReferencesX.
Carreras.
2007.
Experiments with a Higher-OrderProjective Dependency Parser.
In Proc.
of CoNLL-2007 Shared Task.12http://code.google.com/p/whatswrong/X.
Carreras and L. M`arquez.
2005.
Introduction to theCoNLL-2005 Shared Task: Semantic Role Labeling.In Proc.
of CoNLL-2005.X.
Carreras and L. M`arquez.
2004.
Introduction to theCoNLL-2004 Shared Task: Semantic Role Labeling.In Proc.
of CoNLL-2004.W.
Che, Z. Li, Y. Hu, Y. Li, B. Qin, T. Liu and S.Li.
2008.
A Cascaded Syntactic and Semantic De-pendency Parsing System.
In Proc.
of CoNLL-2008Shared Task.E.
Chen, L. Shi and D. Hu.
2008.
Probabilistic Modelfor Syntactic and Semantic Dependency Parsing.
InProc.
of CoNLL-2008 Shared Task.Chinchor, N. and P. Robinson.
1998.
MUC-7Named Entity Task Definition.
In Proc.
of SeventhMessage Understanding Conference (MUC-7).http://www.itl.nist.gov/iaui/894.02/related projects//muc/proceedings/muc 7 toc.html.Chomsky, Noam.
1981.
Lectures on Government andBinding.
Foris Publications, Dordrecht.Y.J.
Chu and T.H.
Liu.
1965.
On the Shortest Ar-borescence of a Directed Graph.
In Science Sinica,14:1396-1400.M.
Ciaramita, G. Attardi, F. Dell?Orletta, and M. Sur-deanu.
2008.
DeSRL: A Linear-Time SemanticRole Labeling System.
In Proc.
of CoNLL-2008Shared Task.M.
Ciaramita and Y. Altun.
2006.
Broad CoverageSense Disambiguation and Information Extractionwith a Supersense Sequence Tagger.
In Proc.
ofEMNLP.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.J.
Edmonds.
1967.
Optimum Branchings.
In Jour-nal of Research of the National Bureau of Standards,71B:233?240.J.
Eisner.
2000.
Bilexical Grammars and Their Cubic-Time Parsing Algorithms.
New Developments inParsing Algorithms, Kluwer Academic Publishers.W.
N. Francis and H. Ku?cera.
1964.
Brown Corpus.Manual of Information to accompany A StandardCorpus of Present-Day Edited American English, foruse with Digital Computers.
Revised 1971, Revisedand Amplified 1979.C.
Fellbaum, editor.
1998.
WordNet: An electroniclexical database.
MIT Press.J.
Gim?enez and L. M`arquez.
2004.
SVMTool: A gen-eral POS tagger generator based on Support VectorMachines.
In Proc.
of LREC.K.
Hacioglu.
2004.
Semantic Role Labeling Using De-pendency Trees.
In Proc.
of COLING-2004.176J.
Henderson, P. Merlo, G. Musillo and I. Titov.
2008.A Latent Variable Model of Synchronous Parsing forSyntactic and Semantic Dependencies.
In Proc.
ofCoNLL-2008 Shared Task.R.
Johansson and P. Nugues.
2008.
Dependency-based Syntactic?Semantic Analysis with PropBankand NomBank.
In Proc.
of CoNLL-2008 SharedTask.R.
Johansson and P. Nugues.
2007.
ExtendedConstituent-to-Dependency Conversion for English.In Proc.
of NODALIDA.X.
Llu?
?s and L. M`arquez.
2008.
A Joint Model forParsing Syntactic and Semantic Dependencies.
InProc.
of CoNLL-2008 Shared Task.D.
Magerman.
1994.
Natural Language Parsing as Sta-tistical Pattern Recognition.
Ph.D. thesis, StanfordUniversity.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a Large Annotated Corpus of En-glish: the Penn Treebank.
Computational Linguis-tics, 19.R.
McDonald, F. Pereira, K. Ribarov and J. Hajic.2005.
Non-Projective Dependency Parsing usingSpanning Tree Algorithms In Proc.
of HLT-EMNLP.A.
Meyers, R. Grishman, M. Kosaka, and S. Zhao.2001.
Covering Treebanks with GLARF.
In Proc.of the ACL/EACL 2001 Workshop on Sharing Toolsand Resources for Research and Education.Meyers, A., R. Reeves, C. Macleod, R. Szekely,V.
Zielinska, B.
Young, and R. Grishman.
2004.The NomBank Project: An Interim Report.
InNAACL/HLT 2004 Workshop Frontiers in CorpusAnnotation, Boston.J.
Nivre, J.
Hall, J. Nilsson and G. Eryigit.
2006.
La-beled Pseudo-Projective Dependency Parsing withSupport Vector Machines.
In Proc.
of CoNLL-XShared Task.J.
Nivre, J.
Hall, S. K?ubler, R. McDonald, J. Nilsson,S.
Riedel, D. Yuret.
2007.
The CoNLL 2007 SharedTask on Dependency Parsing.
In Proc.
of CoNLL-2007.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryi?git, S.K?ubler, S. Marinov, and E. Marsi.
2007b.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language En-gineering, 13(2):95?135.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An Annotated Corpus of Seman-tic Roles.
Computational Linguistics, 31(1).S.
Riedel and I. Meza-Ruiz.
2008.
Collective Seman-tic Role Labelling with Markov Logic.
In Proc.
ofCoNLL-2008 Shared Task.Y.
Samuelsson, O. T?ackstr?om, S. Velupillai, J. Eklund,M.
Fishel and M. Saers.
2008.
Mixing and BlendingSyntactic and Semantic Dependencies.
In Proc.
ofCoNLL-2008 Shared Task.W.
Sun, H. Li and Z. Sui.
2008.
The Integration of De-pendency Relation Classification and Semantic RoleLabeling Using Bilayer Maximum Entropy MarkovModels.
In Proc.
of CoNLL-2008 Shared Task.E.
F. Tjong Kim San and F. De Meulder.
2003.
Intro-duction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.
In Proc.
ofCoNLL-2003.D.
Vickrey and D. Koller.
2008.
Applying SentenceSimplification to the CoNLL-2008 Shared Task.
InProc.
of CoNLL-2008 Shared Task.R.
Weischedel and A. Brunstein.
2005.
BBN pronouncoreference and entity type corpus.
Technical report,Linguistic Data Consortium.H.
Yamada and Y. Matsumoto.
2003.
Statistical De-pendency Analysis with Support Vector Machines.In Proc.
of IWPT.Y.
Zhang, R. Wang and H. Uszkoreit.
2008.
HybridLearning of Dependency Structures from Heteroge-neous Linguistic Resources.
In Proc.
of CoNLL-2008 Shared Task.Zhao, S. 2005.
Information Extraction from MultipleSyntactic Sources.
Ph.D. thesis, NYU.177
