BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 63?70,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsHow to Make the Most of NE Dictionaries in Statistical NERYutaka Sasaki2 Yoshimasa Tsuruoka2 John McNaught1,2 Sophia Ananiadou1,21 National Centre for Text Mining2 School of Computer Science, University of ManchesterMIB, 131 Princess Street, Manchester, M1 7DN, UKAbstractWhen term ambiguity and variability are veryhigh, dictionary-based Named Entity Recogni-tion (NER) is not an ideal solution even thoughlarge-scale terminological resources are avail-able.
Many researches on statistical NER havetried to cope with these problems.
However,it is not straightforward how to exploit exist-ing and additional Named Entity (NE) dictio-naries in statistical NER.
Presumably, addi-tion of NEs to an NE dictionary leads to bet-ter performance.
However, in reality, the re-training of NER models is required to achievethis.
We have established a novel way to im-prove the NER performance by addition ofNEs to an NE dictionary without retraining.We chose protein name recognition as a casestudy because it most suffers the problems re-lated to heavy term variation and ambiguity.In our approach, first, known NEs are identi-fied in parallel with Part-of-Speech (POS) tag-ging based on a general word dictionary andan NE dictionary.
Then, statistical NER istrained on the tagger outputs with correct NElabels attached.
We evaluated performance ofour NER on the standard JNLPBA-2004 dataset.
The F-score on the test set has been im-proved from 73.14 to 73.78 after adding theprotein names appearing in the training data tothe POS tagger dictionary without any modelretraining.
The performance further increasedto 78.72 after enriching the tagging dictionarywith test set protein names.
Our approachhas demonstrated high performance in pro-tein name recognition, which indicates howto make the most of known NEs in statisticalNER.1 IntroductionThe accumulation of online biomedical informa-tion has been growing at a rapid pace, mainly at-tributed to a rapid growth of a wide range of repos-itories of biomedical data and literature.
The auto-matic construction and update of scientific knowl-edge bases is a major research topic in Bioinformat-ics.
One way of populating these knowledge basesis through named entity recognition (NER).
Unfortu-nately, biomedical NER faces many problems, e.g.,protein names are extremely difficult to recognizedue to ambiguity, complexity and variability.
A fur-ther problem in protein name recognition arises atthe tokenization stage.
Some protein names includepunctuation or special symbols, which may cause to-kenization to lose some word concatenation infor-mation in the original sentence.
For example, IL-2and IL - 2 fall into the same token sequence IL- 2 as usually dash (or hyphen) is designated as atoken delimiter.Research into NER is centred around three ap-proaches: dictionary-based, rule-based and machinelearning-based approaches.
To overcome the usualNER pitfalls, we have opted for a hybrid approachcombining dictionary-based and machine learningapproaches, which we call dictionary-based statisti-cal NER approach.
After identifying protein namesin text, we link these to semantic identifiers, such asUniProt accession numbers.
In this paper, we focuson the evaluation of our dictionary-based statisticalNER.2 MethodsOur dictionary-based statistical approach consists oftwo components: dictionary-based POS/PROTEINtagging and statistical sequential labelling.
First,63dictionary-based POS/PROTEIN tagging finds can-didates for protein names using a dictionary.
Thedictionary maps strings to parts of speech (POS),where the POS tagset is augmented with a tagNN-PROTEIN.
Then, sequential labelling appliesto reduce false positives and false negatives in thePOS/PROTEIN tagging results.
Expandability issupported through allowing a user of the NER tool toimprove NER coverage by adding entries to the dic-tionary.
In our approach, retraining is not requiredafter dictionary enrichment.Recently, Conditional Random Fields (CRFs)have been successfully applied to sequence labellingproblems, such as POS tagging and NER, and haveoutperformed other machine learning techniques.The main idea of CRFs is to estimate a conditionalprobability distribution over label sequences, ratherthan over local directed label sequences as with Hid-den Markov Models (Baum and Petrie, 1966) andMaximum Entropy Markov Models (McCallum etal., 2000).
Parameters of CRFs can be efficientlyestimated through the log-likelihood parameter esti-mation using the forward-backward algorithm, a dy-namic programming method.2.1 Training and test dataExperiments were conducted using the training andtest sets of the JNLPBA-2004 data set(Kim et al,2004).Training data The training data set used inJNLPBA-2004 is a set of tokenized sentences withmanually annotated term class labels.
The sentencesare taken from the Genia corpus (version 3.02) (Kimet al, 2003), in which 2,000 abstracts were manu-ally annotated by a biologist, drawing on a set ofPOS tags and 36 biomedical term classes.
In theJNLPBA-2004 shared task, performance in extract-ing five term classes, i.e., protein, DNA, RNA, cellline, and cell type classes, were evaluated.Test Data The test data set used in JNLPBA-2004is a set of tokenized sentences extracted from 404separately collected MEDLINE abstracts, where theterm class labels were manually assigned, followingthe annotation specification of the Genia corpus.2.2 Overview of dictionary-based statisticalNERFigure 1 shows the block diagram of dictionary-based statistical NER.
Raw text is analyzed bya POS/PROTEIN tagger based on a CRF taggingFigure 1: Block diagram of dictionary-based statisticalNERFigure 2: Block diagram of training proceduremodel and dictionary, and then converted into to-ken sequences.
Strings in the text that match withprotein names in the dictionary will be tagged asNN-PROTEIN depending on the context around theprotein names.
Since it is not realistic to enumer-ate all protein names in the dictionary, due to theirhigh variability of form, instead previously unseenforms are predicted to be protein names by statisti-cal sequential labelling.
Finally, protein names areidentified from the POS/PROTEIN tagged token se-quences via a CRF labelling model.Figure 2 shows the block diagram of the train-ing procedure for both POS/PROTEIN tagging andsequential labelling.
The tagging model is createdusing the Genia corpus (version 3.02) and a dic-tionary.
Using the tagging model, MEDLINE ab-stracts used for the JNLPBA-2004 training data setare then POS/PROTEIN-tagged.
The output tokensequences over these abstracts are then integratedwith the correct protein labels of the JNLPBA-2004training data.
This process results in the preparationof token sequences with features and correct proteinlabels.
A CRF labelling model is finally generatedby applying a CRF tool to these decorated token se-quences.64IL/NNP-/- 2/CD-/-mediated/VVDmediated/VVNactivation/NNIL-2/NN-PROTEINIL-2/NN-PROTEIN-/-2/CDmediated/VVNmediated/VVDmediate/VVPmediate/VVactivation/NNIL/NNPIL-2-mediated activation ...POS/PROTEIN taggingLexiconFigure 3: Dictionary based approach2.2.1 Dictionary-based POS/PROTEIN taggingThe dictionary-based approach is beneficial whena sentence contains some protein names that con-flict with general English words.
Otherwise, if thePOS tags of sentences are decided without consider-ing possible occurrences of protein names, POS se-quences could be disrupted.
For example, in ?metproto-oncogene precursor?, met might be falselyrecognized as a verb by a non dictionary-based tag-ger.Given a sentence, the dictionary-based approachextracts protein names as follows.
Find all word se-quences that match the lexical entries, and create atoken graph (i.e., trellis) according to the word order.Estimate the score of every path using the weights ofnode and edges estimated by training using Condi-tional Random Fields.
Select the best path.Figure 3 shows an example of our dictionary-based approach.
Suppose that the input is ?IL-2-mediated activation?.
A trellis is created basedon the lexical entries in a dictionary.
The se-lection criteria for the best path are determinedby the CRF tagging model trained on the Geniacorpus.
In this example, IL-2/NN-PROTEIN-/- mediated/VVN activation/NN is se-lected as the best path.
Following Kudo et al (Kudoet al, 2004), we adapted the core engine of theCRF-based morphological analyzer, MeCab1, to ourPOS/PROTEIN tagging task.
MeCab?s dictionarydatabases employ double arrays (Aoe, 1989) whichenable efficient lexical look-ups.The features used were:?
POS?
PROTEIN1http://sourceforge.net/project/showfiles.php?group id=177856/?
POS-PROTEIN?
bigram of adjacent POS?
bigram of adjacent PROTEIN?
bigram of adjacent POS-PROTEINDuring the construction of the trellis, white spaceis considered as the delimiter unless otherwise statedwithin dictionary entries.
This means that unknowntokens are character sequences without spaces.2.2.2 Dictionary constructionA dictionary-based approach requires the dictio-nary to cover not only a wide variety of biomedicalterms but also entries with:?
all possible capitalization?
all possible linguistic inflectionsWe constructed a freely available, wide-coverageEnglish word dictionary that satisfies these condi-tions.
We did consider the MedPost pos-taggerpackage2 which contains a free dictionary that hasdowncased English words; however, this dictionaryis not well curated as a dictionary and the number ofentries is limited to only 100,000, including inflec-tions.Therefore, we started by constructing an Englishword dictionary.
Eventually, we created a dictionarywith about 266,000 entries for English words (sys-tematically covering inflections) and about 1.3 mil-lion entries for protein names.We created the general English part of the dictio-nary from WordNet by semi-automatically addingPOS tags.
The POS tag set is a minor modifica-tion of the Penn Treebank POS tag set3, in that pro-tein names are given a new POS tag, NN-PROTEIN.Further details on construction of the dictionary nowfollow.Protein names were extracted from the BioThe-saurus4.
After selecting only those termsclearly stated as protein names, 1,341,992 pro-tein names in total were added to the dictionary.2ftp://ftp.ncbi.nlm.nih.gov/pub/lsmith/MedPost/3ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz4http://pir.georgetown.edu/iprolink/biothesaurus/65Nouns were extracted from WordNet?s noun list.Words starting with lower case and upper caseletters were determined as NN and NNP, re-spectively.
Nouns in NNS and NNPS cate-gories were collected from the results of POStagging articles from Plos Biology Journal5with TreeTagger6.Verbs were extracted from WordNet?s verb list.
Wemanually curated VBD, VBN, VBG and VBZverbs with irregular inflections based on Word-Net.
Next, VBN, VBD, VBG and VBZ formsof regular verbs were automatically generatedfrom the WordNet verb list.Adjectives were extracted from WordNet?s adjec-tive list.
We manually curated JJ, JJR and JJSof irregular inflections of adjectives based onthe WordNet irregular adjective list.
Base form(JJ) and regular inflections (JJR, JJS) of adjec-tives were also created based on the list of ad-jectives.Adverbs were extracted from WordNet?s adverblist.
Both the original and capitalised formswere added as RB.Pronouns were manually curated.
PRP and PRP$words were added to the dictionary.Wh-words were manually curated.
As a result,WDT, WP, WP$ and WRB words were addedto the dictionary.Words for other parts of speech were manuallycurated.2.2.3 Statistical prediction of protein namesStatistical sequential labelling was employed toimprove the coverage of protein name recognitionand to remove false positives resulting from the pre-vious stage (dictionary-based tagging).We used the JNLPBA-2004 training data, whichis a set of tokenized word sequences withIOB2(Tjong Kim Sang and Veenstra, 1999) proteinlabels.
As shown in Figure 2, POSs of tokens re-sulting from tagging and tokens of the JNLPBA-2004 data set are integrated to yield training data forsequential labelling.
During integration, when thesingle token of a protein name found after tagging5http://biology.plosjournals.org/6http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html/corresponds to a sequence of tokens from JNLPBA-2004, its POS is given as NN-PROTEIN1, NN-PROTEIN2,..., according to the corresponding tokenorder in the JNLPBA-2004 sequence.Following the data format of the JNLPBA-2004training set, our training and test data use the IOB2labels, which are ?B-protein?
for the first token ofthe target sequence, ?I-protein?
for each remainingtoken in the target sequence, and ?O?
for other to-kens.
For example, ?Activation of the IL 2 precursorprovides?
is analyzed by the POS/PROTEIN taggeras follows.Activation NNof INthe DTIL 2 precursor NN-PROTEINprovides VVZThe tagger output is given IOB2 labels as follows.Activation NN Oof IN Othe DT OIL NN-PROTEIN1 B-protein2 NN-PROTEIN2 I-proteinprecursor NN-PROTEIN3 I-proteinprovides VVZ OWe used CRF models to predict the IOB2 la-bels.
The following features were used in our ex-periments.?
word feature?
orthographic features?
the first letter and the last four letters ofthe word form, in which capital letters ina word are normalized to ?A?, lower caseletters are normalized to ?a?, and digits arereplaced by ?0?, e.g., the word form of IL-2 is AA-0.?
postfixes, the last two and four letters?
POS feature?
PROTEIN featureThe window size was set to ?2 of the current to-ken.3 Results and discussion66Table 1: Experimental RusultsTagging R P FFull 52.91 43.85 47.96(a) POS/PROTEIN tagging Left 61.48 50.95 55.72Right 61.38 50.87 55.63Sequential Labelling R P FFull 63.23 70.39 66.62(b) Word feature Left 68.15 75.86 71.80Right 69.88 77.79 73.63Full 77.17 67.52 72.02(c) (b) + orthographic feature Left 82.51 72.20 77.01Right 84.29 73.75 78.67Full 76.46 68.41 72.21(d) (c) + POS feature Left 81.94 73.32 77.39Right 83.54 74.75 78.90Full 77.58 69.18 73.14(e) (d) + PROTEIN feature Left 82.69 73.74 77.96Right 84.37 75.24 79.54Full 79.85 68.58 73.78(f) (e) + after adding protein names in the Left 84.82 72.85 78.38training set to the dictionary Right 86.60 74.37 80.023.1 Protein name recognition performanceTable 1 shows our protein name recognition results,showing the differential effect of various combina-tions of strategies.
Results are expressed accord-ing to recall (R), precision (P), and F-measure (F),which here measure how accurately our various ex-periments determined the left boundary (Left), theright boundary (Right), and both boundaries (Full)of protein names.
The baseline for tagging (row(a)) shows the protein name detection performanceof our dictionary-based tagging using our large pro-tein name dictionary, where no training for proteinname prediction was involved.
The F-score of thisbaseline tagging method was 47.96.The baseline for sequential labelling (row (b))shows the prediction performance when using onlyword features where no orthographic and POS fea-tures were used.
The F-score of the baseline la-belling method was 66.62.
When orthographic fea-ture was added (row (c)), the F-score increased by5.40 to 72.02.
When the POS feature was added(row (d)), the F-score increased by 0.19 to 72.21.Using all features (row (e)), the F-score reached73.14.
Surprisingly, adding protein names appear-ing in the training data to the dictionary further im-proved the F-score by 0.64 to 73.78, which is thesecond best score for protein name recognition us-ing the JNLPBA-2004 data set.Table 2: After Dictionary EnrichmentMethod R P FTagging Full 79.02 61.87 69.40(+test set Left 82.28 64.42 72.26protein names) Right 80.96 63.38 71.10Labelling full 86.13 72.49 78.72(+test set Left 89.58 75.40 81.88protein names) Right 90.23 75.95 82.47Tagging and labelling speeds were measured us-ing an unloaded Linux server with quad 1.8 GHzOpteron cores and 16GB memory.
The dictionary-based POS/PROTEIN tagger is very fast eventhough the total size of the dictionary is more thanone million.
The processing speed for tagging andsequential labelling of the 4,259 sentences of the testset data took 0.3 sec and 7.3 sec, respectively, whichmeans that in total it took 7.6 sec.
for recognizingprotein names in the plain text of 4,259 sentences.3.2 Dictionary enrichmentThe advantage of the dictionary-based statistical ap-proach is that it is versatile, as the user can easilyimprove its performance with no retraining.
We as-sume the following situation as the ideal case: sup-pose that a user needs to analyze a large amount oftext with protein names.
The user wants to know67the maximum performance achievable for identify-ing protein names with our dictionary-based statis-tical recognizer which can be achieved by addingmore protein names to the current dictionary.
Notethat protein names should be identified in context.That is, recall of the NER results with the ideal dic-tionary is not 100%.
Some protein names in the idealdictionary are dropped during statistical tagging orlabelling.Table 2 shows the scores after each step of dic-tionary enrichment.
The first block (Tagging) showsthe tagging performance after adding protein namesappearing in the test set to the dictionary.
The sec-ond block (Labelling) shows the performance of thesequence labelling of the output of the first step.Note that tagging and the sequence labelling mod-els are not retrained using the test set.3.3 DiscussionIt is not possible in reality to train the recognizeron target data, i.e., the test set, but it would be pos-sible for users to add discovered protein names tothe dictionary so that they could improve the overallperformance of the recognizer without retraining.Rule-based and procedural approaches are takenin (Fukuda et al, 1998; Franzen et al, 2002).
Ma-chine learning-based approaches are taken in (Col-lier et al, 2000; Lee et al, 2003; Kazama et al,2002; Tanabe and Wilbur, 2002; Yamamoto et al,2003; Tsuruoka, 2006; Okanohara et al, 2006).Machine learning algorithms used in these studiesare Naive Bayes, C4.5, Maximum Entropy Models,Support Vector Machines, and Conditional RandomFields.
Most of these studies applied machine learn-ing techniques to tokenized sentences.Table 3 shows the scores reported by other sys-tems.
Tsai et al (Tsai et al, 2006) and Zhou andSu (Zhou and Su, 2004) combined machine learningtechniques and hand-crafted rules.
Tsai et al (Tsaiet al, 2006) applied CRFs to the JNLPBA-2004data.
After applying pattern-based post-processing,they achieved the best F-score (75.12) among thosereported so far.
Kim and Yoon(Kim and Yoon, 2007)also applied heuristic post-processing.
Zhou and Su(Zhou and Su, 2004) achieved an F-score of 73.77.Purely machine learning-based approaches havebeen investigated by several researchers.
TheGENIA Tagger (Tsuruoka, 2006) is trained onthe JNLPBA-2004 Corpus.
Okanohara et al(Okanohara et al, 2006) employed semi-MarkovCRFs whose performance was evaluated against theJNLPBA-2004 data set.
Yamamoto et al (Ya-mamoto et al, 2003) used SVMs for character-based protein name recognition and sequential la-belling.
Their protein name extraction performancewas 69%.
This paper extends the machine learningapproach with a curated dictionary and CRFs andachieved high F-score 73.78, which is the top scoreamong the heuristics-free NER systems.
Table 4shows typical recognition errors found in the recog-nition results that achieved F-score 73.78.
In somecases, protein name boundaries of the JNLPBA-2004 data set are not consistent.
It is also one ofthe reasons for the recognition errors that the dataset contains general protein names, such as domain,family, and binding site names as well as anaphoricexpressions, which are usually not covered by pro-tein name repositories.
Therefore, our impression onthe performance is that an F-score of 73.78 is suffi-ciently high.Furthermore, thanks to the dictionary-based ap-proach, it has been shown that the upper bound per-formance using ideal dictionary enrichment, with-out any retraining of the models, has an F-score of78.72.4 ConclusionsThis paper has demonstrated how to utilize knownnamed entities to achieve better performance in sta-tistical named entity recognition.
We took a two-step approach where sentences are first tokenizedand tagged based on a biomedical dictionary thatconsists of general English words and about 1.3 mil-lion protein names.
Then, a statistical sequencelabelling step predicted protein names that are notlisted in the dictionary and, at the same time, re-duced false negatives in the POS/PROTEIN taggingresults.
The significant benefit of this approach isthat a user, not a system developer, can easily en-hance the performance by augmenting the dictio-nary.
This paper demonstrated that the state-of-the-art F-score 73.78 on the standard JNLPBA-2004data set was achieved by our approach.
Further-more, thanks to the dictionary-based NER approach,the upper bound performance using ideal dictionaryenrichment, without any retraining of the models,yielded F-score 78.72.5 AcknowledgmentsThis research is partly supported by EC IST projectFP6-028099 (BOOTStrep), whose Manchester teamis hosted by the JISC/BBSRC/EPSRC sponsoredNational Centre for Text Mining.68Table 3: Conventional results for protein name recognitionAuthors R P FTsai et al(Tsai et al, 2006) 71.31 79.36 75.12Our system 79.85 68.58 73.78Zhou and Su(Zhou and Su, 2004) 69.01 79.24 73.77Kim and Yoon(Kim and Yoon, 2007) 75.82 71.02 73.34Okanohara et al(Okanohara et al, 2006) 77.74 68.92 73.07Tsuruoka(Tsuruoka, 2006) 81.41 65.82 72.79Finkel et al(Finkel et al, 2004) 77.40 68.48 72.67Settles(Settles, 2004) 76.1 68.2 72.0Song et al(Song et al, 2004) 65.50 73.04 69.07Ro?ssler(Ro?ssler, 2004) 72.9 62.0 67.0Park et al(Park et al, 2004) 69.71 59.37 64.12ReferencesJ.
Aoe, An Efficient Digital Search Algorithm by Usinga Double-Array Structure, IEEE Transactions on Soft-ware Engineering, 15(9):1066?1077, 1989.L.E.
Baum and T. Petrie, Statistical inference for proba-bilistic functions of finite state Markov chains, The An-nals of Mathematical Statistics, 37:1554?1563, 1966.J.
Chang, H. Schutze, R. Altman, GAPSCORE: FindingGene and Protein names one Word at a Time, Bioin-formatics, Vol.
20, pp.
216-225, 2004.N.
Collier, C. Nobata, J. Tsujii, Extracting the Namesof Genes and Gene Products with a Hidden MarkovModel, Proc.
of the 18th International Conferenceon Computational Linguistics (COLING?2000), Saar-brucken, 2000.Jenny Finkel, Shipra Dingare, Huy Nguyen, Malvina Nis-sim, Gail Sinclair and Christopher Manning, Exploit-ing Context for Biomedical Entity Recognition: FromSyntax to the Web, Proc.
of the Joint Workshop on Nat-ural Language Processing in Biomedicine and its Ap-plications (JNLPBA-2004), pp.
88?91, 2004.K.
Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden,and J. Koster, Protein Names and How to Find Them,Int.
J. Med.
Inf., Vol.
67, pp.
49?61, 2002.K.
Fukuda, T. Tsunoda, A. Tamura, and T. Takagi,Toward information extraction: identifying proteinnames from biological papers, PSB, pp.
705-716,1998.J.
Kazama, T. Makino, Y. Ohta, J. Tsujii, Tuning SupportVector Machines for Biomedical Named Entity Recog-nition, Proc.
of ACL-2002 Workshop on Natural Lan-guage Processing in the Biomedical Domain, pp.
1?8,2002.J.-D. Kim, T. Ohta, Y. Tateisi, J. Tsujii: GENIA corpus- semantically annotated corpus for bio-textmining,Bioinformatics 2003, 19:i180-i182.Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,Yuka Tateisi, Introduction to the Bio-Entity Recogni-tion Task at JNLPBA, Proc.
of the Joint Workshop onNatural Language Processing in Biomedicine and itsApplications (JNLPBA-2004), pp.
70?75, 2004.S.
Kim, J. Yoon: Experimental Study on a Two PhaseMethod for Biomedical Named Entity Recognition,IEICE Transactions on Informaion and Systems 2007,E90-D(7):1103?1120.Taku Kudo and Kaoru Yamamoto and Yuuji Matsumoto,Applying Conditional Random Fields to JapaneseMorphological Analysis, Proc.
of Empirical Methodsin Natural Language Processing (EMNLP), pp.
230?237, 2004.J.
Lafferty, A. McCallum, and F. Pereira, ConditionalRandom Fields: Probabilistic Models for Segment-ing and Labeling Sequence Data, Proc.
of ICML-2001,pp.282?289, 2001K.
J. Lee, Y. S. Hwang and H. C. Rim (2003), Two-PhaseBiomedical NE Recognition based on SVMs, Proc.
ofACL 2003 Workshop on Natural Language Processingin Biomedicine, Sapporo, 2003.McCallum A, Freitag D, Pereira F.: Maximum entropyMarkov models for information extraction and seg-mentation, Proceedings of the Seventeenth Interna-tional Conference on Machine Learning, 2000:591-598.Daisuke, Okanohara, Yusuke Miyao, Yoshimasa Tsu-ruoka and Jun?ichi Tsujii, Improving the Scalability ofSemi-Markov Conditional Random Fields for NamedEntity Recognition, Proc.
of ACL 2006, Sydney, 2006.Kyung-Mi Park, Seon-Ho Kim, Do-Gil Lee andHae-Chang Rim.
Boosting Lexical Knowledge forBiomedical Named Entity Recognition, Proc.
of theJoint Workshop on Natural Language Processing inBiomedicine and its Applications (JNLPBA-2004), pp.76-79, 2004.Marc Ro?ssler, Adapting an NER-System for German tothe Biomedical Domain, Proc.
of the Joint Workshopon Natural Language Processing in Biomedicine andits Applications (JNLPBA-2004), pp.
92?95, 2004.Burr Settles, Biomedical Named Entity Recognition Us-ing Conditional Random Fields and Novel Feature69Table 4: Error AnalysisFalse positivesCause Correct extraction Identified term1 dictionary - protein, binding sites2 prefix word trans-acting factor common trans-acting factor3 unknown word - ATTTGCAT4 sequential labelling error - additional proteins5 test set error - Estradiol receptorsFalse negativesCause Correct extraction Identified term1 anaphoric (the) receptor, (the) binding sites -2 coordination (and, or) transcription factors NF-kappa B and AP-1 transcription factors NF-kappa B3 prefix word activation protein-1 protein-1catfish STAT STAT4 postfix word nuclear factor kappa B complex nuclear factor kappa B5 plural protein tyrosine kinase(s) protein tyrosine kinase6 family name, biding site, T3 binding sites -and domain residues 639-656 -7 sequential labelling error PCNA -Chloramphenicol acetyltransferase -8 test set error superfamily member -Sets, Proc.
of the Joint Workshop on Natural Lan-guage Processing in Biomedicine and its Applications(JNLPBA-2004), pp.
104?1007, 2004.Yu Song, Eunju Kim, Gary Geunbae Lee and Byoung-kee Yi, POSBIOTM-NER in the shared task ofBioNLP/NLPBA 2004, Proc.
of the Joint Workshop onNatural Language Processing in Biomedicine and itsApplications (JNLPBA-2004), pp.
100-103, 2004.L.
Tanabe and W. J. Wilbur, Tagging Gene and ProteinNames in Biomedical Text, Bioinformatics, 18(8), pp.1124?1132, 2002.E.F.
Tjong Kim Sang and J. Veenstra, Representing TextChunks,EACL-99, pp.
173-179, 1999.Richard Tzong-Han Tsai, W.-C. Chou, S.-H. Wu, T.-Y.Sung, J. Hsiang, and W.-L. Hsu, Integrating LinguisticKnowledge into a Conditional Random Field Frame-work to Identify Biomedical Named Entities, ExpertSystems with Applications, 30 (1), 2006.Yoshimasa Tsuruoka, GENIA Tagger 3.0,http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/, 2006.K.
Yamamoto, T. Kudo, A. Konagaya and Y. Matsumoto,Protein Name Tagging for Biomedical Annotation inText, in Proc.
of ACL-2003 Workshop on Natural Lan-guage Processing in Biomedicine, Sapporo, 2003.Guofeng Zhou and Jian Su, Exploring Deep KnowledgeResources in Biomedical Name Recognition, Proceed-ings of the Joint Workshop on Natural Language Pro-cessing of Biomedicine and its Applications (JNLPBA-2004), pp.
96-99, 2004.70
