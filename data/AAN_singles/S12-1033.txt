First Joint Conference on Lexical and Computational Semantics (*SEM), pages 246?255,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics#Emotional TweetsSaif M. MohammadEmerging TechnologiesNational Research Council CanadaOttawa, Ontario, Canada K1A 0R6saif.mohammad@nrc-cnrc.gc.caAbstractDetecting emotions in microblogs and so-cial media posts has applications for industry,health, and security.
However, there exists nomicroblog corpus with instances labeled foremotions for developing supervised systems.In this paper, we describe how we created sucha corpus from Twitter posts using emotion-word hashtags.
We conduct experiments toshow that the self-labeled hashtag annotationsare consistent and match with the annotationsof trained judges.
We also show how the Twit-ter emotion corpus can be used to improveemotion classification accuracy in a differentdomain.
Finally, we extract a word?emotionassociation lexicon from this Twitter corpus,and show that it leads to significantly betterresults than the manually crafted WordNet Af-fect lexicon in an emotion classification task.11 IntroductionWe use language not just to convey facts, but alsoour emotions.
Automatically identifying emotionsexpressed in text has a number of applications, in-cluding customer relation management (Bougie etal., 2003), determining popularity of products andgovernments (Mohammad and Yang, 2011), andimproving human-computer interaction (Vela?squez,1997; Ravaja et al, 2006).Twitter is an online social networking and mi-croblogging service where users post and read mes-sages that are up to 140 characters long.
The mes-sages are called tweets.1Email the author to obtain a copy of the hash-tagged tweetsor the emotion lexicon: saif.mohammad@nrc-cnrc.gc.ca.Often a tweet may include one or more words im-mediately preceded with a hash symbol (#).
Thesewords are called hashtags.
Hashtags serve manypurposes, but most notably they are used to indicatethe topic.
Often these words add to the informationin the tweet: for example, hashtags indicating thetone of the message or their internal emotions.From the perspective of one consuming tweets,hashtags play a role in search: Twitter allows peo-ple to search tweets not only through words in thetweets, but also through hashtagged words.
Con-sider the tweet below:We are fighting for the 99% that have beenleft behind.
#OWS #angerA number of people tweeting about the OccupyWall Street movement added the hashtag #OWS totheir tweets.
This allowed people searching fortweets about the movement to access them simplyby searching for the #OWS hashtag.
In this partic-ular instance, the tweeter (one who tweets) has alsoadded an emotion-word hashtag #anger, possibly toconvey that he or she is angry.Currently there are more than 200 million Twitteraccounts, 180 thousand tweets posted every day, and18 thousand Twitter search queries every second.Socio-linguistic researchers point out that Twitter isprimarily a means for people to converse with otherindividuals, groups, and the world in general (Boydet al, 2010).
As tweets are freely accessible to all,the conversations can take on non-traditional formssuch as discussions developing through many voicesrather than just two interlocuters.
For example, theuse of Twitter and Facebook has been credited with246providing momentum to the 2011 Arab Spring andOccupy Wall Street movements (Skinner, 2011; Ray,2011).
Understanding how such conversations de-velop, how people influence one another throughemotional expressions, and how news is shared toelicit certain emotional reactions, are just some ofthe compelling reasons to develop better models forthe emotion analysis of social media.Supervised methods for emotion detection tend toperform better than unsupervised ones.
They usengram features such as unigrams and bigrams (indi-vidual words and two-word sequences) (Aman andSzpakowicz, 2007; Neviarouskaya et al, 2009; Mo-hammad, 2012b).
However, these methods requirelabeled data where utterances are marked with theemotion they express.
Manual annotation is time-intensive and costly.
Thus only a small amount ofsuch text exists.
Further, supervised algorithms thatrely on ngram features tend to classify accuratelyonly if trained on data from the same domain as thetarget sentences (Mohammad, 2012b).
Thus eventhe limited amount of existing emotion-labeled datais unsuitable for use in microblog analysis.In this paper, we show how we automatically cre-ated a large dataset of more than 20,000 emotion-labeled tweets using hashtags.
We compiled labeleddata for six emotions?joy, sadness, anger, fear, dis-gust, and surprise?argued to be the most basic (Ek-man, 1992).
We will refer to our dataset as the Twit-ter Emotion Corpus (TEC).
We show through ex-periments that even though the tweets and hashtagscover a diverse array of topics and were generatedby thousands of different individuals (possibly withvery different educational and socio-economic back-grounds), the emotion annotations are consistent andmatch the intuitions of trained judges.
We also showhow we used the TEC to improve emotion detectionin a domain very different from social media.Finally, we describe how we generated a large lex-icon of ngrams and associated emotions from TEC.This emotion lexicon can be used in many applica-tions, including highlighting words and phrases in apiece of text to quickly convey regions of affect.
Weshow that the lexicon leads to significantly better re-sults than that obtained using the manually craftedWordNet Affect lexicon in an emotion classificationtask.2 Related WorkEmotion analysis can be applied to all kinds of text,but certain domains and modes of communicationtend to have more overt expressions of emotionsthan others.
Genereux and Evans (2006), Mihalceaand Liu (2006), and Neviarouskaya et al (2009) ana-lyzed web-logs.
Alm et al (2005) and Francisco andGerva?s (2006) worked on fairy tales.
Boucouvalas(2002), John et al (2006), and Mohammad (2012a)explored emotions in novels.
Zhe and Boucouvalas(2002), Holzman and Pottenger (2003), and Ma et al(2005) annotated chat messages for emotions.
Liu etal.
(2003) and Mohammad and Yang (2011) workedon email data.
Kim et al (2009) analyzed sadness inposts reacting to news of Michael Jackson?s death.Tumasjan et al (2010) study Twitter as a forum forpolitical deliberation.Much of this work focuses on six Ekman emo-tions.
There is less work on complex emotions, forexample, work by Pearl and Steyvers (2010) that fo-cuses on politeness, rudeness, embarrassment, for-mality, persuasion, deception, confidence, and dis-belief.
Bolen et al (2009) measured tension, depres-sion, anger, vigor, fatigue, and confusion in tweets.One of the advantages of our work is that we can eas-ily collect tweets with hashtags for many emotions,well beyond the basic six.Go et al (2009) and Gonza?lez-Iba?n?ez et al (2011)noted that sometimes people use the hashtag #sar-casm to indicate that their tweet is sarcastic.
Theycollected tweets with hashtags of #sarcasm and#sarcastic to create a dataset of sarcastic tweets.
Wefollow their ideas and collect tweets with hashtagspertaining to different emotions.
Additionally, wepresent several experiments to validate that the emo-tion labels in the corpus are consistent and matchintuitions of trained judges.3 Existing Emotion-Labeled TextThe SemEval-2007 Affective Text corpus has news-paper headlines labeled with the six Ekman emo-tions by six annotators (Strapparava and Mihalcea,2007).
More precisely, for each headline?emotionpair, the annotators gave scores from 0 to 100 indi-cating how strongly the headline expressed the emo-tion.
The inter-annotator agreement as determinedby calculating the Pearson?s product moment corre-247# of % ofemotion instances instances ranger 132 13.2 0.50disgust 43 4.3 0.45fear 247 24.7 0.64joy 344 34.4 0.60sadness 283 28.3 0.68surprise 253 25.3 0.36simple average 0.54frequency-based average 0.43Table 1: Inter-annotator agreement (Pearson?s correla-tion) amongst 6 annotators on the 1000-headlines dataset.lation (r) between the scores given by each anno-tator and the average of the other five annotators isshown in Table 1.
For our experiments, we consid-ered scores greater than 25 to indicate that the head-line expresses the corresponding emotion.The dataset was created for an unsupervised com-petition, and consisted of 250 headlines of trial dataand 1000 headlines of test data.
We will refer tothem as the 250-headlines and the 1000-headlinesdatasets respectively.
However, the data has alsobeen used in a supervised setting through (1) ten-fold cross-validation on the 1000-headlines datasetand (2) using the 1000 headlines as training data andtesting on the 250-headlines dataset (Chaffar andInkpen, 2011).Other datasets with sentence-level annotations ofemotions include about 4000 sentences from blogs,compiled by Aman and Szpakowicz (2007); 1000sentences from stories on topics such as educa-tion and health, compiled by Neviarouskaya et al(2009); and about 4000 sentences from fairy tales,annotated by Alm and Sproat (2005).4 Creating the Twitter Emotion CorpusSometimes people use hashtags to notify others ofthe emotions associated with the message they aretweeting.
Table 2 shows a few examples.
On readingjust the message before the hashtags, most peoplewill agree that the tweeter #1 is sad, tweeter #2 ishappy, and tweeter #3 is angry.However, there also exist tweets such as the fourthexample, where reading just the message before thehashtag does not convey the emotions of the tweeter.Here, the hashtag provides information not present(implicitly or explicitly) in the rest of the message.1.
Feeling left out... #sadness2.
My amazing memory saves the day again!
#joy3.
Some jerk stole my photo on tumblr.
#anger4.
Mika used my photo on tumblr.
#anger5.
School is very boring today :/ #joy6.
to me.... YOU are ur only #fearTable 2: Example tweets with emotion-words hashtags.There are also tweets, such as those shown in ex-amples 5 and 6, that do not seem to express theemotions stated in the hashtags.
This may occurfor many reasons including the use of sarcasm orirony.
Additional context is required to understandthe full emotional import of many tweets.
Tweetstend to be very short, and often have spelling mis-takes, short forms, and various other properties thatmake such text difficult to process by natural lan-guage systems.
Further, it is probable, that onlya small portion of emotional tweets are hashtaggedwith emotion words.Our goal in this paper is to determine if we cansuccessfully use emotion-word hashtags as emotionlabels despite the many challenges outlined above:?
Can we create a large corpus of emotion-labeled hashtags??
Are the emotion annotations consistent, de-spite the large number of annotators, despite nocontrol over their socio-economic and culturalbackground, despite the many ways in whichhashtags are used, and despite the many id-iosyncracies of tweets??
Do the hashtag annotations match with the in-tuitions of trained judges?We chose to collect tweets with hashtags corre-sponding to the six Ekman emotions: #anger, #dis-gust, #fear, #happy, #sadness, and #surprise.Eisenstein et al (2010) collected about 380,000tweets2 from Twitter?s official API.3 Similarly, Goet al (2009) collected 1.6 million tweets.4 However,these datasets had less than 50 tweets that containedemotion-word hashtags.
Therefore, we abandonedthe search-in-corpora approach in favor of the onedescribed below.2http://www.ark.cs.cmu.edu/GeoText3https://dev.twitter.com/docs/streaming-api4https://sites.google.com/site/twittersentimenthelp2484.1 Hashtag-based Search on the TwitterSearch APIThe Archivist5 is a free online service that helpsusers extract tweets using Twitter?s Search API.6For any given query, Archivist first obtains up to1500 tweets from the previous seven days.
Sub-sequently, it polls the Twitter Search API everyfew hours to obtain newer tweets that match thequery.
We supplied Archivist with the six hashtagqueries corresponding to the Ekman emotions, andcollected about 50,000 tweets from those posted be-tween November 15, 2011 and December 6, 2011.We discarded tweets that had fewer than threevalid English words.
We used the Roget Thesaurusas the lexicon of English words.7 This helped filterout most, if not all, of the non-English tweets thathad English emotion hashtags.
It also eliminatedvery short phrases, and some expressions with verybad spelling.
We discarded tweets with the prefix?Rt?, ?RT?, and ?rt?, which indicate that the mes-sages that follow are re-tweets (re-postings of tweetssent earlier by somebody else).
Like Gonza?lez-Iba?n?ez et al (2011), we removed tweets that did nothave the hashtag of interest at the end of the mes-sage.
It has been suggested that middle-of-tweethashtags may not be good labels of the tweets.8 Fi-nally, we were left with about 21,000 tweets, whichformed the Twitter Emotion Corpus (TEC).4.2 Distribution of emotion-word hashtagsTable 3 presents some details of the TEC.
Observethat the distribution of emotions in the TEC is verydifferent from the distribution of emotions in the1000-headlines corpus (see Table 1).
There are moremessages tagged with the hashtag #joy than any ofthe other Ekman emotions.Synonyms can often be used to express the sameconcept or emotion.
Thus it is possible that the truedistribution of hashtags corresponding to emotionsis different from what is shown in Table 3.
In thefuture, we intend to collect tweets with synonyms ofjoy, sadness, fear, etc., as well.5http://archivist.visitmix.com6https://dev.twitter.com/docs/using-search7Roget?s Thesaurus: www.gutenberg.org/ebooks/106818End-of-message hashtags are also much more commonthan hashtags at other positions.# of % ofhashtag instances instances#anger 1,555 7.4#disgust 761 3.6#fear 2,816 13.4#joy 8,240 39.1#sadness 3,830 18.2#surprise 3,849 18.3Total tweets 21,051 100.0# of tweeters 19,059Table 3: Details of the Twitter Emotion Corpus.5 Consistency and Usefulness of EmotionHashtagged TweetsAs noted earlier, even with trained judges, emotionannotation obtains only a modest inter-annotatoragreement (see Table 1).
As shown in Table 3, theTEC has about 21,000 tweets from about 19,000 dif-ferent people.
If TEC were to be treated as manu-ally annotated data (which in one sense, it is), thenit is data created by a very large number of judges,and most judges have annotated just one instance.Therefore, an important question is to determinewhether the hashtag annotations of the tens of thou-sands of tweeters are consistent with one another.
Itwill also be worth determining if this large amountof emotion-tagged Twitter data can help improveemotion detection in sentences from other domains.To answer these questions, we conducted twoautomatic emotion classification experiments de-scribed in the two sub-sections below.
For these ex-periments, we created binary classifiers for each ofthe six emotions using Weka (Hall et al, 2009).9 Forexample, the Fear?NotFear classifier determinedwhether a sentence expressed fear or not.
Notethat, for these experiments, we treated the emotionhashtags as class labels and removed them from thetweets.
Thus a classifier has to determine that atweet expresses anger, for example, without havingaccess to the hashtag #anger.We chose Support Vector Machines (SVM) withSequential Minimal Optimization (Platt, 1999) asthe machine learning algorithm because of its suc-cessful application in various research problems.
Weused binary features that captured the presence orabsence of unigrams and bigrams.9http://www.cs.waikato.ac.nz/ml/weka249Label (X) #gold #right #guesses P R FI.
System using ngrams with freq.
> 1anger 132 35 71 49.3 26.5 34.5disgust 43 8 19 42.1 18.6 25.8fear 247 108 170 63.5 43.7 51.8joy 344 155 287 54.0 45.1 49.1sadness 283 104 198 52.5 36.7 43.2surprise 253 74 167 44.3 29.2 35.2ALL LABELS 1302 484 912 53.1 37.2 43.7II.
System using all ngrams (no filtering)ALL LABELS 1302 371 546 67.9 28.5 40.1III.
System that guesses randomlyALL LABELS 1302 651 3000 21.7 50.0 30.3Table 4: Cross-validation results on the 1000-headlines dataset.
#gold is the number of headlines expressing a partic-ular emotion.
#right is the number these instances the classifier correctly marked as expressing the emotion.
#guessesis the number of instances marked as expressing an emotion by the classifier.In order to set a suitable benchmark for experi-ments with the TEC corpus, we first applied the clas-sifiers to the SemEval-2007 Affective Text corpus.We executed ten-fold cross-validation on the 1000-headlines dataset.
We experimented with using allngrams, as well as training on only those ngrams thatoccurred more than once.The rows under I in Table 4 give a breakdownof results obtained by the EmotionX?NotEmotionXclassifiers.
when they ignored single-occurrence n-grams (where X is one of the six basic emotions).#gold is the number of headlines expressing a par-ticular emotion X .
#right is the number of instancesthat the classifier correctly marked as expressing X .#guesses is the number of instances marked as ex-pressing X by the classifier.
Precision (P ) and recall(R) are calculated as shown below:P =#right#guesses?
100 (1)R =#right#gold?
100 (2)F is the balanced F-score.
The ALL LABELS rowshows the sums of #gold, #right, and #guesses.The II and III rows in the table show overall re-sults obtained by a system that uses all ngrams andby a system that guesses randomly.10 We do not10A system that randomly guesses whether an instance is ex-pressing an emotionX or not will get half of the #gold instancesright.
Further, the system will mark half of all the instances asexpressing emotion X .
For ALL LABELS,#right = #gold2 , and #guesses =#instances?62 .show a breakdown of results by emotions for II andIII due to space constraints.It is not surprising that the emotion classes withthe most training instances and the highest inter-annotator agreement (joy, sadness, and fear) are alsothe classes on which the classifiers perform best (seeTable 1).The F-score of 40.1 obtained using all ngramsis close to 39.6 obtained by Chaffar and Inkpen(2011)?a sanity check for our baseline system.
Ig-noring words that occur only once in the train-ing data seems beneficial.
All classification resultsshown ahead are for the cases when ngrams that oc-curred only once were filtered out.5.1 Experiment I: Can a classifier learn topredict emotion hashtags?We applied the binary classifiers described above tothe TEC.
Table 5 shows ten-fold cross-validation re-sults.
Observe that even though the TEC was cre-ated from tens of thousands of users, the automaticclassifiers are able to predict the emotions (hash-tags) with F-scores much higher than the randombaseline, and also higher than those obtained on the1000-headlines corpus.
Note also that this is de-spite the fact that the random baseline for the 1000-headlines corpus (F = 30.3) is higher than the ran-dom baseline for the TEC (F = 21.7).
The resultssuggest that emotion hashtags assigned to tweets areconsistent to a degree such that they can be used fordetecting emotion hashtags in other tweets.Note that expectedly the Joy?NotJoy classifier250Label #gold #right #guesses P R FI.
System using ngrams with freq.
> 1anger 1555 347 931 37.3 22.31 27.9disgust 761 102 332 30.7 13.4 18.7fear 2816 1236 2073 59.6 43.9 50.6joy 8240 4980 7715 64.5 60.4 62.4sadness 3830 1377 3286 41.9 36.0 38.7surprise 3849 1559 3083 50.6 40.5 45.0ALL LABELS 21051 9601 17420 55.1 45.6 49.9II.
System that guesses randomlyALL LABELS 21051 10525 63,153 16.7 50.0 21.7Table 5: Cross-validation results on the TEC.
The highest F-score is shown in bold.gets the best results as it has the highest number oftraining instances.
The Sadness?NotSadness clas-sifier performed relatively poorly considering theamount of training instances available, whereas theFear-NotFear classifier performed relatively well.
Itis possible that people use less overt cues in tweetswhen they are explicitly giving it a sadness hashtag.5.2 Experiment II: Can TEC improve emotionclassification in a new domain?As mentioned earlier, supervised algorithms per-form well when training and test data are from thesame domain.
However, certain domain adaptationalgorithms may be used to combine training data inthe target domain with large amounts of training datafrom a different source domain.The Daume?
(2007) approach involves the trans-formation of the original training instance featurevector into a new space made up of three copies ofthe original vector.
The three copies correspond tothe target domain, the source domain, and the gen-eral domain.
If X represents an original feature vec-tor from the target domain, then it is transformedinto XOX, where O is a zero vector.
If X repre-sents original feature vector from the source domain,then it is transformed into OXX.
This data is givento the learning algorithm, which learns informationspecific to the target domain, specific to the sourcedomain, as well as information that applies to bothdomains.
The test instance feature vector (whichis from the target domain) is transformed to XOX.Therefore, the classifier applies information specificto the target domain as well as information commonto both the target and source domains, but not infor-mation specific only to the source domain.In this section, we describe experiments on us-ing the Twitter Emotion Corpus for emotion clas-sification in the newspaper headlines domain.
Weapplied our binary emotion classifiers on unseentest data from the newspaper headlines domain?the250-headlines dataset?using each of the followingas a training corpus:?
Target-domain data: the 1000-headlines data.?
Source-domain data: the TEC.?
Target and Source data: A joint corpus of the1000-headlines dataset and the TEC.Additionally, when using the ?Target and Source?data, we also tested the domain adaptation algo-rithm proposed in Daume?
(2007).
Since the Emo-tionX class (the positive class) has markedly fewerinstances than the NotEmotionX class, we assignedhigher weight to instances of the positive class dur-ing training.11 The rows under I in Table 6 give theresults.
(Row II results are for the experiment de-scribed in Section 6, and can be ignored for now.
)We see that the macro-averaged F-score when us-ing target-domain data (row I.a.)
is identical to thescore obtained by the random baseline (row III).However, observe that the precision of the ngramsystem is higher than the random system, and itsrecall is lower.
This suggests that the test data hasmany n-grams not previously seen in the trainingdata.
Observe that as expected, using source-domaindata produces much lower scores (row I.b.)
thanwhen using target-domain training data (row I.a.
).Using both target- and source-domain data pro-duced significantly better results (row I.c.1.)
than11For example, for the anger?NotAnger classifier, if 10 outof 110 instances have the label anger, then they are each givena weight of 10, whereas the rest are given a weight of 1.251# of features P R FI.
System using ngrams in training data:a. the 1000-headlines text (target domain) 1,181 40.2 32.1 35.7b.
the TEC (source domain) 32,954 29.9 26.1 27.9c.
the 1000-headlines text and the TEC (target and source)c.1.
no domain adaptation 33,902 41.7 35.5 38.3c.2.
with domain adaptation 101,706 46.0 35.5 40.1II.
System using ngrams in 1000-headlines and:a. the TEC lexicon 1,181 + 6 44.4 35.3 39.3b.
the WordNet Affect lexicon 1,181 + 6 39.7 30.5 34.5c.
the NRC emotion lexicon 1,181 + 10 46.7 38.6 42.2III.
System that guesses randomly - 27.8 50.0 35.7Table 6: Results on the 250-headlines dataset.
The highest F-scores in I and II are shown in bold.using target-domain data alone (I.a.).
Applying thedomain adaptation technique described in Daume?
(2007), obtained even better results (row I.c.2.).
(Weused the Fisher Exact Test and a confidence inter-val of 95% for all precision and recall significancetesting reported in this paper.)
The use of TECimproved both precision and recall over just usingthe target-domain text.
This shows that the TwitterEmotion Corpus can be leveraged, preferably witha suitable domain adaptation algorithm, to improveemotion classification results even on datasets froma different domain.
It is also a validation of thepremise that the self-labeled emotion hashtags areconsistent, at least to some degree, with the emotionlabels given by trained human judges.6 Creating the TEC Emotion LexiconWord?emotion association lexicons are lists ofwords and associated emotions.
For example, theword victory may be associated with the emotionsof joy and relief.
These emotion lexicons have manyapplications, including automatically highlightingwords and phrases to quickly convey regions of af-fect in a piece of text.
Mohammad (2012b) showsthat these lexicon features can significantly improveclassifier performance over and above that obtainedusing ngrams alone.WordNet Affect (Strapparava and Valitutti, 2004)includes 1536 words with associations to the six Ek-man emotions.12 Mohammad and colleagues com-piled emotion annotations for about 14,000 wordsby crowdsourcing to Mechanical Turk (Mohammad12http://wndomains.fbk.eu/wnaffect.htmland Turney, 2012; Mohammad and Yang, 2011).13This lexicon, referred to as the NRC emotion lexi-con, has annotations for eight emotions (six of Ek-man, trust, and anticipation) as well as for pos-itive and negative sentiment.14 Here, we showhow we created an ngram?emotion association lex-icon from emotion-labeled sentences in the 1000-headlines dataset and the TEC.6.1 MethodGiven a dataset of sentences and associated emo-tion labels, we compute the Strength of Association(SoA) between an n-gram n and an emotion e to be:SoA(n, e) = PMI (n, e)?
PMI (n,?e) (3)where PMI is the pointwise mutual information.PMI (n, e) = logfreq(n, e)freq(n) ?
freq(e)(4)where freq(n, e) is the number of times n occurs ina sentence with label e. freq(n) and freq(e) are thefrequencies of n and e in the labeled corpus.PMI (n,?e) = logfreq(n,?e)freq(n) ?
freq(?e)(5)where freq(n,?e) is the number of times n occurs ina sentence that does not have the label e. freq(?e) isthe number of sentences that do not have the label e.Thus, equation 4 is simplified to:SoA(n, e) = logfreq(n, e) ?
freq(?e)freq(e) ?
freq(n,?e)(6)13http://www.purl.org/net/saif.mohammad/research14Plutchik (1985) proposed a model of 8 basic emotions.252Emotion lexicon # of word types1000-headlines lexicon 152TEC lexicon 11,418WordNet Affect lexicon 1,536NRC emotion lexicon 14,000Table 7: Number of word types in emotion lexicons.Since PMI is known to be a poor estimator of associ-ation for low-frequency events, we ignored ngramsthat occurred less than five times.If an n-gram has a stronger tendency to occur ina sentence with a particular emotion label, than ina sentence that does not have that label, then thatngram?emotion pair will have an SoA score that isgreater than zero.6.2 Emotion lexicons created from the1000-headlines dataset and the TECWe calculated SoA scores for the unigrams and bi-grams in the TEC with the six basic emotions.
Allngram?emotion pairs that obtained scores greaterthan zero were extracted to form the TEC emo-tion lexicon.
We repeated these steps for the 1000-headlines dataset as well.
Table 7 shows the numberof word types in the two automatically generated andthe two manually created lexicons.
Observe that the1000-headlines dataset produces very few entries,whereas the large size of the TEC enables the cre-ation of a substantial emotion lexicon.6.3 Evaluating the TEC lexiconWe evaluate the TEC lexicon by using it for clas-sifying emotions in a setting similar to the one dis-cussed in the previous section.
The test set is the250-headlines dataset.
The training set is the 1000-headlines dataset.
We used binary features that cap-tured the presence or absence of unigrams and bi-grams just as before.
Additionally, we also usedinteger-valued affect features that captured the num-ber of word tokens in a sentence associated with dif-ferent emotions labels in the TEC emotion lexiconand the WordNet Affect lexicon.
For example, if asentence has two joy words and one surprise word,then the joy feature has value 2, surprise has value1, and all remaining affect features have value 0.15We know from the results in Table 6 (I.a.
and I.c)that using the Twitter Emotion Corpus in addition15Normalizing by sentence length did not give better results.to the 1000-headlines training data significantly im-proves results.
Now we investigate if the TEC lex-icon, which is created from TEC, can similarly im-prove performance.
The rows under II in Table 6give the results.Observe that even though the TEC lexicon is aderivative of the TEC that includes fewer unigramsand bigrams, the classifiers using the TEC lexiconproduces an F-score (row II.a.)
significantly higherthan in the scenarios of I.a.
and almost as high as inI.c.2.
This shows that the TEC lexicon successfullycaptures the word?emotion associations that are la-tent in the Twitter Emotion Corpus.
We also find thatthe the classifiers perform significantly better whenusing the TEC lexicon (row II.a.)
than when usingthe WordNet Affect lexicon (row II.b.
), but not aswell as when using the NRC emotion lexicon (rowII.c.).
The strong results of the NRC emotion lexi-con are probably because of its size and because itwas created by direct annotation of words for emo-tions, which required significant time and effort.
Onthe other hand, the TEC lexicon can be easily im-proved further by compiling an even larger set oftweets using synonyms and morphological variantsof the emotion words used thus far.7 Conclusions and Future WorkWe compiled a large corpus of tweets and associ-ated emotions using emotion-word hashtags.
Eventhough the corpus has tweets from several thousandpeople, we showed that the self-labeled hashtag an-notations are consistent.
We also showed how theTwitter emotion corpus can be combined with la-beled data from a different target domain to improveclassification accuracy.
This experiment was espe-cially telling since it showed that self-labeled emo-tion hashtags correspond well with annotations oftrained human judges.
Finally we extracted a largeword?emotion association lexicon from the Twitteremotion corpus.
Our future work includes collect-ing tweets with hashtags for various other emotionsand also hashtags that are near-synonyms of the ba-sic emotion terms described in this paper.AcknowledgmentsWe thank Tara Small and Peter Turney for helpfuldiscussions.
For Archivist, we thank its creators.253ReferencesCecilia O. Alm and Richard Sproat, 2005.
Emotional se-quencing and development in fairy tales, pages 668?674.
Springer.Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat.2005.
Emotions from text: Machine learning for text-based emotion prediction.
In Proceedings of the JointConference on HLT?EMNLP, Vancouver, Canada.Saima Aman and Stan Szpakowicz.
2007.
Identifyingexpressions of emotion in text.
In Vclav Matous?ek andPavel Mautner, editors, Text, Speech and Dialogue,volume 4629 of Lecture Notes in Computer Science,pages 196?205.
Springer Berlin / Heidelberg.Johan Bollen, Alberto Pepe, and Huina Mao.
2009.Modeling public mood and emotion: Twitter sentimentand socio-economic phenomena.
CoRR.Anthony C. Boucouvalas.
2002.
Real time text-to-emotion engine for expressive internet communica-tion.
Emerging Communication: Studies on New Tech-nologies and Practices in Communication, 5:305?318.J.
R. G. Bougie, R. Pieters, and M. Zeelenberg.
2003.Angry customers don?t come back, they get back: Theexperience and behavioral implications of anger anddissatisfaction in services.
Open access publicationsfrom tilburg university, Tilburg University.Danah Boyd, Scott Golder, and Gilad Lotan.
2010.Tweet, Tweet, Retweet: Conversational Aspects ofRetweeting on Twitter.
volume 0, pages 1?10, LosAlamitos, CA, USA.
IEEE Computer Society.Soumaya Chaffar and Diana Inkpen.
2011.
Using a het-erogeneous dataset for emotion analysis in text.
InCanadian Conference on AI, pages 62?67.Hal Daume?.
2007.
Frustratingly easy domain adapta-tion.
In Conference of the Association for Computa-tional Linguistics (ACL), Prague, Czech Republic.Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,and Eric P. Xing.
2010.
A latent variable modelfor geographic lexical variation.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1277?1287, Stroudsburg,PA.
Association for Computational Linguistics.Paul Ekman.
1992.
An argument for basic emotions.Cognition and Emotion, 6(3):169?200.Virginia Francisco and Pablo Gerva?s.
2006.
Automatedmark up of affective information in english texts.
InPetr Sojka, Ivan Kopecek, and Karel Pala, editors,Text, Speech and Dialogue, volume 4188 of LectureNotes in Computer Science, pages 375?382.
SpringerBerlin / Heidelberg.Michel Genereux and Roger P. Evans.
2006.
Distin-guishing affective states in weblogs.
In AAAI-2006Spring Symposium on Computational Approaches toAnalysing Weblogs, pages 27?29, Stanford, California.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twittersentiment classification using distant supervision.
InFinal Projects from CS224N for Spring 2008?2009 atThe Stanford Natural Language Processing Group.Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and NinaWacholder.
2011.
Identifying sarcasm in twitter: acloser look.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies: short papers - Vol-ume 2, pages 581?586, Portland, Oregon.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD, 11:10?18.Lars E. Holzman and William M. Pottenger.
2003.
Clas-sification of emotions in internet chat: An applicationof machine learning using speech phonemes.
Techni-cal report, Leigh University.David John, Anthony C. Boucouvalas, and Zhe Xu.2006.
Representing emotional momentum within ex-pressive internet communication.
In Proceedings ofthe 24th IASTED international conference on Internetand multimedia systems and applications, pages 183?188, Anaheim, CA.
ACTA Press.Elsa Kim, Sam Gilbert, Michael J. Edwards, and ErhardtGraeff.
2009.
Detecting sadness in 140 characters:Sentiment analysis of mourning michael jackson ontwitter.Hugo Liu, Henry Lieberman, and Ted Selker.
2003.A model of textual affect sensing using real-worldknowledge.
In Proceedings of the 8th internationalconference on Intelligent user interfaces, IUI ?03,pages 125?132, New York, NY.
ACM.Chunling Ma, Helmut Prendinger, and Mitsuru Ishizuka.2005.
Emotion estimation and reasoning based on af-fective textual interaction.
In J. Tao and R. W. Pi-card, editors, First International Conference on Af-fective Computing and Intelligent Interaction (ACII-2005), pages 622?628, Beijing, China.Rada Mihalcea and Hugo Liu.
2006.
A corpus-based approach to finding happiness.
In AAAI-2006Spring Symposium on Computational Approaches toAnalysing Weblogs, pages 139?144.
AAAI Press.Saif M. Mohammad and Peter D. Turney.
2012.
Crowd-sourcing a word?emotion association lexicon.
To Ap-pear in Computational Intelligence.Saif M. Mohammad and Tony Yang.
2011.
TrackingSentiment in Mail: How Genders Differ on EmotionalAxes.
In Proceedings of the 2nd Workshop on Com-putational Approaches to Subjectivity and SentimentAnalysis (WASSA 2.011), pages 70?79, Portland, Ore-gon.
Association for Computational Linguistics.254Saif M. Mohammad.
2012a.
From once upon a timeto happily ever after: Tracking emotions in mail andbooks.
To Appear in Decision Support Systems.Saif M. Mohammad.
2012b.
Portable features for emo-tion classification.
In Proceedings of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies (NAACL-HLT 2012), Montreal, Canada.
Association for Com-putational Linguistics.Alena Neviarouskaya, Helmut Prendinger, and MitsuruIshizuka.
2009.
Compositionality principle in recog-nition of fine-grained emotions from text.
In Proceed-ings of the Proceedings of the Third International Con-ference on Weblogs and Social Media (ICWSM-09),pages 278?281, San Jose, California.Lisa Pearl and Mark Steyvers.
2010.
Identifying emo-tions, intentions, and attitudes in text using a gamewith a purpose.
In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches toAnalysis and Generation of Emotion in Text, Los An-geles, California.John Platt.
1999.
Using analytic qp and sparseness tospeed training of support vector machines.
In In Neu-ral Info.
Processing Systems 11, pages 557?563.
MITPress.Robert Plutchik.
1985.
On emotion: The chicken-and-egg problem revisited.
Motivation and Emotion,9(2):197?200.Niklas Ravaja, Timo Saari, Marko Turpeinen, Jari Laarni,Mikko Salminen, and Matias Kivikangas.
2006.
Spa-tial presence and emotions during video game playing:Does it matter with whom you play?
Presence: Tele-operators and Virtual Environments, 15(4):381?392.Tapas Ray.
2011.
The ?story?
of digital excess in revo-lutions of the arab spring.
Journal of Media Practice,12(2):189?196.Julia Skinner.
2011.
Social media and revolution: Thearab spring and the occupy movement as seen throughthree information studies paradigms.
Sprouts: Work-ing Papers on Information Systems, 11(169).Carlo Strapparava and Rada Mihalcea.
2007.
Semeval-2007 task 14: Affective text.
In Proceedings ofSemEval-2007, pages 70?74, Prague, Czech Republic.Carlo Strapparava and Alessandro Valitutti.
2004.Wordnet-Affect: An affective extension of WordNet.In Proceedings of the 4th International Conferenceon Language Resources and Evaluation (LREC-2004),pages 1083?1086, Lisbon, Portugal.Andranik Tumasjan, Timm O Sprenger, Philipp G Sand-ner, and Isabell M Welpe.
2010.
Predicting electionswith twitter : What 140 characters reveal about po-litical sentiment.
Word Journal Of The InternationalLinguistic Association, pages 178?185.Juan D. Vela?squez.
1997.
Modeling emotions andother motivations in synthetic agents.
In Proceedingsof the fourteenth national conference on artificial in-telligence and ninth conference on Innovative appli-cations of artificial intelligence, AAAI?97/IAAI?97,pages 10?15.
AAAI Press.Xu Zhe and A Boucouvalas, 2002.
Text-to-Emotion En-gine for Real Time Internet CommunicationText-to-Emotion Engine for Real Time Internet Communica-tion, pages 164?168.255
