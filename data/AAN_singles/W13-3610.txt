Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 74?81,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsGrammatical Error Correction as Multiclass Classificationwith Single Model?Zhongye Jia, Peilu Wang and Hai Zhao?MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems,Center for Brain-Like Computing and Machine IntelligenceDepartment of Computer Science and Engineering, Shanghai Jiao Tong University800 Dongchuan Road, Shanghai 200240, China{jia.zhongye,plwang1990}@gmail.com,zhaohai@cs.sjtu.edu.cnAbstractThis paper describes our system in the sharedtask of CoNLL-2013.
We illustrate that gram-matical error detection and correction can betransformed into a multiclass classificationtask and implemented as a single-model sys-tem regardless of various error types with theaid of maximum entropy modeling.
Our sys-tem achieves the F1 score of 17.13% on thestandard test set.1 Introduction and Task DescriptionGrammatical error correction is the task of auto-matically detecting and correcting erroneous wordusage and ill-formed grammatical constructions intext (Dahlmeier et al 2012).
This task could be help-ful for hundreds of millions of people around the worldthat are learning English as a second language.
Al-though there have been much of work on grammaticalerror correction, the current approaches mainly focuson very limited error types and the result is far fromsatisfactory.The CoNLL-2013 shared task, compared with theprevious Help Our Own (HOO) tasks focusing on onlydeterminer and preposition errors, considers a morecomprehensive list of error types, including determiner,preposition, noun number, verb form, and subject-verb agreement errors.
The evaluation metric used inCoNLL-2013 is Max-Matching (M2) (Dahlmeier andNg, 2012) precision, recall and F1 between the systemedits and a manually created set of gold-standard ed-its.
The corpus used in CoNLL-2013 is NUS Corpusof Learner English (NUCLE) of which the details aredescribed in (Dahlmeier et al 2013).In this paper, we describe the system submissionfrom the team 1 of Shanghai Jiao Tong Univer-?This work was partially supported by the National Natu-ral Science Foundation of China (Grant No.60903119, GrantNo.61170114, and Grant No.61272248), and the NationalBasic Research Program of China (Grant No.2009CB320901and Grant No.2013CB329401).
?Corresponding authorsity (SJT1).
Grammatical error detection and correc-tion problem is treated as multiclass classification task.Unlike previous works (Dahlmeier et al 2012; Ro-zovskaya et al 2012; Kochmar et al 2012) that traina model upon each error type, we use one single modelfor all error types.
Instead of the original error type, amore detailed version of error types is used as class la-bels.
A rule based system generates labels from thegolden edits utilizing an extended version of Leven-shtein edit distance.
We use maximum entropy (ME)model as classifier to obtain the error types and userules to do the correction.
Corrections are made us-ing rules.
Finally, the corrections are filtered using lan-guage model (LM).2 System ArchitectureOur system is a pipeline of grammatical error detectionand correction.
We treats grammatical error detectionas a classification task.
First all the tokens are relabeledaccording to the golden annotation and a sequence ofmodified version of error types is generated.
This re-labeling task is rule based using an extended versionof Levenshtein edit distance which will be discussedin the following section.
Then with the modified errortypes as the class labels, a classifier using ME modelis trained.
The grammatical error correction is alsorule based, which is basically the reverse of the rela-beling phase.
The modefied version of error types thatwe used is much more detailed than the original fivetypes so that it enables us to use one rule to do the cor-rection for each modified error type.
After all, the cor-rections are filtered by LM, to remove those correctionsthat seem much worse than the original sentence.As typical classification task, we have a training stepand a test step.
The training step consists three phases:?
Error types relabeling.?
Training data refinement.?
ME training.The test step includes three phases:?
ME classification.74?
Error correction according to lebels.?
LM filtering.2.1 Rebeling by Levenshtein Edit Distancewith InflectionIn CoNLL-2013 there are 5 error types but they cannotbe used directly as class labels, since they are too gen-eral for error correction.
For example, the verb formerror includes all verb form inflections such as con-verting a verb to its infinitive form, gerund form, pastetense, paste participle, passive voice and so on.
Previ-ous approaches (Dahlmeier et al 2012; Rozovskaya etal., 2012; Kochmar et al 2012) manually decomposeeach error types to more detailed ones.
For example,in (Dahlmeier et al 2012), the determinater error isdecomposed into:?
replacement determiner (RD): { a?
the }?
missing determiner (MD): { ??
a }?
unwanted determiner (UD): { a?
?
}For limited error types such as merely determinativeserror and preposition error in HOO 2012, manually de-composition may be sufficient.
But for CoNLL-2013with 5 error types including complicated verb inflec-tion, an automatic method to decompose error types isneeded.
We present an extended version of Levenshteinedit distance to decompose error types into more de-tailed class labels and relabel the input with the labelsgenerated.The original Levenshtein edit distance has 4 edittypes: unchange (U), addition (A), deletion (D) andsubstitution (S).
We extend the ?substitution?
editinto two types of edits: inflection (I) and the orig-nal substitution (S).
To judge whether two words canbe inflected from each other, the extended algorithmneeds lemmas as input.
If the two words have thesame lemma, they can be inflected from each other.The extended Levenshtein edit distance with inflec-tion is shown in Algorithm 1.
It takes the source to-kens toksrc, destination tokens tokdst and their lemmaslemsrc, lemdst as input and returns the edits E and theparameters of edits P. For example, for the golden edit{look?
have been looking at}, the edits E returned byDISTANCE will be {A,A,U ,A}, and the parametersP of edits returned by DISTANCE will be {have, been,looking, at}.Then with the output of DISTANCE, the labels canbe generated by calculating the edits between originaltext and golden edits.
For those tokens without errors,we directly assign a special label ???
to them.
Thetricky part of the relabeling algorithm is the problemof the edit ?addition?, A.
A new token can only beadded before or after an existing token.
Thus for la-bels with addition, we must find some token that thelabel can be assigned to.
That sort of token is definedas pivot.
A pivot can be a token that is not changed inAlgorithm 1 Levenshtein edit distance with inflection1: function DISTANCE(toksrc, tokdst, lemsrc,lemdst)2: (lsrc, ldst)?
(len(toksrc), len(tokdst))3: D[0 .
.
.
lsrc][0 .
.
.
ldst]?
04: B[0 .
.
.
lsrc][0 .
.
.
ldst]?
(0, 0)5: E[0 .
.
.
lsrc][0 .
.
.
ldst]?
?6: for i?
1 .
.
.
lsrc do7: D[i][0]?
i8: B[i][0]?
(i?
1, 0)9: E[i][0]?
D10: end for11: for j ?
1 .
.
.
ldst do12: D[0][j]?
j13: B[0][j]?
(0, j ?
1)14: E[0][j]?
A15: end for16: for i?
1 .
.
.
lsrc; j ?
1 .
.
.
ldst do17: if toksrc[i?
1] = tokdst[j ?
1] then18: D[i][j]?
D[i?
1][j ?
1]19: B[i][j]?
(i?
1, j ?
1)20: E[i][j]?
U21: else22: m ?
min(D[i ?
1][j ?
1], D[i ?1][j], D[i][j ?
1])23: if m = D[i?
1][j ?
1] then24: D[i][j]?
D[i?
1][j ?
1] + 125: B[i][j]?
(i?
1, j ?
1)26: if lemsrc[i ?
1] = lemdst[j ?
1]then27: E[i][j]?
S28: else29: E[i][j]?
I30: end if31: else if m = D[i?
1][j] then32: D[i][j]?
D[i?
1][j] + 133: B[i][j]?
(i?
1, j)34: E[i][j]?
D35: else if m = D[i][j ?
1] then36: D[i][j]?
D[i][j ?
1] + 137: B[i][j]?
(i, j ?
1)38: E[i][j]?
A39: end if40: end if41: end for42: (i, j)?
(lsrc, ldst)43: while i > 0 ?
j > 0 do44: insert E[i][j] into head of E45: insert tokdst[j ?
1] into head of P46: (i, j)?
B[i][j]47: end while48: return (E,P)49: end functionan edit, such as the ?apple?
in edit {apple ?
an ap-ple}, or some other types of edit such as the inflectionof ?look?
to ?looking?
in edit {look?
have been look-75ing at}.
In the CoNLL-2013 task, the addition edits aremostly adding articles or determinaters, so when gener-ating the label, adding before the pivot is preferred andonly those trailing edits are added after the last pivot.At last, the label is normalized to upper case.The BNF syntax of labels is defined in Figure 1.
Thethe non-terminal ?inflection-rules?
can be substitutedby terminals of inflection rules that are used for cor-recting the error types of noun number, verb form, andsubject-verb agreement errors.
All the inflection rulesare listed in Table 1.
The ?stop-word?
can be subsi-tuted by terminals of stop words which contains all ar-ticles, determinnaters and prepositions for error typesof determiner and preposition, and a small set of othercommon stop words.
All the stop words are listed inTable 2.?label?
::= ?simple-label?
| ?compound-label??simple-label?
::= ?pivot?
| ?add-before?
| ?add-after??compound-label?
::= ?add-before?
?pivot?| ?pivot?
?add-after?| ?add-before?
?pivot?
?add-after??pivot?
::= ?inflection?
| ?unchange?
| ?substitution?| ?deletion??add-before?
::= ?stop-word?
?| ?stop-word???add-before??add-after?
::= ?
?stop-word?| ??stop-word??add-after??substitution?
::= ?stop-word??inflection?
::= ?inflection-rules??unchange?
::= ??deletion?
::= ?Figure 1: BNF syntax of labelAlgorithm 2 is used to generate the label from theextended Levenshtein edits according to the syntax de-fined in Figure 1.
It takes the original tokens, tokorigand golden edit tokens, tokgold in an annotation andtheir lemmas, lemorig, lemgold as input and returns thegenerated label L. For our previous example of edit{looked ?
have been looking at}, the L returned byRELABEL is {HAVE?BEEN?GERUND?AT}.
Someother examples of relabeling are demonstrated in Ta-ble 3.The correction step is done by rules according to thelabels.
The labels are parsed with a simple LL(1) parserand edits are made according to labels.
A bit of extrawork has to be taken to handle the upper/lower caseproblem.
For additions and substitutions, the wordsadded or substituted are normalized to lowercase.
Forinflections, original case of words are reserved.
Thena bunch of regular expressions are applied to correctupper/lower case for sentence head.Catalog RulesNoun Number LEMMA, NPLURALVerb Number VSINGULAR, LEMMAVerb Tense LEMMA, GERUND, PAST, PARTTable 1: Inflection rulesCatalog WordsDeterminater a an the my your his her its ourtheir this that these thosePreposition about along among around as atbeside besides between by downduring except for from in insideinto of off on onto outside overthrough to toward towards un-der underneath until up upon withwithin withoutModal Verb can could will would shall shouldmust may mightBE be am is are was were beenHAVE have has hadOther many muchTable 2: Stop wordsTokens Edit Labelto {to reveal?revealing} ?reveal GERUNDa {a woman?women} ?woman NPLURALdeveloping {developing world THE?wold ?the developing world} ?a {a?
?}
?in {in?on} ONapple {apple?an apple} AN?Table 3: Examples of relabeling2.2 Training Corpus RefinementThe corpus used to train the grammatical error recog-nition model is highly imbalanced.
The original train-ing corpus has 57,151 sentences and only 3,716 ofthem contain at least one grammatical error, and only5,049 of the total 1,161K words are needed to be cor-rected.
This characteristic makes it hard to get a well-performed machine learning model, since the samplesto be recognized are so sparse and those large amountof correct data will severely affect the machine learn-ing process as it is an optimization on the global train-ing data.
While developing our system, we foundthat only using sentences containing grammatical er-rors will lead to a notable improvement of the result.76Algorithm 2 Relabeling using the extended Leven-shtein edit distance1: function RELABEL(tokorig , tokgold, lemorig ,lemgold)2: (E,P) ?
DISTANCE(tokorig, tokgold,lemorig , lemgold)3: pivot?
number of edits in E that are not A4: L?
?5: L?
?
?6: while i < length of E do7: if E[i] = A then8: L?
L+ label of edit E[i] with P[i]9: i?
i + 110: else11: l?
L+ label of edit E[i] with P[i]12: pivot?
pivot?
113: if pivot = 0 then14: i?
i + 115: while i < length of E do16: l?
l +?+ P[i]17: i?
i + 118: end while19: end if20: push l into L21: L?
?
?22: end if23: end while24: L?
upper case of L25: return L26: end functionInspired by this phenomenon, we propose a method torefine the training corpus which will reduce the errorsparsity of the training data and notably improve therecall rate.The refined training corpus is composed of contextscontaining grammatical errors.
To keep the informa-tion which may have effects on the error identificationand classification, those contexts are selected based onboth syntax tree and n-gram, of which the process isshown in Algorithm 3.
For a word with error, its syntaxcontext of size c is those words in the minimal subtreein the syntax tree with no less than c leaf nodes; and itsn-gram context of size n is n?
1 words before and af-ter itself.
In the CORPUSREFINE algorithm, the input cgives the size of syntax context and n provides the sizeof the n-gram context.
These two parameters decidethe amount of information that may affect the recogni-tion of errors.
To obtain the context, given a sentencecontaining a grammatical error, we first get a minimumsyntax tree whose number of terminal nodes exceed thec threshold, then check whether the achieved contextcontaining the error word?s n-gram, if not, add the n-gram to the context.
At last the context is returned byCORPUSREFINE.An example illustrating this process is presented inFigure 2.
In this example, n and c are both set to 5,Algorithm 3 Training Corpus Refine Algorithm1: function CORPUSREFINE(sentence, c, n)2: context?
?3: if sentence contains no error then4: return ?5: end if6: build the syntax tree T of sentence7: enode?
the node with error in T8: e?
enode9: while True do10: pnode?
parent node of e in T11: cnodes ?
all the children nodes of pnodein T12: if len(cnodes) > c then13: context?
cnodes14: break15: end if16: e?
pnode17: end while18: i?
the position of enode in context19: l?
size of context20: if i < n then21: add (n-i) words before context at the headof context22: end if23: if l-i<n then24: append (l-i) words after context incontext25: end if26: return context27: end functionthe minimal syntax tree and the context decided by itare colored in green.
Since the syntax context in thegreen frame does not contain the error word?s 5-gram,therefore, the 5-gram context in the blue frame is addedto the syntax context and the final achieved context ofthis sentence is ?have to stop all works for the develop-ment?.2.3 LM FilterAll corrections are filtered using a large LM.
Onlythose corrections that are not too much worse than theoriginal sentences are accepted.
Perplexity (PPL) of then-gram is used to judge whether a sentence is good:PPL = 2?
?w?n-gram p(w) log p(w),We use rPPL, the ratio between the PPL before andafter correction, as a metric of information gain.rPPL =PPLcorrectedPPLoriginal,Only those corrections with an rPPL lower than a cer-tain threshold are accepted.77N-gram ContextThen the innovators have to stop all works for the development .SRBNPVP .DT NNS VBPSVPTOVPVBNPPPDT NNS INNPDT NNSyntax ContextFigure 2: Example of training corpus refinement3 FeaturesThe single model approach enables us only to optimizeone feature set for all error type in the task, which candrastically reduce the computational cost in feature se-lection.As many previous works have proposed various offeatures, we first collected features from different pre-vious works including (Dahlmeier et al 2012; Ro-zovskaya et al 2012; HAN et al 2006; Rozovskayaet al 2011; Tetreault, Joel R and Chodorow, Martin,2008).
Then experiments with different features werebuilt to test these features?
effectiveness and only thosehave positive contribution to the final performancewere preserved.
The features we used are presented inTable 4, where word0 is the word that we are generat-ing features for, and word and POS is the word itselfand it?s POS tag for various components.
NPHead de-notes the head of the minimum Noun Phrase (NP) insyntax tree.
wordNP?1 represents the word appearingbefore NP in the sentence.
NC stands for noun com-pound and is composed of the last n words (n ?
2)in NP which are tagged as a noun.
Verb feature isthe word that is tagged as a verb whose direct objectis the NP containing current word.
Adj feature repre-sents the first word in the NP whose POS is adjective.Prep feature denotes the preposition word if it imme-diately precedes the NP.
DPHead is the parent of thecurrent word in the dependency tree, and DPRel is thedependency relation with parent.4 Experiments4.1 Data SetsThe CoNLL-2013 training data consist of 1,397 arti-cles together with gold-standard annotation.
The docu-ments are a subset of the NUS Corpus of Learner En-glish (NUCLE) (Dahlmeier et al 2013).
The officialtest data consists of 50 new essays which are also fromNUCLE.
The first 25 essays were written in response toone prompt.
The remaining 25 essays were written inresponse to a second prompt.
One of the prompts hadbeen used for the training data, while the other promptis new.
More details of the data set are described in (Nget al 2013).We split the the entire training corpus ALL by article.For our training step, we randomly pick 90% articles ofALL and build a training corpus TRAIN of 1,258 arti-cles.
The rest 10% of ALL with 139 articles are fordeveloping corpus DEV .For the final submission, we use the entire corpusALL for relabeling and training.78Feature ExampleLexical featuresword0 (current word) thewordi, (i = ?1,?2,?3) man, stared, at, old, oak,treen words before word0,(n=2, 3, 4)stared+at,man+stared+at.
.
.n words after word0, (n=2,3, 4)old+oak, old+oak+tree .
.
.wordi + POSi, (i =?1,?2,?3)stared+VBD, at+IN.
.
.First word in NP theith word before/after NP,(i = ?1,?2,?3)man, stared, at, period .
.
.wordNP?1 + NP at + ( the + old + oak +tree )Bag of words in NP old+oak+the+treeNC oak treeAdj + NC old oak treeAdj POS + NC JJ+oak treePOS featuresPOS0 (current word POS) NNSPOSi, (i = ?1,?2,?3) NN, VBD, IN .
.
.POS?n + POS?
(n?1) +?
?
?+POS?1, (n = 2, 3, 4)VBD + IN, NN + VBD +IN .
.
.POS1 + POS2 + ?
?
?
+POSn, (n = 2, 3, 4)JJ + NN, JJ + NN + NNS.
.
.Bag of POS in NP DT+JJ+NN+NNHead word featuresNPHead of NP treeNPHead POS NNNPHead +NPHead POStree+NNBag of POS in NP +NPHeadDT JJ NN NN+treewordNP?1 + NPHead at+treeAdj + NPHead old+treeAdj POS + NPHead JJ+treewordNP?1 + Adj +NPHeadat+old+treewordNP?1 +Adj POS +NPHeadat+JJ+treePreposition featuresPrep atPrep + NPHead at+treePrep + Adj + NPHead at+old+treePrep + Adj POS +NPHeadat+JJ+treePrep + NC at+(oak+tree)Prep + NP at + ( the + old + oak +tree )Prep + NPHead POS at+NNPrep + Adj +NPHead POSat+old+NNPrep + Adj POS +NPHead POSat + JJ + NNPrep + Bag of NP POS at + ( DT + JJ + NN )Prep + NPHead POS +NPHeadat + NN + treeLemma featuresLemma theDependency featuresDPHead word treeDPHead POS NNDPRel detTable 4: Features for grammatical error recognition.The example sentence is:?That man stared at the oldoak tree.?
and the current word is ?the?
.Feature ExampleVerb featuresVerb staredVerb POS VBDVerb + NPHead stared+treeVerb + Adj + NPHead stared+old+treeVerb + Adj POS +NPHeadstared+JJ+treeVerb + NP stared+(the+old+oak+tree)Verb + Bag of NP POS stared+(DT+JJ+NN)Table 5: Features ContinuedCount Label1146000 ?3369 ?3088 NPLURAL2766 THE?1470 LEMMA706 A?200?300 IN AN?
THE ARE FOR TO OF100?200ON A IS GERUND PAST VSINGULAR50?100 WITH ?THE AT AN BY THIS INTO5?50 FROM TO?
WAS ABOUT WERE ?ATHESE TO?LEMMA OF?
?OF ARE?
?TO THROUGH BE?PAST AS AMONGIN?
BE?
THEIR THE?LEMMA OVER?ON HAVE?
DURING FOR?
?WITHPART ?IN HAVE WITHIN BE MANY?AN THE?NPLURAL MUCH IS?
WITH?TOWARDS ?FOR HAVE?PART ?ABOUTWILL ?UPON THEIR?
HAVE?PASTHAS?PART HAS?
HAS BY?
BEEN?BE??
AN?LEMMA THAT?
ITS HADFROM?
BETWEEN A?LEMMA4 WERE?
UPON THOSE ON?
MANY?IS??
?FROM CAN AS?3 WILL?LEMMA WILL?
TOWARD THIS?THAT ITS?
HAVE??
?BE AT?
?ASABOUT?2 WOULD WAS?
TO?BE?
THE?
?ONTO IS?PAST IS?GERUND INSIDEHAVE?BEEN?
CAN?LEMMA ?BEEN?AT1 WOULD?LEMMA WITHOUT UN-DER TO?THE TO?THAT?OFTO?HAVE THIS?WILL?
THIS?MAYTHIS?HAS?
THESE?
THE??
?OFTHEIR?LEMMA THE?GERUNDTHE?A?
THAT?HAS?BEEN?THAT?HAS?
SHOULD?
?OVEROF?THE?
OF?THE OF?GERUND OFFOF?A?
MAY?
MAY IS?TO IS?LEMMAINTO?
?INTO IN?THE?
HIS HAVE?ANHAS?PAST HAS?BEEN?GERUNDHAS?BEEN?
HAD??
HAD?
DURING?COULD CAN?BE?
CAN?
BY?GERUND?BY ?BETWEEN BESIDES BEEN?PARTBEEN AT?AN AT?A AS?THE AS?HASAROUND ARE?PAST ARE?A ARE??A??
?OF AM?Table 6: All labels after relabeling794.2 ResourcesWe use the following NLP resources in our sys-tem.
For relabeling and correction, perl mod-ule Lingua::EN::Inflect1 (Conway, 1998) is usedfor determining noun and verb number and Lin-gua::EN::VerbTense2 is used for determining verbtense.
A revised and extended version of maxi-mum entropy model3 is used for ME modeling.
Forlemmatization, the Stanford CoreNLP lemma annota-tor (Toutanova et al 2003; Toutanova and Manning,2000) is used.
The language model is built by theSRILM toolkit (Stolcke and others, 2002).
The corpusfor building LM is the EuroParl corpus (Koehn, 2005).The English part of the German-English parallel cor-pus is actually used.
We use such a corpus to buildLM for the following reasons: 1.
LM for grammaticalerror correction should be trained from corpus that it-self is grammatically correct, and the EuroParl corpushas very good quality of writing; 2. the NUCLE cor-pus mainly contains essays on subjects such as environ-ment, economics, society, politics and so on, which arein the same dormain as those of the EuroParl corpus.4.3 Relabeling the CorpusThere are some complicated edits in the annotationsthat can not be represented by our rules, for examplesubstitution of non-stopwords such as {human?
peo-ple} or {are not short of?
do not lack}.
The relabel-ing phase will ignore those ones thus it may not coverall the edits.
After all, we get 174 labels after relabel-ing on the entire corpus as shown in Table 6.
Theselabels are generated following the syntax defined inFigure1 and terminals defined in Table 1 and Table 2.Directly applying these labels for correction receives anM2 score of Precission = 91.43%,Recall = 86.92%and F1 = 89.12%.
If the non-stopwords non-inflectionedits are included, of course the labels will cover all thegolden annotations and directly applying labels for cor-rection can receive a score up to almost F1 = 100%.But that will get nearly 1,000 labels which is too com-putationally expensive for a classification task.
Cut outlabels with very low frequency such as those exists onlyonce reduces the training time, but does not give signif-icant performance improvement, since it hurts the cov-erage of error detection.
So we use all the labels fortraining.4.4 LM Filter ThresholdTo choose the threshold for rPPL, we run a series oftests on the DEV set with different thresholds.
Fromour empirical observation on those right correctionsand those wrong ones, we find the right ones seldomly1http://search.cpan.org/?dconway/Lingua-EN-Inflect-1.89/2http://search.cpan.org/?jjnapiork/Lingua-EN-VerbTense-3.003/3http://www.nactem.ac.uk/tsuruoka/maxent/have rPPL > 2, so we test thresholds ranging from 1.5to 3.
The curves are shown in Figure 3.0.140.1450.150.1550.160.1650.171.4  1.6  1.8  2  2.2  2.4  2.6  2.8  3Precision, RecallandF1 curvesLM Filter ThresholdPrecisionRecallF1Figure 3: Different thresholds of LM filtersWith higher threshold, more correction with lowerinformation gain will be obtained.
Thus the recallgrows higher but the precission grows lower.
We canobserve a peak of F1 arround 1.8 to 2.0, and the thresh-old chosen for final submission is 1.91.4.5 Evaluation and ResultThe evaluation is done by calculating the M2 precis-sion, recall and F1 score between the system outputand golden annotation.
All the error types are evalu-ated jointly.
Only one run of a team is permitted to besubmitted.
Table 7 shows our result on our DEV dataset and the official test data set.Data Set Precission Recall F1DEV 16.03% 15.88% 15.95%Official 40.04% 10.89% 17.13%Table 7: Evaluation Results5 ConclusionIn this paper, we presented the system from team 1 ofShanghai Jiao Tong University that participated in theHOO 2012 shared task.
Our system achieves an F1score of 17.13% on the official test set based on gold-standard edits.ReferencesDM Conway.
1998.
An algorithmic approach to en-glish pluralization.
In Proceedings of the Second An-nual Perl Conference.
C. Salzenberg.
San Jose, CA,O?Reilly.Daniel Dahlmeier and Hwee Tou Ng.
2012.
BetterEvaluation for Grammatical Error Correction.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 568?572, Montre?al, Canada, June.
Associa-tion for Computational Linguistics.80Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun FengNg.
2012.
NUS at the HOO 2012 Shared Task.
InProceedings of the Seventh Workshop on BuildingEducational Applications Using NLP, pages 216?224, Montre?al, Canada, June.
Association for Com-putational Linguistics.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a Large Annotated Corpus ofLearner English: The NUS Corpus of Learner En-glish.
In Proceedings of the 8th Workshop on Inno-vative Use of NLP for Building Educational Appli-cations, Atlanta, Georgia, USA.NA-RAE HAN, MARTIN CHODOROW, and CLAU-DIA LEACOCK.
2006.
Detecting Errors in En-glish Article Usage by Non-Native Speakers.
Nat-ural Language Engineering, 12:115?129, 5.Ekaterina Kochmar, ?istein Andersen, and TedBriscoe.
2012.
HOO 2012 Error Recognition andCorrection Shared Task: Cambridge University Sub-mission Report.
In Proceedings of the Seventh Work-shop on Building Educational Applications UsingNLP, pages 242?250, Montre?al, Canada, June.
As-sociation for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A Parallel CorpusFor Statistical Machine Translation.
In MT summit,volume 5.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault.
2013.
The conll-2013 shared task on grammatical error correction.
InProceedings of the Seventeenth Conference on Com-putational Natural Language Learning.Alla Rozovskaya, Mark Sammons, Joshua Gioja, andDan Roth.
2011.
University of Illinois System inHOO Text Correction Shared Task.
In Proceedingsof the 13th European Workshop on Natural Lan-guage Generation, pages 263?266.
Association forComputational Linguistics.Alla Rozovskaya, Mark Sammons, and Dan Roth.2012.
The UI System in the HOO 2012 Shared Taskon Error Correction.
In Proceedings of the SeventhWorkshop on Building Educational Applications Us-ing NLP, pages 272?280, Montre?al, Canada, June.Association for Computational Linguistics.Andreas Stolcke et al2002.
SRILM-An ExtensibleLanguage Modeling Toolkit.
In Proceedings of theinternational conference on spoken language pro-cessing, volume 2, pages 901?904.Tetreault, Joel R and Chodorow, Martin.
2008.
TheUps and Downs of Preposition Error Detection inESL Writing.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics-Volume 1, pages 865?872.
Association for Compu-tational Linguistics.Kristina Toutanova and Christopher D Manning.
2000.Enriching the Knowledge Sources Used in a Max-imum Entropy Part-of-Speech Tagger.
In Proceed-ings of the 2000 Joint SIGDAT conference on Em-pirical methods in natural language processing andvery large corpora: held in conjunction with the38th Annual Meeting of the Association for Compu-tational Linguistics-Volume 13, pages 63?70.
Asso-ciation for Computational Linguistics.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich Part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.81
