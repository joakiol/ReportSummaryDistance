Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 495?500, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsIITB-Sentiment-Analysts: Participation in Sentiment Analysisin Twitter SemEval 2013 TaskKaran Chawla, Ankit Ramteke, Pushpak BhattacharyyaDept.
of Computer Science and Engineering, IIT Bombay{chawlakaran,ankitr,pb}@cse.iitb.ac.inAbstractWe propose a method for using discourse rela-tions for polarity detection of tweets.
We havefocused on unstructured and noisy text liketweets on which linguistic tools like parsers andPOS-taggers don?t work properly.
We haveshowed how conjunctions, connectives, modalsand conditionals affect the sentiments in tweets.We have also handled the commonly used ab-breviations, slangs and collocations which areusually used in short text messages like tweets.This work focuses on a Web based applicationwhich produces results in real time.
This ap-proach is an extension of the previous work(Mukherjee et al2012).1.
IntroductionDiscourse relation is an important component ofnatural language processing which connectsphrases and clauses together to establish a cohe-rent relation.
Linguistic constructs like conjunc-tions, connectives, modals, conditionals and ne-gation do alter the sentiments of a sentence.
Forexample, the movie had quite a few memorablemoments but I still did not like it.
The overallpolarity of the sentence is negative even thoughit has one positive and one negative clause.
Thisis because of the presence of the conjunction butwhich gives more weightage to the clause fol-lowing the conjunction.Traditional works in discourse analysis use adiscourse parser (Marcu  et al 2003; Polanyi etal., 2004; Wolf et al 2005; Welner et al 2006;Narayanan et al 2009; Prasad et al 2010).Many of these works and some other works indiscourse (Taboada et al 2008; Zhou et al2011) build on the Rhetorical Structure Theory(RTS) proposed by Mann et al(1988) whichtries to identify the relations between the nucleusand satellite in the sentence.Most of the work is based on well-structured textand the methods applied on that text is not suita-ble for the discourse analysis on micro-blogsbecause of the following reasons:1.
Micro-blogs like Twitter restricts a post(tweet) to be of only 140 characters.
Thus, usersdo not use formal language to discuss theirviews.
Thus, there are abundant spelling mis-takes, abbreviations, slangs, collocations, discon-tinuities and grammatical errors.These differences cause NLP tools like POS-taggers and parsers to fail frequently, as thesetools are built for well-structured text.
Thus,most of the methods described in the previousworks are not well suited for discourse analysison Micro-blogs like text.2.
The web-based applications require afast response time.
Using a heavy linguistic re-source like parsing increases the processing timeand slows down the application.Most of the previous work on discourse analysisdoes not take into consideration the conjunc-tions, connectives, modals, conditionals etc andare based on bag-of-words model with featureslike part-of-speech information, unigrams, bi-grams etc.
along with other domain-specific fea-tures like emoticons, hashtags etc.
Our workharness the importance of discourse connectiveslike conjunctions, connectives, modals, condi-tionals etc and show that along with bag-of-words model, it gives better sentiment classifica-tion accuracy.
This work is the extension of(Mukherjee et al2012).The roadmap for the rest of the paper is as fol-lows: Section 2 studies the effect of discourserelations on sentiment analysis and identifies the495critical ones.
Section 3 talks about the semanticoperators which influence the discourse rela-tions.
Section 4 discusses the lexicon based clas-sification approach.
Section 5 describes the fea-ture engineering of the important features.
Sec-tion 6 gives the list of experiments conductedand analysis of the results.
Conclusion and Fu-ture Work is presented in Section 7.2.
Discourse Relations Critical for Sen-timent Analysis(Mukherjee et al2012) showed that that the fol-lowing discourse relations are critical for SA asall relations are not useful for SA.
Table 1 pro-vides examples of various discourse relations.Violated Expectations and Contrast: In Exam-ple 2, a simple bag-of-words feature based clas-sifier will classify it as positive.
However, it ac-tually represents a negative sentiment.
Such cas-es need to be handled separately.
In Example 5,?memorable" has (+1) score and ?not like" has (-1) score and overall polarity is 0 or objectivewhereas it should be negative as the final verdictfollowing ?but" is the deciding factor.These kinds of sentences refute the neighboringclause.
They can be classified as Conj_Prev inwhich the clause preceding the conjunction ispreferred and Conj_Fol in which the clause fol-lowing the conjunction is preferred.Conclusive or Inferential Conjunctions: Theseare the set of conjunctions, Conj_infer, that tendto draw a conclusion or inference.
Hence, thediscourse segment following them (subsequentlyin Example 11) should be given more weight.Conditionals: In Example 3, ?amazing"represent a positive sentiment.
But the final po-larity should be objective as we are talking of ahypothetical situation.Other Discourse Relations: Sentences underCause-Effect, Similarity, Temporal Sequence,Attribution, Example, Generalization and Elabo-ration, provide no contrasting, conflicting or hy-pothetical information.
They can be handled bytaking a simple bag-of-words model.3.
Semantic Operators Influencing Dis-course RelationsThere are connectives or semantic operatorspresent in the sentences which influence the dis-course relation within a sentence.
For example,in the sentence the cannon camera may bad de-spite good battery life.
The connective despiteincreases the weightage of the previous dis-course element i.e.
bad is weighted up but mayintroduces a certain kind of uncertainty whichcannot be ignored.1.
(I did not study anything throughout the seme-ster), so (I failed in the exams).2.
(Sourav failed to deliver in the penultimate test)despite (great expectations).3.
If (I had bought the amazing Nokia phone), Iwould not be crying).4.
(I love Cannon) and (I also love Sony).5.
(The movie had quite a few memorable moments)but (I still did not like it).6.
(The theater became interesting) after a while.7.
According (to the reviews), (the movie must bebad).8.
(Salman is a bad guy), for instance (he is alwayslate).9.
In addition (to the bad battery life), (the camerais also very costly).10.
In general, (cameras from cannon (take greatpictures).11.
(They were not in favour of that camera) andsubsequently (decided not to buy it).Table 1:  Examples of Discourse CoherentRelationsSimilarity, in the sentence He gave his best inthe movie, but still it was not good enough to winan Oscar.
The connective but increases theweight of the following discourse i.e.
good andwin are weighted up but presence of negationoperator also cannot be ignored.4961.
Modals: Events that are happening or arebound to happen are called realis events.
Andthose events that have possibly occurred or havesome probability to occur in distant future areknown as irrealis events.
And it is important todistinguish between the two as it also alters thesentiments in a piece of text.
Modals depict ir-realis events and just cannot be handled by sim-ple majority valence model.
(Mukherjee et al2012) divided modals into twocategories: Strong_Mod and Weak_Mod.Strong_Mod is the set of modals that express ahigher degree of uncertainty in any situation.Weak_Mod is the set of modals that expresslesser degree of uncertainty and more emphasison certain events or situations.Like conditionals, sentences with strong modalsexpress higher degree of uncertainty, thus dis-course elements near strong modals are weighteddown.
Thus, in the previous example the cannoncamera may bad despite good battery life bad istoned down.Relations AttributesConj_Fol but, however, never-theless, otherwise, yet,still, nonethelessConj_Prev till, until, despite, inspite, though, althoughConj_Inf therefore, furthermore,consequently, thus, asa result, subsequently,eventually, henceConditionals IfStrong_Mod might, could, can,would, mayWeak_Mod should, ought to, neednot, shall, will, mustNeg not, neither, never, no,norTable 2: Discourse Relations and SemanticOperators Essential for Sentiment Analysis2.
Negation: The negation operator inverts thepolarity of the sentence following it.
Usually, tohandle negation a window (typically 3-5 words)is considered and the polarities of all the wordsare reversed.
We have considered the windowsize to be 5 and reverse the polarities of all thewords within the window, till either a conjunc-tion comes or window size exceeds.
For exampleIn the sentence He gave his best in the movie,but still it was not good enough to win an Oscarpolarities of good and win are reversed.4.
Lexicon Based ClassificationWe have used Senti-WordNet (Esuli et al2006),Inquirer (Stone et.
al 1996) and the Bing Liusentiment lexicon (Hu et al2004) to find out theword polarities.
To compensate the bias effectsintroduced by the individual lexicons, we haveused three different lexicons.
The polarities ofthe reviews are given by (Mukherjee et al2012)????
(  ???
?
??????
?
?(???
))???=1??=1?????
?
???
=  ???
???
??
?????
= 0=???
???2??
?????
= 1Above equation finds the weighted, signed po-larity of a review.
The polarity of each word,pol(wij) being +1 or -1, is multiplied with its dis-course weight fij and all the weighted polaritiesare added.
Flipij indicates if the polarity of wij isto be negated.In case there is any conditional or strong modalin the sentence (indicated by ?????
= 1 ), thenthe polarity of every word in the sentence istoned down, by considering half of its assignedpolarity (+12,?12)Thus, if good occurs in the user post twice, itwill contribute a polarity of +1 ?
2 = +2 to theoverall review polarity, if ?????
= 0.
In thepresence of a strong modal or conditional, it willcontribute a polarity of+12?
2 =  +1.497All the stop words, discourse connectives andmodals are ignored during the classificationphase, as they have a zero polarity in the lexicon.We have handled commonly used slangs, ab-breviations and collocations by manually taggingthem as positive, negative or neutral.5.
Feature EngineeringThe features specific for lexicon based classifi-cation for the task sentiment Analysis, identifiedin Section 2.4, are handled as follows:a) The words following the Conj_Fol (Table 2)are given more weightage.
Hence their frequencycount is incremented by 1.We follow a naive weighting scheme wherebywe give a (+1) weightage to every word we con-sider important.
In Example 5, ?memorable" gets(+1) score, while ?did not like" gets a (-2) score,making the overall score (-1) i.e.
the examplesuggests a negative sentiment.b) The weightage of the words occurring beforethe Conj_Prev (Table 2) is increased by 1.
InExample 2, ?failed" will have polarity (-2) in-stead of (-1) and ?great expectations" will havepolarity (+1), making the overall polarity (-1),which conforms to the overall sentiment.c) The weightage of the words in the sentencescontaining conditionals (if) and strong modals(might, could, can, would, may) are toned down.e) The polarity of all words appearing within awindow of 5 from the occurrence of a negationoperator (not, neither, nor, no, never) and beforethe occurrence of a violating expectation con-junction is reversed.f) Exploiting sentence position information, thewords appearing in the first k and last k sen-tences, are given more weightage.
The value of kis set empirically.g) The Negation Bias factor is treated as a para-meter which is learnt from a small set of nega-tive polarity tagged documents.
The frequencycount of all the negative words (in a rule basedsystem) is multiplied with this factor to givenegative words more weightage than positivewords.6.
Experiments and EvaluationFor the lexicon-based approach, we performedtwo types of experiments- sentiment pertainingto a particular instance in a tweet (SemEval-2013 Task A) and generic sentiment analysis ofa tweet (SemEval-2013 Task B).
We treat boththe tasks similarly.6.1 DatasetWe performed experiments on two Datasets:1) SemEval-2013-task 2 Twitter Dataset A con-taining 4435 tweets without any external data.2) SemEval-2013-task 2 Twitter Dataset B con-taining 3813 tweets without any external data.6.2 Results on the Twitter Dataset A and BThe system performs best for the positive classtweets as shown in Table 3 and Table 4 and per-forms badly for the negative class which is dueto the fact that negative tweets can contain sar-casm which is a difficult phenomenon to capture.Also the results of the neutral category are veryless which suggests that our system is biasedtowards subjective tweets and we wish to givethe majority sentiment in the tweets.Class Precision Recall F-scorePositive 0.6706 0.5958 0.6310Negative 0.4124 0.5328 0.4649Neutral 0.0667 0.0063 0.0114Table 3: Results on Twitter Dataset AClass Precision Recall F-scorePositive 0.4809  0.5941 0.5316Negative 0.1753   0.5374 0.2643Neutral 0.6071  0.0104 0.0204Table 4: Results on Twitter Dataset B4986.3 DiscussionThe lexicon based classifier suffers from theproblem of lexeme space where it is not ablehandle all the word senses.
Also, short-noisy textlike tweets often contain various spelling mis-takes like great can be grt, g8t etc.
or tomorrowcan be tom, tomm, tommrrw etc.
which will notbe detected and handled properly.We suggest that a supervised approach compris-ing of the discourse features along with the bag-of-words model and the sense based features willimprove the results.7.
Conclusion and Future WorkWe have showed that discourse connectives,conjunctions, negations and conditionals do alterthe sentiments of a piece of text.
Most of thework on Micro-blogs like twitter is build on bag-of-words model and does not incorporate dis-course relations.
We discussed an approachwhere we can incorporate discourse relationsalong-with bag-of-words model for a web-application where parsers and taggers cannot beused as the results are required in real time.We need to take into consideration word sensesand a supervised approach to use all the featurescollectively.
Also, a spell checker would reallyhelp in the noisy text like in tweets.ReferencesA Agarwal and Pushpak Bhattacharyya.
2005.
Senti-ment Analysis: A New Approach for Effective Use ofLinguistic Knowledge and Exploiting Similarities in aSet of Documents to be classified.
International Con-ference on Natural Language Processing (ICON 05),IIT Kanpur, India, DecemberAR Balamurali, Aditya Joshi and Pushpak Bhattacha-ryya.
2011.
Harnessing WordNet Senses for Super-vised Sentiment Classification.
In Proceedings ofEmpirical Methods in Natural Language Processing(EMNLP).A Esuli and F Sebastiani, 2006.
SentiWordNet: APublicly Available Lexical Resource for OpinionMining.
In Proceedings from International Confe-rence on Language Resources and Evaluation(LREC), Genoa.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proc.
of ACM SIGKDD.Aditya Joshi, AR Balamurali, Pushpak Bhattacharyyaand R Mohanty.
2010.
C-Feel-It: A Sentiment Ana-lyzer for Micro-blogs', Annual Meeting of the Associ-ation of Computational Linguistics (ACL 2011), Ore-gon, USA.William C. Mann and Sandra A. Thompson.
Rhetori-cal Structure Theory: Toward a functional theory oftext organization.
Text, 8 (3), 243-281.
1988R Narayanan, Bing Liu and A Choudhary.
2009.
Sen-timent Analysis of Conditional Sentences.
In Pro-ceedings of Conference on Empirical Methods inNatural Language Processing (EMNLP-09).L Polanyi and A Zaenen.
2004.
Contextual ValenceShifters.
In James G. Shanahan, Yan Qu, JanyceWiebe (eds.
), Computing Attitude and Affect in Text:Theory and Applications, pp.
1-10.BP Ramesh, R Prasad and H Yu.
2010.
Identifyingexplicit discourse connective in biomedical text.
InAnnual Symposium proceedings, AMIA Symposium,Vol.
2010, pp.
657-661.R Soricut and D Marcu.
2003.
Sentence level dis-course parsing using syntactic and lexical informa-tion.
In Proc.
of HLT-NAACLPJ Stone, DC Dunphy, MS Smith, DM Ogilvie andAssociates.
1996.
The General Inquirer: A ComputerApproach to Content Analysis.
The MIT PressSubhabrata Mukherjee and Pushpak Bhattacharyya.2012.
Sentiment Analysis in Twitter with LightweightDiscourse Analysis.
In Proceedings of  COLING2012Subhabrata Mukherjee and Pushpak Bhattacharyya.2012.
Sentiment Analysis in Twitter with LightweightDiscourse Analysis.
In Proceedings of the 21st ACMConference on Information and Knowledge Manage-ment (CIKM), short paper.Subhabrata Mukherjee, AR Balamurali, Akshat Maluand Pushpak Bhattacharyya.
2012.
TwiSent: A Ro-499bust Multistage System for Analyzing Sentiment onTwitter.
In Proceedings of the 21st ACM Conferenceon Information and Knowledge Management (CIKM),poster paper.Maite Taboada, Julian Brooke and Kimberly Voll.2008.
Extracting Sentiment as a Function of Dis-course Structure and Topicality.
Simon Fraser Unive-risty School of Computing Science Technical Report.B Wellner, J Pustejovski, A Havasi, A Rumshiskymand R Suair.
2006.
Classification of discourse cohe-rence relations: An exploratory study using multipleknowledge sources.
In Proc.
of SIGDIALF Wolf and E Gibson.
2005.
Representing DiscourseCoherence: A Corpus-based Study.
ComputationalLinguistics, 31(2), pp.
249-287.Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Weiand Kam-Fai Wong.
2011.
Unsupervised discovery ofdiscourse relations for eliminating intra-sentence po-larity ambiguities.
In Proceedings of EMNLP.500
