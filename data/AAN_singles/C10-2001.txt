Coling 2010: Poster Volume, pages 1?8,Beijing, August 2010Towards the Adequate Evaluation of Morphosyntactic TaggersSzymon Acedan?skiInstitute of Computer Science,Polish Academy of SciencesInstitute of Informatics,University of Warsawaccek@mimuw.edu.plAdam Przepi?rkowskiInstitute of Computer Science,Polish Academy of SciencesInstitute of Informatics,University of Warsawadamp@ipipan.waw.plAbstractThere exists a well-established and almostunanimously adopted measure of taggerperformance, namely, accuracy.
Althoughit is perfectly adequate for small tagsetsand typical approaches to disambiguation,we show that it is deficient when appliedto rich morphological tagsets and proposevarious extensions designed to better cor-relate with the real usefulness of the tag-ger.1 IntroductionPart-of-Speech (PoS) tagging is probably themost common and best researched NLP task, thefirst step in many higher level processing solu-tions such as parsing, but also information re-trieval, speech recognition and machine transla-tion.
There are also well established evaluationmeasures, the foremost of which is accuracy, i.e.,the percent of words for which the tagger assignsthe correct ?
in the sense of some gold standard?
interpretation.Accuracy works well for the original PoS tag-ging task, where each word is assumed to have ex-actly one correct tag, and where the informationcarried by a tag is limited roughly to the PoS ofthe word and only very little morphosyntactic in-formation, as in typical tagsets for English.
How-ever, there are two cases where accuracy becomesless than adequate: the situation where the goldstandard and / or the tagging results contain mul-tiple tags marked as correct for a single word, andthe use of a rich morphosyntactic (or morphologi-cal) tagset.The first possibility is discussed in detail in(Karwan?ska and Przepi?rkowski, 2009), but theneed for an evaluation measure for taggers whichdo not necessarily fully disambiguate PoS was al-ready noted in (van Halteren, 1999), where the useof standard information retrieval measures preci-sion and recall (as well as their harmonic mean,the F-measure) is proposed.
Other natural gen-eralisations of the accuracy measure, able to dealwith non-unique tags either in the gold standard1or in the tagging results, are proposed in (Kar-wan?ska and Przepi?rkowski, 2009).Standard accuracy is less than adequate alsoin case of rich morphosyntactic tagsets, wherethe full tag carries information not only aboutPoS, but also about case, number, gender, etc.Such tagsets are common for Slavic languages,but also for Hungarian, Arabic and other lan-guages.
For example, according to one com-monly used Polish tagset (Przepi?rkowski andWolin?ski, 2003), the form uda has the follow-ing interpretations: fin:sg:ter:perf (a fi-nite singular 3rd person perfective form of theverb UDAC?
?pretend?
), subst:pl:nom:n and1There are cases were it makes sense to manually assigna number of tags as correct to a given word, as any decisionwould be fully arbitrary, regardless of the amount of con-text and world knowledge available.
For example, in someSlavic languages, incl.
Polish, there are verbs which option-ally subcategorise for an accusative or a genitive comple-ment, without any variation in meaning, and there are nounswhich are syncretic between these two cases, so for such?verb + nounacc/gen?
sequences it is impossible to fully dis-ambiguate case; see also (Oliva, 2001).1subst:pl:acc:n (nominative or accusativeplural form of the neuter noun UDO ?thigh?
).Now, assuming that the right interpretation in agiven context is subst:pl:acc:n, accuracywill equally harshly penalise the other nominal in-terpretation (subst:pl:nom:n), which shareswith the correct interpretation not only PoS, butalso the values of gender and number, and thecompletely irrelevant verbal interpretation.
Amore accurate tagger evaluation measure shoulddistinguish these two non-optimal assignmentsand treat subst:pl:nom:n as partially correct.Similarly, the Polish tagset mentioned abovedistinguishes between nouns and gerunds, withsome forms actually ambiguous between thesetwo interpretations.
For example, zadanie may beinterpreted as a nominative or accusative form ofthe noun ZADANIE ?task?, or a nominative or ac-cusative form of the gerund derived from the verbZADAC?
?assign?.
Since gerunds and nouns havevery similar distributions, any error in the assign-ment of part of speech, noun vs. gerund, will mostprobably not matter for a parser of Polish ?
itwill still be able to construct the right tree, pro-vided the case is correctly disambiguated.
How-ever, the ?all-or-nothing?
nature of the accuracymeasure regards the tag differing from the correctone only in part of speech or in case as harshly,as it would regard an utterly wrong interpretation,say, as an adverb.In what follows we propose various evaluationmeasures which differentiate between better andworse incorrect interpretations, cf.
?
2.
The im-plementation of two such measures is describedin ?
3.
Finally, ?
4 concludes the paper.2 Proposed Measures2.1 Full Interpretations and PoSThe first step towards a better accuracy mea-sure might consist in calculating two accu-racy measures: one for full tags, and theother only for fragments of tags represent-ing parts of speech.
Two taggers wronglyassigning either fin:sg:ter:perf (T1) orsubst:pl:nom:n (T2) instead of the correctsubst:pl:acc:n would fare equally well withrespect to the tag-level accuracy, but T2 would be?
rightly ?
evaluated as better with respect tothe PoS-level accuracy.The second example given in ?
1 shows, how-ever, that the problem is more general and that atagger which gets the PoS wrong (say, gerund in-stead of noun) but all the relevant categories (case,number, gender) right may actually be more use-ful in practice than the one that gets the PoS rightat the cost of confusing cases (say, accusative in-stead of nominative).2.2 Positional AccuracyA generalisation of the idea of looking separatelyat parts of speech is to split tags into their compo-nents (or positions) and measure the correctnessof the tag by calculating the F-measure.
For ex-ample, if the (perfective, affirmative) gerundial in-terpretation ger:sg:nom:n:perf:aff is as-signed instead of the correct nominal interpreta-tion subst:sg:nom:n, the tags agree on 3 po-sitions (sg, nom, n), so the precision is 36 , the re-call ?
34 , which gives the F-measure of 0.6.
Obvi-ously, the assignment of the correct interpretationresults in F-measure equal 1.0, and the completelywrong interpretation gives F-measure 0.0.
Takingthese values instead of the ?all-or-nothing?
0 or 1,accuracy is reinterpreted as the average F-measureover all tag assignments.Note that while this measure, let us call it po-sitional accuracy (PA), is more fine-grained thanthe standard accuracy, it wrongly treats all com-ponents of tags as of equal importance and dif-ficulty.
For example, there are many case syn-cretisms in Polish, but practically no ambiguitiesconcerning the category of negation (see the valueaff above), so case is inherently much more diffi-cult than negation, and also much more importantfor syntactic parsing, and as such it should carrymore weight when evaluating tagging results.2.3 Weighted Positional AccuracyIn the current section we make a simplifying as-sumption that weights of positions are absolute,rather than conditional, i.e., that the weight of, say,case does not depend on part of speech, word orcontext.
Once the weights are attained, weightedprecision and recall may be used as in the follow-ing example.2Assume that PoS, case, number and genderhave the same weight, say 2.0, which is 4 timeslarger than that of any other category.
Then, incase ger:sg:nom:n:perf:aff is assignedinstead of the correct subst:sg:nom:n, pre-cision and recall are given by:P = 3?
2.04?
2.0 + 2?
0.5 =23 ,R = 3?
2.04?
2.0 =34 .This results in a higher F-measure than in case ofnon-weighted positional accuracy.The following subsections propose variousways in which the importance of particular gram-matical categories and of the part of speech maybe estimated.2.3.1 Average AmbiguityThe average number of morphosyntactic inter-pretations per word is sometimes given as a roughmeasure of the difficulty of tagging.
For exam-ple, tagging English texts with the Penn Treebanktagset is easier than tagging Czech or Polish, asthe average number of possible tags per word is2.32 in English (Hajic?, 2004, p. 171), while it is3.65 (Hajic?
and Hladk?, 1997, p. 113) and 3.32(Przepi?rkowski, 2008, p. 44) for common tagsetsfor Czech and Polish, respectively.By analogy, one measure of the difficulty of as-signing the right value of a given category or partof speech is the average number of different val-ues of the category per word.2.3.2 Importance for ParsingAll measures mentioned so far are intrinsic (invitro) evaluation measures, independent ?
buthopefully correlated with ?
the usefulness of theresults in particular applications.
On the otherhand, extrinsic (in vivo) evaluation estimates theusefulness of tagging in larger systems, e.g., inparsers.
Full-scale extrinsic evaluation is rarelyused, as it is much more costly and often requiresuser evaluation of the end system.In this and the next subsections we proposeevaluation measures which combine the advan-tages of both approaches.
They are variants ofthe weighted positional accuracy (WPA) measure,where weights correspond to the usefulness of agiven category (or PoS) for a particular task.Probably the most common task taking advan-tage of morphosyntactic tagging is syntactic pars-ing.
Here, weights should indicate to what extentthe parser relies on PoS and particular categoriesto arrive at the correct parse.
Such weights maybe estimated from an automatically parsed corpusin the following way:for each category (including PoS) c docount(c) = 0 {Initialise counts.
}end forfor each sentence s dofor each rule r used in s dofor each terminal symbol (word) t in theRHS of r dofor each category c referred to by r in tdoincrease count(c)end forend forend forend for{Use count(c)?s as weights.
}In prose: whenever a syntactic rule is used, in-crease counts of all morphosyntactic categories(incl.
PoS) mentioned in the terminal symbols oc-curring in this rule.
These counts may be nor-malised or used directly as weights.We assume here that either the parser producesa single parse for any sentence (assumption realis-tic only in case of shallow parsers), or that the bestor at least most probable parse may be selected au-tomatically, as in case of probabilistic grammars,or that parses are disambiguated manually.
In caseonly a non-probabilistic deep parser is available,and parses are not disambiguated manually, theExpectation-Maximisation method may be usedto select a probable parse (De?bowski, 2009) or allparses might be taken into account.Note that, once a parser is available, suchweights may be calculated automatically and usedrepeatedly for tagger evaluation, so the cost of us-ing this measure is not significantly higher thanthe cost of intrinsic measures, while at the sametime the correlation of the evaluation results withthe extrinsic application is much higher.32.3.3 Importance for Corpus SearchThe final variant (many more are imagin-able) of WPA that we would like to de-scribe here concerns another application of tag-ging, namely, for the annotation of corpora.Various corpus search engines, including theIMS Open Corpus Workbench (http://cwb.sourceforge.net/) and Poliqarp (http://poliqarp.sourceforge.net/) allow theuser to search for particular parts of speech andgrammatical categories.
Obviously, the taggershould maximise the quality of the disambigua-tion of those categories which occur frequentlyin corpus queries, i.e., the weights should corre-spond to the frequencies of particular categories(and PoS) in user queries.
Note that the only re-source needed to calculate weights are the logs ofa corpus search engine.An experiment involving an implementation ofthis measure is described in detail in ?
3.2.4 Conditional Weighted PositionalAccuracyThe importance and difficulty of a category maydepend on the part of speech.
For example, af-ter case syncretisms, gender ambiguity is one ofthe main problems for the current taggers of Pol-ish.
But this problem concerns mainly pronounsand adjectives, where the systematic gender syn-cretism is high.
On the other hand, nouns do notinflect for gender, so only some nominal formsare ambiguous with respect to gender.
Moreover,gerunds, which also bear gender, are uniformlyneuter, so here part of speech alone uniquely de-termines the value of this category.A straightforward extension of WPA capitalis-ing on these observations is what we call con-ditional weighted positional accuracy (CWPA),where weights of morphosyntactic categories areconditioned on PoS.Note that not all variants of WPA may be easilygeneralised to CWPA; although such an extensionis obvious for the average ambiguity (?
2.3.1), it isless clear for the other two variants.
For parsing-related WPA, we assume that, even if a given ruledoes not mention the PoS of a terminal symbol,22For example, in unification grammars and constraint-based grammars a terminal may be identified only by thethat PoS may be read off the parse tree, so the con-ditional weights may still be calculated.
On theother hand, logs of a corpus search engine are typ-ically not sufficient to calculate such conditionalweights; e.g., a query for a sequence of 5 genitivewords occurring in logs would have to be rerunon the corpus again in order to find out parts ofspeech of the returned 5-word sequences.
For alarge number of queries on a large corpus, this isa potentially costly operation.It is also not immediately clear how to gener-alise precision and recall from WPA to CWPA.Returning to the example above, where t1 =ger:sg:nom:n:perf:aff is assigned in-stead of the correct t2 = subst:sg:nom:n, wenote that the weights of number, case and gendermay now (and should, at least in case of gender!
)be different for the two parts of speech involved.Hence, precision needs to be calculated with re-spect to the weights for the automatically assignedpart of speech, and recall ?
taking into accountweights for the gold standard part of speech:P =?t?1t?2w(t?1) +?c?C(t1,t2) ?tc1tc2w(c|t?1)w(t?1) +?c?C(t1) w(c|t?1),R =?t?1t?2w(t?2) +?c?C(t1,t2) ?tc1tc2w(c|t?2)w(t?2) +?c?C(t2) w(c|t?2),where t?
is the PoS of tag t, w(p) is the weightof the part of speech p, w(c|p) is the conditionalweight of the category c for PoS p, C(t) is the setof morphosyntactic categories of tag t, C(t1, t2)is the set of morphosyntactic categories commonto tags t1 and t2, tc is the value of category c intag t, and ?ij is the Kronecker delta (equal to 1 ifi = j, and to 0 otherwise).
Hence, for the exampleabove, these formulas may be simplified to:P =?c?
{n,c,g}w(c|ger)w(ger) +?c?
{n,c,g,a,neg}w(c|ger),R =?c?
{n,c,g}w(c|subst)w(subst) +?c?
{n,c,g}w(c|subst),where n, c, g, a and neg stand for number, case,gender, aspect and negation.values of some of its categories, as in the following simplerule, specifying prepositional phrases as a preposition gov-erning a specific case and a non-empty sequence of wordsbearing that case: PPcase=C ?
Pcase=C X+case=C.43 ExperimentTo evaluate behaviour of the proposed metrics, anumber of experiments were performed using themanually disambiguated part of the IPI PAN Cor-pus of Polish (Przepi?rkowski, 2005).
This sub-corpus consists of 880 000 segments.
Two tag-gers of Polish were tested.
TaKIPI (Piasecki andGodlewski, 2006) is a tagger which was used forautomatic disambiguation of the remaining part ofthe aforementioned corpus.
It is a statistical clas-sifier based on decision trees combined with someautomatically extracted, hand-crafted rules.
Thistagger by default sometimes assigns more thanone tag to a segment, what is consistent with thegolden standard.
There is a setting which allowsthis behaviour to be switched off.
This tagger wastested with both settings.
The other tagger is aprototype version of this Brill tagger, presented byAcedan?ski and Go?uchowski in (Acedan?ski andGo?uchowski, 2009).For comparison, four metrics were used: stan-dard metrics for full tags and only parts of speech,as well as Positional Accuracy and Weighted Posi-tional Accuracy.
For the last measure, the weightswere obtained by analysing logs of user queries ofthe Poliqarp corpus search engine.
Occurrencesof queries involving particular grammatical cat-egories were counted and used as weights.
Ob-tained results are presented in Table 1.Table 1: Occurrences of particular grammaticalcategories in query logs of the Poliqarp corpussearch engine.Category # occurrencesPOS 37771CASE 14055NUMBER 2074GENDER 552ASPECT 222PERSON 186DEGREE 81ACCOMMODABILITY 25POST-PREP.
8NEGATION 7ACCENTABILITY 5AGGLUTINATION 43.1 Scored information retrieval metricsIn ?
2 a number of methods of assigning a scoreto a pair of tags were presented.
From now on,let name them scoring functions.
One could usethem directly for evaluation, given that both thetagger and the golden standard always assign asingle interpretation to each segment.
This is notthe case for the corpus we use, hence we pro-pose generalisation of standard information re-trieval metrics (precision, recall and F-measure)as well as strong and weak correctness (Kar-wan?ska and Przepi?rkowski, 2009) to account forscoring functions.Denote by Ti and Gi the sets of tags assigned bythe tagger and the golden standard, accordingly,to the i-th segment of the tagged text.
The set ofall tags in the tagset is denoted by T. The scoringfunction used is score:T ?
T ?
[0, 1].
Also, tosave up on notation, we definescore(t, A) := maxt?
?Ascore(t, t?
)Now, given the text has n segments, we takeP =?ni=1?t?Ti score(t, Gi)?ni=1 |Ti|R =?ni=1?g?Gi score(g, Ti)?ni=1 |Gi|F = 2 ?
P ?RP +RWC =?ni=1 maxt?Ti score(t, Gi)nSC =?ni=1 min({score(t, Gi): t ?
Ti}?
{score(g, Ti): g ?
Gi})nIntuitions for scored precision and recall are thatprecision specifies the percent of tags assigned bythe tagger which have a high score with some cor-responding golden tag.
Analogously recall esti-mates the percent of golden tags which have highscores with some corresponding tag assigned bythe tagger.
The definition of recall is slightly dif-ferent than proposed by Zi?
?ko et al (Zi?
?ko etal., 2007) so that recall is never greater than one.33For example if the golden standard specifies a single tagand the tagger determines two tags which all score 0.6 whencompared with the golden, then if we used equations fromZi?
?ko et al, we would get the recall of 1.2.53.2 Evaluation resultsNow the taggers were trained on the same dataconsisting of 90% segments of the corpus and thentested on the remaining 10%.
Results were 10-fold cross-validated.
They are presented in Ta-bles 2, 3, 4 and 5.As expected, the values obtained with PA andWPA fall between the numbers for standard met-rics calculated with full tags and only the part ofspeech.
What is worth observing is that the useof WPA makes values for scored precision and re-call much closer together.
This can be justifiedby the fact that the golden standard relatively fre-quently contains more than one interpretation forsome tags, which differ only in values of less im-portant grammatical categories.
WPA is resilientto such situations.One may argue that such scoring functions mayhide a large number of tagging mistakes occurringin low-weighted categories.
But this is not thecase as the clearly most common tagging errorsreported in both (Piasecki and Godlewski, 2006)and (Acedan?ski and Go?uchowski, 2009) are forCASE, GENDER and NUMBER.
Also, the moti-vation for weighting grammatical categories is toactually ignore errors in not important ones.
Tobe fair, though, one should make sure that theweights used for evaluation match the actual ap-plication domain of the analysed tagger, and if nospecific domain is known, using a number of mea-sures is recommended.It should also be noted that for classic infor-mation retrieval metrics, the result of weak cor-rectness for TaKIPI is more similar to 92.55% re-ported by the authors (Piasecki and Godlewski,2006) than 91.30% shown in (Karwan?ska andPrzepi?rkowski, 2009) despite using the same testcorpus and very similar methodology4 as the sec-ond paper presents.4 ConclusionThis paper stems from the observation that thecommonly used measure for tagger evaluation,i.e., accuracy, does not distinguish between com-pletely incorrect and partially correct interpreta-4The only difference was not contracting the grammati-cal category of ACCOMMODABILITY present for masculinenumerals in the golden standard.tions, even though the latter may be sufficient forsome applications.
We proposed a way of grad-ing tag assignments, by weighting the importanceof particular categories (case, number, etc.)
andthe part of speech.
Three variants of the weightedpositional accuracy were presented: one intrin-sic and two application-oriented, and an extensionof WPA to conditional WPA was discussed.
Thevariant of WPA related to the needs of the usersof a corpus search engine for the National Corpusof Polish was implemented and its usefulness wasdemonstrated.
We plan to implement the parsing-oriented WPA in the future.We conclude that tagger evaluation is far frombeing a closed chapter and the time has come toadopt more subtle approaches than sheer accuracy,approaches able to cope with morphological rich-ness and oriented towards real applications.ReferencesAcedan?ski, Szymon and Konrad Go?uchowski.
2009.A morphosyntactic rule-based Brill tagger forPolish.
In K?opotek, Mieczys?aw A., AdamPrzepi?rkowski, S?awomir T.
Wierzchon?, andKrzysztof Trojanowski, editors, Advances in In-telligent Information Systems ?
Design and Ap-plications, pages 67?76.
Akademicka OficynaWydawnicza EXIT, Warsaw.De?bowski, ?ukasz.
2009.
Valence extraction usingthe EM selection and co-occurrence matrices.
Lan-guage Resources and Evaluation, 43:301?327.Hajic?, Jan and Barbora Hladk?.
1997.
Probabilisticand rule-based tagger of an inflective language - acomparison.
In Proceedings of the 5th Applied Nat-ural Language Processing Conference, pages 111?118, Washington, DC.
ACL.Hajic?, Jan. 2004.
Disambiguation of Rich Inflection.Karolinum Press, Prague.Janus, Daniel and Adam Przepi?rkowski.
2007.Poliqarp: An open source corpus indexer and searchengine with syntactic extensions.
In Proceedings ofthe ACL 2007 Demo and Poster Sessions, pages 85?88, Prague.Karwan?ska, Danuta and Adam Przepi?rkowski.
2009.On the evaluation of two Polish taggers.
In Goz?dz?-Roszkowski, Stanis?aw, editor, The proceedings ofPractical Applications in Language and ComputersPALC 2009, Frankfurt am Main.
Peter Lang.
Forth-coming.6Oliva, Karel.
2001.
On retaining ambiguity in dis-ambiguated corpora: Programmatic reflections onwhy?s and how?s.
TAL (Traitement Automatique desLangues), 42(2):487?500.Piasecki, Maciej and Grzegorz Godlewski.
2006.
Ef-fective Architecture of the Polish Tagger.
In Sojka,Petr, Ivan Kopecek, and Karel Pala, editors, TSD,volume 4188 of Lecture Notes in Computer Science,pages 213?220.
Springer.Przepi?rkowski, Adam and Marcin Wolin?ski.
2003.The unbearable lightness of tagging: A case study inmorphosyntactic tagging of Polish.
In Proceedingsof the 4th International Workshop on LinguisticallyInterpreted Corpora (LINC-03), EACL 2003, pages109?116.Przepi?rkowski, Adam.
2005.
The IPI PAN Corpus inNumbers.
In Proceedings of the 2nd Language &Technology Conference, Poznan?, Poland.Przepi?rkowski, Adam.
2008.
Powierzchnioweprzetwarzanie je?zyka polskiego.
Akademicka Ofi-cyna Wydawnicza EXIT, Warsaw.van Halteren, Hans.
1999.
Performance of taggers.In van Halteren, Hans, editor, Syntactic WordclassTagging, volume 9 of Text, Speech and LanguageTechnology, pages 81?94.
Kluwer, Dordrecht.Zi?
?ko, Bartosz, Suresh Manandhar, and Richard Wil-son.
2007.
Fuzzy Recall and Precision forSpeech Segmentation Evaluation.
In Proceedingsof 3rd Language & Technology Conference, Poznan,Poland, October.7Table 2: Evaluation results ?
standard information retrieval metrics, full tagsTagger C (%) WC (%) P (%) R (%) F (%)TaKIPI ?
defaults 87.67% 92.10% 89.93% 84.72% 87.25%TaKIPI ?
one tag per seg.
88.68% 91.06% 90.94% 83.78% 87.21%Brill 90.01% 92.44% 92.26% 85.00% 88.49%Table 3: Evaluation results ?
standard information retrieval metrics, PoS onlyTagger C (%) WC (%) P (%) R (%) F (%)TaKIPI ?
defaults 95.56% 97.52% 95.71% 97.61% 96.65%TaKIPI ?
one tag per seg.
96.53% 96.54% 96.58% 96.71% 96.65%Brill 98.17% 98.18% 98.20% 98.26% 98.23%Table 4: Evaluation results ?
scored metrics, Positional AccuracyTagger C (%) WC (%) P (%) R (%) F (%)TaKIPI ?
defaults 95.23% 96.58% 95.69% 95.44% 95.57%TaKIPI ?
one tag per seg.
95.69% 96.10% 96.12% 95.00% 95.56%Brill 97.02% 97.43% 97.42% 96.27% 96.84%Table 5: Evaluation results ?
scored metrics, Weighted PA, Poliqarp weightsTagger C (%) WC (%) P (%) R (%) F (%)TaKIPI ?
defaults 95.20% 96.62% 95.34% 96.56% 95.95%TaKIPI ?
one tag per seg.
95.88% 95.93% 95.97% 95.94% 95.95%Brill 97.34% 97.40% 97.41% 97.34% 97.38%8
