Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 41?48,Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational LinguisticsLatent Structure Perceptron with Feature Inductionfor Unrestricted Coreference ResolutionEraldo Rezende FernandesDepartamento de Informa?ticaPUC-RioRio de Janeiro, Brazilefernandes@inf.puc-rio.brC?
?cero Nogueira dos SantosBrazilian Research LabIBM ResearchRio de Janeiro, Brazilcicerons@br.ibm.comRuy Luiz Milidiu?Departamento de Informa?ticaPUC-RioRio de Janeiro, Brazilmilidiu@inf.puc-rio.brAbstractWe describe a machine learning system basedon large margin structure perceptron for unre-stricted coreference resolution that introducestwo key modeling techniques: latent corefer-ence trees and entropy guided feature induc-tion.
The proposed latent tree modeling turnsthe learning problem computationally feasi-ble.
Additionally, using an automatic featureinduction method, we are able to efficientlybuild nonlinear models and, hence, achievehigh performances with a linear learning algo-rithm.
Our system is evaluated on the CoNLL-2012 Shared Task closed track, which com-prises three languages: Arabic, Chinese andEnglish.
We apply the same system to all lan-guages, except for minor adaptations on somelanguage dependent features, like static listsof pronouns.
Our system achieves an offi-cial score of 58.69, the best one among all thecompetitors.1 IntroductionThe CoNLL-2012 Shared Task (Pradhan et al,2012) is dedicated to the modeling of coreferenceresolution for multiple languages.
The participantsare provided with corpora for three languages: Ara-bic, Chinese and English.
These corpora are pro-vided by the OntoNotes project and, besides accu-rate anaphoric coreference information, contain var-ious annotation layers such as part-of-speech (POS)tagging, syntax parsing, named entities (NE) and se-mantic role labeling (SRL).
The shared task consistsin the automatic identification of coreferring men-tions of entities and events, given predicted infor-mation on other OntoNotes layers.We propose a machine learning system for coref-erence resolution that is based on the large marginstructure perceptron algorithm (Collins, 2002; Fer-nandes and Milidiu?, 2012).
Our system learns a pre-dictor that takes as input a set of candidate men-tions in a document and directly outputs the clus-ters of coreferring mentions.
This predictor com-prises an optimization problem whose objective is afunction of the clustering features.
To embed clas-sic cluster metrics in this objective function is prac-tically infeasible since most of such metrics lead toNP-hard optimization problems.
Thus, we introducecoreference trees in order to represent a cluster bya directed tree over its mentions.
In that way, theprediction problem optimizes over trees instead ofclusters, which makes our approach computationallyfeasible.
Since coreference trees are not given in thetraining data, we assume that these structures are la-tent and use the latent structure perceptron (Fernan-des and Brefeld, 2011; Yu and Joachims, 2009) asthe learning algorithm.To provide high predicting power features toour model, we use entropy guided feature induc-tion (Fernandes and Milidiu?, 2012).
By using thistechnique, we automatically generate several fea-ture templates that capture coreference specific lo-cal context knowledge.
Furthermore, this feature in-duction technique extends the structure perceptronframework by providing an efficient general methodto build strong nonlinear classifiers.Our system is evaluated on the CoNLL-2012Shared Task closed track and achieves the scores4154.22, 58.49 and 63.37 on Arabic, Chinese and En-glish test sets, respectively.
The official score ?
themean over the three languages ?
is 58.69, which isthe best score achieved in the shared task.The remainder of this paper is organized as fol-lows.
In Section 2, we present our machine learningmodeling for the unrestricted coreference resolutiontask.
In Section 3, we present the corpus preprocess-ing steps.
The experimental findings are depicted inSection 4 and, in Section 5, we present our final re-marks.2 Task ModelingCoreference resolution consists in identifying men-tion clusters in a document.
We split this task intotwo subtasks: mention detection and mention clus-tering.
For the first subtask, we apply the strategyproposed in (dos Santos and Carvalho, 2011).
Thesecond subtask requires a complex output.
Hence,we use a structure learning approach that has beensuccessfully applied to many similar structure find-ing NLP tasks (Collins, 2002; Tsochantaridis etal., 2005; McDonald et al, 2006; Fernandes andBrefeld, 2011; Fernandes and Milidiu?, 2012).2.1 Mention DetectionFor each text document, we generate a list of candi-date mentions using the strategy of (dos Santos andCarvalho, 2011).
The basic idea is to use all nounphrases, and, additionally, pronouns and named en-tities, even if they are inside larger noun phrases.
Wedo not include verbs as mentions.2.2 Mention ClusteringIn the mention clustering subtask, a training in-stance (x,y) consists of a set of mentions x froma document and the correct coreferring clusters y.The structure perceptron algorithm learns a predic-tor from a given training set D = {(x,y)} of cor-rect input-output pairs.
More specifically, it learnsthe weight vector w of the parameterized predictorgiven byF (x) = arg maxy??Y(x)s(y?
;w),where Y(x) is the set of clusterings over mentionsx and s is a w-parameterized scoring function overclusterings.We use the large margin structure perceptron(Fernandes and Milidiu?, 2012) that, during training,embeds a loss function in the prediction problem.Hence, it uses a loss-augmented predictor given byF `(x) = arg maxy??Y(x)s(y?
;w) + `(y,y?
),where ` is a non-negative loss function that mea-sures how a candidate clustering y?
differs from theground truth y.
The training algorithm makes in-tense use of the predictor, hence the prediction prob-lem must be efficiently solved.
Letting s be a classicclustering metric is infeasible, since most of suchmetrics lead to NP-hard optimization problems.2.2.1 Coreference TreesIn order to reduce the complexity of the predictionproblem, we introduce coreference trees to representclusters of coreferring mentions.
A coreference treeis a directed tree whose nodes are the coreferringmentions and arcs represent some coreference rela-tion between mentions.
In Figure 1, we present adocument with seven highlighted mentions compris-ing two clusters.
One plausible coreference tree forthe cluster {a1,a2,a3,a4} is presented in Figure 2.North Koreaa1 opened itsa2 doors to the U.S. today,welcoming Secretary of State Madeleine Albrightb1 .Sheb2 says herb3 visit is a good start.
The U.S. remainsconcerned about North Korea?sa3 missile developmentprogram and itsa4 exports of missiles to Iran.Figure 1: Exemplary document with seven highlightedmentions comprising two clusters: {a1,a2,a3,a4} and{b1,b2,b3}.
The letter in the mention subscript indicatesits cluster and the number uniquely identifies the mentionwithin the cluster.
[North Korea]a[its]a [North Korea's]a[its]a12 34Figure 2: Coreference tree for the cluster a in Figure 1.We are not concerned about the semantics under-lying coreference trees, since they are just auxiliary42structures for the clustering task.
However, we ar-gue that this concept is linguistically plausible, sincethere is a dependency relation between coreferringmentions.
Observing the aforementioned example,one may agree that mention a3 (North Korea?s) isindeed more likely to be associated with mention a1(North Korea) than with mention a2 (its), even con-sidering that a2 is closer than a1 in the text.For a given document, we have a forest of coref-erence trees, one tree for each coreferring cluster.However, for the sake of simplicity, we link the rootnode of every coreference tree to an artificial rootnode, obtaining the document tree.
In Figure 3, wedepict a document tree for the text in Figure 1.Figure 3: Document tree with two coreference trees forthe text in Figure 1.
Dashed lines indicate artificial arcs.2.2.2 Latent Structure LearningCoreference trees are not given in the trainingdata.
Thus, we assume that these structures are la-tent and make use of the latent structure perceptron(Fernandes and Brefeld, 2011; Yu and Joachims,2009) to train our models.
We decompose the origi-nal predictor into two predictors, that isF (x) ?
Fy(Fh(x)),where the latent predictor Fh(x) is defined asargmaxh?H(x)?w,?
(x,h)?,H(x) is the set of fea-sible document trees for x and ?
(x,h) is the jointfeature vector representation of mentions x and doc-ument tree h. Hence, the latent predictor finds amaximum scoring rooted tree over the given men-tions x, where a tree score is given by a linear func-tion over its features.
Fy(h) is a straightforwardprocedure that creates a cluster for each subtree con-nected to the artificial root node in the document treeh.In Figure 4, we depict the proposed latent struc-ture perceptron algorithm for the mention cluster-ing task.
Like its univariate counterpart (Rosenblatt,w0 ?
0t?
0while no convergencefor each (x,y) ?
Dh??
argmaxh?H(x,y)?wt,?(x,h)?h??
argmaxh?H(x)?wt,?
(x,h)?+ `r(h, h?
)wt+1 ?
wt + ?
(x, h?)??
(x, h?)t?
t+ 1w ?
1t?ti=1 wiFigure 4: Latent structure perceptron algorithm.1957), the structure perceptron is an online algo-rithm that iterates through the training set.
For eachtraining instance, it performs two major steps: (i)a prediction for the given input using the currentmodel; and (ii) a model update based on the dif-ference between the predicted and the ground truthoutputs.
The latent structure perceptron performs anadditional step to predict the latent ground truth h?using a specialization of the latent predictor and thecurrent model.
This algorithm learns to predict doc-ument trees that help to solve the clustering task.Thereafter, for an unseen document x, the predic-tor Fh(x) and the learned model w are employed toproduce a predicted document tree h which, in turn,is fed to Fy(h) to give the predicted clusters.Golden coreference trees are not available.
How-ever, during training, for a given input x, we havethe golden clustering y.
Thus, we predict the con-strained document tree h?
for the training instance(x,y) using a specialization of the latent predictor?
the constrained latent predictor ?
that makes useof y.
The constrained predictor finds the maximumscoring document tree among all rooted trees of xthat follow the correct clustering y, that is, rootedtrees that only include arcs between mentions thatare coreferent according to y, plus one arc from theartificial node to each cluster.
In that way, the con-strained predictor optimizes over a subset H(x,y)contained in H(x) and, moreover, it guarantees that43Fy(h?)
= y, for any w. The constrained tree isused as the ground truth on each iteration.
There-fore, the model update is determined by the differ-ence between the constrained document tree and thedocument tree predicted by the ordinary predictor.The loss function measures the impurity in thepredicted document tree.
In our modeling, we use asimple loss function that just counts how many pre-dicted edges are not present in the constrained docu-ment tree.
For the arcs from the artificial root node,we use a different loss value.
We set that through theparameter r, which we call the root loss value.We decompose the joint feature vector ?
(x,h)along tree edges, that is, pairs of candidate corefer-ring mentions.
This approach is similar to previousstructure learning modelings for dependency pars-ing (McDonald et al, 2005; Fernandes and Milidiu?,2012).
Thus, the prediction problem reduces to amaximum branching problem, which is efficientlysolved by the Chu-Liu-Edmonds algorithm (Chu andLiu, 1965; Edmonds, 1967).
We also use the aver-aged structure perceptron as suggested by (Collins,2002), since it provides a more robust model.3 Data PreparationIt is necessary to perform some corpus processingsteps in order to prepare training and test data.
Inthis section, we detail the methodology we use togenerate coreference arcs and the features that de-scribe them.3.1 Coreference Arcs GenerationThe input for the prediction problem is a graphwhose nodes are the mentions in a document.
Ide-ally, we could consider the complete graph for eachdocument, thus every mention pair would be an op-tion for building the document tree.
However, sincethe total number of mentions is huge and a big por-tion of arcs can be easily identified as incorrect, wefilter the arcs and, thus, include only candidate men-tion pairs that are more likely to be coreferent.We filter arcs by simply adapting the sievesmethod proposed in (Lee et al, 2011).
However, inour filtering strategy, precision is not a concern andthe application order of filters is not important.
Theobjective here is to build a small set of candidate arcsthat shows good recall.Given a mention pair (mi,mj), where mi appearsbefore mj in the text, we create a directed arc frommi to mj if at least one of the following conditionsholds: (1) the number of mentions between mi andmj is not greater than a given parameter; (2) mj isan alias of mi; (3) there is a match of both mentionsstrings up to their head words; (4) the head wordof mi matches the head word of mj ; (5) test shallowdiscourse attributes match for both mentions; (6) mjis a pronoun and mi has the same gender, number,speaker and animacy of mj ; (7) mj is a pronoun andmi is a compatible pronoun or proper name.Sieves 2 to 7 are obtained from (Lee et al, 2011).We only introduce sieve 1 to lift recall without usingother strongly language-dependent sieves.3.2 Basic FeaturesWe use a set of 70 basic features to describe eachpair of mentions (mi, mj).
The feature set includeslexical, syntactic, semantic, and positional informa-tion.
Our feature set is very similar to the one usedby (dos Santos and Carvalho, 2011).
However, herewe do not use the semantic features derived fromWordNet.
In the following, we briefly describe someof these basic features.Lexical: head word of mi/j ; String matching of(head word of) mi and mj (y/n); Both are pro-nouns and their strings match (y/n); Previous/Nexttwo words of mi/j ; Length of mi/j ; Edit distance ofhead words; mi/j is a definitive NP (y/n); mi/j is ademonstrative NP (y/n); Both are proper names andtheir strings match (y/n).Syntactic: POS tag of the mi/j head word; Previ-ous/Next two POS tags of mi/j ; mi and mj are bothpronouns / proper names (y/n); Previous/Next pred-icate of mi/j ; Compatible pronouns, which checkswhether two pronouns agree in number, gender andperson (y/n); NP embedding level; Number of em-bedded NPs in mi/j .Semantic: the result of a baseline system; senseof the mi/j head word; Named entity type of mi/j ;mi and mj have the same named entity; Semanticrole of mi/j for the prev/next predicate; Concatena-tion of semantic roles of mi and mj for the samepredicate (if they are in the same sentence); Samespeaker (y/n); mj is an alias of mi (y/n).Distance and Position: Distance between mi andmj in sentences; Distance in number of mentions;44Distance in number of person names (applies onlyfor the cases where mi and mj are both pronouns orone of them is a person name); One mention is inapposition to the other (y/n).3.3 Language SpecificsOur system can be easily adapted to different lan-guages.
In our experiments, only small changes areneeded in order to train and apply the system to threedifferent languages.
The adaptations are due to: lackof input features for some languages; different POStagsets are used in the corpora; and creation of staticlist of language specific pronouns.Some input features, that are available for the En-glish corpus, are not available in Arabic and Chinesecorpora.
Namely, the Arabic corpus does not containNE, SRL and speaker features.
Therefore, for thislanguage we do not derive basic features that makeuse of these input features.
For Chinese, we do notuse features derived from NE data, since this data isnot provided.
Additionally, the Chinese corpus usesa different POS tagset.
Hence, some few mappingsare needed during the basic feature derivation stage.The lack of input features for Arabic and Chinesealso impact the sieve-based arcs generation.
ForChinese, we do not use sieve 6, and, for Arabic, weonly use sieves 1, 3, 4 and 7.
Sieve 7 is not usedfor the English corpus, since it is a specialization ofsieve 6.
The first sieve parameter is 4 for Arabic andChinese, and 8 for English.In the arcs generation and basic feature derivationsteps, our system makes use of static lists of lan-guage specific pronouns.
In our experiments, we usethe POS tagging information and the golden coref-erence chains to automatically extract these pronounlists from training corpora.3.4 Entropy Guided Feature InductionIn order to improve the predictive power of our sys-tem, we add complex features that are combinationsof the basic features described in the previous sec-tion.
We use feature templates to generate such com-plex features.
However, we automatically generatetemplates using the entropy guided feature inductionapproach (Fernandes and Milidiu?, 2012; Milidiu?
etal., 2008).
These automatically generated templatescapture complex contextual information and are dif-ficult to be handcrafted by humans.
Furthermore,this feature induction mechanism extends the struc-ture perceptron framework by providing an efficientgeneral method to build strong nonlinear predictors.We experiment with different template sets foreach language.
The main difference between thesesets is basically the training data used to inducethem.
We obtain better results when merging dif-ferent template sets.
For the English language, it isbetter to use a template set of 196 templates, whichmerges two different sets: (a) a set induced usingtraining data that contains mention pairs producedby filters 2 to 6; and (b) another set induced usingtraining data that contains mention pairs producedby all filters.
For Chinese and Arabic, it is better touse template sets induced specifically for these lan-guages merged with the template set (a) generatedfor the English language.
The final set for the Chi-nese language has 197 templates, while the final setfor Arabic has 223.4 Empirical ResultsWe train our system on the corpora provided in theCoNLL-2012 Shared Task.
There are corpora avail-able on three languages: Arabic, Chinese and En-glish.
For each language, results are reported usingthree metrics: MUC, B3 and CEAFe.
We also re-port the mean of the F-scores on these three met-rics, which gives a unique score for each language.Additionally, the official score on the CoNLL-2012shared task is reported, that is the mean of the scoresobtained on the three languages.We report our system results on development andtest sets.
The development results are obtained withsystems trained only on the training sets.
However,test set results are obtained by training on a largerdataset ?
the one obtained by concatenating train-ing and development sets.
During training, we usethe gold standard input features, which produce bet-ter performing models than using the provided au-tomatic values.
That is usually the case on NLPtasks, since golden values eliminate the additionalnoise introduced by automatic features.
On the otherhand, during evaluation, we use the automatic valuesprovided in the CoNLL shared task corpora.In Table 1, we present our system performanceson the CoNLL-2012 development sets for the threelanguages.
Given the size of the Arabic training cor-45LanguageMUC B3 CEAFe MeanR P F1 R P F1 R P F1Arabic 43.00 47.87 45.30 61.41 70.38 65.59 49.42 44.19 46.66 52.52Chinese 54.40 68.19 60.52 64.17 78.84 70.76 51.42 38.96 44.33 58.54English 64.88 74.74 69.46 66.53 78.28 71.93 54.93 43.68 48.66 63.35Official Score 58.14Table 1: Results on the development sets.LanguageMUC B3 CEAFe MeanR P F1 R P F1 R P F1Arabic 34.18 58.85 43.25 50.61 82.13 62.63 57.37 33.75 42.49 49.45Chinese 49.17 76.03 59.72 58.16 86.33 69.50 57.56 34.38 43.05 57.42English 62.75 77.41 69.31 63.88 81.34 71.56 57.46 41.08 47.91 62.92Official Score 56.59Table 2: Results on the development sets without root loss value.LanguageMUC B3 CEAFe MeanR P F1 R P F1 R P F1Arabic 43.63 49.69 46.46 62.70 72.19 67.11 52.49 46.09 49.08 54.22Chinese 52.69 70.58 60.34 62.99 80.57 70.70 53.75 37.88 44.44 58.49English 65.83 75.91 70.51 65.79 77.69 71.24 55.00 43.17 48.37 63.37Official Score 58.69Table 3: Official results on the test sets.Language Parse / MentionsMUC B3 CEAFe MeanR P F1 R P F1 R P F1ArabicAuto / GB 45.18 47.39 46.26 64.56 69.44 66.91 49.73 47.39 48.53 53.90Auto / GM 57.25 76.48 65.48 60.27 79.81 68.68 72.61 46.00 56.32 63.49Golden / Auto 46.38 51.78 48.93 63.53 72.37 67.66 52.57 46.88 49.56 55.38Golden / GB 46.38 51.78 48.93 63.53 72.37 67.66 52.57 46.88 49.56 55.38Golden / GM 56.89 76.27 65.17 60.07 80.02 68.62 72.24 45.58 55.90 63.23ChineseAuto / GB 58.76 71.46 64.49 66.62 79.88 72.65 54.09 42.02 47.29 61.48Auto / GM 61.64 90.81 73.43 63.55 89.43 74.30 72.78 39.68 51.36 66.36Golden / Auto 59.35 74.49 66.07 66.31 81.43 73.10 55.97 41.50 47.66 62.28Golden / GB 59.35 74.49 66.07 66.31 81.43 73.10 55.97 41.50 47.66 62.28Golden / GM 61.70 91.45 73.69 63.57 89.76 74.43 72.84 39.49 51.21 66.44EnglishAuto / GB 64.92 77.53 70.67 64.25 78.95 70.85 56.48 41.69 47.97 63.16Auto / GM 70.69 91.21 79.65 65.46 85.61 74.19 74.71 42.55 54.22 69.35Golden / Auto 67.73 77.25 72.18 66.42 78.01 71.75 56.16 44.51 49.66 64.53Golden / GB 65.65 78.26 71.40 64.36 79.09 70.97 57.36 42.23 48.65 63.67Golden / GM 71.18 91.24 79.97 65.81 85.51 74.38 74.93 43.09 54.72 69.69Table 4: Supplementary results on the test sets alternating parse quality and mention candidates.
Parse quality can beautomatic or golden; and mention candidates can be automatically identified (Auto), golden mention boundaries (GB)or golden mentions (GM).pus and the feature limitations for Arabic and Chi-nese, the performance variations among the threelanguages are no more than expected.
One impor-tant parameter that we introduce in this work is theroot loss value, a different loss function value on arcsfrom the artificial root node.
The effect of this pa-rameter is to diminish the creation of clusters, thusstimulating bigger clusters and adjusting the balancebetween precision and recall.
Using the develop-ment sets for tuning, we set the value of the root lossvalue parameter to 6, 2 and 1.5 for Arabic, Chineseand English, respectively.
In Table 2, we present oursystem performances on the development sets whenwe set this parameter to 1 for all languages, that is46equivalent to not use this parameter at all.
We canobserve, by comparing these results with the onesin Table 1, that this parameter really causes a betterbalancing between precision and recall, and conse-quently increases the F1 scores.
Its effect is accen-tuated on Arabic and Chinese, since the unbalancingissue is worse on these languages.The official results on the test sets are depictedin Table 3.
For Chinese and English, these perfor-mances are virtually identical to the performanceson the development sets.
On the other hand, the offi-cial performance for the Arabic language is signifi-cantly higher than the development set performance.This difference is likely due to the fact that the Ara-bic training set is much smaller than the Chinese andEnglish counterparts.
Thus, by including the devel-opment set in the training of the final Arabic system,we significantly improve the official performance.We report in Table 4 the supplementary resultsprovided by the shared task organizers on the testsets.
These additional experiments investigate twokey aspects of any coreference resolution system:the parse feature and the mention candidates thatare given to the clustering procedure.
We alter-nate the parse feature between the official automaticparse and the golden parse from OntoNotes.
Re-garding mention candidates, we use three differ-ent strategies: automatic mentions (Auto, in Ta-ble 4), golden mention boundaries (GB) and goldenmentions (GM).
Automatic mentions are completelydetected by our system, as described in Section2.1.
Golden mention boundaries comprise all nounphrases in the golden parse tree, even when the au-tomatic parse is used as input feature.
Golden men-tions are all non-singleton mentions, i.e., all men-tions that take part in some entity cluster.
It is im-portant to notice that golden mention information ismuch stronger than golden boundaries.By observing Table 4, it is clear that the most ben-eficial information is golden mentions (compare theAuto/GM results in Table 4 with the results in Table3).
The mean F-score over all languages when us-ing golden mentions is almost 8 points higher thanthe official score.
These results are not surprisingsince to identify non-singleton mentions greatly re-duces the final task complexity.
Golden mentionboundaries (Auto/GB) increase the mean F-score forChinese by almost 3 points.
Conversely, for theother two languages, the results are decreased whenthis information is given.
This is probably due toparameter tuning, since any additional informationpotentially changes the learning problem and, nev-ertheless, we use exactly the same three models ?one per language ?
to produce all the results on Ta-bles 3 and 4.
One can observe, for instance, thatthe recall/precision balance greatly varies among thedifferent configurations in these experiments.
Thegolden parse feature (Golden/Auto) causes big im-provements on the mean F-scores for all languages,specially for Chinese.5 ConclusionIn this paper, we describe a machine learning systembased on large margin latent structure perceptron forunrestricted coreference resolution.
We introducetwo modeling approaches that have direct impacton the final system performance: latent coreferencetrees and entropy guided feature induction.According to our experiments, latent coreferencetrees are powerful enough to model the complex-ity of coreference structures in a document, whileturning the learning problem computationally feasi-ble.
Our empirical findings also show that entropyguided feature induction enables learning of effec-tive nonlinear classifiers.Our system is evaluated on the CoNLL-2012Shared Task closed track, which consists on model-ing coreference resolution for three languages: Ara-bic, Chinese and English.
In order to cope with thismulti-language task, our system needs only minoradaptations on some language dependent features.As future work, we plan to include second orderfeatures and cluster sensitive features.AcknowledgmentsThis work was partially funded by Conselho Na-cional de Desenvolvimento Cient?
?fico e Tecnolo?gico(CNPq), Fundac?a?o de Amparo a` Pesquisa do Es-tado do Rio de Janeiro and Fundac?a?o Cearense deApoio ao Desenvolvimento Cient?
?fico e Tecnolo?gicothrough grants 557.128/2009-9, E-26/170028/2008and 0011-00147.01.00/09, respectively.
The first au-thor was also supported by a CNPq doctoral fel-lowship and by the Instituto Federal de Educac?a?o,Cie?ncia e Tecnologia de Goia?s.47ReferencesY.
J. Chu and T. H. Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and experi-ments with perceptron algorithms.
In Proceedings ofthe ACL-02 Conference on Empirical Methods in Nat-ural Language Processing, pages 1?8.Cicero Nogueira dos Santos and Davi Lopes Carvalho.2011.
Rule and tree ensembles for unrestrictedcoreference resolution.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 51?55, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.J.
Edmonds.
1967.
Optimum branchings.
Journal of Re-search of the National Bureau of Standards, 71B:233?240.Eraldo R. Fernandes and Ulf Brefeld.
2011.
Learningfrom partially annotated sequences.
In Proceedings ofthe European Conference on Machine Learning andPrinciples and Practice of Knowledge Discovery inDatabases (ECML-PKDD), Athens, Greece.Eraldo R. Fernandes and Ruy L. Milidiu?.
2012.
Entropy-guided feature generation for structured learning ofPortuguese dependency parsing.
In Proceedings ofthe Conference on Computational Processing of thePortuguese Language (PROPOR), volume 7243 ofLecture Notes in Computer Science, pages 146?156.Springer Berlin / Heidelberg.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s multi-pass sieve coreference resolution sys-tem at the CoNLL-2011 shared task.
In Proceedingsof the Fifteenth Conference on Computational Natu-ral Language Learning: Shared Task, CoNLL SharedTask 2011, pages 28?34, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL?05,pages 91?98.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with atwo-stage discriminative parser.
In Proceedings ofthe Conference on Computational Natural LanguageLearning (CoNLL), pages 216?220.Ruy L.
Milidiu?, C?
?cero N. dos Santos, and Julio C.Duarte.
2008.
Phrase chunking using entropy guidedtransformation learning.
In Proceedings of ACL2008,Columbus, Ohio.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unrestrictedcoreference in OntoNotes.
In Proceedings of theSixteenth Conference on Computational Natural Lan-guage Learning (CoNLL 2012), Jeju, Korea.Frank Rosenblatt.
1957.
The Perceptron ?
a perceivingand recognizing automaton.
Technical report, CornellAeronautical Laboratory.
Report 85-460-1.I.
Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-tun.
2005.
Large margin methods for structured andinterdependent output variables.
Journal of MachineLearning Research, 6:1453?1484.Chun-Nam Yu and Thorsten Joachims.
2009.
Learningstructural SVMs with latent variables.
In Proceedingsof the International Conference on Machine Learning(ICML).48
