I!
!III!|IIti|.tiII!ZIIliII-IIncorporating Knowledge in Natural Language Learning:A Case StudyYuva l  K rymolowsk iDept.
of Math  and Computer  ScienceBar-Nan University52900 Ramat  Gan, Israelyuvalk@cs, biu.
ac.
ilDan RothDept.
of Computer  ScienceUniversity of IllinoisUrbana, IL 61801danr~cs, uiuc.
eduAbst rac tIncorporating external information during a learn-ing process is expected to improve its efficiency.
Westudy a method for incorporating noun-class infor-mation, in the context of learning to resolve Prepo-sitional Phrase Attachment (PPA) disambiguation.This is done within a recently introduced architec-ture, SNOW, a sparse network of threshold gates uti-lizing the Winnow learning algorithm.
That archi-tecture has already been demonstrated to performremarkably well on a number of natural languagelearning tasks.The knowledge sources used were compiled fromthe WordNet database for general linguistic pur-poses, irrespective of the PPA problem, and are be-ing incorporated into the learning algorithm by en-riching its feature space.
We study two strategiesof using enriched features and the effects of usingclass information at different granularities, as wellas randomly-generated knowledge which serves as acontrol set.Incorporating external knowledge sources withinSNOW yields a statistically significant performanceimprovement.
In addition, we find an interestingrelation between the granularity of the knowledgesources used and the magnitude of the improvement.The encouraging results with noun-class data pro-vide a motivation for carrying out more work ongenerating better linguistic knowledge sources.1 In t roduct ionA variety of inductive learning techniques have beenused in recent years in natural language process-ing.
Given a large training corpus as input andrelying on statistical properties of language usage,statistics-based and machine learning algorithms areused to induce a classifier which can be used to re-solve a disambiguation task.
Applications of thisline of research include ambiguity resolution at dif-ferent levels of sentence analysis: part-of speech tag-ging, word-sense disambiguation, word selection inmachine translation, context-sensitive spelling cor-rection, word selection in speech recognition, andidentification of discourse markers.Many natural anguage inferences, however, seemto rely heavily on semantic and pragmatic knowl-edge about the world and the language, that is notexplicit in the training data.
The ability to incor-porate knowledge from other sources of information,be it knowledge that is acquired across modalities:prepared by a teacher or by an expert, is crucial forgoing beyond low level natural anguage inferences.Within Machine Learning, the use of knowledge isoften limited to that of constraining the hypothesisspace (either before learning or by probabilisticallybiasing the search for the hypothesis) or to tech-niques such as EBL (DeJong, 1981; Mitchell et al,1986; DeJong and Mooney, 1986) which rely on ex-plicit domain knowledge that can be used to explain(usually, prove deductively) the observed examples.The knowledge needed to perform language-understanding related tasks, however, does not existin any explicit form that is amenable to techniques ofthis sort, and many believe that it will never be avail-able in such explicit forms.
An enormous amountof useful "knowledge" may be available, though.Pieces of information that may be found valuable inlanguage-understanding related tasks may include:the root form of a verb; a list of nouns that are insome relation (e.g., are all countries) and can thusappear in similar contexts; a list of verbs that canbe followed by a food item; a list of items you cansee through, things that are furniture, a list of dan-gerous things, etc.This rich collection of information pieces does notform any domain theory to speak of and cannotbe acquired from a single source of information.This knowledge is noisy, incomplete and ambiguous.While some of it may be acquired from text, a lotif it may only be acquired from other modalities,as those used by humans.
We believe that integra-tion of such knowledge is essential for NLP to attainhigh-level natural-language inference.Contrary to this intuition, experiments in text re-trieval and natural language have not shown muchimprovement when incorporating information of thekind humans eem to use (Krovetz and Croft, 1992;Kosmynin and Davidson, 1996; Kar0v and Edelman,1211996; Junker, 1997).
The lack of significant improve-ment in the presence of more "knowledge" may beexplained by the type of-knowledge used, the wayit is incorporated, and the learning algorithms em-ployed.In the present paper we study an effective way ofincorporating incomplete and ambiguous informa-tion sources of the abovementioned type within aspecific learning approach, and focus on the knowl-edge sources that can be effective in doing so.
Thelong-term goal of our work is understanding (1) whattypes of knowledge sources can be used for perfor-mance improvement, and at what granularity leveland (2) which computational mechanisms can makethe best use of these sources.In particular, the effect of noun-class informa-tion on learning Prepositional Phrase Attachment(PPA, cf.
Sec.
2) is studied.
This problem is stud-ied within SNO IF, a sparse architecture utilizing anon-line learning algorithm based on Winnow (Little-stone, 1988).
That algorithm has been applied fornatural anguage disambiguation tasks and relatedproblems and perform remarkably well (Golding andRoth, 1996; Dagan et al, 1997; Roth and Zelenko,1998).The noun-class data was derived from the Word-Net database (Miller, 1990) which was compiled forgeneral linguistic purposes, irrespective of the PPAproblem.
We derived the classes at different granu-larities.
At the highest level, nouns are classifiedaccording to their synsets.
The lower levels areobtained by successively using the hypernym rela-tion defined in WordNet.
In addition, we use theCorelex database (Buitelaar, 1998).
Consisting of126 coarse-grained semantic types covering around40,000 nouns, Corelex defines a large number of sys-tematic polysemous classes that are derived from ananalysis of sense distributions in WordNet.The results indicate that a statistically significantimprovement in performance is achieved when thenoun-class information is incorporated into the data.The absolute performance achieved on the task isslightly better than other systems, although it is stillsignificantly worse than the performance ofa humansubject ested on this task.
The granularity of theclass information appears to be crucial for improvingperformance.
The addition of too many overlappingclasses does not help performance, but with fewerclasses - the improvement is significant.In addition to semantic information, using classescarries with it some structural information.
A classfeature may be viewed as a disjunction of other fea-tures, thereby increasing the expressivity of the hy-pothesis used for prediction.
In order to control forthe possibility that the performance improvementsseen are due mainly to the structural information,we generated random classes.
Some of these had122exactly the same distribution over the original fea-tures as do the semantic lasses.
Surprisingly, wefind that a non-negligible part of the improvementis due merely to the structural information, althoughmost of it can be attributed to the semantic ontentof the classes.Along with promoting work on the incorporationof problem-independent incomplete knowledge intothe learning process, the encouraging results withincorporating noun-class data provide a motivationfor carrying out more work on generating better lin-guistic knowledge sources.The paper is organized as follows: we start by pre-senting the task, PPA and the SNOW architectureand algorithm.
In section 4 we describe the classesand present the main experiments with the semanticand random classes.
Section 5 concludes.2 P repos i t iona l  phrase  a t tachmentThe PPA problem is to decide whether the preposi-tional phrase (PP) attaches to the direct object NPas in Buy the car with the steering wheel (n-attachment) or to the verb phrase buy, as in Buythe car with his money (v-attachment).
PPA  is?
a common cause of structural ambiguity in naturallanguage.Earlier works on this problem (Ratnaparkhi et al,1994; Brill and Resnik, 1994; Collins and Brooks,1995; Zavrel et al, 1997) represented an example bythe 4-tuple <v, nl, p, n2> containing the VP  head,the direct object NP  head, the preposition, and theindirect object NP  head respectively.
The first ex-ample in the previous paragraph is thus representedby <buy, car, with, wheel>.The experiments reported here were done usingdata extracted by Ratnaparkhi et al (1994) fromthe Penn Treebank (Marcus et al, 1993) WSJ cor-pus.
It consists of 20801 training examples and 3097separate test examples.The preposition of turns out to be a very strongindicator for noun attachment.
Among the 3097 testexamples, 925 contain the preposition of; in all but9 of these examples, of has an n attachment.Since almost all (99.1%) of these test cases areclassified correctly regardless of the SNOW archi-tecture or parameter choice, we omit the exampleswhich include of from the test set, as they obscurethe real performance.
Only the last table will in-clude those examples, so results may be comparedwith other systems evaluated on this data set.In summary, our data set consists of 15224 train-ing examples, (5338 tagged n, 9886 tagged v) and2172 test examples (910 and 1262, resp.).
This leadsto a baseline performance of 58.1% if we simply pre-dict according to the most common attachment inthe training corpus: v. (Simply breaking this downto different prepositions does not yield better re-!!
suits.)
For reference, assuming a binomial distribu- tion, the standard eviation on the test set is 0.85%.That figure is a crude estimator of the standard e-viation of the results.A study of the possible features which may be ex-tracted from the data, shows that the best featureset is that composed of all the possible conjunctionsof words in the input 4-tuple.
In addition, lemmatiz-ing all the nouns and verbs yielded a further perfor-mance improvement.
In the following section we willuse the lemmatized data "lemma" as a basic set.3 The SNOW ApproachThe SNOW architecture is a network of thresholdgates.
Nodes in the first layer of the network repre-sent the input features; target nodes are representedby nodes in the second layer.
Links from the first tothe second layer have weights; each target node isthus defined as a (linear) function of the lower levelnodes.For example, in PPA, the two target nodes repre-sent n and v attachments.
Each target node can bethought of as an autonomous subnetwork, althoughthey all feed from the same input.
The subnetworksare sparse in that a target node needs not be con-nected to all nodes in the input layer.
For example,it is not connected to input nodes (features) thatwere never active with it in the same example, orit may disconnect i self from some of the irrelevantinputs while training.Learning in SNOW proceeds in an on-line fash-ion I.
Every example is treated autonomously byeach target subnetwork, viewed as a positive exam-ple of a few subnetworks and a negative xample forthe others.
In PPA, examples labeled n (v, resp.
)are treated as positive for the n (v) target node andas negative for the v (n) target node.
Thus, everyexample isused once by all the nodes to refine theirdefinition, and then discarded.
At prediction time,given an input which activates a subset of the inputnodes, each subnetwork evaluates the total activ-ity it receives.
Subnetworks compete on determin-ing the final prediction; the one which produces thehighest activity gets to determine the prediction.In general, a target node in the SNOW architec-ture is represented by a collection of subnetworks,which we call a cloud, but in the application de-scribed here we have used cloud size of I so this willnot be discussed here.The Winnow local mistake-driven learning algo-rithm (Littlestone, 1988) is used at each target nodeto learn its dependence on the input nodes.
Winnowupdates the weight on the links in a multiplicativefashion.
We do not supply the details of the algo-rithm and just note that it can he implemented in1 In the experimental study we do not update the networkwhile testing.such a way that the update time of the algorithmdepends on the number of active features in the ex-ample rather than the total number of features inthe domain.
The sparse architecture along with therepresentation f each example as a list of activefeatures is reminiscent of infinite attribute modelsof Winnow (Blum, 1992).Theoretical analysis has shown that multiplica-tive update algorithms, like Winnow, have excep-tionally good behavior in the presence of irrelevantattributes, noise, and even a target function chang-ing in time (Littlestone, 1988; Littlestone and War-muth, 1994; Herbster and Warmuth, 1995).
In par-ticular, Winnow was shown to learn efficiently anylinear threshold function (Littlestone, 1988), with amistake bound that depends on the margin betweenpositive and negative xamples.
The key featureof Winnow is that its mistake bound grows linearlywith the number of relevant attributes and only log-arithmically with the total number of attributes n.In particular, Winnow still maintains its abovemen-tioned dependence on the number of total and rele-vant attributes even when no linear-threshold func-tion can make a perfect classification (Littlestone,1991; Kivinen and Warmuth, 1995).Even when there are only two target nodes andthe cloud size is 1, the behavior of SNO Wis differentfrom that of pure Winnow.
While each of the targetnodes is learned using a positive Winnow algorithm,a winner-take-all policy is used to determine the pre-diction.
Thus, we use the learning algorithm here ina more complex way than just as a discriminator.One reason is that the SNOW architecture, influ-enced by the Neuroidal system (Valiant, 1994), isbeing used in a system developed for the purpose oflearning knowledge representations for natural an-guage understanding tasks, and is being evaluatedon a variety of tasks for which the node allocationprocess is of importance.We have experimented extensively with variousarchitectures of SNOWon the PPA problem but canpresent in this paper only a small part of these ex-periments.
The best performance, across a few pa-rameter sets and data, is achieved with a full archi-tecture.
In this case we initially link a target nodeto a/l features which occur in the training (with aconstant initial value), and only then start training.Since training in SNOW is always done in an on-linefashion - each example isused only once for updatingthe weights, and only if a mistake on it was made.4 Incorporat ing  Semant icKnowledgeIn this section we describe the effect of incorporatingsemantic knowledge on learning PPA with SNOW.The information sources are briefly described inSec.
4.1, the experimental results are reported in123IiilIt/lSec.
4.2, and results with random classes, used asa control set, are presented in Sec.
4.3.Winnow has three parameters: a threshold 0 andtwo update parameters, a promotion parameter a >1 and a demotion parameter 0 < ~ < 1.
The experi-ments reported here were made using the full SNOWarchitecture, with/3 = 0.85, a = ~, 0 = 1, and allthe weights initialized to 0.1.4.1 Semantic Data SourcesThe semantic data sources pecify for each noun aset of semantic lasses.
These classes result from ageneral linguistic study, hence not biased so as topresent data in the context of PPA.
In addition, thevocabularies which the semantic data cover overlapsour train and test data vocabulary only partially.Table 1 shows a summary of the class data.
Theknowledge sources which were incorporated are:WordNet(WN):  WordNet-l.6 noun class infor-mation was used at various granularity levels.
In thehighest level, denoted by WN1, nouns are classifiedaccording to their synsets.
The lower levels are ob-tained by successively using the hypernym relationdefined in WordNet.
Thus, WN2 is obtained by re-placing each WN1 synset with the set of hypernymsto which it points, WN3 - by performing a similarprocess on the WN2 hypernyms, etc.
We have usedWN1, WN5, WN10, and WN15, Table 1 lists prop-erties of these datasets.CoreLex(CL): The Corelex database (Buitelaar,1998) was derived from WordNet as part of a linguis-tic research attempting toprovide a unified approachto the systematic polysemy and underspecificationof nouns.
Systematic polysemy is the phenomenaof word senses that are systematically related andtherefore predictable over classes of lexical items.The thesis behind this data base is that acknowl-edging the systematic nature of polysemy allows oneto structure ontologies for lexical semantic process-ing that may help in generating more appropriateinterpretations within context.
The data base es-tablishes an ontology and semantic database of 126semantic types, covering around 40,000 nouns thatwere derived by an analysis of sense distributions inWordNet.It is clear that with such a coarse-grained ontol-ogy, a lot of information is being lost.
This is amany-to-one mapping in which many words fall intoa class due only to one of their senses, and thereare cases of incomplete and inaccurate information.For example, observatory falls into the class ofax't ifact state; words like dog, l ion,  table aremissing from the vocabulary.Format Features (FF): These are two classesinto which one can classify nouns using simpleheuristics.
The first consists of numbers (e.g.,1, 2, 100, three, million), and the second contains124proper nouns.
Each noun beginning with a capitalletter was classified as a proper noun, which clearlygives a very crude approximation.4.2 Experimental  ResultsIn this section we present results of incorporatingvarious emantic data and their combinations.
Sincethe classes were not compiled specifically for thePPA problem, some of the class information maybe irrelevant or even slightly misleading.
The re-sults provide an assesment of the relative relevanceof each knowledge source.When a noun belongs to a class, one may replacethe explicit noun feature by its classes.
Using theclasses in addition to the original noun (Brill andR~nik, 1994; Resnik, 1992; Resnik, 1995)seems,however, a better strategy.
Consider, for example,the feature <prep,indirect-object=n2>.
Supposethe noun n2 belongs to two classes cl  and c2.
Theclass information will be incorporated by creatingtwo additional features: <prep,indirect-object=c 1 >and <prep,indirect-object=c2>, thereby enhancingthe feature set without losing the original informa-tion.
As mentioned above, giving up the originalfeature yielded degraded results.The results of adding features from a single knowl-edge source, presented in Table 2, show that FFhave yielded small improvements over the lena  set,within the noise-level; the WN1 synset informationcaused a slight degradation, and the CL and otherWN knowledge r sulted in a significant improvementover the lemma case.An important property of the CL class informa-tion is that each CL class defines a distinct set ofnouns, as each noun belongs to one CL class.
Thesynset (WN1) distribution differs greatly from thatof the CL classes; each noun may belong to a fewsynsets - allowing more potential conflicts.
Thatproperty of the synset distribution gives rise to theperformance degradation.Another important difference between CL andWN1 classes is their granularity.
There are around60000 synsets, whereas there are only 126 CL classes.The finer synset granularity means that a synset car-ries less information; thus, the CL classes add richerdisjunctions than WN synsets do.
The results of CL,WNS, WN10, and WN15 improve over the FF set,these results are within the noise level (cf.
Sec.
2).The FF set covers relatively few nouns, hence theimprovement it yields is quite small.
The Word-Net and CL vocabularies do not include those begin-ning with a capital as well as numbers, therefore theWN and CL knowledge may be augmented with theFF information without loss of consistency.
Never-theless, since each number-word (e.g., "one", "two",etc.)
belongs to a different synset, augmenting WNIwith a numeric lass is not expected to be very effec-tive because the words "one", "two", and "1" will allII!|I,Il'|I!it|!II!|!nouns in train 6533 2025 3083 4012 4012 4012 4ulznouns in test 1805 150 1107 1559 1559 1559 1559nouns in both 1452 83 902 1322 1322 1322 1322classes in train - 2 110 10029 521 33 9classes in test - 2 92 5216 353 28 9classes in both - 2 91 4863 343 28 9Table 1: Sizes and coverage of the noun vocabulary and classes in the various noun-class ources.leftmost column shows the noun vocabulary size and coverage for the train and test data.The58.1 77.4 77.8 78.6 77.2 79.1 78.5 78.6Table 2: Learning results for a single knowledge source: Baseline refers to simply predicting accordingto the most common attachment in the training corpus, namely (v).
:temma is our basic feature set, as in Sec.
2The other columns present the prediction accuracy when adding each of our knowledge sources eparately.belong to different classes: synset(one), synset(two),and FF(is-number), respectively.As a measure of numeric lass assignment, we haveexamined the words: "one", "two", "three", "ten","hundred" and "million"; only CL, WN3 and sub-sequent WN knowledge sources assign the same hy-pernym to these words, therefore we have augmentedthese sources.The results are presented in Table 3, comparisonwith Table 2 shows that augmenting with FF knowl-edge yielded a slight improvement only for the CLset.
There may be two explanations for that: (i)the CL classes are more appropriate for the PPAproblem than the WN hypernyms, therefore the FFinformation fit with less conflicts.
(ii) The coverageof CL nouns is about 70% that of WN for the testdata (cf.
Table 1), therefore there axe more examplesin which the CL and FF classes do not conflict.
Thisissue requires further study.4.3 Comparison with Random ClassesAdding semantic lass information improved SNOWlearning results.
However, adding class informationis equivalent to adding disjunctions of the originalfeatures and, taldng aside the semantic origin of theclasses, the mere introduction of disjunctions en-riches the knowledge representation a d may yield aperformance improvement.The motivation for using semantic lasses goes,however, beyond this structural information.
Nounswhich haven't appeared in the training data mayappear in the test data under a known class; suchnouns will thus be handled based on the experiencegathered for the class.In this section we attempt to isolate the semanticcontent of the classes from their disjunctive mean-ing.
Random classes, which mimic in different as-pects the structure of the semantic CL classes, wereconstructed.
Comparing the results obtained withthese classes with the results using CL classes, onecan see the influence of the semantic aspect of CLclasses.
Only some of the randomization strategiesused axe described here, these are:\[CL200:\] 200 classes uniformly distributed over CLnouns .\[CL126:\] 126 classes uniformly distributed over CLnouns.
Here the number of classes in CL is main-tained.\[CL-PERM:\] A permutation of CL nouns amongtheir classes.
This random structure preserves theoriginal class distribution of CL.The random class results, shown in Table 4, in-dicate that indeed some of the gain in using classesmay be due to the structural additions.
However,the improved performance introduced by semanti-cally meaningful CL classification is a lot more sig-nificant.4.4 Comparison with other worksThis section presents a comparison of our work withother works on the PPA task.
In order to ob-tain a fair comparison we have tested our systemon the complete data set, including the preposi-tion of (cf.
Sec.
2).
The results are comparedwith a maximum-entropy method (Ratnaparkhi etal., 1994), transformation-based learning (TBL, Brilland Resnik (1994)), an instantiation of the back-off estimation (Collins and Brooks, 1995) and amemory-based method (Zavrel et al, 1997).
Allthese works have used the same train and test dataset.
Table 5 presents the comparison.In all cases, the quoted figures axe the best resultsobtained by the authors; with the exception of theBrill and Resnik (1994) result, which was obtainedby Zavrel et al (1997) using the same method.
Orig-inally, TBL  was evaluated by Brill and Resnik (1994)125I|IIi|!
!|tIBas~linel lemma\[ +CL+FF \ [WN5+FF IWN10+FF \[WN15+FF \[58.1 77.4 79.1 78.8 78.1 77.9Table 3: Learning results for combinat ions of FF and other sources: The four leftmost columnsindicate the classes added to our basic feature set, 1emma.I Accuracy \[ lemma 77.9 \] 77.4 \] lemma+CL I I lTable 4: Random Classes: Results with various randomizations strategies.on a smaller data set.Although all systems have used the same data,they have not used similar feature sets.
Both Collinsand Brooks (1995) and Zavrel et al (1997) have en-hanced the feature generation i various ways; as de-scribed in this paper, this was also done for SNOW.5 ConclusionOver several decades, research on high level infer-ences such as natural language understanding hasemphasized programmed systems, as opposed tothose that learn.
However, experience in AI researchover the past few decades hows that it is unlikelythat hand programming or any form of knowledgeengineering will generate a robust, non-brittle rea-soning system in a complex domain.An approach that puts learning at the center ofhigh level inferencing (Khardon and Roth, 1997;Valiant, 1995} should suggest ways to make progressin massive knowledge acquisition and, in particular,ways of incorporating incomplete and noisy knowl-edge from various information sources uch as dif-ferent modalities, teachers or experts, into a highlyscalable learning process.The present work made preliminary steps in thisdirection.
We have studied ways to incorporate x-ternal knowledge sources into a learning algorithmin order to improve its performance.
This inves-tigation was done within the SNOW architecture,a sparse network of threshold gates utilizing theWinnow on-line learning algorithm.
The linguisticknowledge sources, noun-class datasets, were com-piled for general reasons, irrespective of the taskstudied here.
Knowledge incorporation resulted ina statistically significant performance improvementon PPA, a challenging natural anguage disambigua-tion task which has been investigated extensively.Using random noun classes, we have demonstratedthat the semantic nature of the external knowledgeis essential.
In addition, the granularity of the datawas shown to play an important role in the learningperformance.
A highly granular synset classificationfailed to improve the results.A lot of future work is to be done in order tosubstantiate he results presented here, study moretasks and prepare and investigate the effectivenessof other information sources.ReferencesA.
Blum.
1992.
Learning boolean functions in an infi-nite attribute space.
Machine Learning, 9(4):373-386,October.E.
Brill and P. Resnik.
1994.
A rule-based approach toprepositional phrase attachment disambiguation.
InProc.
of COLING.P.
Buitelaar.
1998.
CoreLex: Systematic Polysemy andUnderspecification.
Ph.D. thesis, Computer ScienceDepartment, Brandeis University, Feb.M.
Collins and J Brooks.
1995.
Prepositional phrase at-t~hment through a backed-off mode.
In Proceedingsof Third the Workshop on Very Large Corpora.I.
Dagan, Y. Karov, and D. Roth.
1997.
Mistake-drivenlearning in text categorization.
In EMNLP-97, TheSecond Conference on Empirical Methods in NaturalLanguage Processing, pages 55-63, August.G.
DeJong and R. Mooney.
1986.
Explanation-basedlearning: An alternative view.
Machine Learning,1(2):145-176.G.
DeJong.
1981.
Generalization based on explanations.In IJCAI, pages 67-70.A.
R. Golding and D. Roth.
1996.
Applying winnowto context-sensitive spelling correction.
In MachineLearning, pages 182-190.M.
Herbster and M. Warmuth.
1995.
Tracking thebest expert.
In Proc.
12th International Conferenceon Machine Learning, pages 286-294.
Morgan Kauf-mann.M.
Junker.
1997.
Sigir poster: The effectiveness of usingthesauri in hr.
In Proc.
of International Conference onResearch and Development in Information Retrieval,SIGIR.Y.
Karov and S. Edelman.
1996.
Learning similarity-based word sense disambiguation from sparse data.
InFourth workshop on very large corpora, pages 42-55,August.R.
Khardoa and D. Roth.
1997.
Learning to reason.Journal of the ACM, 44(5):697-725, Sept.
Earlier ver-sion appeared in AAAI-94.J.
Kivinen and M. K. Warmuth.
1995.
Exponentiatedgradient versus gradient descent for linear predictors.In Proe.
of STOC.
Tech Report UCSC-CRL-94-16.126Ratnapaxkhi Brill and Collins and Zavrel SNOWet al (1994) Resnik (1994) Brooks (1995) et al (1997)81.6 81.9 84.5 84.4 84.8Table 5: Sys tem compar ison:  Comparison of SNOW results with those of previous works.
All the quotedfigures axe the best results obtained by the authors, with the exception of the Brill and Resnik (1994) resultwhich was obtained by Zavrel et al (1997).A.
Kosmynin and I Davidson.
1996.
Using backgroundcontextual knowledge for documents representation.In PODP Workshop, Palo-Alto.R.
Krovetz and W. B. Croft.
1992.
Lexical ambiguityand information retrieval.
A CM TransactionJ on In-formation Systems, 10(2):115-141.N.
Littlestone and M. K. Warmuth.
1994.
The weightedmajority algorithm.
Information and Computation,I08(2):212-261.N.
Littlestone.
1988.
Learning quickly when irrelevantattributes abound: A new linear-threshold algorithm.Machine Learning, 2:285-318.N.
Littlestone.
1991.
Redundant noisy attributes, at-tribute errors, and linear threshold learning usingWinnow.
In Proc.
.~  Annu.
Workshop on Comput.Learning Theory, pages 147-156, San Mateo, CA.Morgan Kaufmann.M.
P. Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313-330, June.George A. Miller.
1990.
Wordnet: An on-line lexi-eal database.
International Journal of Lexicography,3(4):235-312.T.M.
Mitchell, R.M.
Keller, and S.T.
Kedar-Cabelli.1986.
Explanation Based Learning.
Machine Learn.ing, 1(1):47-80.A.
Ratnaparkhi, J. Reynar, and S. Roukos.
1994.
Amaximum entropy model for prepositional phrase at-tachment.
In ARPA, Plainsboro, N J, March.P.
Resnik.
1992.
Wordaet and distributional analy-sis: A class-based approach to lexical discovery.
InAAAI Workshop on Statistically.based Natural Lan-guage Processing Techniques, pages 54--64, July.P.
Resnik.
1995.
Disambiguating noun groupings withrespect to wordnet senses.
In Proceedings of the ThirdAnnual Workshop on Very Large Corpora.D.
Roth and D. Zelenko.
1998.
Part of speech taggingusing a network of linear separators.
In COLING-ACL98, The 17th International Conference on Computa-tional Linguistics.L.
G. Valiant.
1994.
Circuits of the Mind.
Oxford Uni-versity Press, November.L.
G. Valiant.
1995.
Rationality.
In Workshop on Com-putational Learning Theory, pages 3-14, July.J.
Zavrel, W. Daelemans, and J. Veenstra.
1997.
Re-solving pp attachment ambiguities with memory basedlearning.
In Computational Natural Language Learn-ing, Madrid, Spain, July.i!
127
