Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 657?669,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsFinding Good Enough: A Task-Based Evaluation of Query BiasedSummarization for Cross Language Information RetrievalJennifer Williams, Sharon Tam, Wade ShenMIT Lincoln Laboratory Human Language Technology Group244 Wood Street, Lexington, MA 02420 USAjennifer.williams@ll.mit.edu, sharontam@alum.mit.eduswade@ll.mit.eduAbstractIn this paper we present our task-basedevaluation of query biased summarizationfor cross-language information retrieval(CLIR) using relevance prediction.
We de-scribe our 13 summarization methods eachfrom one of four summarization strate-gies.
We show how well our methodsperform using Farsi text from the CLEF2008 shared-task, which we translated toEnglish automtatically.
We report preci-sion/recall/F1, accuracy and time-on-task.We found that different summarizationmethods perform optimally for differentevaluation metrics, but overall query bi-ased word clouds are the best summariza-tion strategy.
In our analysis, we demon-strate that using the ROUGE metric on oursentence-based summaries cannot makethe same kinds of distinctions as our evalu-ation framework does.
Finally, we presentour recommendations for creating much-needed evaluation standards and datasets.1 IntroductionDespite many recent advances in query biasedsummarization for cross-language information re-trieval (CLIR), there are no existing evaluationstandards or datasets to make comparisons amongdifferent methods, and across different languages(Tombros and Sanderson, 1998; Pingali et al.,2007; McCallum et al., 2012; Bhaskar and Bandy-opadhyay, 2012).
Consider that creating thiskind of summary requires familiarity with tech-niques from machine translation (MT), summa-rization, and information retrieval (IR).
In thisThis work was sponsored by the Federal Bureau of Inves-tigation under Air Force Contract FA8721-05-C-0002.
Opin-ions, interpretations, conclusions, and recommendations arethose of the authors and are not necessarily endorsed by theUnited States Government.paper, we arrive at the intersection of each ofthese research areas.
Query biased summariza-tion (also known as query-focused, query-relevant,and query-dependent) involves automatically cap-turing relevant ideas and content from a documentwith respect to a given query, and presenting it as acondensed version of the original document.
Thiskind of summarization is mostly used in search en-gines because when search results are tailored to auser?s information need, the user can find texts thatthey are looking for more quickly and more ac-curately (Tombros and Sanderson, 1998; Mori etal., 2004).
Query biased summarization is a valu-able research area in natural language processing(NLP), especially for CLIR.
Users of CLIR sys-tems meet their information needs by submittingtheir queries in L1to search through documentsthat have been composed in L2, even though theymay not be familiar with L2(Hovy et al., 1999;Pingali et al., 2007).There are no standards for objectively evaluat-ing summaries for CLIR ?
a research gap that webegin to address in this paper.
The problem weexplore is two-fold: what kinds of summaries arewell-suited for CLIR applications, and how shouldthe summaries be evaluated.
Our evaluation is ex-trinsic, that is to say we are interested in how sum-marization affects performance on a different task(Mani et al., 2002; McKeown et al., 2005; Dorret al., 2005; Murray et al., 2009; McCallum etal., 2012).
We use relevance prediction as our ex-trinsic task: a human must decide if a summaryfor a given document is relevant to a particular in-formation need, or not.
Relevance prediction isknown to be useful as it correlates with some au-tomatic intrinsic methods as well (President andDorr, 2006; Hobson et al., 2007).
To the best ofour knowledge, we are the first to apply this eval-uation framework to cross language query biasedsummarization.Each one of the summarization methods that we657present in this paper belongs to one of the fol-lowing strategies: (1) unbiased full machine trans-lated text, (2) unbiased word clouds, (3) query bi-ased word clouds, and (4) query biased sentencesummaries.
The methods and strategies that wepresent are fast, cheap, and language-independent.All of these strategies are extractive, meaning thatwe used existing parts of a document to create thecondensed version, or summary.We approach our task as an engineering prob-lem: the goal is to decide if summaries are goodenough to help CLIR system users find what theyare looking for.
We have simplified the task by as-suming that a set of documents has already beenretrieved from a search engine, as CLIR tech-niques are outside the scope of this paper.
Wepredict that showing the full MT English text asa summarization strategy would not be particu-larly helpful in our relevance prediction task be-cause the words in the text could be mixed-up,or sentences could be nonsensical, resulting inpoor readability.
For the same reasons, we expectthat showing the full MT English text would takelonger to arrive at a relevance decision.
Finally,we predict that query biased summaries will resultin faster, more accurate decisions from the partic-ipants (Tombros and Sanderson, 1998).We treat the actual CLIR search engine as if itwere a black box so that we can focus on evaluat-ing if the summaries themselves are useful.
As astarting point, we begin with some principles thatwe expect to hold true when we evaluate.
Theseprinciples provide us with the kind of frameworkthat we need for a productive and judicious dis-cussion about how well a summarization methodworks.
We encourage the NLP community toconsider the following concepts when developingevaluation standards for this problem:?
End-user intelligiblity?
Query-salience?
Retrieval-relevanceSummaries should be presented to the end-user ina way that is both concise and intelligible, evenif the machine translated text is difficult to under-stand.
Our notions of query-salience and retrieval-relevance capture the expectation that good sum-maries will be efficient enough to help end-usersfulfill their information needs.
For query-salience,we want users to positively identify relevant doc-uments.
Similarly, for retrieval-relevance we wantusers to be able to find as many relevant docu-ments as possible.This paper is structured as follows: Section 2presents related work; Section 3 describes our dataand pre-processing; Section 4 details our sum-marization methods and strategies; Section 5 de-scribes our experiments; Section 6 shows our re-sults and analysis; and in Section 7, we concludeand discuss some future directions for the NLPcommunity.2 Related WorkAutomatic summarization is generally a well-investigated research area.
Summarization is away of describing the relationships of words indocuments to the information content of that doc-ument (Luhn, 1958; Edmunson, 1969; Salton andYang, 1973; Robertson and Walker, 1994; Churchand Gale, 1999; Robertson, 2004).
Recent workhas looked at creating summaries of single andmultiple documents (Radev et al., 2004; Erkan andRadev, 2004; Wan et al., 2007; Yin et al., 2012;Chatterjee et al., 2012), as well as summary eval-uation (Jing et al., 1998; Tombros and Sanderson1998; Mani et al., 1998; Mani et al., 1999; Mani,2001; Lin and Hovy, 2003; Lin, 2004; Nenkovaet al., 2007; Hobson et al., 2007; Owczarzaket al., 2012), query and topic biased summariza-tion (Berger and Mittal, 2000; Otterbacher et al.,2005; Daume and Marcu, 2006; Chali and Joty,2008; Otterbacher et al., 2009; Bando et al., 2010;Bhaskar and Bandyopadhyay, 2012; Harwath andHazen, 2012; Yin et al., 2012), and summarizationacross languages (Pingali et al., 2007; Or?asan andChiorean, 2008; Wan et al., 2010; Azarbonyad etal., 2013).2.1 Query Biased SummarizationPrevious work most closely related to our owncomes from Pingali et al., (2007).
In their work,they present their method for cross-languagequery biased summarization for Telugu and En-glish.
Their work was motivated by the need forpeople to have access to foreign-language docu-ments from a search engine even though the userswere not familiar with the foreign language, intheir case English.
They used language model-ing and translation probability to translate a user?squery into L2, and then summarized each docu-ment in L2with respect to the query.
In their finalstep, they translated the summary from L2back658to L1for the user.
They evaluated their methodon the DUC 2005 query-focused summarizationshared-task with ROUGE scores.
We compare ourmethods to this work also on the DUC 2005 task.Our work demonstrates the first attempt to draw ata comparison between user-based studies and in-trinsic evaluation with ROUGE.
However, one ofthe limitations with evaluating this way is that theshared-task documents and queries are monolin-gual.Bhaskar and Bandyopadhyay (2012) tried asubjective evaluation of extractive cross-languagequery biased summarization for 7 different lan-guages.
They extracted sentences, then scored andranked the sentences to generate query dependentsnippets of documents for their cross lingual in-formation access (CLIA) system.
However, thesnippet quality was determined subjectively basedon scores on a scale of 0 to 1 (with 1 being best).Each score indicated annotator satisfaction for agiven snippet.
Our evaluation methodology is ob-jective: we ask users to decide if a given documentis relevant to an information need, or not.2.2 Machine Translation EffectsMachine translation quality can affect summa-rization quality.
Wan et al.
(2010) researchedthe effects of MT quality prediction on cross-language document summarization.
They gener-ated 5-sentence summaries in Chinese using En-glish source documents.
To select sentences, theyused predicted translation quality, sentence posi-tion, and sentence informativeness.
In their eval-uation, they employed 4 Chinese-speakers to sub-jectively rate summaries on a 5-point scale (5 be-ing best) along the dimensions of content, read-ability, and overall impression.
They showed thattheir approach of using MT quality scores did im-prove summarization quality on average.
Whiletheir findings are important, their work did not ad-dress query biasing or objective evaluation of thesummaries.
We attempt to overcome limitations ofmachine translation quality by using word cloudsas one of our summarization strategies.Knowing when to translate is another challengefor cross-language query biased summarization.Several options exist for when and what to trans-late during the summarization process: (1) thesource documents can be translated, (2) the user?squery can be translated, (3) the final summary canbe translated, or (4) some combination of these.An example of translating only the summariesthemselves can be found in Wan et al., (2010).On the other hand, Pingali et al.
(2007) translatedthe queries and the summaries.
In our work, weused gold-translated queries from the CLEF 2008dataset, and machine translated source documents.We briefly address this in our work, but note that afull discussion of when and what to translate, andthose effects on summarization quality, is outsideof the scope of this paper.2.3 Summarization EvaluationThere has been a lot of work towards developingmetrics for understanding what makes a summarygood.
Evaluation metrics are either intrinsic or ex-trinsic.
Intrinsic metrics, such as ROUGE, mea-sure the quality of a summary with respect to goldhuman-generated summaries (Lin, 2004; Lin andHovy, 2003).
Generating gold standard summariesis expensive and time-consuming, a problem thatpersists with cross-language query biased summa-rization because those summaries must be querybiased as well as in a different language from thesource documents.On the other hand, extrinsic metrics measure thequality of summaries at the system level, by look-ing at overall system performance on downstreamtasks (Jing et al, 1998; Tombros and Sanderson,1998).
One of the most important findings forquery biased summarization comes from Tombrosand Sanderson (1998).
In their monolingual task-based evaluation, they measured user speed andaccuracy at identifying relevant documents.
Theyfound that query biased summarization improvedthe user speed and accuracy when the user wasasked to make relevance judgements for IR tasks.We also expect that our evaluation will demon-strate that user speed and accuracy is better whensummaries are query biased.3 Data and Pre-ProcessingWe used data from the Farsi CLEF 2008 ad hoctask (Agirre et al., 2009).
Each of the queries in-cluded in this dataset consisted of a title, narrative,and description.
Figure 1 shows an example of theelements of a CLEF 2008 query.
All of the au-tomatic query-biasing in this work was based onthe query titles.
For our human relevance predic-tion task on Mechanical Turk, we used the nar-rative version.
The CLEF 2008 dataset includeda ground-truth answer key indicating which docu-659ments were relevant to each query.
For each query,we randomly selected 5 documents that were rele-vant as well as 5 documents that were not relevant.The subset of CLEF 2008 data that we used there-fore consisted of 500 original Farsi documents and50 parallel English-Farsi queries.
Next we will de-scribe our text pre-processing steps for both lan-guages as well as how we created our parallel En-glish documents.Figure 1: Full MT English summary and CLEF2008 English query (title, description, narrative).3.1 English DocumentsAll of our English documents were created auto-matically by translating the original Farsi docu-ments into English (Drexler et al., 2012).
Thetranslated documents were sentence-aligned withone sentence per line.
For all of our summariza-tion experiments (except unbised full MT text),we processed the text as follows: removed extraspaces, removed punctuation, folded to lowercase,and removed digits.
We also removed commonEnglish stopwords2from the texts.3.2 Farsi DocumentsWe used the original CLEF 2008 Farsi docu-ments for two of our summarization methods.
Westemmed words in each document using automaticmorphological analysis with Morfessor CatMAP.We note that within-sentence punctuation was re-moved during this process (Creutz and Lagus,2007).
We also removed Farsi stopwords and dig-its.4 Summarization StrategiesAll of our summarization methods were extrac-tive except for unbiased full machine translatedtext.
In this section, we describe each of our13 summarization methods which we have orga-nized into one of the following strategies: (1) un-biased full machine translated text, (2) unbiased2English and Farsi stopword lists from:http://members.unine.ch/jacques.savoy/clef/index.htmlword cloud summaries, (3) query biased wordcloud summaries, and (4) query biased sentencesummaries.
Regardless of which summarizationmethod used, we highlighted words in yellow thatalso appeard in the query.
Let t be a term indocument d where d ?
DLand DLis a collec-tion of documents in a particular language.
Notethat for our summarization methods, term weight-ings were calculated separately for each language.While |D| = 1000, we calculated term weightingsbased on |DE| = 500 and |DF| = 500.
Finally,let q be a query where q ?
Q and Q is our set of50 parallel English-Farsi CLEF queries.
Assumethat log refers to log10.Figure 2: Full MT English summary and CLEF2008 English query.4.1 Unbiased Full Machine TranslatedEnglishOur first baseline approach was to use all of theraw machine translation output (no subsets ofthe sentences were used).
Each summary there-fore consisted of the full text of an entire doc-ument automatically translated from Farsi to En-glish (Drexler et al., 2012).
Figure 2 shows an ex-ample full text document translated from Farsi toEnglish and a gold-standard English CLEF query.Note that we use this particular document-querypair as an example throughout this paper (docu-ment: H-770622-42472S8, query: 10.2452/552-AH).
According to the CLEF answer key, the sam-ple document is relevant to the sample query.4.2 Unbiased Word CloudsFor our second baseline approach, we rankedterms in a document and displayed them as wordclouds.
Word clouds are one a way to arrangea collection of words where each word can vary660in size.
We used word clouds as a summariza-tion strategy to overcome any potential disfluen-cies from the machine translation output and alsoto see if they are feasible at all for summarization.All of our methods for word clouds used wordsfrom machine translated English text.
Each term-ranking method below generates different rankedlists of terms, which we used to create differentword clouds.
We created one word cloud per doc-ument using the top 12 ranked words.
We usedthe raw term scores to scale text font size, so thatwords with a highter score appeared larger andmore prominent in a word cloud.
Words wereshuffled such that the exact ordering of words wasat random.I: Term Frequency (TF) Term frequency isvery commonly used for finding important termsin a document.
Given a term t in a document d,the number of times that term occurs is:tft,d= |t ?
d|II: Inverse Document Frequency (IDF) Theidf term weighting is typically used in IR andother text categorization tasks to make distinc-tions between documents.
The version of idf thatwe used throughout our work came from Erkanand Radev (2004) and Otterbacher et al.
(2009),in keeping consistent with theirs.
Let N be thenumber of documents in the collection, such thatN = |D| and ntis the number of documents thatcontain term t, such that nt= |{d ?
D : t ?
d}|,then:idft= logN + 10.5?
ntWhile idf is usually thought of as a type ofheuristic, there have been some discussions aboutits theoretical basis (Robertson, 2004; Robertsonand Walker, 1994; Church and Gale, 1999; Saltonand Yang, 1973).
An example of this summary isshown in Figure 3.III: Term Frequency Inverse Document Fre-quency (TFIDF) We use tfidft,dterm weight-ing to find terms which are both rare and impor-tant for a document, with respect to terms acrossall other documents in the collection:tfidft,d= tft,d?
idft4.3 Query Biased Word CloudsWe generated query biased word clouds followingthe same principles as our unbiased word clouds,Figure 3: Word cloud summary for inverse docu-ment frequency (IDF), for query ?Tehran?s stockmarket?.namely the text font scaling and highlighting re-mained the same.IV.
Query Biased Term Frequency (TFQ) InFigure 4 we show a sample word cloud summarybased on query biased term frequency.
We definequery biased term frequency tfQ at the documentlevel, as:tfQt,d,q={2tft,d, if t ?
qtft,d, otherwiseFigure 4: Word cloud summary for query biasedterm frequency (TFQ), for query ?Tehran?s stockmarket?.V.
Query Biased Inverse Document Frequency(IDFQ) Since idf helps with identifying termsthat discriminate documents in a collection, wewould expect that query biased idf would help toidentify documents that are relevant to a query:idfQt,q={2idft, if t ?
qidft, otherwiseVI.
Query Biased TFIDF (TFIDFQ) We de-fine query biased tf ?
idf similarly to our TFQand IDFQ, at the document level:tfidfQt,d,q={2tft,d?
idft, if t ?
qtft,d?
idft, otherwise661Figure 5: Word cloud summary for scaled querybiased term frequency (SFQ) for query ?Tehran?sstock market?.VII.
Query Biased Scaled Frequency (SFQ)This term weighting scheme, which we call scaledquery biased term frequency or sfQ, is a variant ofthe traditional tf?idf weighting.
First, we projectthe usual term frequency into log-space, for a termt in document d with:tfSt,d= log(tft,d)We let tfSt,d?
0 when tft,d= 1.
We believe thatsingleton terms in a document provide no indica-tion that a document is query-relevant, and trea-ment of singleton terms in this way would have thepotential to reduce false-positives in our relevanceprediction task.
Note that scaled term frequencydiffers from Robertson?s (2004) inverse total termfrequency in the sense that our method involves noconsideration of term position within a document.Scaled query biased term frequency, shown in Fig-ure 5, is defined as:sfQt,d,q={2tfSt,d?
idft, if t ?
qtfSt,d?
idft, otherwiseVIII.
Word Relevance (W) We adapted anexisting relevance weighting from Allan et al.,(2003), that was originally formulated for rankingsentences with respect to a query.
However, wemodified their originaly ranking method so that wecould rank individual terms in a document insteadof sentences.
Our method for word relevance, Wis defined as:Wt,d,q= log(tft,d+ 1)?
log(tft,q+ 1)?
idftIn W , term frequency values are smoothed byadding 1.
The smoothing could especially af-fect rare terms and singletons, when tft,dis verylow.
All terms in a query or a document willbe weighted and each term could potentially con-tribute to summary.4.4 Query Biased Sentence SummariesSentences are a canonical unit to use in extractivesummaries.
In this section we describe four differ-ent sentence scoring methods that we used.
Thesemethods show how to calculate sentence scores fora given document with respect to a given query.Sentences for a document were always ranked us-ing the raw score value output generated from ascoring method.
Each document summary con-tained the top 3 ranked sentences where the sen-tences were simply listed out.
Each of these meth-ods used sentence-aligned English machine trans-lated documents, and two of them also used theoriginal Farsi text.IX.
Sentence Relevance (REL) Our sentencerelevance scoring method comes from Allan et al.(2003).
The sentence weight is a summation overwords that appear in the query.
We provide theirsentence scoring formula here.
This calculates therelevance score for a sentence s from document d,to a query q:rel(s|q)=?t?slog(tft,s+1)?
log(tft,q+1)?
idftTerms will occur in either the sentence or thequery, or both.
We applied this method to machinetranlsated English text.
The output of this methodis a relevance score for each sentence in a givendocument.
We used those scores to rank sentencesin each document from our English machine trans-lated text.X.
Query Biased Lexrank (LQ) We imple-mented query biased LexRank, a well-knowngraph-based summarization method (Otterbacheret al., 2009).
It is a modified version of the orig-inal LexRank algorithm (Erkan and Radev, 2004;Page et al., 1998).
The similarity metric, simx,y,also known as idf-modified cosine similarity, mea-sures the distance between two sentences x and yin a document d, defined as:simx,y=?t?x,ytft,x?
tft,y?
(idft)2??t?xtfidf2t,x?
?t?ytfidf2t,yWe used simx,yto score the similarity ofsentence-to-sentence, resulting in a similarity662Figure 6: LQP - projecting Farsi sentence scoresonto parallel English sentences.graph where each vertex was a sentence and eachedge was the cosine similarity between sentences.We normalized the cosine matrix with a similaritythreshold (t = 0.05), so that sentences above thisthreshold were given similarity 1, and 0 otherwise.We used rel(s|q)to score sentence-to-query.
TheLexRank score for each sentence was then calcu-lated as:LQs|q=d?
rels|q?z?Crelz|q+ (1?
d)?
?v?adj[s]sims,v?r?adj[v]simv,rLQv|qwhere C is the set of all sentences in a given doc-ument.
Here the parameter d is just a damper todesignate a probability of randomly jumping toone of the sentences in the graph (d = 0.7).
Wefound the stationary distribution by applying thepower method ( = 5), which is guaranteed toconverge to a stationary distribution (Otterbacheret al., 2009).
The output of LQ is a score for eachsentence from a given document with respect toa query.
We used that score to rank sentences ineach document from our English machine trans-lated text.XI.
Projected Cross-Language Query BiasedLexrank (LQP) We introduce LQP to describea way of scoring and ranking sentences such thatthe L1(English) summaries are biased from theL2(Farsi) query and source document.
Our gold-standard Farsi queries were included with ourCLEF 2008 data, making them more reliable thanwhat we could get from automatic translation.First, sentences from each Farsi document werescored with Farsi queries using LQ, describedabove.
Then each LQ score was projected ontosentence-aligned English.
We demonstrate LQPFigure 7: LQC - Farsi sentence scores are com-bined with parallel English sentence scores to ob-tain sentence re-ranking.in Figure 6.
By doing this, we simulated trans-lating the user?s English query into Farsi with thebest possible query translation, before proceed-ing with summarization.
This approach to cross-language summarization could be of interest forCLIR systems that do query translation on-the-fly.It is also of interest for summarization systems thatneed to utilize previously translated source docu-ments the capability is lacking to translate sum-maries from L2to L1.XII.
Combinatory Query Biased Lexrank(LQC) Another variation of LexRank that weintroduce in this work is LQC, which combinesLexRank scores from both languages to re-ranksentences.
A visual summary of this method isshown in Figure 7.
We accomplished our re-ranking by first running LQ on Farsi and Englishseparately, then adding the two scores together.This combination of Farsi and English scores pro-vided us with a different way to score and ranksentences, compared with LQ and LQP .
Theidea behind combinatory query biased LexRankis to take advantage of sentences which are high-ranking in Farsi but not in English.
The LQCmethod exploits all available resources in ourdataset: L1and L2queries as well as L1and L2documents.5 ExperimentsWe tested each of our summarization methods andoverall strategies in a task-based evaluation frame-work using relevance prediction.
We used Me-chanical Turk for our experiments since it has beenshown to be useful for evaluating NLP systems(Callison-Burch 2009; Gillick and Liu, 2010).
Weobtained human judgments for whether or not adocument was considered relevant to a query, orinformation need.
We measured the relevance663judgements by precision/recall/F1, accuracy, andalso time-on-task based on the average responsetime per Human Intelligence Task (HIT).5.1 Mechanical TurkIn our Mechanical Turk experiment, we used ter-minology from CLEF 2008 to describe a queryas an ?information need?.
All of the MechanicalTurk workers were presented with the followingfor their individual HIT: instructions, an informa-tion need and one summary for a document.
Work-ers were asked to indicate if the given summaryfor a document was relevant to the given informa-tion need (Hobson et al., 2007).
Workers werenot shown the original Farsi source documents.We paid workers $0.01 per HIT.
We obtained 5HITs for each information need and summary pair.We used a built-in approval rate qualification pro-vided by Mechanical Turk to restrict which work-ers could work on our tasks.
Each worker had anapproval rate of at least 95Instructions: Each image below consistsof a statement summarizing the informa-tion you are trying to find from a setof documents followed by a summaryof one of the documents returned whenyou query the documents.
Based on thesummary, choose whether you think thedocument returned is relevant to the in-formation need.
NOTE: It may be diffi-cult to distinguish whether the documentis relevant as the text may be difficultto understand.
Just use your best judg-ment.6 Results and AnalysisWe present our experiment results and additionalanalysis.
First, we report the results of our rel-evance prediction task, showing performance forindividual summarization methods as well as per-formance for the overall strategies.
Then weshow analysis of our results from the monolin-gual question-biased shared-task for DUC 2005,as well as a comparison to previous work.6.1 Results for Individual MethodsOur results are shown in Table 1.
We report perfor-mance for 13 individual methods as well as over-all peformance on the 4 different summarizationstrategies.
To calculate the performance for eachstrategy, we used the arithmetic mean of the corre-sponding individual methods.
We measured preci-sion, recall and F1 to give us a sense of our sum-maries might influence document retrieval in anactual CLIR system.
We also measured accuracyand time-on-task.
For these latter two metrics, wedistinguish between summaries that were relevant(R) and non-relevant (NR).All of the summarization-based methods fa-vored recall over precision: documents weremarked ?relevant?
more often than ?non-relevant?.For many of the methods shown in Table 1, work-ers spent more time correctly deciding ?relevant?than correctly deciding ?non-relevant?.
This sug-gests some workers participated in our MechanicalTurk task purposefully.
For many of the summa-rization methods, workers were able to positivelyidentify relevant documents.From Table 1 we see that Full MT performedbetter on precision than all of the other methodsand strategies, but we note that performance onprecision was generally very low.
This might bedue to Mechanical Turk workers overgeneraliz-ing by marking summaries as relevant when theywere not.
Some individual methods preserve ourprinciple of retrieval-relevance, as indicated bythe higher recall scores for SQF, LQEF, and TFQ.That is to say, these particular query biased sum-marization methods can be used to assist userswith identifying more relevant documents.
The ac-curacy on relevant documents addresses our prin-ciple of query-salience, and it is especially highfor our query-biased methods: LQEF, SQF, LQ,and TFQ.
The results also seem to fit our intuitionthat the summary in Figure 3 seems less relevantto the summaries shown in Figures 4 & 5 eventhough these are the same documents biased onthe same query ?Tehran stock market?.Overall, query biased word clouds outperformthe other summarization strategies for 5 out of7 metrics.
This could be due to the fact thatword clouds provide a very concise and overviewof a document, which is one of the main goalsfor automatic summarization.
Along these lines,word clouds are probably not subject to the effectsof MT quality and we believe it is possible thatMT quality could have had a negative impact onour query biased extracted sentence summaries, aswell as our full MT English texts.664Table 1: Individual method results: precision/recall/F1, time-on-task, and accuracy.
Note that results fortime-on-task and accuracy scores are distinguished for relevant (R) and non-relevant (NR) documents.Precision, Recall, F1 Time-on-Task AccuracySummarization Strategy Prec.
Rec.
F1 R NR R NRUnbiased Full MT English 0.653 0.636 0.644 219.5 77.6 0.696 0.712TF 0.615 0.777 0.686 33.5 34.6 0.840 0.508IDF 0.537 0.470 0.501 84.7 45.8 0.444 0.700TFIDF 0.647 0.710 0.677 33.2 38.2 0.772 0.656Unbiased Word Clouds 0.599 0.652 0.621 50.5 39.5 0.685 0.621TFQ 0.605 0.809 0.692 55.3 82.4 0.864 0.436IDFQ 0.582 0.793 0.671 23.6 31.6 0.844 0.436TFIDFQ 0.599 0.738 0.661 37.9 26.9 0.804 0.500SFQ 0.591 0.813 0.685 55.7 49.4 0.876 0.504W 0.611 0.738 0.669 28.2 28.9 0.840 0.564Query Biased Word Clouds 0.597 0.778 0.675 36.4 34.2 0.846 0.488REL 0.582 0.746 0.654 30.6 44.3 0.832 0.548LQ 0.549 0.783 0.646 64.4 54.8 0.868 0.292LQP 0.578 0.734 0.647 28.2 28.0 0.768 0.472LQC 0.557 0.810 0.660 33.9 38.8 0.896 0.292Query Biased Sentences 0.566 0.768 0.651 39.2 41.5 0.841 0.401Table 2: Comparison of peer systems on DUC2005 shared-task for monolingual question-biasedsummarization, f-scores from ROUGE-2 andROUGE-SU4.Peer ID ROUGE-2 ROUGE-SU417 0.07170 0.129708 0.06960 0.127904 0.06850 0.12770Tel-Eng-Sum 0.06048 0.12058LQ 0.05124 0.09343REL 0.04914 0.090816.2 Analysis with DUC 2005We analysed our summarization methods bycomparing two of our sentence-based methods(LQ and REL) with peers from the monolin-gual question-biased summarization shared-taskfor DUC 2005.
Even though DUC 2005 is a mono-lingual task, we decided to use it as part of ouranalysis for two reasons: (1) to see how well wecould do with query/question biasing while ignor-ing the variables introduced by MT and cross-language text, and (2) to make a comparison toprevious work.
Pingali et al., (2007) also used thisthe same DUC task to assess their cross-languagequery biased summarization system.
Systemsfrom the DUC 2005 question-biased summariza-tion task were evaluated automatically against hu-man gold-standard summaries using ROUGE (Linand Hovy, 2003) .
Our results from the DUC2005 shared-task are shown in Table 2, reportedas ROUGE-2 and ROUGE-SU4 f-scores, as thesetwo variations of ROUGE are the most helpful(Dang, 2005; Pingali et al., 2007).Table 2 shows scores for several top peer sys-tems, as well as results for the Tel-Eng-Summethod from Pingali et al., (2007).
While we havereported f-scores in our analysis, we also note thatour implementations of LQ and REL outperformall of the DUC 2005 peer systems for precision, asshown in Table 3.
We also know that ROUGE can-not be used for comparing sentence summaries toranked lists of words and there are no existing in-trinsic methods to make that kind of comparison.Therefore we were able to successfully comparejust 2 of our sentence-based methods to previouswork using ROUGE.7 Discussion and Future WorkCross-language query biased summarization is animportant part of CLIR, because it helps the userdecide which foreign-language documents theymight want to read.
But, how do we know if665Table 3: Top 3 system precision scores forROUGE-2 and ROUGE-SU4.Peer ID ROUGE-2 ROUGE-SU4LQ 0.08272 0.15197REL 0.0809 0.1504915 0.07249 0.13129a query biased summary is ?good enough?
to beused in a real-world CLIR system?
We want tobe able to say that we can do query biased sum-marization just as well for both monolingual andcross-language IR systems.
From previous work,there has been some variability with regard towhen and what to translate - variables which haveno impact on monolingual summarization.
We at-tempted to address this issue with two of our meth-ods: LQP and LQC.
To fully exploit the MT vari-able, we would need many more relevance pre-diction experiments using humans who know L1and others who know L2.
Unfortunately in ourcase, we were not able to find Farsi speakers onMechanical Turk.
Access to these speakers wouldhave allowed us to try further experiments as wellas other kinds of analysis.Our results on the relevance prediction tasktell us that query biased summarization strategieshelp users identify relevant documents faster andwith better accuracy than unbiased summaries.Our findings support the findings of Tombros andSanderson (1998).
Another important finding isthat now we can weigh tradeoffs so that differentsummarization methods could be used to optimizeover different metrics.
For example, if we wantto optimize for retrieval-relevance we might selecta summarization method that tends to have higherrecall, such as scaled query biased term frequency(SFQ).
Similarly, we could optimize over accu-racy on relevant documents, and use CombinatoryLexRank (LQC) with Farsi and English together.We have shown that the relevance predictiontasks can be crowdsourced on Mechanical Turkwith reasonable results.
The data we used fromthe Farsi CLEF 2008 ad-hoc task included an an-swer key, but there were no parallel English docu-ments.
However, in order for the NLP communityto make strides in evaluating cross-language querybiased summarization for CLIR, we will need star-dards and data.
Optimal data would be paralleldatasets consisting of documents in L1and L2with queries in L1and L2along with an answerkey specifying which documents are relevant tothe queries.
Further we would also need sets ofhuman gold-standard query biased summaries inL1and L2.
These standards and data would al-low us to compare method-to-method across dif-ferent languages, while simultaneously allowingus to tease apart other variables such as: when andwhat to translate, translation quality, methods forbiasing, and type of summarization strategy (sen-tences, words, etc).
And of course it would be bet-ter if this standard dataset was multilingual insteadof billingual, for obvious reasons.We have approached cross-language query bi-ased summarization as a stand-alone problem,treating the CLIR system and document retrievalas a black box.
However, summaries need to pre-serve query-salience: summaries should not makeit more difficult to positively identify relavant doc-uments.
And they should also preserve retrieval-relevance: summaries should help users identifyas many relevant documents as possible.AcknowledgmentsWe would like to express thanks to David Har-wath at MIT Computer Science and Artificial In-telligence Laboratory (CSAIL), who helped us de-velop and implement ideas in this paper.
We alsowant to thank Terry Gleason from MIT LincolnLaboratory for providing machine translations.ReferencesEneko Agirre, Giorgio Maria Di Nunzio, Nicola Ferro,Thomas Mandl, and Carol Peters.
CLEF 2008: Adhoc track overview.
In Evaluating Systems for Mul-tilingual and Multimodal Information Access, pp15?37.
Springer Berlin Heidelberg, 2009.James Allan, Courtney Wade, and Alvaro Bolivar.
Re-trieval and Novelty Detection at the Sentence Level.In Proceedings of the 26th Annual InternationalACM SIGIR Conference on Research and Develop-ment in Informaion Retrieval, (SIGIR ?03).
ACM,New York, NY, USA, 314-321.Hosein Azarbonyad, Azadeh Shakery, and HeshaamFaili.
Exploiting Multiple Translation Resources forEnglish-Persian Cross Language Information Re-trieval.
In P. Forner, H. M?uller, R. Paredes, P. Rosso,and B. Stein, editors, Information Access Evalua-tion.
Multilinguality, Multimodality, and Visualiza-tion, volume 8138 of Lecture Notes in Computer Sci-ence, pp 93?99.
Springer Berlin Heidelberg, 2013.Lorena Leal Bando, Falk Scholer, Andrew Turpin.Constructing Query-biased Summaries: A Compar-ison of Human and System Generated Snippets.
In666Proceedings of the Third Symposium on InformationInteraction in Context (IIiX ?10), ACM 2010, NewYork, NY, USA, 195-204.Adam Berger and Vibhu O Mittal.
Query-RelevantSummarization Using FAQs.
In Proceedings of the38th Annual Meeting on Association for Computa-tional Linguistics, ACL 2000.Pinaki Bhaskar and Sivaji Bandyopadhyay.
Cross-Lingual Query Dependent Snippet Generation.
In-ternational Journal of Computer Science and Infor-mation Technology (IJCSIT), 3(4), 2012.Pinaki Bhaskar and Sivaji Bandyopadhyay.
LanguageIndependent Query Focused Snippet Generation.
InT.
Catarci, P. Forner, D. Hiemstra, A. Pe?nas, andG.
Santucci, editors, Information Access Evaluation.Multilinguality, Multimodality, and Visual Analytics,volume 7488 of Lecture Notes in Computer Science,pp 138?140.
Springer Berlin Heidelberg, 2012.Stephen P. Borgatti, Kathleen M. Carley, David Krack-hardt.
On the Robustness of Centrality MeasuresUnder Conditions of Imperfect Data.
Social Net-works, (28):124?136, 2006.Florian Boudin, St?ephane Huet, and Juan-ManuelTorres-Moreno.
A Graph-Based Approach to Cross-Language Multi-Document Summarization.
Poli-bits, (43):113?118, 2011.Chris Callison-Burch.
Fast, Cheap, and Creative: Eval-uating Translation Quality Using Amazon?s Me-chanical Turk.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pp 286?295, Singapore, ACL2009.Yllias Chali and Shafiq R. Joty.
Unsupervised Ap-proach for Selecting Sentences in Query-BasedSummarization.
In Proceedings of the Twenty-FirstInternational FLAIRS Conference, 2008.Niladri Chatterjee, Amol Mittal, and Shubham Goyal.Single Document Extractive Text SummarizationUsing Genetic Algorithms.
In Emerging Applica-tions of Information Technology (EAIT), 2012 ThirdInternational Conference, pp 19?23, 2012.Kenneth W. Church and William A. Gale.
Inverse Doc-ument Frequency (IDF): A Measure of DeviationsFrom Poisson.
In Natural language processing us-ing very large corpora, pages 283?295.
Springer,1999.Mathias Creutz and Krista Lagus.
Unsupervised Mod-els for Morpheme Segmentation and MorphologyLearning.
ACM Transactions on Speech and Lan-guage Processing, 4(1):3:1?3:34, February 2007.Hoa Trang Dang.
Overview of DUC 2005.
In Pro-ceedings of the Document Understanding Confer-ence, 2005.Hal Daum?e III, Daniel Marcu.
Bayesian Query-Focused Summarization.
In Proceedings of the 21stInternational Conference on Computational Lin-guistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, ACL 2006.Jennifer Drexler, Wade Shen, Terry P. Gleason, Timo-thy R. Anderson, Raymond E. Slyh, Brian M. Ore,and Eric G. Hansen.
The MIT-LL/AFRL IWSLT-2012 MT System.
In Proceedings of the Interna-tional Workshop on Spoken Language Translation(IWSLT), Hong Kong, December 2012.Bonnie J. Dorr, Christof Monz, Stacy President,Richard Schwartz, and David Zajic.
A Methodol-ogy for Extrinsic Evaluation of Text Summarization:Does ROUGE Correlate?
In Proceedings of theACL Workshop on Intrinsic and Extrinsic Evalua-tion Measures for Machine Translation and/or Sum-marization, pp 1-8.
Ann Arbor, ACL 2005.H.
P. Edmundson.
New Methods in Automatic Extract-ing.
In Journal of the ACM, 16(2):264?285, April1969.G?unes?
Erkan and Dragomir R. Radev.
Lexrank:Graph-based lexical centrality as salience in textsummarization.
Journal of Artificial IntelligenceResearch, 22(1):457?479, December 2004.Dan Gillick and Yang Liu.
Non-Expert Evaluationof Summarization Systems is Risky.
In Proceed-ings of NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechan-ical Turk, pp 148-151, Los Angeles, California,USA, June, 2010.David Harwath and Timothy J. Hazen.
Topic Identi-fication Based Extrinsic Evaluation of Summariza-tion Techniques Applied to Conversational Speech.In Proceedings of ICASSP, 2012: 5073-5076.Stacy P. Hobson, Bonnie J. Dorr, Christof Monz, andRichard Schwartz.
Task-Eased Evaluation of TextSummarization Using Relevance Prediction.
In In-formation Processing Management, 43(6): 1482-1499, 2007.Hongyan Jing, Regina Barzilay, Kathleen McKeown,and Michael Elhadad.
Summarization EvaluationMethods: Experiments and Analysis.
In Proceed-ings of American Association for Artificial Ingelli-gence (AAAI), 1998.Reza Karimpour, Amineh Ghorbani, Azadeh Pishdad,Mitra Mohtarami, Abolfazl AleAhmad, Hadi Amiri,and Farhad Oroumchian.
Improving Persian Infor-mation Retrieval Systems Using Stemming and Partof Speech Tagging.
In Proceedings of the 9th Cross-language Evaluation Forum Conference on Evaluat-ing Systems for Multilingual and Multimodal Infor-mation Access, CLEF 2008, pp 89?96, Berlin, Hei-delberg, 2009.
Springer-Verlag.667Chin-Yew Lin.
Looking For A Few Good Metrics:Automatic Summarization Evaluation - How ManySamples Are Enough?
In Proceedings of NTCIRWorkshop 4, Tokyo, Japan, June 2004.Annie Louis and Ani Nenkova.
Automatic SummaryEvaluation without Human Models.
In Proceedingsof Empirical Methods in Natural Language Process-ing, EMNLP 2009.H.
P. Luhn.
The Automatic Creation of Literature Ab-stracts.
IBM Journal of Research and Development,2(2):159?165, April 1958.Inderjeet Mani, Eric Bloedorn, and Barbara Gates.
Us-ing Cohesion and Coherence Models for Text Sum-marization.
In AAAI Symposium Technical ReportSS-989-06, AAAI Press, 69?76, 1998.Inderjeet Mani, David House, Gary Klein, LynetteHirschman, Therese Firmin, and Beth Sundheim.The TIPSTER SUMMAC Text SummarizationEvaluation.
In Proceedings of European Associa-tion for Coputational Linguistics, EACL 1999.Inderjeet Mani.
Summarization Evaluation: AnOverview.
In Proceedings of the NTCIR Workshop,Vol.
2, 2001.Inderjeet Mani, Gary Klein, David House, LynetteHirschman, Therese Firmin, and Beth Sundheim.SUMMAC: A Text Summarization Evaluation.
Nat-ural Language Engineering, 8(1) 43-68.
March2002.Kathleen McKeown, Rebecca J. Passonneau, David K.Elson, Ani Nenkova, and Julia Hirschberg.
Do Sum-maries Help?
A Task-Based Evaluation of Multi-Document Summarization.
In Proceedings of the28th Annual International ACM SIGIR Conferenceon Research and Development in Information Re-trieval, pp 210-217.
ACM 2005.Anthony McCallum, Gerald Penn, Cosmin Munteanu,and Xiaodan Zhu.
Ecological Validity and the Eval-uation of Speech Summarization Quality.
In Pro-ceedings of Workshop on Evaluation Metrics andSystem Comparison for Automatic Summarization.2012 Association for Computational Linguistics,Stroudsburg, PA, USA, 28-35.Tatsunori Mori, Masanori Nozawa, and YoshiakiAsada.
Multi-Answer Focused Multi-DocumentSummarization Using a Question-Answering En-gine.
In Proceedings of the 20th InternationalConference on Computational Linguistics, COLING?04, Stroudsburg, PA, USA, ACL 2004.Gabriel Murray, Thomas Kleinbauer, Peter Poller,Tilman Becker, Steve Renals, and Jonathan Kilgour.Extrinsic Summarization Evaluation: A DecisionAudit Task.
ACM Transactions on Speech and Lan-guage Processing, 6(2) Article 2, October 2009.Ani Nenkova and Kathleen McKeown.
A Survey ofText Summarization Techniques.
In C. C. Aggarwaland C. Zhai, editors, Mining Text Data, pp 43?76.Springer US, 2012.Constantin Or?asan and Oana Andreea Chiorean.
Eval-uation of a Cross-Lingual Romanian-English Multi-Document Summariser.
In Proceedings of Lan-guage Resources and Evaluation Conference, LREC2008.Jahna Otterbacher, G?unes?
Erkan, and Dragomir RRavev.
Using Random Walks for Question-focusedSentence Retrieval.
In Proceedings of Human Lan-guage Technology Conference on Empirical Meth-ods in Natural Language Processing, Vancouver,Canada, pp 915-922, EMNLP 2005.Jahna Otterbacher, G?unes?
Erkan, and Dragomir R.Ravev.
Biased LexRank: Passage Retrieval UsingRandom Walks With Question-Based Priors.
In In-formation Processing Management, 45(1), January2009, pp 42-54.Karolina Owczarzak, John M. Conroy, Hoa TrangDang, and Ani Nenkova.
An Assessment of the Ac-curacy of Automatic Evaluation in Summarization.In Proceedings of the Workshop on Evaluation Met-rics and System Comparison for Automatic Summa-rization, pp 1-9, Montr?eal, Canada, ACL 2012.Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd.
The Pagerank Citation Ranking:Bringing Order to the Web.
Technical report, Stan-ford Digital Library Technologies Project, 1998.Prasad Pingali, Jagadeesh Jagarlamudi, and VasudevaVarma.
Experiments in Cross Language Query Fo-cused Multi-Document Summarization In Work-shop on Cross Lingual Information Access Address-ing the Information Need of Multilingual Societies,IJCAI 2007.Stacy F. President and Bonnie J. Dorr.
Text Sum-marization Evaluation: Correlating Human Perfor-mance on an Extrinsic Task With Automatic In-trinsic Metrics.
No.
LAMP-TR-133.
University ofMaryland College Park Language and Media Pro-cessing Laboratory Institute for Advanced ComputerStudies (UMIACS), 2006.Dragomir R. Radev, Hongyan Jing, Malgorzata Sty?s,and Daniel Tam.
Centroid-Based Summarizationof Multiple Documents.
InProceedings of In-formaion Processing Management, 40(6):919?938,Nov.
2004.Stephen Robertson.
Understanding Inverse Docu-ment Frequency: on Theoretical Arguments for IDF.Journal of Documentation, 60(5):503?520, 2004.Stephen E. Robertson and Steve Walker.
Some SimpleEffective Approximations to the 2-Poisson Modelfor Probabilistic Weighted Retrieval.
In Proceed-ings of the 17th annual international ACM SIGIR668conference on Research and development in infor-mation retrieval, pp 232?241.
Springer-Verlag NewYork, Inc., 1994.Gerard Salton and Chung-Shu Yang.
On the Specifica-tion of Term Values in Automatic Indexing.
Journalof Documentation, 29(4):351?372, 1973.Anastasios Tombros and Mark Sanderson.
Advantagesof Query Biased Summaries in Information Re-trieval.
In Proceedings of the 21st annual interna-tional ACM SIGIR conference on Research and de-velopment in information retrieval, pp 2?10.
ACM,1998.Xiaojun Wan and Jianguo Xiao.
Graph-BasedMulti-Modality Learning for Topic-Focused Multi-Document Summarization.
In Proceedings of the21st international jont conference on Artifical intel-ligence (IJCAI?09), San Francisco, CA, USA, 1586-1591.Xiaojun Wan, Huiying Li, and Jianguo Xiao.
Cross-Language Document Summarization Based on Ma-chine Translation Quality Prediction.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics (ACL ?10).
Associ-ation for Computational Linguistics, Stroudsburg,PA, USA, 917-926.Xiaojun Wan, Houping Jia, Shanshan Huang, and Jian-guo Xiao.
Summarizing the Differences in Multilin-gual News.
In Proceedings of the 34th InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval, SIGIR ?11, pp 735?744, New York, NY, USA, 2011.
ACM.Wenpeng Yin, Yulong Pei, Fan Zhang, and Lian?enHuang.
SentTopic-MultiRank: A Novel RankingModel for Multi-Document Summarization.
In Pro-ceedings of COLING, pages 2977?2992, 2012.Junlin Zhang, Le Sun, and Jinming Min.
Usingthe Web Corpus to Translate the Queries in Cross-Lingual Information Retrieval.
In Proceedings in2005 IEEE International Conference on NaturalLanguage Processing and Knowledge Engineering,2005, IEEE NLP-KE ?05, pp 493?498, 2005.669
