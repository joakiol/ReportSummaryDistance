Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 805?814,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsPairwise Neural Machine Translation EvaluationFrancisco Guzm?an Shafiq Joty Llu?
?s M`arquez and Preslav NakovALT Research GroupQatar Computing Research Institute ?
HBKU, Qatar Foundation{fguzman,sjoty,lmarquez,pnakov}@qf.org.qaAbstractWe present a novel framework for ma-chine translation evaluation using neuralnetworks in a pairwise setting, where thegoal is to select the better translation froma pair of hypotheses, given the referencetranslation.
In this framework, lexical,syntactic and semantic information fromthe reference and the two hypotheses iscompacted into relatively small distributedvector representations, and fed into amulti-layer neural network that models theinteraction between each of the hypothe-ses and the reference, as well as betweenthe two hypotheses.
These compact repre-sentations are in turn based on word andsentence embeddings, which are learnedusing neural networks.
The framework isflexible, allows for efficient learning andclassification, and yields correlation withhumans that rivals the state of the art.1 IntroductionAutomatic machine translation (MT) evaluation isa necessary step when developing or comparingMT systems.
Reference-based MT evaluation, i.e.,comparing the system output to one or more hu-man reference translations, is the most commonapproach.
Existing MT evaluation measures typ-ically output an absolute quality score by com-puting the similarity between the machine andthe human translations.
In the simplest case, thesimilarity is computed by counting word n-grammatches between the translation and the reference.This is the case of BLEU (Papineni et al, 2002),which has been the standard for MT evaluation foryears.
Nonetheless, more recent evaluation mea-sures take into account various aspects of linguis-tic similarity, and achieve better correlation withhuman judgments.Having absolute quality scores at the sentencelevel allows to rank alternative translations for agiven source sentence.
This is useful, for instance,for statistical machine translation (SMT) parame-ter tuning, for system comparison, and for assess-ing the progress during MT system development.The quality of automatic MT evaluation metricsis usually assessed by computing their correlationwith human judgments.
To that end, quality rank-ings of alternative translations have been createdby human judges.
It is known that assigning anabsolute score to a translation is a difficult taskfor humans.
Hence, ranking-based evaluations,where judges are asked to rank the output of 2 to 5systems, have been used in recent years, whichhas yielded much higher inter-annotator agree-ment (Callison-Burch et al, 2007).These human quality judgments can be used totrain automatic metrics.
This supervised learningcan be oriented to predict absolute scores, e.g., us-ing regression (Albrecht and Hwa, 2008), or rank-ings (Duh, 2008; Song and Cohn, 2011).
A partic-ular case of the latter is used to learn in a pair-wise setting, i.e., given a reference and two al-ternative translations (or hypotheses), the task isto decide which one is better.
This setting em-ulates closely how human judges perform evalu-ation assessments in reality, and can be used toproduce rankings for an arbitrarily large numberof hypotheses.
In this pairwise setting, the chal-lenge is to learn, from a pair of hypotheses, whichare the features that help to discriminate the betterfrom the worse translation.
Although the pairwisesetting does not produce absolute quality scores(i.e., it is not an evaluation metric applicable to asingle translation), it is useful and arguably suf-ficient for most evaluation and MT developmentscenarios.11We do not argue that the pairwise approach is betterthan the direct estimation of human quality scores.
Both ap-proaches have pros and cons; we see them as complementary.805Recently, Guzm?an et al (2014a) presented alearning framework for this pairwise setting, basedon preference kernels and support vector ma-chines (SVM).
They obtained promising resultsusing syntactic and discourse-based structures.However, using convolution kernels over complexstructures comes at a high computational cost bothat training and at testing time because the use ofkernels requires that the SVM operate in the muchslower dual space.
Thus, some simplification isneeded to make it practical.
While there are somesolutions in the kernel-based learning frameworkto alleviate the computational burden, in this pa-per we explore an entirely different direction.We present a novel neural-based architecture forlearning in the pairwise setting for MT evalua-tion.
Lexical, syntactic and semantic informationfrom the reference and the two hypotheses is com-pacted into relatively small distributed vector rep-resentations and fed into the input layer, togetherwith a set of individual real-valued features com-ing from simple pre-existing MT evaluation met-rics.
A hidden layer, motivated by our intuitionson the pairwise ranking problem, is used to cap-ture interactions between the relevant input com-ponents.
Finally, we present a task-oriented costfunction, specifically tailored for this problem.Our evaluation results on the WMT12 metricstask benchmark datasets (Callison-Burch et al,2012) show very high correlation with humanjudgments.
These results clearly surpass (Guzm?anet al, 2014a) and are comparable to the best pre-viously reported results for this dataset, achievedby DiscoTK (Joty et al, 2014), which is a muchheavier combination-based metric.Another advantage of the proposed architectureis efficiency.
Due to the vector-based compres-sion of the linguistic structure and the relativelyreduced size of the network, testing is fast, whichwould greatly facilitate the practical use of this ap-proach in real MT evaluation and development.Finally, we empirically show that syntactically-and semantically-oriented embeddings can be in-corporated to produce sizeable and cumulativegains in performance over a strong combinationof pre-existing MT evaluation measures (BLEU,NIST, METEOR, and TER).
This is promising ev-idence towards our longer-term goal of defining ageneral platform for integrating varied linguisticinformation and for producing more informed MTevaluation measures.2 Related WorkContemporary MT evaluation measures haveevolved beyond simple lexical matching, andnow take into account various aspects of lin-guistic structures, including synonymy and para-phrasing (Lavie and Denkowski, 2009), syn-tax (Gim?enez and M`arquez, 2007; Popovi?c andNey, 2007; Liu and Gildea, 2005), seman-tics (Gim?enez and M`arquez, 2007; Lo et al,2012), and even discourse (Comelles et al, 2010;Wong and Kit, 2012; Guzm?an et al, 2014b; Jotyet al, 2014).
The combination of several ofthese aspects has led to improved results in metricevaluation campaigns, such as the WMT metricstask (Bojar et al, 2014).In this paper, we present a general frameworkfor learning to rank translations in the pairwisesetting, using information from several linguisticrepresentations of the translations and references.This work has connections with the ranking-basedapproaches for learning to reproduce human judg-ments of MT quality.
In particular, our setting issimilar to that of Duh (2008), but differs from itboth in terms of the feature representation and ofthe learning framework.
For instance, we integrateseveral layers of linguistic information, while Duh(2008) only used lexical and POS matches as fea-tures.
Secondly, we use information about boththe reference and the two alternative translationssimultaneously in a neural-based learning frame-work capable of modeling complex interactionsbetween the features.Another related work is that of Kulesza andShieber (2004), in which lexical and syntactic fea-tures, together with other metrics, e.g., BLEU andNIST, are used in an SVM classifier to discrimi-nate good from bad translations.
However, theirsetting is not pairwise comparison, but a classifi-cation task to distinguish human- from machine-produced translations.
Moreover, in their work,using syntactic features decreased the correla-tion with human judgments dramatically (althoughclassification accuracy improved), while in ourcase the effect is positive.In our previous work (Guzm?an et al, 2014a),we introduced a learning framework for the pair-wise setting, based on preference kernels andSVMs.
We used lexical, POS, syntactic anddiscourse-based information in the form of tree-like structures to learn to differentiate better fromworse translations.806However, in that work we used convolution ker-nels, which is computationally expensive and doesnot scale well to large datasets and complex struc-tures such as graphs and enriched trees.
This in-efficiency arises both at training and testing time.Thus, here we use neural embeddings and multi-layer neural networks, which yields an efficientlearning framework that works significantly betteron the same datasets (although we are not usingexactly the same information for learning).To the best of our knowledge, the applicationof structured neural embeddings and a neural net-work learning architecture for MT evaluation iscompletely novel.
This is despite the growing in-terest in recent years for deep neural nets (NNs)and word embeddings with application to a myr-iad of NLP problems.
For example, in SMT wehave observed an increased use of neural nets forlanguage modeling (Bengio et al, 2003; Mikolovet al, 2010) as well as for improving the transla-tion model (Devlin et al, 2014; Sutskever et al,2014).Deep learning has spread beyond languagemodeling.
For example, recursive NNs have beenused for syntactic parsing (Socher et al, 2013a)and sentiment analysis (Socher et al, 2013b).
Theincreased use of NNs by the NLP community isin part due to (i) the emergence of tools such asword2vec (Mikolov et al, 2013a) and GloVe (Pen-nington et al, 2014), which have enabled NLP re-searchers to learn word embeddings, and (ii) uni-fied learning frameworks, e.g., (Collobert et al,2011), which cover a variety of NLP tasks suchas part-of-speech tagging, chunking, named entityrecognition, and semantic role labeling.While in this work we make use of widely avail-able pre-computed structured embeddings, thenovelty of our work goes beyond the type of infor-mation considered as input, and resides on the wayit is integrated to a neural network architecture thatis inspired by our intuitions about MT evaluation.3 Neural Ranking ModelOur motivation for using neural networks for MTevaluation is twofold.
First, to take advantage oftheir ability to model complex non-linear relation-ships efficiently.
Second, to have a frameworkthat allows for easy incorporation of rich syntac-tic and semantic representations captured by wordembeddings, which are in turn learned using deeplearning.3.1 Learning TaskGiven two translation hypotheses t1and t2(and areference translation r), we want to tell which ofthe two is better.2Thus, we have a binary classifi-cation task, which is modeled by the class variabley, defined as follows:y ={1 if t1is better than t2given r0 if t1is worse than t2given r(1)We model this task using a feed-forward neuralnetwork (NN) of the form:p(y|t1, t2, r) = Ber(y|f(t1, t2, r)) (2)which is a Bernoulli distribution of y with param-eter ?
= f(t1, t2, r), defined as follows:f(t1, t2, r) = sig(wTv?
(t1, t2, r) + bv) (3)where sig is the sigmoid function, ?
(x) defines thetransformations of the input x through the hiddenlayer, wvare the weights from the hidden layer tothe output layer, and bvis a bias term.3.2 Network ArchitectureIn order to decide which hypothesis is better giventhe tuple (t1, t2, r) as input, we first map the hy-potheses and the reference to a fixed-length vec-tor [xt1,xt2,xr], using syntactic and semantic em-beddings.
Then, we feed this vector as input toour neural network, whose architecture is shownin Figure 1.f(t1,t2,r)?
(t1,r) ?
(t2,r)h12h1rh2rvxt2xrxt1t1t2rsentences  embeddings pairwise nodes pairwise featuresoutput layerFigure 1: Overall architecture of the neural network.In our architecture, we model three types of in-teractions, using different groups of nodes in thehidden layer.
We have two evaluation groups h1rand h2rthat model how similar each hypothesis tiis to the reference r.2In this work, we do not learn to predict ties, and ties areexcluded from our training data.807The vector representations of the hypothesis(i.e., xt1or xt2) together with the reference(i.e., xr) constitute the input to the hidden nodesin these two groups.
The third group of hiddennodes h12, which we call similarity group, mod-els how close t1and t2are.
This might be usefulas highly similar hypotheses are likely to be com-parable in quality, irrespective of whether they aregood or bad in absolute terms.The input to each of these groups is repre-sented by concatenating the vector representationsof the two components participating in the inter-action, i.e., x1r= [xt1,xr], x2r= [xt2,xr],x12= [xt1,xt2].
In summary, the transformation?
(t1, t2, r) = [h12,h1r,h2r] in our NN architec-ture can be written as follows:h1r= g(W1rx1r+ b1r)h2r= g(W2rx2r+ b2r)h12= g(W12x12+ b12)where g(.)
is a non-linear activation function (ap-plied component-wise), W ?
RH?Nare the asso-ciated weights between the input layer and the hid-den layer, and b are the corresponding bias terms.In our experiments, we used tanh as an activationfunction, rather than sig, to be consistent with howparts of our input vectors were generated.3In addition, our model allows to incorporate ex-ternal sources of information by enabling skip arcsthat go directly from the input to the output, skip-ping the hidden layer.
In our setting, these arcsrepresent pairwise similarity features between thetranslation hypotheses and the reference (e.g., theBLEU scores of the translations).
We denote thesepairwise external feature sets as ?1r= ?
(t1, r)and ?2r= ?
(t2, r).
When we include the externalfeatures in our architecture, the activation at theoutput, i.e., eq.
(3), can be rewritten as follows:f(t1, t2, r) = sig(wTv[?
(t1, t2, r), ?1r, ?2r] + bv)3.3 Network TrainingThe negative log likelihood of the train-ing data for the model parameters?
= (W12,W1r,W2r,wv,b12,b1r,b2r, bv)can be written as follows:J?= ?
?nynlog y?n?+ (1?
yn) log (1?
y?n?
)(4)3Many of our input representations consist of word em-beddings trained with neural networks that used tanh as anactivation function.In the above formula, y?n?= fn(t1, t2, r) isthe activation at the output layer for the n-thdata instance.
It is also common to use a reg-ularized cost function by adding a weight decaypenalty (e.g., L2or L1regularization) and to per-form maximum aposteriori (MAP) estimation ofthe parameters.
We trained our network withstochastic gradient descent (SGD), mini-batchesand adagrad updates (Duchi et al, 2011), usingTheano (Bergstra et al, 2010).4 Experimental SetupIn this section, we describe the different aspectsof our general experimental setup (we will discusssome extensions thereof in Section 6), startingwith a description of the input representations weuse to capture the syntactic and semantic charac-teristics of the two hypothesis translations and thecorresponding reference, as well as the datasetsused to evaluate the performance of our model.4.1 Word Embedding VectorsWord embeddings play a crucial role in our model,since they allow us to model complex relationsbetween the translations and the reference usingsyntactic and semantic vector representations.Syntactic vectors.
We generate a syntactic vectorfor each sentence using the Stanford neural parser(Socher et al, 2013a), which generates a 25-dimensional vector as a by-product of syntacticparsing using a recursive NN.
Below we will referto these vectors as SYNTAX25.Semantic vectors.
We compose a semantic vectorfor a given sentence using the average of the em-bedding vectors for the words it contains (Mitchelland Lapata, 2010).
We use pre-trained, fixed-length word embedding vectors produced by(i) GloVe (Pennington et al, 2014), (ii) COM-POSES (Baroni et al, 2014), and (iii) word2vec(Mikolov et al, 2013b).Our primary representation is based on 50-dimensional GloVe vectors, trained on Wikipedia2014+Gigaword 5 (6B tokens), to which below wewill refer as WIKI-GW25.Furthermore, we experiment with WIKI-GW300, the 300-dimensional GloVe vectorstrained on the same data, as well as with the CC-300-42B and CC-300-840B, 300-dimensionalGloVe vectors trained on 42B and on 840B tokensfrom Common Crawl.808We also experiment with the pre-trained, 300-dimensional word2vec embedding vectors, orWORD2VEC300, trained on 100B words fromGoogle News.
Finally, we use COMPOSES400,the 400-dimensional COMPOSES vectors trainedon 2.8 billion tokens from ukWaC, the EnglishWikipedia, and the British National Corpus.4.2 Tuning and Evaluation DatasetsWe experiment with datasets of segment-levelhuman rankings of system outputs from theWMT11, WMT12 and WMT13 Metrics sharedtasks (Callison-Burch et al, 2011; Callison-Burchet al, 2012; Mach?a?cek and Bojar, 2013).
We focuson translating into English, for which the WMT11and WMT12 datasets can be split by source lan-guage: Czech (cs), German (de), Spanish (es), andFrench (fr); WMT13 also has Russian (ru).4.3 Evaluation ScoreWe evaluate our metrics in terms of correlationwith human judgments measured using Kendall?s?
.
We report ?
for the individual languages as wellas macro-averaged across all languages.Note that there were different versions of ?
atWMT over the years.
Prior to 2013, WMT used astrict version, which was later relaxed at WMT13and further revised at WMT14.
See (Mach?a?cekand Bojar, 2014) for a discussion.
Here we use thestrict version used at WMT11 and WMT12.4.4 Experimental SettingsDatasets: We train our neural models on WMT11and we evaluate them on WMT12.
We further usea random subset of 5,000 examples from WMT13as a validation set to implement early stopping.Early stopping: We train on WMT11 for up to10,000 epochs, and we calculate Kendall?s ?
onthe development set after each epoch.
We then se-lect the model that achieves the highest ?
on thevalidation set; in case of ties for the best ?
, weselect the latest epoch that achieved the highest ?
.Network parameters: We train our neural net-work using SGD with adagrad, an initial learningrate of ?
= 0.01, mini-batches of size 30, and L2regularization with a decay parameter ?
= 1e?4.We initialize the weights for our matrices by sam-pling from a uniform distribution following (Ben-gio and Glorot, 2010).
We further set the sizeof each of our pairwise hidden layers H to fournodes, and we normalize the input data using min-max to map the feature values to the range [?1, 1].5 Experiments and ResultsThe main findings of our experiments are shownin Table 1.
Section I of Table 1 shows the re-sults for four commonly-used metrics for MT eval-uation that compare a translation hypothesis tothe reference(s) using primarily lexical informa-tion like word and n-gram overlap (even thoughsome allow paraphrases): BLEU, NIST, TER,and METEOR (Papineni et al, 2002; Doddington,2002; Snover et al, 2006; Denkowski and Lavie,2011).
We will refer to the set of these four met-rics as 4METRICS.
These metrics are not tunedand achieve Kendall?s ?
between 18.5 and 23.5.Section II of Table 1 shows the results for multi-layer neural networks trained on vectors fromword embeddings only: SYNTAX25 and WIKI-GW25.
These networks achieve modest ?
valuesaround 10, which should not be surprising: theyuse very general vector representations and haveno access to word or n-gram overlap or to lengthinformation, which are very important features tocompute similarity against the reference.
How-ever, as will be discussed below, their contributionis complementary to the four previous evaluationmetrics and will lead to significant improvementsin combination with them.Section III of Table 1 shows the results for neu-ral networks that combine the four metrics from4METRICS with SYNTAX25 and WIKI-GW25.We can see that just combining the four metricsin a flat neural net (i.e., no hidden layer), whichis equivalent to a logistic regression, yields a ?
of27.06, which is better than the best of the four met-rics by 3.5 points absolute, and also better by over1.5 points absolute than the best metric that par-ticipated at the WMT12 metrics task competition(SPEDE07PP with ?
= 25.4).
Indeed, 4METRICSis a strong mix that involves not only simple lex-ical overlap but also approximate matching, para-phrases, edit distance, lengths, etc.
Yet, adding to4METRICS the embedding vectors yields sizeablefurther improvements: +1.5 and +2.0 points abso-lute when adding SYNTAX25 and WIKI-GW25,respectively.
Finally, adding both yields evenfurther improvements close to ?
of 30 (+2.64 ?points), showing that lexical semantics and syn-tactic representations are complementary.Section IV of Table 1 puts these numbers in per-spective: it lists the ?
for the top three systems thatparticipated at WMT12, whose scores ranged be-tween 22.9 and 25.4.809System Details Kendall?s ?I 4METRICS: commonly-used individual metrics cz de es fr AVGBLEU no learning 15.88 18.56 18.57 20.83 18.46NIST no learning 19.66 23.09 20.41 22.21 21.34TER no learning 17.80 25.31 22.86 21.05 21.75METEOR no learning 20.82 26.79 23.81 22.93 23.59II NN using embedding vectors: syntactic & semanticSYNTAX25 multi-layer NN 8.00 13.03 12.11 7.42 10.14WIKI-GW25 multi-layer NN 14.31 11.49 9.24 4.99 10.01III NN using 4METRICS+ embedding vectors4METRICS logistic regression 23.46 29.95 27.49 27.36 27.064METRICS+SYNTAX25 multi-layer NN 26.09 30.58 29.30 28.07 28.514METRICS+WIKI-GW25 multi-layer NN 25.67 32.50 29.21 28.92 29.074METRICS+SYNTAX25+WIKI-GW25 multi-layer NN 26.30 33.19 30.38 28.92 29.70IV Comparison to previous results on WMT12DiscoTK (Joty et al, 2014) Best on the WMT12 dataset na na na na 30.5SPEDE07PP 1st at the WMT12 competition 21.2 27.8 26.5 26.0 25.4METEOR?2nd at WMT12 the competition 21.2 27.5 24.9 25.1 24.7(Guzm?an et al, 2014a) Preference kernel approach 23.1 25.8 22.6 23.2 23.7AMBER 3rd at the WMT12 competition 19.1 24.8 23.1 24.5 22.9Table 1: Kendall?s tau (? )
on the WMT12 dataset for various metrics.
Notes: (i) the version of METEOR that took part in theWMT12 competition (marked with?in section IV of the table) is different from the one used in our experiments (section I ofthe table), (ii) values marked as na were not reported by the authors.We can see that 4METRICS is much strongerthan the winner at WMT12, and thus arguably abaseline hard to improve upon.
While our resultsare slightly behind those of DiscoTK (Joty et al,2014), we should note that we only combine fourmetrics, plus the vectors, while DiscoTK com-bines over 20 metrics, many of which are costlyto compute.On the other hand, we work in a ranking frame-work, i.e., we are not interested in producing anabsolute score, but in making pairwise decisionsonly.
Mapping these pairwise decisions into an ab-solute score is challenging and in our experimentsit leads to a slight drop in ?
(results omitted hereto save space).The only other result on WMT12 by authorsworking with our pairwise framework is our ownprevious work (Guzm?an et al, 2014a), where weused a preference kernel approach to combine syn-tactic and discourse trees with lexical information;as we can see, our earlier results are 6 absolutepoints lower than those we achieve here.
More-over, our NN approach offers advantages overSVMs in terms of computational cost.Based on these results, we can conclude thatword embeddings, whether syntactic or semantic,offer generalizations that efficiently complementvery strong metric combinations, and thus shouldbe considered when designing future MT evalua-tion metrics.6 DiscussionIn this section, we explore how different parts ofour framework can be modified to improve its per-formance, or how it can be extended for furthergeneralization.
First, we explore variations of thefeature sets from the perspective of both the pair-wise features and the embeddings.
Then, we ana-lyze the role of the network architecture and of thecost function used for learning.6.1 Fine-Grained Pairwise FeaturesWe have shown that our NN can integrate syntacticand semantic vectors with scores from other met-rics.
In fact, ours is a more general framework,where one can integrate the components of a met-ric instead of its score, which could yield betterlearning.
Below, we demonstrate this for BLEU.BLEU has different components: the n-gramprecisions, the n-gram matches, the total num-ber of n-grams (n=1,2,3,4), the lengths of the hy-potheses and of the reference, the length ratio be-tween them, and BLEU?s brevity penalty.
We willrefer to this decomposed BLEU as BLEUCOMP.Some of these features were previously used inSIMPBLEU (Song and Cohn, 2011).The results of using the components ofBLEUCOMP as features are shown in Table 2.
Wesee that using a single-layer neural network, whichis equivalent to logistic regression, outperformsBLEU by more than +1 ?
points absolute.810Kendall?s ?System Details cz de es fr AVGBLEU no learning 15.88 18.56 18.57 20.83 18.46BLEUCOMP logistic regression 18.18 21.13 19.79 19.91 19.75BLEUCOMP+SYNTAX25 multi-layer NN 20.75 25.32 24.85 23.88 23.70BLEUCOMP+WIKI-GW25 multi-layer NN 22.96 26.63 25.99 24.10 24.92BLEUCOMP+SYNTAX25+WIKI-GW25 multi-layer NN 22.84 28.92 27.95 24.90 26.15BLEU+SYNTAX25+WIKI-GW25 multi-layer NN 20.03 25.95 27.07 23.16 24.05Table 2: Kendall?s ?
on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU.
For comparison,the last line shows a combination using BLEU instead of BLEUCOMP.Source Alone Comb.WIKI-GW25 10.01 29.70WIKI-GW300 9.66 29.90CC-300-42B 12.16 29.68CC-300-840B 11.41 29.88WORD2VEC300 7.72 29.13COMPOSES400 12.35 28.54Table 3: Average Kendall?s ?
on WMT12 for semantic vec-tors trained on different text collections.
Shown are results(i) when using the semantic vectors alone, and (ii) when com-bining them with 4METRICS and SYNTAX25.
The improve-ments over WIKI-GW25 are marked in bold.As before, adding SYNTAX25 and WIKI-GW25 improves the results, but now by a moresizable margin: +4 for the former and +5 for thelatter.
Adding both yields +6.5 improvement overBLEUCOMP, and almost 8 points over BLEU.We see once again that the syntactic and seman-tic word embeddings are complementary to the in-formation sources used by metrics such as BLEU,and that our framework can learn from richer pair-wise feature sets such as BLEUCOMP.6.2 Larger Semantic VectorsOne interesting aspect to explore is the effect ofthe dimensionality of the input embeddings.
Here,we studied the impact of using semantic vectorsof bigger sizes, trained on different and larger textcollections.
The results are shown in Table 3.We can see that, compared to the 50-dimensionalWIKI-GW25, 300-400 dimensional vectors aregenerally better by 1-2 ?
points absolute whenused in isolation; however, when used in combina-tion with 4METRICS+SYNTAX25, they do not of-fer much gain (up to +0.2), and in some cases, weobserve a slight drop in performance.
We suspectthat the variability across the different collectionsis due to a domain mismatch.
Yet, we defer thisquestion for future work.Kendall?s ?Details cz de es fr AVGsingle-layer 25.86 32.06 30.03 28.45 29.10multi-layer 26.30 33.19 30.38 28.92 29.70Table 4: Kendall?s tau (? )
on the WMT12 dataset for al-ternative architectures using 4METRICS+SYNTAX25+WIKI-GW25 as input.6.3 Deep vs. Flat Neural NetworkOne interesting question is how much of the learn-ing is due to the rich input representations, andhow much happens because of the architecture ofthe neural network.
To answer this, we exper-imented with two settings: a single-layer neuralnetwork, where all input features are fed directlyto the output layer (which is logistic regression),and our proposed multi-layer neural network.The results are shown in Table 4.
We can seethat switching from our multi-layer architecture toa single-layer one yields an absolute drop of 0.6?
.
This suggests that there is value in using thedeeper, pairwise layer architecture.6.4 Task-Specific Cost FunctionAnother question is whether the log-likelihoodcost function J(?)
(see Section 3.3) is the mostappropriate for our ranking task, provided that it isevaluated using Kendall?s ?
as defined below:?
=concord.?
disc.?
tiesconcord+ disc.+ ties(5)where concord., disc.
and ties are the number ofconcordant, disconcordant and tied pairs.Given an input tuple (t1, t2, r), the logistic costfunction yields larger values of ?
= f(t1, t2, r) ify = 1, and smaller if y = 0, where 0 ?
?
?
1 isthe parameter of the Bernoulli distribution.
How-ever, it does not model directly the probabilitywhen the order of the hypotheses in the tuple isreversed, i.e., ?
?= f(t2, t1, r).811Kendall?s ?Details cz de es fr AVGLogistic 26.30 33.19 30.38 28.92 29.70Kendall 27.04 33.60 29.48 28.54 29.53Log.+Ken.
26.90 33.17 30.40 29.21 29.92Table 5: Kendall?s tau (? )
on WMT12 for alternative costfunctions using 4METRICS+SYNTAX25+WIKI-GW25.For our specific task, given an input tuple(t1, t2, r), we want to make sure that the differencebetween the two output activations ?
= ?
?
?
?ispositive when y = 1, and negative when y = 0.Ensuring this would take us closer to the actualobjective, which is Kendall?s ?
.
One possible wayto do this is to introduce a task-specific cost func-tion that penalizes the disagreements similarly tothe way Kendall?s ?
does.4In particular, we de-fine a new Kendall cost as follows:J?= ??nynsig(??
?n) + (1?
yn) sig(?
?n)(6)where we use the sigmoid function sig as a differ-entiable approximation to the step function.The above cost function penalizes disconcor-dances, i.e., cases where (i) y = 1 but ?
< 0,or (ii) when y = 0 but ?
> 0.
However, we alsoneed to make sure that we discourage ties.
We doso by adding a zero-mean Gaussian regularizationterm exp(??
?2/2) that penalizes the value of ?getting close to zero.
Note that the specific val-ues for ?
and ?
are not really important, as longas they are large.
In particular, in our experiments,we used ?
= ?
= 100.Table 5 shows a comparison of the two costfunctions: (i) the standard logistic cost, and (ii) ourKendall cost.
We can see that using the Kendallcost enables effective learning, although it is even-tually outperformed by the logistic cost.
Our in-vestigation revealed that this was due to a combi-nation of slower convergence and poor initializa-tion.
Therefore, we further experimented with asetup where we first used the logistic cost to pre-train the neural network, and then we switched tothe Kendall cost in order to perform some finertuning.
As we can see in Table 5 (last row), do-ing so yielded a sizable improvement over usingthe Kendall cost only; it also improved over usingthe logistic cost only.4Other variations for ranking tasks are possible, e.g., (Yihet al, 2011).7 Conclusions and Future WorkWe have presented a novel framework for learn-ing a tunable MT evaluation metric in a pairwiseranking setting, given pre-existing pairwise humanpreference judgments.In particular, we used a neural network, wherethe input layer encodes lexical, syntactic and se-mantic information from the reference and the twotranslation hypotheses, which is efficiently com-pacted into relatively small embeddings.
The net-work has a hidden layer, motivated by our intuitionabout the problem, which captures the interactionsbetween the relevant input components.
Unlikepreviously proposed kernel-based approaches, ourframework allows us to do both training and in-ference efficiently.
Moreover, we have shown thatit can be trained to optimize a task-specific costfunction, which is more appropriate for the pair-wise MT evaluation setting.The evaluation results have shown that our NNmodel yields state-of-the-art results when usinglexical, syntactic and semantic features (the lattertwo based on compact embeddings).
Moreover,we have shown that the contribution of the differ-ent information sources is additive, thus demon-strating that the framework can effectively inte-grate complementary information.
Furthermore,the framework is flexible enough to exploit dif-ferent granularities of features such as n-grammatches and other components of BLEU (whichindividually work better than using the aggregatedBLEU score).
Finally, we have presented evidencesuggesting that using the pairwise hidden layers isadvantageous over simpler flat models.In future work, we would like to experimentwith an extension that allows for multiple refer-ences.
We further plan to incorporate featuresfrom the source sentence.
We believe that ourframework can support learning similarities be-tween the two translations and the source, for animproved MT evaluation.
Variations of this ar-chitecture might be useful for related tasks suchas Quality Estimation and hypothesis re-rankingfor Machine Translation, where no references areavailable.Other aspects worth studying as a complementto the present work include (i) the impact of thequality of the syntactic analysis (translations areoften just a ?word salad?
), (ii) differences acrosslanguage pairs, and (iii) the relevance of the do-main the semantic representations are trained on.812ReferencesJoshua Albrecht and Rebecca Hwa.
2008.
Regressionfor machine translation evaluation at the sentencelevel.
Machine Translation, 22(1-2):1?27.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
Asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proceedingsof the 52nd Annual Meeting of the Associationfor Computational Linguistics, ACL ?14, pages238?247, Baltimore, Maryland, USA.Yoshua Bengio and Xavier Glorot.
2010.
Understand-ing the difficulty of training deep feedforward neuralnetworks.
In Proceedings of AI & Statistics 2010,volume 9, pages 249?256, Chia Laguna Resort, Sar-dinia, Italy.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-jardins, Joseph Turian, David Warde-Farley, andYoshua Bengio.
2010.
Theano: a CPU andGPU math expression compiler.
In Proceedingsof the Python for Scientific Computing Conference,SciPy ?10, Austin, Texas.Ondrej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Johannes Leveling,Christof Monz, Pavel Pecina, Matt Post, HerveSaint-Amand, Radu Soricut, Lucia Specia, and Ale?sTamchyna.
2014.
Findings of the 2014 workshopon statistical machine translation.
In Proceedings ofthe Ninth Workshop on Statistical Machine Transla-tion, WMT ?14, pages 12?58, Baltimore, Maryland,USA.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2007.
(Meta-) evaluation of machine translation.
In Pro-ceedings of the Second Workshop on StatisticalMachine Translation, WMT ?07, pages 136?158,Prague, Czech Republic.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011workshop on statistical machine translation.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, WMT ?11, pages 22?64, Edin-burgh, Scotland.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the Sev-enth Workshop on Statistical Machine Translation,WMT ?12, pages 10?51, Montr?eal, Canada.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Elisabet Comelles, Jes?us Gim?enez, Llu?
?s M`arquez,Irene Castell?on, and Victoria Arranz.
2010.Document-level automatic MT evaluation based ondiscourse representations.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, WMT ?10, pages 333?338, Uppsala, Sweden.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, WMT ?11, pages 85?91, Edin-burgh, Scotland.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In Proceedings of the52nd Annual Meeting of the Association for Com-putational Linguistics, ACL ?14, pages 1370?1380,Baltimore, Maryland, USA.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology Research, HLT ?02, pages 138?145,San Francisco, California, USA.
Morgan KaufmannPublishers.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
Journal of MachineLearning Research, 12:2121?2159.Kevin Duh.
2008.
Ranking vs. regression in ma-chine translation evaluation.
In Proceedings of theThird Workshop on Statistical Machine Translation,WMT ?08, pages 191?194, Columbus, Ohio, USA.Jes?us Gim?enez and Llu?
?s M`arquez.
2007.
Linguis-tic features for automatic evaluation of heterogenousMT systems.
In Proceedings of the Second Work-shop on Statistical Machine Translation, WMT ?07,pages 256?264, Prague, Czech Republic.Francisco Guzm?an, Shafiq Joty, Llu?
?s M`arquez,Alessandro Moschitti, Preslav Nakov, and MassimoNicosia.
2014a.
Learning to differentiate bet-ter from worse translations.
In Proceedings of the2014 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?14, pages 214?220,Doha, Qatar.Francisco Guzm?an, Shafiq Joty, Llu?
?s M`arquez, andPreslav Nakov.
2014b.
Using discourse structureimproves machine translation evaluation.
In Pro-ceedings of 52nd Annual Meeting of the Associationfor Computational Linguistics, ACL ?14, pages 687?698, Baltimore, Maryland, USA.813Shafiq Joty, Francisco Guzm?an, Llu?
?s M`arquez, andPreslav Nakov.
2014.
DiscoTK: Using discoursestructure for machine translation evaluation.
In Pro-ceedings of the Ninth Workshop on Statistical Ma-chine Translation, WMT ?14, pages 402?408, Balti-more, Maryland, USA.Alex Kulesza and Stuart M. Shieber.
2004.
A learn-ing approach to improving sentence-level MT evalu-ation.
In Proceedings of the 10th International Con-ference on Theoretical and Methodological Issues inMachine Translation.Alon Lavie and Michael Denkowski.
2009.
The ME-TEOR metric for automatic evaluation of machinetranslation.
Machine Translation, 23(2?3):105?115.Ding Liu and Daniel Gildea.
2005.
Syntactic fea-tures for evaluation of machine translation.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 25?32, Ann Ar-bor, Michigan, USA.Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.2012.
Fully automatic semantic MT evaluation.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, WMT ?12, pages 243?252,Montr?eal, Canada.Matou?s Mach?a?cek and Ond?rej Bojar.
2013.
Results ofthe WMT13 metrics shared task.
In Proceedings ofthe Eighth Workshop on Statistical Machine Trans-lation, WMT ?13, pages 45?51, Sofia, Bulgaria.Matou?s Mach?a?cek and Ond?rej Bojar.
2014.
Results ofthe WMT14 metrics shared task.
In Proceedings ofthe Ninth Workshop on Statistical Machine Transla-tion, WMT ?14, pages 293?301, Baltimore, Mary-land, USA.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock?y, and Sanjeev Khudanpur.
2010.
Re-current neural network based language model.In 11th Annual Conference of the InternationalSpeech Communication Association, pages 1045?1048, Makuhari, Chiba, Japan.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013a.
Distributed representa-tions of words and phrases and their composition-ality.
In Advances in Neural Information Process-ing Systems 26, NIPS ?13, pages 3111?3119.
LakeTahoe, California, USA.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, NAACL-HLT ?13, pages746?751, Atlanta, Georgia, USA.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1439.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meting of the Association for Com-putational Linguistics, ACL ?02, pages 311?318,Philadelphia, Pennsylvania, USA.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for wordrepresentation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?14, pages 1532?1543, Doha, Qatar.Maja Popovi?c and Hermann Ney.
2007.
Word errorrates: Decomposition over POS classes and applica-tions for error analysis.
In Proceedings of the Sec-ond Workshop on Statistical Machine Translation,WMT ?07, pages 48?55, Prague, Czech Republic.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th Biennial Conference of theAssociation for Machine Translation in the Ameri-cas, AMTA ?06, Cambridge, Massachusetts, USA.Richard Socher, John Bauer, Christopher D. Manning,and Ng Andrew Y.
2013a.
Parsing with compo-sitional vector grammars.
In Proceedings of the51st Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers),ACL ?13, pages 455?465, Sofia, Bulgaria.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts.
2013b.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?13, pages 1631?1642, Seattle, Wash-ington, USA.Xingyi Song and Trevor Cohn.
2011.
Regression andranking based optimisation for sentence-level MTevaluation.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, WMT ?11, pages123?129, Edinburgh, Scotland.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural net-works.
In Proceedings of the Neural InformationProcessing Systems, NIPS ?14, Montreal, Canada.Billy Wong and Chunyu Kit.
2012.
Extending ma-chine translation evaluation metrics with lexical co-hesion to document level.
In Proceedings of the2012 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL ?12,pages 1060?1068, Jeju Island, Korea.Wen-tau Yih, Kristina Toutanova, John C. Platt, andChristopher Meek.
2011.
Learning discriminativeprojections for text similarity measures.
In Pro-ceedings of the Fifteenth Conference on Compu-tational Natural Language Learning, CoNLL ?11,pages 247?256, Portland, Oregon, USA.814
