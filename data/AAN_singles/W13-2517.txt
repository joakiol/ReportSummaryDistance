Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 138?143,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsFinding More Bilingual Webpages with High Credibility viaLink AnalysisChengzhi Zhang?Nanjing University of Science and TechnologyNanjing, ChinaXuchen Yao?Johns Hopkins UniversityBaltimore, MD, USAChunyu KitCity University of Hong Kong, Hong Kong SAR, ChinaAbstractThis paper presents an efficient approachto finding more bilingual webpage pairswith high credibility via link analysis, us-ing little prior knowledge or heuristics.It extends from a previous algorithm thattakes the number of bilingual URL pairsthat a key (i.e., a URL pairing pattern) canmatch as the objective function to searchfor the best set of keys yielding the greatestnumber of webpage pairs within targetedbilingual websites.
Enhanced algorithmsare proposed to match more bilingual web-pages following the credibility based onstatistical analysis of the link relationshipof the seed websites available.
With about12,800 seed websites as test set, the en-hanced algorithms improve precision overbaseline by more than 5%, from 94.06%to 99.40%, and hence find above 20%more true bilingual URL pairs, illustratingthat significantly more bilingual webpageswith high credibility can be mined with thehelp of the link analysis.1 IntroductionParallel corpora of bilingual text (bitext) are indis-pensable language resources for many data-driventasks of natural language processing, such as sta-tistical machine translation (Brown et al 1990),cross-language information retrieval (Davis andDunning, 1995; Oard, 1997), and bilingual lexi-cal acquisition (Gale and Church, 1991; Melamed,1997; Jiang et al 2009), to name but a few.
Ageneral way to develop such corpora from webtexts starts from exploring the structure of knownbilingual websites, which are usually organized?Performed while a research associate at City Universityof Hong Kong.
?Performed while a visiting student at City University ofHong Kong.by their web masters in a way to facilitate bothnavigation and maintenance (Nie, 2010).
Themost common strategy is to create a parallel struc-ture in terms of URL hierarchies, exploiting someknown naming conventions for webpages of corre-sponding languages (Huang and Tilley, 2001; Nie,2010).
Following available structures and nam-ing conventions, researchers have been exploringvarious means to mine parallel corpora from theweb and a good number of such systems havedemonstrated the feasibility and practicality in au-tomatic acquisition of parallel corpora from bilin-gual and/or multilingual web sites, e.g., STRAND(Resnik, 1998; Resnik, 1999; Resnik and Smith,2003), BITS (Ma and Liberman, 1999), PTMiner(Chen and Nie, 2000), PTI (Chen et al 2004),WPDE (Zhang et al 2006), the DOM tree align-ment model (Shi et al 2006), PagePairGetter (YEet al 2008) and Bitextor (Espla`-Gomis and For-cada, 2010).Most of these systems are run in three steps:first, bilingual websites are identified and crawled;second, pairs of parallel webpages are extracted;and finally, the extracted pairs are validated (Kitand Ng, 2007).
Among them, prior knowledgeabout parallel webpages, mostly in the form of adhoc heuristics for identifying webpage languagesor pre-defined patterns for matching or comput-ing similarity between webpages, is commonlyused for webpage pair extraction (Chen and Nie,2000; Resnik and Smith, 2003; Zhang et al 2006;Shi et al 2006; Yulia and Shuly, 2010; Toma?set al 2008).
Specifically, these systems exploitsearch engines and heuristics across webpage an-chors to locate candidate bilingual websites andthen identify webpage pairs based on pre-definedURL matching patterns.
However, ad hoc heuris-tics cannot exhaust all possible patterns.
Manywebpages do not even have any language labelin their anchors, not to mention many untrust-worthy labels.
Also, using a limited set of pre-138defined URL patterns inevitably means to give upall reachable bilingual webpages that fall outsidetheir coverage.Addressing such weaknesses of the previous ap-proaches, we instead present an efficient bilingualweb mining system based on analyzing link rela-tionship of websites without resorting to prior adhoc knowledge.
This approach extends, on top ofre-engineering, the previous work of Kit and Ng(2007).
It aims at (1) further advancing the ideaof finding bilingual webpages via automatic dis-covery of non-ad-hoc bilingual URL pairing pat-terns, (2) applying the found pairing patterns todig out more bilingual webpage pairs, especiallythose involving a deep webpage unaccessible byweb crawling, (3) discovering more bilingual web-sites (and then more bilingual webpages) withhigh credibility via statistical analysis of bilingualURL patterns and link relationship of availableseed websites.
The results from our experimentson 12, 800 seed websites show that the proposedalgorithms can find considerably more bilingualwebpage pairs on top of the baseline, achievinga significant improvement of pairing precision bymore than 5%.2 AlgorithmThis section first introduces the idea of unsuper-vised detection of bilingual URL pairing patterns(?2.1) and then continues to formulate the use ofthe detected patterns to explore more websites, in-cluding deep webpages (?2.2), and those not in-cluded in our initial website list (?2.3).2.1 Bilingual URL Pattern DetectionOur current research is conducted on top of there-implementation of the intelligent web agent toautomatically identify bilingual URL pairing pat-terns as described in Kit and Ng (2007).
The un-derlying assumption for this approach is that ratherthan random matching, parallel webpages havestatic pairing patterns assigned by web masters forengineering purpose and these patterns are put inuse to match as many pairs of URLs as possiblewithin the same domain.
Given a URL u from theset U of URLs of the same domain, the web agentgoes through the set U?
{u} of all other URLs andfinds among them all those that differ from u by asingle token1 ?
a token is naturally separated by1If language identification has been done on webpages, itonly needs to go through all URLs of the other language.a special set of characters including slash /, dot .,hyphen -, and underscore in a URL.
Then, thesingle-token difference of a candidate URL pairsis taken as a candidate of URL paring pattern,and all candidate patterns are put in competitionagainst each other in a way to allow a stronger one(that matches more candidate URL pairs) to winover a weaker one (that matches fewer).
For in-stance, the candidate pattern <en,zh> can be de-tected from the following candidate URL pair:www.legco.gov.hk/yr99-00/en/fc/esc/e0.htmwww.legco.gov.hk/yr99-00/zh/fc/esc/e0.htmThe re-implementation has achieved a num-ber of improvements on the original algorithmthrough re-engineering, including the followingmajor ones.1.
It is enhanced from token-based to character-based URL matching.
Thus, more gen-eral patterns, such as <e,c>, can be aggre-gated from a number of weaker ones like<1e,1c>, <2e,2c>, ..., etc., many of whichmay otherwise fail to survive the competition.2.
The original algorithm is speeded up fromO(|U |2) to O(|U |) time, by building in-verted indices for URLs and establishingconstant lookup time for shortest matchingURL strings.23.
The language detection component has beenexpanded from bilingual to multi-lingual andhence had the capacity to practically handlemultilingual websites such as those from EUand UN.When detected URL patterns are used to matchURLs in a web domain for identifying bilingualwebpages, noisy patterns (most of which are pre-sumably weak keys) would better be filtered out.A straightforward strategy to do this is by thresh-olding the credibility of a pattern, which can bedefined asC(p, w) = N(p, w)|w|.where N(p, w) is the number of webpagesmatched into pairs by pattern p within website w,and |w| the size ofw in number of webpages.
Notethat this is the local credibility of a key with re-spect to a certain website w. Empirically, Kit and2Achieved by utilizing SecondString http://secondstring.sf.net/139Ng (2007) set a threshold of 0.1 to rule out weaknoisy keys.Some patterns happen to generalize across do-mains.
The global credibility of such a pattern p isthus computed by summing over all websites in-volved, in a way that each webpage matched by pis counted in respect to the local credibility of p inthe respective website:C(p) =?wC(p, w)N(p, w).Interestingly, it is observed that many weak keysruled out by the threshold 0.1 are in fact good pat-terns with a nice global credibility value.
In prac-tice, it is important to ?rescue?
a local weak keywith strong global credibility.
A common practiceis to do it straightforwardly with a global credibil-ity threshold, e.g., C(p)> 500 as for the currentwork.Finally, the bilingual credibility of a website isdefined asC(w) = maxpC(p, w).It will be used to measure the bilingual degree of awebsite in a later phase of our work, for which anassumption is that bilingual websites tend to linkwith other bilingual websites.2.2 Deep Webpage RecoverySome websites contain webpages that cannot becrawled by search engines.
These webpages donot ?exist?
until they are created dynamically asthe result of a specific search, mostly triggered byJavaScript or Flash actions.
This kind of webpagesas a whole is called deep web.
Specifically, weare interested in the case where webpages in onelanguage are visible but their counterparts in theother language are hidden.
A very chance that wemay have to unearth these deep hidden webpagesis that their URLs follow some common namingconventions for convenience of pairing with theirvisible counterparts.Thus for each of those URLs still missing apaired URL after the URL matching using ourbilingual URL pattern collection, a candidate URLwill be automatically generated with each applica-ble pattern in the collection for a trial to access itspossibly hidden counterpart.
If found, then markthem as a candidate pair.
For example, the pattern<english,tc chi> is found applicable to thefirst URL in Table 1 and accordingly generates thesecond as a candidate link to its English counter-part, which turns out to be a valid page.2.3 Incremental Bilingual WebsiteExplorationStarting with a seed bilingual website list of sizeN , bilingual URL pairing patterns are first mined,and then used to reach out for other bilingual web-sites.
The assumption for this phase of work isthat bilingual websites are more likely to be ref-erenced by other bilingual websites.
Accordingly,a weighted version of PageRank is formulated forprediction.Firstly, outgoing links and PageRank are usedas baselines.
Linkout(w) is the total number ofoutgoing links from website w, and the PageRankof w is defined as (Brin and Page, 1998):PageRank(w) = rN+(1?r)?w?M(w)PageRank(w)Linkout(w),whereM(w) is the set of websites that link tow inthe seed set of N bilingual websites, and r?
[0, 1]a damping factor empirically set to 0.15.
Initially,the PageRank value of w is 1.
In order to re-duce time and space cost, both Linkout(w) andPageRank(w) are computed only in terms of therelationship of bilingual websites in the seed set.The WeightedPageRank(w) is defined as thePageRank(w) weighted by w?s credibility C(w).To reach out for a related website s outside theinitial seed set of websites, our approach firstfinds the set R(s) of seed websites that haveoutgoing links to s, and then computes the sumof these three values over each outgoing link,namely,?wLinkout(w),?wPageRank(w), and?wWeightedPageRank(w) for each w?R(s), forthe purpose of measuring how ?likely?
s is bilin-gual.
An illustration of link relationship of thiskind is presented in Figure 1.In practice, the exploration of related websitescan be combined with bilingual URL pattern de-tection to literately harvest both bilingual websitesand URL patterns, e.g., through the following pro-cedure:1.
Starting from a seed set of websites as thecurrent set, detect bilingual URL patterns andthen use them to identify their bilingual web-pages.2.
Select the top K linked websites fromthe seed set according to either?Linkout,?PageRank, or?WeightedPageRank.140(1) http://www.fehd.gov.hk/tc chi/LLB web/cagenda 20070904.htm(2) http://www.fehd.gov.hk/english/LLB web/cagenda 20070904.htmTable 1: Illustration of URL generation for a deep webpageRelated websitess 1Seed websites[1,0.12, 0.08][1, 0.21,0.13][2,0.56, 0.29][1,0.02, 0.01][1,0.03, 0.02][0,0,0][1, 0.03,0.01][1,0.12, 0.08][1, 0.21,0.13][3, 0.77,0.42][3, 0.59,0.13][0,0, 0]s 2 s 3 s 4 s 5w 2w 6w 3w 4w 5w 7w 1Figure 1: Illustration of link relationship of seed websites and related websites, with associated?Linkout,?PageRank and?WeightedPageRank in square brackets and with arrows to indicate outgo-ing links from a seed website to others.3.
Add the top K selected websites to the cur-rent set, and repeat the above steps for desirediterations.3 EvaluationThe implementation of our method results in Pup-Sniffer,3 a Java-based tool that has been releasedfor free.
A series of experiments were conductedwith it to investigate the performance of the pro-posed method on about 12, 800 seed websites.
Aweb interface was also implemented for evaluat-ing the candidate bilingual webpage pairs identi-fied by our system.3.1 Seed WebsitesThe initial seed websites were collected from tworesources, namely?
Hong Kong Website Directory4 and?
Hong Kong World Wide Web Database.5After the removal of invalid ones, 12, 800websiteswere finally acquired as our seed set.63http://code.google.com/p/pupsniffer4http://www.852.com5http://www.cuhk.edu.hk/hkwww.htm6http://mega.ctl.cityu.edu.hk/?czhang22/pupsniffer-eval/Data/All_Seed_Websites_List.txt3.2 URL Pattern Detection and DeepWebpage RecoveryThe enhanced algorithm described in Section 2.1above was ran to extract credible URL patterns.
Ingeneral, the extracted patterns are valid as long asthe threshold is not too low ?
it is set to C(p, w)>0.1 in our experiments.
A number of strongest pat-terns found are presented in Table 2 for demon-stration.
Most of them, especially <en,tc> and<eng,chi>, are very intuitive patterns.
A fulllist of URL pairing patterns detected in our exper-iments is also available.7 Particularly interesting isthat all these patterns were identified in an unsu-pervised fashion without any manual heuristics.Using these patterns, the original algorithm re-trieved about 290K candidate bilingual webpagepairs.
By the simple trick of rescuing weak lo-cal patterns with the global credibility thresholdC(p) > 500, 10K more webpage pairs were fur-ther found.
Additionally, other 16K webpagepairs were dug out from deep webpages by auto-matically generating paired webpages with the aidof identified URL patterns.7http://mega.ctl.cityu.edu.hk/?czhang22/pupsniffer-eval/Data/Pattern_Credibility_LargeThan100.txt141Pattern C(p)<en,tc> 13997.36<eng,tc> 12869.56<english,tc chi> 11436.12<english,chinese> 11032.46<eng,chi> 7824.86Table 2: Top 5 patterns with their global credibilityvalues.Method Pairs PrecisionKit and Ng (2007) 290,247 94.06%Weak key rescue 10,015 89.27%Deep page recovery 15,825 95.02%Incremental exploration 37,491 99.40%Total 348, 058 94.72%True pair increment 55, 674 20.76%Table 3: Number of bilingual webpage pairs foundand their precision from sampled evaluation.3.3 Website ExplorationTo go beyond the original 12, 800 websites, the in-cremental algorithm described in Section 2.3 wasrun for one iteration to find outside bilingual web-sites directly linked from the seeds.
The top 500of them, ranked by?Linkout,?PageRank and?WeightedPageRank, respectively, were manu-ally checked by five students, giving the curvesof the total number of true bilingual websites andoverall precision per top N websites as plottedin Figure 2.
These results show that almost 50%of the top 500 related outside websites ranked by?WeightedPageRank are true bilingual websites.A higher precision indicates more bilingual web-page pairs correctly matched by the URL patternsin use.After one iteration of the incremental algorithm,37K more candidate bilingual webpage pairs werefound in the related outside websites, besides the290K by the original algorithm.
Table 3 presentsthe number of webpage pairs identified by eachalgorithm with a respective precision drawn fromrandom sampling.
These results suggest that ourproposed enhancement is able to harvest above20% more bilingual webpage pairs without de-grading the overall precision.
Error analysis showsthat around 80% of errors were due to mistakesin language identification for webpages.
For in-stance, some Japanese webpages were mistakenlyrecognized as Chinese ones.????
?05010015020025030050 100 150 200 250 300 350 400 450 500# truebilingual websitesN?Linkout ?PageRank ?WeightedPagerRank0.400.450.500.550.600.650.7050 100 150 200 250 300 350 400 450 500PrecisionN?Linkout ?PageRank ?WeightedPagerRankFigure 2: Number and precision of true bilingualwebsites found per top N outside websites rankedby various criteria.4 ConclusionIn this paper we have presented an efficient ap-proach to mining bilingual webpages via com-puting highly credible bilingual URL pairing pat-terns.
With the aid of these patterns learned inan unsupervised way, our research moves on toexploring the possibility of rescuing weak localkeys by virtue of global credibility, uncoveringdeep bilingual webpages by generating candidateURLs using available keys, and also developingan incremental algorithm for mining more bilin-gual websites that are linked from the known bilin-gual websites in our seed set.
Experimental resultsshow that these several enhanced algorithms im-prove the precision over the baseline from 94.06%to 99.40% and, more importantly, help discoverabove 20% more webpage pairs while maintain-ing a high overall precision.AcknowledgementsThe research described in this paper was supportedin part by the Research Grants Council (RGC)of Hong Kong SAR, China, through the GRF142grant 9041597 (CityU 144410), National NaturalScience Foundation of China through the grantNo.
70903032, and Project of the Education Min-istry of China?s Humanities and Social Sciencesthrough the grant No.
13YJA870020.ReferencesSergey Brin and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual web search engine.Computer networks and ISDN systems, 30(1):107?117.Peter F Brown, John Cocke, Stephen A Della Pietra,Vincent J Della Pietra, Fredrick Jelinek, John D Laf-ferty, Robert L Mercer, and Paul S Roossin.
1990.A statistical approach to machine translation.
Com-putational linguistics, 16(2):79?85.Jiang Chen and Jian-Yun Nie.
2000.
Parallel webtext mining for cross-language ir.
In Proc.
of RIAO,pages 62?77.Jisong Chen, Rowena Chau, and Chung-Hsing Yeh.2004.
Discovering parallel text from the world wideweb.
In Proceedings of the second workshop onAustralasian information security, Data Mining andWeb Intelligence, and Software Internationalisation-Volume 32, pages 157?161.Mark W Davis and Ted E Dunning.
1995.
A trec eval-uation of query translation methods for multi-lingualtext retrieval.
In Fourth Text Retrieval Conference,pages 483?498.Miquel Espla`-Gomis and Mikel L Forcada.
2010.Combining content-based and URL-based heuris-tics to harvest aligned bitexts from multilingual siteswith bitextor.
The Prague Bulletin of MathematicalLinguistics, 93(1):77?86.William A Gale and Kenneth W Church.
1991.
Iden-tifying word correspondences in parallel texts.
InProceedings of the workshop on Speech and NaturalLanguage, pages 152?157.Shihong Huang and Scott Tilley.
2001.
Issues of con-tent and structure for a multilingual web site.
In Pro-ceedings of the 19th annual international conferenceon Computer documentation, pages 103?110.Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu,and Qingsheng Zhu.
2009.
Mining bilingual datafrom the web with adaptively learnt patterns.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP: Volume 2, pages 870?878.Chunyu Kit and Jessica Yee Ha Ng.
2007.
An in-telligent web agent to mine bilingual parallel pagesvia automatic discovery of URL pairing patterns.In Proceedings of the 2007 IEEE/WIC/ACM Inter-national Conferences on Web Intelligence and In-telligent Agent Technology - Workshops: Workshopon Agents and Data Mining Interaction (ADMI-07),pages 526?529.Xiaoyi Ma and Mark Liberman.
1999.
BITS: Amethod for bilingual text search over the web.
InMachine Translation Summit VII, pages 538?542.I.
Dan Melamed.
1997.
A word-to-word model oftranslational equivalence.
In Proceedings of theEighth Conference on European Chapter of the As-sociation for Computational Linguistics, pages 490?497.Jian-Yun Nie.
2010.
Cross-Language Information Re-trieval.
Morgan and Claypool Publishers.Douglas W Oard.
1997.
Cross-language text re-trieval research in the USA.
In Proceedings of theThird DELOS Workshop: Cross-Language Informa-tion Retrieval, pages 7?16.Philip Resnik and Noah A Smith.
2003.
The webas a parallel corpus.
Computational Linguistics,29(3):349?380.Philip Resnik.
1998.
Parallel strands: A preliminaryinvestigation into mining the web for bilingual text.In D. Farwell, L. Gerber, and E. Hovy, editors, Ma-chine Translation and the Information Soup: ThirdConference of the Association for Machine Transla-tion in the Americas (AMTA-98), pages 72?82.Philip Resnik.
1999.
Mining the web for bilingual text.In Proceedings of the 37th annual meeting of the As-sociation for Computational Linguistics on Compu-tational Linguistics, pages 527?534.Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.2006.
A DOM tree alignment model for min-ing parallel data from the web.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, pages 489?496.Jesu?s Toma?s, Jordi Bataller, Francisco Casacuberta,and Jaime Lloret.
2008.
Mining Wikipedia as a par-allel and comparable corpus.
In Language Forum,volume 34.Sha-ni YE, Ya-juan LV, Yun Huang, and Qun Liu.2008.
Automatic parallel sentences extraction fromweb.
Journal of Chinese Information Processing,22:67?73.T Yulia and W Shuly.
2010.
Automatic acquisi-tion of parallel corpora from website with dynamiccontent.
In Proceedings of the 7th InternationalConference on Language Resources and Evaluation(LREC-2010), pages 3389?3392.Ying Zhang, Ke Wu, Jianfeng Gao, and Phil Vines.2006.
Automatic acquisition of chinese?english par-allel corpus from the web.
In Advances in Informa-tion Retrieval, pages 420?431.
Springer.143
