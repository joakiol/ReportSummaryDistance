Towards Automatic Error Analysis ofMachine Translation OutputMaja Popovic?
?RWTH Aachen UniversityHermann Ney?
?RWTH Aachen UniversityEvaluation and error analysis of machine translation output are important but difficulttasks.
In this article, we propose a framework for automatic error analysis and classificationbased on the identification of actual erroneous words using the algorithms for computationof Word Error Rate (WER) and Position-independent word Error Rate (PER), which is justa very first step towards development of automatic evaluation measures that provide morespecific information of certain translation problems.
The proposed approach enables the use ofvarious types of linguistic knowledge in order to classify translation errors in many differentways.
This work focuses on one possible set-up, namely, on five error categories: inflectionalerrors, errors due to wrong word order, missing words, extra words, and incorrect lexicalchoices.
For each of the categories, we analyze the contribution of various POS classes.
Wecompared the results of automatic error analysis with the results of human error analysis inorder to investigate two possible applications: estimating the contribution of each error typein a given translation output in order to identify the main sources of errors for a giventranslation system, and comparing different translation outputs using the introduced errorcategories in order to obtain more information about advantages and disadvantages of differentsystems and possibilites for improvements, as well as about advantages and disadvantages ofapplied methods for improvements.
We used Arabic?English Newswire and Broadcast Newsand Chinese?English Newswire outputs created in the framework of the GALE project, severalSpanish and English European Parliament outputs generated during the TC-Star project, andthree German?English outputs generated in the framework of the fourth Machine TranslationWorkshop.
We show that our results correlate very well with the results of a human error analy-sis, and that all our metrics except the extra words reflect well the differences between differentversions of the same translation system as well as the differences between different translationsystems.?
Now at DFKI ?
German Research Centre for Artificial Intelligence, Alt-Moabit 91c, 10559 Berlin,Germany.
E-mail: maja.popovic@dfki.de.??
Lehrstuhl fu?r Informatik 6 ?
Computer Science Department, Ahornstrasse 55, 52056 Aachen, Germany.E-mail: ney@informatik.rwth-aachen.de.Submission received: 8 August 2008; revised submission received: 6 December 2010; accepted for publication:6 March 2011.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 41.
IntroductionThe evaluation of machine translation output is an important and at the same timedifficult task for the progress of the field.
Because there is no unique reference trans-lation for a text (as for example in speech recognition), automatic measures are hardto define.
Human evaluation, although of course providing (at least in principle) themost reliable judgments, is costly and time consuming.
A great deal of effort has beenspent on finding measures that correlate well with human judgments when determiningwhich one of a set of translation systems is the best (be it different versions of the samesystem in the development phase or a set of ?competing?
systems, as for example in amachine translation evaluation).However, most of the work has been focused just on best?worst decisions, namely,finding a ranking between different machine translation systems.
Although this is use-ful information and helps in the continuous improvement of machine translation (MT)systems, MT researches often would find it helpful to have additional information abouttheir systems.
What are the strengths of their systems?
Where do they make errors?
Doesa particular modification improve some aspect of the system, although perhaps it doesnot improve the overall score in terms of one of the standard measures?
Does a worse-ranked system outperform a best-ranked one in any aspect?
Hardly any systematicwork has been done in this direction and developers must resort to looking at thetranslation outputs in order to obtain an insight of the actual problems of their systems.A framework for human error analysis and error classification has been proposed byVilar et al (2006), but as every human evaluation, this is also a difficult and time-consuming task.This article presents a framework for automatic analysis and classification of errorsin a machine translation output which is just a very first step in this direction.
Thebasic idea is to extend the standard error rates using linguistic knowledge.
The firststep is the identification of the actual erroneous words using the algorithms for thecalculation of Word Error Rate (WER) and Position-independent word Error Rate (PER).The extracted erroneous words can then be used in combination with different types oflinguistic knowledge, such as base forms, Part-of-Speech (POS) tags, Name Entity (NE)tags, compound words, suffixes, prefixes, and so on, in order to obtain various detailsabout the nature of actual errors, for example, error categories (e.g., morphologicalerrors, reordering errors, missing words), contribution of different word classes (e.g.,POS, NE), and so forth.The focus of this work is the definition of the following error categories: inflectional errors reordering errors missing words extra words incorrect lexical choicesand the comparison of the results of automatic error analysis with those obtained byhuman error analysis for these categories.
Each error category can be further classi-fied according to POS tags (e.g., inflectional errors of verbs, missing pronouns).
Thetranslation outputs used for the comparison of human and automatic error analysis658Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation Outputwere produced in the frameworks of the GALE1 project, the TC-STAR2 project, and thefourth Workshop on Statistical Machine Translation3 (WMT09).
The comparison withhuman error analysis is done considering two possible applications: estimating thecontribution of each error category in a particular translation output, and comparingdifferent translation outputs using these categories.
In addition, we show how the newerror measures can be used to get more information about the differences betweentranslation systems trained on different source and target languages, between differenttraining set-ups for a same phrase-based translation system, as well as between differenttranslation systems.1.1 Related WorkA number of automatic evaluation measures for machine translation output have beeninvestigated in recent years.
The BLEU metric (Papineni et al 2002) and the closelyrelated NIST metric (Doddington 2002), along with WER and PER, have been widelyused by many machine translation researchers.
The Translation Edit Rate (TER) (Snoveret al 2006) and the CDER measure (Leusch, Ueffing, and Ney 2006) are based on the editdistance (WER) but allow reordering of blocks.
TER uses an edit distance with additionalcosts for shifts of word sequences.
The CDER measure drops certain constraints for thehypothesis: Only the words in the reference have to be covered exactly once, whereasthose in the hypothesis can be covered zero, one, or multiple times.
Preprocessing andnormalization methods for improving the evaluation using the standard measures WER,PER, BLEU, and NIST are investigated by Leusch et al (2005).
The same set of measuresis examined by Matusov et al (2005) in combination with automatic sentence segmen-tation in order to enable evaluation of translation output without sentence boundaries(e.g., translation of speech recognition output).
The METEOR metric (Banerjee and Lavie2005) first counts the number of exact word matches between the output and thereference.
In a second step, unmatched words are converted into stems or synonymsand then matched.
A method that uses the concept of maximum matching string (MMS)is presented by Turian, Shen, and Melamed (2003).
IQ (Gime?nez and Amigo?
2006) isa framework for automatic evaluation in which evaluation metrics can be combined.Nevertheless, none of these measures or extensions takes into account any details aboutactual translation errors, for example, what the contribution of verbs is in the overallerror rate, how many full forms are wrong although their base forms are correct, or howmany words are missing.
A framework for human error analysis and error classificationhas been proposed by Vilar et al (2006), where a classification scheme (Llitjo?s, Carbonell,and Lavie 2005) is presented together with a detailed analysis of the obtained results.Automatic error analysis is still a rather unexplored area.
A method for automaticidentification of patterns in translation output using POS sequences is proposed byLopez and Resnik (2005) in order to see how well a translation system is capableof capturing systematic reordering patterns.
Using relative differences between WERand PER for three POS classes (nouns, adjectives, and verbs) is proposed by Popovic?et al (2006) for the estimation of inflectional and reordering errors.
Semi-automaticerror analysis (Kirchhoff et al 2007) is carried out in order to identify problematic1 GALE ?
Global Autonomous Language Exploitation.
http://www.arpa.mil/ipto/programs/gale/index.htm.2 TC-STAR ?
Technology and Corpora for Speech to Speech Translation.
http://www.tc-star.org/.3 EACL 09 Fourth Workshop on Statistical Machine Translation.
http://www.statmt.org/wmt09/.659Computational Linguistics Volume 37, Number 4characteristics of source documents such as genre, domain, language, and so on.
Zhouet al (2008) propose a diagnostic evaluation of linguistic check-points obtained auto-matically by aligning parsed source and target sentences.
For each check-point, thenumber of matched n-grams of the references is then calculated.
Linguistically basedreordering along with the syntax-based evaluation of reordering patterns is describedin Xiong et al (2010).In this work, we propose a novel framework for automatic error analysis of ma-chine translation output based on WER and PER, and systematically investigate a set ofpossible methods to carry out an error analysis at the word level.2.
A Framework for Automatic Error AnalysisThe basic idea for automatic error analysis described in this work is to take into ac-count details from the WER (edit distance) and PER algorithms, namely, to identify allerroneous words which are actually contributing to the error rate, and then to combinethese words with different types of linguistic knowledge.
The general procedure forautomatic error analysis and classification is shown in Figure 1.
An overview of thestandard error rates WER and PER is given in Section 2.1, and methods for extractingactual errors are described in the following sections.In this article, we carried out the error analysis at the word level and we usedbase forms of the words and POS tags as linguistic knowledge.
However, the analy-sis described in this work is just a first step towards automatic error analysis andpresents only one of many possibilities?this framework enables the integration ofvarious knowledge sources such as deeper lingustic knowledge, the introduction ofsource words (possibly with additional linguistic information) if appropriate alignmentinformation is available, and so forth.
Investigation at the word group/phrase levelinstead of only at the word level is possible as well.
The error analysis presented in thisFigure 1General procedure for automatic error analysis based on the standard word error rates andlinguistic information.660Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation Outputwork is language-independent?nevertheless, availability of base forms and POS tagsfor the particular target language is a requisite.2.1 Standard Word Error Rates (Overview)The standard procedure for evaluating machine translation output is done by com-paring the hypothesis document hyp with the given reference document ref , each oneconsisting of K sentences (or segments).
The reference document ref consists of NR ?
1reference translations of the source text.
NR = 1 stands for the case when only a singlereference translation is available, and NR > 1 denotes the case of multiple references.Let the length of the hypothesis sentence hypk be denoted as Nhypk , and the lengthof each reference sentence Nref k,r.
Then, the total hypothesis length of the document isNhyp =?k Nhypk and the total reference length is Nref =?k N?ref k, where N?ref k is definedas the length of the reference sentence with the lowest sentence-level error rate asshown to be optimal with respect to the correlation with the human evaluation score?sadequacy and fluency (Leusch et al 2005).
The overall error rate is then obtained bynormalizing the total number of errors over the total reference length.The word error rate (WER) is based on the Levenshtein distance (Levenshtein1966)?the minimum number of substitutions, deletions, and insertions that have to beperformed to convert the generated text hyp into the reference text ref .
A shortcoming ofthe WER is the fact that it does not allow reorderings of words, although the word orderof the hypothesis can be different from the word order of the reference even though itis a correct translation.
The position-independent word error rate (PER) is also based onsubstitutions, deletions, and insertions but without taking the word order into account.The PER is always lower than or equal to the WER.
On the other hand, a shortcoming ofthe PER is the fact that it does not penalize a wrong word order.Calculation of WER: The WER of the hypothesis hyp with respect to the referenceref is calculated asWER = 1NrefK?k=1minr{dL(ref k,r, hypk)}(1)where dL(ref k,r, hypk) is the Levenshtein distance between the reference sentence ref k,rand the hypothesis sentence hypk.
The calculation is performed using a dynamic pro-gramming algorithm.Calculation of PER: Define n(w, setw) as the number of occurrences of a word win a multi-set of words setw.
The PER can be calculated using the counts n(e, hypk) andn(e, ref k,r) of a word e in the hypothesis sentence hypk and the reference sentence ref k,rrespectively:PER = 1NrefK?k=1minr{dPER(ref k,r, hypk)}(2)wheredPER(ref k,r, hypk) =12(|Nref k,r ?
Nhypk |+?e|n(e, ref k,r) ?
n(e, hypk)|)(3)661Computational Linguistics Volume 37, Number 42.2 Identification of WER ErrorsThe dynamic programming algorithm for WER enables a simple and straightforwardidentification of each erroneous word which actually contributes to WER.
An example ofa reference sentence and hypothesis sentence along with the corresponding Levenshteinalignment and the actual words participating in WER is shown in Table 1.
The referencewords involved in WER are denoted as reference errors, and hypothesis errors refer tothe hypothesis words participating in WER.Table 2 presents an example of introducing linguistic knowledge in the form ofbase forms and POS tags.
This allows us to compute the contribution of each POS classp to the overall WER, that is, WER(p).
If werrk is the multi-set of erroneous words insentence k according to alignment links to the best reference and p is a POS class, thenN(WER(p)) =?e?p n(e, werrk) is the number of WER errors in werrk produced by wordsbelonging to the POS class p. For the substitution and the deletion errors, POS tags ofTable 1Example for illustration of actual errors: (a) a reference sentence and a corresponding hypothesissentence; (b) a corresponding Levenshtein alignment; (c) actual words which participate in theword error rate.
(a) Reference and hypothesis examplereference: hypothesis:Mister Commissioner, twenty-four Mrs Commissioner, sometimeshours sometimes can be too much time.
twenty-four hours is too much time.
(b) Levenshtein alignmentref: hyp:Mister MrsCommissioner Commissioner, ,sometimestwenty-four twenty-fourhours hourssometimes iscanbetoo toomuch muchtime time.
.
(c) WER errorsreference errors hypothesis errors error typeMister Mrs substitutionsometimes insertionsometimes is substitutioncan deletionbe deletion662Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation OutputTable 2WER errors and linguistic knowledge: Actual words which participate in the word error ratewith their corresponding base forms and POS classes.reference errors hypothesis errors error typeMister#Mister#N Mrs#Mrs#N substitutionsometimes#sometimes#ADV insertionsometimes#sometimes#ADV is#be#V substitutioncan#can#V deletionbe#be#V deletionthe reference words are used, and for the insertion errors, POS classes of the hypothesiswords are used.
The WER for the word class p can be calculated as the standard WER bynormalizing the number of errors over the total reference length:WER(p) = 1NrefK?k=1?e?pn(e, werrk) (4)Standard WER of the whole sentence is equal to 5/12 = 41.7%.
The contribution ofnouns is WER(N) = 1/12 = 8.3%, of verbs is WER(V) = 2/12 = 16.7%, and of adverbsis WER(ADV) = 2/12 = 16.7%.2.3 Identification of PER ErrorsIn contrast to WER, the standard efficient algorithms for the calculation of PER do notgive precise information about contributing words.
However, it is possible to identifyall words in the hypothesis which do not have a counterpart in the reference, and viceversa.
These words will be referred to as PER errors.An illustration of PER errors is given in Table 3.
The number of errors contributingto the standard PER according to Equation (3) is 3?there are two substitutions andone deletion.
The problem with standard PER is that it is not possible to detect whichwords are deletion errors, which are insertion errors, and which words are substitutionerrors.
Therefore we introduce alternative PER-based measures which correspond tothe precision, recall, and F?measure.
Let herrk refer to the multi-set of words in thehypothesis sentence k which do not appear in the reference sentence k (referred to ashypothesis errors).
Analogously, let rerrk denote the multi-set of words in the referenceTable 3PER errors: Actual words that participate in the position-independent word error rate.reference errors hypothesis errorsMister Mrscan isbe663Computational Linguistics Volume 37, Number 4sentence k which do not appear in the hypothesis sentence k (referred to as referenceerrors).
Then the following measures can be calculated: recall-based (reference) PER (RPER):RPER = 1NrefK?k=1?en(e, rerrk) (5) precision-based (hypothesis) PER (HPER):HPER = 1NhypK?k=1?en(e, herrk) (6) F-based PER (FPER):FPER = 1Nref + Nhyp?K?k=1?e(n(e, rerrk) + n(e, herrk))(7)For the example sentence presented in Table 1, the number of hypothesis errors?e n(e, herrk) is 2 and the number of reference errors?e n(e, rerrk) is 3.
The number of er-rors contributing to the standard PER is 3 according to Equation (3), since |Nref ?
Nhyp| =1 and?e |n(e, ref k) ?
n(e, hypk)| = 5.
The standard PER is normalized over the referencelength Nref = 12, thus being equal to 25%.
The RPER considers only the reference errors,RPER = 3/12 = 25%, and HPER only the hypothesis errors, HPER = 2/11 = 18.2%.
TheFPER is the sum of hypothesis and reference errors divided by the sum of hypothesisand reference length: FPER = (2 + 3)/(11 + 12) = 5/23 = 21.7%.The contribution of nouns in the reference translation is RPER(N) = 1/12 = 8.3%, inthe hypothesis is HPER(N) = 1/11 = 9.1%, and together FPER(N) = 2/23 = 8.7%.
Thecontribution of verbs in the reference is RPER(V) = 2/12 = 16.7%, in the hypothesis isHPER(V) = 1/11 = 9.1%, and together FPER(V) = 3/23 = 13%.It should be noted that only the links between raw words are considered both forWER as well as for RPER and HPER calculation; POS tags are added afterwards as anadditional knowledge.
Exact distribution of errors over POS classes depends on the exactimplementation of the WER and the RPER and HPER algorithms.
For example, if light#Aand light#N occur in the reference and light#V occurs in the hypothesis, there are twopossibilities: either light#A is linked to light#V and light#N is a missing word, or light#Nis linked to light#V so that light#A is a missing word.3.
Methods for Automatic Error Analysis and ClassificationThe error details described in Section 2.2 and Section 2.3 can be combined with differenttypes of linguistic knowledge in different ways.
Examples with the base forms and POStags as linguistic knowledge are presented in Tables 2 and 4.
The described error rates664Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation OutputTable 4PER errors and linguistic knowledge: Actual words that participate in the position-independentword error rate and their corresponding base forms and POS classes.reference errors hypothesis errorsMister#Mister#N Mrs#Mrs#Ncan#can#V is#be#Vbe#be#Vof particular POS classes give more details than the overall standard error rates and canbe used for error analysis to some extent.
However, for more precise information aboutcertain phenomena some kind of further analysis is required.
In this work, we examinethe following error categories: inflectional errors ?
using RPER or HPER errors and base forms; reordering errors ?
using WER and RPER or HPER errors; missing words ?
using WER and RPER errors with base forms; extra words ?
using WER and HPER errors with base forms; incorrect lexical choice ?
errors which belong neither to inflectional errorsnor to missing or extra words.Furthermore, the contribution of various POS classes for the described error categoriesis estimated.It should be noted that the base forms and POS tags are needed both for thereference(s) and for the hypothesis.
The performance of morpho-syntactic analysis isslightly lower on the hypothesis, but this does not seem to influence the performanceof the error analysis tools.
We choose to use reference words for all cases where it canbe chosen between the reference and the hypothesis, however.
Nevertheless, it wouldbe interesting to investigate the use of hypothesis words in future experiments andcompare the results.3.1 Inflectional ErrorsAn inflectional error occurs if the base form of the generated word is correct but the fullform is wrong.
Inflectional errors can be estimated using RPER errors and base formsin the following way: From each reference?hypothesis sentence pair, only erroneouswords which have common base forms are taken into account:N(infl) =K?k=1?en(e, rerrk) ?K?k=1?ebn(eb, rberrk) (8)where eb denotes the base form of the word e and rberrk stands for the multi-set ofbase form errors in the reference.
The number of words with erroneous base forms(representing a multi-set of non?inflectional errors) is subtracted from the number oftotal errors.
For example, from the PER errors presented in Table 3, the word is will be665Computational Linguistics Volume 37, Number 4detected as an inflectional error because it shares the same base form with the referenceerror be.An analogous definition is possible using HPER errors and base forms; as explainedat the beginning of this section, however, we choose to use the reference words becausethe results of the morpho-syntactic analysis are slightly more reliable for the referencesthan for the hypotheses.3.2 Reordering ErrorsDifferences of word order in the hypothesis with respect to the reference are takeninto account only by WER and not by PER.
Therefore, a word which occurs both inthe reference and in the hypothesis but is marked as a WER error is considered as areordering error.
The contribution of reference reordering errors can be estimated in thefollowing way:N(reord) =K?k=1?e(n(e, suberrk) + n(e, delerrk) ?
n(e, rerrk))(9)where suberrk represents the multi-set of WER substitution errors, delerrk the multi-setof WER deletion errors, and rerrk the multi-set of RPER errors.
A definition using HPERerrors with substitutions and insertions is also possible; this work, however, is focusedon the reference errors.
For the example in Table 1, the word sometimes is identified as areordering error.3.3 Missing WordsMissing words can be identified using the WER and PER errors in the following way:The words considered as missing are those which occur as deletions in WER errors andat the same time occur only as reference PER errors without sharing the base form withany hypothesis error, that is, as a non?inflectional RPER error:N(miss) =K?k=1?eb?rberrkn(e, delerrk) (10)The multi-set of deletion WER errors is defined as delerrk, and rberrk stands for themulti-set of base form RPER errors.
The use of both WER and RPER errors is muchmore reliable than using only the WER deletion errors because not all deletion er-rors are produced by missing words?a number of WER deletions appear due to re-ordering errors.
The information about the base form is used in order to eliminateinflectional errors.
For the example in Table 1, the word can will be identified asmissing.3.4 Extra WordsAnalogously to missing words, extra words are also detected from the WER and PERerrors: The words considered as extra are those that occur as insertions in WER errors666Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation Outputand at the same time occur only as hypothesis PER errors without sharing the base formwith any reference error:N(extra) =K?k=1?eb?hberrkn(e, inserrk) (11)where inserrk is the multi-set of insertion WER errors and hberrk is the multi-set of baseform HPER errors.
In the example in Table 1 none of the words will be classified as anextra word.3.5 Incorrect Lexical ChoiceThe erroneous words in the reference translation that are classified neither as inflectionalerrors nor as missing words are considered as incorrect lexical choice:N(lex) =K?k=1?ebn(eb, rberrk) ?
N(miss) (12)As in the case of the inflectional and reordering errors, a definition using hypothesiserrors and extra words is also possible, but in this work we choose to use referenceerrors.
In the example in Table 1, the word Mister in the reference (or the word Mrs inthe hypothesis) is considered as an incorrect lexical choice.4.
Comparison with Human Error AnalysisIn order to compare the results of the proposed automatic error analysis with humanerror analysis, the methods described in the previous sections are applied on severaltranslation outputs with the available results of human error analysis.
These translationoutputs were produced in the framework of the GALE project, the TC-STAR project, andthe shared task of the fourth Statistical Machine Translation Workshop (WMT09).The two main goals of the comparison with the human evaluation are: to examine the distribution of errors over the categories, that is, how wellthe automatic methods are capable of capturing differences between errorcategories and determining which of those are particularly problematic fora given translation system; to examine the differences between the numbers of errors in each categoryfor different translation outputs, that is, how well the automatic methodsare capable of capturing differences between systems.4.1 Human Error AnalysisHuman error analysis and classification is a time-consuming and difficult task, and itcan be done in various ways.
For example, in order to find errors in a translation outputit can be useful to have one or more reference translations.
There are often severalcorrect translations of a given source sentence, however, and some of them might notcorrespond to the reference translations, which poses difficulties for evaluation and667Computational Linguistics Volume 37, Number 4error analysis.
The errors can be counted by doing a direct strict comparison betweenthe references and the translation outputs, which is then very similar to automaticerror analysis.
But much more flexibility can be allowed: substitution of words andexpressions by synonyms, syntactically correct different word order, and so on, whichis a more natural way.
It is also possible to use the references only for the semanticaspect, namely, to look only whether the main meaning is preserved.
It is even possiblenot to use a reference translation at all, but compare the translation output with thesource text.
There are also other aspects that may differ between human evaluations,for example, counting each problematic word as an error or counting groups of wordsas one error, and so forth.
Furthermore, the human error classification is definitely notunambigous?often it is not easy to determine in which particular error category someerror exactly belongs, sometimes one word can be assigned to more than one category,and variations between different human evaluators are possible.
For error categoriesdescribed in previous sections, especially difficult is disambiguating between incorrectlexical choice and missing words or extra words.
For example, if the translation outputis the day before yesterday and translation reference is yesterday, it could be considered asa group of incorrectly translated words, but also as a group of extra words.
Similarly,there are several possible interpretations of errors if the one who will come is translated aswhich comes.In this work, three types of human error analysis are used: a strict one, comparing the output with a given reference (similar to theautomatic error analysis) (Table 5 ); a flexible one, where syntactically correct differences in word order,substitutions by synonyms, and correct alternative expressions are notconsidered as errors; less strict than the previous method (Table 5); a free one, where the reference is taken into account only from the semanticpoint of view (Vilar et al 2006); less strict than the previous two methods.The results of both human and automatic error analysis for all analyzed textsare presented in the following sections.
In addition, the Pearson (r) and Spearmanrank (?)
correlation coefficients between human and automatic results are calculated.Both coefficients assess how well a monotonic function describes the relationship be-tween two variables: The Pearson correlation assumes a linear relationship between thevariables, and the Spearman correlation takes only rank into account.
Thus Spearman?sTable 5Examples of two variants of human error analysis, a strict and a flexible one; the marked errorsare detected with respect to the reference, whereas no errors are detected when the error analysisis more flexible.reference translation obtained outputwe celebrated the fifteenth anniversary we have held the fifteenth anniversaryI think this is a good moment I believe that this is a good opportunityto achieve these ends for these purposesin 2002 in the year 2002in Europe we must also learn also in Europe we must learn668Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation Outputrank correlation coefficient is equivalent to a Pearson correlation on ranks.
A Pearsoncorrelation of +1 means that there is a perfect positive linear relationship between thevariables, and a Spearman correlation of +1 that the ranking using both variables isexactly the same.
A Pearson correlation of ?1 means that there is a perfect negativelinear relationship between variables, and a Spearman correlation of ?1 that there is anexactly inverse ranking.
A correlation of 0 means there is no linear relationship betweenthe two variables.
Thus, the higher value of r and ?, the more similar the metrics are.4.2 Distribution of Errors Over CategoriesThe goal of the experiments described in this section is to examine the distributionof errors over the categories, that is, how well the automatic methods are capable ofcapturing differences between error categories and determining which of those areparticularly problematic for a given translation system.
For each of the error categories,the distribution of errors over the basic POS classes?nouns (N), verbs (V), adjectives(A), adverbs (ADV), pronouns (PRON), determiners (DET), prepositions (PREP), conjunc-tions (CON), numerals (NUM), and punctuation marks (PUN)?is analyzed as well, thusobtaining more details about errors, namely, which POS classes are the main sourceof errors for a particular error category.
For the GALE corpora, the strict human erroranalysis is carried out, and for the TC-STAR corpora, the free one.4.2.1 Results on GALE Corpora.
The raw error counts for each category obtained on theGALE corpora both by human and automatic error classification are shown in Table 6.
Itcan be seen that both the results of the human analysis as well as the automatic analysisshow the same tendencies: For the Arabic-to-English Broadcast News translation, themain sources of errors are extra words and incorrect lexical choice, for the Newswirecorpus the predominant problem is incorrect lexical choice, and for the Chinese-to-English the majority of errors are caused by missing words, followed by incorrect lexicalchoices and wrong word order.Three examples of human and automatic error analysis are presented in Table 7.In the first sentence, the words Japanese and friendly are classified into the same cat-egory both by human and by automatic analysis, namely, as a reordering error anda missing word, respectively.
The words feeling for represent an example where thehuman analysis assigns the error to the category of missing words, but the automaticanalysis classifies it as a lexical error.
Similarly, the words can feel are considered as extrawords by humans, but as lexical errors by automatic tools.
These examples illustrate theprevious statements about difficulties in disambiguation between missing words andextra words vs. lexical errors.
In the second sentence, the inflectional error based/baseis detected both by humans and by automatic tools.
However, contribution is classifiedTable 6Results (raw error counts) of human (left) and automatic (right) error analysis for theGALE corpora.output infl order miss ext lexArEn BN 20 / 23 39 / 66 79 / 63 127 / 137 135 / 147ArEn NW 22 / 24 30 / 41 97 / 102 73 / 76 140 / 131CnEn NW 38 / 40 127 / 171 288 / 244 95 / 117 203 / 239669Computational Linguistics Volume 37, Number 4Table 7Examples of human and automatic error analysis from the GALE corpora.
Words in bold italicare assigned to the same error category both by human and automatic error analysis, and wordsin bold represent differences.ref: ... , although the Japanese friendly feelings for China added an increase , ...hyp: ... , although China can feel the Japanese increase , ...errors: friendly ?
missing(hum,aut)feelings for ?
missing(hum)/lexical(aut)Japanese ?
order(hum,aut)can feel ?
extra(hum)/lexical(aut)ref: ...the amount of their monthly contribution is based in accordance withthe wages of previous year...hyp: ...the amount of their monthly wages base in accordance with theprevious year...errors: contribution ?
lexical(hum)/missing(aut)wages ?
missing(hum)/order(aut)based ?
inflectional(hum,aut)ref: ... of local party committees.
Secretaries of the Commission ...hyp: ... of local party committees of the provincial Commission ...errors: Secretaries ?
missing(hum,aut)provincial ?
extra(hum,aut)as a lexical error by humans and as a missing word by automatic tools.
The humananalysis classified the word wages as missing.
The word is detected as an error alsoby the automatic tool; nevertheless it is considered as a reordering error because itis present only as an WER error and neither as an HPER nor as an RPER error.
Thethird sentence illustrates a total agreement between the human and automatic errorclassification: Both words are assigned to the same category.
Results for the ten basic POSclasses are shown in Table 8, and again from both human and automatic error analysisthe same conclusions can be drawn.Table 9 presents correlations between the results of the human and automaticanalysis.
The correlation function presented in Table 9(a) is measured between the errorcounts in each category, and Table 9(b) presents the correlation between the error countsfor each POS class within a particular error category.
It can be seen that the automaticmeasures have very high correlation coefficients with respect to the results of humanevaluation.
The correlations for the inflectional error category are higher than for theother categories, which can be explained by the fact mentioned in previous sections thatthe disambiguation between missing words, extra words, and incorrect lexical choice isoften difficult, both for humans and for machines.4.2.2 Results on TC-STAR Corpora.
The experiments on the TC-STAR corpora are similar tothose on the GALE corpora.
There are some differences, however, because human errorclassification is carried out in a somewhat different way and completely independently.The error categories considered by the human error analysis were inflectional errors,missing words, reordering errors, and incorrect lexical choice?that is, the same as inthe GALE experiments except extra words.
The distribution of errors over POS tags isnot analyzed on this corpora, but the following details about inflectional errors areinvestigated: verb tense errors, verb person errors, adjective gender errors, and adjective670Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation OutputTable 8Results (raw error counts) of human (left) and automatic (right) error analysis for the GALEcorpora: Distribution of different error types over basic POS classes.ArEn BN V N A ADV PRON DET PREP CON NUM PUNinfl 15 / 17 3 0 0 2 / 3 0 0 0 0 0order 6 / 10 14 / 15 3 / 6 1 / 2 0 5 / 10 3 / 8 3 / 7 2 / 1 1 / 6miss 29 / 15 10 / 14 4 / 2 6 / 4 11 / 9 3 / 2 8 / 5 1 1 / 0 6 / 11ext 11 19 / 23 4 / 3 9 / 8 8 / 15 33 / 36 25 / 21 7 / 4 3 / 5 8 / 11lex 22 / 32 23 / 22 7 7 / 9 17 / 16 5 24 10 / 7 6 14 / 19ArEn NW V N A ADV PRON DET PREP CON NUM PUNinfl 18 2 1 0 1 / 3 0 0 0 0 0order 5 9 / 10 4 / 3 5 0 / 1 2 / 3 3 / 9 1 / 2 1 0 / 2miss 25 / 35 14 / 12 4 / 3 5 / 4 18 / 14 9 / 10 15 / 14 5 / 3 0 2 / 7ext 8 / 11 12 / 17 2 1 / 3 7 / 4 10 / 8 12 / 10 5 0 15 / 16lex 38 / 27 24 / 22 4 / 7 8 / 9 22 / 23 8 / 7 23 / 17 7 / 10 2 4 / 7CnEn NW V N A ADV PRON DET PREP CON NUM PUN Vinfl 14 / 16 24 0 0 0 0 0 0 0 0order 12 / 13 52 / 71 16 / 12 3 / 2 4 / 1 12 / 22 16 / 22 3 / 5 4 / 5 5 / 18miss 49 / 45 75 / 73 12 / 7 14 / 13 17 / 13 24 / 15 50 / 36 23 / 13 5 19 / 22ext 6 18 / 38 5 / 9 1 / 0 2 / 1 21 / 20 23 / 24 5 / 2 0 / 4 14 / 13lex 21 / 47 87 / 72 13 7 / 11 5 / 11 13 / 10 32 / 37 9 / 15 4 / 6 12 / 19number errors.
The category of inflectional errors is also different: It is obtained as a sumof these particular inflectional categories.
Correlation coefficients are calculated both forgeneral error categories and for inflectional details.The results of this error classification are shown in Table 10, and it can be seen thathuman and automatic error analysis again produce similar trends.
It can be seen asTable 9Correlation coefficients for the GALE corpora: Spearman rank ?
(left column) and Pearson r(right column) coefficient.
(a) Error categoriesoutput ?
rArEn BN 0.900 0.955ArEn NW 1.000 0.994CnEn NW 1.000 0.930(b) Distribution of errors over POS classesdistribution of errors over POS classesinfl order miss extra lexoutput ?
r ?
r ?
r ?
r ?
rArEn BN 0.997 0.999 0.927 0.872 0.918 0.790 0.870 0.947 0.924 0.924ArEn NW 0.979 0.994 0.870 0.804 0.921 0.922 0.912 0.916 0.894 0.960CnEn NW 1.000 0.998 0.812 0.961 0.927 0.973 0.879 0.853 0.788 0.914671Computational Linguistics Volume 37, Number 4Table 10Results (raw error counts) of human (left) and automatic (right) error analysis for the TC-STARcorpora.output infl order miss lexEsEn1 FTE 18 / 77 37 / 156 47 / 138 70 / 336EsEn2 FTE 27 / 136 28 / 215 46 / 154 47 / 477EsEn1 VT 24 / 80 43 / 104 40 / 113 82 / 268EnEs1 FTE 89 / 264 45 / 194 58 / 206 89 / 451EnEs2 FTE 72 / 197 33 / 169 38 / 111 64 / 416EnEs1 VT 82 / 223 31 / 178 72 / 187 84 / 485well that the numbers of errors obtained by automatic methods is much higher than thenumbers obtained by the free human evaluation.Table 11 presents the results for the inflectional details about verbs and adjectives(i.e., tense, person, gender, and number).
Both human and automatic error analysisindicate that the most problematic inflectional category is the tense of verbs, especiallyfor the translation into Spanish.Correlation coefficients are shown in Table 12.
It can be seen that for this corpusthe correlations for the error categories, although all rather high (above 0.5), are lowerthan for the GALE corpus.
This is due to the free human evaluation which is carried outon this corpora, that is, without taking the reference translation strictly into account.However, for the inflectional error analysis the correlations are very high, above 0.9.4.3 Differences Between Translation Systems and Methods for ImprovementsThe focus of this set of experiments is to examine how well the automatic methods arecapable of capturing differences between systems and methods for improvements inorder to estimate advantages and disadvantages of different translation systems; get ideas for improvements of translation performance; estimate advantages and disadvantages of applied methods forimprovements.Table 11Results (raw error counts) of human (left) and automatic (right) error analysis for TC-STARcorpora ?
inflectional details: tense (Vten) and person (Vper) of verbs, gender (Agen), andnumber (Anum) of adjectives.output Vten Vper Agen AnumEsEn1 FTE 14 / 56 4 / 21 0 0EsEn2 FTE 22 / 98 5 / 38 0 0EsEn1 VT 16 / 59 8 / 21 0 0EnEs1 FTE 44 / 131 24 / 82 12 / 26 9 / 25EnEs2 FTE 31 / 94 18 / 52 12 / 26 11 / 25EnEs1 VT 36 / 120 23 / 75 13 / 14 10 / 14672Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation OutputTable 12Correlation coefficients for the TC-STAR corpora: Spearman rank ?
(left column) and Pearson r(right column).output error infl.
errorscategories over POS classes?
r ?
rEsEn1 FTE 0.800 0.935 1.000 0.996EsEn2 FTE 0.800 0.552 1.000 0.983EsEn1 VT 0.800 0.978 1.000 0.991EnEs1 FTE 0.950 0.754 1.000 0.987EnEs2 FTE 0.600 0.572 1.000 0.998EnEs1 VT 1.000 0.538 0.950 0.990The experiments should also show how reliable each error category is for the compar-ison of translation outputs.
For each of the error categories, the basic POS classes areanalyzed as well in order to estimate which POS classes of each category are reliable forthe comparison of translation outputs.The investigations are carried out on six Spanish-to-English TC-STAR outputs gen-erated by phrase-based systems (Vilar et al 2005) and three German-to-English WMT09outputs produced in the framework of the fourth shared translation task (Callison-Burch et al 2009).
Two of the WMT09 outputs are generated by standard phrase-basedsystems (Zens, Och, and Ney 2002) and one by a hierarchical phrase-based system(Chiang 2007).
For the TC-STAR outputs two reference translations are available for theautomatic error analysis, and for the WMT09 outputs only a single reference is available.For all texts, the flexible human error analysis is carried out.
The following sectionssummarize all the results along with the Spearman and Pearson correlation coefficientscalculated across the different translation outputs.4.3.1 Results on TC-STAR Corpora.
The error analyses were carried out on six Spanish-to-English outputs generated by phrase-based translation systems built on differentsizes of training corpora in order to examine the effects of data sparseness (Popovic?
andNey 2006b).
In addition, the effects of local POS-based word reorderings of nouns andadjectives (Popovic?
and Ney 2006a) were analyzed in order to examine improvementsof the baseline system.
Adjectives in the Spanish language are usually placed after thecorresponding noun, whereas for English it is the other way around.
Therefore, localreorderings of nouns and adjective groups in the source language were applied.
If thesource language is Spanish, each noun is moved behind the corresponding adjectivegroup.
If the source language is English, each adjective group is moved behind thecorresponding noun.
An adverb followed by an adjective (e.g., more important) or twoadjectives with a coordinate conjunction in between (e.g., economic and political) aretreated as an adjective group.
Reorderings were applied in the source language, thentraining and search were performed using the transformed source language data.
Mod-ifications of the training and search procedure were not necessary.
In this work, onlySpanish-to-English translation is analyzed.
The English outputs of following trainingset-ups for the same phrase-based translation system are examined: training on the full bilingual corpus ?
a large task?specific corpus (1.3M)673Computational Linguistics Volume 37, Number 4 training on a small task?specific corpus (13k) training only on a conventional dictionaryThe effects of local reorderings are investigated for each size of the training corpus.Table 13 presents the raw error counts in each category for each of the six translationoutputs in the form Nhum/Naut.
In the last row of the table, the Spearman and Pearsoncorrelation coefficients ?
and r for each category across different translation outputs areshown.
In addition, the correlations for error distributions within a translation output(as in Section 4.2) are presented as well in the rightmost column, and it can be seen that,as in the previous experiments, they are high for each of the translation outputs.
As forthe correlations of error categories across translation outputs, the class of inflectional er-rors and of incorrect lexical choice have very high correlations, which suggests that thesetwo categories reflect well the differences between translation outputs.
The reorderingerrors and missing words are also suitable for comparison, whereas the category of extrawords has even a negative correlation?it is not possible to draw conclusions aboutdifferences between translation outputs looking into this category.From both the human and the automatic error analysis it can be concluded that thenumber of inflectional errors is low, and becomes slightly higher for the system trainedon a dictionary, although there is a number of reordering errors, missing words, andextra words.
The most problematic category is the incorrect lexical choice, especiallyfor the small training corpora.
The number of reordering errors and missing words isalso increasing when the corpus size is decreasing, although to a lesser extent.
The POS-based local reordering technique reduces the number of reordering errors, especially forthe small training corpora, and does not harm the other error categories.4.3.2 Results on WMT Corpora.
Three outputs generated by three German-to-Englishstatistical translation systems are analyzed in order to examine the differences betweena standard phrase-based translation model and a hierarchical translation model.
In thestandard phrase-based model, phrases are defined as non-empty contiguous sequencesof words.
The hierarchical phrase-based model is an extension of this model where thephrases are allowed to have ?gaps?
(i.e., non-contiguous parts of the source sentenceare allowed to be translated into possibly non-contiguous parts of the target sentence).Table 13Results (raw error counts) of human (left) and automatic (right) error analysis for six differentSpanish-to-English TC-STAR systems; Spearman (left) and Pearson (right) correlation coefficientsfor each translation output across error categories (last column) and for each error categoryacross different translation outputs (last row).system infl order miss extra lex ?/r1.3M 7 / 11 24 / 47 38 / 48 31 / 37 64 / 184 0.9 / 0.92+reord.
adj.
7 / 11 16 / 46 37 / 49 25 / 31 61 / 184 0.9 / 0.9113k 7 / 12 35 / 74 55 / 57 26 / 34 134 / 223 0.9 / 0.98+reord.
adj.
8 / 13 19 / 50 54 / 60 25 / 40 132 / 217 0.9 / 0.98dictionary 14 / 18 45 / 88 104 / 84 33 / 30 378 / 414 0.9 / 0.99+reord.
adj.
14 / 19 21 / 64 104 / 80 35 / 32 375 / 391 0.9 / 0.99?/r 0.94 / 0.99 0.83 / 0.85 0.87 / 0.99 ?0.19 / ?0.34 0.99 / 0.99674Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation OutputIn this way, long-distance dependencies and reorderings can be modelled better.
Inaddition, we investigate the effects of long range POS-based reorderings of Germanverbs (Popovic?
and Ney 2006a) used to improve the phrase-based system.
Verbs in theGerman language can often be placed at the end of a clause.
This is mostly the case withinfinitives and past participles, but there are many cases when other verb forms alsooccur at the clause end.
For the translation from German into English, the followingverb types were moved towards the beginning of a clause: infinitives, infinitives withthe German infinitive particle zu, finite verbs, past participles, and verb negations.Analogously to the local reorderings described in the previous section, training andsearch are performed using the transformed source language data.Table 14 presents the raw error counts in the form Nhum/Naut for all categories and alltranslation outputs.
Spearman and Pearson correlation coefficients are shown as well foreach translation output across error categories (rightmost column), and for each errorcategory across different translation outputs (last row).
Again, the correlations acrossthe error categories are very high for all translation outputs, although slightly smallerthan for the TC-STAR data.
It can be seen that the extra words also have the weakestcorrelation among error categories, although the coefficients are not negative as in thecase of the TC-STAR data.
This confirms the previous hypothesis that this error categoryis not suitable for looking into details about differences between translation outputs.From the results of both error analyses it can be seen that for all translation outputsthe inflectional errors are again the least problematic category.
On the other hand,there is a large number of reordering errors, missing words, and especially lexicalerrors.
The main advantage of the hierarchical system compared to the standard phrase-based system is the smaller number of missing words, and the main disadvantage thelarger number of reordering errors.
This looks rather surprising at first, because thehierarchical system should actually better deal with various reorderings, both local andlong-range.
Nevertheless, the produced reorderings are rather unconstrained so that anumber of them are beneficial, but there are also a number of reorderings that makethe translation quality worse.
One possibility for improving the hierarchical system isto introduce certain phrase/gap constraints using POS tags or deeper syntax knowledgesources.
The long-range POS-based reorderings of verbs used to improve the standardphrase-based system reduced the number of reordering errors, and also the numberof lexical errors.
A discrepancy considering the missing words can be observed forthis method: The human error analysis reports reduction of missing words whereasthis is not captured by automatic tools.
Deeper analysis regarding different POS classescould possibly reveal more details about this.
It also can be seen that the hierarchicalTable 14Results (raw error counts) of human (left) and automatic (right) error analysis for three differentGerman-to-English WMT systems; Spearman (left) and Pearson (right) correlation coefficientsfor each translation output across error categories (last column) and for each error categoryacross different translation outputs (last row).system infl order miss extra lex ?/rphrase-based 12 / 32 60 / 235 204 / 199 52 / 40 189 / 521 0.70 / 0.72+reorder 16 / 44 41 / 212 172 / 200 30 / 56 163 / 495 0.7 / 0.74hierarchical 17 / 46 100 / 274 107 / 153 68 / 99 171 / 508 0.90 / 0.91?/r 1.00 / 0.90 1.00 / 0.99 0.60 / 0.90 0.5 / 0.62 1.00 / 0.96675Computational Linguistics Volume 37, Number 4Table 15Examples of errors in three different German-to-English translation systems: phrase-based,phrase-based with POS-based reorderings, and hierarchical.
Bold words denote differencesbetween systems.reference: The total amount designated for assistance to the systemis to be divided into two parts.phrase-based: The aid to the systemcertain amount in two parts.+reorder: To help the system certain total amount to be dividedinto two parts.hierarchical: The for the system to help certain total amount will bedivided into two parts.reference: Retailers are to decide for themselves if they wantto pass on the price increases to their customers.phrase-based: The retailers themselves must decide whetherthe doubling their customers pass.+reorder: Retailers should decide for themselves whether they wantto pass on increasing costs to their customers.hierarchical: The retailers themselves must decide whether they wishto pass on to their customers the price increase.reference: We have made great progress towards an agreementthat will be effective on the market, declared therepresentative of the Bush administration, Henry Paulson.phrase-based: We have great progress towards an agreement,the marktwirksam be, said the representatives of theadministration, Bush Henry Paulson.+reorder: We have made great progress towards an agreement,which will be marktwirksam, said the representatives of theadministration, Bush Henry Paulson.hierarchical: We have made great strides in the direction of an agreement,which will be marktwirksam, said the representatives of theBush administration, Henry Paulson.reference: The search for Fosset was called off a month afterhe had disappeared.phrase-based: The search for Fosset was a month after whose disappearancehas been abandoned.+reorder: The search for Fosset was abandoned a month afterhis disappearance.hierarchical: The search Fosset was a month after the disappearancehave been abandoned.system produces fewer lexical errors than the baseline phrase-based system, but morelexical errors than the phrase-based system with reorderings.
Syntactic constraints forthe hierarchical model might improve this error category as well.Table 15 presents examples of differences between translation systems and effectsof improvements along with some differences between human and automatic erroranalysis.4 The first sentence represents an example for the missing words errorproblem?one can see that there are a number of missing words in the output of thephrase-based system.
All of these missing words are detected both by human and4 It should be noted that it was impossible to show visually all details worthy of explanation.676Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation Outputby automatic error analysis, except for the word amount, which is considered by theautomatic tools as a reordering error (for the same reason as the example in Table 7).When the reordering technique is applied, the number of errors is reduced and thetranslation becomes understandable although still not grammatically correct: Thesequence total amount becomes a reordering error both for humans and for automatictools.
For the words designed and is, the confusion between missing words (human)and lexical errors (automatic) is present.
The hierarchical system generates evenfewer missing words, although neither this system nor the phrase-based system withreorderings is able to overcome the reordering problem.
It should also be noted thatfor all three systems, the word assistance is detected as a word translated by incorrectlexical choice aid/to help, whereas by the humans it is not considered as an error.The second example illustrates improvements of the standard phrase-based systemyielded by POS-based reorderings, as well as advantages and disadvantages of thehierarchical system.
The baseline phrase-based system produces several missing words(they want, price increases), as well as reordering errors (pass on).
When the reorderingsare applied, these errors are no longer present and the overall number of errors isreduced, although there are still some words considered as errors only by automatictools (i.e., if and price as lexical errors, to as a reordering error).
The output of thehierarchical system also does not contain missing words, and the problem of the verbreordering is solved as well (pass on), but some other reordering errors are introduced:the price increase.
It should be noted that for this sentence automatic tools detect thatwant is a lexical error because it is translated as wish, whereas the humans of course donot consider this an error.The third sentence shows advantages of the hierarchical system.
The output of thephrase-based system has several missing words (made, that will) which are not presentwhen the reorderings are applied, and are also not present in the output of the hierar-chical system.
The same happens with the verb reordering error be.
However, the nounreordering error Bush is not resolved by the POS-based reorderings of verbs, whereas itis at the correct position in the hierarchical system output.
Apart from this, all outputscontain an inflectional error representative/representatives, as well as a lexical choice errorcaused by an out-of-vocabulary word effective on market/marktwirksam.
For the phrase-based output, this error is classified as a lexical error both by humans and by automatictools, but for the other two outputs, the automatic tools classified the words effective onas missing words.The fourth example shows a case when the automatic error analysis cannot detectdifferences between translation outputs.
The baseline phrase-based output containsverb reordering errors has been abandoned.
The same error is present in the output of thehierarchical system, whereas the sentence obtained by the phrase-based system withreorderings is completely correct.
Nevertheless, these reordering errors are not at alldetected by automatic error analysis?it detects reordering errors only if exactly thesame words in the reference and in the hypothesis are present.
Therefore there are morediscrepancies between the human and automatic error analysis for all three outputs:called off is detected as missing, disappeared as a lexical error, and in the hierarchicalsystem output have been are considered as extra words.
This example, together with thefact that in general the automatic error analysis detects much higher numbers of lexicalerrors than the human evaluation, indicates that the automatic error analysis could beimproved by using a list of synonyms.4.3.3 Correlations of the POS Classes Across the Translation Outputs.
Correlation coefficientsof the POS classes across different TC-STAR and WMT09 translation outputs for each677Computational Linguistics Volume 37, Number 4error category are presented in Table 16.
For the cases when both error analyses detectedzero errors, the correlation coefficients are omitted.
It can be seen that the verbs, nouns,and adverbs have high correlations for each of the error categories (except extra words),as well as that the inflectional and reordering errors have high correlations for almost allPOS classes.
These error categories and POS classes are used in the further experimentsdescribed in the following section.On the other hand, it can be seen that for the missing words, extra words, andlexical errors, the correlations for some of the POS classes are low or the behavior isdependent on the data set.
As already mentioned in Section 4.2, the main source ofdiscrepancies between human and automatic error analysis in general is the difficulty ofdisambiguation between missing words, lexical errors, and extra words.
Nevertheless,a deeper analysis (an ?error analysis of error analysis?)
should be carried out in orderTable 16Spearman (above) and Pearson (below) correlation coefficients for each POS class within oneerror category across six different TC-STAR and three different WMT09 English translationoutputs.
The coefficients are omitted for the cases when both error analyses detected zero errors.
(a) TC-STAR translation outputsV N A ADV PRON DET PREP CON NUM PUNinfl 0.96 0.61 1.000.96 0.02 1.00order 0.69 0.76 0.86 0.44 0.54 0.50 0.94 0.66 0.66 0.530.85 0.89 0.98 0.37 0.02 0.66 0.92 0.03 0.03 0.01miss 0.71 0.76 0.04 0.54 1.00 0.64 0.17 0.87 0.660.67 0.83 ?0.15 0.94 0.99 0.61 ?0.31 0.88 0.03ext 0.61 ?0.40 0.53 0.86 0.57 0.39 0.21 0.66 0.660.78 ?0.47 0.47 0.95 0.58 0.20 0.01 0.01 0.03lex 0.99 1.00 0.83 0.93 0.59 0.47 0.99 0.76 0.660.99 0.99 0.96 0.93 0.98 0.97 0.94 0.71 0.03(b) WMT09 translation outputsV N A ADV PRON DET PREP CON NUM PUNinfl 1.00 0.500.99 0.01order 1.00 1.00 0.88 0.50 0.88 1.00 1.00 0.12 0.88 ?0.620.99 0.98 0.69 0.72 0.72 1.00 0.99 ?0.28 0.94 ?0.92miss 1.00 0.50 0.12 1.00 1.00 0.50 0.50 ?0.12 ?0.12 ?0.620.99 0.73 0.01 0.97 0.89 0.60 0.95 ?0.50 ?0.50 ?0.94ext 0.12 0.50 0.50 ?0.50 ?0.50 0.50 0.50 0.88 0.500.19 0.52 0.01 ?0.50 ?0.42 0.89 0.94 0.94 ?0.06lex 1.00 0.50 ?0.50 1.00 0.12 ?1.00 1.00 ?1.00 0.62 ?0.620.99 0.03 ?0.87 1.00 ?0.19 ?0.98 0.99 ?1.00 0.04 ?0.66678Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation Outputto better understand all the details.
Such an analysis is an interesting and importantdirection for future work.5.
Comparison of Translation SystemsThe experiments in the previous sections showed that the proposed error categories(with the exception of extra words) can be useful for obtaining more information aboutdifferences between translation outputs.
In this section, we will show some applications.In order to be able to compare translation outputs generated under different conditions(different target languages, different test set sizes, etc.
), we introduce word error ratesfor each error category, that is, we normalize the number of errors over the total refer-ence length.
We do not omit the extra words because this category is informative forthe distribution of errors in a particular translation output.
Thus the following novelmetrics are defined:INFER (inflectional error rate):Number of RPER (reference PER) or HPER (hypothesis PER) errors caused bywrong choice of the full word form normalized over the (closest) reference length.RER (reordering error rate):Number of WER errors that do not occur either as RPER (reference PER) or as HPER(hypothesis PER) errors normalized over the (closest) reference length.MISER (missing word error rate):Number of WER deletions that are not caused by wrong full form choice normal-ized over the (closest) reference length.EXTER (extra word error rate):Number of WER insertions that are not caused by wrong full form choice normal-ized over the (closest) reference length.LEXER (lexical error rate):Number of errors caused neither by wrong full form choice nor by deleting orinserting words normalized over the (closest) reference length.
?ER (sum of error rates):Sum of all error categories.An overview about how these metrics behave in comparison with the standard worderror rates WER, PER, and TER along with Spearman and Pearson correlation coefficientsis presented in Table 17.
The BLEU score is also shown as illustration.
All error rates arecalculated on the translation outputs analyzed in Section 4.
We can see that the sum of allerror categories ?ER is always greater than PER, lower than WER, and similar, althoughin a majority of cases lower, than TER.In the following experiments, we use the error rates to get more details aboutdifferences between different language pairs: The error rates are calculated for six translationoutputs generated in the framework of the WMT09 shared task in order tosee the differences between the target languages as well as the differencesin English outputs generated by distinct source languages. methods for improvement: Following the ideas from Section 4.3, moredetails about the effects of POS-based reordering methods are analyzed:local reorderings on the TC-STAR data and long range reorderings on theWMT09 data.679Computational Linguistics Volume 37, Number 4 different translation systems: Outputs generated in the second TC-STARevaluation by five distinct translation systems (three statistic and tworule-based) are analyzed.
In addition, an interesting example from theWMT09 shared task is presented.Table 17Error categories ?
novel error rates (raw error counts in five error categories normalized overreference length) compared with standard word error rates WER, PER, and TER.
The BLEU scoreis also shown as illustration.
(a) Standard error rates (%) (including the BLEU score) and novel error rates.% BLEU WER PER TER INFER RER MISER EXTER LEXER ?ERArEn (BN) 59.7 29.6 22.4 28.0 1.5 4.2 4.0 8.7 9.3 27.6ArEn (NW) 72.1 18.8 14.5 17.8 1.1 1.9 4.9 3.6 6.2 17.8CnEn (NW) 58.0 34.6 21.4 30.0 1.6 6.8 9.6 4.6 9.5 32.1EsEn 1.3M 60.9 31.1 25.2 29.6 1.0 4.1 4.1 3.2 15.9 28.2EsEn +reord 61.7 30.0 24.9 28.8 1.0 3.9 4.2 2.7 15.9 27.9EsEn 13k 48.3 37.6 28.9 36.0 1.0 6.4 4.9 2.9 19.1 34.3EsEn +reord 51.3 35.8 28.7 34.8 1.1 4.3 5.1 3.4 18.6 32.6EsEn dict 21.6 57.9 47.7 56.3 1.5 7.5 7.2 2.6 35.4 54.2EsEn +reord 25.7 54.8 46.8 53.6 1.6 5.5 6.8 2.7 33.4 50.1DeEn phrase 16.9 66.4 47.5 60.5 2.1 14.8 12.5 2.5 32.8 64.8DeEn +reord 18.4 65.5 47.2 59.6 2.8 13.3 12.6 3.5 31.2 63.4DeEn hier 17.2 71.9 48.3 64.7 2.9 17.2 9.6 6.2 32.0 68.0(b) Spearman (above) and Pearson (below) correlation coefficients between standard error rates(including 1-BLEU) and novel error rates (error categories).1-BLEU WER PER TER INFER RER MISER EXTER LEXER?ER 0.972 1.000 0.937 1.000 0.701 0.930 0.804 ?0.302 0.8650.982 0.998 0.959 0.990 0.853 0.915 0.814 ?0.103 0.914LEXER 0.834 0.865 0.921 0.865 0.451 0.698 0.582 ?0.6170.960 0.926 0.988 0.957 0.604 0.695 0.592 ?0.348EXTER ?0.274 ?0.302 ?0.397 ?0.302 0.222 ?0.089 ?0.138?0.172 ?0.107 ?0.225 ?0.146 0.264 0.070 ?0.115MISER 0.787 0.804 0.598 0.804 0.813 0.8430.749 0.791 0.668 0.744 0.836 0.849RER 0.944 0.930 0.804 0.930 0.7920.832 0.902 0.769 0.858 0.905INFER 0.726 0.701 0.540 0.7010.771 0.839 0.710 0.794TER 0.972 1.000 0.9370.995 0.995 0.987PER 0.902 0.9370.988 0.967WER 0.9720.986680Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation Output5.1 Different Language PairsTable 18 presents the error rates and the BLEU scores for six translation outputs fromthe News domain generated by phrase-based systems in the framework of the WMT09shared task.
The test data used in the WMT shared tasks are very suitable for suchcomparison because they are parallel for all language pairs.
For each language paironly a baseline phrase-based system is used in order to focus only on the language-dependent differences.
It can be seen that for all English outputs, independently of thesource language, the inflectional error rate (INFER) of about 2% is the smallest errorrate.
For the other target languages this is not the case: The French and Spanish outputshave an INFER between 6% and 7%, and the German output more than 8%.
Theseresults could be expected knowing that the English language is not morphologicallyrich, whereas Spanish, French, and especially German are.The highest reordering error rate (RER) is present when translating from and intoGerman, and the highest missing word error rate (MISER) can be observed for theEnglish output generated from the German source text.
The category of extra wordshas been shown not to be reliable for comparison of translation outputs, although it canbe seen that all EXTER scores are rather low in comparison to the other error categories.The highest lexical error rate (LEXER) of 35% can be observed for the German output,followed by the English output generated from the German source text (33%).
Thelexical error rates for the translation from and into Spanish and French are lower andsimilar, between 29% and 30%.From the BLEU score and other standard error measures it can be seen that the transla-tion from and particularly into German is the hardest.
It can also be observed that trans-lation into French and Spanish is more difficult than translation from these languagesinto English.
The results of the error analysis give more details, for example, such thatfor translation from and into German language the number of reordering errors ishigher than for the other language pairs.
Furthermore, when translating from Germaninto English, a high number of missing words should be expected.
For translation intoGerman, morphology is an important issue.
A high number of inflectional errors is alsopresent for the French and Spanish outputs?higher INFER is the main reason whytranslation into French and Spanish is more difficult than the other translation direction.5.2 Methods for Improvement?More Details About POS-Based ReorderingsIn order to better understand the reordering problems and improvements obtainedby POS-based reordering techniques, we investigate some more details.
For the fullTable 18Error categories for different source and target languages: examples of German-to-English,French-to-English, Spanish-to-English, English-to-German, English-to-French, andEnglish-to-Spanish WMT09 translation outputs.% BLEU INFER RER MISER EXTER LEXER ?ERde-en 17.0 2.0 12.3 13.2 4.4 33.0 65.0fr-en 23.2 2.2 11.3 9.6 6.1 29.9 59.1es-en 23.0 2.2 11.3 9.8 5.8 29.6 58.9en-de 12.7 8.4 12.2 9.9 4.8 35.0 70.3en-fr 21.7 6.2 11.0 9.7 4.7 29.9 61.4en-es 21.4 6.9 11.6 8.4 5.1 29.3 61.4681Computational Linguistics Volume 37, Number 4TC-STAR training corpus, we separate the test corpus into two sets?one containing sen-tences whose source sentences have been actually reordered and the other containingthe rest of the sentences.
Then we calculate the overall RER measure as well as the RERof noun?adjective groups and the RER of verbs for each of the sets translated by bothsystems (without and with reorderings).
The results in Table 19 show that the overallRER of the reordered set is decreased by the local reorderings whereas for the rest ofthe sentences a small increase can be observed.
Furthermore, it can be noted that forthe reordered set the RER of verbs is significantly smaller than the RER of nouns andadjectives which has been improved by local reorderings.
For the rest of the sentencesthere are no significant differences either between RERs of different POS groups orbetween the system with reorderings and the baseline system.
The same tendenciesoccur for the other translation direction.In order to better understand the long-range differences in word order, we carriedout a similar experiment on the WMT09 German-to-English translation output: Weseparate the test corpus into two sets, and calculate the RERs.
In addition, we alsocalculate the MISERs, because the category of missing words has also been shown tobe rather problematic for the translation from German into English, and to be improvedby long-range reorderings.
The results are presented in Table 20.
It can be observed thatthe reordered part of the test corpus has a higher RER, which is improved by long-rangereorderings.
However, the RER of the other part is also indirectly improved.
Lookinginto the specific POS classes, it can be seen that the reordering error rate of nouns andadjectives RER (N,A) is only slightly higher for the reordered sentences than for the rest,whereas the RER (V) is significantly higher for the reordered sentences.
When the long-range reorderings are applied, the RER (V) is reduced, but indirectly also the RER (N,A).As for missing words, the overall MISER is similar for both test sets.
For the reorderedset it is reduced by applying long-range reorderings, and for the rest of the sentencesit is slightly increased.
The number of missing verbs is much higher for the reorderedTable 19Effects of local POS-based reorderings on reordering error rates for the Spanish?Englishtranslation for reordered sentences and for the rest: overall RER, RER of nouns and adjectives,and RER of verbs.
(a) English outputSpanish?English RER RER (N,A) RER (V)reordered baseline 6.0 2.6 0.9reorder adjectives 5.4 2.1 0.9not reordered baseline 4.2 1.3 1.0reorder adjectives 4.3 1.3 1.0(b) Spanish outputEnglish?Spanish RER RER (N,A) RER (V)reordered baseline 6.5 2.3 0.5reorder adjectives 6.3 2.2 0.5not reordered baseline 4.6 1.2 0.6reorder adjectives 4.7 1.2 0.6682Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation OutputTable 20Effects of long range POS-based reorderings on reordering error rates and missing word errorrates for the German?English translation for reordered sentences and for the rest.
(a) Reordering error rates: overall RER, RER of nouns and adjectives, and RER of verbs.German?English RER RER (N,A) RER (V)reordered baseline 12.9 1.9 1.8reorder verbs 12.4 1.8 1.6not reordered baseline 9.8 1.4 0.7reorder verbs 9.5 1.3 0.7(b) Missing word error rates: overall MISER, MISER of nouns and adjectives, and MISER of verbs.German?English MISER MISER (N,A)/MISER (N) MISER (V)reordered baseline 13.1 2.0 / 2.8 3.5reorder verbs 12.4 2.0 / 2.8 2.9not reordered baseline 13.6 2.2 / 3.4 2.3reorder verbs 13.9 2.1 / 3.5 2.4set than for the rest, whereas the number of missing nouns and adjectives is similar forboth sets.
The MISER (V) of the reordered set is significantly reduced by long-rangereorderings, whereas the MISER (N,A) remains the same.
For the rest of the sentences,there are basically no differences between different POS classes and systems with andwithout reorderings.The experiments have also shown that the local reorderings are more targeted tothe specific word classes (i.e., nouns and adjectives), improving mostly these words,whereas long-range reorderings do not affect only the words which are actuallyreordered (in this case: verbs) but they also introduce some indirect improvements,such as reducing errors of other word classes and reducing the number of missingwords.
This happens due to better alignment learning and better phrase extractionenabled by applying long-range reorderings.5.3 Different Translation SystemsFor the translation outputs analyzed in the previous section, the same phrase-basedtranslation system is used for all experiments.
In order to examine how the new errorrates reflect the differences between distinct translation systems, we carried out an erroranalysis of different translation outputs generated by five distinct translation systemsin the second TC-STAR evaluation.
A total of nine different systems participated in theevaluation, and we selected five representative systems for our experiments which willbe referred to as A, B, C, D, and E. For the English language we used the outputsof four systems A, B, C, and D, and for Spanish additionally the output of a systemE.
The systems A, B, and C are statistical phrase-based and the systems D and E arerule-based.In Table 21 the new error rates for all translation outputs are presented along withthe BLEU score as the official metric of the evaluation.
For translation into English, thesystems A, B, and C have very similar BLEU scores as well as all error categories.
The683Computational Linguistics Volume 37, Number 4Table 21Error categories for different Spanish-to-English and English-to-Spanish TC-STAR translationsystems.
(a) English outputsEnglish BLEU INFER RER MISER EXTER LEXER ?ERA 53.5 2.5 5.8 4.7 4.1 14.5 31.6B 53.1 2.3 5.7 4.8 3.5 13.9 30.2C 52.8 2.1 5.9 5.6 3.2 14.1 30.9D 45.4 2.6 6.9 3.7 5.2 17.5 35.9(b) Spanish outputsSpanish BLEU INFER RER MISER EXTER LEXER ?ERA 50.0 4.8 5.6 4.6 4.1 15.2 34.4B 48.2 4.8 5.8 5.3 3.8 15.1 34.8C 49.6 4.9 5.6 5.3 3.1 14.7 33.7D 38.9 5.5 6.7 4.7 4.4 19.4 40.7E 38.6 5.1 7.8 4.6 4.7 19.0 41.2worst ranked system according to the BLEU is system D, and from the error rates it canbe seen that the main problem for this system is the incorrect lexical choice.
The numberof reordering errors is also larger for this system than for the others.5For translation into Spanish, the BLEU scores are similar for the three systems A, B,and C with the two systems D and E having lower scores.
The error rates show that themain differences between systems A, B, and C on the one side and systems D and E onthe other are incorrect lexical choices.
The number of reordering errors is also higher forsystems D and E, and system D in addition has a higher INFER than the other systems.WMT09 systems.
Another interesting example can be found for the WMT09 French?English translation task of News data: Among twenty participants, the output gener-ated by the Google system6 has the best BLEU score as well as the best human sentenceranking score (Callison-Burch et al 2007), whereas the scores for the translation pro-duced by University of Geneva (Wehrli, Nerima, and Scherrer 2009) are the lowest.
Thesentence rank of a translation system is defined as the percentage of sentences for whichthis system is judged by human evaluators to be better or equal than any other system.The results for these two systems along with two additional medium ranked sys-tems, the statistical Limsi system and a rule based rbmt3 system, can be seen in Table 22.The BLEU score and the official human rank score are shown along with the five errorcategories.Similarly to the TC-STAR systems, the main problem of the worst-ranked systemsis the lexical error rate LEXER.
The reordering error rate RER also has the same rankas the official overall scores.
Nevertheless, the Geneva translation output significantlyoutperforms the other systems in terms of the inflectional error rate INFER.
Looking5 The number of extra words too, although in previous experiments this error category is shown not to bereliable for system comparison.6 http://translate.google.com.684Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation OutputTable 22Human sentence rank (%), BLEU score, and error categories for four French-to-English WMT09translation systems: the best ranked Google, the worst ranked Geneva, and two medium rankedsystems (Limsi (statistical) and rbmt3 (rule-based)).FrEn RANK BLEU INFER RER MISER EXTER LEXER ?ERgoogle 76.0 31.0 2.1 10.2 9.3 5.2 26.9 53.8limsi 65.0 26.0 2.3 11.5 6.4 9.4 30.1 59.8rbmt3 54.0 20.0 2.4 11.9 5.6 11.0 34.7 65.6geneva 34.0 14.0 1.5 12.2 5.8 13.0 36.9 69.6Table 23Inflectional error rates of verbs, nouns, and adjectives in English outputs produced by fourWMT09 translation systems: the best ranked Google, the worst ranked Geneva, and two mediumranked systems (Limsi (statistical) and rbmt3 (rule-based)).FrEn google limsi rbmt3 genevaINFER V 1.4 1.5 1.5 0.8N 0.7 0.7 0.6 0.5A 0.1 0.1 0.1 0.1into the details about POS classes in Table 23, it can be seen that the INFER of verbs isthe main reason for the low INFER of the Geneva translation.6.
DiscussionThis work describes a framework for automatic error analysis of translation output thatpresents just a first step towards the development of automatic evaluation measureswhich provide partial and more specific information of certain translation problems.The basic idea is to use the actual erroneous words extracted from the standard worderror rates WER and PER in combination with linguistic knowledge in order to obtainmore information about the translation errors and to perform further analysis of par-ticular phenomena.
The overall goal is to get a better overview of the nature of actualtranslation errors?to identify strong and weak points of a translation system, to getideas about possible improvements of the system, to analyze improvements achievedby particular methods, and to better understand the differences between different trans-lation systems.There are many possible ways to carry out automatic error analysis using theproposed framework.
The focus of this work is classifying errors into the following fivecategories: morphological (inflectional) errors, reordering errors, missing words, extrawords, and incorrect lexical choice.
In addition, the distribution of these error typesover POS classes is investigated.
This method can be applied to any language pair; theprerequisite is the availability of a morpho-syntactic analyzer for the target language.The results of the proposed method are compared with the results obtained byhuman error analysis.
Detailed experiments on different types of corpora and variouslanguage pairs are carried out in order to investigate two applications of error analysis:estimating the contribution of each error category within one translation output, andcomparing different translation outputs using the introduced error categories.
For thedistribution of error categories within a translation output, we show that the results of685Computational Linguistics Volume 37, Number 4automatic error analysis correlate very well with the results of human error analysisfor all translation outputs.
In addition, we show that the differences between results ofhuman and automatic error analysis occur mainly due to the difficulty of disambigua-tion between missing words, lexical errors, and extra words.
As for the comparisonof different translation outputs, we show that all error categories except extra wordscorrelate well with the human analysis.
We also show that verbs, nouns, and adverbscorrelate well in all error categories (except extra words), as well as that inflectionaland reordering errors correlate well for almost all POS classes.
Nevertheless, for missingwords, lexical errors, and extra words, some of the POS classes have low correlations.The main reason for these discrepancies is again the problematic disambiguation be-tween the three error categories.
A deeper analysis should be carried out in order tounderstand all details, such as to examine which words/POS classes are classified in oneparticular category by humans but in another category automatically.
Such an ?erroranalysis of error analysis?
is an interesting and important direction for future work.The work described in this article also opens many other directions for future work.The proposed framework can be extended in various ways such as going beyond theword level, introducing deeper linguistic categories, using other alignments apart fromWER and RPER/HPER (such as TER, GIZA, etc.
), investigating the contribution of sourcewords if source?target algnment information is available, measuring correlations withthe human error analysis carried out without reference translations, measuring inter-and intra-annotator agreement for human error analysis and its effects, and so forth.7.
ConclusionsThis work presents a first step towards automatic error analysis of machine transla-tion output.
We show that the results obtained by the proposed framework correlatevery well with the results of human error analysis, as well as that the main sourceof discrepancies is disambiguation between missing words, lexical errors, and extrawords.
The new error rates are then calculated for various translation outputs in orderto compare them.
Different source and target languages are compared to see particularproblems for each language pair and translation direction.
An analysis of improvementsyielded by POS-based reorderings is carried out as well.
Finally, we show how the newmeasures can show differences between distinct translation systems, namely, what arethe weak/strong points of particular systems.
The presented framework offers a numberof possibilities for future work, both to improve the proposed metrics as well as toinvestigate other set-ups.AcknowledgmentsThis work was partly funded by theEuropean Union under the integratedproject TC-STAR ?
Technology andCorpora for Speech to Speech Translation(IST-2002-FP6-506738); by the QuaeroProgramme, funded by OSEO, FrenchState agency for innovation; and by theDefense Advanced Research ProjectAgency (DARPA) under contract No.HR0011-06-C-0023.
Any opinions, findings,conclusions, or recommendations expressedin this work are those of the authors and donot necessarily reflect the views of DARPA.Special thanks to Adria` de Gispert,Deepa Gupta, Patrik Lambert, andNecip Fazil Ayan.ReferencesBanerjee, Satanjeev and Alon Lavie.2005.
METEOR: An automatic metricfor MT evaluation with improvedcorrelation with human judgements.In Proceedings of the Workshop on Intrinsicand Extrinsic Evaluation Measures forMT and/or Summarization, pages 65?72,Ann Arbor, MI.686Popovic?
and Ney Towards Automatic Error Analysis of Machine Translation OutputCallison-Burch, Chris, Cameron Fordyce,Philipp Koehn, Christof Monz, and JoshSchroeder.
2007.
(Meta-)Evaluation ofmachine translation.
In Proceedings of the2nd ACL 07 Workshop on Statistical MachineTranslation (WMT 07), pages 136?158,Prague.Callison-Burch, Chris, Philipp Koehn,Christof Monz, and Josh Schroeder.2009.
Findings of the 2009 Workshopon Statistical Machine Translation.In Proceedings of the 4th EACL 09 Workshopon Statistical Machine Translation (WMT 09),pages 1?28, Athens.Chiang, David.
2007.
Hierarchicalphrase-based translation.
ComputationalLinguistics, 33(2):201?228.Doddington, George.
2002.
Automaticevaluation of machine translation qualityusing n-gram co-occurrence statistics.In Proceedings of the ARPA Workshopon Human Language Technology,pages 128?132, San Diego, CA.Gime?nez, Jesu?s and Enrique Amigo?.
2006.IQMT: A framework for automaticmachine translation evaluation.In Proceedings of the 5th InternationalConference on Language Resources andEvaluation (LREC 06), pages 685?690,Genoa.Kirchhoff, Katrin, Owen Rambow,Nizar Habash, and Mona Diab.
2007.Semi-automatic error analysis forlarge-scale statistical machine translation.In Proceedings of the MT Summit XI,pages 289?296, Copenhagen.Leusch, Gregor, Nicola Ueffing, andHermann Ney.
2006.
CDER: EfficientMT evaluation using block movements.In Proceedings of the 11th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL 06),pages 241?248, Trento.Leusch, Gregor, Nicola Ueffing, David Vilar,and Hermann Ney.
2005.
Preprocessingand normalization for automaticevaluation of machine translation.In Proceedings of the Workshop on Intrinsicand Extrinsic Evaluation Measures for MTand/or Summarization, pages 17?24,Ann Arbor, MI.Levenshtein, Vladimir Iosifovich.
1966.Binary codes capable of correctingdeletions, insertions and reversals.Soviet Physics Doklady, 10(8):707?710.Llitjo?s, Ariadna Font, Jaime G. Carbonell,and Alon Lavie.
2005.
A framework forinteractive and automatic refinement oftransfer-based machine translation.In Proceedings of the 10th AnnualConference of the European Associationfor Machine Translation (EAMT 05),pages 87?96, Budapest.Lopez, Adam and Philip Resnik.
2005.Pattern visualization for machinetranslation output.
In Proceedings ofHLT/EMNLP on Interactive Demonstrations,pages 12?13, Vancouver.Matusov, Evgeny, Gregor Leusch, OliverBender, and Hermann Ney.
2005.Evaluating machine translation outputwith automatic sentence segmentation.In Proceedings of the International Workshopon Spoken Language Translation (IWSLT 05),pages 148?154, Pittsburgh, PA.Papineni, Kishore, Salim Roukos, ToddWard, and Wie-Jing Zhu.
2002.
BLEU:A method for automatic evaluation ofmachine translation.
In Proceedings of the40th Annual Meeting of the Associationfor Computational Linguistics (ACL 02),pages 311?318, Philadelphia, PA.Popovic?, Maja, Adria` de Gispert, DeepaGupta, Patrik Lambert, Hermann Ney,Jose?
B. Marin?o, Marcello Federico, andRafael Banchs.
2006.
Morpho-syntacticinformation for automatic error analysisof statistical machine translation output.In Proceedings of the 1st NAACL 06Workshop on Statistical Machine Translation(WMT 06), pages 1?6, New York, NY.Popovic?, Maja and Hermann Ney.
2006a.POS-based word reorderings for statisticalmachine translation.
In Proceedings of the5th International Conference on LanguageResources and Evaluation (LREC 06),pages 1278?1283, Genoa.Popovic?, Maja and Hermann Ney.
2006b.Statistical machine translation with asmall amount of bilingual training data.In Proceedings of the LREC 06 Workshop onStrategies for Developing Machine Translationfor Minority Languages, pages 25?29, Genoa.Snover, Matthew, Bonnie Dorr, RichardSchwartz, Linnea Micciulla, and JohnMakhoul.
2006.
A study of translationerror rate with targeted human annotation.In Proceedings of the 7th Conference of theAssociation for Machine Translation in theAmericas (AMTA 06), pages 223?231,Boston, MA.Turian, Joseph, Luke Shen, andI.
Dan Melamed.
2003.
Evaluation ofmachine translation and its evaluation.In Proceedings of the MT Summit IX,pages 23?28, New Orleans, LA.Vilar, David, Evgeny Matusov, Sas?a Hasan,Richard Zens, and Hermann Ney.
2005.687Computational Linguistics Volume 37, Number 4Statistical Machine Translation ofEuropean Parliamentary Speeches.In Proceedings of the MT Summit X,pages 259?266, Phuket.Vilar, David, Jia Xu, Luis Fernando D?Haro,and Hermann Ney.
2006.
Error analysisof statistical machine translation output.In Proceedings of the 5th InternationalConference on Language Resources andEvaluation (LREC 06), pages 697?702,Genoa.Wehrli, Eric, Luka Nerima, andYves Scherrer.
2009.
Deep linguisticmultilingual translation and bilingualdictionaries.
In Proceedings of the 4thEACL 09 Workshop on Statistical MachineTranslation (WMT 09), pages 90?94,Athens.Xiong, Deyi, Min Zhang, AiTi Aw, andHaizhou Li.
2010.
Linguistically annotatedreordering: Evaluation and analysis.Computational Linguistics, 36(3):535?568.Zens, Richard, Franz Josef Och, andHermann Ney.
2002.
Phrase-basedstatistical machine translation.
In 25thGerman Conference on Artificial Intelligence(KI2002), pages 18?32, Aachen.Zhou, Ming, Bo Wang, Shujie Liu, Mu Li,Dongdong Zhang, and Tiejun Zhao.2008.
Diagnostic evaluation of machinetranslation systems using automaticallyconstructed linguistic check-points.In Proceedings of the 22nd InternationalConference on Computational Linguistics(CoLing 2008), pages 1121?1128,Manchester.688
