Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 67?71,Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational LinguisticsA distributional similarity approach to the detection of semantic change inthe Google Books Ngram corpusKristina GulordavaDISI, University of TrentoTrento, Italykgulordava@gmail.comMarco BaroniCIMeC, University of TrentoTrento, Italymarco.baroni@unitn.itAbstractThis paper presents a novel approach for auto-matic detection of semantic change of wordsbased on distributional similarity models.
Weshow that the method obtains good resultswith respect to a reference ranking producedby human raters.
The evaluation also analyzesthe performance of frequency-based methods,comparing them to the similarity method pro-posed.1 IntroductionRecently a large corpus of digitized books was madepublicly available by Google (Mitchel et al, 2010).It contains more than 5 millions of books publishedbetween the sixteenth century and today.
Computa-tional analysis of such representative diachronic datamade it possible to trace different cultural trendsin the last centuries.
Mitchel et al (2010) exploitthe change in word frequency as the main measurefor the quantitative investigation of cultural and lin-guistic phenomena; in this paper, we extend this ap-proach by measuring the semantic similarity of theword occurrences in two different time points usingdistributional semantics model (Turney and Pantel,2010).Semantic change, defined as a change of oneor more meanings of the word in time (Lehmann,1992), is of interest to historical linguistics and isrelated to the natural language processing task ofunknown word sense detection (Erk, 2006).
Devel-oping automatic methods for identifying changes inword meaning can therefore be useful for both theo-retical linguistics and a variety of NLP applicationswhich depend on lexical information.Some first automatic approaches to the seman-tic change detection task were recently proposed bySagi et al (2009) and Cook and Stevenson (2010).These works focus on specific types of semanticchange, i.e., Sagi et al (2009) aim to identify widen-ing and narrowing of meaning, while Cook andStevenson (2010) concentrate on amelioration andpejoration cases.
Their evaluation of the proposedmethods is rather qualitative, concerning just a fewexamples.In present work we address the task of auto-matic detection of the semantic change of words inquantitative way, comparing our novel distributionalsimilarity approach to a relative-frequency-basedmethod.
For the evaluation, we used the GoogleBooks Ngram data from the 1960s and 1990s, tak-ing as a reference standard a ranking produced byhuman raters.
We present the results of the methodproposed, which highly correlate with the humanjudgements on a test set, and show the underlyingrelations with relative frequency.2 Google Books Ngram corpusThe overall data published online by Google repre-sent a collection of digitized books with over 500billion words in 7 different languages distributed inn-gram format due to copyright limitations (Mitchelet al, 2010).
An n-gram is a sequence of n words di-vided by space character; for each n-gram it is spec-ified in which year it occurred and how many times.For our diachronic investigation we used theAmerican English 2-grams corpus (with over 150millions 2-grams) and extracted two time slices fromthe 1960s and 1990s time periods.
More precisely,we automatically selected 2-grams with year of oc-currence between 1960 and 1964 for the 1960s slice,67and between 1995 and 1999 for the 1990s slice, andsummed up the number of occurrences of each 2-gram for both corpora.
After preprocessing, we ob-tained well-balanced 60s and 90s corpora containingaround 25 and 28 millions of 2-grams, respectively.We consider the 60s and 90s to be interesting timeframes for the evaluation, having in mind that a lotof words underwent semantic change between thesedecades due to many significant technological andsocial movements.
At the same time, the 60s areclose enough so that non-experts should have goodintuitions about semantic change between then andnow, which, in turn, makes it possible to collect ref-erence judgments from human raters.3 Measuring semantic change3.1 Relative frequencyMany previous diachronic studies in corpus linguis-tics focused on changes of relative frequency ofthe words to detect different kinds of phenomena(Hilpert and Gries, 2009; Mitchel et al, 2010).
In-tuitively, such approach can also be applied to de-tect semantic change, as one would expect that manywords that are more popular nowadays with respectto the past (in our case: the 60s) have changed theirmeaning or gained an alternative one.
Semanticchange could explain a significant growth of the rel-ative frequency of the word.Therefore we decided to take as a competing mea-sure for evaluation the logarithmic ratio between fre-quency of word occurrence in the 60s and frequencyof word occurrence in the 90s1.3.2 Distributional similarityIn the distributional semantics approach (see for ex-ample Turney and Pantel, 2010), the similarity be-tween words can be quantified by how frequentlythey appear within the same context in large cor-pora.
These distributional properties of the wordsare described by a vector space model where eachword is associated with its context vector.
The way acontext is defined can vary in different applications.The one we use here is the most common approach1The logarithmic ratio helps intuition (terms more popular inthe 60s get negative scores, terms more popular in the 90s havesimilarly scaled positive scores), but omitting the logarithmictransform produced similar results in evaluation.which considers contexts of a word as a set of allother words with which it co-occurs.
In our case wedecided to use 2-grams, that is, only words that oc-cur right next to the given word are considered aspart of its context.
The window of length 2 was cho-sen for practical reasons given the huge size or theGoogle Ngram corpus, but it has been shown to pro-duce good results in previous studies (e.g.
Bullinariaand Levy, 2007).
The words and their context vec-tors create a so called co-occurrence matrix, whererow elements are target words and column elementsare context terms.The scores of the constructed co-occurrence ma-trix are given by local mutual information (LMI)scores (Evert, 2008) computed on the frequencycounts of corresponding 2-grams2.
If words w1 andw2 occurred C(w1, w2) times together and C(w1)and C(w2) times overall in corpus then local mutualinformation score is defined as follows:LMI = C(w1, w2) ?
log2C(w1, w2)NC(w1)C(w2),where N is the overall number of 2-gram in the cor-pus.Given the words w1, w2 their distributional simi-larity is then measured as the cosine product of theircontext vectors v1,v2: sim(w1, w2) = cos(v1,v2).We apply this model to measure similarity of aword occurrences in two corpora of different timeperiods in the following way.
The set of context el-ements is fixed and remains the same for both cor-pora; for each corpus, a context vector for a word isextracted independently, using counts in this corpusas discussed above.
In this way, each word will havea 60s vector and a 90s vector, with the same dimen-sions (context elements), but different co-occurrencecounts.
The vectors can be compared by computingthe cosine of their angle.
Since the context vectorsare computed in the same vector space, the proce-dure is completely equivalent to calculating similar-ity between two different words in the same corpora;the context vectors can be considered as belong-ing to one co-occurrence matrix and correspond-ing to two different row elements word 60s andword 90s.2LMI proved to be a good measure for different semantictasks, see for example the work of Baroni and Lenci, 2010.68group examples sim freqmore frequent users 0.29 -0.94in 90s sleep 0.23 -0.32disease 0.87 -0.3card 0.17 -0.1more frequent dealers 0.16 0.04in 60s coach 0.25 0.12energy 0.79 0.14cent 0.99 1.13Table 1: Examples illustrating word selection with simi-larity (sim) and log-frequency (freq) metric values.We use the described procedure to measure se-mantic change of a word in two corpora of interest,and hence between two time periods.
High similar-ity value (close to 1) would suggest that a word hasnot undergone semantic change, while obtaining lowsimilarity (close to 0) should indicate a noticeablechange in the meaning and the use of the word.4 Experiments4.1 Distributional space constructionTo be able to compute distributional similarity forthe words in the 60s and 90s corpora, we randomlychose 250,000 mid-frequency words as the contextelements of the vector space.
We calculated 60s-to-90s similarity values for a list of 10,000 randomlypicked mid-frequency words.
Among these words,48.4% had very high similarity values (> 0.8), 50%average similarity (from 0.2 to 0.8) and only 1.6%had very low similarity (< 0.2).
According to ourprediction, this last group of words would be theones that underwent semantic change.To test such hypothesis in a quantitative way somereference standard must be available.
Since for ourtask there was no appropriate database containingwords classified for semantic change, we decided tocreate a reference categorization using human judge-ments.4.2 Human evaluationFrom the list of 10,000 words we chose 100 as arepresentative random subset containing words withdifferent similarities from the whole scale from 0to 1 and taken from different frequency range, i.e.,words that became more frequent in 90s (60%) andwords that became less frequent (40%) (see Tablesim-HR freq-HR sim-freqall words 0.386??
0.301??
0.380?
?frequent in 90s 0.445??
0.184 0.278?frequent in 60s 0.163 0.310 0.406?Table 2: Correlation between similarity (sim), frequency(freq) and human ranking (HR) values for all words,words more frequent in 60s and more frequent in 90s.Values statistically significant for p = 0.01(0.05) in one-sample t-test are marked with ??(?
).1 for examples).
Human raters were asked to rankthe resulting list according to their intuitions aboutchange in last 40 years on a 4-point scale (0: nochange; 1: almost no change; 2: somewhat change;3: changed significantly).
We took the average ofjudgments as the reference value with which distri-butional similarity scores were compared.
For the5 participants, the inter-rater agreement, computedas an average of pair-wise Pearson correlations, was0.51 (p < 0.01).
It shows that the collected judge-ments were highly correlated and the average judge-ment can be considered an enough reliable referencefor semantic change measurements evaluation.5 Results and discussionTo assess the performance of our similarity-basedmeasure, we computed the correlations between thevalues it produced for our list of words and the av-erage human judgements (Table 2).
The Pearsoncorrelation value obtained was equal to 0.38, whichis reasonably high given 0.51 inter-rater agreement.The frequency measure had a lower correlation(0.3), though close to the similarity measure perfor-mance.
Yet, the correlation of 0.38 between the twomeasures in question suggests that, even if they per-form similarly, their predictions could be quite dif-ferent.In fact, if we consider separately two groups ofwords: the ones whose frequency increased in the90s (log-freq < 0), that is, the ones that are morepopular nowadays, and those whose frequency in-stead decreased in the 90s (log-freq > 0), that is,the ones that were more popular in the 60s, we canmake some interesting observations (see Table 2).Remarkably, similarity performs better for the wordsthat are popular nowadays while the frequency-based measure performs better for the words that69were popular in the 60s.We can see the origin of this peculiar asymme-try in behavior of similarity and frequency measuresin the following phenomenon.
As we already men-tioned, if a word became popular, the reason can bea new sense it acquired (a lot of technological termsare of this kind: ?disk?, ?address?, etc).
The changein such words, that are characterized by a significantgrowth in frequency (log-freq  0), is detected bythe human judges, as well as by the similarity mea-sure.
However, other cases such as ?spine?, ?smok-ing?
are also characterized by a significant growthin frequency, but no semantic change was reportedby raters (nor by the similarity measure).
If wordfrequency instead decreases, intuitively, a changein word meaning is less probable.
These intuitionstogether can explain the behavior of the frequencymeasure: for the test set as a whole its performanceis quite high, as it captures this asymmetrical dis-tribution of words that change meanings, despite itsfailure to reliably indicate semantic change for in-dependent words.
A strong evidence for this inter-pretation is also that, if the frequency measure ismade symmetric, that is, equal for the words thatdecreased and the ones that increased in frequency,it dramatically drops in performance, showing a cor-relation of just 0.04 with human ranking.Some interesting observation regarding the per-formance of the similarity measure can be made af-ter accurate investigation of ?false-positive?
exam-ples ?
the ones that have low similarity but wereranked as ?not changed?
by raters ?
like ?sleep?and ?parent?.
It is enough to have a look at theirhighest weighted co-occurrences to admit that thecontext of their usage has indeed changed (Table3).
These examples show the difference betweenthe phenomenon of semantic change in linguisticsand the case of context change.
It is well knownthat the different contexts that distributional seman-tics catches do not always directly refer to what lin-guists would consider distinct senses (Reisinger andMooney, 2010).
Most people would agree that theword ?parent?
has the same meaning now as it had40 years before, still the social context in which itis used has evidently changed, reflected by the morefrequent ?single parent family(ies)?
collocate foundin the 90s.
The same is true for ?sleep?, whose usagecontext did not change radically, but might have a?parent?
?sleep?60s p. company 2643 deep s. 3803p.
education 1905 s. well 1403p.
corporation 1617 cannot s. 1124p.
material 1337 long s. 1102p.
body 1082 sound s. 1101p.
compound 818 dreamless s. 844common p. 816 much s. 77090s p. families 17710 REM s. 20150single p. 10724 s. apnea 14768p.
company 8367 deep s. 8482p.
education 5884 s. disorders 8427p.
training 5847 s. deprivation 6108p.
involvement 5591 s. disturbances 5973p.
family 5042 s. disturbance 5251Table 3: Examples of the top weighted 2-grams contain-ing ?sleep?
and ?parent?.more prominent negative orientation.The distributional similarity measure capturestherefore two kinds of phenomena: the semanticchange in its linguistic definition, that is, change ofmeaning or acquiring a new sense (e.g., ?virus?, ?vir-tual?
), but also the change in the main context inwhich the word is used.
The latter, in turn, can bean important preliminary evidence of the onset ofmeaning change in its traditional sense, accordingto recent studies on language change (Traugott andDasher, 2002).
Moreover, context changes have cul-tural and social origins, and therefore the similaritymeasure can also be used for collecting evidence ofinterest to the humanities and social sciences.6 ConclusionsIn this paper we introduced and evaluated anovel automatic approach for measuring semanticchange with a distributional similarity model.
Thesimilarity-based measure produces good results, ob-taining high correlation with human judgements ontest data.
The study also suggests that the methodcan be suitable to detect both ?proper?
semanticchange of words, and cases of major diachronic con-text change.
Therefore, it can be useful for historicallinguistic studies as well as for NLP tasks such asnovel sense detection.
Some interesting phenomenarelated to changes in relative frequency were alsodiscovered, and will be the object of further investi-gations.70ReferencesMarco Baroni, Alessandro Lenci.
2010.
Distributionalmemory: A general framework for corpus-based se-mantics.
Computational Linguistics, 36(4):673?721.MIT Press, Cambridge, MA, USA.John A. Bullinaria, Joseph P. Levy.
2007.
ExtractingSemantic Representations from Word Co-occurrenceStatistics: A Computational Study.
Behavior ResearchMethods, 39: 510-526.Paul Cook, Suzanne Stevenson.
2010.
Automaticallyidentifying changes in the semantic orientation ofwords.
Proceedings of the 7th International Confer-ence on Language Resources and Evaluation.
Valletta,Malta: 28?34.Katrin Erk.
2006.
Unknown word sense detection as out-lier detection.
Proceedings of the Human LanguageTechnology of the North American Chapter of the ACL.New York, USA: 128?135.Stefan Evert.
2008.
Corpora and collocations.
In A.Ldeling and M. Kyt (eds.
), Corpus Linguistics.
An In-ternational Handbook, article 58.
Mouton de Gruyter,Berlin.Hilpert Martin, Stefan Th.
Gries.
2009.
Assessingfrequency changes in multi-stage diachronic corpora:applications for historical corpus linguistics and thestudy of language acquisition.
Literary and Linguis-tic Computing, 34(4): 385-40.Winfred P. Lehmann.
1992.
Historical linguistics: an in-troduction.
(3.
ed.)
Routledge & Kegan Paul, London.Jean-Baptiste Michel*, Yuan Kui Shen, AvivaPresser Aiden, Adrian Veres, Matthew K. Gray,William Brockman, The Google Books Team,Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Pe-ter Norvig, Jon Orwant, Steven Pinker, Martin A.Nowak, and Erez Lieberman Aiden*.
2010.
Quanti-tative Analysis of Culture Using Millions of DigitizedBooks.
Science (Published online ahead of print:12/16/2010).Joseph Reisinger, Raymond Mooney.
2010.
A MixtureModel with Sharing for Lexical Semantics.
Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing.
MIT, Massachusetts, USA:1173?1182.Eyal Sagi, Stefan Kaufmann, Brady Clark.
2009.
Se-mantic Density Analysis: Comparing Word Meaningacross Time and Phonetic Space.
Proceedings of theEACL 2009 Workshop on GEMS: Geometrical Mod-els of Natural Language Semantics.
Athens, Greece:104?111.Elizabeth C. Traugott, Richard B. Dasher.
2002.
Reg-ularity in Semantic Change.
Cambridge UniversityPress.Peter Turney, Patrick Pantel.
2010.
From Frequency toMeaning: Vector Space Models of Semantics.
Journalof Artificial Intelligence Research (JAIR), 37(1):141-188.
AI Access Foundation.71
