Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 178?182, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsLIMSI : Cross-lingual Word Sense Disambiguation usingTranslation Sense ClusteringMarianna ApidianakiLIMSI-CNRSRue John Von Neumann91403 Orsay Cedex, Francemarianna@limsi.frAbstractWe describe the LIMSI system for theSemEval-2013 Cross-lingual Word Sense Dis-ambiguation (CLWSD) task.
Word senses arerepresented by means of translation clustersin different languages built by a cross-lingualWord Sense Induction (WSI) method.
OurCLWSD classifier exploits the WSI output forselecting appropriate translations for targetwords in context.
We present the design of thesystem and the obtained results.1 IntroductionThis paper describes the LIMSI system that partici-pated in the Cross-Lingual Word Sense Disambigua-tion (CLWSD) task of SemEval-2013.
The goal ofCLWSD is to predict semantically correct transla-tions for ambiguous words in context (Resnik andYarowsky, 2000; Carpuat and Wu, 2007; Apidi-anaki, 2009).
The CLWSD task of the SemEval-2013evaluation campaign is a lexical sample task for En-glish nouns and is divided into two subtasks: thebest subtask where systems are asked to provide aunique good translation for words in context; theout-of-five (oof) subtask where systems can proposeup to five semantically related translations for eachtarget word instance (Lefever and Hoste, 2013).
TheCLWSD lexical sample contains 20 nouns and thetest set is composed of 50 instances per noun.
Sys-tem performance is evaluated by comparing the sys-tem output to a set of gold standard annotations infive languages: French, Spanish, Italian, Dutch andGerman.
Participating systems have to provide con-textually appropriate translations for target words incontext in each or a subset of the target languages.We apply the CLWSD method proposed by Apid-ianaki (2009) to three bilingual tasks: English-Spanish, English-French and English-Italian.
Themethod exploits the translation clusters generated inthe three target languages by a cross-lingual WordSense Induction (WSI) method.
The WSI methodclusters the translations of target words in a parallelcorpus using source language context vectors.
Thesame vectors are exploited during disambiguation inorder to select the most appropriate translations fornew instances of the target words in context.2 System Description2.1 Translation clusteringContrary to monolingual WSI methods which groupthe instances of the words into clusters describ-ing their senses, the cross-lingual WSI method usedhere clusters the translations of words in a paral-lel corpus.
The corpus used for French consistsof the English-French parts of Europarl (version 7)(Koehn, 2005) and of the JRC-Acquis corpus (Stein-berger et al 2006), joined together.
For English-Spanish and English-Italian we only use the corre-sponding parts of Europarl.
The corpora are firsttokenized and lowercased using the Moses scripts,then lemmatized and tagged by part-of-speech (PoS)using the TreeTagger (Schmid, 1994).
Words in thecorpus are replaced by a lemma and PoS tag pair be-fore word alignment, to resolve categorical ambigu-ities in context.
The corpus is aligned in both trans-lation directions with GIZA++ (Och and Ney, 2000)178Target word French Spanish Italianrange{ensemble, diversite?, palette,nombre} {domaine} {porte?e}{e?ventail, nombre, gamme, se?rie,ensemble}{gama, serie, abanico,diversidad, variedad, espectro,conjunto} {cantidad, alcance,a`mbito, nu?mero, tipo, espectro,rango} {amplitud}{serie, gamma, spettro, numero,ventaglio} {ampiezza, portata}{settore, ambito}{diversita?, fascia}mood{climat, atmosphe`re}, {esprit,atmosphe`re, ambiance, humeur}{opinion} {volonte?}
{attitude}{clima, atmo`sfera, ambiente}{a`nimo, sentimiento} {talante}{a`nimo, clima, ambiente}{a`nimo, humor, ambiente}{clima} {atmosfera}{chiarezza, predisposizione}{opinione} {atteggiamento}mission{ope?ration, mandat}{de?le?gation, commission}{de?le?gation, ta?che, voyage,ope?ration}{funcio?n, cometido, objetivo,tarea} {viaje, tarea, delegacio?n}{tarea, mandato, cometido}{mandato, obiettivo, compito,mission, funzione, operazione,}{viaggio, mission, commissione,delegazione}Table 1: Sense clusters generated by the WSI method in the three languages.and three bilingual lexicons are built from the align-ment results (one for each language pair) containingintersecting alignments.
The lexicons contain nountranslations of each English target word in the threelanguages.
We keep French translations that trans-late the target words at least 10 times in the train-ing corpus; for Spanish and Italian, where the corpuswas smaller, the translation frequency threshold wasset to 5.For each translation Ti of a word w, we extract thecontent words that occur in the same sentence as wwhenever it is translated by Ti.
These constitute thefeatures of the vector built for the translation.
Let Nbe the number of features retained for each Ti fromthe corresponding source contexts.
Each feature Fj(1 ?
j ?
N ) receives a total weight tw(Fj , Ti) de-fined as the product of the feature?s global weight,gw(Fj), and its local weight with that translation,lw(Fj , Ti).
The global weight of a feature Fj is afunction of the number Ni of translations (Ti?s) towhich Fj is related, and of the probabilities (pij) thatFj co-occurs with instances of w translated by eachof the Ti?s:gw(Fj) = 1?
?Ti pij log(pij)Ni(1)Each of the pij?s is computed as the ratio betweenthe co-occurrence frequency of Fj with w whentranslated as Ti, denoted as cooc frequency(Fj , Ti),and the total number of features (N ) seen with Ti:pij =cooc frequency(Fj , Ti)N(2)The local weight lw(Fj , Ti) between Fj and Ti di-rectly depends on their co-occurrence frequency:lw(Fj , Ti) = log(cooc frequency(Fj , Ti)) (3)The pairwise similarity of the translation vectorsis calculated using the Weighted Jaccard Coeffi-cient (Grefenstette, 1994).
The similarity score ofeach translation pair is compared to a threshold lo-cally defined for each w, which serves to distinguishstrongly related translations from semantically un-related ones.
The semantically related translationsof a word w are then grouped into clusters.
Trans-lation pairs with a score above the threshold form aset of initial clusters that might be further enrichedwith other translations through an iterative proce-dure, provided that there are other translations thatare strongly related to the elements in the cluster.1The clustering stops when all the translations of whave been clustered and all their relations have beenchecked.
The algorithm performs a soft clusteringso translations might be found in different clusters.Final clusters are characterized by global connectiv-ity, meaning that all their elements are linked by per-tinent relations.
Table 1 gives examples of clustersgenerated for CLWSD target words in the three lan-guages.
The clusters group translations carrying thesame sense and their overlaps describe relations be-tween senses.
The translation clusters serve as thetarget words?
candidate senses from which one hasto be selected during disambiguation.1The thresholding procedure and the clustering algorithmare described in detail in Apidianaki and He (2010).179Subtask MetricSpanish French ItalianLIMSI BaselineBestsystem LIMSI BaselineBestsystem LIMSI BaselineBestsystemBestP/R 24,7 23,23 32,16 24,56 25,73 30,11 21,2 20,21 25,66Mode P/R 32,09 27,48 37,11 22,16 20,19 26,62 23,06 19,88 31,61OOFP/R 49,01 53,07 61,69 45,37 51,35 59,8 40,25 42,62 53,57Mode P/R 51,41 57,34 64,65 39,54 47,42 57,57 47,21 41,68 56,61OOF P/R 98,6 - - 101,75 - - 90,23 - -(dupl) Mode P/R 51,41 - - 39,54 - - 47,21 - -Table 2: Results at the SemEval 2013 CLWSD task.2.2 Word Sense DisambiguationThe vectors used for clustering the translations alsoserve for disambiguating new instances of the tar-get words in context.
The new contexts are tok-enized, lowercased, PoS tagged and lemmatized tofacilitate comparison with the vectors.
We use thefeatures shared by each pair of clustered transla-tions, or the vector corresponding to the translationin an one-element cluster.
If no CFs exist betweenthe new context and a pair of translations, WSD isperformed by comparing context information sep-arately to the vector of each clustered translation.Once the common features (CFs) between the vec-tors and the new context are identified, a score iscalculated corresponding to the mean of the weightsof the CFs with the translations (weights assigned tothe features during WSI).
In formula 4, CFj is theset of CFs and NCF is the number of translations Ticharacterized by a CF.wsd score =?NCFi=1?j w(Ti, CFj)NCF ?
|CFj |(4)The cluster containing the highest ranked transla-tion or translation pair is selected and assigned tothe new target word instance.
If the translations arepresent in more than one clusters, a new score is cal-culated using equation 4 and by taking into accountthe weights of the CFs with the other translations(Ti?s) in the cluster.3 EvaluationSystems participating to the CLWSD task have toprovide the most plausible translation for a wordin context in the best subtask, and five semanti-cally correct translations in oof.
The baselines pro-vided by the organizers are based on the output ofGIZA++ alignments on Europarl.
The best base-line corresponds to the most frequent translation ofthe target word in the corpus and the oof baselineto the five most frequent translations.
Our CLWSDsystem makes predictions in three languages for all1000 test instances.
If the selected cluster containsfive translations, all of them are proposed in theoof subtask while if it is bigger, the five most fre-quent translations are selected.
In case of smallerclusters, the best translation is repeated in the out-put until reaching five suggestions.
Duplicate sug-gestions were allowed in previous cross-lingual Se-mEval tasks as a means to boost translations withhigh confidence (Mihalcea et al 2010).
However,as in this year?s CLWSD task the oof system outputhas been post-processed by the organizers to keeponly unique translations, the number of predictionsmade by our system for some words has been signif-icantly reduced.
This has had a negative impact onthe oof results, as we will show in the next section.For selecting best translations, each translation ofa target word w is scored separately by comparing itsvector to the new context.
In case the highest-rankedtranslation has a score lower than 1, the system fallsback to using the most frequent translation (MFT).To note that frequency information differs from theone used in the MFT baseline because words in ourcorpus were replaced by a lemma and PoS tag pairprior to alignment.
The discrepancy is more ap-parent in French where MFT is the most frequenttranslation of the target word in the joint Europarland JRC-Acquis corpus.
Five teams participated tothe CLWSD task with a varying number of systems:twelve systems provided output for Spanish and tenfor French and Italian.1804 ResultsThe results obtained by our system for the bestand oof evaluations in the three languages (Span-ish, French and Italian) are presented in Table 2.
Wecontrast them with the baselines provided by the or-ganizers and with the score of the system that per-formed best in each subtask.
Our system made sug-gestions for all test instances, so recall (R) coincideswith precision (P).
The baselines are quite challeng-ing, as noted in Lefever and Hoste (2010), especiallythe oof one which contains the five most frequentEuroparl translations.
These often correspond to themost frequent translations from different sense clus-ters and cover multiple senses of the target word.Our system outperforms the best baseline in alllanguages except for French, where the best scorelies near below the baseline.
This is not surprisinggiven that the training corpus for French is the jointEuroparl and JRC-Acquis corpus, which causes adiscrepancy between the selected best translationsand the baseline.
The mode precision and recallscores reflect the capacity of the system to predictthe translations that were most frequently selectedby the annotators for each instance and are thus con-sidered as the most plausible ones.
Our system out-performs the mode best baselines for all languages.In the oof task, the system has been penalizedby the elimination of duplicate translations fromthe output after submission.
In previous work, theCLWSD system gave very good results when applied,with some slight variations, to the out-of-ten subtaskof the SemEval-2010 Cross-Lingual Lexical Substi-tution task where duplicates served to promote trans-lations with high confidence (Mihalcea et al 2010;Apidianaki, 2011).
Here, after the post-processingstep, oof suggestions contain in many cases less thanfive translations which explains the low scores.
InTable 2 we provide oof results before and after post-processing the output and show how the system wasaffected by this change in evaluation.
By boostingplausible translations, precision and recall scores gethigher while mode scores are naturally not affected.2As the other systems might have been impacted todifferent extents by this change, we cannot estimate2Precision scores might be inflated, as in the case of French,because the credit for each item is not divided by the number ofpredictions and the annotation frequencies are used.how this affects the global system ranking.5 Discussion and future workWe presented a CLWSD system that uses translationclusters as candidate senses.
Disambiguation is per-formed by comparing the feature vectors that servedfor clustering to the context of new target word in-stances.
We observe that the use of a bigger cor-pus ?
as in the case of French ?
not only does nothelp in this task but actually has a negative impacton the results.
This is due to the inclusion of transla-tions that are not present in the gold standard (builtfrom Europarl) and to the discrepancy between mostfrequent translations in the large corpus and the Eu-roparl MFT baselines.
This discrepancy affects allthree languages, as words in the training corporawere replaced by lemma and PoS tag pairs prior toalignment.It is important to note that our CLWSD method ex-ploits the output of another unsupervised semanticanalysis method (WSI) which groups the translationsinto clusters.
This is an important feature of the sys-tem and affects the results in two ways.
First, thetranslation clusters of a word constitute its candi-date senses from which the CLWSD method selectsthe most appropriate one for a given context.
Thismeans that no variation regarding the contents of acluster is permitted and that different instances aretagged by the same set of translations, contrary tothe gold standard annotations which might, at thesame time, be very close and contain some varia-tions.
In the system output, this is the case onlywhen overlapping clusters are selected for differentinstances.
Moreover, given that the WSI method isautomatic and that the clusters are not manually val-idated, the noise that might be introduced duringclustering is propagated and reflected in the disam-biguation results.
So, if a cluster contains one ormore noisy translations, these occur in the disam-biguation output and naturally count as wrong pre-dictions.
However, in an application setting likeMachine Translation (MT), the translation clusterscould be filtered using information from the targetlanguage context.
Future work will focus on inte-grating this method into MT systems and examiningways for optimally taking advantage of CLWSD pre-dictions in this context.181ReferencesMarianna Apidianaki and Yifan He.
2010.
An algorithmfor cross-lingual sense clustering tested in a MT eval-uation setting.
In Proceedings of the 7th InternationalWorkshop on Spoken Language Translation (IWSLT-10), pages 219?226, Paris, France.Marianna Apidianaki.
2009.
Data-driven SemanticAnalysis for Multilingual WSD and Lexical Selectionin Translation.
In Proceedings of the 12th Confer-ence of the European Chapter of the Association forComputational Linguistics (EACL-09), pages 77?85,Athens, Greece.Marianna Apidianaki.
2011.
Unsupervised Cross-Lingual Lexical Substitution.
In Proceedings of theFirst workshop on Unsupervised Learning in NLP inconjunction with EMNLP, pages 13?23, Edinburgh,Scotland, July.
Association for Computational Lin-guistics.Marine Carpuat and Dekai Wu.
2007.
Improving statisti-cal machine translation using word sense disambigua-tion.
In EMNLP-CoNLL, pages 61?72.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publishers,Norwell, MA.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings of MTSummit X, pages 79?86, Phuket, Thailand.Els Lefever and Veronique Hoste.
2010.
SemEval-2010Task 3: Cross-lingual Word Sense Disambiguation.In Proceedings of the 5th International Workshop onSemantic Evaluations (SemEval-2), ACL 2010, pages15?20, Uppsala, Sweden.Els Lefever and Ve?ronique Hoste.
2013.
SemEval-2013Task 10: Cross-Lingual Word Sense Disambiguation.In Proceedings of the 7th International Workshop onSemantic Evaluation (SemEval 2013), in conjunctionwith the Second Joint Conference on Lexical and Com-putational Semantcis (*SEM 2013), pages 63?72, At-lanta, USA.Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010.SemEval-2010 Task 2: Cross-Lingual Lexical Sub-stitution.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluations (SemEval-2), ACL2010, pages 9?14, Uppsala, Sweden.Franz Josef Och and Hermann Ney.
2000.
Im-proved statistical alignment models.
In Proceedingsof the 38th Annual Meeting of the Association forComputational Linguistics (ACL?00), pages 440?447,Hongkong, China.Philip Resnik and David Yarowsky.
2000.
Distinguish-ing Systems and Distinguishing Senses: New Evalua-tion Methods for Word Sense Disambiguation.
Natu-ral Language Engineering, 5(3):113?133.Helmut Schmid.
1994.
Probabilistic Part-of-Speech Tag-ging Using Decision Trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing, pages 44?49, Manchester, UK.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Tomaz?
Erjavec, and Dan Tufis?.
2006.The JRC-Acquis: A multilingual aligned parallel cor-pus with 20+ languages.
In Proceedings of the 5thInternational Conference on Language Resources andEvaluation (LREC?2006), pages 2142?2147.182
