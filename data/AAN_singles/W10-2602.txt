Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 8?15,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsContext Adaptation in Statistical Machine Translation Using Models withExponentially Decaying CacheJo?rg TiedemannDepartment of Linguistics and PhilologyUppsala University, Uppsala/Swedenjorg.tiedemann@lingfil.uu.seAbstractWe report results from a domain adapta-tion task for statistical machine translation(SMT) using cache-based adaptive lan-guage and translation models.
We applyan exponential decay factor and integratethe cache models in a standard phrase-based SMT decoder.
Without the need forany domain-specific resources we obtain a2.6% relative improvement on average inBLEU scores using our dynamic adapta-tion procedure.1 IntroductionMost data-driven approaches to natural lan-guage processing (NLP) are subject to the well-known problem of lack of portability to new do-mains/genres.
Usually there is a substantial dropin performance when testing on data from a do-main different to the training data.
Statistical ma-chine translation is no exception.
Despite its pop-ularity, standard SMT approaches fail to provide aframework for general application across domainsunless appropriate training data is available andused in parameter estimation and tuning.The main problem is the general assumptionof independent and identically distributed (i.i.d.
)variables in machine learning approaches appliedin the estimation of static global models.
Recently,there has been quite some attention to the prob-lem of domain switching in SMT (Zhao et al,2004; Ueffing et al, 2007; Civera and Juan, 2007;Bertoldi and Federico, 2009) but ground breakingsuccess is still missing.
In this paper we reportour findings in dynamic model adaptation usingcache-based techniques when applying a standardmodel to the task of translating documents from avery different domain.The remaining part of the paper is organized asfollows: First, we will motivate the chosen ap-proach by reviewing the general phenomenon ofrepetition and consistency in natural language text.Thereafter, we will briefly discuss the dynamic ex-tensions to language and translation models ap-plied in the experiments presented in the secondlast section followed by some final conclusions.2 MotivationDomain adaptation can be tackled in various ways.An obvious choice for empirical systems is to ap-ply supervised techniques in case domain-specifictraining data is available.
It has been shown thatsmall(er) amounts of in-domain data are suffi-cient for such an approach (Koehn and Schroeder,2007).
However, this is not really a useful alter-native for truly open-domain systems, which willbe confronted with changing domains all the timeincluding many new, previously unknown onesamong them.There are also some interesting approaches todynamic domain adaptation mainly using flexiblemixture models or techniques for the automatic se-lection of appropriate resources (Hildebrand et al,2005; Foster and Kuhn, 2007; Finch and Sumita,2008).
Ideally, a system would adjust itself to thecurrent context (and thus to the current domain)without the need of explicit topic mixtures.
There-fore, we like to investigate techniques for generalcontext adaptation and their use in out-of-domaintranslation.There are two types of properties in natural lan-guage and translation that we like to explore.
Firstof all, repetition is very common ?
much morethan standard stochastic language models wouldpredict.
This is especially true for content words.See, for instance, the sample of a medical docu-ment shown in figure 1.
Many content words arerepeated in close context.
Hence, appropriate lan-guage models should incorporate changing occur-rence likelihoods to account for these very com-mon repetitions.
This is exactly what adaptive lan-guage models try to do (Bellegarda, 2004).8?They may also have episodes of depression .
Abilify isused to treat moderate to severe manic episodes and toprevent manic episodes in patients who have responded tothe medicine in the past .
The solution for injection is usedfor the rapid control of agitation or disturbed behaviourwhen taking the medicine by mouth is not appropriate .The medicine can only be obtained with a prescription .
?Figure 1: A short example from a document fromthe European Medicines Agency (EMEA)Another known fact about natural language is con-sistency which is also often ignored in statisticalmodels.
A main problem in most NLP applica-tions is ambiguity.
However, ambiguity is largelyremoved within specific domains and contexts inwhich ambiguous items have a well-defined andconsistent meaning.
This effect of ?meaning con-sistency?
also known as the principle of ?one senseper discourse?
has been applied in word sensedisambiguation with quite some success (Gale etal., 1992).
For machine translation this meansthat adapting to the local domain and sticking toconsistent translation choices within a discourseseems to be better than using a global static modeland context independent translations of sentencesin isolation.
For an illustration, look at the exam-ples in figure 2 taken from translated movie subti-tles.
Interesting is not only the consistent meaningof ?honey?
within each discourse but also the con-sistent choice among equivalent translations (syn-onyms ?a?lskling?
och ?gumman?).
Here, the dis-tinction between ?honey?
and ?sweetheart?
hasbeen transferred to Swedish using consistent trans-lations.The 10 commandments Kerd ma luiTo some land flowing withmilk and honey!Till ett land fullt av mjo?lkoch honung.I?ve never tasted honey.Jag har aldrig smakat ho-nung....Mari honey ...Mari, gumman ...Sweetheart, where areyou going?A?lskling, var ska du?...Who was that, honey?Vem var det, gumman?Figure 2: Consistency in subtitle translationsIn summary: Repetition and consistency are veryimportant when modeling natural language andtranslation.
A proper translation engine shouldmove away from translating sentences in isolationbut should consider wider context to include thesediscourse phenomena.
In the next section we dis-cuss the cache-based models that we implementedto address this challenge.3 Cache-based ModelsThe main idea behind cache-based language mod-els (Kuhn and Mori, 1990) is to mix a large global(static) language model with a small local (dy-namic) model estimated from recent items in thehistory of the input stream.
It is common to usesimple linear interpolations and fixed cache sizes k(100-5000 words) to achieve this: P (wn|history) =(1?
?
)Pn?gram(wn|history) + ?Pcache(wn|history)Due to data sparseness one is usually restrictedto simple cache models.
However, unigram mod-els are often sufficient and smoothing is not nec-essary due to the interpolation with the smoothedbackground model.
From the language model-ing literature we know that caching is an effi-cient way to reduce perplexity (usually leading tomodest improvements on in-domain data and largeimprovements on out-of-domain data).
Table 1shows this effect yielding 53% reduction of per-plexity on our out-of-domain data.different settings for ?cache 0.05 0.1 0.2 0.30 376.1 376.1 376.1 376.150 270.7 259.2 256.4 264.9100 261.1 246.6 239.2 243.3500 252.2 233.1 219.1 217.01000 240.6 218.0 199.2 192.92000 234.6 209.6 187.9 179.15000 235.3 209.1 185.8 175.810000 237.6 210.7 186.6 176.120000 239.9 212.5 187.7 176.7Table 1: Perplexity of medical texts (EMEA) us-ing a language model estimated on Europarl and aunigram cache componentEven though a simple unigram cache is quite ef-fective it now requires a careful optimization of itssize.
In order to avoid the dependence on cachesize and to account for recency a decaying factorcan be introduced (Clarkson and Robinson, 1997):Pcache(wn|wn?k..wn?1) ?1Zn?1?i=n?kI(wn = wi)e??
(n?i)Here, I(A) = 1 if A is true and 0 otherwise.
Zis a normalizing constant.
Figure 3 illustrates theeffect of cache decay on our data yielding anothersignificant reduction in perplexity (even though91681701721741761781801821840  0.0005  0.001  0.0015  0.002  0.0025  0.003perplexitydecay ratiocache size = 2000cache size = 5000cache size = 10000Figure 3: Out-of-domain perplexity using lan-guage models with decaying cache.the improvement is much less impressive than theone obtained by introducing the cache).The motivation of using these successful tech-niques in SMT is obvious.
Language models playa crucial role in fluency ranking and a better fitto real data (supporting the tendency of repetition)should be preferred.
This, of course, assumes cor-rect translation decisions in the history in our SMTsetting which will almost never be the case.
Fur-thermore, simple cache models like the unigrammodel may wrongly push forward certain expres-sions without considering local context when us-ing language models to discriminate between var-ious translation candidates.
Therefore, success-fully applying these adaptive language models inSMT is surprisingly difficult (Raab, 2007) espe-cially due to the risk of adding noise (leading toerror propagation) and corrupting local dependen-cies.In SMT another type of adaptation can be ap-plied: cache-based adaptation of the translationmodel.
Here, not only the repetition of contentwords is supported but also the consistency oftranslations as discussed earlier.
This techniquehas already been tried in the context of interactivemachine translation (Nepveu et al, 2004) in whichcache features are introduced to adapt both the lan-guage model and the translation model.
However,in their model they require an automatic align-ment of words in the user edited translation and thesource language input.
In our experiments we in-vestigate a close integration of the caching proce-dure into the decoding process of fully automatictranslation.
For this, we fill our cache with trans-lation options used in the best (final) translationhypothesis of previous sentences.
In our imple-mentation of the translation model cache we useagain a decaying factor in order to account for re-cency.
For known source language items (fn forwhich translation options exist in the cache) thefollowing formula is used to compute the cachetranslation score:?cache(en|fn) =?Ki=1 I(?en, fn?
= ?ei, fi?)
?
e?
?i?Ki=1 I(fn = fi)Unknown items receive a score of zero.
This scoreis then used as an additional feature in the standardlog-linear model of phrase-based SMT1.4 ExperimentsOur experiments are focused on the unsuperviseddynamic adaptation of language and translationmodels to a new domain using the cache-basedmixture models as described above.
We applythese techniques to a standard task of translat-ing French to English using a model trained onthe publicly available Europarl corpus (Koehn,2005) using standard settings and tools such as theMoses toolkit (Koehn et al, 2007), GIZA++ (Ochand Ney, 2003) and SRILM (Stolcke, 2002).
Thelog-linear model is then tuned as usual with mini-mum error rate training (Och, 2003) on a separatedevelopment set coming from the same domain(Europarl).
We modified SRILM to include a de-caying cache model and implemented the phrasetranslation cache within the Moses decoder.
Fur-thermore, we added the caching procedures andother features for testing the adaptive approach.Now we can simply switch the cache models onor off using additional command-line argumentswhen running Moses as usual.4.1 Experimental SetupFor testing we chose to use documents from themedical domain coming from the EMEA corpusthat is part of the freely available collection ofparallel corpora OPUS2 (Tiedemann, 2009).
Thereason for selecting this domain is that these doc-uments include very consistent instructions andrepetitive texts which ought to favor our cachingtechniques.
Furthermore, they are very different1Logarithmic values are used in the actual implementationwhich are floored to a low constant in case of zero ?
scores.2The OPUS corpus is available at this URL:http://www.let.rug.nl/tiedeman/OPUS/.10from the training data and, thus, domain adapta-tion is very important for proper translations.
Werandomly selected 102 pairs of documents with al-together 5,478 sentences.
Sentences have an aver-age length of about 19 tokens with a lot of varia-tion among them.
Documents are compiled fromthe European Public Assessment Reports (EPAR)which reflect scientific conclusions at the end of acentralized evaluation procedure for medical prod-ucts.
They include a lot of domain-specific ter-minology, short facts, lists and tables but also de-tailed textual descriptions of medicines and theiruse.
The overall lowercased type/token ratio in theEnglish part of our test collection is about 0.045which indicates quite substantial repetitions in thetext.
This ratio is, however, much higher for indi-vidual documents.In the experiment each document is processedindividually in order to apply appropriate dis-course breaks.
The baseline score for applying astandard phrase-based SMT model yields an aver-age score of 28.67 BLEU per document (28.60 persentence) which is quite reasonable for an out-of-domain test.
Intuitively, the baseline performanceshould be crucial for the adaptation.
As discussedearlier the cache-based approach assumes correcthistory and better baseline performance should in-crease the chance of adding appropriate items tothe cache.4.2 Applying the LM CacheIn our first experiment we applied a decaying uni-gram cache in the language model.
We performeda simple linear search on a separate developmentset for optimizing the interpolation weight whichgave as a value of ?
= 0.001.
The size of the cachewas set to 10,000 and the decay factor was set to?
= 0.0005 (according to our findings in figure3).
The results on our test data compared to thestandard model are illustrated (with white boxes)in figure 4.There is quite some variation in the effect of thecache LM on our test documents.
The translationsof most EMEA documents could be improved ac-cording to BLEU scores, some of them substan-tially, whereas others degraded slightly.
Note thatthe documents differ in size and some of them arevery short which makes it a bit difficult to interpretand directly compare these scores.
On average theBLEU score is improved by 0.43 points per doc-ument and 0.39 points per sentence.
This might-2-1012345BLEUscoredifferencecache LM vs. standard LMcache TM vs. standard TMFigure 4: The differences in BLEU between astandard model and models with cache for 102EMEA documents (sorted by overall BLEU scoregain ?
see figure 5)be not as impressive as we were hoping for af-ter the tremendous perplexity reduction presentedearlier.
However, considering the simplicity of theapproach that does not require any additional re-sources nor training it is still a valuable achieve-ment.4.3 Applying the TM CacheIn the next experiment we tested the effect of theTM cache on translation quality.
Using our hy-pothesis of translation consistency we expectedanother gain on our test set.
In order to reduceproblems of noise we added two additional con-straints: We only cache phrases that contain atleast one word longer than 4 characters (a simplis-tic attempt to focus on content words rather thanfunction words) and we only cache translation op-tions for which the transition costs (of adding thisoption to the current hypothesis) in the global de-coding model is larger than a given threshold (anattempt to use some notion of confidence for thecurrent phrase pair; in our experiments we used alog score of -4).
Using this setup and applying thephrase cache in decoding we obtained the resultsillustrated with filled boxes in the figure 4 above.Again, we can observe a varied outcome butmostly improvements.
The impact of the phrasetranslation cache (with a size of 5,000 items) is notas strong as for the language model cache whichmight be due to the rather conservative settings(?
= 0.001, ?
= 0.001) and the fact that matchingphrase pairs are less likely to appear than matchingtarget words.
On average the gain is about 0.27511BLEU points per document (0.26 per sentence).4.4 Combining the Cache ModelsFinally, we applied both types of cache in onecommon system using the same settings from theindividual runs.
The differences to the baselinemodel are shown in figure 5.-2-1012345BLEUscoredifferencecache models vs. standard modelsFigure 5: The BLEU score differences between astandard model and a model with cache for bothTM and LM (sorted by BLEU score gain).In most cases, applying the two types of cachetogether has a positive effect on the final BLEUscore.
Now, we see only a few documents witha drop in translation performance.
On averagethe gain has increased to about 0.78 BLEU pointsper document (0.74 per sentence) which is about2.7% relative improvement compared to the base-line (2.6% per sentence).5 DiscussionOur experiments seem to suggest that cachingcould be a way to improve translation quality ona new domain.
However, the differences are smalland the assumption that previous translation hy-potheses are good enough to be cached is risky.One obvious question is if the approach is ro-bust enough to be helpful in general.
If that isthe the case we should also see positive effectson in-domain data where a cache model could ad-just to topical shifts within that domain.
In orderto test this ability we ran an experiment with the2006 test data from the workshop on statistical ma-chine translation (Koehn and Monz, 2006) usingthe same models and settings as above.
This re-sulted in the following scores (lowercased BLEU):BLEUbaseline = 32.46 (65.0/38.3/25.4/17.6, BP=0.999)BLEUcache = 31.91 (65.1/38.1/25.1/17.3, BP=0.991)Clearly, the cache models failed on this test eventhough the difference between the two runs is notlarge.
There is a slight improvement in unigrammatches (first value in brackets) but a drop onlarger n-gram scores and also a stronger brevitypenalty (BP).
This could be an effect of the sim-plicity of the LM cache (a simple unigram model)which may improve the choice of individual lexi-cal items but without respecting contextual depen-dencies.One difference is that the in-domain data wastranslated in one step without clearing the cache attopical shifts.
EMEA documents were translatedone by one with empty caches at the beginning.
Itis now the question if proper initialization is es-sential and if there is a correlation between docu-ment length and the effect of caching.
How muchdata is actually needed to take advantage of cacheditems and is there a point where a positive effectdegrades because of topical shifts within the docu-ment?
Let us, therefore, have a look at the relationbetween document length and BLEU score gain inour test collection (figure 6).-2-10123450  200  400  600  800  1000  1200  1400  1600  1800  2000BLEUscoredifferencedocument lengthFigure 6: Correlation between document lengths(in number of tokens) and BLEU score gains withcaching.Concluding from this figure there does not seemto be any correlation.
The length of the documentdoes not seem to influence the outcome.
Whatelse could be the reason for the different behaviouramong our test documents?
One possibility is thequality of baseline translations assuming that bet-ter performance increases the chance of cachingcorrect translation hypotheses.
Figure 7 plots theBLEU score gains in comparison with the baselinescores.Again, no immediate correlation can be seen.12-2-101234510  15  20  25  30  35  40  45BLEUscoredifferencebaseline BLEUFigure 7: Correlation between baseline BLEUscores and BLEU score gains with cachingThe baseline performance does not seem to giveany clues for a possible success of caching.
Thiscomes as a surprise as our intuitions suggested thatgood baseline performance should be essential forthe adaptive approach.Another reason for their success should be theamount of repetition (especially among contentwords) in the documents to be translated.
An in-dication for this can be given by type/token ratiosassuming that documents with lower ratios containa larger amount of repetitive text.
Figure 8 plotsthe type/token ratios of all test documents in com-parison with the BLEU score gains obtained withcaching.-2-10123450.25  0.3  0.35  0.4  0.45  0.5  0.55BLEUscoredifferencetype/token ratioFigure 8: Correlation between type/token ratiosand BLEU score gains with cachingOnce again there does not seem to be any obvi-ous correlation.
So far we could not identify anyparticular property of documents that might helpto reliably predict the success of caching.
The an-swer is probably a combination of various factors.Further experiments are needed to see the effect ondifferent data sets and document types.Note that some results may also be an artifactof the automatic evaluation metrics applied.
Qual-itative evaluations using manual inspection couldprobably reveal important aspects of the cachingapproach.
However, tracing changes caused bycaching is rather difficult due to the interactionwith other factors in the global decoding process.Some typical cases may still be identified.
Fig-ure 9 shows an example of a translation that hasbeen improved in the cached model by making thetranslation more consistent (this is from a docu-ment that actually got a lower BLEU score in theend with caching).baseline: report ( evaluation of european public epar )vivanzain the short epar publicthis document is a summary of the european public toevaluation report ( epar ) .cache: report european public assessment ( epar )vivanzaepar to sum up the publicthis document is a summary of the european public as-sessment report ( epar ) .reference: european public assessment report ( epar )vivanzaepar summary for the publicthis document is a summary of the european public as-sessment report ( epar ) .Figure 9: A translation improved by caching.Other improvements may not be recognized by au-tomatic evaluation metrics and certain acceptabledifferences may be penalized.
Look, for instance,at the examples in figure 10.This is, of course, not a general claim thatcache-based translations are more effected by thisproblem than, for example, the baseline system.However, this could be a direction for further in-vestigations to quantify these issues.6 ConclusionsIn this paper we presented adaptive language andtranslation models that use an exponentially de-caying cache.
We applied these models to a do-main adaptation task translating medical docu-ments with a standard model trained on Europarl.On average the dynamic adaptation approach ledto a gain of about 2.6% relative BLEU points persentence.
The main advantage of this approach isthat it does not require any domain-specific train-13baseline: the medication is issued on orders .cache: the medication is issued on prescription-only .reference: the medicine can only be obtained with a prescription .baseline: benefix is a powder keg , and a solvent to dissolve the injection for .cache: benefix consists of a powder and a solvent to dissolve the injection for .reference: benefix is a powder and solvent that are mixed together for injection .baseline: the principle of active benefix is the nonacog alfa ( ix coagulation factor of recombinant ) which favoursthe coagulation blood .cache: the principle of benefix is the nonacog alfa ( ix coagulation factor of recombinant ) which favours thecoagulation blood .reference: benefix contains the active ingredient nonacog alfa ( recombinant coagulation factor ix , which helpsblood to clot ) .baseline: in any case , it is benefix used ?cache: in which case it is benefix used ?reference: what is benefix used for ?baseline: benefix is used for the treatment and prevention of saignements among patients with haemophilia b ( adisorder he?morragique hereditary due to a deficiency in factor ix ) .cache: benefix is used for the treatment and prevention of saignements among patients suffering haemophiliab ( a disorder he?morragique hereditary due to a lack factor in ix ) .reference: benefix is used for the treatment and prevention of bleeding in patients with haemophilia b ( an inher-ited bleeding disorder caused by lack of factor ix ) .baseline: benefix can be used for adults and children over 6 years .cache: benefix can be used for adults and children of more than 6 yearsreference: benfix can be used in adults and children over the age of 6.Figure 10: Examples translations with and without caching.ing, tuning (assuming that interpolation weightsand other cache parameters can be fixed after someinitial experiments) nor the incorporation of anyother in-domain resources.
Cache based adapta-tion can directly be applied to any new domainand similar gains should be possible.
However, ageneral conclusion cannot be drawn from our ini-tial results presented in this paper.
Further exper-iments are required to verify these findings and toexplore the potentials of cache-based techniques.The main obstacle is the invalid assumption thatinitial translations are correct.
The success of theentire method crucially depends on this assump-tion.
Error propagation and the reinforcement ofwrong decisions is the largest risk.
Therefore,strategies to reduce noise in the cache are impor-tant and can still be improved using better selec-tion criteria.
A possible strategy could be to iden-tify simple cases in a first run that can be usedto reliably fill the cache and to use the full cachemodel on the entire text in a second run.
Anotheridea for improvement is to attach weights to cacheentries according to the translation costs assignedby the model.
These weights could easily be incor-porated into the cache scores returned for match-ing items.
In future, we would like to explore theseideas and also possibilities to combine cache mod-els with other types of adaptation techniques.ReferencesJerome R. Bellegarda.
2004.
Statistical languagemodel adaptation: review and perspectives.
SpeechCommunication, 42:93?108.Nicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translationwith monolingual resources.
In StatMT ?09: Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, pages 182?189, Morristown, NJ,USA.
Association for Computational Linguistics.Jorge Civera and Alfons Juan.
2007.
Domain adap-tation in statistical machine translation with mixturemodelling.
In Proceedings of the Second Workshopon Statistical Machine Translation, pages 177?180,Prague, Czech Republic.
Association for Computa-tional Linguistics.P.R.
Clarkson and A. J. Robinson.
1997.
Languagemodel adaptation using mixtures and an exponen-tially decaying cache.
In International Confer-ence on Acoustics, Speech, and Signal Processing(ICASSP), pages 799?802, Munich, Germany.Andrew Finch and Eiichiro Sumita.
2008.
Dynamicmodel interpolation for statistical machine transla-tion.
In StatMT ?08: Proceedings of the Third Work-shop on Statistical Machine Translation, pages 208?215, Morristown, NJ, USA.
Association for Compu-tational Linguistics.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for SMT.
In Proceedings of theSecond Workshop on Statistical Machine Transla-tion, pages 128?135, Prague, Czech Republic.
As-sociation for Computational Linguistics.14William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One sense per discourse.
In HLT?91: Proceedings of the workshop on Speech andNatural Language, pages 233?237, Morristown, NJ,USA.
Association for Computational Linguistics.Almut Silja Hildebrand, Matthias Eck, Stephan Vo-gel, and Alex Waibel.
2005.
Adaptation of thetranslation model for statistical machine translationbased on information retrieval.
In Proceedings ofthe 10th Conference of the European Association forMachine Translation (EAMT), pages 133?142, Bu-dapest.Philipp Koehn and Christof Monz, editors.
2006.
Pro-ceedings on the Workshop on Statistical MachineTranslation.
Association for Computational Lin-guistics, New York City, June.Philipp Koehn and Josh Schroeder.
2007.
Experi-ments in domain adaptation for statistical machinetranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation, pages 224?227,Prague, Czech Republic.
Association for Computa-tional Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InACL ?07: Proceedings of the 45th Annual Meet-ing of the ACL on Interactive Poster and Demon-stration Sessions, pages 177?180, Morristown, NJ,USA.
Association for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of the10th Machine Translation Summit (MT Summit X).Roland Kuhn and Renato De Mori.
1990.
A cache-based natural language model for speech recogni-tion.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 12(6):570?583.Laurent Nepveu, Lapalme, Guy, Langlais, Philippe,and George Foster.
2004.
Adaptive Language andTranslation Models for Interactive Machine Trans-lation.
In Proceedings of the 9th Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 190?197, Barcelona, Spain.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In ACL ?03:Proceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics, pages 160?167, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Martin Raab.
2007.
Language Modeling for MachineTranslation.
VDM Verlag, Saarbru?cken, Germany.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the 7thinternational conference on spoken language pro-cessing (ICSLP 2002), pages 901?904, Denver, CO,USA.Jo?rg Tiedemann.
2009.
News from OPUS - A collec-tion of multilingual parallel corpora with tools andinterfaces.
In Recent Advances in Natural LanguageProcessing, volume V, pages 237?248.
John Ben-jamins, Amsterdam/Philadelphia.Nicola Ueffing, Gholamreza Haffari, and AnoopSarkar.
2007.
Semi-supervised model adaptationfor statistical machine translation.
Machine Trans-lation, 21(2):77?94.Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language model adaptation for statistical machinetranslation with structured query models.
In COL-ING ?04: Proceedings of the 20th international con-ference on Computational Linguistics, page 411,Morristown, NJ, USA.
Association for Computa-tional Linguistics.15
