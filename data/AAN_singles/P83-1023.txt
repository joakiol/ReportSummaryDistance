MENU-BASED NATURAL LANGUAGE UNDERSTANDINGHarry R. Tennant, Kenneth M. Ross,Richard M. Saenz, Craig W. Thompson,and James R. Mi l lerComputer Science LaboratoryCentral Research LaboratoriesTexas Instruments IncorporatedDallas, TexasABSTRACTThis paper describes the NLMenu System, amenu-based natural language understanding system.Rather than requiring the user to type his inputto the system, input to NLMenu is made by selec-ting items from a set of dynamically changingmenus.
Active menus and items are determined bya predictive left-corner parser that accesses asemantic grammar and lexicon.
The advantage ofthis approach is that all inputs to the NLMenuSystem can be understood thus giving a 0% fai lurerate.
A companion system that can automaticallygenerate interfaces to relational databases isalso discussed.relat ively straightforward queries that PLANEScould understand.
Additionally, users did notsuccessfully adapt to the system's l imitationsafter some amount of use.One class of problem that caused negative andfalse user expectations was the user's ab i l i ty  todistinguish between the l imitations in the system'sconceptual coverage and the system's l inguist iccoverage.
Often, users would attempt to para-phrase a sentence many times when the reason forthe system's lack of understanding was due to th~fact that the system did not have data about thequery being asked ( i .e.
the question exceeded theconceptual coverage of the system).
Conversely,users' queries would often fa i l  because they werephrased in a way that the system could not handle( i .e.
the question exceeded the l inguist iccoverage of the system).I INTRODUCTIONMuch research into the building of naturallanguage interfaces has been going on for the past15 years.
The primary direction that this re-search has taken is to improve and extend thecapabilities and coverage of natural languageinterfaces.
Thus, work has focused on constructingand using new formalisms (both syntactically andsemantically based) and on improving the grammarsand/or semantics necessary for characterizing therange of sentences to be handled by the system.The ultimate goal of this work is to give naturallanguage interfaces the ab i l i ty  to understandlarger and larger classes of input sentences.Tennant (1980) is one of the few attempts toconsider the problem of evaluating naturallanguage interfaces.
The results reported byTennant concerning his evaluation of the PLANESSystem are discouraging.
These results show thata major problem with PLANES was the negativeexpectations created by the system's inab i l i ty  tounderstand input sentences.
The inab i l i ty  ofPLANES to handle sentences that were input causedthe users to infer that many other sentences wou|dnot be correctly handled.
These inferences aboutPLANES' capabilities resulted in much user frus-tration because of their very limited assumptionsabout what PLANES could understand.
I t  renderedthem unable to successfully solve many of theproblems they were assigned as part of the evalu-ation of PLANES, even though these problems hadbeen specif ically designed to correspond to someThe problem pointed out by Tennant seems to bea general problem that must be faced by any naturallanguage interface.
I f  the system is unable tounderstand user inputs, then the user wi l l  inferthat many other sentences cannot be understood.Often, these expectations serve to severely l imitthe classes of sentences that users input, thusmaking the natural language interface v i r tua l lyunusable for them.
I f  natural language interfacesare to be made usable for novice users, with l i t t leor no knowledge of the domain of the system towhich they are interfacing, then negative and falseexpectations about system capabilit ies and per-formance must be prevented.The most obvious way to prevent users of anatural language interface from having negativeexpectations is expand the coverage of that inter-face to the point where practical ly all inputsare understood.
By doing this,  most sentences thatare input wi l l  be understood and few negativeexpectations wi l l  be created for the user.
Thenusers wi l l  have enough confidence in the naturallanguage interface to attempt to input a wide rangeof sentences, most of which wi l l  be understood.However, natural language interfaces with theab i l i ty  to understand v i r tua l ly  all input sentencesare far beyond current technology.
Thus, users~vill continue to have many negative expectationsabout system coverage.A possible solution to this problem is the useof a set of training sessions to teach the user thesyntax of the system.
However, there are severalproblems with this.
First,  i t  does not allow151untrained novices to use such a system.
Second,i t  assumes that infrequent users wi l l  take withthem and remember what they learned about thecoverage of the system.
Both of these areunreasonable restrictions.I I  A DESCRIPTION OF THE NLMENU SYSTEMIn this paper, we wi l l  employ a technique thatapplies current technology (current grammar formal-isms, parsing techniques, etc.)
to make naturallanguage interface systems meet the criteria ofusability by novice users.
To do this, userexpectations must closely match system performance.Thus, the interface system must somehow make i tclear to the user what the coverage of the systemis.
Rather than requiring the user to type hisinput to the natural language understanding system,the user is presented with a set of menus on theupper half of a high resolution bit map display.He can choose the words and phrases that make uphis query with a mouse.
As the user chooses items,they are inserted into a window on the lower halfof the screen so that he can see the sentence heis constructing.
As a sentence is constructed,the active menus and items in them change toreflect only.
the legal choices, given the portionof the sentence that has already been input.
Atany point in the construction of a natural languagesentence, only those words or phrases that couldlegally come next wi l l  be displayed for the userto select.Sentences which cannot be processed by thenatural language system can never be input to thesystem, giving a 0% failure rate.
In this way, thescope and limitations of the system are madeimmediately clear to the user and only understand-able sentences can be input.
Thus, all queriesfal l  within the l inguistic and conceptual coverageof the system.A.
The Grammar FormalismThe grammars used in the NLMenu System arecontext-free semantic grammars written with phrasestructure rules.
These rules may contain thestandard abbreviatory conventions used by l in-guists for writing phrase structure rules.
Curlybrackets ({}, sometimes called braces) are used toindicate optional elements in a rule.
Addition-al ly,  square brackets (\[\])  are used as well.
Theyhave two uses.
First, in conjunction with curlybrackets.
Since i t  is d i f f i cu l t  to allow rules tobe written in two dimensions as linguists do,where alternatives in curly brackets are writtenone below the other, we require that each alter-native be put in square brackets.
Thus, the rulebelow in (i) would be written as shown in (2).
(2) A --> B {\[C X\] \[E Y\]} DNote that for single alternatives, the squarebrackets can be deleted without loss of informa-tion.
We permit this and therefore {A B} isequivalent o { \ [A \ ] \ [B \ ]} .
The second use of squarebrackets is inside of parentheses.
An example ofthis appears in rule (3) below.
(3) Q --> R (\[M N\] V)This rule is an abbreviation for two rules, Q -->R M N and Q --> R V.Any arbitrary context-free grammar is per-mitted except for those grammars containing twoclasses of rules.
These are rules of the form X--> null and rules that generate cycles, forexample, A --> B, B --> C, C --> D and D --> A.The elimination of the second class of rules causesno d i f f i cu l ty  and does not impair a grammar writerin any way.
I f  the second class of rules werepermitted, an in f in i te  number of parses wouldresult for sentences of grarm~ars using them.
Theelimination of the f i r s t  class of rules causes asmall inconvenience in that i t  prevents grammarwriters from using the existence of null nodes inparse trees to account for certainunboundeddependencies like those found in questions like"Who do you think I saw?"
which are said in somel inguistic theories to contain a null noun phraseafter the word "saw".
However, alternativegrammatical treatments, not requiring a null nounphrase, are also commonly used.
Thus, theprohibition of such rules requires that thesealternative grammatical treatments be used.In addition to synactic information indicatingthe allowable sentences, the grammar formalismalso contains semantic information that determineswhat the meaning of each input sentence is.
Thisis done by using lambda calculus.
The mechanism issimilar to the one used in Montague Grammar andthe various theories that build on Montague'swork.
Associated with every word in the lexicon,there is a translation.
This translation is aportion of the meaning of a sentence in which theword appears.
In order to properly combine thetranslations of the words in a sentence together,there is a rule associated with each context-freerule indicating the order in which the transla-tions of the symbols on the right side of thearrow of a context-free rule are to be combined.These rules are parenthesized l ists  of numberswhere the number i refers to the f i r s t  item af terthe arrow, the number 2 to the second, etc.For example, for the rule X --> A B C 0,a possible rule indicat ing how to combine trans-lat ions might be (3 (I 2 4)).
This rule meansthat the t rans lat ion of A is taken as a functionand applied to the t rans lat ion of B as i tsargument.
This result ing new trans lat ion is thentaken as a function and applied to the trans la-t ion of 4 as i ts  argument.
This result ing trans-la t ion  is then the argument to the t rans lat ion of3 which is the function.
In general, the trans la-t ion of leftmost number applies to the t rans lat ionof the number to i ts  r ight  as the argument.
Theresult  of this then is a function which appliesto the t rans lat ion of the item to i ts  r ight  as the152argument.
However, parentheses can override thisas in the example above.
For rules containingabbreviatory conventions, one translation rulemust be written for every possible expansion ofthe rule.Translations that are functions are of theform "(lambda x (.
.
.
x .
.
. )
) .
When this isapplied to an item l ike "c" as the argument, "c"is plugged in for every occurrence of x after the"lambda x" that is not within the scope of a moredeeply embedded "lambda x".
This is called lambdaconversion and the result is just the expressionwith the "lambda x" stripped off of the front andthe substitution made.B.
The ParserThe parser used in the NLMenu system is animplementation of an enhanced version of the modi-fied left-corner algorithm described in Ross(1982).
Ross (1982) is a continuation of the workdescribed in Ross (1981) and builds on that workand on the work of Griff iths and Petrick (1965).The enhancements enable the parser to parse a wordat a time and to predict the set of next possiblewords in a sentence, given the input that has comebefore.Griff iths and Petrick (1965) propose severalalgorithms for recognizing sentences of context-free grammars in the general case.
One of thesealgorithms, the NBT (Non-selective Bottom to Top)Algorithm, has since been called the "left-corner"algorithm.
Of late, interest has been rekindledin left-corner parsers.
Slocum (1981) shows thata left-corner parser inspired by Griff iths andPetrick's algorithm performs quite well whencompared with parsers based on a Cocke-Kasami-Younger algorithm (see Younger 1967).Although algorithms to recognize or parsecontext-free grammars can be stated in terms ofpush-down store automata, G+P state theiralgorithm in terms of Turing machines to makeits operation clearer.
A somewhat modifiedversion of their algorithm wi l l  be given in thenext section.
These modifications transform therecognition algorithm into a parsing algorithm.The G+P algorithm employs two push downstacks.
The modified algorithm to be given belowwil l  use three, called alpha, beta and gamma.Turing machine instructions are of the followingform, where A, B, C, D, E and F can be arbitrarystrings of symbols from the terminal and non-terminal alphabet.\[A,B,C\] ---> \[D,E,F\] i f  "Conditions"This is to be interpreted as fol lows-I f  A is on top of stack alpha,B is on top of stack beta,C is on top of stack gamma,and "Conditions" are satisfiedthen replace A by D, B by E, and C by F.The modified algorithm follows-(1 \[VI,X,Y\] ---> \[B,V2 ... Vn t X,A Y\]i f  A --- Vl V2 ...  Vn is arule of the phrase structuregrammar X is in the set ofnonterminals and Y isanything(2 \[X,t,A\] ---> \[A X,~,~\]i f  A is in the set ofnonterminals(3 \[B,B,Y\] ---> \[B,B,Y\]i f  B is in the set ofnonterminals or terminalsTo begln, put the terminal string to beparsed followed by END on stack alpha.
Put thenonterminal which is to be the root node of thetree to be constructed followed by END on stackbeta.
Put END on stack gamma.
The symbol t isneither a terminal nor a nonterminal.
When END ison top of each stack, the string has been recog-nized.
I f  none of the turing machine instructionsapply and END is not on the top of each stack,the path which led to this situation was a badpath and does not yield a valid parse.The rules necessary to give a parse tree canbe stated informally (i.e.
not in terms of turingmachine instructions) as follows:When (I) is applied, attach Vl beneath A.When (3) is applied, attach the B on alphaB as the right daughter of the top symbolon gamma.Note that there is a formal statement of theparsing version of NBT in Griff iths (1965).However, i t  is somewhat more complicated andobscures what is going on during the parse.Therefore, the informal procedure given abovewi l l  be used instead.The SBT (Selective Bottom to Top) algorithmis a selective version of the NBT algorithm andis also given in G+P.
The only difference betweenthe two is that the SBT algorithm employs a selec-tive technique for increasing the efficiency ofthe algorithm.
In the terminology of G+P, aselective technique is one that eliminates badparse paths before trying them.
The selectivetechnique employed is the use of a reachabilitymatrix.
A reachability matrix indicates whethereach non-terminal node in the grammar can dominateeach terminal or non-terminal in the grammar in atree where that terminal or non-terminal is on theleft-most branch.
To use i t ,  an additional con-dition is put on rule (i) requiring that X canreach down to A.Ross (1981) modifies the SBT Algorithm tod i rec t ly  handle grammar rules u t i l i z ing  severalabbreviatory conventions that are often used whenwr i t ing  grammars.
Thus, parentheses ( ind icat ingoptional nodes) and cur ly brackets ( ind icat ingthat the items wi th in  are a l ternat ives)  can appear153in rules that the parser accesses when parsing astring.
These modifications wi l l  not be discussedin this paper but the parser employed in theNLMenu System incorporates them because efficiencyis increased, as discussed in Ross (1981).At this point, the statement of the algorithmis completely neutral with respect to controlstructure.
At the beginning of a parse, there isonly one 3-tuple.
However, because the algorithmis non-deterministic, there are potentiallypoints during a parse at which more than oneturing machine instruction can apply.
Each of theparse paths resulting from an application of adifferent turing machine instruction to the sameparser state sends the parser off on a possibleparse path.
Each of these possible paths couldresult in a valid parse and all must be followedto completion.
In order to assure this, i t  isnecessary to proceed in some principled way.One strategy is to push one state as far asi t  wi l l  go.
That is, apply one of the rules thatare applicable, get a new state, and then applyone of the applicable rules to that new state.This can continue until either no rules apply ora parse is found.
I f  no rules apply, i t  was abad parse path.
I f  a parse is found, i t  is oneof possibly many parses for the sentence.
Ineither case, the algorithm must continue on andpursue all other alternative paths.
One way todo this and assure that all alternatives arepursued is to backtrack to the last choice point,pick another applicable rule, and continue in themanner described earlier.
By doing this until theparser has backed up throughall possible choicepoints, all parses of the sentence wi l l  be found.A parser that works in this manner is a depth-f i r s t  backtracking parser.
This is probably themost straightforward control structure for a left -corner parser.Alternative control structures are possible.Rather than pursuing one path as far as possible,one could go down one parse path, leave that pathbefore i t  is finished and then start another.
Thef i r s t  parse path could then be pursued later fromthe point at which i t  was stopped.
I t  is neces-sary to use an alternative control structure toenable parsing to begin before the entire inputstring is available.To enable the parser to function as describedabove, the control structure for a depth-firstparser described earlier is used.
To introducethe abi l i ty to begin parsing given only a subsetof the input string, the item MORE is insertedafter the last input item that is given to theparser.
I f  no other instructions apply and MOREis on top of stack alpha, the parser must beginto backtrack as described earlier.
Additionally,the contents of stack beta and gamma must besaved.
Once all backtracking is completed,additional input is put on alpha and parsingbegins again with a set of states, each containingthe new input string on alpha and one of the savedtuples containing beta and gamma.
Each of thesestates is a distinct parse path.To parse a word at a time, the f i rs t  word ofthe sentence followed by MORE is put on alpha.The parser wi l l  then go as far as i t  can, giventhis word, and a set of tuples containing betaand gamma wi l l  result.
Then, each of these tuplesalong with the next word is passed to the parser.The abi l i ty to parse a word at a time is essentialfor the NLMenu System.
However, i t  is alsobeneficial for more traditional natural languageinterfaces.
I t  can increase the perceived speedof any parser since work can proceed as the useris typing and composing his input.
Note that arubout fac i l i ty  can be added by saving the beta-gamma tuples that result after parsing for eachof the words.
Such a fac i l i ty  is used by theNLMenu System.The abi l i ty  to predict the set of possiblenth words of a sentence, given the f i r s t  n-1words of the sentence is the final modificationnecessary to enable this parser to be used formenu-based natural language understanding.
Thisfeature can be added in a straightforward way.Given any beta-gamma pair representing one of theparse paths active after n-1 words of the sentencehave been input, i t  is possible to determine theset of words that wi l l  allow that state to con-tinue.
This is by examing the top-most symbol onstack beta of the tuple.
I t  represents the mostimmediate goal of that parse state.
To determineall the words that can come next, given that goal,the set of all nodes that are reachable from thatnode as a lef t  daughter must be determined.
Thisinformation is easily obtainable from the reach-abi l i ty  matrix discussed earlier.
Once the setof reachable nodes is determined, all that needbe done is find the subset of these that candominate lexical material.
I f  this is done forall of the beta-gamma pairs that resulted afterparsing the f i rs t  n-1 words and the union of thesets that result is taken, the resulting set isa l i s t  of all of the lexical categories thatcould come next.
The l i s t  of next words is easilydetermined from this.I l l  APPLICATIONS OF THE NLMENU SYSTEMSAlthough a wide class of applications areappropriate for menu-based natural languageinterfaces, our effort thus far has concentratedon building interfaces to relational databases.This has had several important consequences.First, i t  has made i t  easy to compare our inter-faces to those that have been built  by othersbecause a prime application area for naturallanguage interfaces has been to databases.Second, the process of producing an interface toany arbitrary set of relations has been automated.A.
Comparison to Existin 9 SystemsWe have run a series of pi lot studies toevaluate the performance of an NLMenu interface to154the parts-suppliers database described in Data(1977).
These studies were similar to the onesdescribed in Tennant (1980) that evaluated thePLANES system.
Our results were more encouragingthan Tennant's.
They indicated that bothexperienced computer users and naive subjectscan successfully use a menu-based natural languageinterface to a database to solve problems.
Allsubjects were successfully able to solve al l  oftheir problems.Comments from subjects indicated that al-though the phrasing of a query might not have beenexactly how the subject would have chosen to askthe question in an unconstrained, traditionalsystem, the subjects were not bothered by this andcould find the alternative phrasing without anyd i f f icu l ty .
One factor that appeared to beimportant in this was the displaying of the entireset of menus at all  times.
In cases where i t  wasnot clear which item on an active menu would leadto the users desired query, users looked at theinactive menus for hints on how to proceed.Additionally, the existence of a rubout fac i l i tythat enabled users to rubout phrases they hadinput as far back as desired encouraged them toexplore the system to determine how a sentencemight be phrased.
There was no penalty for choos-ing an item which did not allow a user to continuehis question in the way he desired.
All that theuser had to do was rub i t  out and pick again.B.
Automatically Buildin~ NLMenu Interfaces ToRelational DatabasesThe system outlined in this section is a com-panion system to NLMenu.
I t  allows NLMenu inter-faces to an arbitrary set of relations to beconstructed in a quick and concise way.
Otherresearchers have examined the problem of construc-ting portable natural language interfaces.
Theseinclude Kaplan (1979), Harris (1979), Hendrix andLewis (1981), and Grosz et.
al.
(1982).
Whilethe work described here shares similar it ies,  i tdiffers in several ways.
Our interface specifi-cation dialogue is simple, short, and is supportedby the database data dictionary.
I t  is intendedfor the informed user, not necessarily a databasedesigner and certainly Dot a grammar expert.Information is obtained from this informed userthrough a menu-based natural language dialogue.Thus, the interface that builds interfaces isextremely easy to use.i .
ImplementationThe system for automatically generatingNLMenu interfaces to relational databases isdivided into two basic components.
One component,BUILD-INTERFACE, produces a domain specific datastructure called a "portable spec" by engaging theuser in an NLMenu dialog.
The other component,MAKE-PORTABLE-INTERFACE, generates a semanticgrammar and lexicon from the "portable spec".The MAKEZPORTABLE-INTERFACE componenttakes as input a "portable spec", uses i t  toinstantiate a domain independent core grammar andlexicon, and returns a semantic grammar and asemantic lexicon pair, which defines an NLMENUinterface.
The core grammar and lexicon can besmall (21 grammar ules and 40 lexical entries atpresent), but the size of the resulting semanticgrammars and lexicons wi l l  depend on the portablespec.A portable-spec onsists of a l i s t  ofcategories.
The categories are as follows.
TheCOVERED TABLES l i s t  specifies al l  relations orviews that the interface wi l l  cover.
The retr ie-val, insertion, deletion and modification rela-tions specify ACCESS RIGHTS for the coveredtables.
Non-numeric attributes, CLASSIFY ATTRI-BUTES according to type.
Computable attributesare numeric attributes that are averageable,summable, etc.
A user may choose not to coversome attributes in interface.
IDENTIFYING ATTRI-BUTES are attributes that can be used to identifythe rows.
Typically, identifying-attributes wi l linclude the key attributes, but may include otherattributes i f  they better identify tuples (rows)or may even not include a ful l  key i f  one seeks toidentify sets of rows together.
TWO TABLE JOINSspecify supported join paths between tables.THREE TABLE JOINS specify supported "relation-ships" (in the entity-relationship data modelsense) where one relation relates 2 others.
TheEDITED ITEMS specification records old and newvalues for menu phrases and the window they appearin.
The EDITED HELP provides a way for users toadd to, modify or replace automatically generatedhelp messages associated with a menu item.
Valuesto these last categories record changes that auser makes to his default menu screen to customizephrasings or help messages for an application.The BUILD-INTERFACES component is a menu-based natural language interface and thus isreally another application of the NLMenu system toan interface problem.
I t  e l ic i ts  the informationrequired to build up a "portable spec" from theuser.
In addition to allowing the user to createan interface, i t  also allows the user to modify orcombine existing interfaces.
The user may alsogrant interfaces to other users, revoke them, ordrop them.
The database management system controlswhich users have access to which interfaces.2.
AdvantagesThe system for automatically constructingNLMenu interfaces enjoys seyeral practical andtheoretical advantages.
These advantages areoutlined below.End-users can construct natural languageinterfaces to their own data in minutes, notweeksor years, and without the aid of a grammar special-ist.
There is heavy dependence on a data diction-ary but not on l inguistic information.The interface builder can control cover-age.
He can decide to make an interface thatcovers only a semantically related subset of his155tables.
He can choose to include some attributesand hide other attributes so that they cannot bementioned.
He can choose to support various kindsof joins with natural language phrases.
He canmirror the access rights of a user in his inter-face, so that the interface w i l l  allow him toinsert, delete, and modify as well as just re-trieve and only from those tables that he has thespecified privileges on.
Thus, interfaces arehighly tunable and the term "coverage" can begiven precise def init ion.
Patchy coverage isavoided because of the uniform way in which theinterface is constructed.Automatically generated natural languageinterfaces are robust with respect to databasechanges; interfaces are easy to change i f  the useradds or deletes tables or changes table descrip-tions.
One need only modify the portable specto reflect the changes and regenerate the inter-face.Automatically generated NLMenu interfacesare guaranteed to be correct (bug free).
The in-teraction in which users specify the parametersdefining an interface, ensures that parametersare val id, i .e.
they correspond to real tables,attributes and domains.
Instantiating adebugged core grammar with valid parametersyields a correct interface.Natural language interfaces are con-structed from semantically related tables that theuser owns or has been granted and they ref lect hisaccess privileges (retr ieval) ,  insertion, etc).By extension, natural language interfaces becomedatabase objects in their own right.
They aresharable (grantable and revokable) in a controlledway.
A user can have several such NLMenu inter-faces.
Each gives him a user-view of a semanti-cally related set of data.
This notion of a viewis l ike the notion of a database schema found innetwork and hierarchical but not relationalsystems.
In relational systems, there is noconvenient way for grouping tables together thatare semantically related.
Furthermore, an NLMenuinterface can be treated as an object and can begranted to other users, so a user acting as adatabase administrator can make NLMenu interfacesfor classes of users too naive to build themthemselves ( l ike executives).
Furthermore, inter-faces can be combined by merging portable specsand so user's can combine different,  related user-views i f  they wish.Since an interface covers exactly andonly the data and operations that the user chooses,i t  can be considered to be a "model of the user" inthat i t  provide a well-bounded language that re-f lects a semantically related view of the user'sdata and operations.A f inal advantage is that even i f  anautomatically generated interface is for somereason not quite what is needed for someapplication, i t  is much easier to f i r s t  generatean interface this way and then modify i t  to suitspecific needs than i t  is to build the entireinterface by hand.
This has been demonstratedalready in the prototype where an automaticallygenerated interface required for an applictionfor another group at TI was manually altered toprovide pictorial  database capabil it ies.Taken together, the advantages l istedabove pave the way for low cost, maintainableinterfaces to relational database systems.
Manyof the advantages are novel when considered withrespect to past work.
This approach makes i tpossible for a much broader class of users andapplications to use menu-based, natural languageinterfaces to databases.3.
Features of NLMenu Interfaces toDatabasesThe NLMenu system does not store thewords that correspond to open class data baseattributes in the lexicon as many other systemsdo.
Instead, a meta category called an "expert"is stored in the lexicon.
They may be usersupplied or defaulted and they are arbitrarychunks of code.
Possible implementations includedirect ly doing a database lookup and presentingthe user with a l i s t  of items to choose from orpresenting the user with a type in window whichis constrained to only allow input in the desiredtype or format (for example, for a date).Many systems allow e l l ips is  to permit theuser to, in effect, ask a parameterized query.
Weapproach this problem by making al l  phrases thatwere generated by experts be "mouse sensitive" inthe sentence.
To change the value of a data item,al l  that needs to be done is to move the mouseover the sentence.
When a data item is encoun-tered, i t  is boxed by the mouse cursor.
To changei t ,  one merely clicks on the mouse.
The expertwhich or ig inal ly  produced that data item is thencalled, allowing the user to change that item tosomething else.The grammars produced by the automaticgeneration system permit ambiguity.
However,the ambiguity occurs in a small set of well-defined situations involving relative clauseattachment.
Because of this,  i t  has been possibleto define a bracketed and indented format thatclearly indicates the source of ambiguity to theuser and allows him to choose between alternativereadings.
Additionally, by constraining theparser to obey several human parsing strategies,as described in Ross (1981), the user is displayeda set of possible readings in which the mostl ike ly  candidate comes f i r s t .
The user is toldthat the f irs 't  bracketed structure is most pro-bably the one he intended.IV CONCLUSIONSThe menu approach to natural language inputhas many advantages over the traditional typingapproach.
Most importantly, every sentence that156is input is understood.
Thus, a 100% success ratefor queries input is achieved.
Implementationtime is greatly decreased because the grammarsrequired can be much smaller.
Generally, writinga thorough grammar for an application of a naturallanguage understanding system consumes most ofthe development time.
Note that the reason largergrammars are needed in traditional systems is thatevery possible paraphrase of a sentence must beunderstood.
In a menu-based system, only oneparaphrase is needed.
The user wi l l  be guidedto this paraphrase by the menus.The fact that the menu-based naturallanguage understanding systems guide the userto the input he desires is also beneficial fortwo other reasons.
First, confused users whodon't know how to formulate their input need notcompose their input without help.
They only needto recognize their input by looking at the menus.They need not formulate their input in a vacuum.Secondly, the extent of the system's conceptualcoverage wi l l  be apparent.
The user wi l l  imme-diately know what the system knows about and whati t  does not know about.Only allowing for one paraphrase of eachallowable query not only makes the grammarsmaller.
The lexicon is smaller as well.
NLMenulexicons must be smaller because i f  they were thesize of a lexicon standardly used for a naturallanguage interface, the menus would be much toolarge and would therefore be unmanageable.
Thus,i t  is possible that l imitations wi l l  be imposed onthe system by the size of the menus.
Menus cannecessarily not be too big or the user w i l l  beswamped with choices and wi l l  be unable to findthe right one.
Several points must be made here.First,  even though an inactive menu containing,say, a class of modifiers, might have one hundredmodifiers, i t  is l i ke ly  that al l  of these wi l lnever be active at the same time.
Given asemantic grammar with five different classes ofnouns, i t  wi l l  most l ikely be the case that onlyone f i f th  of the modifiers wi l l  make sense as amodifier for any of those nouns.
Thus, an activemodifier menu wi l l  have roughly twenty items ini t .
We have constructed NLMenu interfaces toabout ten databases, some reasonably large, andwe have had no problem with the size of the menusgetting unmanageable.The NLMenu System and the companion system toautomatically build NLMenu interfaces that aredescribed in this paper are both implemented inLisp Machine Lisp on an LMI Lisp Machine.
I t  hasalso proved to be feasible to put them on a micro-computer.
Two factors were responsible for this:the word by word parse and the smaller grammars.Parsing a word at a time means that most of thework necessary to parse a sentence is done beforethe sentence has been completely input.
Thus,the perceived parse time is much less than i totherwise would be.
Parse time is also madefaster by the smaller grammars because i t  is afunction of grammar size so the smaller thegrammar, the faster the parse wi l l  be performed.Smaller grammars can be dealt with much moreeasily on a microcomputer with limited memoryavailable.
Both systems have been implementedin C on the Texas Instruments ProfessionalComputer.
These implementation are based onthe Lisp Machine implementations but were doneby another division of TI.
These second imple-mentations wi l l  be available as a softwarepackage that w i l l  interface either local ly toRSI s Oracle relational DBMS which uses S .
.
.
.as the query language or to various remotecomputers running DBMS's that use SQL 3.0 astheir  query language.V REFERENCESData, C. J.
An introduction to database systems.New York: Addison-Wesley, 1977.Gr i f f i ths ,  T. On procedures for constructingstructural descriptions for three parsingalgorithms, Communications of the ACM, 1965, 8,594.Griff i ths, T. and Petrick, S. R., On the relativeefficiencies of context-free grammar ecogni-zers, Communications of the ACM, 1965, 8,289-300.Grosz, B., Appelt, D., Archbold, A., Moore, R.,Hendrix, G., Hobbs, J., Martin, P., Robinson,J., Sagalowicz, D., and Warren, P. TEAM: Atransportable natural language system.Technical Note 263, SRI International, MenloPark, California.
Apri l ,  1982.Harris, L. Experience with ROBOT in 12 commercialnatural language database query applications.Proceedings of the sixth IJCAI.
1979.Hendrix, G. and Lewis, W. Transportable naturallanguage interfaces to databases.
Proceeaingsof the 19th Annual Meetin 9 of the ACL.
1981.Kaplan, S. J.
Cooperative responses from aportable natural language query system.
Ph.D.Dissertation, University of Pennsylvania,Computer Science Department, 1979.Konolige, K. A Framework for a portable NLinterface to large databases.
TechnicaiNote197, SRI International, Menlo Park, CA,October, 1979.Ross, K. Parsing English phrase structure, Ph.D.Dissertation, Department of Linguistics,University of Massachusetts~ 1981.Ross, K. An improved left-corner parsingalgorithm.
Proceedings of COLING 82.333-338.1982,Slocum, J.
A practical comparison of parsingstrategies, Proceedings of  the 19th AnnualMeeting of the ACL.
1981, I-6.
?57Tennant, H. R. Evaluation of natural languageprocessors.
Ph.D. Dissertation Departmentof Computer Science, University of I l l inois1980.Thompson, C. W. SURLY: A single user relationalDBMS.
Technical Report, Computer ScienceDepartment, University of Tennessee, Knoxville,1979.Ullman, J.
Principles of Database SystemsComputer Science Press, 1980.Younger, D. Recognition and parsing of context-free language in time n3.
Informationand Control, 1967, 10, 189-208158
