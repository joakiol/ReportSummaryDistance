Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372?376,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsSeed-Based Event Trigger Labeling:How far can event descriptions get us?Ofer Bronstein1, Ido Dagan1, Qi Li2, Heng Ji2, Anette Frank3,41Computer Science Department, Bar-Ilan University2Department of Computer Science, Rensselaer Polytechnic Institute3Department of Computational Linguistics, Heidelberg University4Research Training Group AIPHES, Dept.
of Computational Linguistics, Heidelberg Universityoferbr@gmail.com dagan@cs.biu.ac.il{liq7,jih}@rpi.edu frank@cl.uni-heidelberg.deAbstractThe task of event trigger labeling is typi-cally addressed in the standard supervisedsetting: triggers for each target event typeare annotated as training data, based onannotation guidelines.
We propose an al-ternative approach, which takes the exam-ple trigger terms mentioned in the guide-lines as seeds, and then applies an event-independent similarity-based classifier fortrigger labeling.
This way we can skipmanual annotation for new event types,while requiring only minimal annotatedtraining data for few example events atsystem setup.
Our method is evaluated onthe ACE-2005 dataset, achieving 5.7% F1improvement over a state-of-the-art super-vised system which uses the full trainingdata.1 IntroductionEvent trigger labeling is the task of identifying themain word tokens that express mentions of pre-specified event types in running text.
For example,in ?20 people were wounded in Tuesday?s airportblast?, ?wounded?
is a trigger of an Injure eventand ?blast?
is a trigger of an Attack.
The taskboth detects trigger tokens and classifies them toappropriate event types.
While this task is oftena component within the broader event extractiontask, labeling both triggers and arguments, this pa-per focuses on trigger labeling.Most state-of-the-art event trigger labeling ap-proaches (Ji and Grishman, 2008; Liao and Grish-man, 2010b; Hong et al, 2011; Li et al, 2013)follow the standard supervised learning paradigm.For each event type, experts first write annotationguidelines.
Then, annotators follow them to labelevent triggers in a large dataset.
Finally, a classi-fier is trained over the annotated triggers to labelthe target events.The supervised paradigm requires major humanefforts both in producing high-quality guidelinesand in dataset annotation for each new event type.Given the rich information embedded in the guide-lines, we raise in this paper the following researchquestion: how well can we perform by leverag-ing only the lexical knowledge already availablein quality guidelines for new event types, withoutrequiring annotated training data for them?To address this question, we propose a seed-based approach for the trigger labeling task (Sec-tion 2).
Given the description for a new event type,which contains some examples of triggers, we firstcollect these triggers into a list of seeds.
Then,at the labeling phase, we consider each text tokenas a candidate for a trigger and assess its similar-ity to the seed list.
In the above example, givenseeds such as ?explosion?
and ?fire?
for the Attackevent type, we identify that the candidate token?blast?
is a hyponym of ?explosion?
and synonymof ?fire?
and infer that ?blast?
is a likely Attacktrigger.In our method, such similarity indicators are en-coded as a small set of event-independent clas-sification features, based on lexical matches andexternal resources like WordNet.
Using event-independent features allows us to train the systemonly once, at system setup phase, requiring anno-tated triggers in a training set for just a few pre-selected event types.
Then, whenever a new eventtype is introduced for labeling, we only need tocollect a seed list for it from its description, andprovide it as input to the system.We developed a seed-based system (Section 3),based on a state-of-the-art fully-supervised eventextraction system (Li et al, 2013).
When evalu-ated on the ACE-2005 dataset,1our system outper-forms the fully-supervised one (Section 4), eventhough we don?t utilize any annotated triggers ofthe test events during the labeling phase, and only1http://projects.ldc.upenn.edu/ace372ElectInjureMeetTraining(generic ?
once at system setup)Few training event typesGeneric ModelTrigger Labeling(per new target event type)Training withsimilarity featuresApply Model:compare seeds withtokens in test documentsusing similarity featuresDie?
Seed List: die, kill, dead, ??
Annotated triggers in corpus(small amount, e.g.
10)?
e.g.
?Jackson died in 2009...?Attack?
Seed List: explosion, fire, stab...?
Annotated triggers in corpus(small amount, e.g.
15)Labeled Trigger Mentionsof Meet?
Seed List: meet, talks, summit,conference, meeting, visit??
(No annotated triggers)Figure 1: Flow of the seed-based approachuse the seed triggers appearing in the ACE anno-tation guidelines.
This result contributes to thebroader line of research on avoiding or reducingannotation cost in information extraction (Section5).
In particular, it suggests the potential utility ofthe seed-based approach in scenarios where man-ual annotation per each new event is too costly.2 Seed-Based Problem SetupThis section describes our setup, as graphically il-lustrated in Figure 1.Similarly to the supervised setting, our ap-proach assumes that whenever a new event type isdefined as target, an informative event descriptionshould be written for it.
As a prominent example,we consider Section 5 of the ACE-2005 event an-notation guidelines,2which provides a descriptionfor each event type.
The description includes ashort verbal specification plus several illustratingexample sentences with marked triggers, spanningon average less than a page per event type.As event descriptions specify the intended eventscope, they inherently include representative ex-amples for event triggers.
For instance, the ACE-2005 guidelines include: ?MEET Events includetalks, summits, conferences, meetings, visits,.
.
.
?,followed by an example: ?Bush and Putin met thisweek.
.
.
?.
We thus collect triggers mentioned ineach event description into a seed list for the eventtype, which is provided as input to our trigger la-beling method.
Triggers from the above quotedsentences are hence included in the Meet seed list,shown in Figure 1.As mentioned in the Introduction, our method(Section 3) is based on event-independent features2https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-events-guidelines-v5.4.3.pdfthat identify similarities between a candidate trig-ger and a given seed list.
To train such generic fea-tures, our training requires several arbitrary train-ing event types, with a small amount of annotatedtriggers, from which it learns weights for the fea-tures.
In our evaluation (Section 4) we use 5 train-ing event types, with a total of 30 annotated trig-ger mentions (compared to roughly 5000 used bythe baseline fully-supervised system).
In this set-ting, the training phase is required only once dur-ing system setup, while no further training is re-quired for each new target event type.In summary, our setup requires: (1) a seed listper target event type; (2) a small number of anno-tated triggers for few training event types, alongwith their seed lists (at system setup).3 MethodThis section describes the method we designedto implement the seed-based approach.
To as-sess our approach, we compare it (Section 4) withthe common fully-supervised approach, which re-quires annotated triggers for each target eventtype.
Therefore, we implemented our system byadapting the state-of-the-art fully-supervised eventextraction system of Li et al (2013), modifyingmechanisms relevant for features and for triggerlabels, as described below.
Hence the systems arecomparable with respect to using the same pre-processing and machine learning infrastructure.3.1 The Fully-Supervised SystemThe event extraction system of Li et al (2013) la-bels triggers and their arguments for a set of targetevent types L, for which annotated training docu-ments are provided.
The system utilizes a struc-tured perceptron with beam search (Collins andRoark, 2004; Huang et al, 2012).
To label trig-gers, the system scans each sentence x, and cre-ates candidate assignments y, that for each tokenxiassign each possible label yi?
L ?
{?}
(?meaning xiis not a trigger at all).
The score of anassignment (xi, yi) is calculated as w ?
f , where fis the binary feature vector calculated for (xi, yi),and w is the learned feature weight vector.The classifier?s features capture various proper-ties of xiand its context, such as its unigram andits containing bigrams.
These features are highlylexicalized, resulting in a very large feature space.Additionally, each feature is replicated and pairedwith each label yi, allowing the system to learn373Feature DescriptionSameLemmaDo the candidate token and a seed share thesame lemma?Synonym Is a seed a WN synonym of the candidate token?Hypernym Is a seed a WN hypernym or instance-hypernymof the candidate token?SimilarityRelationsDoes one of these WN relations hold between aseed and a candidate token?
Synonym, Hyper-nym, Instance Hypernym, Part Holonym, Mem-ber Holonym, Substance Meronym, EntailmentTable 1: Similarity features using WordNet (WN).For the last two features we allow up to 2 levelsof transitivity (e.g.
hypernym of hypernym), andconsider also derivations of candidate tokens.different weights for different labels, e.g., feature(Unigram:?visited?, Meet) will have a differentweight than (Unigram:?visited?, Attack).3.2 The Seed-Based SystemTo implement the seed-based approach for triggerlabeling, we adapt only the trigger classificationpart in the Li et al (2013) fully-supervised sys-tem, ignoring arguments.
Given a set of new targetevent types T we classify every test sentence oncefor each event type t ?
T .
Hence, when classi-fying a sentence for t, the labeling of each tokenxiis binary, where yi?
{>,?}
marks whetherxiis a trigger of type t (>) or not (?).
For in-stance xi=?visited?
labeled as > when classifyingfor t=Meet, means xiis labeled as a Meet trigger.To score the binary label assignment (xi, yi), weuse a small set of features that assess the similar-ity between xiand t?s given seed list.We implement our approach with a basic setof binary features (Table 1), which are turned onif similarity is found for at least one seed in thelist.
We use a single knowledge resource (Word-Net (Fellbaum, 1998)) for expansion.3As in thefully-supervised system, each feature is replicatedfor each label in {>,?
}, learning separately howwell a feature can predict a trigger (>) and anon-trigger (?).
As labels are event-independent,features are event-independent as well, and theirweights can be learned generically (Figure 1).Since we label each token independently foreach event type t, multiple labels may be assignedto the same token.
If a single-label setting is re-quired, standard techniques can be applied, suchas choosing a single random label, or the highestscoring one.3This could be potentially extended, e.g.
with paraphrasedatabases, like (Ganitkevitch et al, 2013).4 Evaluation4.1 SettingWe evaluate our seed-based approach (Section 2)in comparison to the fully-supervised approachimplemented by Li et al (2013) (Section 3).
Tomaintain comparability, we use the ACE-2005documents with the same split as in (Ji and Grish-man, 2008; Liao and Grishman, 2010b; Li et al,2013) to 40 test documents and 559 training doc-uments.
However, some evaluation settings dif-fer: Li et al (2013) train a multi-class model forall 33 ACE-2005 event types, and classify all to-kens in the test documents into these event types.Our approach, on the other hand, trains an event-independent binary classifier, while testing on newevent types that are different from those utilizedfor training.
We next describe how this setup isaddressed in our evaluation.Per-Event Classification To label the test doc-uments to all 33 event types, we classify each to-ken in the test documents once for each test eventtype.4Training Event Types When we label for a testevent type t, we use a model that was trained ondifferent pre-selected training event types.
Sincewe need to label for all event types, we cannot usethe same model for testing them all, since then theevent types used to train this model could not betested.
Thus, for each t we use a model trainedon 5 randomly chosen training event types, differ-ent than t.5Additionally, to avoid a bias causedby a particular random choice, we build 10 differ-ent models, each time choosing a different set of 5training event types.
Then, we label the test docu-ments for t 10 times, once by each model.
Whenmeasuring performance we compute the averageof these 10 runs for each t, as well as the variancewithin these runs.Annotated Triggers Training event types re-quire annotated triggers from the training docu-ments.
To maintain consistency between differ-ent sets of training event types, we use a fixed to-tal of 30 annotated trigger tokens for each set of4To maintain comparability with the single-label classifi-cation results of Li et al (2013), we randomly choose a sin-gle label for our classification in the few (7) cases where ityielded two labels for the same token.5Li et al (2013) internally split the training documents to?train?
and ?dev?.
Accordingly, our training event types aresplit to 3 ?train?
events and 2 ?dev?
events (with annotationstaken from the ?train?
and ?dev?
documents respectively).374Micro-Avg.
(%) VarPrec Rec F1AvgSeed-Based 80.6 67.1 73.2 0.04Li et al (2013) 73.7 62.3 67.5 -Ji and Grishman (2008) 67.6 53.5 59.7 -Table 2: Seed-based performance compared tofully-supervised systems, plus average F1vari-ance (%) over the 10 test runs per test event type.training event types.
The amounts of 5 trainingevent types and 30 annotated triggers were chosento demonstrate that such small amounts, requiringlittle manual effort at system setup, yield high per-formance (larger training didn?t improve results,possibly due to the small number of features).Seed Lists To build the seed lists for all eventtypes, we manually extracted all triggers men-tioned in Section 5 of the ACE-2005 guidelines,as described in Section 2.6This resulted in lists of4.2 seeds per event type on average, which is fairlysmall.
For comparison, each event type has an av-erage of 46 distinct trigger terms in the trainingcorpus used by the fully-supervised method.4.2 ResultsTable 2 shows our system?s precision, recall andF1,7and the average variance of F1within the 10runs of each test event type.
The very low varianceindicates that the system?s performance does notdepend much on the choice of training event types.We compare our system?s performance to thepublished trigger classification results of the base-line system of (Li et al, 2013) (its globally op-timized run, when labeling both triggers and ar-guments).
We also compare to the sentence-levelsystem in (Ji and Grishman, 2008) which uses thesame dataset split.
Our system outperforms thefully-supervised baseline by 5.7% F1, which isstatistically significant (two-tailed Wilcoxon test,p < 0.05).
This shows that there is no per-formance hit for the seed-based method on thisdataset, even though it does not require any anno-tated data for new tested events, thus saving costlyannotation efforts.6Our seed lists are publicly available for download at:https://goo.gl/sErDW97We report micro-average as typical for this task.
Macro-average results are a few points lower for our system and forthe system of Li et al (2013), maintaining similar relativedifference.5 Related WorkOur work contributes to the broader research di-rection of reducing annotation for information ex-traction.
One such IE paradigm, including Pre-emptive IE (Shinyama and Sekine, 2006), On-demand IE (Sekine, 2006; Sekine and Oda, 2007)and Open IE (Etzioni et al, 2005; Banko etal., 2007; Banko et al, 2008), focuses on un-supervised relation and event discovery.
We, onthe other hand, follow the same goal as fully-supervised systems in targeting pre-specified eventtypes, but aim at minimal annotation cost.Bootstrapping methods (such as (Yangarber etal., 2000; Agichtein and Gravano, 2000; Riloff,1996; Greenwood and Stevenson, 2006; Liaoand Grishman, 2010a; Stevenson and Greenwood,2005; Huang and Riloff, 2012)) have been widelyapplied in IE.
Most work started from a smallset of seed patterns, and repeatedly expandedthem from unlabeled corpora.
Relying on unla-beled data, bootstrapping methods are scalable buttend to produce worse results (Liao and Grish-man, 2010a) than supervised models due to se-mantic drift (Curran et al, 2007).
Our method canbe seen complementary to bootstrapping frame-works, since we exploit manually crafted linguis-tic resources which are more accurate but may notcover all domains and languages.Our approach is perhaps closest to (Roth et al,2009).
They addressed a different IE task ?
re-lation extraction, by recognizing entailment be-tween candidate relation mentions and seed pat-terns.
While they exploited a supervised recogniz-ing textual entailment (RTE) system, we show thatusing only simple WordNet-based similarity fea-tures and minimal training yields relatively highperformance in event trigger labeling.6 Conclusions and Future WorkIn this paper we show that by utilizing the in-formation embedded in annotation guidelines andlexical resources, we can skip manual annotationfor new event types.
As we match performance ofa state-of-the-art fully-supervised system over theACE-2005 benchmark (and even surpass it), weoffer our approach as an appealing way of reduc-ing annotation effort while preserving result qual-ity.
Future research may explore additional fea-tures and knowledge resources, investigate alter-native approaches for creating effective seed lists,and extend our approach to argument labeling.375AcknowledgmentsThis work was partially supported by the Euro-pean Commission (project EXCITEMENT, FP7ICT-287923), and the U.S. DARPA DEFT Pro-gram No.
FA8750-13-2-0041, ARL NS-CTANo.
W911NF-09-2-0053, NSF CAREER IIS-1523198.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snow-ball: extracting relations from large plain-text col-lections.
In Proc.
Fifth ACM International Confer-ence on Digital Libraries, pages 85?94.M.
Banko, M. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfor the web.
In Proc.
IJCAI, pages 2670?2676.M.
Banko, O. Etzioni, and T. Center.
2008.
The trade-offs between open and traditional relation extraction.In Proc.
ACL, pages 28?36.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Proc.ACL, pages 111?118.James R Curran, Tara Murphy, and Bernhard Scholz.2007.
Minimising semantic drift with mutual exclu-sion bootstrapping.
In Proc.
PACLING, pages 172?180.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised named-entity extraction fromthe web: An experimental study.
Artificial Intelli-gence, 165:91?134.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proc.
NAACL-HLT, pages 758?764.Mark A. Greenwood and Mark Stevenson.
2006.
Im-proving semi-supervised acquisition of relation ex-traction patterns.
In Proc.
Workshop on InformationExtraction Beyond The Document, pages 29?35.Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao,Guodong Zhou, and Qiaoming Zhu.
2011.
Usingcross-entity inference to improve event extraction.In Proc.
ACL, pages 1127?1136.Ruihong Huang and Ellen Riloff.
2012.
Bootstrappedtraining of event extraction classifiers.
In Proc.
ACL,pages 286?295.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Proc.NAACL, pages 142?151.Heng Ji and Ralph Grishman.
2008.
Refining event ex-traction through cross-document inference.
In Proc.ACL, pages 254?262.Qi Li, Heng Ji, and Liang Huang.
2013.
Joint eventextraction via structured prediction with global fea-tures.
In Proc.
ACL, pages 73?82.Shasha Liao and Ralph Grishman.
2010a.
Filteredranking for bootstrapping in event extraction.
InProc.
COLING, pages 680?688.Shasha Liao and Ralph Grishman.
2010b.
Using doc-ument level cross-event inference to improve eventextraction.
In Proc.
ACL, pages 789?797.Ellen Riloff.
1996.
Automatically generating extrac-tion patterns from untagged text.
In Proc.
AAAI,pages 1044?1049.Dan Roth, Mark Sammons, and V. G. Vinod Vydis-waran.
2009.
A framework for entailed relationrecognition.
In Proc.
ACL-IJCNLP Short Papers,pages 57?60.Satoshi Sekine and Akira Oda.
2007.
System demon-stration of on-demand information extraction.
InProc.
ACL Demo and Poster Sessions, pages 17?20.Satoshi Sekine.
2006.
On-demand information extrac-tion.
In Proc.
COLING-ACL Poster Sessions, pages731?738.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted rela-tion discovery.
In Proc.
NAACL, pages 304?311.Mark Stevenson and Mark A. Greenwood.
2005.
Asemantic approach to IE pattern induction.
In Proc.ACL, pages 379?386.Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen.
2000.
Automatic acquisitionof domain knowledge for information extraction.
InProc.
COLING, pages 940?946.376
