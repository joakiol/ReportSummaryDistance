Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 689?696,Sydney, July 2006. c?2006 Association for Computational LinguisticsNoun Phrase Chunking in HebrewInfluence of Lexical and Morphological FeaturesYoav Goldberg  and  Meni Adler  and  Michael ElhadadComputer Science DepartmentBen Gurion University of the NegevP.O.B 653 Be'er Sheva 84105, Israel{yoavg,adlerm,elhadad}@cs.bgu.ac.ilAbstractWe present a method for Noun Phrasechunking in Hebrew.
We show that thetraditional definition of base-NPs as non-recursive noun phrases does not apply inHebrew, and propose an alternative defi-nition of Simple NPs.
We review syntac-tic properties of Hebrew related to nounphrases, which indicate that the task ofHebrew SimpleNP chunking is harderthan base-NP chunking in English.
As aconfirmation, we apply methods knownto work well for English to Hebrew data.These methods give low results (F from76 to 86) in Hebrew.
We then discuss ourmethod, which applies SVM inductionover lexical and morphological features.Morphological features improve the av-erage precision by ~0.5%, recall by ~1%,and F-measure by ~0.75, resulting in asystem with average performance of 93%precision, 93.4% recall and 93.2 F-measure.
*1 IntroductionModern Hebrew is an agglutinative Semitic lan-guage, with rich morphology.
Like most othernon-European languages, it lacks NLP resourcesand tools, and specifically there are currently noavailable syntactic parsers for Hebrew.
We ad-dress the task of NP chunking in Hebrew as a*This work was funded by the Israel Ministry of Sci-ence and Technology under the auspices of theKnowledge Center for Processing Hebrew.
Addi-tional funding was provided by the Lynn and WilliamFrankel Center for Computer Sciences.first step to fulfill the need for such tools.
Wealso illustrate how this task can successfully beapproached with little resource requirements, andindicate how the method is applicable to otherresource-scarce languages.NP chunking is the task of labelling nounphrases in natural language text.
The input to thistask is free text with part-of-speech tags.
Theoutput is the same text with brackets around basenoun phrases.
A base noun phrase is an NPwhich does not contain another NP (it is not re-cursive).
NP chunking is the basis for manyother NLP tasks such as shallow parsing, argu-ment structure identification, and informationextractionWe first realize that the definition of base-NPsmust be adapted to the case of Hebrew (andprobably other Semitic languages as well) to cor-rectly handle its syntactic nature.
We proposesuch a definition, which we call simple NPs andassess the difficulty of chunking such NPs byapplying methods that perform well in English toHebrew data.
While the syntactic problem inHebrew is indeed more difficult than in English,morphological clues do provide additional hints,which we exploit using an SVM learningmethod.
The resulting method reaches perform-ance in Hebrew comparable to the best resultspublished in English.2 Previous WorkText chunking (and NP chunking in particular),first proposed by Abney (1991), is a well studiedproblem for English.
The CoNLL2000 sharedtask (Tjong Kim Sang et al, 2000) was generalchunking.
The best result achieved for the sharedtask data was by Zhang et al(2002), whoachieved NP chunking results of 94.39% preci-sion, 94.37% recall and 94.38 F-measure using a689generalized Winnow algorithm, and enhancingthe feature set with the output of a dependencyparser.
Kudo and Matsumoto (2000) used anSVM based algorithm, and achieved NP chunk-ing results of 93.72% precision, 94.02% recalland 93.87 F-measure for the same shared taskdata, using only the words and their PoS tags.Similar results were obtained using ConditionalRandom Fields on similar features (Sha andPereira, 2003).The NP chunks in the shared task data arebase-NP chunks ?
which are non-recursive NPs,a definition first proposed by Ramshaw andMarcus (1995).
This definition yields good NPchunks for English, but results in very short anduninformative chunks for Hebrew (and probablyother Semitic languages).Recently, Diab et al(2004) used SVM basedapproach for Arabic text chunking.
Their chunksdata was derived from the LDC Arabic TreeBankusing the same program that extracted the chunksfor the shared task.
They used the same featuresas Kudo and Matsumoto (2000), and achievedover-all chunking performance of 92.06% preci-sion, 92.09% recall and 92.08 F-measure (Theresults for NP chunks alone were not reported).Since Arabic syntax is quite similar to Hebrew,we expect that the issues reported below apply toArabic results as well.3 Hebrew Simple NP ChunksThe standard definition of English base-NPs isany noun phrase that does not contain anothernoun phrase, with possessives treated as a specialcase, viewing the possessive marker as the firstword of a new base-NP (Ramshaw and Marcus,1995).
To evaluate the applicability of this defi-nition to Hebrew, we tested this definition on theHebrew TreeBank (Sima?an et al 2001) pub-lished by the Hebrew Knowledge Center.
Weextracted all base-NPs from this TreeBank,which is similar in genre and contents to theEnglish one.
This results in extremely simplechunks.EnglishBaseNPsHebrewBaseNPsHebrewSimpleNPsAvg # of words 2.17 1.39 2.49% length 1 30.95 63.32 32.83% length 2 39.35 35.48 32.12% length 3 18.68 0.83 14.78% length 4 6.65 0.16 9.47% length 5 2.70 0.16 4.56% length > 5 1.67 0.05 6.22Table 1.
Size of Hebrew and English NPsTable 1 shows the average number of words in abase-NP for English and Hebrew.
The Hebrewchunks are basically one-word groups aroundNouns, which is not useful for any practical pur-pose, and so we propose a new definition for He-brew NP chunks, which allows for some nested-ness.
We call our chunks Simple NP chunks.3.1 Syntax of NPs in HebrewOne of the reasons the traditional base-NP defi-nition fails for the Hebrew TreeBank is related tosyntactic features of Hebrew ?
specifically,smixut (construct state ?
used to express nouncompounds), definite marker and the expressionof possessives.
These differences are reflected tosome extent by the tagging guidelines used toannotate the Hebrew Treebank and they result intrees which are in general less flat than the PennTreeBank ones.Consider the example base noun phrase [Thehomeless people].
The Hebrew equivalent is(1)  which by the non-recursive NP definition will bebracketed as:     , or, loosely translatingback to English: [the home]less [people].In this case, the fact that the bound-morphemeless appears as a separate construct state wordwith its own definite marker (ha-) in Hebrewwould lead the chunker to create two separateNPs for a simple expression.
We present belowsyntactic properties of Hebrew which are rele-vant to NP chunking.
We then present our defini-tion of Simple NP Chunks.Construct State: The Hebrew genitive case isachieved by placing two nouns next to each other.This is called ?noun construct?, or smixut.
Thesemantic interpretation of this construct is varied(Netzer and Elhadad, 1998), but it specificallycovers possession.
The second noun can betreated as an adjective modifying the next noun.The first noun is morphologically marked in aform known as the construct form (denoted byconst).
The definite article marker is placed onthe second word of the construction:(2) beit sefer / house-[const] bookSchool(3) beit ha-sefer / house-[const] the-bookThe schoolThe construct form can also be embedded:(4) 690misrad ro$ ha-mem$alaOffice-[const poss] head-[const] the-governmentThe prime-minister?s officePossessive: the smixut form can be used to indi-cate possession.
Other ways to express posses-sion include the possessive marker  - ?$el?
/?of?
- (5), or adding a possessive suffix on thenoun (6).
The various forms can be mixed to-gether, as in (7):(5) ha-bait $el-i / the-house of-[poss 1st person]My house(6) beit-i / house-[poss 1st person]My house(7) misrad-o $el ro$ ha-mem$alaOffice-[poss 3rd] of head-[const] the-governmentThe prime minister officeAdjective: Hebrew adjectives come after thenoun, and agree with it in number, gender anddefinite marker:(8) ha-tapu?ah ha-yarok / the-Apple the-greenThe green appleSome aspects of the predicate structure in He-brew directly affect the task of NP chunking, asthey make the decision to ?split?
NPs more orless difficult than in English.Word order and the preposition 'et': Hebrewsentences can be either in SVO or VSO form.
Inorder to keep the object separate from the sub-ject, definite direct objects are marked with thespecial preposition 'et', which has no analog inEnglish.Possible null equative: The equative form inHebrew can be null.
Sentence (9) is a non-nullequative, (10) a null equative, while (11) and(12) are predicative NPs, which look very similarto the null-equative form:(9) 	ha-bait hu gadolThe-house is bigThe house is big(10) 	ha-bait gadolThe-house bigThe house is big(11) 	bait gadolHouse bigA big house(12) 	ha-bait ha-gadolThe-house the-bigThe big houseMorphological Issues: In Hebrew morphology,several lexical units can be concatenated into asingle textual unit.
Most prepositions, the defi-nite article marker and some conjunctions areconcatenated as prefixes, and possessive pro-nouns and some adverbs are concatenated as suf-fixes.
The Hebrew Treebank is annotated over asegmented version of the text, in which prefixesand suffixes appear as separate lexical units.
Onthe other hand, many bound morphemes in Eng-lish appear as separate lexical units in Hebrew.For example, the English morphemes re-, ex-,un-, -less, -like, -able, appear in Hebrew as sepa-rate lexical units ?
, 	, , , ,, .
In our experiment, we use as input to thechunker the text after it has been morphologi-cally disambiguated and segmented.
Ouranalyzer provides segmentation and PoS tagswith 92.5% accuracy and full morphology with88.5% accuracy (Adler and Elhadad, 2006).3.2 Defining Simple NPsOur definition of Simple NPs is pragmatic.
Wewant to tag phrases that are complete in theirsyntactic structure, avoid the requirement of tag-ging recursive structures that include full clauses(relative clauses for example) and in general, tagphrases that have a simple denotation.
To estab-lish our definition, we start with the most com-plex NPs, and break them into smaller parts bystating what should not appear inside a SimpleNP.
This can be summarized by the followingtable:Outside SimpleNP ExceptionsPrepositional PhrasesRelative ClausesVerb PhrasesApposition1Some conjunctions(Conjunctions aremarked according to theTreeBank guidelines)2.% related PPs areallowed:5% of the salesPossessive  - '$el' /'of' - is not consid-ered a PPTable 2.
Definition of Simple NP chunksExamples for some Simple NP chunks resultingfrom that definition:1Apposition structure is not annotated in the TreeBank.
Asa heuristic, we consider every comma inside a non conjunct-ive NP which is not followed by an adjective or an adjectivephrase to be marking the beginning of an apposition.2As a special case, Adjectival Phrases and possessive con-junctions are considered to be inside the Simple NP.691   	   [This phenomenon] was highlighted yesterday at[the labor and welfare committee-const of theKnesset] that dealt with [the topic-const of for-eign workers employment-const].						3[The employers] do not expect to succeed in at-tracting [a significant number of Israeli workers]for [the fruit-picking] because of [the low salaries]paid for [this work].This definition can also yield some rather longand complex chunks, such as: 	[The conquests of Genghis Khan and his MongolTartar army]	!		         		!		According to [reports of local government offi-cials], [factories] on [Tartar territory] earned in[the year] that passed [a sum of 3.7 billion Rb (2.2billion dollars)], which [Moscow] took [almost all].Note that Simple NPs are split, for example, bythe preposition ?on?
([factories] on [Tartar terri-tory]), and by a relative clause ([a sum of 3.7BnRb] which [Moscow] took [almost all]).3.3 Hebrew Simple NPs are harderthan English base NPsThe Simple NPs derived from our definition arehighly coherent units, but are also more complexthan the non-recursive English base NPs.As can be seen in Table 1, our definition of Sim-ple NP yields chunks which are on average con-siderably longer than the English chunks, withabout 20% of the chunks with 4 or more words(as opposed to about 10% in English) and a sig-nificant portion (6.22%) of chunks with 6 ormore words (1.67% in english).Moreover, the baseline used at the CoNLLshared task4 (selecting the chunk tag which wasmost frequently associated with the current PoS)3For readers familiar with Hebrew and feel that  isan adjective and should be inside the NP, we note that this isnot the case ?
 here is actually a Verb in the Beinoniform and the definite marker is actually used as relativemarker.4http://www.cnts.ua.ac.be/conll2000/chunking/gives far inferior results for Hebrew SimpleNPs(see Table 3).4 Chunking Methods4.1 Baseline ApproachesWe have experimented with different knownmethods for English NP chunking, which re-sulted in poor results for Hebrew.
We describehere our experiment settings, and provide thebest scores obtained for each method, in com-parison to the reported scores for English.All tests were done on the corpus derived fromthe Hebrew Tree Bank.
The corpus contains5,000 sentences, for a total of 120K tokens (ag-glutinated words) and 27K NP chunks (more de-tails on the corpus appear below).
The last 500sentences were used as the test set, and all theother sentences were used for training.
The re-sults were evaluated using the CoNLL sharedtask evaluation tools 5 .
The approaches testedwere Error Driven Pruning (EDP) (Cardie andPierce, 1998) and Transformational Based Learn-ing of IOB tagging (TBL) (Ramshaw and Mar-cus, 1995).The Error Driven Pruning method does nottake into account lexical information and usesonly the PoS tags.
For the Transformation Basedmethod, we have used both the PoS tag and theword itself, with the same templates as describedin (Ramshaw and Marcus, 1995).
We tried theTransformational Based method with more fea-tures than just the PoS and the word, but ob-tained lower performance.
Our best results forthese methods, as well as the CoNLL baseline(BASE), are presented in Table 3.
These resultsconfirm that the task of Simple NP chunking isharder in Hebrew than in English.4.2 Support Vector MachinesWe chose to adopt a tagging perspective forthe Simple NP chunking task, in which eachword is to be tagged as either B, I or O depend-ing on wether it is in the Beginning, Inside, orOutside of the given chunk, an approach firsttaken by Ramshaw and Marcus (1995), andwhich has become the de-facto standard for thistask.
Using this tagging method, chunking be-comes a classification problem ?
each token ispredicted as being either I, O or B, given featuresfrom a predefined linguistic context (such as the5http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt692words surrounding the given word, and their PoStags).One model that allows for this prediction isSupport Vector Machines - SVM (Vapnik,1995).
SVM is a supervised machine learningalgorithm which can handle gracefully a large setof overlapping features.
SVMs learn binary clas-sifiers, but the method can be extended to multi-class classification (Allwein et al, 2000; Kudoand Matsumoto, 2000).SVMs have been successfully applied to manyNLP tasks since (Joachims, 1998), and specifi-cally for base phrase chunking (Kudo and Ma-tsumoto, 2000; 2003).
It was also successfullyused in Arabic (Diab et al, 2004).The traditional setting of SVM for chunkinguses for the context of the token to be classified awindow of two tokens around the word, and thefeatures are the PoS tags and lexical items (wordforms) of all the tokens in the context.
Some set-tings (Kudo and Matsumoto, 2000) also includethe IOB tags of the two ?previously tagged?
to-kens as features (see Fig.
1).This setting (including the last 2 IOB tags)performs nicely for the case of Hebrew SimpleNPs chunking as well.Linguistic features are mapped to SVM fea-ture vectors by translating each feature such as?PoS at location n-2 is NOUN?
or ?word at loca-tion n+1 is DOG?
to a unique vector entry, andsetting this entry to 1 if the feature occurs, and 0otherwise.
This results in extremely large yetextremely sparse feature vectors.EnglishBaseNPsHebrew Sim-pleNPsMethodPrec Rec Prec Rec FBASE 72.58 82.14 64.7 75.4 69.78EDP 92.7 93.7 74.6 78.1 76.3TBL 91.3 91.8 84.7 87.7 86.2Table 3.
Baseline results for Simple NP chunkingSVM Chunking in HebrewWORD POS CHUNK NA B-NP NOUN I-NP PREP ONAME B-NPPREP O NA B-NP  NOUN I-NPFigure 1.
Linguistic features considered in thebasic SVM setting for Hebrew4.3 Augmentation of MorphologicalFeaturesHebrew is a morphologically rich language.
Re-cent PoS taggers and morphological analyzersfor Hebrew (Adler and Elhadad, 2006) addressthis issue and provide for each word not only thePoS, but also full morphological features, such asGender, Number, Person, Construct, Tense, andthe affixes' properties.
Our system, currently,computes these features with an accuracy of88.5%.Our original intuition is that the difficulty ofSimple NP chunking can be overcome by relyingon morphological features in a small context.These features would help the classifier decideon agreement, and split NPs more accurately.Since SVMs can handle large feature sets, weutilize additional morphological features.
In par-ticular, we found the combination of the Numberand the Construct features to be most effective inimproving chunking results.
Indeed, our experi-ments show that introducing morphological fea-tures improves chunking quality by as much as3-point in F-measure when compared with lexi-cal and PoS features only.5 Experiment5.1 The CorpusThe Hebrew TreeBank6 consists of 4,995 handannotated sentences from the Ha?aretz newspa-per.
Besides the syntactic structure, every wordis PoS annotated, and also includes morphologi-cal features.
The words in the TreeBank aresegmented:   (instead of ).Our morphological analyzer also provides suchsegmentation.We derived the Simple NPs structure from theTreeBank using the definition given in Section3.2.
We then converted the original HebrewTreeBank tagset to the tagset of our PoS tagger.For each token, we specify its word form, itsPoS, its morphological features, and its correctIOB tag.
The result is the Hebrew Simple NPchunks corpus 7 .
The corpus consists of 4,995sentences, 27,226 chunks and 120,396 seg-mented tokens.
67,919 of these tokens are cov-ered by NP chunks.
A sample annotated sentenceis given in Fig.
2.6http://mila.cs.technion.ac.il/website/english/resources/corpora/treebank/index.html7http://www.cs.bgu.ac.il/~nlpproj/chunkingFeatureSetEstimated Tag693 PREPOSITION NA NA N NA N NA N NA NA O DEF_ART NA NA N NA N NA N NA NA B-NP NOUN M S N NA N NA N NA NA I-NP AUXVERB M S N 3 N PAST N NA NA O ADJECTIVE M S N NA N NA N NA NA O	 ADVERB NA NA N NA N NA N NA NA O VERB NA NA N NA Y TOINF N NA NA OET_PREP NA NA N NA N NA N NA NA B-NP DEF_ART NA NA N NA N NA N NA NA I-NP NOUN F S N NA N NA N NA NA I-NP.
PUNCUATION NA NA N NA N NA N NA NA OFigure 2.
A Sample annotated sentence5.2 Morphological Features:The PoS tagset we use consists of 22 tags:ADJECTIVE ADVERB ET_PREPAUXVERB CONJUNCTION DEF_ARTDETERMINER EXISTENTIAL INTERJECTIONINTEROGATIVE MODAL NEGATIONPARTICLE NOUN NUMBERPRONOUN PREFIX PREPOSITIONUNKNOWN PROPERNAME PUNCTUATIONVERBFor each token, we also supply the followingmorphological features (in that order):Feature Possible ValuesGender (M)ale, (F)emale,(B)oth (unmarked case), (NA)Number (S)ingle, (P)lurar, (D)ual,can be (ALL), (NA)Construct (Y)es, (N)oPerson (1)st, (2)nd, (3)rd, (123)all, (NA)To-Infinitive (Y)es, (N)oTense Past, Present, Future, Beinoni,Imperative, ToInf, BareInf(has) Suffix (Y)es, (N)oSuffix-Num (M)ale, (F)emale, (B)oth, (NA)Suffix-Gen (S)ingle, (P)lurar, (D)ual, (DP)-dual plural, can be (ALL), (NA)As noted in (Rambow and Habash 2005), onecannot use the same tagset for a Semitic lan-guage as for English.
The tagset we have de-rived has been extensively validated throughmanual tagging by several testers and cross-checked for agreement.5.3 Setup and EvaluationFor all the SVM chunking experiments, we usethe YamCha 8  toolkit (Kudo and Matsumoto,2003).
We use forward moving tagging, usingstandard SVM with polynomial kernel of degree2, and C=1.
For the multiclass classification, we8http://chasen.org/~taku/software/yamcha/use pairwise voting.
For all the reported experi-ments, we chose the context to be a ?2/+2 tokenswindows, centered at the current token.We use the standard metrics of accuracy (% ofcorrectly tagged tokens), precision, recall and F-measure, with the only exception of normalizingall punctuation tokens from the data prior toevaluation, as the TreeBank is highly inconsis-tent regarding the bracketing of punctuations,and we don?t consider the exclusions/inclusionsof punctuations from our chunks to be errors(i.e., ?
[a book ,] [an apple]?
?
[a book] , [an ap-ple]?
and ?
[a book] [, an apple]?
are all equiva-lent chunkings in our view).All our development work was done with thefirst 500 sentences allocated for testing, and therest for training.
For evaluation, we used a 10-fold cross-validation scheme, each time with dif-ferent consecutive 500 sentences serving for test-ing and the rest for training.5.4 Features UsedWe run several SVM experiments, each with thesettings described in section 5.3, but with a dif-ferent feature set.
In all of the experiments thetwo previously tagged IOB tags were included inthe feature set.
In the first experiment (denotedWP) we considered the word and PoS tags of thecontext tokens to be part of the feature set.In the other experiments, we used differentsubsets of the morphological features of the to-kens to enhance the features set.
We found thatgood results were achieved by using the Numberand Construct features together with the wordand PoS tags (we denote this WPNC).
Bad re-sults were achieved when using all the morpho-logical features together.
The usefulness of fea-ture sets was stable across all tests in the ten-foldcross validation scheme.5.5 ResultsWe discuss the results of the WP and WPNCexperiments in details, and also provide the re-sults for the WPG (using the Gender feature),and ALL (using all available morphological fea-tures) experiments, and P (using only PoS tags).As can be seen in Table 4, lexical informationis very important: augmenting the PoS tag withlexical information boosted the F-measure from77.88 to 92.44.
The addition of the extra mor-phological features of Construct and Numberyields another increase in performance, resultingin a final F-measure of 93.2%.
Note that the ef-fect of these morphological features on the over-all accuracy (the number of BIO tagged cor-694rectly) is minimal (Table 5), yet the effect on theprecision and recall is much more significant.
Itis also interesting to note that the Gender featurehurts performance, even though Hebrew hasagreement on both Number and Gender.
We donot have a good explanation for this observation?
but we are currently verifying the consistencyof the gender annotation in the corpus (in par-ticular, the effect of the unmarked gender tag).We performed the WP and WPNC experimenton two forms of the corpus: (1) WP,WPNC usingthe manually tagged morphological features in-cluded in the TreeBank and (2) WPE, WPNCEusing the results of our automatic morphologicalanalyzer, which includes about 10% errors (bothin PoS and morphological features).
With themanual morphology tags, the final F-measure is93.20, while it is 91.40 with noise.
Interestingly,the improvement brought by adding morphologi-cal features to chunking in the noisy case(WPNCE) is almost 3.0 F-measure points (asopposed to 0.758 for the "clean" morphologycase WPNC).Features Acc Prec Rec FP 91.77 77.03 78.79 77.88WP 97.49 92.54 92.35 92.44WPE 94.87 89.14 87.69 88.41WPG 97.41 92.41 92.22 92.32ALL 96.68 90.21 90.60 90.40WPNC 97.61 92.99 93.41 93.20WPNCE 96.99 91.49 91.32 91.40Table 4.
SVM results for HebrewFeatures Prec Rec FWPNC 0.456 1.058 0.758WPNCE 2.35 3.60 2.99Table 5.
Improvement over WP5.6 Error Analysis and the Effect ofMorphological FeaturesWe performed detailed error analysis on theWPNC results for the entire corpus.
At the indi-vidual token level, Nouns and Conjunctionscaused the most confusion, followed by Adverbsand Adjectives.
Table 6 presents the confusionmatrix for all POSs with a substantial amount oferrors.
IO means that the correct chunk tag wasI, but the system classified it as O.
By examin-ing the errors on the chunks level, we identified 7common classes of errors:Conjunction related errors: bracketing ?
[a]and [b]?
instead of ?
[a and b]?
and vice versa.Split errors: bracketing [a][b] instead of [a b]Merge errors: bracketing [a b] instead of [a][b]Short errors: bracketing ?a [b]?
or ?
[a] b?
in-stead of [a b]Long errors: bracketing ?
[a b]?
instead of ?[a]b?
or ?a [b]?Whole Chunk errors: either missing a wholechunk, or bracketing something which doesn?toverlap with a chunk at all (extra chunk).Missing/ExtraToken errors: this is a general-ized form of conjunction errors: either ?
[a] T[b]?
instead of ?
[a T b]?
or vice versa, where Tis a single token.
The most frequent of suchwords (other than the conjuncts) was   - thepossessive '$el'.Table 6.
WPNC Confusion MatrixThe data in Table 6 suggests that Adverbs andAdjectives related errors are mostly of the?short?
or ?long?
types, while the Noun (includ-ing proper names and pronouns) related errorsare of the ?split?
or ?merge?
types.The most frequent error type was conjunctionrelated, closely followed by split and merge.Much less significant errors were cases of extraAdverbs or Adjectives at the end of the chunk,and missing adverbs before or after the chunk.Conjunctions are a major source of errors forEnglish chunking as well (Ramshaw and Marcus,1995, Cardie and Pierce, 1998)9, and we plan toaddress them in future work.
The split and mergeerrors are related to argument structure, whichcan be more complicated in Hebrew than in Eng-lish, because of possible null equatives.
The too-long and too-short errors were mostly attachmentrelated.
Most of the errors are related to linguis-tic phenomena that cannot be inferred by the lo-calized context used in our SVM encoding.
Weexamine the types of errors that the addition of9Although base-NPs are by definition non-recursive,they may still contain CCs when the coordinators are?trapped?
: ?
[securities and exchange commission]?
orconjunctions of adjectives.695Number and Construct features fixed.
Table 7summarizes this information.ERROR WP WPNC # Fixed % FixedCONJUNCTION 256 251 5 1.95SPLIT 198 225 -27 -13.64MERGE 366 222 144 39.34LONG (ADJ AFTER) 120 117 3 2.50EXTRA CHUNK 89 88 1 1.12LONG (ADV AFTER) 77 81 -4 -5.19SHORT (ADV AFTER) 67 65 2 2.99MISSING CHUNK 50 54 -4 -8.00SHORT (ADV BEFORE) 53 48 5 9.43EXTRATOK 47 47 0 0.00Table 7.
Effect of Number and Construct informa-tion on most frequent error classesThe error classes most affected by the numberand construct information were split and merge ?WPNC has a tendency of splitting chunks, whichresulted in some unjustified splits, but compen-sates this by fixing over a third of the mergingmistakes.
This result makes sense ?
construct andlocal agreement information can aid in the identi-fication of predicate boundaries.
This confirmsour original intuition that morphological featuresdo help in identifying boundaries of NP chunks.6 Conclusion and Future workWe have noted that due to syntactic features suchas smixut, the traditional definition of base NPchunks does not translate well to Hebrew andprobably to other Semitic languages.
We definedthe notion of Simple NP chunks instead.
Wehave presented a method for identifying HebrewSimple NPs by supervised learning using SVM,providing another evidence for the suitability ofSVM to chunk identification.We have also shown that using morphologicalfeatures enhances chunking accuracy.
However,the set of morphological features used should bechosen with care, as some features actually hurtperformance.Like in the case of English, a large part of theerrors were caused by conjunctions ?
this prob-lem clearly requires more than local knowledge.We plan to address this issue in future work.ReferencesMeni Adler and Michael Elhadad, 2006.
Unsuper-vised Morpheme-based HMM for Hebrew Mor-phological Disambiguation.
In Proc.
ofCOLING/ACL  2006, Sidney.Steven P. Abney.
1991.
Parsing by Chunks.
In RobertC.
Berwick, Steven P. Abney, and Carol Tennyeditors, Principle Based Parsing.
Kluwer Aca-demic Publishers.Erin L. Allwein, Robert E. Schapire, and YoramSinger.
2000.
Reducing Multiclass to Binary: AUnifying Approach for Margin Classifiers.
Journalof Machine Learning Research, 1:113-141.Claire Cardie and David Pierce.
1998.
Error-DrivenPruning of Treebank Grammars for Base NounPhrase Identification.
In Proc.
of COLING-98,Montr?al.Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.2004.
Automatic Tagging of Arabic Text: FromRaw Text to Base Phrase Chunks, In Proc.
ofHLT/NAACL 2004, Boston.Nizar Habash and Owen Rambow, 2005.
Arabic To-kenization, Part-of-speech Tagging and Mor-phological Disambiguation in One Fell Swoop.
InProc.
of ACL 2005, Ann Arbor.Thorsten Joachims.
1998.
Text Categorization withSupport Vector Machines: Learning with ManyRelevant Features.
In Proc.
of ECML-98,Chemnitz.Taku Kudo and Yuji Matsumato.
2000.
Use of Sup-port Vector Learning for Chunk Identification.
InProc.
of CoNLL-2000 and LLL-2000, Lisbon.Taku Kudo and Yuji Matsumato.
2003.
Fast Methodsfor Kernel-Based Text Analysis.
In Proc.
of ACL2003, Sapporo.Yael Netzer-Dahan and Michael Elhadad, 1998.
Gen-eration of Noun Compounds in Hebrew: Can Syn-tactic Knowledge be Fully Encapsulated?
In Proc.of INLG-98, Ontario.Lance A. Ramshaw and Mitchel P. Marcus.
1995.Text Chunking Using Transformation-based Learn-ing.
In Proc.
of the 3rd ACL Workshop on VeryLarge Corpora.
Cambridge.Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmanand N. Nativ, 2001.
Building a Tree-bank of Mod-ern Hebrew Text, in Traitement Automatique desLangues 42(2).Fei Sha and Fernando Pereira.
2003.
Shallow Parsingwith Conditional Random Fields.
Technical ReportCIS TR MS-CIS-02-35, University of Pennsylvania.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.Introduction to the CoNLL-2000 Shared Task:Chunking.
In Proc.
of CoNLL-2000 and LLL-2000,Lisbon.Vladimir Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer Verlag, New York, NY.Tong Zhang, Fred Damerau and David Johnson.2002.
Text Chunking based on a Generalization ofWinnow.
Journal of Machine Learning Research,2: 615-637.696
