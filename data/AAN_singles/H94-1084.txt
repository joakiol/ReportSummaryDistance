Integrated Text and Image Understanding for DocumentUnderstandingSttzalzo, e Liebowitz T~tylor, Deborah A. Dahl, Mark Lips/~utz, Carl Weir,Lewis M. Norton, Roslyn Nilson and Marcia Liueb~trgcrUnisys CorporationPaoli, PA 19301ABSTRACTBecause of the complexity of documents and the variety of ap-plications which must be supported, document understand-ing requires the integration of image understanding with textunderstanding.
Our docum(,nt understanding technology isimplemented in a system called IDUS (Intelligent DocumentUndcrstanding System), which creates the data for a textretrieval application and the automatic generation of hyper-ttrxt li.ks.
This paper summarizes the areas of research dur-ing IDUS development where we have found the most benelitfrom the integration of image and text understanding.1.
INTRODUCTIONAs more and more of our daily transactions involve comput-ers, we would expect the volume of paper documents gener-ated to decrease.
However, exactly the opposite is happening.
('onsiderable amounts of information are still generated onlyin paper form.
This, compounded by volumes of legacy l)a.perdocuments till cluttering offices, creates a need for efficientmethods for converting hardcopy material into a computer-usable form.
However, because of the Sol)histication of ap-plications requiring electronic documents (e.g.
routing andretrieval) and the complexity of the documents themselves,it is not sufficient o simply scan and perform OCR (opticalcharacter ecognition) on documents; deeper understandingof the document is needed.Comprehensive document understanding involves determin-ing the form (layout), as well as the function and the meaningof the document.
Document understanding is thus a technol-ogy area which benefits greatly from the integration of textunderstanding with image understanding.
Text understand-ing is necessary to operate on the textual content of the docu-ment and image understanding is necessary to operate on thepixel content of the document.
We have found great benefitfrom intertwining the two technologies instead of employingthem in a pipeline fashion.
We expect that more sophisti-cated document applications in the future will require evencloser knitting.Our document understanding technology is implemented ina system called IDUS (Intelligent Document UnderstandingSystem, described in Section 2), which creates the data fora text retrieval application and the automatic generation ofhypertext links.
This paper summarizes areas of researchduring IDUS development where we have found the mostbenefit from the integration of image processing with naturallanguage/text processing: Document layout analysis (Section3), OCR correction (Section 4), and Text analysis (Section4).
We also discuss two applications we have implemented(Sections 5 and 6) and future plans in Section 7.2.
GENERAL IDUS SYSTEMDESCRIPT IONIDUS employs four general technologies - image understand-ing, OCR, do(:ument layout analysis and text understand-ing - in a knowledge-based cooperative fashion\[l~ 2\].
Thecurre,t implemenlation is on a SPAl~Cstation TM II withthe UNIX TM operating system using the 'C'  and Prologprogramlning languages.
OCR is performed with the Xe-rox hnaging Systems ScanWorX TM Application Program-mer's Interface toolkit.
All features are accessible via anX-Windows TM/Mot i f  TM user interface.After scanning the document page(s), IDUS works pagg-by-page, pcrtorming image-based segmentation to initially locatcthe primitiw~ regions of text and nontext which are manipu-lated d,r iug the logical and functional analysis of the docu-ment.
Each text unit's content is internally homogeneous inphysical attributes uch as font size and spacing style.The ASCII text associated with each block is found throughOCR and a set of features based both on text attr ib.tes(e.g., number of text lines, font size and type) and geometricattributes (e.g., location on page, size of block) is used torefine the segmentation and organize the blocks into properlogical groupings, i.e., "articles".
The ASCII text for each"article" is assembled in a proper reading order.
During thisprocess the column structure of the document is determi,(,(I,and noise and nontext blocks are eliminated.
A text process-ing component performs a finguistic analysis to extract keyideas from each article and then represent them by a seman-tic component, the case frame.
Each "article" text is savedas part of the document corpus and may be retrieved througha query interface.3.
DOCUMENT LAYOUTANALYS ISDocument layout analysis determines the intra- and inter-page physical, logical, functional and topical organization ofa document.
Many applications (e.g.
automatic indexing ordatabase population) require some level of understanding ofthe document's textual components.
However, it is also criti-cal to discover the document layout structure for two reasons:(1) document layout attributes uch as position of text.
andfont specifications are key clues into the relative importanceof textual content on a page and (2) understanding the layout421puts the text in the correct reading order for OCR conver-sion to ASCII/machine-readable text.
Although image-basedtechniques provide valuable information for the physical de-composition of the document and feature inpnt to the logicaland fnnctional areas, the incorporation of textual informationensures a more comprehensive layout analysis.The current implementation f the document layout analysismodule of IDUS consists of physical analysis, logical analysis,and a rudimentary functional component.
We are currentlydeveloping a more sophisticated fnnctional analysis.3.1.
Physical AnalysisPhysical analysis of the document determines the primitivesegments hat make up each page and their attributes.
Prim-itives include both text and non-text segments (pholographs,graphics, rule lines, etc.).
Attributes may include chromaticfeatures (coh)r or gray scale) and typesetting features (mar-gins, point size and face style).
Initially, physical analysisis achieved through page segmentation f the image of eachpage of the document and OC, R (also an image-based pro-cess).
Page segmentation starts with a binary image of a doc-ument page and partitions it into fnndamental units of text ornon-text.
A desirable segmentation will form text units bymerging adjacent portions of text with similar physical at-tributes, such as font size and spacing, within the frameworkof the physical ayout of the document.
The final output ofthe segment ation is a list of polygonal blocks, their locationson the page,, and the locations of horizontal rule lines on thepage\[~\].Each text region in the document image is fed to the OCR toobtain its AS('II characters, as well as text attributes uchas facestyh,, pointsize, aad indentation.
These attributes areused as input to the logical page analysis along with theimage-based feature output from the image segmentation.3.2.
Logical AnalysisLogical analysis groups the primitive regions into fimctionalunits both within and among pages and deternfines the read-ing order of the text units through manipulation of bothimage-based and text-based features.
Logical analysis is firstdone on a page level anti then on a document level.The emphasis of the current logical analysis module has beenat the page level\[2\] where we group appropriately (e.g., intoarticles in the case of a newspaper page) the text componentswhich comprise a document page, sequence them in the cor-rect reading order and establish the dominance pattern (e.g.,find the lead article).
A list of text region locations, rule linelocations, associated ASCII text (as found from an OCR) forthe text blocks, and a list of text attributes (such as face styleand point size) are input to logical page analysis.Transforming the blocks produced by the image segmenta-tion into a logical structure is accomplished via the rule-basedmethods developed by Tsujimoto and Asada\[3\].
A geometricstructure tree is created from the image segmentation, then aset of rules are applied to transform the geometric structuretree to a logical structure tree.
In addition to the locationand size of the blocks, these rules employ a gross functionalclassification as to whether a block is a head or a body.
Headblocks are defined as those which serve in some way as labelsor pointers, such as headlines and captions; body blocks arethose which are referenced by head blocks and which havesubstantive content.
Our head/hody classification relies o,features calculated for each block and for the page as a whole.The inputs to the feature calculations are the outputs of im-age segmentation and OCR.
Construction of the geometricstructure tree is predicated on the way that head blocks arepositioned relative to their associated body blocks.The Tsujimoto and Asada method for building the geomet-ric structure tree assumes that the layout is well behavedin terms of uniformity of column configurations and place-ment of blocks along column boundaries.
By finding ways ofcreating the geometric structure tree when the layout doesnot behave in the above manner, we have extended their ap-proach.Key to both the basic melhod and the enhancements is th,,determination of a page's underlying column structure.
V~'ecompute column boundaries by applying area-based criteriato the set of all page-spanning column chains, where neigh-boring links represent contiguous blocks.
The method is in-dependent of the width of individual columns.To handle complex layouts we create multiple geometricstructure trees for the page which are merged uring the cre-ation of a logical structure tree.
For a document with mnl-tiple sets of column bounds, we look for consistent columnpatterns in subregions of the document and construct a geo-metric structure tree within each subregion.
The individualtrees can be merged by treating each as a child of "root", toform a single tree for the page.
Phenomena like inset I)lo~ksw\]fich interrupt straight column flows are "peeled off" in!o a,overlay plane.
Separate trees are constructed for this plato.and the base plane and then merged so that an inset blockis placed in the context of, but logically subordinate to, itssurroumling article.
The logical transformation then applieslayout rules which associate blocks in the geometric struc!
u retree(s) into "article" groupings in the proper reading order.Text analysis enhances logical analysis particularly in thearea of determining the reading order.
Continuity of arli-cles in a document may be very simple or very complex.
Auarticle may require many pages, but flow through the (h),'-ument in a linear fashion.
In this case continuity may bedetermined by proximity.
Popular periodicals, though, oftenhave nmltiple articles per page with continuations separatedby many pages.
In fact, continuations may occur on pageswhich precede the initial text, as well as the more typical caseof those which succeed the initial text.q'o determine where an article continues, it may be sufficiet,tto look for text such as "continued from page 13" or "colllin-ned on next page".
However, there are several cases where.
;tdeeper linguistic interpretation will be necessary to find th,.article flow.
Consider the scenario of nmltiple articles p,.rpage.
If reading order is determined solely h'om geometti,:and typesetting attributes, then it is possible that some textcomponents will be grouped incorrectly.
If the reading orderin this case nmst be verified, it may be necessary to incor-porate more sophisticated hnguistic processing, for example,422comparing the last.
sentence fragment of one text region andverifying that is consistent with the sentence fragment of thecandidate following region (either syntactically or semanti-cally).3.3.
Functional AnalysisFunctional analysis labels each unit (a~ determined throughh)gical analysis) by role or function within the document.
Forexample, apopular journal article will have a title, author ef-erence, body text and photos.
Functional analysis combinesimage-based and text-based features.Functional labeling of regions, first at a high level to dis-criminate between head and body or text and non-text, andultimately to deternfine tile particular ole a region plays,can be enhanced through the use of textual clues.
Oftenthese clues would be used in conjunction with format cluesand clues derived from pixel-level features\[4\].The simplest type of textual clue is string-based patternmatching or basic keyword search (such as looking for "Fig.
"or "Figure" to identify a non-text region).
Since OCR sys-tems tend to be overeager in recognizing regions as text, areasonable cluc combination would have the system look forthe presence of a left-justified or centered figure tag below aregion with a high number of character recognition errors.
Ifb,)th conditions are met, there is strong support for designat-ing lqat region as non-text.kVhih, much can be gained fr,)m simple string-based patternmatching, there are olher inh~rmalion retrieval techniques ofwtrying kinds and dvgrees of complexity which can be ap-plied to advantage.
%'e mention a few below, using informald(,scriptions.Siring-based pattern matching with boolean operators and po-sitional constraints.
The "Figure" example above falls in thisc~ttegory.
As another example, we might classify a block asan atfiliation block (hence giving evidence that the documentis a scholarly article) if it contains "(gollege" or "University"within two lines of a (U.S.) state name or abbreviation.Adherence to a simple grammar.
The section headings forscholarly articles can be characterized by a grammar whichspecifies a sequence of digits and dots followed by an alpha-numeric string.
As another example, newspaper headlinesand captions may conform to a certain style which has asyntactic basis, such as noun phrase followed by present par-ticiple for a picture caption : "Heroic firefighter escuing catfrom tree".k)'equency o\] string occurrences.
In government Requests forProposals (RFPs), phrases of the form the offerer shall andthe contractor shall occur with much higher frequency thanin other types of documents.
This frequency information canbc used to infer document type.D'equency o\] syntactic structure occurrences.
Newspaper ar-tic'les typically contain a higher frequency of appositive con-structions than other types of prose.
Using syntactic analysisto detect he presence or absence of appositive constructionswould license the inference that a newspaper writing style waspresent in a given text block, and therefore that a newspaperdocument type should be expected.3.4.
Topical AnalysisTopical analysis includes referential links among components,such as from the body of a text to a photograph it mentions.Examples of references include both explicit references (e.g.,"see Figure 1.5") and implicit references (e.g.. "following thedefinition described in the introduction").
The topical orga-nizalion could also accomnmdate xt summary temphttes orother interpretive notes.
Linguistic processing should play akey role in determining tile topical organization.4.
OPTICAL CHARACTERRECOGNITION CORRECTIONLanguage understanding serves two functions in documentinterpretation.
First it is important if the text of tile doc-ument must be interpreted and processed ill some way, forexample, to fill a database.
In addition, natural anguag,.understanding technology call be used to imprt,ve tilt.
a,:cu-racy of optical character recognition (OCR) output, whetherthe ultimate goal is to interpret tile text, to find textual clues,or simply to obtain accurate character recognition.Our method for using linguistic context to improve OCRoutput\[5\] is based on extending work previously done inSl)ok(:n language uuderstanding\[6, 7\], where a natural lan-guag,' system is used to constrain tile output of a spee('hrecognizer.
We have applied linguistic constraints to theOCR using a variation of the N-best interface with th,:natural language processing system, PUNDIT.
PUNDIT isa large, domain-independent naturai language processingsystem which is modul~tr in design, and includes distinctsyntactic\[8\], semantic\[9\] and application\[10\] components.The data used for this experiment were obtained from pro-cessing facsimile output, since optical character ecognitionfor these documents typically results in a high error rate.
Weselected a domain of input texts on which the natural an-guage system was already known to perform well, air trav('lplanning information\[Ill.
The OCR output is sent to theAlternative Generator, which consults with a lexicon of al-lowable words and a spelling corrector, developed as part.
ofthis project.
The spelhng corrector by itself successfully cor-rects many OCR errors.
The output of the spelling correctoris also used to generate an ordered list of N-best alternatiw"sentences for each input.
These sentences are sent to PUNDIT,which selects the first meaningful alternative sentence.The generation of an N-best list of sentence candidates i asimple matter of taking a "cross-product" of all word alter-natives across the words of a given sentence.
The score ofeach candidate sentence is the product of tile scores of th,~words in it; words without nmltil)le alternatives are assigw,,Ia score of 1.0.
N-best sentence candidates are presented tothe natural language system in ascending order of score.
Thesentence with the lowest score that is accepted by the naturallanguage system is taken to be the output of the iutegralt,dOCR-natural language recognition system.The system was tested with 120 sentences of air travel plan-423ning data on which the natural anguage system had previ-ously been trained.
A known text was formatted in ~TEX inboth roman and italic fonts, and printed on an Imagen TM8/3(10 laser printer.
It was then sent through a Fujitsudex9 TM facsimile machine and received by a Lanier 2000 TMfacsimile machine.
The faxed output was used as input toa Xerox ScanWorX TM scanner and OCR system.
Figure 1shows a typical sentence after scanning, after spelling cor-rection (with N-best sentence candidates), and after naturallanguage correction.Performance was measured using software originally designedto ewduate speech recognizers, distributed by the NationalInstitute of Standards and Technology \[14\].
The word errorrate for output directly from the OCR was 14.9% for theroman font and 11.3% for the italic font.
After the output wassent through the alternative generator, it was scored on thebasis of the first candidate of the N-best set of alternatives.The error rate was reduced to 6% for the roman font and to3.2% for the italic font.
Finally, the output from the naturallanguage system was scored, resulting in a final error rate of5.2% for the roman font and 3.1% for the italic font.Most of the improvement seen in these results is the effectof the spelling corrector, although there is also a small butconsistent improvement in word accuracy due to natural lan-guage correction.
Spelling correction and natural anguagecorrection have a much bigger effect on application accuracy.Sending the uncorrected OCR output directly into the nat-nral language system h)r processing without correction leadsto a 73% average weighted error rale as measured by theARPA error metric defined for ATIS\[14\].This high error rate is due to the fact that the entire sen-tence must be nearly correct in order to correctly performthe database task.
Spelling correction improves the errorrate to 33%.
Finally, with natural anguage correction, theapplication error rate improves to 28%.The tagged articles are input to the parser, which uses agrammar consisting of 400 productions to derive regular-ized parses which reflect each sentence's logical structure.
'\]'he parser achieves domain independence byusing an 80,01illword lexicon, and by using time-onts and a skip-and-lit nm(:h-anism to partially parse sentences for which it lacks appro-priate productions.
Thus, no sentence will be left comph:tclyunanalyzed; the system will extract as much information asit can from eyery sentence.
Average parsing time is approxi-mately 0.5 see/sentence.The semantic omponent represents he meaning of the sen-tence in the form of a case frame, a data structure whichincludes a fl'ame, representing a situation and typically cor-responding to a verb, as well as its case roles, which representthe participants in the situation and which typically corre-spond to the subjects and objects of verbs\[17\].
The semant i(:component of IDUS takes the parsed output and maps it intocase frames reflecting each sentence's semantic ontent.The domain-independent case-frame generator uses unigramfrequencies of case frame mappings based on training dataobtained from a corpus in the Air Travel Planning domainto select the most probable case frame mappings h)r th(,arguments of verbs\[7, 18\].
'\]'his training data is supple-mented by additional mapping rules based on the semanticsof prepositions occurring in the University of PennsylvaniaTreebank\[19\].6.
APPL ICAT IONSWe have developed two applications to demonstrate he util-ity of document understanding from hardcopy material: t,,xtretrieval and automatic hypertext generation.Articles found on nmlti-artiele pages during the docum~.,tlayout analysis are extracted separately and stored in a cor-pus.
The text retrieval interface to IDUS allows the user topose a query in natural language.5.
TEXT UNDERSTANDINGOnce the ASCII text is assembled in the correct reading or-der, we can employ text understanding to support a doc-ument application.
The text understanding module is ageneric, fast, robust, domain-independent, natural anguageprocessing system consisting of components for syntactic andsemantic processing.
The syntactic omponent includes asta-tistical part-of-speech tagger developed at the University ofPennsylvania\[15\], and a robust Prolog parser developed atNew York University\[16\].
The semantic omponent is a caseframe interpreter developed at Unisys.Trigram and unigram probabilities are used by the part-of-speech tagger to assign parts of speech to words in a cor-pus.
It was trained on the 1M word Brown corpus of generalEnglish.
The tagger includes models for part-of-speech as-signment for unknown words and heuristics for recognizingproper nouns.
Thus, unknown words resulting from OCR er-rors, new proper names, and unusual technical terminologycan be correctly tagged, and will not cause problems duringlater processing.A natural anguage query is made and a search for matcl,.sin the tagged corpus is performed based on part.
of spe?
',:htagging of the query.
Every sentence in tile tagged c,)rl)usreceives a score based on: (1) the ratio of tile numl)er ofdistinct words and their associated part-of-speech in commonbetween the query aml the sentence and (2) the total numlwrof words in the sentence and the query.
The score of a,entire article is the maximum score of any sentence in lit,.article.
Grammatical function words such as prepositions,conjunctions, and pronouns are ignored during the matchingprocess.
By basing the comparison on tagged words ratherthan raw words, fewer irrelevant articles are retrieved becauseparts-of-speech orresponding togrammatical function wordscan be ignored and because words in the query and corpusmust have the same part of speech in order to count as amatch.
After articles have been retrieved, a menu then popsup with a list of "hits".
The user then has an option toexamine the ASCII text associated with the article and/orthe image from which the article came which affords the userthe fnll richness of context.
An example of the text retrievalapplication is shown in Figure 2.We have also developed a hypertext generation module which424Uncor rected  OCROutput :What i s  the la tes t  fgght f iom Phaadelphi& to BostonA f ter  Spe l l ing  Cor rect ion :f l ightWhat is the la tes t  n ightr ightfrom Philadelphia to BostonAf ter  Natura l  Language Cor rect ion :What i s  the la tes t  f l ight  from Ph i lade lph ia  to BostonFigure 1: Successive improvement in OCR output occurs with spelling and natural language correction.converts the output of the IDUS system to a hypertext repre-sentation.
The target of this application was legacy technicalmanuals.An initial prototype system was developed that generatesSGMI,\[20\] aml hypertext\[21\] finks from raw OCR output.The output of this prototype is provided directly as inputto the Unisys hypertext system IDE/AS, TM, which encodeshypertext links as SGML tags.
The main steps are the classi-fication of the individual text lines, the grouping of them intofunctional units, the labeling of these units and embeddingthe corresponding SGML tags.We employ a two-stage fuzzy grammar to classify and grouplines into logical units regardless of physical page boundariesin the original document.
The first stage tokenizes each line,recognizing key character sequences, such as integers epa-rated by periods, which wouhl be useful dowttst.ream in deter-mining line type, in this case some level of (sub)section.
Thismethod allows us to overcome certain kinds of OCR errors.The second stage finds a best match of the sequence of tokensrepresenting each fine against a dictionary of token sequencesto arrive at a line classification.
Classifications include pageheader, page footer, blank line, ordinary line, four levels ofsection indicator lines and list item lines.
The validity of theindividual ine classifications i  then checked in context.
If aline type does not fit with respect, to its neighboring fines, itis reclassified.A further set of rules is applied to group lines togetherinto functional units, label the units and choose titles forthem.
With this preparation, the SGML tags upon whichthe IDE/AS TM hypertext system reties are generated and in-serted.From the unit labels and titles an active index is generated;a click on any entry takes the reader directly to the corre-sponding frame.
Additional processing of each line searchesfor cross-references to paragraphs (e.g., "See paragraph 1.2"),tables and figures in order to insert tags which transformsthe references into "hotspots".
Paragraph, table and figurecross-references are also activated.
Text blocks are fed to thehypertext generation module from IDUS as fines of ASCIIcharacters.7.
FUTURE PLANSCurrently, we are working on enhancements to IDUS includethe ability to analyze new classes of information, speciticallymulti-lingual busine.-s documents and newspapers.
This in-voh'es extending logical analysis to multiple page doeum,,ntsand the tighter coupling of text and image understamling.Re ferences1.
S. L. Taylor, M. Lipshutz, D. A. Dahl, and C.
Weir.
"An intelligent document understanding system," inSecond International Con\]erence on Document A,ablsisand Recognition, (Tsuknba City, Japan), October l!J!~3.2.
S, L. Taylor, M. Lipshutz, and C. Weir, "l)ocunwl~tstructure interpretation by integrating multiple knowl-edge sources," in Symposium on Documtnt A,,,l,l.~isand Information Retrieval, (Las Vegas, Nevada).
16 18March 1992.3.
S. Tsujimoto anti H. Asada, "Understanding mulli-articled documents," in lOlh Int.
Conf.
on Patter,Recognition, (Atlantic City, N J), 16-21 June 1990, pp.551-556.4.
J. Fisher, "Logical structure descriptions of segmenteddocument images," in First International Confer~n,eon Document Analysis and Recognition, (Saint-Malo.France), 30 September - 2 October 1991.5.
D. A. Dahl, L. M. Norton, and S. L. Taylor, "Improv-ing OCR accuracy with linguistic knowledge," in 2 ,dSymposium on Document Analysis and Retrieval, Apr.1993.6.
D. A. Dahl, L. Hirschman, L. M. Norton, M. C.Linebarger, D. Magerman, and C. N. Ball, "Trainingand evaluation of a spoken language understanding sys-tem," in Proceedings of the DARPA Speech and Lan-guage Workshop, (Hidden Valley, PA), June 1990.7.
L. M. Norton, hi.
C. Linebarger, D. A. Dahl, andN.
Nguyeu, "Augmented role filling capabilities for s,-mantle interpretation of natural anguage," in Proceol-ings of the DARPA Speech and Language ll~rksht,l,.
(Pacific Grove, CA), February 1991.8.
L. Hirschman and J. Dowding, "Restriction grammar:A logic grammar," in P. Saint-Dizier and S.
Szpakowic/.eds., (Logic and Logic Grammars for Language Proc?.~.~-ing), pp.
141-167.
Ellis Horwood, 1990.9. hi.
Palmer, Semantic Processing .for Finite Dom,i,~.Cambridge, England: Cambridge University Press,1990.425fine Edit ~ew 9_pUtts ~etp, .ll,~.-,i =\[t,i ~i ~1 | ii iiI pil l i lt l l  Z 1'~'~1 .
.
.
.
.
.
.
.~ .
~  .
.~~~.~'~' t  1 ' ~ 1 ' ' I ~ ?
~  " ;:~ ' : * / ;  ; AC I IVA IE  V lONLDN011.S  .
.
.
.
.
.
.
.
.
.
. "
....?
~rl~tm ka ~sd.
t~a~-~;~.~E2gF~ r~ ~:, ~.~  ~ ~ ~-~ ,~ ENTER QUERY: '~-m CR l , iA~I rebCh l l~gOulputAt&~l j i l1 ,?ot tmlm, l .wm~j~.~h .
.
.
.
p~oh lem:U lo fac tory ( luotao l lO f ree  1 R .
.
.
.
.
.
.
.
.
.
.
.
.
~_I : l l t~ l l~  ~ l U ~  for  l irdrl ' ied ~nl,~iloyt)e~ on ly  is  r iot  su f f idml l  for  Ule i r  t l lEl|hi lR ileel\]l~ .
| I I ~ J '~ 'z l  * ~1 Ifi IhIll tact , Ule wmla 's  m~lst populous sooety  is fac.mg a cnsts Of Cmldoals , | H ?
~ .~.
,  I,~t ~ ~OIk~.
* S 7 mdhort u~aes.~ , morro than dmdde Rle number Of prem;mtal  and irxtr~naunl~l sin( IS nsmg ~ .
| a ~ nt~,rl~rsi~~vr~t~eneed~r~nd~ms~Pu~at~nbur .eaucratsarePmss~n~Umna1~n~a~e~mdus1r~st re~h I l l  ~, .~- .a ,=.
,~ .~,ult~t quk F;Iy by 100 ,uillimz , Io a pear of I..5 bit Ikm a yeot .
| Im -~ .
'~ ,~,~-="Check oflr .
.
.
.
.
.
.
.
.
Imp/ lo t  = .
!
.
.
.
.
O!
SUlppNy ~ aemana Ove?
central plamdrl 9 .
I H ~,~,~P;~'.
"~='=;I m~Jht yea~ ago ,  say  them are l int  not mmJSI1 prophytaallc.~ tO go am*tna.
I I I .,;i*,~,) ;~ .
(31Wna's hwje  ~ of baby boomers ,  born in 'the t~6111S when r~lalrwlan It4ao's OdttlrM SevoIuti011 crippled i l i  '~?
, "the rmtJon'$ emllr~t'oMc blrth-conbl01 ~ , h~re  re~l?1N~d ellildbe~dtn 9 age .
| I II he~havea lsodeve loped loosersexua l  habits : I i l  Jlr~ld~tmtaltlut~lpl~l\]t=t".
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t ?
~ ,.
n,~*..,',..,~.rkrdtrl.t,,-t~,,..t*,z.',,'.
ly,,.4n.,~,,znd.~a~t,~,,,,,a,~,e~i~,m,,,.~_nm 1 ~,a .t~t I ~.
ir--~,~--1 V-Z77--II":: !.
.
.
.
.
.
.
.
.
.
U ~ '~= %% " i. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
M ~; . '
?
? "
:Figure 2: Text retrieval application for the IDUS prototype system.10.
C. N. Ball, D. Dahl, L. M. Norton, L. tlirschman.C.
~Veir.
and M. Liuebarger, "Answers and questions:Processing messages and queries," in Proceedings of theDA RPA Speech and Natural Language Workshop, (CapeCod, MA), Oct. 1989.11.
P. Price, "Evaluation of spoken language systems: theATIS domain," in Proccedings of the Speech and NaturulLanguage Workshop, Morgan Kaufmann, 1990, pp.
91-95.12.
C. Filhnore, "The ease for case," in Bach and Harms,eds., (Universals in Linguistic Theory), pp.
1-88.
NewYork: Holt, Reinhart, and Winston, 1980.13.
D. A. Dahl and C. N. Ball, "Reference resolution inPUNDIT," in P. Saiut-Dizier and S. Szpakowicz, eds.,(Logic and logic grammars \]or iunguage processing).
El-lis Horwood Linfited, 1990.14.
D. S. Pallett, "DARPA resource management andATIS benchmark poster session," in Proceedings of theD A RPA Speech and Language Workshop, (Pacific Grove,CA), February 1991.15.
K. W. Church, "A stochastic parts program and nounphrase parser for unrestricted text," in Proceedings ollthe Second Conilerence on Applied Natural LanguageProcessing, (Austin), ACL, February 1988, pp.
136-143.16.
T. Strzalkowski and B. Vauthey, "Information retrievalusing robust natural language processing," in Proceed-ings off the Thirtieth Annual Meeting of the Associationfor Computational Linguistics, 1992, pp.
104-111.17.
C. Fillmore, "The case for ease reopened," in P. Coh,and J. Sadock, eds., (Syntax and Semantics, ~,~htmr8: Grammatical Relations).
New York: Academic Press,1977.18.
C. T. Hemphill, J. J. Godfrey, and G. R.
Doddington.
"The ATIS  spoken language systems pilot corpus," inProceedings of the DARPA Speech and Lanouage Work-shop, (Hidden Valley, PA), June 1990.19.
M. Marcus, "Very large annotated atabase of AmericanEnglish," ill Proceedings of the DARPA Speech and La,-9uage Workshop, (Hidden Valley, PA), Morgan Kaubmann, June 1990.20.
E. van Herwijnin, Practical SGML Norwell, MA:Kluwer Academic Publishers, 1990.21.
J. Nielsen, Hypertext and Hypermedia.
Academic Prcss,Inc., 1990.426
