A Tra inable Message  Unders tand ing  System*Amit BaggaJoyce Yue ChaiDepartment of Computer ScienceBox 90129, Duke UniversityDurham, NC 27708-0129Internet: {amit, chai}Qcs.duke.edu1 Introduction and BackgroundThe Message Understanding Conferences (MUCs)have given a great impetus to research in informa-tion extraction (IE).
The systems which have par-ticipated in the MUCs have been quite successfulat extracting information from the domains thatthey have been trained on (MUC-4, 1992), (MUC-5, 1993), (MUC-6, 1995).
The precision and recallstatistics were around 60% and 50% respectively forMUC-6.
However, these systems are domain depen-dent and customizing them to a new domain is a longand tedious process.
For example, porting BBN'sPLUM system from the Joint Ventures (MUC-5) do-main to the Microelectronics (MUC-5) domain tookapproximately 3 person weeks (Weischedel, 1993).Moreover, training and adapting these systems toa particular domain is done by a group of compu-tational inguists.
These linguists determine all theways in which the target information is expressed ina given corpus and then think of all the plausiblevariants of these ways, so that appropriate regularpatterns can be written.The explosion in the amount of free text mate-rial on the Internet, and the use of this informationby people from all walks of life, has made the is-sue of generalized information extraction a centralone in Natural Language Processing.
Many sys-tems, including ones from NYU (Grishman, 1995),BBN (Weischedel, 1995), SRI (Appelt, 1995), SRA(Krupka, 1995), MITRE (Aberdeen, 1995), and theUniversity of Massachusetts (Fisher, 1995), havetaken steps to make the process of customizing asystem for a particular domain an easy one.
Appeltet al write, "If information extraction systems aregoing to be used in a wide variety of applications,it will ultimately be necessary for the end users tobe able to customize the systems themselves in arelatively short time."
(Appelt, 1995)We have built a system that attempts to provideSupported by Fellowships from IBM Corporation.any user with the ability to efficiently create andcustomize, for his or her own application, an infor-mation extraction system with competitive precisionand recall statistics.
This paper will present he the-ory of the system and some details of an implemen-tation.
It will also describe a test of the system inwhich a 3 hour training session produced precisionand recall statistics in the 60% levels and above.2 System ArchitectureAs illustrated in Figure 1, there are three mainstages in the running of the system: the TrainingProcess, Rule Generalization, and the Scanning Pro-cess.
During the Training Process, the user, with thehelp of a graphical user interface (GUI), takes a fewprototypical articles from the domain that the sys-tem is being trained on, and creates rules (patterns)for the target information contained in the train-ing articles.
These rules are specific to the trainingarticles and they are generalized so that they canbe run on other articles from the domain.
The RuleGeneralization routines, with the help of WordNet 1,generalize the specific rules generated by the Train-ing Process.
The system can now be run on a largenumber of articles from the domain (Scanning Pro-cess).
The output of the Scanning Process, for eacharticle, is a semantic network (Quillian, 1968) forthat article which can then be used by a Postpro-cessor to fill templates, answer queries, or generateabstracts.2.1 Tools Used By the  System2.1.1 IBM and Local  D ic t ionar iesThe system uses IBM's LanguageWare EnglishDictionary, IBM's Computing Terms Dictionary,and a local dictionary of our choice.
The IBM dictio-l WordNet is an on-line lexical reference system de-veloped by George Miller and his group at PrincetonUniversity.Bagga ~ Chai 1 Trainable Message UnderstandingAmit Bagga and Joyce Yue Chai (1997) A Trainable Message Understanding System.CoNLL97: Computational Natural Language Learning, ACL pp 1-8.
@ 1997 Association for Computational LinguisticsIn T.M.
Ellison (ed.
)d~ \[ Tokenizer, Preprocessor &~/PartialParse~ ~ /"~ ~ ~e-Osage T~ ==- ' \[ Training Interface\[ Rule GeneralizationRoutines ~ ~Wor~et~New ,ArticlePreprocessor & PartialParser 8~ I sense Classifier I <~ \[ Rule Matching Rout/nes \[q=Semantic NetworkFigure 1: The Architecturenaries contain about 150,000 words while the localdictionary contains about 100 words.2.1.2 Gazet teerThe system also uses a gazetteer consisting of ap-proximately 250 names of cities, states, and coun-tries.2.1.3 WordNetThe system also uses WordNet (Miller, 1990).WordNet is an on-line lexical reference system inwhich English nouns, verbs, and adjectives are orga-nized into synonym sets (synsets), each representingone underlying lexical concept (meaning or sense).Different relations link these synsets.
WordNet con-tains approximately 95,600 word forms organizedinto some 70,100 synsets (Miller, 1990).Consider the following example from (Miller,1990): the synsets {board, plank} and {board, com-mittee} each serve as unambiguous designators oftwo different meanings (or senses) of the noun board.Also associated with each synset is a short Englishdescription (gloss) of the meaning expressed by thesynset.
So the synset {board} (a person's meals,provided regularly for money), consisting only ofthe noun board itself, can be distinguished from theother senses by the gloss associated with it.In addition, WordNet attempts to organize differ-ent senses of a word based on the frequency of usageof the senses.
For example, a listing of all the syn-onyms of board yields 8 different synsets, each des-ignating a different sense of the noun.
The mostcommonly used sense (Sense 1) is {board, plank}while the least commonly used sense (Sense 8) is{board} (a flat portable surface (usually rectangu-lar) designed for board games).An important relationship resent in WordNet,that is used extensively by our system, is the hy-ponymy/hypernymy (the subset/superset, or theISA) relationship.
A concept represented by thesynset {x, xl, .
.
.}
is said to be a hyponym ofthe concept represented by the synset {y, yl, .
.
-}if native speakers of English accept sentences con-structed for such frames as An x is a (kind of) y. Ifthis holds then we can also say that {y, Yl, --. }
is ahypernym of {x, Xl, .
.
.
}.
For example, {maple}is a hyponym of {tree}, and {tree} is a hyponym of{plant}.
Hyponymy is transitive and asymmetrical,and it generates a hierarchical semantic structure inwhich a hyponym is said to be below its superordi-nate.More details about WordNet can be found in(Miller, 1990a).2.2 The TokenizerThe Tokenizer accepts ASCII characters as inputand produces a stream of tokens (words) as output.It also determines sentence boundaries.2.3 The  PreprocessorThe Preprocessor accepts the tokens produced bythe Tokenizer as input and produces a "word" stackas output.
While creating the "word" stack, the pre-processor, using finite-state rules, tries to identifysome important entities, like names of companies,proper names, etc., contained in the article.
GroupsBagga 8J Chai 2 Trainable Message Understandingof words that comprise these entities are collectedtogether and put in one slot in the "word" stack.They are considered as one item for all future pro-cessing.The Preprocessor identifies the following entities:?
cities, states, and countries?
names of companies?
software packages?
e-mail and Web addresses?
file and directory names?
dates, times, dollar amounts, telephone num-bers, and Zip codes?
proper names.2.4 Part ia l  ParserThe Partial Parser accepts the word stack producedby the Preprocessor as input and produces a se-quence of non-overlapping phrases as output.
Thesephrases are stored in a phrase stack.
The headwordof each phrase is also identified and stored on thephrase stack.
The parser recognizes noun groups,verb groups and preposition groups.The Partial Parser is a finite-state parser andis largely borrowed from SRI's FASTUS system(Hobbs, 1993).
The parser uses 14 finite-state rulesto identify the noun groups, 7 rules to identify theverb groups, and one rule to identify the prepositiongroups.
The last words of each group are identifiedas their headwords.The output of the parser, the phrase stack, is usedby both the Training and the Scanning processes.2.5 The  Tra in ing  In ter faceThere are two parts to the Training Process: identi-fication of the (WordNet) sense usage of headwordsof interest, and the building of specific rules.
Train-ing is done by a user with the help of a graphicaluser Training Interface.
Figure 2 shows a snapshotof the Training Interface.
The Training Interfacetakes as input the phrase stack produced by the Par-tial Parser.
Sense usage tables, and a collection ofspecific rules are built as a result of the TrainingProcess.2.5.1 Ident i f i ca t ion  of  Sense UsageThe Training Process yields a collection oftraining-article-specific rules which are then gener-alized by the Rule Generalization routines.
Thesegeneralization routines make use of the WordNethypernym relationship by replacing the headwordspresent in a rule with one of its more general super-ordinates in the WordNet hierarchy.
For this to bedone, the Rule Generalization routines must knowthe sense (concept) of usage of the headword in thearticle it appears in, because the WordNet hierarchyis sense dependent.
Moreover, training articles oftencontain headwords that are not used in their mostfrequent sense.
For example, in a domain which ad-vertises job openings, the noun opening will mostlikely be used as an "opportunity for employmentor promotion" (Sense 4), rather than the most com-monly occuring sense: "an open or empty space inor between things" (Sense 1).
Therefore, for eachof the headwords of interest, the user, based on thegloss associated with the senses provided by Word-Net, has to decide which sense is being used in thearticle.
If the user does not train on a particularheadword, then, by default, it is assumed to havebeen used in its most commonly occuring sense.The system, for each headword appearing in thearticle, keeps a count of the frequency of occurrenceof the senses associated with it.
All this informationis stored in the form of a table (Sense-Usage Table)which is later used by the Scanning Process.
Duringthe Scanning Process, the system does not have thehelp of the user to determine what sense of a partic-ular headword is being used.
Neither can the systemsimply assume the most commonly occuring sense.Therefore, the Sense Classifier determines the sensesof the headwords of the phrases based on the Sense-Usage Table built during the Training Process.
Thesense identified by the Sense Classifier is then usedfor all future processing.2.5.2 Bu i ld ing  the  Specif ic  Ru lesThe user builds the collection of rules by actuallybuilding semantic networks for the training articlesusing the Training Interface.
Specifically, the userscans the phrase stack one entry at a t ime and se-lects phrases that he or she feels should be trans-lated to the output semantic network.
Then theselected phrases are translated to nodes or tran-sitions of the network using GUI provided opera-tions.
There are two operations used to build thesemantic network: the ADD.NODE operation andthe ADD.RELATION operation.The ADD_NODE operation allows the user toadd a node to the semantic network while theADD_RELATION operation allows the user toadd a transition between two nodes.
For theADD_RELATION operation, if either (or both) ofthe two nodes do not exist, the ADD_NODE op-eration is automatically invoked and the node isadded to the network.
Since the ADD.RELATIONBagga ~ Chai Trainable Message UnderstandingFigure 2: Snapshot of the Training Interfaceoperation subsumes the ADD.NODE operation,we only consider the ADD_RELATION operation.The system automatically creates and saves anADDA:~ELATION rule for each ADD_RELATIONoperation performed by the user during training.The user executes an ADD_RELATION operationby identifying the two objects (nodes) and the rela-tionship (transition) between them.
For example,consider the phrase stack of the sentence "IBM Cor-poration seeks job candidates in Louisville, KY withHTML experience."
as shown in Figure 2.
Supposethe user executes an ADD.RELATION operation byidentifying IBM Corporation, and job candidates asthe two nodes, and seeks as the transition connectingthe nodes.
The rule corresponding to this operationis shown in Figure 3.
The left hand side (LHS) of therule specifies the conditions that need to be satisfiedfor the RHS to be executed.
For each rule, thereare three conditions in the LHS.
Each condition is a4-tuple consisting of the following fields: the head-word of the phrase, the type of phrase, the WordNetsense number identified by the user (default is 1), theheadword "type" identified by the Preprocessor (de-fault is "other_type").
The three conditions presentin the LHS of the rule need not appear contiguouslyin a sentence of the article.Training a 3000 byte article (approximately 1page) takes approximately 10 minutes.
The num-ber of articles that the system must be trained ondepends on the domain and the user's expectationsof precision and recall.
The more you train the sys-tem, the better the precision and recall.
We arecurrently working on the problem of trying to pre-dict the right number of articles on which the systemmust be trained, for a particular domain, to obtaintarget recall and precision statistics.3 Genera l i za t ionRules created as a result of the Training Process arevery specific and can only be applied to exactly thesame patterns as the ones present during the train-ing.
In order to make the specific rules applicableto a large number of unseen articles in the domain,a comprehensive g neralization mechanism is neces-sary.
We are not only interested in the generalizationitself, but also in the strategy to control the degreeof generalization for various applications in differentdomains.3.1 Degree  of  Genera l i za t ionThe hierarchical organization of WordNet (Miller,1990) provides the possibility of automatic rule gen-eralization of the rules.
Philip Resnik has donesome work earlier in using the hierarchical struc-ture of WordNet (Resnik, 1995a) (Resnik, 19955).With the large amount of information on seman-tic classification and taxonomy provided in Word-Net, many ways of incorporating WordNet's eman-tic features with generalization are foreseeable.
Al-though, at this stage, we only concentrate on theHypernym/Hyponym feature.~.From the training process, the specific rules con-tain three entities on the LHS as shown in Fig-ure 3.
Each entity is a quadruple, in the form ofBagga ~ Chai 4 Trainable Message Understanding\[IBM Corporation, NG, 1,company\], [seek, VG, 1, other_type\], [candidate, NG, 2, other_type\]ADD..NODE(IBM Corporation), ADD_NODE(candidate),ADD__RELATION(seek, IBM Corporation, candidate)Figure 3: A Sample Rule(Wl, el, Sl, tl), (W2, e2, 82, t2),(~3, c3, 83, $3)ADD_NODE(wx), ADD_NODE(w3), ADD_RELATION(w2, wl, w3)Figure 4: An Abstract Specific Rulesp = (w,e,s,t),  where w is the headword of thetrained phrase, c is the part of the speech of theword, s is the sense number epresenting the mean-ing of w, t is the semantic type identified by the pre-processor for w. An abstract specifi c rule is shownin Figure 4.For each sp = (w, e, s, t), if w exists in WordNet,then there is a corresponding synset in WordNet.The hyponym/hypernym hierarchical structure pro-vides a way of locating the superordinate conceptsof sp.
By following additional hypernyms, we willget more and more generalized concepts and eventu-ally reach the most general concept, such as {person,human being,...}.
Based on this scenario, for eachconcept, different degrees of generalization can beachieved by adjusting the distance between this con-cept and the most general concept in the WordNethierarchy.
The function to accomplish this task isGeneralize(sp, h), which returns a synset list h levelsabove the specific concept represented by sp in thehierarchy.
An example is shown in Figure 5.sp = (IBM Corporation, NG, 1, company)generalized at degree 1Generalize(sp, 1) = {business, concem}generalized at degree 2Generalize(sp, 2) = { enterprise}generalized at degree 3Generalize(sp, 3) = {organization}generalized at degree 5Generalize(sp, 5) = {group, social group}Figure 5: Degrees of Generalization for a SpecificConcept3.2 Genera l i zed  Ru lesThe process of generalizing rules consists of replacingeach sp = (w, e, s, t) in the specific rules by a moregeneral superordinate synset from its hypernym treein WordNet by performing the Generalize(sp, h)function.
The degree of generalization for rulesvaries with the variation of h in Generalize(sp, h).For example, Figure 6 shows the rule in Figure 3generalized to two different degrees.Figure 7 shows an abstract generalized rule.
TheC symbol signifies the subsumption relationship.Therefore, a C b signifies that a is subsumed by b,or, in WordNet terms, concept b is a superordinateconcept of concept a.
The generalized rule statesthat the RHS of the rule gets executed if all of thefollowing conditions hold:?
A sentence contains three phrases (not neces-sarily contiguous) with headwords W1, W2, andw3.?
The quadruples corresponding to these head-words are ( W1, C1, S1, T1) , ( W2 , C2, $2,T2), and(W3, C3, $3, T3).?
The synsets, in WordNet, corresponding to thequadruples, are subsumed by Generalize(spl,hi), Generalize(sp2, h2), and Generalize(sp3,h3) respectively.4 Scanning New Art ic lesThe goal of generalizing the rules is to generate se-mantic networks for unseen articles.
The semanticnetworks are built with the help of the ADD.NODEand the ADD.RELATION operations present in theRHS of the rules.
The Scanning Process consists ofthe following steps:?
Parse the unseen article and segment it intophrases belonging to one of NG, VG, or PG(c,).?
Identify the headword (Wi) for each phrase.Bagga ~4 Chai Trainable Message Understanding\[{enterprise}\], \[seek, VG, 1, other_type\], [{applicant}\]> ADD..NODE({enterprise}), ADD_NODE({applicant}),ADD.RELATION(seek, {enterprise}, {applicant})\[{organization}\], \[seek, VG, 1, other_type\], [{person}\]> ADD_NODE({organization}), ADD..NODE({person}),ADD.RELATION(seek, {organization}, {person})Figure 6: Specific Rule in General Forms(Wl, C1, S1, T1) (.~ Generalize(spl, hi), (W2, C2, $2, T2) C Generalize(sp2, h2),(W3, C3, $3, T3) C Generalize(sp3, h3)> ADD_NODE(W1), ADD_NODE(W3), ADD_RELATION(W2,W1, W3)Figure 7: Generalized Rule?
Use the Preprocessor to identify the type (Ti)for each headword.Use the Sense Classifier (as described in Sec-tion 2.5.1 to assign the appropriate sense (Si)to each headword.Each phrase can now be uniquely representedby (W~, C~, Si, 7~).
Match (14~, Ci, Si, 7~) withthe LHS of a generalized rule.If the three entities \[Generalize(spi, h )\] sub-sume three phrases \[(W~, C~, S~, 7~)\], within asingle sentence in the article, the rule is firedand the RHS of the rule executed.If we train on IBM Corporation seeks job candi-dates and generate the rule as in Figure 3, Table 1lists some sentences that can be processed as thedegree of generalization.10095908580757065605550403020100!
i !
i i8 train-arts - -.
i=.- i .
-Z.- .
-SZL.. .
.
.
.
.
.
.~.. .
16 train-arts .
.
.
.
.- -  .
.
.
.
.
- - .
.
.
.
.
.
.
.
.
.
.
.
.
.
.I !
I I I1 2 3 4 5degree of generalizationFigure 8: Precision vs.
Degree of Generalization10095908580757065605550403020100i i i i i8 t ra in -ar ts  - -  "16 t ra in -ar ts  .
.
.
.
.24  t ra ln -ar l s  .
.
.
.
.
.i .
I !o 1 5I !
I2 3 4degree ol generalizationFigure 9: Recall vs.
Degree of Generalization5 Exper imentsWe designed an experiment to investigate how train-ing and the generalization strategy affect meaningextraction.
We trained our system on three sets ofarticles from the triangle.jobs USENET newsgroup,with emphasis on the following seven facts:?
Company Name.
Examples: IBM, Metro Infor-mation Services, DCR Inc.?
Position/Title.
Examples: programmer, finan-cial analyst, software ngineer.?
Experience/Skill.
Example: 5 years experiencein Oracle.?
Location.
Examples: Winston-Salem, NorthCarolina.?
Benefit.
Examples: company matching funds,comprehensive h alth plan.Bagga ~ Chai 6 Trainable Message Understandingdegree Noun Phrase Verb Phrase Noun Phrase0 GTE (any company) seeks, looks for, searches job candidates1 Auction Agency (any business) seeks, looks for, searches bidder2 Motor Factory (any enterprise) seeks, looks for, searches engineers3 Police (any organization) seeks, looks for, searches the fugitive (any person)4 Biology Lab (any group) seeks, looks for, searches missing frog (any life form)Table 1: Sample Sentences that Can Be Processed in the Scanning Part?
Salary.
Examples: $32/hr, 60K.?
Contact Info.
Examples: Fax is 919-660-6519,email address.The first training set contained 8 articles; thesecond set contained 16 articles including the firstset; and the third set contained 24 articles includ-ing those in the first two sets.
For rules from eachtraining set, seven levels of generalization were per-formed.
Based on the generalized rules at each level,the system was run on 80 unseen articles from thesame newsgroup to test its performance on the ex-traction of the seven facts.The evaluation process consisted of the followingstep: first, each unseen article was studied to seehow many facts of interest were present in the ar-ticle; second, the semantic transitions produced bythe system were examined to see if they correctlycaught any facts of interest.
The precision and recallcurves with respect o the degree of generalizationare shown in Figures 8 and 9 respectively.In the precision vs. degree of generalization graph(Figure 8), precision decreases from 96.1% to 68.4%for the first training set as the degree of generaliza-tion increases from 0 to 6.
The first set of eight train-ing articles has better performance on precision.
Thefact that precision decreases with increased numbersof training articles eems to be counter intuitive ini-tially.
But, as the number of training articles in-crease, the the number of rules increase; which in-creases the chance that some piece of irrelevant infor-mation may trigger one of the rules, thereby decreas-ing the precision.
In the recall vs. degree of gener-alization graph (Figure 9), for the third training setof 24 articles, recall increases from 48.2% to 76.1%as generalization degree increases.
As expected, thethird training set out-performed the other two train-ing sets on recall.In Figure 9, there is a jump in recall as we go fromgeneralization degree 3 to generalization degree 4.This gives rise to the following important question:Why does a certain degree of generalization have abig impact on extracting a fact(s)?
Moreover, withthe increase in the degree of generalization, preci-sion tends to fall while recall tends to increase.
Thequestion that arises here is: What degree of general-ization gives us the best compromise between preci-sion and recall?
We are currently conducting furtherresearch that will help us answer such questions.6 ConclusionThis paper describes a trainable system for mean-ing extraction.
The critical parts in the system arethe preprocessor, the partial parser, the training in-terface, the rule interpreter, the rule generalizationroutines, and the rule matching routines.Our system allows a person to train a small num-ber of texts from a particular domain, to get thedesired information from a larger corpus of texts.The training effort is reduced to a few hours and theperson training the system need not be a linguist ordomain expert.7 AcknowledgmentWe wish to thank Jerry Hobbs of SRI for providingus with the finite-state rules for the parser.We also wish to thank our advisor Dr. Alan W.Biermann for all his help, advise, and support.Re ferencesAberdeen, John, et al 1995.
MITRE: Description ofthe ALEMBIC System Used for MUC-6, Pro-ceedings of the Sixth Message UnderstandingConference (MUC-6), pp.
141-155, November1995.Appelt, Douglas E., et al 1995.
SRI International:Description of the FASTUS System Used forMUC-6, Proceedings ofthe Sixth Message Un-derstanding Conference (MUC-6), pp.
237-248, November 1995.Fisher, David, et al 1995.
Description of the UMassSystem as Used for MUC-6, Proceedings ofthe Sixth Message Understanding Conference(MUC-6), pp.
127-140, November 1995.Bagga 8J Chai 7 Trainable Message UnderstandingGrishman, Ralph.
1995.
The NYU System forMUC-6 or Where's the Syntax?
Proceedingsof the Sixth Message Understanding Confer-ence (MUC-6), pp.
167-175, November 1995.Hobbs, J., et al 1995.
FASTUS: A system for Ex-tracting Information from Text, Human Lan-guage Technology, pp.
133-137, 1993.Krupka, George R. 1995.
Description of the SRASystem as Used for MUC-6, Proceedings ofthe Sixth Message Understanding Conference(MUC-6), pp.
221-235, November 1995.Miller, G.A.
1990.
Introduction to WordNet: AnOn-Line Lexical Database.
WordNet Manuals,pp.
10-32, August 1993.Miller, G.A., et al 1990a.
Five Papers on Word-Net, Cognitive Science Laboratory, PrincetonUniversity, No.
43, July 1990.Proceedings of the Fourth Message UnderstandingConference (MUC-4), June 1992, San Mateo:Morgan Kaufmann.Proceedings of the Fifth Message UnderstandingConference (MUC-5), August 1993, San Ma-teo: Morgan Kaufmann.Proceedings of the Sixth Message UnderstandingConference (MUC-6), November 1995, SanFrancisco: Morgan Kaufmann.Quillian, M. Ross.
1968 Semantic Memory, InSemantic Information Processing, M.
Minsky(ed.).
Cambridge, Massachusetts: MIT Press,1968, pp.
216-270.Resnik, Philip.
1995a Using Information Content oEvaluate Semantic Similarity in a Taxonomy,Proceedings ofIJCAI-95, 1995.Resnik, Philip.
1995b Disambiguating Noun Group-ings With Respect o WordNet Senses, ThirdWorkshop on Very Large Corpora, 1995.Weischedel, R., et al 1993.
BBN: Description of thePLUM System as Used for MUC-5, Proceed-ings of the Fifth Message Understanding Con-ference (MUC-5), pp.
93-107, August 1993.Weischedel, Ralph.
1995.
BBN: Description of thePLUM System as Used for MUC-6, Proceed-ings of the Sixth Message Understanding Con-ference (MUC-6), pp.
55-69, November 1995.Bagga ~ Chai 8 Trainable Message Understanding
