Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 67?71,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsAn Empirical Study on Development Set Selection Strategyfor Machine Translation Learning?Cong Hui12, Hai Zhao12?, Yan Song3, Bao-Liang Lu121Center for Brain-Like Computing and Machine IntelligenceDepartment of Computer Science and Engineering, Shanghai Jiao Tong University2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent SystemsShanghai Jiao Tong University, 800 Dong Chuan Rd., Shanghai 200240, China3Department of Chinese, Translation and Linguistics, City University of Hong Konghuicong@sjtu.edu.cn, {zhaohai,blu}@cs.sjtu.edu.cnAbstractThis paper describes a statistical machinetranslation system for our participationfor the WMT10 shared task.
Based onMOSES, our system is capable of translat-ing German, French and Spanish into En-glish.
Our main contribution in this workis about effective parameter tuning.
Wediscover that there is a significant perfor-mance gap as different development setsare adopted.
Finally, ten groups of devel-opment sets are used to optimize the modelweights, and this does help us obtain a sta-ble evaluation result.1 IntroductionWe present a machine translation system that rep-resents our participation for the WMT10 sharedtask from Brain-like Computing and Machine In-telligence Lab of Shanghai Jiao Tong University(SJTU-BCMI Lab).
The system is based on thestate-of-the-art SMT toolkit MOSES (Koehn et al,2007).
We use it to translate German, French andSpanish into English.
Though different develop-ment sets used for training parameter tuning willcertainly lead to quite different performance, weempirically find that the more sets we combine to-gether, the more stable the performance is, and adevelopment set similar with test set will help theperformance improvement.2 System DescriptionThe basic model of the our system is a log-linearmodel (Och and Ney, 2002).
For given source lan-?This work was partially supported by the National Natu-ral Science Foundation of China (Grant No.
60903119, GrantNo.
60773090 and Grant No.
90820018), the National BasicResearch Program of China (Grant No.
2009CB320901), andthe National High-Tech Research Program of China (GrantNo.2008AA02Z315).
?corresponding authorguage strings, the target language string t will beobtained by the following equation,t?I1 =argmaxtI1{p?m1 (tI1 | sJ1 )}=argmaxtI1{ exp[?Mm=1 ?mhm(tI1, sJ1 )]?t?I1 exp[?Mm=1 ?mhm(t?I1, sJ1 )]},where hm is the m-th feature function and ?m isthe m-th model weight.
There are four main partsof features in the model: translation model, lan-guage model, reordering model and word penalty.The whole model has been well implemented bythe state-of-the-art statistical machine translationtoolkit MOSES.For each language that is required to translatedinto English, two sets of bilingual corpora are pro-vided by the shared task organizer.
The first setis the new release (version 5) of Europarl cor-pus which is the smaller.
The second is a com-bination of other available data sets which is thelarger.
In detail, two corpora, europarl-v5 andnews-commentary10 are for German, europarl-v5and news-commentary10 plus undoc for Frenchand Spanish, respectively.
Details of training dataare in Table 1.
Only sentences with length 1 to 40are acceptable for our task.
We used the larger setfor our primary submission.We adopt word alignment toolkit GIZA++ (Ochand Ney, 2003) to learn word-level alignment withits default setting and grow-diag-final-and param-eters.
Given a sentence pair and its correspondingword-level alignment, phrases will be extracted byusing the approach in (Och and Ney, 2004).
Phraseprobability is estimated by its relative frequencyin the training corpus.
Lexical reordering is deter-mined by using the default setting of MOSES withmsd-bidirectional parameter.For training the only language model (English),the data sets are extracted from monolingual partsof both europarl-v5 and news-commentary10,67sentences words(s) words(t)de small 1540549 35.76M 38.53Mlarge 1640818 37.95M 40.64Mfr small 1683156 44.02M 44.20Mlarge 8997997 251.60M 228.50Mes small 1650152 43.17M 41.25Mlarge 7971200 236.24M 207.79MTable 1: Bilingual training corpora from Ger-man(de), French(fr) and Spanish(es) to English.which include 1968914 sentences and 47.48Mwords.
And SRILM is adopted with 5-gram, in-terpolate and kndiscount settings (Stolcke, 2002).The next step is to estimate feature weights byoptimizing translation performance on a develop-ment set.
We consider various combinations of 10development sets with 18207 sentences to get astable performance in our primary submission.We use the default toolkits which are providedby WMT10 organizers for preprocessing (i.e., to-kenize) and postprocessing (i.e., detokenize, re-caser).3 Development Set Selection3.1 MotivationGiven the previous feature functions, the modelweights will be obtained by optimizing the follow-ing maximum mutual information criterion, whichcan be derived from the maximum entropy princi-ple:?
?M1 = argmax?M1{S?i=1log p?M1 (ti | si)}As usual, minimum error rate training (MERT) isadopted for log-linear model parameter estimation(Och, 2003).
There are many improvements onMERT in existing work (Bertoldi et al, 2009; Fos-ter and Kuhn, 2009), but there is no demonstrationthat the weights with better performance on thedevelopment set would lead to a better result onthe unseen test set.
In our experiments, we foundthat different development sets will cause signifi-cant BLEU score differences, even as high as onepercent.
Thus the remained problem will be howto effectively choose the development set to obtaina better and more stable performance.3.2 Experimental SettingsOur empirical study will be demonstrated throughGerman to English translation on the smaller cor-pus.
The development sets are all developmentsets and test sets from the previous WMT sharedtranslation task as shown in Table 2, and labeledas dev-0 to dev-9.
Meanwhile, we denote 10 batchsets from batch-0 to batch-9 where the batch-i setis the combination of dev- sets from dev-0 to dev-i.The test set is newstest2009, which includes 2525sentences, 54K German words and 58K Englishwords, and news-test2008, which includes 2051sentences, 41K German words and 43K Englishwords.id name sent w(de) w(en)dev-0 dev2006 2000 49K 53Kdev-1 devtest2006 2000 48K 52Kdev-2 nc-dev2007 1057 23K 23Kdev-3 nc-devtest2007 1064 24K 23Kdev-4 nc-test2007 2007 45K 44Kdev-5 nc-test2008 2028 45K 44Kdev-6 news-dev2009 2051 41K 43Kdev-7 test2006 2000 49K 54Kdev-8 test2007 2000 49K 54Kdev-9 test2008 2000 50K 54KTable 2: Development data.3.3 On the Scale of Development SetHaving 20 different development sets (10 dev- setsand batch- sets), 20 models are correspondinglytrained.The decode results on the test set are sum-marized in Table 3 and Figure 1.
The dotted linesare the performances of 10 different developmentsets on the two test sets, we will see that thereis a huge gap between the highest and the lowestscore, and there is not an obvious rule to follow.
Itwill bring about unsatisfied results if a poor devel-opment set is chosen.
The solid lines representsthe performances of 10 incremental batch sets onthe two test sets, the batch processing still gives apoor performance at the beginning, but the resultsbecome better and more stable when the develop-ment sets are continuously enlarged.
This sort ofresults suggest that a combined development setmay produce reliable results in the worst case.
Ourprimary submission used the combined develop-ment set and the results as Table 4.68id 09-dev 09-batch 08-dev 08-batch0 16.46 16.46 16.38 16.381 16.67 16.25 16.66 16.442 16.74 16.20 16.94 16.223 16.15 16.83 16.18 17.024 16.44 16.73 16.64 16.895 16.50 16.97 16.75 17.136 17.15 17.03 17.67 17.247 16.51 17.00 16.34 17.098 17.03 16.97 17.15 17.229 16.25 16.99 16.24 17.26Table 3: BLEU scores on the two testsets(newstest2009 & news-test2008), which usetwo data set sequences(dev- sequence & batch- se-quence) to optimize model weights.de-en fr-en es-en18.90 24.30 26.40Table 4: BLEU scores of our primary submission.3.4 On BLEU Score DifferenceTo compare BLEU score differences between testset and development set, we consider two groupsof BLEU score differences, For each developmentset, dev-i, the BLEU score difference will be com-puted between b1 from which adopts itself as thedevelopment set and b2 from which adopts testset as the development set.
For the test set, theBLEU score difference will be computed betweenb?1 from which adopts each development set, dev-i,as the development set and b?2 from which adoptsitself as the development set.These two groups of results are illustrated inFigure 2 (the best score of the test set under selftuning, newstest2009 is 17.91).
The dotted lineshave the inverse trend with the dotted in Figure1(because the addition of these two values is con-stant), and the solid lines have the same trendwith the dotted, which means that the good per-formance is mutual between test set and develop-ment sets: if tuning using A set could make a goodresult over B set, then vice versa.3.5 On the Similarity between DevelopmentSet and Test SetThis experiment is motivated by (Utiyama et al,2009), where they used BLEU score to measurethe similarity of a sentences pair and then ex-tracted sentences similar with those in test set to0 1 2 3 4 5 6 7 8 900.511.522.5DATA SET IDDIFF ofBLEU SCORE09?Ddev09?DtestFigure 2: The trend of BLEU score differencesconstruct a specific tuning set.
In our experiment,we will try to measure data set similarity instead.Given two sets of sentences, one is called as candi-date(cnd) set and the other reference(ref) set.
Forany cnd sentence, we let the whole ref set to be itsreference and then multi-references BLEU score iscomputed for cnd set.
There comes a problem thatthe sentence penalty will be constant for any cndsentence, we turn to calculate the average lengthof whose sentences which have common n-gramwith the given cnd sentence.Now we may define three measures.
The mea-sure which uses dev- and batch- sets as cnd setsand news-test2009 set as ref set is defined asprecision-BLEU , and the measure which uses theabove sets on the contrary way is defined as recall-BLEU.
Then F1-BLEU is defined as the harmonicmean of precision-BLEU and recall-BLEU.
Theseresults are illustrated in Figure 3.
From the fig-ure, we find that F1-BLEU plays an importantrole to predict the goodness of a development set,F1-BLEU scores of batch- sets have an ascendingcurve and batch data set sequence will cause a sta-ble good test performance, the point on dev- setswhich has high F1-BLEU(eg, dev-0,4,5) wouldalso has a good test performance.3.6 Related WorkThe special challenge of the WMT shared task isdomain adaptation, which is a hot topic in recentyears and more relative to our experiments.
Manyexisting works are about this topic (Koehn andSchroeder, 2007; Nakov, 2008; Nakov and Ng,2009; Paul et al, 2009; Haque et al, 2009).
How-ever, most of previous works focus on language69model, translation phrase table, lexicons modeland factored translation model, few of them payattention to the domain adaptation on the develop-ment set.
For future work we consider to use somemachine learning approaches to select sentences indevelopment sets more relevant with the test set inorder to further improve translation performance.4 ConclusionIn this paper, we present our machine translationsystem for the WMT10 shared task and perform anempirical study on the development set selection.According to our experimental results, Choosingdifferent development sets would play an impor-tant role for translation performance.
We find thata development set with higher F1-BLEU yieldsbetter and more stable results.ReferencesNicola Bertoldi, Barry Haddow, and Jean BaptisteFouet.
2009.
Improved Minimum Error Rate Train-ing in Moses.
The Prague Bulletin of MathematicalLinguistics, 91:7?16.George Foster and Roland Kuhn.
2009.
Stabiliz-ing minimum error rate training.
In Proceedingsof the 4th Workshop on Statistical Machine Trans-lation(WMT), Boulder, Colorado, USA.Rejwanul Haque, Sudip Kumar Naskar, Josef Van Gen-abith, and Andy Way.
2009.
Experiments on Do-main Adaptation for English?Hindi SMT.
In 7th In-ternational Conference on Natural Language Pro-cessing(ICNLP), Hyderabad, India.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine transla-tion.
In Proceedings of the 2nd Workshop on Sta-tistical Machine Translation(WMT), Prague, CzechRepublic.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine trans-lation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguis-tics(ACL), Prague, Czech Republic.Preslav Nakov and Hwee Tou Ng.
2009.
NUSat WMT09: domain adaptation experiments forEnglish-Spanish machine translation of news com-mentary text.
In Proceedings of the 4th Workshop onStatistical Machine Translation(WMT), Singapore.Preslav Nakov.
2008.
Improving English-Spanish sta-tistical machine translation: Experiments in domainadaptation, sentence paraphrasing, tokenization, andrecasing.
In Proceedings of the 3rd Workshop onStatistical Machine Translation(WMT), Columbus,Ohio, USA.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics(ACL), Philadelphia, Pennsylva-nian, USA.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41th Annual Meeting of the Association for Compu-tational Linguistics(ACL), Sapporo, Japan.Michael Paul, Andrew Finch, and Eiichiro Sumita.2009.
NICT@ WMT09: model adaptation andtransliteration for Spanish-English SMT.
In Pro-ceedings of the 4th Workshop on Statistical MachineTranslation(WMT), Singapore.Andreas Stolcke.
2002.
SRILM: an extensible lan-guage modeling toolkit.
In 7th International Con-ference on Spoken Language Processing(ICSLP),Denver, Colorado, USA.Masao Utiyama, Hirofumi Yamamoto, and EiichiroSumita.
2009.
Two methods for stabilizing MERT:NICT at IWSLT 2009.
In Proceedings of Inter-national Workshop on Spoken Language Transla-tion(IWSLT), Tokyo, Japan.700 1 2 3 4 5 6 7 8 91515.51616.51717.518DATA SET IDBLEUSCORE09?dev09?batch08?dev08?batchFigure 1: The BLEU score trend in Tabel 3, we will see that the batch lines output a stable and goodperformance.0 1 2 3 4 5 6 7 8 91015202530DATA SET IDBLEUVALUEpDevpBatchrDevrBatchfDevfBatchFigure 3: The precision(p), recall(r) and F1(f) BLEU score on the dev(Dev) and batch(Batch) sets basedon the comparison with news-test2009 set.71
