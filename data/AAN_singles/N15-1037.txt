Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 335?344,Denver, Colorado, May 31 ?
June 5, 2015. c?2015 Association for Computational LinguisticsInterpreting Compound Noun Phrases Using Web Search QueriesMarius Pas?caGoogle Inc.1600 Amphitheatre ParkwayMountain View, California 94043mars@google.comAbstractA weakly-supervised method is applied toanonymized queries to extract lexical interpre-tations of compound noun phrases (e.g., ?for-tune 500 companies?).
The interpretationsexplain the subsuming role (?listed in?)
thatmodifiers (fortune 500) play relative to heads(companies) within the noun phrases.
Exper-imental results over evaluation sets of nounphrases from multiple sources demonstratethat interpretations extracted from querieshave encouraging coverage and precision.
Thetop interpretation extracted is deemed relevantfor more than 70% of the noun phrases.1 IntroductionMotivation: Semantic classes of interest to Webusers are often expressed as lexical class labels (e.g.,?fortune 500 companies?, ?italian composers?,?victorinox knives?).
Each class label hints at theimplicit properties shared among its instances (e.g.,general electric, gaetano donizetti, swiss army jet-setter respectively).
Class labels allow for the orga-nization of instances into hierarchies, which in turnallows for the systematic development of knowl-edge repositories.
This motivates research effortsto acquire as many relevant class labels of instancesas possible, which have received particular empha-sis (Wang and Cohen, 2009; Dalvi et al, 2012; Flatiet al, 2014).
The efforts are part of the larger area ofextracting open-domain facts and relations (Bankoet al, 2007; Hoffart et al, 2013; Yao and VanDurme, 2014), ultimately delivering richer results inWeb search.Different methods can associate instances (gen-eral electric) with both class labels (?fortune 500companies?)
and facts (<general electric, foundedin, 1892>) extracted from text.
But the class labelstend to be extracted, maintained and used separatelyfrom facts.
Beyond organizing the class labels hi-erarchically (Kozareva and Hovy, 2010), the mean-ing of a class label is rarely explored (Nastase andStrube, 2008), nor is it made available downstreamto applications using the extracted data.Contributions: The method introduced in this paperis the first to exploit Web search queries to uncoverthe semantics of open-domain class labels in par-ticular; and of compound noun phrases in general.The method extracts candidate, lexical interpreta-tions of compound noun phrases from queries.
Theinterpretations turn implicit properties or subsumingroles (?listed in?, ?from?, ?made by?)
that mod-ifiers (fortune 500, italian, victorinox) play withinlonger noun phrases (?fortune 500 companies?,?italian composers?, ?victorinox knives?)
into ex-plicit strings.
The roles of modifiers relative to headsof noun phrase compounds cannot be characterizedin terms of a finite list of possible compounding re-lationships (Downing, 1977).
Hence, the interpreta-tions are not restricted to a closed, pre-defined set.Experimental results over evaluation sets of nounphrases from multiple sources demonstrate that in-terpretations can be extracted from queries for a sig-nificant fraction of the input noun phrases.
Withoutrelying on syntactic analysis, extracted interpreta-tions induce implicit bracketings over the interpretednoun phrases.
The bracketings reveal the multiplesenses, some of which are more rare but still plau-sible, in which the same noun phrase can be some-times explained.
The quality of interpretations is en-couraging, with at least one interpretation deemedrelevant among the top 3 retrieved for 77% of the335noun phrases with extracted interpretations.
The topinterpretation is deemed relevant for more than 70%of the noun phrases.Applications: The extracted interpretations canserve as a bridge connecting class labels and facts.Relevant interpretations allow one to potentially de-rive missing facts (<general electric, listed in, for-tune 500>) from existing class labels (<generalelectric, fortune 500 companies>) and vice versa.In addition, relevant interpretations of class la-bels are themselves class labels inferred for thesame instances.
Examples are <general electric,companies listed in fortune 500>, or <generalelectric, companies in fortune 500>, based on<general electric, fortune 500 companies>.
Ifthe input class labels are organized hierarchi-cally (<fortune 500 companies, companies>), in-terpretations explain why more specific class la-bels (?fortune 500 companies?, ?german com-panies?, ?dow jones industrial average compa-nies?, ?french companies?)
do not merely belongunder more general ones (?companies?
), but doso along shared interpretations (companies?listedin?
{fortune 500, dow jones industrial averagecompanies}; vs.
{companies?from?
{germany,france}); and, more generally, aid in the better un-derstanding of noun phrases.2 Interpreting Noun PhrasesHypothesis: Let N be a compound noun phrase,containing a head H preceded by modifiers M .Each of H and M may contain one or multiple to-kens.
Being a compound, the sequence of modifiersand head in N act as a single noun (Downing, 1977;Hendrickx et al, 2013).
IfN is relevant and of inter-est to Web users, then in a sufficiently large corpus itwill eventually be referred to in relatively more ver-bose search queries, which explain the implicit rolethat modifiers M play relative to the head H .Acquisition from Queries: To illustrate the intu-ition above, consider the noun phrases ?water an-imals?
and ?zone 7 plants?.
If enough Web usersare interested in the concepts represented by thesenoun phrases, then the phrases are likely to be sub-mitted as search queries.
In addition, some Webusers seeking similar information are likely to sub-mit queries that make the role of the modifiers waterand zone 7 explicit, such as ?animals living in wa-ter?
or ?plants that grow in zone 7?.As illustrated in Figure 1, the extraction methodanimals that grow in water   animals who live in water   plants that grow in zone 11plants that grow in zone 7   plants that grow well in zone 7   plants for zone 10animals living in coral reef   animals living in freshwater   animals living in waterplants for zone 7   plants for planting zone 10   plants for planting zone 7justices of the washington state supreme court   supreme court justices in ohiosupreme court justices in oregon   supreme court justices in washington statesupreme court justices born in new jersey   justices of the vermont supreme courtjustices of the warren court   supreme court justices from new hampshirejustices of the australian high court   justices of the california supreme courtaquatic animalsNoun phraseNoun phrasezone 7 plantsNoun phraseNoun phrasewater animals Source queriesanimals who live in waterSource queriesplants that grow in zone 7plants that grow in zone 11Source queriesanimals living in coral reefSource queriesanimals living in freshwateranimals living in waterjustices of the california supreme courtjustices of the australian high courtjustices of the washington state supreme courtjustices of the warren courtcourt justicescalifornia supremeSource queriesCandidate interpretations for noun phrasesaquatic animalswater animalszone 7 plantscalifornia supreme court justices(supreme court justices)H born in (california)M(plants)H that grow in (zone 7)M(animals)H living in (water)M(animals)H who live in (water)M(animals)H living in (water)M(justices)H of the (california supreme court)M(animals)H who live in (water)Mcalifornia supreme courtwashington state supreme courtvermont supreme courtwarren courtwaterairpond wateraquaticwatercoral reeffreshwaterwetlandzone 7zone 10zone 11zone 6zone 8californiaoregonnew hampshirenew jerseyohiofreshwatercoral reefVariants (siblings)supreme court justices born in new jerseyQuery logsQueriesFigure 1: Overview of extraction of interpretations ofnoun phrases from Web search queriesproposed in this paper takes as input a vocabulary ofnoun phrases, as well as a set of anonymized queriesfrom which possible interpretations for the nounphrases must be extracted.
The extraction consists ofseveral steps: (1) the selection of a subset of queriesthat may be candidate interpretations of some yet-to-be-specified noun phrases; (2) the matching of theselected queries to the noun phrases to interpret; and(3) the aggregation of matched queries into candi-date interpretations extracted for a noun phrase.Queries as Candidate Interpretations: The in-put queries are matched against the extraction pat-terns from Table 1.
The use of targeted patternsin information extraction has been suggested be-fore (Hearst, 1992; Fader et al, 2011).
In ourcase, the patterns match queries that start with anarbitrary ngram H , followed by what is likely a336Extraction Pattern?
Examples of Matched QueriesPassive constructs:H [VBN|VBD|VBG] [<anything>] M [<anything>]?
(plants)Hgrown in (zone 7)M?
(supreme court justices)Hborn in (new jersey)M?
(medicinal plants)Hused as (ayurvedic)Mdrugs?
(manipulatives)Hused in (elementary math)MPrepositional constructs:H [IN|TO] [<anything>] M [<anything>]?
(plants)Hfor (zone 7)M?
(justices)Hof the (california supreme court)M?
(medicinal plants)Hin (ayurvedic)Mproducts?
(math manipulatives)Hfor (elementary)MlevelRelative pronoun constructs:H [that|who|which] [<anything>] M [<anything>]?
(plants)Hthat grow in (zone 7)M?
(animals)Hwho live in (water)M?
(medicinal plants)Hthat are used in (ayurveda)M?
(math manipulatives)Hthat are taught in the(elementary)MclassroomTable 1: Extraction patterns matched against queries toidentify candidate interpretations (H , M=head and mod-ifier of a hypothetical noun phrase)passive, prepositional or relative-pronoun construct,followed by another ngram M , and optionally fol-lowed by other tokens.
The ngrams H and Mcontain one or more tokens.
The patterns effec-tively split matching queries into four consecutivesequences of tokens Q=[Q1Q2Q3Q4], where Hand M correspond to Q1and Q3, and Q4may beempty.
For example, the pattern in the lower portionof Table 1 matches the query ?
(plants)Hthat growin (zone 7)M?, which is one of the queries shown inthe upper portion of Figure 1.Mapping Noun Phrases to Interpretations: Eachnoun phrase to interpret is split into all possible de-compositions of two consecutive sequences of to-kens N=[N1N2], where the two sequences corre-spond to a hypothetical modifier and a hypotheticalhead of the noun phrase.
For example, the nounphrase ?zone 7 plants?
is split into [?zone?, ?7plants?]
and separately into [?zone 7?, ?plants?].
IfN1andQ3, andN2andQ1respectively, match, thenthe matching query Q (e.g., ?
(plants)Hthat grow in(zone 7)M?)
is retained as a candidate interpretationof the noun phrase N (?
(zone 7)M(plants)H?
), asshown in the middle portion of Figure 1.Mapping via Modifier Variants: At its simplest,the matching of the hypothetical modifier relies onstrict string matching.
Alternatively, original modi-fiers in the noun phrases to interpret may be matchedto queries via expansion variants.
Variants arephrases that likely play the same role, and there-fore share interpretations, as modifiers relative tothe head in a noun phrase.
Variants allow for theextraction of candidate interpretations that may oth-erwise not be available in the input data.
For ex-ample, in Figure 1, the variant new jersey availablefor california allows for the matching of californiain the noun phrase ?
(california)M(supreme courtjustices)H?, with new jersey in the query ?
(supremecourt justices)Hborn in (new jersey)M?.
The candi-date interpretation ?
(supreme court justices)Hbornin (california)M?
is extracted for the noun phrase?
(california)M(supreme court justices)H?, eventhough the query ?supreme court justices born incalifornia?
is not present among the input queries.Possible sources of variants include distribution-ally similar phrases (Lin and Wu, 2009), where thephrases most similar to a modifier would act asits variants.
Mappings from adjectival modifiers innoun phrases (e.g., aquatic in ?aquatic animals?
inFigure 1) into the nominal counterparts (e.g., wa-ter) that are likely to occur in interpretations (e.g.,?
(animals)Hwho live in (water)M?)
are also useful.Concretely, as described later in Section 3, variantsare generated using WordNet (Fellbaum, 1998), dis-tributional similarities and Wikipedia.Aggregation of Candidate Interpretations: Can-didate interpretations of a noun phrase are aggre-gated from source queries that matched the nounphrase.
The frequency score of a candidate inter-pretation is the weighted sum of the frequencies ofsource queries from which the candidate interpre-tation is collected, possibly via variants of modi-fiers.
In the weighted sum, the weights are similarityscores between the original modifier from the nounphrase, on one hand, and the variant from the sourcequery into which the modifier was mapped, on theother hand.
For example, in Figure 1, the frequencyscore of the candidate interpretation ?
(plants)Hthatgrow in (zone 7)M?
for the noun phrase ?
(zone 7)M(plants)H?
is the weighted sum of the frequencies ofthe source queries ?plants that grow in zone 7?
and?plants that grow in zone 11?.
The weights for thevariants zone 7 and zone 11 relative to the originalmodifier zone 7 may be 1.0 (identity) and 0.8 (distri-butional similarity), whereas the weights of adjecti-val modifiers such as water for aquatic may be 1.0.Separately from the frequency score, a penalty scoreis computed that penalizes interpretations containingextraneous tokens.
Specifically, the penalty counts337the number of nouns or adjectives located outsidethe modifier and head.
Candidate interpretations ex-tracted for a noun phrase are ranked in increasingorder of their penalty scores or, in case of ties, indecreasing order of their frequency scores.3 Experimental SettingSources of Textual Data: The experiments relyon a random sample of around 1 billion fully-anonymized Web search queries in English.
Thesample is drawn from queries submitted to a general-purpose Web search engine.
Each query is availableindependently from other queries, and is accompa-nied by its frequency of occurrence in the query logs.Sources of Variants: The original form of the mod-ifiers is denoted as orig-phrase.
Three types of vari-ant phrases are collected for the purpose of match-ing modifiers within noun phrases to interpret, withphrases from queries.
Relations encoded as Value-Of, Related-Noun and Derivationally-Related rela-tions in WordNet (Fellbaum, 1998) are the source ofadj-noun variants.
They map around 6,000 adjec-tives into one or more nouns (e.g., (french?france),(electric?electricity), (aquatic?water)).
A repos-itory of distributionally similar phrases, collectedin advance (Lin and Wu, 2009) from a sample ofaround 200 million Web documents in English, isthe source of dist-sim variants.
For each of around 1million phrases, the variants consist of their 50 mostsimilar phrases (e.g., art garfunkel?
{carly simon,melissa manchester, aaron neville, ..}).A snapshot of all Wikipedia articles in English, asavailable in June 2014, is the source of wiki-templvariants.
For each of around 50,000 phrases, theirwiki-templ variants are collected from Wikipediacategories sharing a common parent Wikipedia cat-egory (e.g., ?albums by artist?)
and having a com-mon head (?art garfunkel albums?, ?black sabbathalbums?, ?metallica albums?).
The different mod-ifiers (art garfunkel, black sabbath, metallica) thataccompany the shared head are collected as vari-ants of one another.
Among the four types of vari-ants, wiki-templ variants are applied only when thenoun phrase to interpret, and the source Wikipediacategory names from which the variants were col-lected, have the same head.
For example, X=artgarfunkel?
{black sabbath, metallica, 50 cent, ..}is applied only in the context of the noun phrase ?Xalbums?.Vocabularies of Noun Phrases: The extractionVocabulary Relative CoverageR Q I I / QListQ 406,249 406,249 277,193 0.682IsA 613,148 405,262 282,927 0.698WikiC 248,615 87,878 63,518 0.723Table 2: Relative coverage of noun phrase interpretation,over noun phrases from various vocabularies (R=numberof raw noun phrases; Q=subset of noun phrases from Rthat are queries; I=subset of noun phrases from Q withsome extracted interpretation(s); I/Q=fraction of nounphrases from Q that are present in I)method acquires interpretations from queries, fornoun phrases from three vocabularies.
ListQ is a setof phrases X (e.g., ?aramaic words?)
from queries inthe form [list of X], where the frequency of the query[X] is at most 100 times higher than the frequency ofthe query [list of X], and the frequency of the latter isat least 5.
IsA is a set of class labels (e.g., ?academyaward nominees?
), originally extracted from Webdocuments via Hearst patterns (Hearst, 1992), andassociated with at least 25 instances each (e.g., zerodark thirty).
WikiC is a set of Wikipedia categoriesthat contain some tokens in lowercase beyond prepo-sitions and determiners, and whose heads are plural-form nouns (e.g., ?french fiction writers?).
Onlyphrases that are one of the full-length queries fromthe input set of Web search queries are retained inthe respective sets, as vocabularies of noun phrasesto interpret; other phrases are discarded.Parameter Settings: The noun phrases to interpretand queries are both part-of-speech tagged (Brants,2000).
From among candidate interpretations ex-tracted for a noun phrase, interpretations whosepenalty score is higher than 1 are discarded.
Whencomputing the frequency score of a candidate in-terpretation as the weighted sum of the frequenciesof source queries, the weights assigned to variousvariants are 1.0, for orig-phrase, adj-noun and wiki-templ variants; and the available distributional simi-larity scores within [0.0, 1.0], for dist-sim variants.4 Evaluation ResultsRelative Coverage: Because it is not feasible tomanually compile the exhaustive sets of all stringforms of valid interpretations of all (or many) nounphrases, we compute relative instead of absolutecoverage.
As illustrated in Table 2, some inter-pretations are extracted from queries for more than500,000 of the noun phrases from all input vocabu-338Gold Set: Sample of Noun PhrasesListQ: 1911 pistols, 2009 movies, alabama sororities,alaskan towns, american holidays, aramaic words, argumen-tative essays, arm loans, army ranks, .., yugioh moviesIsA: academy award nominees, addicting games, advancedweapons systems, android tablet, application layer protocols,astrological signs, automotive parts, .., zip codeWikiC: 2k sports games, aaliyah songs, advertising slogans,airline tickets, alan jackson songs, ancient romans, andreabocelli albums, athletic shoes, .., wii accessoriesTable 3: Gold sets of 100 noun phrases per vocabularylaries, or around 70% of all input noun phrases.Precision of Interpretations: From an input vo-cabulary, an initial weighted sample of 150 nounphrases with some extracted interpretations is manu-ally inspected.
The sampling weight is the frequencyof the noun phrases as queries.
A noun phrasefrom the selected sample is either retained, or dis-carded if deemed to be a non-interpretable phrase.A noun phrase is not interpretable if it is in fact aninstance (?new york?, ?alicia keys?)
rather than aclass; or it is not a properly formed noun phrase(?watch movies?
); or does not refer to a meaningfulclass (?3 significant figures?).
The manual inspec-tion ends, once a sample of 100 noun phrases hasbeen retained.
The procedure gives weighted ran-dom samples of 100 noun phrases, drawn from eachof the ListQ, IsA and WikiC vocabularies.
The sam-ples, shown in Table 3, constitute the gold sets ofphrases ListQ, IsA and WikiC, over which precisionof interpretations is computed.
Note that, since thesamples are random, Wikipedia categories that con-tribute to the automatic construction of wiki-templvariants may be selected as gold phrases in WikiC.This is the case for three of the gold phrases in Wi-kiC.The top 20 interpretations extracted for each goldphrase are manually annotated with correctness la-bels.
As shown in Table 4, an interpretation is an-notated as: correct and generic, or correct and spe-cific, if relevant; okay, if useful but containing non-essential information; or wrong.
To compute theprecision score over a gold set of phrases, the cor-rectness labels are converted to numeric values.
Pre-cision of a ranked list of extracted interpretations isthe average of the correctness values of the interpre-tations in the list.Table 5 provides a comparison of precision scoresat various ranks in the extracted lists of interpreta-tions, as an average over all phrases from a gold set.Label (Score) ?
Examples of (Noun Phrase: Interpretation)cg (1.0) ?
(good short stories: short stories that are good),(bay area counties: counties in the bay area), (fourth gradesight words: sight words in fourth grade), (army ranks: ranksfrom the army), (who wants to be a millionaire winners: win-ners of who wants to be a millionaire), (us visa: visa for us)cs (1.0) ?
(brazilian dances: dances of the brazilian cul-ture), (tsunami charities: charities that gave to the tsunami),(stephen king books: books published by stephen king),(florida insurance companies: insurance companies head-quartered in florida), (florida insurance companies: insur-ance companies operating in florida), (us visa: visa requiredto enter us)qq (0.5) ?
(super smash bros brawl characters: charactersmeant to be in super smash bros brawl), (carribean islands:islands by the carribean), (pain assessment tool: tool forrecording pain assessment)xw (0.0) ?
(periodic functions: functions of periodic dis-tributions), (simpsons episodes: episodes left of simpsons),(atm card: card left in wachovia atm)Table 4: Examples of interpretations manually anno-tated with each correctness label (cg=correct generic;cs=correct specific; qq=okay; xw=incorrect)Gold Set Precision@N@1 @3 @5 @10 @20ListQ 0.770 0.708 0.655 0.568 0.465IsA 0.730 0.598 0.530 0.423 0.329WikiC 0.780 0.647 0.561 0.455 0.357Table 5: Average precision, at various ranks in the rankedlists of interpretations extracted for noun phrases fromvarious sets of gold phrasesAt ranks 1, 5 and 20, precision scores vary between0.770, 0.655 and 0.465 respectively, for the ListQgold set; and between 0.730, 0.530 and 0.329 re-spectively, for the IsA gold set.Presence of Relevant Interpretations: Sometimesit is difficult to even manually enumerate as many as20 distinct, relevant string forms of interpretationsfor a given noun phrase.
Measuring precision at aparticular rank (e.g., 20) in a ranked list of interpre-tations may be too conservative.
Table 6 summa-rizes a different type of scoring metric, namely thepresence of any relevant interpretation, among theinterpretations extracted up to a particular rank.
Rel-evance is flexibly defined, by requiring the interpre-tations to have been assigned a certain correctnesslabel, then computing the average number of goldphrases for which such interpretations are present upto a particular rank.
When considering only inter-pretations annotated as correct and generic or cor-rect and specific, in the second row of each vertical339GoldSetSelectedCorrect-ness LabelsAverage presence of any interpreta-tions with any of the selected cor-rectness labelscg cs qq @1 @3 @5 @10 @20ListQ ?
?
?
0.790 0.860 0.870 0.880 0.880?
?0.750 0.810 0.830 0.840 0.840?0.720 0.780 0.790 0.800 0.800?0.030 0.160 0.360 0.450 0.460IsA?
?
?0.750 0.800 0.810 0.830 0.830?
?0.710 0.770 0.790 0.800 0.800?0.650 0.690 0.710 0.710 0.720?0.060 0.220 0.350 0.480 0.520WikiC?
?
?0.810 0.900 0.920 0.930 0.930?
?0.750 0.830 0.860 0.860 0.870?0.640 0.730 0.750 0.750 0.760?0.110 0.210 0.370 0.520 0.560Table 6: Average of scores indicating the presence or ab-sence of any interpretations annotated with a correctnesslabel from a particular subset of correctness labels.
Com-puted over interpretations extracted up to various ranksin the ranked lists of extracted interpretations (cg=correctgeneric; cs=correct specific; qq=okay)Noun Phrase ?
Multiple-Bracketing Interpretationsafrican american women writers ?
(writers)Hwho wroteabout (african american)Mwomen, (women writers)Hwhoare (african american)M, (writers)Hwho cover (africanamerican)Mwomen struggleschinese traditional instruments ?
(traditional instruments)Hof (china)M, (instruments)Hused in (chinese traditional)Mmusicelementary math manipulatives ?
(manipulatives)Hfor (elementary math)M, (math manipulatives)Hin the(elementary)Mclassroom, (manipulatives)Hused in (ele-mentary math)M, (math manipulatives)Hfor (elementary)Mlevelglobal corporate tax rates ?
(corporate tax rates)Haroundthe (world)M, (tax rates)Hon (global corporate)MprofitsTable 7: Sample of noun phrases from the ListQ goldset, whose top 10 extracted interpretations induce mul-tiple pairs of a head and a modifier of the noun phrases(H=head; M=modifier)portion in Table 6, the scores at rank 5 are 0.830 forListQ, 0.790 for IsA and 0.860 for WikiC.
Alterna-tively, in the fourth rows of each vertical portion, thescores at rank 5 are 0.360, 0.350 and 0.370 respec-tively.
The scores indicate that at least one of the top5 interpretations is correct and specific for about athird of the noun phrases in the gold sets.Induced Modifiers, Heads and Interpretations:When a candidate interpretation is extracted for anoun phrase, the interpretation effectively inducesa particular bracketing over the noun phrase, as itsplits it into a modifier and a head.
For an ambiguousPresence of Multiple BracketingsVocabulary ListQ IsA WikiCFraction of Noun Phrases 0.110 0.124 0.051Table 8: Fraction of noun phrases that have some ex-tracted interpretation(s) and contain at least 3 tokens,whose interpretations induce multiple (rather than single)bracketings over interpreted noun phrases.
The presenceof multiple bracketings for a noun phrase is equivalent tothe presence of multiple pairs of a head and a modifier,as induced by the top 10 interpretations extracted for thenoun phraseNoun Phrase ?
Extracted Interpretationsbeatles songs ?
(songs)Hsung by the (beatles)M, (songs)Habout the (beatles)Mcompany accounts ?
(accounts)Hmaintained by the(company)M, (accounts)Howed to a (company)Mflorida insurance companies ?
(insurance companies)Hheadquartered in (florida)M, (insurance companies)Hinsur-ing in (florida)Mgerman food ?
(food)Heaten in (germany)M, (food)Hproduced in (germany)M, (food)Hthat originated in(germany)Mmath skills ?
(skills)Hneeded for (math)M, (skills)Hlearned in (math)M, (skills)Hgained from studying (math)Mmichael jackson song ?
(song)Hwritten by (michaeljackson)M, (song)Hsung by (michael jackson)M, (song)Habout (michael jackson)MTable 9: Sample of alternative relevant interpretations ex-tracted among the top 20 interpretations for noun phrasesfrom the ListQ gold set (H=head; M=modifier)noun phrase, multiple bracketings may be possible,each corresponding to a different interpretation.
In-terpretations extracted from queries do capture suchmultiple bracketings, even for phrases from the goldsets, as illustrated in Table 7.
Over all noun phrasesfrom the input vocabularies that have some extractedinterpretations and contain at least 3 tokens, about10% (ListQ and IsA) and 5% (WikiC) of the nounphrases have multiple bracketings induced by theirtop 10 interpretations, as shown in Table 8.Table 9 shows examples of noun phrases withmultiple extracted interpretations that induce identi-cal bracketings, but capture distinct interpretations.Impact of Variants: Variants of modifiers providealternatives in extracting candidate interpretations,even when the modifiers from the noun phrases arenot present in their original form in the interpreta-tions.
For example, the adj-noun variant ethiopiaof the modifier ethiopian leads to the extraction ofthe interpretation ?runners from ethiopia?
for thenoun phrase ?ethiopian runners?.
Similarly, wiki-340Vocab Variant Typeorig-phrase adj-noun dist-sim wiki-templInterpretations produced by variant type (not exclusive):ListQ 0.453 0.089 0.642 0.017IsA 0.389 0.121 0.597 0.003WikiC 0.191 0.097 0.603 0.425Interpretations produced only by variant type (exclusive):ListQ 0.281 0.069 0.450 0.005IsA 0.299 0.099 0.491 0.001WikiC 0.086 0.076 0.351 0.225Table 10: Impact of various types of variants of modifiers,on the coverage of noun phrase interpretations.
Com-puted as the fraction of the top 10 extracted interpreta-tions produced by a particular variant type, and possiblyby other variant types (upper portion); or produced onlyby a particular variant type, and by no other variant types(lower portion) (Vocab=vocabulary of noun phrases)templ variants metallica and 50 cent of the mod-ifier art garfunkel, in the context ?X albums?, al-low for the extraction of the interpretation ?albumssold by art garfunkel?
for the noun phrase ?art gar-funkel albums?, via the interpretations ?albums soldby metallica?
and ?albums sold by 50 cent?.Table 10 quantifies the impact of various types ofvariants, on the coverage of noun phrase interpre-tations.
The scores provided for each variant typecorrespond to either non-exclusive (upper portion ofthe table) or exclusive (lower portion) contributionof that variant type towards some extracted interpre-tations.
In other words, in the lower portion, thescores capture the fraction of the top 10 interpreta-tions that are produced only by that particular varianttype.
Three conclusions can be drawn from the re-sults.
First, all variant types contribute to increasingcoverage, relative to using only orig-phrase variants.Second, dist-sim variants have a particularly strongimpact.
Third, wiki-templ variants have a strong im-pact, but only when the contexts from which theywere collected match the context of the noun phrasebeing interpreted.
On the WikiC vocabulary in thelower portion of Table 10, the scores for wiki-templillustrate the potential that contextual variants havein extracting additional interpretations.Table 11 again quantifies the impact of varianttypes, but this time on the coverage and, more im-portantly, accuracy of interpretations extracted forphrases from the gold sets.
The scores are com-puted over the ranked lists of interpretations fromthe ListQ gold set, as certain types of variants aretemporarily disabled in ablation experiments.
Theupper portion of the table shows results when onlyVariant Types Impact on PrecisionO A D W Cvg P@5 C@5?- - - 74 0.433 0.581-?- - 16 0.474 0.562- -?- 66 0.478 0.651- - -?2 0.166 0.500-?
?
?73 0.484 0.657?-?
?97 0.641 0.835?
?-?83 0.448 0.590?
?
?- 99 0.649 0.828?- - - 74 0.433 0.581?
?- - 81 0.453 0.592?-?- 96 0.635 0.833?- -?76 0.429 0.578?
?
?
?100 0.655 0.830Table 11: Impact of various types of variants of modifiers,on the precision of noun phrase interpretations.
Com-puted over the ListQ gold set, at rank 5 in the ranked listsof extracted interpretations, when various variant typesare allowed (?)
or temporarily not allowed (-) to produceinterpretations (O=orig-phrase variant type; A=adj-nounvariant type; D=dist-sim variant type; W=wiki-templvariant type; Cvg=number of noun phrases from the goldset with some interpretation(s) produced by the allowedvariant types; P@5=precision at rank 5; C@5=averagepresence of any interpretations annotated as correct andgeneric (cg) or correct and specific (cs), among interpre-tations up to rank 5)one of the variant types is enabled.
It shows thatnone of the variant types, taken in isolation, canmatch what they achieve when combined together,in terms of both coverage and accuracy.
The middleportion of the table shows results when all but oneof the variant types are enabled.
Each of the vari-ant types incrementally contributes to higher cover-age and accuracy over the combination of the othervariant types.
The incremental contribution of wiki-templ variants is the smallest.
The lower portion ofTable 11 gives the incremental contribution of thevariant types, relative to using only the orig-phrasevariant type.
The last row of Table 11 correspondsto all variant types being enabled.Discussion: Independently of the choice of the tex-tual data source (e.g., documents, queries) fromwhich interpretations are extracted, a noun phraseis intuitively more difficult to interpret if it is rela-tively more rare or more complex (i.e., longer).
Ad-ditional experiments quantify the effect, by measur-ing the correlation between the presence of some ex-tracted interpretations for an input noun phrase, onone hand; and the frequency of the noun phrase as aquery (in Table 12), on the other hand.
In Table 12,341Vocabulary Noun PhrasesWith : Without Interpretation(s)I : ?I AI: A?IMI: M?IListQ 2.14 : 1 2.93 : 1 2.65 : 1IsA 2.31 : 1 5.76 : 1 3.26 : 1WikiC 2.60 : 1 3.72 : 1 3.63 : 1Table 12: Correlation between coverage, measured as thepresence of some extracted interpretation(s) for a nounphrase, on one hand; and frequency of the noun phraseas a query, on the other hand (I=number of noun phrasesthat are queries and have some extracted interpretation(s);?I=number of noun phrases that are queries and do nothave any extracted interpretation(s); A=average queryfrequency of noun phrases as queries; M=median queryfrequency of noun phrases as queries)0.0000.0010.0040.0100.0200.0500.1000.2000.4000.8002 3 4 5 6 7 8 9 10FractionofnounphrasesNumber of tokens in noun phraseListQIsAWikiCFigure 2: Ability to extract interpretations for nounphrases, as a function of the length of noun phrases.Computed as the fraction of noun phrases from an inputvocabulary with a particular number of tokens, for whichthere are some extracted interpretation(s)the effect is visible in that query frequency is higherfor noun phrases with some extracted interpretationsvs.
noun phrases with none.
For example, the aver-age query frequency is almost three times higher forthe former than for the latter, for the ListQ vocabu-lary.
Similarly, in Figure 2, a larger fraction of theinput noun phrases with a particular number of to-kens have some extracted interpretations, when thenumber of tokens is lower rather than higher.
Theeffect is somewhat less pronounced for, but still ap-plicable to, the WikiC vocabulary, with some ex-tracted interpretations being present for 75%, 71%,63%, and 37% of the noun phrases containing 2, 3,4 and 8 tokens respectively.
That a larger fraction ofthe longer noun phrases can be interpreted in the Wi-kiC vocabulary is attributed to the role of wiki-templvariants in extracting interpretations that would oth-erwise not be available.Interpretations from Queries vs.
Documents: Forcompleteness, additional experiments evaluate theinterpretations extracted from queries, relative toa gold standard introduced in (Hendrickx et al,2013).
The gold standard consists of a gold setof 181 compound noun phrases (e.g., ?account-ing principle?
and ?application software?
), theirmanually-assembled gold paraphrases (e.g., ?prin-ciple of accounting?, ?software to make applica-tions?
), and associated scoring metrics referred toas non-isomorphic and isomorphic.
Note that, incomparison to the ListQ, IsA and WikiC evaluationsets, the gold standard in (Hendrickx et al, 2013)may contain relatively less popular gold phrases.
Asmany as 45 gold paraphrases are available per goldphrase on average.
They illustrate the difficulty ofany attempt to manually assemble exhaustive setsof all strings that are valid interpretations of a nounphrase.
For example, the gold paraphrases of thegold phrase blood cell include ?cell that is foundin the blood?, but not the arguably equally-relevant?cell found in the blood?.
In addition, more than onehuman annotators independently provide the samegold paraphrase for only a tenth of all gold para-phrases.
See (Hendrickx et al, 2013) for details onthe gold standard and scoring metrics.
The gold setis added as another input vocabulary to the methodproposed here.
After inspection of a training set ofcompound noun phrases also introduced in (Hen-drickx et al, 2013), the parameter settings are mod-ified to only retain interpretations whose penaltyscore is 0.The isomorphic and non-isomorphic scores re-ward coverage and accuracy respectively.
Forthe ranked candidate interpretations extracted fromqueries for the gold set, they are 0.037 and 0.556 re-spectively.
In comparison to previous methods thatoperate over documents instead of queries, the iso-morphic score is much lower for our method (e.g.,0.037 vs. 0.130 (Van de Cruys et al, 2013)).
Itsuggests that queries cannot reliably provide an ex-haustive list of all possible strings available in thegold standard for each gold phrase.
However, thenon-isomorphic score is higher for our method thanfor the best method operating over documents (i.e.,0.556 vs. 0.548 (Hendrickx et al, 2013)).
In fact,the non-isomorphic score using queries would be0.745 instead of 0.556, if it were computed over onlythe 135 gold noun phrases with some extracted in-terpretations.
The results suggests that the methodproposed here extracts more accurate interpretationsfrom queries, than previous methods extract from342documents.
Higher accuracy is preferable in scenar-ios like Web search, where it is important to accu-rately trigger structured results.Error Analysis: The relative looseness of the ex-traction patterns applied to queries causes inter-pretations containing undesirable tokens to be ex-tracted.
In addition, part-of-speech tagging er-rors lead to interpretations receiving artificially lowpenalty scores, and therefore being considered to beof higher quality than they should be.
For example,phd in the interpretation ?job for phd in chemistry?is incorrectly tagged as a past participle verb.
As aresult, the computed penalty score is too low.Occasionally, the presence of additional tokenswithin an interpretation is harmless (e.g., ?issuesof controversy in society?
for ?controversial is-sues?, ?foods allowed on a high protein low carbdiet?
for ?high protein low carb foods?
), if notnecessary (e.g., ?dances with brazilian origin?for ?brazilian dances?, ?artists of the surrealistmovement?
for ?surrealist artists?, ?options withweekly expirations?
for ?weekly options?).
But of-ten it leads to incorrect interpretations (e.g., ?townsof alaska map?
for ?alaska towns?, ?processes inchemical vision?
for ?chemical processes?
).Variants of modifiers occasionally lead to incor-rect interpretations for a noun phrase, even if the in-terpretations may be correct for the individual vari-ants.
The phenomenon is an instance of semanticdrift, wherein variants do share many properties butstill diverge in others.
Examples are ?words that arebleeped similarly?
extracted for ?bleeped words?via the variant bleeped?spelled.
Separately, lin-guistic constructs that negate or at least alter thedesired meaning affect the understanding of text ingeneral and also affect the extraction of interpreta-tions in particular.
Examples are ?heaters with noelectricity?
for ?electric heaters?, and ?animal thatused to be endangered?
for ?endangered animal?.5 Related WorkRelevant interpretations extracted from queries actas a potential bridge between facts, on one hand,and class labels, on the other hand, available for in-stances.
The former might be inferred from the lat-ter and vice versa.
There are two previous studiesthat are relevant to the task of extracting facts fromexisting noun phrases.
First, (Yahya et al, 2014)extract facts for attributes of instances, without re-quiring the presence of the verbal predicates usu-ally employed (Fader et al, 2011) in open-domaininformation extraction.
Second, in (Nastase andStrube, 2008), relations encoded implicitly withinWikipedia categories are converted into explicit rela-tions.
As an example, the relation <deconstructingharry, directed, woody allen> is obtained fromthe fact that deconstructing harry is listed under?movies directed by woody allen?
in Wikipedia.The method in (Nastase and Strube, 2008) relieson manually-compiled knowledge, and does not at-tempt to interpret compound noun phrases.Since relevant interpretations paraphrase the nounphrases which they interpret, a related area of re-search is paraphrase acquisition (Madnani and Dorr,2010; Ganitkevitch et al, 2013).
Previous methodsfor the acquisition of paraphrases of compound nounphrases (Kim and Nakov, 2011; Van de Cruys et al,2013) operate over documents, and may rely on textanalysis tools including syntactic parsing (Nakovand Hearst, 2013).
In contrast, the method proposedhere extracts interpretations from queries, and ap-plies part of speech tagging.
Queries were used as atextual data source in other tasks in open-domain in-formation extraction (Jain and Pennacchiotti, 2010;Pantel et al, 2012).6 ConclusionInterpretations extracted from queries explain theroles that modifiers play within longer nounphrases.
Current work explores the interpretationof noun phrases containing multiple modifiers (e.g.,?
(french)M1( healthcare)M2(companies)H?
byseparately interpreting ?
(french)M1(companies)H?and ?(healthcare)M2(companies)H?
); the group-ing of lexically different but semantically equivalentinterpretations (e.g., ?dances of brazilian origin?,?dances from brazil?
); the collection of more vari-ants from Wikipedia and other resources; the incor-poration of variants of heads (physicists?scientistsfor interpreting the phrase ?belgian physicists?
),which likely need to be more conservatively appliedthan for modifiers; and the use of query sessions, asan alternative to sets of disjoint queries.AcknowledgmentsThe paper benefits from comments from Jutta De-gener, Mihai Surdeanu and Susanne Riehemann.Data extracted by Haixun Wang and Jian Li is thesource of the IsA vocabulary of noun phrases usedin the evaluation.343ReferencesM.
Banko, Michael J Cafarella, S. Soderland, M. Broad-head, and O. Etzioni.
2007.
Open information ex-traction from the Web.
In Proceedings of the 20th In-ternational Joint Conference on Artificial Intelligence(IJCAI-07), pages 2670?2676, Hyderabad, India.T.
Brants.
2000.
TnT - a statistical part of speech tagger.In Proceedings of the 6th Conference on Applied Natu-ral Language Processing (ANLP-00), pages 224?231,Seattle, Washington.B.
Dalvi, W. Cohen, and J. Callan.
2012.
Websets: Ex-tracting sets of entities from the Web using unsuper-vised information extraction.
In Proceedings of the5th ACM Conference on Web Search and Data Mining(WSDM-12), pages 243?252, Seattle, Washington.P.
Downing.
1977.
On the creation and use of Englishcompound nouns.
Language, 53:810?842.A.
Fader, S. Soderland, and O. Etzioni.
2011.
Identifyingrelations for open information extraction.
In Proceed-ings of the 2011 Conference on Empirical Methodsin Natural Language Processing (EMNLP-11), pages1535?1545, Edinburgh, Scotland.C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lexi-cal Database and Some of its Applications.
MIT Press.T.
Flati, D. Vannella, T. Pasini, and R. Navigli.
2014.Two is bigger (and better) than one: the WikipediaBitaxonomy project.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics (ACL-14), pages 945?955, Balti-more, Maryland.J.
Ganitkevitch, B.
Van Durme, and C. Callison-Burch.2013.
PPDB: The paraphrase database.
In Proceed-ings of the 2013 Conference of the North AmericanAssociation for Computational Linguistics (NAACL-HLT-13), pages 758?764, Atlanta, Georgia.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14th In-ternational Conference on Computational Linguistics(COLING-92), pages 539?545, Nantes, France.I.
Hendrickx, Z. Kozareva, P. Nakov, D. ?O Se?aghdha,S.
Szpakowicz, and T. Veale.
2013.
SemEval-2013task 4: Free paraphrases of noun compounds.
In Pro-ceedings of the 7th International Workshop on Seman-tic Evaluation (SemEval-14), pages 138?143, Atlanta,Georgia.J.
Hoffart, F. Suchanek, K. Berberich, and G. Weikum.2013.
YAGO2: a spatially and temporally enhancedknowledge base from Wikipedia.
Artificial Intelli-gence Journal.
Special Issue on Artificial Intelligence,Wikipedia and Semi-Structured Resources, 194:28?61.A.
Jain and M. Pennacchiotti.
2010.
Open entity ex-traction from Web search query logs.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics (COLING-10), pages 510?518,Beijing, China.N.
Kim and P. Nakov.
2011.
Large-scale noun com-pound interpretation using bootstrapping and the Webas a corpus.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing(EMNLP-11), pages 648?658, Edinburgh, Scotland.Z.
Kozareva and E. Hovy.
2010.
A semi-supervisedmethod to learn and construct taxonomies using theweb.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing(EMNLP-10), pages 1110?1118, Cambridge, Mas-sachusetts.D.
Lin and X. Wu.
2009.
Phrase clustering for discrim-inative learning.
In Proceedings of the 47th AnnualMeeting of the Association for Computational Linguis-tics (ACL-IJCNLP-09), pages 1030?1038, Singapore.N.
Madnani and B. Dorr.
2010.
Generating phrasal andsentential paraphrases: a survey of data-driven meth-ods.
Computational Linguistics, 36(3):341?387.P.
Nakov and M. Hearst.
2013.
Semantic interpreta-tion of noun compounds using verbal and other para-phrases.
ACM Transactions on Speech and LanguageProcessing, 10(3):1?51.V.
Nastase and M. Strube.
2008.
Decoding Wikipediacategories for knowledge acquisition.
In Proceedingsof the 23rd National Conference on Artificial Intelli-gence (AAAI-08), pages 1219?1224, Chicago, Illinois.P.
Pantel, T. Lin, and M. Gamon.
2012.
Mining entitytypes from query logs via user intent modeling.
InProceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics (ACL-12), pages563?571, Jeju Island, Korea.T.
Van de Cruys, S. Afantenos, and P. Muller.
2013.MELODI: A supervised distributional approach forfree paraphrasing of noun compounds.
In Proceedingsof the 7th International Workshop on Semantic Evalu-ation (SemEval-14), pages 144?147, Atlanta, Georgia.R.
Wang and W. Cohen.
2009.
Automatic set instanceextraction using the Web.
In Proceedings of the 47thAnnual Meeting of the Association for ComputationalLinguistics (ACL-IJCNLP-09), pages 441?449, Singa-pore.M.
Yahya, S. Whang, R. Gupta, and A. Halevy.
2014.ReNoun: Fact extraction for nominal attributes.
InProceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing (EMNLP-14), pages 325?335, Doha, Qatar.X.
Yao and B.
Van Durme.
2014.
Information extractionover structured data: Question Answering with Free-base.
In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics (ACL-14), pages 956?966, Baltimore, Maryland.344
