A two-stage statistical word segmentation system for ChineseGuohong FuDept of LinguisticsThe University of Hong KongPokfulam Road, Hong Kongghfu@hkucc.hku.hkK.K.
LukeDept of LinguisticsThe University of Hong KongPokfulam Road, Hong Kongkkluke@hkusua.hku.hkAbstractIn this paper we present a two-stagestatistical word segmentation system forChinese based on word bigram and word-formation models.
This system wasevaluated on Peking University corpora atthe First International Chinese WordSegmentation Bakeoff.
We also giveresults and discussions on this evaluation.1 IntroductionWord segmentation is very important for Chineselanguage processing, which aims to recognize theimplicit word boundaries in Chinese text.
Duringthe past decades, great success has been achieved inChinese word segmentation (Nie, et al 1995; Yao,1997; Fu and Wang, 1999; Wang et al 2000;Zhang, et al 2002).
However, there still remain twodifficult problems, i.e.
ambiguity resolution andunknown word (so-called OOV word) identification,while developing a practical segmentation systemfor large open applications.In this paper, we present a two-stage statisticalword segmentation system for Chinese.
In the firststage, we employ word bigram model to segmentknown words (viz.
the words included in the systemdictionary) in input.
In the second stage, we developa hybrid algorithm to perform unknown wordidentification incorporating word contextualinformation, word-formation patterns and wordjuncture model.The rest of this paper is organized as follows:Section 2 presents a word bigram solution forknown word segmentation.
Section 3 describes ahybrid approach for unknown word identification.In section 4, we report the results of our system atthe SIGHAN evaluation program, and in the finalsection we give our conclusions on this work.2 The first stage: Segmentation of knownwordsIn a sense, known word segmentation is a processof disambiguation.
In our system, we use wordbigram language models and Viterbi algorithm(1967) to resolve word boundary ambiguities inknown word segmentation.For a particular input Chinese character stringncccC L21= , there is usually more than onepossible segmentation mwwwW L21= according togiven system dictionary.
Word bigram segmentationaims to find the most appropriate segmentationmwwwW L21?
=  that maximizes the probability?=?miiir wwP11)|( , i.e.?=??=miiirWrWwwPCWPW11)|(maxarg)|(maxarg?
(1)where )|( 1?iir wwP  is the probability that word lwwill occur given previous word 1?iw , which can beeasily estimated from segmented corpus usingmaximum likelihood estimation (MLE), i.e.)()()|(111???
?iiiiir wCountwwCountwwP              (2)To avoid the problem of data sparseness in MLE,here we apply the linear interpolation technique(Jelinek and Mercer, 1980) to smooth the estimatedword bigram probabilities.3 The second stage: Unknown wordidentificationThe second stage mainly concerns unknown wordssegmentation that remains unresolved in first stage.This section describes a hybrid algorithm forunknown word identification, which can incorporateword juncture model, word-formation patterns andcontextual information.
To avoid the complicatednormalization of the probabilities of differentdimensions, the simple superposition principle isalso used in merging these models.3.1 Word juncture modelWord juncture model score an unknown word byassigning word juncture type.
Obviously, mostunknown words appear as a string of known wordsafter segmentation in first stage.
Therefore,unknown word identification can be viewed as aprocess of re-assigning correct word juncture typeto each known word pair in input.
Given a knownword string nwwwW L21= , between each word pair)11(1 ??
?+ niww ii  is a word juncture.
In general,there are two types of junctures in unknown wordidentification, namely word boundary (denoted byBt ) and non-word boundary (denoted by Nt ).Let )( 1+ii wwt denote the type of a word juncture1+ii ww , and ))(( 1+iir wwtP denote the relevantconditional probability, then)())(())((111+++ =iiiidefiir wwCountwwtCountwwtP              (3)Thus, the word juncture probability )( UCJM wP of aparticular unknown word jiiU wwww L1+=)1( nji ???
can be calculated by??=++?
?
?=1111 ))(())(())(()(jilllNrjjBriiBrUCJM cctPwwtPwwtPwP  (4)In a sense, word juncture model mirrors the affinityof known word pairs in forming an unknown word.For a word juncture ),( 1+ii ww , the larger theprobability ))(( 1+iiNr wwtP , the more likely the twowords are merged together into one new word.3.2 Word-formation patternsWord-formation pattern model scores an unknownword according to the probability of how eachinternal known word contributes to its formation.
Ingeneral, a known word w  may take one of thefollowing four patterns while forming a word: (1)w itself is a word.
(2) w is the beginning of anunknown word.
(3) w is at the middle of anunknown word.
(4) w appears at the end of anunknown word.
For convenience, we use S , B ,M and E  to denote the four patterns respectively.Let )(wpttn denote a particular pattern of w  in anunknown word and ))(( wpttnPr  denote the relevantprobability, then)())(())((wCountwpttnCountwpttnPdefr =              (5)Obviously, 1))(( =?pttnr wpttnP .
And  1- ))(( wSPr  is theword-formation power of the known word w .Let  )( Upttn wP be the overall word-formationpattern probability of a certain unknown wordlU wwww L21=  , then?
?=Ui wwirUpttn wpttnPwP ))(()(              (6)Theoretically speaking, a known word can take anypattern while forming an unknown word.
But it isnot even in probability for different known wordsand different patterns.
For example, the word ?
(xing4, nature) is more likely to act as the suffix ofwords.
According to our investigation on thetraining corpus, the character?
appears at the endof a multiword in more than 93% of cases.3.3 Hybrid algorithm for unknown wordidentificationCurrent algorithm for unknown word identificationconsists of three major components: (1) anunknown word extractor firstly extracts a fragmentof known words nwww L21 that that may haveunknown words based on the related word-formation power and word juncture probability andits left and right contextual word Lw , Rw from theoutput of the first stage.
(2) A candidate wordconstructor then generates a lattice of all possiblenew segmentations }|{ 21 mUU xxxWW L=  that mayinvolve unknown words from the extractedfragment.
(3) A decoder finally incorporates wordjuncture model )( UWJM WP , word-formationpatterns )( Upttn WP  and word bigram probability)( Ubigram WP to score these candidates, and thenapplies the Viterbi algorithm again to find the bestnew segmentation mU xxxW L21?
=  that has themaximum score:}))|()()(({maxarg)}()()({maxarg?,,11?=?++=++=niiiriCJMipttnWUbigramUCJMUpttnWUxxPxPxPWPWPWPWUUL(7)where Lwx =0  and Rn wx =+1 .
Let Uw  denote anyunknown word in the training corpus.
If ix  is anunknown word, then)()()|(111???
?=iwUiiir xCountwxCountxxP U .4 ExperimentsOur system participated in both closed and opentests on Peking University corpora at the FirstInternational Chinese Word Segmentation Bakeoff.This section reports the results and discussions onits evaluation.4.1 MeasuresIn the evaluation program of the First InternationalChinese Word Segmentation Bakeoff, six measuresare employed to score the performance of a wordsegmentation system, namely test recall (R), testprecision (denoted by P), the balanced F-measure(F), the out-of-vocabulary (OOV) rate for the testcorpus, the recall on OOV words (ROOV), and therecall on in-vocabulary (Riv) words.
OOV is definedas the set of words in the test corpus not occurringin the training corpus in the closed test, and the setof words in the test corpus not occurring in thelexicon used in the open test.4.2 Experimental lexicons and corporaAs shown in Table 1, we only used the training datafrom Peking University corpus to train our systemin both the open and closed tests.
As for thedictionary, we compiled a dictionary for the closedtest from the training corpus, which contained 55,226 words, and used a dictionary in the open testthat contained about 65, 269 words.Items # words inlexicon# train.words# test.wordsClosed 55,226 1,121,017 17,194Open 65,269 1,121,017 17,194Table 1: Experimental lexicons and corpora4.3 Experimental results and discussionItems F R P OOV ROOV RivClosed 0.939 0.936 0.942 0.069 0.675 95.5Open 0.937 0.933 0.941 0.094 0.762 95.0Table 2: Test results on PK corpusSegmentation speed: There are in all about 28,458characters in the test corpus.
It takes about 3.21and 3.07 seconds in all for our system to performfull segmentation (including known wordsegmentation and unknown word identification) onthe closed and open test corpus respectively,running on an ACER notebook (TM632XC-P4M).This indicates that our system is able to processabout 531,925~556,182 characters per minute.Results and discussions: The results for the closedand open test are presented in Table 2.
We candraw some conclusions from these results.Firstly, the overall performance of our system isvery stable in both the closed and open tests.
Asshown in Table 2, the out-of-vocabulary (OOV)rate is 6.9% in the closed test and 9.4% in the opentest.
However, the overall test F-measure drops byonly 0.2 percent in the open test, compared with theclosed test.Secondly, our approach can handle most unknownwords in the input.
As can be seen from Table 2,the recall on OOV words are 67.5% the closed-testand 76.2% in the open-test.
Wang et al(2000) andYao (1997) have proposed a character juncturemodel and word-formation patterns for Chineseunknown word identification.
However, theirapproaches can only work for the unknown wordsthat are made up of pure monosyllable character inthat they are character-based methods.
To addressthis problem, we introduce both word juncturemodel and word-based word-formation patterns intoour system.
As a result, our system can deal withdifferent unknown words that consist of differentknown words, including monosyllable charactersand multiword.Although our system is effective for mostambiguities and unknown words in the input, it hasits inherent deficiencies.
Firstly, to avoid datasparseness, we do not differentiate known wordsand unknown words while estimating word juncturemodels and word-formation patterns from thetraining corpus.
This simplification may introducesome noises into these models for identifyingunknown words.
Our further investigations showthat the precision on OOV words is still very low,i.e.
67.1% for closed test and 72.5% for open test.As a result, our system may yield a number ofmistaken unknown words in the processing.Secondly, we regard known word segmentation andunknown word identification as two independentstages in our system.
This strategy is obviouslysimple and more easily applicable.
However, it doesnot work while the input contains a mixture ofambiguities and unknown words.
For example,there was a sentence ??????????
inthe test corpus, where, the string ????
is afragment mixed with ambiguity and unknownwords.
The correct segmentation should be ??/?
?/, where??
(Zhonghang, the Bank of China) isa abbreviation of organization name, and ??
(Changge) is a place name.
Actually, this fragmentis segmented as?/?
?/?/ in the first stage of oursystem.
However, the unknown word identificationstage does not have a mechanism to split the word??
(Hangzhang, president) and  finally resulted inwrong segmentation.5 ConclusionsThis paper presents a two-stage statistical wordsegmentation system for Chinese.
In the first stage,word bigram model and Viterbi algorithm areapplied to perform known word segmentation oninput plain text, and then a hybrid approach isemployed in the second stage to incorporate wordbigram probabilities, word juncture model andword-based word-formation patterns to detect OOVwords.
The experiments on Peking Universitycorpora have shown that the present system basedon fairly simple word bigram and word-formationmodels can achieve a F-score of 93.7% or above.
Infuture work, we hope to improve our strategies onestimating word juncture model and word-formationpatterns and develop an integrated segmentationtechnique that can perform known wordsegmentation and unknown word identification atone time.AcknowledgmentsWe would like to thank all colleagues of the FirstInternational Chinese Word Segmentation Bakeofffor their evaluations of the results and the Instituteof Computational Linguistics, Peking University forproviding the experimental corpora.ReferencesFu, Guohong and Xiaolong Wang.
1999.
UnsupervisedChinese word segmentation and unknown wordidentification.
In: Proceedings of NLPRS?99,Beijing, China, 32-37.Jelinek, Frederick, and Robert L. Mercer.
1980.Interpolated estimation of Markov source parametersfrom sparse data.
In: Proceedings of Workshop onPattern Recognition in Practice, Amsterdam, 381-397.Nie, Jian-Yuan, M.-L. Hannan and W.-Y.
Jin.
1995.Unknown word detection and segmentation ofChinese using statistical and heuristic knowledge.Communication of COLIPS, 5(1&2): 47-57.Viterbi, A.J.
1967.
Error bounds for convolutionalcodes and an asymmetrically optimum decodingalgorithm.
IEEE Transactions on InformationTheory, IT-13(2): 260-269.Wang, Xiaolong, Fu Guohong, Danial S.Yeung, JamesN.K.Liu, and Robert Luk.
2000.
Models andalgorithms of Chinese word segmentation.
In:Proceedings of the International Conference onArtificial Intelligence (IC-AI?2000), Las Vegas,Nevada, USA, 1279-1284.Yao, Yuan.
1997.
Statistics Based approaches towardsChinese language processing.
Ph.D. thesis.
NationalUniversity of Singapore.Zhang, Hua-Ping, Qun Liu, Hao Zhang, and Xue-QiCheng.
2002.
Automatic recognition of Chineseunknown words based on roles tagging.
In:Proceedings of The First SIGHAN Workshop onChinese Language Processing, Taiwan, 71-77.
