The Rhythm of Lexical Stress in ProseDoug BeefermanSchool  of  Computer  Sc ienceCarneg ie  Mel lon  Un ivers i tyP i t t sburgh ,  PA  15213, USAdougb+@cs, cmu.
eduAbst ract"Prose rhythm" is a widely observed butscarcely quantified phenomenon.
We de-scribe an information-theoretic model formeasuring the regularity of lexical stress inEnglish texts, and use it in combinationwith trigram language models to demon-strate a relationship between the probabil-ity of word sequences in English and theamount of rhythm present in them.
Wefind that the stream of lexical stress in textfrom the Wall Street Journal has an en-tropy rate of less than 0.75 bits per sylla-ble for common sentences.
We observe thatthe average number of syllables per wordis greater for rarer word sequences, and tonormalize for this effect we run control ex-periments to show that the choice of wordorder contributes significantly to stress reg-ularity, and increasingly with lexical prob-ability.1 Int roduct ionRhythm inheres in creative output, asserting itself asthe meter in music, the iambs and trochees of poetry,and the uniformity in distances between objects inart and architecture.
More subtly there is widely be-lieved to be rhythm in English prose, reflecting thearrangement of words, whether deliberate or sub-conscious, to enhance the perceived acoustic signalor reduce the burden of remembrance for the readeror author.In this paper we describe an information-theoreticmodel based on lexical stress that substantiates thiscommon perception and relates stress regularity inwritten speech (which we shall equate with the in-tuitive notion of "rhythm") to the probability of thetext itself.
By computing the stress entropy rate forboth a set of Wall Street Journal sentences and a ver-sion of the corpus with randomized intra-sententialword order, we also find that word order contributessignificantly to rhythm, particularly within highlyprobable sentences.
We regard this as a first step inquantifying the extent to which metrical propertiesinfluence syntactic hoice in writing.1.1 BasicsIn speech production, syllables are emitted as pulsesof sound synchronized with movements of the mus-culature in the rib cage.
Degrees of stress arise fromvariations in the amount of energy expended by thespeaker to contract hese muscles, and from otherfactors such as intonation.
Perceptually stress ismore abstractly defined, and it is often associatedwith "peaks of prominence" in some representationof the acoustic input signal (Ochsner, 1989).Stress as a lexical property, the primary concernof this paper, is a function that maps a word to asequence of discrete levels of physical stress, approx-imating the relative emphasis given each syllablewhen the word is pronounced.
Phonologists distin-guish between three levels of lexical stress in English:primary, secondary, and what we shall call weakfor lack of a better substitute for unstressed.
Forthe purposes of this paper we shall regard stressesas symbols fused serially in time by the writer orspeaker, with words acting as building blocks of pre-defined stress sequences that may be arranged arbi-trarily but never broken apart.The culminative property of stress states that ev-ery content word has exactly one primary-stressedsyllable, and that whatever syllables remain are sub-ordinate to it.
Monosyllabic function words such asthe and of usually receive weak stress, while contentwords get one strong stress and possibly many sec-ondary and weak stresses.It has been widely observed that strong and weaktend to alternate at "rhythmically ideal disyllabicdistances" (Kager, 1989a).
"Ideal" here is a complexfunction involving production, perception, and manyunknowns.
Our concern is not to pinpoint his ideal,nor to answer precisely why it is sought by speakersand writers, but to gauge to what extent it is sought.We seek to investigate, for example, whether theavoidance of primary stress clash, the placement oftwo or more strongly stressed syllables in succession,influences yntactic hoice.
In the Wall Street Jour-302nal corpus we find such sentences as "The fol-low-ing is-sues re-cent-ly were f i led with the Se-cur-i-ties and Ex-change Com-mis-sion".
The phrase"recently were filed" can be syntactically permutedas "were filed recently", but this clashes filed withthe first syllable of recently.
The chosen sentenceavoids consecutive primary stresses.
Kager postu-lates with a decidedly information theoretic under-tone that the resulting binary alternation is "simplythe maximal degree of rhythmic organization com-patible with the requirement that adjacent stressesare to be avoided."
(Kager, 1989a)Certainly we are not proposing that a hard deci-sion based only on metrical properties of the outputis made to resolve syntactic hoice ambiguity, in thecase above or in general.
Clearly semantic empha-sis has its say in the decision.
But it is our beliefthat rhythm makes a nontrivial contribution, andthat the tools of statistics and information theorywill help us to estimate it formally.
Words are thebuilding blocks.
How much do their selection (dic-tion) and their arrangement (syntax) act to enhancerhythm?1.2 Past  mode ls  and  quant i f i cat ionsLexical stress is a well-studied subject at the intra-word level.
Rules governing how to map a word'sorthographic or phonetic transcription to a sequenceof stress values have been searched for and studiedfrom rules-based, statistical, and connectionist per-spectives.Word-external stress regularity has been deniedthis level of attention.
Patterns in phrases andcompound words have been studied by Halle (Halleand Vergnaud, 1987) and others, who observe andreformulate such phenomena s the emphasis ofthe penultimate constituent in a compound noun(National Center for Supercomputing Applications,for example.)
Treatment of lexical stress acrossword boundaries i scarce in the literature, however.Though prose rhythm inquiry is more than a hun-dred years old (Ochsner, 1989), it has largely beendismissed by the linguistic community as irrelevantto formal models, as a mere curiosity for literaryanalysis.
This is partly because formal methods ofinquiry have failed to present a compelling case forthe existence of regularity (Harding, 1976).Past attempts to quantify prose rhythm may beclassified as perception-oriented or signal-oriented.In both cases the studies have typically focussed onregularities in the distance between peaks of promi-nence, or interstress intervals, either perceived bya human subject or measured in the signal.
Theformer class of experiments relies on the subjectivesegmentation of utterances by a necessarily limitednumber of participants--subjects tapping out therhythms they perceive in a waveform on a recordingdevice, for example (Kager, 1989b).
To say nothingof the psychoacoustic biases this methodology intro-duces, it relies on too little data for anything but asterile set of means and variances.Signal analysis, too, has not yet been applied tovery large speech corpora for the purpose of inves-tigating prose rhythm, though the technology nowexists to lend efficiency to such studies.
The ex-periments have been of smaller scope and gearedtoward detecting isochrony, regularity in absolutetime.
Jassem et al(Jassem, Hill, and Witten, 1984)use statistical techniques such as regression to ana-lyze the duration of what they term rhythm units.Jassem postulates that speech is composed of extra-syllable narrow rhythm units with roughly fixed du-ration independent of the number of syllable con-stituents, surrounded by varia.ble-length anacruses.Abercrombie (Abercrombie, 1967) views speech ascomposed of metrical feet of variable length that be-gin with and are conceptually highlighted by a singlestressed syllable.Many experiments lead to the common conclu-sion that English is stress-timed, that there is someregularity in the absolute duration between strongstress events.
In contrast to postulated syllable-timed languages like French in which we find exactlythe inverse ffect, speakers of English tend to expandand to contract syllable streams so that the dura-tion between bounding primary stresses matches theother intervals in the utterance.
It is unpleasantfor production and perception alike, however, whentoo many weak-stressed syllables are forced intosuch an interval, or when this amount of "padding"varies wildly from one interval to the next.
Proserhythm analysts o far have not considered the syl-lable stream independent from syllabic, phonemic,or interstress duration.
In particular they haven'tmeasured the regularity of the purely lexical stream.They have instead continually re-answered questionsconcerning isochrony.Given that speech can be divided into interstressunits of roughly equal duration, we believe the moreinteresting question is whether a speaker or writermodifies his diction and syntax to fit a regular num-ber of syllables into each unit.
This question canonly be answered by a lexical approach, an approachthat pleasingly lends itself to efficient experimenta-tion with very large amounts of data.2 Stress entropy rateWe regard every syllable as having either strong orweak stress, and we employ a purely lexical, con-text independent mapping, a pronunciation dictio-nary a, to tell us which syllables in a word receivewhich level of stress.
We base our experiments ona binary-valued symbol set E1 = {W, S} and on aternary-valued symbol set E2 = {W, S, P}, where'W' indicates weak stress, 'S' indicates trong stress,1 We use the ll6,000-entry CMU Pronouncing Dictio-nary version 0.4 for all experiments in this paper.303i (,Figure 2: A 5-gram model viewed as a first-orderMarkov chainand 'P' indicates a pause.
Abstractly the dictionarymaps words to sequences of symbols from {primary,secondary, unstressed}, which we interpret by down-sampling to our binary system--primary stress isstrong, non-stress i  weak, and secondary stress ('2')we allow to be either weak or strong depending onthe experiment we are conducting.We represent a sentence as the concatenation ofthe stress sequences of its constituent words, with?
'P' symbols (for the N2 experiments) breaking thestream where natural pauses occur.Traditional approaches to lexicai language mod-eling provide insight on our analogous problem, inwhich the input is a stream of syllables rather thanwords and the values are drawn from a vocabu-lary N of stress levels.
We wish to create a modelthat yields approximate values for probabilities ofthe form p(sklso, s l , .
.
.
,  Sk-1), where si E ~ is thestress symbol at syllable i in the text.
A model withseparate parameters for each history is prohibitivelylarge, as the number of possible histories grows ex-ponentially with the length of the input; and forthe same reason it is impossible to train on limiteddata.
Consequently we partition the history spaceinto equivalence classes, and the stochastic n-gramapproach that has served lexicai language modelingso well treats two histories as equivalent if they endin the same n - 1 symbols.As Figure 2 demonstrates, an n-gram model issimply a stationary Markov chain of order k = n -1, or equivalently a first-order Markov chain whosestates are labeled with tuples from Ek.To gauge the regularity and compressibility of thetraining data we can calculate the entropy rate of thestochastic process as approximated by our model, anupper bound on the expected number of bits neededto encode each symbol in the best possible encod-ing.
Techniques for computing the entropy rate ofa stationary Markov chain are well known in infor-mation theory (Cover and Thomas, 1991).
If {Xi}is a Markov chain with stationary distribution ttand transition matrix P, then its entropy rate isH(X)  = - ~.i,j I'tiPij logpij.The probabilities in P can be trained by ac-cumulating, for each (sx,s2, .
.
.
,sk)  E E k, thek-gram count in C(s l , sz , .
.
.
, sk )  in the trainingdata, and normalizing by the (k - 1)-gram countC(sl, s2 , .
.
.
,  s l , -1 ) .The stationary distribution p satisfies pP  = #,or equivalently #k = ~j  #jPj,k (Parzen, 1962).
Ingeneral finding p for a large state space requires aneigenvector computation, but in the special case ofan n-gram model it can be shown that the value in pcorresponding to the state (sl, s2 , .
.
.
,  sk) is simplythe k-gram frequency C(sl, s2, .
.
.
,  sk)/N, where Nis the number of symbols in the data.
2 We thereforecan compute the entropy rate of a stress sequencein time linear in both the amount of data and thesize of the state space.
This efficiency will enable usto experiment with values of n as large as seven; forlarger values the amount of training data, not time,is the limiting factor.3 MethodologyThe training procedure ntails simply counting thenumber of occurrences of each n-gram for the train-ing data and computing the stress entropy rate bythe method described.
As we treat each sentence asan independent event, no cross-sentence n-grams arekept: only those that fit between sentence bound-aries are counted.3.1 The  mean ing  o f  s t ress  ent ropy  ra teWe regard these experiments as computing the en-tropy rate of a Markov chain, estimated from train-ing data, that approximately models the emission ofsymbols from a random source.
The entropy ratebounds how compressible the training sequence is,and not precisely how predictable unseen sequencesfrom the same source would be.
To measure the effi-cacy of these models in prediction it would be neces-sary to divide the corpus, train a model on one sub-set, and measure the entropy rate of the other withrespect o the trained model.
Compression can takeplace off-line, after the entire training set is read,while prediction cannot "cheat" in this manner.But we claim that our results predict how effectiveprediction would be, for the small state space in ourMarkov model and the huge amount of training datatranslate to very good state coverage.
In languagemodeling, unseen words and unseen n-grams are aserious problem, and are typically combatted withsmoothing techniques such as the backoff model andthe discounting formula offered by Good and Tur-ing.
In our case, unseen "words" never occur, for2This ignores edge effects, for ~--~s C(sl, s2,.
.
.
,  sa) =N - k + 1, but this discrepancy is negligible when N isvery large.304Lis ten to me close ly I'll en deav or to ex plain /S W S S S W S W S W S W S Pwhat sep ar ates a char la tan from a Char le magneW S W 2 W S W W S W S W 2 PFigure 1: A song lyric exemplifies a highly regular stress stream (from the musical Pippin by StephenSchwartz.
)the tiniest of realistic training sets will cover the bi-nary or ternary vocabulary.
Coverage of the n-gramset is complete for our prose training texts for n ashigh as eight; nor do singleton states (counts thatoccur only once), which are the bases of Turing's es-t imate of the frequency of untrained states in newdata, occur until n = 7.3.2 Lexicallzing stressLexical stress is the "backbone of speech rhythm"and the primary tool for its analysis.
(Baum, 1952)While the precise acoustical prominences of sylla-bles within an utterance are subject to certain word-external hierarchical constraints observed by Halle(Halle and Vergnaud, 1987) and others, lexical stressis a local property.
The stress patterns of individ-ual words within a phrase or sentence are generallycontext independent.One source of error in our method is the ambiguityfor words with multiple phonetic transcriptions thatdiffer in stress assignment.
Highly accurate tech-niques for part-of-speech labeling could be used forstress pattern disambiguation when the ambiguityis purely lexical, but often the choice, in both pro-duction and perception, is dialectal.
It would bestraightforward to divide among all alternatives thecount for each n-gram that includes a word withmultiple stress patterns, but in the absence of reli-able frequency information to weight each patternwe chose simply to use the pronunciation listed firstin the dictionary, which is judged by the lexicogra-pher to be the most popular.
Very little accuracyis lost in making this assumption.
Of the 115,966words in the dictionary, 4635 have more than onepronunciation; of these, 1269 have more than onedistinct stress pattern; of these, 525 have differentprimary stress placements.
This smallest class has afew common words (such as "refuse" used as a nounand as a verb), but most either occur infrequently intext (obscure proper nouns, for example), or have aprimary pronunciation that is overwhelmingly morecommon than the rest.4 Exper imentsThe efficiency of the n-gram training procedure al-lowed us to exploit a wealth of data--over 60 mil-lion syl lables--from 38 million words of Wall StreetJournal text.
We discarded sentences not completelycovered by the pronunciation dictionary, leaving 36.1million words and 60.7 million syllables for experi-mentation.Our first experiments used the binary ~1 alpha-bet.
The maximum entropy rate possible for thisprocess is one bit per syllable, and given the unigramdistribution of stress values in the data (55.2% areprimary), an upper bound of slightly over 0.99 bitscan be computed.
Examining the 4-gram frequenciesfor the entire corpus (Figure 3a) sharpens this sub-stantially, yielding an entropy rate estimate of 0.846bits per syllable.
Most frequent among the 4-gramsare the patterns WSWS and SWSW, consistent withthe principle of binary alternation mentioned in sec-tion 1.The 4-gram estimate matches quite closely withthe estimate of 0.852 bits that can be derived fromthe distribution of word stress patterns excerptedin Figure 3b.
But both measures overestimate theentropy rate by ignoring longer-range dependenciesthat become vident when we use larger values of n.For n = 6 we obtain a rate of 0.795 bits per syllableover the entire corpus.Since we had several thousand times more datathan is needed to make reliable estimates of stressentropy rate for values of n less than 7, it was prac-tical to subdivide the corpus according to some cri-terion, and calculate the stress entropy rate for eachsubset as well as for the whole.
We chose to divide atthe sentence level and to partition the 1.59 millionsentences in the data based on a likelihood measuresuitable for testing the hypothesis from section 1.A lexical trigram backoff-smoothed languagemodel was trained on separate data to estimate thelanguage perplexity of each sentence in the corpus.Sentence perplexity PP(S) is the inverse of sentence1probability normalized for length, 1/P(S)r~7, whereP(S) is the probability of the sentence according tothe language model and ISI is its word count.
Thismeasure gauges the average "surprise" after reveal-ing each word in the sentence as judged by the tri-gram model.
The question of whether more probableword sequences are also more rhythmic can be ap-proximated by asking whether sentences with lowerperplexity have lower stress entropy rate.Each sentence in the corpus was assigned to oneof one hundred bins according to its perplexity--sentences with perplexity between 0 and 10 were as-signed to the first bin; between 10 and 20, the sec-3053e406W'u'4W: 0.78~ WSk'-~: 6.91~ SWt,/W: 2.96~ SSWW: 3.94~~S:  2.94~ WSWS: 11.00~ S~F~S: 7.80~ SSWS: 8.59~I~SW: 6.97~ WSSW: 6.16~ SWSW: 11.21~ SSSW: 6.25~k'WSS: 3.71~ WSSS: 6.06~ SWSS: 8.48~ SSSS: 6.27~S 45.87~SW 18.94~W 9.54~(b) s~r~ s.74~ws 5.14~WSW 4.54~Figure 3: (a) The corpus frequencies of all binary stress 4-grams (based on 60.7 million syllables), withsecondary stress mapped to "weak" (W).
(b) The corpus frequencies of the top six lexical stress patterns.Wail Sb'eet Jouinal sylaldes per tmtd, by perpledty binWan Street Journal sentences2.5e+Q6==13 !
:~ te~65QO000Wall Street Journal Iraining symbols (sylabl=), by perple~dty binWall Street Journal se~llences100 2~0 300 400 500 600 700 800 900 1000 L=~gu~e peq~zay1.781,761,741.72t.7 |~.
1.68_=1.661.641,621.61.581.56(a)I i f i I I I i I100 200 300 400 500 600 700 8{\]0 900 1000Language peq31e~yFigure 4: The amount of training data, in syllables,in each perplexity bin.
The bin at perplexity level ppcontains all sentences in the corpus with perplexityno less than pp and no greater than pp + 10.
Thesmallest count (at bin 990) is 50662.ond; and so on.
Sentences with perplexity greaterthan 1000, which numbered roughly 106 thousandout of 1.59 million, were discarded from all exper-iments, as 10-unit bins at that level captured toolittle data for statistical significance.
A histogramshowing the amount of training data (in syllables)per perplexity bin is given in Figure 4.It is crucial to detect and understand potentialsources of bias in the methodology so far.
It is clearthat the perplexity bins are well trained, but not yetthat they are comparable with each other.
Figure 5shows the average number of syllables per word insentences that appear in each bin.
That this func-tion is roughly increasing agrees with our intuitionthat sequences with longer words are rarer.
But itbiases our perplexity bins at the extremes.
Earlybins, with sequences that have a small syllable rateper word (1.57 in the 0 bin, for example), are pre-disposed to a lower stress entropy rate since primarystresses, which occur roughly once per word, aremore frequent.
Later bins are also likely to be prej-udiced in that direction, for the inverse reason: TheFigure 5: The average number of syllables per wordfor each perplexity bin.increasing frequency of multisyllabic words makesit more and more fashionable to transit to  a weak-stressed syllable following a primary stress, sharpen-ing the probability distribution and decreasing en-tropy.This is verified when we run the stress entropyrate computation for each bin.
The results for n-gram models of orders 3 through 7, for the casein which secondary lexical stress is mapped to the"weak" level, are shown in Figure 6.All of the rates calculated are substantially lessthan a bit, but this only reflects the stress regu-larity inherent in the vocabulary and in word se-lection, and says nothing about word arrangement.The atomic elements in the text stream, the words,contribute regularity independently.
To determinehow much is contributed by the way they are gluedtogether, we need to remove the bias of word choice.For this reason we settled on a model size, n = 6,and performed a variety of experiments with boththe original corpus and with a control set that con-tained exactly the same bins with exactly the samesentences, but mixed up.
Each sentence in thecontrol set was permuted with a pseudorandom se-quence of swaps based on an insensitive function ofthe original; that is to say, identical sentences in the30610.050.9"~ 0.85 ?i0.80.78Wall Street Journal BINARY stress entropy rates, by pe~exily bin, i3-gram model ~4-gr~ model -.5-gram modelS-gram rnodd ~- .7-gram model ~.
-  -?
.
.
.
,il,J i i i i I I I i100 200 300 400 500 600 700 800 goo 1000Language ~Olex i tyFigure 6: n-gram stress entropy rates for ~z, weaksecondary stresscorpus were shuffled the same way and sentencesdiffering by only one word were shuffled similarly.This allowed us to keep steady the effects of mul-tiple copies of the same sentence in the same per-plexity bin.
More importantly, these tests hold ev-erything constant--diction, syllable count, syllablerate per word--except for syntax, the arrangementof the chosen words within the sentence.
Compar-ing the unrandomized results with this control ex-periment allows us, therefore, to factor out every-thing but word order.
In particular, subtracting thestress entropy rates of the original sentences fromthe rates of the randomized sentences gives us a fig-ure, relative entropy, that estimates how many bitswe save by knowing the proper word order given theword choice.
The results for these tests for weakand strong secondary stress are shown in Figures 7and 8, including the difference curves between therandomized-word and original entropy rates.The consistently positive difference functiondemonstrates that there is some extra stress regu-larity to be had with proper word order, about ahundredth of a bit on average.
The difference issmall indeed, but its consistency over hundreds ofwell-trained ata points puts the observation on sta-tistically solid ground.The negative slopes of the difference curves sug-gests a more interesting conclusion: As sentence per-plexity increases, the gap in stress entropy rate be-tween syntactic sentences and randomly permutedsentences narrows.
Restated inversely, using entropyrates for randomly permuted sentences as a baseline,sentences with higher sequence probability are rela-tively more rhythmical in the sense of our definitionfrom section 1.To supplement the ~z binary vocabulary tests weran the same experiments with ~2 = {0, 1, P}, in-troducing a pause symbol to examine how stress be-haves near phrase boundaries.
Commas, dashes,semicolons, colons, ellipses, and all sentence-terminating punctuation in the text, which were re-moved in the E1 tests, were mapped to a single pausesymbol for E~.
Pauses in the text arise not onlyfrom semantic onstraints but also from physiologi-cal limitations.
These include the "breath groups"of syllables that influence both vocalized and writ-ten production.
(Ochsner, 1989).
The results forthese experiments are shown in Figures 9 and 10.Expectedly, adding the symbol increases the confu-sion and hence the entropy, but the rates remain lessthan a bit.
The maximum possible rate for a ternarysequence is log 2 3 ~ 1.58.The experiments in this section were repeatedwith a larger perplexity interval that partitionedthe corpus into 20 bins, each covering 50 units ofperplexity.
The resulting curves mirrored the finer-grain curves presented here.5 Conc lus ions  and  fu ture  workWe have quantified lexical stress regularity, mea-sured it in a large sample of written English prose,and shown there to be a significant contribution fromword order that increases with lexical perplexity.This contribution was measured by comparing theentropy rate of lexical stress in natural sentenceswith randomly permuted versions of the same.
Ran-domizing the word order in this way yields a fairlycrude baseline, as it produces asyntactic sequencesin which, for example, single-syllable function wordscan unnaturally clash.
To correct for this we modi-fied the randomization algorithm to permute onlyopen-class words and to fix in place determiners,particles, pronouns, and other closed-class words.We found the entropy rates to be consistently mid-way between the fully randomized and unrandom-ized values.
But even this constrained randomiza-tion is weaker than what we'd like.
Ideally we shouldfactor out semantics as well as word choice, compar-ing each sentence in the corpus with its grammaticalvariations.
While this is a difficult experiment to doautomatically, we're hoping to approximate it usinga natural anguage generation system based on linkgrammar under development by the author.Also, we're currently testing other data sourcessuch as the Switchboard corpus of telephone speech(Godfrey, Holliman, and McDaniel, 1992) to mea-sure the effects of rhythm in more spontaneous andgrammatically relaxed texts.6 AcknowledgmentsComments from John Lafferty, Georg Niklfeld, andFrank Dellaert contributed greatly to this paper.The work was supported in part by an ARPAAASERT award, number DAAH04-95-1-0475.307Wall Street Journal BINARY stress entmpJ rates, by pefideagy bin; secondap/slxess mapped to WEAK0.81 .
.
.
.Randomized Wall Street Journal sentences -h-,o.o ~'N*v~,0.700.750.74 I I I I I I I I I100 200 300 400 500 6(:0 700 800 900 10~OLan~a9~ pe~pl?alyi 0,780,77Wal Street Journal BINARY stpess entropy rate differences, by perplexity b~n; secondary stress mapped 1o WEAK0.025 ~ , , , ,WSJ randomized ~nus nomaedornized0.020.0150.005i I I i i i i i i100 200 300 400 500 600 700 800 900 1000Langu~je pe~p~ex~Figure 7: 6-gram stress entropy rates and difference curve for El, weak secondary stress0.762 0.75i 0.74"Wail Slmet J~mal BINARY alress enbopy rates, by pel~e~ty bin; seco~daly stress map~ to ~RONG Wall Street Journal BINARY sVess entropy into differences, by pe~ ~;  s~daw stm~ ~ to STRONG0.79 0.024 , , / , , .Wag Street Journal sentences ~ I WSJ randon~zed minus nonmndon~zedRandomized Wall Street Journal sentences -~---- 0.022/ ~*V,?~ oo2~ ~ ", 0.0180.0140,0120.010.730.0080.72, 0.0060,71 I I I I I I I I I 0,004 I I I I I I I I I100 200 300 400 500 600 700 800 900 1000 0 100 200 300 400 500 600 700 800 900 1000Language pelpleagy Language perplexityFigure 8: 6-gram entropy rates and difference curve for El, strong secondary stress3080.940.93Wall Street Journal TERNARY stress ent ropy  rates ,  by per~ex~ty bin; secomlary stress mapped to STRONG0.97~,  Randomized wW2 i,Sstl:eel, ~Ur~all :ene~e~c: .+--~-o.~ ' ~+~t**  , z~~; ;i '*' "0.92 ~ %0.91i i I I l i i i I100 200 300 400 500 600 700 800 900 10OOLanguage perplexityWall Street Journal TERNARY stress entropy ra te  d i f fe rences ,  by pmple~ty bin; secondary stress mapped to WEAK0.05 .
,0.0450.040.0~50.030.0250.020.0150.010.005, i , ,WSJ randomized minus nonrandomized -,,,--I I I I I I I I I100 200 300 400 800 600 700 800 900 1000Language pe~plexilyFigure 9: 6-gram entropy rates and difference curve for ~ ,  weak secondary stressWall Street Journal TERNARY sYess entropy rates, by perple}iffy bin; secorldaPj sb'ess mapped to STRONG Wall Street Journal TERNARY stress entrppy rate differences, by perplexity b~n; seco~a~ =~s ~ to STRONG0.94 +.L , , , , , , , , , 0.05. , , ,~a%*~.. * W~dl Street Journal senterces ~ WSJ randomized minus n~mmloralzed' "~ ~ Randomized WaO Street Journal sentences -*--.
/ "U+"*% o.o~ o8~ "~!~0.92 ~ $ ~:~081 4 , I .~  ~'~,j j~v~.
,:~ ' "  IN ,.Ii~ Vi~ i ,  .
*0.9 ' ; ' '0.89 ~ ~0.880.8700.040 .~0.0.
',0.0250.0~0.01~0.010.005i i i i i i ~ i i i i i i i i I i i100 200 300 400 500 000 700 800 ~0 1000 0 100 200 300 ~x}O 5(\]0 600 700 800 900 1000Language pelple.,dly La.~ge perplexityFigure 10: 6-gram entropy rates and difference curve for E2, strong secondary stressReferencesAbercrombie, D. 1967.
Elements of general phonet-ics.
Edinburgh University Press.Baum, P. F. 1952.
The Other Harmony of Prose.Duke University Press.
?Cover, T. M. and J.
A. Thom~.
1991.
Elements ofinformation theory.
John Wiley & Sons, Inc.Godfrey, J., E. Holliman, and J. McDaniel.
1992.Switchboard: Telephone speech corpus for re-search development.
In Proc.
ICASSP-92, pages1-517-520.Halle, M. and J. Vergnaud.
1987.
An essay onstress.
The MIT Press.Harding, D. W. 1976.
Words into rhythm: Englishspeech rhythm in verse and prose.
Cambridge Uni-versity Press.Jassem, W., D. R. Hill, and I. H. Witten.
1984.Isochrony in English speech: its statistical valid-ity and linguistic relevance.
In D. Gibbon andH.
Richter, editors, Intonation, rhythm, and ac-cent: Studies in Discourse Phonology.
Walter deGruyter, pages 203-225.Kager, R. 1989a.
A metrical theory of stress anddestressing in English and Dutch.
Foris Publica-tions.Kager, R. 1989b.
The rhythm of English prose.Foris Publications.Ochsner, R.S.
1989.
Rhythm and writing.
TheWhitson Publishing Company.Parzen, E. 1962.
Stochastic processes.
Holden-Day.309
