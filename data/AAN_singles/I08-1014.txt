Determining the Unithood of Word Sequences using a ProbabilisticApproachWilson Wong, Wei Liu and Mohammed BennamounSchool of Computer Science and Software EngineeringUniversity of Western AustraliaCrawley WA 6009{wilson,wei,bennamou}@csse.uwa.edu.auAbstractMost research related to unithood were con-ducted as part of a larger effort for the deter-mination of termhood.
Consequently, nov-elties are rare in this small sub-field of termextraction.
In addition, existing work weremostly empirically motivated and derived.We propose a new probabilistically-derivedmeasure, independent of any influences oftermhood, that provides dedicated measuresto gather linguistic evidence from parsedtext and statistical evidence from Googlesearch engine for the measurement of unit-hood.
Our comparative study using 1, 825test cases against an existing empirically-derived function revealed an improvement interms of precision, recall and accuracy.1 IntroductionAutomatic term recognition, also referred to as termextraction or terminology mining, is the process ofextracting lexical units from text and filtering themfor the purpose of identifying terms which charac-terise certain domains of interest.
This process in-volves the determination of two factors: unithoodand termhood.
Unithood concerns with whether ornot a sequence of words should be combined toform a more stable lexical unit.
On the other hand,termhood measures the degree to which these sta-ble lexical units are related to domain-specific con-cepts.
Unithood is only relevant to complex terms(i.e.
multi-word terms) while termhood (Wong etal., 2007a) deals with both simple terms (i.e.
single-word terms) and complex terms.
Recent reviews by(Wong et al, 2007b) show that existing research onunithood are mostly carried out as a prerequisite tothe determination of termhood.
As a result, thereis only a small number of existing measures dedi-cated to determining unithood.
Besides the lack ofdedicated attention in this sub-field of term extrac-tion, the existing measures are usually derived fromterm or document frequency, and are modified asper need.
As such, the significance of the differentweights that compose the measures usually assumean empirical viewpoint.
Obviously, such methodsare at most inspired by, but not derived from formalmodels (Kageura and Umino, 1996).The three objectives of this paper are (1) to sepa-rate the measurement of unithood from the determi-nation of termhood, (2) to devise a probabilistically-derived measure which requires only one thresh-old for determining the unithood of word se-quences using non-static textual resources, and (3)to demonstrate the superior performance of the newprobabilistically-derived measure against existingempirical measures.
In regards to the first objective,we will derive our probabilistic measure free fromany influence of termhood determination.
Follow-ing this, our unithood measure will be an indepen-dent tool that is applicable not only to term extrac-tion, but many other tasks in information extractionand text mining.
Concerning the second objective,we will devise our new measure, known as the Oddsof Unithood (OU), which are derived using BayesTheorem and founded on a few elementary probabil-ities.
The probabilities are estimated using Googlepage counts in an attempt to eliminate problems re-lated to the use of static corpora.
Moreover, only103one threshold, namely, OUT is required to controlthe functioning of OU .
Regarding the third objec-tive, we will compare our new OU against an ex-isting empirically-derived measure called Unithood(UH) (Wong et al, 2007b) in terms of their preci-sion, recall and accuracy.In Section 2, we provide a brief review on some ofexisting techniques for measuring unithood.
In Sec-tion 3, we present our new probabilistic approach,the measures involved, and the theoretical and in-tuitive justification behind every aspect of our mea-sures.
In Section 4, we summarize some findingsfrom our evaluations.
Finally, we conclude this pa-per with an outlook to future work in Section 5.2 Related WorksSome of the most common measures of unithoodinclude pointwise mutual information (MI) (Churchand Hanks, 1990) and log-likelihood ratio (Dunning,1994).
In mutual information, the co-occurrence fre-quencies of the constituents of complex terms areutilised to measure their dependency.
The mutualinformation for two words a and b is defined as:MI(a, b) = log2p(a, b)p(a)p(b) (1)where p(a) and p(b) are the probabilities of occur-rence of a and b.
Many measures that apply sta-tistical techniques assuming strict normal distribu-tion, and independence between the word occur-rences (Franz, 1997) do not fare well.
For handlingextremely uncommon words or small sized corpus,log-likelihood ratio delivers the best precision (Kurzand Xu, 2002).
Log-likelihood ratio attempts toquantify how much more likely one pair of words isto occur compared to the others.
Despite its poten-tial, ?How to apply this statistic measure to quan-tify structural dependency of a word sequence re-mains an interesting issue to explore.?
(Kit, 2002).
(Seretan et al, 2004) tested mutual information, log-likelihood ratio and t-tests to examine the use of re-sults from web search engines for determining thecollocational strength of word pairs.
However, noperformance results were presented.
(Wong et al, 2007b) presented a hybrid approachinspired by mutual information in Equation 1, andC-value in Equation 3.
The authors employ Googlepage counts for the computation of statistical evi-dences to replace the use of frequencies obtainedfrom static corpora.
Using the page counts, the au-thors proposed a function known as Unithood (UH)for determining the mergeability of two lexical unitsax and ay to produce a stable sequence of words s.The word sequences are organised as a set W ={s, ax, ay} where s = axbay is a term candidate,b can be any preposition, the coordinating conjunc-tion ?and?
or an empty string, and ax and ay caneither be noun phrases in the form Adj?N+ or an-other s (i.e.
defining a new s in terms of other s).The authors define UH as:UH(ax, ay) =??????????????????????????????
?1 if (MI(ax, ay) > MI+) ?
(MI+ ?
MI(ax, ay)?
MI?
?ID(ax, s) ?
IDT ?ID(ay, s) ?
IDT ?IDR+ ?
IDR(ax, ay)?
IDR?
)0 otherwise(2)where MI+, MI?, IDT , IDR+ and IDR?are thresholds for determining mergeability deci-sions, and MI(ax, ay) is the mutual information be-tween ax and ay, while ID(ax, s), ID(ay, s) andIDR(ax, ay) are measures of lexical independenceof ax and ay from s. For brevity, let z be either ax oray, and the independence measure ID(z, s) is thendefined as:ID(z, s) ={log10(nz ?
ns) if(nz > ns)0 otherwisewhere nz and ns is the Google page count for z ands respectively.
On the other hand, IDR(ax, ay) =ID(ax,s)ID(ay ,s) .
Intuitively, UH(ax, ay) states that the twolexical units ax and ay can only be merged in twocases, namely, 1) if ax and ay has extremely highmutual information (i.e.
higher than a certain thresh-old MI+), or 2) if ax and ay achieve average mu-tual information (i.e.
within the acceptable range oftwo thresholds MI+ and MI?)
due to both of theirextremely high independence (i.e.
higher than thethreshold IDT ) from s.(Frantzi, 1997) proposed a measure known asCvalue for extracting complex terms.
The measure104is based upon the claim that a substring of a termcandidate is a candidate itself given that it demon-strates adequate independence from the longer ver-sion it appears in.
For example, ?E.
coli food poi-soning?, ?E.
coli?
and ?food poisoning?
are accept-able as valid complex term candidates.
However,?E.
coli food?
is not.
Therefore, some measuresare required to gauge the strength of word combina-tions to decide whether two word sequences shouldbe merged or not.
Given a word sequence a to beexamined for unithood, the Cvalue is defined as:Cvalue(a) ={log2|a|fa if |a| = glog2|a|(fa ?
?l?La fl|La| ) otherwise(3)where |a| is the number of words in a, La is theset of longer term candidates that contain a, g isthe longest n-gram considered, fa is the frequencyof occurrence of a, and a /?
La.
While certain re-searchers (Kit, 2002) consider Cvalue as a termhoodmeasure, others (Nakagawa and Mori, 2002) acceptit as a measure for unithood.
One can observe thatlonger candidates tend to gain higher weights due tothe inclusion of log2|a| in Equation 3.
In addition,the weights computed using Equation 3 are purelydependent on the frequency of a.3 A Probabilistically-derived Measure forUnithood DeterminationWe propose a probabilistically-derived measure fordetermining the unithood of word pairs (i.e.
po-tential term candidates) extracted using the head-driven left-right filter (Wong, 2005; Wong et al,2007b) and Stanford Parser (Klein and Manning,2003).
These word pairs will appear in the form of(ax, ay) ?
A with ax and ay located immediatelynext to each other (i.e.
x + 1 = y), or separatedby a preposition or coordinating conjunction ?and?(i.e.
x + 2 = y).
Obviously, ax has to appear beforeay in the sentence or in other words, x < y for allpairs where x and y are the word offsets produced bythe Stanford Parser.
The pairs in A will remain aspotential term candidates until their unithood havebeen examined.
Once the unithood of the pairs inA have been determined, they will be referred to asterm candidates.
Formally, the unithood of any twolexical units ax and ay can be defined asDefinition 1 The unithood of two lexical units is the?degree of strength or stability of syntagmatic com-binations and collocations?
(Kageura and Umino,1996) between them.It is obvious that the problem of measuring theunithood of any pair of words is the determinationof their ?degree?
of collocational strength as men-tioned in Definition 1.
In practical terms, the ?de-gree?
mentioned above will provide us with a way todetermine if the units ax and ay should be combinedto form s, or left alone as separate units.
The collo-cational strength of ax and ay that exceeds a certainthreshold will demonstrate to us that s is able to forma stable unit and hence, a better term candidate thanax and ay separated.
It is worth pointing that thesize (i.e.
number of words) of ax and ay is not lim-ited to 1.
For example, we can have ax=?NationalInstitute?, b=?of?
and ay=?Allergy and InfectiousDiseases?.
In addition, the size of ax and ay has noeffect on the determination of their unithood usingour approach.As we have discussed in Section 2, most ofthe conventional practices employ frequency of oc-currence from local corpora, and some statisticaltests or information-theoretic measures to determinethe coupling strength between elements in W ={s, ax, ay}.
Two of the main problems associatedwith such approaches are:?
Data sparseness is a problem that is well-documented by many researchers (Keller et al,2002).
It is inherent to the use of local corporathat can lead to poor estimation of parametersor weights; and?
Assumption of independence and normality ofword distribution are two of the many problemsin language modelling (Franz, 1997).
Whilethe independence assumption reduces text tosimply a bag of words, the assumption of nor-mal distribution of words will often lead to in-correct conclusions during statistical tests.As a general solution, we innovatively employ re-sults from web search engines for use in a proba-bilistic framework for measuring unithood.As an attempt to address the first problem, weutilise page counts by Google for estimating theprobability of occurrences of the lexical units in W .105We consider the World Wide Web as a large generalcorpus and the Google search engine as a gatewayfor accessing the documents in the general corpus.Our choice of using Google to obtain the page countwas merely motivated by its extensive coverage.
Infact, it is possible to employ any search engines onthe World Wide Web for this research.
As for thesecond issue, we attempt to address the problem ofdetermining the degree of collocational strength interms of probabilities estimated using Google pagecount.
We begin by defining the sample space, N asthe set of all documents indexed by Google searchengine.
We can estimate the index size of Google,|N | using function words as predictors.
Functionwords such as ?a?, ?is?
and ?with?, as opposed tocontent words, appear with frequencies that are rel-atively stable over many different genres.
Next, weperform random draws (i.e.
trial) of documents fromN .
For each lexical unit w ?
W , there will be a cor-responding set of outcomes (i.e.
events) from thedraw.
There will be three basic sets which are ofinterest to us:Definition 2 Basic events corresponding to eachw ?
W :?
X is the event that ax occurs in the document?
Y is the event that ay occurs in the document?
S is the event that s occurs in the documentIt should be obvious to the readers that since the doc-uments in S have to contain all two units ax and ay,S is a subset of X ?
Y or S ?
X ?
Y .
It is worthnoting that even though S ?
X ?
Y , it is highlyunlikely that S = X ?
Y since the two portionsax and ay may exist in the same document withoutbeing conjoined by b.
Next, subscribing to the fre-quency interpretation of probability, we can obtainthe probability of the events in Definition 2 in termsof Google page count:P (X) = nx|N | (4)P (Y ) = ny|N |P (S) = ns|N |where nx, ny and ns is the page count returned asthe result of Google search using the term [+?ax?],[+?ay?]
and [+?s?
], respectively.
The pair ofquotes that encapsulates the search terms is thephrase operator, while the character ?+?
is the re-quired operator supported by the Google search en-gine.
As discussed earlier, the independence as-sumption required by certain information-theoreticmeasures and other Bayesian approaches may not al-ways be valid, especially when we are dealing withlinguistics.
As such, P (X ?
Y ) 6= P (X)P (Y )since the occurrences of ax and ay in documents areinevitably governed by some hidden variables andhence, not independent.
Following this, we definethe probabilities for two new sets which result fromapplying some set operations on the basic events inDefinition 2:P (X ?
Y ) = nxy|N | (5)P (X ?
Y \ S) = P (X ?
Y )?
P (S)where nxy is the page count returned by Googlefor the search using [+?ax?
+?ay?].
DefiningP (X?Y ) in terms of observable page counts, ratherthan a combination of two independent events willallow us to avoid any unnecessary assumption of in-dependence.Next, referring back to our main problem dis-cussed in Definition 1, we are required to estimatethe strength of collocation of the two units ax anday.
Since there is no standard metric for such mea-surement, we propose to address the problem froma probabilistic perspective.
We introduce the proba-bility that s is a stable lexical unit given the evidences possesses:Definition 3 Probability of unithood:P (U |E) = P (E|U)P (U)P (E)where U is the event that s is a stable lexical unitand E is the evidences belonging to s. P (U |E) isthe posterior probability that s is a stable unit giventhe evidence E. P (U) is the prior probability that sis a unit without any evidence, and P (E) is the priorprobability of evidences held by s. As we shall seelater, these two prior probabilities will be immaterialin the final computation of unithood.
Since s caneither be a stable unit or not, we can state that,P (U?
|E) = 1?
P (U |E) (6)106where U?
is the event that s is not a stable lexical unit.Since Odds = P/(1 ?
P ), we multiply both sidesof Definition 3 by (1?
P (U |E))?1 to obtain,P (U |E)1?
P (U |E) =P (E|U)P (U)P (E)(1?
P (U |E)) (7)By substituting Equation 6 in Equation 7 and later,applying the multiplication rule P (U?
|E)P (E) =P (E|U?
)P (U?)
to it, we will obtain:P (U |E)P (U?
|E) =P (E|U)P (U)P (E|U?
)P (U?)
(8)We proceed to take the log of the odds in Equation 8(i.e.
logit) to get:log P (E|U)P (E|U?)
= logP (U |E)P (U?
|E) ?
logP (U)P (U?)
(9)While it is obvious that certain words tend to co-occur more frequently than others (i.e.
idioms andcollocations), such phenomena are largely arbitrary(Smadja, 1993).
This makes the task of decidingon what constitutes an acceptable collocation dif-ficult.
The only way to objectively identify sta-ble lexical units is through observations in samplesof the language (e.g.
text corpus) (McKeown andRadev, 2000).
In other words, assigning the apri-ori probability of collocational strength without em-pirical evidence is both subjective and difficult.
Assuch, we are left with the option to assume thatthe probability of s being a stable unit and not be-ing a stable unit without evidence is the same (i.e.P (U) = P (U?)
= 0.5).
As a result, the second termin Equation 9 evaluates to 0:log P (U |E)P (U?
|E) = logP (E|U)P (E|U?)
(10)We introduce a new measure for determining theodds of s being a stable unit known as Odds of Unit-hood (OU):Definition 4 Odds of unithoodOU(s) = log P (E|U)P (E|U?
)Assuming that the evidences in E are independentof one another, we can evaluate OU(s) in terms of:OU(s) = log?i P (ei|U)?i P (ei|U?
)(11)=?ilog P (ei|U)P (ei|U?
)(a) The area with darkershade is the set X ?
Y \ S.Computing the ratio of P (S)and the probability of this areawill give us the first evidence.
(b) The area with darkershade is the set S?.
Comput-ing the ratio of P (S) and theprobability of this area (i.e.P (S?)
= 1?
P (S)) will giveus the second evidence.Figure 1: The probability of the areas with darkershade are the denominators required by the evi-dences e1and e2for the estimation of OU(s).where ei are individual evidences possessed by s.With the introduction of Definition 4, we can ex-amine the degree of collocational strength of axand ay in forming s, mentioned in Definition 1 interms of OU(s).
With the base of the log in Def-inition 4 more than 1, the upper and lower boundof OU(s) would be +?
and ?
?, respectively.OU(s) = +?
and OU(s) = ??
corresponds tothe highest and the lowest degree of stability of thetwo units ax and ay appearing as s, respectively.
Ahigh1 OU(s) would indicate the suitability for thetwo units ax and ay to be merged to form s. Ulti-mately, we have reduced the vague problem of thedetermination of unithood introduced in Definition1 into a practical and computable solution in Defini-tion 4.
The evidences that we propose to employ fordetermining unithood are based on the occurrencesof s, or the event S if the readers recall from Defini-tion 2.
We are interested in two types of occurrencesof s, namely, the occurrence of s given that ax anday have already occurred or X ?
Y , and the occur-rence of s as it is in our sample space, N .
We referto the first evidence e1as local occurrence, whilethe second one e2as global occurrence.
We willdiscuss the intuitive justification behind each type ofoccurrences.
Each evidence ei captures the occur-rences of s within a different confinement.
We willestimate these evidences in terms of the elementaryprobabilities already defined in Equations 4 and 5.The first evidence e1captures the probability ofoccurrences of s within the confinement of ax and ay1A subjective issue that may be determined using a threshold107or X?Y .
As such, P (e1|U) can be interpreted as theprobability of s occurring within X ?
Y as a stableunit or P (S|X ?
Y ).
On the other hand, P (e1|U?
)captures the probability of s occurring in X ?
Y notas a unit.
In other words, P (e1|U?)
is the probabilityof s not occurring in X ?
Y , or equivalently, equalto P ((X ?
Y \ S)|(X ?
Y )).
The set X ?
Y \ S isshown as the area with darker shade in Figure 1(a).Let us define the odds based on the first evidence as:OL =P (e1|U)P (e1|U?)
(12)Substituting P (e1|U) = P (S|X ?
Y ) andP (e1|U?)
= P ((X ?
Y \ S)|(X ?
Y )) into Equa-tion 12 will give us:OL =P (S|X ?
Y )P ((X ?
Y \ S)|(X ?
Y ))= P (S ?
(X ?
Y ))P (X ?
Y )P (X ?
Y )P ((X ?
Y \ S) ?
(X ?
Y ))= P (S ?
(X ?
Y ))P ((X ?
Y \ S) ?
(X ?
Y ))and since S ?
(X?Y ) and (X?Y \S) ?
(X?Y ),OL =P (S)P (X ?
Y \ S) if(P (X ?
Y \ S) 6= 0)and OL = 1 if P (X ?
Y \ S) = 0.The second evidence e2captures the probabilityof occurrences of s without confinement.
If s is astable unit, then its probability of occurrence in thesample space would simply be P (S).
On the otherhand, if s occurs not as a unit, then its probability ofnon-occurrence is 1?P (S).
The complement of S,which is the set S?
is shown as the area with darkershade in Figure 1(b).
Let us define the odds basedon the second evidence as:OG =P (e2|U)P (e2|U?)
(13)Substituting P (e2|U) = P (S) and P (e2|U?)
= 1 ?P (S) into Equation 13 will give us:OG =P (S)1?
P (S)Intuitively, the first evidence attempts to capturethe extent to which the existence of the two lexicalunits ax and ay is attributable to s. Referring backto OL, whenever the denominator P (X ?Y \S) be-comes less than P (S), we can deduce that ax anday actually exist together as s more than in otherforms.
At one extreme when P (X ?
Y \ S) = 0,we can conclude that the co-occurrence of ax anday is exclusively for s. As such, we can also refer toOL as a measure of exclusivity for the use of ax anday with respect to s. This first evidence is a goodindication for the unithood of s since the more theexistence of ax and ay is attributed to s, the strongerthe collocational strength of s becomes.
Concerningthe second evidence, OG attempts to capture the ex-tent to which s occurs in general usage (i.e.
WorldWide Web).
We can consider OG as a measure ofpervasiveness for the use of s. As s becomes morewidely used in text, the numerator in OG will in-crease.
This provides a good indication on the unit-hood of s since the more s appears in usage, the like-lier it becomes that s is a stable unit instead of an oc-currence by chance when ax and ay are located nextto each other.
As a result, the derivation of OU(s)using OL and OG will ensure a comprehensive wayof determining unithood.Finally, expanding OU(s) in Equation 11 usingEquations 12 and 13 will give us:OU(s) = logOL + logOG (14)= log P (S)P (X ?
Y \ S) + logP (S)1?
P (S)As such, the decision on whether ax and ay shouldbe merged to form s can be made based solely onthe Odds of Unithood (OU) defined in Equation 14.We will merge ax and ay if their odds of unithoodexceeds a certain threshold, OUT .4 Evaluations and DiscussionsFor this evaluation, we employed 500 news arti-cles from Reuters in the health domain gathered be-tween December 2006 to May 2007.
These 500 arti-cles are fed into the Stanford Parser whose output isthen used by our head-driven left-right filter (Wong,2005; Wong et al, 2007b) to extract word sequencesin the form of nouns and noun phrases.
Pairs of wordsequences (i.e.
ax and ay) located immediately nextto each other, or separated by a preposition or theconjunction ?and?
in the same sentence are mea-108sured for their unithood.
Using the 500 news arti-cles, we managed to obtain 1, 825 pairs of words tobe tested for unithood.We performed a comparative study of ournew probabilistic approach against the empirically-derived unithood function described in Equation 2.Two experiments were conducted.
In the first one,we assessed our probabilistically-derived measureOU(s) as described in Equation 14 where the de-cisions on whether or not to merge the 1, 825 pairsare done automatically.
These decisions are knownas the actual results.
At the same time, we inspectedthe same list manually to decide on the merging ofall the pairs.
These decisions are known as the idealresults.
The threshold OUT employed for our evalu-ation is determined empirically through experimentsand is set to ?8.39.
However, since only one thresh-old is involved in deciding mergeability, training al-gorithms and data sets may be employed to automat-ically decide on an optimal number.
This option isbeyond the scope of this paper.
The actual and idealresults for this first experiment are organised intoa contingency table (not shown here) for identify-ing the true and the false positives, and the true andthe false negatives.
In the second experiment, weconducted the same assessment as carried out in thefirst one but the decisions to merge the 1, 825 pairsare based on the UH(ax, ay) function described inEquation 2.
The thresholds required for this func-tion are based on the values suggested by (Wong etal., 2007b), namely, MI+ = 0.9, MI?
= 0.02,IDT = 6, IDR+ = 1.35, and IDR?
= 0.93.Table 1: The performance of OU(s) (from Exper-iment 1) and UH(ax, ay) (from Experiment 2) interms of precision, recall and accuracy.
The lastcolumn shows the difference in the performance ofExperiment 1 and 2.Using the results from the contingency tables,we computed the precision, recall and accuracy forthe two measures under evaluation.
Table 1 sum-marises the performance of OU(s) and UH(ax, ay)in determining the unithood of 1, 825 pairs of lex-ical units.
One will notice that our new measureOU(s) outperformed the empirically-derived func-tion UH(ax, ay) in all aspects, with an improvementof 2.63%, 3.33% and 2.74% for precision, recall andaccuracy, respectively.
Our new measure achieved a100% precision with a lower recall at 95.83%.
Aswith any measures that employ thresholds as a cut-off point in accepting or rejecting certain decisions,we can improve the recall of OU(s) by decreasingthe threshold OUT .
In this way, there will be lessfalse negatives (i.e.
pairs which are supposed to bemerged but are not) and hence, increases the recallrate.
Unfortunately, recall will improve at the ex-pense of precision since the number of false pos-itives will definitely increase from the existing 0.Since our application (i.e.
ontology learning) re-quires perfect precision in determining the unithoodof word sequences, OU(s) is the ideal candidate.Moreover, with only one threshold (i.e.
OUT ) re-quired in controlling the function of OU(s), we areable to reduce the amount of time and effort spenton optimising our results.5 Conclusion and Future WorkIn this paper, we highlighted the significance of unit-hood and that its measurement should be given equalattention by researchers in term extraction.
We fo-cused on the development of a new approach thatis independent of influences of termhood measure-ment.
We proposed a new probabilistically-derivedmeasure which provide a dedicated way to deter-mine the unithood of word sequences.
We refer tothis measure as the Odds of Unithood (OU).
OU isderived using Bayes Theorem and is founded upontwo evidences, namely, local occurrence and globaloccurrence.
Elementary probabilities estimated us-ing page counts from the Google search engine areutilised to quantify the two evidences.
The newprobabilistically-derived measure OU is then eval-uated against an existing empirical function knownas Unithood (UH).
Our new measure OU achieved aprecision and a recall of 100% and 95.83% respec-tively, with an accuracy at 97.26% in measuring theunithood of 1, 825 test cases.
OU outperformed UHby 2.63%, 3.33% and 2.74% in terms of precision,109recall and accuracy, respectively.
Moreover, our newmeasure requires only one threshold, as compared tofive in UH to control the mergeability decision.More work is required to establish the coverageand the depth of the World Wide Web with regardsto the determination of unithood.
While the Web hasdemonstrated reasonable strength in handling gen-eral news articles, we have yet to study its appropri-ateness in dealing with unithood determination fortechnical text (i.e.
the depth of the Web).
Similarly,it remains a question the extent to which the Webis able to satisfy the requirement of unithood deter-mination for a wider range of genres (i.e.
the cov-erage of the Web).
Studies on the effect of noises(e.g.
keyword spamming) and multiple word senseson unithood determination using the Web is anotherfuture research direction.AcknowledgementThis research was supported by the Australian En-deavour International Postgraduate Research Schol-arship, and the Research Grant 2006 by the Univer-sity of Western Australia.ReferencesK.
Church and P. Hanks.
1990.
Word association norms,mutual information, and lexicography.
ComputationalLinguistics, 16(1):22?29.T.
Dunning.
1994.
Accurate methods for the statistics ofsurprise and coincidence.
Computational Linguistics,19(1):61?74.K.
Frantzi.
1997.
Incorporating context information forthe extraction of terms.
In Proceedings of the 35th An-nual Meeting on Association for Computational Lin-guistics, Spain.A.
Franz.
1997.
Independence assumptions consideredharmful.
In Proceedings of the 8th Conference on Eu-ropean Chapter of the Association for ComputationalLinguistics, Madrid, Spain.K.
Kageura and B. Umino.
1996.
Methods of automaticterm recognition: A review.
Terminology, 3(2):259?289.F.
Keller, M. Lapata, and O. Ourioupina.
2002.
Usingthe web to overcome data sparseness.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), Philadelphia.C.
Kit.
2002.
Corpus tools for retrieving and derivingtermhood evidence.
In Proceedings of the 5th EastAsia Forum of Terminology, Haikou, China.D.
Klein and C. Manning.
2003.
Accurate unlexicalizedparsing.
In Proceedings of the 41st Meeting of the As-sociation for Computational Linguistics.D.
Kurz and F. Xu.
2002.
Text mining for the extrac-tion of domain relevant terms and term collocations.In Proceedings of the International Workshop on Com-putational Approaches to Collocations, Vienna.K.
McKeown and D. Radev.
2000.
Collocations.
InR.
Dale, H. Moisl, and H. Somers, editors, Handbookof Natural Language Processing.
Marcel Dekker.H.
Nakagawa and T. Mori.
2002.
A simple but powerfulautomatic term extraction method.
In Proceedings ofthe International Conference On Computational Lin-guistics (COLING).V.
Seretan, L. Nerima, and E. Wehrli.
2004.
Usingthe web as a corpus for the syntactic-based colloca-tion identification.
In Proceedings of the InternationalConference on on Language Resources and Evaluation(LREC), Lisbon, Portugal.F.
Smadja.
1993.
Retrieving collocations from text:Xtract.
Computational Linguistics, 19(1):143?177.W.
Wong, W. Liu, and M. Bennamoun.
2007a.
Deter-mining termhood for learning domain ontologies in aprobabilistic framework.
In Proceedings of the 6thAustralasian Conference on Data Mining (AusDM),Gold Coast.W.
Wong, W. Liu, and M. Bennamoun.
2007b.
Deter-mining the unithood of word sequences using mutualinformation and independence measure.
In Proceed-ings of the 10th Conference of the Pacific Associa-tion for Computational Linguistics (PACLING), Mel-bourne, Australia.W.
Wong.
2005.
Practical approach to knowledge-based question answering with natural language un-derstanding and advanced reasoning.
Master?s thesis,National Technical University College of Malaysia,arXiv:cs.CL/0707.3559.110
