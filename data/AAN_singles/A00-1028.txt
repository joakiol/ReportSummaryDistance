Experiments with Corpus-based LFG SpecializationNico la  Cancedda and  Chr i s te r  SamuelssonXerox Research Centre Europe,6, chemin de Maupertuis38240 Meylan, France{cancedda.lsamuelsson}@xrce.xerox.comAbst rac tSophisticated grammar formalisms, uch as LFG, al-low concisely capturing complex linguistic phenom-ena.
The powerful operators provided by such for-malisms can however introduce spurious ambigu-ity, making parsing inefficient.
A simple form ofcorpus-based grammar pruning is evaluated experi-mentally on two wide-coverage grammars, one En-giish and one French.
Speedups of up to a factor 6were obtained, at a cost in grammatical coverage ofabout 13%.
A two-stage architecture allows achiev-ing significant speedups without introducing addi-tional parse failures.1 In t roduct ionExpressive grammar formalisms allow grammar de-velopers to capture complex linguistic generaliza-tions concisely and elegantly, thus greatly facilitat-ing grammar development and maintenance.
(Car-rol, 1994) found that the empirical performancewhen parsing with unification-based grammars isnowhere near the theoretical worst-case complexity.Nonetheless, directly parsing with such grammars,in the form they were developed, can be very ineffi-cient.
For this reason, grammars are typically com-piled into representations that allow faster parsing.This does however not solve the potential problemof the grammars overgenerating considerably, thusallowing large amounts of spurious ambiguity.
In-deed, a current rend in high-coverage parsing, es-pecially when employing a statistical model of lan-guage, see, e.g., (Collins 97), is to allow the grammarto massively overgenerate and instead isambiguateby statistical means during or after parsing.
If thebenefits resulting from more concise grammatical de-scriptions are to outweigh the costs of spurious am-biguity, the latter must be brought down.In such a situation, corpus-based compilationtechniques can drastically improve parsing perfor-mance without burdening the grammar developer.The initial, and much seminal work in this areawas been carried out by Rayner and coworkers, see(Rayner 1988), (Samuelsson and Rayner 91) and(Rayner and Carter 1996).
In the current article,we apply similar ideas to Lexical Functional Gram-mar (LFG) in the incarnation of the Xerox Linguis-tic Environment (XLE).
The goal is to investigateto what extent corpus-based compilation techniquescan reduce overgeneration a d spurious ambiguity,and increase parsing efficiency, without jeopardiz-ing coverage.
The rest of the article is organizedas follows: Section 2 presents the relevant aspectsof the LFG formalism and the pruning strategy em-ployed, Section 3 describes the experimental setup,Section 4 reports the experimental results and Sec-tion 5 relates this to other work.2 LFG and  Grammar  Prun ingThe LFG formalism (Kaplan and Bresnan, 1982) al-lows the right-hand sides (RHS) of grammar rules toconsist of a regular expression over grammar sym-bols.
This makes it more appropriate to refer tothe grammar rules as rule schemata, since each RHScan potentially be expanded into a (possibly infinite)number of distinct sequences of grammar symbols,each corresponding to a traditional phrase-structurerule.
As can easily be imagined, the use of regular-expression operators uch as Kleene-star and com-plementation may introduce a considerable amountof spurious ambiguity.
Moreover, the LFG formal-ism provides operators which - -  although not in-creasing its theoretical expressive power - -  allowrules to be written more concisely.
Examples of suchoperators are the ignore operator, which allows skip-ping any sequence of grammar symbols that matchesa given pattern; the shuffle operator, which allowsa set of grammar symbols to occur in any order;and the linear precedence operator, which allows par-tially specifying the order of grammar symbols.The pruning method we propose consists in elim-inating complex operators from the grammar de-scription by considering how they were actually in-stantiated when parsing a corpus.
In LFGs, eachrule scheme corresponds to a particular grammarsymbol, since different expansions of the same sym-bol are expressed as alternatives in the regular ex-pression on its RHS.
We can define a specific paththrough the RHS of a rule scheme by the choices~tf~ 211 204made when matching it against some sequence ofgrammar symbols.
Our training data allows us toderive, for each training example, the choices madeat each rule expansion.
By applying these choices tothe rule scheme in isolation, we can derive a phrase-structure rule from it,.The grammar is specialized, or pruned, by retain-ing all and only those phrase-structure ules thatcorrespond to a path taken through a rule schemewhen expanding some node in some training exam-ple.
Since the grammar formalism requires that eachLHS occur only in one rule scheme in the gram-mar, extracted rules with the same LHS symbol aremerged into a single rule scheme with a disjunctionoperator at its top level.
For instance, if a rulescheme with the structureA ~ B*{CI D}is expanded in the training data only in the followingwaysA -> CA --+ BCA -+ BDthen it will be replaced by a rule scheme with thefollowing structureA --+ {C IBC\ ]BD}The same approach is taken to replace all regular-expression operators, other than concatenation, withthe actual sequences of grammar symbols that arematched against them.
A more realistic example,taken from the actual data, is shown in Figure 1:none of the optional alternative portions followingthe V is ever used in any correct parse in the corpus.Moreover, the ADVP preceding the V occurs only0 or 1 times in correct parses.Like other unification-based formalisms, lexicalfunctional grammars allow grammar ules to be an-notated with sets of feature-based constraints, herecalled "functional descriptions", whose purpose isboth to enforce additional constraints on rule appli-cability and to build an enriched predicate-argumentstructure called "f-structure", which, together withthe parse tree, constitutes the output of the parsingprocess.
As these constraints are maintained verba-tim in the specialized version of the rule scheme, thisposes no problem for this form of grammar pruning.3 Exper imenta l  SetupThe experiments carried out to determine the ef-fectiveness of corpus-based specialization were per-formed as illustrated in Figure 2.
Two broad-coverage LFG grammars were used, one for Frenchand one for English, both of which were developedwithin the Pargram project (Butt et al, 1999) dur-ing several years time.
The French grammar consistsof 133 rule schemata, the English grammar of 8.5 ruleschemata.Each gralmnar is equipped with a treebank, whichwas developed for other purposes than grammar spe-cialization.
Each treebank was produced by lettingthe system parse a corpus of technical documenta-tion.
Any sentence that did not obtain any parsewas discarded.
At this point, the French corpuswas reduced to 960 sentences, and the English cor-pus to 970.
The average sentence length was 9 forFrench and 8 for English.
For each sentence, a hu-man expert then selected the most appropriate anal-ysis among those returned by the parser.In the current experiments, each treebank wasused to specialize the grammar it had been devel-oped with.
A set of 10-fold cross-validation experi-ments was carried out to measure several interestingquantities under different conditions.
This meansthat, for each language, the corpus was randomlysplit into ten equal parts, and one tenth at a timewas held out for testing while the remaining ninetenths were used to specialize the grammar, and theresults were averaged over the ten runs.. For eachgrammar the average number of parses per sentence,the fraction of sentences which still received at leastone parse (angparse) and the fraction of sentences forwhich the parse selected by the expert was still de-rived (coverage) were measured 1.
The average CPUtime required by parsing was also measured, and thiswas used to compute the speedup with respect o theoriginal grammar.The thus established results constitute one datapoint in the trade-off between ambiguity reductionon one side, which is in turn related to parsing speed,and loss in coverage on the other.
In order to deter-mine other points of this trade-off, the same set.
ofexperiments was performed where speciMization wasinhibited for certain rule schemata.
In particular, foreach grammar, the two rule schemata that receivedthe largest number of distinct expansions in the cor-pora were determined.
These proved to be thoseassociated with the LHS symbols 'VPverb\[main\]'and 'NP'  for the French grammar, and 'VPv'  and'NPadj'  for the English one.
2 The experiments wererepeated while inhibiting specialization of first thescheme with the most expansions, and then the twomost expanded schemata.Measures of coverage and speedup are important1 As long  as we are  in teres ted  in preserv ing  the  f - s t ructureass igned to  sentences ,  th i s  not ion  of  coverage  is s t r i c te r  thannecessary .
The  same f - s t ructure  can  in fac t  be  ass igned bymore  than  one  parse ,  so that  in  some cases  a sentence  is con-s idered  out  of  coverage  ven if  the  spec ia l i zed  grammar  ass ignsto  it  the  cor rect  f - s t ruc ture .2 'VPv '  and  'VPverb \ [main \ ] '  cover  VPs  headed by  a mainverb .
'NPad j '  covers  NPs  w i th  ad jec t ives  a t tached.205The original rule:l/Pperfp --+ADVP*SE (t ADJUNCT)($ ADV_TYPE) = t,padv~/r{ @M_Head_Perfp I@M_Head_Passp }@( Anaph_Ctrl $){ AD VP+SE ('~ ADJUNCT)($ ADV_TYPE) = vpadvis replaced by the following:ADVP,\[.E (~ ADJUNCT)(.l.
ADV_TYPE)  = vpadvl/'Pperfp --+@PPadjunct @PPcase_obl{@M.Head_Pevfp \[@M..Head_Passp}@( Anaph_Ctrl ~ )V{ @M_Head_Perfp I@M_Head_Passp }@( Anaph_Ctrl ~)Figure 1: The pruning of a rule from the actual French grammar.
The "*" and the "+" signs have the usualinterpretation as in regular expressions.
A sub-expression enclosed in parenthesis optional.
Alternativesub-expressions are enclosed in curly brackets and separated by the "\[" sign.
An "@" followed by an identifieris a macro expansion operator, and is eventually replaced by further functional descriptions.Corpus--..,,0.1\[DisambiguatedTreebank treebankHumanexpertGrammarspecializationSpecializedgrammarFigure 2: The setting for our experiments on grammar specialization.indicators of what can be achieved with this form ofgrammar pruning.
However, they could potentiallybe misleading, since failure times for uncovered sen-tences might be considerably ower than their pars-ing times, had they not been out of coverage.
Ifthe pruned grammar fails more frequently on sen-tences which take longer to parse, the measuredspeedup might be artificiMly high.
This is easilyrealized, as simply removing the hardest sentencesfroln the corpus would cause a decrease ill the av-erage parsing time, and thus result in a speedup,without any pruning at all.
To factor out the contri-bution of uncovered sentences fi'om the results, theperformance of a two-stage architecture analogousto that of (Samuelsson and Rayner, 1991) was siln-ulated, in which the pruned grammar is attempted206"A Sentence"Parser with specializedgrammarFails1SucceedsL_Time = Timespecialize dParser with originalgrammarTime = Timespecialize d + Time originalFigure 3: A schematic representation of the simu-lated two-stage coverage-preserving architecture.first, and the sentence is passed on to the originalunpruned grammar whenever the pruned grammarfails to return a parse (see Figure 3).
The mea-sured speedup of this simulated architecture, whichpreserves the anyparse measure of the original gram-mar, takes into account the contribution of uncov-ered sentences, as it penalizes weeping difficult sen-tences under the carpet.4 Experimental ResultsThe results of the experiments described in the sec-tion above are summarized in the table in Figure 4.The upper part of the table refers to experimentswith the French grammar, the lower part to exper-iments with the English grammar.
For each lan-guage, the first line presents data gathered for theoriginal grammar for comparison with the prunedgrammars.
The figures in the second line were col-lected by pruning the grammar based on the wholecorpus, and then testing on the corpus itself.
Thegrammars obtained in this way contain 516 and 388disjuncts - -  corresponding to purely concatenativerules - -  for French and English respectively.
Any-parse and coverage are not, of course, relevant inthis case, but the statistics on parsing time are, es-pecially the one on the maximum parsing time.
Foreach iteration in the 10-fold cross-validation experi-ment, the maximum parsing time was retained, andthose ten times were eventually averaged.
If pruningtended to leave sentences which take long to parseuncovered, then we would observe a significant dif-ference between the average over ma.ximum times onthe grammar trained and tested on the same corpus(which parses all sentences, including the hardest),and the average over maximum times for grammarstrained and tested on different sets.
The fact thatthis does not seem to be the case indicates that prun-ing does not penalize difficult sentences.
Note alsothat the average number of parses per sentence issignificantly smaller than with the full grammar, ofalmost a factor of 9 in the case of the French gram-inar.The third line contains results for the fully prunedgrammar .
In the case of the French grammar  aspeedup of about 6 is obtained with a loss in cov-erage of 13%.
The smaller speedup gained withthe English grammar can be explained by the factthat here, the parsing times are lower in general,and that a non-negligible part of this time, espe-cially that needed for morphological nalysis, is un-affected by pruning.
Even in the case of the Englishgrammar, though, speedup is substantial (2.67).
Forboth grammars, the reduction in the average max-inmm parsing time is particularly good, confirmingour hypothesis that tr imming the grammar by re-moving heavy constructs makes it considerably moreefficient.
A partially negative note comes from theaverage number of disjuncts in the prun.ed grain-mars, which is 501 for French and 374 for English.Comparing this figures to the number of disjuncts ingrammars pruned on the full corpus (516 and 388),we find that after training on nine tenths of the cor-pus, adding the last tenth still leads to an increaseof 3-4% in the size of the resulting grammars.
Inother words, the marginal gain of further trainingexamples is still significant after considering about900 sentences, indicating that the training corporaare somewhat too small.The last two lines for each language show figuresfor grammars with pruning inhibited on the mostvariable and the two most variable symbols respec-tively.
For both languages, inhibiting pruning on themost variable symbol has the expected effect of in-creasing both parsing time and coverage.
Inhibitingpruning also on the second most variable symbol hasahnost no effect for French, and only a small effectfor English.The table in Figure 5 summarizes the measureson the simulated two-stage architecture.
For bothlanguages the best trade-off, once the distributionof uncovered sentences has been taken into account,is achieved by the fully pruned grammars.5 Re la ted  WorkThe work presented in the current article is relatedto previous work on corpus-based grammar spe-cialization as presented in (Rayner, 1988; Salnuels-son and Rayner, 1991; Rayner and Carter, 1996;Samuelsson, 1994; Srinivas a.nd Joshi, 1995; Neu-mann, 1997).207Parses/sentenceFrenchoriginal grammar 1941test = training 219Anyparse Coverage Avg.
time(secs.)Max.
time(secs.
)Speedup1.00 1.OO 1.52 78.5 11.00 1.00 0.28 5.62 5.430.91 0.25 First-order pruning 164 0.87 5.69 6.08no pruning on'VPverb\[main\]' 1000 0.94 0.91 0.42 8.70 3.62no pruning on'Vpverb\[main\]' and 'NP'First-order pruning0.94 0.92 0.42 8.42 1279 3.620.881.00 1.00 0.56 31.73 11.00 1.00 0.23 3.92 2.430.94 0.210.91Eng l i shoriginal grammar 58test = training 24210.96 0.320.35no pruning on'VPv'0.9625no pruning on'Vpv' and 'NPadj '  31 0.933.9211.0611.16Figure 4: The results of the experiments on LFG specialization.Avg.
CPU time (secs.)
SpeedupFrench0.570 2.670.616 2.470.614 2.48First-order pruningno pruning on 'VPverb\[main\]'no pruning on 'VPverb\[main\]' and 'NP'Eng l i shFirst-order pruning 0.311 1.81no pruning on 'VPv'  0.380 1.47no pruning on 'VPv'  and 'NPadj'  0.397 1.402.671.751.60Figure 5: Results for the simulated two-stage architecture.The line of work described in (Rayner, 1988;Samuelsson and Rayner, 1991; Rayner and Carter,1996; Samuelsson, 1994) deals with unification-based grammars that already have a purely-concatenative context-fi'ee backbone, and is moreconcerned with a different t~orm of specialization,consisting in the application of explanation-basedlearning (EBL).
Here, the central idea is to collectthe most frequently occurring subtrees in a treebankand use them as atomic units for parsing.
The citedworks differ mainly in the criteria adopted for select-ing subtrees fi'om the treebank.
In (Rayner, 1988;Samuelsson and Rayner, 1991; Rayner and Carter,1996) these criteria are handcoded: all subtrees at-isfying some properties are selected, and a new gram-mar rule is created by flattening each such subtree,i.e., by taking the root as lefl.-hand side and the yieldas right-hand side, and in the process performing allunifications corresponding to the thus removed in-ternal nodes.
Experiments carried out on a corpusof 15,000 trees from the ATIS domain using a ver-sion of the SRI Core Language Engine resulted in aspeedup of about 3.4 at a cost of 5% in gralmnati-cal coverage, which however was compensated by anincrease in parsing accuracy.Finding suitable tree-cutting criteria requires aconsiderable amount of work, and must be repeatedfor each new grammar and for each new domain towhich the grammar is to be specialized.
Samuelsson(Samuelsson, 1994) proposes a technique to auto-matically selects what subtrees to retain.
The se-lection of appropriate subtrees is done by choosinga subset of nodes at which to cut trees.
Cutnodesare determined by computing the entropy of eachnode, and selecting only those nodes whose entropyexceeds a given threshold.
Intuitively, nodes withlow entropy indicate locations in the trees where agiven symbol was expanded using a predictable setof rules, at least most of the times, so that the lossof coverage that derives from ignoring the remain-ing cases is low.
Nodes with high entropy, on theother hand, indicate positions in which there is ahigh uncertainty in what rule was used to expandthe symbol, so that it is better to preserve all alter-natives.
Several schemas are proposed to computeentropies, each leading to a different trade-off be-~fllRtween coverage reduction and speedup.
In general,results are not quite as good as those obtained usinghandcoded criteria, though of course the specializedgrammar is obtained fully automatically, and thuswith much less effort.When ignoring issues related to the elimination ofcomplex operators t"1"o111 the RHS of rule schemata,the grammar-pruning strategy described in the cur-rent article is equivalent to explanation-based l arn-ing where all nodes have been selected,as eutnodes.Conversely, EBL can be viewed as higher-ordergrammar pruning, removing not grammar ules, butgramlnar-rule combinations.Some of the work done on data-oriented parsing(DOP) (Bod, 1993; Bod and Scha, 1996; Bod andKaplan, 1998; Sima'an, 1999) can also be consideredrelated to our work, as it can be seen as a way tospecialize in an gBL-like way the (initially unknown)grammar implicitly underlying a treebank.
(Srinivas and aoshi, 1995) and (Neumann, 1997)apply EBL to speed up parsing with tree-adjoininggrammars and sentence generation with HPSGs re-spectively, though they do so by introducing newcomponents in their systems rather then by modify-ing the grammars they use.6 Conclus ionsSophisticated grammar formalisms are very usefuland convenient when designing high-coverage ram-mars for natural languages.
Very expressive gram-matical constructs can make the task of develop-ing and maintaining such a large resource consid-erably easier.
On the other hand, their use can re-sult in a considerable increase in grammatical am-biguity.
Gramnaar-compilation techniques based ongrammar structure alone are insufficient remedies inthose cases, as they cannot access the informationrequired to determine which alternatives to retainand which alternatives to discard.The current article demonstrates that a relativelysimple pruning technique, employing the kind of ref-erence corpus that is typically used for grammar de-velopment and thus often already available, can sig-nificantly improve parsing performance.
On largelexical functional grammars, speedups of up to afactor 6 were observed, at the price of a. reductionin grammatical coverage of about 13%.
A simpletwo-stage architecture was also proposed that pre-serves the anyparse measure of the original gram-mar, demonstrating that significant speedups can beobtained without increasing the number of parsingfailures.Future work includes extending the study ofcorpus-based grammar specialization from first-order grammar pruning to higher-order gram-mar pruning, thus extending previous work onexplanation-based learning for parsing, aad apply-ing it to the LFG fornaalism.ReferencesRens Bod and Ronald Kaplan.
1998.
A probabilisticcorpus-driven model for lexical-functional naly-sis.
In Proceedings of Coling-ACL-98, Montreal,Canada.R.
Bod and R. Scha.
1996.
Data-oriented lan-guage processing: An overview.
Technical report,ILLC, University of Amsterdam, Alnsterdam, TheNetherlands.Rens Bod.
1993.
Using an annotated corpus as astochastic grammar.
In Proceedings of EACL-93,Utrecht, The Netherlands.M.
Butt, T.H.
King, M.E.
Nifio, and F. Segond.1999.
A Grammar Writer's Cookbook.
CSLI Pub-lications, Stanford, CA.John Carrol.
1994.
Relating complexity to practicalperformance in parsing with wide-coverage uni-fication grammars.
In Proceedings of (ACL '94),Las Cruces, New Mexico, June.Ronald Kaplan and Joan Bresnan.
1982.
Lexical-functional grammar: A formal system for gram-matical representation.
In Joan Bresnan, editor,The Mental Representation f Grammatical Rela-tions, pages 173-281.
MIT Press.G/inter Neumann.
1997.
Applying explanation-based learning to control and speeding-up natu-ral language generation.
In Proceedings of A CL-EACL-97, Madrid, Spain.Manny Rayner and David Carter.
1996.
Fast pars-ing using pruning and grammar specialization.
InProceedings of the ACL-96, Santa.
Cruz, CA.Manny Rayner.
1988.
Applying explanation-basedgeneralization to natural-language processing.In Proceedings of the International Conferenceon Fifth Generation Computer Systems, Tokyo,Japan.Christer Samuelsson and Manny Rayner.
1991.Quantitative evaluation of explanation-basedlearning as an optimization tool for a large-scalenatural language system.
In Proceedings of theIJCAI-91, Sydney, Oz.Christer Samuelsson.
1994.
Grammar specializationthrough entropy thresholds.
In Proceedings of theACL-94, Las Cruces, New Mexico.
Available ascmp-lg/9405022.Khalil Sima'an.
1999.
Learning Efficient Dis-ambiguation.
Ph.D. thesis, Institute for Logic,Language and Computation, Amsterdam, TheNetherlands.B.
Srinivas and A. Joshi.
1995.
Some novel appli-cations of explanation-based learning to parsinglexicalized tree-adjoining ramlnars.
In Proceed-ings of the ACL-95, Cambridge, MA.209
