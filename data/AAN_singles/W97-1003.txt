A Preliminary Study of Word Clustering Based on SyntacticBehaviorWide R. Hogenhout and Yuji MatsumotoNara Institute of Science and Technology8916-5 Takayama-cho, Ikoma-shi, Nara 630-01, Japan{marc-h ,mat su}~is, aist-nara, ac.
jpAbstractWe show how a treebank can be used tocluster words on the basis of their syntac-tic behavior.
The resulting clusters repre-sent distinct types of behavior with muchmore precision than parts of speech.
As anexample we show how prepositions can beautomatical ly subdivided by their syntac-tic behavior and discuss the appropriate-ness of such a subdivision.
Applications ofthis work are also discussed.S worksverbNP ~$mithJohn Smith works fastproper  noun proper  noun verb  adverbFigure 1: Sentence with Parse Tree and Headwords1 IntroductionThe construction of classes of words, or calculationof distances between words, has frequently drawnthe interest of researchers in natural language pro-cessing.
Many of these studies aimed at findingclasses based on co-occurrences, often combined withthe aim of establishing semantic similarity betweenwords (McMahon and Smith, 1996, Brown et el.,1992, Dagan, Markus, and Markovitch, 1993, Da-gan, Pereira, and Lee, 1994, Pereira and Tishby,1992, Grefenstette, 1992).We suggest a method for clustering words purelyon the basis of syntactic behavior.
We show howthe necessary data for such clustering can easily bedrawn from a publicly available treebank, and howdistinct types of behavior can be discovered.
Al-though a part of speech tag set can be thought ofas a classification based on syntactic behavior, wecan construct an arbitrary number of clusters, or abinary tree of words that share their part of speech.We discuss in detail a binary word tree for prepo-sitions that was created by syntactic-behavior basedclustering, to show what sort of properties are re-vealed by the clustering and what one can learn fromthis about language.
We also discuss various waysin which this kind of clustering can be used in NLPapplications.2 Headwords and DependenciesThe data we extract are based on the concept ofheadwords.
Such headwords are chosen for everyconstituent in the parse tree by means of a simpleset of rules.
These have been used in various studiesin this field, see (Collins, 1996, Magerman, 1995, Je-linek et el., 1994).
Every headword is propagatedup through the tree such that every parent receivesa headword from the head-child.
Figure 1 gives anexample of a parse tree with headwords.Following the techniques uggested by (Collins,1996), a parse tree can subsequently be described asa set of dependencies.
Every word except the head-word of the sentence depends on the lowest head-word it is covered by.
The syntactic relation is thengiven by the triple of nonterminals: the modifyingnonterminal, the modified nonterminal, and the non-terminal that covers the joint phrase.
Table 1 givesan example of such a description.On one point our method is different from themethod suggested by Collins.
Collins uses a reducedsentence in which every basic noun phrase (i.e., anoun phrase that has no noun phrase as a child)is reduced to its headword.
The reason for this isthat it improves co-occurrence counts and adjacencystatistics.
We however do not reduce the sentenceHogenhout ~ Matsumoto 16 Word Clustering from Syntactic BehaviorWide R. Hogenhout and Yuji Matsumoto (1997) Word Cluster ing from Syntact ic  Behavior.(ed.)
CoNLL97: Computational Natural Language Learning, ACL pp 16-24.
(~) 1997 Association for Computational LinguisticsIn T.M.
EUisonsince we do.
not need to consider adjacency statis-tics or unresolved ambiguities, and therefore neverface the problem that a word in a basic noun phrase,that is not the headword, is adjacent o or modifiessomething outside of the basic noun phrase.Table 1 gives the relations for one sentence, butinstead of considering one sentence we collect suchpatterns for the whole corpus and study statisticsfor individual words.
In this way it can be dis-covered that, for example, a particular verb is of-ten used transitively, or that a particular preposi-tion is mostly used to produce locative prepositionalphrases.
Words can be distinct or similar in this re-spect, but note that this is not related to semanticsimilarity.
Words such as eat and drink have a se-mantic similarity, but may be completely differentin their syntactic behavior, whereas tend and appeardo not have an obvious semantic relation, but theydo have a similarity since they can both be used asraising verbs, as will be exemplified later.Throughout this paper we will use the term"word" to refer to words for which the part of speechhas already been disambiguated.
In tables and fig-ures we emphasize this by indicating the part ofspeech together with the word.Table 1: Dependencies for the sentence John Smithworks fastdependent word head word relationJohn Smith NP -(proper noun) (proper noun)Smith works NP S VP(proper noun)fast(adverb)(verb)works VP -(verb)3 Collecting Statistics for IndividualWordsThe next step we take, is eliminating one of the twowords in this table of dependencies.
Consider ta-bles 2 and 3.
These show we can take three "ob-servations" from the sentence by eliminating eitherthe headword or the dependent word.
If headwordsare eliminated we obtain three observations, for thewords John, Smith and fast.
If dependent words areeliminated we also obtain three observations, two forworks and one for Smith.By collecting the observations over the entire cor-pus we can see to/by what sort of words and withwhat kind of relations aword modifies or is modified.We consider the following distributions:p(R, talwdt~) (1)p(R, tdlWhth) (2)where R indicates the triple representing the syn-tactic relation, Wd a dependent word that modifiesheadword Wh, and td and th their respective partof speech tags.
For example, in the second line oftable 3, which corresponds to distribution 1, R is(NP,S,VP), th is "verb", wd is "Smith" and td is"proper noun".Statistics of the distributions (1) and (2) can eas-ily be taken from a treebank.
We took such datafrom the Wall Street Journal Treebank, calculatingthe probabilities with the Maximum Likelihood Es-timator:f(R, th, Wdtd)p(R, th\]Wdtd) = ER',t' f(R',t ' ,  Wdtd)where f stands for frequency.
Note that we only ex-tract the dependency relations, and ignore the struc-ture of the sentence beyond these relations.
Thisshows the equation for distribution (1), distribution(2) is calculated likewise.Compare the dependency behavior of the propernouns Nippon and Rep. in table 4.
The word Nipponis Japanese for Japan, and mainly occurs in namesof companies.
The word Rep. is the abbreviationof Republic, and obviously occurs mainly in namesof countries.
As can be seen, the word Rep. oc-curs far more frequently, but the distributions arehighly similar.
Both always modify another propernoun, about 33% of the time forming an NP-SBJ and67% of the time an NP.
Both are a particular kindof proper noun that almost always modifies otherproper nouns and almost never appears by itself.It also became clear that the noun company is verydifferent from a noun such as hostage, since companyoften is the subject of a verb, while hostage is rarelyin the subject position.
Both are also very differentfrom the noun year, which is frequently used as theobject of a preposition.The gerund including has an extremely strong ten-dency to produce prepositional phrases, as in "Safetyadvocates, including some members of Congress,...",making it different from most other gerunds.
A pasttense such as fell has an unusual high frequency asthe head of a sentence rather than a verb phrase,which is probably a peculiarity of the Wall StreetJournal ( "Stock prices fell... ").Our observation is that among words which havethe same part of speech, some word groups exhibitbehavior that is extremely similar, while others dis-play large differences.
The method we suggest aimsHogenhout E4 Matsumoto 17 Word Clustering from Syntactic Behaviorat making a clustering based on such behavior.
Byusing this technique any number of clusters can beobtained, sometimes far beyond what humans canbe expected to recognize as distinct categories.Table 2: Dependencies with dependent words elimi-nated.dependent word head word relation* Smith NP -(proper noun) (proper noun)* works NP S VP(proper noun) (verb)* works !
- VP(adverb) (verb)Table 3: Dependencies with headwords eliminated.dependent word head word relationJohn * NP -(proper noun) (proper noun)Smith * NP S VP(proper noun) (verb)fast * VP -(adverb) (verb)Table 4: Distribution of dependencies of the wordsNippon and Rep., as proper nouns.dep.
word headword tagNipponproper n.Rep.proper n.proper nounproper nounproper nounrelationNP-SBJNP- NP-SBJproper noun NPfreq.3623454 Compar i son  w i th  Co-Occur renceBased  C lus ter ingClustering of words based on syntactic behavior hasto our knowledge not been carried out before, butclustering has been applied with the goal of obtain-ing classes based on co-occurrences.
Such clusterswere used in particular for interpolated n-gram lan-guage models.By looking at co-occurrences it is possible to findgroups of words such as \[director, chief, professor,commissioner, commander, superintendent\].
Themost prominent method for discovering their simi-larity is by finding words that tend to co-occur withthese words.
In this case they may for example co-occur with words such as decide and lecture.The group of verbs \[tend, plan, continue, want,need, seem, appear\] also share a similarity, but onehas to look at structures rather than meaning or co-occurrences to see why.
All these verbs tend to occurin the same kind of structures, as can be seen in thefollowing examples from the Wall Street Journal.The funds '  share  pr i ces  tend  toswing more than  the  broader  market.Investors continue to pour cashinto money funds.Cray Research did not want to funda project that did not includeSeymour.No one has worked out the players'average age, but most appear to bein their late 30s.What these verbs share is the property that theyoften modify an entire clause (marked as 'S' in theWall Street Journal Treebank) rather than nounphrases or prepositional phrases, usually forming asubject raising construction.
This is only a tendency,since all of them can be used in a different way aswell, but the tendency is strong enough to make theirusage quite similar.
Co-occurrence based clusteringignores the structure in which the word occurs, andwould therefore not be the right method to find re-lated similarities.As mentioned, co-occurrence based clusteringmethods often also aim at producing semanticallymeaningful clusters.
Various methods are based onMutual Information between classes, see (Brown etal., 1992, McMahon and Smith, 1996, Kneser andNey, 1993, Jardino and Adda, 1993, Martin, Lier-mann, and Ney, 1995, Ueberla, 1995).
This mea-sure cannot be applied in our case since we look atstructure and ignore other words, and consequentlyalgorithms using that measure cannot be applied tothe problem we deal with.The mentioned studies use word-clusters for in-terpolated n-gram language models.
Another ap-plication of hard clustering methods (in particularbottom-up variants) is that they can also producea binary tree, which can be used for decision-treebased systems uch as the SPATTER parser (Mager-man, 1995) or the ATR Decision-Tree Part-Of-Speech Tagger (Black et al, 1992, Ushioda, 1996).Hogenhout ~ Matsumoto 18 Word Clustering from Syntactic BehaviorIn this case a decision tree contains binary questionsto decide the properties of a word.We present a hard clustering algorithm, in thesense that every word belongs to exactly one cluster(or is one leaf in the binary word-tree of a particularpart of speech).
Besides hard algorithms there havealso been studies to soft clustering (Pereira, Tishby,and Lee, 1993, Dagan, Pereira, and Lee, 1994) wherethe distribution of every word is smoothed with thenearest k words rather than placed in a class whichsupposedly has a uniform behavior.
In fact, in (Da-gan, Markus, and Markovitch, 1993) it was arguedthat reduction to a relatively small number of pre-determined word classes or clusters may lead to sub-stantial loss of information.
On the other hand,when using soft clusteringit is not possible to give ayes/no answer about class membership, and binaryword trees cannot be constructed.5 Similarity Measure and AlgorithmThe choice of the clustering algorithm is to some ex-tent independent from the way data is collected, butas mentioned clustering is carried out on the basisof distributional similarity, and methods using Mu-tual Information are not applicable.
The algorithmwe present here is meant to demonstrate how syn-tactic behavior can be used for clustering.
However,we feel the optimal choice for the clustering methoddepends on the application it will be used for.Studies in distribution based clustering often usethe KuUback-Leibler (KL) distance, see for example(Pereira, Tishby, and Lee, 1993, Dagan, Pereira, andLee, 1994).
However, this distance is not symmetri-cal, and since we are (for the time being) interestedin hard clustering it is desirable to have a symmet-rical measure.
We could possibly use Jeffery's Infor-mation, i.e.
the sum of the KL-distances:D(p,q) = ~p(x) log~p(x)~ += \q(x ) \ ]q(x) loglq(x)~ (3) "We have tried this distance measure, but in manycases we have found it to have undesirable ffects,primarily because the goal of our algorithm is join-ing words (and their statistics) together to makeone cluster, and a distorted image results from thismeasure when words have different total frequen-cies.
Furthermore, Jeffery's Information is undefinedif either distribution has a value of 0 and the othernot.
For this reason they would have to be smoothedwith, for example, a part of speech based distribu-tion, such asp(R, thlWdtd) ---- A(R, thlWdtd) +(1 - A)(R, th\[td), (4)but we wanted to avoid using an unlexical distri-bution since we believe lexical information is morevaluable.Instead we suggest a different measure.
Assumethere are a number of patterns i = 1...n, and ob-served frequencies al...an for word wata, and bl...bnfor word Wbt b.
Also, let A = ~i  ai and B = ~i  bi.The Maximum-Likelihood estimates for Wa are thuscalculated as pa(x) = a~/A and likewise for Wb.We now define the distance between words asai (~(A+B)~ i(woto, btb) doj  logi (a i7  ~)  +b' log (~ (A + B) ~which can be interpreted as the sum of KL-distancesbetween a hypothetical word that would be createdif the observations ofthe words Wata and Wbtb wouldbe joined together, and Wata and Wbtb respectively.Like Jeffery's Information, this measure is symmet-rical, although not a true distance since it does notobey the triangle inequality.This measure is more appropriate for two reasons.First, this distribution is better tailored toward mak-ing clusters where observations will be joined to-gether.
Second, we take this sum to be zero forvalues of i when ai = bi = 0 (no observations foreither word), therefore pre-smoothing is not neces-sary.The equation can easily be transformed into theformM(w~t~,wbtb) = l og ( - -~-~)+log( - -~- )+~ -~log (a~b~)  + ~log  (a~b~)  (5)which makes calculation significantly faster sincepatterns for which only one word has a non-zerofrequency do not need to be calculated within thesummation, as they always becomes zero.The Algor i thm The algorithm initially regardsevery word as a 1-element cluster, and works bot-tom up towards a set of clusters.
The strategy of agreedy algorithm is followed, every time finding thetwo clusters that have the least distance betweenthem and merging them until the desired number ofclusters is reached.
However, only words with theHogenhout ~ Matsumoto 19 Word Clustering from Syntactic Behaviorsame part of speech may be merged, so distancesbetween words that have different parts of speechare never calculated.
Words can therefore receive a'combined tag' consisting of their part of speech tag,and a syntactic behavior tag.
This is similar to whatMcMahon (1996) refers to as a structural tag.The algorithm is actually applied twice, once toclustering for dependent-context (1) and once toclustering for head-context (2).An obvious problem with this sort of clustering islow frequency words.
For many words only a one or afew observations are available, which may give someinformation about what Sort of word it is, but whichdoes not give a reliable estimate of the distributions.We will mention a solution to this problem later.
Inthe example we present only words for which at least25 observations are available.One problem with co-occurrence based clusteringthat has been pointed out in the past is that ofalmost-linear dendrograms, caused by the propertiesof Mutual Information.
We have not encounteredthis problem with the described algorithm.6 Resu l t :  the  Case  o f  P repos i t ionsWe present a binary word tree that was producedby the algorithm described in the previous section.The main goal of this is to show what sort of prop-erties are revealed by this clustering, and what kindof words are problematic.
Even in situations wherewords are clustered by syntactic behavior withoutmaking a binary tree, it can be useful to study thetype of properties that decide syntactic behavior.Please refer to figure 2 for an example of the re-sults obtained with clustering.
This is a dendro-gram that reflects the clustering process from loosewords until the point were they are all merged intoone cluster.
The dendrogram shows the result forprepositions, although only those prepositions wereconsidered for which at least 25 observations wereavailable.
In the division of words over the partsof speech we follow the tagging scheme of the WallStreet Journal Treebank, and for example subordi-nators such as while, if and because are included inthe prepositions.
Of course it is possible to use amore fine grained tag set, when available.
On theother hand, as will be shown later, the algorithmdoes decide to classify most subordinators into onecluster.offtoward --"l..J""'-Ibeyond ~ \[within ~ \[throughout ~ \[near I !behindwithout ~..through Ilike ~ ~ \[around--------- I \]outs ide - - -~J-~ lacross ~ I \]under, Iong lbelow Jabove !
in ~ on atagainst ---J \]!i=' , Ibysinceduring \]until----------~beforeafterForWithLike ~ \]With?ut---~J IIOn , , ' i t - -~InAt ~ UnderAmong \]desp i te  ~unlikeOfAsOverSinceDuringUntil,BeforeAfterperamidexceptalonguponFromAbout,whetherthat l \]down !nextago SOso tbecauseBecausethough | Though I\[whi le -m-  |i funlessIf ~ ~While --------------i F-althoughAlthough iI FEDi,\],GCBAFigure 2: Clustering Result for Dependency Behav-ior of PrepositionsHogenhout ~ Matsumoto 20 Word Clustering from Syntactic BehaviorWe will discuss the major distinctions made bythe algorithm.
At first it may not be clear whywords should be divided in this way, but inspectionof the data from the corpus shows that many of thesechoices are very natural.
We also discuss in whichcases the dendrogram does not form natural cate-gories.The first partition, marked A, is a quite natu-ral division.
The upper branch (from off throughAbout) are prepositions that usually cover somephrase themselves, whereas the prepositions in thelower branch usually do not cover any phrase.The preposition whether occurs, for example, instructures uch as'' We have no useful informationon (SBAR whether (S users are atr isk)),  ~ ~ said James A. Talcottof Boston's Dana-Farber CancerInstitute.where in our headword-selection scheme whether de-pends on the headword are.
(Even if this is changed,they still become one cluster because of the typicalpatterns with S and SBAR.
)For comparison, the preposition below usually oc-curs in structures uch asMagna recent ly  cut  i t s  quar ter lyd iv idend in  ha l f  and the  company'sC lass  A shares  are (VP wa l low ing(PP-LOC fa r  be low the i r  52-weekhigh of 16.125 Canadian do l la rs(US$ 13 .73) ) ) .where it is the headword of a prepositional phrasebefore it modifies the verb.The partition marked with B is not a natural di-vision; it rather separates a set of prepositions thatdo not fit in elsewhere.
The prepositions from perthrough About are not similar to each other or toother prepositions in their behavior.Partition C again resembles to groups that canbe characterized easily.
The prepositions by throughAfter, the lower branch of C, depended almost exclu-sively on verbs.
The prepositions from off throughabout, the upper branch of C, depend on more var-ied headwords.
Most of these frequently depend onboth nouns and verbs.
The following example showsaround depending on a noun, although around alsotends to depend on cardinal numbers.You now may drop by the Voice ofAmerica offices in Washington andread the text of what the Voice isbroadcasting to those 130 mi l l ionpeople (PP-LOC around the world)who tune in to it each week.An example for the lower branch of C isA plan to br ing the stock to marketbefore year end apparently (VP wasupset (PP by the recent weakness ofFrankfurt  share prices) ).The prepositions at the upper branch of partitionD tend to form a higher amount of PP-TMP typephrases, as inAnd in  each case ,  he says ,  a sharpdrop in  s tock  pr i ces  (VP began(PP-TMP w i th in  a year ) ) .although, while this is strongly the case for theprepositions within and throughout, it is not the casefor behind.At partition E prepositions with a preference forverbs are at the upper branch.
Prepositions thatalmost exclusively deal with verbs were separatedat C, but here the distinction is less absolute.
Theprepositions at the upper branch of E have a chanceof about two thirds to depend on a verb, whereasthis is only one third at the lower branch.Partition F is once again a very clear, natural di-vision.
The prepositions in, on and at have a strongtendency to form phrases of the type PP-LOC as inMr.
Nixon is  t rave l ing  (PP-LOCin China) as a pr ivate  c i t i zen ,but he has made c lear  that  he i san unof f i c ia l  envoy fo r  the  Bushadmin is t ra t  ion.while the prepositions at the lower branch, ofthrough about have much lower frequencies for theselocative phrases.The division at G is also very clear when the dataare inspected.
The upper branch reflects preposi-tions for which the covering phrase (the middle partof the triple representing the grammatical relation)is mostly VP or NP.
The prepositions For throughAfter at the lower branch of G are mainly coveredby phrases of type S. A preposition such as duringis found in structures uch asFu j i t su  sa id  i t  (VP b id  theequ iva lent  of  less  than  a U.S.penny on three  separate  mun ic ipa lcont rac ts  (PP-TMP dur ing the  pasttwo years ) ) .while a preposition such as without is usually foundin the PP-S-VP pattern:Hogenhout ~ Matsumoto 21 Word Clustering from Syntactic Behavior(S In fact, (PP without WallStreet firms trading for theirown accounts), the stock-indexarbitrage trading opportunities forthe big funds (VP may be all themore abundant).
)At H this is further divided in words that tendmore to depend on loose words, PP type phrases(such as without in the last example) or S typephrases, at the lower branch, and those that usu-ally depend on heads of a VP.As for the division at point I, the prepositions nextthrough Although share the property that their cov-ering phrase (the middle part of the triple represent-ing the grammatical relation) is often of the typeSBAR-ADV or SBAR-PRP.
The prepositions at theupper branch, whether through down, mainly sharenot having this property.While the status of the upper branch of J is some-what unclear, the lower branch of J is a perfectlyclear and intuitive group.
All of the words fromthough through Although appear almost exclusivelyin the patterns (-,SBAR-ADV,S), (-,S,S), (-,SBAR-PRP,S) and (-,SBAR-PRP,-).
An example isThe group says standardizedachievement test scores are greatlyinflated because teachers often'Cteach the test'' as Mrs. Yeargindid, (SBAR-ADV although (S most arenever caught)) .where in our headword scheme are becomes theheadword of the SBAR-ADV type phrase.Concluding, many of the divisions made by thealgorithm are quite natural.
There are some partsof speech (such as nouns and verbs) were a muchlarger number of words is included in the hierarchy,while some other parts of speech, for example per-sonal pronouns, produce very small hierarchies.
Ingeneral the hierarchy is more interesting for parts ofspeech that are used in a varied way, and less inter-esting for, for example, symbols uch as the percent-age sign, that are used in a monotone way.It is interesting to see that capitalization turns outto be a meaningful predictor about the way a wordwill be used for some words, but not for others.
Theword pair so and So, and the pair because and Be-cause are clustered next to each other, which indi-cates that they modify the same kind of structures,independent of whether they are at the beginningof the sentence.
The word pair under and Under,and the pair after and After on the other hand arerather far apart, indicating that their usage changessubstantially when they become the first word of thesentence.7 ApplicationsA first application of this work, of which we carriedout a first step in this article, is the lexicographi-cal one of studying word behavior.
Some proper-ties of words, such as the peculiar behavior of thegerund including or the similarities between prepo-sitions such as though and while only becomes clearonce the corpus data is analyzed in the way we de-scribed.
When inspecting manually, the binary wordtree representation appears to be the most easy tounderstand.A second application of the binary word tree canbe found in decision-tree based systems uch as theSPATTER parser (Magerman, 1995) or the ATRDecision-Tree Part-Of-Speech Tagger, as describedby Ushioda (Ushioda, 1996).
In this case it is nec-essary to use a hard-clustering method, such that abinary word tree can be constructed by the cluster-ing process, as we did in the example in the previoussections.A decision tree classifies data according to itsproperties by asking successive (often binary) ques-tions.
In the case of a part of speech tagger or aparsing system, it is particularly important for thesystem to ask lexicalizing questions.
However, ques-tions about individual words such as "Is this theword display?"
are not efficient on a large scale sinceit would easily require thousands of questions.
A bi-nary tree allows one to separate the vocabulary intotwo parts at every question, which is efficient whenthese two parts are maximally different.
In that caseit is possible to obtain as much information as pos-sible with a small number of questions.
A conditionfor this application is that trees may not be veryunbalanced, as the extreme case of a linear tree be-comes equal to asking word-by-word.
As mentioned,the method we suggest did not produce a very unbal-anced tree for the parts of speech in the Wall StreetJournal Treebank.A third application can be found in InformationRetrieval.
This can be seen from the example ofincluding: words with such behavior have little con-tent because they have a rather functional role in thesentence.
This can be seen in the sentence "Safetyadvocates, including some members of Congress,..."where terms such as Safety advocates or membersof Congress indicate much more about the topic ofthe sentence than the relatively empty word includ-ing.
It is possible to cluster words and decide whichclusters are likely to indicate the topic, and whichare not likely to do so.
For this application a widerHogenhout 84 Matsumoto 22 Word Clustering from Syntactic Behaviorvariety of algorithms can be applied; words can forexample be exchanged or shuffled between classes toimprove the entire model.A fourth application is class-based smoothing ofinterpolated n-gram models.
The co-occurrencebased classes described in the literature are, ofcourse, created with this as objective function, buton the other hand the classes we suggest clearly con-tain information that is inaccessible to co-occurrencebased classes.
It is possible that a combination ofco-occurrence based classes and classes of syntacticbehavior would give better results, but this wouldhave to be demonstrated experimentally.In some of these applications words with a low fre-quency cannot be ignored because of their quantity,but at the same time the algorithm cannot rely tooheavily on their observations.
A possible solutionis to carry out clustering without these words, anddistribute the low-frequency words over the leaves ofthe tree afterwards.
A solution along this line waschosen for co-occurrence based clustering in (McMa-hon and Smith, 1996), where a first algorithm han-dles more frequent words, and a second algorithmadds the low-frequency words afterwards.8 ConclusionWe have presented a method which constructsclasses of words with similar syntactic behavior, orbinary trees that reflect word similarity, by cluster-ing words using treebank data.
In this way it is pos-sible to discover particular types of behavior, such asthe peculiar behavior of the gerund including, verbsthat modify an entire clause (raising verbs), nounsthat prefer either subject position or object position,or prepositions that prefer locative phrases.Most of the classes found in this way would notbe found if clustering were performed on the basisof co-occurrences, ashas been described in the liter-ature.
For example, the verbs \[tend, plan, continue,want, need, seem, appear\] share a particular sentencestructure rather than, say, the sort of noun that be-comes the object.As became clear from the case study of preposi-tions, the clustering process reveals similarities inthe syntactic structure in which words appear whichin some cases can be clearly felt by intuition.
Forexample, the words in, on and at often are the headof locative prepositional phrases, and a prepositionsuch as within usually is the head of a temporalprepositional phrase.
Using this method these in-tuitions can be quantified.One of the applications we described is that ofa decision-tree based system for syntactic analysis.We are currently applying the method in this appli-cation, which will be described in later publications.9 AcknowledgementsWe would like to express our appreciation to theanonymous reviewers who have provided many valu-able comments and criticisms.ReferencesBlack, E., F. Jelinek, R. Mercer, and S. Roukos.1992.
Decision tree models applied to the la-beling of text with parts-of-speech.
In Pro-ceedings DARPA Speech and Natural LanguageWorkshop, pages 117-121.Brown, P. F., S. A. Della Pietra, P. V. deSouza,J.
C. Lai, and R. L. Mercer.
1992.
Class-basedn-gram models of natural anguage.
Computa-tional Linguistics, 18(4):467-479.Collins, M. J.
1996.
A new statistical parser basedon bigram lexical dependencies.
In Proceedingsof the 3~th Annual Meeting of the Associationfor Computational Linguistics, pages 184-191.Dagan, I., S. Markus, and S. Markovitch.
1993.
Con-textual similarity and estimation from sparsedata.
In Proceedings of the 31st Annual Meet-ing of the Association for Computational Lin-guistics, pages 164-171.Dagan, I., F. Pereira, and L. Lee.
1994.
Similarity-based estimation of word cooccurrence prob-abilities.
In Proceedings of the 32nd AnnualMeeting of the Association for ComputationalLinguistics, pages 272-278.Grefenstette, G. 1992.
Finding semantic similarityin raw text: the Deese antonyms.
In WorkingNotes, Fall Symposium Series, AAAI, pages61-68.Jardino, M. and G. Adda.
1993.
Automatic wordclassification using simulated annealing.
InICASSP 93, pages II 41-44.Jelinek, F., J. Lafferty, D. Magerman, R. Mercer,A.
Ratnaparkhi, and S~ Roukos.
1994.
Decisiontree parsing using a hidden derivation model.In ARPA: Proceedings of the Human LanguageTechnology Workshop, pages 272-277.Kneser, R. and H. Ney.
1993.
Improved clusteringtechniques for class-based statistical languagemodelling.
In Proceedings of European Confer-ence on Speech Communication and Technol-ogy, pages 973-976.Hogenhout ~ Matsumoto 23 Word Clustering from Syntactic BehaviorMagerman, D. M. 1995.
Statistical decision-treemodels for parsing.
In Proceedings of the 33dAnnual Meeting of the Association for Com-putational Linguistics, pages 276-283.Martin, M., J. Liermann, and H. Ney.
1995.
Al-gorithms for bigram and trigram word cluster-ing.
In Proceedings of European Conference onSpeech Communication and Technology, pages1253-1256.McMahon, John G. and Francis J. Smith.1996.
Improving statistical anguage modelperformance with automatically generatedword hierarchies.
Computational Linguistics,22(2):217-247.Pereira, F. and N. Tishby.
1992.
Distributional sim-ilarity, phase transitions and hierarchical clus-tering.
In Working Notes, Fall Symposium Se-ries, AAAI, pages 108-112.Pereira, F., N. Tishby, and L. Lee.
1993.
Distri-butional clustering of English words.
In Pro-ceedings of the 31st Annual Meeting of the As-sociation for Computational Linguistics, pages183-190.Ueberla, J.
1995.
More efficient clustering of n-gramsfor statistical language modeling.
In Proceed-ings of European Conference on Speech Com-munication and Technology, pages 1257-1260.Ushioda, Akira.
1996.
Hierarchical clustering ofwords and application to nlp tasks.
In Proceed-ings of the Fourth Workshop on Very LargeCorpora.Hogenhout ~4 Matsumoto 24 Word Clustering from Syntactic Behavior
