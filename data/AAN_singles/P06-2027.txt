Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 207?214,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomatic Creation of Domain TemplatesElena Filatova*, Vasileios Hatzivassiloglou?
and Kathleen McKeown**Department of Computer ScienceColumbia University{filatova,kathy}@cs.columbia.edu?Department of Computer ScienceThe University of Texas at Dallasvh@hlt.utdallas.eduAbstractRecently, many Natural Language Processing(NLP) applications have improved the quality oftheir output by using various machine learning tech-niques to mine Information Extraction (IE) patternsfor capturing information from the input text.
Cur-rently, to mine IE patterns one should know in ad-vance the type of the information that should becaptured by these patterns.
In this work we pro-pose a novel methodology for corpus analysis basedon cross-examination of several document collec-tions representing different instances of the samedomain.
We show that this methodology can beused for automatic domain template creation.
As theproblem of automatic domain template creation israther new, there is no well-defined procedure forthe evaluation of the domain template quality.
Thus,we propose a methodology for identifying what in-formation should be present in the template.
Usingthis information we evaluate the automatically cre-ated domain templates through the text snippets re-trieved according to the created templates.1 IntroductionOpen-ended question-answering (QA) systemstypically produce a response containing a vari-ety of specific facts proscribed by the questiontype.
A biography, for example, might contain thedate of birth, occupation, or nationality of the per-son in question (Duboue and McKeown, 2003;Zhou et al, 2004; Weischedel et al, 2004; Fila-tova and Prager, 2005).
A definition may containthe genus of the term and characteristic attributes(Blair-Goldensohn et al, 2004).
A response to aquestion about a terrorist attack might include theevent, victims, perpetrator and date as the tem-plates designed for the Message UnderstandingConferences (Radev and McKeown, 1998; Whiteet al, 2001) predicted.
Furthermore, the type of in-formation included varies depending on context.
Abiography of an actor would include movie names,while a biography of an inventor would include thenames of inventions.
A description of a terroristevent in Latin America in the eighties is differentfrom the description of today?s terrorist events.How does one determine what facts are im-portant for different kinds of responses?
Oftenthe types of facts that are important are hand en-coded ahead of time by a human expert (e.g., asin the case of MUC templates).
In this paper, wepresent an approach that allows a system to learnthe types of facts that are appropriate for a par-ticular response.
We focus on acquiring fact-typesfor events, automatically producing a template thatcan guide the creation of responses to questionsrequiring a description of an event.
The templatecan be tailored to a specific time period or coun-try simply by changing the document collectionsfrom which learning takes place.In this work, a domain is a set of events of a par-ticular type; earthquakes and presidential electionsare two such domains.
Domains can be instanti-ated by several instances of events of that type(e.g., the earthquake in Japan in October 2004, theearthquake in Afghanistan in March 2002, etc.
).1The granularity of domains and instances can bealtered by examining data at different levels of de-tail, and domains can be hierarchically structured.An ideal template is a set of attribute-value pairs,with the attributes specifying particular functionalroles important for the domain events.In this paper we present a method of domain-independent on-the-fly template creation.
Ourmethod is completely automatic.
As input it re-quires several document collections describing do-main instances.
We cross-examine the input in-stances, we identify verbs important for the major-ity of instances and relationships containing theseverbs.
We generalize across multiple domain in-stances to automatically determine which of theserelations should be used in the template.
We re-port on data collection efforts and results from fourdomains.
We assess how well the automaticallyproduced templates satisfy users?
needs, as man-ifested by questions collected for these domains.1Unfortunately, NLP terminology is not standardizedacross different tasks.
Two NLP tasks most close to ourresearch are Topic Detection and Tracking (TDT) (Fiscuset al, 1999) and Information Extraction (IE) (Marsh andPerzanowski, 1997).
In TDT terminology, our domains aretopics and our instances are events.
In IE terminology, ourdomains are scenarios and our domain templates are scenariotemplates.2072 Related WorkOur system automatically generates a templatethat captures the generally most important infor-mation for a particular domain and is reusableacross multiple instances of that domain.
Decid-ing what slots to include in the template, and whatrestrictions to place on their potential fillers, isa knowledge representation problem (Hobbs andIsrael, 1994).
Templates were used in the mainIE competitions, the Message Understanding Con-ferences (Hobbs and Israel, 1994; Onyshkevych,1994; Marsh and Perzanowski, 1997).
One of therecent evaluations, ACE,2 uses pre-defined framesconnecting event types (e.g., arrest, release) to aset of attributes.
The template construction taskwas not addressed by the participating systems.The domain templates were created manually byexperts to capture the structure of the facts sought.Although templates have been extensively usedin information extraction, there has been littlework on their automatic design.
In the Concep-tual Case Frame Acquisition project (Riloff andSchmelzenbach, 1998), extraction patterns, a do-main semantic lexicon, and a list of conceptualroles and associated semantic categories for thedomain are used to produce multiple-slot caseframes with selectional restrictions.
The systemrequires two sets of documents: those relevant tothe domain and those irrelevant.
Our approachdoes not require any domain-specific knowledgeand uses only corpus-based statistics.The GISTexter summarization sys-tem (Harabagiu and Maiorano, 2002) usedstatistics over an arbitrary document collectiontogether with semantic relations from WordNet.The created templates heavily depend on the top-ical relations encoded in WordNet.
The templatemodels an input collection of documents.
If thereis only one domain instance described in the inputthan the template is created for this particularinstance rather than for a domain.
In our work,we learn domain templates by cross-examiningseveral collections of documents on the sametopic, aiming for a general domain template.
Werely on relations cross-mentioned in differentinstances of the domain to automatically prioritizeroles and relationships for selection.Topic Themes (Harabagiu and La?ca?tus?u, 2005)used for multi-document summarization mergevarious arguments corresponding to the same se-2http://www.nist.gov/speech/tests/ace/index.htmmantic roles for the semantically identical verbphrases (e.g., arrests and placed under arrest).Atomic events also model an input documentcollection (Filatova and Hatzivassiloglou, 2003)and are created according to the statistics col-lected for co-occurrences of named entity pairslinked through actions.
GISTexter, atomic events,and Topic Themes were used for modeling a col-lection of documents rather than a domain.In other closely related work, Sudo et al (2003)use frequent dependency subtrees as measured byTF*IDF to identify named entities and IE patternsimportant for a given domain.
The goal of theirwork is to show how the techniques improve IEpattern acquisition.
To do this, Sudo et al con-strain the retrieval of relevant documents for aMUC scenario and then use unsupervised learn-ing over descriptions within these documents thatmatch specific types of named entities (e.g., Ar-resting Agency, Charge), thus enabling learningof patterns for specific templates (e.g., the Ar-rest scenario).
In contrast, the goal of our workis to show how similar techniques can be used tolearn what information is important for a givendomain or event and thus, should be includedinto the domain template.
Our approach allows,for example, learning that an arrest along withother events (e.g., attack) is often part of a ter-rorist event.
We do not assume any prior knowl-edge about domains.
We demonstrate that frequentsubtrees can be used not only to extract specificnamed entities for a given scenario but also tolearn domain-important relations.
These relationslink domain actions and named entities as well asgeneral nouns and words belonging to other syn-tactic categories.Collier (1998) proposed a fully automaticmethod for creating templates for information ex-traction.
The method relies on Luhn?s (1957) ideaof locating statistically significant words in a cor-pus and uses those to locate the sentences in whichthey occur.
Then it extracts Subject-Verb-Objectpatterns in those sentences to identify the mostimportant interactions in the input data.
The sys-tem was constructed to create MUC templates forterrorist attacks.
Our work also relies on corpusstatistics, but we utilize arbitrary syntactic pat-terns and explicitly use multiple domain instances.Keeping domain instances separated, we cross-examine them and estimate the importance of aparticular information type in the domain.2083 Our Approach to Template CreationAfter reading about presidential elections in dif-ferent countries on different years, a reader has ageneral picture of this process.
Later, when read-ing about a new presidential election, the reader al-ready has in her mind a set of questions for whichshe expects answers.
This process can be calleddomain modeling.
The more instances of a partic-ular domain a person has seen, the better under-standing she has about what type of informationshould be expected in an unseen collection of doc-uments discussing a new instance of this domain.Thus, we propose to use a set of document col-lections describing different instances within onedomain to learn the general characteristics of thisdomain.
These characteristics can be then used tocreate a domain template.
We test our system onfour domains: airplane crashes, earthquakes, pres-idential elections, terrorist attacks.4 Data Description4.1 Training DataTo create training document collections we usedBBC Advanced Search3 and submitted queries ofthe type ?domain title + country?.
For example,?
?presidential election?
USA?.In addition, we used BBC?s Advanced Searchdate filter to constrain the results to different dateperiods of interest.
For example, we used knowndates of elections and allowed a search for articlespublished up to five days before or after each suchdate.
At the same time for the terrorist attacks orearthquakes domain the time constraints we sub-mitted were the day of the event plus ten days.Thus, we identify several instances for each ofour four domains, obtaining a document collec-tion for each instance.
E.g., for the earthquake do-main we collected documents on the earthquakesin Afghanistan (March 25, 2002), India (January26, 2001), Iran (December 26, 2003), Japan (Oc-tober 26, 2004), and Peru (June 23, 2001).
Usingthis procedure we retrieve training document col-lections for 9 instances of airplane crashes, 5 in-stances of earthquakes, 13 instances of presiden-tial elections, and 6 instances of terrorist attacks.4.2 Test DataTo test our system, we used document clustersfrom the Topic Detection and Tracking (TDT) cor-3http://news.bbc.co.uk/shared/bsp/search2/advanced/news_ifs.stmpus (Fiscus et al, 1999).
Each TDT topic has atopic label, such as Accidents or Natural Disas-ters.4 These categories are broader than our do-mains.
Thus, we manually filtered the TDT topicsrelevant to our four training domains (e.g., Acci-dents matching Airplane Crashes).
In this way, weobtained TDT document clusters for 2 instancesof airplane crashes, 3 instances of earthquakes, 6instances of presidential elections and 3 instancesof terrorist attacks.
The number of the documentscorresponding to the instances varies greatly (fromtwo documents for one of the earthquakes up to156 documents for one of the terrorist attacks).This variation in the number of documents pertopic is typical for the TDT corpus.
Many of thecurrent approaches of domain modeling collapsetogether different instances and make the decisionon what information is important for a domainbased on this generalized corpus (Collier, 1998;Barzilay and Lee, 2003; Sudo et al, 2003).
We,on the other hand, propose to cross-examine theseinstances keeping them separated.
Our goal is toeliminate dependence on how well the corpus isbalanced and to avoid the possibility of greaterimpact on the domain template of those instanceswhich have more documents.5 Creating TemplatesIn this work we build domain templates aroundverbs which are estimated to be important for thedomains.
Using verbs as the starting point weidentify semantic dependencies within sentences.In contrast to deep semantic analysis (Fillmoreand Baker, 2001; Gildea and Jurafsky, 2002; Prad-han et al, 2004; Harabagiu and La?ca?tus?u, 2005;Palmer et al, 2005) we rely only on corpus statis-tics.
We extract the most frequent syntactic sub-trees which connect verbs to the lexemes used inthe same subtrees.
These subtrees are used to cre-ate domain templates.For each of the four domains described in Sec-tion 4, we automatically create domain templatesusing the following algorithm.Step 1: Estimate what verbs are important forthe domain under investigation.
We initiate ouralgorithm by calculating the probabilities for allthe verbs in the document collection for one do-main ?
e.g., the collection containing all the in-stances in the domain of airplane crashes.
We4In our experiments we analyze TDT topics used inTDT-2 and TDT-4 evaluations.209discard those verbs that are stop words (Salton,1971).
To take into consideration the distributionof a verb among different instances of the domain,we normalize this probability by its VIF value(verb instance frequency), specifying in how manydomain instances this verb appears.Score(vbi) =countvbi?vbj?comb coll countvbj?
VIF(vbi) (1)VIF(vbi) = # of domain instances containing vbi# of all domain instances (2)These verbs are estimated to be the most impor-tant for the combined document collection for allthe domain instances.
Thus, we build the domaintemplate around these verbs.
Here are the top tenverbs for the terrorist attack domain:killed, told, found, injured, reported,happened, blamed, arrested, died, linked.Step 2: Parse those sentences which contain thetop 50 verbs.
After we identify the 50 most impor-tant verbs for the domain under analysis, we parseall the sentences in the domain document collec-tion containing these verbs with the Stanford syn-tactic parser (Klein and Manning, 2002).Step 3: Identify most frequent subtrees containingthe top 50 verbs.
A domain template should con-tain not only the most important actions for the do-main, but also the entities that are linked to theseactions or to each other through these actions.
Thelexemes referring to such entities can potentiallybe used within the domain template slots.
Thus,we analyze those portions of the syntactic treeswhich contain the verbs themselves plus other lex-emes used in the same subtrees as the verbs.
To dothis we use FREQuent Tree miner.5 This softwareis an implementation of the algorithm presentedby (Abe et al, 2002; Zaki, 2002), which extractsfrequent ordered subtrees from a set of orderedtrees.
Following (Sudo et al, 2003) we are inter-ested only in the lexemes which are near neighborsof the most frequent verbs.
Thus, we look only forthose subtrees which contain the verbs themselvesand from four to ten tree nodes, where a node iseither a syntactic tag or a lexeme with its tag.
Weanalyze not only NPs which correspond to the sub-ject or object of the verb, but other syntactic con-stituents as well.
For example, PPs can potentiallylink the verb to locations or dates, and we want toinclude this information into the template.
Table 1contains a sample of subtrees for the terrorist at-tack domain mined from the sentences containing5http://chasen.org/?taku/software/freqt/nodes subtree8 (SBAR(S(VP(VBD killed)(NP(QP(IN at))(NNS people)))))8 (SBAR(S(VP(VBD killed)(NP(QP(JJS least))(NNS people)))))5 (VP(ADVP)(VBD killed)(NP(NNS people)))6 (VP(VBD killed)(NP(ADJP(JJ many))(NNS people)))5 (VP(VP(VBD killed)(NP(NNS people))))7 (VP(ADVP(NP))(VBD killed)(NP(CD 34)(NNS people)))6 (VP(ADVP)(VBD killed)(NP(CD 34)(NNS people)))Table 1: Sample subtrees for the terrorist attack domain.the verb killed.
The first column of Table 1 showshow many nodes are in the subtree.Step 4: Substitute named entities with their re-spective tags.
We are interested in analyzing awhole domain, not just an instance of this do-main.
Thus, we substitute all the named entitieswith their respective tags, and all the exact num-bers with the tag NUMBER.
We speculate that sub-trees similar to those presented in Table 1 canbe extracted from a document collection repre-senting any instance of a terrorist attack, with theonly difference being the exact number of causal-ities.
Later, however, we analyze the domain in-stances separately to identity information typi-cal for the domain.
The procedure of substitut-ing named entities with their respective tags previ-ously proved to be useful for various tasks (Barzi-lay and Lee, 2003; Sudo et al, 2003; Filatova andPrager, 2005).
To get named entity tags we usedBBN?s IdentiFinder (Bikel et al, 1999).Step 5: Merge together the frequent subtrees.
Fi-nally, we merge together those subtrees whichare identical according to the information encodedwithin them.
This is a key step in our algorithmwhich allows us to bring together subtrees fromdifferent instances of the same domain.
For exam-ple, the information rendered by all the subtreesfrom the bottom part of Table 1 is identical.
Thus,these subtrees can be merged into one which con-tains the longest common pattern:(VBD killed)(NP(NUMBER)(NNS people))After this merging procedure we keep only thosesubtrees for which each of the domain instanceshas at least one of the subtrees from the initial setof subtrees.
This subtree should be used in the in-stance at least twice.
At this step, we make surethat we keep in the template only the informationwhich is generally important for the domain ratherthan only for a fraction of instances in this domain.We also remove all the syntactic tags as we wantto make this pattern as general for the domain aspossible.
A pattern without syntactic dependenciescontains a verb together with a prospective tem-210plate slot corresponding to this verb:killed: (NUMBER) (NNS people)In the above example, the prospective templateslots appear after the verb killed.
In other cases thedomain slots appear in front of the verb.
Two ex-amples of such slots, for the presidential electionand earthquake domains, are shown below:(PERSON) won(NN earthquake) struckThe above examples show that it is not enough toanalyze only named entities, general nouns con-tain important information as well.
We term thestructure consisting of a verb together with the as-sociated slots a slot structure.
Here is a part of theslot structure we get for the verb killed after cross-examination of the terrorist attack instances:killed (NUMBER) (NNS people)(PERSON) killed(NN suicide) killedSlot structures are similar to verb frames, whichare manually created for the PropBank annota-tion (Palmer et al, 2005).6 An example of thePropBank frame for the verb to kill is:Roleset kill.01 ?cause to die?
:Arg0:killerArg1:corpseArg2:instrumentThe difference between the slot structure extractedby our algorithm and the PropBank frame slots isthat the frame slots assign a semantic role to eachslot, while our algorithm gives either the type ofthe named entity that should fill in this slot or putsa particular noun into the slot (e.g., ORGANIZA-TION, earthquake, people).
An ideal domain tem-plate should include semantic information but thisproblem is outside of the scope of this paper.Step 6: Creating domain templates.
After we getall the frequent subtrees containing the top 50 do-main verbs, we merge all the subtrees correspond-ing to the same verb and create a slot structure forevery verb as described in Step 5.
The union ofsuch slot structures created for all the importantverbs in the domain is called the domain template.From the created templates we remove the slotswhich are used in all the domains.
For example,(PERSON) told.2The presented algorithm can be used to create atemplate for any domain.
It does not require pre-defined domain or world knowledge.
We learn do-main templates from cross-examining documentcollections describing different instances of thedomain of interest.6http://www.cs.rochester.edu/?gildea/Verbs/6 EvaluationThe task we deal with is new and there is no well-defined and standardized evaluation procedure forit.
Sudo et al (2003) evaluated how well theirIE patterns captured named entities of three pre-defined types.
We are interested in evaluating howwell we capture the major actions as well as theirconstituent parts.There is no set of domain templates which arebuilt according to a unique set of principles againstwhich we could compare our automatically cre-ated templates.
Thus, we need to create a goldstandard.
In Section 6.1, we describe how the goldstandard is created.
Then, in Section 6.2, we eval-uate the quality of the automatically created tem-plates by extracting clauses corresponding to thetemplates and verifying how many answers fromthe questions in the gold standard are answered bythe extracted clauses.6.1 Stage 1.
Information Included intoTemplates: Interannotator AgreementTo create a gold standard we asked people to createa list of questions which indicate what is importantfor the domain description.
Our decision to aimfor the lists of questions and not for the templatesthemselves is based on the following considera-tions: first, not all of our subjects are familiar withthe field of IE and thus, do not necessarily knowwhat an IE template is; second, our goal for thisevaluation is to estimate interannotator agreementfor capturing the important aspects for the domainand not how well the subjects agree on the tem-plate structure.We asked our subjects to think of their expe-rience of reading newswire articles about variousdomains.7 Based on what they remember from thisexperience, we asked them to come up with a listof questions about a particular domain.
We askedthem to come up with at most 20 questions cover-ing the information they will be looking for givenan unseen news article about a new event in thedomain.
We did not give them any input informa-tion about the domain but allowed them to use anysources to learn more information about the do-main.We had ten subjects, each of which created onelist of questions for one of the four domains under7We thank Rod Adams, Cosmin-Adrian Bejan, SashaBlair-Goldensohn, Cyril Cerovic, David Elson, David Evans,Ovidiu Fortu, Agustin Gravano, Lokesh Shresta, John Yundt-Pacheco and Kapil Thadani for the submitted questions.211Jaccard metricDomain subj1 and subj1 and subj2 andsubj2 (and subj3) MUC MUCAirplane crash 0.54 - -Earthquake 0.68 - -Presidential Election 0.32 - -Terrorist Attack 0.50 0.63 0.59Table 2: Creating gold standard.
Jaccard metric values for in-terannotator agreement.analysis.
Thus, for the earthquake and terrorist at-tack domains we got two lists of questions; for theairplane crash and presidential election domainswe got three lists of questions.After the questions lists were created we studiedthe agreement among annotators on what infor-mation they consider is important for the domainand thus, should be included in the template.
Wematched the questions created by different anno-tators for the same domain.
For some of the ques-tions we had to make a judgement call on whetherit is a match or not.
For example, the followingquestion created by one of the annotators for theearthquake domain was:Did the earthquake occur in a well-known areafor earthquakes (e.g.
along the San Andreasfault), or in an unexpected location?We matched this question to the following threequestions created by the other annotator:What is the geological localization?Is it near a fault line?Is it near volcanoes?Usually, the degree of interannotator agreementis estimated using Kappa.
For this task, though,Kappa statistics cannot be used as they requireknowledge of the expected or chance agreement,which is not applicable to this task (Fleiss et al,1981).
To measure interannotator agreement weuse the Jaccard metric, which does not requireknowledge of the expected or chance agreement.Table 2 shows the values of Jaccard metric for in-terannotator agreement calculated for all four do-mains.
Jaccard metric values are calculated asJaccard(domaind) = |QSdi ?
QSdj ||QSdi ?
QSdj |(3)where QSdi and QSdj are the sets of questions cre-ated by subjects i and j for domain d. For the air-plane crash and presidential election domains weaveraged the three pairwise Jaccard metric values.The scores in Table 2 show that for some do-mains the agreement is quite high (e.g., earth-quake), while for other domains (e.g., presiden-tial election) it is twice as low.
This differencein scores can be explained by the complexity ofthe domains and by the differences in understand-ing of these domains by different subjects.
Thescores for the presidential election domain are pre-dictably low as in different countries the roles ofpresidents are very different: in some countries thepresident is the head of the government with a lotof power, while in other countries the president ismerely a ceremonial figure.
In some countries thepresidents are elected by general voting while inother countries, the presidents are elected by par-liaments.
These variations in the domain cause thesubjects to be interested in different issues of thedomain.
Another issue that might influence the in-terannotator agreement is the distribution of thepresidential election process in time.
For example,one of our subjects was clearly interested in thepre-voting situation, such as debates between thecandidates, while another subject was interestedonly in the outcome of the presidential election.For the terrorist attack domain we also com-pared the lists of questions we got from our sub-jects with the terrorist attack template created byexperts for the MUC competition.
In this templatewe treated every slot as a separate question, ex-cluding the first two slots which captured informa-tion about the text from which the template fillerswere extracted and not about the domain.
The re-sults for this comparison are included in Table 2.Differences in domain complexity were stud-ied by IE researchers.
Bagga (1997) suggests aclassification methodology to predict the syntac-tic complexity of the domain-related facts.
Hut-tunen et al (2002) analyze how component sub-events of the domain are linked together and dis-cuss the factors which contribute to the domaincomplexity.6.2 Stage 2.
Quality of the AutomaticallyCreated TemplatesIn section 6.1 we showed that not all the domainsare equal.
For some of the domains it is much eas-ier to come to a consensus about what slots shouldbe present in the domain template than for others.In this section we describe the evaluation of thefour automatically created templates.Automatically created templates consist of slotstructures and are not easily readable by humanannotators.
Thus, instead of direct evaluation ofthe template quality, we evaluate the clauses ex-tracted according to the created templates and212check whether these clauses contain the answersto the questions created by the subjects during thefirst stage of the evaluation.
We extract the clausescorresponding to the test instances according tothe following procedure:1.
Identify all the simple clauses in the docu-ments corresponding to a particular test in-stance (respective TDT topic).
For example,for the sentenceHer husband, Robert, survived Thursday?sexplosion in a Yemeni harbor that killed atleast six crew members and injured 35.only one part is output:that killed at least six crew members andinjured 352.
For every domain template slot check all thesimple clauses in the instance (TDT topic)under analysis.
Find the shortest clause (orsequence of clauses) which includes both theverb and other words extracted for this slot intheir respective order.
Add this clause to thelist of extracted clauses unless this clause hasbeen already added to this list.3.
Keep adding clauses to the list of extractedclauses till all the template slots are analyzedor the size of the list exceeds 20 clauses.The key step in the above algorithm is Step 2.
Bychoosing the shortest simple clause or sequenceof simple clauses corresponding to a particulartemplate slot, we reduce the possibility of addingmore information to the output than is necessaryto cover each particular slot.In Step 3 we keep only the first twenty clausesso that the length of the output which potentiallycontains an answer to the question of interest is notlarger than the number of questions provided byeach subject.
The templates are created from theslot structures extracted for the top 50 verbs.
Thehigher the estimated score of the verb (Eq.
1) forthe domain the closer to the top of the template theslot structure corresponding to this verb will be.We assume that the important information is morelikely to be covered by the slot structures that areplaced near the top of the template.The evaluation results for the automatically cre-ated templates are presented in Figure 1.
We cal-culate what average percentage of the questions iscovered by the outputs created according to thedomain templates.
For every domain, we presentthe percentage of the covered questions separatelyfor each annotator and for the intersection of ques-tions (Section 6.1).0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%80.00%Attack Earthquake Presidentialelection Plane crashIntersectSubj1Subj2Subj3Figure 1: Evaluation results.For the questions common for all the annota-tors we capture about 70% of the answers forthree out of four domains.
After studying the re-sults we noticed that for the earthquake domainsome questions did not result in a template slotand thus, could not be covered by the extractedclauses.
Here are two of such questions:Is it near a fault line?Is it near volcanoes?According to the template creation procedure,which is centered around verbs, the chances thatextracted clauses would contain answers to thesequestions are low.
Indeed, only one of the threesentence sets extracted for the three TDT earth-quake topics contain an answer to one of thesequestions.Poor results for the presidential election domaincould be predicted from the Jaccard metric valuefor interannotator agreement (Table 2).
There isconsiderable discrepancy in the questions createdby human annotators which can be attributed to thegreat variation in the presidential election domainitself.
It must be also noted that most of the ques-tions created for the presidential election domainwere clearly referring to the democratic electionprocedure, while some of the TDT topics catego-rized as Elections were about either election fraudor about opposition taking over power without theformal resignation of the previous president.Overall, this evaluation shows that using au-tomatically created domain templates we extractsentences which contain a substantial part of theimportant information expressed in questions forthat domain.
For those domains which have smalldiversity our coverage can be significantly higher.7 ConclusionsIn this paper, we presented a robust method fordata-driven discovery of the important fact-types213for a given domain.
In contrast to supervised meth-ods, the fact-types are not pre-specified.
The re-sulting slot structures can subsequently be usedto guide the generation of responses to questionsabout new instances of the same domain.
Our ap-proach features the use of corpus statistics derivedfrom both lexical and syntactic analysis acrossdocuments.
A comparison of our system outputfor four domains of interest shows that our ap-proach can reliably predict the majority of infor-mation that humans have indicated are of interest.Our method is flexible: analyzing document col-lections from different time periods or locations,we can learn domain descriptions that are tailoredto those time periods and locations.Acknowledgements.
We would like to thank Re-becca Passonneau and Julia Hirschberg for thefruitful discussions at the early stages of this work;Vasilis Vassalos for his suggestions on the eval-uation instructions; Michel Galley, Agustin Gra-vano, Panagiotis Ipeirotis and Kapil Thadani fortheir enormous help with evaluation.This material is based upon work supportedin part by the Advanced Research Devel-opment Agency (ARDA) under Contract No.NBCHC040040 and in part by the Defense Ad-vanced Research Projects Agency (DARPA) underContract No.
HR0011-06-C-0023.
Any opinions,findings and conclusions expressed in this mate-rial are those of the authors and do not necessarilyreflect the views of ARDA and DARPA.ReferencesKenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura,and Setsuo Arikawa.
2002.
Optimized substructure dis-covery for semi-structured data.
In Proc.
of PKDD.Amit Bagga.
1997.
Analyzing the complexity of a domainwith respect to an Information Extraction task.
In Proc.7th MUC.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proc.
of HLT/NAACL.Daniel Bikel, Richard Schwartz, and Ralph Weischedel.1999.
An algorithm that learns what?s in a name.
Ma-chine Learning Journal Special Issue on Natural Lan-guage Learning, 34:211?231.Sasha Blair-Goldensohn, Kathleen McKeown, and An-drew Hazen Schlaikjer, 2004.
Answering DefinitionalQuestions: A Hybrid Approach.
AAAI Press.Robin Collier.
1998.
Automatic Template Creation for Infor-mation Extraction.
Ph.D. thesis, University of Sheffield.Pablo Duboue and Kathleen McKeown.
2003.
Statisticalacquisition of content selection rules for natural languagegeneration.
In Proc.
of EMNLP.Elena Filatova and Vasileios Hatzivassiloglou.
2003.Domain-independent detection, extraction, and labeling ofatomic events.
In Proc.
of RANLP.Elena Filatova and John Prager.
2005.
Tell me what you doand I?ll tell you what you are: Learning occupation-relatedactivities for biographies.
In Proc.
of EMNLP/HLT.Charles Fillmore and Collin Baker.
2001.
Frame semanticsfor text understanding.
In Proc.
of WordNet and OtherLexical Resources Workshop, NAACL.Jon Fiscus, George Doddington, John Garofolo, and AlvinMartin.
1999.
NIST?s 1998 topic detection and trackingevaluation (TDT2).
In Proc.
of the 1999 DARPA Broad-cast News Workshop, pages 19?24.Joseph Fleiss, Bruce Levin, and Myunghee Cho Paik, 1981.Statistical Methods for Rates and Proportions.
J. Wiley.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Sanda Harabagiu and Finley La?ca?tus?u.
2005.
Topic themesfor multi-document summarization.
In Proc.
of SIGIR.Sanda Harabagiu and Steven Maiorano.
2002.
Multi-docu-ment summarization with GISTexter.
In Proc.
of LREC.Jerry Hobbs and David Israel.
1994.
Principles of templatedesign.
In Proc.
of the HLT Workshop.Silja Huttunen, Roman Yangarber, and Ralph Grishman.2002.
Complexity of event structure in IE scenarios.
InProc.
of COLING.Dan Klein and Christopher Manning.
2002.
Fast exact infer-ence with a factored model for natural language parsing.In Proc.
of NIPS.Hans Luhn.
1957.
A statistical approach to mechanized en-coding and searching of literary information.
IBM Journalof Research and Development, 1:309?317.Elaine Marsh and Dennis Perzanowski.
1997.
MUC-7 eval-uation of IE technology: Overview of results.
In Proc.
ofthe 7th MUC.Boyan Onyshkevych.
1994.
Issues and methodology fortemplate design for information extraction system.
InProc.
of the HLT Workshop.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.
TheProposition Bank: An annotated corpus of semantic roles.Computational Linguistics, 31(1):71?106.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Mar-tin, and Daniel Jurafsky.
2004.
Shallow semantic parsingusing support vector machines.
In Proc.
of HLT/NAACL.Dragomir Radev and Kathleen McKeown.
1998.
Gener-ating natural language summaries from multiple on-linesources.
Computational Linguistics, 24(3):469?500.Ellen Riloff and Mark Schmelzenbach.
1998.
An empiricalapproach to conceptual case frame acquisition.
In Proc.
ofthe 6th Workshop on Very Large Corpora.Gerard Salton, 1971.
The SMART retrieval system.
Prentice-Hall, NJ.Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003.An improved extraction pattern representation model forautomatic IE pattern acquisition.
In Proc.
of ACL.Ralph Weischedel, Jinxi Xu, and Ana Licuanan, 2004.
Hy-brid Approach to Answering Biographical Questions.AAAI Press.Michael White, Tanya Korelsky, Claire Cardie, Vincent Ng,David Pierce, and Kiri Wagstaff.
2001.
Multi-documentsummarization via information extraction.
In Proc.
ofHLT.Mohammed Zaki.
2002.
Efficiently mining frequent trees ina forest.
In Proc.
of SIGKDD.Liang Zhou, Miruna Ticrea, and Eduard Hovy.
2004.
Multi-document biography summarization.
In Proc.
of EMNLP.214
