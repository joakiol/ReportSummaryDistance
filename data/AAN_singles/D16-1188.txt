Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1827?1837,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsRegularizing Text Categorization with Clusters of WordsKonstantinos Skianis Fran?ois RousseauLIX, ?cole Polytechnique, Francekskianis@lix.polytechnique.frMichalis VazirgiannisAbstractRegularization is a critical step in supervisedlearning to not only address overfitting, butalso to take into account any prior knowledgewe may have on the features and their de-pendence.
In this paper, we explore state-of-the-art structured regularizers and we pro-pose novel ones based on clusters of wordsfrom LSI topics, word2vec embeddings andgraph-of-words document representation.
Weshow that our proposed regularizers are fasterthan the state-of-the-art ones and still improvetext classification accuracy.
Code and data areavailable online1.1 IntroductionHarnessing the full potential in text data has alwaysbeen a key task for the NLP and ML communities.The properties hidden under the inherent high di-mensionality of text are of major importance in taskssuch as text categorization and opinion mining.Although simple models like bag-of-words man-age to perform well, the problem of overfitting stillremains.
Regularization as proven in Chen andRosenfeld (2000) is of paramount importance inNatural Language Processing and more specificallylanguage modeling, structured prediction, and clas-sification.
In this paper we build upon the work ofYogatama and Smith (2014b) who introduce priorknowledge of data as a regularization term.
One ofthe most popular structured regularizers, the grouplasso (Yuan and Lin, 2006), was proposed to avoidlarge L2 norms for groups of weights.1https://goo.gl/mKqvroIn this paper, we propose novel linguistic struc-tured regularizers that capitalize on the clusterslearned from texts using the word2vec and graph-of-words document representation, which can be seenas group lasso variants.
The extensive experimentswe conducted demonstrate these regularizers canboost standard bag-of-words models on most casestested in the task of text categorization, by imposingadditional unused information as bias.2 Background & NotationWe place ourselves in the scenario where we con-sider a prediction problem, in our case text catego-rization, as a loss minimization problem, i. e. wedefine a loss function L(x,?, y) that quantifies theloss between the prediction h?,b(x) of a classifierparametrized by a vector of feature weights ?
and abias b, and the true class label y ?
Y associated withthe example x ?
X .
Given a training set of N datapoints {(xi, yi)}i=1...N , we want to find the optimalset of feature weights ??
such that:??
= argmin?N?i=1L(xi,?, yi)?
??
?empirical risk(1)In the case of logistic regression with binary predic-tions (Y = {?1,+1}), h?,b(x) = ?>x + b andL(x,?, y) = e?yh?,b(x) (log loss).2.1 RegularizationOnly minimizing the empirical risk can lead to over-fitting, that is, the model no longer learns the un-derlying pattern we are trying to capture but fits the1827noise contained in the training data and thus resultsin poorer generalization (e. g., lower performanceson the test set).
For instance, along with some fea-ture space transformations to obtain non-linear de-cision boundaries in the original feature space, onecould imagine a decision boundary that follows ev-ery quirk of the training data.
Additionally, if twohypothesis lead to similar low empirical risks, oneshould select the ?simpler?
model for better general-ization power, simplicity assessed using some mea-sure of model complexity.Loss+Penalty Regularization takes the form ofadditional constraints to the minimization problem,i.
e. a budget on the feature weights, which are of-ten relaxed into a penalty term ?(?)
controlled viaa Lagrange multiplier ?.
We refer to the book ofBoyd and Vandenberghe (2004) for the theory be-hind convex optimization.
Therefore, the overallexpected risk (Vapnik, 1991) is the weighted sumof two components: the empirical risk and a reg-ularization penalty term, expression referred to as?Loss+Penalty" by Hastie et al (2009).
Given atraining set of N data points {(xi, yi)}i=1...N , wenow want to find the optimal set of feature weights??
such that:??
= argmin?N?i=1L(xi,?, yi)?
??
?empirical risk+ ??(?)?
??
?penalty term?
??
?expected risk(2)L1 and L2 regularization The two most usedpenalty terms are known as L1 regularization, a. k. a.lasso (Tibshirani, 1996), and L2 regularization,a.
k. a. ridge (Hoerl and Kennard, 1970) as they cor-respond to penalizing the model with respectivelythe L1 and L2 norm of the feature weight vector ?:??
= argmin?N?i=1L(xi,?, yi) + ?p?j=1|?j | (3)??
= argmin?N?i=1L(xi,?, yi) + ?p?j=1?j 2 (4)Prior on the feature weights L1 (resp.
L2) reg-ularization can be interpreted as adding a Laplacian(resp.
Gaussian) prior on the feature weight vector.Indeed, given the training set, we want to find themost likely hypothesis h?
?
H, i. e. the one withmaximum a posteriori probability:h?
= argmaxh?H(P(h|{(xi, yi)}i=1...N ))= argmaxh?H(P({yi}i|{xi}i, h)P(h|{xi}i)P({yi}i|{xi}i))= argmaxh?H(P({yi}i|{xi}i, h)P(h|{xi}i))= argmaxh?H(P({yi}i|{xi}i, h)P(h)) (5)= argmaxh?H( N?i=1(P(yi|xi, h))P(h))(6)= argmaxh?H( N?i=1(logP(yi|xi, h))+ logP(h))= argminh?H??????N?i=1(?
logP(yi|xi, h))?
??
?empirical risk?
logP(h)?
??
?penalty term?????
?For the derivation, we assumed that the hypothesish does not depend on the examples alone (Eq.
5)and that the N training labeled examples are drawnfrom an i.i.d.
sample (Eq.
6).
In that last form, wesee that the loss function can be interpreted as a neg-ative log-likelihood and the regularization penaltyterm as a negative log-prior over the hypothesis.Therefore, if we assume a multivariate Gaussianprior on the feature weight vector of mean vector 0and covariance matrix ?
= ?2I (i. e. independentfeatures of same prior standard deviation ?
), we doobtain the L2 regularization:P(h) = 1?(2pi)p|?|e?
12?>??1?
(7)?
?
logP(h) = 12?2?>I?
+ p2 log(2pi?
)argmax= ???
?2 2, ?
=12?2 (8)And similarly, if we assume a multivariate Lapla-cian prior on the feature weight vector (i. e. ?i ?Laplace(0, 1?
)), we obtain L1-regularization.
Inpractice, in both cases, the priors basically mean thatwe expect weights around 0 on average.
The maindifference between L1 and L2 regularization is thatthe Laplacian prior will result in explicitly settingsome feature weights to 0 (feature sparsity) whilethe Gaussian prior will only result in reducing theirvalues (shrinkage).18282.2 Structured regularizationIn L1 and L2 regularizations, features are consideredas independent, which makes sense without any ad-ditional prior knowledge.
However, similar featureshave similar weights in the case of linear classifiers?
equal weights for redundant features in the ex-treme case ?
and therefore, if we have some priorknowledge on the relationships between features, weshould include that information for better general-ization, i. e. include it in the regularization penaltyterm.
Depending on how the similarity between fea-tures is encoded, e. g., through sets, trees (Kim andXing, 2010; Liu and Ye, 2010; Mairal et al, 2010) orgraphs (Jenatton et al, 2010), the penalization termvaries but in any case, we take into account the struc-ture between features, hence the ?structured regular-ization?
terminology.
It should not be confused with?structured prediction?
where this time the outcomeis a structured object as opposed to a scalar (e. g., aclass label) classically.Group lasso Bakin (1999) and later Yuan and Lin(2006) proposed an extension of L1 regularizationto encourage groups of features to either go to zero(as a group) or not (as a group), introducing groupsparsity in the model.
To do so, they proposed toregularize with the L1,2 norm of the feature weightvector:?(?)
= ??g?g?
?g?2 (9)where ?g is the subset of feature weights restrictedto group g. Note that the groups can be overlapping(Jacob et al, 2009; Schmidt and Murphy, 2010; Je-natton et al, 2011; Yuan et al, 2011) even though itmakes the optimization harder.2.3 LearningIn our case we use a logistic regression loss functionin order to integrate our regularization terms easily.L(x,?, y) = log(1 + exp(?y?Tx)) (10)It is obvious that the framework can be extended toother loss functions (e. g., hinge loss).For the case of structured regularizers, there exista plethora of optimization methods such group lasso.Since our tasks involves overlapping groups, we se-lect the method of Yogatama and Smith (2014b).Algorithm 1 ADMM for overlapping group-lassoRequire: augmented Lagrangian variable ?, regulariza-tion strengths ?glas and ?las1: while update in weights not small do2: ?
= argmin??las(?)+L(?
)+ ?2V?i=1Ni(?i?
?i)23: for g = 1 to G do4: vg = prox?glas,?g?
(zg)5: end for6: u = u+ ?(v?M?
)7: end whileTheir method uses the alternating directions methodof multipliers (Hestenes, 1969; Powell, 1969).Now given the lasso penalty for each feature andthe group lasso regularizer, the problem becomes:min?,v ?las(?)
+ ?glas(v) +D?d=1L(xd,?, yd) (11)so that v = M?, where v is a copy-vector of ?.
Thecopy-vector v is needed because the group-lasso reg-ularizer contains overlaps between the used groups.M is an indicator matrix of size L ?
V , where L isthe sum of the total sizes of all groups, and its onesshow the link between the actual weights ?
and theircopies v. Following Yogatama and Smith (2014b)a constrained optimization problem is formed, thatcan be transformed to an augmented Lagrangianproblem:?las(?)
+ ?glas(v) + L(?)
+ u>(v?M?)+?2?v?M?
?22(12)Essentially, the problem becomes the iterative up-date of ?, v and u:min??las(?)+L(?
)+u>M?+ ?2?v?M?
?22 (13)minv ?glas(v) + u>v + ?2?v?M?
?22 (14)u = u + ?(v?M?)
(15)Convergence Yogatama and Smith (2014b)proved that ADMM for sparse overlapping grouplasso converges.
It is also shown that a goodapproximate solution is reached in a few tens ofiterations.
Our experiments confirm this as well.18293 Structured Regularization in NLPIn recent efforts there are results to identify usefulstructures in text that can be used to enhance the ef-fectiveness of the text categorization in a NLP con-text.
Since the main regularization approach weare going to use are variants of the group lasso,we are interested on prior knowledge in terms ofgroups/clusters that can be found in the training textdata.
These groups could capture either semantic, orsyntactic structures that affiliate words to communi-ties.
In our work, we study both semantic and syn-tactic properties of text data, and incorporate them instructured regularizer.
The grouping of terms is pro-duced by either LSI or clustering in the word2vec orgraph-of-words space.3.1 Statistical regularizersIn this section, we present statistical regularizers,i.
e. with groups of words based on co-occurrences,as opposed to syntactic ones (Mitra et al, 1997).Network of features Sandler et al (2009) intro-duced regularized learning with networks of fea-tures.
They define a graph G whose edges are non-negative with larger weights indicating greater sim-ilarity.
Conversely, a weight of zero means that twofeatures are not believed a priori to be similar.
Pre-vious work (Ando and Zhang, 2005; Raina et al,2006; Krupka and Tishby, 2007) shows such sim-ilarities can be inferred from prior domain knowl-edge and statistics computed on unlabeled data.The weights of G are mapped in a matrix P ,where Pij ?
0 gives the weight of the directed edgefrom vertex i to vertex j.
The out-degree of eachvertex is constrained to sum to one, ?j Pij = 1, sothat no feature ?dominates" the graph.?network(?)
= ?net?
?>kM?k (16)whereM = ?
(I?P )>(I?P )+?I .
The matrix Mis symmetric positive definite, and therefore it pos-sesses a Bayesian interpretation in which the weightvector ?, is a priori normally distributed with meanzero and covariance matrix 2M?1.
However, pre-liminary results show poorer performance comparedto structured regularizers in larger datasets.Sentence regularizer Yogatama and Smith(2014b) proposed to define groups as the sentencesin the training dataset.
The main idea is to definea group dd,s for every sentence s in every trainingdocument d so that each group holds weights foroccurring words in its sentence.
Thus a word can bea member of one group for every distinct (training)sentence it occurs in.
The regularizer is:?sen(?)
=D?d=1Sd?s=1?d,s?
?d,s?2 (17)where Sd is the number of sentences in document d.Since modern text datasets typically contain thou-sands of sentences and many words appear in morethan one sentence, the sentence regularizer couldpotentially lead to thousands heavily overlappinggroups.
As stated in the work of Yogatama andSmith (2014b), a rather important fact is that the reg-ularizer will force all the weights of a sentence, if itis recognized as irrelevant.
Respectively, it will keepall the weights of a relevant sentence, even thoughthe group contains unimportant words.
Fortunately,the problem can be resolved by adding a lasso regu-larization (Friedman et al, 2010).3.2 Semantic regularizersIn this section, we present semantic regularizersthat define groups based on how semantically closewords are.LDA regularizer Yogatama and Smith (2014a)considered topics as another type of structure.
It isobvious that textual data can contain a huge num-ber of topics and especially topics that overlap eachother.
Again the main idea is to penalize weightsfor words that co-occur in the same topic, instead oftreating the weight of each word separately.Having a training corpus, topics can be easily ex-tracted with the help of the latent Dirichlet alocation(LDA) model (Blei et al, 2003).
In our experiments,we form a group by extracting the n most probablewords in a topic.
We note that the extracted topicscan vary depending the text preprocessing methodswe apply on the data.LSI regularizer Latent Semantic Indexing (LSI)can also be used in order to identify topics or groupsand thus discover correlation between terms (Deer-wester et al, 1990).
LSI uses singular value de-composition (SVD) on the document-term matrix to183022212222222 22 221 111 111112111 1llllllll lllllmethodsolutsystemlinearalgebraequatm?dimensionlambdamatricproposnumerspecialkindA method for solution of systems of linear algebraic equations with m-dimensional lambdamatrices.
A system of linear algebraic equationswith m-dimensional lambda matrices isconsidered.
The proposed method of searchingfor the solution of this system lies in reducing itto a numerical system of a special kind.Figure 1: A Graph-of-words example.identify latent variables that link co-occurring termswith documents.
The main basis behind LSI is thatwords being used in the same contexts (i. e. the doc-uments) tend to have similar meanings.
We usedLSI as a baseline and compare it with other stan-dard baselines as well as other proposed structuredregularizers.
In our work we keep the top 10 wordswhich contribute the most in a topic.The regularizer for both LDA and LSI is:?LDA,LSI(?)
=K?k=1??
?k?2 (18)where K is the number of topics.3.3 Graphical regularizersIn this section we present our proposed regularizersbased on graph-of-words and word2vec.
Essentiallythe word2vec space can be seen as a large graphwhere nodes represent terms and edges similaritiesbetween them.Graph-of-words regularizer Following the ideaof the network of features, we introduce a simplerand faster technique to identify relationships be-tween features.
We create a big collection graphfrom the training documents, where the nodes cor-respond to terms and edges correspond to co-occurrence of terms in a sliding window.
We presenta toy example of a graph-of-words in Figure 1.A critical advantage of graph-of-words is that iteasily encodes term dependency and term order (viaedge direction).
The strength of the dependence be-tween two words can also be captured by assigninga weight to the edge that links them.Graph-of-words was originally an idea of Mihal-cea and Tarau (2004) and Erkan and Radev (2004)who applied it to the tasks of unsupervised keywordextraction and extractive single document summa-rization.
Rousseau and Vazirgiannis (2013) andMalliaros and Skianis (2015) showed it performswell in the tasks of information retrieval and text cat-egorization.
Notably, the former effort ranked nodesbased on a modified version of the PageRank algo-rithm.Community detection on graph-of-words Ourgoal is to identify groups or communities of words.Having constructed the collection-level graph-of-words, we can now apply community detection al-gorithms (Fortunato, 2010).In our case we use the Louvain method, a commu-nity detection algorithm for non-overlapping groupsdescribed in the work of Blondel et al (2008).
Es-sentially it is a fast modularity maximization ap-proach, which iteratively optimizes local communi-ties until we reach optimal global modularity givensome perturbations to the current community state.The regularizer becomes:?gow(?)
=C?c=1??
?c?2 (19)where c ranges over the C communities.
Thus ?ccorresponds to the sub-vector of ?
such that the cor-responding features are present in the community c.Note that in this case we do not have overlappinggroups, since we use a non-overlapping version ofthe algorithm.As we observe that the collection-level graph-of-words does not create well separated communities ofterms, overlapping community detection algorithms,like the work of Xie et al (2013) fail to identify?good" groups and do not offer better results.Word2vec regularizer Mikolov et al (2013) pro-posed the word2vec method for learning continu-ous vector representations of words from large textdatasets.
Word2vec manages to capture the ac-tual meaning of words and map them to a multi-dimensional vector space, giving the possibility of1831applying vector operations on them.
We introduceanother novel regularizer method, by applying un-supervised clustering algorithms on the word2vecspace.Clustering on word2vec Word2vec contains mil-lions of words represented as vectors.
Sinceword2vec succeeds in capturing semantic similaritybetween words, semantically related words tend togroup together and create large clusters that can beinterpreted as ?topics".In order to extract these groups, we use a fastclustering algorithm such as K-Means (Macqueen,1967) and especially Minibatch K-means.
The reg-ularizer is:?word2vec(?)
=K?k=1??
?k?2 (20)whereK is the number of clusters we extracted fromthe word2vec space.Clustering these semantic vectors is a very inter-esting area to study and could be a research topic byitself.
The actual clustering output could vary as wechange the number of clusters we are trying to iden-tify.
In this paper we do not focus on optimizing theclustering process.4 ExperimentsWe evaluated our structured regularizers on severalwell-known datasets for the text categorization task.Table 1 summarizes statistics about the ten datasetswe used in our experiments.4.1 DatasetsTopic categorization.
From the 20 Newsgroups2dataset, we examine four binary classification tasks.We end up with binary classification problems,where we classify a document according to tworelated categories: comp.sys: ibm.pc.hardwarevs.
mac.hardware; rec.sport: baseball vs.hockey; sci: med vs. space and alt.atheism vs.soc.religion.christian.
We use the 20NG datasetfrom Python.Sentiment analysis.
The sentiment analysisdatasets we examined include movie reviews2http://qwone.com/~jason/20Newsgroups/dataset train dev test # words # sents20NGscience 949 238 790 25787 16411sports 957 240 796 21938 14997religion 863 216 717 18822 18853comp.
934 234 777 16282 10772Sentimentvote 1175 257 860 19813 43563movie 1600 200 200 43800 49433books 1440 360 200 21545 13806dvd 1440 360 200 21086 13794electr.
1440 360 200 10961 10227kitch.
1440 360 200 9248 8998Table 1: Descriptive statistics of the datasets(Pang and Lee, 2004; Zaidan and Eisner, 2008)3,floor speeches by U.S.
Congressmen deciding?yea"/?nay" votes on the bill under discussion(Thomas et al, 2006)3 and product reviews fromAmazon (Blitzer et al, 2007)4.4.2 Experimental setupAs features we use unigram frequency concatenatedwith an additional unregularized bias term.
We re-produce standard regularizers like lasso, ridge, elas-tic and state-of-the-art structured regularizers likesentence, LDA as baselines and compare them withour proposed methods.For LSI, LDA and word2vec we use the gensimpackage (R?ehu?r?ek and Sojka, 2010) in Python.
Forthe learning part we used Matlab and specificallycode by Schmidt et al (2007).We split the training set in a stratified manner toretain the percentage of classes.
We use 80% of thedata for training and 20% for validation.All the hyperparameters are tuned on the develop-ment dataset, using accuracy as the evaluation crite-rion.
For lasso and ridge regularization, we choose?
from {10?2, 10?1, 1, 10, 102}.
For elastic net,we perform grid search on the same set of valuesas ridge and lasso experiments for ?rid and ?las.For the LDA, LSI, sentence, graph-of-words (GoW),word2vec regularizers, we perform grid search onthe same set of values as ridge and lasso experi-ments for the ?, ?glas, ?las parameters.
In the casewe get the same accuracy on the development data,the model with the highest sparsity is selected.
For3http://www.cs.cornell.edu/~ainur/data.html4http://www.cs.jhu.edu/~mdredze/datasets/sentiment/1832dataset no reg.
lasso ridge elastic group lassoLDA LSI sentence GoW word2vec20NGscience 0.946 0.916 0.954 0.954 0.968 0.968* 0.942 0.967* 0.968*sports 0.908 0.907 0.925 0.920 0.959 0.964* 0.966 0.959* 0.946*religion 0.894 0.876 0.895 0.890 0.918 0.907* 0.934 0.911* 0.916*computer 0.846 0.843 0.869 0.856 0.891 0.885* 0.904 0.885* 0.911*Sentimentvote 0.606 0.643 0.616 0.622 0.658 0.653 0.656 0.640 0.651movie 0.865 0.860 0.870 0.875 0.900 0.895 0.895 0.895 0.890books 0.750 0.770 0.760 0.780 0.790 0.795 0.785 0.790 0.800dvd 0.765 0.735 0.770 0.760 0.800 0.805* 0.785 0.795* 0.795*electr.
0.790 0.800 0.800 0.825 0.800 0.815 0.805 0.820 0.815kitch.
0.760 0.800 0.775 0.800 0.845 0.860* 0.855 0.840 0.855*Table 2: Accuracy results on the test sets.
Bold font marks the best performance for a dataset.
* indicates statistical significanceof improvement over lasso at p < 0.05 using micro sign test for one of our models LSI, GoW and word2vec (underlined).dataset no reg.
lasso ridge elastic group lassoLDA LSI sentence GoW word2vec20NGscience 100 1 100 63 19 20 86 19 21sports 100 1 100 5 60 11 6.4 55 44religion 100 1 100 3 94 31 99 10 85computer 100 2 100 7 40 35 77 38 18Sentimentvote 100 1 100 8 15 16 13 97 13movie 100 1 100 59 72 81 55 90 62books 100 3 100 14 41 74 72 90 99dvd 100 2 100 28 64 8 8 58 64electr.
100 4 100 6 10 8 43 8 9kitch.
100 5 100 79 73 44 27 75 46Table 3: Fraction (in %) of non-zero feature weights in each model for each dataset: the smaller, the more compact the model.LDA we set the number of topics to 1000 and wekeep the 10 most probable words of each topic asa group.
For LSI we keep 1000 latent dimensionsand we select the 10 most significant words pertopic.
For the clustering process on word2vec we ranMinibatch-Kmeans for max 2000 clusters.
For eachword belonging to a cluster, we also keep the top 5or 10 nearest words so that we introduce overlappinggroups.
The intuition behind this is that words canbe part of multiple ?concepts" or topics, thus theycan belong to many clusters.4.3 ResultsIn Table 2 we report the results of our experimentson the aforementioned datasets, and we distinguishour proposed regularizers LSI, GoW, word2vec withunderlining.
Our results are inline and confirm thatof (Yogatama and Smith, 2014a) showing the advan-tages of using structured regularizers in the text cat-egorization task.
The group based regularizers per-form systematically better than the baseline ones.We observe that the word2vec clustering basedregularizers performs very well - achieving best per-formance for three out of the ten data sets while it isquite fast with regards to execution time as it appearsin Table 3 (i. e. it is four to ten times faster than thesentence based one).The LSI based regularization, proposed for thefirst time in this paper, performs surprisingly wellas it achieves the best performance for three of theten datasets.
This is somehow interpreted by thefact that this method extracts the inherent dimen-sions that best represent the different semantics ofthe documents - as we see as well in the anecdotal1833dataset GoW word2vec20NG science 79 691sports 137 630religion 35 639computer 95 594Table 4: Number of groups.dataset lasso ridge elastic group lassoLDA LSI sentence GoW word2vec20NGscience 10 1.6 1.6 15 11 76 12 19sports 12 3 3 7 20 67 5 9religion 12 3 7 10 4 248 6 20computer 7 1.4 0.8 8 6 43 5 10Table 5: Time (in seconds) for learning with best hyperparameters.= 0piscataway combination jil@donuts0.uucpjamie reading/seeing chamblissleft-handedness abilities lubinacad sci obesity page erythromycin bottom6= 0and space the launch health for use thatmedical youspace cancer and nasahiv health shuttle for tobacco thatcancer that research center spacehiv aids are use theorykeyboard data telescope available are fromsystem information space ftpTable 6: Examples with LSI regularizer.= 0village townedc fashionable trendy trendy fashionablepoints guard guardingcrown title champion champions6= 0numbness tingling dizziness feverslaryngitis bronchitis undergo undergoingundergoes undergone healedmankind humanity civilization planetnasa kunin lang tao kay kongTable 7: Examples with word2vec regularizer.examples in Table 6, 7, 8.
This method proves aswell very fast as it appears in Table 5 (i.e.
it is threeto sixty times faster than the sentence based one).The GoW based regularization although very fast,did not outperform the other methods (while it has avery good performance in general).
It remains tobe seen whether a more thorough parameter tuningand community detection algorithm selection wouldimprove further the accuracy of the method.In Table 3 we present the feature space sizes re-tained by each of the regularizers for each dataset.As expected the lasso regularizer sets the vast major-ity of the features?
weights to zero, and thus a verysparse feature space is generated.
This fact has asa consequence the significant decrease in accuracyperformance.
Our proposed structured regularizers= 0islands inta spain galapagos canary originatedanodise advertises jewelry mercedes benzesdiamond trendyoctave chanute lillienthal6= 0vibrational broiled relieving succumbspacewalks dna nf-psychiatry itselfcommented usenet golded insects alternateself-consistent retrospectTable 8: Examples with graph-of-words regularizer.managed to perform better in most of the cases, in-troducing more sparse models compared to the state-of-the-art regularizers.4.4 Time complexityAlthough certain types of structured regularizersimprove significantly the accuracy and addressthe problem of overfitting, they require a notableamount of time in the learning process.As seen in Yogatama and Smith (2014b), a con-siderable disadvantage is the need of search forthe optimal hyperparameters: ?glas, ?lasso , and ?,whereas standard baselines like lasso and ridge onlyhave one hyperparameter and elastic net has two.Parallel grid search can be critical for finding theoptimal set of hyperparameters, since there is no de-pendency on each other, but again the process canbe very expensive.
Especially for the case of thesentence regularizer, the process can be extremelyslow due to two factors.
First, the high number ofsentences in text data.
Second, sentences consist ofheavily overlapping groups, that include words reap-pearing in one or more sentences.
On the contrary,as it appears on Table 4, the number of clusters in theclustering based regularizers is significantly smallerthan that of the sentences - and definitely controlledby the designer - thus resulting in much faster com-putation.
The update of v still remains time consum-ing for small datasets, even with parallelization.Our proposed structured regularizers are consid-erably faster in reaching convergence, since they of-1834fer a smaller number of groups with less overlappingbetween words.
For example, on the computer sub-set of the 20NG dataset, learning models with thebest hyperparameter value(s) for lasso, ridge, andelastic net took 7, 1.4, and 0.8 seconds, respectively,on an Intel Xeon CPU E5-1607 3.00 GHz machinewith 4 cores and 128GB RAM.
Given the best hyper-parameter values the LSI regularizer takes 6 secondsto converge, the word2vec regularizer takes 10 sec-onds to reach convergence, the graph-of-words takes4 seconds while the sentence regularizer requires 43seconds.
Table 5 summarizes required learning timeon 20NG datasets.We also need to consider the time needed to ex-tract the groups.
For word2vec, Minibatch K-meansrequires 15 minutes to cluster the pre-trained vectorsby Google.
The clustering is executed only once.Getting the clusters of words that belong to the vo-cabulary of each dataset requires 20 minutes, but canbe further optimized.
Finding also the communitiesin the graph-of-words approach with the Louvain al-gorithm, is very fast and requires a few minutes de-pending on the size and structure of the graph.In Tables 6, 7, 8 we show examples of our pro-posed regularizers-removed and -selected groups (inv) in the science subset of the 20NG dataset.
Wordswith weights (in w) of magnitude greater than 10?3are highlighted in red (sci.med) and blue (sci.space).5 Conclusion & Future WorkThis paper proposes new types of structured regular-izers to improve not only the accuracy but also theefficiency of the text categorization task.
We mainlyfocused on how to find and extract semantic and syn-tactic structures that lead to sparser feature spacesand therefore to faster learning times.
Overall, ourresults demonstrate that linguistic prior knowledgein the data can be used to improve categorizationperformance for baseline bag-of-words models, bymining inherent structures.
We only considered lo-gistic regression because of its interpretation for L2regularizers as Gaussian prior on the feature weightsand following Sandler et al (2009), we considereda non-diagonal covariance matrix for L2 based onword similarity before moving to group lasso as pre-sented in the paper.
We are not expecting a signif-icant change in results with different loss functionsas the proposed regularizers are not log loss specific.Future work could involve a more thorough in-vestigation on how to create and cluster graphs, i. e.covering weighted and/or signed cases.
Finding bet-ter clusters in the word2vec space is also a criti-cal part.
This is not only restricted in finding thebest number of clusters but what type of clusterswe are trying to extract.
Gaussian Mixture Models(McLachlan and Basford, 1988) could be applied inorder to capture overlapping groups at the cost ofhigh complexity.
Furthermore, topical word embed-dings (Liu et al, 2015) can be considered for reg-ularization.
This approach could enhance the reg-ularization on topic specific datasets.
Additionally,we plan on exploring alternative regularization algo-rithms diverging from the group-lasso method.ReferencesRie Kubota Ando and Tong Zhang.
2005.
A frameworkfor learning predictive structures from multiple tasksand unlabeled data.
Journal of Machine Learning Re-search, 6:1817?1853.Sergey Bakin.
1999.
Adaptive regression and modelselection in data mining problems.
Ph.D., The Aus-tralian National University, Canberra, Australia, May.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boomboxes and blenders:Domain adaptation for sentiment classification.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, ACL ?07, pages440?447.
ACL.Vincent D Blondel, Jean-Loup Guillaume, Renaud Lam-biotte, and Etienne Lefebvre.
2008.
Fast un-folding of communities in large networks.
Jour-nal of Statistical Mechanics: Theory and Experiment,2008(10):P10008.Stephen Boyd and Lieven Vandenberghe.
2004.
ConvexOptimization.
Cambridge University Press, New York,NY, USA.Stanley F. Chen and Ronald Rosenfeld.
2000.
A surveyof smoothing techniques for ME models.
IEEE Trans-actions on Speech and Audio Processing, 8(1):37?50.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41(6):391?407.1835G?nes Erkan and Dragomir R. Radev.
2004.
LexRank:graph-based lexical centrality as salience in text sum-marization.
Journal of Artificial Intelligence Re-search, 22(1):457?479.Santo Fortunato.
2010.
Community detection in graphs.Physics reports, 486(3):75?174.Jerome H. Friedman, Trevor Hastie, and Robert Tibshi-rani.
2010.
A note on the group lasso and a sparsegroup lasso.
Technical report, Department of Statis-tics, Stanford University.Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-man.
2009.
The elements of statistical learning, vol-ume 2.
Springer.Magnus R. Hestenes.
1969.
Multiplier and gradientmethods.
Journal of Optimization Theory and Appli-cations, 4:303?-320.Arthur E. Hoerl and Robert W. Kennard.
1970.
Ridgeregression: Biased estimation for nonorthogonal prob-lems.
Technometrics, 12(1):55?67.Laurent Jacob, Guillaume Obozinski, and Jean-PhilippeVert.
2009.
Group Lasso with Overlap and GraphLasso.
In Proceedings of the 26th International Con-ference on Machine Learning, ICML ?09, pages 433?440.Rodolphe Jenatton, Julien Mairal, Francis Bach, andGuillaume Obozinski.
2010.
Proximal methods forsparse hierarchical dictionary learning.
In Proceed-ings of the 27th International Conference on MachineLearning, ICML ?10, pages 487?494.Rodolphe Jenatton, Jean-Yves Audibert, and FrancisBach.
2011.
Structured variable selection withsparsity-inducing norms.
Journal of Machine Learn-ing Research, 12:2777?2824.Seyoung Kim and Eric P. Xing.
2010.
Tree-guided grouplasso for multi-task regression with structured sparsity.In Proceedings of the 27th International Conferenceon Machine Learning, ICML ?10, pages 543?550.Eyal Krupka and Naftali Tishby.
2007.
IncorporatingPrior Knowledge on Features into Learning.
In Pro-ceedings of the 11th International Conference on Arti-ficial Intelligence and Statistics, volume 2 of AISTATS?07, pages 227?234.Jun Liu and Jieping Ye.
2010.
Moreau-Yosida Regular-ization for Grouped Tree Structure Learning.
In Ad-vances in Neural Information Processing Systems 23,NIPS ?10, pages 1459?1467.Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and MaosongSun.
2015.
Topical word embeddings.
In Proceed-ings of the 29th national conference on Artificial intel-ligence, pages 2418?2424.J.
Macqueen.
1967.
Some methods for classification andanalysis of multivariate observations.
In In 5-th Berke-ley Symposium on Mathematical Statistics and Proba-bility, pages 281?297.Julien Mairal, Rodolphe Jenatton, Francis Bach, andGuillaume Obozinski.
2010.
Network flow algorithmsfor structured sparsity.
In Advances in Neural Infor-mation Processing Systems 23, NIPS ?10, pages 1558?1566.Fragkiskos D. Malliaros and Konstantinos Skianis.
2015.Graph-based term weighting for text categorization.In Proceedings of the 2015 IEEE/ACM InternationalConference on Advances in Social Networks Analysisand Mining 2015, pages 1473?1479.G.J.
McLachlan and K.E.
Basford.
1988.
Mixture Mod-els: Inference and Applications to Clustering.
MarcelDekker, New York.Rada Mihalcea and Paul Tarau.
2004.
Textrank: Bring-ing order into texts.
In Proceedings of the 2004 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?04, pages 404?411.Tomas Mikolov, Kai Chen, Greg S. Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In Proceedings of Workshopat International Conference on Learning Representa-tions, ICLR ?13.Mandar Mitra, Chris Buckley, Amit Singhal, and ClaireCardie.
1997.
An Analysis of Statistical and SyntacticPhrases.
In Proceedings of the 5th International Con-ference on Computer-Assisted Information Retrieval,volume 97 of RIAO ?97, pages 200?214.Bo Pang and Lilian Lee.
2004.
A sentimental educa-tion: sentiment analysis using subjectivity summariza-tion based on minimum cuts.
In Proceedings of the42nd Annual Meeting on Association for Computa-tional Linguistics, ACL ?04, pages 271?278.M.
J. D. Powell.
1969.
A method for nonlinear con-straints in minimization problems.
R. Fletcher editor,Optimization, pages 283?-298.Rajat Raina, Andrew Y. Ng, and Daphne Koller.
2006.Constructing Informative Priors Using Transfer Learn-ing.
In Proceedings of the 23rd International Con-ference on Machine Learning, ICML ?06, pages 713?720.Radim R?ehu?r?ek and Petr Sojka.
2010.
Software Frame-work for Topic Modelling with Large Corpora.
In Pro-ceedings of the LREC 2010 Workshop on New Chal-lenges for NLP Frameworks, pages 45?50.Fran?ois Rousseau and Michalis Vazirgiannis.
2013.Graph-of-word and tw-idf: New approach to ad hocir.
In Proceedings of the 22nd ACM international con-ference on Information and knowledge management,CIKM ?13, pages 59?68.Ted Sandler, John Blitzer, Partha P. Talukdar, and Lyle H.Ungar.
2009.
Regularized learning with networks offeatures.
In Advances in Neural Information Process-ing Systems 22, NIPS ?09, pages 1401?1408.1836Mark W. Schmidt and Kevin Murphy.
2010.
Convexstructure learning in log-linear models: Beyond pair-wise potentials.
In Proceedings of the 13th Interna-tional Conference on Artificial Intelligence and Statis-tics, AISTATS ?10, pages 709?716.
JMLR Workshopand Conference Proceedings.Mark W. Schmidt, Glenn Fung, and R?mer Rosales.2007.
Fast optimization methods for L1 regulariza-tion: A comparative study and two new approaches.In Proceedings of the 18th European Conference onMachine Learning, ECML ?07, pages 286?297.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Getout the vote: Determining support or opposition fromCongressional floor-debate transcripts.
In Proceed-ings of the 2006 Conference on Empirical Methodsin Natural Language Processing, EMNLP ?06, pages327?335.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety.
Series B (Methodological), pages 267?288.Vladimir Naumovich Vapnik.
1991.
Principles of RiskMinimization for Learning Theory.
In Advances inNeural Information Processing Systems 4, NIPS ?91,pages 831?838.Jierui Xie, Stephen Kelley, and Boleslaw K. Szyman-ski.
2013.
Overlapping community detection in net-works: The state-of-the-art and comparative study.ACM Computing Surveys, 45(4):43:1?43:35.Dani.
Yogatama and Noah A. Smith.
2014a.
Linguisticstructured sparsity in text categorization.
In Proceed-ings of the 52nd Annual Meeting of the Association forComputational Linguistics, ACL ?14, pages 786?796.Dani Yogatama and Noah A. Smith.
2014b.
Making themost of bag of words: Sentence regularization with al-ternating direction method of multipliers.
In Proceed-ings of the 31st International Conference on MachineLearning, volume 32 of ICML ?14, pages 656?664.Ming Yuan and Yi Lin.
2006.
Model selection and es-timation in regression with grouped variables.
Jour-nal of the Royal Statistical Society.
Series B (StatisticalMethodology), 68(1):49?67.Lei Yuan, Jun Liu, and Jieping Ye.
2011.
Efficientmethods for overlapping group lasso.
In Advances inNeural Information Processing Systems 24, NIPS ?11,pages 352?360.Omar Zaidan and Jason Eisner.
2008.
Modeling annota-tors: A generative approach to learning from annotatorrationales.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,EMNLP ?08, pages 31?40.1837
