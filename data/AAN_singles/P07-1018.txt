Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 136?143,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsAssisting Translators in Indirect Lexical TransferBogdan Babych, Anthony Hartley, Serge SharoffCentre for Translation StudiesUniversity of Leeds, UK{b.babych,a.hartley,s.sharoff}@leeds.ac.ukOlga MudrayaDepartment of LinguisticsLancaster University, UKo.mudraya@lancs.ac.ukAbstractWe present the design and evaluation of atranslator?s amenuensis that uses compa-rable corpora to propose and rank non-literal solutions to the translation of expres-sions from the general lexicon.
Using dis-tributional similarity and bilingual diction-aries, the method outperforms establishedtechniques for extracting translationequivalents from parallel corpora.
The in-terface to the system is available at:http://corpus.leeds.ac.uk/assist/v05/1 IntroductionThis paper describes a system designed to assisthumans in translating expressions that do not nec-essarily have a literal or compositional equivalentin the target language (TL).
In the spirit of (Kay,1997), it is intended as a translator's amenuensis"under the tight control of a human translator ?
tohelp increase his productivity and not to supplant him".One area where human translators particularlyappreciate assistance is in the translation of expres-sions from the general lexicon.
Unlike equivalenttechnical terms, which generally share the samepart-of-speech (POS) across languages and are inthe ideal case univocal, the contextually appropri-ate equivalents of general language expressions areoften indirect and open to variation.
While thetransfer module in RBMT may acceptably under-generate through a many-to-one mapping betweensource and target expressions, human translators,even in non-literary fields, value legitimate varia-tion.
Thus the French expression il faillit ?chouer(lit.
: he faltered to fail) may be variously renderedas he almost/nearly/all but failed; he was on theverge/brink of failing/failure; failure loomed.
Allof these translations are indirect in that they in-volve lexical shifts or POS transformations.Finding such translations is a hard task that canbenefit from automated assistance.
'Mining' suchindirect equivalents is difficult, precisely becauseof the structural mismatch, but also because of thepaucity of suitable aligned corpora.
The approachadopted here includes the use of comparable cor-pora in source and target languages, which arerelatively easy to create.
The challenge is to gener-ate a list of usable solutions and to rank them suchthat the best are at the top.Thus the present system is unlike SMT (Och andNey, 2003), where lexical selection is effected by atranslation model based on aligned, parallel cor-pora, but the novel techniques it has developed areexploitable in the SMT paradigm.
It also differsfrom now traditional uses of comparable corporafor detecting translation equivalents (Rapp, 1999)or extracting terminology (Grefenstette, 2002),which allows a one-to-one correspondence irre-spective of the context.
Our system addresses diffi-culties in expressions in the general lexicon, whosetranslation is context-dependent.The structure of the paper is as follows.
In Sec-tion 2 we present the method we use for miningtranslation equivalents.
In Section 3 we present theresults of an objective evaluation of the quality ofsuggestions produced by the system by comparingour output against a parallel corpus.
Finally, inSection 4 we present a subjective evaluation focus-ing on the integration of the system into the work-flow of human translators.2 MethodologyThe software acts as a decision support system fortranslators.
It integrates different technologies for136extracting indirect translation equivalents fromlarge comparable corpora.
In the following subsec-tions we give the user perspective on the systemand describe the methodology underlying each ofits sub-tasks.2.1 User perspectiveUnlike traditional dictionaries, the system is adynamic translation resource in that it can success-fully find translation equivalents for units whichhave not been stored in advance, even for idiosyn-cratic multiword expressions which almost cer-tainly will not figure in a dictionary.
While oursystem can rectify gaps and omissions in staticlexicographical resources, its major advantage isthat it is able to cope with an open set of transla-tion problems, searching for translation equivalentsin comparable corpora in runtime.
This makes itmore than just an extended dictionary.Contextual descriptorsFrom the user perspective the system extracts indi-rect translation equivalents as sets of contextualdescriptors ?
content words that are lexically cen-tral in a given sentence, phrase or construction.The choice of these descriptors may determine thegeneral syntactic perspective of the sentence andthe use of supporting lexical items.
Many transla-tion problems arise from the fact that the mappingbetween such descriptors is not straightforward.The system is designed to find possible indirectmappings between sets of descriptors and to verifythe acceptability of the mapping into the TL.
Forexample, in the following Russian sentence, thebolded contextual descriptors require indirecttranslation into English.????
????????
?????
???????????-??????
????
?, ?
???????
???????????????
????????????
(Children attend badly repaired schools, inwhich [it] is missing the most necessary)Combining direct translation equivalents ofthese words (e.g., translations found in the OxfordRussian Dictionary ?
ORD) may produce a non-natural English sentence, like the literal translationgiven above.
In such cases human translators usu-ally apply structural and lexical transformations,for instance changing the descriptors?
POS and/orreplacing them with near-synonyms which fit to-gether in the context of a TL sentence (Munday,2001: 57-58).
Thus, a structural transformation of?????
?????????????????
(badly repaired) maygive in poor repair while a lexical transformationof ?????????
??????
????????????
([it] is missingthe most necessary) gives lacking basic essentials.Our system models such transformations of thedescriptors and checks the consistency of the re-sulting sets in the TL.Using the systemHuman translators submit queries in the form ofone or more SL descriptors which in their opinionmay require indirect translation.
When the transla-tors use the system for translating into their nativelanguage, the returned descriptors are usually suf-ficient for them to produce a correct TL construc-tion or phrase around them (even though the de-scriptors do not always form a naturally soundingexpression).
When the translators work into a non-native language, they often find it useful to gener-ate concordances for the returned descriptors toverify their usage within TL constructions.For example, for the sentence above translatorsmay submit two queries: ?????
????????-?????????
(badly repaired) and ?????????????????????
(missing necessary).
For the firstquery the system returns a list of descriptor pairs(with information on their frequency in the Englishcorpus) ranked by distributional proximity to theoriginal query, which we explain in Section 2.2.
Atthe top of the list come:bad repair = 30  (11.005)bad maintenance = 16  (5.301)bad restoration = 2  (5.079)poor repair = 60  (5.026)?Underlined hyperlinks lead translators to actualcontexts in the English corpus, e.g., poor repairgenerates a concordance containing a desirable TLconstruction which is a structural transformation ofthe SL query:in such a poor state of repairbridge in as poor a state of repair as the highwaysbuilding in poor repair.dwellings are in poor repair;Similarly, the result of the second query maygive the translators an idea about possible lexicaltransformation:missing need = 14  (5.035)important missing = 8 (2.930)missing vital = 8  (2.322)lack necessary = 204  (1.982)?essential lack = 86  (0.908)?137The concordance for the last pair of descriptorscontains the phrase they lack the three essentials,which illustrates the transformation.
The resultingtranslation may be the following:Children attend schools that are in poor re-pair and lacking basic essentialsThus our system supports translators in makingdecisions about indirect translation equivalents in anumber of ways: it suggests possible structural andlexical transformations for contextual descriptors;it verifies which translation variants co-occur inthe TL corpus; and it illustrates the use of thetransformed TL lexical descriptors in actual con-texts.2.2 Generating translation equivalentsWe have generalised the method used in our previ-ous study (Sharoff et al, 2006) for extractingequivalents for continuous multiword expressions(MWEs).
Essentially, the method expands thesearch space for each word and its dictionary trans-lations with entries from automatically computedthesauri, and then checks which combinations arepossible in target corpora.
These potential transla-tion equivalents are then ranked by their similarityto the original query and presented to the user.
Therange of retrievable equivalents is now extendedfrom a relatively limited range of two-word con-structions which mirror POS categories in SL andTL to a much wider set of co-occurring lexicalcontent items, which may appear in a different or-der, at some distance from each other, and belongto different POS categories.The method works best for expressions from thegeneral lexicon, which do not have establishedequivalents, but not yet for terminology.
It relieson a high-quality bilingual dictionary (en-ru ~30k,ru-en ~50K words, combining ORD and the corepart of Multitran) and large comparable corpora(~200M En, ~70M Ru) of news texts.For each of the SL query terms q the systemgenerates its dictionary translation Tr(q) and itssimilarity class S(q) ?
a set of words with a similardistribution in a monolingual corpus.
Similarity ismeasured as the cosine between collocation vec-tors, whose dimensionality is reduced by SVD us-ing the implementation by Rapp (2004).
The de-scriptor and each word in the similarity class arethen translated into the TL using ORD or the Mul-titran dictionary, resulting in {Tr(q)?
Tr(S(q))}.On the TL side we also generate similarity classes,but only for dictionary translations of query termsTr(q) (not for Tr(S(q)), which can make output toonoisy).
We refer to the resulting set of TL words asa translation class T.T = {Tr(q) ?
Tr(S(q)) ?
S(Tr(q))}Translation classes approximate lexical andstructural transformations which can potentially beapplied to each of the query terms.
Automaticallycomputed similarity classes do not require re-sources like WordNet, and they are much moresuitable for modelling translation transformations,since they often contain a wider range of words ofdifferent POS which share the same context, e.g.,the similarity class of the word lack contains wordssuch as absence, insufficient, inadequate, lost,shortage, failure, paucity, poor, weakness, inabil-ity, need.
This clearly goes beyond the range oftraditional thesauri.For multiword queries, the system performs aconsistency check on possible combinations ofwords from different translation classes.
In particu-lar, it computes the Cartesian product for pairs oftranslation classes T1 and T2 to generate the set Pof word pairs, where each word (w1 and w2) comesfrom a different translation class:P = T1 ?
T2 = {(w1, w2) | w1 ?
T1 and w2 ?
T2}Then the system checks whether each word pairfrom the set P exists in the database D of discon-tinuous content word bi-grams which actually co-occur in the TL corpus:P?
= P ?
DThe database contains the set of all bi-grams thatoccur in the corpus with a frequency ?
4 within awindow of 5 words (over 9M bigrams for eachlanguage).
The bi-grams in D and in P are sortedalphabetically, so their order in the query is notimportant.Larger N-grams (N > 2) in queries are split intocombinations of bi-grams, which we found to bean optimal solution to the problem of the scarcityof higher order N-grams in the corpus.
Thus, forthe query gain significant importance the systemgenerates P?1(significant importance), P?2(gain impor-tance), P?3(gain significant) and computes P?
as:P?
= {(w1,w2,w3)| (w1,w2) ?
P?1 & (w1, w3) ?
P?2& (w2,w3) ?
P?3 },which allows the system to find an indirect equiva-lent ????????
???????
????????
(lit.
: receiveweighty meaning).138Even though P?
on average contains about 2% -4% of the theoretically possible number of bi-grams present in P, the returned number of poten-tial translation equivalents may still be large andcontain much noise.
Typically there are severalhundred elements in P?, of which only a few arereally useful for translation.
To make the systemusable in practice, i.e., to get useful solutions toappear close to the top (preferably on the firstscreen of the output), we developed methods ofranking and filtering the returned TL contextualdescriptor pairs, which we present in the followingsections.2.3 Hypothesis rankingThe system ranks the returned list of contextualdescriptors by their distributional proximity to theoriginal query, i.e.
it uses scores cos(vq, vw) gener-ated for words in similarity classes ?
the cosine ofthe angle between the collocation vector for a wordand the collocation vector for the query or diction-ary translation of the query.
Thus, words whoseequivalents show similar usage in a comparablecorpus receive the highest scores.
These scores arecomputed for each individual word in the output,so there are several ways to combine them toweight words in translation classes and word com-binations in the returned list of descriptors.We established experimentally that the best wayto combine similarity scores is to multiply weightsW(T) computed for each word within its translationclass T. The weight W(P?
(w1,w2)) for each pair ofcontextual descriptors (w1, w2)?P?
is computed as:W(P?
(w1,w2)) = W(T(w1)) ?
W(T(w2));Computing W(T(w)), however, is not straightfor-ward either, since some words in similarity classesof different translation equivalents for the queryterm may be the same, or different words from thesimilarity class of the original query may have thesame translation.
Therefore, a word w within atranslation class may have come by several routessimultaneously, and may have done that severaltimes.
For each word w in T there is a possibilitythat it arrived in T either because it is in Tr(q) oroccurs   n times in Tr(S(q)) or k times in S(Tr(q)).We found that the number of occurrences n andk of each word w in each subset gives valuable in-formation for ranking translation candidates.
In ourexperiments we computed the weight W(T) as thesum of similarity scores which w receives in eachof the subsets.
We also discovered that rankingimproves if for each query term we compute inaddition a larger (and potentially noisy) space ofcandidates that includes TL similarity classes oftranslations of the SL similarity class S(Tr(S(q))).These candidates do not appear in the system out-put, but they play an important role in ranking thedisplayed candidates.
The improvement may bedue to the fact that this space is much larger, andmay better support relevant candidates since thereis a greater chance that appropriate indirect equiva-lents are found several times within SL and TLsimilarity classes.
The best ranking results wereachieved when the original W(T) scores were mul-tiplied by 2 and added to the scores for the newlyintroduced similarity space S(Tr(S(q))):W(T(w))= 2?
(1 if w?Tr(q) )+2??
( cos(vq, vTr(w)) | {w | w?
Tr(S(q)) } ) +2??
( cos(vTr(q), vw) | {w | w?
S(Tr(q)) } ) +?
(cos(vq, vTr(w))?cos (vTr(q), vw) |{w | w?
S(Tr(S(q))) } )For example, the system gives the followingranking for the indirect translation equivalents ofthe Russian phrase ???????
????????
(lit.
: weightymeaning) ?
figures in brackets represent W(P?
)scores for each pair of TL descriptors:1. significant importance = 7 (3.610)2. significant value = 128    (3.211)3. measurable value = 6       (2.657)?8.
dramatic importance = 2    (2.028)9. important significant = 70 (2.014)10. convincing importance = 6 (1.843)The Russian similarity class for ???????
(weighty, ponderous) contains: ????????????
(convincing) (0.469), ????????
(significant)(0.461), ????????
(notable) (0.452) ?????-??????
(dramatic) (0.371).
The equivalent ofsignificant is not at the top of the similarity class ofthe Russian query, but it appears at the top of thefinal ranking of pairs in P?, because this hypothesisis supported by elements of the set formed byS(Tr(S(q))); it appears in similarity classes for no-table (0.353) and dramatic (0.315), which contrib-uted these values to the W(T) score of significant:W(T(significant)) =2 ?
(Tr(????????
)=significant (0.461))+ (Tr(????????
)=notable (0.452)?
S(notable)=significant (0.353))+ (Tr(???????????
)=dramatic (0.371)?
S(dramatic)= significant (0.315))The word dramatic itself is not usable as atranslation equivalent in this case, but its similarity139class contains the support for relevant candidates,so it can be viewed as useful noise.
On the otherhand, the word convincing does not receive suchsupport from the hypothesis space, even though itsRussian equivalent is ranked higher in the SL simi-larity class.2.4 Semantic filteringRanking of translation candidates can be furtherimproved when translators use an option to filterthe returned list by certain lexical criteria, e.g., todisplay only those examples that contain a certainlexical item, or to require one of the items to be adictionary translation of the query term.
However,lexical filtering is often too restrictive: in manycases translators need to see a number of relatedwords from the same semantic field or subject do-main, without knowing the lexical items in ad-vance.
In this section we present the semantic fil-ter, which is based on Russian and English seman-tic taggers which use the same semantic field tax-onomy for both languages.The semantic filter displays only those itemswhich have specified semantic field tags or tagcombinations; it can be applied to one or bothwords in each translation hypothesis in P?.
Thedefault setting for the semantic filter is the re-quirement for both words in the resulting TL can-didates to contain any of the semantic field tagsfrom a SL query term.In the next section we present evaluation resultsfor this default setting (which is applied when theuser clicks the Semantic Filter button), but humantranslators have further options ?
to filter by tagsof individual words, to use semantic classes fromSL or TL terms, etc.For example, applying the default semantic filterfor the output of the query ?????
???????-??????????
(badly repaired) removes the high-lighted items from the list:1. bad repair = 30       (11.005)[2. good repair = 154     (8.884) ]3. bad rebuild = 6       (5.920)[4. bad maintenance = 16  (5.301) ]5. bad restoration = 2   (5.079)6. poor repair = 60      (5.026)[7. good rebuild = 38     (4.779) ]8. bad construction = 14 (4.779)Items 2 and 7 are generated by the system be-cause good, well and bad are in the same similar-ity cluster for many words (they often share thesame collocations).
The semantic filter removesexamples with good and well on the grounds thatthey do not have any of the tags which come fromthe word ?????
(badly): in particular, instead oftag A5?
(Evaluation: Negative) they have tag A5+(Evaluation: Positive).
Item 4 is removed on thegrounds that the words ?????????????????
(repaired) and maintenance do not have any tagsin common ?
they appear ontologically too farapart from the point of view of the semantic tagger.The core of the system?s multilingual semantictagging is a knowledge base in which single wordsand MWEs are mapped to their potential semanticfield categories.
Often a lexical item is mapped tomultiple semantic categories, reflecting its poten-tial multiple senses.
In such cases, the tags are ar-ranged by the order of likelihood of meanings,with the most prominent first.3 Objective evaluationIn the objective evaluation we tested the perform-ance of our system on a selection of indirect trans-lation problems, extracted from a parallel corpusconsisting mostly of articles from English andRussian newspapers (118,497 words in the R-Edirection, 589,055 words in the E-R direction).
Ithas been aligned on the sentence level by JAPA(Langlais et al, 1998), and further on the wordlevel by GIZA++ (Och and Ney, 2003).3.1 Comparative performanceThe intuition behind the objective evaluationexperiment is that the capacity of our tool to findindirect translation equivalents in comparable cor-pora can be compared with the results of automaticalignment of parallel texts used in translation mod-els in SMT: one of the major advantages of theSMT paradigm is its ability to reuse indirectequivalents found in parallel corpora (equivalentsthat may never come up in hand-crafted dictionar-ies).
Thus, automatically generated GIZA++ dic-tionaries with word alignment contain many exam-ples of indirect translation equivalents.We use these dictionaries to simulate the genera-tor of translation classes T, which we recombine toconstruct their Cartesian product P, similarly to theprocedure we use to generate the output of our sys-tem.
However, the two approaches generate indi-rect translation equivalence hypotheses on the ba-sis of radically different material: the GIZA dic-tionary uses evidence from parallel corpora of ex-140isting human translations, while our system re-combines translation candidates on the basis oftheir distributional similarity in monolingual com-parable corpora.
Therefore we took GIZA as abaseline.Translation problems for the objective evalua-tion experiment were manually extracted from twoparallel corpora: a section of about 10,000 wordsof a corpus of English and Russian newspapers,which we also used to train GIZA, and a section ofthe same length from a corpus of interviews pub-lished on the Euronews.net website.We selected expressions which representedcases of lexical transformations (as illustrated inSection 0), containing at least two content wordsboth in the SL and TL.
These expressions wereconverted into pairs of contextual descriptors ?e.g., recent success, reflect success ?
and submit-ted to the system and to the GIZA dictionary.
Wecompared the ability of our system and of GIZA tofind indirect translation equivalents which matchedthe equivalents used by human translators.
Theoutput from both systems was checked to seewhether it contained the contextual descriptorsused by human translators.
We submitted 388 pairsof descriptors extracted from the newspaper trans-lation corpus and 174 pairs extracted from the Eu-ronews interview corpus.
Half of these pairs wereRussian, and the other half English.We computed recall figures for 2-word combi-nations of contextual descriptors and single de-scriptors within those combinations.
We also showthe recall of translation variants provided by theORD on this data set.
For example, for the query?????????
????????????
([it] is missing neces-sary [things]) human translators give the solutionlacking essentials; the lemmatised descriptors arelack and essential.
ORD returns direct translationequivalents missing and necessary.
The GIZA dic-tionary in addition contains several translationequivalents for the second term (with alignmentprobabilities) including: necessary ~0.332, need~0.226, essential ~0.023.
Our system returns bothdescriptors used in human translation as a pair ?lack essential (ranked 41 without filtering and 22with the default semantic filter).
Thus, for a 2-wordcombination of the descriptors only the output ofour system matched the human solution, which wecounted as one hit for the system and no hits forORD or GIZA.
For 1-word descriptors we counted2 hits for our system (both words in the humansolution are matched), and 1 hit for GIZA ?
itmatches the word essential ~0.023 (which also il-lustrates its ability to find indirect translationequivalents).2w descriptors 1w descriptorsnews interv news intervORD 6.7% 4.6% 32.9% 29.3%GIZA++ 13.9% 3.4% 35.6% 29.0%Our system 21.9% 19.5% 55.8% 49.4%Table 1 Conservative estimate of recallIt can be seen from Table 1 that for the newspa-per corpus on which it was trained, GIZA covers awider set of indirect translation variants than ORD.But our recall is even better both for 2-word and 1-word descriptors.However, note that GIZA?s ability to retrievefrom the newspaper corpus certain indirect transla-tion equivalents may be due to the fact that it haspreviously seen them frequently enough to gener-ate a correct alignment and the corresponding dic-tionary entry.The Euronews interview corpus was not used fortraining GIZA.
It represents spoken language andis expected to contain more ?radical?
transforma-tions.
The small decline in ORD figures here canbe attributed to the fact that there is a difference ingenre between written and spoken texts and conse-quently between transformation types in them.However, the performance of GIZA drops radi-cally on unseen text and becomes approximatelythe same as the ORD.This shows that indirect translation equivalentsin the parallel corpus used for training GIZA aretoo sparse to be learnt one by one and successfullyapplied to unseen data, since solutions which fitone context do not necessarily suit others.The performance of our system stays at aboutthe same level for this new type of text; the declinein its performance is comparable to the decline inORD figures, and can again be explained by thedifferences in genre.3.2 Evaluation of hypothesis rankingAs we mentioned, correct ranking of translationcandidates improves the usability of the system.Again, the objective evaluation experiment givesonly a conservative estimate of ranking, becausethere may be many more useful indirect solutionsfurther up the list in the output of the system whichare legitimate variants of the solutions found in the141parallel corpus.
Therefore, evaluation figuresshould be interpreted in a comparative rather thenan absolute sense.We use ranking by frequency as a baseline forcomparing the ranking described in Section 2.3 ?by distributional similarity between a candidateand the original query.Table 2 shows the average rank of human solu-tions found in parallel corpora and the recall ofthese solutions for the top 300 examples.
Sincethere are no substantial differences between thefigures for the newspaper texts and for the inter-views, we report the results jointly for 556 transla-tion problems in both selections (lower rank fig-ures are better).Recall Average rank2-word descriptorsfrequency (baseline) 16.7% rank=93.7distributional similarity 19.5% rank=44.4sim.
+ semantic filter 14.4% rank=26.71-word descriptorsfrequency (baseline) 48.2% rank=42.7distributional similarity 52.8% rank=21.6sim.
+ semantic filter 44.1% rank=11.3Table 2 Ranking: frequency, similarity and filterIt can be seen from the table that ranking bysimilarity yields almost a twofold improvement forthe average rank figures compared to the baseline.There is also a small improvement in recall, sincethere are more relevant examples that appearwithin the top 300 entries.The semantic filter once again gives an almosttwofold improvement in ranking, since it removesmany noisy items.
The average is now within thetop 30 items, which means that there is a highchance that a translation solution will be displayedon the first screen.
The price for improved rankingis decline in recall, since it may remove some rele-vant lexical transformations if they appear to beontologically too far apart.
But the decline issmaller: about 26.2% for 2-word descriptors and16.5% for 1-word descriptors.
The semantic filteris an optional tool, which can be used to great ef-fect on noisy output: its improvement of rankingoutweighs the decline in recall.Note that the distribution of ranks is not normal,so in Figure 1 we present frequency polygons forrank groups of 30 (which is the number of itemsthat fit on a single screen, i.e., the number of itemsin the first group (r030) shows solutions that willbe displayed on the first screen).
The majority ofsolutions ranked by similarity appear high in thelist (in fact, on the first two or three screens).010203040506070r030r060r090r120r150r180r210r240r270r300similarityfrequencyFigure 1 Frequency polygons for ranks4 Subjective evaluationThe objective evaluation reported above uses asingle reference translation and is correspondinglyconservative in estimating the coverage of the sys-tem.
However, many expressions studied havemore than one fluent translation.
For instance, inpoor repair is not the only equivalent for the Rus-sian expression ?????
?????????????????.
It isalso possible to translate it as unsatisfactory condi-tion, bad state of repair, badly in need of repair,and so on.
The objective evaluation shows that thesystem has been able to find the suggestion usedby a particular translator for the problem studied.
Itdoes not tell us whether the system has found someother translations suitable for the context.
Suchlegitimate translation variation implies that the per-formance of a system should be studied on the ba-sis of multiple reference translations, though typi-cally just two reference translations are used (Pap-ineni, et al 2001).
This might be enough for thepurposes of a fully automatic MT tool, but in thecontext of a translator's amanuensis which dealswith expressions difficult for human translators, itis reasonable to work with a larger range of ac-ceptable target expressions.With this in mind we evaluated the performanceof the tool with a panel of 12 professional transla-tors.
Problematic expressions were highlighted andthe translators were asked to find suitable sugges-tions produced by the tool for these expressionsand rank their usability on a scale from 1 to 5 (notacceptable to fully idiomatic, so 1 means that nousable translation was found at all).Sentences themselves were selected from prob-lems discussed on professional translation forumsproz.com and forum.lingvo.ru.
Given the range ofcorpora used in the system (reference and newspa-142per corpora), the examples were filtered to addressexpressions used in newspapers.The goal of the subjective evaluation experimentwas to establish the usefulness of the system fortranslators beyond the conservative estimate givenby the objective evaluation.
The intuition behindthe experiment is that if there are several admissi-ble translations for the SL contextual descriptors,and system output matches any of these solutions,then the system has generated something useful.Therefore, we computed recall on sets of humansolutions rather than on individual solutions.
Wematched 210 different human solutions to 36 trans-lation problems.
To compute more realistic recallfigures, we counted cases when the system outputmatches any of the human solutions in the set.Table 3 compares the conservative estimate of theobjective evaluation and the more realistic estimateon a single data set.2w default 2w with sem filtConservative  32.4%; r=53.68 21.9%; r=34.67Realistic 75.0%;   r=7.48 61.1%;   r=3.95Table 3 Recall and rank for 2-word descriptorsSince the data set is different, the figures for theconservative estimate are higher than those for theobjective evaluation data set.
However, the tableshows the there is a gap between the conservativeestimate and the realistic coverage of the transla-tion problems by the system, and that real coverageof indirect translation equivalents is potentiallymuch higher.Table 4 shows averages (and standard deviation?)
of the usability scores divided in four groups: (1)solutions that are found both by our system and theORD; (2) solutions found only by our system; (3)solutions found only by ORD (4) solutions foundby neither:system (+) system (?
)ORD (+) 4.03 (0.42) 3.62 (0.89)ORD (?)
4.25 (0.79) 3.15 (1.15)Table 4 Human scores and ?
for system outputIt can be seen from the table that human users findthe system most useful for those problems wherethe solution does not match any of the direct dic-tionary equivalents, but is generated by the system.5 ConclusionsWe have presented a method of finding indirecttranslation equivalents in comparable corpora, andintegrated it into a system which assists translatorsin indirect lexical transfer.
The method outper-forms established methods of extracting indirecttranslation equivalents from parallel corpora.We can interpret these results as an indicationthat our method, rather than learning individualindirect transformations, models the entire familyof transformations entailed by indirect lexicaltransfer.
In other words it learns a translation strat-egy which is based on the distributional similarityof words in a monolingual corpus, and applies thisstrategy to novel, previously unseen examples.The coverage of the tool and additional filteringtechniques make it useful for professional transla-tors in automating the search for non-trivial, indi-rect translation equivalents, especially equivalentsfor multiword expressions.ReferencesGregory Grefenstette.
2002.
Multilingual corpus-basedextraction and the very large lexicon.
In: Lars Borin,editor, Language and Computers, Parallel corpora,parallel worlds, pages 137-149.
Rodopi.Martin Kay.
1997.
The proper place of men and ma-chines in language translation.
Machine Translation,12(1-2):3-23.Philippe Langlais, Michel Simard, and Jean V?ronis.1998.
Methods and practical issues in evaluatingalignment techniques.
In Proc.
Joint COLING-ACL-98, pages 711-717.Jeremy Munday.
2001.
Introducing translation studies.Theories and Applications.
Routledge, New York.Franz Josef Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19-51.Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J.(2001).
Bleu: a method for automatic evaluation ofmachine translation, RC22176 W0109-022: IBM.Reinhard Rapp.
1999.
Automatic identification of wordtranslations from unrelated English and German cor-pora.
In Procs.
the 37th ACL, pages 395-398.Reinhard Rapp.
2004.
A freely available automaticallygenerated thesaurus of related words.
In Procs.
LREC2004, pages 395-398, Lisbon.Serge Sharoff, Bogdan Babych and Anthony Hartley2006.
Using Comparable Corpora to Solve ProblemsDifficult for Human Translators.
In: Proceedings ofthe COLING/ACL 2006 Main Conference PosterSessions, pp.
739-746.143
