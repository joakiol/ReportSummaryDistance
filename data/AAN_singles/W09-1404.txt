Proceedings of the Workshop on BioNLP: Shared Task, pages 28?36,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUZurich in the BioNLP 2009 Shared TaskKaarel KaljurandInstitute ofComputational LinguisticsUniversity of ZurichSwitzerlandkalju@cl.uzh.chGerold SchneiderInstitute ofComputational LinguisticsUniversity of ZurichSwitzerlandgschneid@cl.uzh.chFabio Rinaldi?Institute ofComputational LinguisticsUniversity of ZurichSwitzerlandrinaldi@cl.uzh.chAbstractWe describe a biological event detectionmethod implemented for the BioNLP 2009Shared Task 1.
The method relies entirely onthe chunk and syntactic dependency relationsprovided by a general NLP pipeline which wasnot adapted in any way for the purposes ofthe shared task.
The method maps the syn-tactic relations to event structures while be-ing guided by the probabilities of the syntacticfeatures of events which were automaticallylearned from the training data.
Our methodachieved a recall of 26% and a precision of44% in the official test run, under ?strict equal-ity?
of events.1 IntroductionThis paper describes the adaptation of an existingtext mining system to the BioNLP shared task.
Thesystem has been originally created for participationin the BioCreative1 protein-protein interaction task(Rinaldi et al, 2008) and further developed for aninternal project based on the IntAct dataset of pro-tein interactions (Kerrien et al, 2006).
We decidedto participate only in Task 1 of the BioNLP sharedtask, mainly because of lack of time and resources.Our event annotation method relied on variouspreprocessing steps and an existing state of the artdependency parser, which provided the input to theevent annotator.
As all the linguistic processing wasperformed by the preprocessor and the parser, theideas implemented for the event annotator could re-main simple while still producing reasonable results.
?Corresponding author1http://www.biocreative.org/Thus, the event annotator performed a straightfor-ward rewriting of syntactic structures to event struc-tures, guided by the information on the syntacticnature of events that we obtained from the train-ing data.
In this sense our system can be used asa reference for a comparison to other systems thatrely completely on a dependency parser deliveredanalysis that is rewritten into event structures usingknowledge gained from the training data.Our system consists of a preprocessing phase thatuses a pipeline of NLP tools, described in section 2of this paper.
Linguistic resources are learned auto-matically from the preprocessed training data (sec-tion 3).
A Prolog-implemented event generator isapplied directly to the preprocessing results and isguided by the relative frequencies of syntactic fea-tures provided in the resources (section 4).
Thisis followed by a postprocessing step that removessome unlikely event structures, makes sure that allevents that violate the well-formedness rules are fil-tered out, and finally serializes the event structuresinto the requested output format.
In section 5 wepresent an illustrative example of the events gener-ated by this approach and discuss some implicationsof the event model adopted in the shared task.
Insection 6, we describe the evaluation that we per-formed during the training period, the final officialresults on the test data, and some alternative evalu-ations performed in parallel to the official one.
Insection 7 we draw conclusions and describe futurework.2 PreprocessingAside from a format conversion step necessary todeal with the data provided by the shared task, the28preprocessing phase is largely based on an existingpipeline of NLP tools, that we have developed in theOntoGene project2 (Rinaldi et al, 2006; Rinaldi etal., 2008).2.1 Tokenization, sentence splitting,part-of-speech taggingFor tokenization, sentence splitting, and part-of-speech (POS) tagging we used LingPipe3.
Ling-Pipe produces very granular tokens by default, e.g.a character sequence from abstract 10395645caspase-3-like (CPP32/Yama/apopain)which contains multiple hyphens and slashes (asusual for biomedical texts) is split into 12 (ratherthan just 4) tokenscaspase, -, 3, -, like, (, CPP32, /, Yama, /,apopain, )allowing a more detailed detection of terms(shown in boldface in the examples) and trigger-words which would stay token-internal if a less gran-ular tokenization was used.The models used for sentence splitting and POS-tagging come with the LingPipe distribution and aretrained on the GENIA corpus (Kim et al, 2003),thus providing a biomedical text aware sentencesplitting and POS-tagging.2.2 Term annotationCorrectly detecting multi-word terms in the text cansubstantially improve the parsing results, becauselong noun sequences would be grouped together andthe parser can only focus on the heads of the groupsand ignore the rest.
In this task, however, we de-cided to keep things simple and rely on chunking asthe only means of noun grouping.Thus, we only annotated the terms provided bythe task organizers in the a1-files (i.e.
protein men-tions).
We made the assumption that terms are se-quences of tokens as defined by the LingPipe tok-enizer.
Whereas in the vast majority of cases this co-incides with the tokenization used by the organizers,there are 10 cases in the training data where this as-sumption is violated (e.g.
?IkappaB-alphaS32/36A?2http://www.ontogene.org/3http://alias-i.com/lingpipe/contains the term ?IkappaB-alpha?
but according toLingPipe, the tokens are ?IkappaB?, ?-?, ?alphaS32?,?/?, ?36A?
).As the last step of term annotation, we recon-nected tokens which were separated by hyphens andslashes, unless the tokens were part of terms.
Thisallowed for a more reliable processing with toolswhich are not optimized to deal with symbols likehyphens and slashes if these are padded with white-space.2.3 Lemmatization using MorphaLemmatization was performed using Morpha (Min-nen et al, 2001), which provides an accurate lemma-tization given that the input contains part-of-speechinformation.
We used the lemma information even-tually only as part of the input to the dependencyparser, i.e.
for the other aspects of event annotationlemmas were ignored.2.4 Chunking using LTCHUNKChunking can considerably reduce parsing com-plexity, while hardly affecting performance (Prins,2005).
In order to group contiguous sequences ofnouns and verbs, we used LTCHUNK (Mikheev,1997).
LTCHUNK annotates all noun and verbgroups in the sentences.
A chunk is an importantunit in the analysis of biomedical texts.
Consider anNP chunk likeT cell-receptor-induced FasL upregula-tionwhich contains two event triggers, amounting to amention of a complex event.After applying LTCHUNK, we also detectedchunk heads, with a simple algorithm ?
select lastnoun in noun groups, select last verb in verb groups.This selection is done on the basis of POS-tags.2.5 Dependency parsing using Pro3GresPro3Gres (Schneider, 2008) is a robust, deep-syntactic, broad-coverage probabilistic dependencyparser, which identifies grammatical relations be-tween the heads of chunks, including the majorityof long-distance dependencies.
The output is a hi-erarchical structure of relations (represented as thedirected arrows in the example shown in figure 1).29Figure 1: Dependency-syntax tree of the title of abstract 9360945: ?Transcription factor NF-kappaB regulates in-ducible Oct-2 gene expression in precursor B lymphocytes.?
The dependency relations link together the heads of the5 chunks.The parser uses a hand-written grammar express-ing linguistic competence, and a statistical languagemodel that calculates lexicalized attachment proba-bilities, thus expressing linguistic performance.
Theparser expresses distinctions that are especially im-portant for a predicate-argument based deep syntac-tic representation, as far as they are expressed inthe training data generated from the Penn Treebank(Marcus et al, 1993).
This includes prepositionalphrase attachments, control structures, appositions,relative clause anaphora, participles, gerunds, andargument/adjunct distinctions.
The dependency la-bel set is similar to the one used in the Stanfordscheme, the parser achieves state-of-the-art perfor-mance (Haverinen et al, 2008).We have slightly adapted Pro3Gres to the biomed-ical domain.
A class of nouns that varies consider-ably in the biomedical domain are relational nouns.They are syntactically marked because they can haveseveral prepositional phrase arguments.
Biomedicalrelational nouns like ?overexpression?
or ?transcrip-tion?
are absent from the Penn Treebank or rare.
Wehave used an unsupervised approach based on (Hin-dle, D and Rooth, M, 1991) to learn relational nounsfrom Medline.A new relation type, hyph, has been added to con-nect tokens to hyphens and slashes, and thus betterdeal with these characters in biomedical texts.2.6 Preprocessor outputThe preprocessor produces 5 Prolog-formatted filesfor each abstract.
Each of these files is token-centered and affiliates a token ID with a group (ei-ther sentence, chunk, or term) that contains this to-ken, or maps it to a syntactically related (either asthe head or the dependent) token.?
Tokens maps each token to its lemma, POS-tag, and character offsets?
Chunks maps each token to its containingchunk, chunk?s type (noun or verb group), andchunk?s head?
Terms maps each token to its containing term,term?s type, term?s ID (assigned by the a1-file,or the a2-file in case of processing the trainingdata)?
Sentences maps each sentence ID to the list ofIDs of the tokens in the sentence?
Dependencies maps each token to its imme-diate head and dependent, and to the types ofthese dependency relationsThese files are the input to the resource generatordescribed below, and later (together with the gener-ated resources), the input to the event annotator.3 ResourcesThe 800 abstracts of the training data were usedduring development for the generation of three re-sources which are described in this section.
For theofficial testing we used the concatenation of trainingand development data (i.e.
950 abstracts).
The re-sources were generated automatically from the a1-and a2-files; and from the preprocessed version oftxt-, a1- and a2-files.
The resulting data files includefrequencies of the total occurrence of an item (e.g.word, syntactic configuration) and the frequency ofits occurrence in an event.All the words in the resources were lowercasedbut not lemmatized.
Resources were stored asProlog-formatted files.30Frequency Event type Event arguments149 Gene expression Theme(T)28 Transcription Theme(T)2 Localization Theme(T), AtLoc(T)1 Positive regulation Theme(T)1 Positive regulation Theme(E)Table 1: Frequency distribution of the event structuresthat are triggered by the word form ?expressed?
which intotal triggered an event 181 times in the training data.
?T?means that the argument is filled by a term, ?E?
meansthat the argument is filled by an event.3.1 WordsThe word frequencies file provides a simple prob-abilistic model for excluding stopwords, as we ob-served that many different function words some-times triggered events in the training data.
Wewanted to exclude such words to obtain a better pre-cision.
The words-resource can be queried using asimple interfaceword_to_freq(+Word, -F)which maps every word to its frequency.3.2 Event types and argumentsUsing the training data, we created a mapping fromeach candidate trigger-word to the possible eventtypes and the permissible event frames.
A sample ofthis mapping is illustrated in table 1.
The argumentshave a type (e.g.
Theme) but their filler is abstractedto be either ?T?
(for terms) or ?E?
(for events).This resource can be queried via the interfaceeword_to_event(+EventWord,-EventType, -EventArgs, -F1, -F2)which maps every trigger-word to its possibleevent type and arguments.
The returned frequenciesshow how often the event structure was triggeredby the trigger-word, and how often the trigger-wordtriggered an event in total.3.3 Domination paths between termsThe most sophisticated of the resources that wegenerated recorded the syntactic paths between theterms (from a1- and a2-files) observed in the train-ing data, and counted how often these paths werepresent in events, connecting triggers with event ar-gument fillers.
With each term, also its type (e.g.Positive regulation, Protein) was recorded.For the syntactic paths, we only considered dom-ination paths where one of the terms is the head andthe other the dependent, defined as follows.Definition 1 (Domination between chunks)Term t1 dominates term t2 if t1 ?
c1 andt2 ?
c2 and there exists a directed syntactic pathh(c1) ?
.
.
.
?
h(c2), where h(?)
is the head of thegiven chunk.For example, in figure 1, the term ?regulates?dominates all the other tokens, among them the term?expression?
(which is the head of its chunk), and theProtein-term ?Oct-2?.
Note that this definition doesnot require the terms to be in the chunk head posi-tion.
However, this decision did not affect the resultssignificantly.The chunk-internal domination relation is definedfor terms which are chunk-internal and thus ?invisi-ble?
to the dependency parser because the parser ig-nores everything but the head of the chunk.
This re-lation captures the default syntactic dependency be-tween nouns in noun groups where the head nounusually follows its dependents.Definition 2 (Chunk-internal domination) Termt1 dominates term t2 if t1, t2 ?
c and i(t1) > i(t2),where i(?)
is the sequential index of the given termin the chunk.For example, in figure 1, in the 3rd chunk, theterm ?expression?
dominates the terms ?Oct-2?
and?inducible?
; and furthermore, ?Oct-2?
dominates ?in-ducible?.The stored syntactic path is a list of dependencyrelations from the dependent to the head, or anempty list if both terms are in the same chunk.Instead of domination, we also considered usingthe asymmetric relation of ?connectedness?, wheretwo terms are connected if either of the terms dom-inates the other, or if both are dominated by sometoken in the tree.
This relation, however, seemed todecrease precision much more than increase recall.In order to query the domination resource wedesigned a simple query interface that allows forpartially instantiated input.
For example the query(where the underscores denote uninstantiated parts)?- find_path_freq(bind, ?Binding?,_, ?Protein?,[modpp | _ ],F1, F2).31asks how often there is a domination relation be-tween the head term ?bind?
if it has the type Bind-ing and some dependent term with type Protein,such that the dependency path starts with the rela-tion modpp.
The frequency counts resulting fromthis query tell the frequency of this configuration inevents (F1), and in total (F2).
This information al-lows the computation of the conditional probabilityof an argument of an event given the event type, thetrigger-word, the argument word, the argument type,and the syntactic path between trigger and argument.4 Event generationThe event generation relied fully on the syntax treeand chunk information that was delivered by the pre-processing module.
No fall-back to a surface co-occurrence of words was used.
We only consideredwords and structures seen in the training data as pos-sible parts of events.
Such a design entails relativelygood precision at lower recall.For each of the generation steps described below,a probability threshold decided whether to continuethe ?building?
of the event given the trigger-word,the event arguments template or the argument in-stantiation.
The thresholds were set manually aftersome experimentation.
We did not try to automat-ically decide the best performing thresholds.
Deci-sions are taken locally, possibly cutting some localminima.
A simple maximum-likelihood estimation(MLE) approach was used.4.1 Trigger generationTrigger candidates were generated from the tokenlist of each sentence in the analyzed abstract.
Fig-ure 2 shows a browser-based visualization approachthat we created as a support in our work.
In the caseof the training data, the annotations come the a1-and a2-files provided by the organizers.
In the caseof the development and test data, the annotations forthe triggers are those generated by the system.We only considered one-token trigger-words be-cause multi-token triggers were less frequent in thetraining data, where only about 8% of the trigger-word forms contained a space character.
Also, manyof these multiword triggers contain a token that ex-ists as a trigger on its own (e.g.
?transcriptional reg-ulation?
triggers the Regulation-event in the trainingdata, as does ?regulation?
), allowing us to generate asensible event structure even if it does not match agold standard event under the ?strict equality?.
To-kens that had been seen to trigger an event in thetraining data with probability higher than 0.12 wereconsidered further.In MLE terms, we calculate the probability of agiven token to be a trigger as follows:p(Trigger |Token) = f(Token ?
(Token = Trigger))f(Token)(1)4.2 Event type and arguments templategenerationNext, trigger-words were mapped to event type andargument template structures.
In MLE terms, wecalculated the probability of an event structure (i.e.the combination of event type and arguments tem-plate) given the trigger-word.p(EventStruct |Trigger) = f(Trigger ?
EventStruct)f(Trigger)(2)Again, only high probability structures were con-sidered further.
We used the probability threshold of0.25 for simple event structures (i.e.
not containingnested events), and 0.1 for complex event structures(only regulation events in the shared task).4.3 Event argument fillingThe inclusion of a protein as an argument of an eventwas based on the syntactic domination of the triggerof the event over the term of the protein.
We at-tempted to generate simple events of all types seenin the training data.For complex events, the trigger-words of the mainand the embedded events had to be in a dominationrelationship.
We generated regulation-events withonly 1-level embedding.
Although more complexembeddings are possible (see example below), theseare not very frequent.prevents T cell-receptor-induced FasLupregulationIn order to flexibly deal with sparse data, we per-formed a sequence of queries, one less instantiated32Figure 2: Example of an annotated sentence from abstract 10080948 in the training data.than the previous one, weighted the results accord-ingly and calculated the weighted mean to be the fi-nal probability for including the argument.find_path_freq(HWord, HType, DWord, DType, Path,C1_1, C2_1),find_path_freq(_, HType, _, DType, Path,C1_2, C2_2),find_path_freq(_, HType, _, DType, _,C1_3, C2_3)In MLE terms, we calculate the probability thata syntactic configuration fills an argument slot.Syntactic configurations consist of the head wordHWord, the head event type HType, the dependentword DWord, the dependent event type DType, andthe syntactic path Path between them.p(Arg |HWord, HType, DWord, DType, Path) =1w1+w2+w3 ?
(w1 ?
f(HWord, HType, DWord, DType, Path?Arg)f(HWord, HType, DWord, DType, Path) +w2 ?
f(HType, DType, Path?Arg)f(HType, DType, Path) +w3 ?
f(HType, DType?Arg)f(HType, DType) ) (3)The weigths were set as w1 = 3, w2 = 2 andw3 = 1.2.
The fact that the weights decrease ap-proximates a back-off model.
Only if the final prob-ability was higher than 0.3 the event was further con-sidered.
For complex events, we used formula 3 asgiven, but for simple events, where DWord is a pro-tein, DWord was always left uninstantiated.4.4 PostprocessingDuring the postprocessing step some unlikely eventstructures were filtered out.
This filtering is delayeduntil all the events have been generated, because ex-cluding the unwanted events is difficult during cre-ation time as sometimes extrospection is required.Also, the postprocessing step acts as a safety netthat filters out well-formedness errors (e.g.
argu-ment sharing violations), thus making sure that thesubmission to the evaluation system is not rejectedby the system.
Finally, the set of generated events isserialized into the BioNLP a2-format.5 Example and discussionAs an example of application of our approach, con-sider again the syntactic tree shown in figure 1.Our approach results in the generation of the eventsshown in figure 3, given that ?regulates?, ?inducible?,and ?expression?
are trigger-words, and ?Oct-2?
is ana1-annotated protein.Figure 3: Visualization of two simple event structuresregulates(Oct-2) and expression(Oct-2), and a complexstructure regulates(expression(Oct-2)).We call events like regulates(Oct-2) ?shortcutevents?, as there exists an alternative and longerpath ?
regulates(expression) and expression(Oct-2)?
that connects the trigger to its event argument.These ?shortcut events?
are filtered out in the post-processing step as unlikely events.It is useful to observe that the particular view ofevent structures defined by the BioNLP shared taskis by no means unchallenged.
Whether nested eventsare necessary in a representation of biological rele-vant relations is a question which is open to debate.While from the linguistic perspective they do offer amore adequate representation of the content matterof the text, from the biological point of view thesestructures are redundant in many cases.
The exam-ple used in this section is illustrative.From the biologist?s perspective, ?A regulates theexpression of B?
is a way to express that A regu-lates B.
Obviously such a short-circuit is not in allcases possible, but the point is that the biologist33might be interested only in the direct biological in-teractions, and be inclined to ignore the linguisticrepresentation of that interaction.
This is the pointof view taken for example in the Protein-ProteinInteraction task of the latest BioCreative competi-tion (Krallinger et al, 2008).
In that case, all lin-guistic structures used to better characterize the in-teraction are purposefully ignored, and only the bareinteraction is preserved.Since BioCreative aimed at simulating the pro-cess of database curation, and was based on datasetsprovided by real-word interaction databases such asIntAct (Kerrien et al, 2006) and MINT (Zanzoni etal., 2002), there is reasonable motivation for takingthis alternative view into consideration.
At the veryleast, a mapping from complex events to simple in-teractions should always be provided.The difference in the approach towards interpre-tation of literature fragments has a direct impact onthe resources used and the success of each approach.Our own development in the past couple of years hasbeen driven by the BioCreative model (Rinaldi et al,2008), and therefore we tended to ignore intermedi-ate structures in protein interactions.
For example,in (Schneider et al, 2009) we present a lexical re-source that aims at capturing ?transparent?
relations,i.e.
words that express a relation that from the bio-logical point of view can be ignored because of itstransitivity properties, such as ?expression of Oct-2?
in the example above.
This resource, althoughcertainly useful from the biological point of view,proved to be useless in the shared task, due to thedifferent level of granularity in the representation ofevents.6 Official evaluation and additionalexperimentsWe mainly trained and evaluated using the ?strictequality?
evaluation criteria as our reference.
Theresults on the development data are shown in table2.
With more relaxed equality definitions, the resultswere always a few percentage points better.
Our re-sults in the official testrun are shown in table 3.Good results for some event structures (notablyPhosphorylation) are due to the simple textual repre-sentation of these events.
For example, Phosphory-lation is always triggered by a form or derivation of?phosphorylate?, and these forms rarely trigger anyother types of events.
Furthermore, according to theparsed training data, the probability of a Phospho-rylation-event, given a syntactic domination relationbetween a Phosphorylation-trigger and a protein is0.92.
Also, 56% of these domination paths are ei-ther chunk-internal or over a single modpp depen-dency relation, making them easy to detect.In parallel to the approach used in our official sub-mission we considered some variants, aimed at max-imizing either recall or precision, as well as an alter-native approach based on machine learning.A high recall baseline method, which generatesall possible event structures in a given sentence,achieves 81% recall on simple events, with preci-sion dropping to 11%.
One of the reasons why thismethod does not reach 100% recall is the fact thatit only annotates event candidates with single-tokentriggers that have been seen in the training data.The filter described in section 4.3 has a major ef-fect on precision.
If it is removed, precision dropsby 11%, while the gain in recall is only 3% ?
re-call 35.10%, precision 37.88%, F-score 36.44%.
In-stead, if we keep w1 but set w2 = w3 = 0 in formula3, precision increases to 56%, while recall drops to27%.
Increasing the probability thresholds to furtherimprove precision results in the precision of 60% butthis remains the ceiling in our experiments.Additionally, we performed separate experimentswith a machine-learning approach which considersa more varied set of features, including surface in-formation and syntax coming from an ensemble ofparsers.
However, the limited time and resourcesavailable to us during the competition did not al-low us to go beyond the results achieved using theapproach described in detail in this paper.
Sinceour best score on the development data was 27%(about 10% inferior to our consolidated approach),we opted for not considering this approach in ourofficial submission.The fact that this approach was based on a de-composition of events into their arguments led us torealize some fundamental limitations in the officialevaluation measures.
In particular, none of the orig-inally implemented measures would give credit tothe partial recognition of an event (i.e.
correct trig-ger word and at least one correct argument, but notall).
We contend that such partial recognition can be34Event class Precision Recall F-Score True pos.
False pos.
False neg.Simple events 56.71 48.20 52.11 389 297 418Complex events 38.03 19.25 25.56 189 308 793All events 48.86 32.31 38.90 578 605 1211Table 2: Results on the development data of 150 abstracts, measured using ?strict equality?.Event class gold (match) answer (match) Recall Precision F-ScoreLocalization 174 (31) 34 (31) 17.82 91.18 29.81Binding 347 (102) 287 (102) 29.39 35.54 32.18Gene expression 722 (370) 515 (370) 51.25 71.84 59.82Transcription 137 (28) 148 (28) 20.44 18.92 19.65Protein catabolism 14 (8) 16 (8) 57.14 50.00 53.33Phosphorylation 135 (78) 84 (78) 57.78 92.86 71.23Simple events total 1529 (617) 1084 (617) 40.35 56.92 47.23Regulation 291 (29) 120 (29) 9.97 24.17 14.11Positive regulation 983 (138) 533 (138) 14.04 25.89 18.21Negative regulation 379 (55) 158 (55) 14.51 34.81 20.48Complex events total 1653 (222) 811 (222) 13.43 27.37 18.02All events total 3182 (839) 1895 (839) 26.37 44.27 33.05Table 3: Results on the test data of 260 abstracts, measured using ?strict equality?, as reported by the BioNLP 2009online evaluation system.useful in a practical annotation task, and yet the of-ficial scores doubly punish such an outcome (onceas a FP and once as a FN).
This is a problem alreadyobserved in previous evaluation challenges, howeverwe believe that a simple solution in this case consistsin decomposing the events (for evaluation purposes)in their constituent roles and arguments.
In otherwords, each event is given as much ?weight?
as itsnumber of roles.
The correct recognition of an eventwith two roles would therefore lead to two TP, but itspartial recognition (one argument) would still leadto one TP, which we think is a more fair evaluationin case of partial recognition.
Our suggestion waslater implemented by the organizers as an additionalscoring criteria.7 Conclusions and future workWe have described a biological event detectionmethod that relies on the chunk and syntactic de-pendency relations obtained during the preprocess-ing stage.
No fall-back strategy that is based on e.g.surface patterns was designed for this task.
This isconsistent with our approach to biomedical event de-tection ?
relation extraction is entirely based on ex-isting syntactic information about the sentences, andcan be ported easily if the definition of relations andevents is changed, as in the case of other competi-tions which use a different notion of relations (e.g.BioCreative).As the chunker and the dependency parser forma core of the described system, their limitations andimprovements have a fundamental effect on the fur-ther processing.
In parallel to a thorough error anal-ysis which can drive further development of our con-solidated approach, we intend to further explore theenhanced flexibility provided by the machine learn-ing approach briefly mentioned in section 6.
In bothcases, we intend to use the BioNLP shared task eval-uation site as a reference in order to compare them,not only against each other, but also against the re-sults of other participants.AcknowledgementsThis research is partially funded by the Swiss Na-tional Science Foundation (grant 100014-118396/1).Additional support is provided by Novartis PharmaAG, NITAS, Text Mining Services, CH-4002, Basel,Switzerland.
The authors would like to thank thetwo anonymous reviewers of BioNLP 2009 for theirvaluable feedback.35References[Haverinen et al2008] Katri Haverinen, Filip Ginter,Sampo Pyysalo, and Tapio Salakoski.
2008.
Accu-rate conversion of dependency parses: targeting thestanford scheme.
In Proceedings of Third Interna-tional Symposium on Semantic Mining in Biomedicine(SMBM 2008), Turku, Finland.
[Hindle, D and Rooth, M1991] Hindle, D and Rooth, M.1991.
Structural Ambiguity and Lexical Relations.Meeting of the Association for Computational Linguis-tics, pages 229?236.
[Kerrien et al2006] S. Kerrien, Y. Alam-Faruque,B.
Aranda, I. Bancarz, A. Bridge, C. Derow, E. Dim-mer, M. Feuermann, A. Friedrichsen, R. Huntley,C.
Kohler, J. Khadake, C. Leroy, A. Liban, C. Lieftink,L.
Montecchi-Palazzi, S. Orchard, J. Risse, K. Robbe,B.
Roechert, D. Thorneycroft, Y. Zhang, R. Apweiler,and H. Hermjakob.
2006.
IntAct ?
Open SourceResource for Molecular Interaction Data.
NucleicAcids Research.
[Kim et al2003] J.D.
Kim, T. Ohta, Y. Tateisi, and J. Tsu-jii.
2003.
GENIA corpus ?
a semantically annotatedcorpus for bio-textmining.
Bioinformatics, 19(1):180?182.
[Krallinger et al2008] Martin Krallinger, Florian Leit-ner, Carlos Rodriguez-Penagos, and Alfonso Valencia.2008.
Overview of the protein-protein interaction an-notation extraction task of BioCreative II.
Genome Bi-ology, 9(Suppl 2):S4.
[Marcus et al1993] M Marcus, B Santorini, andM Marcinkiewicz.
1993.
Building a Large An-notated Corpus of English: the Penn Treebank.Computational Linguistics, 19:313?330.
[Mikheev1997] A Mikheev.
1997.
Automatic rule induc-tion for unknown word guessing.
Computational Lin-guistics, 23(3):405?423.
[Minnen et al2001] G Minnen, J Carroll, and D Pearce.2001.
Applied morphological processing of English.Natural Language Engineering, 7(3):207?223.
[Prins2005] Robbert Prins.
2005.
Finite-State Pre-Processing for Natural Language Analysis.
Ph.D. the-sis, Behavioral and Cognitive Neurosciences (BCN)research school, University of Groningen.
[Rinaldi et al2006] Fabio Rinaldi, Gerold Schneider,Kaarel Kaljurand, Michael Hess, and Martin Ro-macker.
2006.
An Environment for Relation Miningover Richly Annotated Corpora: the case of GENIA.BMC Bioinformatics, 7(Suppl 3):S3.
[Rinaldi et al2008] Fabio Rinaldi, Thomas Kappeler,Kaarel Kaljurand, Gerold Schneider, Manfred Klen-ner, Simon Clematide, Michael Hess, Jean-Marc vonAllmen, Pierre Parisot, Martin Romacker, and ThereseVachon.
2008.
OntoGene in BioCreative II.
GenomeBiology, 9(Suppl 2):S13.
[Schneider et al2009] Gerold Schneider, Kaarel Kalju-rand, Thomas Kappeler, and Fabio Rinaldi.
2009.Detecting protein-protein interactions in biomedicaltexts using a parser and linguistic resources.
In CI-CLing 2009, 10th International Conference on Intel-ligent Text Processing and Computational Linguistics,Mexico City, Mexico.
[Schneider2008] Gerold Schneider.
2008.
Hybrid Long-Distance Functional Dependency Parsing.
Ph.D. the-sis, Faculty of Arts, University of Zurich.
[Zanzoni et al2002] A. Zanzoni, L. Montecchi-Palazzi,M.
Quondam, G. Ausiello, M. Helmer-Citterich, andG.
Cesareni.
2002.
MINT: a Molecular INTeractiondatabase.
FEBS Letters, 513(1):135?140.36
