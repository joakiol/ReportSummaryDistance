Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 79?82,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Models for Inuktitut-English Word AlignmentCharles Schafer and Elliott Franco Dr?abekDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD 21218, USA{cschafer,edrabek}@cs.jhu.eduAbstractThis paper presents a set of techniques for bitext word align-ment, optimized for a language pair with the characteristics ofInuktitut-English.
The resulting systems exploit cross-lingualaffinities at the sublexical level of syllables and substrings, aswell as regular patterns of transliteration and the tendency to-wards monotonicity of alignment.
Our most successful systemswere based on classifier combination, and we found differentcombination methods performed best under the target evalua-tion metrics of F-measure and alignment error rate.1 IntroductionConventional word-alignment methods have been suc-cessful at treating many language pairs, but may be lim-ited in their ability to generalize beyond the Western Eu-ropean language pairs for which they were originallydeveloped, to pairs which exhibit more complex diver-gences in word order, morphology and lexical granular-ity.
Our approach to Inuktitut-English alignment was tocarefully consider the data in identifying difficulties par-ticular to Inuktitut-English as well as possible simplify-ing assumptions.
We used these observations to constructa novel weighted finite-state transducer alignment modelas well as a specialized transliteration model.
We com-bined these customized systems with 3 systems basedon IBM Model 4 alignments under several methods ofclassifier combination.
These combination strategies al-lowed us to produce multiple submissions targeted at thedistinct evaluation measures via a precision/recall trade-off.2 Special Characteristics of theInuktitut-English Alignment ProblemGuided by the discussion of Inuktitut in Mallon (1999),we examined the Nunavut Hansards training and hand-labeled trial data sets in order to identify special chal-lenges and exploitable characteristics of the Inuktitut-English word alignment problem.
We were able to iden-tify three: (1) Importance of sublexical Inuktitut units;(2) 1-to-N Inuktitut-to-English alignment cardinality; (3)Monotonicity of alignments.2.1 Types and TokensInuktitut has an extremely productive agglutinative mor-phology, and an orthographic word may combine verymany individual morphemes.
As a result, in Inuktitut-English bitext we observe Inuktitut sentences with manyfewer word tokens than the corresponding English sen-tences; the ratio of English to Inuktitut tokens in thetraining corpus is 1.85.1 This suggests the importance oflooking below the Inuktitut word level when computinglexical translation probabilities (or alignment affinities).To reinforce the point, consider that the ratio of trainingcorpus types to tokens is 0.007 for English, and 0.194 forInuktitut.
In developing a customized word alignmentsolution for Inuktitut-English, a major goal was to han-dle the huge number of Inuktitut word types seen onlyonce in the training corpus (337798 compared to 8792for English), without demanding the development of amorphological analyzer.2.2 AlignmentConsidering English words in English sentence order,4.7% of their alignments to Inuktitut were found to beretrograde; that is, involving a decrease in Inuktitut wordposition with respect to the previous English word?saligned Inuktitut position.
Since this method of countingretrograde alignments would assign a low count to massmovements of large contiguous chunks, we also mea-sured the number of inverted alignments over all pairsof English word positions.
That is, the sum?e?a=|e|?1a=1 ?b=|e|b=a+1?i1?I(e,a)?i2?I(e,b)(1 if i1 > i2)was computed over all Inuktitut alignment sets I(e, x),for e the English sentence and x the English word po-sition.
Dividing this sum by the obvious denominator(replacing (1 if i1 > i2) with (1) in the sum) yielded avalue of 1.6% inverted alignments.Table 1 shows a histogram of alignment cardinalitiesfor both English and Inuktitut.
Ninety-four percent ofEnglish word tokens, and ninety-nine percent of thosehaving a non-null alignment, align to exactly one Inuk-titut word.
In development of a specialized word alignerfor this language pair (Section 3), we made use of theobserved reliability of these two properties, monotonic-ity and 1-to-N cardinality.3 Alignment by Weighted Finite-StateTransducer CompositionWe designed a specialized alignment system to handlethe above-mentioned special characteristics of Inuktitut-1Though this ratio increases to 2.21 when considering only longersentences (20 or more English words), ignoring common short, formu-laic sentence pairs such as ( Hudson Bay ) ( sanikiluaq ) .79% Words Having Specified Alignment CardinalityNULL 1 2 3 4 5 6 7English 5 94 <1 <1 0 0 0 0Inuktitut 3 43 20 14 10 5 3 2Table 1: Alignment cardinalities for English-Inuktitut wordalignment, computed over the trial data.English alignment.
Our weighted finite-state transducer(WFST) alignment model, illustrated in Figure 1, struc-turally enforces monotonicity and 1-to-N cardinality, andexploits sublexical information by incorporating associ-ation scores between English words and Inuktitut wordsubstrings, based on co-occurrence in aligned sentences.For each English word, an association score was com-puted not only with each Inuktitut word, but also witheach Inuktitut character string of length ranging from2 to 10 characters.
This is similar to the technique de-scribed in Martin et al (2003) as part of their construc-tion of a bilingual glossary from English-Inuktitut bi-text.
However, our goal is different and we keep all theEnglish-Inuktitut associations, rather than selecting onlythe ?best?
ones using a greedy method, as do they.
Addi-tionally, before extracting all substrings from each Inuk-titut word, we added a special character to the word?sbeginning and end (e.g., makkuttut ?
makkuttut ), inorder to exploit any preferences for word-initial or -finalplacement.The heuristic association score chosen wasp(worde |wordi) ?
p(wordi |worde), computed over allthe aligned sentence pairs.
We have in the past observedthis to be a useful indicator of word association, and ithas the nice property of being in the range (0,1].The WFST aligner is a composition of 4 transduc-ers.2 The structure of the entire WFST composition en-forces monotonicity, Inuktitut-to-English 1-N cardinal-ity, and Inuktitut word fertilities ranging between 1 and7.
This model was implemented using the ATT finite-state toolkit (Mohri et al, 1997).
In Figure 1, [1] isa linear transducer mapping each English position in aparticular English test sentence to the word at that posi-tion.
It is constructed so as to force each English wordto participate in exactly 1 alignment.
[2] is a single-statetransducer mapping English word to Inuktitut substrings(or full words) with weights derived from the associationscores.3 [3] is a transducer mapping Inuktitut substrings(and full words) to their position in the Inuktitut test sen-tence.
Its construction allows a single Inuktitut positionto correspond to multiple English positions, while en-forcing monotonicity.
[4] is a transducer regulating theallowed ?fertility?
values of Inuktitut words; each Inuk-titut word is permitted a fertility of between 1 and 7.
Thefertility values are assigned the probabilities correspond-ing to observed relative frequencies in the trial data, and2Bracketed numbers in the following discussion refer to the compo-nent transducers as illustrated in Figure 1.3Transducers [2] and [4] are shared across all sentence decodings.11 11 1 1<epsilon> (0.82)<epsilon>(1.56)<epsilon>(1.90)<epsilon><epsilon><epsilon>...............illug/1_pijjut/1atuqa/2_inna/2_amm/3_am/3kkutt/4_makku/4ajunga/5akainnar/5<epsilon><epsilon> <epsilon> <epsilon> <epsilon>_pijjutigillugu_/1 _innatuqait_/2 _amma_/3 _makkuttut_/4 _uqausiqakainnarumajunga_/51/in 2/regards 3/to 4/elders.
.
.6/youth5/andin regards to elders and youth i want to make general commentspijjutigillugu innatuqait amma makkuttut uqausiqakainnarumajungaand/_am (.54)youth/_makku (1.10)youth/_makkuttut_ (3.89)regards/_pijjutigillugu_ (3.49)regards/_pijjuti (2.98)......[1]elders/_inna (0.90)elders/_innat (1.09)general/_uqausi (4.54)and/_amma (.49)and/_amm (.49).
.
.
[2][3][4](1.90)22 22 2 2<epsilon> (0.82)<epsilon>(1.56)<epsilon><epsilon><epsilon><epsilon>.
.
.Figure 1: WFST alignment system in composition order, in-stantiated for an example sentence from the development (trial)data.
To save space, only a representative portion of each ma-chine is drawn.
Transition weights are costs in the tropical(min,+) semiring, derived from negative logs of probabilitiesand association scores.
Nonzero costs are indicated in paren-theses.are not conditioned on the identity of the Inuktitut word.4 English-Inuktitut TransliterationAlthough in this corpus English and Inuktitut are bothwritten in Roman characters, English names are signifi-cantly transformed when rendered in Inuktitut text.
Con-sider the following English/Inuktitut pairs from the train-ing corpus: Chartrand/saaturaan, Chretien/kurittianand the set of training corpus-attested Inuktitut render-ings of Williams, Campbell, and McLean shown in Ta-ble 2(A) (which does not include variations containingthe common -mut lexeme, meaning ?to [a person]?
(Mal-lon, 1999)).Clearly, not only does the English-to-Inuktitut trans-formation radically change the name string, it does soin a nondeterministic way which appears to be influ-enced not only by the phonological preferences of Inuk-titut but also by differing pronunciations of the name inquestion and possibly by differing conventions of trans-lators (note, for example, maklain versus mikliin forMcLean).We trained a probabilistic finite-state transducer(FST) to identify English-Inuktitut transliterated pairsin aligned sentences.
Training string pairs were ac-quired from the training bitext in the following manner.Whenever single instances of corresponding honorificswere found in a sentence pair ?
these included the cor-respondences (Ms , mis); (Mrs , missa/missis); (Mr ,80(A) (B)Williams McLean k shailiams makalain k -4.2 s -7.2uialims makkalain q -6.2uilialums maklaain wuiliam maklain b ui -5.8uiliammas maklainn p -4.3 v -6.1uiliams maklait v -5.0uilians makli ouliams maklii z a -4.2viliams makliik j -5.2 aa -4.6makliin s -5.8 uu -4.9Campbell maklin u -5.1kaampu malain chkaampul matliin s -5.6 ukaamvul miklain k -6.8 uu -5.5kamvul mikliin u -5.6miklin a -6.2Table 2: (A) Training-corpus-attested renderings of Williams,Campbell, and McLean.
(B) Top learned Inuktitut substi-tutions and their log probabilities for several English (shownunderlined) orthographic characters (and character sequences).Where top substitutions for English characters are shown, noneequal or better were omitted.mista/mistu) ?
the immediately following capitalized En-glish words (up to 2) were extracted and the same num-ber of Inuktitut words were extracted to be used as train-ing pairs.
Thus, given the appearance in aligned sen-tences of ?Mr.
Quirke?
and ?mista kuak?, the trainingpair (Quirke,kuak) would be extracted.
Common dis-tractions such as ?Mr Speaker?
were filtered out.
In or-der to focus on the native English name problem (Inuk-titut name rendering into English is much less noisy) theEnglish extractions were required to have appeared in alarge, news-corpus-derived English wordlist.
This pro-cedure resulted in a conservative, high-quality list of 434unique name pairs.
The probabilistic FST model we se-lected was that of a memoryless (single-state) transducerrepresenting a joint distribution over character substitu-tions, English insertions, and Inuktitut insertions.
Thismodel is identical to that presented in Ristad and Yianilos(1997).
Prior to training, common English digraphs (e.g.,?th?
and ?sh?)
were mapped to unique single characters,as were doubled consonants.
Inuktitut ?ng?
and commontwo-vowel sequences were also mapped to unique singlecharacters to elicit higher-quality results from the memo-ryless transduction model employed.
Some results of thetransducer training are displayed in Table 2(B).
Proba-bilistic FST weight training was accomplished using theDyna modeling language and DynaMITE parameter op-timization toolkit (Eisner et al 2004).
The translitera-tion modeling described here differs from such previoustransliteration work as Stalls and Knight (1998) in thatthere is no explicit modeling of pronunciation, only a di-rect transduction between written forms.In applying transliteration on trial/test data, thefollowing criteria were used to select English words fortransliteration: (1) Word is capitalized (2) Word is not inthe exclusion list.4 For the top-ranked transliteration ofthe English word present in the Inuktitut sentence, alloccurrences of that word in that sentence are marked asaligned to the English word.We have yet to evaluate English-Inuktitut translitera-tion in isolation on a large test set.
However, accuracyon the workshop trial data was 4/4 hypotheses correct,and on test data 2/6 correct.
Of the 4 incorrect testhypotheses, 2 were mistakes in identifying the correcttransliteration, and 2 mistakes resulted from attemptingto transliterate an English word such as ?Councillors?which should not be transliterated.
Even with a rela-tively low accuracy, the transliteration model, which isused only as an individual voter in combination systems,is unlikely to vote for the incorrect choice of another sys-tem.
Its purpose under system combination is to push agood alignment link hypothesis up to the required votethreshold.55 IBM Model 4 AlignmentsAs a baseline and contributor to our combination sys-tems, we ran GIZA++ (Och and Ney, 2000), to producealignments based on IBM Model 4.
The IBM align-ment models are asymmetric, requiring that one lan-guage be idenitifed as the ?e?
language, whose wordsare allowed many links each, and the other as the ?f?
lan-guage, whose words are allowed at most one link each.Although the observed alignment cardinalities naturallysuggest identifying Inuktitut as the ?e?
language and En-glish as the ?f?
language, we ran both directions for com-pleteness.As a crude first attempt to capture sublexical corre-spondences in the absence of a method for morphemesegmentation, we developed a rough syllable segmenter(spending approximately 2 person-hours), ran GIZA++to produce alignments treating the syllables as words,and chose, for each English word, the Inuktitut word orwords the largest number of whose syllables were linkedto it.In the nomenclature of our results tables, giza++ syl-labized refers to the latter system, giza++ E(1)-I(N) rep-resents GIZA++ run with English as the ?e?
language,and giza++ E(N)-I(1) sets English as the ?f?
language.6 System Performance and CombinationMethodsWe observed the 4 main systems (3 GIZA++ variants andWFST) to have significantly different performance pro-files in terms of precision and recall.
Consistently, WFST4Exclusion list was compiled as follows: (a) capitalized words in2000 randomly selected English training sentences were examined,Words such as Clerk, Federation, and Fisheries, which are frequentlycapitalized but should not be transliterated, were put into the exclusionlist; in addition, any word with frequency > 50 in the training corpuswas excluded, on the rationale that common-enough words would havewell-estimated translation probabilities already.
50 may seem like ahigh threshold until one considers the high variability of the transliter-ation process as demonstrated in Table 2(A).5Refer to Section 6 for detailed descriptions of voting.81SYSTEM P R F AER |H|/|T |Individual system performance Trial Datagiza++ E(1)-I(N) 63.4 26.6 37.5 32.9 0.42giza++ E(N)-I(1) 68.2 59.4 63.5 28.6 0.87giza++ syllabized 83.6 44.5 58.1 18.3 0.53WFST 70.3 72.7 71.5 27.8 1.03Combination system performance Trial DataF/AER Emphasis 85.4 63.5 72.9 12.3 0.74AER Emphasis (1) 92.6 44.2 59.9 8.8 0.48AER Emphasis (2) 95.1 38.0 54.3 9.5 0.40F Emphasis 74.8 77.6 76.2 21.9 1.04Recall Emphasis 66.9 82.1 73.8 28.9 1.23Individual system performance Test Datagiza++ E(1)-I(N) 49.7 18.6 27.0 45.2 0.37giza++ E(N)-I(1) 64.6 56.2 60.1 32.7 0.87giza++ syllabized 84.9 44.0 57.9 15.6 0.52WFST 65.4 68.3 66.8 33.7 1.04(submitted) Combination system performance Test DataF/AER Emphasis 84.4 58.6 69.2 14.3 0.69AER Emphasis (1) 90.7 39.4 54.9 11.5 0.43AER Emphasis (2) 96.7 32.3 48.4 9.5 0.33F Emphasis 70.7 73.8 72.2 26.7 1.04Recall Emphasis 62.6 81.7 70.1 34.2 1.31Table 3: System performance evaluated on trial and test data.The precision, recall and F-measure cited are the unlabeledversion (?probable,?
in the nomenclature of this shared task).The gold standard truth for trial data contained 710 alignments.The test gold standard included 1972 alignments.
The column|H|/|T | lists ratio of hypothesis set size to truth set size for eachsystem.won out on F-measure while giza++ syllabized attainedbetter alignment error rate (AER).
Refer to Table 3 fordetails of performance on trial and test data.We investigated a number of system combinationmethods, three of which were finally selected for usein submitted systems.
There were two basic methods ofcombination: per-link voting and per-English-word vot-ing.6 In per-link voting, an alignment link is included ifit is proposed by at least a certain number of the partic-ipating individual systems.
In per-English-word voting,the best outgoing link is chosen for each English word(the link which is supported by the greatest number of in-dividual systems).
Any ties are broken using the WFSTsystem choice.
A high-recall variant of per-English-wordvoting was included in which ties at vote-count 1 (in-dicating a low-confidence decision) are not broken, butrather all systems?
choices are submitted as hypotheses.The transliteration model described in Section 4 wasincluded as a voter in each combination system, though itmade few hypotheses (6 on the test data).
Composition ofthe submitted systems was as follows: F/AER Empha-6Combination methods we elected not to submit included votingwith trained weights and various stacked classifiers.
The reasoning wasthat with such a small development data set ?
25 sentences ?
it wasunsafe to put faith in any but the simplest of classifier combinationschemes.sis - per-link voting with decision criterion >= 2 votes,over all 5 described systems (WFST, 3 GIZA++ vari-ants, transliteration).
AER Emphasis (I) per-link voting,>= 2 votes, over all systems except giza++ E(N)-I(1).AER Emphasis (II) per-link voting, >= 3 votes, overall systems.
F Emphasis per-English-word voting, overall systems, using WFST as tiebreaker.
Recall Empha-sis per-English-word voting, over all systems, high-recallvariant.We elected to submit these systems because eachtailors to a distinct evaluation criterion (as suggestedby the naming convention).
Experiments on trial dataconvinced us that minimizing AER and maximizing F-measure in a single system would be difficult.
Mini-mizing AER required such high-precision results that thetradeoff in recall greatly lowered F-measure.
It is inter-esting to note that system combination does provide aconvenient means for adjusting alignment precision andrecall to suit the requirements of the problem or evalua-tion standard at hand.7 ConclusionsWe have presented several individual and combined sys-tems for word alignment of Inuktitut-English bitext.
Themost successful individual systems were those targetedto the specific characteristics of the language pair.
Thecombined systems generally outperformed the individualsystems, and different combination methods were able tooptimize for performance under different evaluation met-rics.
In particular, per-English-word voting performedwell on F-measure, while per-link voting performed wellon AER.Acknowledgements: Many thanks to Eric Goldlust, DavidSmith, and Noah Smith for help in using the Dyna language.ReferencesJ.
Eisner, E. Goldlust, and N. A. Smith.
2004.
Dyna: A declarativelanguage for implementing dynamic programs.
In Proceedings of the42nd Annual Meeting of the Association for Computational Linguistics(ACL 2004), Companion Volume, pages 218-221.M.
Mallon.
1999.
Inuktitut linguistics for technocrats.
Technical re-port, Ittukuluuk Language Programs, Iqaluit, Nunavut, Canada.J.
Martin, H. Johnson, B. Farley, and A. Maclachlan.
2003.
Align-ing and using an English-Inuktitut parallel corpus.
In Proceedings ofWorkshop on Building and Using Parallel Texts: Data Driven MachineTranslation and Beyond, HLT-NAACL 2003.M.
Mohri, F. Pereira, and M. Riley.
1997.ATT General-purpose finite-state machine software tools.http://www.research.att.com/sw/tools/fsm/.F.
J. Och and H. Ney.
2000.
Improved statistical alignment models.
InProceedings of the 38th Annual Meeting of the Association for Compu-tational Linguistics, pages 440?447.E.
S. Ristad and P. N. Yianilos.
1997.
Learning string edit distance.
InMachine Learning: Proceedings of the Fourteenth International Con-ference, pages 287?295.B.
Stalls and K. Knight.
1998.
Translating names and technical termsin arabic text.
In Proceedings of the COLING/ACL Workshop on Com-putational Approaches to Semitic Languages.82
