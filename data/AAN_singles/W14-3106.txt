Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 38?44,Baltimore, Maryland, USA, June 27, 2014.c?2014 Association for Computational LinguisticsSPIED: Stanford Pattern-based Information Extraction and DiagnosticsSonal Gupta Christopher D. ManningDepartment of Computer ScienceStanford University{sonal, manning}@cs.stanford.eduAbstractThis paper aims to provide an effectiveinterface for progressive refinement ofpattern-based information extraction sys-tems.
Pattern-based information extrac-tion (IE) systems have an advantage overmachine learning based systems that pat-terns are easy to customize to cope witherrors and are interpretable by humans.Building a pattern-based system is usuallyan iterative process of trying different pa-rameters and thresholds to learn patternsand entities with high precision and recall.Since patterns are interpretable to humans,it is possible to identify sources of errors,such as patterns responsible for extract-ing incorrect entities and vice-versa, andcorrect them.
However, it involves timeconsuming manual inspection of the ex-tracted output.
We present a light-weighttool, SPIED, to aid IE system develop-ers in learning entities using patterns withbootstrapping, and visualizing the learnedentities and patterns with explanations.SPIED is the first publicly available tool tovisualize diagnostic information of multi-ple pattern learning systems to the best ofour knowledge.1 IntroductionEntity extraction using rules dominates commer-cial industry, mainly because rules are effective,interpretable by humans, and easy to customize tocope with errors (Chiticariu et al., 2013).
Rules,which can be hand crafted or learned by a sys-tem, are commonly created by looking at the con-text around already known entities, such as surfaceword patterns (Hearst, 1992) and dependency pat-terns (Yangarber et al., 2000).
Building a pattern-based learning system is usually a repetitive pro-cess, usually performed by the system developer,of manually examining a system?s output to iden-tify improvements or errors introduced by chang-ing the entity or pattern extractor.
Interpretabil-ity of patterns makes it easier for humans to iden-tify sources of errors by inspecting patterns thatextracted incorrect instances or instances that re-sulted in learning of bad patterns.
Parametersrange from window size of the context in surfaceword patterns to thresholds for learning a candi-date entity.
At present, there is a lack of toolshelping a system developer to understand resultsand to improve results iteratively.Visualizing diagnostic information of a systemand contrasting it with another system can makethe iterative process easier and more efficient.
Forexample, consider a user trying to decide on thecontext?s window size in surface words patterns.And the user deliberates that part-of-speech (POS)restriction of context words might be required fora reduced window size to avoid extracting erro-neous mentions.1By comparing and contrastingextractions of two systems with different parame-ters, the user can investigate the cases in which thePOS restriction is required with smaller windowsize, and whether the restriction causes the systemto miss some correct entities.
In contrast, compar-ing just accuracy of two systems does not allowinspecting finer details of extractions that increaseor decrease accuracy and to make changes accord-ingly.In this paper, we present a pattern-based entitylearning and diagnostics tool, SPIED.
It consistsof two components: 1. pattern-based entity learn-ing using bootstrapping (SPIED-Learn), and 2. vi-sualizing the output of one or two entity learningsystems (SPIED-Viz).
SPIED-Viz is independentof SPIED-Learn and can be used with any pattern-based entity learner.
For demonstration, we usethe output of SPIED-Learn as an input to SPIED-1A shorter context size usually extracts entities withhigher recall but lower precision.38Viz.
SPIED-Viz has pattern-centric and entity-centric views, which visualize learned patternsand entities, respectively, and the explanations forlearning them.
SPIED-Viz can also contrast twosystems by comparing the ranks of learned enti-ties and patterns.
In this paper, as a concrete ex-ample, we learn and visualize drug-treatment (DT)entities from unlabeled patient-generated medicaltext, starting with seed dictionaries of entities formultiple classes.
The task was proposed and fur-ther developed in Gupta and Manning (2014b)and Gupta and Manning (2014a).Our contributions in this paper are: 1. wepresent a novel diagnostic tool for visual-ization of output of multiple pattern-basedentity learning systems, and 2. we release thecode of an end-to-end pattern learning sys-tem, which learns entities using patterns in abootstrapped system and visualizes its diag-nostic output.
The pattern learning code isavailable at http://nlp.stanford.edu/software/patternslearning.shtml.The visualization code is available athttp://nlp.stanford.edu/software/patternviz.shtml.2 Learning Patterns and EntitiesBootstrapped systems have been commonly usedto learn entities (Riloff, 1996; Collins and Singer,1999).
SPIED-Learn is based on the system de-scribed in Gupta and Manning (2014a), whichbuilds upon the previous bootstrapped pattern-learning work and proposed an improved mea-sure to score patterns (Step 3 below).
It learnsentities for given classes from unlabeled text bybootstrapping from seed dictionaries.
Patternsare learned using labeled entities, and entities arelearned based on the extractions of learned pat-terns.
The process is iteratively performed untilno more patterns or entities can be learned.
Thefollowing steps give a short summary of the itera-tive learning of entities belonging to a class DT:1.
Data labeling: The text is labeled using theclass dictionaries, starting with the seed dic-tionaries in the first iteration.
A phrasematching a dictionary phrase is labeled withthe dictionary?s class.2.
Pattern generation: Patterns are generated us-ing the context around the positively labeledentities to create candidate patterns for DT.3.
Pattern learning: Candidate patterns arescored using a pattern scoring measure andthe top ones are added to the list of learnedpatterns for DT.
The maximum number ofpatterns learned is given as an input to thesystem by the developer.4.
Entity learning: Learned patterns for the classare applied to the text to extract candidate en-tities.
An entity scorer ranks the candidateentities and adds the top entities to DT?s dic-tionary.
The maximum number of entitieslearned is given as an input to the system bythe developer.5.
Repeat steps 1-4 for a given number of itera-tions.SPIED provides an option to use any of the pat-tern scoring measures described in (Riloff, 1996;Thelen and Riloff, 2002; Yangarber et al., 2002;Lin et al., 2003; Gupta and Manning, 2014b).
Apattern is scored based on the positive, negative,and unlabeled entities it extracts.
The positive andnegative labels of entities are heuristically deter-mined by the system using the dictionaries and theiterative entity learning process.
The oracle labelsof learned entities are not available to the learningsystem.
Note that an entity that the system consid-ered positive might actually be incorrect, since theseed dictionaries can be noisy and the system canlearn incorrect entities in the previous iterations,and vice-versa.
SPIED?s entity scorer is the sameas in Gupta and Manning (2014a).Each candidate entity is scored using weights ofthe patterns that extract it and other entity scoringmeasures, such as TF-IDF.
Thus, learning of eachentity can be explained by the learned patterns thatextract it, and learning of each pattern can be ex-plained by all the entities it extracts.3 Visualizing Diagnostic InformationSPIED-Viz visualizes learned entities and patternsfrom one or two entity learning systems, and thediagnostic information associated with them.
Itoptionally uses the oracle labels of learned enti-ties to color code them, and contrast their ranksof correct/incorrect entities when comparing twosystems.
The oracle labels are usually determinedby manually judging each learned entity as cor-rect or incorrect.
SPIED-Viz has two views: 1. apattern-centric view that visualizes patterns of one39Score of the entity inthis system and theother system, alongwith a link to searchit on Google.An star sign for anentity indicates theentity label is notprovided and it wasnot extracted by theother system.A trophy signindicates that theentity is correctand was notextracted by theother system.List of entitieslearned at eachiteration.
Greencolor indicatesthat the entity iscorrect and redcolor indicatesthat the entity isincorrect.List of patternsthat extracted theentity.
Theirdetails are similarto the detailsshown in thepattern-centricview.Figure1:EntitycentricviewofSPIED-Viz.Theinterfaceallowstheusertodrilldowntheresultstodiagnoseextractionofcorrectandincorrectentities,andcontrastthedetailsofthetwosystems.Theentitiesthatarenotlearnedbytheothersystemaremarkedwitheitheratrophy(correctentity),athumbsdown(incorrectentity),orastaricon(oraclelabelmissing),foreasyidentification.40List of entitiesconsidered aspositive, negative,and unlabeled bythe system when itlearned thispattern.An exclamationsign indicates thatless than half ofthe unlabeledentities wereeventually learnedwith correct label.Details of thepattern.Green color ofentity indicatesthat the entity waslearned by thesystem and theoracle assigned itthe ?correct?
label.List of patternslearned at eachiteration.
Bluepattern indicatesthat the patternwas not learned bythe other system.Figure2:PatterncentricviewofSPIED-Viz.41Figure 3: When the user click on the compare icon for an entity, the explanations of the entity extractionfor both systems (if available) are displayed.
This allows direct comparison of why the two systemslearned the entity.to two systems, and 2. an entity centric view thatmainly focuses on the entities learned.
Figure 1shows a screenshot of the entity-centric view ofSPIED-Viz.
It displays following information:Summary: A summary information of each sys-tem at each iteration and overall.
It showsfor each system the number of iterations, thenumber of patterns learned, and the numberof correct and incorrect entities learned.Learned Entities with provenance: It showsranked list of entities learned by each system,along with an explanation of why the entitywas learned.
The details shown include theentity?s oracle label, its rank in the other sys-tem, and the learned patterns that extractedthe entity.
Such information can help the userto identify and inspect the patterns responsi-ble for learning an incorrect entity.
The inter-face also provides a link to search the entityalong with any user provided keywords (suchas domain of the problem) on Google.System Comparison: SPIED-Viz can be used tocompare entities learned by two systems.
Itmarks entities that are learned by one systembut not by the other system, by either display-ing a trophy sign (if the entity is correct), athumbs down sign (if the entity is incorrect),or a star sign (if the oracle label is not pro-vided).The second view of SPIED-Viz is pattern-centric.
Figure 2 shows a screenshot of the pattern-centric view.
It displays the following informa-tion.Summary: A summary information of each sys-tem including the number of iterations andnumber of patterns learned at each iterationand overall.Learned Patterns with provenance: It showsranked list of patterns along with the entitiesit extracts and their labels.
Note that each pat-tern is associated with a set of positive, neg-ative and unlabeled entities, which were usedto determine its score.2It also shows the per-centage of unlabeled entities extracted by apattern that were eventually learned by thesystem and assessed as correct by the oracle.A smaller percentage means that the patternextracted many entities that were either neverlearned or learned but were labeled as incor-rect by the oracle.Figure 3 shows an option in the entity-centricview when hovering over an entity opens a win-dow on the side that shows the diagnostic informa-tion of the entity learned by the other system.
Thisdirect comparison is to directly contrast learningof an entity by both systems.
For example, it canhelp the user to inspect why an entity was learnedat an earlier rank than the other system.An advantage of making the learning entitiescomponent and the visualization component inde-pendent is that a developer can use any patternscorer or entity scorer in the system without de-pending on the visualization component to providethat functionality.2Note that positive, negative, and unlabeled labels are dif-ferent from the oracle labels, correct and incorrect, for thelearned entities.
The former refer to the entity labels consid-ered by the system when learning the pattern, and they comefrom the seed dictionaries and the learned entities.
A positiveentity considered by the system can be labeled as incorrectby the human assessor, in case the system made a mistake inlabeling data, and vice-versa.424 System DetailsSPIED-Learn uses TokensRegex (Chang andManning, 2014) to create and apply surface wordpatterns to text.
SPIED-Viz takes details oflearned entities and patterns as input in a JSONformat.
It uses Javascript, angular, and jquery tovisualize the information in a web browser.5 Related WorkMost interactive IE systems focus on annotationof text, labeling of entities, and manual writingof rules.
Some annotation and labeling tools are:MITRE?s Callisto3, Knowtator4, SAPIENT (Li-akata et al., 2009), brat5, Melita (Ciravegna et al.,2002), and XConc Suite (Kim et al., 2008).
Akbiket al.
(2013) interactively helps non-expert usersto manually write patterns over dependency trees.GATE6provides the JAPE language that recog-nizes regular expressions over annotations.
Othersystems focus on reducing manual effort for de-veloping extractors (Brauer et al., 2011; Li et al.,2011).
In contrast, our tool focuses on visualizingand comparing diagnostic information associatedwith pattern learning systems.WizIE (Li et al., 2012) is an integrated environ-ment for annotating text and writing pattern ex-tractors for information extraction.
It also gener-ates regular expressions around labeled mentionsand suggests patterns to users.
It is most similarto our tool as it displays an explanation of the re-sults extracted by a pattern.
However, it is focusedtowards hand writing and selection of rules.
In ad-dition, it cannot be used to directly compare twopattern learning systems.What?s Wrong With My NLP?7is a tool forjointly visualizing various natural language pro-cessing formats such as trees, graphs, and entities.It can be used alongside our system to visualizethe patterns since we mainly focus on diagnosticinformation.6 Future Work and ConclusionWe plan to add a feature for a user to providethe oracle label of a learned entity using the in-terface.
Currently, the oracle labels are assignedoffline.
We also plan to extend SPIED to visualize3http://callisto.mitre.org4http://knowtator.sourceforge.net5http://brat.nlplab.org6http://gate.ac.uk7https://code.google.com/p/whatswrongdiagnostic information of learned relations from apattern-based relation learning system.
Anotheravenue of future work is to evaluate SPIED-Vizby studying its users and their interactions withthe system.
In addition, we plan to improve thevisualization by summarizing the diagnostic infor-mation, such as which parameters led to what mis-takes, to make it easier to understand for systemsthat extract large number of patterns and entities.In conclusion, we present a novel diagnostictool for pattern-based entity learning that visual-izes and compares output of one to two systems.It is light-weight web browser based visualization.The visualization can be used with any pattern-based entity learner.
We make the code of an end-to-end system freely available for research pur-pose.
The system learns entities and patterns usingbootstrapping starting with seed dictionaries, andvisualizes the diagnostic output.
We hope SPIEDwill help other researchers and users to diagnoseerrors and tune parameters in their pattern-basedentity learning system in an easy and efficient way.ReferencesAlan Akbik, Oresti Konomi, and Michail Melnikov.2013.
Propminer: A workflow for interactive infor-mation extraction and exploration using dependencytrees.
In ACL (Conference System Demonstrations),pages 157?162.Falk Brauer, Robert Rieger, Adrian Mocan, and Woj-ciech M. Barczynski.
2011.
Enabling informationextraction by inference of regular expressions fromsample entities.
In CIKM, pages 1285?1294.Angel X. Chang and Christopher D. Manning.
2014.TokensRegex: Defining cascaded regular expres-sions over tokens.
In Stanford University TechnicalReport.Laura Chiticariu, Yunyao Li, and Frederick R. Reiss.2013.
Rule-based information extraction is dead!long live rule-based information extraction systems!In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?13, pages 827?832.Fabio Ciravegna, Alexiei Dingli, Daniela Petrelli, andYorick Wilks.
2002.
User-system cooperationin document annotation based on information ex-traction.
In In Proceedings of the 13th Interna-tional Conference on Knowledge Engineering andKnowledge Management, EKAW02, pages 122?137.Springer Verlag.Michael Collins and Yoram Singer.
1999.
Unsuper-vised models for named entity classification.
In Pro-ceedings of the Joint SIGDAT Conference on Empir-43ical Methods in Natural Language Processing andVery Large Corpora, pages 100?110.Sonal Gupta and Christopher D. Manning.
2014a.
Im-proved pattern learning for bootstrapped entity ex-traction.
In Proceedings of the Eighteenth Confer-ence on Computational Natural Language Learning(CoNLL).Sonal Gupta and Christopher D. Manning.
2014b.
In-duced lexico-syntactic patterns improve informationextraction from online medical forums.
Under Sub-mission.Marti A Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th International Conference on Computationallinguistics, COLING ?92, pages 539?545.Jin-Dong Kim, Tomoko Ohta, and Jun ichi Tsujii.2008.
Corpus annotation for mining biomedicalevents from literature.
BMC Bioinformatics.Yunyao Li, Vivian Chu, Sebastian Blohm, HuaiyuZhu, and Howard Ho.
2011.
Facilitating pat-tern discovery for relation extraction with semantic-signature-based clustering.
In Proceedings of the20th ACM International Conference on Informa-tion and Knowledge Management, CIKM ?11, pages1415?1424.Yunyao Li, Laura Chiticariu, Huahai Yang, Freder-ick R. Reiss, and Arnaldo Carreno-fuentes.
2012.Wizie: A best practices guided development envi-ronment for information extraction.
In Proceedingsof the ACL 2012 System Demonstrations, ACL ?12,pages 109?114.Maria Liakata, Claire Q, and Larisa N. Soldatova.2009.
Semantic annotation of papers: Interface &enrichment tool (sapient).
In Proceedings of theBioNLP 2009 Workshop, pages 193?200.Winston Lin, Roman Yangarber, and Ralph Grishman.2003.
Bootstrapped learning of semantic classesfrom positive and negative examples.
In Proceed-ings of the ICML 2003 Workshop on The Continuumfrom Labeled to Unlabeled Data in Machine Learn-ing and Data Mining.Ellen Riloff.
1996.
Automatically generating extrac-tion patterns from untagged text.
In Proceedingsof the 13th National Conference on Artificial Intelli-gence, AAAI?96, pages 1044?1049.Michael Thelen and Ellen Riloff.
2002.
A bootstrap-ping method for learning semantic lexicons usingextraction pattern contexts.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?02, pages 214?221.Roman Yangarber, Ralph Grishman, and PasiTapanainen.
2000.
Automatic acquisition ofdomain knowledge for information extraction.
InProceedings of the 18th International Conferenceon Computational Linguistics, COLING ?00, pages940?946.Roman Yangarber, Winston Lin, and Ralph Grishman.2002.
Unsupervised learning of generalized names.In Proceedings of the 19th International Conferenceon Computational Linguistics, COLING ?02.44
