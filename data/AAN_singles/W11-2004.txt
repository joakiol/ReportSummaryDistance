Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 18?29,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsA Two-Stage Domain Selection Framework for Extensible Multi-DomainSpoken Dialogue SystemsMikio NakanoHonda Research Institute JapanWako, Saitama, Japannakano@jp.honda-ri.comShun SatoTokyo Denki UniversityHatoyama, Saitama, Japanrela.relakuma@gmail.comKazunori KomataniNagoya UniversityNagoya, Aichi, Japankomatani@nuee.nagoya-u.ac.jpKyoko Matsuyama?Kyoto UniversityKyoto, Kyoto, Japanmatuyama@kuis.kyoto-u.ac.jpKotaro FunakoshiHonda Research Institute JapanWako, Saitama, Japanfunakoshi@jp.honda-ri.comHiroshi G. OkunoKyoto UniversityKyoto, Kyoto, Japanokuno@i.kyoto-u.ac.jpAbstractThis paper describes a general and effectivedomain selection framework for multi-domainspoken dialogue systems that employ dis-tributed domain experts.
The framework con-sists of two processes: deciding if the currentdomain continues and estimating the probabil-ities for selecting other domains.
If the currentdomain does not continue, the domain withthe highest activation probability is selected.Since those processes for each domain expertcan be designed independently from other ex-perts and can use a large variety of informa-tion, the framework achieves both extensibil-ity and robustness against speech recognitionerrors.
The results of an experiment usinga corpus of dialogues between humans anda multi-domain dialogue system demonstratethe viability of the proposed framework.1 IntroductionAs spoken dialogue interfaces are becoming morewidely utilized, they will be expected to be able toengage in dialogues in a wide variety of topics.
Par-ticularly, spoken dialogue interfaces for office robots(Asoh et al, 1999) and multimodal kiosk systems(Gustafson and Bell, 2000) are expected to deal withpeople?s various requests, unlike automated call cen-ter systems that are dedicated to specific tasks.One effective methodology to build such a sys-tem is to integrate systems in small domains byemploying distributed multi-domain system archi-tecture.
This architecture has distributed modules?Currently with Panasonic Corporation.that independently manage their own dialogue stateand knowledge for speech understanding and ut-terance generation (e.g., Lin et al (1999)).
Froman engineering viewpoint, such architecture has anadvantage in that each domain expert can be de-signed independently and that it is easy to add newdomains.
It enables each domain expert to em-ploy a dialogue strategy very different from thosefor other domains.
For example, the strategy maybe frame-based mixed-initiative, finite-state-basedsystem-initiative, or plan-based dialogue manage-ment (McTear, 2004).One of the crucial issues with distributed multi-domain spoken dialogue systems is how to select anappropriate domain for each user utterance so thatthe system can appropriately understand it and an-swer it.
So far several methods have been proposedbut none of them satisfy two basic requirements atthe same time: the ability to be used with a varietyof domain experts (extensibility) and being robustagainst ASR (Automatic Speech Recognition) errors(robustness).
We suspect that this is one of themain reasons why not many multi-domain spokendialogue systems have been developed even thoughtheir utility is widely recognized.This paper presents a new general framework fordomain selection that satisfies the above two require-ments.
In our framework, each expert needs to havetwo additional submodules: one for estimating theprobability that it is newly activated, and one for de-ciding domain continuation when it is already acti-vated.
Since these submodules can be designed in-dependently from those of other experts, there is norestriction on designing experts in our framework,18and thus extensibility is achieved.
Robustness is alsoachieved because those submodules can be designedso that they can utilize domain-dependent informa-tion, including information on speech understandingand dialogue history, without detracting from ex-tensibility.
Especially the submodule for decidingdomain continuation has the ability to utilize dia-logue history to avoid erroneous domain shifts thatoften occur in previous approaches.
Note that we donot focus on classifying each utterance without con-textual information (e.g., Chu-Carroll and Carpenter(1999)).
Rather, we try to estimate the user inten-tion with regard to continuing and shifting domainsin the course of dialogues.In what follows, Section 2 explains the distributedmulti-domain spoken dialogue system architectureand requirements for domain selection.
Section 3discusses previous work, and Section 4 presents ourproposed framework.
Section 5 describes an exam-ple implementation and its evaluation results, andSection 6 concludes the paper.2 Domain Selection in Multi-DomainSpoken Dialogue Systems2.1 Distributed ArchitectureIn distributed multi-domain spoken dialogue archi-tecture (Figure 1), distributed modules indepen-dently manage their own dialogue state and knowl-edge for speech understanding and utterance gener-ation (Lin et al, 1999; Salonen et al, 2004; Pakucs,2003; Nakano et al, 2008).
Although those modulesare referred to with various names in that literature,we call them domain experts in this paper.
In thisarchitecture, when an input utterance is received, itsASR results are sent to domain experts.
They try tounderstand the ASR results using their own knowl-edge for understanding.
The domain selector gathersinformation from those experts and decides whichexpert should deal with the utterance and then de-cide on the system utterances.
In this paper, the do-main expert engaging in understanding user utter-ances and deciding system utterances is called acti-vated.2.2 Example SystemsSo far many multi-domain spoken dialogue sys-tems based on distributed architecture have beenuser utteranceinformation for domain selectiondomain selectoractivate/deactivatesystem utterance(from the activatedexpert)speech understanding utterance generationdomain expert 1domain expert 2domain expert 3dialoguehistoryFigure 1: Distributed multi-domain spoken dialogue sys-tem architecture.built and have demonstrated their ability to engagein dialogues in a variety of domains.
For exam-ple, several systems integrated information provid-ing and database searches in multiple domains (Linet al, 1999; Komatani et al, 2006; O?Neill et al,2004; Gustafson and Bell, 2000).
Some other sys-tems integrated domain experts that employ verydifferent dialogue strategies.
Lee et al (2009) andNakano et al (2006) integrated task-oriented andnon-task-oriented dialogue managements.
Nakanoet al (2008) integrated domain experts for not onlydialogues but also tasks requiring physical actions.Below we explain an example system that weused to collect dialogue data for the domain se-lection experiment described in Section 5.
It is aJapanese system that can provide information onUNESCOWorld Heritage Sites using speech, slides,and Microsoft Agent1 gestures.
It employs the fol-lowing ten domain experts:A question answering (QA) expert: It has adatabase consisting of question-answer pairs onWorld Heritage Sites (Narimatsu et al, 2010).
Eachquestion-answer pair consists of a set of examplequestion sentences and answers to them.
The an-swers consist of one or more utterances, plus slides.Keyphrase-based matching is used to select an ex-ample question that matches the input utterance.Eight interactive presentation (IP) experts: Eachof them explains in detail a famous World Her-itage Site and adds a detailed explanation or skipssome explanation according to the user?s interrup-tions (Nishimura et al, 2007).
Patterns of interrup-tion utterances are described in the expert?s knowl-edge base.1http://www.microsoft.com/MSAgent/19utterance expertS1: Hello.
I can answer questions about fa-mous World Heritage Sites in Italy, Ger-many, Greece, and other areas.
I can alsoexplain some World Heritage Sites in de-tail.
Please ask me anything.QAU1: Show me the list of the World HeritageSites in Italy.QAS2: (show a slide of the list) I can tell you aboutthe World Heritage Sites in these areas inItaly.QAU2: Can you show me the list of World Her-itage Sites you can explain in detail?QAS3: (show a slide of the list) I can explain thesein detail.QAU3: Can you explain Maritime Greenwich?
RUS4: Maritime Greenwich, right?
RUU4: Yes.
RUS5: (show a slide on Greenwich) Okay.
I?ll ex-plain Maritime Greenwich, a historic porttown known for Greenwich Mean Time.IP1U5: What is Greenwich Mean Time?
IP1S6: Greenwich Mean Time is the mean solartime at the Royal Greenwich Observatory.IP1S means system utterance and U means user utterance.IP1 is one of the IP experts.Figure 2: An example dialogue with the system for datacollection (translation from Japanese).A request understanding (RU) expert: It under-stands the user?s request to start one of the interac-tive presentations explained above, and engages ina dialogue to confirm the request.
When the under-standing finishes, the understood request is sent toa module called task planner (Nakano et al, 2008;Nakano et al, 2011).
The task planner then activatesanother expert to perform the requested presentation(S5 in Figure 2).Figure 2 shows an example dialogue between ahuman and this system.
Note that user utterancesare relatively short and include words related to spe-cific World Heritage Sites or area names.
If thosewords are misrecognized, domain selection is diffi-cult unless dialogue context information is used.This figure also indicates the domain experts thatunderstood each user utterance and selected eachsystem utterance.
The domain expert that shoulddeal with a user utterance is decided based on the setof user utterances that the expert is designed to dealwith.
The domains of utterances U1 and U3 are dif-ferent because the QA expert has knowledge for un-derstanding U1 and the RU expert has knowledge forunderstanding U3.
Thus, in this study, the domain ofeach utterance is determined based on the design ofthe experts employed in the system.
If none of theexperts can deal with an utterance, it is consideredas an out-of-domain utterance.
Sometimes the cor-rect domain needs to be determined using contextualinformation.
For example, utterance U4 ?Yes?
canappear in all domains, but, since this is a reply to S4,its domain is RU.This definition of domain is different from that ofdomain (or topic) recognition and adaptation stud-ies in text, monologue, and human-human conver-sation processing, in which reference domains areannotated based on human perspectives rather thansystem perspectives.
From a human perspective, alluser utterances in Figure 2 may be in ?World Her-itage Site?
domain.
However, it is not always easyto build domain experts according to such domaindefinitions, because different dialogue tasks in onesuch domain may require different dialogue strate-gies (such as question answering and request under-standing).2.3 Requirements for Domain SelectionWe pursue a method for domain selection that canbe used in distributed architecture.
Such a methodmust satisfy the following two requirements.Extensibility It must not detract from the extensi-bility of distributed architecture, that is, any kind ofexpert must be able to be incorporated, and each ex-pert must be able to be designed independently fromother experts.
This requires the interface betweeneach domain expert and the domain selector to be assimple as possible.Robustness It needs to be robust against ASR er-rors; that is, the system needs to be able to avoiderroneous domain transition caused by ASR errors.3 Previous WorkSo far various methods for domain selection havebeen proposed, but, as far as we know, no methodsatisfies both extensibility and robustness.
Isobe etal.
(2003) estimate a score for each domain from the20ASR result and select the domain with the highestscore (hereafter referred to as RECSCORE).
Sinceeach domain expert has only to output a numericscore, it satisfies extensibility.
However, becausethis method does not take into account dialogue con-text, it tends to erroneously shift domains when thescore of some experts becomes high by chance.
Forexample, if U4:?Yes?
in Figure 2 is recognized as?Italy?
with a high recognition score in the QA ex-pert, the domain erroneously shifts to QA and thesystem explains about World Heritage Sites in Italy.Thus this method is not robust.To avoid erroneous domain shifts, Lin et al(1999) give preference to the preceding domain(the domain in which the previous system utterancewas made) by adding a certain value to the scoreof the preceding domain (hereafter called REC-SCORE+BIAS ).
However, to what extent the do-main tends to continue varies depending on the dia-logue context.
For example, if a dialogue task in onedomain finishes (e.g., when an IP expert finishes itspresentation and says ?This is the end of the presen-tation.
Do you have any questions??
), the domain islikely to shift.
So, adding a fixed score does not al-ways work.
O?Neill et al?s (2004) system does notchange the dialogue domain until it finishes a taskin the domain, but it cannot recover from erroneousdomain shifts.To achieve robustness against ASR errors, severaldomain selection methods based on a classifier thatuses features concerning dialogue history as well asones concerning speech understanding results havebeen developed (Komatani et al, 2006; Ikeda et al,2008; Lee et al, 2009).
These studies, however, usesome features available only in some specific typeof domain experts, such as features concerning slot-filling, so they cannot be used with other kinds ofdomain experts.
That is, these methods do not sat-isfy extensibility.Methods that use classifiers based on word (andn-gram) frequencies have been developed for utter-ance classification (e.g., Chu-Carroll and Carpenter(1999)), topic estimation for ASR of speech cor-pora (e.g., Hsu and Glass (2006) and Heidel andLee (2007)) and human-human dialogues (Lane andKawahara, 2005).
These methods can be applied todomain selection in multi-domain spoken dialoguesystems.
However, since they require training datain the same set of domains as the target system, itdetracts from extensibility.
In addition, they are notrobust because they cannot utilize a variety of di-alogue and understanding related features.
Wordfrequencies are not always effective when two do-mains share words as in our system described in Sec-tion 2.2.4 Proposed Framework4.1 Basic IdeaTo achieve extensibility, we need to restrict the infor-mation that each expert sends to the domain selectorto a simple one such as numeric scores.
AlthoughRECSCORE and RECSCORE+BIAS satisfy this, theywould not achieve high accuracy as explained above.One possible extension to those methods to im-prove accuracy is to use not only recognition scoresbut also various expert-dependent features such asones concerning dialogue history and speech under-standing.
Each expert first estimates the probabilitythat the input utterance is in its domain using suchfeatures, and then the expert with the highest proba-bility is selected (hereafter called MAXPROB).
Thismethod retains extensibility because the domain se-lector does not directly use those expert-dependentfeatures.
However, it suffers from the same prob-lem as RECSCORE and RECSCORE+BIAS; if one ofthe experts other than the preceding domain?s expertoutputs a high probability by mistake, the domainshifts regardless of the dialogue state in the preced-ing domain?s expert.We focus attention on the fact that the domaindoes not often shift.
Our idea is to decide if the do-main continues or not by using information availablein the preceding domain?s expert.
This prevents er-roneous domain shifts when the utterance is consid-ered not to change the domain.
When it is decidedthat the currently active domain does not continue,each remaining expert estimates the probability ofbeing newly activated using information available inthe expert, and the expert whose probability is thehighest is selected as the new domain expert.We further refine this idea in two ways.
One is bytaking into account how likely the input utterance isto activate one of the other domain experts.
We pro-pose to use the maximum value of probabilities forother experts?
activation (maximum activation prob-21user utteranceexpert for the preceding domainexperts for other domainsmaximumprobabilityselect thepreceding domainselect the domain with maximumactivation probabilityfeaturesStage 1Stage 2...domain continuationdecision makerselect expert with maximumactivationprobabilityspeech understandingfeatures activation probability estimatorspeech understandingfeatures activation probability estimatorspeech understandingdecision is tocontinue?
yesnoout-of-domainhandle as an out-of-domain utterancedialoguehistorydialoguehistorydialoguehistoryFigure 3: Two-stage domain selection framework.ability) in the decision regarding domain continua-tion.
Since the maximum activation probability isjust a numeric score, this does not spoil extensibil-ity.
Unlike RECSCORE and RECSCORE+BIAS, inour method, even if the maximum activation prob-ability is very high, the preceding domain?s expertcan decide to continue or not to continue based onits internal state.
This makes it possible to retain ro-bustness.The other refinement is to explicitly deal with ut-terances that are not in any domains (out-of-domain(OOD) utterances).
They include fillers and mur-murs.
They should be treated separately, becausethey appear context-independently.
So we make theexpert detect OOD utterances when deciding do-main continuation.
That is, it performs three-foldclassification, continue, not-continue, and OOD.4.2 Two-Stage Domain Selection FrameworkThis idea can be summarized as a domain selectionframework which consists of two stages (Figure 3).It assumes that each domain expert has two submod-ules: activation probability estimator and a domaincontinuation decision maker, which use informationavailable in the expert itself.When a new input utterance is received, at Stage1, the activation probability estimators of all non-activated experts estimate probabilities and sendthem to the domain selector.
Then at Stage 2, thedomain selector sends their maximum value to theexpert of the preceding domain and asks it to decidewhether it continues to deal with the new input utter-ances or does not continue, or it deals with the utter-ance as out-of-domain.
If it decides not to continue,the domain selector selects the expert that outputsthe highest probability at Stage 1.The reason we use the term ?framework?
is thatit does not specify the details of the algorithm andfeatures used in each domain expert?s submodulesfor domain selection.
It rather specifies the inter-faces of those submodules.
Note that RECSCORE,RECSCORE+BIAS, and MAXPROB can be consid-ered as one of the implementations of this frame-work.
This framework, however, allows developersto use a wider variety of features and gives flexibilityin designing those submodules.5 Example Implementation andEvaluationSince the proposed framework is an extension ofthe previous methods, if the activation probabilityestimator and domain continuation decision makerfor each expert are designed well and trained usingenough data, it should outperform previous methodsthat satisfy extensibility.
We believe that this theo-retical consideration and an experimental result us-ing a human-system dialogue corpus show the via-bility of the framework.
Below we explain our im-plementation and an experiment.5.1 DataFor the implementation and evaluation, we used acorpus of dialogues between human users and theWorld Heritage Site information system describedin Section 2.2.
Domain selection of this system wasperformed using hand-crafted rules.35 participants (17 males and 18 females) whoseages range from 19 to 57 were asked to engage in22domain preceding training training testdomain data A data B dataRU RU 134 169 145QA 51 102 59IP 21 16 23subtotal 206 287 227QA RU 46 55 51QA 783 870 888IP 59 87 66subtotal 888 1,012 1,005IP RU 2 1 3QA 7 11 18IP 311 305 277subtotal 320 317 298OOD RU 24 19 39QA 168 155 183IP 66 68 113subtotal 258 242 335total 1,672 1,858 1,865Table 1: Number of utterances in each domain in thetraining and test data.conversation with the system four times.
Each ses-sion lasted eight minutes.
For each utterance, thecorrect domain or an OOD label was manually an-notated.
We also annotated its preceding domain,i.e., the domain in which the previous system utter-ance was made.
It can be different from the previoususer utterance?s domain because of the system?s er-roneous domain selection.
Utterances including re-quests in two domains at the same time should begiven an OOD label but there are no such utterances.We used data from 23 participants (3,530 utterances)for training and those from the remaining 12 par-ticipants (1,865 utterances) for testing.
We furthersplit the training data into training data A (1,672utterances) and B (1,858 utterances) to train eachof the two submodules.
Each training data set in-cludes data from two sessions for each participant.Table 1 shows detailed numbers of utterances in thedata sets.5.2 Implementation5.2.1 Expert ClassesAmong the ten experts, eight IP (Interactive Pre-sentation) experts have the same dialogue strategyand most of the predicted user utterance patterns.
Inaddition, the number of training utterances for eachexpert class QA IP RULM for ASR trigram trigram finite-stategrammarlanguage keyphrase keyphrase finite-stateunderstanding -based -based transducervocabulary 1,140 407 79size (word)phone error 10.95 19.47 23.60rate (%)Table 2: Speech understanding in each expert.IP expert?s domain is small.
We therefore used alltraining utterances in the IP domains to build a com-mon ASR language model (LM), a common acti-vation probability estimator, and a common domaincontinuation decision maker for all IP experts.
Here-after we call the set of IP experts the IP expert class.The RU (Request Understanding) expert and the QA(Question Answer) expert are themselves also expertclasses.5.2.2 Speech UnderstandingFor all experts, we used the Julius speech recog-nizer and the acoustic model in the Japanese modelrepository (Kawahara et al, 2004).2 Features ofspeech understanding in each expert class are shownin Table 2.
Compared to the system used for datacollection, LMs are enhanced based on the trainingdata.
We obtained the ASR performance on the ut-terances in each domain in the test data in terms ofphone error rates.
This is because Japanese has nostandard word boundaries so it is not easy to cor-rectly compute word error rates.
The poor perfor-mance of ASR for IP is mainly due to the smallamount of training utterances for LM and that forRU is mainly due to out-of-grammar utterances.5.2.3 Stage 1For Stage 1, we used logistic regression to es-timate the probability that a non-activated expertwould be activated by a user utterance.
Features forlogistic regression include those concerning speechrecognition and understanding results as well as dia-logue history (see Table 5 for the full list of features).These features are expert-dependent.
This makes itpossible to estimate how the input utterance is suit-2Multiple LMs can be used at the same time with Julius.23able to the dialogue context more precisely than us-ing just features available in any kind of expert.To train the activation probability estimators, wefitted logistic regression coefficients using Wekadata mining toolkit ver.3.6.2 (Witten and Frank,2005)3 and training data A.
In the training for eachexpert class, we used utterances whose precedingdomain was not that of the class because activationprobabilities are estimated only for such utterancesduring domain selection.
If the utterance is in a do-main of the expert class, it is assigned an activate la-bel and otherwise not-activate.
Next, we performedfeature selection to avoid overfitting.
We used back-ward stepwise selection so that the weighted (by thesizes of activate and not-activate labels) average ofthe F1 scores for training set B could be maximized.Table 6 lists the remaining features and their sig-nificances in terms of the F1 score obtained wheneach feature is removed.
Then, we duplicated theactivate-labeled utterances in the training data A sothat the ratio of activate-labeled utterances to not-activate-labeled utterances became 1 to 3.
This isbecause the training data include a larger number ofnot-activate-labeled utterances and thus the resultswould be biased.
The ratio was decided by trial anderror so that the weighted average of the F1 scoresfor training data B becomes high.5.2.4 Stage 2For Stage 2, we used multi-class support vectormachines (SVMs)4 to decide if the activated expertshould continue to be activated, should not continue,or should regard the input utterance as OOD.
Weused the same set of features as Stage 1 as wellas the maximum activation probability obtained atStage 1.
The training data for the SVM of each ex-pert class is the set of utterances in training data Bwhose preceding domain is in that expert class, be-cause domain continuation is decided only for suchutterances during domain selection.
They are la-beled continue, not-continue, or OOD.
Next, weperformed backward stepwise feature selection sothat the weighted average of F1 scores for continue,not-continue, and OOD utterance detection on train-ing data A could be maximized.
Remaining fea-3Multinominal logistic regression model with a ridge esti-mator with Weka?s default values.4Weka?s SMO with the linear kernel and its default values.tures are listed in Table 7.
The maximum activa-tion probability was found to be significant in all ex-pert classes.
This suggests our two-stage frameworkthat uses maximum activation probability is viable.Then, we duplicated utterances with not-continue la-bel and OOD label in the training data so that theratio of continue, not-continue, and OOD utterancesbecame 3:1:1.
This is because the number of utter-ances with the continue label is far greater than oth-ers.
The ratio was experimentally decided by trialand error so that the weighted average of F1 scoreson training data A becomes high.5.3 Evaluation5.3.1 Compared MethodsWe compared the full implementation described inSection 5.2 (FULLIMPL hereafter) with the follow-ing four methods which satisfy extensibility.
Notethat the first three methods were mentioned in Sec-tion 4.RECSCORE: This chooses the expert class whoserecognition score is the maximum (Isobe et al,2003).
We used the ASR acoustic score normalizedby the duration of the utterance.
If the IP expert classwas chosen, the IP expert that had been most re-cently activated was chosen, because, in this system,domain shifts to other IP experts never occur due tothe system constraints and the user did not try to doit.
If none of the experts had a higher score than afixed threshold, it recognized the utterance as OOD.The threshold was experimentally determined usingthe training data so that the weighted (by the sizesof OOD and non-OOD utterances) average of theF1 scores of OOD/non-OOD classification is max-imized.RECSCORE+BIAS: This is the same as REC-SCORE except that a fixed value (bias) is added tothe score used in RECSCORE for the expert of thepreceding domain.
This is basically the same as Linet al?s (1999) method but we use a different recog-nition score since the recognition score they usedcannot be used in our system due to the differenceof speech understanding methods.
The most appro-priate bias for each expert class was decided usingthe training data so that the weighted average of theF1 scores could be maximized.
OOD detection wasdone in the same way as RECSCORE.24method class recall prec- F1 weightedision ave. F1RECSCORE cont.
0.763 0.867 0.812shift 0.559 0.239 0.335OOD 0.501 0.848 0.630 0.789RECSCORE cont.
0.917 0.824 0.868+BIAS shift 0.400 0.421 0.410OOD 0.501 0.848 0.630 0.838MAXPROB cont.
0.925 0.843 0.882shift 0.282 0.264 0.273OOD 0.275 0.477 0.348 0.832NOACTIV cont.
0.875 0.890 0.882PROB shift 0.464 0.385 0.421OOD 0.785 0.843 0.813 0.849FULLIMPL cont.
0.902 0.907 0.904shift 0.591 0.565 0.578OOD 0.824 0.829 0.826 0.883CLASSIFIER cont.
0.956 0.881 0.917(reference) shift 0.545 0.759 0.635OOD 0.755 0.885 0.815 0.899Table 3: Evaluation results (?cont.?
means ?continue.?
).MAXPROB: The activation probabilities for all ex-perts were obtained using logistic regression and theexpert whose probability was the maximum was se-lected.
IP experts that had never been activated wereexcluded because they cannot be activated due tosystem constraint.
For logistic regression, in addi-tion to the features used in FULLIMPL, the previousdomain was used as a feature so that domain conti-nuity was taken into account.
Feature selection wasalso performed.
The probability that the utterance isOOD was estimated in the same way using the fea-tures concerning speech understanding.
If the maxi-mum probability of OOD detection was greater thanthe maximum activation probability, then the utter-ance was considered to be OOD.NOACTIVPROB: This is the same as FULLIMPLexcept that Stage 2 does not use the result of Stage1, i.e., maximum activation probability.5.3.2 Evaluation ResultsTo evaluate the domain selection, we focused ondomain shifts rather than the selected domain.
Weclassified the domain selection results into domaincontinuations, domain shifts, and OOD utterancedetection.
As the evaluation metric, we used theweighted average of F1 scores for those classes.Here the weight is the ratio of those classes of cor-rect labels.
Note that shifting to an incorrect do-main is counted as a false positive when calculat-ing precision for domain shifts.
Table 3 shows theresults.
In addition, the confusion matrices for thethree best methods are shown in Table 4.
We foundFULLIMPL outperforms the other four methods.
Wealso found that the differences between the results ofthe compared methods are all statistically significant(p < .01) by two-tailed binomial tests.For reference, we also evaluated a classifier-basedmethod that uses features from all the experts.
Notethat this method does not satisfy extensibility be-cause it requires training data in the same set of do-mains as the target system.
We evaluated this justfor estimating how well our proposed method workswhile satisfying extensibility.
It classifies each ut-terance into one of four categories: the QA expert?sdomain, the RU expert?s domain, the most recentlyactivated IP expert?s domain, and OOD.
If no IP ex-pert has been activated before the utterance, three-fold classification was performed.
The training andtest data were split depending on whether one of theIP experts has been activated before, and trainingand testing were separately conducted.
The trainingdata A was used for training SVM classifiers.
Thenfeature selection was performed using the trainingdata B.
The performance of this method is shownas CLASSIFIER in Tables 3 and 4.
Although thismethod outperforms FULLIMPL, FULLIMPL?s per-formance is close to this method.
This shows thatour method does not degrade its performance verymuch even though it satisfies extensibility.5.3.3 DiscussionOne of the reasons why FULLIMPL outperformsother methods is that its precision for domain shiftsis relatively higher than the other methods.
Thissuggests it can avoid erroneous domain shifts, thusthe proposed two-stage framework is more robust.RECSCORE+BIAS performed relatively well despiteit used only limited features.
We guess this is be-cause adding preferences to the preceding domainwas effective since domain shifts are rare in thesedata.
Its low F1 score for OOD utterances suggestsusing just recognition scores is insufficient to detectthem.
The comparison of FULLIMPL with NOAC-TIVPROB shows the effectiveness of using maxi-mum activation probability in the second stage.The F1 score for domain shifts is low even with25RECSCORE+BIAS:estimated resultcorrect cont.
correct wrong OOD totalshift shiftcontinue 1,201 - 82 27 1,310shift 115 88 14 3 220OOD 142 - 25 168 335total 1,458 88 121 198 1,865NOACTIVPROB:estimated resultcorrect cont.
correct wrong OOD totalshift shiftcontinue 1,146 - 123 41 1,310shift 92 102 18 8 220OOD 50 - 22 263 335total 1,288 102 163 312 1,865FULLIMPL:estimated resultcorrect cont.
correct wrong OOD totalshift shiftcontinue 1,181 - 77 52 1,310shift 70 130 15 5 220OOD 51 - 8 276 335total 1,302 130 100 333 1,865CLASSIFIER (reference):estimated resultcorrect cont.
correct wrong OOD totalshift shiftcontinue 1,252 - 30 28 1310shift 92 120 3 5 220OOD 77 - 5 253 335total 1,421 120 38 286 1,865Table 4: Confusion matrices for the domain shifts.FULLIMPL, although it is higher than those withother methods.
One typical reason for this is thatwhen one keyword in the ASR result of an utter-ance to shift the domain is also in the vocabulary ofthe preceding domain?s expert, the selection tends tocontinue the previous domain by mistake.
For ex-ample, an utterance ?tell me about other World Her-itage Sites?
to shift from an IP domain to the QAdomain is sometimes misclassified as an IP domainutterance, because ?World Heritage Sites?
is also inIP domains?
vocabulary.
We think this is becausethe training data do not include a sufficient amountof utterances that shift domains, and that a largeramount of training data would solve this problem.6 Concluding RemarksThis paper presented a novel general framework fordomain selection in extensible multi-domain spokendialogue systems.
This framework makes it possi-ble to build a robust domain selector because of itsflexibility in exploiting features and taking into ac-count domain continuity.
An experiment with datacollected with an example multi-domain system sup-ported the viability of the proposed framework.
Webelieve that this framework will promote the devel-opment of multi-domain spoken dialogue systemsand conversational robots/agents.Among future work is to investigate how accuratethe activation probability estimator and the domaincontinuation decision maker in each domain expertshould be for achieving a reasonable accuracy in do-main selection.
We also plan to conduct experimentswith systems that have a larger number of domainexperts to verify the scalability of this framework.In addition, we will explore a way to estimate theconfidence of the domain selection to reduce erro-neous domain selections.AcknowledgmentsThe authors would like to thank Hiroshi Tsujino,Yuji Hasegawa, and Hiromi Narimatsu for their sup-port for this research.ReferencesHideki Asoh, Toshihiro Matsui, John Fry, Futoshi Asano,and Satoru Hayamizu.
1999.
A spoken dialog systemfor a mobile office robot.
In Proc.
6th Eurospeech,pages 1139?1142.Jennifer Chu-Carroll and Bob Carpenter.
1999.
Vector-based natural language call routing.
ComputationalLinguistics, 25(3):361?388.Joakim Gustafson and Linda Bell.
2000.
Speech tech-nology on trial: Experiences from the August system.Natural Language Engineering, 6(3&4):273?286.Aaron Heidel and Lin-shan Lee.
2007.
Robust topic in-ference for latent semantic language model adaptation.In Proc.
ASRU-07, pages 177?182.Bo-June (Paul) Hsu and James Glass.
2006.
Style andtopic language model adaptation using HMM-LDA.In Proc.
EMNLP ?06, pages 373?381,.Satoshi Ikeda, Kazunori Komatani, Tetsuya Ogata, andHiroshi G. Okuno.
2008.
Extensibility verification26of robust domain selection against out-of-grammar ut-terances in multi-domain spoken dialogue system.
InProc.
Interspeech-2008 (ICSLP), pages 487?490.T.
Isobe, S. Hayakawa, H. Murao, T. Mizutani,K.
Takeda, and F. Itakura.
2003.
A study on do-main recognition of spoken dialogue systems.
In Proc.Eurospeech-2003, pages 1889?1892.Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-sunobu Itou, and Kiyohiro Shikano.
2004.
Recentprogress of open-source LVCSR engine Julius andJapanese model repository.
In Proc.
Interspeech-2004(ICSLP), pages 3069?3072.Kazunori Komatani, Naoyuki Kanda, Mikio Nakano,Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata,and Hiroshi G. Okuno.
2006.
Multi-domain spo-ken dialogue system with extensibility and robustnessagainst speech recognition errors.
In Proc.
7th SIGdialWorkshop, pages 9?17.Ian R. Lane and Tatsuya Kawahara.
2005.
Incorporatingdialogue context and topic clustering in out-of-domaindetection.
In Proc.
ICASSP-2005, pages 1045?1048.Cheongjae Lee, Sangkeun Jung, Seokhwan Kim, andGary Geunbae Lee.
2009.
Example-based dialogmodeling for practical multi-domain dialog system.Speech Communication, 51(5):466?484.Bor-shen Lin, Hsin-ming Wang, and Lin-shan Lee.
1999.A distributed architecture for cooperative spoken dia-logue agents with coherent dialogue state and history.In Proc.
ASRU-99.Michael F. McTear.
2004.
Spoken Dialogue Technology.Springer.Mikio Nakano, Atsushi Hoshino, Johane Takeuchi,Yuji Hasegawa, Toyotaka Torii, Kazuhiro Nakadai,Kazuhiko Kato, and Hiroshi Tsujino.
2006.
A robotthat can engage in both task-oriented and non-task-oriented dialogues.
In Proc.
Humanoids-2006, pages404?411.Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, andHiroshi Tsujino.
2008.
A framework for building con-versational agents based on a multi-expert model.
InProc.
9th SIGdial Workshop, pages 88?91.Mikio Nakano, Yuji Hasegawa, Kotaro Funakoshi, Jo-hane Takeuchi, Toyotaka Torii, Kazuhiro Nakadai,Naoyuki Kanda, Kazunori Komatani, Hiroshi G.Okuno, and Hiroshi Tsujino.
2011.
A multi-expertmodel for dialogue and behavior control of conversa-tional robots and agents.
Knowledge-Based Systems,24(2):248?256.Hiromi Narimatsu, Mikio Nakano, and Kotaro Fu-nakoshi.
2010.
A classifier-based approach tosupporting the augmentation of the question-answerdatabase for spoken dialogue systems.
In Proc.
2ndIWSDS, pages 182?187.Yoshitaka Nishimura, Shinichiro Minotsu, Hiroshi Dohi,Mitsuru Ishizuka, Mikio Nakano, Kotaro Funakoshi,Johane Takeuchi, Yuji Hasegawa, and Hiroshi Tsujino.2007.
A markup language for describing interactivehumanoid robot presentations.
In Proc.
IUI?07, pages333?336.Ian O?Neill, Philip Hanna, Xingkun Liu, and MichaelMcTear.
2004.
Cross domain dialogue modelling:an object-based approach.
In Proc.
Interspeech-2004(ICSLP), pages 205?208.Botond Pakucs.
2003.
Towards dynamic multi-domaindialogue processing.
In Proc.
Eurospeech-2003, pages741?744.Esa-Pekka Salonen, Mikko Hartikainen, Markku Tu-runen, Jaakko Hakulinen, and J. Adam Funk.
2004.Flexible dialogue management using distributed anddynamic dialogue control.
In Proc.
Interspeech-2004(ICSLP), pages 197?200.Ian H.Witten and Eibe Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques, 2nd Edi-tion.
Morgan Kaufmann, San Francisco.27expert Featuresclassall Fi,r1 If SRRi,1 is obtained or notclasses Fi,r2 If SRRi,1 contains a filler or noti = ru, Fi,r3 min (CMs of words in SRRi,1)ip, qa Fi,r4 avg (CMs of words in SRRi,1)Fi,r5 (acoustic score of SRRi,1) / durationFi,r6 LM score of SRRi,1Fi,r7 # of words in SRRi,1Fi,r8 # of words in SRRi,allFi,r9 (Fi,r5 - (acoustic score of SRRlv,1))/ durationRU Fru,r10 If SRRru,1 is an affirmative responseFru,r11 If SRRru,1 is a denial responseFru,r12 # of ASR results with LMruFru,r13 If SRRru,1 contains the name of aWorld Heritage SiteFru,r14 max (CMs of words comprising thename of a World Heritage Site)Fru,r15 ave (CMs of words comprising thename of a World Heritage Site)Fru,h1 If SRRru,1 is an affirmative response(Stage 2 only)Fru,h2 # of turns since this expert is acti-vatedFru,h3 # of denial responses recognizedsince this expert is activatedFru,h4 Fru,h4/Fru,h3Fru,h5 If the previous system utterance is aconfirmation request to a user requestfor starting a presentationFru,h6 If the previous system utterance isan utterance to react to a non-understandable user utteranceFru,h7 If the system has made a confirma-tion request to a user request for start-ing a presentation since this expertwas activatedFru,h8 If the system has made an utteranceto react to a non-understandable userutterance since this expert was acti-vatedFru,h9 If the system has made a confirma-tion request to a user request for start-ing a presentation beforeFru,h10 If the system has made an utteranceto react to a non-understandable userutterance beforeexpert FeaturesclassIP Fip,r10 If the SRRip,1 is out of databaseFip,r11Pj((# of keyphrases in SRRip,j) / (# of words inSRRip,j) ) / (# of ASR results)Fip,r12 mini( # of keyphrasei in SRRip,all / (# of ASR re-sults))Fip,r13 maxi( # of keyphrasei in SRRip,all / (# of ASR re-sults))Fip,r14 avg( CM of keyphrasei in SRRip,1)Fip,r15 mini ( CM of keyphrasei in SRRip,1)Fip,r16 maxi ( CM of keyphrasei in SRRip,1)Fip,h1 If this expert has been activated beforeFip,h2 Same as Fru,h2Fip,h3 If the previous system utterance is the final utter-ance of the presentationFip,h4 If the previous system utterance is an utterance toreact to a user interruptionFip,h5 Same as Fru,h6Fip,h6 If the system has made the final utterance of the pre-sentation since this expert was activatedFip,h7 If the system has made an utterance to react to a userinterruption since this expert was activatedFip,h8 Same as Fru,h8Fip,h9 If the system has made the final utterance of the pre-sentation beforeFip,h10 If the system has made an utterance to react to a userinterruption beforeFip,h11 Same as Fru,h10QA Fqa,r10 Same as Fip,r12Fqa,r11 Same as Fip,r13Fqa,r12 Same as Fip,r14Fqa,r13 Same as Fip,r15Fqa,r14 Same as Fip,r16Fqa,r15 Same as Fip,r17Fqa,r16 If SRRqa,1 is an acknowledgmentFqa,h1 Same as Fru,h1Fqa,h2 Same as Fru,h2Fqa,h3 Same as Fru,h3Fqa,h4 Fqa,h4/Fqa,h3Fqa,h5 If the previous system utterance is the final utter-ance of an answerFqa,h6 Same as Fru,h6Fqa,h7 If the system has made the final utterance of an an-swer since this expert was activatedFqa,h8 Same as Fru,h8Fqa,h9 If the system has made the final utterance of an an-swer beforeFqa,h10 Same as Fru,h10SRRi,j means j-th speech recognition result with the language model (LM) for expert class i. SRRi,all means all the recognitionresults in the n-best list.
Fi,rx are speech understanding related features and Fi,hx are dialogue history related features.
SRRlv,jis an ASR result with a large-vocabulary (60,250 words) statistical model (Kawahara et al, 2004), which we used for utteranceverification.
CM means confidence measure.Table 5: Features used in the experiment.28expert class(F1 scoreobtainedafter featureselection)remaining features (F1 score obtainedwhen each feature is removed)RU(0.948) Fru,r9 (0.922), Fru,h8 (0.939), Fru,r5(0.940), Fru,r14 (0.941), Fru,r2(0.944), Fru,h9 (0.944), Fru,h5(0.944), Fru,r13 (0.945), Fru,h10(0.945), Fru,r10 (0.946), Fru,r8(0.946), Fru,r7 (0.946)IP(0.837) Fip,r7 (0.771), Fip,r6 (0.772), Fip,h9(0.781), Fip,h7 (0.781), Fip,h11(0.786), Fip,r4 (0.79), Fip,r2 (0.799),Fip,r16 (0.809), Fip,r5 (0.809), Fip,r3(0.809), Fip,h4 (0.809), Fip,r9 (0.814),Fip,r15 (0.833), Fip,r12 (0.834), Fip,r13(0.835), Fip,h10 (0.836)QA(0.836) Fqa,r14 (0.813), Fqa,r7 (0.817),Fqa,r16 (0.817), Fqa,r10 (0.818),Fqa,h6 (0.820), Fqa,r6 (0.822), Fqa,r3(0.831), Fqa,r5 (0.832)Table 6: Features that remained after feature selection atStage 1 and their significances in terms of the F1 scoreobtained when each feature is removed.expert class(F1 scoreobtainedafter featureselection)remaining features (F1 score obtainedwhen each feature is removed)RU(0.773) Fru,r3 (0.728), Fru,a (0.737), Fru,h5(0.743), Fru,h1 (0.751), Fru,r9 (0.754),Fru,h10 (0.757), Fru,h8 (0.757),Fru,r5 (0.758), Fru,r2 (0.759), Fru,r13(0.762), Fru,r14 (0.763), Fru,h9(0.767), Fru,r15 (0.768), Fru,r10(0.768), Fru,h3 (0.772)IP(0.827) Fip,h5 (0.808), Fip,r5 (0.809), Fip,r4(0.810), Fip,r6 (0.811), Fip,a (0.812),Fip,h4 (0.812), Fip,r13 (0.813), Fip,h3(0.817), Fip,r15 (0.818), Fip,r3 (0.818),Fip,h10 (0.819), Fip,r12 (0.820),Fip,h7 (0.821), Fip,r11 (0.822), Fip,r10(0.822), Fip,h8 (0.822), Fip,h6 (0.822),Fip,r2 (0.824), Fip,r8 (0.824), Fip,h9(0.824), Fip,h2 (0.825)QA(0.873) Fqa,a (0.838), Fqa,r5 (0.857), Fqa,h1(0.859), Fqa,r3 (0.862), Fqa,r6 (0.865),Fqa,h8 (0.867), Fqa,r7 (0.868), Fqa,r15(0.870), Fqa,r8 (0.870), Fqa,h7 (0.870),Fqa,r12 (0.871), Fqa,r2 (0.871), Fqa,r16(0.871), Fqa,h4 (0.871), Fqa,h3 (0.871),Fqa,r11 (0.872), Fqa,h6 (0.872), Fqa,h5(0.872)Table 7: Features that remained after feature selection atStage 2 and their significances in terms of the F1 scoreobtained when each feature is removed.
Fru,a, Fip,a, andFqa,a are the maximum activation probabilities obtainedat Stage 1.29
