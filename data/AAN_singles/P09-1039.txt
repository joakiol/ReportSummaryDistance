Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342?350,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPConcise Integer Linear Programming Formulationsfor Dependency ParsingAndre?
F. T.
Martins??
Noah A. Smith?
Eric P.
Xing?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal{afm,nasmith,epxing}@cs.cmu.eduAbstractWe formulate the problem of non-projective dependency parsing as apolynomial-sized integer linear pro-gram.
Our formulation is able to handlenon-local output features in an efficientmanner; not only is it compatible withprior knowledge encoded as hard con-straints, it can also learn soft constraintsfrom data.
In particular, our model is ableto learn correlations among neighboringarcs (siblings and grandparents), wordvalency, and tendencies toward nearly-projective parses.
The model parametersare learned in a max-margin frameworkby employing a linear programmingrelaxation.
We evaluate the performanceof our parser on data in several naturallanguages, achieving improvements overexisting state-of-the-art methods.1 IntroductionMuch attention has recently been devoted to in-teger linear programming (ILP) formulations ofNLP problems, with interesting results in appli-cations like semantic role labeling (Roth and Yih,2005; Punyakanok et al, 2004), dependency pars-ing (Riedel and Clarke, 2006), word alignmentfor machine translation (Lacoste-Julien et al,2006), summarization (Clarke and Lapata, 2008),and coreference resolution (Denis and Baldridge,2007), among others.
In general, the rationale forthe development of ILP formulations is to incorpo-rate non-local features or global constraints, whichare often difficult to handle with traditional algo-rithms.
ILP formulations focus more on the mod-eling of problems, rather than algorithm design.While solving an ILP is NP-hard in general, fastsolvers are available today that make it a practicalsolution for many NLP problems.This paper presents new, concise ILP formu-lations for projective and non-projective depen-dency parsing.
We believe that our formula-tions can pave the way for efficient exploitation ofglobal features and constraints in parsing applica-tions, leading to more powerful models.
Riedeland Clarke (2006) cast dependency parsing asan ILP, but efficient formulations remain an openproblem.
Our formulations offer the followingcomparative advantages:?
The numbers of variables and constraints arepolynomial in the sentence length, as opposed torequiring exponentially many constraints, elim-inating the need for incremental procedures likethe cutting-plane algorithm;?
LP relaxations permit fast online discriminativetraining of the constrained model;?
Soft constraints may be automatically learnedfrom data.
In particular, our formulations han-dle higher-order arc interactions (like siblingsand grandparents), model word valency, and canlearn to favor nearly-projective parses.We evaluate the performance of the new parserson standard parsing tasks in seven languages.
Thetechniques that we present are also compatiblewith scenarios where expert knowledge is avail-able, for example in the form of hard or soft first-order logic constraints (Richardson and Domin-gos, 2006; Chang et al, 2008).2 Dependency Parsing2.1 PreliminariesA dependency tree is a lightweight syntactic repre-sentation that attempts to capture functional rela-tionships between words.
Lately, this formalismhas been used as an alternative to phrase-basedparsing for a variety of tasks, ranging from ma-chine translation (Ding and Palmer, 2005) to rela-tion extraction (Culotta and Sorensen, 2004) andquestion answering (Wang et al, 2007).Let us first describe formally the set of legal de-pendency parse trees.
Consider a sentence x =342?w0, .
.
.
, wn?, where wi denotes the word at the i-th position, and w0 = $ is a wall symbol.
We formthe (complete1) directed graph D = ?V,A?, withvertices in V = {0, .
.
.
, n} (the i-th vertex corre-sponding to the i-th word) and arcs in A = V 2.Using terminology from graph theory, we say thatB ?
A is an r-arborescence2 of the directedgraph D if ?V,B?
is a (directed) tree rooted at r.We define the set of legal dependency parse treesof x (denoted Y(x)) as the set of 0-arborescencesof D, i.e., we admit each arborescence as a poten-tial dependency tree.Let y ?
Y(x) be a legal dependency tree forx; if the arc a = ?i, j?
?
y, we refer to i as theparent of j (denoted i = pi(j)) and j as a child ofi.
We also say that a is projective (in the sense ofKahane et al, 1998) if any vertex k in the span ofa is reachable from i (in other words, if for any ksatisfying min(i, j) < k < max(i, j), there is adirected path in y from i to k).
A dependency treeis called projective if it only contains projectivearcs.
Fig.
1 illustrates this concept.3The formulation to be introduced in ?3 makesuse of the notion of the incidence vector associ-ated with a dependency tree y ?
Y(x).
This isthe binary vector z , ?za?a?A with each compo-nent defined as za = I(a ?
y) (here, I(.)
denotesthe indicator function).
Considering simultane-ously all incidence vectors of legal dependencytrees and taking the convex hull, we obtain a poly-hedron that we call the arborescence polytope,denoted by Z(x).
Each vertex of Z(x) can beidentified with a dependency tree in Y(x).
TheMinkowski-Weyl theorem (Rockafellar, 1970) en-sures that Z(x) has a representation of the formZ(x) = {z ?
R|A| | Az ?
b}, for some p-by-|A|matrix A and some vector b in Rp.
However, it isnot easy to obtain a compact representation (wherep grows polynomially with the number of wordsn).
In ?3, we will provide a compact represen-tation of an outer polytope Z?
(x) ?
Z(x) whoseinteger vertices correspond to dependency trees.Hence, the problem of finding the dependency treethat maximizes some linear function of the inci-1The general case where A ?
V 2 is also of interest; itarises whenever a constraint or a lexicon forbids some arcsfrom appearing in dependency tree.
It may also arise as aconsequence of a first-stage pruning step where some candi-date arcs are eliminated; this will be further discussed in ?4.2Or ?directed spanning tree with designated root r.?3In this paper, we consider unlabeled dependency parsing,where only the backbone structure (i.e., the arcs without thelabels depicted in Fig.
1) is to be predicted.Figure 1: A projective dependency graph.Figure 2: Non-projective dependency graph.those that assume each dependency decision is in-dependent modulo the global structural constraintthat dependency graphs must be trees.
Such mod-els are commonly referred to as edge-factored sincetheir parameters factor relative to individual edgesof the graph (Paskin, 2001; McDonald et al,2005a).
Edge-factored models have many computa-tional benefits, most notably that inference for non-projective dependency graphs can be achieved inpolynomial time (McDonald et al, 2005b).
The pri-mary problem in treating each dependency as in-dependent is that it is not a realistic assumption.Non-local information, such as arity (or valency)and neighbouring dependencies, can be crucial toobtaining high parsing accuracies (Klein and Man-ning, 2002; McDonald and Pereira, 2006).
How-ever, in the data-driven parsing setting this can bepartially adverted by incorporating rich feature rep-resentations over the input (McDonald et al, 2005a).The goal of this work is to further our currentunderstanding of the computational nature of non-projective parsing algorithms for both learning andinference within the data-driven setting.
We start byinvestigating and extending the edge-factored modelof McDonald et al (2005b).
In particular, we ap-peal to the Matrix Tree Theorem for multi-digraphsto design polynomial-time algorithms for calculat-ing both the partition function and edge expecta-tions over all possible dependency graphs for a givensentence.
To motivate these algorithms, we showthat they can be used in many important learningand inference problems including min-risk decod-ing, training globally normalized log-linear mod-els, syntactic language modeling, and unsupervisedlearning via the EM algorithm ?
none of which havepreviously been known to have exact non-projectiveimplementations.We then switch focus to models that account fornon-local information, in particular arity and neigh-bouring parse decisions.
For systems that model ar-ity constraints we give a reduction from the Hamilto-nian graph problem suggesting that the parsing prob-lem is intractable in this case.
For neighbouringparse decisions, we extend the work of McDonaldand Pereira (2006) and show that modeling verticalneighbourhoods makes parsing intractable in addi-tion to modeling horizontal neighbourhoods.
A con-sequence of these results is that it is unlikely thatexact non-projective dependency parsing is tractablefor any model assumptions weaker than those madeby the edge-factored models.1.1 Related WorkThere has been extensive work on data-driven de-pendency parsing for both projective parsing (Eis-ner, 1996; Paskin, 2001; Yamada and Matsumoto,2003; Nivre and Scholz, 2004; McDonald et al,2005a) and non-projective parsing systems (Nivreand Nilsson, 2005; Hall and No?va?k, 2005; McDon-ald et al, 2005b).
These approaches can often beclassified into two broad categories.
In the first cat-egory are those methods that employ approximateinference, typically through the use of linear timeshift-reduce parsing algorithms (Yamada and Mat-sumoto, 2003; Nivre and Scholz, 2004; Nivre andNilsson, 2005).
In the second category are thosethat employ exhaustive inference algorithms, usu-ally by making strong independence assumptions, asis the case for edge-factored models (Paskin, 2001;McDonald et al, 2005a; McDonald et al, 2005b).Recently there have also been proposals for exhaus-tive methods that weaken the edge-factored assump-tion, including both approximate methods (McDon-ald and Pereira, 2006) and exact methods through in-teger linear programming (Riedel and Clarke, 2006)or branch-and-bound algorithms (Hirakawa, 2006).For grammar based models there has been limitedwork on empirical systems for non-projective pars-ing systems, notable exceptions include the workof Wang and Harper (2004).
Theoretical studies ofnote include the work of Neuhaus and Bo?ker (1997)showing that the recognition problem for a mini-$ Figure 1: A projective dependency graph.Figure 2: Non-projective dependency graph.those that assume each dependency decision is in-dependent modulo the global structural constraintthat dependency graphs must be trees.
Such mod-els are commonly referred to as edge-factored sincetheir parameters factor relative to individual edgesof the graph (Paskin, 2001; McDonald et al,2005a).
Edge-factored models have many computa-tional benefits, most notably that inference for non-projective dependency graphs can be achieved inpolynomial time (McDonald et al, 2005b).
The pri-mary problem in treating each dependency as in-dependent is that it is not a realistic assumption.Non-local information, such as arity (or valency)and eighbouring dependencies, can be crucial toobtaining high parsing accuracies (Klein and Man-ning, 2002; McDonald and P reira, 2006).
How-ever, i the data-dr ven parsing etting this can bepartially adverted by incorporating rich feature rep-resentations over the input (McDonald et al, 2005a).The goal of this work is to further our curr ntunderstanding of the computational nature of non-projective parsing algorithms for both learning andinference within the data-driven setting.
We start byinvestigating and extending the edge-factored modelof McDonald et al (2005b).
In particular, we ap-peal to the Matrix Tree Theorem for multi-digraphsto design polynomial-time algorithms for calculat-ing both the partition function and edge expecta-tions over all possible dependency graphs for a givensentence.
To motivate these algorithms, we showthat they can be used in many important learningand inference problems including min-risk decod-ing, training globally normalized log-linear mod-els, syntactic language modeling, and unsupervisedlearning via the EM algorithm ?
none of which havepreviously been known t have exact n n-projectiveimplementations.We th switch focus to models that account fornon-local information, in particular arity and neigh-bouring parse decisions.
For systems that model ar-ity constraints we give a reduction from the Hamilto-nian graph problem suggesting that the parsing prob-lem is intractable in this case.
For neighbouringparse decisions, we extend the work of McDonaldand Pereira (2006) and show that modeling verticalneighbourhoods makes parsing intractable in addi-tion to modeling horizontal neighbourhoods.
A con-sequence of these results is that it is unlikely thatexact non-projective dependency parsing is tractablefor any model assumptions weaker than those madeby the edge-factored models.1.1 Related W rkThere has been extensive work on data-driven de-pendency parsing for both projective parsing (Eis-ner, 1996; Paskin, 2001; Yamada and Matsumoto,2003; Nivre and Scholz, 2004; McDonald et al,2005a) and non-projective parsing systems (Nivreand Nilsson, 2005; Hall and No?va?k, 2005; McDon-ald et al, 2005b).
These approaches can often beclassified into two broad categories.
In the first cat-egory are those methods that employ approximateinference, typically through the use of linear timeshift-reduce parsing algorit ms (Yamad and Mat-sumoto, 2003; Nivre and Scholz, 2004; Nivre andNilsson, 2005).
I the second category are thosethat employ exhaustive inference algorithms, usu-ally by making strong independence assumptions, ais the case for edge-factored mod ls (Paskin, 2001;McDonald et al, 2005a; McDonald et al, 2005b).Recently there have also been proposals for exhaus-tive methods that weaken the edge-factored assump-tion, including both approximate methods (McDon-ald and Pereira, 2006) and exact methods through in-teger linear programming (Riedel and Clarke, 2006)or branch-and-bound algorithms (Hirakawa, 2006).For grammar based models there has been limitedwork on empirical systems for non-projective pars-ing systems, notable exceptions include the workof Wang and Harper (2004).
Theoretical studies ofnote include the work of Neuhaus and Bo?ker (1997)showing that the recognition problem for a mini-$Figure 1: A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sen-tences; examples from McDonald and Satta (2007).dence vectors can be cast as an ILP.
A similar ideawas applied to word alignment by Lacoste-Julienet al (2006), where permutations (rather than ar-borescences) were the combinatorial structure be-ing requiring representation.Letting X denote the set of possible sentences,define Y ,?x?X Y(x).
Given a labeled datasetL , ?
?x1, y1?, .
.
.
, ?xm, ym??
?
(X ?
Y)m, weaim to learn a parser, i.e., a function h : X ?
Ythat given x ?
X ou puts a legal dependency parsey ?
Y(x).
T e f ct that t er e xponentiallyma y candidates in Y(x) makes dependency pars-ing a structured classification problem.2.2 Arc Factorizat o and L c lityThere has been much recent work on dependencyparsing using graph-based, transition-based, andhybrid methods; see Nivre and McDonald (2008)for an overview.
Typical graph-based methodsconsider linear classifiers of the fo mhw(x) = argmaxy?Y w>f(x, y), (1)where f(x, y) is a vector of features and w is thecorresponding weight vector.
One wants hw tohave small expected loss; the ty ical loss func-tion is th Hamming loss, `(y?
; y) , |{?i, j?
?y?
: ?i, j?
/?
y}|.
Tractability is usually ensuredby strong factorization assumptions, like the oneunderlying the arc-factored model (Eisner, 1996;McDonald et al, 2005), which forbids any featurethat depends on two or more arcs.
This induces adecomposition of the feature vector f(x, y) as:f(x, y) =?a?y fa(x).
(2)Under this decomposition, each arc receives ascore; parsing amounts to choosing the configu-ration that maximizes the overall score, which, as343shown by McDonald et al (2005), is an instanceof the maximal arborescence problem.
Combi-natorial algorithms (Chu and Liu, 1965; Edmonds,1967) can solve this problem in cubic time.4 Ifthe dependency parse trees are restricted to beprojective, cubic-time algorithms are available viadynamic programming (Eisner, 1996).
While inthe projective case, the arc-factored assumptioncan be weakened in certain ways while maintain-ing polynomial parser runtime (Eisner and Satta,1999), the same does not happen in the nonprojec-tive case, where finding the highest-scoring treebecomes NP-hard (McDonald and Satta, 2007).Approximate algorithms have been employed tohandle models that are not arc-factored (althoughfeatures are still fairly local): McDonald andPereira (2006) adopted an approximation basedon O(n3) projective parsing followed by a hill-climbing algorithm to rearrange arcs, and Smithand Eisner (2008) proposed an algorithm based onloopy belief propagation.3 Dependency Parsing as an ILPOur approach will build a graph-based parserwithout the drawback of a restriction to local fea-tures.
By formulating inference as an ILP, non-local features can be easily accommodated in ourmodel; furthermore, by using a relaxation tech-nique we can still make learning tractable.
The im-pact of LP-relaxed inference in the learning prob-lem was studied elsewhere (Martins et al, 2009).A linear program (LP) is an optimization prob-lem of the formminx?Rd c>xs.t.
Ax ?
b.
(3)If the problem is feasible, the optimum is attainedat a vertex of the polyhedron that defines the con-straint space.
If we add the constraint x ?
Zd, thenthe above is called an integer linear program(ILP).
For some special parameter settings?e.g.,when b is an integer vector and A is totally uni-modular5?all vertices of the constraining polyhe-dron are integer points; in these cases, the integerconstraint may be suppressed and (3) is guaran-teed to have integer solutions (Schrijver, 2003).Of course, this need not happen: solving a gen-eral ILP is an NP-complete problem.
Despite this4There is also a quadratic algorithm due to Tarjan (1977).5A matrix is called totally unimodular if the determinantsof each square submatrix belong to {0, 1,?1}.fact, fast solvers are available today that make thisa practical solution for many problems.
Their per-formance depends on the dimensions and degreeof sparsity of the constraint matrix A.Riedel and Clarke (2006) proposed an ILP for-mulation for dependency parsing which refinesthe arc-factored model by imposing linguisticallymotivated ?hard?
constraints that forbid some arcconfigurations.
Their formulation includes an ex-ponential number of constraints?one for eachpossible cycle.
Since it is intractable to throwin all constraints at once, they propose a cutting-plane algorithm, where the cycle constraints areonly invoked when violated by the current solu-tion.
The resulting algorithm is still slow, and anarc-factored model is used as a surrogate duringtraining (i.e., the hard constraints are only used attest time), which implies a discrepancy betweenthe model that is optimized and the one that is ac-tually going to be used.Here, we propose ILP formulations that elim-inate the need for cycle constraints; in fact, theyrequire only a polynomial number of constraints.Not only does our model allow expert knowledgeto be injected in the form of constraints, it is alsocapable of learning soft versions of those con-straints from data; indeed, it can handle featuresthat are not arc-factored (correlating, for exam-ple, siblings and grandparents, modeling valency,or preferring nearly projective parses).
While, aspointed out by McDonald and Satta (2007), theinclusion of these features makes inference NP-hard, by relaxing the integer constraints we obtainapproximate algorithms that are very efficient andcompetitive with state-of-the-art methods.
In thispaper, we focus on unlabeled dependency parsing,for clarity of exposition.
If it is extended to labeledparsing (a straightforward extension), our formu-lation fully subsumes that of Riedel and Clarke(2006), since it allows using the same hard con-straints and features while keeping the ILP poly-nomial in size.3.1 The Arborescence PolytopeWe start by describing our constraint space.
Ourformulations rely on a concise polyhedral repre-sentation of the set of candidate dependency parsetrees, as sketched in ?2.1.
This will be accom-plished by drawing an analogy with a networkflow problem.Let D = ?V,A?
be the complete directed graph344associated with a sentence x ?
X , as stated in?2.
A subgraph y = ?V,B?
is a legal dependencytree (i.e., y ?
Y(x)) if and only if the followingconditions are met:1.
Each vertex in V \ {0} must have exactly oneincoming arc in B,2.
0 has no incoming arcs in B,3.
B does not contain cycles.For each vertex v ?
V , let ??
(v) , {?i, j?
?A | j = v} denote its set of incoming arcs, and?+(v) , {?i, j?
?
A | i = v} denote its set ofoutgoing arcs.
The two first conditions can be eas-ily expressed by linear constraints on the incidencevector z:?a???
(j) za = 1, j ?
V \ {0} (4)?a???
(0) za = 0 (5)Condition 3 is somewhat harder to express.
Ratherthan adding exponentially many constraints, onefor each potential cycle (like Riedel and Clarke,2006), we equivalently replace condition 3 by3?.
B is connected.Note that conditions 1-2-3 are equivalent to 1-2-3?, in the sense that both define the same set Y(x).However, as we will see, the latter set of condi-tions is more convenient.
Connectedness of graphscan be imposed via flow constraints (by requir-ing that, for any v ?
V \ {0}, there is a directedpath in B connecting 0 to v).
We adapt the singlecommodity flow formulation for the (undirected)minimum spanning tree problem, due to Magnantiand Wolsey (1994), that requires O(n2) variablesand constraints.
Under this model, the root nodemust send one unit of flow to every other node.By making use of extra variables, ?
, ?
?a?a?A,to denote the flow of commodities through eacharc, we are led to the following constraints in ad-dition to Eqs.
4?5 (we denote U , [0, 1], andB , {0, 1} = U ?
Z):?
Root sends flow n:?a?
?+(0) ?a = n (6)?
Each node consumes one unit of flow:?a???
(j)?a ??a?
?+(j)?a = 1, j ?
V \ {0} (7)?
Flow is zero on disabled arcs:?a ?
nza, a ?
A (8)?
Each arc indicator lies in the unit interval:za ?
U, a ?
A.
(9)These constraints project an outer bound of the ar-borescence polytope, i.e.,Z?
(x) , {z ?
R|A| | (z,?)
satisfy (4?9)}?
Z(x).
(10)Furthermore, the integer points of Z?
(x) are pre-cisely the incidence vectors of dependency treesin Y(x); these are obtained by replacing Eq.
9 byza ?
B, a ?
A.
(11)3.2 Arc-Factored ModelGiven our polyhedral representation of (an outerbound of) the arborescence polytope, we cannow formulate dependency parsing with an arc-factored model as an ILP.
By storing the arc-local feature vectors into the columns of a matrixF(x) , [fa(x)]a?A, and defining the score vec-tor s , F(x)>w (each entry is an arc score) theinference problem can be written asmaxy?Y(x)w>f(x, y) = maxz?Z(x)w>F(x)z= maxz,?s>zs.t.
A[z?]?
bz ?
B(12)whereA is a sparse constraint matrix (withO(|A|)non-zero elements), and b is the constraint vec-tor; A and b encode the constraints (4?9).
Thisis an ILP with O(|A|) variables and constraints(hence, quadratic in n); if we drop the integerconstraint the problem becomes the LP relaxation.As is, this formulation is no more attractive thansolving the problem with the existing combinato-rial algorithms discussed in ?2.2; however, we cannow start adding non-local features to build a morepowerful model.3.3 Sibling and Grandparent FeaturesTo cope with higher-order features of the formfa1,...,aK (x) (i.e., features whose values depend onthe simultaneous inclusion of arcs a1, .
.
.
, aK on345a candidate dependency tree), we employ a lin-earization trick (Boros and Hammer, 2002), defin-ing extra variables za1...aK , za1 ?
.
.
.
?zaK .
Thislogical relation can be expressed by the followingO(K) agreement constraints:6za1...aK ?
zai , i = 1, .
.
.
,Kza1...aK ?
?Ki=1 zai ?K + 1.
(13)As shown by McDonald and Pereira (2006) andCarreras (2007), the inclusion of features thatcorrelate sibling and grandparent arcs may behighly beneficial, even if doing so requires resort-ing to approximate algorithms.7 Define Rsibl ,{?i, j, k?
| ?i, j?
?
A, ?i, k?
?
A} and Rgrand ,{?i, j, k?
| ?i, j?
?
A, ?j, k?
?
A}.
To includesuch features in our formulation, we need to addextra variables zsibl , ?zr?r?Rsibl and zgrand ,?zr?r?Rgrand that indicate the presence of siblingand grandparent arcs.
Observe that these indica-tor variables are conjunctions of arc indicator vari-ables, i.e., zsiblijk = zij ?
zik and zgrandijk = zij ?
zjk.Hence, these features can be handled in our formu-lation by adding the following O(|A| ?
|V |) vari-ables and constraints:zsiblijk ?
zij , zsiblijk ?
zik, zsiblijk ?
zij + zik ?
1(14)for all triples ?i, j, k?
?
Rsibl, andzgrandijk ?
zij , zgrandijk ?
zjk, zgrandijk ?
zij+zjk?1(15)for all triples ?i, j, k?
?
Rgrand.
Let R , A ?Rsibl ?
Rgrand; by redefining z , ?zr?r?R andF(x) , [fr(x)]r?R, we may express our inferenceproblem as in Eq.
12, with O(|A| ?
|V |) variablesand constraints.Notice that the strategy just described to han-dle sibling features is not fully compatible withthe features proposed by Eisner (1996) for pro-jective parsing, as the latter correlate only con-secutive siblings and are also able to place spe-cial features on the first child of a given word.The ability to handle such ?ordered?
features isintimately associated with Eisner?s dynamic pro-gramming parsing algorithm and with the Marko-vian assumptions made explicitly by his genera-tive model.
We next show how similar features6Actually, any logical condition can be encoded with lin-ear constraints involving binary variables; see e.g.
Clarke andLapata (2008) for an overview.7By sibling features we mean features that depend onpairs of sibling arcs (i.e., of the form ?i, j?
and ?i, k?
); bygrandparent features we mean features that depend on pairsof grandparent arcs (of the form ?i, j?
and ?j, k?
).can be incorporated in our model by adding ?dy-namic?
constraints to our ILP.
Define:znext siblijk ,????
?1 if ?i, j?
and ?i, k?
areconsecutive siblings,0 otherwise,zfirst childij ,{1 if j is the first child of i,0 otherwise.Suppose (without loss of generality) that i < j <k ?
n. We could naively compose the constraints(14) with additional linear constraints that encodethe logical relationznext siblijk = zsiblijk ?
?j<l<k ?zil,but this would yield a constraint matrix withO(n4) non-zero elements.
Instead, we define aux-iliary variables ?jk and ?ij :?jk ={1, if ?l s.t.
pi(l) = pi(j) < j < l < k0, otherwise,?ij ={1, if ?k s.t.
i < k < j and ?i, k?
?
y0, otherwise.
(16)Then, we have that znext siblijk = zsiblijk ?
(?
?jk) andzfirst childij = zij?(?
?ij), which can be encoded viaznext siblijk ?
zsiblijk zfirst childij ?
zijznext siblijk ?
1 ?
?jk zfirst childij ?
1 ?
?ijznext siblijk ?
zsiblijk ?
?jk zfirst childij ?
zij ?
?ijThe following ?dynamic?
constraints encode thelogical relations for the auxiliary variables (16):?j(j+1) = 0 ?i(i+1) = 0?j(k+1) ?
?jk ?i(j+1) ?
?ij?j(k+1) ?
?i<jzsiblijk ?i(j+1) ?
zij?j(k+1) ?
?jk +?i<jzsiblijk ?i(j+1) ?
?ij + zijAuxiliary variables and constraints are definedanalogously for the case n ?
i > j > k. Thisresults in a sparser constraint matrix, with onlyO(n3) non-zero elements.3.4 Valency FeaturesA crucial fact about dependency grammars is thatwords have preferences about the number and ar-rangement of arguments and modifiers they ac-cept.
Therefore, it is desirable to include features346that indicate, for a candidate arborescence, howmany outgoing arcs depart from each vertex; de-note these quantities by vi ,?a?
?+(i) za, foreach i ?
V .
We call vi the valency of the ith ver-tex.
We add valency indicators zvalik , I(vi = k)for i ?
V and k = 0, .
.
.
, n?
1.
This way, we areable to penalize candidate dependency trees thatassign unusual valencies to some of their vertices,by specifying a individual cost for each possiblevalue of valency.
The following O(|V |2) con-straints encode the agreement between valency in-dicators and the other variables:?n?1k=0 kzvalik =?a?
?+(i) za, i ?
V (17)?n?1k=0 zvalik = 1, i ?
Vzvalik ?
0, i ?
V, k ?
{0, .
.
.
, n?
1}3.5 Projectivity FeaturesFor most languages, dependency parse trees tendto be nearly projective (cf.
Buchholz and Marsi,2006).
We wish to make our model capable oflearning to prefer ?nearly?
projective parses when-ever that behavior is observed in the data.The multicommodity directed flow model ofMagnanti and Wolsey (1994) is a refinement of themodel described in ?3.1 which offers a compactand elegant way to indicate nonprojective arcs, re-quiring O(n3) variables and constraints.
In thismodel, every node k 6= 0 defines a commodity:one unit of commodity k originates at the rootnode and must be delivered to node k; the vari-able ?kij denotes the flow of commodity k in arc?i, j?.
We first replace (4?9) by (18?22):?
The root sends one unit of commodity to eachnode:?a???
(0)?ka ??a?
?+(0)?ka = ?1, k ?
V \ {0} (18)?
Any node consumes its own commodity and noother:?a???
(j)?ka ??a?
?+(j)?ka = ?kj , j, k ?
V \ {0} (19)where ?kj , I(j = k) is the Kronecker delta.?
Disabled arcs do not carry any flow:?ka ?
za, a ?
A, k ?
V (20)?
There are exactly n enabled arcs:?a?A za = n (21)?
All variables lie in the unit interval:za ?
U, ?ka ?
U, a ?
A, k ?
V (22)We next define auxiliary variables ?jk that indi-cate if there is a path from j to k. Since each ver-tex except the root has only one incoming arc, thefollowing linear equalities are enough to describethese new variables:?jk =?a???
(j) ?ka, j, k ?
V \ {0}?0k = 1, k ?
V \ {0}.
(23)Now, define indicators znp , ?znpa ?a?A, whereznpa , I(a ?
y and a is nonprojective).From the definition of projective arcs in ?2.1, wehave that znpa = 1 if and only if the arc is active(za = 1) and there is some vertex k in the span ofa = ?i, j?
such that ?ik = 0.
We are led to thefollowing O(|A| ?
|V |) constraints for ?i, j?
?
A:znpij ?
zijznpij ?
zij ?
?ik, min(i, j) ?
k ?
max(i, j)znpij ?
?
?max(i,j)?1k=min(i,j)+1 ?ik + |j ?
i| ?
1There are other ways to introduce nonprojectiv-ity indicators and alternative definitions of ?non-projective arc.?
For example, by using dynamicconstraints of the same kind as those in ?3.3,we can indicate arcs that ?cross?
other arcs withO(n3) variables and constraints, and a cubic num-ber of non-zero elements in the constraint matrix(omitted for space).3.6 Projective ParsingIt would be straightforward to adapt the con-straints in ?3.5 to allow only projective parse trees:simply force znpa = 0 for any a ?
A.
But there aremore efficient ways of accomplish this.
While it isdifficult to impose projectivity constraints or cycleconstraints individually, there is a simpler way ofimposing both.
Consider 3 (or 3?)
from ?3.1.Proposition 1 Replace condition 3 (or 3?)
with3??.
If ?i, j?
?
B, then, for any k = 1, .
.
.
, nsuch that k 6= j, the parent of k must satisfy(defining i?
, min(i, j) and j?
, max(i, j)):?????i?
?
pi(k) ?
j?, if i?
< k < j?,pi(k) < i?
?
pi(k) > j?, if k < i?
or k > j?or k = i.347Then, Y(x) will be redefined as the set of projec-tive dependency parse trees.We omit the proof for space.
Conditions 1, 2, and3??
can be encoded with O(n2) constraints.4 ExperimentsWe report experiments on seven languages, six(Danish, Dutch, Portuguese, Slovene, Swedishand Turkish) from the CoNLL-X shared task(Buchholz and Marsi, 2006), and one (English)from the CoNLL-2008 shared task (Surdeanu etal., 2008).8 All experiments are evaluated usingthe unlabeled attachment score (UAS), using thedefault settings.9 We used the same arc-factoredfeatures as McDonald et al (2005) (included in theMSTParser toolkit10); for the higher-order modelsdescribed in ?3.3?3.5, we employed simple higherorder features that look at the word, part-of-speechtag, and (if available) morphological informationof the words being correlated through the indica-tor variables.
For scalability (and noting that someof the models require O(|V | ?
|A|) constraints andvariables, which, when A = V 2, grows cubicallywith the number of words), we first prune the basegraph by running a simple algorithm that ranks thek-best candidate parents for each word in the sen-tence (we set k = 10); this reduces the number ofcandidate arcs to |A| = kn.11 This strategy is sim-ilar to the one employed by Carreras et al (2008)to prune the search space of the actual parser.
Theranker is a local model trained using a max-margincriterion; it is arc-factored and not subject to anystructural constraints, so it is very fast.The actual parser was trained via the onlinestructured passive-aggressive algorithm of Cram-mer et al (2006); it differs from the 1-best MIRAalgorithm of McDonald et al (2005) by solv-ing a sequence of loss-augmented inference prob-lems.12 The number of iterations was set to 10.The results are summarized in Table 1; for thesake of comparison, we reproduced three strong8We used the provided train/test splits except for English,for which we tested on the development partition.
For train-ing, sentences longer than 80 words were discarded.
For test-ing, all sentences were kept (the longest one has length 118).9http://nextens.uvt.nl/?conll/software.html10http://sourceforge.net/projects/mstparser11Note that, unlike reranking approaches, there are still ex-ponentially many candidate parse trees after pruning.
Theoracle constrained to pick parents from these lists achieves> 98% in every case.12The loss-augmented inference problem can also be ex-pressed as an LP for Hamming loss functions that factor overarcs; we refer to Martins et al (2009) for further details.baselines, all of them state-of-the-art parsers basedon non-arc-factored models: the second ordermodel of McDonald and Pereira (2006), the hy-brid model of Nivre and McDonald (2008), whichcombines a (labeled) transition-based and a graph-based parser, and a refinement of the latter, dueto Martins et al (2008), which attempts to ap-proximate non-local features.13 We did not repro-duce the model of Riedel and Clarke (2006) sincethe latter is tailored for labeled dependency pars-ing; however, experiments reported in that paperfor Dutch (and extended to other languages in theCoNLL-X task) suggest that their model performsworse than our three baselines.By looking at the middle four columns, we cansee that adding non-arc-factored features makesthe models more accurate, for all languages.
Withthe exception of Portuguese, the best results areachieved with the full set of features.
We canalso observe that, for some languages, the valencyfeatures do not seem to help.
Merely modelingthe number of dependents of a word may not beas valuable as knowing what kinds of dependentsthey are (for example, distinguishing among argu-ments and adjuncts).Comparing with the baselines, we observe thatour full model outperforms that of McDonald andPereira (2006), and is in line with the most ac-curate dependency parsers (Nivre and McDonald,2008; Martins et al, 2008), obtained by com-bining transition-based and graph-based parsers.14Notice that our model, compared with these hy-brid parsers, has the advantage of not requiring anensemble configuration (eliminating, for example,the need to tune two parsers).
Unlike the ensem-bles, it directly handles non-local output featuresby optimizing a single global objective.
Perhapsmore importantly, it makes it possible to exploitexpert knowledge through the form of hard globalconstraints.
Although not pursued here, the samekind of constraints employed by Riedel and Clarke(2006) can straightforwardly fit into our model,after extending it to perform labeled dependencyparsing.
We believe that a careful design of fea-13Unlike our model, the hybrid models used here as base-lines make use of the dependency labels at training time; in-deed, the transition-based parser is trained to predict a la-beled dependency parse tree, and the graph-based parser usethese predicted labels as input features.
Our model ignoresthis information at training time; therefore, this comparisonis slightly unfair to us.14See also Zhang and Clark (2008) for a different approachthat combines transition-based and graph-based methods.348[MP06][NM08][MDSX08]ARC-FACTORED+SIBL/GRANDP.+VALENCY+PROJ.
(FULL)FULL, RELAXEDDANISH 90.60 91.30 91.54 89.80 91.06 90.98 91.18 91.04 (-0.14)DUTCH 84.11 84.19 84.79 83.55 84.65 84.93 85.57 85.41 (-0.16)PORTUGUESE 91.40 91.81 92.11 90.66 92.11 92.01 91.42 91.44 (+0.02)SLOVENE 83.67 85.09 85.13 83.93 85.13 85.45 85.61 85.41 (-0.20)SWEDISH 89.05 90.54 90.50 89.09 90.50 90.34 90.60 90.52 (-0.08)TURKISH 75.30 75.68 76.36 75.16 76.20 76.08 76.34 76.32 (-0.02)ENGLISH 90.85 ?
?
90.15 91.13 91.12 91.16 91.14 (-0.02)Table 1: Results for nonprojective dependency parsing (unlabeled attachment scores).
The three baselines are the second ordermodel of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al (2008).
Thefour middle columns show the performance of our model using exact (ILP) inference at test time, for increasing sets of features(see ?3.2??3.5).
The rightmost column shows the results obtained with the full set of features using relaxed LP inferencefollowed by projection onto the feasible set.
Differences are with respect to exact inference for the same set of features.
Boldindicates the best result for a language.
As for overall performance, both the exact and relaxed full model outperform the arc-factored model and the second order model of McDonald and Pereira (2006) with statistical significance (p < 0.01) accordingto Dan Bikel?s randomized method (http://www.cis.upenn.edu/?dbikel/software.html).tures and constraints can lead to further improve-ments on accuracy.We now turn to a different issue: scalability.
Inprevious work (Martins et al, 2009), we showedthat training the model via LP-relaxed inference(as we do here) makes it learn to avoid frac-tional solutions; as a consequence, ILP solverswill converge faster to the optimum (on average).Yet, it is known from worst case complexity the-ory that solving a general ILP is NP-hard; hence,these solvers may not scale well with the sentencelength.
Merely considering the LP-relaxed versionof the problem at test time is unsatisfactory, as itmay lead to a fractional solution (i.e., a solutionwhose components indexed by arcs, z?
= ?za?a?A,are not all integer), which does not correspond to avalid dependency tree.
We propose the followingapproximate algorithm to obtain an actual parse:first, solve the LP relaxation (which can be donein polynomial time with interior-point methods);then, if the solution is fractional, project it onto thefeasible set Y(x).
Fortunately, the Euclidean pro-jection can be computed in a straightforward wayby finding a maximal arborescence in the directedgraph whose weights are defined by z?
(we omitthe proof for space); as we saw in ?2.2, the Chu-Liu-Edmonds algorithm can do this in polynomialtime.
The overall parsing runtime becomes poly-nomial with respect to the length of the sentence.The last column of Table 1 compares the ac-curacy of this approximate method with the ex-act one.
We observe that there is not a substantialdrop in accuracy; on the other hand, we observeda considerable speed-up with respect to exact in-ference, particularly for long sentences.
The av-erage runtime (across all languages) is 0.632 sec-onds per sentence, which is in line with existinghigher-order parsers and is much faster than theruntimes reported by Riedel and Clarke (2006).5 ConclusionsWe presented new dependency parsers based onconcise ILP formulations.
We have shown hownon-local output features can be incorporated,while keeping only a polynomial number of con-straints.
These features can act as soft constraintswhose penalty values are automatically learnedfrom data; in addition, our model is also compati-ble with expert knowledge in the form of hard con-straints.
Learning through a max-margin frame-work is made effective by the means of a LP-relaxation.
Experimental results on seven lan-guages show that our rich-featured parsers outper-form arc-factored and approximate higher-orderparsers, and are in line with stacked parsers, hav-ing with respect to the latter the advantage of notrequiring an ensemble configuration.AcknowledgmentsThe authors thank the reviewers for their com-ments.
Martins was supported by a grant fromFCT/ICTI through the CMU-Portugal Program,and also by Priberam Informa?tica.
Smith wassupported by NSF IIS-0836431 and an IBM Fac-ulty Award.
Xing was supported by NSF DBI-0546594, DBI-0640543, IIS-0713379, and an Al-fred Sloan Foundation Fellowship in ComputerScience.349ReferencesE.
Boros and P.L.
Hammer.
2002.
Pseudo-Boolean op-timization.
Discrete Applied Mathematics, 123(1?3):155?225.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.of CoNLL.X.
Carreras, M. Collins, and T. Koo.
2008.
TAG,dynamic programming, and the perceptron for effi-cient, feature-rich parsing.
In Proc.
of CoNLL.X.
Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proc.
of CoNLL.M.
Chang, L. Ratinov, and D. Roth.
2008.
Constraintsas prior knowledge.
In ICML Workshop on PriorKnowledge for Text and Language Processing.Y.
J. Chu and T. H. Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.J.
Clarke and M. Lapata.
2008.
Global inferencefor sentence compression an integer linear program-ming approach.
JAIR, 31:399?429.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
JMLR, 7:551?585.A.
Culotta and J. Sorensen.
2004.
Dependency treekernels for relation extraction.
In Proc.
of ACL.P.
Denis and J. Baldridge.
2007.
Joint determinationof anaphoricity and coreference resolution using in-teger programming.
In Proc.
of HLT-NAACL.Y.
Ding and M. Palmer.
2005.
Machine translation us-ing probabilistic synchronous dependency insertiongrammar.
In Proc.
of ACL.J.
Edmonds.
1967.
Optimum branchings.
Journalof Research of the National Bureau of Standards,71B:233?240.J.
Eisner and G. Satta.
1999.
Efficient parsing forbilexical context-free grammars and head automatongrammars.
In Proc.
of ACL.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proc.
of COL-ING.S.
Kahane, A. Nasr, and O. Rambow.
1998.
Pseudo-projectivity: a polynomially parsable non-projectivedependency grammar.
In Proc.
of COLING-ACL.S.
Lacoste-Julien, B. Taskar, D. Klein, and M. I. Jor-dan.
2006.
Word alignment via quadratic assign-ment.
In Proc.
of HLT-NAACL.T.
L. Magnanti and L. A. Wolsey.
1994.
OptimalTrees.
Technical Report 290-94, Massachusetts In-stitute of Technology, Operations Research Center.A.
F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.2008.
Stacking dependency parsers.
In Proc.
ofEMNLP.A.
F. T. Martins, N. A. Smith, and E. P. Xing.
2009.Polyhedral outer approximations with application tonatural language parsing.
In Proc.
of ICML.R.
T. McDonald and F. C. N. Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proc.
of EACL.R.
McDonald and G. Satta.
2007.
On the complex-ity of non-projective data-driven dependency pars-ing.
In Proc.
of IWPT.R.
T. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.2005.
Non-projective dependency parsing usingspanning tree algorithms.
In Proc.
of HLT-EMNLP.J.
Nivre and R. McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
InProc.
of ACL-HLT.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2004.Semantic role labeling via integer linear program-ming inference.
In Proc.
of COLING.M.
Richardson and P. Domingos.
2006.
Markov logicnetworks.
Machine Learning, 62(1):107?136.S.
Riedel and J. Clarke.
2006.
Incremental integerlinear programming for non-projective dependencyparsing.
In Proc.
of EMNLP.R.
T. Rockafellar.
1970.
Convex Analysis.
PrincetonUniversity Press.D.
Roth and W. T. Yih.
2005.
Integer linear program-ming inference for conditional random fields.
InICML.A.
Schrijver.
2003.
Combinatorial Optimization:Polyhedra and Efficiency, volume 24 of Algorithmsand Combinatorics.
Springer.D.
A. Smith and J. Eisner.
2008.
Dependency parsingby belief propagation.
In Proc.
of EMNLP.M.
Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez,and J. Nivre.
2008.
The conll-2008 shared taskon joint parsing of syntactic and semantic dependen-cies.
Proc.
of CoNLL.R.
E. Tarjan.
1977.
Finding optimum branchings.
Net-works, 7(1):25?36.M.
Wang, N. A. Smith, and T. Mitamura.
2007.
Whatis the Jeopardy model?
A quasi-synchronous gram-mar for QA.
In Proceedings of EMNLP-CoNLL.Y.
Zhang and S. Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proc.
of EMNLP.350
