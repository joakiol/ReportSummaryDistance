Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3?14,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsYour click decides your fate: Inferring Information Processing andAttrition Behavior from MOOC Video Clickstream InteractionsTanmay Sinha1, Patrick Jermann2, Nan Li3, Pierre Dillenbourg31Language Technologies Institute, Carnegie Mellon University, Pittsburgh PA 15213, USA2Center for Digital Education, EPFL, CH 1015, Switzerland3Computer-Human Interaction in Learning and Instruction, EPFL, CH 1015, Switzerland1tanmays@andrew.cmu.edu,2,3<firstname.lastname>@epfl.chAbstractIn this work, we explore video lec-ture interaction in Massive Open OnlineCourses (MOOCs), which is central to stu-dent learning experience on these educa-tional platforms.
As a research contribu-tion, we operationalize video lecture click-streams of students into cognitively plau-sible higher level behaviors, and constructa quantitative information processing in-dex, which can aid instructors to better un-derstand MOOC hurdles and reason aboutunsatisfactory learning outcomes.
Our re-sults illustrate how such a metric inspiredby cognitive psychology can help answercritical questions regarding students?
en-gagement, their future click interactionsand participation trajectories that lead toin-video & course dropouts.
Implicationsfor research and practice are discussed.1 IntroductionMushrooming as a scalable lifelong learn-ing paradigm, Massive Open Online Courses(MOOCs) have enjoyed significant limelight in re-cent years, both in industry and academia (Hag-gard et al., 2013).
The euphoria is about thetransformative potential of MOOCs to revolution-ize online education (North et al., 2014), by con-necting and fostering interaction among millionsof learners who otherwise would never have metand providing autonomy to these learners to grap-ple with the course instruction at their own pace ofunderstanding.
However, despite this expediency,there is also considerable skepticism in the learn-ing analytics research community about MOOCproductiveness (Nawrot and Antoine, 2014), pri-marily because of unsatisfactory learning out-comes that plague these educational platforms andinduce a funnel of participation (Clow, 2013).With a ?one size fits all?
approach that MOOCsfollow, scaled up class sizes and lack of face toface interaction coupled with such high studentteacher ratios (Guo and Katharina, 2014), stu-dents?
motivation to follow the course oscillates(Davis et al., 2014).
This is comprehensibly re-flected in escalating attrition rates in MOOCs, eversince they have started maturing (Belanger andThornton, 2013; Schmidt and Zach, 2013; Yang etal., 2013).
Because it is not feasible for MOOC in-structors to manually provide individualized atten-tion that caters to different backgrounds, diverseskill levels, learning goals and preferences of stu-dents, there is an increasing need to make directedefforts towards automatically providing better per-sonalized content in e-learning (Sinha et al., 2013;Lie et al., 2014; Sinha, 2014a).
The provisionof guidance with regard to the organization of thestudy and regulation of learning is a domain thatalso needs to be addressed.A prerequisite for such an undertaking is thatwe, as MOOC researchers, understand how di-verse ecologies of participation develop as stu-dents interact with the course material (Fischer,2011), and how learners distribute their attentionwith multiple forms of computer mediated inputsin MOOCs.
Learning in a MOOC requires thatstudents apply self regulation.
While substantialresearch has been done on studying MOOC dis-cussion forums (Ramesh et al., 2013; Brinton etal., 2013; Anderson et al., 2014; Sinha, 2014b),grading strategies for assignments (Tillmann et al.,2013; Kulkarni et al., 2014) and deployment ofreputation systems (Coetzee et al., 2014), innerworkings of students?
interaction while watchingMOOC video lectures have been much less fo-cused upon.
Given that roughly 5% (Huang et al.,2014) of students actually participate in MOOCdiscussion forums, it would be legitimate to askwhether choosing video lectures as units of analy-sis would be more insightful.
After 330,000 reg-3istrations in MOOC courses at EPFL in 2013, ourexperience reflects that out of the 100% studentswho register, 75% show up: 50% of them primar-ily watch video lectures and the rest 25% addition-ally work out homeworks and assignments.
Thus,majority of students have video lecture viewing astheir primary MOOC activity.Video lectures form a primary and an extremelycrucial part of MOOC instruction design.
Theyserve as gateways to draw students into the course.Concept discussions, demos and tutorials that areheld within these short video lectures, not onlyguide learners to complete course assignments,but also encourage them to discuss the taughtsyllabus on MOOC discussion forums.
Specificto the context of video lectures, prior work hascut teeth on a)how video production style (slides,code, classroom, khan academy style etc) relatesto students?
engagement (Guo et al., 2014), b)whatfeatures of the video lecture and instruction de-livery, such as slide transitions (change in visualcontent), instructor changing topic (topic model-ing and ngram analysis) or variations in instruc-tor?s acoustic stream (volume, pitch, speakingrate), lead to peaks in viewership activity (Kimet al., 2014b).
There has been increasing focuson unveiling numerous facets of complexity ofraw click-level interactions resulting from studentactivities within individual MOOC videos (Kimet al., 2014a; Sinha et al., 2014).
However, tothe best of our knowledge, we present the firststudy that describes usage of such detailed click-stream information to form cognitive video watch-ing states that summarize student clickstream.
In-stead of using summative features that express stu-dent engagement, we leverage recurring click be-haviors of students interacting with MOOC videolectures, to construct their video watching profile.Based on these richly logged interactions of stu-dents, we develop computational methods that an-swer critical questions such as a)how long will stu-dents grapple with the course material and whatwill their engagement trajectory look like, b)whatfuture click interactions will characterize their be-havior, c)whether students are ultimately going tosurvive through the end of the video and course.As an effort to improve the second generation ofMOOC offerings, we perform a hierarchical threelevel clickstream analysis, rooted in foundationsof cognitive psychology.
Incidentally, we exploreat a micro level whether, and how, cognitive mindstates govern the formation and occurrence of mi-cro level click patterns.
Towards this end, we alsodevelop a quantitative information processing in-dex and monitor its variations among different stu-dent partitions that we define for the MOOC.
Suchan operationalization can help course instructorsto reason how students?
navigational style reflectscognitive resource allocation for meaning process-ing and retention of concepts taught in the MOOC.Our metric aids MOOC designers in identifyingwhich part of the videos might require editing.The goal is to develop an explanatory techno-cognitive model which shows that a set of metricsderived from low-level behaviors are meaningful,and can in turn be used to make effective predic-tions on high-level behaviors intuitively.In the remainder of this paper, we describe ourstudy context in the next section.
In section 3,we motivate our three level hierarchical MOOCvideo clickstream analysis (operations, actions, in-formation processing activities), describing rele-vant related work along the way, along with thetechnical approach followed.
In section 4, we val-idate our developed methodology by setting upcertain machine learning experiments, specificallyengagement prediction, next click state prediction,in-video and complete course dropout prediction.Implications for future work and conclusion is pre-sented in section 5.2 Study ContextThe data for our current study comes from an in-troductory programming MOOC ?Functional Pro-gramming in Scala?
that was offered on the Cours-era MOOC platform in 2012.
This MOOC com-prises 48 video lectures (10 Gb of JSON data),which has been parsed and preprocessed into aconvenient format for experimentation.
In theseinteraction logs, every click of students on theMOOC video player is registered (play, pause,seek forward, seek backward, scroll forward,scroll backward, ratechange).
We have informa-tion about the rate at which the video lecture isplayed, total time spent on playing the video andtime spent on/in-between various click events suchas play, pause, seek etc.
In total, 65969 stu-dents registered for the course, and 36536 of themhad 762137 logged video interaction sessions con-taining the aforementioned types of click events.If a video is played till the end, then an auto-matic video-end pause is generated.
Otherwise,4the Coursera platform unfortunately does not logwhether or not a student has left the video in themiddle, leaving the true video engagement timeunknown.
To avoid biased data, we only includevideo sessions containing video-end pauses.
Thishas yielded a dataset of 222021 video sessionsfrom 21952 students for our analysis in this paper.3 Operationalizing the Clickstream3.1 Level 1 (Operations)From our raw clickstream data, we construct a de-tailed encoding of students?
clicks in the follow-ing 8 categories: Play (Pl), Pause (Pa), SeekFw(Sf), SeekBw (Sb), ScrollFw (SSf), ScrollBw(SSb), RatechangeFast (Rf), RatechangeSlow(Rs).
When two seeks happen within a small timerange (< 1 sec), we group these seek events into ascroll.
Additionally, to encode ?Rf?
and ?Rs?, welook for the playrate of the click event that occursjust before the ?Ratechange?
click and compare itwith students?
currently changed playrate, to de-termine whether he has sped up/slowed down hisplaying speed.
The reason behind encoding click-streams to such specific categories, accommodat-ing scrolling behavior and clicks representative ofincrease and decrease in video playing speed, is toexperimentally analyze and understand the impactof such a granularity on our experiments, whichare designed with an objective to capture the mot-ley of differently motivated behavioral watchingstyle in students.As a next step, we concatenate these clickevents for every student, for every video lec-ture watched.
Thus, the output from level 1 isthis string of symbols that characterizes the se-quence of clickstream events (video watching statesequence).
For e.g: PlPaSfSfPaSbPa.., PlSSb-PaRsRsPl..3.2 Level 2 (Behavioral Actions)Existing literature on web usage mining says thatrepresenting clicks using higher level categories,instead of raw clicks, better exposes the brows-ing pattern of users.
This might be because highlevel categories have better noise tolerance thannaive clickstream logs.
The results obtained fromgrouping clickstream sequences at per click res-olution are often difficult to interpret, as sucha fine resolution leads to a wide variety of se-quences, many of which are semantically equiv-alent.
Therefore, to get more insights into stu-dent behavior in MOOCs, we group clicks en-coded at very fine granularity into meaningful be-havioral categories.
Doing this also reduces se-quence length which is easily interpretable.
Thereis some existing literature (Banerjee and Ghosh,2000; Wang et al., 2013), that just considers clickas a binary event (yes/no) and discusses formationof concept based categories based on the area/subarea of the stimulus where the click was made.To summarize a students?
clickstream, we ob-tain n-grams with maximum frequency from theclickstream sequence (a contiguous sequence of?n?
click actions).
Such a simple n-gram represen-tation convincingly captures the most frequentlyoccurring click actions that students make in con-junction with each other (n=4 was empirically de-termined as a good limit on clickstream subse-quence overspecificity).
Then, we construct sevensemantically meaningful behavioral categories us-ing these discovered n-grams, selecting represen-tative click groups that occur within top ?k?
mostfrequent n-grams (k=100).
Each behavioral cate-gory acts like a latent variable, which is difficult tomeasure from data directly.?
Rewatch: PlPaSbPl, PlSbPaPl, PaSbPlSb,SbSbPaPl, SbPaPlPa, PaPlSbPa?
Skipping:SfSfSfSf, PaPlSfSf, PlSfSfSf, SfS-fSfPa, SfSfPaPl, SfSfSfSSf, SfSfSSfSf, Sf-PaPlPa, PlPaPlSf?
Fast Watching: PaPlRfRf, RfPaPlPa, RfRf-PaPl, RsPaPlRf, PlPaPlRf (click group ofRatechange fast clicks while playing or paus-ing video lecture content, indicating speedingup)?
Slow Watching: RsRsPaPl, RsPaPlPa,PaPlRsRs, PlPaPlRs, PaPlRsPa, PlRsPaPl(click group of Ratechange slow clicks whileplaying or pausing video lecture content, in-dicating slowing down)?
Clear Concept: PaSbPlSSb, SSbSbPaPl,PaPlSSbSb, PlSSbSbPa (a combination ofSeekBw and ScrollBw clicks, indicating hightussle with the video lecture content)?
Checkback Reference: SbSbSbSb, PlSbS-bSb, SbSbSbPa, SbSbSbSf, SfSbSbSb, Sb-PlSbSb, SSbSbSbSb (a wave of SeekBwclicks)?
Playrate Transition: RfRfRsRs, RfRfRfRs,RfRsRsRs, RsRsRsRf, RsRsRfRf, RfRfRfRf(a wave of ratechange clicks)5Case (Full, No, Partialmatch)Clickstream A Clickstream B Fuzzy string matchingverdict1:Varying clickstreamlengthPlPaPlSfPaSfSbSbPl PlPaPlSfPaSfSbSbPlPaSbSbSbRfRsRf(learner has performed lot more clicks)Weight(P,A)>Weight(P,B)2:Behavioral pattern ap-pears more than oncePlPaPlSfPaSfSbSbPl PlPaPlSfPaSfSbSbPlPlSfPaSf(pattern is more characteristic as it ap-pears 2 times)Weight(P,A)<Weight(P,B)3:No appearance of be-havioral patternRfSbSbRs SSfSSfRsRsRsSfSfSfSfRfRfRfRfRf(string length doesn?t matter)Weight(P,A)6=(P,B)(very low weight)4:Variation in number ofindividual clicksRfSbSbRsPlSbPaSb RfSbSbRsPlSbSfPaSfSb(more clicks from pattern appear)Weight(P,A)<Weight(P,B)5:Variation in scatteringof individual clicksRfSbRsPlSbSfPaSfSb(less scattering)RfSbRsPlSbSSbSfPlSbRsPaSbRfSf(more scattering)Weight(P,A)>Weight(P,B)6:Reverse order of indi-vidual click appearanceRfRsSbSfPaSfSbPl(order reversed)RfRsPlSbSfPaSfSb(order maintained)Weight(P,A)<Weight(P,B)Table 1: Fuzzy string similarity weights for the sample behavioral action P(?PlSfPaSf?).
Weight(P, A/B)represents the similarity of the pattern P w.r.t clickstream sequence A or B.In an attempt to quantify the importance of eachof the above behavioral actions in characterizingthe clickstream, we adopt a fuzzy string match-ing approach.
Using this approach, we assigna weight to each of the grouped behavioral pat-terns for a given students?
video watching state se-quence (based on similarity of click groups presentin each behavioral category, with the full click-stream sequence).
The fuzzy string method (Van,2014) is justified because it caters to the noise thatmight be present in raw clickstream logs of stu-dents, in six different ways, as mentioned in Ta-ble 1.
After identifying these cases and meticu-lous experimental evaluation, we apply the follow-ing distance metrics and tuning parameters: Co-sine similarity metric between the vector of countsof n-gram (n=4) occurrences for Cases 1 and 2,Levenshtein similarity metric for Cases 3 (weightfor deletion=0, weight for insertion and substitu-tion=1), 4, 5, 6 (weight for deletion=0.1, weightfor insertion, substitution=1).As a next step, all subcategories of click groupsthat lie within each behavioral category are aggre-gated by summing up the individual fuzzy stringsimilarity weights that are obtained with respectto every students?
clickstream sequence.
Then,we perform a discretization of these summed upweights, for each behavioral category, by equalfrequency (High/Low).
The concern of adding uptwo distance metrics that do not lie in the samerange, is thus alleviated, because the dichotomiza-tion automatically places highly negative values inthe ?Low?
category and positive values closer to0 in the ?High?
category.
The result is a click-stream vector for each video viewing session ofthe student, where every element of the vectortells us about the weight (importance) of a behav-ioral category for characterizing the clickstream.Thus, the output from level 2 is such a summarizedclickstream vector.
For e.g: (Skipping=High, FastWatching=High, Checkback Reference=Low, Re-watch=Low, ....).3.3 Level 3 (Information Processing)Watching MOOC videos is an interaction betweenthe student and the medium, and therefore the con-ceptualization of higher-order thinking eventuallyleading to knowledge acquisition (Chi, 2000), isunder control of both the a)student (who decideswhat video segment to watch, when and in whatorder to watch, how hard an effort be made totry and understand a specific video segment) and,b)medium/video lecture (the content or featuresof which decides what capacity allocation is re-quired by the student to fully process the informa-tion contained).Research has consistently found that the levelof cognitive engagement is an important aspect ofstudent participation (Carini et al., 2006).
Thiscognitive processing is influenced by the appeti-tive (approach) and aversive (avoidance) motiva-tional systems of a student, which activate in re-sponse to motivationally relevant stimuli in the en-vironment (Cacioppo and Wendi, 1999).
For ex-ample, in the context of MOOCs, the appetitivesystem?s goal is in-depth exploration and infor-mation intake, while the aversive system primar-ily serves as a motivator for not attending to cer-tain MOOC video segments.
Thus, click behaviorsrepresentative of appetitive motivational systemare rewatch/clear concept/slow watching, whileclick behaviors representative of aversive motiva-6Figure 1: Relating students?
information processing to click behaviors exhibited in the MOOC, based onvideo lecture perceptiontional system are skipping/fast watching.
In thiswork, we try to construct students?
informationprocessing index, based on the ?Limited CapacityInformation Processing Approach?
(Basil, 1994;Lang et al., 1996; Lang, 2000), which asserts thatpeople independently allocate limited amount ofcognitive resources to tasks from a shared pool.Figure 1 depicts this idea.We must acknowledge the fact that video watch-ing in MOOCs requires students to recall facts thatthey already know (specific chunks of declarativeknowledge (Anderson, 2014).
This helps them tobuild a mental representation of the informationpresented in a MOOC video lecture segment, fol-low and understand the concept being currentlytaught.
However, it must be noted that dependingon the a)expertise level, which decides how avail-able the past knowledge is and how hard is it toretrieve the previously known facts, b)perceptionof video lecture as difficult or simple to under-stand, c)motivation to learn or just have a look atthe video lecture to seek specific outcomes, cog-nitive resource allocation would vary among thesetime sensitive subprocesses in stage 1 and 2 of thepipeline (depicted in Figure 1).
This in turn, wouldbe reflected by the underlying non linear navi-gational patterns that students have, specificallythe nature of clicks which they make to adjustthe speed of information processing (by pausing,seeking forward/backward, ratechange clicks), asresponses to the stimuli.Consider an example of students who watch theMOOC lecture, primarily because of reasons suchas gaining familiarity with the topic.
Such stu-dents would purposely not allocate their process-ing resources to ?memory?
part of the informationprocessing pipeline (encode, store, retrieve).
Ad-ditionally, they will decode and process minimalinformation that is required to follow the story.On the contrary, students who watch the MOOClecture, with the aim of scoring well in post-tests(MOOC quizzes and assignments), would allocatehigh cognitive processing to understand, learn andretain information from the lecture.
Thus, suchstudents would process information more fullyand thoroughly, despite a possibility of cognitiveoverload.In order to relate our behavioral actions con-structed from the raw clickstream with this richand informative stream of literature, we create ataxonomy of behavioral actions exhibited in theclickstream to construct a quantitative ?Informa-tion Processing Index (IPI)?.
Figure 2 reflectsthe proposed hierarchy of information processingfrom high to low using linear weight assignments.We omit the line of reasoning that goes behinddefining the precise position of each behavioral ac-tion in this hierarchy due to lack of space.
How-ever, the details can be found in (Sinha, 2014c).Negative weights are necessary to distinguish be-tween ?high?
and ?low?
weights for each behav-ioral action.
For example, if skipping=high isweighted -3, skipping=low will be weighted +3 onthe information processing index.
Students?
infor-7mation processing index is defined as follows:Information Processing Index (IPI) =(?1)j7?i=1WeightAssign(Behavioral Action i),j=1,2 depending on whether the behavioral actionis weighted low or high.Figure 2: Linear weight assignments for behav-ioral clickstream actions, according to the infor-mation processing hierarchy developedOne of the focal utilities of developing sucha quantitative index is that meaningful interven-tion could be provided in real time to students, asthey steadily build up their video watching pro-file while interacting with MOOC video lectures.Viewing throught the lens of the Goldilocks prin-ciple (Kidd et al., 2012), our metric can poten-tially help instructors in understanding and differ-entiating between students looking away from theMOOC visual sequence, because of too simple ortoo complex representation.
Adaptive presenta-tion of instructional materials is another learningscience application where leveraging our metricwould be beneficial.Specifically, when IPI > 0, it can be inferredthat high information processing is being doneby students.
Therefore MOOC instructors needto check for coherency in pace of instruction de-livery and students?
understanding.
This mightalso hint towards redesigning specific video lec-ture segments and simplifying them so that theybecome easier to follow.
On the contrary, whenIPI < 0, low information processing is being doneby students.
Therefore MOOC instructors needto help students better engage with the course,by providing them additional interesting read-ing/assignment material, or fixing video lecturecontent such that it captures students?
attention.The neutral case of IPI = 0 occurs when students?locally exhibited high and low information pro-cessing needs in their evolving clickstream se-quence counterbalance each other.
So, interven-tions need to made depending on the video lecturesegment, where IPI was >0 or <0.4 Validation ExperimentsWe use machine learning to validate the method-ology developed in section 3.1 and 3.2 for sum-marizing students?
clickstream, ensuring that thesame student does not appear in the train and testfolds.
The motivation behind setting up these ex-periments is to automatically measure students?length of interaction with MOOC video lectures,understand how they develop their video watch-ing profile and discern what viewing profile of stu-dents leads to in-video and course dropouts.
Fur-thermore, we validate the methodology developedin section 3.3 by statistically analyzing variationsof IPI and testing its sensitivity to student attritionusing survival models.4.1 Machine Learning Experiment Design4.1.1 How much do you engage?Students, while watching MOOC video lecturescan pause, seek, scroll and change rate of thevideo.
Thus, it is meaningful to quantify students?engagement as the summation of video playingtime, seeks & pauses, multiplied by the playbackrate.
For example, if a student plays 700 secs outof a 1000 sec video, pauses 2 times for 100 secseach, at an average play rate of 1.5, he effectivelyengages with the video for (700+200)?1.5=1350secs.
Such an interaction measure multiplied byplayback rate, is representative of effective videolecture content covered.Research Question 1: Can students?
click-stream sequence predict length of students?
inter-action with the video lecture?Settings: The data for this experiment comesfrom a randomly chosen video lecture 4-6 (6th lec-ture in the 4th week of the course, with not toomany initial lurkers and not too many dropouts).For experimental purposes, engagement times forstudents are discretized by equal frequency into 2categories (High/Low).
The dependent variable isstudent engagement (High: 1742 examples, Low:1741 examples).
L2 regularized Logistic Regres-sion is used as the training algorithm (with 10fold cross validation annotated by student-id and8rare feature extraction threshold being 2).
As fea-tures, we extract N-grams of length 4 and 5, se-quence length and regular expressions from stu-dents?
clickstream sequences.
In the changedsetup, we consider summarized behavioral actionvectors (output from level 2) as column features.4.1.2 Are you bored or challenged?Next, we focus our attention on how clickstreamsequences evolve.
If we know that students?
in-teraction with the video lecture is going to befor a long time (reflected by high engagement),it could have been the case that they were strug-gling at the current level of instruction (for exam-ple, a high combination of pause/seek backwardevents).
Therefore, if such a phenomenon canbe detected in real time video lecture interaction,such learners can be presented with reinforcementcourse material before moving forward.
Alterna-tively, if we know that students?
interaction withthe video lecture is going to be for a short time (re-flected by low engagement), they could be boredor are quite likely to skip course content forwardoften.
Such students can be presented with ad-vanced study material.
However, in order to de-velop such a real time knowledge model and tailortargeted interventions at students, we need to studythe trajectory of click sequence formation.Research Question 2: Can we precisely predictwhat will be the next sequence of clicks that leadsstudents to different engagement states?Settings: The data for this experiment comesfrom the same video lecture 4-6 (6th lecture inthe 4th week of the course).
The dependentvariable is next click state of students (Pa, Pl,Sf, SSf, Sb, SSb, Rf, Rs).
L2 regularized Lo-gistic Regression is used as the training algo-rithm (with 5 fold cross validation annotated bystudent-id and rare feature extraction thresholdbeing 5).
If we want to predict the click atthe ithinstant, we extract the following featuresfrom 0 till (i-1)thinstant: a)Engagement with thevideo lecture as defined for Research Question1(High/Low); b)Proportion of click events belong-ing to Pl/Pa/Sf/SSf/Sb/SSb/Rf/Rs (representativeof kind of interaction with the stimulus); c)N-grams of length 4,5 and sequence length fromstudents?
clickstream sequences.
In the changedsetup, we consider summarized behavioral actionvectors (output from level 2) as column features.4.1.3 Will you drop out of the video?As students progress through the video, theyslowly build up their video watching profile byinteracting with the stimulus in different propor-tions, which in turn depend on their click actionsequences.
This motivates our next machine learn-ing experiment, which seeks to derive utility fromthe first two experiments.
Navigating away fromthe video without completing it fully is an out-come of low student engagement.
A student ismore likely to watch till the end of a video, if thelecture activates his thinking.
Thus, it would be in-teresting to investigate, whether the nature of stu-dents?
interaction provides us a hint about in-videodropout behavior.
Prior work has made a prelim-inary study on how in-video dropout is correlatedwith length of the video, and how in-video dropoutvaries among first time watchers and rewatchers(Kim et al., 2014a).
However, we consider videointeraction features at a much finer granularity,representative of how students progress throughthe video.
In doing so, we use detailed clickstreaminformation, including seek, scroll and ratechangebehavior, in addition to merely play and pause in-formation.Research Question 3: What video watchingprofile of students leads to in-video dropouts?Settings: The data for this experiment comesfrom the same video lecture 4-6 (6th lecture inthe 4th week of the course).
The dependentvariable is the binary variable, in-video dropout(0/1).
To address the skewed class distribution,cost sensitive L2 regularized Logistic Regressionis used as the training algorithm (with 10 foldcross validation annotated by student-id and rarefeature extraction threshold being 2).
To ex-tract the interaction footprint of students beforethey drop out of the video, we extract the fol-lowing features: a)N-grams of length 4,5 andsequence length from students?
clickstream se-quences; b)Proportion of click events belonging toPl/Pa/Sf/SSf/Sb/SSb/Rf/Rs (representative of kindof interaction with the stimulus); c)Engagementwith the video lecture as defined for ResearchQuestion 1(High/Low); e)Last click action beforedropout happened; f)Time spent after the last clickaction was made (discretized by equal frequencyto High/Low).
In the changed setup, we con-sider summarized behavioral action vectors (out-put from level 2) as column features.94.1.4 Will you watch videos and stay till thecourse end?We may expect that when students find the coursetoo tough to follow, uninteresting or boring, theywill not engage with future videos.
On the con-trary, when students seem very interested in un-derstanding the video and exhibit lots of rewatch-ing behavior, we might expect them to stay on tillthe course end video lectures.
Students who donot stay till the last week of the course (exhibit anyvideo lecture viewing), are considered as completecourse dropouts.
One principal application of de-tecting these dropouts early could be recommen-dation of selected future video lectures to watch(for example, where an interesting concept, casestudy or application is going to be discussed), topositively motivate and pull these students backinto the MOOC.Research Question 4: Can we discover pat-terns in the video watching trajectory of studentsthat can predict when are students most likely notto view future video lectures?Settings: The data for this experiment comesfrom all 48 videos of ?Functional Program-ming in Scala?
MOOC (4710 non-dropouts, 9596dropouts).
To address the skewed class distribu-tion, cost sensitive L2 regularized Logistic Re-gression is used as the training algorithm (with 5fold cross validation annotated by student-id andrare feature extraction threshold being 5).
Thedependent variable is the binary variable, com-plete course dropout (0/1), indicating whether thestudent ultimately stayed on (watched any videolecture) till the last course week.
Engagement(time in seconds) of a student is discretized byequal frequency into High and Low categories,considering all interactions in each video lectureseparately (because length of each video differs,so the discretization criteria would also differ foreach video).
Video play proportion((video playedlength/video length)*100*average play rate) for astudent is discretized by equal width (Very Low:<50%, Low: 50-100%, High: 100-150%, VeryHigh: >150%).
IPI for a student is discretized byequal frequency (Very Low: <-1.00, Low: [-1.00,1.00], High: [1.00, 3.00], Very High: >3.00).The discretization criteria (equal width, frequencyand number of bins) was experimentally deter-mined.
Development of trajectories for each ofthese factors is indicated in Figure 3.
To extractthe interaction footprint of students before theydrop out of the course, we extract the followingfeatures: a)N-grams of length 4,5 and sequencelength from ?Engagement trajectory?, ?Video PlayProportion trajectory?
and ?IPI trajectory?
of stu-dents for the videos watched from 0 to (n-1)th in-stant, b)Engagement, Video Play Proportion andIPI trajectories for the nth instance (attribute forthe last video lecture watched before droppingout), c)Proportion of different symbol representa-tions in the trajectories (for example, in a trajec-tory such as HLLHH, proportion(H)=60%, pro-portion(L)=40%.Figure 3: Example depicting how different opera-tionalized trajectories of students are formed4.2 ResultsResults of the four machine learning experiments,along with the most representative (weighted) fea-tures that characterize classes, are reported in table2.
There are two important positives here: a)Thesummarized behavioral action vectors from level2 are able to achieve nearly similar values of ac-curacy and kappa when compared to the raw levelclicks.
This means that we can reason differentmeaningful video viewing behaviors of studentswithout getting our hands dirty in examining noisyand continually occurring raw clicks, b)Our met-ric of interest, i.e the false negative rate1is lowerfor Case 1.B and Case 3.B, as compared to Case1.A and Case 3.A, which shows the effectivenessof the clickstream summarization approach (level2) in pre-deciphering the fate of students to someextent.Additionally, we leverage a statistical analy-sis technique referred as survival analysis (Miller,2011), to quantify the extent to which our summa-rized behavioral clickstream action vectors and IPIare sensitive to students?
dropout.
In this model-ing scheme, dropout variable is 1 on the students?last week of active participation (in terms of videolecture viewing), and is 0 for all other weeks.Our investigation results indicate that a)Students?1False negative rate of 0.x means that we correctly iden-tify (100-(100*0.x))% of dropouts10ResearchQuestionCondition AccuracyKappaFalse NegativeRateMost representative (weighted) features that char-acterize classes1.
EngagementPredictionA)Raw Clicks0.810.630.24 High (skipping=low, playrate transition=low, re-watch=high, slow watching=low, checkback refer-ence=low, clear concept=high)B)SummarizedBehavioralActionVectors0.750.490.15 Low (skipping=high, playrate transition=high, re-watch=low, slow watching=high, checkback refer-ence=high, clear concept=low)2.
Next ClickPredictionA)Raw Clicks0.680.57- SeekFw (playratetransition=low, skipping=low, fastwatching=high, clearconcept=low)SeekBw (checkbackreference=high, rewatch=low,playratetransition=low, propSeekBw, clearcon-cept=high)B)SummarizedBehavioralActionVectors0.660.54- Ratechangefast (playratetransition=high, re-watch=low, checkbackreference=low)Ratechangeslow (playratetransition=high, clearcon-cept=high)3.
In-videodropoutPredictionA)Raw Clicks0.900.690.19 Non dropouts (skipping=low, clearconcept=high,slow watching=high, Checkbackreference=low,rewatch=high, engagementfromStart=low, engage-mentlastClick=high)B)SummarizedBehavioralActionVectors0.900.700.15 Dropouts (skipping=high, clearconcept=low,slow watching=low, engagementfromStart=high,rewatch=low, engagementlastClick=low, checkback-reference=high)4.
CompleteCourse dropoutPredictionOperationalizedtrajectories0.800.570.143 Non dropouts (trajectory IPI=H H H H, trajec-tory eng=H H H VL H, trajectory vpp=H H H L H)Dropouts (trajectory IPI=H H VL VL VL, trajec-tory eng=H L H L L, trajectory vpp=H H H H VL)Table 2: Performance metrics for machine learning experiments.
Random baseline performance is 0.5dropout in the MOOC is 37% less likely, if theyhave one standard deviation greater IPI than aver-age (Hazard ratio: 0.6367, p<0.001).
Such stu-dents grapple more with the course material toachieve their desired learning outcomes (as re-flected by their video lecture participation), b)Ifstudents?
rewatching behavior changes from lowto high, they are 33% less likely to dropout (Haz-ard ratio: 0.6734, p<0.001), c)As students startwatching more proportion of the video lecture,they are 37% less likely to dropout of the MOOC(Hazard ratio: 0.6334, p<0.001).
This is indica-tive of their continued interest in the video lecture.Next, to discern how IPI fluctuates among dif-ferent student partitions and validate whether ouroperationalization produces meaningful results,we plot figures 4, 5 and perform statistical tests,specifically z test (testing significance of differ-ence between means for large sample sizes, whenpopulation standard deviation is known).
Pop-ulation refers to all students in the MOOC be-ing currently studied.
The right half of figure 4depicts the variation of average IPI, among highversus low engagers and in-video dropouts ver-sus non dropouts, in the same video lecture 4-6 from the course, that we have been perform-ing our experiments on.
Similar findings werealso confirmed with other randomly chosen coursevideos.
The left half of figure 4 shows the fre-quency distribution of average IPI.
This figureconcurs with our intuitions.
The average IPI issignificantly higher for students with ?High?
en-gagement (|z|=8.296, p<0.01) and ?Non In-videodropouts?
(|z|=22.54, p<0.01).
This is also re-flected in the histogram, which clearly showsthat many non in-video dropouts have positiveIPI that pushes up the average.
Because the ef-fect is smaller in low engagers versus high en-gagers, we see a more similar frequency distri-bution of average information processing indicesin these 2 bins, as compared to contrasting differ-ences in the histogram for in-video dropouts andnon dropouts.
In order to generalize these find-ings, we also look at the variations of averageIPI among some other student partitions that wemade for the whole course.
?Viewers?
are stu-dents who have watched or interacted with somevideo lecture but have not done the exercises; the?Active?
students additionally turn in homeworkalso.
MOOC dropouts are those students whocease to actively participate in the MOOC (we areconcerned with video lecture viewing only) before11Figure 4: Variation of Average Information Processing Indices(IPI) for Video 4-6Figure 5: Variation of Average Information Processing Indices(IPI) for the full coursethe last week, i.e, students who do not finish thecourse.
An important observation in figure 5 isthat IPI is clearly able to distinguish between Non-dropouts and Dropouts (|z|=9.06, p<0.01).
Thisis also reflected in the histogram in the left half offigure 5, which verifies that more ?Non dropouts?have positive IPI.
More is the information pro-cessing done by students, greater is the video lec-ture involvement, higher are the chances to derivetrue utility from video lecture and remain excitedand motivated to stay in the course.
We also ob-tain striking differences between ?Active?
versus?Viewers?
(|z|=10.45, p<0.01).
Intuitively too,we expect ?Viewers?
to have higher IPI than ?Ac-tive?
class, because as their primary MOOC activ-ity, ?Viewers?
grapple more with the video lecture.5 ConclusionIn this work, we have begun to lay a foundation forresearch investigating students?
information pro-cessing behavior while interacting with MOOCvideo lectures.
Focusing the center of gravity onthe human mind, we applied a cognitive videowatching model to explain the dynamic processof cognition involved in MOOC video clickstreaminteraction.
This paved way for the developmentof a simple, yet potent IPI using linear weight as-signments, which can be effectively used as anoperationalization for making predictions regard-ing critical learner behavior.
We could contem-plate that IPI significantly varies among differentstudent partitions.
This actually happens becauseof presence of smaller substructures inside theselarger groupings, that are similar in their click be-haviors.
Deciphering unique ways of video lec-ture interaction in such smaller clusters using ap-proaches such as Markov based clustering, wouldbe very meaningful for course instructors, to de-sign customized learning solutions for studentswithin them (Sinha, 2014c).
It would make senseto incorporate student demographics to better un-derstand some latent factors, such as playbackspeed choices due to native language differencesversus engagement etc.
In our recent work (Sinhaet al., 2014), we have been seeking to gain bet-ter visibility into how combined representations ofvideo clickstream behavior and discussion forumfootprint can provide insights on interaction path-ways that lead students to central activities.12ReferencesAnderson, A., Huttenlocher, D., Kleinberg, J., &Leskovec, J.
(2014, April).
?Engaging with massiveonline courses?.
In Proceedings of the 23rd interna-tional conference on World wide web (pp.
687-698).International World Wide Web Conferences SteeringCommittee.Anderson, J. R. (2014).
?Rules of the mind?.
Psychol-ogy Press.Banerjee, A., & Ghosh, J.
(2000).
?Concept-basedclustering of clickstream data?.Basil, M. D. (1994).
?Multiple resource theory I ap-plication to television viewing?.
Communication Re-search, 21(2), 177-207Belanger, Y., & Thornton, J.
(2013).
?Bioelectricity:A Quantitative Approach Duke Universitys FirstMOOC?.Brinton, C. G., Chiang, M., Jain, S., Lam, H., Liu,Z., & Wong, F. M. F. (2013).
?Learning about so-cial learning in moocs: From statistical analysis togenerative model?.
arXiv preprint arXiv:1312.2159.Cacioppo, J. T., and Wendi L. G. (1999).
?Emotion?.Annual Reviews: Psychology, 50, 191-214.Carini, R. M., Kuh, G. D., & Klein, S. P. (2006).
?Stu-dent engagement and student learning: Testing thelinkages?.
Research in Higher Education, 47(1), 1-32.Chi, M. T. (2000).
?Self-explaining expository texts:The dual processes of generating inferences and re-pairing mental models?.
Advances in instructionalpsychology, 5, 161-238.Clow, D. (2013).
?MOOCs and the funnel of participa-tion?
In Proceedings of the Third International Con-ference on Learning Analytics and Knowledge, pp.185-189.
ACMCoetzee, D., Fox, A., Hearst, M. A., & Hartmann,B.
(2014, February).
?Should your MOOC forumuse a reputation system??.
In Proceedings of the17th ACM conference on Computer supported coop-erative work & social computing (pp.
1176-1187).ACM.Davis, H. C., Dickens, K., Leon Urrutia, M., Vera, S.,del Mar, M., & White, S. (2014).
?MOOCs for Uni-versities and Learners An analysis of motivating fac-tors?.Fischer, G. (2011).
?Understanding, fostering, and sup-porting cultures of participation?.
Interactions 18,no.
3: 42-53.Guo, P. J., & Reinecke, K. (2014, March).
?Demo-graphic differences in how students navigate throughMOOCs?.
In Proceedings of the first ACM confer-ence on Learning@ scale conference (pp.
21-30).ACM.Guo, Philip J., Juho Kim, and Rob Rubin.
(2014).
?How video production affects student engagement:An empirical study of mooc videos?
ACM Learingat Scale(L@S), pp.
41-50.Haggard, S., S. Brown, R. Mills, A. Tait, S. Warburton,W.
Lawton, and T. Angulo.
(2013).
?The maturingof the MOOC: Literature review of Massive OpenOnline Courses and other forms of online distancelearning?
BIS Research Paper 130Huang, J., Dasgupta, A., Ghosh, A., Manning, J., &Sanders, M. (2014, March).
?Superposter behav-ior in MOOC forums?.
In Proceedings of the firstACM conference on Learning@ scale conference(pp.
117-126).
ACM.Kidd, C., Piantadosi, S. T., & Aslin, R. N. (2012).
?TheGoldilocks effect: Human infants allocate attentionto visual sequences that are neither too simple nortoo complex?.
PLoS One, 7(5), e36399.Kim, J., Guo, P. J., Seaton, D. T., Mitros, P., Gajos,K.
Z., & Miller, R. C. (2014a, March).
?Understand-ing in-video dropouts and interaction peaks inonlinelecture videos?.
In Proceedings of the first ACM con-ference on Learning@ scale conference (pp.
31-40).ACM.Kim, J., Shang-Wen L., Carrie J. C., Krzysztof Z. G.,Robert C. M. (2014b).
?Leveraging Video Interac-tion Data and Content Analysis to Improve VideoLearning?
CHI 2014 Workshop on Learning Inno-vation at ScaleKulkarni, C. E., Socher, R., Bernstein, M. S., & Klem-mer, S. R. (2014, March).
?Scaling short-answergrading by combining peer assessment with algo-rithmic scoring?.
In Proceedings of the first ACMconference on Learning@ scale conference (pp.
99-108).
ACM.Lang, A., John N., and Byron R. (1996).
?Negativevideo as structure: Emotion, attention, capacity,and memory?.
Journal of Broadcasting & ElectronicMedia, 40(4), 460-477Lang, A.
(2000).
?The limited capacity model of me-diated message processing?.
Journal of communica-tion, 50(1), 46-70.Lie M. T., Debjanee B., Judy K. (2014).
?Online learn-ing at scale: user modeling requirements towardsmotivation and personalisation?.
In Learning Inno-vations at Scale CHI?
14 WorkshopMiller Jr, Rupert G. (2011).
?Survival analysis?.
Vol.66.
John Wiley & SonsNawrot, I., and Antoine D. (2014).
?Building engage-ment for MOOC students: introducing support fortime management on online learning platforms.?
InProceedings of the companion publication of the23rd international conference on World wide webcompanion, pp.
1077-1082.13North, S. M., Ronny R., and Max M. N. (2014).
?ToAdapt MOOCS, or Not?
That is No Longer theQuestion.?
Universal Journal of Educational Re-search 2(1): 69-72Ramesh, A., Goldwasser, D., Huang, B., Daum H. III,and Getoor, L. (2013).
?Modeling Learner Engage-ment in MOOCs using Probabilistic Soft Logic?.
InNIPS Workshop on Data Driven EducationSchmidt, D. C., and Zach M. (2013).
?Producing andDelivering a Coursera MOOC on Pattern-OrientedSoftware Architecture for Concurrent and Net-worked Software?Sinha, T., Banka, A., Kang, D. K., (2013).
?Leveraginguser profile attributes for improving pedagogical ac-curacy of learning pathways?.
In Proceedings of 3rdAnnual International Conference on Education andE-Learning(EeL 2013), SingaporeSinha, T. (2014a).
?Together we stand, Together wefall, Together we win: Dynamic team formation inmassive open online courses?
In Fifth InternationalConference on the Applications of Digital Informa-tion and Web Technologies (ICADIWT) pp.
107-112.IEEESinha, T. (2014b).
?Supporting MOOC Instructionwith Social Network Analysis?.
arXiv preprintarXiv:1401.5175Sinha, T. (2014c).
?Your click decides your fate?
:Leveraging clickstream patterns from MOOC videosto infer students?
information processing & attritionbehavior.
arXiv preprint arXiv:1407.7143.Sinha, T., Li, N., Jermann, P., Dillenbourg, P. (2014).Capturing attrition intensifying structural traits fromdidactic interaction sequences of MOOC learners.Proceedings of the 2014 Empirical Methods in Nat-ural Language Processing Workshop on ModelingLarge Scale Social Interaction in Massively OpenOnline Courses, Qatar, October 2014Tillmann, N., De Halleux, J., Xie, T., Gulwani, S., andBishop, J.
(2013).
?Teaching and learning program-ming and software engineering via interactive gam-ing?.
In ICSE, 11171126Van der L., Mark PJ.
(2014).
?The stringdist Packagefor Approximate String Matching?
The R JournalWang, G., Tristan K., Christo W., Xiao W., HaitaoZ., and Ben Y.
Z.
(2013).
?You are how you click:Clickstream analysis for sybil detection?
In USENIXSecurity Symposium (Washington, DC)Yang, D., Sinha T., Adamson D., and Rose C. P.
(2013).
?Turn on, Tune in, Drop out: Anticipating studentdropouts in Massive Open Online Courses?
In NIPSWorkshop on Data Driven Education14
