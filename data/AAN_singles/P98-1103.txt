Context Management with Topics for Spoken Dialogue SystemsKristiina Jokinen and H idek i  Tanaka  and Ak io  YokooATR Interpret ing Te lecommunicat ions Research Laborator ies2-2 Hikaridai,  Seika-cho, Soraku-gunKyoto  619-02 Japanemail : {kj okinen\[tanakah\[ayokoo}~itl, air.
co. jpAbst ractIn this paper we discuss the use of discourse con-text in spoken dialogue systems and argue that theknowledge of the domain, modelled with the help ofdialogue topics is important in maintaining robust-ness of the system and improving recognition accu-racy of spoken utterances.
We propose a topic modelwhich consists of a domain model, structured into atopic tree, and the Predict-Support algorithm whichassigns topics to utterances on the basis of the topictransitions described in the topic tree and the wordsrecognized in the input utterance.
The algorithmuses a probabilistic topic type tree and mutual infor-mation between the words and different opic types,and gives recognition accuracy of 78.68% and preci-sion of 74.64%.
This makes our topic model highlycomparable to discourse models which are based onrecognizing dialogue acts.1 In t roduct ionOne of the fragile points in integrated spoken lan-guage systems is the erroneous analyses of the initialspeech input.
1 The output of a speech recognizer hasdirect influence on the performance of other mod-ules of the system (dealing with dialogue manage-ment, translation, database search, response plan-ning, etc.
), and the initial inaccuracy usually getsaccumulated in the later stages of processing.
Per-formance of speech recognizers can be improved bytuning their language model and lexicon, but prob-lems still remain with the erroneous ranking of thebest paths: information content of the selected ut-terances may be wrong.
It is thus essential to usecontextual information to compensate various errorsin the output, to provide expectations of what willbe said next and to help to determine the appropri-ate dialogue state.However, negative ffects of an inaccurate contexthave also been noted: cumulative rror in discoursecontext drags performance of the system below therates it would achieve were contextual information1 Alexandersson (1996) remarks that with a 3000 word lex-icon, a 75 % word accuracy means that in practice the wordlattice does not contain the actually spoken sentence,not used (Qu et al, 1996; Church and Gale, 1991).Successful use of context thus presupposes appro-priate context management: (1) features that definethe context are relevant for the processing task, and(2) construction of the context is accurate.In this paper we argue in favour of using onetype of contextual information, topic information,to maintain robustness of a spoken language sys-tem.
Our model deals with the information contentof utterances, and defines the context in terms oftopic types, related to the current domain knowl-edge and represented in the form of a topic tree.To update the context with topics we introduce thePredict-Support algorithm which selects utterancetopics on the basis of topic transitions described inthe topic tree and words recognized in the currentutterance.
At present, the algorithm is designed asa filter which re-orders the candidates produced bythe speech recognizer, but future work encompassesintegration of the algorithm into a language modeland actual speech recognition process.The paper is organised as follows.
Section 2 re-views the related previous research and sets out ourstarting point.
Section 3 presents the topic modeland the Predict-Support algorithm, and section 4gives results of the experiments conducted with themodel.
Finally, section 5 summarises the propertiesof the topic model, and points to future research.2 Previous researchPrevious research on using contextual informationin spoken language systems has mainly dealt withspeech acts (Nagata nd Morimoto, 1994; Reithingerand Maier, 1995; MSller, 1996).
In dialogue sys-tems, speech acts seem to provide a reasonable firstapproximation of the utterance meaning: they ab-stract over possible linguistic realisations and, deal-ing with the illocutionary force of utterances, canalso be regarded as a domain-independent aspect ofcommunication.
22Of course, most dialogue systems include domain depen-dent acts to cope with the particular equirements of the do-main, cf.Alexandersson (1996).
Speech acts are also relatedto the task: information providing, appointment negotiat-631However, speech acts concern a rather abstractlevel of utterance modelling: they represent thespeakers' intentions, but ignore the semantic on-tent of the utterance.
Consequently, context modelswhich use only speech act information tend to beless specific and hence less accurate.
Nagata andMorimoto (1994) report prediction accuracy of 61.7%, 77.5 % and 85.1% for the first, second and thirdbest dialogue act (in their terminology: Illocution-ary Force Type) prediction, respectively, while Rei-thinger and Maier (1995) report the correspondingaccuracy rates as 40.28 %, 59.62 % and 71.93 %,respectively.
The latter used structurally varied i-alogues in their tests and noted that deviations fromthe defined ialogue structures made the recognitionaccuracy drop drastically.To overcome prediction i accuracies, speech actbased context models are accompanied with the in-formation about he task or the actual words used.Reithinger and Maier (1995) describe plan-based re-pairs, while MSller (1996) argues in favour of domainknowledge.
Qu et al (1996) show that to minimizecumulative contextual errors, the best method, with71.3% accuracy, is the Jumping Context approachwhich relies on syntactic and semantic nformationof the input utterance rather than strict prediction ofdialogue act sequences.
Recently also keyword-basedtopic identification has been applied to dialoguemove (dialogue act) recognition (Garner, 1997).Our goal is to build a context model for a spo-ken dialogue system, and we emphasise especiallythe system's robustness, i.e.
its capability to pro-duce reliable and meaningful responses in presenceof various errors, disfluencies, unexpected input andout-of-domain utterances, tc.
(which are especiallynotorious when dealing with spontaneous speech).The model is used to improve word recognition ac-curacy, and it should also provide auseful basis forother system odules.However, we do not aim at robustness ona merelymechanical level of matching correct words, butrather, on the level of maintaining the informationcontent of the utterances.
Despite the vaguenessof such a term, we believe that speech act basedcontext models are less robust due to the fact thatthe information content of the utterances is ignored.Consistency ofthe information exchanged in (task-oriented) conversations is one of the main sources fordialogue coherence, and so pertinent inthe contextmanagement besides speech acts.
Deviations from apredefined dialogue structure, multifunctionality ofutterances, various ide-sequences, di fluencies, etc.cannot be dealt with on a purely abstract level ofillocution, but require knowledge of the domain, ex-pressed in the semantic ontent of the utterances.ion, argumentation etc.
have different communicative pur-poses which are reflected in the set of necessary speech acts.Moreover, in multilingual applications, like speech-to-speech translation systems, the semantic ontentof utterances plays an important role and an inte-grated system must also produce asemantic analysisof the input utterance.
Although the goal may be ashallow understanding only, it is not enough that thesystem knows that the speaker uttered a "request":the type of the request is also crucial.We thus reckon that appropriate context manage-ment should provide descriptions of what is said,and that the recognition of the utterance topic is animportant task of spoken dialogue systems.3 The Topic ModelIn AI-based dialogue modelling, topics are associ-ated with a particular discourse ntity, focus, whichis currently in the centre of attention and whichthe participants want to focus their actions on, e.g.Grosz and Sidner (1986).
The topic (focus) is ameans to describe thematically coherent discoursestructure, and its use has been mainly supported byarguments regarding anaphora resolution and pro-cessing effort (search space limits).
Our goal is touse topic information i  predicting likely content ofthe next utterance, and thus we are more interestedin the topic types that describe the information con-veyed by utterances than the actual topic entity.Consequently, instead of tracing salient entities inthe dialogue and providing heuristics for differentshifts of attention, we seek a formalisation of theinformation structure of utterances in terms of thenew information that is exchanged in the course ofthe dialogue.The purpose of our topic model is to assist speechprocessing, and so extensive and elaborated reason-ing about plans and world knowledge is not avail-able.
Instead a model that relies on observed facts(= word tokens) and uses statistical information ispreferred.
We also expect he topic model to be gen-eral and extendable, so that if it is to be applied toa different domain, or more factors in the recogni-tion of the information structure of the utterances 3are to be taken into account, the model could easilyadapt to these changes.The topic model consists of the following parts:1. domain knowledge structured into a topic tree2.
prior probabilities of different topic shifts3.
topic vectors describing the mutual informationbetween words and topic types4.
Predict-Support algorithm to measure similar-ity between the predicted topics and the topicssupported by the input utterance.Below we describe ach item in detail.3For instance, sentential stress and pitch accent are im-portant in recognizing topics in spontaneous speech.632Figure 1: A partial topic tree.3.1 Topic  t reesOriginally "focus trees" were proposed by (McCoyand Cheng, 1991) to trace foci in NL generation sys-tems.
The branches of the tree describe what sortof shifts are cognitively easy to process and can beexpected to occur in dialogues: random jumps fromone branch to another are not very likely to occur,and if they do, they should be appropriately marked.The focus tree is a subgraph of the world knowledge,built in the course of the discourse on the basis ofthe utterances that have occurred.
The tree bothconstrains and enables prediction of what is likelyto be talked about next, and provides a top-downapproach to dialogue coherence.Our topic tree is an organisation of the domainknowledge in terms of topic types, bearing resem-blance to the topic tree of Carcagno and Iordanskaja(1993).
The nodes of the tree 4 correspond to topictypes which represent clusters of the words expectedto occur at a particular point of the dialogue.
Fig-ure 1 shows a partial topic tree in a hotel reservationdomain.For our experiments, topic trees were hand-codedfrom our dialogue corpus.
Since this is time-consuming and subjective, an automatic lusteringprogram, using the notion of a topic-binder, is cur-rently under development.Our corpus contains 80 dialogues from the bilin-gual ATR Spoken Language Dialogue Database.4We will continue talking about a topic tree, although instatistical modelling, the tree becomes a topic network wherethe shift probability between odes which are not daughtersor sisters of each other is close to zero.The dialogues deal with hotel reservation and touristinformation, and the total number of utterances i4228.
(Segmentation is based on the informationstructure so that one utterance contains only onepiece of new information.)
The number of differentword tokens is 27058, giving an average utterancelength 6,4 words.The corpus is tagged with speech acts, using asurface pattern oriented speech act classification ofSeligman et al (1994), and with topic types.
Thetopics are assigned to utterances on the basis of thenew information carried by the utterance.
New in-formation (Clark and Haviland, 1977; Vallduvl andEngdahl, 1996) is the locus of information related tothe sentential nuclear stress, and identified in regardto the previous context as the piece of informationwith which the context is updated after uttering theutterance.
Often new information includes the verband the following noun phrase.More than one third of the utterances (1747) con-tain short fixed phrases (Let me confirm; thank you;good.bye; ok; yes), and temporizers (well, ah, uhm).These utterances do not request or provide informa-tion about the domain, but control the dialogue interms of time management requests or convention-alised dialogue acts (feedback-acknowledgements,thanks, greetings, closings, etc.)
The special topictype IAM, is assigned to these utterances to signifytheir role in InterAction Management.
The topictype MIX is reserved for utterances which contain in-formation ot directly related to the domain (safetyof the downtown area, business taking longer thanexpected, a friend coming for a visit etc.
), thus mark-ing out-of-domain utterances.
Typically these utter-ances give the reason for the request.The number of topic types in the corpus is 62.Given the small size of the corpus, this was consid-ered too big to be used successfully in statistical cal-culations, and they were pruned on the basis of thetopic tree: only the topmost nodes were taken intoaccount and the subtopics merged into approproatemother topics.
Figure 2 lists the pruned topic typesand their frequencies in the corpus.tag count ~ interpretationiam 1747 41.3 Interaction Managementroom 826 19.5 Room, its propertiesstay 332 7.9 Staying periodname 320 7.6 Name, spellingres 310 7.3 Make/change/extend/cancel reservationpaym 250 5.9 Payment methodcontact 237 5.6 Contact Infomeals 135 3.2 Meals (breakfast, dinner)mix 71 1.7 Single unique topicsFigure 2: Topic tags for the experiment.6333.2 Topic shiftsOn the basis of the tagged dialogue corpus, proba-bilities of different opic shifts were estimated.
Weused the Carnegie Mellon Statistical Language Mod-eling (CMU SLM) Toolkit, (Clarkson and Rosen-feld, 1997) to calculate probabilities.
This builds atrigram backoff model where the conditional proba-blilities are calculated as follows:p(w3\[wl, w2) =p3(wl, w2, w3)bo_wt2(wl, w2) x p(w31w2)p(w3lw2)if trigram existsif bigram (wl,w2)existsotherwise.p(w21wl) =p2(wl, w2) if bigram existsbo_wtl(wl) ?
pl(w2) otherwise.3.3 Topic vectorsEach word type may support several topics.
For in-stance, the occurrence of the word room in the utter-ance I'd like to make a room reservation, supportsthe topic MAKERESERVATION, but in the utteranceWe have only twin rooms available on the 15th.
itsupports the topic ROOM.
To estimate how well thewords support he different topic types, we measuredmutual information between each word and the topictypes.
Mutual information describes how much in-formation a word w gives about a topic type t, andis calculated as follows (ln is log base two, p(tlw )the conditional probability of t given w, and p(t)the probability of t ) :I(w,t) = In p(w,t) - -  In p(t\[w)p(w) .
p(t) p(t)If a word and a topic are negatively correlated,mutual information is negative: the word signalsabsence of the topic rather than supports its pres-ence.
Compared with a simple counting whether theword occurs with a topic or not, mutual informationthus gives a sophisticated and intuitively appealingmethod for describing the interdependence betweenwords and the different topic types.Each word is associated with a topic vector, whichdescribes how much information the word w carriesabout each possible topic type ti:topvector( mi( w, t l ), mi( w, t 2 ), ..., mi( w, t ,  ) )For instance, the topic vector of the word room is:topvector  (room, \[mi (0.
21409750769169117, cont  act  ) ,mi (-5.
5258041314543815, iam),mi (-3.
831955835588453 ,meals ) ,mi (0 ,mix),mi ( ml * 2697134113673~ ~ na ive  )mi (-2.
720924523199709, paym) ,mi (0.
9687353561881407 , res ) ,mi ( I .
9035899442740105, room),mi (-4.130179669884547, stay)  \] ).The word supports the topics ROOM and MAKE-RESERVATION (res), but gives no information aboutMIX (out-of-domain) topics, and its presence ishighly indicative that the utterance is not at leastIAM or STAY.
It also supports CONTACT becausethe corpus contains utterances like I 'm in room 213which give information about how to contact hecustomer who is staying at a hotel.The topic vectors are formed from the corpus.
%Veassume that the words are independently related tothe topic types, although in the case of natural lan-guage utterances this may be too strong a constraint.3.4 The Predict -Support  Algor i thmTopics are assigned to utterances given the previoustopic sequence (what has been talked about) andthe words that carry new information (what is actu-ally said).
The Predict-Support Algorithm goes asfollows:1.
Prediction: get the set of likely next topics inregard to the previous topic sequences using thetopic shift model.2.
Support: link each Newlnfo word wj of the in-put to the possible topics types by retrievingits topic vector.
For each topic type ti, add upthe amounts of mutual information rni(wj;ti)by which it is supported by the words wj, andrank the topic types in the descending order ofmutual information.3.
Selection:(a) Default: From the set of predicted topics,select he most supported topic as the cur-rent topic.
(b) What-is-said heuristics: If the predictedtopics do not include the supported topic,rely on what is said, and select the mostsupported topic as the current topic (cf.the Jumping Context approach in Qu etal.
(1996)).
(c) What-is-talked-about heuristics: If thewords do not support any topic (e.g.
all thewords are unknown or out-of-domain), relyon what is predicted and select the mostlikely topic as the current opic.3 shows schematically how the algorithm Figureworks.634U 1 - w11,  w12,  ..., W lm - - -> T 1U 2 - w21"  w22,  ..., W2m ---> T 2U 3 - w31,  w32,  ..., W3m - - -> T 3UnPrediction:T n - max p(Tk  I Tk_2Tk .
1)Tkm Wnl  , wn2 ....... wnm - -> T nmi(W*nl  .Ta) .
mi( 'Wrt2,T a ) .
.
.
.
i (Wnm.T  a )mi (Wnl  ,T b )  miff,*V n2 ,T  b)  ?
.
.
m i0&'nm,T  b)rn i (Wnl  ,T k) mi(Wn2,T k) .
.
.
mi(Wnm,Tk)m Support:mi(Un,Tk  ) = ~ mi0 /Vn i ,Tk  ) T n - max  mi (Un ,T  k)i= l  T kSelect:.Default: T n =max ml(Un,T k) and Tn= max p(TkITk.2Tk_l)}T k T kWhnt/s s~/d: T n -max mi(Un,T k)TkWhat is tnl'~d about: Tn = max p(T k I Tk.2Tk.
1 )TkFigure 3: Scheme of the Predict-Support Algorithm.Using the probabilities obtained by the trigrambackoff model, the set of likely topics is actually aset of all topic types ordered according to their like-lihood.
However, the original idea of the topic treesis to constrain topic shifts (transitions from a nodeto its daughters or sisters are favoured, while shiftsto nodes in separate branches are less likely to oc-cur unless the information under the current nodeis exhaustively discussed), and to maintain this re-strictive property, we take into consideration onlytopics which have probability greater than an arbi-trary limit p.Instead of having only one utterance analysedat the time and predicting its topic, a speech rec-ognizer produces a word lattice, and the topic isto be selected among candidates for several wordstrings.
We envisage the Predict-Support algorithmwill work in the described way in these cases as well.However, an extra step must be added in the se-lection process: once the topics are decided for then-best word strings in the lattice, the current topicis selected among the topic candidates as the high-est supported topic.
Consequently, the word stringassociated with the selected topic is then picked upas the current utterance.We must make two caveats for the performanceof the algorithm, related to the sparse data prob-lem in calculating mutual information.
First, thereis no difference between out-of-domain words andunknown but in-domain words: both are treated asproviding no information about the topic types.
Ifsuch words are rare, the algorithm works fine sincethe other words in the utterance usually supportthe correct opic.
However, if such words occur ?re-quently, there is a difference in regard to whetherthe unknown words belong to the domain or not.Repeated out-of-domain words may signal a shift toa new topic: the speaker has simply jumped intoa different domain.
Since the out-of-domain wordsdo not contribute to any expected topic type, thetopic shift is not detected.
On the other hand, ifunknown but in-domain words are repeated, mu-tual information by which the topic types are sup-ported is too coarse and fails to make necessary dis-tinctions; hence, incorrect opics can be assigned.For instance, if lunch is an unknown word, the ut-terance Is lunch included?
may get an incorrecttopic type ROOMPRICE since this is supported bythe other words of the utterance whose topic vec-tors were build on the basis of the training corpusexamples like Is tax included?The other caveat is opposite to unknown words.If a word occurs in the corpus but only with a par-ticular topic type, mutual information between theword and the topic becomes high, while it is zerowith the other topics.
This co-occurrence may justbe an accidental fact due to a small training cor-pus, and the word can indeed occur with other topictypes too.
In these cases it is possible that the algo-rithm may go wrong: if none of the predicted topicsof the utterance is supported by the words, we relyon the What-is-said heuristics and assign the highlysupported but incorrect opic to the utterance.
Forinstance, if included has occurred only with ROOM-PRICE, the utterance Is lunch included?
may stillget an incorrect opic, even though lunch is a knownword: mutual information mi(included, RoomPrice)may be greater than mi(lunch, Meals).4 Exper imentsWe tested the Predict-Support algorithm usingcross-validation  our corpus.
The accuracy resultsof the first predictions are given in Table 4.
PP isthe corpus perplexity which represents the averagebranching factor of the corpus, or the number of al-ternatives from which to choose the correct label ata given point.For the pruned topic types, we reserved 10 ran-domly picked dialogues for testing (each test file con-tained about 400-500 test utterances), and used theother 70 dialogues for training in each test cycle.The average accuracy rate, 78.68 % is a satisfactoryresult.
We also did another set of cross-validationtests using 75 dialogues for training and 5 dialoguesfor testing, and as expected, a bigger training cor-pus gives better recognition results when perplexitystays the same.To compare how much difference a bigger num-ber of topic tags makes to the results, we con-ducted cross-validation tests with the original 62topic types.
A finer set of topic tags does worsen635Test type PPTopics = 10train = 70 files 3.82Topics = 10train = 75 files 3.74Topics = 62train = 70 files 5.59Dacts = 32train = 70 files 6.22PS-aigorithm BO model78.68 41.3080.55 40.3364.96 41.3258.52 19.80Figure 4: Accuracy results of the first predictions.the accuracy, but not as much as we expected: theSupport-part of the algorithm effectively remediesprediction inaccuracies.Since the same corpus is also tagged with speechacts, we conducted similar cross-validation testswith speech act labels.
The recognition rates areworse than those of the 62 topic types, althoughperplexity is almost the same.
We believe that thisis because speech acts ignore the actual content ofthe utterance.
Although our speech act labels aresurface-oriented, they correlate with only a few fixedphrases (I would like to; please), and are thus lesssuitable to convey the semantic focus of the utter-ances, expressed by the content words than topics,which by definition deal with the content.As the lower-bound experiments we conductedcross-validation tests using the trigram backoff-model, i.e.
relying only on the context which recordsthe history of topic types.
For the first ranked pre-dictions the accuracy rate is about 40%, which is onthe same level as the first ranked speech act predic-tions reported in Reithinger and Mater (1995).The average precision of the Predict-Support al-gorithm is also calculated (Table 5).
Precision is theratio of correctly assigned tags to the total numberof assigned tags.
The average precision for all thepruned topic types is 74.64%, varying from 95.63%for ROOM to 37.63% for MIx.
If MIx is left out,the average precision is 79.27%.
The poor precisionfor MIX is due to the unknown word problem withmutual information.Topic type Precision Topic type Precisioncontact 55.75 paym 83.25iammealsname79.13 res 62.1382.13 room 95.6388.12 stay 88.00mix 37.63 Average 74.64Figure 5: Precision results for different opic types.The results of the topic recognition show that themodel performs well, and we notice a considerableimprovement in the accuracy rates compared to ac-curacy rates in speech act recognition cited in section2 (modulo perplexity).
Although the rates are some-what optimistic as we used transcribed ialogues (=the correct recognizer output), we can still safelyconclude that topic information provides a promis-ing starting point in attempts to provide an accuratecontext for the spoken dialogue systems.
This canbe further verified in the perplexity measures for theword recognition: compared to a general anguagemodel trained on non-tagged ialogues, perplexitydecreases by 20 % for a language model which istrained on topic-dependent dialogues, and by 14 %if we use an open test with unknown words includedas well (Jokinen and Morimoto, 1997).At the end we have to make a remark concerningthe relevance of speech acts: our argumentation isnot meant to underestimate heir use for other pur-poses in dialogue modelling, but rather, to empha-sise the role of topic information in successful con-text management: in our opinion the topics providea more reliable and straighforward approximation ofthe utterance meaning than speech acts, and shouldnot be ignored in the definition of context modelsfor spoken dialogue systems.5 Conc lus ionsThe paper has presented a probabilistic topic modelto be used as a context model for spoken dialoguesystems.
The model combines both top-down andbottom-up approaches to topic modelling: the topictree, which structures domain knowledge, providesexpectations of likely topic shifts, whereas the infor-mation structure of the utterances i linked to thetopic types via topic vectors which describe mutualinformation between the words and topic types.
ThePredict-Support Algorithm assigns topics to utter-ances, and achieves an accuracy rate of 78.68 %, anda precision rate of 74.64%.The paper also suggests that the context needed tomaintain robustness of spoken dialogue systems canbe defined in terms of topic types rather than speechacts.
Our model uses actually occurring words andtopic information of the domain, and gives highlycompetitive results for the first ranked topic predic-tion: there is no need to resort to extra informationto disambiguate the three best candidates.
Con-struction of the context, necessary to improve wordrecognition and for further processing, becomes thusmore accurate and reliable.Research on statistical topic modelling and com-bining topic information with spoken language sys-tems is still new and contains everal aspects for fu-ture research.
We have mentioned automatic do-main modelling, in which clustering methods canbe used to build necessary topic trees.
Another re-search issue is the coverage of topic trees.
Topictrees can be generalised in regard to world knowl-edge, but this requires deep analysis of the utterancemeaning, and an inference mechanism to reason onconceptual relations.
We will explore possibilities to636extract semantic ategories from the parse tree andintegrate these with the topic knowledge.
We willalso investigate further the relation between topicsand speech acts, and specify their respective roles incontext management for spoken dialogue systems.Finally, statistical modelling is prone to sparse dataproblems, and we need to consider ways to overcomeinaccuracies in calculating mutual information.Re ferencesJ.
Alexandersson.
1996.
Some ideas for the auto-matic acquisition of dialogue structure.
In Dia-logue Management in Natural Language Process-ing Systems, pages 149-158.
Proceedings of the1 lth Twente Workshop on Language Technology,Twente.D.
Carcagno and Lidija Iordanskaja.
1993.
Contentdetermination and text structuring: two interre-lated processes.
In H. Horacek and M. Zock, edi-tors, New Concepts in Natural Language Genera-lion, pages 10-26.
Pinter Publishers, London.K.
W. Church and W. A. Gale.
1991.
Probabil-ity scoring for spelling correction.
Statistics andComputing, (1):93-103.H.
H. Clark and S. E. Haviland.
1977.
Comprehen-sion and the given-new contract.
In R. O. Freedle,editor, Discourse Production and Comprehension,Vol.
1.
Ablex.P.
Clarkson and R. Rosenfeld.
1997.
Statisticallanguage modeling using the CMU-Cambridgetoolkit.
In Eurospeech-97, pages 2707-2710.P.
Garner.
1997.
On topic identification and di-alogue move recognition.
Computer Speech andLanguage, 11:275-306.B.
J. Grosz and C. L. Sidner.
1986.
Attention, in-tentions, and the structure of discourse."
Compu-tational Linguistics, 12(3):175-204.K.
Jokinen and T. Morimoto.
1997.
Topic informa-tion and spoken dialogue systems.
In NLPRS-97,pages 429-434.
Proceedings of the Natural Lan-guage Processing Pacific Rim Symposium 1997,Phuket, Thailand.K.
McCoy and J. Cheng.
1991.
Focus of attention:Constraining what can be said next.
In C. L.Paris, W. R. Swartout, and W. C. Moore, ed-itors, Natural Language Generation in ArtificialIntelligence and Computational Linguistics, pages103-124.
Kluwer Academic Publishers, Norwell,Massachusetts.J-U.
MSller.
1996.
Using DIA-MOLE for unsuper-vised learning of domain specific dialogue actsfrom spontaneous language.
Technical ReportFBI-HH-B-191/96, University of Hamburg.M.
Nagata and T. Morimoto.
1994.
An information-theoretic model of discourse for next utterancetype prediction.
In Transactions of InformationProcessing Society of Japan, volume 35:6, pages1050-1061.Y.
Qu, B.
Di Eugenio, A. Lavie, L. Levin, and C. P.Ros~.
1996.
Minimizing cumulative rror in dis-course context.
In Dialogue Processing in SpokenDialogue Systems, pages 60-64.
Proceedings of theECAI'96 Workshop, Budapest, Hungary.N.
Reithinger and E. Maier.
1995.
Utilizing statisti-cal dialogue act processing in verbmobil.
In Pro-ceedings of the 33rd Annual Meeting of the ACL,pages 116-121.M.
Seligman, L. Fais, and M. Tomokiyo.
1994.A bilingual set of communicative act labels forspontaneous dialogues.
Technical Report ATRTechnical Report TR-IT-81, ATR InterpretingTelecommunications Research Laboratories, Ky-oto, Japan.E.
Vallduvi and E. Engdahl.
1996.
The linguisticrealization of information packaging.
Linguistics,34:459-519.637
