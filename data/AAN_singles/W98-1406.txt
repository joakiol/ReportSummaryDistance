De-Constraining Text GenerationStephen Beale, Sergei Nirenburg , Evelyne Viegast and Leo Wanner?tComputing Research LaboratoryBox 30001, Dept.
3CRLNew Mexico Sate UniversityLas Cruces, NM 88003-0001 USAsb,sergei,viegas@crl.nmsu.edu?Computer Science Department, ISUniversity of StuttgartBreitwiesenstr.
20-22D-70565 Stuttgart, .Germanywannerlo~informatik.uni-stuttgart.deAbstractWe argue that the current, predominantly task-oriented, approaz~hes to modularizing text?
generation, while plausible and useful conceptually, set up spurious conceptual nd operationalconstraints.
We propose a data-driven approach to modularization a d illustrate how it elimi-nates ?
?the previously ubiquitous constraints on combination ofevidence across modules and on?
control.
We also briefly overview the constraint-based control architecture that enables such anapproach and facilitates near linear-time processing with realistic texts.-1 IntroductionThis paper addresses the area of text generation known as microplanning \[Levelt1989, Panaget1994,Huang and Fiedler1996\], or sentence planning \[Rambow and Korelsky1992\]; [Wanner and Hovy1996\].Microplanning involves low-level discourse structuring and marking, sentence boundary planning,clause?internal structuring and all of the varied subtasks involved in lexical choice, These complextasks are often modularized and treated separately.
The general argument is that since sentenceplanning tasks are not single-step operations, since they do not have to be performed in strict se-quence, and since the planner's operation is non-deterministic, each sentence planning task shouldbe implemented by a separate module or by several modules (see, e.g., \[Wanner and Hovy1996\]).Such an argument is natural if generation is viewed as a set of coarse-grained tasks.
Indeed, with theexception of a few researchers (\[Elhadad et a1.1997\] and the incrementalists li ted below), the task-oriented view is standard in the generation community.
Unfortunately, task-oriented generationsets up barriers among the components of the generation process, primarily because, in a realisticscenario, the tasks are intertwined to a high degree.
Overcoming these barriers has become a centraltopic in generation research (see below).
In  our approach the basis of modularization is sought inthe nature of the input data to the generation process, in our case, a text meaning representation,formulated largely in terms of an ontology.
This data-oriented approach is similar to that taken bymany incremental generators \[De Smedt1990, Reithinger1992\], although these tend to concentrateon syntactic processing.
But see \[Kilger1997\], who explicitly addresses microplanning.
We feel thatour work provides an optimal path between task-oriented generators (which face problems due tothe interrelationships between the tasks) and traditional incremental generation (which does nottake advantage of problem decomposition as discussed below).In what follows we describe our ontology-based modularization, the kind of constraints which canbe automatically set up within such a paradigm, and the control mechanism we employ to process it.We focus on the task of lexicalization, but other microplanning tasks have been handled similarly.We conclude with a discussion of the avoidable barriers inherent in most current approaches, along?
48S lock -40  + ."
( ~  " - -  core ference  w l th  prev ious  text  a ?
+"Lml~rdere  ind lc~ted  t i i I I  l i l le  IPlew ?on lp l l r l y ,  wh ich  w lU  cont inue  to  be  I l s t~c l  on  IJhe i l lm~k market ,  w i l l  be  d iv ided  In to  n l r l~  d lv i s iomulwhich  cor respond to  the  n ine  work ing  d iv i s ions  o f  Ms l rm ~.d  i t lachet~:"Figure 1: Input Semantic Representation to the Mikrokosmos Generatorwith their attempts at circumventing them, and how our approach eliminates many of the problems.We also point out differences between our approach and that of the incremental generators.2 Ontology-Based ModularizationIn contrast o modularization by tasks such as discourse structuring, clause structuring and lex-ical choice, the Mikrokosmos project (http://crl.nmsu.edu/Research/Projects/mikro/index.html)attempts to modularize on the ontological and linguistic data that serves as inputs to the textgeneration process, that is, based on the types of inputs we expect, not on the types of processingwe need to perform.
A typical semantic representation that serves as the input to the generationprocess is shown in Figure 1.
This semantic input was produced by the Mikrokosmos analyzer froman  input Spanish text.The generation lexicon in our approach is essentially the same as the analysis lexicon, but witha different indexing scheme: on ontological concepts instead of NL lexical units 1.
(\[Stede1996\] isan example of another generator with a comparable l xicon structure, although our work is richer,including collocational constraints, for example).
The generation lexicon contains information (suchas, for instance, semantics-to-syntax dependency mappings) that drives the generation process,with the help of several dedicated microtheories that deal with issues such as focus and reference(values of which are among the elements of our input representations).
The Mikrokosmos Spanishcore lexicon is complete with 7000 word senses defined; the English core lexicon is still underdevelopment with a projected size of over 10,000 word senses.
Both of these core lexicons can beexpanded with lexical rules to contain around 30,000 entries (\[Viegas et al1996\]).Lexicon entries in both analysis and generation can be thought of as "objects" or "modules"corresponding to each unit in the input.
Such a module has the task of realizing the associateduni t ,  while communicating with other objects around it, if necessary (similar to \[De Smedt1990\]).1In semantic analysis, the input is a set of words, thus the lexicon is indexed on Words.
In generation, the inputis concepts, So it is indexed on concepts.49iS T O C K - M A R K E T##o~ant i?s  \] f ~ .~ t l  .
STOCK-MARKET s~ n ~s  ~l~'.
'ctcd S T O C K - M A I F t  K E T  - #x.oG?
'~d ~ \ [ .
,~ 'A I~\ [O I  ~"L\[ .=  .
.
.
.
.
\]Figure ?2: OB JECT Lexicon Entries - A Simplified ViewEach module can be involved in carrying out several of the ?tasks like those listed by Wannerand Hovy.
For ?
instance, modules for specific ?events or properties are used in setting up clause andsentence structures as well as lexical choice, as will be shown below.
Interactions and constraints ?flow freely, with the control mechanism dynamically tracking the connections 2.
One outcome ofthis division of  labor between declarative data and the control architecture is that the bulk ofknowledge processing resides in the lexicon, indexed for both analysis and generation.
This hasgreatly simplified knowledge acquisition in general \[Nirenburg et a1.1996\] and made it easier to?
adapt analysis knowledge sources to generation as well as to convert knowledge sources acquiredfor one language to use with texts in another.Below we sketch out how this organization works.
We begin by describing the ?
main types oflexicon entries with the goal of demonstrating how each performs various generation ?tasks.
Wethen take a look at the different types of constraints associated with each kind of entry.IIIIIIII2.1 Types  o f  Lex icon  Ent r iesThe main types of lexicon entries correspond to the ontological categories of OB JECTS,  EVENTSand PROPERTIES  (for simplicity, we will avoid discussion of synonyms and stylistic variations):?
Objects .
In English, Objects are typically realized by nouns, although the actual mapping might berather complex \[Beale and Viegas1996\].
In general, object generation lexicon entries can have one-to-one mappings between concept and lexical unit, or can contain additional semantic restrictions, bothof which are illustrated in Figure 2.
The use of collocational information is described below.
??
, Events.
?
Events, as shown pictorially in Figure 3, can be realized as verbs ("divided") or nouns("division") in English.?
Furthermore, the lexicon entries for events typically determine the structure?
of the nascent clause by mapping the expected case roles into elements of the verb subcategorizationframe (or other structures).
Rules for planning passives and relative clauses, for instance, are alsoavailable.
These rules can be used to fulfill different grammatical nd clause combination requirementsas described below.
Conceptually, all the entries produced by these rules can be thought of as beingresident in the lexicon.
Practically speaking, many of them can be produced on the fly automatically,reducing the strain on knowledge acquisition.?
Propert ies .
Properties 3 are perhaps the most interesting of the input types discussed here because?
??
they are so flexible.
They can be realized as adjectives , relative clauses, complex noun phrases and?
complete sentences.
Often a property is included in the definition of another object or event, such asin Figure 2, where the LOCATION is included in the object entry.
CASE-ROLE-RELATIONS2Alth0ugh our constraint-based planner supports truth maintenance operations, in the "fuzzy" domain of naturallanguage semantics it is often more appropriate to speak of "preferences"SRELATION and ATTRIBUTE are the subtypes of PROPERTY.50DIV IDE\[ $emnglc$expectedp~ss Ivonomlna l i za t ionre l~t lveB.ub j  VAt -1?
d lv ldo l~ VAR2 IBM d iv ided App le  i n topp4c~u~?
sg~ conapanh~sroot  ? '
\ [~m o"?b J~t  VAR3#u~ VAR2dW'l lde ( ix~s | )pp4,d , I  u"tr~m'" ,Applm w,w.$ d lvk led Lnto ~ix?k i l t  VAtZa  comi~s i~ bJt IBM.PP-~eUam~t  (opo ,  omm~ .soot  "by"?
?b J~t  VA IL Id iwLcdon o f ,App le  In to  ...IBM,  who d lvdded App le  gngo ... 1Jsg~ compan ies ,  which  IBM d iv ided  --.
\]JMkppl?.
whkh  was d iv ided  In to  ... \[ JFigure 3: EVENT Lexicon Entries ?A Simplified ViewORGANIZAT ION- INVOLVED- INi gemaant~s ~ ~ng~t i~\] VARI expected generated root "him" The co ?pony  has stoc~org~l~Vedo~ obj VAI~~?
VAR2 Tke company's ~ck.
.
.
.
p. .
.v ,  .h~,h  has ,~ .k  \]the coml~tny , whose ?took I?
Figure 4: PROPERTY Lexicon Entries - A Simplified Viewtypically are consumed by the event entry, except in the case of some nominalizations.
DISCOUI~SE-?
RELATIONS contribute to setting up sentence boundaries, sentence ordering and pronominalization.Figure 4 is an example of RELATION.?
2.2 Const ra in ts  ?
in  Sentence  P lann ingThe above generation lexicon entries are the primary knowledge sources used in the generationprocess 4.
Five different types of constraints are automatically generated which constrain the combi:nations of entries allowed to globally realize a semantic input.
The Mikrokosmos control mechanismefficiently processes constraints to produce optimal global answers.B inding Constraints.
One of the primary advantages of input-based modularization is that theindividual knowledge sources (lexicon entries) can be grounded in the input they expect to bematched against.
For instance, in Figure 3, the semantic?
input expected shows three variables,corresponding?to the three case roles normally associated with a DIVIDE event.
The process o flinking these variables to the actual semantic structures for a particular input is known as binding.4Due to space limitations, we are glossing over important generation microtheories such as sentence boundarydetermination and corderence implementation.51A S S E R T I V E  A C Troot  SAY~ m proo?  "
r l~A ' rob Jwoog VAIRL2  (~ l~u~)CONSTRAINT  PRODUCED:V A I ~ 2  : c lause  ~ r b ind ingD IV IDE-31  : c lauseFigure 5: Grammatical ConstraintsSTOCK-MARKET \]I ?
I/I?
?
P R E F E RLOCATIONI i.-pp ( ,o~,o  I~- on-pp Our.face) \]I at-pe a,l,~e~ IFigure 6: Collocational Constraints in Lexicon Entries ?For the input shown in Figure 1, VAR2 will be bound to CONGLOMERATE-32  and VAR3will be bound to CORPORATION-34.
??
Notice that, for this example, no AGENT exists for the DIVIDE-31 event, so that VAR1will be left unbound.
Binding constraints will simply eliminate any syntactic hoices that containnon-optional unbound variables.
In this case, it will rule out the first syntactic realization forDIVIDE shown in Figure 3.The grounding of the input afforded by the binding process also allows us to simplify the othertypes of constraints described below.
Each of these types of constraints ~ automatically processedin our system, in task-based systems typically require complex rules to be acquired manually.Grammat ica l  Const ra ints .
An example of a grammatical constraint is shown in Figure 5.
Alexicon entry can specify grammatical constraints on the realization of any of the variables in it.One ?possible syntactic realization for ASSERTIVE-ACT is shown.
It requires its VAR2 to berealized as a clause.
This particular entry allows the system to produce "John said that Bill wentto  the store" but  not "John said that Bill."
A comparison with Figure 1 shows that the bindingprocess will link VAR2 of the ASSERT IVE-ACT entry to DIVIDE-31.
In effect, the?resultingconstraint will eliminate any realization for DIVIDE-31 (in Figure 3) that does not produce a fullclause at the top-level, through nominalization and relativization.
It should be stressed that thisfiltering occurs only in conjunction with the given ?realization of ASSERTIVE-ACT;  there maybe other realizations that would go fine with, for example, a nominalized realization of DIVIDE.CoUocat ional  Const ra ints .
F igure 6 illustrates the familiar notion of collocational constraints.Again, the fact that the  lexicon entry is  grounded in the input allows a simple representationof collocations.
In this case, the different realizations of LOCATION usually correspond to thesemantic type Of the object.
Collocations :can be used to override the default.
The co-occurrencezone of the STOCK-MARKET entry simply states that if it is used as the range of a LOCATION52IIIIiiiiIi* |  ?
I !.
~ case  ro /eS i tuat ion  1 :  L ink  th roughd i scourse  re la t ionS i tuat ion  2:  L ink  th rough?
case  ro le  re la t ionIS i tuat ion  3 :  Imp l i c i t  l i nk ,Figure 7: Inputs that May Lead to Clause Combinationsrelation, then the LOCATION relation should be introduced with "on."
This produces an Englishcollocation such as "the stock is sold on  the stock market" as opposed to the less natural "... sold a tthe stock market."
Notice that no additional work on collocations needs to be performed beyondthe declarative knowledge ncoding.
The constraint-based control architecture will identify andassign preferences to collocations.C lause  Combinat ion  Const ra in ts .
Various kinds of constraints arise when clauses are com-bined :to form complex sentences.
The strategies for clause combination come from three sources: ??
Directly from a lexicon entry associated with an input.
For example, a discourse relation such asCONCESSION might directly set up the syntax to produce a sentence structure such as "AlthoughJim admired her reasoning, he rejected her thesis."?
Verbs which take complement clauses as arguments also set up complex sentence structures and imposegrammatical constraints (if present) on the individual clause realizations: "John said that he went tothe store" or "John likes to play baseball."?
Indirectly, from a language-specific source of clause combination techniques (such as relative clauseformation or coordination i English).These three sources correspond to the three input situations depicted in Figure 7.
The first twohave explicit relations linking two EVENTs .
The first (the non-case-role relation) will have a?
corresponding lexicon entry which directly sets up the sentence structure, along with specific on-straints on the individual clauses.
The second possibility typically occurs with EVENTs  thattake complement clauses as  case-role arguments.
The lexicon entries for these usually will specifythe  complex clause structure needed.
The third situation has no explicit connection in the input;therefore, some sort of language-specific combination strategy must be used to fill the same task.Even though the latter case appears to be a situation that requires a task-oriented procedure,in reality it is as easy to use general purpose structure constraints along with a declarative repre-sentation of possible transformations available.
Assuming, for the sake of illustration, that due tosome external reason a single sentence realization o f  two clauses is preferred 5, a general purposestructural constraint prevents two clauses from embedding a single referent into distinct syntacticstructures.
For instance, 1 and 2 below are grammatical, but 3 is not, because both the clauses tryto use "conglomerate" as their subject.1.
The conglomerate, whose stock is sold on the stock market, was divided into nine corporations.SConstraints which might produce such a preference can come from a variety of Sources; a common one is therealizations of discourse relations.532.
The conglomerate , which was divided into nine corporations, i  sold on the stock market.3.
*The conglomerate was divided into nine corporations i  sold on the stock market.The general purpose constraint will automatically prevent such a realization and trigger the con-sideration of subordinate clause transformations.In addition, the examples of clause combination given above and in Figure 7 all contain e~amplesof coreference across clause boundaries.
Although coreference r alization has its own microtheorythat is triggered by instances of coreference in the ?text, clause combination techniques may interactwith it.
For instance, the lexicon entry for a RELATION might specify that a pronoun be usedin the second clause.The important hing to note for this presentation is that these types of constraint are eitherdirectly found in the lexicon or are produced automatically by the planner.
Special situations uchas coreference an be easily identified because the lexicon entries ar e grounded in their inputs.
Thismethod appears to be much simpler than those needed by task-based generators.Semant ic  Match ing  Constra ints .
Matching constraints take into account he fact that, firstof  all, certain lexicon entries may match multiple elements of the input structure and, secondly,that the matches that do occur may be imperfect or incomplete.In general, the semantic matcher keepstrack of which lexicon entries cover which parts of theinput, which require other plans to be used with it, and which have some sort of semantic mismatchwith the input.
The following sums up the types of mismatches that might be present, each of whichreceives a different penalty (penalties are tracked by the control mechanism and help determinewhich combination of realizations i optimal):?
slots present in input that are missing in lexicon entry ->  undergeneration penalty, plan separately?
extra slots in lexicon entry ~ overgeneration penalty?
slot filler discrepancies (different, or more or less specific)- constant filler valuesHUMAN (age 13-19) - i.e.
"teenager" from English input vs.HUMAN (age 12-16) i.e.
"age b~te" in French lexicon- concept fillersHUMAN (origin FRANCE) vs.HUMAN (origin EUROPE)?
A more detailed explanation of these issues is presented in \[Beale and Viegas1996\].
The impor-tant thing to note here is that input-based modularization i our knowledge sources enables thistype of constraint to be tracked automatically.
In combination with the other constraints describedabove, we Can avoid the complex mechanisms needed by task-based generators for interacting real-izations of input semantics.
:3 E f f i c ient  Const ra in t -based  Process ing  ?The Mikrokosmo s project utilizes an efficient, constraint:directed control architecture called Hunter-Gatherer (HG).
\[Beale t a1.1996\] overviews how it enables emantic analysis to be performed innear linear-time.
Its use in generation is quite similar.
\[Beale1997\] describes HG in detail.Consider Figure 8, a representation f the constraint interactions present in a section of Figure1.
Each label, such as D IV IDE,  is realizable by the set of choices pecified in the lexicon.
Each54.I - !Figure 8: Problem Decomposition* aJFigure 9: Sub-problem i!il!i!IIIsolid line represents an instance of one of the above constraint types.
For example, D IV IDE andORG-INVOLVED-IN are connected because of the structural constraint described above (theyboth cannot set up a structure which nests the realization of CONGLOMERATE=32 into differentsubject positions).The key to the efficient constraint-based planner Hunter-Gatherer is its ability to identify con-straints and partition the overall problem into relatively independent subproblems.
These subprob-lems are tackled independently and the results are combined using solution synthesis techniques.This "divide-and-conquer" methodology substantially reduces the ?number of combinations thathave to be tested, while ?
guaranteeing an Optimal answer.
For example, in Figure 8, if we assumethat each node had 5 possible choices (a conservative assumption), there would be 51?, or almost10 million combinations of choices to examine.
Using the partitions hown in dotted lines, however,HG only examines 1200 combinations, In general, HG is able to process emantic analysis andgeneration problems for natural anguage in near linear-time \[Beale t a1.1996\].
=While a detailed explanation of Hunter-Gatherer is beyond the scope of this paper, ?
it is fairlyeasy to explain the source of its power.
Consider Figure 9, a single subproblem from Figure 8.The key thing to note is ?that, of the three nodes, BUY, LOCATION and STOCK-MARKET,only BUY is connected by constraints to entities outside the subproblem.
This tells us that bylooking only at this subproblem we will not be able to determine?the optimal global choice forBUY, ?since there are constraints we cannot take into account.
What we can do, howeve r , is, foreach possible choice for BUY, pick the choices for LOCATION and STOCK-MARKET thatoptimize it.
Later, when we combine the results of this subproblem with other subproblems andthus determine which choice for BUY is optimal, we will already have determined the choices forLOCATION and STOCK-MARKET that go best with it.The following sums up the advantages Hunter-Gatherer has for text generation:55?
Its knowledge i s fully declarative.
Note that this is allowed by unification processors \[Elhadad et a1.1997\],but HG gives the added benefits of speed and capability of "fuzzy" constraint processing.?
It allows "exhaustive" numeration f local combinations.?
It eliminates the need to make early decisions.?
It facilitates interacting constraints, and accepts constraints from any source, .while still utilizingmodular, declarative knowledge.
- "?
It guarantees optimal answers (as measured by preferences).?
It is very fast (near linear-time).4 Comparison to Other Generation SystemsRelated work ?
exists in two areas: (i) the processing strategy of microplanning tasks, and (ii) thenature and organization of resources used by the microplanner.There is a strong tendency in generation to deal with microplanning tasks in a small ?numberof modules, ?which are either structurally or functionally motivated.
However, it is recognized thatmany of the tasks are highly intertwined, so that, in principle, the modules hould run in paralleland nearly Constantly exchange information.
We consider this as a clear hint that a coarse-grained,task-oriented division of microp!anning sets up artificial barriers.
Repeated efforts of researchersto try and breach those barriers confirm our view.\[Elhadad et a1.1997\] recognizes that constraints on lexical choice come from a wide variety ofsources and are multidirectional, making it difficult to determine a systematic ordering in whichthey should be taken into account.
They propose a backtracking mechanism within a unificationframework to ?overcome the problem.
\[Rubinoff1992\] is perhaps the most strongly focused on thisissue.
He argues that the accepted ivision into components "ultimately interferes with some of thedecisions necessary in the generation process."
He utilizes annotations as a feedback mechanism toprovide the planning stages with linguistically relevant knowledge.?
Another area of research that belies the unnatural task-based ivision widely accepted by.textgeneration researchers today is the attempts to control sentence planning tasks.
\[Nirenburg et al 1989\]and more recently, \[Wanner and Hovy1996\] advocate a blackboard control mechanism, arguing thatthe order of sentence planning tasks cannot be pre-determined.
Behind this difficulty is the real-?
ity that different linguistic phenomena have different, unpredictable r quirements.
Grammatical,stylistic and collocati0nal constraints combine at unexpected times during the various tasks of sen-tence planning.?
Blackboard architectures, theoretically, can be used to allow a certain thread ofoperation to suspend operation until a needed bit of information is available.
Unfortunately, inthe best case, such an architecture is inefficient and difficult to control.
In practice, such systems,as is admitted in both papers above, resort to a ?
"default (processing) sequence for the modules"along with a simplistic truth-maintenance system which ultimately becomes a fail-and-backtracktype of control, completely negating the spirit of the blackboard?
system.
While these shortcomingsmight eventually be ?overcome, the fact remains that it was the unnatural division into tasks thatnecessitated ?the blackboard processing in the first place.?
In this paper, we propose an input data-oriented division of the microplanning task--similarto the way many incremental generators \[De Smedt1990, Reithinger1992, Kilger and Finkler1995\]divide the task of surface processing.
However, the processing of input units as done by the Hunter-Gathere r ---our microplanning enginc differs significantly from the processing in the incrementalgenerators cited.?
Thus ,  an important feature of HC is that it possesses a strategy for dividingthe problem of verbalizing a semantic structure into relatively independent subproblems.
The55I!IIIilIiiIIIsubproblems can be of different size.
Into which subproblems the problem is divided depends onconstraints that hold between units in the input structure.
This strategy ?greatly contributes to theefficiency of HG.
In traditional incremental generators, a unit in the input structure is considered tobe a subproblem.
Furthermore, HG is bidirectional, i.e., it is usable for both analysis and generation.References\[Be.ale and Viegas1996\] S. Beale and E. Viegas.
1996.
Intelligent planning meets intelligent planners.
Pro-eeedings of the Workshop on Gaps and Bridges: New Directions in Planning and Natural Language Gen-eration, ECAI'96, Budapest, pages 59--64.\[Beale t a1.1996\] S. Beale, S. Nirenburg, and K. Mahesh.
1996.
Hunter-gatherer: Three search techniquesintegrated for natural language semantics.
Proc.
Thirteenth National Conference on Artificial Intelligence(AAAI96), Portland, Oregon. "
:\[Beale1997\] S. Beale.
1997.
Hunter-gatherer: Applying constraint satisfaction, branch-and-bound and solu-tion synthesis to computational semantics.
Ph.D.
Diss., Program in Language and Information Technolo-gies, School of Computer Science, Carnegie Mellon University.\[De Smedt1990\] K: De Smedt.
1990.
IPF: An Incremental Parallel Formulator.
In R. Dale, C.S.
Mellishland M. Zock, editors, Current Research in Natural Language Generation.
Academic Press.\[Elhadad et a1.1997\] M. Elhadadl J. Robin, and K. McKeown.
1997.
Floating constraints in lexical choice.Computational Linguistics (2), 23:195-239.\[Huang and Fiedler1996\] X. Huang and A. Fiedler.
1996.
Paraphrasing and aggregating argumentative textusing text structure.
In Proc.
of the 8th INLG, Herstmonceux.\[Kilger and Finkler1995\] A. Kilger and W. Finkler.
1995.
Incremental Generation for Real-Time Applica-tions.
Technical Report RR-95-11, DFKI.\[Kilger1997\] A. Kilger.
1997.
Microplanning in Verbmobil as a Constraint-Satisfaction Problem.
In DFKIWorkshop on Generation, pages 47-53, Saarbriicken.\[Levelti989 \] Willem J.M.
Levelt.
1989.
Speaking.
The MIT Press, Cambridge, MA.\[Nirenburg eta1.1989\] S. Nirenburg, V. Lesser, and N. Nyberg.
1989.
Controlling a language generationplanner.
Proc.
of IJCAI-89, pages 1524-1530.\[Nirenburg etal.1996\] S. Nirenburg, S. Beale, S. Helmreich, K. Mahesh, E. Viegas, and R. Zajac.
1996.
Twoprinciples and six techniques for rapid mt development.
Proc.
of AMTA-96.\[Panaget1994\] F. Panaget.
1994.
Using a Textual ?Representation Level Component in the Context ofDiscourse or Dialogue Generation.
In Proceedings of the 7th INLG, Kennebunkport.\[Rambow and Korelsky1992\] Oi Rambow and T. Korelsky.
1992.
Applied text generation.
Applied Confer-ence on Natural Language Processing, Trento, Italy.\[Reithinger1992\] N. Reithinger.
1992.
Eine parallele Architektur zur inkrementellen Generierung multi-modaler DialogbeitrSge.
Infix Verlag, St. Augustin.\[Rubinoff1992\] R. Rubinoff.
1992.
Integrating text planning and linguistic hoice by annotating linguisticstructures.
Proc.
6th international Workshop On Natural Language Generation, 7?ento, Italy.\[Stede1996\] M. Stede.
1996.
Lexieal Semantics and Knowledge Representation i  Multilingual ?SentenceGeneration.
Ph.D. thesis, University of Toronto.\[Viegas et a1.1996\] E. Viegas, B. Onyshkevych, V. Raskin, and S. Nirenburg.
1996.
From submit O submittedvia submission: on lexical rules in large-scale xicon acquisition.
In Proceedings of the 3gth Annual meetingof the Association for Computational Linguistics, CA.\[Wanner and Hovy1996\] L. Wanner and E. Hovy.
1996.
The healthdoc sentence planner.
Proc.
EighthInternational Natural Language Generation Workshop (INLG-96).57
