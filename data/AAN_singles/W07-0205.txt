TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 33?36,Rochester, April 2007 c?2007 Association for Computational LinguisticsUnigram Language Models using Diffusion Smoothing over GraphsBruno JedynakDept.
of Appl.
Mathematics and StatisticsCenter for Imaging SciencesJohns Hopkins UniversityBaltimore, MD 21218-2686bruno.jedynak@jhu.eduDamianos KarakosDept.
of Electrical and Computer EngineeringCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218-2686damianos@jhu.eduAbstractWe propose to use graph-based diffusiontechniques with data-dependent kernelsto build unigram language models.
Ourapproach entails building graphs, whereeach vertex corresponds uniquely to aword from a closed vocabulary, and theexistence of an edge (with an appropri-ate weight) between two words indicatessome form of similarity between them.
Inone of our constructions, we place an edgebetween two words if the number of timesthese words were seen in a training setdiffers by at most one count.
This graphconstruction results in a similarity ma-trix with small intrinsic dimension, sincewords with the same counts have the sameneighbors.
Experimental results from abenchmark task from language modelingshow that our method is competitive withthe Good-Turing estimator.1 Diffusion over Graphs1.1 NotationLet G = (V,E) be an undirected graph, where Vis a finite set of vertices, and E ?
V ?
V is theset of edges.
Also, let V be a vocabulary of words,whose probabilities we want to estimate.
Each ver-tex corresponds uniquely to a word, i.e., there is aone-to-one mapping between V and V .
Without lossof generality, we will use V to denote both the setof words and the set of vertices.
Moreover, to sim-plify notation, we assume that the letters x, y, z willalways denote vertices of G.The existence of an edge between x, y will bedenoted by x ?
y.
We assume that the graphis strongly connected (i.e., there is a path betweenany two vertices).
Furthermore, we define a non-negative real valued function w over V ?
V , whichplays the role of the similarity between two words(the higher the value of w(x, y), the more similarwords x, y are).
In the experimental results section,we will compare different measures of similarity be-tween words which will result in different smooth-ing algorithms.
The degree of a vertex is defined asd(x) =?y?V :x?yw(x, y).
(1)We assume that for any vertex x, d(x) > 0; that is,every word is similar to at least some other word.1.2 Smoothing by Normalized DiffusionThe setting described here was introduced in (Szlamet al, 2006).
First, we define a Markov chain {Xt},which corresponds to a random walk over the graphG.
Its initial value is equal to X0, which has dis-tribution pi0.
(Although pi0 can be chosen arbitrar-ily, we assume in this paper that it is equal to theempirical, unsmoothed, distribution of words over atraining set.)
We then define the transition matrix asfollows:T (x, y) = P (X1 = y|X0 = x) = d?1(x)w(x, y).
(2)This transition matrix, together with pi0, induces adistribution over V , which is equal to the distribu-33tion pi1 of X1:pi1(y) =?x?VT (x, y)pi0(x).
(3)This distribution can be construed as a smoothedversion of pi0, since the pi1 probability of an un-seen word will always be non-zero, if it has a non-zero similarity to a seen word.
In the same way, awhole sequence of distributions pi2, pi3, .
.
.
can becomputed; we only consider pi1 as our smoothed es-timate in this paper.
(One may wonder whether thestationary distribution of this Markov chain, i.e., thelimiting distribution of Xt, as t ?
?, has any sig-nificance; we do not address this question here, asthis limiting distribution may have very little depen-dence on pi0 in the Markov chain cases under con-sideration.
)1.3 Smoothing by Kernel DiffusionWe assume here that for any vertex x, w(x, x) = 0and that w is symmetric.
Following (Kondor andLafferty, 2002), we define the following matrix overV ?
VH(x, y) = w(x, y)?
(x ?
y)?
d(x)?
(x = y), (4)where ?
(u) is the delta function which takes thevalue 1 if property u is true, and 0 otherwise.
Thenegative of the matrix H is called the Laplacian ofthe graph and plays a central role in spectral graphtheory (Chung, 1997).
We further define the heatequation over the graph G as?
?tKt = HKt, t > 0, (5)with initial condition K0 = I , where Kt is a time-dependent square matrix of same dimension as H ,and I is the identity matrix.
Kt(x, y) can be inter-preted as the amount of heat that reaches vertex xat time t, when starting with a unit amount of heatconcentrated at y.
Using (1) and (4), the right handside of (5) expands toHKt(x, y) =?z:z?xw(x, z) (Kt(z, y)?Kt(x, y)) .
(6)From this equation, we see that the amount of heatat x will increase (resp.
decrease) if the currentamount of heat at x (namely Kt(x, y)) is smaller(resp.
larger) than the weighted average amount ofheat at the neighbors of x, thus causing the systemto reach a steady state.The heat equation (5) has a unique solution whichis the matrix exponential Kt = exp(tH), (see (Kon-dor and Lafferty, 2002)) and which can be definedequivalently asetH = limn?+?
(I +tHn)n(7)or asetH = I + tH +t22!H2 +t33!H3 + ?
?
?
(8)Moreover, if the initial condition is replaced byK0(x, y) = pi0(x)?
(x = y) then the solution ofthe heat equation is given by the matrix productpi1 = Ktpi0.
In the following, pi0 will be the em-pirical distribution over the training set and t will bechosen by trial and error.
As before, pi1 will providea smoothed version of pi0.2 Unigram Language ModelsLet Tr be a training set of n tokens, and T a sepa-rate test set of m tokens.
We denote by n(x),m(x)the number of times the word x has been seen inthe training and test set, respectively.
We assume aclosed vocabulary V containing K words.
A uni-gram model is a probability distribution pi over thevocabulary V .
We measure its performace usingthe average code length (Cover and Thomas, 1991)measured on the test set:l(pi) = ?1|T |?x?Vm(x) log2 pi(x).
(9)The empirical distribution over the training set ispi0(x) =n(x)n. (10)This estimate assigns a probability 0 to all unseenwords, which is undesirable, as it leads to zero prob-ability of word sequences which can actually be ob-served in practice.
A simple way to smooth suchestimates is to add a small, not necessarily integer,count to each word leading to the so-called add-?estimate pi?
, defined aspi?
(x) =n(x) + ?n + ?K.
(11)34One may observe thatpi?
(x) = (1??
)pi0(x)+?1K, with ?
= ?Kn + ?K.
(12)Hence add-?
estimators perform a linear interpola-tion between pi0 and the uniform distribution overthe entire vocabulary.In practice, a much more efficient smoothingmethod is the so-called Good-Turing (Orlitsky et al,2003; McAllester and Schapire, 2000).
The Good-Turing estimate is defined aspiGT (x) =rn(x)+1(n(x) + 1)nrn(x), if n(x) < M= ?pi0(x), otherwise,where rj is the number of distinct words seen j timesin the training set, and ?
is such that piGT sums upto 1 over the vocabulary.
The threshold M is em-pirically chosen, and usually lies between 5 and 10.
(Choosing a much larger M decreases the perfor-mance considerably.
)The Good-Turing estimator is used frequently inpractice, and we will compare our results against it.The add-?
will provide a baseline, as well as an ideaof the variation between different smoothers.3 Graphs over sets of wordsOur objective, in this section, is to show how to de-sign various graphs on words; different choices forthe edges and for the weight function w lead to dif-ferent smoothings.3.1 Full Graph and add-?
SmoothersThe simplest possible choice is the complete graph,where all vertices are pair-wise connected.
In thecase of normalized diffusion, choosingw(x, y) = ??
(x = y) + 1, (13)with ?
6= 0 leads to the add-?
smoother with param-eter ?
= ?
?1n.In the case of kernel smoothing with the completegraph and w ?
1, one can show, see (Kondor andLafferty, 2002) thatKt(x, y) = K?1(1 + (K ?
1)e?Kt)if x = y= K?1(1?
e?Kt)if x 6= y.This leads to another add-?
smoother.3.2 Graphs based on countsA more interesting way of designing the word graphis through a similarity function which is based onthe training set.
For the normalized diffusion case,we propose the followingw(x, y) = ?(|n(x)?
n(y)| ?
1).
(14)That is, 2 words are ?similar?
if they have been seena number of times which differs by at most one.
Theobtained estimator is denoted by piND.
After somealgebraic manipulations, we obtainpiND(y) =1nn(y)+1?j=n(y)?1jrjrj?1 + rj + rj+1.
(15)This estimator has a Good-Turing ?flavor?.
For ex-ample, the total mass associated with the unseenwords is?y;n(y)=0pi1(y) =1nr11 + r1r0 +r2r0.
(16)Note that the estimate of the unseen mass, in the caseof the Good-Turing estimator, is equal to n?1r1,which is very close to the above when the vocabu-lary is large compared to the size of the training set(as is usually the case in practice).Similarly, in the case of kernel diffusion, wechoose w ?
1 andx ?
y ??
|n(x)?
n(y)| ?
1 (17)The time t is chosen to be |V |?1.
The smoother can-not be computed in closed form.
We used the for-mula (7) with n = 3 in the experiments.
Largervalues of n did not improve the results.4 Experimental ResultsIn our experiments, we used Sections 00-22 (con-sisting of ?
106 words) of the UPenn Treebank cor-pus for training, and Sections 23-24 (consisting of?
105 words) for testing.
We split the training setinto 10 subsets, leading to 10 datasets of size ?
105tokens each.
The first of these sets was further splitin subsets of size ?
104 tokens each.
Averaged re-sults are presented in the tables below for variouschoices of the training set size.
We show the meancode-length, as well as the standard deviation (when35mean code length stdpi?, ?
= 1 12.94 0.05piGT 11.40 0.08piND 11.42 0.08piKD 11.51 0.08Table 1: Results with training set of size ?
104.mean code length stdpi?, ?
= 1 11.10 0.03piGT 10.68 0.06piND 10.69 0.06piKD 10.74 0.08Table 2: Results with training set of size ?
105.available).
In all cases, we chose K = 105 as thefixed size of our vocabulary.The results show that piND, the estimate ob-tained with the Normalized Diffusion, is competi-tive with the Good-Turing piGT .
We performed aKolmogorov-Smirnov test in order to determine ifthe code-lengths obtained with piND and piGT in Ta-ble 1 differ significantly.
The result is negative (P-value = .65), and the same holds for the larger train-ing set in Table 2 (P-value=.95).
On the other hand,piKD (obtained with Kernel Diffusion) is not as effi-cient, but still better than add-?
with ?
= 1.5 Concluding RemarksWe showed that diffusions on graphs can be usefulfor language modeling.
They yield naturally smoothestimates, and, under a particular choice of the ?sim-ilarity?
function between words, they are competi-tive with the Good-Turing estimator, which is con-sidered to be the state-of-the-art in unigram lan-guage modeling.
We plan to perform more exper-mean code lengthpi?, ?
= 1 10.34piGT 10.30piND 10.30piKD 10.31Table 3: Results with training set of size ?
106.iments with other definitions of similarity betweenwords.
For example, we expect similarities basedon co-occurence in documents, or based on notionsof semantic closeness (computed, for instance, usingthe WordNet hierarchy) to yield significant improve-ments over estimators which are only based on wordcounts.ReferencesF.
Chung.
1997.
Spectral Graph Theory.
Number 92in CBMS Regional Conference Series in Mathematics.American Mathematical Society.Thomas M. Cover and Joy A. Thomas.
1991.
Elementsof Information Theory.
John Wiley & Sons, Inc.Risi Imre Kondor and John Lafferty.
2002.
Diffusionkernels on graphs and other discrete input spaces.
InICML ?02: Proceedings of the Nineteenth Interna-tional Conference on Machine Learning, pages 315?322.David McAllester and Robert E. Schapire.
2000.
On theconvergence rate of Good-Turing estimators.
In Proc.13th Annu.
Conference on Comput.
Learning Theory.Alon Orlitsky, Narayana P. Santhanam, and Junan Zhang.2003.
Always Good Turing: Asymptotically optimalprobability estimation.
In FOCS ?03: Proceedings ofthe 44th Annual IEEE Symposium on Foundations ofComputer Science.Arthur D. Szlam, Mauro Maggioni, and Ronald R. Coif-man.
2006.
A general framework for adaptive regular-ization based on diffusion processes on graphs.
Tech-nical report, YALE/DCS/TR1365.36
