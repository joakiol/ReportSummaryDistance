In: Proceedings of CoNLL-2000 and LLL-2000, pages 83-86, Lisbon, Portugal, 2000.Experiments on Unsupervised Learning for Extracting RelevantFragments from Spoken Dialog CorpusKonstant in  BiatovAT&T Bell-Labs Research180 Park Avenue07932-0971, F lorham Park, N J, USA,Kbiatov@aol.
tomAbst rac tIn this paper are described experiments on un-supervised learning of the domain lexicon andrelevant phrase fragments from a dialog cor-pus.
Suggested approach is based on using do-main independent words for chunking and us-ing semantical predictional power of such wordsfor clustering and automatic extraction phrasefragments relevant o dialog topics.1 In t roduct ionWe are interested in rapid development of spo-ken dialog understanding systems.
We presentexperiments on unsupervised learning of thedomain lexicon and relevant phrase fragmentsfrom dialog corpus.Pereira (1993) described a method for auto-matically clustering words according to theirdistribution in particular syntactic ontext, forexample verbs and direct objects of these verbs.By using preexisting concepts from Wordnetdatabase Resnik (1998) described how to pre-dict words meaning from their distributionalcontext.
Both mentioned methods are fullyunsupervised and are focused only on follow-ing word distribution.
They describe the de-pendences between verb and noun as a directobject of the verb.
A new method for gather-ing phrases into clusters was described by Arai(1999).
This method uses following and preced-ing words distribution and call-types, associatedwith each utterance, but requires at the begin-ning labeling and transcribing a small numberof the utterances.In contrast with the mentioned methodolo-gies, we are interested in finding a limited setof domain independent words (less than 1000)including prepositions, adverbs and adjectivesand using these words for unsupervised clus-tering and automatic extraction of the relevantknowledge from dialog corpus.2 Descr ip t ion  o f  the  a lgor i thmThere are four main steps in our approach.First step is to make automatically labelingand to chunk each sentence from spoken dia-log corpus into a set of short subphrases.
Weassume that in the spoken dialog a sentenceconsists of slightly related subphrases.
In ourexperiment for labeling and chunking we usea relatively small set of domain independentwords such as prepositions, determiners, arti-cles, modals and adverbs.
For example articles:a, an, the; prepositions: in, with,  about ,  un-der, for, of, to; determiners: some, many.The domain independent words are grouped insubvocabularies.
For instance, subvocabulary<article> includes words a, an, the.
Somesubvocabularies include only one word.
If agiven sentence includes article A we'll replace itby the label (<article>A), article THE we'llreplace by the label (<article>THE) and soon.
Very important feature of our algorithm isthat some of the words selected for tags can pre-dict the semantics of the followed words or sub-phrases.
In all cases we could characterize thisprediction as possibility.
For example the wordf rom predict semantics of the followed wordsor subphrases as a "start point", a "reason", a"source" or someth ing  else.
For each of such tagwords  we create separate subvocabulary.
In theprocess of labeling we examine  given sentencef rom left to right and  replace the tag words  bythe labels.
For labeling we use tools based onAT&T CHRONUS system described by Levin(1995).In the process of chunking we examine thesentence from left to right.
In one chunk we put83one tag word label or tag word labels followingone by one and other non tag words on the rightup to but excluding next tag word label.
Thereare two examples of the chunks:(<what>WHAT) YPE,(<pronouns>I) (<article>A) FARE.We'll describe ach non tag word by the vectorof the features.
Every component of the vectorcorresponds to subvocabulary of the tag wordsas it is described below:component  1 ~ (<article>...)component  2 ~ (<determiner>...)component  3 -4 (<modal>...)component  4 ~ (<of>OF)component  5 --4 (<to>TO)component  n --~ (<from>FROM)Every component mean how many times tagword label was in the left context of describednon tag word.
Every component is an integer.Thus we have the list of non tag words and vec-tors of integers corresponding to this words.Second step is to cluster the words from allchunks by using the vectors of the features.In this step we extract from chunks the wordswhich have enough semantically charged tagsin the left context and group such words in theclusters.For clustering we take from the list the firstnon tag word and check if the number of differ-ent tags (number of non zero components of thevector) is more then threshold.
The thresholdvalue must be greater then the number of tagwords having low semantical predictional power(articles, modals, auxiliaries, determiners).
Inour experiments we used threshold values from6 up to 9.
If the number of different ags fortested vector is more than threshold we'll con-sider this vector as a centre of the cluster andthen looking for other vectors neighbouring totested vector.
When the neighbouring vectorsare selected we'll remove them from the list ofvectors.
This procedure we'll repeat for all vec-tors non selected as a member of the class.
Forthis experiments we have used distance measurebased on Hamming metrics.In the third step we go back to the chunksand extract chunks which include words fromone cluster.
In this way we generate the clustersof the chunks.In the forth step we reduce the number of thechunk's clusters.
We make union of all chunk'sclusters except one tested cluster and then inter-sect this one with chunk's union.
If all chunksfrom tested cluster are inside of the union wedelete this tested chink cluster.Let us consider baseline algorithm which use"stop words" known in information retrievalsystems.
The idea of this algorithm is to deletethe stop words from given sentence and returnall of the remaining words as lexicon items.There are some principal differences betweenbaseline algorithm and suggested algorithm.In suggested algorithm we are looking for thewords which have enough semantically chargedtags in the left context and then extract chunkswhich include selected words.
In the baseline al-gorithm we are looking for only words remainingafter deleting "stop words".3 The  resu l ts  o f  exper imentsBelow we show examples of labeling and chunk-ing the phrases.
As an example we use twophrase from ATIS dialog corpus which includesnearly 20K sentences about flights, reservations,tickets, prices, car rent, flight classes and others.WHAT TYPE OF AIRCRAFT ISUSED FOR THIS FLIGHTIS A MEAL SERVED FOR THISFLIGHTAfter labeling we'll have followed labeledphrase:(<what>WHAT) YPE (<of>OF)AIRCRAFT (<auxiliary>IS)USED (<for>FOR)(<determiner>THIS) FLIGHT(<auxiliary>IS) (<article>A)MEAL SERVED (<for>FOR)(<determiner>THIS) FLIGHTIn one chunk we put the tag word label or se-quence of tag word labels from the left contextand other non tag words on the right up butexclude the next tag word label.
Below is thelist of the chunks for those two sentences.
(<what>WHAT) TYPE(<of>OF)  AIRCRAFT84(<auxiliary>IS) USED(<for>FOR) (<determiner>THiS) FLIGHT(<auxiliary>IS)(<article>A) MEAL SERVEDWe have divided the corpus into two parts.
Foreach part we did the labeling, chunking andclustering by using Hamming metrics for dis-tance measure.
Below we present the words ex-tracted from both parts of the corpus.The words extracted from the first 15K sen-tences.AIRLINE, AIRLINES, AIRPORT,AVAILABLE, BREAKFAST, BUSI-NESS, CITY, CLASS, COACH,COST, DAY,  DINNER, EACH,EARLIEST, EARLY, ECONOMY,FARE, FARES, FLIGHT, FLIGHTS,FLY, FLYING, GET, GO, GO-ING, GROUND, INFORMATION,LATEST, LESS, LUNCH, MAKE,MEAL, MEALS, MOST, NON-STOP, CLASS, NUMBER, OTHER,PLANE, PRICE, RENTAL, RE-STRICTIONS, RETURN, ROUND,SERVE, SERVED, SERVICE, SHOW,STOP, STOPS, TAKE, TIME,TRANSPORTATION, TRIP.There are 54 words.
Near 80% of the wordscould be considered as having strict relationsto the dialog topics.
There are such words asAIRLINE, CLASS, COACH, COST, MEALS.To understand does this approach is robust weapplied the same methodology for the last 5Ksentences ofthe corpus.
The words extracted inthis experiments are:AIRPORT, FLY, FLIGHT.STOPOVER, INFORMATION.FLIGHTS, AIRCRAFT, CITIESCOST, LESS, FARE, TRIP, ROUND.TRANSPORTATION, GROUND.LIST, FARES, CAR, TIMESNUMBER, NONSTOP, AIRLINEMEALS.
AVAILABLE, AIRLINES.With exception of STOPOVER, CITIES,TIMES all other words are among those ex-tracted from first 15K sentences.Below we present as an example the contentsof the chunk's cluster for word COST extractedfrom the first 15K sentences:LOWEST COST FARE, LIKE COST,FLIGHT COST, KNOW COST,FLIGHTS COST, LOVEST COSTAIRFARE, LIMOUSINE COST,RENTAL CAR COST, LIMOUSINESERVICE COST, WITHIN CITY,NEED COST, LOWEST COSTFARE ORIGIN_CITY, CHEAP-EST COST FARE DEST_CITY,AVERAGE COST, AIRPORTTRANSPORTATION COST, CARRENTAL COST, TAXI COST,MID SIZE CAR COST, ROUNDTRIP COST CITY, ROUND TRIPCOST, FARES COST, SEE TOTALCOST, SHOW COSTS, GIVE AP-PROXIMATE COST, AIR TAXICOST, COST GET, TOTAL COST,SAME COST, COST FLY, COSTLESS, COST TRAVEL ORIG_AIRP,COACH COST, COST ASSOCI-ATED, ECONOMY ROUND TRIPTICKET COST ORIGIN_CITY,DESCENDING COST, FARE CODEF COST, COST TAKING COACHORIGIN_CITYAnd from 5K last sentences:AIR TAXI COST, COST, COSTLESS, SEE COST, ROUND TRIPCOST, COST ASSOCIATED,ECONOMY ROUND TRIP TICKETCOST, DESCENDING COST,FLIGHT COST FARE CODE FCOST, COACH FARE COST,COST TAKING COACH ORI-GIN_CITY, COST NUMBER, CARRENTAL COST, COST INFORMA-TION, COST NUMBER, FLIGHTCOST LESS, COST COACH FAREROUND TRIP TICKET, WHOSECOST, LEAST EXPENSIVE COST,ECONOMY CLASS COST.4 Conc lus ionThe experiments how that the suggestedmethod gives robust results for relevant knowl-edge extraction from dialog corpus.85ReferencesK.
Arai, J. Wright, G. Riccardi, and A. Gorin.1999.
Grammar fragments aquisition using syn-tactic and semantic lustering.
Speech Communi-cation, 27:43-62.E.
Levin and R. Pieraccini.
1995.
Chronus, the nextgeneration.
In Proceedings of 1995 ARPA SpokenLanguage Systems Technical Workshop.
Austin,Texas.F.
Pereira, N. Tishby, and L. Lee.
1993.
Distribu-tional clustering of english words.
In Proceedingsof the 31st Annual Meeting of the Association ofComputational Linguistics, pages 183-190.
Asso-ciation for Computitional Linguistics.P.
Resnik.
1998.
Wordnet and class-based probabili-ties.
In C. Fellbaum, editor, WORDNET An elec-tonic lexical database, pages 239-263.
The MITPress, Cambridge, Massachusetts, London, Eng-land.86
