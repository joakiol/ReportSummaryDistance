Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 55?60,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsDefinition and Analysis of Intermediate Entailment LevelsRoy Bar-Haim, Idan Szpektor, Oren GlickmanComputer Science DepartmentBar Ilan UniversityRamat-Gan 52900, Israel{barhair,szpekti,glikmao}@cs.biu.ac.ilAbstractIn this paper we define two intermediatemodels of textual entailment, which corre-spond to lexical and lexical-syntactic lev-els of representation.
We manually anno-tated a sample from the RTE dataset ac-cording to each model, compared the out-come for the two models, and exploredhow well they approximate the notion ofentailment.
We show that the lexical-syntactic model outperforms the lexicalmodel, mainly due to a much lower rateof false-positives, but both models fail toachieve high recall.
Our analysis alsoshows that paraphrases stand out as adominant contributor to the entailmenttask.
We suggest that our models and an-notation methods can serve as an evalua-tion scheme for entailment at these levels.1 IntroductionTextual entailment has been proposed recently asa generic framework for modeling semantic vari-ability in many Natural Language Processing ap-plications, such as Question Answering, Informa-tion Extraction, Information Retrieval and Docu-ment Summarization.
The textual entailment rela-tionship holds between two text fragments, termedtext and hypothesis, if the truth of the hypothesis canbe inferred from the text.Identifying entailment is a complex task that in-corporates many levels of linguistic knowledge andinference.
The complexity of modeling entail-ment was demonstrated in the first PASCAL Chal-lenge Workshop on Recognizing Textual Entailment(RTE) (Dagan et al, 2005).
Systems that partici-pated in the challenge used various combinations ofNLP components in order to perform entailment in-ferences.
These components can largely be classi-fied as operating at the lexical, syntactic and seman-tic levels (see Table 1 in (Dagan et al, 2005)).
How-ever, only little research was done to analyze thecontribution of each inference level, and on the con-tribution of individual inference mechanisms withineach level.This paper suggests that decomposing the com-plex task of entailment into subtasks, and analyz-ing the contribution of individual NLP componentsfor these subtasks would make a step towards bet-ter understanding of the problem, and for pursuingbetter entailment engines.
We set three goals in thispaper.
First, we consider two modeling levels thatemploy only part of the inference mechanisms, butperform perfectly at each level.
We explore howwell these models approximate the notion of entail-ment, and analyze the differences between the out-come of the different levels.
Second, for each of thepresented levels, we evaluate the distribution (andcontribution) of each of the inference mechanismstypically associated with that level.
Finally, we sug-gest that the definitions of entailment at differentlevels of inference, as proposed in this paper, canserve as guidelines for manual annotation of a ?goldstandard?
for evaluating systems that operate at aparticular level.
Altogether, we set forth a possiblemethodology for annotation and analysis of entail-55ment datasets.We introduce two levels of entailment: Lexicaland Lexical-Syntactic.
We propose these levels asintermediate stages towards a complete entailmentmodel.
We define an entailment model for eachlevel and manually evaluate its performance over asample from the RTE test-set.
We focus on thesetwo levels as they correspond to well-studied NLPtasks, for which robust tools and resources exist,e.g.
parsers, part of speech taggers and lexicons.
Ateach level we included inference types that representcommon practice in the field.
More advanced pro-cessing levels which involve logical/semantic infer-ence are less mature and were left beyond the scopeof this paper.We found that the main difference between thelexical and lexical-syntactic levels is that the lexical-syntactic level corrects many false-positive infer-ences done at the lexical level, while introducingonly a few false-positives of its own.
As for iden-tifying positive cases (recall), both systems exhibitsimilar performance, and were found to be comple-mentary.
Neither of the levels was able to iden-tify more than half of the positive cases, whichemphasizes the need for deeper levels of analysis.Among the different inference components, para-phrases stand out as a dominant contributor to theentailment task, while synonyms and derivationaltransformations were found to be the most frequentat the lexical level.Using our definitions of entailment models asguidelines for manual annotation resulted in a highlevel of agreement between two annotators, suggest-ing that the proposed models are well-defined.Our study follows on previous work (Vander-wende et al, 2005), which analyzed the RTE Chal-lenge test-set to find the percentage of cases inwhich syntactic analysis alone (with optional useof thesaurus for the lexical level) suffices to decidewhether or not entailment holds.
Our study extendsthis work by considering a broader range of infer-ence levels and inference mechanisms and providinga more detailed view.
A fundamental difference be-tween the two works is that while Vanderwende et aldid not make judgements on cases where additionalknowledge was required beyond syntax, our entail-ment models were evaluated over all of the cases,including those that require higher levels of infer-ence.
This allows us to view the entailment model ateach level as an idealized system approximating fullentailment, and to evaluate its overall success.The rest of the paper is organized as follows: sec-tion 2 provides definitions for the two entailmentlevels; section 3 describes the annotation experimentwe performed, its results and analysis; section 4 con-cludes and presents planned future work.2 Definition of Entailment LevelsIn this section we present definitions for two en-tailment models that correspond to the Lexical andLexical-Syntactic levels.
For each level we de-scribe the available inference mechanisms.
Table 1presents several examples from the RTE test-set to-gether with annotation of entailment at the differentlevels.2.1 The Lexical entailment levelAt the lexical level we assume that the text T andhypothesis H are represented by a bag of (possiblymulti-word) terms, ignoring function words.
At thislevel we define that entailment holds between T andH if every term h in H can be matched by a corre-sponding entailing term t in T .
t is considered as en-tailing h if either h and t share the same lemma andpart of speech, or t can be matched with h through asequence of lexical transformations of the types de-scribed below.Morphological derivations This inference mech-anism considers two terms as equivalent if one canbe obtained from the other by some morphologi-cal derivation.
Examples include nominalizations(e.g.
?acquisition ?
acquire?
), pertainyms (e.g.
?Afghanistan ?
Afghan?
), or nominal derivationslike ?terrorist ?
terror?.Ontological relations This inference mechanismrefers to ontological relations between terms.
Aterm is inferred from another term if a chain of validontological relations between the two terms exists(Andreevskaia et al, 2005).
In our experiment weregarded the following three ontological relationsas providing entailment inferences: (1) ?synonyms?(e.g.
?free ?
release?
in example 1361, Table 1);(2) ?hypernym?
(e.g.
?produce ?
make?)
and (3)?meronym-holonym?
(e.g.
?executive ?
company?).56No.
Text Hypothesis Task Ent.
Lex.Ent.Syn.Ent.322 Turnout for the historic vote for the firsttime since the EU took in 10 new mem-bers in May has hit a record low of45.3%.New members joined theEU.IR true false true1361 A Filipino hostage in Iraq was released.
A Filipino hostage wasfreed in Iraq.CD true true true1584 Although a Roscommon man by birth,born in Rooskey in 1932, Albert ?TheSlasher?
Reynolds will forever be aLongford man by association.Albert Reynolds was bornin Co. Roscommon.QA true true true1911 The SPD got just 21.5% of the votein the European Parliament elections,while the conservative opposition par-ties polled 44.5%.The SPD is defeated bythe opposition parties.IE true false false2127 Coyote shot after biting girl in VanierPark.Girl shot in park.
IR false true falseTable 1: Examples of text-hypothesis pairs, taken from the PASCAL RTE test-set.
Each line includes theexample number at the RTE test-set, the text and hypothesis, the task within the test-set, whether entailmentholds between the text and hypothesis (Ent.
), whether Lexical entailment holds (Lex.
Ent.)
and whetherLexical-Syntactic entailment holds (Syn.
Ent.
).Lexical World knowledge This inference mech-anism refers to world knowledge reflected at thelexical level, by which the meaning of one termcan be inferred from the other.
It includes bothknowledge about named entities, such as ?Tal-iban ?
organization?
and ?Roscommon ?
Co.Roscommon?
(example 1584 in Table 1), and otherlexical relations between words, such as WordNet?srelations ?cause?
(e.g.
?kill ?
die?)
and ?entail?
(e.g.
?snore ?
sleep?
).2.2 The Lexical-syntactic entailment levelAt the lexical-syntactic level we assume that thetext and the hypothesis are represented by the set ofsyntactic dependency relations of their dependencyparse.
At this level we ignore determiners and aux-iliary verbs, but do include relations involving otherfunction words.
We define that entailment holds be-tween T and H if the relations within H can be?covered?
by the relations in T .
In the trivial case,lexical-syntactic entailment holds if all the relationscomposing H appear verbatim in T (while addi-tional relations within T are allowed).
Otherwise,such coverage can be obtained by a sequence oftransformations applied to the relations in T , whichshould yield all the relations in H .One type of such transformations are the lexicaltransformations, which replace corresponding lexi-cal items, as described in sub-section 2.1.
When ap-plying morphological derivations it is assumed thatthe syntactic structure is appropriately adjusted.
Forexample, ?Mexico produces oil?
can be mapped to?oil production by Mexico?
(the NOMLEX resource(Macleod et al, 1998) provides a good example forsystematic specification of such transformations).Additional types of transformations at this levelare specified below.Syntactic transformations This inference mech-anism refers to transformations between syntacticstructures that involve the same lexical elements andpreserve the meaning of the relationships betweenthem (as analyzed in (Vanderwende et al, 2005)).Typical transformations include passive-active andapposition (e.g.
?An Wang, a native of Shanghai ?An Wang is a native of Shanghai?
).57Entailment paraphrases This inference mecha-nism refers to transformations that modify the syn-tactic structure of a text fragment as well as someof its lexical elements, while holding an entailmentrelationship between the original text and the trans-formed one.
Such transformations are typically de-noted as ?paraphrases?
in the literature, where awealth of methods for their automatic acquisitionwere proposed (Lin and Pantel, 2001; Shinyama etal., 2002; Barzilay and Lee, 2003; Szpektor et al,2004).
Following the same spirit, we focus here ontransformations that are local in nature, which, ac-cording to the literature, may be amenable for largescale acquisition.
Examples include: ?X is Y manby birth ?
X was born in Y?
(example 1584 in Ta-ble 1), ?X take in Y ?
Y join X?1 and ?X is holybook of Y ?
Y follow X?2.Co-reference Co-references provide equivalencerelations between different terms in the text andthus induce transformations that replace one termin a text with any of its co-referenced terms.
Forexample, the sentence ?Italy and Germany haveeach played twice, and they haven?t beaten anybodyyet.
?3 entails ?Neither Italy nor Germany havewon yet?, involving the co-reference transformation?they ?
Italy and Germany?.Example 1584 in Table 1 demonstrates theneed to combine different inference mechanismsto achieve lexical-syntactic entailment, requiringworld-knowledge, paraphrases and syntactic trans-formations.3 Empirical AnalysisIn this section we present the experiment that weconducted in order to analyze the two entailmentlevels, which are presented in section 2, in terms ofrelative performance and correlation with the notionof textual entailment.3.1 Data and annotation procedureThe RTE test-set4 contains 800 Text-Hypothesispairs (usually single sentences), which are typical1Example no 322 in the PASCAL RTE test-set.2Example no 1575 in the PASCAL RTE test-set.3Example no 298 in the PASCAL RTE test-set.4The complete RTE dataset can be obtained athttp://www.pascal-network.org/Challenges/RTE/Datasets/to various NLP applications.
Each pair is annotatedwith a boolean value, indicating whether the hypoth-esis is entailed by the text or not, and the test-setis balanced in terms of positive and negative cases.We shall henceforth refer to this annotation as thegold standard.
We constructed a sample of 240 pairsfrom four different tasks in the test-set, which corre-spond to the main applications that may benefit fromentailment: information extraction (IE), informationretrieval (IR), question answering (QA), and compa-rable documents (CD).
We randomly picked 60 pairsfrom each task, and in total 118 of the cases werepositive and 122 were negative.In our experiment, two of the authors annotated,for each of the two levels, whether or not entailmentcan be established in each of the 240 pairs.
The an-notators agreed on 89.6% of the cases at the lexicallevel, and 88.8% of the cases at the lexical-syntacticlevel, with Kappa statistics of 0.78 and 0.73, re-spectively, corresponding to ?substantial agreement?
(Landis and Koch, 1977).
This relatively high levelof agreement suggests that the notion of lexical andlexical-syntactic entailment we propose are indeedwell-defined.Finally, in order to establish statistics from the an-notations, the annotators discussed all the examplesthey disagreed on and produced a final joint deci-sion.3.2 Evaluating the different levels of entailmentL LSTrue positive (118) 52 59False positive (122) 36 10Recall 44% 50%Precision 59% 86%F1 0.5 0.63Accuracy 58% 71%Table 2: Results per level of entailment.Table 2 summarizes the results obtained from ourannotated dataset for both lexical (L) and lexical-syntactic (LS) levels.
Taking a ?system?-orientedperspective, the annotations at each level can beviewed as the classifications made by an idealizedsystem that includes a perfect implementation of theinference mechanisms in that level.
The first two58rows show for each level how the cases, which wererecognized as positive by this level (i.e.
the entail-ment holds), are distributed between ?true positive?(i.e.
positive according to the gold standard) and?false positive?
(negative according to the gold stan-dard).
The total number of positive and negativepairs in the dataset is reported in parentheses.
Therest of the table details recall, precision, F1 and ac-curacy.The distribution of the examples in the RTE test-set cannot be considered representative of a real-world distribution (especially because of the con-trolled balance between positive and negative exam-ples).
Thus, our statistics are not appropriate foraccurate prediction of application performance.
In-stead, we analyze how well these simplified modelsof entailment succeed in approximating ?real?
en-tailment, and how they compare with each other.The proportion between true and false positivecases at the lexical level indicates that the correla-tion between lexical match and entailment is quitelow, reflected in the low precision achieved by thislevel (only 59%).
This result can be partly attributedto the idiosyncracies of the RTE test-set: as reportedin (Dagan et al, 2005), samples with high lexicalmatch were found to be biased towards the negativeside.
Interestingly, our measured accuracy correlateswell with the performance of systems at the PAS-CAL RTE Workshop, where the highest reported ac-curacy of a lexical system is 0.586 (Dagan et al,2005).As one can expect, adding syntax considerably re-duces the number of false positives - from 36 to only10.
Surprisingly, at the same time the number of truepositive cases grows from 52 to 59, and correspond-ingly, precision rise to 86%.
Interestingly, neitherthe lexical nor the lexical-syntactic level are able tocover more than half of the positive cases (e.g.
ex-ample 1911 in Table 1).In order to better understand the differences be-tween the two levels, we next analyze the overlapbetween them, presented in Table 3.
Looking atTable 3(a), which contains only the positive cases,we see that many examples were recognized only byone of the levels.
This interesting phenomenon canbe explained on the one hand by lexical matches thatcould not be validated in the syntactic level, and onthe other hand by the use of paraphrases, which areLexical-SyntacticH ?
T H; TLexical H ?
T 38 14H; T 21 45(a) positive examplesLexical-SyntacticH ?
T H; TLexical H ?
T 7 29H; T 3 83(b) negative examplesTable 3: Correlation between the entailment lev-els.
(a) includes only the positive examples fromthe RTE dataset sample, and (b) includes only thenegative examples.introduced only in the lexical-syntactic level.
(e.g.example 322 in Table 1).This relatively symmetric situation changes as wemove to the negative cases, as shown in Table 3(b).By adding syntactic constraints, the lexical-syntacticlevel was able to fix 29 false positive errors, misclas-sified at the lexical level (as demonstrated in exam-ple 2127, Table 1), while introducing only 3 newfalse-positive errors.
This exemplifies the impor-tance of syntactic matching for precision.3.3 The contribution of various inferencemechanismsInference Mechanism f 4R %Synonym 19 14.4% 16.1%Morphological 16 10.1% 13.5%Lexical World knowledge 12 8.4% 10.1%Hypernym 7 4.2% 5.9%Mernoym 1 0.8% 0.8%Entailment Paraphrases 37 26.2% 31.3%Syntactic transformations 22 16.9% 18.6%Coreference 10 5.0% 8.4%Table 4: The frequency (f ), contribution to recall(4R) and percentage (%), within the gold standardpositive examples, of the various inference mecha-nisms at each level, ordered by their significance.59In order to get a sense of the contribution of thevarious components at each level, statistics on the in-ference mechanisms that contributed to the coverageof the hypothesis by the text (either full or partial)were recorded by one annotator.
Only the positivecases in the gold standard were considered.For each inference mechanism we measured itsfrequency, its contribution to the recall of the relatedlevel and the percentage of cases in which it is re-quired for establishing entailment.
The latter alsotakes into account cases where only partial cover-age could be achieved, and thus indicates the signif-icance of each inference mechanism for any entail-ment system, regardless of the models presented inthis paper.
The results are summarized in Table 4.From Table 4 it stands that paraphrases are themost notable contributors to recall.
This result in-dicates the importance of paraphrases to the en-tailment task and the need for large-scale para-phrase collections.
Syntactic transformations arealso shown to contribute considerably, indicating theneed for collections of syntactic transformations aswell.
In that perspective, we propose our annota-tion framework as means for evaluating collectionsof paraphrases or syntactic transformations in termsof recall.Finally, we note that the co-reference moderatecontribution can be partly attributed to the idiosyn-cracies of the RTE test-set: the annotators wereguided to replace anaphors with the appropriate ref-erence, as reported in (Dagan et al, 2005).4 ConclusionsIn this paper we presented the definition of two en-tailment models, Lexical and Lexical-Syntactic, andanalyzed their performance manually.
Our experi-ment shows that the lexical-syntactic level outper-forms the lexical level in all measured aspects.
Fur-thermore, paraphrases and syntactic transformationsemerged as the main contributors to recall.
Theseresults suggest that a lexical-syntactic frameworkis a promising step towards a complete entailmentmodel.Beyond these empirical findings we suggest thatthe presented methodology can be used genericallyto annotate and analyze entailment datasets.In future work, it would be interesting to analyzehigher levels of entailment, such as logical inferenceand deep semantic understanding of the text.AcknowledgementsWe would like to thank Ido Dagan for helpful discus-sions and for his scientific supervision.
This workwas supported in part by the IST Programme of theEuropean Community, under the PASCAL Networkof Excellence, IST-2002-506778.
This publicationonly reflects the authors?
views.ReferencesAlina Andreevskaia, Zhuoyan Li and Sabine Bergler.2005.
Can Shallow Predicate Argument StructuresDetermine Entailment?.
In Proceedings of PascalChallenge Workshop on Recognizing Textual Entail-ment, 2005.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of HLT-NAACL2003.
pages 16-23, Edmonton, Canada.Ido Dagan, Bernardo Magnini and Oren Glickman.
2005.The PASCAL Recognising Textual Entailment Chal-lenge.
In Proceedings of Pascal Challenge Workshopon Recognizing Textual Entailment, 2005.J.R.
Landis and G.G.
Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33:159-174.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for Question Answering.
Natural LanguageEngineering, 7(4):343-360.C.
Macleod, R. Grishman, A. Meyers, L. Barrett and R.Reeves.
1998.
Nomlex: A lexicon of nominalizations.In Proceedings of the 8th International Congress of theEuropean Association for Lexicography, 1998.
Lie`ge,Belgium: EURALEX.Yusuke Shinyama and Satoshi Sekine, Kiyoshi Sudo andRalph Grishman.
2002.
Automatic paraphrase acqui-sition from news articles.
In Proceedings of HumanLanguage Technology Conference (HLT 2002).
SanDiego, USA.Idan Szpektor, Hristo Tanev, Ido Dagan and Bonnaven-tura Coppola.
2004.
Scaling Web-based Acquistionof Entailment Relations.
In Proceedings of EMNLP2004.Lucy Vanderwende, Deborah Coughlin and Bill Dolan.2005.
What Syntax Contribute in Entailment Task.
InProceedings of Pascal Challenge Workshop on Recog-nizing Textual Entailment, 2005.60
