Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 82?90,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsGenERRate: Generating Errors for Use in Grammatical Error DetectionJennifer FosterNational Centre for Language TechnologySchool of ComputingDublin City University, Irelandjfoster@computing.dcu.ie?istein E. AndersenComputer LaboratoryUniversity of CambridgeUnited Kingdomoa223@cam.ac.ukAbstractThis paper explores the issue of automaticallygenerated ungrammatical data and its use inerror detection, with a focus on the task ofclassifying a sentence as grammatical or un-grammatical.
We present an error generationtool called GenERRate and show how Gen-ERRate can be used to improve the perfor-mance of a classifier on learner data.
We de-scribe initial attempts to replicate CambridgeLearner Corpus errors using GenERRate.1 IntroductionIn recent years automatically generated ungrammat-ical data has been used in the training and evalu-ation of error detection systems, in evaluating therobustness of NLP tools and as negative evidencein unsupervised learning.
The main advantage ofusing such artificial data is that it is cheap to pro-duce.
However, it is of little use if it is not a real-istic model of the naturally occurring and expensivedata that it is designed to replace.
In this paper weexplore the issues involved in generating syntheticdata and present a tool called GenERRate which canbe used to produce many different kinds of syntacti-cally noisy data.
We use the tool in two experimentsin which we attempt to train classifiers to distinguishbetween grammatical and ungrammatical sentences.In the first experiment, we show how GenERRatecan be used to improve the performance of an ex-isting classifier on sentences from a learner corpusof transcribed spoken utterances.
In the second ex-periment we try to produce a synthetic error corpusthat is inspired by the Cambridge Learner Corpus(CLC)1, and we evaluate the difference between aclassifier?s performance when trained on this dataand its performance when trained on original CLCmaterial.
The results of both experiments providepointers on how to improve GenERRate, as well ashighlighting some of the challenges associated withautomatically generating negative evidence.The paper is organised as follows: In Section 2,we discuss the reasons why artificial ungrammaticaldata has been used in NLP and we survey its usein the field, focussing mainly on grammatical errordetection.
Section 3 contains a description of theGenERRate tool.
The two classification experimentswhich use GenERRate are described in Section 4.Problematic issues are discussed in Section 5 andavenues for future work in Section 6.2 Background2.1 Why artifical error data is usefulBefore pointing out the benefit of using artificialnegative evidence in grammatical error detection, itis worth reminding ourselves of the benefits of em-ploying negative evidence, be it artificial or natu-rally occurring.
By grammatical error detection, wemean either the task of distinguishing the grammat-ical from the ungrammatical at the sentence levelor more local targeted error detection, involving theidentification, and possibly also correction, of par-ticular types of errors.
Distinguishing grammati-cal utterances from ungrammatical ones involves theuse of a binary classifier or a grammaticality scor-1http://www.cambridge.org/elt/corpus/learner_corpus2.htm82ing model.
Examples are Andersen (2006; 2007),Okanohara and Tsujii (2007), Sun et al (2007) andWagner et al (2007).
In targeted error detection,the focus is on identifying the common errors madeeither by language learners or native speakers (de-pending on the application).
For ESL applications,this includes the detection of errors involving ar-ticles (Han et al, 2006; De Felice and Pulman,2008; Gamon et al, 2008), prepositions (De Feliceand Pulman, 2008; Gamon et al, 2008; Tetreaultand Chodorow, 2008), verb forms (Lee and Seneff,2008b), mass/count noun confusions (Brockett et al,2006) and word order (Metcalf and Meurers, 2006).The presence of a pattern in a corpus of well-formed language is positive evidence that the pat-tern is well-formed.
The presence of a pattern in ancorpus of ill-formed language is negative evidencethat the pattern is erroneous.
Discriminative tech-niques usually lead to more accurate systems thanthose based on one class alone.
The use of the twotypes of evidence can be seen at work in the systemdescribed by Lee and Seneff (2008b): Verb phrasesare parsed and their parse trees are examined.
Ifthe parse trees resemble the ?disturbed?
trees thatstatistical parsers typically produce when an incor-rect verb form is used, the verb phrase is consid-ered a likely candidate for correction.
However, toavoid overcorrection, positive evidence in the formof Google n-gram statistics is also employed: a cor-rection is only applied if its n-gram frequency ishigher than that of the original uncorrected n-gram.The ideal situation for a grammatical error de-tection system is one where a large amount of la-belled positive and negative evidence is available.Depending on the aims of the system, this labellingcan range from simply marking a sentence as un-grammatical to a detailed description of the erroralong with a correction.
If an error detection sys-tem employs machine learning, the performance ofthe system will improve as the training set size in-creases (up to a certain point).
For systems whichemploy learning algorithms with large feature sets(e.g.
maximum entropy, support vector machines),the size of the training set is particularly importantso that overfitting is avoided.
The collection of alarge corpus of ungrammatical data requires a gooddeal of manual effort.
Even if the annotation onlyinvolves marking the sentence as correct/incorrect,it still requires that the sentence be read and a gram-maticality judgement applied to it.
If more detailedannotation is applied, the process takes even longer.Some substantially-sized annotated error corpora doexist, e.g.
the Cambridge Learner Corpus, but theseare not freely available.One way around this problem of lack of availabil-ity of suitably large error-annotated corpora is to in-troduce errors into sentences automatically.
In orderfor the resulting error corpus to be useful in an errordetection system, the errors that are introduced needto resemble those that the system aims to detect.Thus, the process is not without some manual effort:knowing what kind of errors to introduce requiresthe inspection of real error data, a process similarto error annotation.
Once the error types have beenspecified though, the process is fully automatic andallows large error corpora to be compiled.
If the setof well-formed sentences into which the errors areintroduced is large and varied enough, it is possi-ble that this will result in ungrammatical sentencestructures which learners produce but which havenot yet been recorded in the smaller naturally occur-ring learner corpora.
To put it another way, the sametype of error will appear in lexically and syntacti-cally varied contexts, which is potentially advanta-geous when training a classifier.2.2 Where artificial error data has been usedArtificial errors have been employed previously intargeted error detection.
Sjo?bergh and Knutsson(2005) introduce split compound errors and word or-der errors into Swedish texts and use the resultingartificial data to train their error detection system.These two particular error types are chosen becausethey are frequent errors amongst non-native Swedishspeakers whose first language does not contain com-pounds or has a fixed word order.
They compare theresulting system to three Swedish grammar check-ers, and find that their system has higher recall at theexpense of lower precision.
Brockett et al (2006)introduce errors involving mass/count noun confu-sions into English newswire text and then use the re-sulting parallel corpus to train a phrasal SMT systemto perform error correction.
Lee and Seneff (2008b)automatically introduce verb form errors (subject?verb agreement errors, complementation errors anderrors in a main verb after an auxiliary) into well-83formed text, parse the resulting text and examine theparse trees produced.Both Okanohara and Tsujii (2007) and Wagner etal.
(2007) attempt to learn a model which discrimi-nates between grammatical and ungrammatical sen-tences, and both use synthetic negative data whichis obtained by distorting sentences from the BritishNational Corpus (BNC) (Burnard, 2000).
The meth-ods used to distort the BNC sentences are, however,quite different.
Okanohara and Tsujii (2007) gener-ate ill-formed sentences by sampling a probabilisticlanguage model and end up with ?pseudo-negative?examples which resemble machine translation out-put more than they do learner texts.
Indeed, ma-chine translation is one of the applications of theirresulting discriminative language model.
Wagneret al (2007) introduce grammatical errors of thefollowing four types into BNC sentences: context-sensitive spelling errors, agreement errors, errors in-volving a missing word and errors involving an extraword.
All four types are considered equally likelyand the resulting synthetic corpus contains errorsthat look like the kind of slips that would be madeby native speakers (e.g.
repeated adjacent words) aswell as errors that resemble learner errors (e.g.
miss-ing articles).
Wagner et al (2009) report a drop inaccuracy for their classification methods when ap-plied to real learner texts as opposed to held-out syn-thetic test data, reinforcing the earlier point that ar-tificial errors need to be tailored for the task at hand(we return to this in Section 4.1).Artificial error data has also proven useful inthe automatic evaluation of error detection systems.Bigert (2004) describes how a tool called Missplelis used to generate context-sensitive spelling errorswhich are then used to evaluate a context-sensitivespelling error detection system.
The performanceof general-purpose NLP tools such as part-of-speechtaggers and parsers in the face of noisy ungrammat-ical data has been automatically evaluated using ar-tificial error data.
Since the features of machine-learned error detectors are often part-of-speech n-grams or word?word dependencies extracted fromparser output (De Felice and Pulman, 2008, for ex-ample), it is important to understand how part-of-speech taggers and parsers react to particular gram-matical errors.
Bigert et al (2005) introduce artifi-cial context-sensitive spelling errors into error-freeSwedish text and then evaluate parsers and a part-of-speech tagger on this text using their performanceon the error-free text as a reference.
Similarly, Fos-ter (2007) investigates the effect of common Englishgrammatical errors on two widely-used statisticalparsers using distorted treebank trees as references.The procedure used by Wagner et al (2007; 2009) isused to introduce errors into the treebank sentences.Finally, negative evidence in the form of automat-ically distorted sentences has been used in unsuper-vised learning.
Smith and Eisner (2005a; 2005b)generate negative evidence for their contrastive es-timation method by moving or removing a word in asentence.
Since the aim of this work is not to detectgrammatical errors, there is no requirement to gener-ate the kind of negative evidence that might actuallybe produced by either native or non-native speakersof a language.
The negative examples are used toguide the unsupervised learning of a part-of-speechtagger and a dependency grammar.We can conclude from this survey that syntheticerror data is useful in a variety of NLP applications,including error detection and evaluation of error de-tectors.
In Section 3, we describe an automatic errorgeneration tool, which has a modular design and isflexible enough to accommodate the generation ofthe various types of synthetic data described above.3 Error Generation ToolGenERRate is an error generation tool which ac-cepts as input a corpus and an error analysis fileconsisting of a list of errors and produces an error-tagged corpus of syntactically ill-formed sentences.The sentences in the input corpus are assumed to begrammatically well-formed.
GenERRate is imple-mented in Java and will be made available to down-load for use by other researchers.23.1 Supported Error TypesError types are defined in terms of their corrections,that is, in terms of the operations (insert, delete, sub-stitute and move) that are applied to a well-formedsentence to make it ill-formed.
As well as beinga popular classification scheme in the field of er-ror analysis (James, 1998), it has the advantage of2http://www.computing.dcu.ie/?jfoster/resources/generrate.html84being theory-neutral.
This is important in this con-text since it is hoped that GenERRate will be usedto create negative evidence of various types, be itL2-like grammatical errors, native speaker slips ormore random syntactic noise.
It is hoped that Gen-ERRate will be easy to use for anyone working inlinguistics, applied linguistics, language teaching orcomputational linguistics.The inheritance hierarchy in Fig.
1 shows the er-ror types that are supported by GenERRate.
Webriefly describe each error type.Errors generated by removing a word?
DeletionError: Generated by selecting a wordat random from the sentence and removing it.?
DeletionPOSError: Extends DeletionError byallowing a specific POS to be specified.?
DeletionPOSWhereError: Extends Deletion-POSError by allowing left and/or right context(POS tag or start/end) to be specified.Errors generated by inserting a word?
InsertionError: Insert a random word at a ran-dom position.
The word is chosen either fromthe sentence itself or from a word list, and thischoice is also random.?
InsertionFromFileOrSentenceError: Thisdiffers from the InsertionError in that the de-cision of whether to use the sentence itself or aword list is not made at random but supplied inthe error type specification.?
InsertionPOSError: Extends InsertionFrom-FileOrSentenceError by allowing the POS ofthe new word to be specified.?
InsertionPOSWhereError: Analogous to theDeletionPOSWhereError, this extends Inser-tionPOSError by allowing left and/or right con-text to be specified.Errors generated by moving a word?
MoveError: Generated by randomly selectinga word in the sentence and moving it to anotherposition, randomly chosen, in the sentence.?
MovePOSError: A word tagged with thespecified POS is randomly chosen and movedto a randomly chosen position in the sentence.?
MovePOSWhereError: Extends Move-POSError by allowing the change in positionsubst,word,an,a,0.2subst,NNS,NN,0.4subst,VBG,TO,0.2delete,DT,0.1move,RB,left,1,0.1Figure 2: GenERRate Toy Error Analysis Fileto be specified in terms of direction andnumber of words.Errors generated by substituting a word?
SubstError: Replace a random word by aword chosen at random from a word list.?
SubstWordConfusionError: Extends Sub-stError by allowing the POS to be specified(same POS for both words).?
SubstWordConfusionNewPOSError: Simi-lar to SubstWordConfusionError, but allowsdifferent POSs to be specified.?
SubstSpecificWordConfusionError:Replace a specific word with another(e.g.
be/have).?
SubstWrongFormError: Replace a wordwith a different form of the same word.
Thefollowing changes are currently supported:noun number (e.g.
word/words), verb number(write/writes), verb form (writing/written), ad-jective form (big/bigger) and adjective/adverb(quick/quickly).
Note that this is the only er-ror type which is language-specific.
At the mo-ment, only English is supported.3.2 Input CorpusThe corpus that is supplied as input to GenERRatemust be split into sentences.
It does not have to bepart-of-speech tagged, but it will not be possible togenerate many of the errors if it is not.
GenERRatehas been tested using two part-of-speech tagsets,the Penn Treebank tagset (Santorini, 1991) and theCLAWS tagset (Garside et al, 1987).3.3 Error Analysis FileThe error analysis file specifies the errors that Gen-ERRate should attempt to insert into the sentencesin the input corpus.
A toy example with the Penntagset is shown in Fig.
2.
The first line is an instanceof a SubstSpecificWordConfusion error.
The second85ErrorDeletionDeletionPOSDeletionPOSWhereInsertionInsertionFromFileOrSentenceInsertionPOSInsertionPOSWhereMoveMovePOSMovePOSWhereSubstSubstWordConfusionSubstWordConfusionNewPOS SubstSpecificWordConfusionSubstWrongFormFigure 1: GenERRate Error Typesand third are instances of the SubstWrongFormEr-ror type.
The fourth is a DeletionPOSError, and thefifth is a MovePOSWhereError.
The number in thefinal column specifies the desired proportion of theparticular error type in the output corpus and is op-tional.
However, if it is present for one error type, itmust be present for all.
The overall size of the out-put corpus is supplied as a parameter when runningGenERRate.3.4 Error GenerationWhen frequency information is not supplied in theerror analysis file, GenERRate iterates through eacherror in the error analysis file and each sentence inthe input corpus, tries to insert an error of this typeinto the sentence and writes the resulting sentenceto the output file together with a description of theerror.
GenERRate includes an option to write thesentences into which an error could not be insertedand the reason for the failure to a log file.
When theerror analysis file does include frequency informa-tion, a slightly different algorithm is used: for eacherror, GenERRate selects sentences from the inputfile and attempts to generate an instance of that erroruntil the desired number of errors has been producedor all sentences have been tried.4 Classification ExperimentsWe describe two experiments which involve theuse of GenERRate in a binary classification taskin which the classifiers attempt to distinguish be-tween grammatically well-formed and ill-formedsentences or, more precisely, to distinguish betweensentences in learner corpora which have been anno-tated as erroneous and their corrected counterparts.In the first experiment we use GenERRate to cre-ate ungrammatical training data using informationabout error types gleaned from a subset of a corpusof transcribed spoken utterances produced by ESLlearners in a classroom environment.
The classifieris one of those described in Wagner et al (2007).In the second experiment we try to generate a CLC-inspired error corpus and we use one of the simplestclassifiers described in Andersen (2006).
Our aimis not to improve classification performance, but totest the GenERRate tool, to demonstrate how it canbe used and to investigate differences between syn-thetic and naturally occurring datasets.4.1 Experiments with a Spoken LanguageLearner CorpusWagner et al (2009) train various classifiers todistinguish between BNC sentences and artificiallyproduced ungrammatical versions of BNC sentences(see ?2).
They report a significant drop in accuracywhen they apply these classifiers to real learner data,including the sentences in a corpus of transcribedspoken utterances.
The aim of this experiment is toinvestigate to what extent this loss in accuracy canbe reduced by using GenERRate to produce a morerealistic set of ungrammatical training examples.The spoken language learner corpus contains over4,000 transcribed spoken sentences which were pro-duced by learners of English of all levels and witha variety of L1s.
The sentences were produced ina classroom setting and transcribed by the teacher.The transcriptions were verified by the students.
All86of the utterances have been marked as erroneous.4.1.1 SetupA 200-sentence held-out section of the corpus isanalysed by hand and a GenERRate error analysisfile containing 89 errors is compiled.
The most fre-quent errors are those involving a change in noun orverb number or an article deletion.
GenERRate thenapplies this error analysis file to 440,930 BNC sen-tences resulting in the same size set of synthetic ex-amples (?new-ungram-BNC?).
Another set of syn-thetic sentences (?old-ungram-BNC?)
is producedfrom the same input using the error generation pro-cedure used by Wagner et al (2007; 2009).
Table 1shows examples from both sets.Two classifiers are then trained, one on the orig-inal BNC sentences and the old-ungram-BNC sen-tences, and the other on the original BNC sentencesand the new-ungram-BNC sentences.
Both classi-fiers are tested on 4,095 sentences from the spo-ken language corpus (excluding the held-out sec-tion).
310 of these sentences are corrected, resultingin a small set of grammatical test data.
The classi-fier used is the POS n-gram frequency classifier de-scribed in Wagner et al (2007).3 The features arethe frequencies of the least frequent n-grams (2?7)in the input sentence.
The BNC (excluding thosesentences that are used as training data) is used asreference data to compute the frequencies.
Learningis carried out using the Weka implementation of theJ48 decision tree algorithm.44.1.2 ResultsThe results of the experiment are displayed in Ta-ble 2.
The evaluation measures used are precision,recall, total accuracy and accuracy on the grammat-ical side of the test data.
Recall is the same as accu-racy on the ungrammatical side of the test data.The results are encouraging.
There is a signifi-cant increase in accuracy when we train on the new-ungram-BNC set instead of the old-ungram-BNCset.
This increase is on the ungrammatical side of3Wagner et al (2009) report accuracy figures in the range55?70% for their various classifiers (when tested on synthetictest data), but the best performance is obtained by combiningparser-output and n-gram POS frequency features using deci-sion trees in a voting scheme.4http://www.cs.waikato.ac.nz/ml/weka/the test data, i.e.
an increase in recall, demonstrat-ing that by analysing a small set of data from ourtest domain, we can automatically create more effec-tive training data.
This is useful in a scenario wherea small-to-medium-sized learner corpus is availablebut which is not large enough to be split into a train-ing/development/test set.
These results seem to indi-cate that reasonably useful training data can be cre-ated with minimum effort.
Of course, the accuracy isstill rather low but we suspect that some of this dif-ference can be explained by domain effects ?
thesentences in the training data are BNC written sen-tences (or distorted versions of them) whereas thesentences in the learner corpus are transcribed spo-ken utterances.
Re-running the experiments usingthe spoken language section of the BNC as trainingdata might yield better results.4.2 A CLC-Inspired CorpusWe investigate to what extent it is possible to cre-ate a large error corpus inspired by the CLC usingthe current version of GenERRate.
The CLC is a30-million-word corpus of learner English collectedfrom University of Cambridge ESOL exam papersat different levels.
Approximately 50% of the CLChas been annotated for errors and corrected.4.2.1 SetupWe attempt to use GenERRate to insert errorsinto corrected CLC sentences.
In order to dothis, we need to create a CLC-specific error anal-ysis file.
In contrast to the previous experiment,we do this automatically by extracting erroneousPOS trigrams from the error-annotated CLC sen-tences and encoding them as GenERRate errors.This results in approximately 13,000 errors of thefollowing types: DeletionPOSWhereError, Inser-tionPOSWhereError, MovePOSWhereError, Sub-stWordConfusionError, SubstWordConfusionNew-POSError, SubstSpecificWordConfusionError andSubstWrongFormError.
Frequencies are extracted,and errors occurring only once are excluded.Three classifiers are trained.
The first is trainedon corrected CLC sentences (the grammatical sec-tion of the training set) and original CLC sentences(the ungrammatical section).
The second classifieris trained on corrected CLC sentences and the sen-tences that are generated from the corrected CLC87Old-Ungram-BNC New-Ungram-BNCBiogas production production is growing rapidly Biogas productions is growing rapidlyEmil as courteous and helpful Emil courteous and was helpfulI knows what makes you tick I know what make you tickHe did n?t bother to lift his eyes from the task hand He did n?t bother lift his eyes from the task at handTable 1: Examples from two synthetic BNC setsTraining Data Precision Recall Accuracy Accuracy on GrammaticalBNC/old-ungram-BNC 95.5 37.0 39.8 76.8BNC/new-ungram-BNC 94.9 51.6 52.4 63.2Table 2: Spoken Language Learner Corpus Classification Experimentsentences using GenERRate (we call these ?faux-CLC?).
The third is trained on corrected CLC sen-tences and a 50/50 combination of CLC and faux-CLC sentences.
In all experiments, the grammat-ical section of the training data contains 438,150sentences and the ungrammatical section 454,337.The classifiers are tested on a held-out section ofthe CLC containing 43,639 corrected CLC sen-tences and 45,373 original CLC sentences.
To trainthe classifiers, the Mallet implementation of NaiveBayes is used.5 The features are word unigramsand bigrams, as well as part-of-speech unigrams, bi-grams and trigrams.
Andersen (2006) experimentedwith various learning algorithms and, taking into ac-count training time and performance, found NaiveBayes to be optimal.
The POS-tagging is carried outby the RASP system (Briscoe and Carroll, 2002).4.2.2 ResultsThe results of the CLC classification experimentare presented in Table 3.
There is a 6.2% drop inaccuracy when we move from training on originalCLC sentences to artificially generated sentences.This is somewhat disappointing since it means thatwe have not completely succeeded in replicating theCLC errors using GenERRate.
Most of the accu-racy drop is on the ungrammatical side, i.e.
the cor-rect/faux model classifies more incorrect CLC sen-tences as correct than the correct/incorrect model.This drop in accuracy occurs because some fre-quently occurring error types are not included in theerror analysis file.
One reason for the gap in cover-age is the failure of the part-of-speech tagset to makesome important distinctions.
The corrected CLC5http://mallet.cs.umass.edu/sentences which were used to generate the faux-CLC set were tagged with the CLAWS tagset, andalthough more fine-grained than the Penn tagset, itdoes not, for example, make a distinction betweenmass and count nouns, a common source of error.Another important reason for the drop in accuracyare the recurrent spelling errors which occur in theincorrect CLC test set but not in the faux-CLC testset.
It is promising, however, that much of the per-formance degradation is recovered when a mixtureof the two types of ungrammatical training data isused, suggesting that artificial data could be used toaugment naturally occurring training sets5 Limitations of GenERRateWe present three issues that make the task of gener-ating synthetic error data non-trivial.5.1 Sophistication of Input FormatThe experiments in ?4 highlight coverage issueswith GenERRate, some of which are due to the sim-plicity of the supported error types.
When linguis-tic context is supplied for deletion or insertion er-rors, it takes the form of the POS of the words im-mediately to the left and/or right of the target word.Lee and Seneff (2008a) analysed preposition errorsmade by Japanese learners of English and found thata greater proportion of errors in argument preposi-tional phrases (look at him) involved a deletion thanthose in adjunct PPs (came at night).
The only wayfor such a distinction to be encoded in a GenERRateerror analysis file is to allow parsed input to be ac-cepted.
This brings with it the problem, however,that parsers are less accurate than POS-taggers.
An-other possible improvement would be to make use88Training Data Precision Recall Accuracy Accuracy on GrammaticalHeld-Out Test DataCorrect/Incorrect CLC 69.7 42.6 61.3 80.8Correct/Faux CLC 62.0 30.7 55.1 80.5Correct/Incorrect+Faux CLC 69.7 38.2 60.0 82.7Table 3: CLC Classification Experimentof WordNet synsets in order to choose the new wordin substitution errors.5.2 Covert ErrorsA covert error is an error that results in a syntac-tically well-formed sentence with an interpretationdifferent from the intended one.
Covert errors are anatural phenomenon, occurring in real corpora.
Leeand Seneff (2008b) give the example I am preparingfor the exam which has been annotated as erroneousbecause, given its context, it is clear that the per-son meant to write I am prepared for the exam.
Theproblems lie in deciding what covert errors shouldbe handled by an error detection system and how tocreate synthetic data which gets the balance right.When to avoid: Covert errors can be producedby GenERRate as a result of the sparse linguisticcontext provided for an error in the error analysisfile.
An inspection of the new-ungram-BNC setshows that some error types are more likely to re-sult in covert errors.
An example is the SubstWrong-FormError when it is used to change a noun fromsingular to plural.
This results in the sentence Butthere was no sign of Benny?s father being changedto the well-formed but more implausible But therewas no sign of Benny?s fathers.
The next version ofGenERRate should include the option to change theform of a word in a certain context.When not to avoid: In the design of GenERRate,particularly in the design of the SubstWrongFormEr-ror type, the decision was made to exclude tense er-rors because they are likely to result in covert er-rors, e.g.
She walked home?
She walks home.
Butin doing so we also avoid generating examples likethis one from the spoken language learner corpus:When I was a high school student, I go to bed at oneo?clock.
These tense errors are common in L2 dataand their omission from the faux-CLC training setis one of the reasons why the performance of thismodel is inferior to the real-CLC model.5.3 More complex errorsThe learner corpora contain some errors that arecorrected by applying more than one transforma-tion.
Some are handled by the SubstWrongFormEr-ror type (I spend a long time to fish?I spend a longtime fishing) but some are not (She is one of reasonI became interested in English ?
She is one of thereasons I became interested in English).6 ConclusionWe have presented GenERRate, a tool for automati-cally introducing syntactic errors into sentences andshown how it can be useful for creating synthetictraining data to be used in grammatical error detec-tion research.
Although we have focussed on thebinary classification task, we also intend to test Gen-ERRate in targeted error detection.
Another avenuefor future work is to explore whether GenERRatecould be of use in the automatic generation of lan-guage test items (Chen et al, 2006, for example).Our immediate aim is to produce a new version ofGenERRate which tackles some of the coverage is-sues highlighted by our experiments.AcknowledgmentsThis paper reports on research supported by the Uni-versity of Cambridge ESOL Examinations.
We arevery grateful to Cambridge University Press for giv-ing us access to the Cambridge Learner Corpus andto James Hunter from Gonzaga College for sup-plying us with the spoken language learner corpus.We thank Ted Briscoe, Josef van Genabith, JoachimWagner and the reviewers for their very helpful sug-gestions.89References?istein E. Andersen.
2006.
Grammatical error detection.Master?s thesis, Cambridge University.
?istein E. Andersen.
2007.
Grammatical error detectionusing corpora and supervised learning.
In Ville Nurmiand Dmitry Sustretov, editors, Proceedings of the 12thStudent Session of the European Summer School forLogic, Language and Information, Dublin.Johnny Bigert, Jonas Sjo?bergh, Ola Knutsson, and Mag-nus Sahlgren.
2005.
Unsupervised evaluation ofparser robustness.
In Proceedings of the 6th CICling,Mexico City.Johnny Bigert.
2004.
Probabilistic detection of context-sensitive spelling errors.
In Proceedings of the 4thLREC, Lisbon.Ted Briscoe and John Carroll.
2002.
Robust accuratestatistical annotation of general text.
In Proceedingsof the 3rd LREC, Las Palmas.Chris Brockett, William B. Dolan, and Michael Gamon.2006.
Correcting ESL errors using phrasal SMT tech-niques.
In Proceedings of the 21st COLING and the44th ACL, Sydney.Lou Burnard.
2000.
User reference guide for the BritishNational Corpus.
Technical report, Oxford UniversityComputing Services.Chia-Yin Chen, Liou Hsien-Chin, and Jason S. Chang.2006.
Fast ?
an automatic generation system forgrammar tests.
In Proceedings of the COLING/ACL2006 Interactive Presentation Sessions, Sydney.Rachele De Felice and Stephen G. Pulman.
2008.A classifier-based approach to preposition and deter-miner error correction in L2 English.
In Proceedingsof the 22nd COLING, Manchester.Jennifer Foster.
2007.
Treebanks gone bad: Parser evalu-ation and retraining using a treebank of ungrammaticalsentences.
International Journal on Document Analy-sis and Recognition, 10(3-4):129?145.Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-dre Klementiev, William B. Dolan, Dmitriy Belenko,and Lucy Vanderwende.
2008.
Using contextualspeller techniques and language modelling for ESL er-ror correction.
In Proceedings of the 3rd IJCNLP, Hy-derabad.Roger Garside, Geoffrey Leech, and Geoffrey Sampson,editors.
1987.
The Computational Analysis of En-glish: a Corpus-Based Approach.
Longman, London.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineering,12(2):115?129.Carl James.
1998.
Errors in Language Learning andUse: Exploring Error Analysis.
Addison WesleyLongman.John Lee and Stephanie Seneff.
2008a.
An analysis ofgrammatical errors in non-native speech in English.
InProceedings of the 2008 Spoken Language TechnologyWorkshop, Goa.John Lee and Stephanie Seneff.
2008b.
Correcting mis-use of verb forms.
In Proceedings of the 46th ACL,Columbus.Vanessa Metcalf and Detmar Meurers.
2006.
Towardsa treatment of word order errors: When to use deepprocessing ?
and when not to.
Presentation at the NLPin CALL Workshop, CALICO 2006.Daisuke Okanohara and Jun?ichi Tsujii.
2007.
Adiscriminative language model with pseudo-negativesamples.
In Proceedings of the 45th ACL, Prague.Beatrice Santorini.
1991.
Part-of-speech tagging guide-lines for the Penn Treebank project.
Technical report,University of Pennsylvania, Philadelphia, PA.Jonas Sjo?bergh and Ola Knutsson.
2005.
Faking errors toavoid making errors.
In Proceedings of RANLP 2005,Borovets.Noah A. Smith and Jason Eisner.
2005a.
ContrastiveEstimation: Training log-linear models on unlabeleddata.
In Proceedings of the 43rd ACL, Ann Arbor.Noah A. Smith and Jason Eisner.
2005b.
Guiding unsu-pervised grammar induction using contrastive estima-tion.
In Proceedings of the IJCAI Workshop on Gram-matical Inference Applications, Edinburgh.Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,Zhongyang Xiong, John Lee, and Chin-Yew Lin.2007.
Detecting erroneous sentences using automat-ically mined sequential patterns.
In Proceedings of the45rd ACL, Prague.Joel R. Tetreault and Martin Chodorow.
2008.
The upsand downs of preposition error detection in ESL writ-ing.
In Proceedings of the 22nd COLING, Manchester.Joachim Wagner, Jennifer Foster, and Josef van Gen-abith.
2007.
A comparative evaluation of deep andshallow approaches to the automatic detection of com-mon grammatical errors.
In Proceedings of the jointEMNLP/CoNLL, Prague.Joachim Wagner, Jennifer Foster, and Josef van Genabith.2009.
Judging grammaticality: Experiments in sen-tence classification.
CALICO Journal.
Special Issueon the 2008 Automatic Analysis of Learner LanguageCALICO Workshop.
To Appear.90
