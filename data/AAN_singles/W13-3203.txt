Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20?29,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsA Structured Distributional Semantic Model : Integrating Structure withSemanticsKartik Goyal?
Sujay Kumar Jauhar?
Huiying Li?Mrinmaya Sachan?
Shashank Srivastava?
Eduard HovyLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon University{kartikgo,sjauhar,huiyingl,mrinmays,shashans,hovy}@cs.cmu.eduAbstractIn this paper we present a novel approach(SDSM) that incorporates structure in dis-tributional semantics.
SDSM representsmeaning as relation specific distributionsover syntactic neighborhoods.
We em-pirically show that the model can effec-tively represent the semantics of singlewords and provides significant advantageswhen dealing with phrasal units that in-volve word composition.
In particular, wedemonstrate that our model outperformsboth state-of-the-art window-based wordembeddings as well as simple approachesfor composing distributional semantic rep-resentations on an artificial task of verbsense disambiguation and a real-world ap-plication of judging event coreference.1 IntroductionWith the advent of statistical methods for NLP,Distributional Semantic Models (DSMs) haveemerged as powerful method for representingword semantics.
In particular, the distributionalvector formalism, which represents meaning by adistribution over neighboring words, has gainedthe most popularity.DSMs are widely used in information re-trieval (Manning et al 2008), question answer-ing (Tellex et al 2003), semantic similarity com-putation (Wong and Raghavan, 1984; McCarthyand Carroll, 2003), automated dictionary building(Curran, 2003), automated essay grading (Lan-dauer and Dutnais, 1997), word-sense discrimina-tion and disambiguation (McCarthy et al 2004;?
*Equally contributing authorsSch?tze, 1998), selectional preference model-ing (Erk, 2007) and identification of translationequivalents (Hjelm, 2007).Systems that use DSMs implicitly make a bagof words assumption: that the meaning of a phrasecan be reasonably estimated from the meaning ofits constituents.
However, semantics in naturallanguage is a compositional phenomenon, encom-passing interactions between syntactic structures,and the meaning of lexical constituents.
It fol-lows that the DSM formalism lends itself poorlyto composition since it implicitly disregards syn-tactic structure.
For instance, the distributions for?Lincoln?, ?Booth?, and ?killed?
when mergedproduce the same result regardless of whether theinput is ?Booth killed Lincoln?
or ?Lincoln killedBooth?.
As suggested by Pantel and Lin (2000)and others, modeling the distribution over prefer-ential attachments for each syntactic relation sep-arately can yield greater expressive power.Attempts have been made to model linguisticcomposition of individual word vectors (Mitchelland Lapata, 2009), as well as remedy the inher-ent failings of the standard distributional approach(Erk and Pad?, 2008).
The results show vary-ing degrees of efficacy, but have largely failed tomodel deeper lexical semantics or compositionalexpectations of words and word combinations.In this paper we propose an extension to thetraditional DSM model that explicitly preservesstructural information and permits the approxima-tion of distributional expectation over dependencyrelations.
We extend the generic DSM model byrepresenting a word as distributions over relation-specific syntactic neighborhoods.
One can thinkof the Structured DSM (SDSM) representationof a word/phrase as several vectors defined overthe same vocabulary, each vector representing the20word?s selectional preferences for a different syn-tactic argument.
We argue that this represen-tation captures individual word semantics effec-tively, and is better able to express the semanticsof composed units.The overarching theme of our framework ofevaluation is to explore the semantic space of theSDSM.
We do this by measuring its ability to dis-criminate between varying surface forms of thesame underlying concept.
We perform the follow-ing set of experiments to evaluate its expressivepower, and conclude the following:1.
Experiments with single words on similar-ity scoring and substitute selection: SDSMperforms at par with window-based distribu-tional vectors.2.
Experiments with phrasal units on two-wordcomposition: state-of-the-art results are pro-duced on the dataset from Mitchell and Lap-ata (2008) in terms of correlation with humanjudgment.3.
Experiments with larger structures on thetask of judging event coreferentiality: SDSMshows superior performance over state-of-the-art window-based word embeddings, andsimple models for composing distributionalsemantic representations.2 Related WorkDistributional Semantic Models are based on theintuition that ?a word is characterized by the com-pany it keeps?
(Firth, 1957).
While DSMs havebeen very successful on a variety of NLP tasks,they are generally considered inappropriate fordeeper semantics because they lack the ability tomodel composition, modifiers or negation.Recently, there has been a surge in studies tomodel a stronger form of semantics by phrasingthe problem of DSM compositionality as one ofvector composition.
These techniques derive themeaning of the combination of two words a andb by a single vector c = f(a, b).
Mitchell andLapata (2008) propose a framework to define thecomposition c = f(a, b, r,K) where r is the re-lation between a and b, and K is the additionalknowledge used to define composition.While the framework is quite general, mostmodels in the literature tend to disregard K andr and are generally restricted to component-wiseaddition and multiplication on the vectors to becomposed, with slight variations.
Dinu and Lap-ata (2010) and S?aghdha and Korhonen (2011) in-troduced a probabilistic model to represent wordmeanings by a latent variable model.
Subse-quently, other high-dimensional extensions byRudolph and Giesbrecht (2010), Baroni and Zam-parelli (2010) and Grefenstette et al(2011), re-gression models by Guevara (2010), and recursiveneural network based solutions by Socher et al(2012) and Collobert et al(2011) have been pro-posed.Pantel and Lin (2000) and Erk and Pad?
(2008)attempted to include syntactic context in distri-butional models.
However, their approaches donot explicitly construct phrase-level meaning fromwords which limits their applicability to real worldproblems.
A quasi-compositional approach wasalso attempted in Thater et al(2010) by a system-atic combination of first and second order contextvectors.
To the best of our knowledge the formu-lation of composition we propose is the first to ac-count for K and r within the general frameworkof composition c = f(a, b, r,K).3 Structured Distributional SemanticsIn this section, we describe our Structured Distri-butional Semantic framework in detail.
We firstbuild a large knowledge base from sample englishtexts and use it to represent basic lexical units.Next, we describe a technique to obtain the repre-sentation for larger units by composing their con-stituents.3.1 The PropStoreTo build a lexicon of SDSM representations fora given vocabulary we construct a propositionknowledge base (the PropStore) by processing thetext of Simple English Wikipedia through a de-pendency parser.
Dependency arcs are stored as3-tuples of the form ?w1, r, w2?, denoting occur-rences of words w1 and word w2 related by thesyntactic dependency r. We also store sentenceidentifiers for each triple for reasons describedlater.
In addition to the words?
surface-forms, thePropStore also stores their POS tags, lemmas, andWordnet supersenses.The PropStore can be used to query for pre-ferred expectations of words, supersenses, re-lations, etc., around a given word.
In theexample in Figure 1, the query (SST(W1)21Figure 1: Sample sentences & triples= verb.consumption, ?, dobj) i.e., ?what isconsumed?, might return expectations [pasta:1,spaghetti:1, mice:1 .
.
.
].
In our implementation,the relations and POS tags are obtained using theFanseparser (Tratz and Hovy, 2011), supersensetags using sst-light (Ciaramita and Altun, 2006),and lemmas are obtained from Wordnet (Miller,1995).3.2 Building the RepresentationNext, we describe a method to represent lexicalentries as structured distributional matrices usingthe PropStore.The canonical form of a concept C (word,phrase etc.)
in the SDSM framework is a matrixMC , whose entry MCij is a list of sentence identi-fiers obtained by querying the PropStore for con-texts in which C appears in the syntactic neigh-borhood of the word j linked by the dependencyrelation i.
As with other distributional models inthe literature, the content of a cell is the frequencyof co-occurrence of its concept and word under thegiven relational constraint.This canonical matrix form can be interpretedin several different ways.
Each interpretation isbased on a different normalization scheme.1.
Row Norm: Each row of the matrix is inter-preted as a distribution over words that attachto the target concept with the given depen-dency relation.MCij =Mij?jMij?i2.
Full Norm: The entire matrix is interpretedas a distribution over the word-relation pairswhich can attach to the target concept.MCij =Mij?i,jMij?i, jFigure 2: Mimicking composition of two words3.
Collapsed Vector Norm: The columns ofthe matrix are collapsed to form a standardnormalized distributional vector trained ondependency relations rather than sliding win-dows.MCj =?iMij?i,jMij?j3.3 Mimicking CompositionalityFor representing intermediate multi-word phrases,we extend the above word-relation matrix sym-bolism in a bottom-up fashion.
The combina-tion hinges on the intuition that when lexical unitscombine to form a larger syntactically connectedphrase, the representation of the phrase is givenby its own distributional neighborhood within theembedded parse tree.
The distributional neighbor-hood of the net phrase can be computed using thePropStore given syntactic relations anchored on itsparts.
For the example in Figure 1, we can com-pose SST(w1) = Noun.person and Lemma(W1)= eat with relation ?nsubj?
to obtain expectationsaround ?people eat?
yielding [pasta:1, spaghetti:1. .
. ]
for the object relation ([dining room:2, restau-rant:1 .
.
.]
for the location relation, etc.)
(See Fig-ure 2).
Larger phrasal queries can be built to an-swer questions like ?What do people in China eatwith?
?, ?What do cows do?
?, etc.
All of this helps22us to account for both relation r and knowledgeKobtained from the PropStore within the composi-tional framework c = f(a, b, r,K).The general outline to obtain a composition oftwo words is given in Algorithm 1.
Here, wefirst determine the sentence indices where the twowords w1 and w2 occur with relation r. Then,we return the expectations around the two wordswithin these sentences.
Note that the entire algo-rithm can conveniently be written in the form ofdatabase queries to our PropStore.Algorithm 1 ComposePair(w1, r, w2)M1 ?
queryMatrix(w1)M2 ?
queryMatrix(w2)SentIDs?M1(r) ?M2(r)return ((M1?
SentIDs) ?
(M2?
SentIDs))Similar to the two-word composition process,given a parse subtree T of a phrase, we obtainits matrix representation of empirical counts overword-relation contexts.
This procedure is de-scribed in Algorithm 2.
Let the E = {e1 .
.
.
en}be the set of edges in T , ei = (wi1, ri, wi2)?i =1 .
.
.
n.Algorithm 2 ComposePhrase(T )SentIDs?
All Sentences in corpusfor i = 1?
n doMi1 ?
queryMatrix(wi1)Mi2 ?
queryMatrix(wi2)SentIDs?
SentIDs ?
(M1(ri) ?M2(ri))end forreturn ((M11?
SentIDs) ?
(M12?
SentIDs)?
?
?
?
(Mn1?
SentIDs) ?
(Mn2?
SentIDs))3.4 Tackling SparsityThe SDSM model reflects syntactic properties oflanguage through preferential filler constraints.But by distributing counts over a set of relationsthe resultant SDSM representation is compara-tively much sparser than the DSM representationfor the same word.
In this section we present someways to address this problem.3.4.1 Sparse Back-offThe first technique to tackle sparsity is to backoff to progressively more general levels of lin-guistic granularity when sparse matrix represen-tations for words or compositional units are en-countered or when the word or unit is not in thelexicon.
For example, the composition ?Balthazareats?
cannot be directly computed if the named en-tity ?Balthazar?
does not occur in the PropStore?sknowledge base.
In this case, a query for a su-persense substitute ?
?Noun.person eat?
?
can beissued instead.
When supersenses themselves failto provide numerically significant distributions forwords or word combinations, a second back-offstep involves querying for POS tags.
With coarserlevels of linguistic representation, the expressivepower of the distributions becomes diluted.
Butthis is often necessary to handle rare words.
Notethat this is an issue with DSMs too.3.4.2 DensificationIn addition to the back-off method, we also pro-pose a secondary method for ?densifying?
distri-butions.
A concept?s distribution is modified byusing words encountered in its syntactic neighbor-hood to infer counts for other semantically similarwords.
In other terms, given the matrix represen-tation of a concept, densification seeks to popu-late its null columns (which each represent a word-dimension in the structured distributional context)with values weighted by their scaled similarities towords (or effectively word-dimensions) that actu-ally occur in the syntactic neighborhood.For example, suppose the word ?play?
had an?nsubj?
preferential vector that contained the fol-lowing counts: [cat:4 ; Jane:2].
One might thenpopulate the column for ?dog?
in this vector witha count proportional to its similarity to the wordcat (say 0.8), thus resulting in the vector [cat:4 ;Jane:2 ; dog:3.2].
These counts could just as wellbe probability values or PMI associations (suitablynormalized).
In this manner, the k most similarword-dimensions can be densified for each wordthat actually occurs in a syntactic context.
As withsparse back-off, there is an inherent trade-off be-tween the degree of densification k and the expres-sive power of the resulting representation.3.4.3 Dimensionality ReductionThe final method tackles the problem of sparsityby reducing the representation to a dense low-dimensional word embedding using singular valuedecomposition (SVD).
In a typical term-documentmatrix, SVD finds a low-dimensional approxima-tion of the original matrix where columns becomelatent concepts while similarity structure betweenrows are preserved.
The PropStore, as described inSection 3.1, is an order-3 tensor with w1, w2 and23rel as its three axes.
We explore the following twopossibilities to perform dimensionality reductionusing SVD.Word-word matrix SVD.
In this experiment,we preserve the axes w1 and w2 and ignore the re-lational information.
Following the SVD regime (W = U?V T ) where ?
is a square diagonal ma-trix of k largest singular values, and U and V arem?
k and n?
k matrices respectively.
We adoptmatrixU as the compacted concept representation.Tensor SVD.
To remedy the relation-agnosticnature of the word-word SVD matrix represen-tation, we use tensor SVD (Vasilescu and Ter-zopoulos, 2002) to preserve the structural infor-mation.
The mode-n vectors of an order-N tensorA?RI1?I2?...
?IN are the In-dimensional vectorsobtained from A by varying index in while keep-ing other indices fixed.
The matrix formed by allthe mode-n vectors is a mode-n flattening of thetensor.
To obtain the compact representations ofconcepts we thus first apply mode w1 flatteningand then perform SVD on the resulting tensor.4 Single Word EvaluationIn this section we describe experiments and re-sults for judging the expressive power of the struc-tured distributional representation for individualwords.
We use a similarity scoring task and a lexi-cal substitute selection task for the purpose of thisevaluation.
We compare the SDSM representa-tion to standard window-based distributional vec-tors trained on the same corpus (Simple EnglishWikipedia).
We also experiment with differentnormalization techniques outlined in Section 3.2,which effectively lead to structured distributionalrepresentations with distinct interpretations.We experimented with various similarity met-rics and found that the normalized cityblock dis-tance metric provides the most stable results.CityBlock(X,Y ) =ArcTan(d(X,Y ))d(X,Y )d(X,Y ) =1|R|?r?Rd(Xr, Yr)Results in the rest of this section are thus reportedusing the normalized cityblock metric.
We alsoreport experimental results for the two methodsof alleviating sparsity discussed in Section 3.4,namely, densification and SVD.4.1 Similarity ScoringOn this task, the different semantic representationswere used to compute similarity scores betweentwo (out of context) words.
We used a datasetfrom Finkelstein et al(2002) for our experiments.It consists of 353 pairs of words along with an av-eraged similarity score on a scale of 1.0 to 10.0obtained from 13?16 human judges.4.2 Lexical Substitute SelectionIn the second task, the same set of semantic repre-sentations was used to produce a similarity rank-ing on the Turney (2002) ESL dataset.
This datasetcomprises 50 words that appear in a context (wediscarded the context in this experiment), alongwith 4 candidate lexical substitutions.
We eval-uate the semantic representations on the basis oftheir ability to discriminate the top-ranked candi-date.14.3 Results and DiscussionTable 1 summarizes the results for the window-based baseline and each of the structured distri-butional representations on both tasks.
It showsthat our representations for single words are com-petitive with window based distributional vectors.Densification in certain conditions improves ourresults, but no consistent pattern is discernible.This can be attributed to the trade-off between thegain from generalization and the noise introducedby semantic drift.Hence we resort to dimensionality reduction asan additional method of reducing sparsity.
Table2 gives correlation scores on the Finkelstein et al(2002) dataset when SVD is performed on the rep-resentations, as described in Section 3.4.3.
Wegive results when 100 and 500 principal compo-nents are preserved for both SVD techniques.These experiments suggest that though afflictedby sparsity, the proposed structured distributionalparadigm is competitive with window-based dis-tributional vectors.
In the following sections weshow that that the framework provides consid-erably greater power for modeling compositionwhen dealing with units consisting of more thanone word.1While we are aware of the standard lexical substitutioncorpus from McCarthy and Navigli (2007) we chose the onementioned above for its basic vocabulary, lower dependenceon context, and simpler evaluation framework.24Model Finklestein (Corr.)
ESL (% Acc.
)DSM 0.283 0.247Collapsed 0.260 0.178FullNorm 0.282 0.192RowNorm 0.236 0.264Densified RowNorm 0.259 0.267Table 1: Single Word EvaluationModel CorrelationmatSVD100 0.207matSVD500 0.221tenSVD100 0.267tenSVD500 0.315Table 2: Finklestein: Correlation using SVD5 Verb Sense Disambiguation usingCompositionIn this section, we examine how well our modelperforms composition on a pair of words.
Wederive the compositional semantic representationsfor word pairs from the M&L dataset (Mitchelland Lapata, 2008) and compare our performancewith M&L?s additive and multiplicative models ofcomposition.5.1 DatasetThe M&L dataset consists of polysemous intransi-tive verb and subject pairs that co-occur at least 50times in the BNC corpus.
Additionally two land-mark words are given for every polysemous verb,each corresponding to one of its senses.
The sub-ject nouns provide contextual disambiguation forthe senses of the verb.
For each [subject, verb,landmark] tuple, a human assigned score on a 7-point scale is provided, indicating the compatibil-ity of the landmark with the reference verb-subjpair.
For example, for the pair ?gun bomb?, land-mark ?thunder?
is more similar to the verb thanlandmark ?prosper?.
The corpus contains 120 tu-ples and altogether 3600 human judgments.
Re-liability of the human ratings is examined by cal-culating inter-annotator Spearman?s ?
correlationcoefficient.5.2 Experiment procedureFor each tuple in the dataset, we derive the com-posed word-pair matrix for the reference verb-subjpair based on the algorithm described in Section3.3 and query the single-word matrix for the land-mark word.
A few modifications are made to ad-just the algorithm for the current task:1.
In our formulation, the dependency relationneeds to be specified in order to composea pair of words.
Hence, we determine thefive most frequent relations between w1 andw2 by querying the PropStore.
We then usethe algorithm in Section 3.3 to compose theverb-subj word pair using these relations, re-sulting in five composed representations.2.
The word pairs in M&L corpus are ex-tracted from a parsed version of the BNC cor-pus, while our PropStore is built on SimpleWikipedia texts, whose vocabulary is signif-icantly different from that of the BNC cor-pus.
This causes null returns in our PropStorequeries, in which case we back-off to retriev-ing results for super-sense tags of both thewords.
Finally, the composed matrix and thelandmark matrix are compared against eachother by different matrix distance measures,which results in a similarity score.
For a [sub-ject, verb, landmark] tuple, we average thesimilarity scores yielded by the relations ob-tained in 1.The Spearman Correlation ?
between our sim-ilarity ratings and the ones assigned by humanjudges is computed over all the tuples.
Follow-ing M&L?s experiments, the inter-annotator agree-ment correlation coefficient serves an upper boundon the task.5.3 Results and DiscussionAs in Section 4, we choose the cityblock mea-sure as the similarity metric of choice.
Table 3shows the evaluation results for two word compo-sition.
Except for row normalization, both formsof normalization in the structured distributionalparadigm show significant improvement over theresults reported by M&L.
The results are statisti-cally significant at p-value = 0.004 and 0.001 forFull Norm and Collapsed Vector Norm, respec-tively.Model ?M&L combined 0.19Row Norm 0.134Full Norm 0.289Collapsed Vector Norm 0.259UpperBound 0.40Table 3: Two Word Composition EvaluationThese results validate our hypothesis that the in-tegration of structure into distributional semantics25as well as our framing of word composition to-gether outperform window-based representationsunder simplistic models of composition such asaddition and multiplication.
This finding is furtherre-enforced in the following experiments on eventcoreferentiality judgment.6 Event Coreference JudgmentGiven the SDSM formulation and assuming nosparsity constraints, it is possible to calculateSDSM matrices for composed concepts.
However,are these correct?
Intuitively, if they truly capturesemantics, the two SDSM matrix representationsfor ?Booth assassinated Lincoln?
and ?Booth shotLincoln with a gun" should be (almost) the same.To test this hypothesis we turn to the task of pre-dicting whether two event mentions are coreferentor not, even if their surface forms differ.While automated resolution of entity coref-erence has been an actively researched area(Haghighi and Klein, 2009; Stoyanov et al 2009;Raghunathan et al 2010), there has been rela-tively little work on event coreference resolution.Lee et al(2012) perform joint cross-documententity and event coreference resolution using thetwo-way feedback between events and their argu-ments.In this paper, however, we only consider coref-erentiality between pairs of events.
Formally,two event mentions generally refer to the sameevent when their respective actions, agents, pa-tients, locations, and times are (almost) the same.Given the non-compositional nature of determin-ing equality of locations and times, we representeach event mention by a triple E = (e, a, p) forthe event, agent, and patient.While linguistic theory of argument realiza-tion is a debated research area (Levin and Rap-paport Hovav, 2005; Goldberg, 2005), it is com-monly believed that event structure (Moens andSteedman, 1988) centralizes on the predicate,which governs and selects its role arguments(Jackendoff, 1987).
In the corpora we use forour experiments, most event mentions are verbs.However, when nominalized events are encoun-tered, we replace them by their verbal forms.
Weuse SRL Collobert et al(2011) to determine theagent and patient arguments of an event mention.When SRL fails to determine either role, its empir-ical substitutes are obtained by querying the Prop-Store for the most likely word expectations for therole.
The triple (e, a, p) is thus the compositionof the triples (a, relagent, e) and (p, relpatient, e),and hence a complex object.
To determine equal-ity of this complex composed representation wegenerate three levels of progressively simplifiedevent constituents for comparison:Level 1: Full Composition:Mfull = ComposePhrase(e, a, p).Level 2: Partial Composition:Mpart:EA = ComposePair(e, r, a)Mpart:EP = ComposePair(e, r, p).Level 3: No Composition:ME = queryMatrix(e)MA = queryMatrix(a)MP = queryMatrix(p).To judge coreference betweenevents E1 and E2, we compute pair-wise similarities Sim(M1full,M2full),Sim(M1part:EA,M2part:EA), etc., for eachlevel of the composed triple representation.
Fur-thermore, we vary the computation of similarityby considering different levels of granularity(lemma, SST), various choices of distance metric(Euclidean, Cityblock, Cosine), and score nor-malization techniques (Row-wise, Full, Columncollapsed).
This results in 159 similarity-basedfeatures for every pair of events, which are usedto train a classifier to make a binary decision forcoreferentiality.6.1 DatasetsWe evaluate our method on two datasets and com-pare it against four baselines, two of which usewindow based distributional vectors and two thatemploy weaker forms of composition.IC Event Coreference Corpus: The dataset(citation suppressed), drawn from 100 news arti-cles about violent events, contains manually cre-ated annotations for 2214 pairs of co-referentand non-coreferent events each.
Where available,events?
semantic role-fillers for agent and patientare annotated as well.
When missing, empiricalsubstitutes were obtained by querying the Prop-Store for the preferred word attachments.EventCorefBank (ECB) corpus: This corpus(Bejan and Harabagiu, 2010) of 482 documentsfrom Google News is clustered into 45 topics,with event coreference chains annotated over eachtopic.
The event mentions are enriched with se-mantic roles to obtain the canonical event struc-ture described above.
Positive instances are ob-26IC Corpus ECB CorpusPrec Rec F-1 Acc Prec Rec F-1 AccSDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834Table 4: Cross-validation Performance on IC and ECB datasettained by taking pairwise event mentions withineach chain, and negative instances are generatedfrom pairwise event mentions across chains, butwithin the same topic.
This results in 11039 posi-tive instances and 33459 negative instances.6.2 Baselines:To establish the efficacy of our model, we com-pare SDSM against a purely window-based base-line (DSM) trained on the same corpus.
In our ex-periments we set a window size of three words toeither side of the target.
We also compare SDSMagainst the window-based embeddings trained us-ing a recursive neural network (SENNA) (Col-lobert et al 2011) on both datsets.
SENNA em-beddings are state-of-the-art for many NLP tasks.The second baseline uses SENNA to generatelevel 3 similarity features for events?
individualwords (agent, patient and action).
As our finalset of baselines, we extend two simple techniquesproposed by Mitchell and Lapata (2008) that useelement-wise addition and multiplication opera-tors to perform composition.
The two baselinesthus obtained are AVC (element-wise addition)and MVC (element-wise multiplication).6.3 Results and Discussion:We experimented with a number of common clas-sifiers, and selected decision-trees (J48) as theygive the best classification accuracy.
Table 4 sum-marizes our results on both datasets.The results reveal that the SDSM model con-sistently outperforms DSM, SENNA embeddings,and the MVC and AVC models, both in termsof F-1 score and accuracy.
The IC corpus com-prises of domain specific texts, resulting in highlexical overlap between event mentions.
Hence,the scores on the IC corpus are consistently higherthan those on the ECB corpus.The improvements over DSM and SENNA em-beddings, support our hypothesis that syntax lendsgreater expressive power to distributional seman-tics in compositional configurations.
Furthermore,the increase in predictive accuracy over MVC andAVC shows that our formulation of compositionof two words based on the relation binding themyields a stronger form of composition than simpleadditive and multiplicative models.Next, we perform an ablation study to deter-mine the most predictive features for the task ofdetermining event coreferentiality.
The forwardselection procedure reveals that the most informa-tive attributes are the level 2 compositional fea-tures involving the agent and the action, as well astheir individual level 3 features.
This correspondsto the intuition that the agent and the action are theprincipal determiners for identifying events.
Fea-tures involving the patient and level 1 features areleast useful.
The latter involves full composition,resulting in sparse representations and hence havelow predictive power.7 Conclusion and Future WorkIn this paper we outlined an approach that intro-duces structure into distributional semantics.
Wepresented a method to compose distributional rep-resentations of individual units into larger com-posed structures.
We tested the efficacy of ourmodel on several evaluation tasks.
Our model?sperformance is competitive for tasks dealing withsemantic similarity of individual words, eventhough it suffers from the problem of sparsity.Additionally, it outperforms window-based ap-proaches on tasks involving semantic composi-tion.
In future work we hope to extend this for-malism to other semantic tasks like paraphrase de-tection and recognizing textual entailment.AcknowledgmentsThe authors would like to thank the anonymous re-viewers for their valuable comments and sugges-tions to improve the quality of the paper.
Thiswork was supported in part by the followinggrants: NSF grant IIS-1143703, NSF award IIS-1147810, DARPA grant FA87501220342.27ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 1183?1193, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Cosmin Adrian Bejan and Sanda Harabagiu.
2010.Unsupervised event coreference resolution with richlinguistic features.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 1412?1422, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informa-tion extraction with a supersense sequence tagger.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?06, pages 594?602, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Ronan Collobert, Jason Weston, L?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
J. Mach.
Learn.
Res., 999888:2493?2537,November.James Richard Curran.
2003.
From distributional tosemantic similarity.
Technical report.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedingsof the 2010 Conference on Empirical Methods inNatural Language Processing, EMNLP ?10, pages1162?1172, Stroudsburg, PA, USA.
Association forComputational Linguistics.Katrin Erk and Sebastian Pad?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?08,pages 897?906, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing search in context: The con-cept revisited.
In ACM Transactions on InformationSystems, volume 20, pages 116?131, January.John R. Firth.
1957.
A Synopsis of Linguistic Theory,1930-1955.
Studies in Linguistic Analysis, pages 1?32.Adele E. Goldberg.
2005.
Argument Realization: Cog-nitive Grouping and Theoretical Extensions.Edward Grefenstette, Mehrnoosh Sadrzadeh, StephenClark, Bob Coecke, and Stephen Pulman.
2011.Concrete sentence spaces for compositional distri-butional models of meaning.
In Proceedings of theNinth International Conference on ComputationalSemantics, IWCS ?11, pages 125?134, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of the 2010 Workshop onGEometrical Models of Natural Language Seman-tics, GEMS ?10, pages 33?37, Stroudsburg, PA,USA.
Association for Computational Linguistics.Aria Haghighi and Dan Klein.
2009.
Simple coref-erence resolution with rich syntactic and semanticfeatures.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing: Volume 3 - Volume 3, EMNLP ?09, pages 1152?1161, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Hans Hjelm.
2007.
Identifying cross language termequivalents using statistical machine translation anddistributional association measures.
In Proceedingsof NODALIDA, pages 97?104.
Citeseer.Ray Jackendoff.
1987.
The status of thematic roles inlinguistic theory.
Linguistic Inquiry, 18(3):369?411.Thomas K Landauer and Susan T. Dutnais.
1997.A solution to plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological review,pages 211?240.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entityand event coreference resolution across documents.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,EMNLP-CoNLL ?12, pages 489?500, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Beth Levin and Malka Rappaport Hovav.
2005.
Argu-ment Realization.
Cambridge University Press.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, New York,NY, USA.Diana McCarthy and John Carroll.
2003.
Disam-biguating nouns, verbs, and adjectives using auto-matically acquired selectional preferences.
Comput.Linguist., 29(4):639?654, December.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.In Proceedings of the 4th International Workshopon Semantic Evaluations (SemEval-2007), Prague,Czech Republic, pages 48?53.28Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In Proceedings of the 42nd AnnualMeeting on Association for Computational Linguis-tics, ACL ?04, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.George A. Miller.
1995.
Wordnet: a lexical databasefor english.
Commun.
ACM, 38(11):39?41, Novem-ber.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In In Proceedingsof ACL-08: HLT, pages 236?244.Jeff Mitchell and Mirella Lapata.
2009.
Languagemodels based on semantic composition.
In Proceed-ings of the 2009 Conference on Empirical Methodsin Natural Language Processing: Volume 1 - Volume1, EMNLP ?09, pages 430?439, Stroudsburg, PA,USA.
Association for Computational Linguistics.Marc Moens and Mark Steedman.
1988.
Temporal on-tology and temporal reference.
Computational lin-guistics, 14(2):15?28.Patrick Pantel and Dekang Lin.
2000.
Word-for-wordglossing with contextually similar words.
In Pro-ceedings of the 1st North American chapter of theAssociation for Computational Linguistics confer-ence, NAACL 2000, pages 78?85, Stroudsburg, PA,USA.
Association for Computational Linguistics.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?10,pages 492?501, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Sebastian Rudolph and Eugenie Giesbrecht.
2010.Compositional matrix-space models of language.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, ACL ?10,pages 907?916, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Hinrich Sch?tze.
1998.
Automatic word sense dis-crimination.
Comput.
Linguist., 24(1):97?123.Diarmuid ?
S?aghdha and Anna Korhonen.
2011.Probabilistic models of similarity in syntactic con-text.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?11, pages 1047?1057, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic com-positionality through recursive matrix-vector spaces.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,EMNLP-CoNLL ?12, pages 1201?1211, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrasecoreference resolution: making sense of the state-of-the-art.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 2 - Volume 2,ACL ?09, pages 656?664, Stroudsburg, PA, USA.Association for Computational Linguistics.Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fer-nandes, and Gregory Marton.
2003.
Quantitativeevaluation of passage retrieval algorithms for ques-tion answering.
In SIGIR, pages 41?47.Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In Pro-ceedings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, ACL ?10, pages948?957, Stroudsburg, PA, USA.
Association forComputational Linguistics.Stephen Tratz and Eduard Hovy.
2011.
A fast, ac-curate, non-projective, semantically-enriched parser.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?11, pages 1257?1268, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Peter D. Turney.
2002.
Mining the web for synonyms:Pmi-ir versus lsa on toefl.
CoRR.M.
Alex O. Vasilescu and Demetri Terzopoulos.
2002.Multilinear analysis of image ensembles: Tensor-faces.
In In Proceedings of the European Confer-ence on Computer Vision, pages 447?460.S.
K. M. Wong and Vijay V. Raghavan.
1984.
Vectorspace model of information retrieval: a reevaluation.In Proceedings of the 7th annual international ACMSIGIR conference on Research and development ininformation retrieval, SIGIR ?84, pages 167?185,Swinton, UK.
British Computer Society.29
