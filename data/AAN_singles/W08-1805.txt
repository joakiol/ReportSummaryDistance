Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 34?41Manchester, UK.
August 2008A Data Driven Approach to Query Expansion in Question AnsweringLeon Derczynski, Jun Wang, Robert Gaizauskas and Mark A. GreenwoodDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 PortobelloSheffield S1 4DP UK{aca00lad, acp07jw}@shef.ac.uk{r.gaizauskas, m.greenwood}@dcs.shef.ac.ukAbstractAutomated answering of natural languagequestions is an interesting and useful prob-lem to solve.
Question answering (QA)systems often perform information re-trieval at an initial stage.
Information re-trieval (IR) performance, provided by en-gines such as Lucene, places a bound onoverall system performance.
For example,no answer bearing documents are retrievedat low ranks for almost 40% of questions.In this paper, answer texts from previousQA evaluations held as part of the TextREtrieval Conferences (TREC) are pairedwith queries and analysed in an attemptto identify performance-enhancing words.These words are then used to evaluate theperformance of a query expansion method.Data driven extension words were foundto help in over 70% of difficult questions.These words can be used to improve andevaluate query expansion methods.
Sim-ple blind relevance feedback (RF) was cor-rectly predicted as unlikely to help overallperformance, and an possible explanationis provided for its low value in IR for QA.1 IntroductionThe task of supplying an answer to a question,given some background knowledge, is often con-sidered fairly trivial from a human point of view,as long as the question is clear and the answer isc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.known.
The aim of an automated question answer-ing system is to provide a single, unambiguous re-sponse to a natural language question, given a textcollection as a knowledge source, within a certainamount of time.
Since 1999, the Text RetrievalConferences have included a task to evaluate suchsystems, based on a large pre-defined corpus (suchas AQUAINT, containing around a million newsarticles in English) and a set of unseen questions.Many information retrieval systems performdocument retrieval, giving a list of potentially rel-evant documents when queried ?
Google?s and Ya-hoo!
?s search products are examples of this type ofapplication.
Users formulate a query using a fewkeywords that represent the task they are trying toperform; for example, one might search for ?eif-fel tower height?
to determine how tall the Eiffeltower is.
IR engines then return a set of referencesto potentially relevant documents.In contrast, QA systems must return an exact an-swer to the question.
They should be confidentthat the answer has been correctly selected; it isno longer down to the user to research a set of doc-ument references in order to discover the informa-tion themselves.
Further, the system takes a naturallanguage question as input, instead of a few user-selected key terms.Once a QA system has been provided with aquestion, its processing steps can be described inthree parts - Question Pre-Processing, Text Re-trieval and Answer Extraction:1.
Question Pre-Processing TREC questionsare grouped into series which relate to a giventarget.
For example, the target may be ?Hinden-burg disaster?
with questions such as ?What typeof craft was the Hindenburg??
or ?How fast couldit travel??.
Questions may include pronouns ref-34erencing the target or even previous answers, andas such require processing before they are suitablefor use.2.
Text Retrieval An IR component will returna ranked set of texts, based on query terms.
At-tempting to understand and extract data from anentire corpus is too resource intensive, and so an IRengine defines a limited subset of the corpus thatis likely to contain answers.
The question shouldhave been pre-processed correctly for a useful setof texts to be retrieved ?
including anaphora reso-lution.3.
Answer Extraction (AE) Given knowledgeabout the question and a set of texts, the AE sys-tem attempts to identify answers.
It should be clearthat only answers within texts returned by the IRcomponent have any chance of being found.Reduced performance at any stage will have aknock-on effect, capping the performance of laterstages.
If questions are left unprocessed and fullof pronouns (e.g.,?When did it sink??)
the IR com-ponent has very little chance of working correctly?
in this case, the desired action is to retrievedocuments related to the Kursk submarine, whichwould be impossible.IR performance with a search engine such asLucene returns no useful documents for at least35% of all questions ?
when looking at the top20 returned texts.
This caps the AE componentat 65% question ?coverage?.
We will measure theperformance of different IR component configura-tions, to rule out problems with a default Lucenesetup.For each question, answers are provided in theform of regular expressions that match answer text,and a list of documents containing these answersin a correct context.
As references to correct doc-uments are available, it is possible to explore adata-driven approach to query analysis.
We deter-mine which questions are hardest then concentrateon identifying helpful terms found in correct doc-uments, with a view to building a system than canautomatically extract these helpful terms from un-seen questions and supporting corpus.
The avail-ability and usefulness of these terms will providean estimate of performance for query expansiontechniques.There are at least two approaches which couldmake use of these term sets to perform query ex-pansion.
They may occur in terms selected forblind RF (non-blind RF is not applicable to theTREC QA task).
It is also possible to build a cata-logue of terms known to be useful according to cer-tain question types, thus leading to a dictionary of(known useful) expansions that can be applied topreviously unseen questions.
We will evaluate andalso test blind relevance feedback in IR for QA.2 Background and Related WorkThe performance of an IR system can be quanti-fied in many ways.
We choose and define mea-sures pertinent to IR for QA.
Work has been doneon relevance feedback specific to IR for QA, whereit is has usually be found to be unhelpful.
We out-line the methods used in the past, extend them, andprovide and test means of validating QA relevancefeedback.2.1 Measuring QA PerformanceThis paper uses two principle measures to describethe performance of the IR component.
Coverageis defined as the proportion of questions where atleast one answer bearing text appears in the re-trieved set.
Redundancy is the average numberof answer bearing texts retrieved for each ques-tion (Roberts and Gaizauskas, 2004).Both these measures have a fixed limit n on thenumber of texts retrieved by a search engine for aquery.
As redundancy counts the number of textscontaining correct answers, and not instances ofthe answer itself, it can never be greater than thenumber of texts retrieved.The TREC reference answers provide two waysof finding a correct text, with both a regular expres-sion and a document ID.
Lenient hits (retrievals ofanswer bearing documents) are those where the re-trieved text matches the regular expression; stricthits occur when the document ID of the retrievedtext matches that declared by TREC as correct andthe text matches the regular expression.
Some doc-uments will match the regular expression but notbe deemed as containing a correct answer (thisis common with numbers and dates (Baeza-Yatesand Ribeiro-Neto, 1999)), in which case a lenientmatch is found, but not a strict one.The answer lists as defined by TREC do not in-clude every answer-bearing document ?
only thosereturned by previous systems and marked as cor-rect.
Thus, false negatives are a risk, and strictmeasures place an approximate lower bound onthe system?s actual performance.
Similarly, lenient35matches can occur out of context, without a sup-porting document; performance based on lenientmatches can be viewed as an approximate upperbound (Lin and Katz, 2005).2.2 Relevance FeedbackRelevance feedback is a widely explored techniquefor query expansion.
It is often done using a spe-cific measure to select terms using a limited set ofranked documents of size r; using a larger set willbring term distribution closer to values over thewhole corpus, and away from ones in documentsrelevant to query terms.
Techniques are used toidentify phrases relevant to a query topic, in or-der to reduce noise (such as terms with a low cor-pus frequency that relate to only a single article)and query drift (Roussinov and Fan, 2005; Allan,1996).In the context of QA, Pizzato (2006) employsblind RF using the AQUAINT corpus in an attemptto improve performance when answering factoidquestions on personal names.
This is a similar ap-proach to some content in this paper, though lim-ited to the study of named entities, and does notattempt to examine extensions from the existinganswer data.Monz (2003) finds a negative result when apply-ing blind feedback for QA in TREC 9, 10 and 11,and a neutral result for TREC 7 and 8?s ad hoc re-trieval tasks.
Monz?s experiment, using r = 10and standard Rocchio term weighting, also founda further reduction in performance when r wasreduced (from 10 to 5).
This is an isolated ex-periment using just one measure on a limited setof questions, with no use of the available answertexts.Robertson (1992) notes that there are issueswhen using a whole document for feedback, asopposed to just a single relevant passage; as men-tioned in Section 3.1, passage- and document-levelretrieval sets must also be compared for their per-formance at providing feedback.
Critically, wewill survey the intersection between words knownto be helpful and blind RF terms based on initialretrieval, thus showing exactly how likely an RFmethod is to succeed.3 MethodologyWe first investigated the possibility of an IR-component specific failure leading to impairedcoverage by testing a variety of IR engines andconfigurations.
Then, difficult questions wereidentified, using various performance thresholds.Next, answer bearing texts for these harder ques-tions were checked for words that yielded a per-formance increase when used for query expansion.After this, we evaluated how likely a RF-based ap-proach was to succeed.
Finally, blind RF was ap-plied to the whole question set.
IR performancewas measured, and terms used for RF compared tothose which had proven to be helpful as extensionwords.3.1 IR EnginesA QA framework (Greenwood, 2004a) was origi-nally used to construct a QA system based on run-ning a default Lucene installation.
As this onlycovers one IR engine in one configuration, it isprudent to examine alternatives.
Other IR enginesshould be tested, using different configurations.The chosen additional engines were: Indri, basedon the mature INQUERY engine and the Lemurtoolkit (Allan et al, 2003); and Terrier, a newer en-gine designed to deal with corpora in the terabyterange and to back applications entered into TRECconferences (Ounis et al, 2005).We also looked at both passage-level anddocument-level retrieval.
Passages can be de-fined in a number of ways, such as a sentence,a sliding window of k terms centred on the tar-get term(s), parts of a document of fixed (andequal) lengths, or a paragraph.
In this case,the documents in the AQUAINT corpus containparagraph markers which were used as passage-level boundaries, thus making ?passage-level?and ?paragraph-level?
equivalent in this paper.Passage-level retrieval may be preferable for AE,as the number of potential distracters is some-what reduced when compared to document-levelretrieval (Roberts and Gaizauskas, 2004).The initial IR component configuration was withLucene indexing the AQUAINT corpus at passage-level, with a Porter stemmer (Porter, 1980) and anaugmented version of the CACM (Jones and vanRijsbergen, 1976) stopword list.Indri natively supports document-level indexingof TREC format corpora.
Passage-level retrievalwas done using the paragraph tags defined in thecorpus as delimiters; this allows both passage- anddocument-level retrieval from the same index, ac-cording to the query.All the IR engines were unified to use the Porter36Coverage RedundancyYear Len.
Strict Len.
StrictLucene2004 0.686 0.636 2.884 1.6242005 0.703 0.566 2.780 1.1552006 0.665 0.568 2.417 1.181Indri2004 0.690 0.554 3.849 1.5272005 0.694 0.512 3.908 1.0562006 0.691 0.552 3.373 1.152Terrier2004 - - - -2005 - - - -2006 0.638 0.493 2.520 1.000Table 1: Performance of Lucene, Indri and Terrier at para-graph level, over top 20 documents.
This clearly shows thelimitations of the engines.stemmer and the same CACM-derived stopwordlist.The top n documents for each question in theTREC2004, TREC2005 and TREC2006 sets wereretrieved using every combination of engine, andconfiguration1 .
The questions and targets wereprocessed to produce IR queries as per the defaultconfiguration for the QA framework.
Examiningthe top 200 documents gave a good compromisebetween the time taken to run experiments (be-tween 30 and 240 minutes each) and the amountone can mine into the data.
Tabulated results areshown in Table 1 and Table 2.
Queries have hadanaphora resolution performed in the context oftheir series by the QA framework.
AE compo-nents begin to fail due to excess noise when pre-sented with over 20 texts, so this value is enough toencompass typical operating parameters and leavespace for discovery (Greenwood et al, 2006).A failure analysis (FA) tool, an early versionof which is described by (Sanka, 2005), providedreporting and analysis of IR component perfor-mance.
In this experiment, it provided high levelcomparison of all engines, measuring coverageand redundancy as the number of documents re-trieved, n, varies.
This is measured because a per-fect engine will return the most useful documentsfirst, followed by others; thus, coverage will behigher for that engine with low values of n.3.2 Identification of Difficult QuestionsOnce the performance of an IR configuration overa question set is known, it?s possible to producea simple report listing redundancy for each ques-tion.
A performance reporting script accesses the1Save Terrier / TREC2004 / passage-level retrieval;passage-level retrieval with Terrier was very slow using ourconfiguration, and could not be reliably performed using thesame Terrier instance as document-level retrieval.Coverage RedundancyYear Len.
Strict Len.
StrictIndri2004 0.926 0.837 7.841 2.6632005 0.935 0.735 7.573 1.9692006 0.882 0.741 6.872 1.958Terrier2004 0.919 0.806 7.186 2.3802005 0.928 0.766 7.620 2.1302006 0.983 0.783 6.339 2.067Table 2: Performance of Indri and Terrier at document levelIR over the AQUAINT corpus, with n = 20FA tool?s database and lists all the questions ina particular set with the strict and lenient redun-dancy for selected engines and configurations.
En-gines may use passage- or document-level config-urations.Data on the performance of the three engines isdescribed in Table 2.
As can be seen, the cover-age with passage-level retrieval (which was oftenfavoured, as the AE component performs best withreduced amounts of text) languishes between 51%and 71%, depending on the measurement method.Failed anaphora resolution may contribute to thisfigure, though no deficiencies were found upon vi-sual inspection.Not all documents containing answers are noted,only those checked by the NIST judges (Bilottiet al, 2004).
Match judgements are incomplete,leading to the potential generation of false nega-tives, where a correct answer is found with com-plete supporting information, but as the informa-tion has not been manually flagged, the system willmark this as a failure.
Assessment methods arefully detailed in Dang et al (2006).
Factoid per-formance is still relatively poor, although as only1.95 documents match per question, this may be aneffect of such false negatives (Voorhees and Buck-land, 2003).
Work has been done into creatingsynthetic corpora that include exhaustive answersets (Bilotti, 2004; Tellex et al, 2003; Lin andKatz, 2005), but for the sake of consistency, andeasy comparison with both parallel work and priorlocal results, the TREC judgements will be used toevaluate systems in this paper.Mean redundancy is also calculated for a num-ber of IR engines.
Difficult questions were thosefor which no answer bearing texts were found byeither strict or lenient matches in any of the top ndocuments, using a variety of engines.
As soon asone answer bearing document was found by an en-gine using any measure, that question was deemednon-difficult.
Questions with mean redundancy of37zero are marked difficult, and subjected to furtheranalysis.
Reducing the question set to just diffi-cult questions produces a TREC-format file for re-testing the IR component.3.3 Extension of Difficult QuestionsThe documents deemed relevant by TREC mustcontain some useful text that can help IR engineperformance.
Such words should be revealed bya gain in redundancy when used to extend an ini-tially difficult query, usually signified by a changefrom zero to a non-zero value (signifying that rele-vant documents have been found where none werebefore).
In an attempt to identify where the use-ful text is, the relevant documents for each difficultquestion were retrieved, and passages matching theanswer regular expression identified.
A script isthen used to build a list of terms from each passage,removing words in the question or its target, wordsthat occur in the answer, and stopwords (based onboth the indexing stopword list, and a set of stemscommon within the corpus).
In later runs, num-bers are also stripped out of the term list, as theirvalue is just as often confusing as useful (Baeza-Yates and Ribeiro-Neto, 1999).
Of course, answerterms provide an obvious advantage that would notbe reproducible for questions where the answer isunknown, and one of our goals is to help query ex-pansion for unseen questions.
This approach mayprovide insights that will enable appropriate queryexpansion where answers are not known.Performance has been measured with both thequestion followed by an extension (Q+E), as wellas the question followed by the target and thenextension candidates (Q+T+E).
Runs were alsoexecuted with just Q and Q+T, to provide non-extended reference performance data points.
Ad-dition of the target often leads to gains in perfor-mance (Roussinov et al, 2005), and may also aidin cases where anaphora resolution has failed.Some words are retained, such as titles, as in-cluding these can be inferred from question or tar-get terms and they will not unfairly boost redun-dancy scores; for example, when searching for a?Who?
question containing the word ?military?,one may want to preserve appellations such as?Lt.?
or ?Col.
?, even if this term appears in the an-swer.This filtered list of extensions is then used to cre-ate a revised query file, containing the base ques-tion (with and without the target suffixed) as wellas new questions created by appending a candidateextension word.Results of retrievals with these new question areloaded into the FA database and a report describ-ing any performance changes is generated.
Theextension generation process also creates customanswer specifications, which replicate the informa-tion found in the answers defined by TREC.This whole process can be repeated with vary-ing question difficulty thresholds, as well as alter-native n values (typically from 5 to 100), differentengines, and various question sets.3.4 Relevance Feedback PerformanceNow that we can find the helpful extension words(HEWs) described earlier, we?re equipped to eval-uate query expansion methods.
One simplistic ap-proach could use blind RF to determine candidateextensions, and be considered potentially success-ful should these words be found in the set of HEWsfor a query.
For this, term frequencies can bemeasured given the top r documents retrieved us-ing anaphora-resolved query Q.
After stopwordand question word removal, frequent terms are ap-pended to Q, which is then re-evaluated.
Thishas been previously attempted for factoid ques-tions (Roussinov et al, 2005) and with a limitedrange of r values (Monz, 2003) but not validatedusing a set of data-driven terms.We investigated how likely term frequency (TF)based RF is to discover HEWs.
To do this, theproportion of HEWs that occurred in initially re-trieved texts was measured, as well as the propor-tion of these texts containing at least one HEW.Also, to see how effective an expansion method is,suggested expansion terms can be checked againstthe HEW list.We used both the top 5 and the top 50 documentsin formulation of extension terms, with TF as aranking measure; 50 is significantly larger than theoptimal number of documents for AE (20), withoutoverly diluting term frequencies.Problems have been found with using entiredocuments for RF, as the topic may not be thesame throughout the entire discourse (Robertsonet al, 1992).
Limiting the texts used for RF toparagraphs may reduce noise; both document- andparagraph-level terms should be checked.38EngineYear LuceneParaIndriParaIndriDocTerrierDoc2004 76 72 37 422005 87 98 37 352006 108 118 59 53Table 3: Number of difficult questions, as defined by thosewhich have zero redundancy over both strict and lenient mea-sures, at n = 20.
Questions seem to get harder each year.Document retrieval yields fewer difficult questions, as moretext is returned for potential matching.EngineLucene Indri TerrierParagraph 226 221 -Document - 121 109Table 4: Number of difficult questions in the 2006 task, as de-fined above, this time with n = 5.
Questions become harderas fewer chances are given to provide relevant documents.4 ResultsOnce we have HEWs, we can determine if theseare going to be of significant help when chosen asquery extensions.
We can also determine if a queryexpansion method is likely to be fruitful.
Blind RFwas applied, and assessed using the helpful wordslist, as well as RF?s effect on coverage.4.1 Difficult Question AnalysisThe number of difficult questions found at n =20 is shown in Table 3.
Document-level retrievalgave many fewer difficult questions, as the amountof text retrieved gave a higher chance of findinglenient matches.
A comparison of strict and lenientmatching is in Table 5.Extensions were then applied to difficult ques-tions, with or without the target.
The performanceof these extensions is shown in Table 6.
Resultsshow a significant proportion (74.4%) of difficultquestions can benefit from being extended withnon-answer words found in answer bearing texts.4.2 Applying Relevance FeedbackIdentifying HEWs provides a set of words thatare useful for evaluating potential expansion terms.Match typeStrict LenientYear2004 39 492005 56 662006 53 49Table 5: Common difficult questions (over all three enginesmentioned above) by year and match type; n = 20.Difficult questions used 118Variations tested 6683Questions that benefited 87 (74.4%)Helpful extension words (strict) 4973Mean helpful words per question 42.144Mean redundancy increase 3.958Table 6: Using Terrier Passage / strict matching, retrieving 20docs, with TREC2006 questions / AQUAINT.
Difficult ques-tions are those where no strict matches are found in the top 20IRT from just one engine.2004 2005 2006HEW found in IRT 4.17% 18.58% 8.94%IRT containing HEW 10.00% 33.33% 34.29%RF words in HEW 1.25% 1.67% 5.71%Table 7: ?Helpful extension words?
: the set of extensions that,when added to the query, move redundancy above zero.
r =5, n = 20, using Indri at passage level.Using simple TF based feedback (see Section 3.4),5 terms were chosen per query.
These words hadsome intersection (see Table 7) with the exten-sion words set, indicating that this RF may lead toperformance increases for previously unseen ques-tions.
Only a small number of the HEWs occur inthe initially retrieved texts (IRTs), although a no-ticeable proportion of IRTs (up to 34.29%) containat least one HEW.
However, these terms are prob-ably not very frequent in the documents and un-likely to be selected with TF-based blind RF.
Themean proportion of RF selected terms that wereHEWs was only 2.88%.
Blind RF for question an-swering fails here due to this low proportion.
Strictmeasures are used for evaluation as we are inter-ested in finding documents which were not pre-viously being retrieved rather than changes in thedistribution of keywords in IRT.Document and passage based RF term selectionis used, to explore the effect of noise on terms, anddocument based term selection proved marginallysuperior.
Choosing RF terms from a small set ofdocuments (r = 5) was found to be marginallybetter than choosing from a larger set (r = 50).In support of the suggestion that RF would be un-r5 50 BaselineRank Doc Para Doc Para5 0.253 0.251 0.240 0.179 0.31210 0.331 0.347 0.331 0.284 0.43420 0.438 0.444 0.438 0.398 0.55350 0.583 0.577 0.577 0.552 0.634Table 8: Coverage (strict) using blind RF.
Both document-and paragraph-level retrieval used to determine RF terms.39Question:Who was the nominal leader after the overthrow?Target: Pakistani government overthrown in 1999Extension word RedundancyKashmir 4Pakistan 4Islamabad 2.5Question: Where did he play in college?Target: Warren MoonExtension word RedundancyNFL 2.5football 1Question: Who have commanded the division?Target: 82nd Airborne divisionExtension word RedundancyGen 3Col 2decimated 2officer 1Table 9: Queries with extensions, and their mean redundancyusing Indri at document level with n = 20.
Without exten-sions, redundancy is zero.likely to locate HEWs, applying blind RF consis-tently hampered overall coverage (Table 8).5 DiscussionHEWs are often found in answer bearing texts,though these are hard to identify through sim-ple TF-based RF.
A majority of difficult questionscan be made accessible through addition of HEWspresent in answer bearing texts, and work to deter-mine a relationship between words found in initialretrieval and these HEWs can lead to coverage in-creases.
HEWs also provide an effective meansof evaluating other RF methods, which can be de-veloped into a generic rapid testing tool for queryexpansion techniques.
TF-based RF, while findingsome HEWs, is not effective at discovering exten-sions, and reduces overall IR performance.There was not a large performance changebetween engines and configurations.
Strictparagraph-level coverage never topped 65%, leav-ing a significant number of questions where nouseful information could be provided for AE.The original sets of difficult questions for in-dividual engines were small ?
often less than the35% suggested when looking at the coverage fig-ures.
Possible causes could include:Difficult questions being defined as those forwhich average redundancy is zero: This limitmay be too low.
To remedy this, we could increasethe redundancy limit to specify an arbitrary num-ber of difficult questions out of the whole set.The use of both strict and lenient measures: Itis possible to get a lenient match (thus marking aquestion as non-difficult) when the answer text oc-curs out of context.Reducing n from 20 to 5 (Table 4) increasedthe number of difficult questions produced.
Fromthis we can hypothesise that although many searchengines are succeeding in returning useful docu-ments (where available), the distribution of thesedocuments over the available ranks is not one thatbunches high ranking documents up as those im-mediately retrieved (unlike a perfect engine; seeSection 3.1), but rather suggests a more even dis-tribution of such documents over the returned set.The number of candidate extension words forqueries (even after filtering) is often in the rangeof hundreds to thousands.
Each of these wordscreates a separate query, and there are two varia-tions, depending on whether the target is includedin the search terms or not.
Thus, a large numberof extended queries need to be executed for eachquestion run.
Passage-level retrieval returns lesstext, which has two advantages: firstly, it reducesthe scope for false positives in lenient matching;secondly, it is easier to scan result by eye and de-termine why the engine selected a result.Proper nouns are often helpful as extensions.We noticed that these cropped up fairly regularlyfor some kinds of question (e.g.
?Who?).
Espe-cially useful were proper nouns associated withlocations - for example, adding ?Pakistani?
toa query containing the word Pakistan lifted re-dundancy above zero for a question on PresidentMusharraf, as in Table 9.
This reconfirms workdone by Greenwood (2004b).6 Conclusion and Future WorkIR engines find some questions very difficult andconsistently fail to retrieve useful texts even withhigh values of n. This behaviour is common overmany engines.
Paragraph level retrieval seems togive a better idea of which questions are hard-est, although the possibility of false negatives ispresent from answer lists and anaphora resolution.Relationships exist between query words andhelpful words from answer documents (e.g.
witha military leadership themes in a query, adding theterm ?general?
or ?gen?
helps).
Identification ofHEWs has potential use in query expansion.
Theycould be used to evaluate RF approaches, or asso-ciated with question words and used as extensions.Previous work has ruled out relevance feedback40in particular circumstances using a single rankingmeasure, though this has not been based on analy-sis of answer bearing texts.
The presence of HEWsin IRT for difficult questions shows that guided RFmay work, but this will be difficult to pursue.
BlindRF based on term frequencies does not increase IRperformance.
However, there is an intersection be-tween words in initially retrieved texts and wordsdata driven analysis defines as helpful, showingpromise for alternative RF methods (e.g.
based onTFIDF).
These extension words form a basis forindicating the usefulness of RF and query expan-sion techniques.In this paper, we have chosen to explore onlyone branch of query expansion.
An alternative datadriven approach would be to build associations be-tween recurrently useful terms given question con-tent.
Question texts could be stripped of stopwordsand proper nouns, and a list of HEWs associatedwith each remaining term.
To reduce noise, thenumber of times a particular extension has helpeda word would be counted.
Given sufficient sampledata, this would provide a reference body of HEWsto be used as an aid to query expansion.ReferencesAllan, J., J. Callan, K. Collins-Thompson, B. Croft,F.
Feng, D. Fisher, J. Lafferty, L. Larkey, TN Truong,P.
Ogilvie, et al 2003.
The Lemur Toolkit for Lan-guage Modeling and Information Retrieval.Allan, J.
1996.
Incremental Relevance Feedback forInformation Filtering.
In Research and Developmentin IR, pages 270?278.Baeza-Yates, R. and B. Ribeiro-Neto.
1999.
ModernInformation Retrieval.
Addison Wesley.Bilotti, M.W., B. Katz, and J. Lin.
2004.
What WorksBetter for Question Answering: Stemming or Mor-phological Query Expansion.
Proc.
IR for QA Work-shop at SIGIR 2004.Bilotti, M.W.
2004.
Query Expansion Techniques forQuestion Answering.
Master?s thesis, MassachusettsInstitute of Technology.Dang, H.T., J. Lin, and D. Kelly.
2006.
Overview ofthe TREC 2006 QA track.
Proc.
15th Text REtrievalConf..Greenwood, M.A., M. Stevenson, and R. Gaizauskas.2006.
The University of Sheffield?s TREC 2006Q&A Experiments.
In Proc.
15th Text REtrievalConferenceGreenwood, M.A.
2004a.
AnswerFinder: QuestionAnswering from your Desktop.
In Proc.
7th AnnualColloquium for the UK SIG for Computational Lin-guistics (CLUK ?04).Greenwood, M.A.
2004b.
Using Pertainyms to Im-prove Passage Retrieval for Questions RequestingInformation about a Location.
In Proc.
Workshopon IR for QA (SIGIR 2004).Jones, K.S.
and C.J.
van Rijsbergen.
1976.
IR TestCollections.
J. of Documentation, 32(1):59?75.Lin, J. and B. Katz.
2005.
Building a Reusable TestCollection for Question Answering.
J. American So-ciety for Information Science and Technology.Monz, C. 2003.
From Document Retrieval to QuestionAnswering.
ILLC Dissertation Series 2003, 4.Ounis, I., G. Amati, V. Plachouras, B.
He, C. Macdon-ald, and D. Johnson.
2005.
Terrier IR Platform.Proc.
27th European Conf.
on IR (ECIR 05), San-tiago de Compostela, Spain, pages 517?519.Pizzato, L.A., D. Molla, and C. Paris.
2006.
Pseudo-Relevance Feedback using Named Entities for Ques-tion Answering.
Australasian Language TechnologyWorkshop (ALTW2006), pages 83?90.Porter, M. 1980.
An Algorithm for Suffix StrippingProgram.
Program, 14(3):130?137.Roberts, I and R Gaizauskas.
2004.
Evaluating PassageRetrieval Approaches for Question Answering.
InProc.
26th European Conf.
on IR.Robertson, S.E., S. Walker, M. Hancock-Beaulieu,A.
Gull, and M. Lau.
1992.
Okapi at TREC.
InText REtrieval Conf., pages 21?30.Roussinov, D. and W. Fan.
2005.
Discretization BasedLearning Approach to Information Retrieval.
InProc.
2005 Conf.
on Human Language Technologies.Roussinov, D., M. Chau, E. Filatova, and J.A.
Robles-Flores.
2005.
Building on Redundancy: Fac-toid Question Answering, Robust Retrieval and the?Other?.
In Proc.
14th Text REtrieval Conf.Sanka, Atheesh.
2005.
Passage Retrieval for QuestionAnswering.
Master?s thesis, University of Sheffield.Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.2003.
Quantitative Evaluation of Passage RetrievalAlgorithms for Question Answering.
Proc.
26th An-nual Int?l ACM SIGIR Conf.
on R&D in IR, pages41?47.Voorhees, E. and L. P. Buckland, editors.
2003.
Proc.12th Text REtrieval Conference.41
