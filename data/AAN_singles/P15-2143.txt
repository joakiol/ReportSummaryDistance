Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 870?876,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLabeled Grammar Induction with Minimal SupervisionYonatan Bisk Christos Christodoulopoulos Julia HockenmaierDepartment of Computer ScienceThe University of Illinois at Urbana-Champaign201 N. Goodwin Ave, Urbana, IL 61801{bisk1,christod,juliahmr}@illinois.eduAbstractNearly all work in unsupervised grammarinduction aims to induce unlabeled de-pendency trees from gold part-of-speech-tagged text.
These clean linguistic classesprovide a very important, though unreal-istic, inductive bias.
Conversely, inducedclusters are very noisy.
We show here,for the first time, that very limited hu-man supervision (three frequent words percluster) may be required to induce labeleddependencies from automatically inducedword clusters.1 IntroductionDespite significant progress on inducingpart-of-speech (POS) tags from raw text(Christodoulopoulos et al., 2010; Blunsomand Cohn, 2011) and a small number of notableexceptions (Seginer, 2007; Spitkovsky et al.,2011; Christodoulopoulos et al., 2012), mostapproaches to grammar induction or unsupervisedparsing (Klein and Manning, 2004; Spitkovskyet al., 2013; Blunsom and Cohn, 2010) arebased on the assumption that gold POS tags areavailable to the induction system.
Although mostapproaches treat these POS tags as arbitrary, ifrelatively clean, clusters, it has also been shownthat the linguistic knowledge implicit in thesetags can be exploited in a more explicit fashion(Naseem et al., 2010).
The presence of POS tagsis also essential for approaches that aim to returnricher structures than the standard unlabeleddependencies.
Boonkwan and Steedman (2011)train a parser that uses a semi-automaticallyconstructed Combinatory Categorial Grammar(CCG, Steedman (2000)) lexicon for POS tags,while Bisk and Hockenmaier (2012; 2013) showthat CCG lexicons can be induced automaticallyif POS tags are used to identify nouns and verbs.However, assuming clean POS tags is highlyunrealistic for most scenarios in which one wouldwish to use an otherwise unsupervised parser.In this paper we demonstrate that the simple?universal?
knowledge of Bisk and Hockenmaier(2013) can be easily applied to induced clus-ters given a small number of words labeled asnoun, verb or other, and that this small amountof knowledge is sufficient to produce labeled syn-tactic structures from raw text, something that hasnot yet been proposed in the literature.
Specifi-cally, we will provide a labeled evaluation of in-duced CCG parsers against the English (Hock-enmaier and Steedman, 2007) and Chinese (Tse,2013) CCGbanks.
To provide a direct compari-son to the dependency induction literature, we willalso provide an unlabeled evaluation on the 10 de-pendency corpora that were used for the task ofgrammar induction from raw text in the PASCALChallenge on Grammar Induction (Gelling et al.,2012).The system of Christodoulopoulos et al.
(2012)was the only participant competing in the PAS-CAL Challenge that operated over raw text (in-stead of gold POS tags).
However, their approachdid not outperform the six baseline systems pro-vided.
These baselines were two versions of theDMV model (Klein and Manning, 2004; Gillen-water et al., 2011) run on varying numbers of in-duced Brown clusters (described in section 2.1).We will therefore compare against these baselinesin our evaluation.Outside of the shared task, Spitkovsky et al.
(2011) demonstrated impressive performance us-ing Brown clusters but did not provide evaluation870for languages other than English.The system we propose here will use a coarse-grained labeling comprised of three classes, whichmakes it substantially simpler than traditionaltagsets, and uses far fewer labeled tokens thanis customary for weakly-supervised approaches(Haghighi and Klein, 2006; Garrette et al., 2015).2 Our ModelsOur goal in this work will be to produce la-beled dependencies from raw text.
Our approachis based on the HDP-CCG parser of Bisk andHockenmaier (2015) with their extensions to cap-ture lexicalization and punctuation, which, to ourknowledge, is the only unsupervised approach toproduce labeled dependencies.
It first induces aCCG from POS-tagged text, and then estimates amodel based on Hierarchical Dirichlet Processes(Teh et al., 2006) over the induced parse forests.The HDP model uses a hyperparameter whichcontrols the amount of smoothing to the base mea-sure of the HDP.
Setting this value will prove im-portant when moving between datasets of drasti-cally different sizes.The induction algorithm assumes that a) verbsmay be predicates (with category S), b) verbs cantake nouns (with category N) or sentences as ar-guments (leading to categories of the form S|N,(S|N)|N, (S|N)|S etc.
), c) any word can act as amodifier, i.e.
have a category of the form X|Xif it is adjacent to a word with category X orX|Y, and d) modifiers X|X can take nouns or sen-tences as arguments ((X|X)|N).
Our contributionin this paper will be to show that we can replacethe gold POS tags used by Bisk and Hockenmaier(2013) with automatically induced word clusters,and then use very minimal supervision to identifynoun and verb clusters.2.1 Inducing Word ClustersWe will evaluate three clustering approaches:Brown Clusters Brown clusters (Brown et al.,1992) assign each word to a single cluster using anaglomerative clustering that maximizes the proba-bility of the corpus under a bigram class condi-tional model.
We use Liang?s implementation1.BMMM The Bayesian Multinomial MixtureModel2(BMMM, Christodoulopoulos et al.
2011)is also a hard clustering system, but has the ability1https://github.com/percyliang/brown-cluster2https://github.com/christos-c/bmmmto incorporate multiple types of features either ata token level (e.g.
?1 context word) or at a typelevel (e.g.
morphology features derived from theMorfessor system (Creutz and Lagus, 2006)).
Thecombination of these features allows BMMM tobetter capture morphosyntactic information.Bigram HMM We also evaluate unsupervisedbigram HMMs, since the soft clustering they pro-vide may be advantageous over the hard Brownand BMMM clusters.
But it is known that un-supervised HMMs may not find good POS tags(Johnson, 2007), and in future work, more sophis-ticated models (e.g.
Blunsom and Cohn (2011)),might outperform the systems we use here.In all cases, we assume that we can identifypunctuation marks, which are moved to their owncluster and ignored for the purposes of tagging andparsing evaluation.2.2 Identifying Noun and Verb ClustersTo induce CCGs from induced clusters, we needto label them as {noun, verb, other}.
This needsto be done judiciously; providing every cluster theverb label, for example, leads to the model iden-tifying prepositions as the main sentential predi-cates.We demonstrate here that labeling three fre-quent words per cluster is sufficient to outperformstate-of-the-art performance on grammar induc-tion from raw text in many languages.
We emu-late having a native speaker annotate words for usby using the universal tagset (Petrov et al., 2012)as our source of labels for the most frequent threewords per cluster (we map the tags NOUN, NUM,PRON to noun, VERB to verb, and all others toother).
The final labeling is a majority vote, whereeach word type contributes a vote for each label itcan take (see Table 4 for some examples).
This ap-proach could easily be scaled to allow more wordsper cluster to vote.
But we will see that three percluster is sufficient to label most tokens correctly.3 Experimental SetupWe will focus first on producing CCG labeledpredicate-argument dependencies for English andChinese and will then apply our best settings toproduce a comparison with the tree structures ofthe languages of the PASCAL Shared Task.
Alllanguages will be trained on sentences of up tolength 20 (not counting punctuation).
All clus-ter induction algorithms are treated as black boxes871and run over the complete datasets in advance.This alleviates having to handle tagging of un-known words.To provide an intuition for the performance ofthe induced word clusters, we provide two stan-dard metrics for unsupervised tagging:Many-to-one (M-1) A commonly used mea-sure, M-1 relies on mapping each cluster to themost common POS tag of its words.
However, M-1 can be easily inflated by inducing more clusters.V-Measure Proposed by Rosenberg andHirschberg (2007), V-Measure (VM) measuresthe information-theoretic distance between twoclusterings and has been shown to be robust to thenumber of induced clusters (Christodoulopouloset al., 2010).
Both of these metrics are knownto be highly dependent on the gold annotationstandards they are compared against, and maynot correlate with downstream performance atparsing.Of more immediate relevance to our task is theability to accurately identify nouns and verbs:Noun, Verb, and Other Recall We measurethe (token-based) recall of our three-way labelingscheme of clusters as noun/verb/other against theuniversal POS tags of each token.4 Experiment 1: CCG-based EvaluationExperimental Setup For our primary experi-ments, we train and test our systems on the Englishand Chinese CCGbanks, and report directed la-beled F1 (LF1) and undirected unlabeled F1 (UF1)over CCG dependencies (Clark et al., 2002).
Forthe labeled evaluation, we follow the simplifica-tion of CCGbank categories proposed by Bisk andHockenmaier (2015): for English to remove mor-phosyntactic features, map NP to N and changeVP modifiers (S\NP)|(S\NP) to sentential modi-fiers (S|S); for Chinese we map both M and QP toN.
In the CCG literature, UF1 is commonly usedbecause undirected dependencies do not penalizeargument vs. adjunct distinctions, e.g.
for prepo-sitional phrases.
For this reason we will includeUF1 in the final test set evaluation (Table 2).We use the published train/dev/test splits, usingthe dev set for choosing a cluster induction algo-rithm, and then present final performance on thetest data.
We induce 36 tags for English and 37for Chinese to match the number of tags present inthe treebanks (excluding symbol and punctuationtags).Tagging Labeling ParsingM-1 VM N / V / O LF1 GoldEnglishBrown 62.4 56.3 85.6 59.4 81.2 23.3BMMM 66.8 58.7 81.0 81.2 82.7 26.6 38.8HMM 51.1 41.7 76.3 63.3 82.6 25.8ChineseBrown 66.0 50.1 88.9 28.6 91.3 10.2BMMM 64.8 50.0 94.4 48.7 87.0 10.5 16.6HMM 46.3 30.8 68.0 44.6 76.7 3.13Table 1: Tagging evaluation (M-1, VM, N/V/ORecall) and directed labeled CCG-Dependencyperformance (LF1) as compared to the use of goldPOS tags (Gold) for three clustering algorithms.Results Table 1 presents the parsing and taggingdevelopment results on the two CCG corpora.
Interms of tagging performance, we can see that thetwo hard clustering systems significantly outper-form the HMM, but the relative performance ofBrown and BMMM is mixed.More importantly, we see that, at least for En-glish, despite clear differences in tagging perfor-mance, the parsing results (LF1) are much moresimilar.
In Chinese, we see that the performanceof the two hard clustering systems is almost iden-tical, again, not representative of the differencesin the tagging scores.
The N/V/O recall scores inboth languages are equally poor predictors of pars-ing performance.
However, these scores show thathaving only three labeled tokens per class is suffi-cient to capture most of the necessary distinctionsfor the HDP-CCG.
All of this confirms the ob-servations of Headden et al.
(2008) that POS tag-ging metrics are not correlated with parsing per-formance.
However, since BMMM seems to havea slight overall advantage, we will be using it asour clustering system for the remaining experi-ments.Since the goal of this work was to produce la-beled syntactic structures, we also wanted to eval-uate our performance against that of the HDP-CCG system that uses gold-standard POS tags.
Aswe can see in the last two columns of our develop-ment results in Table 1 and in the final test resultsof Table 2, our system is within 2/3 of the labeledperformance of the gold-POS-based HDP-CCG3.Figure 1 shows an example labeled syntacticstructure induced by the model.
We can seethe system successfully learns to attach the final3To put this result into its full perspective, the LF1 perfor-mance of a supervised CCG system (Hockenmaier and Steed-man, 2002), HWDep model, trained on the same length-20dataset and tested on the simplified CCGbank test set is 80.3.872This GoldEnglish 26.0 / 51.1 37.1 / 64.9Chinese 10.3 / 33.5 15.6 / 39.8Table 2: CCG parsing performance (LF1/UF1) onthe test set with and without gold tags.hertz equipment is a major supplier of rental equipment in the u.s. , france , spain and the u.k .N/N N S\N (S\S)/N N/N N (N\N)/N N/N N (S\S)/N N/N N/N , N/N , N/N and N/N N .> > >punc >punc >N N N/N N/N N/N N< >S N\N< >N N>N> >S\S N>N< >S N>S\S<S>puncShertz equipment is a major supplier of rental equipment in the u.s. , france , spain a d the u.k .N/N N S\N (S\S)/N N (N\N)/N N/N N (S\S)/N N/N N , N/N , N/N and N/N .> > >punc >punc >N N N/N N/N N/N N< >S N\N< >N N>N> >S\S N>N< >S N>S\S<S>puncShertz equipment is a major supplier of rental equip ent in the u.s. , france , spain and the u.k .N/N N \N (S\S)/ N/N N (N\N)/N N/ (S\S)/N N/ N/ , N/N , N and N/ N .> > >punc >punc >N N/ N/N N/N N< >S N\N< >N N>N> >S\S N>N< >S N>S\S<S>puncShertz equipment is a major supplier of rental equipment in the u.s. , france , spain and the u.k .N/N N S\N (S\S)/N N/N N (N\N)/N N/N N (S\S)/N N/N N/N , N/N , N/N and N/N N .> > >punc >punc >N N N/N N/N N/N N< >S N\N< >N>N> >S\S N>N< >S N>S\S<S>puncShertz equipment is a major supplier of rental equipment in the u.s. , france , spain and the u.k .N/N N S\N (S\S)/N N/N N (N\N)/N N/N N (S\S)/N N/N N/N , N/N , N/N and N/N N .> > >punc >punc >N N N/N N/N N/N N< >S N\N< >N N>N> >S\S N>N< >S N>S\S<S>puncShertz equipment is majo supplier of rental equipment in the u.s. , france , spain and the u.k .N/ N S\N (S\S)/ N/N N ( \ )/ /N N (S\S)/N N/N / , / , N/N and N/N N .> > >punc >punc >N N /N N/N /N N< >S N\N< >N N>N> >S\S N>N< >S N>S\S<S>puncShertz equipment is a major supplier of rental equipment in the u.s. , france , spain and the u.k .N/N N S\N (S\S)/N N/N ( \N)/N N/N (S\S)/N N/N , N/N , N/N and N/N N .> > >punc >punc >N N/ /N /N N< >S N\N< >N N>N> >S\S N>N< >S N>S\S<S>puncShertz equipment is a ajor supplier of rental equipment i the u.s. , france , spain and the u.k .N/N N S\N (S\S)/N N/N ( \ )/ N/N N (S\S)/N N/N N/N , N/N , N/N and N/N N .> > >punc >punc >N N N/N N/N N/N N< >S N\N< >N N>N> >S\S N>N< >S N>S\S<S>puncSHertz equipment is a major supplier of rental equipmentN/N N S\N (S\S)/N N/N N (N\N)/N N/N N> >N N< >S N\N<N>N>S\S<S1Figure 1: A sample derivation from the WSJ Sec-tion 22 demonstrating the system is learning mostof the correct categories of CCGbank but has in-correctly analyzed the determiner as a preposition.prepositional phrase, but mistakes the verb for in-transitive and treats the determiner a as a prepo-sition.
The labeled and undirected recall for thisparse are 5/8 and 7/8 respectively.5 Experiment 2: PASCAL Shared TaskExperimental Setup During the PASCALshared task, participants were encouraged to trainover the complete union of the data splits.
Wedo the same here, use the dev set for choosinga HDP-CCG hyperparameter, and then presentfinal results for comparison on the test section.We vary the hyperparamter for this evaluationbecause the datasets fluctuate dramatically insize from 9K to 700K tokens on sentences up tolength 20.
Rather than match all of the tagsets, wesimply induce 49 (excluding punctuation) classesfor every language.
The actual tagsets vary from20 to 304 tags (median 39, mean 78).Results We now present results for the 10 cor-pora of the PASCAL shared task (evaluated on allsentence lengths).
Table 3 presents the test per-formance for each language with the best hyper-parameter chosen from the set {100, 1000, 2500}.Also included are the best published results fromthe joint tag/dependency induction shared task(ST) as well as the results from Bisk and Hock-enmaier (2013), the only existing numbers formultilingual CCG induction (BH) with gold part-of-speech tags.
Note that the systems in ST donot have access to any gold-standard POS tags,whereas our system has access to the gold tags forVM N / V / O This ST @15 BHCzech250042 86 / 67 / 67 9.49 33.2 12.2 50.7English250059 87 / 76 / 85 43.8 24.4 51.6 62.9CHILDES250068 84 / 97 / 89 47.2 42.2 47.5 73.3Portuguese250055 88 / 81 / 69 55.5 31.7 55.8 70.5Dutch100050 81 / 81 / 82 39.9 33.7 43.8 54.4Basque100052 2 / 78 / 95 31.1 28.7 35.2 45.0Swed sh100050 89 / 74 / 85 45.8 28.2 52.9 66.9Slovene100050 83 / 75 / 79 18.5 19.2 23.6 46.4Danish10059 95 / 79 / 82 16.1 31.9 17.8 58.5Arabic10051 85 / 76 / 90 34.5 44.4 43.7 65.1Average 54 78 / 78 / 82 34.2 31.8 38.4 59.4Table 3: Tagging VM and N/V/O Recall along-side Directed Accuracy for our approach and thebest shared task baseline.
Additionally, we pro-vide results for length 15 to compare to previ-ously published results ([ST]: Best of the PAS-CAL joint tag/dependency induction shared tasksystems; [BH]: Bisk and Hockenmaier (2013).the three most frequent words of each cluster.The languages are sorted by the number of non-punctuation tokens in sentences of up to length20.
Despite our average performance (34.2) beingslightly higher than the shared task (31.8), the st.deviation is substantial (?
= 15.2 vs ?ST= 7.5).It seems apparent from the results that while datasparsity may play a role in affecting performance,the more linguistically interesting thread appearsto be morphology.
Czech is perhaps a prime ex-ample, as it has twice the data of the next largestlanguage (700K tokens vs 336K in English), butour approach still performs poorly.Finally, while we saw that the hard clusteringsystems outperformed the HMM for our experi-ments, this is perhaps best explained by analyzingthe average number of gold fine-grained tags perlexical type in each of the corpora.
We found,counterintuitively, that the ?difficult?
languageshad lower average number of tags per type (1.01for Czech, 1.03 for Arabic) than English (1.17)which was the most ambiguous.
This is likely dueto morphology distinguishing otherwise ambigu-ous lemmas.6 Cluster AnalysisIn Table 4, we present the three most frequentwords from several clusters produced by theBMMM for English and Chinese.
We also pro-vide a noun/verb/other label for each of the wordsin the list.
One can clearly see that there are manyambiguous cases where having three labels voting873English Labels Chinese Chinese gloss Labelsshares, sales, business N, N, N ??,??,??
simultaneously, politics, production O, N, Nthe, its, their O, N, N ??,??,??
advance, hold, begin V, V, Vother, interest, chief O, N, O ?,?,?
in, have, for O, V, Oof, in, on O, O, O ??,??,??
China, Taiwan, USA N, N, Nup, expected, made O, V, V ?,?,?
also, will, then O, O, Obe, make, sell V, V, V ?,?,?
big, many, high O, N, O *offer, issue, work N, N, N * ?,??,??
is, desire, representative V, V, NTable 4: The top three words in BMMM clusters with their noun/verb/other labels.
In two cases (markedwith *) all three of the most frequent words also occurred as a verb at least one third of the time.on the class label proves a beneficial signal.
Wehave also marked two classes with * to draw thereader?s attention to a fully noun cluster in En-glish and an other cluster in Chinese which arehighly ambiguous.
Specifically, in both of thesecases the frequent words also occur frequently asverbs, providing additional motivation for a bettersoft-clustering algorithm in future work.How to most effectively use seed knowledgeand annotation is still an open question.
Ap-proaches range from labeling frequent words likethe work of Garrette and Baldridge (2013) to therecently introduced active learning approach ofStratos and Collins (2015).
In this work, we wereable to demonstrate high noun and verb recall withthe use of a very small set of labeled words be-cause they correspond to an existing clustering.In contrast, we found that labeling even the 1000most frequent words led to very few clusters beingcorrectly identified; e.g.
in English, using the 1000most frequent words results in identifying 2 verband 5 noun clusters, compared to our method?s 9verb and 16 noun clusters.
This is because themost frequent words tend to be clustered in a fewvery large clusters resulting in low coverage.Stratos and Collins (2015) demonstrated, simi-larly, that using a POS tagger?s confidence scoreto find ambiguous classes can lead to a highly ef-fective adaptive learning procedure, which strate-gically labels very few words for a very highly ac-curate system.
Our results align with this research,leading us to believe that this paradigm of guidedminimal supervision is a fruitful direction for fu-ture work.7 ConclusionsIn this paper, we have produced the first labeledsyntactic structures from raw text.
There remainsa noticeable performance gap due to the use of in-duced clusters in lieu of gold tags.
Based on ourfinal PASCAL results, there are several languageswhere our performance greatly exceeds the cur-rently published results, but equally many wherewe fall short.
It also appears to be the case that thisproblem correlates with morphology (e.g.
Arabic,Danish, Slovene, Basque, Czech) and some of thelowest performing intrinsic evaluations of the clus-tering and N/V/O labeling (Czech and Basque).In principle, the BMMM is taking morphologi-cal information into account, as it is provided withthe automatically produced suffixes of Morfessor.Unfortunately, its treatment of them simply as fea-tures from a ?black box?
appears to be too naivefor our purposes.
Properly modeling the rela-tionship between prefixes, stems and suffixes bothwithin the tag induction and parsing framework islikely necessary for a high performing system.Moving forward, additional raw text for train-ing, as well as enriching the clustering with in-duced syntactic information (Christodoulopouloset al., 2012) may close this gap.8 AcknowledgmentsWe want to thank Dan Roth and Cynthia Fisherfor their insight on the task.
Additionally, wewould like to thank the anonymous reviewersfor their useful questions and comments.
Thismaterial is based upon work supported by theNational Science Foundation under Grants No.1053856, 1205627, 1405883, by the National In-stitutes of Health under Grant HD054448, and byDARPA under agreement number FA8750-13-2-0008.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the author(s) and do not necessarily reflectthe views of the National Science Foundation, theNational Institutes of Health, DARPA or the U.S.Government.874ReferencesYonatan Bisk and Julia Hockenmaier.
2012.
SimpleRobust Grammar Induction with Combinatory Cat-egorial Grammars.
In Proceedings of the Twenty-Sixth Conference on Artificial Intelligence (AAAI-12), pages 1643?1649, Toronto, Canada, July.Yonatan Bisk and Julia Hockenmaier.
2013.
An HDPModel for Inducing Combinatory Categorial Gram-mars.
Transactions of the Association for Computa-tional Linguistics, pages 75?88.Yonatan Bisk and Julia Hockenmaier.
2015.
Prob-ing the linguistic strengths and limitations of unsu-pervised grammar induction.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics.Phil Blunsom and Trevor Cohn.
2010.
UnsupervisedInduction of Tree Substitution Grammars for De-pendency Parsing.
Proceedings of the 2010 Con-ference on Empirical Methods of Natural LanguageProcessing, pages 1204?1213, October.Phil Blunsom and Trevor Cohn.
2011.
A hierarchi-cal pitman-yor process hmm for unsupervised partof speech induction.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages865?874, Portland, Oregon, USA, June.Prachya Boonkwan and Mark Steedman.
2011.
Gram-mar Induction from Text Using Small Syntactic Pro-totypes.
In Proceedings of 5th International JointConference on Natural Language Processing, pages438?446, Chiang Mai, Thailand, November.Peter F Brown, Peter V deSouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-Based n-gram Models of Natural Language.Computational Linguistics, 18.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsuper-vised PoS induction: How far have we come?
InProceedings of EMNLP, pages 575?584.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2011.
A Bayesian Mixture Modelfor Part-of-Speech Induction Using Multiple Fea-tures.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, Edinburgh, Scotland, UK., July.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2012.
Turning the pipeline intoa loop: iterated unsupervised dependency parsingand PoS induction.
In WILS ?12: Proceedings ofthe NAACL-HLT Workshop on the Induction of Lin-guistic Structure, June.Stephen Clark, Julia Hockenmaier, and Mark Steed-man.
2002.
Building deep dependency structuresusing a wide-coverage ccg parser.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 327?334, Philadelphia,Pennsylvania, USA, July.Mathias Creutz and Krista Lagus.
2006.
Morfessor inthe Morpho challenge.
In Proceedings of the PAS-CAL Challenge Workshop on Unsupervised Segmen-tation of Words into Morphemes, pages 12?17.Dan Garrette and Jason Baldridge.
2013.
Learninga Part-of-Speech Tagger from Two Hours of Anno-tation.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 138?147, Atlanta, Georgia, June.Dan Garrette, Chris Dyer, Jason Baldridge, and Noah ASmith.
2015.
Weakly-Supervised Grammar-Informed Bayesian CCG Parser Learning.
In Pro-ceedings of the Association for the Advancement ofArtificial Intelligence.Douwe Gelling, Trevor Cohn, Phil Blunsom, andJo?ao V Graca.
2012.
The PASCAL Challengeon Grammar Induction.
In NAACL HLT Workshopon Induction of Linguistic Structure, pages 64?80,Montr?eal, Canada, June.Jennifer Gillenwater, Kuzman Ganchev, Jo?ao V Graca,Fernando Pereira, and Ben Taskar.
2011.
Pos-terior Sparsity in Unsupervised Dependency Pars-ing.
The Journal of Machine Learning Research,12:455?490, February.Aria Haghighi and Dan Klein.
2006.
Prototype-DrivenGrammar Induction.
In Association for Computa-tional Linguistics, pages 881?888, Morristown, NJ,USA.William P. Headden, III, David McClosky, and EugeneCharniak.
2008.
Evaluating unsupervised part-of-speech tagging for grammar induction.
In Proceed-ings of the 22Nd International Conference on Com-putational Linguistics - Volume 1, COLING ?08,pages 329?336, Stroudsburg, PA, USA.Julia Hockenmaier and Mark Steedman.
2002.
Gen-erative models for statistical parsing with combina-tory categorial grammar.
In Proceedings of 40th An-nual Meeting of the Association for ComputationalLinguistics, pages 335?342, Philadelphia, Pennsyl-vania, USA, July.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and Depen-dency Structures Extracted from the Penn Treebank.Computational Linguistics, 33:355?396, September.Mark Johnson.
2007.
Why doesn?t EM find goodHMM POS-taggers.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), January.875Dan Klein and Christopher D Manning.
2004.
Corpus-Based Induction of Syntactic Structure: Models ofDependency and Constituency.
In Proceedings ofthe 42nd Meeting of the Association for Compu-tational Linguistics (ACL?04), Main Volume, pages478?485, Barcelona, Spain, July.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using universal linguistic knowl-edge to guide grammar induction.
In Proceedings ofthe 2010 Conference on Empirical Methods in Nat-ural Language Processing, pages 1234?1244, Cam-bridge, MA, October.Slav Petrov, Dipanjan Das, and Ryan McDonald.2012.
A Universal Part-of-Speech Tagset.
In Pro-ceedings of the Eighth International Conference onLanguage Resources and Evaluation (LREC-2012),pages 2089?2096, Istanbul, Turkey, May.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clus-ter evaluation measure.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 410?420, Prague, Czech Republic, June.Yoav Seginer.
2007.
Fast Unsupervised IncrementalParsing.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 384?391, Prague, Czech Republic, June.Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,and Daniel Jurafsky.
2011.
Unsupervised Depen-dency Parsing without Gold Part-of-Speech Tags.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1281?1290, Edinburgh, Scotland, UK., July.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2013.
Breaking Out of Local Optima withCount Transforms and Model Recombination: AStudy in Grammar Induction.
In Empirical Methodsin Natural Language Processing.Mark Steedman.
2000.
The Syntactic Process.
TheMIT Press, September.Karl Stratos and Michael Collins.
2015.
Simple semi-supervised pos tagging.
In Proceedings of the 1stWorkshop on Vector Space Modeling for NaturalLanguage Processing, pages 79?87, Denver, Col-orado, June.Yee-Whye Teh, Michael I Jordan, Matthew J Beal, andDavid M Blei.
2006.
Hierarchical Dirichlet Pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Daniel Tse.
2013.
Chinese CCGBank: Deep Deriva-tions and Dependencies for Chinese CCG Parsing.Ph.D.
thesis, The University of Sydney.876
