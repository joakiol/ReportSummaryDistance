Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 473?480,Sydney, July 2006. c?2006 Association for Computational LinguisticsFactorizing Complex Models: A Case Study in MentionDetectionRadu Florian, Hongyan Jing, Nanda Kambhatla and Imed ZitouniIBM TJ Watson Research CenterYorktown Heights, NY 10598{raduf,hjing,nanda,izitouni}@us.ibm.comAbstractAs natural language understanding re-search advances towards deeper knowledgemodeling, the tasks become more and morecomplex: we are interested in more nu-anced word characteristics, more linguisticproperties, deeper semantic and syntacticfeatures.
One such example, explored inthis article, is the mention detection andrecognition task in the Automatic ContentExtraction project, with the goal of iden-tifying named, nominal or pronominal ref-erences to real-world entities?mentions?and labeling them with three types of in-formation: entity type, entity subtype andmention type.
In this article, we investi-gate three methods of assigning these re-lated tags and compare them on severaldata sets.
A system based on the methodspresented in this article participated andranked very competitively in the ACE?04evaluation.1 IntroductionInformation extraction is a crucial step toward un-derstanding and processing natural language data,its goal being to identify and categorize impor-tant information conveyed in a discourse.
Exam-ples of information extraction tasks are identifi-cation of the actors and the objects in writtentext, the detection and classification of the rela-tions among them, and the events they participatein.
These tasks have applications in, among otherfields, summarization, information retrieval, datamining, question answering, and language under-standing.One of the basic tasks of information extractionis the mention detection task.
This task is verysimilar to named entity recognition (NER), as theobjects of interest represent very similar concepts.The main difference is that the latter will identify,however, only named references, while mention de-tection seeks named, nominal and pronominal ref-erences.
In this paper, we will call the identifiedreferences mentions ?
using the ACE (NIST, 2003)nomenclature ?
to differentiate them from entitieswhich are the real-world objects (the actual person,location, etc) to which the mentions are referringto1.Historically, the goal of the NER task was to findnamed references to entities and quantity refer-ences ?
time, money (MUC-6, 1995; MUC-7, 1997).In recent years, Automatic Content Extractionevaluation (NIST, 2003; NIST, 2004) expanded thetask to also identify nominal and pronominal refer-ences, and to group the mentions into sets referringto the same entity, making the task more compli-cated, as it requires a co-reference module.
The setof identified properties has also been extended toinclude the mention type of a reference (whether itis named, nominal or pronominal), its subtype (amore specific type dependent on the main entitytype), and its genericity (whether the entity pointsto a specific entity, or a generic one2), besides thecustomary main entity type.
To our knowledge,little research has been done in the natural lan-guage processing context or otherwise on investi-gating the specific problem of how such multiple la-bels are best assigned.
This article compares threemethods for such an assignment.The simplest model which can be considered forthe task is to create an atomic tag by ?gluing?
to-gether the sub-task labels and considering the newlabel atomic.
This method transforms the prob-lem into a regular sequence classification task, sim-ilar to part-of-speech tagging, text chunking, andnamed entity recognition tasks.
We call this modelthe all-in-one model.
The immediate drawbackof this model is that it creates a large classifica-tion space (the cross-product of the sub-task clas-sification spaces) and that, during decoding, par-tially similar classifications will compete instead ofcooperate - more details are presented in Section3.1.
Despite (or maybe due to) its relative sim-plicity, this model obtained good results in severalinstances in the past, for POS tagging in morpho-logically rich languages (Hajic and Hladka?, 1998)1In a pragmatic sense, entities are sets of mentionswhich co-refer.2This last attribute, genericity, depends only looselyon local context.
As such, it should be assigned whileexamining all mentions in an entity, and for this reasonis beyond the scope of this article.473and mention detection (Jing et al, 2003; Florianet al, 2004).At the opposite end of classification methodol-ogy space, one can use a cascade model, which per-forms the sub-tasks sequentially in a predefined or-der.
Under such a model, described in Section 3.3,the user will build separate models for each sub-task.
For instance, it could first identify the men-tion boundaries, then assign the entity type, sub-type, and mention level information.
Such a modelhas the immediate advantage of having smallerclassification spaces, with the drawback that it re-quires a specific model invocation path.In between the two extremes, one can use a jointmodel, which models the classification space in thesame way as the all-in-one model, but where theclassifications are not atomic.
This system incor-porates information about sub-model parts, suchas whether the current word starts an entity (ofany type), or whether the word is part of a nomi-nal mention.The paper presents a novel contrastive analysisof these three models, comparing them on severaldatasets in three languages selected from the ACE2003 and 2004 evaluations.
The methods describedhere are independent of the underlying classifiers,and can be used with any sequence classifiers.
Allexperiments in this article use our in-house imple-mentation of a maximum entropy classifier (Flo-rian et al, 2004), which we selected because of itsflexibility of integrating arbitrary types of features.While we agree that the particular choice of classi-fier will undoubtedly introduce some classifier bias,we want to point out that the described procedureshave more to do with the organization of the searchspace, and will have an impact, one way or another,on most sequence classifiers, including conditionalrandom field classifiers.3The paper is organized as follows: Section 2 de-scribes the multi-task classification problem andprior work, Section 3.3 presents and contrasts thethree meta-classification models.
Section 4 outlinesthe experimental setup and the obtained results,and Section 5 concludes the paper.2 Multi-Task ClassificationMany tasks in Natural Language Processing in-volve labeling a word or sequence of words witha specific property; classic examples are part-of-speech tagging, text chunking, word sense disam-biguation and sentiment classification.
Most of thetime, the word labels are atomic labels, containinga very specific piece of information (e.g.
the word3While not wishing to delve too deep into the issueof label bias, we would also like to point out (as itwas done, for instance, in (Klein, 2003)) that the labelbias of MEMM classifiers can be significantly reducedby allowing them to examine the right context of theclassification point - as we have done with our model.is noun plural, or starts a noun phrase, etc).
Thereare cases, though, where the labels consist of sev-eral related, but not entirely correlated, properties;examples include mention detection?the task weare interested in?, syntactic parsing with func-tional tag assignment (besides identifying the syn-tactic parse, also label the constituent nodes withtheir functional category, as defined in the PennTreebank (Marcus et al, 1993)), and, to a lesserextent, part-of-speech tagging in highly inflectedlanguages.4The particular type of mention detection that weare examining in this paper follows the ACE gen-eral definition: each mention in the text (a refer-ence to a real-world entity) is assigned three typesof information:5?
An entity type, describing the type of the en-tity it points to (e.g.
person, location, organi-zation, etc)?
An entity subtype, further detailing the type(e.g.
organizations can be commercial, gov-ernmental and non-profit, while locations canbe a nation, population center, or an interna-tional region)?
A mention type, specifying the way the en-tity is realized ?
a mention can be named(e.g.
John Smith), nominal (e.g.
professor),or pronominal (e.g.
she).Such a problem ?
where the classification consistsof several subtasks or attributes ?
presents addi-tional challenges, when compared to a standardsequence classification task.
Specifically, there areinter-dependencies between the subtasks that needto be modeled explicitly; predicting the tags inde-pendently of each other will likely result in incon-sistent classifications.
For instance, in our runningexample of mention detection, the subtype task isdependent on the entity type; one could not have aperson with the subtype non-profit.
On the otherhand, the mention type is relatively independent ofthe entity type and/or subtype: each entity typecould be realized under any mention type and vice-versa.The multi-task classification problem has beensubject to investigation in the past.
Caruanaet al (1997) analyzed the multi-task learning4The goal there is to also identify word propertiessuch as gender, number, and case (for nouns), moodand tense (for verbs), etc, besides the main POS tag.The task is slightly different, though, as these proper-ties tend to have a stronger dependency on the lexicalform of the classified word.5There is a fourth assigned type ?
a flag specifyingwhether a mention is specific (i.e.
it refers at a clearentity), generic (refers to a generic type, e.g.
?the sci-entists believe ..?
), unspecified (cannot be determinedfrom the text), or negative (e.g.
?no person would dothis?).
The classification of this type is beyond thegoal of this paper.474(MTL) paradigm, where individual related tasksare trained together by sharing a common rep-resentation of knowledge, and demonstrated thatthis strategy yields better results than one-task-at-a-time learning strategy.
The authors used a back-propagation neural network, and the paradigm wastested on several machine learning tasks.
It alsocontains an excellent discussion on how and whythe MTL paradigm is superior to single-task learn-ing.
Florian and Ngai (2001) used the same multi-task learning strategy with a transformation-basedlearner to show that usually disjointly handledtasks perform slightly better under a joint model;the experiments there were run on POS taggingand text chunking, Chinese word segmentation andPOS tagging.
Sutton et al (2004) investigatedthe multitask classification problem and used a dy-namic conditional random fields method, a gener-alization of linear-chain conditional random fields,which can be viewed as a probabilistic generaliza-tion of cascaded, weighted finite-state transducers.The subtasks were represented in a single graphi-cal model that explicitly modeled the sub-task de-pendence and the uncertainty between them.
Thesystem, evaluated on POS tagging and base-nounphrase segmentation, improved on the sequentiallearning strategy.In a similar spirit to the approach presented inthis article, Florian (2002) considers the task ofnamed entity recognition as a two-step process:the first is the identification of mention boundariesand the second is the classification of the identifiedchunks, therefore considering a label for each wordbeing formed from two sub-labels: one that spec-ifies the position of the current word relative in amention (outside any mentions, starts a mention, isinside a mention) and a label specifying the men-tion type .
Experiments on the CoNLL?02 datashow that the two-process model yields consider-ably higher performance.Hacioglu et al (2005) explore the same task, in-vestigating the performance of the AIO and thecascade model, and find that the two models havesimilar performance, with the AIO model having aslight advantage.
We expand their study by addingthe hybrid joint model to the mix, and further in-vestigate different scenarios, showing that the cas-cade model leads to superior performance most ofthe time, with a few ties, and show that the cas-cade model is especially beneficial in cases wherepartially-labeled data (only some of the componentlabels are given) is available.
It turns out though,(Hacioglu, 2005) that the cascade model in (Ha-cioglu et al, 2005) did not change to a ?mentionview?
sequence classification6 (as we did in Section3.3) in the tasks following the entity detection, toallow the system to use longer range features.6As opposed to a ?word view?.3 Classification ModelsThis section presents the three multi-task classifi-cation models, which we will experimentally con-trast in Section 4.
We are interested in performingsequence classification (e.g.
assigning a label toeach word in a sentence, otherwise known as tag-ging).
Let X denote the space of sequence elements(words) and Y denote the space of classifications(labels), both of them being finite spaces.
Our goalis to build a classifierh : X+ ?
Y+which has the property that |h (x?
)| = |x?| ,?x?
?
X+(i.e.
the size of the input sequence is preserved).This classifier will select the a posteriori most likelylabel sequence y?
= argmaxy??
p(y??|x?
); in our casep (y?|x?)
is computed through the standard Markovassumption:p (y1,m| x?)
=?ip (yi|x?, yi?n+1,i?1) (1)where yi,j denotes the sequence of labels yi..yj .Furthermore, we will assume that each label yis composed of a number of sub-labels y =(y1y2 .
.
.
yk)7; in other words, we will assume thefactorization of the label space into k subspacesY = Y1 ?
Y2 ?
.
.
.?
Yk.The classifier we used in the experimental sec-tion is a maximum entropy classifier (similar to(McCallum et al, 2000))?which can integrate sev-eral sources of information in a rigorous manner.It is our empirical observation that, from a perfor-mance point of view, being able to use a diverseand abundant feature set is more important thanclassifier choice, and the maximum entropy frame-work provides such a utility.3.1 The All-In-One ModelAs the simplest model among those presented here,the all-in-one model ignores the natural factoriza-tion of the output space and considers all labels asatomic, and then performs regular sequence clas-sification.
One way to look at this process is thefollowing: the classification space Y = Y1 ?
Y2 ?.
.
.
?
Yk is first mapped onto a same-dimensionalspace Z through a one-to-one mapping o : Y ?
Z;then the features of the system are defined on thespace X+ ?Z, instead of X+ ?
Y.While having the advantage of being simple, itsuffers from some theoretical disadvantages:?
The classification space can be very large, be-ing the product of the dimensions of sub-taskspaces.
In the case of the 2004 ACE datathere are 7 entity types, 4 mention types andmany subtypes; the observed number of actual7We can assume, without any loss of generality, thatall labels have the same number of sub-labels.475All-In-One Model Joint ModelB-PERB-LOCB-ORG B-B-MISCTable 1: Features predicting start of an entity inthe all-in-one and joint modelssub-label combinations on the training data is401.
Since the dynamic programing (Viterbi)search?s runtime dependency on the classifica-tion space is O (|Z|n) (n is the Markov depen-dency size), using larger spaces will negativelyimpact the decoding run time.8?
The probabilities p (zi|x?, zi?n,i?1) requirelarge data sets to be computed properly.
Ifthe training data is limited, the probabilitiesmight be poorly estimated.?
The model is not friendly to partial evaluationor weighted sub-task evaluation: different, butpartially similar, labels will compete againsteach other (because the system will return aprobability distribution over the classificationspace), sometimes resulting in wrong partialclassification.9?
The model cannot directly use data that isonly partially labeled (i.e.
not all sub-labelsare specified).Despite the above disadvantages, this model hasperformed well in practice: Hajic and Hladka?
(1998) applied it successfully to find POS se-quences for Czech and Florian et al (2004) re-ports good results on the 2003 ACE task.
Mostsystems that participated in the CoNLL 2002 and2003 shared tasks on named entity recognition(Tjong Kim Sang, 2002; Tjong Kim Sang andDe Meulder, 2003) applied this model, as theymodeled the identification of mention boundariesand the assignment of mention type at the sametime.3.2 The Joint ModelThe joint model differs from the all-in-one modelin the fact that the labels are no longer atomic: thefeatures of the system can inspect the constituentsub-labels.
This change helps alleviate the data8From a practical point of view, it might not be veryimportant, as the search is pruned in most cases to onlya few hypotheses (beam-search); in our case, pruningthe beam only introduced an insignificant model searcherror (0.1 F-measure).9To exemplify, consider that the system outputs thefollowing classifications and probabilities: O (0.2), B-PER-NAM (0.15), B-PER-NOM (0.15); even the latter2 suggest that the word is the start of a person mention,the O label will win because the two labels competedagainst each other.Detect Boundaries   & Entity TypesAssemble full tagDetect Entity Subtype Detect Mention TypeFigure 1: Cascade flow example for mention detec-tion.sparsity encountered by the previous model by al-lowing sub-label modeling.
The joint model the-oretically compares favorably with the all-in-onemodel:?
The probabilities p (yi|x?, yi?n,i?1) =p((y1i , .
.
.
, yki) |x?,(yji?n,i?1)j=1,k)mightrequire less training data to be properlyestimated, as different sub-labels can bemodeled separately.?
The joint model can use features that predictjust one or a subset of the sub-labels.
Ta-ble 1 presents the set of basic features thatpredict the start of a mention for the CoNLLshared tasks for the two models.
While thejoint model can encode the start of a mentionin one feature, the all-in-one model needs touse four features, resulting in fewer counts perfeature and, therefore, yielding less reliably es-timated features (or, conversely, it needs moredata for the same estimation confidence).?
The model can predict some of the sub-tagsahead of the others (i.e.
create a dependencystructure on the sub-labels).
The model usedin the experimental section predicts the sub-labels by using only sub-labels for the previouswords, though.?
It is possible, though computationally expen-sive, for the model to use additional datathat is only partially labeled, with the modelchange presented later in Section 3.4.3.3 The Cascade ModelFor some tasks, there might already exist a naturalhierarchy among the sub-labels: some sub-labelscould benefit from knowing the value of other,primitive, sub-labels.
For example,?
For mention detection, identifying the men-tion boundaries can be considered as a primi-tive task.
Then, knowing the mention bound-aries, one can assign an entity type, subtype,and mention type to each mention.?
In the case of parsing with functional tags, onecan perform syntactic parsing, then assign thefunctional tags to the internal constituents.476Words Since Donna Karan International went public in 1996 ...Labels O B-ORG I-ORG I-ORG O O O O ...Figure 2: Sequence tagging for mention detection: the case for a cascade model.?
For POS tagging, one can detect the mainPOS first, then detect the other specific prop-erties, making use of the fact that one knowsthe main tag.The cascade model is essentially a factorizationof individual classifiers for the sub-tasks; in thisframework, we will assume that there is a moreor less natural dependency structure among sub-tasks, and that models for each of the subtaskswill be built and applied in the order defined bythe dependency structure.
For example, as shownin Figure 1, one can detect mention boundaries andentity type (at the same time), then detect mentiontype and subtype in ?parallel?
(i.e.
no dependencyexists between these last 2 sub-tags).A very important advantage of the cascademodel is apparent in classification cases whereidentifying chunks is involved (as is the case withmention detection), similar to advantages thatrescoring hypotheses models have: in the secondstage, the chunk classification stage, it can switchto a mention view, where the classification unitsare entire mentions and words outside of mentions.This allows the system to make use of aggregatefeatures over the mention words (e.g.
all the wordsare capitalized), and to also effectively use a largerMarkov window (instead of 2-3 words, it will use 2-3 chunks/words around the word of interest).
Fig-ure 2 contains an example of such a case: the cas-cade model will have to predict the type of theentire phrase Donna Karan International, in thecontext ?Since <chunk> went public in ..?, whichwill give it a better opportunity to classify it as anorganization.
In contrast, because the joint modeland AIO have a word view of the sentence, will lackthe benefit of examining the larger region, and willnot have access at features that involve partial fu-ture classifications (such as the fact that anothermention of a particular type follows).Compared with the other two models, this clas-sification method has the following advantages:?
The classification spaces for each subtask areconsiderably smaller; this fact enables the cre-ation of better estimated models?
The problem of partially-agreeing competinglabels is completely eliminated?
One can easily use different/additional data totrain any of the sub-task models.3.4 Adding Partially Labeled DataAnnotated data can be sometimes expensive tocome by, especially if the label set is complex.
Butnot all sub-tasks were created equal: some of themmight be easier to predict than others and, there-fore, require less data to train effectively in a cas-cade setup.
Additionally, in realistic situations,some sub-tasks might be considered to have moreinformational content than others, and have prece-dence in evaluation.
In such a scenario, one mightdecide to invest resources in annotating additionaldata only for the particularly interesting sub-task,which could reduce this effort significantly.To test this hypothesis, we annotated additionaldata with the entity type only.
The cascade modelcan incorporate this data easily: it just adds itto the training data for the entity type classifiermodel.
While it is not immediately apparent howto incorporate this new data into the all-in-one andjoint models, in order to maintain fairness in com-paring the models, we modified the procedures toallow for the inclusion.
Let T denote the originaltraining data, and T ?
denote the additional train-ing data.For the all-in-one model, the additional trainingdata cannot be incorporated directly; this is an in-herent deficiency of the AIO model.
To facilitate afair comparison, we will incorporate it in an indi-rect way: we train a classifier C on the additionaltraining data T ?, which we then use to classify theoriginal training data T .
Then we train the all-in-one classifier on the original training data T ,adding the features defined on the output of ap-plying the classifier C on T .The situation is better for the joint model: thenew training data T ?
can be incorporated directlyinto the training data T .10 The maximum entropymodel estimates the model parameters by maxi-mizing the data log-likelihoodL =?(x,y)p?
(x, y) log q?
(y|x)where p?
(x, y) is the observed probability dis-tribution of the pair (x, y) and q?
(y|x) =1Z?j exp (?j ?
fj (x, y)) is the conditional MEprobability distribution as computed by the model.In the case where some of the data is partially an-notated, the log-likelihood becomesL =?
(x,y)?T ?T ?p?
(x, y) log q?
(y|x)10The solution we present here is particular forMEMM models (though similar solutions may exist forother models as well).
We also assume the reader is fa-miliar with the normal MaxEnt training procedure; wepresent here only the differences to the standard algo-rithm.
See (Manning and Schu?tze, 1999) for a gooddescription.477=?(x,y)?Tp?
(x, y) log q?
(y|x)+?
(x,y)?T ?p?
(x, y) log q?
(y|x) (2)The only technical problem that we are faced withhere is that we cannot directly estimate the ob-served probability p?
(x, y) for examples in T ?, sincethey are only partially labeled.
Borrowing theidea from the expectation-maximization algorithm(Dempster et al, 1977), we can replace this proba-bility by the re-normalized system proposed prob-ability: for (x, yx) ?
T ?, we defineq?
(x, y) = p?
(x) ?
(y ?
yx) q?
(y|x)?y?
?yx q?
(y?|x)?
??
?=q??
(y|x)where yx is the subset of labels from Y which areconsistent with the partial classification of x in T ?.?
(y ?
yx) is 1 if and only if y is consistent withthe partial classification yx.11 The log-likelihoodcomputation in Equation (2) becomesL =?(x,y)?Tp?
(x, y) log q?
(y|x)+?
(x,y)?T ?q?
(x, y) log q?
(y|x)To further simplify the evaluation, the quantitiesq?
(x, y) are recomputed every few steps, and areconsidered constant as far as finding the optimum?
values is concerned (the partial derivative com-putations and numerical updates otherwise becomequite complicated, and the solution is no longerunique).
Given this new evaluation function, thetraining algorithm will proceed exactly the sameway as in the normal case where all the data isfully labeled.4 ExperimentsAll the experiments in this section are run on theACE 2003 and 2004 data sets, in all the threelanguages covered: Arabic, Chinese, and English.Since the evaluation test set is not publicly avail-able, we have split the publicly available data intoa 80%/20% data split.
To facilitate future compar-isons with work presented here, and to simulate arealistic scenario, the splits are created based onarticle dates: the test data is selected as the last20% of the data in chronological order.
This way,the documents in the training and test data setsdo not overlap in time, and the ones in the testdata are posterior to the ones in the training data.Table 2 presents the number of documents in thetraining/test datasets for the three languages.11For instance, the full label B-PER is consistentwith the partial label B, but not with O or I.Language Training TestArabic 511 178Chinese 480 166English 2003 658 139English 2004 337 114Table 2: Datasets size (number of documents)Each word in the training data is labeled withone of the following properties:12?
if it is not part of any entity, it?s labeled as O?
if it is part of an entity, it contains a tag spec-ifying whether it starts a mention (B -) or isinside a mention (I -).
It is also labeled withthe entity type of the mention (seven possibletypes: person, organization, location, facility,geo-political entity, weapon, and vehicle), themention type (named, nominal, pronominal,or premodifier13), and the entity subtype (de-pends on the main entity type).The underlying classifier used to run the experi-ments in this article is a maximum entropy modelwith a Gaussian prior (Chen and Rosenfeld, 1999),making use of a large range of features, includ-ing lexical (words and morphs in a 3-word win-dow, prefixes and suffixes of length up to 4, Word-Net (Miller, 1995) for English), syntactic (POStags, text chunks), gazetteers, and the output ofother information extraction models.
These fea-tures were described in (Florian et al, 2004), andare not discussed here.
All three methods (AIO,joint, and cascade) instantiate classifiers based onthe same feature types whenever possible.
In termsof language-specific processing, the Arabic systemuses as input morphological segments, while theChinese system is a character-based model (the in-put elements x ?
X are characters), but it hasaccess to word segments as features.Performance in the ACE task is officially eval-uated using a special-purpose measure, the ACEvalue metric (NIST, 2003; NIST, 2004).
Thismetric assigns a score based on the similarity be-tween the system?s output and the gold-standardat both mention and entity level, and assigns dif-ferent weights to different entity types (e.g.
theperson entity weights considerably more than a fa-cility entity, at least in the 2003 and 2004 evalu-ations).
Since this article focuses on the mentiondetection task, we decided to use the more intu-itive (unweighted) F-measure: the harmonic meanof precision and recall.12The mention encoding is the IOB2 encoding pre-sented in (Tjong Kim Sang and Veenstra, 1999) andintroduced by (Ramshaw and Marcus, 1994) for thetask of base noun phrase chunking.13This is a special class, used for mentions that mod-ify other labeled mentions; e.g.
French in ?Frenchwine?.
This tag is specific only to ACE?04.478For the cascade model, the sub-task flow is pre-sented in Figure 1.
In the first step, we identifythe mention boundaries together with their entitytype (e.g.
person, organization, etc).
In prelimi-nary experiments, we tried to ?cascade?
this task.The performance was similar on both strategies;the separated model would yield higher recall atthe expense of precision, while the combined modelwould have higher precision, but lower recall.
Wedecided to use in the system with higher precision.Once the mentions are identified and classified withthe entity type property, the data is passed, in par-allel, to the mention type detector and the subtypedetector.For English and Arabic, we spent three person-weeks to annotate additional data labeled withonly the entity type information: 550k words forEnglish and 200k words for Arabic.
As mentionedearlier, adding this data to the cascade model is atrivial task: the data just gets added to the train-ing data, and the model is retrained.
For the AIOmodel, we have build another mention classifier onthe additional training data, and labeled the orig-inal ACE training data with it.
It is importantto note here that the ACE training data (calledT in Section 3.4) is consistent with the additionaltraining data T ?
: the annotation guidelines for T ?are the same as for the original ACE data, but weonly labeled entity type information.
The result-ing classifications are then used as features in thefinal AIO classifier.
The joint model uses the addi-tional partially-labeled data in the way describedin Section 3.4; the probabilities q?
(x, y) are updatedevery 5 iterations.Table 3 presents the results: overall, the cascademodel performs significantly better than the all-in-one model in four out the six tested cases - thenumbers presented in bold reflect that the differ-ence in performance to the AIO model is statisti-cally significant.14 The joint model, while manag-ing to recover some ground, falls in between theAIO and the cascade models.When additional partially-labeled data wasavailable, the cascade and joint models receive astatistically significant boost in performance, whilethe all-in-one model?s performance barely changes.This fact can be explained by the fact that the en-tity type-only model is in itself errorful; measuringthe performance of the model on the training datayields a performance of 82 F-measure;15 thereforethe AIO model will only access partially-correct14To assert the statistical significance of the results,we ran a paired Wilcoxon test over the series obtainedby computing F-measure on each document in the testset.
The results are significant at a level of at least0.009.15Since the additional training data is consistent inthe labeling of the entity type, such a comparison is in-deed possible.
The above mentioned score is on entitytypes only.Language Data+ A-I-O Joint CascadeArabic?04 no 59.2 59.1 59.7yes 59.4 60.0 60.7English?04 no 72.1 72.3 73.7yes 72.5 74.1 75.2Chinese?04 no 71.2 71.7 71.7English ?03 no 79.5 79.5 79.7Table 3: Experimental results: F-measure on thefull labelLanguage Data+ A-I-O Joint CascadeArabic?04 no 66.3 66.5 67.5yes 66.4 67.9 68.9English?04 no 77.9 78.1 79.2yes 78.3 80.5 82.6Chinese?04 no 75.4 76.1 76.8English ?03 no 80.4 80.4 81.1Table 4: F-measure results on entity type onlydata, and is unable to make effective use of it.In contrast, the training data for the entity typein the cascade model effectively triples, and thischange is reflected positively in the 1.5 increase inF-measure.Not all properties are equally valuable: the en-tity type is arguably more interesting than theother properties.
If we restrict ourselves to eval-uating the entity type output only (by projectingthe output label to the entity type only), the differ-ence in performance between the all-in-one modeland cascade is even more pronounced, as shown inTable 4.
The cascade model outperforms here boththe all-in-one and joint models in all cases exceptEnglish?03, where the difference is not statisticallysignificant.As far as run-time speed is concerned, the AIOand cascade models behave similarly: our imple-mentation tags approximately 500 tokens per sec-ond (averaged over the three languages, on a Pen-tium 3, 1.2Ghz, 2Gb of memory).
Since a MaxEntimplementation is mostly dependent on the num-ber of features that fire on average on a example,and not on the total number of features, the jointmodel runs twice as slow: the average number offeatures firing on a particular example is consider-ably higher.
On average, the joint system can tagapproximately 240 words per second.
The traintime is also considerably longer; it takes 15 times aslong to train the joint model as it takes to train theall-in-one model (60 mins/iteration compared to4 mins/iteration); the cascade model trains fasterthan the AIO model.One last important fact that is worth mention-ing is that a system based on the cascade modelparticipated in the ACE?04 competition, yieldingvery competitive results in all three languages.4795 ConclusionAs natural language processing becomes more so-phisticated and powerful, we start focus our at-tention on more and more properties associatedwith the objects we are seeking, as they allow fora deeper and more complex representation of thereal world.
With this focus comes the question ofhow this goal should be accomplished ?
either de-tect all properties at once, one at a time througha pipeline, or a hybrid model.
This paper presentsthree methods through which multi-label sequenceclassification can be achieved, and evaluates andcontrasts them on the Automatic Content Extrac-tion task.
On the ACE mention detection task,the cascade model which predicts first the mentionboundaries and entity types, followed by mentiontype and entity subtype outperforms the simple all-in-one model in most cases, and the joint model ina few cases.Among the proposed models, the cascade ap-proach has the definite advantage that it can easilyand productively incorporate additional partially-labeled data.
We also presented a novel modifica-tion of the joint system training that allows for thedirect incorporation of additional data, which in-creased the system performance significantly.
Theall-in-one model can only incorporate additionaldata in an indirect way, resulting in little to nooverall improvement.Finally, the performance obtained by the cas-cade model is very competitive: when paired with acoreference module, it ranked very well in the ?En-tity Detection and Tracking?
task in the ACE?04evaluation.ReferencesR.
Caruana, L. Pratt, and S. Thrun.
1997.
Multitasklearning.
Machine Learning, 28:41.Stanley F. Chen and Ronald Rosenfeld.
1999.
A gaus-sian prior for smoothing maximum entropy models.Technical Report CMU-CS-99-108, Computer Sci-ence Department, Carnegie Mellon University.A.
P. Dempster, N. M. Laird, , and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal statistical Soci-ety, 39(1):1?38.R.
Florian and G. Ngai.
2001.
Multidimensionaltransformation-based learning.
In Proceedings ofCoNLL?01, pages 1?8.R.
Florian, H. Hassan, A. Ittycheriah, H. Jing,N.
Kambhatla, X. Luo, N Nicolov, and S Roukos.2004.
A statistical model for multilingual entity de-tection and tracking.
In Proceedings of the HumanLanguage Technology Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: HLT-NAACL 2004, pages 1?8.R.
Florian.
2002.
Named entity recognition as ahouse of cards: Classifier stacking.
In Proceedingsof CoNLL-2002, pages 175?178.Kadri Hacioglu, Benjamin Douglas, and Ying Chen.2005.
Detection of entity mentions occuring in en-glish and chinese text.
In Proceedings of HumanLanguage Technology Conference and Conference onEmpirical Methods in Natural Language Process-ing, pages 379?386, Vancouver, British Columbia,Canada, October.
Association for ComputationalLinguistics.Kadri Hacioglu.
2005.
Private communication.J.
Hajic and Hladka?.
1998.
Tagging inflective lan-guages: Prediction of morphological categories for arich, structured tagset.
In Proceedings of the 36thAnnual Meeting of the ACL and the 17th ICCL,pages 483?490, Montre?al, Canada.H.
Jing, R. Florian, X. Luo, T. Zhang, and A. It-tycheriah.
2003.
HowtogetaChineseName(Entity):Segmentation and combination issues.
In Proceed-ings of EMNLP?03, pages 200?207.Dan Klein.
2003.
Maxent models, conditional estima-tion, and optimization, without the magic.
Tutorialpresented at NAACL-03 and ACL-03.C.
D. Manning and H. Schu?tze.
1999.
Foundations ofStatistical Natural Language Processing.
MIT Press.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of en-glish: The penn treebank.
Computational Linguis-tics, 19:313?330.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum entropy markov modelsfor information extraction and segmentation.
In Pro-ceedings of ICML-2000.G.
A. Miller.
1995.
WordNet: A lexical database.Communications of the ACM, 38(11).MUC-6.
1995.
The sixth mes-sage understanding conference.www.cs.nyu.edu/cs/faculty/grishman/muc6.html.MUC-7.
1997.
The seventh mes-sage understanding conference.www.itl.nist.gov/iad/894.02/related projects/muc/proceedings/muc 7 toc.html.NIST.
2003.
The ACE evaluation plan.www.nist.gov/speech/tests/ace/index.htm.NIST.
2004.
The ACE evaluation plan.www.nist.gov/speech/tests/ace/index.htm.L.
Ramshaw and M. Marcus.
1994.
Exploring the sta-tistical derivation of transformational rule sequencesfor part-of-speech tagging.
In Proceedings of theACL Workshop on Combining Symbolic and Statis-tical Approaches to Language, pages 128?135.C.
Sutton, K. Rohanimanesh, and A. McCallum.2004.
Dynamic conditional random fields: Factor-ized probabilistic models for labeling and segment-ing sequence data.
In In Proceedings of the Twenty-First International Conference on Machine Learning(ICML-2004).Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the conll-2003 shared task:Language-independent named entity recognition.
InWalter Daelemans and Miles Osborne, editors, Pro-ceedings of CoNLL-2003, pages 142?147.
Edmonton,Canada.E.
F. Tjong Kim Sang and J. Veenstra.
1999.
Repre-senting text chunks.
In Proceedings of EACL?99.E.
F. Tjong Kim Sang.
2002.
Introduction to the conll-2002 shared task: Language-independent named en-tity recognition.
In Proceedings of CoNLL-2002,pages 155?158.480
