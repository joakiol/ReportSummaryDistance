Proceedings of the 12th Conference of the European Chapter of the ACL, pages 790?798,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsFast Full Parsing by Linear-Chain Conditional Random FieldsYoshimasa Tsuruoka??
Jun?ichi Tsujii???
Sophia Ananiadou???
School of Computer Science, University of Manchester, UK?
National Centre for Text Mining (NaCTeM), UK?
Department of Computer Science, University of Tokyo, Japan{yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.ukAbstractThis paper presents a chunking-based dis-criminative approach to full parsing.
Weconvert the task of full parsing into a seriesof chunking tasks and apply a conditionalrandom field (CRF) model to each levelof chunking.
The probability of an en-tire parse tree is computed as the productof the probabilities of individual chunk-ing results.
The parsing is performed in abottom-up manner and the best derivationis efficiently obtained by using a depth-first search algorithm.
Experimental re-sults demonstrate that this simple parsingframework produces a fast and reasonablyaccurate parser.1 IntroductionFull parsing analyzes the phrase structure of a sen-tence and provides useful input for many kindsof high-level natural language processing such assummarization (Knight and Marcu, 2000), pro-noun resolution (Yang et al, 2006), and infor-mation extraction (Miyao et al, 2008).
One ofthe major obstacles that discourage the use of fullparsing in large-scale natural language process-ing applications is its computational cost.
For ex-ample, the MEDLINE corpus, a collection of ab-stracts of biomedical papers, consists of 70 millionsentences and would require more than two yearsof processing time if the parser needs one secondto process a sentence.Generative models based on lexicalized PCFGsenjoyed great success as the machine learningframework for full parsing (Collins, 1999; Char-niak, 2000), but recently discriminative modelsattract more attention due to their superior accu-racy (Charniak and Johnson, 2005; Huang, 2008)and adaptability to new grammars and languages(Buchholz and Marsi, 2006).A traditional approach to discriminative fullparsing is to convert a full parsing task into a seriesof classification problems.
Ratnaparkhi (1997)performs full parsing in a bottom-up and left-to-right manner and uses a maximum entropy clas-sifier to make decisions to construct individualphrases.
Sagae and Lavie (2006) use the shift-reduce parsing framework and a maximum en-tropy model for local classification to decide pars-ing actions.
These approaches are often calledhistory-based approaches.A more recent approach to discriminative fullparsing is to treat the task as a single structuredprediction problem.
Finkel et al (2008) incor-porated rich local features into a tree CRF modeland built a competitive parser.
Huang (2008) pro-posed to use a parse forest to incorporate non-localfeatures.
They used a perceptron algorithm to op-timize the weights of the features and achievedstate-of-the-art accuracy.
Petrov and Klein (2008)introduced latent variables in tree CRFs and pro-posed a caching mechanism to speed up the com-putation.In general, the latter whole-sentence ap-proaches give better accuracy than history-basedapproaches because they can better trade off deci-sions made in different parts in a parse tree.
How-ever, the whole-sentence approaches tend to re-quire a large computational cost both in trainingand parsing.
In contrast, history-based approachesare less computationally intensive and usually pro-duce fast parsers.In this paper, we present a history-based parserusing CRFs, by treating the task of full parsing asa series of chunking problems where it recognizeschunks in a flat input sequence.
We use the linear-790Estimated  volume  was   a   light  2.4  million  ounces  .VBN         NN    VBD DT  JJ    CD     CD NNS   .QPNPFigure 1: Chunking, the first (base) level.volume          was   a   light    million       ounces .NP             VBD DT  JJ          QP            NNS   .NPFigure 2: Chunking, the 2nd level.chain CRF model to perform chunking.Although our parsing model falls into the cat-egory of history-based approaches, it is one stepcloser to the whole-sentence approaches becausethe parser uses a whole-sequence model (i.e.CRFs) for individual chunking tasks.
In otherwords, our parser could be located somewherebetween traditional history-based approaches andwhole-sentence approaches.
One of our motiva-tions for this work was that our parsing modelmay achieve a better balance between accuracyand speed than existing parsers.It is also worth mentioning that our approach issimilar in spirit to supertagging for parsing withlexicalized grammar formalisms such as CCG andHPSG (Clark and Curran, 2004; Ninomiya et al,2006), in which significant speed-ups for parsingtime are achieved.In this paper, we show that our approach is in-deed appealing in that the parser runs very fastand gives competitive accuracy.
We evaluate ourparser on the standard data set for parsing exper-iments (i.e.
the Penn Treebank) and compare itwith existing approaches to full parsing.This paper is organized as follows.
Section 2presents the overall chunk parsing strategy.
Sec-tion 3 describes the CRF model used to performindividual chunking steps.
Section 4 describes thedepth-first algorithm for finding the best derivationof a parse tree.
The part-of-speech tagger used inthe parser is described in section 5.
Experimen-tal results on the Penn Treebank corpus are pro-vided in Section 6.
Section 7 discusses possibleimprovements and extensions of our work.
Sec-tion 8 offers some concluding remarks.volume          was                    ounces          .NP             VBD                    NP           .VPFigure 3: Chunking, the 3rd level.volume                           was                   .NP                               VP                .SFigure 4: Chunking, the 4th level.2 Full Parsing by ChunkingThis section describes the parsing framework em-ployed in this work.The parsing process is conceptually very sim-ple.
The parser first performs chunking by iden-tifying base phrases, and converts the identifiedphrases to non-terminal symbols.
It then performschunking for the updated sequence and convertsthe newly recognized phrases into non-terminalsymbols.
The parser repeats this process until thewhole sequence is chunked as a sentenceFigures 1 to 4 show an example of a parsing pro-cess by this framework.
In the first (base) level,the chunker identifies two base phrases, (NP Es-timated volume) and (QP 2.4 million), and re-places each phrase with its non-terminal symboland head1.
In the second level, the chunker iden-tifies a noun phrase, (NP a light million ounces),and converts it into NP.
This process is repeateduntil the whole sentence is chunked at the fourthlevel.
The full parse tree is recovered from thechunking history in a straightforward way.This idea of converting full parsing into a se-ries of chunking tasks is not new by any means?the history of this kind of approach dates back to1950s (Joshi and Hopely, 1996).
More recently,Brants (1999) used a cascaded Markov model toparse German text.
Tjong Kim Sang (2001) usedthe IOB tagging method to represent chunks andmemory-based learning, and achieved an f-scoreof 80.49 on the WSJ corpus.
Tsuruoka and Tsu-jii (2005) improved upon their approach by using1The head word is identified by using the head-percolation table (Magerman, 1995).7910100020003000400050000  5  10  15  20  25  30#sentencesHeightFigure 5: Distribution of tree height in WSJ sec-tions 2-21.a maximum entropy classifier and achieved an f-score of 85.9.
However, there is still a large gapbetween the accuracy of chunking-based parsersand that of widely-used practical parsers such asCollins parser and Charniak parser (Collins, 1999;Charniak, 2000).2.1 Heights of TreesA natural question about this parsing framework ishow many levels of chunking are usually needed toparse a sentence.
We examined the distribution ofthe heights of the trees in sections 2-21 of the WallStreet Journal (WSJ) corpus.
The result is shownin Figure 5.
Most of the sentences have less than20 levels.
The average was 10.0, which means weneed to perform, on average, 10 chunking tasks toobtain a full parse tree for a sentence if the parsingis performed in a deterministic manner.3 Chunking with CRFsThe accuracy of chunk parsing is highly depen-dent on the accuracy of each level of chunking.This section describes our approach to the chunk-ing task.A common approach to the chunking problemis to convert the problem into a sequence taggingtask by using the ?BIO?
(B for beginning, I forinside, and O for outside) representation.
For ex-ample, the chunking process given in Figure 1 isexpressed as the following BIO sequences.B-NP I-NP O O O B-QP I-QP O OThis representation enables us to use the linear-chain CRF model to perform chunking, since thetask is simply assigning appropriate labels to a se-quence.3.1 Linear Chain CRFsA linear chain CRF defines a single log-linearprobabilistic distribution over all possible tag se-quences y for the input sequence x:p(y|x) = 1Z(x) expT?t=1K?k=1?kfk(t, yt, yt?1,x),where fk(t, yt, yt?1,x) is typically a binary func-tion indicating the presence of feature k, ?k is theweight of the feature, and Z(X) is a normalizationfunction:Z(x) =?yexpT?t=1K?k=1?kfk(t, yt, yt?1,x).This model allows us to define features on statesand edges combined with surface observations.The weights of the features are determined insuch a way that they maximize the conditional log-likelihood of the training data:L?
=N?i=1log p(y(i)|x(i)) + R(?
),where R(?)
is introduced for the purpose of regu-larization which prevents the model from overfit-ting the training data.
The L1 or L2 norm is com-monly used in statistical natural language process-ing (Gao et al, 2007).
We used L1-regularization,which is defined asR(?)
= 1CK?k=1|?k|,where C is the meta-parameter that controls thedegree of regularization.
We used the OWL-QNalgorithm (Andrew and Gao, 2007) to obtain theparameters that maximize the L1-regularized con-ditional log-likelihood.3.2 FeaturesTable 1 shows the features used in chunking forthe base level.
Since the task is basically identicalto shallow parsing by CRFs, we follow the featuresets used in the previous work by Sha and Pereira(2003).
We use unigrams, bigrams, and trigramsof part-of-speech (POS) tags and words.The difference between our CRF chunker andthat in (Sha and Pereira, 2003) is that we couldnot use second-order CRF models, hence we couldnot use trigram features on the BIO states.
We792Symbol Unigrams s?2, s?1, s0, s+1, s+2Symbol Bigrams s?2s?1, s?1s0, s0s+1, s+1s+2Symbol Trigrams s?3s?2s?1, s?2s?1s0, s?1s0s+1, s0s+1s+2, s+1s+2s+3Word Unigrams h?2, h?1, h0, h+1, h+2Word Bigrams h?2h?1, h?1h0, h0h+1, h+1h+2Word Trigrams h?1h0h+1Table 1: Feature templates used in the base level chunking.
s represents a terminal symbol (i.e.
POS tag)and the subscript represents a relative position.
h represents a word.found that using second order CRFs in our taskwas very difficult because of the computationalcost.
Recall that the computational cost for CRFsis quadratic to the number of possible states.
Inour task, we need to consider the states for all non-terminal symbols, whereas their work is only con-cerned with noun phrases.Table 2 shows feature templates used in the non-base levels of chunking.
In the non-base levels ofchunking, we can use a richer set of features thanthe base-level chunking because the chunker hasaccess to the information about the partial treesthat have been already created.
In addition to thefeatures listed in Table 1, the chunker looks intothe daughters of the current non-terminal sym-bol and use them as features.
It also uses thewords and POS tags around the edges of the re-gion covered by the current non-terminal symbol.We also added a special feature to better capturePP-attachment.
The chunker looks at the head ofthe second daughter of the prepositional phrase toincorporate the semantic head of the phrase.4 Searching for the Best ParseThe probability for an entire parse tree is com-puted as the product of the probabilities output bythe individual CRF chunkers:score =h?i=0p(yi|xi), (1)where i is the level of chunking and h is the heightof the tree.
The task of full parsing is then tochoose the series of chunking results that maxi-mizes this probability.It should be noted that there are cases wheredifferent derivations (chunking histories) lead tothe same parse tree (i.e.
phrase structure).
Strictlyspeaking, therefore, what we describe here as theprobability of a parse tree is actually the proba-bility of a single derivation.
The probabilities ofthe derivations should then be marginalized overto produce the probability of a parse tree, but inthis paper we ignore this effect and simply focusonly on the best derivation.We use a depth-first search algorithm to find thehighest probability derivation.
Figure 6 shows thealgorithm in pseudo-code.
The parsing process isimplemented with a recursive function.
In eachlevel of chunking, the recursive function first in-vokes a CRF chunker to obtain chunking hypothe-ses for the given sequence.
For each hypothesiswhose probability is high enough to have possibil-ity of constituting the best derivation, the functioncalls itself with the sequence updated by the hy-pothesis.
The parsing process is performed in abottom up manner and this recursive process ter-minates if the whole sequence is chunked as a sen-tence.To extract multiple chunking hypotheses fromthe CRF chunker, we use a branch-and-boundalgorithm rather than the A* search algorithm,which is perhaps more commonly used in previousstudies.
We do not give pseudo code, but the ba-sic idea is as follows.
It first performs the forwardViterbi algorithm to obtain the best sequence, stor-ing the upper bounds that are used for pruning inbranch-and-bound.
It then performs a branch-and-bound algorithm in a backward manner to retrievepossible candidate sequences whose probabilitiesare greater than the given threshold.
Unlike A*search, this method is memory efficient because itis performed in a depth-first manner and does notrequire priority queues for keeping uncompletedhypotheses.It is straightforward to introduce beam search inthis search algorithm?we simply limit the num-ber of hypotheses generated by the CRF chunker.We examine how the width of the beam affects theparsing performance in the experiments.793Symbol Unigrams s?2, s?1, s0, s+1, s+2Symbol Bigrams s?2s?1, s?1s0, s0s+1, s+1s+2Symbol Trigrams s?3s?2s?1, s?2s?1s0, s?1s0s+1, s0s+1s+2, s+1s+2s+3Head Unigrams h?2, h?1, h0, h+1, h+2Head Bigrams h?2h?1, h?1h0, h0h+1, h+1h+2Head Trigrams h?1h0h+1Symbol & Daughters s0d01, ... s0d0mSymbol & Word/POS context s0wj?1, s0pj?1, s0wk+1 , s0pk+1Symbol & Words on the edges s0wj , s0wkFreshness whether s0 has been created in the level just belowPP-attachment h?1h0m02 (only when s0 = PP)Table 2: Feature templates used in the upper level chunking.
s represents a non-terminal symbol.
hrepresents a head percolated from the bottom for each symbol.
d0i is the ith daughter of s0.
wj is thefirst word in the range covered by s0.
wj?1 is the word preceding wj .
wk is the last word in the rangecovered by s0.
wk+1 is the word following wk.
p represents POS tags.
m02 represents the head of thesecond daughter of s0.Word Unigram w?2, w?1, w0, w+1, wi+2Word Bigram w?1w0, w0w+1, w?1w+1Prefix, Suffix prefixes of w0suffixes of w0(up to length 10)Character features w0 has a hyphenw0 has a numberw0 has a capital letterw0 is all capitalNormalized word N(w0)Table 3: Feature templates used in the POS tagger.w represents a word and the subscript represents arelative position.5 Part-of-Speech TaggingWe use the CRF model also for POS tagging.The CRF-based POS tagger is incorporated in theparser in exactly the same way as the other lay-ers of chunking.
In other words, the POS taggingprocess is treated like the bottom layer of chunk-ing, so the parser considers multiple probabilistichypotheses output by the tagger in the search al-gorithm described in the previous section.5.1 FeaturesTable 3 shows the feature templates used in thePOS tagger.
Most of them are standard featurescommonly used in POS tagging for English.
Weused unigrams and bigrams of neighboring words,prefixes and suffixes of the current word, and somecharacteristics of the word.
We also normalizedthe current word by lowering capital letters andconverting all the numerals into ?#?, and used thenormalized word as a feature.6 ExperimentsWe ran parsing experiments using the Wall StreetJournal corpus.
Sections 2-21 were used as thetraining data.
Section 22 was used as the devel-opment data, with which we tuned the feature setand parameters for learning and parsing.
Section23 was reserved for the final accuracy report.The training data for the CRF chunkers werecreated by converting each parse tree in the train-ing data into a list of chunking sequences likethe ones presented in Figures 1 to 4.
We trainedthree CRF models, i.e., the POS tagging model,the base chunking model, and the non-base chunk-ing model.
The training took about two days on asingle CPU.We used the evalb script provided by Sekine andCollins for evaluating the labeled recall/precisionof the parser outputs2.
All experiments were car-ried out on a server with 2.2 GHz AMD Opteronprocessors and 16GB memory.6.1 Chunking PerformanceFirst, we describe the accuracy of individualchunking processes.
Table 4 shows the resultsfor the ten most frequently occurring symbols onthe development data.
Noun phrases (NP) are the2The script is available at http://nlp.cs.nyu.edu/evalb/.
Weused the parameter file ?COLLINS.prm?.7941: procedure PARSESENTENCE(x)2: PARSE(x, 1, 0)3:4: function PARSE(x, p, q)5: if x is chunked as a complete sentence6: return p7: H ?
PERFORMCHUNKING(x, q/p)8: for h ?
H in descending order of theirprobabilities do9: r ?
p?
h.probability10: if r > q then11: x?
?
UPDATESEQUENCE(x, h)12: s?
PARSE(x?, r, q)13: if s > q then14: q ?
s15: return q16:17: function PERFORMCHUNKING(x, t)18: perform chunking with a CRF chunker and19: return a set of chunking hypotheses whose20: probabilities are greater than t.21:22: function UPDATESEQUENCE(x, h)23: update sequence x according to chunking24: hypothesis h and return the updated25: sequence.Figure 6: Searching for the best parse with adepth-first search algorithm.
This pseudo-code il-lustrates how to find the highest probability parse,but in the real implementation, the function needsto keep track of chunking histories as well as prob-abilities.most common symbol and consist of 55% of allphrases.
The accuracy of noun phrases recognitionwas relatively high, but it may be useful to designspecial features for this particular type of phrase,considering the dominance of noun phrases in thecorpus.
Although not directly comparable, Shaand Pereira (2003) report almost the same levelof accuracy (94.38%) on noun phrase recognition,using a much smaller training set.
We attributetheir superior performance mainly to the use ofsecond-order features on state transitions.
Table 4also suggests that adverb phrases (ADVP) and ad-jective phrases (ADJP) are more difficult to recog-nize than other types of phrases, which coincideswith the result reported in (Collins, 1999).It should be noted that the performance reportedin this table was evaluated using the gold standardsequences as the input to the CRF chunkers.
In theSymbol # Samples Recall Prec.
F-scoreNP 317,597 94.79 94.16 94.47VP 76,281 91.46 91.98 91.72PP 66,979 92.84 92.61 92.72S 33,739 91.48 90.64 91.06ADVP 21,686 84.25 85.86 85.05ADJP 14,422 77.27 78.46 77.86QP 14,308 89.43 91.16 90.28SBAR 11,603 96.42 96.97 96.69WHNP 8,827 95.54 97.50 96.51PRT 3,391 95.72 90.52 93.05: : : : :all 579,253 92.63 92.62 92.63Table 4: Chunking performance (section 22, allsentences).Beam Recall Prec.
F-score Time (sec)1 86.72 87.83 87.27 162 88.50 88.85 88.67 413 88.69 89.08 88.88 614 88.72 89.13 88.92 925 88.73 89.14 88.93 11910 88.68 89.19 88.93 179Table 5: Beam width and parsing performance(section 22, all sentences).real parsing process, the chunkers have to use theoutput from the previous (one level below) chun-ker, so the quality of the input is not as good asthat used in this evaluation.6.2 Parsing PerformanceNext, we present the actual parsing performance.The first set of experiments concerns the relation-ship between the width of beam and the parsingperformance.
Table 5 shows the results obtainedon the development data.
We varied the width ofthe beam from 1 to 10.
The beam width of 1 cor-responds to deterministic parsing.
Somewhat un-expectedly, the parsing accuracy did not drop sig-nificantly even when we reduced the beam widthto a very small number such as 2 or 3.One of the interesting findings was that re-call scores were consistently lower than precisionscores throughout all experiments.
A possible rea-son is that, since the score of a parse is definedas the product of all chunking probabilities, theparser could prefer a parse tree that consists ofa small number of chunk layers.
This may stem795from the history-based model?s inability of prop-erly trading off decisions made by different chun-kers.Overall, the parsing speed was very high.
Thedeterministic version (beam width = 1) parsed1700 sentences in 16 seconds, which means thatthe parser needed only 10 msec to parse one sen-tence.
The parsing speed decreases as we increasethe beam width.The parser was also memory efficient.
Thanksto L1 regularization, the training process did notresult in many non-zero feature weights.
The num-bers of non-zero weight features were 58,505 (forthe base chunker), 263,889 (for the non-base chun-ker), and 42,201 (for the POS tagger).
The parserrequired only 14MB of memory to run.There was little accuracy difference between thebeam width of 4 and 5, so we adopted the beamwidth of 4 for the final accuracy report on the testdata.6.3 Comparison with Previous WorkTable 6 shows the performance of our parser onthe test data and summarizes the results of previ-ous work.
Our parser achieved an f-score of 88.4on the test data, which is comparable to the accu-racy achieved by recent discriminative approachessuch as Finkel et al (2008) and Petrov & Klein(2008), but is not as high as the state-of-the-artaccuracy achieved by the parsers that can incor-porate global features such as Huang (2008) andCharniak & Johnson (2005).
Our parser was moreaccurate than traditional history-based approachessuch as Sagae & Lavie (2006) and Ratnaparkhi(1997), and was significantly better than previouscascaded chunking approaches such as Tsuruoka& Tsujii (2005) and Tjong Kim Sang (2001).Although the comparison presented in the tableis not perfectly fair because of the differences inhardware platforms, the results show that our pars-ing model is a promising addition to the parsingframeworks for building a fast and accurate parser.7 DiscussionOne of the obvious ways to improve the accuracyof our parser is to improve the accuracy of in-dividual CRF models.
As mentioned earlier, wewere not able to use second-order features on statetransitions, which would have been very useful,due to the problem of computational cost.
Incre-mental feature selection methods such as grafting(Perkins et al, 2003) may help us to incorporatesuch higher-order features, but the problem of de-creased efficiency of dynamic programming in theCRF would probably need to be addressed.In this work, we treated the chunking problemas a sequence labeling problem by using the BIOrepresentation for the chunks.
However, semi-Markov conditional random fields (semi-CRFs)can directly handle the chunking problem byconsidering all possible combinations of subse-quences of arbitrary length (Sarawagi and Cohen,2004).
Semi-CRFs allow one to use a richer setof features than CRFs, so the use of semi-CRFsin our parsing framework should lead to improvedaccuracy.
Moreover, semi-CRFs would allow us toincorporate some useful restrictions in producingchunking hypotheses.
For example, we could nat-urally incorporate the restriction that every chunkhas to contain at least one symbol that has justbeen created in the previous level3.
It is hard forthe normal CRF model to incorporate such restric-tions.Introducing latent variables into the CRF modelmay be another promising approach.
This is themain idea of Petrov and Klein (2008), which sig-nificantly improved parsing accuracy.A totally different approach to improving theaccuracy of our parser is to use the idea of ?self-training?
described in (McClosky et al, 2006).The basic idea is to create a larger set of trainingdata by applying an accurate parser (e.g.
rerank-ing parser) to a large amount of raw text.
We canthen use the automatically created treebank as theadditional training data for our parser.
This ap-proach suggests that accurate (but slow) parsersand fast (but not-so-accurate) parsers can actuallyhelp each other.Also, since it is not difficult to extend our parserto produce N-best parsing hypotheses, one couldbuild a fast reranking parser by using the parser asthe base (hypotheses generating) parser.8 ConclusionAlthough the idea of treating full parsing as a se-ries of chunking problems has a long history, therehas not been a competitive parser based on thisparsing framework.
In this paper, we have demon-strated that the framework actually enables us to3For example, the sequence VBD DT JJ in Figure 2 can-not be a chunk in the current level because it would have beenalready chunked in the previous level if it were.796Recall Precision F-score Time (min)This work (deterministic) 86.3 87.5 86.9 0.5This work (search, beam width = 4) 88.2 88.7 88.4 1.7Huang (2008) 91.7 UnkFinkel et al (2008) 87.8 88.2 88.0 >250*Petrov & Klein (2008) 88.3 3*Sagae & Lavie (2006) 87.8 88.1 87.9 17Charniak & Johnson (2005) 90.6 91.3 91.0 UnkTsuruoka & Tsujii (2005) 85.0 86.8 85.9 2Collins (1999) 88.1 88.3 88.2 39**Tjong Kim Sang (2001) 78.7 82.3 80.5 UnkCharniak (2000) 89.6 89.5 89.5 23**Ratnaparkhi (1997) 86.3 87.5 86.9 UnkTable 6: Parsing performance on section 23 (all sentences).
* estimated from the parsing time on thetraining data.
** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run theparsers.build a competitive parser if we use CRF mod-els for each level of chunking and a depth-firstsearch algorithm to search for the highest proba-bility parse.Like other discriminative learning approaches,one of the advantages of our parser is its general-ity.
The design of our parser is very generic, andthe features used in our parser are not particularlyspecific to the Penn Treebank.
We expect it to bestraightforward to adapt the parser to other projec-tive grammars and languages.This parsing framework should be useful whenone needs to process a large amount of text orwhen real time processing is required, in whichthe parsing speed is of top priority.
In the deter-ministic setting, our parser only needed about 10msec to parse a sentence.AcknowledgmentsThis work described in this paper has beenfunded by the Biotechnology and Biological Sci-ences Research Council (BBSRC; BB/E004431/1)and the European BOOTStrep project (FP6 -028099).
The research team is hosted by theJISC/BBSRC/EPSRC sponsored National Centrefor Text Mining.ReferencesGalen Andrew and Jianfeng Gao.
2007.
Scalable train-ing of L1-regularized log-linear models.
In Pro-ceedings of ICML, pages 33?40.Thorsten Brants.
1999.
Cascaded markov models.
InProceedings of EACL.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL-X, pages 149?164.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of ACL, pages 173?180.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of NAACL 2000,pages 132?139.Stephen Clark and James R. Curran.
2004.
The impor-tance of supertagging for wide-coverage CCG pars-ing.
In Proceedings of COLING 2004, pages 282?288.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, condi-tional random field parsing.
In Proceedings of ACL-08:HLT, pages 959?967.Jianfeng Gao, Galen Andrew, Mark Johnson, andKristina Toutanova.
2007.
A comparative study ofparameter estimation methods for statistical naturallanguage processing.
In Proceedings of ACL, pages824?831.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL-08:HLT, pages 586?594.Aravind K. Joshi and Phil Hopely.
1996.
A parserfrom antiquity.
Natural Language Engineering,2(4):291?294.797Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization - step one: Sentence compres-sion.
In Proceedings of AAAI/IAAI, pages 703?710.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of ACL, pages276?283.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of HLT-NAACL.Yusuke Miyao, Rune Saetre, Kenji Sage, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2008.
Task-orientedevaluation of syntactic parsers and their representa-tions.
In Proceedings of ACL-08:HLT, pages 46?54.Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-ruoka, Yusuke Miyao, and Jun?ichi Tsujii.
2006.Extremely lexicalized models for accurate and fastHPSG parsing.
In Proceedings of EMNLP 2006,pages 155?163.Simon Perkins, Kevin Lacker, and James Theiler.2003.
Grafting: fast, incremental feature selectionby gradient descent in function space.
The Journalof Machine Learning Research, 3:1333?1356.Slav Petrov and Dan Klein.
2008.
Discriminativelog-linear grammars with latent variables.
In Ad-vances in Neural Information Processing Systems 20(NIPS), pages 1153?1160.Adwait Ratnaparkhi.
1997.
A linear observed time sta-tistical parser based on maximum entropy models.In Proceedings of EMNLP 1997, pages 1?10.Kenji Sagae and Alon Lavie.
2006.
A best-first proba-bilistic shift-reduce parser.
In Proceedings of COL-ING/ACL, pages 691?698.Sunita Sarawagi and William W. Cohen.
2004.
Semi-markov conditional random fields for informationextraction.
In Proceedings of NIPS.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proceedings ofHLT-NAACL.Erik Tjong Kim Sang.
2001.
Transforming a chunkerto a parser.
In J. Veenstra W. Daelemans, K. Sima?anand J. Zavrel, editors, Computational Linguistics inthe Netherlands 2000, pages 177?188.
Rodopi.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Chunkparsing revisited.
In Proceedings of IWPT, pages133?140.Xiaofeng Yang, Jian Su, and Chew Lim Tan.
2006.Kernel-based pronoun resolution with structuredsyntactic features.
In Proceedings of COLING/ACL,pages 41?48.798
