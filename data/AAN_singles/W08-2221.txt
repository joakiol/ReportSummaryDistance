Boeing?s NLP System and theChallenges of SemanticRepresentationPeter ClarkPhil HarrisonThe Boeing Company (USA)email: peter.e.clark@boeing.comAbstractWe describe Boeing?s NLP system, BLUE, comprising a pipeline of aparser, a logical form (LF) generator, an initial logic generator, and fur-ther processingmodules.
The initial logic generator produces logic whosestructure closely mirrors the structure of the original text.
The subsequentprocessing modules then perform, with somewhat limited scope, addi-tional transformations to convert this into a more usable representationwith respect to a specific target ontology, better able to support inference.Generating a semantic representation is challenging, due to the wide va-riety of semantic phenomena which can occur in text.
We identify sev-enteen such phenomena which occurred in the STEP 2008 "shared task"texts, comment on BLUE?s ability to handle them or otherwise, and dis-cuss the more general question of what exactly constitutes a "semanticrepresentation", arguing that a spectrum of interpretations exist.263264 Clark and Harrison1 System Description1.1 Overview and ScopeAs our contribution to the 2008 STEP Symposium?s ?shared task?
of comparing se-mantic representations (Bos, 2008), we describe Boeing?s NLP system, BLUE (Boe-ing Language Understanding Engine), and subsequently analyze its performance onthe task?s shared texts.
BLUE consists of a pipeline of a parser, logical form (LF)generator, an initial logic generator, and subsequent processing modules.
The parserhas broad coverage and is domain general.
The logical form generator currently dealswith a (reasonably large) subset of linguistic phenomena, including simple sentences,prepositional phrases, compound nouns, ordinal modifiers, proper nouns, some sim-ple types of coordination, adverbs, negation, comparatives, and modals.
The initiallogic generator performs a straightforward transformation of the LF to first-order logicsyntax.
Subsequent processing modules then perform word sense disambiguation, se-mantic role labeling, coreference resolution, and some limited metonymic and othertransformations.
The overall system currently produces output expressed in one oftwo target ontologies, namely WordNet and the University of Texas at Austin?s Com-ponent Library (CLib) (Barker et al, 2001).
In this paper we illustrate the system?suse with WordNet?s ontology.
The overall system was originally developed for inter-preting a controlled language called CPL (Clark et al, 2007), but also often makesreasonable interpretations of more complex, open text sentences, as we illustrate here.1.2 Parsing and the Logical Form GeneratorParsing is performed using SAPIR, a mature, bottom-up, broad coverage chart parser(Harrison and Maxwell, 1986).
The parser?s cost function is biased by a database ofmanually and corpus-derived ?tuples?
(good parse fragments), as well as hand-codedpreference rules.
During parsing, the system also generates a logical form (LF), asemi-formal structure between a parse and full logic.
The LF is a simplified andnormalized tree structure with logic-type elements, generated by rules parallel to thegrammar rules, that contains variables (prefixed by underscores ?_?)
and additionalexpressions for other sentence constituents.
Variables can represent noun phrases,propositions, and even verb phrases (e.g., ?To solve this problem is difficult?
).Some disambiguation decisions are performed at this stage (e.g., structural, part ofspeech), while others are deferred (e.g., word senses, semantic roles), and there is noexplicit quantifier scoping.
Various syntactic properties and relationships are capturedin the LF, including: S (sentence), PP (prepositional phrase), NN (noun compound),PN (proper name), PLUR (plural), PLUR-N (numbered plural).
Tense, aspect, andpolarity are also recorded in the LF.
For example:Boeing?s NLP System and the Challenges of Semantic Representation 265;;; LF for "An object is thrown with a horizontal speed of 20 m/sfrom a cliff that is 125 m high.
"(DECL((VAR _X1 "an" "object")(VAR _X3 NIL (PLUR-N "20" "m/s"))(VAR _X2 "a" "horizontal speed" (PP "of" _X3))(VAR _X4 "a" "cliff"(DECL NIL (S (PRESENT) _X4 "be"(S-ADJ _X4 (DEGREE(MEASUREMENT "125" "m") "high"))))))(S (PRESENT) NIL "throw" _X1(PP "with" _X2) (PP "from" _X4)))1.3 The Initial Logic GeneratorThe LF is then used to generate ground logical assertions of the form r(x,y), con-taining Skolem instances (denoting existentially quantified variables) by applying aset of simple, syntactic rewrite rules recursively to it.
Verbs are reified as individuals,Davidsonian-style.
At this stage of processing, the binary predicates are: subject (syn-tactic subject), sobject (syntactic object), mod (modifier), all the prepositions, value(for physical quantities), number-of-elements (for numbered plurals), and named (forproper names).
For example, the above LF is translated into ?syntactic logic?
(addi-tional predicates indicating part of speech, tense, aspect, determiners, and polarity arenot shown):;;; "An object is thrown with a horizontal speed;;; of 20 m/s from a cliff that is 125 m high.
""object"(object01),value(quantity01,[20,m/s_n1]),"m/s"(m/s_n1),"speed"(speed01),"horizontal"(horizontal01),mod(speed01,horizontal01),"of"(speed01,quantity01),"cliff"(cliff01),"be"(be01),subject(be01,cliff01),sobject(be01,height01),value(height01,[125,m_n1]),"m"(m_n1),"height"(height01),"throw"(throw01),sobject(throw01,object01),"with"(throw01,speed01),"from"(throw01,cliff01).1.4 Subsequent Processing ModulesWhile the output of the basic system is in a logic syntax, it is not coherent enoughto support inference as it preserves many difficult linguistic phenomena (ambiguity,266 Clark and Harrisonmetonymy, etc.).
Further semantic interpretation involves disambiguation and align-ing the interpretation with the target ontology we are using.
In general, this is a com-plex task and our system only makes limited steps in this direction using five modules:word sense disambiguation (WSD); semantic role labeling (SRL); coreference reso-lution (including across different parts of speech); metonymy resolution (with respectto the target ontology); and structural transformations.
We describe these modulesbelow.Word Sense Disambiguation When usingWordNet?s ontology, each synset inWord-Net is a target concept for WSD.
BLUE currently performs naive word sense dis-ambiguation by simply selecting the most common synset for a given word+part-of-speech using context-independent frequency statistics.
When using the ComponentLibrary (CLib) ontology, BLUE exploits hand-authored mappings between WordNetsynsets and CLib concepts: Given a word, e.g., ?cliff?, BLUE first finds WordNetsynsets for the word, then climbs WordNet?s taxonomy from those synsets until itfinds synsets mapped to CLib concepts, and returns those CLib concepts, again usingpreference based on context-independent frequency statistics.
Verb nominalizationsmap to the denominalized verb, thus ?fall?
(n) and ?falling?
(n) both map to synsets for?fall?
(v).Semantic Role Labeling With both ontologies, BLUE uses the same relational vo-cabulary of approximately 100 binary semantic relations, drawn from the relation setused by UT?s Component Library.
Semantic role labeling (SRL), for both for verb-noun and noun-noun relationships, is performed using a set of hand-authored SRLrules, e.g., ?from?
(x,y) is labeled as origin(x,y) if x is a movement event and y is anobject.
In cases where the rules are not adequate to clearly identify a semantic relation,the relation is left as a syntactic relation.Coreference Coreference (e.g., ?the ball?)
is computed by searching for a previousentity in the discourse with the same word and qualifiers as in the referring nounphrase.
(Coreference using synonyms or types producedmore errors than it removed).Metonymy Often a sentence relates entities in a way inconsistent with the target on-tology.
For example, with the Component Library (CLib) ontology,movement proper-ties (e.g., speed, acceleration) are defined as properties of the movement events, ratherthan of the object moving.
Thus a phrase like ?the initial speed of the ball?
is metony-mous (with respect to CLib) for ?the initial speed of the movement of the ball?.
Thismodule spots and corrects such metonymies using a small set of metonymy resolutionrules.
Note that metonymy resolution is ontology-specific, reflecting design decisionsabout what is and is not an allowable expression in the target ontology.Structural Transformations Often, the structure of the syntactic and (desired) se-mantic representations differ, and so some structural transformations are necessary.For example, in the basic processing, verbs (e.g., ?weigh?)
are reified as individualswith semantic roles, e.g., ?weigh?
(w), subject(w,x), sobject(w,y), whereas the targetontology stipulates that some particular verbs denote relations e.g., ?weigh?
corre-sponds to the CLib relation weight(x,y), not a Weigh event.
(This is indicated in CLibby the relation weigh() being associated with synsets for the verb ?weigh?).
Similarly,nouns associated with relations will be transformed to introduce that relation into therepresentation, e.g., ?weight?
(y), ?of?
(y,x) will be transformed to weight(x,y).
ThisBoeing?s NLP System and the Challenges of Semantic Representation 267module makes these and other transformations.
The verbs ?be?
and ?have?
are sim-ilarly mapped to relations, but with the extra step that the target relation depends onthe arguments.
A small set of rules determines the appropriate relation to use.2 Semantic Formalism2.1 Form (Syntax)Our system produces output in a subset of first-order logic, illustrated later in thispaper.
For the most part, it simply outputs a flat list of ground assertions contain-ing Skolemized existential variables, and does not handle universal quantification (asignificant limitation for expository rather than story-like texts).
In addition, BLUEallows propositions to themselves be arguments to other propositions as a nested struc-ture, e.g., for modals:;;; "The man wanted to leave the house"isa(leave01,leave_v1), etc, ...agent(want01,man01),object(want01,[agent(leave01,man01),object(leave01,house01)]).2.2 Ontology (Content)As described earlier, BLUE currently uses two alternative conceptual vocabularies,namely the concepts in WordNet (with minor extensions) or the Component Library.BLUE?s relational vocabulary is approximately 100 semantic relations drawn from theComponent Library.13 ExampleWe illustrate our system using an example from Project Halo (Clark et al, 2007),where the system is used to interpret multi-sentence science questions posed to aknowledge-based system.
While BLUE produces a slightly better output for this textusing the Component Library ontology, we illustrate it using WordNet?s ontology forconsistency with our output for the other shared task texts (we use WordNet for theseas WordNet has broader coverage).
We also discuss our system further in Section 4on additional sentences.The first three sentences are (largely) a question from an AP Physics exam, thefourth is a hand-written simplified version of the third sentence.
Our system is able tocreate coherent representations of sentences 1, 2, and 4, i.e., sufficient for the KB toanswer the question correctly, but not of sentence 3.Shared Task Text 1:(1.1) An object is thrown with a horizontal speed of 20 m/s from a cliffthat is 125 m high.
(1.2) The object falls for the height of the cliff.
(1.3) If air resistance is negligible, how long does it take the object to fall1http://www.cs.utexas.edu/users/mfkb/RKF/trunktree/components/specs/slotdictionary.html268 Clark and Harrisonto the ground?
(1.4) What is the duration of the fall?Semantic Representation(1.1) "An object is thrown with a horizontal speed of 20 m/sfrom a cliff that is 125 m high.
"isa(object01,object_n1),isa(speed01,velocity_n1),isa(horizontal01,horizontal_a1),isa(cliff01,cliff_n1),isa(height01,height_n1),isa(throw01,throw_v1),height(cliff01,height01),value(speed01,[20,m/s_n1]),mod(speed01,horizontal01),value(height01,[125,m_n1]),object(throw01,object01),"with"(throw01,speed01),origin(throw01,cliff01).Here object01 etc.
denote Skolem instances, object_n1 etc.
denote WordNet con-cepts (synsets).
Note word and role disambiguation, adjective-noun transformation(?high??
height()), ?be?
interpretation, and handling of units of measurement (?125m?, ?20 m/s?).
Using WordNet?s ontology, this interpretation is not perfect as twosemantic roles have (undesirably) been left underspecified (?mod?
and ?with?).
(1.2) "The object falls for the height of the cliff.
"isa(fall01,fall_v1),height(cliff01,height01),agent(fall01,object01),distance(fall01,height01).Note coreference with first sentence (?height?, ?cliff?, ?object?)
and semantic rolelabeling (?for height??
height()).
(1.3) "If air resistance is negligible, how long does ittake the object to fall to the ground?
"(See the STEP Shared TaskWeb site2 for BLUE?s semantic representation).
BLUE?srepresentation for this is largely incoherent, in particular a ?take?
event is created witha proposition (meaning ?length of fall to the ground?)
as its 2nd argument.
(1.4) "What is the duration of the fall?
"isa(fall01,fall_v1),isa(duration01,duration_n1),duration(fall01,duration01),query-for(duration01).Note noun-verb coreference ("fall"(n) ?
fall01) and query variable identification.2http://www.sigsem.orgBoeing?s NLP System and the Challenges of Semantic Representation 2694 Performance on All Shared TextsAs part of the STEP 2008 Symposium, seven groups (including us) each submitted aparagraph of text and then all groups ran their NLP systems on all texts (Bos, 2008).We now discuss BLUE?s capabilities further in the context of these shared texts.
Forthis exercise, we made some minor bug fixes to the system but did not significantlychange or extend the final output representations.
In the below discussion we refer tothe text and sentence numbers in the form (text#.sentence#).
Sometimes text snippetshave been simplified for clarity.What constitutes a Semantic Representation?The notion of a semantic representation can be interpreted in several ways.
At oneextreme, a representation which captures all the salient linguistic structure and phe-nomena could be considered ?semantic?.
Such representations will have structuresomewhat similar to the syntactic structure of the original text, and the task of in-terpreting the inferential consequences of those structures is then left to downstreamprocessing, and considered part of commonsense reasoning rather than ?language un-derstanding?.
At the other extreme, one might require the full logical interpretationof the text to be explicit, in order that the representation be truely ?semantic?, on thegrounds that if the representation does not explicitly support inference of valid con-sequences, the meaning has not been captured.
Various positions exist between thesetwo extremes.
For example, one might represent:(4.2) ?a vaccine prevents cervical cancer?asa.
prevents(vaccine,cervical-cancer); orb.
?v type-of(v,vaccine) & ?x,y isa(x,v), isa(y, cervical-cancer)?
prevents(x,y); orc.
the logic for, approximately, "for all people given (a specific type of) vaccine, theywill not subsequently develop cervical cancer"Similarly, one might represent "typical" in(7.4) "turbines had a typical power rating of 150 kW."asa.
have(turbine,power-rating), value(power-rating, 150kW), typical(power-rating); orb.
logic for (say) "the mode of the power rating of the set of turbines is (approxi-mately) 150kW"Clearly the more a representation is syntax-like, the more a downstream reasoningcomponent will need have to do to identify its inferential consequences.
Conversely,the more a representation makes the meaning explicit, the harder it is to generatethose representations in the first place as even simple sentences can often have highlycomplex meanings.
At what point one considers a representation ?semantic?
is matter270 Clark and Harrisonfor debate; what is clear is that there is often a significant journey to make to get fromtext to valid inferential consequences of that text.
From a pragmatic point of view,like most other language systems BLUE generates representations which are moresyntactically structured.
This means that, whether one considers them ?semantic?or not, considerable additional machinery would typically be needed for performinginference using them.Some other examples of simple sentences with complex meanings include:?
(6.2) ?selling a range of produce?meaning, approximately, ?the number of typesof produce sold is reasonably large?;?
(6.4) ?research has fluctuated with tax incentives?
meaning, approximately, aqualitative relationship exists between the amount of research and the amountof tax incentives;?
(7.2) ?electricity distribution spread to farms?
appealing to the abstract notion ofa spatial region, and meaning, approximately, that the region grows with time.Even with a more syntactic notion of ?semantic representation?, there are numerousmore specific issues which need to be addressed.
Below we identify some which arisein the shared task texts, and comment on our system?s ability to handle them.4.1 Word Sense Disambiguation (WSD)BLUE currently uses a naive, context-independent approach to WSD.
While the naiveguess is often right, there are many cases in the shared texts of unusual senses whichBLUE will miss, e.g., (1.3) ?how long [time] does it take?, (2.2) ?led to [inspired thedevelopment of] a vaccine?, (6.2) ?turn [generate] a profit?.An interesting phenomenon is seen with: (6.2) ?Greensgrow [is] a plot of land andis selling its own vegetables?.
which mixes senses of ?Greensgrow?
as a piece ofland and an institution in the same sentence, causing challenges for standard WSD.One might consider ?Greensgrow?
as denoting an institution and thus ?Greensgrowis a plot of land?
as metonymy, or ?Greensgrow?
as a complex concept with variousfacets.
In either case some complex processing is required.4.2 Semantic Role Labelling (SRL)SRL is itself challenging.
In many cases BLUE has left the relation underspecified(especially noun-noun relations), and has occasionally maked mistakes, e.g.,(3.2) "A table in the corner"is-inside(table01, corner01).
(5.1) "I have problem"has-part(i01,problem01).4.3 CoordinationBLUE will multiply out coordinates, e.g.
:(3.4) "The atmosphere was warm and friendly""be"(atmosphere01, warm01).
"be"(atmosphere01, friendly01).Boeing?s NLP System and the Challenges of Semantic Representation 271Sometimes this multiplication is inappropriate, for example below, BLUE mis-interprets each place as being in both England and France simultaneously:(4.3) "They visited places in England and France"object(visit01,place01),is-inside(place01,England01),is-inside(place01,France01).Note that the alternative ?places in Africa and South of the Equator?
would not beinconsistent as these areas do overlap; domain knowledge is thus required to under-stand the intended semantics.BLUE does not distribute modifiers across coordinates, and thus misses the distri-bution (7.1) ?wind-energy technology and applications?
?
?wind-energy technologyand wind-energy applications?.4.4 Coreference and AnaphoraBLUE performs definite reference resolution based on name, e.g., (1.2) ?an object...theobject...?, and across part of speech, e.g., verb-noun (1.4) ?falls... the fall...?, andadjective-noun (1.2) ?high...
height...?, but not across different names, e.g., (6.3)?Greenslow...
The farm...?.
BLUE does not currently do anaphoric reference reso-lution, so leaves occurrences of ?it?
etc.
unresolved.There are some interesting complex examples of coreference also in the texts:?
(3.3) ?The waiter took the order?The two referents are not mentioned earlier, but are understood by the reader torefer to objects in the described scene.
A system should thus realize that ?Thewaiter?
is the waiter in the restaurant, and ?the order?
is John?s order.?
(2.2) ?Cervical cancer is caused by a virus.
That has been known...?Here the anaphor (?That?)
refers to a proposition rather than an object in theworld.?
(2.3) ?other cancers?This refers to the set of cancers except those previously mentioned, requiringdiscourse analysis to fully capture the semantics.4.5 Generics and Universal QuantificationBLUE interprets generics as statements relating individuals, thus requiring furtherdownstream interpretation of those individuals and transformation of the representa-tion (e.g., to universal quantification and conditionals) for correct inference.
In gen-eral, generics are complex to interpret; not only are quantifications ambiguous, butalso generics typically require substantial unstated information to be filled in for theinterpretation to be meaningful.
For example,(2.1) ?Cervical cancer is caused by a virus?272 Clark and Harrisonshould ultimately be interpreted as (something like) ?An event involving a viruscan create an incidence of cervical cancer?.
An even fuller semantics, requiring moreworld knowledge, would be that the event is infection of a person and that the can-cer incidence is in the same person.
How far one should go to reach this degree ofinterpretation in a ?semantic representation?
is open to debate.Adjectives and adverbs can modify the expectation of measurement results on anensemble, again requiring special representational machinery.
Examples include:(7.4) ?turbines have a typical power rating of...?
(7.5) ?turbines are commonly rated as?
(6.1) ?the community often lacks it [fresh food]?4.6 TimeBLUE ignores tense and aspect information (although it extracts it in the intermediatelogical form), a gap for complete semantics.
However, BLUE will handle some refer-ences to events situated in time and in relation to other events.
In the examples belowof BLUE?s interpretation, note the use of temporal predicates:(5.5) "...made in 1945"time-int-during(make01,year1945)(4.4) "ensures operation until 1999"time-ends(ensure01, year1999).
(5.3) "...yelled.
Then the propellant exploded"next-event(yell01,explode01).
(5.4) "When they killed, they were crouching"time-at(crouch01,kill01).However BLUE does not recognize more complex time references as such, e.g.,(7.1) ?the 1930s?,(7.2) ?the early 1970s?,(7.4) ?mid-?80s?
(misparsed as an adjective), and(7.3) ?the past 30 years?In addition, facts or beliefs may themselves be situated in time, requiring a time-stamp or situation to be attached to an assertion (which BLUE does not do), for ex-ample (6.3) ?revenue of $450,000 in 2007" A particularly complex example is (5.6)"Initially it was suspected that..." meaning, approximately, X suspected Y at the timeimmediately after the previously described event.
Computationally disentangling themeaning of this sentence is a formidable challenge.4.7 Plurals and CollectivesBLUE represents a numbered collective as an individual with a number-of-elements()predicate attached, for example:Boeing?s NLP System and the Challenges of Semantic Representation 273(4.2) "seven people developed"isa(person01, person_n1),agent(develop01, person01),number-of-elements(person01,7).where person01 denotes the collective of 7 people.
(While it is strictly incorrectto assert person01 as an instance of the class person_n1, there are pragmatic benefitsfor doing so.)
Unnumbered plurals and generics are naively represented as singleindividuals at present.4.8 ?Light?
nouns and verbsArguably, some nouns and verbs do not denote objects and events in the world in aliteral sense.
For example, ?X occurred?
can be taken to mean just ?X?, rather thanthere being a separate ?occur?
event.
Recognizing and transforming these requiresspecial processing machinery.
BLUE handles a few examples, e.g., ?X occurred?
?
?X?, but not the following in the shared texts:?
(1.3) ?how long does it [the fall] take?
meaning ?how [temporally] long is thefall??
(4.3) ?doing internship?
meaning ?interning??
(5.4) ?an explosion happened?
meaning ?There was an explosion??
(7.1) ?development was underway?
meaning ?There was development?4.9 Adjectives and AdverbsWhile an adjective or adverb can be trivially attached to a noun/verb, as BLUE does,an elaborated representation of its meaning, as required for inference, is very chal-lenging and context-dependent.
Challenging examples include:?
(6.1) ?North Philadelphia?
; what is the extent of this region??
(7.1) ?modern development??
(4.3) ?similar places?
; other entities with properties close to some currentlymentioned entity?
(4.3) ?future trainer?
; a non-intersective adjective (like ?fake gun?)?
(5.4) ?crouching unnaturally?4.10 Modals and Higher-Order ExpressionsBLUE will handle some modal expressions by placing a proposition as an argumentto another proposition, for example:(4.5) "We would like our school to work similarly..."agent(like01,we01).object(like01,[agent(work01,school01),manner(work01,similarly01)])274 Clark and Harrison(5.6) "It was suspected that this storage reduced thepowder?s stability.
"object(suspect01,["of"(stability01,powder01),agent(reduce01,storage01),object(reduce01,stability01)]).A particularly complex example (which BLUE does not handle) is (5.3) ?Theywerecrouching, which suggested that they knew that an explosion would happen.?
where apast event implies belief in a future event?s occurrence.4.11 Uncertainty and possibilityThe phrases (2.3) ?cancers may be caused by viruses?
and (5.6) ?the storage mighthave reduced stability?
have a complex semantics concerning possibility.
While werepresent the may/might aspect in the initial logical form, BLUE ignores it in thesubsequent representations.4.12 MetonymyThe occurrence of metonymy is somewhat subjective because metonymy is relative toa target ontology.
A full semantic interpretation would include metonymy resolutionwhere present.
BLUE will resolve some special cases of metonymy, in particular withrespect to the Component Library ontology, but those did not occur in these texts.Some metonymy-like examples in the shared texts (which BLUE did not resolve)include:(6.3) ?The [people of the] farm hopes to make a profit?
(2.1) ?.cancer is caused by a virus [infection].?
(7.3) ?
[The amount of] research has fluctuated?4.13 Implicit ArgumentsSometimes a verb or noun has implicit arguments that an interpretation should makeexplicit.
BLUE will recognize some implicit arguments for modals, e.g.
: that .thefarm.
is the implied object making the profit in the below:(6.3) "The farm hopes to make a profit"agent(hope01,farm01),object(hope01,[ agent(make02,farm01), ; implicit arg foundobject(make02,profit02)])but not in other cases, such as in (6.3) ?revenue of $450,000" ?
"the revenue ofthe farm was $450,000?.In general, many relationships are unstated in text and need to be inferred for a fullunderstanding.
Text 3 (the restaurant story) is particularly challenging in this regard.4.14 Special ConstructsThere are some specialized grammatical constructs which are not inherently com-plex to handle, but require special processing.
Examples include money, e.g., (6.3)?$10,000", dates, e.g., (7.4) "mid-?80s", and units of measure, e.g., (1.1) "m/s".
BLUEcurrently only recognizes the latter, which is hard-coded as a single token.
BLUE alsodoes not handle quote characters, e.g., (5.1) ... yelled ?I have a problem?
...Boeing?s NLP System and the Challenges of Semantic Representation 2754.15 Proper NamesBLUE will recognize proper names and encode them with a specific named() predi-cate, e.g.,:(4.2) "Joao Pedro Fonseca"isa(Fonseca01, person_n1),named(Fonseca01, ["Joao","Pedro","Fonseca"]).4.16 Physical quantitiesPhysical quantities need special processing.
BLUE represents physical quantities us-ing a special predicate (called value()) linking the quantity to its magnitude and unitof measurement, e.g.,:(1.1) "125 m"value(height01,[125,m_n1]).4.17 QuestionsBLUE recognizes several question types (?what is the...?, ?what is a...?, ?howmany...?,?how much...?, ?is it true that...?, ?why...?, ?how...?)
and represents them using specialannotations on the variable/proposition in the query, for example:(1.4) "What is the duration?
"query-for(duration01).5 Summary and ConclusionOur language system, BLUE, is able to generate representational structures for manytexts, capturing numerous linguistic phenomena while also missing or misinterpretinga variety of others.
We have presented a small catalog of these phenomena, and com-ments on BLUE?s ability to handle them or otherwise.
As discussed, BLUE?s outputrepresentation is still fairly linguistic in structure, and despite some transformationswould often require substantial downstream processing to identify the explicit mean-ing and inferential consequences of those structures.
Despite this, for cases where thegap between syntax and final logical semantics is small, in particular for the controlledlanguage subset it was originally designed to support, it can generate useful output,and thus constitutes a small step along the way to language understanding.ReferencesBarker, K., B. Porter, and P. Clark (2001).
A library of generic concepts for composingknowledge bases.
In Proc.
1st Int Conf on Knowledge Capture (K-Cap?01), pp.
14?21.
ACM.Bos, J.
(2008).
Introduction to the Shared Task on Comparing Semantic Representa-tions.
In J. Bos and R. Delmonte (Eds.
), Semantics in Text Processing.
STEP 2008Conference Proceedings, Volume 1 of Research in Computational Semantics, pp.257?261.
College Publications.276 Clark and HarrisonClark, P., J. Chaw, J. Thompson, and P. Harrison (2007).
Capturing and answeringquestions posed to a knowledge-based system.
In D. Sleeman and K. Barker (Eds.
),Proc Int Conf on Knowledge Capture (KCap?07), pp.
63?70.Clark, P., P. Harrison, J. Thompson, R. Wojcik, T. Jenkins, and D. Israel (2007).
Read-ing to learn: An investigation into language understanding.
In Proc.
AAAI SpringSymposium on Machine Reading.
AAAI.Harrison, P. and M. Maxwell (1986).
A new implementation of GPSG.
In Proc.
6thCanadian Conf on AI (CSCSI-86), pp.
78?83.
