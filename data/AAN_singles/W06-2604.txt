A Multiclassifier based Document Categorization System: profiting fromthe Singular Value Decomposition Dimensionality Reduction TechniqueAna ZelaiaUPV-EHUBasque Countryccpzejaa@si.ehu.esIn?aki AlegriaUPV-EHUBasque Countryacpalloi@si.ehu.esOlatz ArregiUPV-EHUBasque Countryacparuro@si.ehu.esBasilio SierraUPV-EHUBasque Countryccpsiarb@si.ehu.esAbstractIn this paper we present a multiclassifierapproach for multilabel document classifi-cation problems, where a set of k-NN clas-sifiers is used to predict the category oftext documents based on different trainingsubsampling databases.
These databasesare obtained from the original trainingdatabase by random subsampling.
In or-der to combine the predictions generatedby the multiclassifier, Bayesian voting isapplied.
Through all the classification pro-cess, a reduced dimension vector represen-tation obtained by Singular Value Decom-position (SVD) is used for training andtesting documents.
The good results of ourexperiments give an indication of the po-tentiality of the proposed approach.1 IntroductionDocument Categorization, the assignment of nat-ural language texts to one or more predefinedcategories based on their content, is an impor-tant component in many information organizationand management tasks.
Researchers have con-centrated their efforts in finding the appropriateway to represent documents, index them and con-struct classifiers to assign the correct categories toeach document.
Both, document representationand classification method are crucial steps in thecategorization process.In this paper we concentrate on both issues.
Onthe one hand, we use Latent Semantic Indexing(LSI) (Deerwester et al, 1990), which is a vari-ant of the vector space model (VSM) (Salton andMcGill, 1983), in order to obtain the vector rep-resentation of documents.
This technique com-presses vectors representing documents into vec-tors of a lower-dimensional space.
LSI, whichis based on Singular Value Decomposition (SVD)of matrices, has showed to have the ability to ex-tract the relations among words and documents bymeans of their context of use, and has been suc-cessfully applied to Information Retrieval tasks.On the other hand, we construct a multiclassi-fier (Ho et al, 1994) which uses different train-ing databases.
These databases are obtained fromthe original training set by random subsampling.We implement this approach by bagging, and usethe k-NN classification algorithm to make the cat-egory predictions for testing documents.
Finally,we combine all predictions made for a given doc-ument by Bayesian voting.The experiment we present has been evaluatedfor Reuters-21578 standard document collection.Reuters-21578 is a multilabel document collec-tion, which means that categories are not mutu-ally exclusive because the same document may berelevant to more than one category.
Being awareof the results published in the most recent litera-ture, and having obtained good results in our ex-periments, we consider the categorization methodpresented in this paper an interesting contributionfor text categorization tasks.The remainder of this paper is organized as fol-lows: Section 2, discusses related work on docu-ment categorization for Reuters-21578 collection.In Section 3, we present our approach to deal withthe multilabel text categorization task.
In Section4 the experimental setup is introduced, and detailsabout the Reuters database, the preprocessing ap-plied and some parameter setting are provided.
InSection 5, experimental results are presented anddiscussed.
Finally, Section 6 contains some con-clusions and comments on future work.252 Related WorkAs previously mentioned in the introduction, textcategorization consists in assigning predefinedcategories to text documents.
In the past twodecades, document categorization has receivedmuch attention and a considerable number of ma-chine learning based approaches have been pro-posed.
A good tutorial on the state-of-the-art ofdocument categorization techniques can be foundin (Sebastiani, 2002).In the document categorization task we can findtwo cases; (1) the multilabel case, which meansthat categories are not mutually exclusive, becausethe same document may be relevant to more thanone category (1 to m category labels may be as-signed to the same document, being m the to-tal number of predefined categories), and (2) thesingle-label case, where exactly one category isassigned to each document.
While most machinelearning systems are designated to handle multi-class data1, much less common are systems thatcan handle multilabel data.For experimentation purposes, there are stan-dard document collections available in the pub-lic domain that can be used for document catego-rization.
The most widely used is Reuters-21578collection, which is a multiclass (135 categories)and multilabel (the mean number of categories as-signed to a document is 1.2) dataset.
Many ex-periments have been carried out for the Reuterscollection.
However, they have been performed indifferent experimental conditions.
This makes re-sults difficult to compare among them.
In fact, ef-fectiveness results can only be compared betweenstudies that use the same training and testing sets.In order to lead researchers to use the same train-ing/testing divisions, the Reuters documents havebeen specifically tagged, and researchers are en-couraged to use one of those divisions.
In ourexperiment we use the ?ModApte?
split (Lewis,2004).In this section, we analize the category sub-sets, evaluation measures and results obtained inthe past and in the recent years for Reuters-21578ModApte split.2.1 Category subsetsConcerning the evaluation of the classificationsystem, we restrict our attention to the TOPICS1Categorization problems where there are more than twopossible categories.group of categories that labels Reuters dataset,which contains 135 categories.
However, manycategories appear in no document and conse-quently, and because inductive based learningclassifiers learn from training examples, these cat-egories are not usually considered at evaluationtime.
The most widely used subsets are the fol-lowing:?
Top-10: It is the set of the 10 categorieswhich have the highest number of documentsin the training set.?
R(90): It is the set of 90 categories whichhave at least one document in the training setand one in the testing set.?
R(115): It is the set of 115 categories whichhave at least one document in the training set.In order to analyze the relative hardness of thethree category subsets, a very recent paper hasbeen published by Debole and Sebastiani (Deboleand Sebastiani, 2005) where a systematic, compar-ative experimental study has been carried out.The results of the classification system we pro-pose are evaluated according to these three cate-gory subsets.2.2 Evaluation measuresThe evaluation of a text categorization system isusually done experimentally, by measuring the ef-fectiveness, i.e.
average correctness of the catego-rization.
In binary text categorization, two knownstatistics are widely used to measure this effective-ness: precision and recall.
Precision (Prec) is thepercentage of documents correctly classified into agiven category, and recall (Rec) is the percentageof documents belonging to a given category thatare indeed classified into it.In general, there is a trade-off between preci-sion and recall.
Thus, a classifier is usually evalu-ated by means of a measure which combines pre-cision and recall.
Various such measures havebeen proposed.
The breakeven point, the value atwhich precision equals recall, has been frequentlyused during the past decade.
However, it hasbeen recently criticized by its proposer ((Sebas-tiani, 2002) footnote 19).
Nowadays, the F1 scoreis more frequently used.
The F1 score combinesrecall and precision with an equal weight in thefollowing way:F1 =2 ?
Prec ?
RecPrec + Rec26Since precision and recall are defined only forbinary classification tasks, for multiclass problemsresults need to be averaged to get a single perfor-mance value.
This will be done using microav-eraging and macroaveraging.
In microaveraging,which is calculated by globally summing over allindividual cases, categories count proportionallyto the number of their positive testing examples.In macroaveraging, which is calculated by aver-aging over the results of the different categories,all categories count the same.
See (Debole andSebastiani, 2005; Yang, 1999) for more detailedexplanation of the evaluation measures mentionedabove.2.3 Comparative ResultsSebastiani (Sebastiani, 2002) presents a tablewhere lists results of experiments for various train-ing/testing divisions of Reuters.
Although we areaware that the results listed are microaveragedbreakeven point measures, and consequently, arenot directly comparable to the ones we present inthis paper, F1, we want to remark some of them.In Table 1 we summarize the best results reportedfor the ModApte split listed by Sebastiani.Results reported by R(90) Top-10(Joachims, 1998) 86.4(Dumais et al, 1998) 87.0 92.0(Weiss et.al., 1999) 87.8Table 1: Microaveraged breakeven point resultsreported by Sebastiani for the Reuters-21578ModApte split.In Table 2 we include some more recent re-sults, evaluated according to the microaveragedF1 score.
For R(115) there is also a good result,F1 = 87.2, obtained by (Zhang and Oles, 2001)2.3 Proposed ApproachIn this paper we propose a multiclassifier baseddocument categorization system.
Documents inthe training and testing sets are represented in areduced dimensional vector space.
Different train-ing databases are generated from the original train-2Actually, this result is obtained for 118 categories whichcorrespond to the 115 mentioned before and three more cat-egories which have testing documents but no training docu-ment assigned.Results reported by R(90) Top-10(Gao et al, 2003) 88.42 93.07(Kim et al, 2005) 87.11 92.21(Gliozzo and Strapparava, 2005) 92.80Table 2: F1 results reported for the Reuters-21578ModApte split.ing dataset in order to construct the multiclassifier.We use the k-NN classification algorithm, whichaccording to each training database makes a pre-diction for testing documents.
Finally, a Bayesianvoting scheme is used in order to definitively as-sign category labels to testing documents.In the rest of this section we make a brief re-view of the SVD dimensionality reduction tech-nique, the k-NN algorithm and the combination ofclassifiers used.3.1 The SVD Dimensionality ReductionTechniqueThe classical Vector SpaceModel (VSM) has beensuccessfully employed to represent documents intext categorization tasks.
The newer method ofLatent Semantic Indexing (LSI) 3 (Deerwester etal., 1990) is a variant of the VSM in which doc-uments are represented in a lower dimensionalspace created from the input training dataset.
Itis based on the assumption that there is someunderlying latent semantic structure in the term-document matrix that is corrupted by the wide va-riety of words used in documents.
This is referredto as the problem of polysemy and synonymy.
Thebasic idea is that if two document vectors representtwo very similar topics, many words will co-occuron them, and they will have very close semanticstructures after dimension reduction.The SVD technique used by LSI consists in fac-toring term-document matrix M into the productof three matrices, M = U?V T where ?
is a di-agonal matrix of singular values in non-increasingorder, and U and V are orthogonal matrices of sin-gular vectors (term and document vectors, respec-tively).
Matrix M can be approximated by a lowerrank Mp which is calculated by using the p largestsingular values of M .
This operation is calleddimensionality reduction, and the p-dimensional3http://lsi.research.telcordia.com,http://www.cs.utk.edu/?lsi27space to which document vectors are projected iscalled the reduced space.
Choosing the right di-mension p is required for successful applicationof the LSI/SVD technique.
However, since thereis no theoretical optimum value for it, potentiallyexpensive experimentation may be required to de-termine it (Berry and Browne, 1999).For document categorization purposes (Dumais,2004), the testing document q is also projected tothe p-dimensional space, qp = qTUp?
?1p , and thecosine is usually calculated to measure the seman-tic similarity between training and testing docu-ment vectors.In Figure 1 we can see an ilustration of the doc-ument vector projection.
Documents in the train-ing collection are represented by using the term-document matrix M , and each one of the docu-ments is represented by a vector in the Rm vec-tor space like in the traditional vector space model(VSM) scheme.
Afterwards, the dimension p is se-lected, and by applying SVD vectors are projectedto the reduced space.
Documents in the testingcollection will also be projected to the same re-duced space.d1 d2d3d4d5d2d3d4d5d6d7d9603 d1d9603d6d7......RmReuters-21578, ModApte, TrainingVSMSVDMR pd1 d2 d9603Mp = Up?pV TpFigure 1: Vectors in the VSM are projected to thereduced space by using SVD.3.2 The k nearest neighbor classificationalgorithm (k-NN)k-NN is a distance based classification approach.According to this approach, given an arbitrary test-ing document, the k-NN classifier ranks its near-est neighbors among the training documents, anduses the categories of the k top-ranking neighborsto predict the categories of the testing document(Dasarathy, 1991).
In this paper, the training andtesting documents are represented as reduced di-mensional vectors in the lower dimensional space,and in order to find the nearest neighbors of agiven document, we calculate the cosine similar-ity measure.In Figure 2 an ilustration of this phase can beseen, where some training documents and a test-ing document q are projected in the R p reducedspace.
The nearest to the qp testing document areconsidered to be the vectors which have the small-est angle with qp.
According to the category labelsof the nearest documents, a category label predic-tion, c, will be made for testing document q.d34d61d23d135d509k?NNR pcqpFigure 2: The k-NN classifier is applied to qp test-ing document and c category label is predicted.We have decided to use the k-NN classifier be-cause it has been found that on the Reuters-21578database it performs best among the conventionalmethods (Joachims, 1998; Yang, 1999) and be-cause we have obtained good results in our pre-vious work on text categorization for documentswritten in Basque, a highly inflected language (Ze-laia et al, 2005).
Besides, the k-NN classificationalgorithm can be easily adapted to multilabel cat-egorization problems such as Reuters.3.3 Combination of classifiersThe combination of multiple classifiers has beenintensively studied with the aim of improving theaccuracy of individual components (Ho et al,1994).
Two widely used techniques to implementthis approach are bagging (Breiman, 1996), thatuses more than one model of the same paradigm;and boosting (Freund and Schapire, 1999), inwhich a different weight is given to different train-ing examples looking for a better accuracy.In our experiment we have decided to constructa multiclassifier via bagging.
In bagging, a set oftraining databases TDi is generated by selecting ntraining examples drawn randomly with replace-ment from the original training database TD of nexamples.
When a set of n1 training examples,28n1 < n, is chosen from the original training col-lection, the bagging is said to be applied by ran-dom subsampling.
This is the approach used in ourwork.
The n1 parameter has been selected via tun-ing.
In Section 4.3 the selection will be explainedin a more extended way.According to the random subsampling, given atesting document q, the classifier will make a la-bel prediction ci based on each one of the train-ing databases TDi.
One way to combine the pre-dictions is by Bayesian voting (Dietterich, 1998),where a confidence value cvicj is calculated foreach training database TDi and category cj to bepredicted.
These confidence values have been cal-culated based on the original training collection.Confidence values are summed by category.
Thecategory cj that gets the highest value is finallyproposed as a prediction for the testing document.In Figure 3 an ilustration of the whole ex-periment can be seen.
First, vectors in theVSM are projected to the reduced space by usingSVD.
Next, random subsampling is applied to thetraining database TD to obtain different trainingdatabases TDi.
Afterwards the k-NN classifier isapplied for each TDi to make category label pre-dictions.
Finally, Bayesian voting is used to com-bine predictions, and cj , and in some cases ck aswell, will be the final category label prediction ofthe categorization system for testing document q.In Section 4.3 the cases when a second categorylabel prediction ck is given are explained.d1 d2 d9603d1 d2d3d4d5d2d3d4d5d6d7d9603 d1Reuters?21578, ModApte, Testd9603d6d7...q1 q2 q3299qqd34d61d23d135d509TD2TD1 TD30Reuters?21578, ModApte, Train...k?NN k?NNd50d778d848d638d256d98d2787d33d1989d55d4612d9VSMVSMSVDk?NNRandomSubsamplingBayesian votingTDRm RmR p R p R pR pMMp=Up?pV Tpqp=qT Up?
?1pc1 c2 c30 cj ,(ck)qpqpqpFigure 3: Proposed approach for multilabel docu-ment categorization tasks.4 Experimental SetupThe aim of this section is to describe the documentcollection used in our experiment and to give anaccount of the preprocessing techniques and pa-rameter settings we have applied.When machine learning and other approachesare applied to text categorization problems, a com-mon technique has been to decompose the mul-ticlass problem into multiple, independent binaryclassification problems.
In this paper, we adopt adifferent approach.
We will be primarily interestedin a classifier which produces a ranking of possi-ble labels for a given document, with the hope thatthe appropriate labels will appear at the top of theranking.4.1 Document CollectionAs previously mentioned, the experiment reportedin this paper has been carried out for the Reuters-21578 dataset4 compiled by David Lewis and orig-inally collected by the Carnegie group from theReuters newswire in 1987.
We use one of themost widely used training/testing divisions, the?ModApte?
split, in which 75 % of the documents(9,603 documents) are selected for training and theremaining 25 % (3299 documents) to test the ac-curacy of the classifier.Document distribution over categories in boththe training and the testing sets is very unbalanced:the 10 most frequent categories, top-10, account75% of the training documents; the rest is dis-tributed among the other 108 categories.According to the number of labels assigned toeach document, many of them (19% in trainingand 8.48% in testing) are not assigned to any cat-egory, and some of them are assigned to 12.
Wehave decided to keep the unlabeled documents inboth the training and testing collections, as it issuggested in (Lewis, 2004)5.4.2 PreprocessingThe original format of the text documents is inSGML.
We perform some preprocessing to fil-ter out the unused parts of a document.
We pre-served only the title and the body text, punctua-tion and numbers have been removed and all let-ters have been converted to lowercase.
We have4http://daviddlewis.com/resources/testcollections5In the ?ModApte?
Split section it is suggested as fol-lows: ?If you are using a learning algorithm that requireseach training document to have at least TOPICS category,you can screen out the training documents with no TOPICScategories.
Please do NOT screen out any of the 3,299 docu-ments - that will make your results incomparable with otherstudies.
?29used the tools provided in the web6 in order to ex-tract text and categories from each document.
Wehave stemmed the training and testing documentsby using the Porter stemmer (Porter, 1980)7.
Byusing it, case and flection information are removedfrom words.
Consequently, the same experimenthas been carried out for the two forms of the doc-ument collection: word-forms and Porter stems.According to the dimension reduction, we havecreated the matrices for the two mentioned doc-ument collection forms.
The sizes of the train-ing matrices created are 15591 ?
9603 for word-forms and 11114 ?
9603 for Porter stems.
Differ-ent number of dimensions have been experimented(p = 100, 300, 500, 700).4.3 Parameter settingWe have designed our experiment in order to op-timize the microaveraged F1 score.
Based on pre-vious experiments (Zelaia et al, 2005), we haveset parameter k for the k-NN algorithm to k = 3.This way, the k-NN classifier will give a categorylabel prediction based on the categories of the 3nearest ones.On the other hand, we also needed to decidethe number of training databases TDi to create.
Ithas to be taken into account that a high number oftraining databases implies an increasing computa-tional cost for the final classification system.
Wedecided to create 30 training databases.
However,this is a parameter that has not been optimized.There are two other parameters which have beentuned: the size of each training database and thethreshold for multilabeling.
We now briefly givesome cues about the tuning performed.4.3.1 The size of the training databasesAs we have previously mentioned, documentshave been randomly selected from the originaltraining database in order to construct the 30 train-ing databases TDi used in our classification sys-tem.
There are n = 9, 603 documents in the orig-inal Reuters training collection.
We had to decidethe number of documents to select in order to con-struct each TDi.
The number of documents se-lected from each category preserves the propor-tion of documents in the original one.
We haveexperimented to select different numbers n1 < n6http://www.lins.fju.edu.tw/?tseng/Collections/Reuters-21578.html7http://tartarus.org/martin/PorterStemmer/of documents, according to the following formula:n1 =115?i=12 + tij , j = 10, 20, .
.
.
, 70,where ti is the total number of training documentsin category i.
In Figure 4 it can be seen the vari-ation of the n1 parameter depending on the valueof parameter j.
We have experimented different jvalues, and evaluated the results.
Based on the re-sults obtained we decided to select j = 60, whichmeans that each one of the 30 training databaseswill have n1 = 298 documents.
As we can see,the final classification system will be using train-ing databases which are quite smaller that the orig-inal one.
This gives a lower computational cost,and makes the classification system faster.Parametern1Parameter j2003004005006007008009001000110010  20  30  40  50  60  70Figure 4: Random subsampling rate.4.3.2 Threshold for multilabelingThe k-NN algorithm predicts a unique cate-gory label for each testing document, based on theranked list of categories obtained for each trainingdatabase TDi8.
As previously mentioned, we useBayesian voting to combine the predictions.The Reuters-21578 is a multilabel database, andtherefore, we had to decide in which cases to as-sign a second category label to a testing document.Given that cj is the category with the highest valuein Bayesian voting and ck the next one, the secondck category label will be assigned when the fol-lowing relation is true:cvck > cvcj ?
r, r = 0.1, 0.2, .
.
.
, 0.9, 1In Figure 5 we can see the mean number of cate-gories assigned to a document for different values8It has to be noted that unlabeled documents have beenpreserved, and thus, our classification system treats unlabeleddocuments as documents of a new category30of r. Results obtained were evaluated and basedon them we decided to select r = 0.4, which cor-responds to a ratio of 1.05 categories.Parameter rMultilabelingRatio0.9811.021.041.061.081.11.121.141.160.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Figure 5: Threshold for multilabeling.5 Experimental ResultsIn Table 3 microaveraged F1 scores obtainedin our experiment are shown.
As it could beexpected, a simple stemming process increasesslightly results, and it can be observed that the bestresult for the three category subsets has been ob-tained for the stemmed corpus, even though gainis low (less than 0.6).The evaluation for the Top-10 category subsetgives the best results, reaching up to 93.57%.
Infact, this is the expected behavior, as the number ofcategories to be evaluated is small and the numberof documents in each category is high.
For thissubset the best result has been obtained for 100dimensions, although the variation is low amongresults for 100, 300 and 500 dimensions.
Whenusing higher dimensions results become poorer.According to the R(90) and R(115) subsets, thebest results are 87.27% and 87.01% respectively.Given that the difficulty of these subsets is quitesimilar, their behavior is also analogous.
As wecan see in the table, most of the best results forthese subsets have been obtained by reducing thedimension of the space to 500.6 Conclusions and Future WorkIn this paper we present an approach for multilabeldocument categorization problems which consistsin a multiclassifier system based on the k-NN al-gorithm.
The documents are represented in a re-duced dimensional space calculated by SVD.
Wewant to emphasize that, due to the multilabel char-acter of the database used, we have adapted theDimension reductionCorpus 100 300 500 700Words(10) 93.06 93.17 93.44 92.00Porter(10) 93.57 93.20 93.50 92.57Words(90) 84.90 86.71 87.09 86.18Porter(90) 85.34 86.64 87.27 86.30Words(115) 84.66 86.44 86.73 85.84Porter(115) 85.13 86.47 87.01 86.00Table 3: Microaveraged F1 scores for Reuters-21578 ModApte split.classification system in order for it to be multilabeltoo.
The learning of the system has been unique(9603 training documents) and the category labelpredictions made by the classifier have been eval-uated on the testing set according to the three cat-egory sets: top-10, R(90) and R(115).
The mi-croaveraged F1 scores we obtain are among thebest reported for the Reuters-21578.As future work, we want to experiment withgenerating more than 30 training databases, andin a preliminary phase select the best among them.The predictions made using the selected trainingdatabases will be combined to obtain the final pre-dictions.When there is a low number of documents avail-able for a given category, the power of LSI getslimited to create a space that reflects interestingproperties of the data.
As future work we wantto include background text in the training col-lection and use an expanded term-document ma-trix that includes, besides the 9603 training doc-uments, some other relevant texts.
This may in-crease results, specially for the categories with lessdocuments (Zelikovitz and Hirsh, 2001).In order to see the consistency of our classi-fier, we also plan to repeat the experiment for theRCV1 (Lewis et al, 2004), a new benchmark col-lection for text categorization tasks which consistsof 800,000 manually categorized newswire storiesrecently made available by Reuters.7 AcknowledgementsThis research was supported by the Univer-sity of the Basque Country (UPV00141.226-T-15948/2004) and Gipuzkoa Council in a European31Union Program.ReferencesBerry, M.W.
and Browne, M.: Understanding SearchEngines: Mathematical Modeling and Text Re-trieval.
SIAM Society for Industrial and AppliedMathematics, ISBN: 0-89871-437-0, Philadelphia,(1999)Breiman, L.: Bagging Predictors.
Machine Learning,24(2), 123?140, (1996)Cristianini, N., Shawe-Taylor, J. and Lodhi, H.: LatentSemantic Kernels.
Proceedings of ICML?01, 18thInternational Conference on Machine Learning, 66?73, Morgan Kaufmann Publishers, (2001)Dasarathy, B.V.: Nearest Neighbor (NN) Norms:NN Pattern Recognition Classification Techniques.IEEE Computer Society Press, (1991)Debole, F. and Sebastiani, F.: An Analysis of the Rela-tive Hardness of Reuters-21578 Subsets.
Journal ofthe American Society for Information Science andTechnology, 56(6),584?596, (2005)Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer,T.K.
and Harshman, R.: Indexing by Latent Seman-tic Analysis.
Journal of the American Society forInformation Science, 41, 391?407, (1990)Dietterich, T.G.
: Machine-Learning Research: FourCurrent Directions.
The AI Magazine, 18(4), 97?136, (1998)Dumais, S.T., Platt, J., Heckerman, D. and Sahami,M.
: Inductive Learning Algorithms and Repre-sentations for Text Categorization.
Proceedings ofCIKM?98: 7th International Conference on Infor-mation and Knowledge Management, ACM Press,148?155 (1998)Dumais, S.: Latent Semantic Analysis.
ARIST, An-nual Review of Information Science Technology, 38,189?230, (2004)Freund, Y. and Schapire, R.E.
: A Short Introduction toBoosting.
Journal of Japanese Society for ArtificialIntelligence, 14(5), 771-780, (1999)Gao, S., Wu, W., Lee, C.H.
and Chua, T.S.
: A Maxi-mal Figure-of-Merit Learning Approach to Text Cat-egorization.
Proceedings of SIGIR?03: 26th An-nual International ACM SIGIR Conference on Re-search and Development in Information Retrieval,174?181, ACM Press, (2003)Gliozzo, A. and Strapparava, C.: Domain Kernelsfor Text Categorization.
Proceedings of CoNLL?05:9th Conference on Computational Natural LanguageLearning, 56?63, (2005)Ho, T.K., Hull, J.J. and Srihari, S.N.
: Decision Combi-nation in Multiple Classifier Systems.
IEEE Trans-actions on Pattern Analysis and Machine Intelli-gence, 16(1), 66?75, (1994)Joachims, T. Text Categorization with Support VectorMachines: Learning with Many Relevant Features.Proceedings of ECML?98: 10th European Confer-ence onMachine Learning, Springer 1398, 137?142,(1998)Kim, H., Howland, P. and Park, H.: Dimension Re-duction in Text Classification with Support VectorMachines.
Journal of Machine Learning Research,6, 37?53, MIT Press, (2005)Lewis, D.D.
: Reuters-21578 Text Catego-rization Test Collection, Distribution 1.0.http://daviddlewis.com/resources/testcollectionsREADME file (v 1.3), (2004)Lewis, D.D., Yang, Y., Rose, T.G.
and Li, F.: RCV1: ANew Benchmark Collection for Text CategorizationResearch.
Journal of Machine Learning Research,5, 361?397, (2004)Porter, M.F.
: An Algorithm for Suffix Stripping.
Pro-gram, 14(3), 130?137, (1980)Salton, G. and McGill, M.: Introduction to ModernInformation Retrieval.
McGraw-Hill, New York,(1983)Sebastiani, F.: Machine Learning in Automated TextCategorization.
ACM Computing Surveys, 34(1),1?47, (2002)Weiss, S.M., Apte, C., Damerau, F.J., Johnson, D.E.,Oles, F.J., Goetz, T. and Hampp, T.: MaximizingText-Mining Performance.
IEEE Intelligent Sys-tems, 14(4),63?69, (1999)Yang, Y.
An Evaluation of Statistical Approaches toText Categorization.
Journal of Information Re-trieval.
Kluwer Academic Publishers, 1,(1/2), 69?90, (1999)Zelaia, A., Alegria, I., Arregi, O. and Sierra, B.: An-alyzing the Effect of Dimensionality Reduction inDocument Categorization for Basque.
Proceedingsof L&TC?05: 2nd Language & Technology Confer-ence, 72?75, (2005)Zelikovitz, S. and Hirsh, H.: Using LSI for TextClassification in the Presence of Background Text.Proceedings of CIKM?01: 10th ACM InternationalConference on Information and Knowledge Man-agement, ACM Press, 113?118, (2001)Zhang, T. and Oles, F.J.: Text Categorization Basedon Regularized Linear Classification Methods.
In-formation Retrieval, 4(1): 5?31, Kluwer AcademicPublishers, (2001)32
