AUTOMATIC SPEECH RECOGNITION AND ITSAPPLICATION TO INFORMATION EXTRACTIONSadaoki FuruiDepartment of Computer ScienceTokyo institute of Technology2-12-1, Ookayama, Meguro-ku, Tokyo, 152-8552 Japanfurui@cs.titech.ac.jpABSTRACTThis paper describes recent progress and theauthor's perspectives of speech recognitiontechnology.
Applications of speech recognitiontechnology can be classified into two main areas,dictation and human-computer dialogue systems.In the dictation domain, the automatic broadcastnews transcription is now actively investigated,especially under the DARPA project.
Thebroadcast news dictation technology has recentlybeen integrated with information extraction andretrieval technology and many applicationsystems, such as automatic voice documentindexing and retrieval systems, are underdevelopment.
In the human-computer interactiondomain, a variety of experimental systems forinformation retrieval through spoken dialogue arebeing investigated.
In spite of the remarkablerecent progress, we are still behind our ultimategoal of understanding free conversational speechuttered by any speaker under any environment.This paper also describes the most importantresearch issues that we should attack in order toadvance to our ultimate goal of fluent speechrecognition.pattern recognition paradigm, a data-drivenapproach which makes use of a rich set of speechutterances from a large population of speakers,the use of stochastic acoustic and languagemodeling, and the use of dynamic programming-based search methods.A series of (D)ARPA projects have been a majordriving force of the recent progress in researchon large-vocabulary, continuous-speechrecognition.
Specifically, dictation of speechreading newspapers, uch as north Americabusiness newspapers including the Wall StreetJournal (WSJ), and conversational speechrecognition using an Air Travel InformationSystem (ATIS) task were actively investigated.More recent DARPA programs are the broadcastnews dictation and natural conversational speechrecognition using Switchboard and Call Hometasks.
Research on human-computer dialoguesystems, the Communicator p ogram, has alsostarted \[ 1 \].
Various other systems have beenactively investigated in US, Europe and Japanstimulated by DARPA projects.
Most of themcan be classified into either dictation systems orhuman-computer dialogue systems.1.
INTRODUCTIONThe field of automatic speech recognition haswitnessed a number of significant advances inthe past 5 - 10 years, spurred on by advances insignal processing, algorithms, computationalarchitectures, and hardware.
These advancesinclude the widespread adoption of a statisticalFigure 1 shows a mechanism of state-of-the-artspeech recognizers \[2\].
Common features ofthese systems are the use of cepstral parametersand their regression coefficients as speechfeatures, triphone HMMs as acoustic models,vocabularies of several thousand or several tenthousand entries, and stochastic language modelssuch as bigrams and trigrams.
Such methods have11been applied not only to English but also toFrench, German, Italian, Spanish, Chinese andJapanese.
Although there are several language-specific characteristics, similar recognitionresults have been obtained.Speec~ inputAcousticanalysis I~XI'..X TI Gl?bal search: ~'-P(xr"xTIwr"wk) Ph?nemeinvent?ryl I| maximize Pronunciation lexicon\[IP( xr.. xT IWr..wt).P(wr..wt )l?ver Wl'" wt J,,P(wl""wk) tLanguagemodel \[1Recognizedword sequenceworld domain of obvious value has lead to rapidtechnology transfer of speech recognition i toother esearch areas and applications.
Since thevariations in speaking style and accent as wellas in channel and environment conditions aretotally unconstrained, broadcast newsis a superb stress test hat requires newalgorithms to work across widelyvarying conditions.
Algorithms needto solve a specific problem withoutdegrading any other condit ion.Another advantage of this domain isthat news is easy to collect and thesupply of data is boundless.
The datais found speech; it is completelyuncontrived.Fig.
1 - Mechanism of state-of-the-art speech recognizers.The remainder of this paper is organized asfollows.
Section 2 describes recent progress inbroadcast news dictation and its application toinformation extraction, and Section 3 describeshuman-computer dialogue systems.
In spite ofthe remarkable r cent progress, we are still farbehind our ultimate goal of understanding freeconversational speech uttered by any speakerunder any environment.
Section 4describes howto increase the robustness of speech recognition,and Section 5 describes perspectives of linguisticmodeling for spontaneous speech recognition/understanding.
Section 6 concludes the paper.2.
BROADCAST NEWS DICTATION ANDINFORMATION EXTRACTION2.1 DARPA Broadcast News Dictation ProjectWith the introduction ofthe broadcast news testbed to the DARPA project in 1995, the researcheffort took a profound step forward.
Many ofthe deficiencies ofthe WSJ domain were resolvedin the broadcast news domain \[3\].
Mostimportantly, the fact that broadcast news is a real-2.2 Japanese Broadcast NewsDictation SystemWe have been developing a large-vocabulary continuous-speech recognition(LVCSR) system for Japanese broadcast-newsspeech transcription \[4\]\[5\].
This is a part of ajoint research with the NHK broadcast companywhose goal is the closed-captioning of TVprograms.
The broadcast-news manuscripts hatwere used for constructing the language modelswere taken from the period between July 1992?
and May 1996, and comprised roughly 500ksentences and 22M words.
To calculate word n-gram language models, we segmented thebroadcast-news manuscripts into words by usinga morphological analyzer since Japanesesentences are written without spaces betweenwords.
A word-frequency list was derived for thenews manuscripts, and the 20k most frequentlyused words were selected as vocabulary words.This 20k vocabulary covers about 98% of thewords in the broadcast-news manuscripts.
Wecalculated bigrams and trigrams and estimatedunseen -grams using Katz's back-off smoothingmethod.Japanese text is written by a mixture of threekinds of characters: Chinese characters (Kanji)12and two kinds of Japanese characters (Hira-ganaand Kata-kana).
Most Kanji have multiplereadings, and correct readings can only bedecided according to context.
Conventionallanguage models usually assign equal probabilityto all possible readings of each word.
This causesrecognit ion errors because the assignedprobability is sometimes very different from thetrue probability.
We therefore constructed alanguage model that depends on the readings ofwords in order to take into account the frequencyand context-dependency of the readings.Broadcast news speech includes filled pauses atthe beginning and in the middle of sentences,which cause recognition errors in our languagemodels that use news manuscripts written priorto broadcasting.
To cope with this problem, weintroduced filled-pause modeling into thelanguage model.Table 1 - Experimental results of Japanese broadcast newsdictation with various language models (word error ate \[%\])Evaluation sets Languagemodel m/c m/n f/c f/nLM1 17.6 37.2 14.3 41.2LM2 16.8 35.9 13.6 39.3LM3 14.2 33.1 12.9 38.1News speech data, from TV broadcasts in July1996, were divided into two parts, a clean partand a noisy part, and were separately evaluated.The clean part consisted of utterances with nobackground noise, and the noisy part consistedof utterances with background noise.
The noisypart included spontaneous speech such as reportsby correspondents.
We extracted 50 maleutterances and 50 female utterances for each part,yielding four evaluation sets; male-clean (m/c),male-noisy (m/n), female-clean (f/c), female-noisy (fin).
Each set included utterances by fiveor six speakers.
All utterances were manuallysegmented into sentences.
Table 1 shows theexperimental results for the baseline languagemodel (LM 1) and the new language models.
LM2is the reading-dependent language model, andLM3 is a modification of LM2 by filled-pausemodeling.
For clean speech, LM2 reduced theword error rate by 4.7 % relative to LM1, andLM3 model reduced the word error rate by 10.9% relative to LM2 on average.2.3 Information Extraction in the DARPAProjectNews is fi l led with events, people, andorganizations and all manner of relations amongthem.
The great richness of material and thenaturally evolving content in broadcast news hasleveraged its value into areas of research wellbeyond speech recognition.
In the DARPAproject, the Spoken Document Retrieval (SDR)of TREC and the Topic Detection and Tracking(TDT) program are supported by the samematerials and systems that have beendeveloped inthe broadcast news dictationarena \[3\].
BBN'sRough'n'Reddy systemextracts tructural features of broadcastnews.
CMU's Informedia \[6\], MITRE'sBroadcast Navigator, and SRI's Maestrohave all exploited the multi-media featuresof news producing a wide range ofcapabilities for browsing news archivesinteractively.
These systems integratevarious diverse speech and languagetechnologies including speech recognition,speaker change detection, speaker identification,name extaction, topic classif ication andinformation retrieval.2.4 Information Extraction from JapaneseBroadcast NewsSummarizing transcribed news speech is usefulfor retrieving or indexing broadcast news.
Weinvestigated a method for extracting topic wordsfrom nouns in the speech recognition results onthe basis of a significance measure \[4\]\[5\].
Theextracted topic words were compared with "true"topic words, which were given by three humansubjects.
The results are shown in Figure 2.13When the top five topic words were chosen(recall=13%), 87% of them were correct onaverage.75"~ 5025 Speech-q3- TextI i i i0 25 50 75 100Recall\[%\]Fig.
2 - Topic word extraction results.3.
HUMAN-COMPUTER DIALOGUESYSTEMS3.1 Typical Systems in US and EuropeRecently anumber of sites have been workingon human-computer dialogue systems.
Thefollowings are typical examples.
(a) The View4You systemat the Un ivers i ty  ofKarksruheThe University of Karlsruhefocuses its speech researchon a content-addressablemultimedia informationretrieval system, under amulti-lingual environment,where  quer ies  andmultimedia documents mayappear  in mul t ip lelanguages \[7\].
The system iscalled "View4You" andtheir research is conductedin cooperation with theInformedia project at CMU\[6\].
In the View4Yousystem, German and Servocroatian publicnewscasts are recorded aily.
The newscasts areautomatically segmented and an index is createdfor each of the segments by means of automaticspeech recognition.
The user can query thesystem in natural anguage by keyboard orthrough aspeech utterance.
The system returnsa list of segments which is sorted by relevancewith respect to the user query.
By selecting asegment, he user can watch the correspondingpart of the news show on his/her computer screen.The system overview is shown in Fig.
3.
(b) The SCAN- speech content based audionavigator at AT&T LabsSCAN (Speech Content based Audio Navigator)is a spoken document retrieval system developedat AT&T Labs integrating speaker-independent,large-vocabulary speech recognition withinformation-retrieval to support query-basedretrieval of information from speech archives \[8\].Initial development focused on the applicationof SCAN to the broadcast news domain.
Anoverview of the system architecture is providedin Fig.
4.
The system consists of threecomponents: (1) a speaker-independent large-vocabulary speech recognition engine which(Satellite r ceiver )~ Video( MPEG-coder ) MPEO-video~ MPEG-audioC Segm nter )~ MPEG-audio , Segment boundaries~peech recognizer) MPEO-auaio Text Segment boundariesI Result output \]- - -~  \[ (Thesaurus)Video query server ).~ ResultFront-endText Onput speech recognizer~Ilnternet wWW~spaperlFig.
3 - System overview of the View4You system.14Intonational Iphrase boundary \[detection IClassificationRecognitionUser interfaceInformationretrievalFig.
4 - Overview of the SCAN spoken document system architecture.segments the speech archive and generatestranscripts, (2) an information-retrieval enginewhich indexes the transcriptions and formulateshypotheses regarding document relevance touser-submitted queries and (3) a graphical-user-interface which supports search and localcontextual navigation based on the machine-generated  t ranscr ip ts  and graph ica lrepresentations f query-keyword distribution ithe retrieved speech transcripts.
The speechrecognition component of SCAN includes anintonational phrase boundary detection moduleand a c lass i f i ca t ion  module ,  Thesesubcomponents preprocess the speech data beforepassing the speech to the recognizer itself.
(c )  TheGALAXY- I Iconversat ionalsystem at MITGalaxy is a client-server architecturedeveloped at MITfor accessing on-line informationus ing  spokendialogue \[9\].
Ithasserved  as thetes tbed  fo rdeveloping humanlanguagePhonetechnology atMIT for severalyears.
Recently, they haveinitiated a significant redesignof the GALAXY architectureto make it eas ie r  forresearchers to develop theirown applications, using eitherexclusively their own serversor intermixing them withservers developed by others.This redesign was done in partdue to the fact that GALAXYhas been designed as the firstreference architecture for thenew DARPA Communicator program.
Theresulting configuration of the GALAXY-IIarchitecture is shown in Fig.
5.
The boxes inthis figure represent various human languagetechnology servers as well as information anddomain servers.
The label in italics next to eachbox identifies the corresponding MIT systemcomponent.
Interactions between servers aremediated by the hub and managed in the hubscript.
A particular dialogue session is initiatedby a user either through interaction with agraphical interface at a Web site, through directtelephone dialup, or through adesktop agent.DECTALK& ENVOICEText-to-speech, conversion \[Audio serverSpeech recognitionSUMMITGENESIS\[ Language I generationD-ServerDialogue I management\[ App.ca,ion \[ ' back-endsI-SorvorContext trackingDiscourseFrame \] constructionTINAFig.
5 - Architecture of GALAXY-II.15(d) The ARISE train travel informationsystem at LIMSIThe ARISE (Automatic Railway InformationSystems for Europe) projects aims developingprototype telephone information services for traintravel information i  several European countries\[ 10\].
In collaboration with the Vecsys companyand with the SNCF (the French Railways),LIMSI has developed a prototype telephoneservice providing timetables, imulated fares andreservations, and information on reductions andservices for the main French intercityconnections.
A prototype French/English servicefor the high speed trains between Paris andLondon is also under development.
The systemis based on the spoken language systemsdeveloped for the RailTel project \[11\] and theESPRIT Mask project \[12\].
Compared to theRailTel system, the main advances inARISE arein dialogue management, confidence measures,inclusion of optional spell mode for ci, ty/stationnames, and barge-in capability to allow morenatural interaction between the user and themachine.3.2 Designing a Multimodal Dialogue Systemfor Information RetrievalWe have recently investigated a paradigm fordesigning multimodal dialogue systems \[13\].
Anexample task of the system was to retrieveparticular information about different shops inthe Tokyo Metropolitan area, such as their names,addresses and phone numbers.
The systemaccepted speech and screen touching as input,and presented retrieved information on a screendisplay or by synthesized speech as shown in Fig.6.
The speech recognition part was modeled bythe FSN (finite state network) consisting ofkeywords and fillers, both of which wereimplemented by the DAWG (directed acyclicword-graph) structure.
The number ofkeywordswas 306, consisting of district names andbusiness names.
The fillers accepted roughly100,000 non-keywords/phrases occuring inspontaneous speech.
A variety of dialoguestrategies were designed and evaluated based onan objective cost function having a set of actionsand states as parameters.
Expected ialogue costThe speech recognizer usesn-gram backoff languagemodels estimated on thetranscriptions of spokenqueries.
Since the amountof language model trainingdata is smal l ,  somegrammatical c asses, suchas cities, days and months,are used to provide morerobust estimates of the n-gram probabilities.
Aconf idence  score isassociated with eachInput~ Speechrecognizersc  ey'Output~ Speech Lsynthesizer \]-DialoguemanagerFig.
6 - Multimodal dialogue system structure for information retrieval.hypothesized word, and if the score is below anempirical ly determined threshold, thehypothesized word is marked as uncertain.
Theuncertain words are ignored by the understandingcomponent or used by the dialogue manager tostart clarification subdialogues.was calculated for each strategy, and the beststrategy was selected according to the keywordrecognition accuracy.164.
ROBUST SPEECHRECOGNIT ION4.1  Automat icadaptationUl t imate ly ,  speechrecogn i t ion  systemsshould be capable offrobust ,  speaker -independent or speaker-adaptive, cont inuousspeech recogni t ion ?Figure 7 shows maincauses  o f  acoust i cvariation in speech \[14\].
~.It is crucial to establishmethods that are robustaga ins t  vo ice  var ia t ion  due toind iv idua l i ty ,  the phys ica l  andpsychological condition of the speaker,telephone sets, microphones, networkcharacteristics, additive backgroundnoise, speaking styles, and so on.Figure 8 shows main methods formaking speech recognition systemsrobust against voice variation.
It is alsoimportant for the systems to imposefew res t r i c t ions  on tasks  andvocabulary.
To solve these problems,it is essential to develop automaticadaptation techniques.Extract ion and normal izat ion of.
(adaptation to) voice individuality isone of the most important issues \[ 14\].A smal l  percentage  of  peop leoccasionally cause systems to produceexceptionally low recognition rates?This is an example of the "sheep andgoats"  phenomenon.
Speakeradaptation (normalization) methodscan usua l ly  be c lass i f ied  intosupervised ( text-dependent)  andunsuperv ised ( text - independent)methods?
Unsupervised, on-l ine,INoiSe.
Other speakers \] fDtstortlon ~b i'" ?
Background noise| |N?ise | ?
Reverberations .J / Ech?es l" / /~Dropouts  )-!
Channel ~ recognition-1 I systemSpeaker Task/context?
Voice quality ?
Man-machine?
Pitch dialogue?
Gender ?
Dictation?
Dialect ?
Free conversationSpeaking style ?
Interview?
Stress/emotion Phonetic/prosodic?
Speaking rate context?
Lombard effectMicrophone?
Distortion?
Electrical noiseDirectional |characteristics JFig.
7 - Main causes of acoustic variation in speech.\[ ............... fClose-talking microphone/ (Microphone array Microphone?
fAuditory modelsAnalysis and feature xtraction ..... ~(EIH, SMC, PLP)/" Adaptive filteringJ \[ Noise subtraction.
."
.
,~ \] Comb filtering venture-level normmizatiorv/ 1 (n ,~t ' .
t r , ' j l  .
.
.
.
i nnada t tion r'--x ~'v .
.
.
.
.
.
.
.
.
vv .
.
.~  p a. , / ~ Cepstral mean normalization/ l A cepstra, ~.
RASTAr ( Noise addition| J HMM (de) composition(PMC)........................... "~ Model transformation(MLLR)Model-level t ......
I, Bayesian adaptive l arningnormalization/I _ ' ,adaptation ~ Distance// f'Frequency weighting measure?
~ ' \[ \[similarity  ...... ~ Weighted cepstral distance| I I measures \[ I.Cepstrum projection measure(Reference~ / /I temolates/I ~ ~ .
.I~models ) Word spottm Robust matching~--- ~-- ~ .
.
/ t.utterance v nncation\]Linguisti c processing t ....
Language model adaptationFig.
8 - Main methods to cope with voice variation inspeech recognition.17instantaneous/incremental ad ptation is ideal,since the system works as if it were a speaker-independent system, and it performs increasinglybetter as it is used.
However, since we have toadapt many phonemes using a limited size ofutterances including only a limited number ofphonemes, it is crucial to use reasonablemodeling of speaker-to-speaker variablity orconstraints.
Modeling of the mechanism ofspeech production is expected to provide a usefulmodeling of speaker-to-speaker va iability.4.2 On-line speaker adaptation in broadcastnews dictationSince, in broadcast news, each speaker uttersseveral sentences in succession, the recognitionerror rate can be reduced by adapting acousticmodels incrementally within a segment hatcontains only one speaker.
We applied on-line,unsupervised, instantaneous and incrementalspeaker adaptation combined with automaticdetection of speaker changes \[4\].
The MLLR \[ 15\]-MAP \[ 16\] and VFS (vector-field smoothing)\[17\] methods were instantaneously andincrementally carried out for each utterance.
Theadaptation process is as follows.
For the firstinput utterance, the speaker-independ?nt modelis used for both recognition and adaptation, andthe first speaker-adapted model is created.
Forthe second input utterance, the likelihood valueof the utterance given the speaker-independentmodel and that given the speaker-adapted modelare calculated and compared.
If the former valueis larger, the utterance is considered to be thebeginning of a new speaker, and another speaker-adapted model is created.
Otherwise, the existingspeaker-adapted model is incrementally adapted.For the succeeding input utterances, peakerchanges are detected in the same way bycomparing the acoustic likelihood values of eachutterance obtained from the speaker-independentmodel and some speaker-adapted models.
If thespeaker-independent model yields a largerlikelihood than any of the speaker-adaptedmodels, a speaker change is detected and a newspeaker-adapted model is constructed.Experimental results show that the adaptationreduced the word error rate by 11.8 % relative tothe speaker-independent models.5.
PRESPECTIVES OF LANGUAGEMODELING5.1 Language modeling for spontaneousspeech recognitionOne of the most important issues for speechrecognition is how to create language models(rules) for spontaneous peech.
Whenrecognizing spontaneous speech in dialogues, itis necessary to deal with variations that are notencountered when recognizing speech that is readfrom texts.
These variations include extraneouswords, out-of-vocabulary words, ungrammaticalsentences, disfluency, partial words, repairs,hesitations, and repetitions.
It is crucial todevelop robust and flexible parsing algorithmsthat match the characteristics of spontaneousspeech.
A paradigm shift from the presenttranscription-based approach to a detection-basedapproach will be important to solve suchproblems \[2\].
How to extract contextualinformation, predict users' responses, and focuson key words are very important issues.Stochastic language modeling, such as bigramsand trigrams, has been a very powerful tool, soit would be very effective to extend its utility byincorporating semantic knowledge.
It would alsobe useful to integrate unification grammars andcontext-free grammars for efficient wordprediction.
Style shifting is also an importantproblem in spontaneous speech recognition.
Intypical laboratory experiments, peakers arereading lists of words rather than trying toaccomplish a real task.
Users actually trying toaccomplish a task, however, use a differentlinguistic style.
Adaptation of linguistic modelsaccording to tasks, topics and speaking styles isa very important issue, since collecting a largelinguistic database for every new task is difficultand costly.185.2 Message-Driven Speech Recognit ionState-of-the-art automatic speech recognitionsystems employ the criterion of maximizingP(/4,qX), where W is a word sequence, and X isan acoustic observation sequence.
This criterionis reasonable for dictating read speech.
However,the ultimate goal of automatic speech recognitionis to extract he underlying messages of thespeaker f om the speech signals.
Hence we needto model the process of speech generation andrecognition as shown in Fig.
9 \[ 18\], where M isthe message (content) hat a speaker intended toconvey.models in the same way as in usual recognitionprocesses.
We assume that P(M) has a uniformprobability for all M. Therefore, we only need toconsider further the term P(~M).
We assumethat P (~M)  can be expressed as follows.P(WW/) -P( M) P( WI M) P( XI W)Message ~ Linguistic ~ Acoustic ~.~ Speechsource channel channel recognizer?
Language ?
SpeakerVocabulary ReverberationGrammar NoiseSemantics Transmission-Context characteristicsHabits MicrophoneFig.
9 - A communication - theoretic view of speech generation andrecognition.According to this model, the speech recognitionprocess is represented as the maximization fthefollowing a posteriori probability \[4\]\[5\],(4)where ~, 0<-/1.<1, is a weighting factor.
P(W),the first term of the right hand side, represents apart of P (~M)  that is independent of Mand canbe given by a general statistical language model.P'(WIM), the second term of the right hand side,represents he part ofP(WIA D that depends onM.
We consider that M isrepresented bya co-occurrenceof  words  based on thedistributional hypothesis byHarris \[ 19\].
Since this approachformulates P'(WIM) withoutexplicitly representing M, it canuse information about thespeaker's message M withoutbe ing  a f fec ted  by thequantization problem of topicclasses.
This new formulationof speech recognit ion wasappl ied  to the Japanesebroadcast news dictation, and it was found thatword error rates for the clean set were slightlyreduced by this method.maxP(MIX) = max\]~ P(MIW)P(WIX).
(1) M M WUsing Bayes' rule, Eq.
(1) can be expressed asmaxP(MIX) = maxZ P(XIW) P(WIM) P(M)M w P(X) (2)For simplicity, we can approximate he equationasP(XlW) P(W1M) P(M) max P(MIX) = max (3)M M, w P(X)P(X1W) is calculated using hidden Markov6.
CONCLUSIONSSpeech recognition technology has made aremarkable progress in the past 5 - 10 years.Based on the progress, various applicationsystems have been developed using dictation andspoken dialogue technology.
One of the mostimportant applications i  information extractionand retrieval.
Using the speech recognitiontechnology, broadcast news can be automaticallyindexed, producing awide range of capabilitiesfor browsing news archives interactively.
Sincespeech is the most natural and eff icientcommunication method between humans,19automatic speech recognition will continue tofind applications, uch as meeting/conferencesummarization, automatic closed captioning, andinterpreting telephony.
It is expected that speechrecognizer will become the main input device ofthe "wearable" computers that are now activelyinvestigated.
In order to materialize theseapplications, we have to solve many problems.The most important issue is how to make thespeech recognition systems robust againstacoustic and lingustic variation in speech.
In thiscontext, aparadigm shitt from speech recognitionto understanding where underlying messages ofthe speaker, that is, meaning/context that thespeaker intended to convey are extracted, insteadof transcribing all the spoken words, will beindispensable.REFERENCES\[ 1 \] http://fofoca.mitre.org\[2\] S. Furui: "Future directions in speech informationprocessing", Proc.
16th ICA and 135th MeetingASA, Seattle, pp.
1-4 (1998)\[3\] F. Kubala: "Broadcast news is good news",DARPA Broadcast News Workshop, Virginia(1999)\[4\] K. Ohtsuki, S. Furui, N. Sakurai, A. Iwasaki andZ.-P. Zhang: "Improvements in Japanese broadcastnews transcription", DARPA Broadcast NewsWorkshop, Virginia (1999)\[5\] K. Ohtsuki, S. Furui, A. Iwasaki and N. Sakurai:"~lessage-driven speech recognition and topic-word extraction", Proc.
IEEE Int.
Conf.
Acoust.,Speech, Signal Process., Phoenix, pp.
625-628(1999)\[6\] M. Witbrock and A. G. Hauptmann: "Speechrecognition and information retrieval:Experiments in retrieving spoken documents",Proc.
DARPA Speech Recognition Workshop,Virginia, pp.
160-164 (1997).
See also http://www.informedia.cs.cmu.edu/\[7\] T. Kemp, P. Geutner, M. Schmidt, B. Tomaz, M.Weber, M. Westphal and A. Waibel: "Theinteractive systems labs View4You video indexingsystem", Proc.
Int.
Conf.
Spoken LanguageProcessing, Sydney, pp.
1639-1642 (1998)\[8\] J. Choi, D. Hindle, J. Hirschberg, I. Magrin-Chagnolleau, C. Nakatani, F. Pereira, A. Singhaland S. Whittaker: "SCAN - speech content basedaudio navigator: asystems overview", Proc.
Int.Conf.
Spoken Language Processing, Sydney, pp.2867-2870 (1998)\[9\] S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmidand V. Zue: "GALAXY-II: a reference architecturefor conversational system development", Proc.
Int.Conf.
Spoken Language Processing, Sydney, pp.931-934 (1998)\[10\] L. Lamel, S. Rosset, J. L. Gauvain and S.Bennacef: "The LIMSI ARISE system for traintravel information", Proc.
IEEE Int.
Conf.
Acoust.,Speech, Signal Process., Phoenix, pp.
501-504(1999)\[11\] L. F. Lamel, S. K. Bennacef, S. Rosset, L.Devillers, S. Foukia, J. J. Gangolf and J. L.Gauvain: "The LIMSI RailTel system: Field trialof a telephone service for rail travel information",Speech Communication, 23, pp.
67-82 (1997)\[12\] J. L. Gauvain, J. J. Gangolf and L. Lamel:"Speech recognition for an information Kiosk",Proc.
Int.
Conf.
Spoken Language Processing,Philadelphia, pp.
849-852 (1998)\[13\] S. Furui and K. Yamaguchi: "Designing amultimodal dialogue system for informationretrieval", Proc.
Int.
Conf.
Spoken LanguageProcessing, Sydney, pp.
1191-1194 (1998)\[14\] S. Furui: "Recent advances in robust speechrecognition", Proc.
ESCA-NATO Workshop onRobust Speech Recognition for UnknownCommunication Channels, Pont-a-Mousson,France, pp.
11-20 (1997)\[ 15\] C. J. Leggetter and P. C. Woodland: "Maximumlikelihood linear egression for speaker adaptationof continuous density hidden Markov models",Computer Speech and Language, pp.
171-185(1995).\[16\] J.
-L. Gauvain and C.-H. Lee: "Maximum aposteriori estimation for multivariate Gaussianmixture observations of Markov chains" IEEETrans.
on Speech and Audio Processing, 2, 2, pp.291-298 (1994).\[17\] K. Ohkura, M. Sugiyama nd S. Sagayama:"Speaker adaptation based on transfer vector fieldsmoothing with continuous mixture densityHMMs", Proc.
Int.
Conf.
Spoken LanguageProcessing, Banff, pp.
369-372 (1992)\[18\] B.-H. Juang: "Automatic speech recognition:Problems, progress & prospects", IEEE Workshopon Neural Networks for Signal Processing (1996)\[19\] Z. S. Harris: "Co-occurrence and transformationin linguistic structure", Language, 33, pp.
283-340 (1957)20
