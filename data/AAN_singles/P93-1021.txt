A LANGUAGE- INDEPENDENT ANAPHORA RES()LUTIONSYSTEM FOR UNDERSTANDING MULTILINGUAL TEXTSChinatsu Aone and Douglas McKeeSystems Research and Appl icat ions (SRA)2000 15th Street NorthArl ington, VA 22201aonec@sra.com, ckeed@sra.comAbst ractThis paper describes a new discourse modulewithin our multilingual NLP system.
Because ofits unique data-driven architecture, the discoursemodule is language-independent.
Moreover, theuse of hierarchically organized multiple knowledgesources makes the module robust and trainable usingdiscourse-tagged corpora.
Separating discourse phe-nomena from knowledge sources makes the discoursemodule easily extensible to additional phenomena.1 In t roduct ionThis paper describes a new discourse module withinour multilingual natural anguage processing systemwhich has been used for understanding texts in En-glish, Spanish and Japanese (el.
\[1, 2\])) The follow-ing design principles underlie the discourse module:?
Language-independence: No processing code de-pends on language-dependent fac s.?
Extensibility: It is easy to handle additional phe-nomena.?
Robustness: The discourse module does its besteven when its input is incomplete or wrong.?
Trainability: The performance can be tuned forparticular domains and applications.In the following, we first describe the architectureof the discourse module.
Then, we discuss how itsperformance is evaluated and trained using discourse-tagged corpora.
Finally, we compare our approach toother research.1 Our system has been used in several data  extract ion tasksand a pro to type  nlachine t rans lat ion  systeln.per fo .m .
.
.
.
~nt i  ~u2k c$~ " e dv.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.r .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
o .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
- ,l : ) i~  ~ Modu leFigure 1: Discourse Architecture2 Discourse  Arch i tec tureOur discourse module consists of two discourse pro-cessing submodules (the Discourse A dministralor andthe Resolution Engine), and three discourse knowl-edge bases (the Discourse Knowledge Source KB,the Discourse Phenomenon KB, and the DiscourseDomain KB).
The Discourse Administrator is adevelopment-time tool for defining the three dis-course KB's.
The Resolution Engine, on the otherhand, is the run-time processing module which ac-tually performs anaphora resolution using these dis-course KB's.The Resolution Engine also has access to an ex-ternal discourse data structure called the global dis-course world, which is created by the top-level textprocessing controller.
The global discourse worldholds syntactic, semantic, rhetorical, and other infor-mation about the input text derived by other partsof the system.
The architecture is shown in Figure i.2.1 D iscourse  Data  S t ruc turesThere are four major discourse data types within theglobal discourse world: Discourse World (DW), \[)is-156course Clause (DC), Discourse Marker (DM), andFile Card (FC), as shown in Figure 2.The global discourse world corresponds to an entiretext, and its sub-discourse worlds correspond to sub-components of the text such as paragraphs.
Discourseworlds form a tree representing a text's structure.A discourse clause is created for each syntacticstructure of category S by the semantics module.
Itcan correspond to either a full sentence or a part of aflfll sentence.
Each discourse clause is typed accord-ing to its syntactic properties.A discourse marker (cf.
Kamp \[14\], or "discourseentity" in Ayuso \[3\]) is created for each noun or verbin the input sentence during semantic interpietation.A discourse marker is static in that once it is intro-duced to the discourse world, the information withinit is never changed.Unlike a discourse marker, a file card (cf.
Heim \[11\],"discourse referent" in Karttunen \[15\], or "discourseentity" in Webber \[19\]) is dynamic in a sense thatit is continually updated as the discourse process-ing proceeds.
While an indefinite discourse markerstarts a file card, a definite discourse marker updatesan already existing file card corresponding to its an-tecedent.
In this way, a file card keeps track of allits co-referring discourse markers, and accumulatessemantic information within them.2 .2  D iscourse  Admin is t ra torOur discourse module is customized at developmenttime by creating and modifying the three discourseKB's using the Discourse Administrator.
First, a dis-course domain is established for a particular NLP ap-plication.
Next, a set of discourse phenomena whichshould be handled within that domain by the dis-course module is chosen (e.g.
definite NP, 3rd per-son pronoun, etc.)
because some phenomena maynot be necessary to handle for a particular applica-tion domain.
Then, for each selected discourse phe-nomenon, a set of discourse knowledge sources arechosen which are applied during anaphora resolution,since different discourse phenomena require differentsets of knowledge sources.2.2.1 D iscourse  Knowledge Source  KBThe discourse knowledge source KB houses smallwell-defined anaphora resolution strategies.
Eachknowledge source (KS) is an object in the hierarchi-cally organized KB, and information in a specific KScan be inherited from a more general KS.There are three kinds of KS's: a generator, a filterand an orderer.
A generator is used to generate pos-w w  hi* Ed i t  '~4=1p/ 10 .
.
.
.
.
J't-- "F '~- ' '=~ IiFigure 3: Discourse Knowledge Source KBsible antecedent hypotheses from the global discourseworld.
Unlike other discourse systems, we have multi-ple generators because different discourse phenomenaexhibit different antecedent distribution patterns (cf.Guindon el al.
\[10\]).
A filter is used to eliminate im-possible hypotheses, while an orderer is used to rankpossible hypotheses in a preference order.
The KStree is shown in Figure 3.Each KS contains three slots: ks-flmction, ks-data,and ks-language.
The ks-function slot contains afunctional definition of the KS.
For example, the func-tional definition of the Syntactic-Gender filter defineswhen the syntactic gender of an anaphor is compati-ble with that of an antecedent hypothesis.
A ks-dataslot contains data used by ks-function.
The sepa-ration of data from function is desirable because aparent KS can specify ks-function while its sub-KS'sinherit the same ks-function but specify their owndata.
For example, in languages like English andJapanese, the syntactic gender of a pronoun imposesa semantic gender restriction on its antecedent.
AnEnglish pronoun "he", for instance, can never referto an NP whose semantic gender is female like "Ms.Smith".
The top-level Semantic-Gender KS, then,defines only ks-flmction, while its sub-KS's for En-glish and Japanese specify their own ks-data and in-herit the same ks-function.
A ks-language slot speci-fies languages if a particular KS is applicable for spe-cific languages.Most of the KS's are language-independent (e.g.all the generators and the semantic type filters), andeven when they are language-specific, the function157(de f f rame d iscourse -wor ld  (d i scourse -d* ta -s t ruc ture )datelocat iontop icspos i t iond iscourse -c lausess u b -d i scou  rse-wor lds~; DWdate  of the  text; l oc~t ion  where  the  text  is o r ig inated; semant ic  concepts  wh ich  cor respond to g lobM top ics  of the  text; the  cor respond ing  character  pos i t ion  in the  text; ~ l ist of d i scourse  c lauses  in the  cur rent  DW; a l ist of DWs subord inate  to  the  cur rent  one(de f f rame d iscourse -c lause  (d i scourse -d~ta -s t ruc ture  ; D( :d i scourse -markers  ; ~ l ist of d iscourse  m~rkers  in  the  cur rent  D(:~syntax  ; ~n f - s t ruc ture  for the  cur rent  DCparse - t ree  ; ~ p~rse  t ree  of th i s  Ssemant ics  ; ~ semant ic  (KB)  ob jec t  represent ing  the  cur rent  DCpos i t ion  ; the  cor respond ing  character  pos i t ion  in  the  textd~te  ; date  of the  cur rent  DC~loca.t ion ; Ioco.t lon of the  cur rent  D(2subord inate -d i scourse -c l suse  ; a DC," subord inate  to the  cur rent  D(:coord in~te -d l scourse -c la t tses )  ; coord inate  DC 's  wh ich  a con jo ined  sentence  cons i s ts  ofII (de l l  di  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ker (d l  d tu re '  ;DM .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Jrpos i t ion  ; the  cor respond ing  character  pos i t ion  in the  textd iscourse -c lause  ; a po in ter  b~ck  to DC:syntax  ; an  f - s t ruc ture  for  the  cur rent  DMsemant ics  ; a semant ic  (KB)  ob jec tfi le card)  ; a po in ter  to  the  f i le card(de f f r&me f i l e -card  (d i scourse -d~t~-s t ruc ture )co - re fe r r ing -d i scou  rse -m~r  kersu pd  a ted-semant ic - in fo ); FC:a l ist  of co - re fe r r ing  DM's; a semant ic  (KB)  ob jec t  wh ich  conta ins  cumulat ive  sem&nt lcsFigure 2: Discourse World, Discourse Clause, Discourse Marker, and File Carddefinitions are shared.
In this way, much of the dis-course knowledge source KB is sharable across differ-ent languages.2.2.2 D iscourse  Phenomenon KBThe discourse phenomenon KB contains hierarchi-cally organized discourse phenomenon objects asshown in Figure 4.
Each discourse phenomenon ob-ject has four slots (alp-definition, alp-main-strategy,dp-backup-strategy, and dp-language) whose valuescan be inherited.
The dp-definilion of a discoursephenomenon object specifies a definition of the dis-course phenomenon so that an anaphoric discoursemarker can be classified as one of the discourse phe-nomena.
The dp-main-strategy slot specifies, for eachphenomenon, a set of KS's to apply to resolve thisparticular discourse phenomenon.
The alp-backup-strategy slot, on the other hand, provides a set ofbackup strategies to use in case the main strategyfails to propose any antecedent hypothesis.
The dp-language slot specifies languages when the discoursephenomenon is only applicable to certain languages(e.g.
Japanese "dou" ellipsis).When different languages use different sets of KS'sfor main strategies or backup strategies for the samediscourse phenomenon, language specific dp-main-strategy or dp-backup-strategy values are specified.For example, when an anaphor is a 3rd person pro-noun in a partitive construction (i.e.
3PRO-Partitive-Parent) 2, Japanese uses a different generator for themain strategy (Current-and-Previous-DC) than En-glish and Spanish (Current-and-Previous-Sentence).2e.g.
"three of them" ill English, "tres de ellos" in Spanish,"uchi san-nin" in JapaamseBecause the discourse KS's are independent of dis-course phenomena, the same discourse KS can beshared by different discourse phenomena.
For exam-ple, the Semantic-Superclass filter is used by bothDefinite-NP and Pronoun, and the Recency ordereris used by most discourse phenomena.2.2.3 D iscourse  Domain  KBThe discourse domain KB contains discourse domainobjects each of which defines a set of discourse phe-nomena to handle \[n a particular domain.
Sincetexts in different domains exhibit different sets of dis-course phenomena, and since different applicationseven within the same domain may not have to handlethe same set of discourse phenomena, the discoursedomain KB is a way to customize and constrain theworkload of the discourse module.2.3 Reso lu t ion  Eng ineThe Resolution Engine is the run-time processingmodule which finds the best antecedent hypothesisfor a given anaphor by using data in both the globaldiscourse world and the discourse KB's.
The Resolu-tion Engine's basic operations are shown in Figure 5.2.3.1 F ind ing  AntecedentsThe Resolution Engine uses the discourse phe-nomenon KB to classify an anaphor as one of thediscourse phenomena (using dp-definition values) andto determine a set of KS's to apply to the anaphor(using dp-main-strategy values).
The Engine thenapplies the generator KS to get an initial set of hy-potheses and removes those that do not pass tile filter158; ?
-~ .
~ _ .
.
.
.
.
.
_-._~_-'~ ~,~, - ,~-~ .... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 4: Discourse Phenomenon KBFor each anaphor ic  discourse marker  ill the current  sentence:F ind -AntecedentInput:  aalaphor to resolve, global discourse worldGet -KSs - fo r -D iscourse -PhenomenonInput:  anaphor  to resolve, discourse phenomenon KBOutput :  a set of discourse KS'sApp ly -KSsh lput :  aalaphor to resolve, global discourse world, discourse KS'sOutput :  the best  hypothes isOutput :  the best  hypothes isUpdate -D iscourse -Wor ldInput:  anaphor ,  best  hypothes is ,  global discourse worldOutput :  updated  global discourse worldFigure 5: Resolution Engine OperationsKS's.
If only one hypothesis rernains, it is returned asthe anaphor's referent, but there may be more thanone hypothesis or none at all.When there is more than one hypothesis, ordererKS's are invoked.
However, when more than one or-derer KS could apply to the anaphor, we face theproblem of how to combine the preference values re-turned by these multiple orderers.
Some anaphoraresolution systems (cf.
Carbonell and Brown \[6\], l~ichand LuperFoy \[16\], Rimon el al.
\[17\]) assign scoresto antecedent hypotheses, and the hypotheses areranked according to their scores.
Deciding the scoresoutput by the orderers as well as the way the scoresare combined requires more research with larger data.In our current system, therefore, when there are mul-tiple hypotheses left, the most "promising" ordereris chosen for each discourse phenomenon.
In Section3, we discuss how we choose such an orderer for eachdiscourse phenomenon by using statistical preference.In the future, we will experiment with ways for eachorderer to assign "meaningful" scores to hypotheses.When there is no hypothesis left after the mainstrategy for a discourse phenomenon is performed, aseries of backup strategies pecified in the discoursephenomenon KB are invoked.
Like the main strut-egy, a backup strategy specifies which generators, fil-ters, and orderers to use.
For example, a backupstrategy may choose a new generator which gener-ates more hypotheses, or it may turn off some of thefilters used by the main strategy to accept previouslyrejected hypotheses.
How to choose a new generatoror how to use only a subset of filters can be deter-mined by training the discourse module on a corpustagged with discourse relations, which is discussed inSection 3.Thus, for example, in order to resolve a 3rd per-son pronoun in a partitive in an appositive (e.g.anaphor ID=1023 in Figure 7), the phenomenon KBspecifies the following main strategy for Japanese:generator = Head-NP, filters = {Semantic-Amount,Semantic-Class, Semantic-Superclass}, orderer = Re-cency.
This particular generator is chosen because inalmost every example in 50 Japanese texts, this typeof anaphora has its antecedent in its head NP.
Nosyntactic filters are used because the anaphor has nouseful syntactic information.
As a backup strategy,a new generator, Adjacent-NP, is chosen in case theparse fails to create an appositive relation betweenthe antecedent NP ID=1022 and the anaphor.159The AIDS Surveillance Committeeconfirmed 7A1DSpatients yesterday.IDM-1semantics: Patient.101 IThree of them werehemophiliac.DM-2semantics: Person.102FC-5coreferring-DM's: {DM-I DM-2}semantics: PatienL101 P^erson.102Figure 6: Updating Discourse World2.3.2 Updat ing  the  G loba l  D iscourse  Wor ldAfter each anaphor resolution, the global discourseworld is updated as it would be in File Change Se-mantics (cf.
Helm \[11\]), and as shown in Figure 6.First, the discourse marker for the anaphor is in-corporated into the file card to which its antecedentdiscourse marker points so that the co-referring dis-course markers point to the same file card.
Then, thesemantics information of the file card is updated sothat it reflects the union of the information from allthe co-referring discourse markers.
In this way, a filecard accumulates more information as the discourseprocessing proceeds.The motivation for having both discourse markersand file cards is to make the discourse processing amonotonic operation.
Thus, the discourse process-ing does not replace an anaphoric discourse markerwith its antecedent discourse marker, but only createsor updates file cards.
This is both theoretically andcomputationally advantageous because the discourseprocessing can be redone by just retracting the filecards and reusing the same discourse markers.2 .4  Advantages  o f  Our  ApproachNow that we have described the discourse module indetail, we summarize its unique advantages.
First,it is the only working language-independent discoursesystem we are aware of.
By "language-independent,"we mean that the discourse module can be used fordifferent languages if discourse knowledge is addedfor a new language.Second, since the anaphora resolution algorithm isnot hard-coded in the Resolution Engine, but is keptin the discourse KB's, the discourse module is ex-tensible to a new discourse phenomenon by choosingexisting discourse KS's or adding new discourse KS'swhich the new phenomenon requires.Making the discourse module robust is another im-portant goal especially when dealing with real-worldinput, since by the time the input is processed andpassed to the discourse module, the syntactic or se-mantic information of the input is often not as accu-rate as one would hope.
The discourse module mustbe able to deal with partial information to make adecision.
By dividing such decision-making into mul-tiple discourse KS's and by letting just the applicableKS's fire, our discourse module handles partial infor-mation robustly.Robustness of the discourse module is also mani-fested when the imperfect discourse KB's or an inac-curate input cause initial anaphor esolution to fail.When the main strategy fails, a set of backup strate-gies specified in the discourse phenomenon KB pro-vides alternative ways to get the best antecedent hy-pothesis.
Thus, the system tolerates its own insuffi-ciency in the discourse KB's as well as degraded inputin a robust fashion.3 Evaluating and Training theDiscourse ModuleIn order to choose the most effective KS's for a par-ticular phenomenon, as well as to debug and trackprogress of the discourse module, we must be able toevaluate the performance of discourse processing.
Toperform objective valuation, we compare the resultsof running our discourse module over a corpus witha set of manually created discourse tags.
Examplesof discourse-tagged text are shown in Figure 7.
Themetrics we use for evaluation are detailed in Figure 8.3 .1  Eva luat ing  the  D iscourse  Modu leWe evaluate overall performance by calculating re-call and precision of anaphora resolution results.
Thehigher these measures are, the better the discoursemodule is working.
In addition, we evaluate the dis-course performance over new texts, using blackboxevaluation (e.g.
scoring the results of a data extrac-tion task.
)To calculate a generator's failure vale, a filter's falsepositive rate, and an orderer's effectiveness, the algo-rithms in Figure 9 are used.
33 .2  Choos ing  Ma in  S t ra teg iesThe uniqueness of our approach to discourse analysisis also shown by the fact that our discourse mod-ule can be trained for a particular domain, similarto the ways grammars have been trained (of.
Black3,,Tile remain ing antecedent  hypotheses"  are the hypothe-ses left after all the filters are appl ied for all anaphor .160Overall Performance: Recall = No~I, Precision = N?/NhI Number of anaphors in inputArc.
Number of correct resolutionsNh Number of resolutions attemptedFilter: Recall = OPc/IPc, \['recision = OPc/OPIPOPOF~1 - OP/IP- o r~/ IF~Number of correct pairs in inputNumber of pairs in inputNumber of pairs output and passed by filterNumber of correct pairs output by filterFraction of input pairs filtered outFraction of correct answers filtered out (false positive rate)Generator: Recall = N?/I, \['recision = Nc/NhINhgcNh/I1 - N~/ INumber of anaphors in inputNumber of hypotheses in inputNumber of times correct answer in outputAverage number of hypothesesFraction of correct answers not returned (failure rate)Orderer:I Number of anaphors in inputN?
Number of correct answers output firstNc/I Success rate (effectiveness)Figure 8: Metrics used for Evaluating and Training DiscourseFor each discourse phenomenon,given anaphor and antecedent pairs in the corpus,calculate how often the generator fails to generate the antecedents.For each discourse phenomenon,given anaphor and antecedent pairs in the corpus,for each filter,calculate how often the filter incorrectly eliminates the antecedents.For each anaphor exhibiting a given discourse phenomenon i  the corpus,given the remaining antecedent hypotheses for the anaphor,for each applicable orderer,test if the orderer chooses the correct antecedent as the best hypothesis.Figure 9: Algorithms for Evaluating Discourse Knowledge Sources161<DM ID=-I000>T 1 ' ~'.~.~4S\]~<./DM> (<DM ID=1001 Type=3PARTA\[The AIDS Surveillance Corru~ttee of the Health and Welfare Ministry(Chairman, Prof?.~or Emeritus Junlchi Sh/okawa), on the 6~h, newlyCOnfirmed 7 AIDS patients (of them 3arc dead) and 17 iafec~d pcop!
?.\]<DM IDol 020 Typc-~DNP Ref=1000>~'/',: -?
'~)~ ~ ~,:.~.~" J~D M >(7)-~ "k~<DM ID=1021>IKIJ~.</DM>~<DM lD=1022 Type=BE Ref=1021>~\[~'\]~.
:~'~</DM> (<DMID=1023 Type=3PARTA Ref=1021>5</DM>~-' Jx)  .
<DM ID=I02AType-ZPARTF Ref=1020></DM>--j ~,~ ' -~.~ '~.~1~)~.
<DMID=1025 Typc--ZPARTF Ref=1020></DM><\[}M ID=I026>~J~,</DM> (<DM ID=1027 Typc=JDEL Ref=1026>~\[4 of ~ 7 ~:wly discovered patients were male homosexuals<t022>(of them<1023> 2 are dead), I is heterosexual woaran, and 2 (ditto l)are by contaminated blood product.\]La Comis io~n de Te 'cn icos  de l  SIDA in fo rmo'  dyerde que ex is ten  <DM ID=2000>196 enfermos  de<DM ID=2OOI>SIDA</DM></DM> en la  ComunidadVa lenc iana .
De <DM ID=2002 Type=PRO Ref f i000>el los</DM>, 147 cor responden a Va lenc ia ;  34 ,  a A l i cante ;y 15,  a Caste l lo 'n .
Mayor i ta r iamente  <DM ID=2003Type=DNP Ref=2001>la  enfermedad</DM> afecta  a <DMID=2004 Type=GEN~Ios hombres</DM>, con 158 cases .Entre <DN ID=2OOfi Type=DNP Ref=2OOO>los afectados</DM> se encuentran nueve nin~os menores  de 13 an'os.Figure 7: Discourse Tagged Corpora\[4\]).
As Walker \[lS\] reports, different discourse algo-rithms (i.e.
Brennan, Friedman and Pollard's center-ing approach \[5\] vs. Hobbs' algorithm \[12\]) performdifferently on different ypes of data.
This suggeststhat different sets of KS's are suitable for differentdomains.In order to determine, for each discourse phe-nomenon, the most effective combination of gener-ators, filters, and orderers, we evaluate overall per-formance of the discourse module (cf.
Section 3.1) atdifferent rate settings.
We measure particular gen-erators, filters, and orders for different phenomenato identify promising strategies.
We try to mini-mize the failure rate and the false positive rate whileminimizing the average number of hypotheses thatthe generator suggests and maximizing the numberof hypotheses that the filter eliminates.
As for or-derers, those with highest effectiveness measures arechosen for each phenomenon.
The discourse moduleis "trained" until a set of rate settings at which theoverall performance of the discourse module becomeshighest is obtained.Our approach is more general than Dagan and Itai\[7\], which reports on training their anaphora reso-lution component so that "it" can be resolved to itscorrect antecedent using statistical data on lexical re-lations derived from large corpora.
We will certainlyincorporate such statistical data into our discourseKS's.3 .3  Determin ing  Backup St ra teg iesIf the main strategy for resolving a particular anaphorfails, a backup strategy that includes either a newset of filters or a new generator is atternpted.
Sincebackup strategies are eml)loyed only when the mainstrategy does not return a hypothesis, a backup strat-egy will either contain fewer filters than the mainstrategy or it will employ a generator that returnsmore hypotheses.If the generator has a non-zero failure rate 4, a newgenerator with more generating capability is chosenfrom the generator tree in the knowledge source KBas a backup strategy.
Filters that occur in the mainstrategy but have false positive rates above a certainthreshold are not included in the backup strategy.4 Re la ted  WorkOur discourse module is similar to Carbonell andBrown \[6\] and Rich and LuperFoy's \[16\] work in us-ing multiple KS's rather than a monolithic approach(cf.
Grosz, Joshi and Weinstein \[9\], Grosz and Sidner\[8\], Hobbs \[12\], Ingria and Stallard \[13\]) for anaphoraresolution.
However, the main difference is that oursystem can deal with multiple languages as well asmultiple discourse phenomena 5 because of our morefine-grained and hierarchically organized KS's.
Also,our system can be evaluated and tuned at a low levelbecause ach KS is independent of discourse phenom-ena and can be turned off and on for automatic eval-uation.
This feature is very important because weuse our system to process real-world data in differentdomains for tasks involving text understanding.References\[i\] Chinatsu Aone, Hatte Blejer, Sharon Flank,Douglas McKee, and Sandy Shinn.
TheMurasaki Project: Multilingual Natural Lan-guage Understanding.
In Proceedings of theARPA Human Language Technology Workshop,1993.\[2\] Chinatsu Aone, Doug McKee, Sandy Shinn,and Hatte Blejer.
SRA: Description of theSOLOMON System as Used for MUC-4.
In Pro-ceedings of Fourth Message Understanding Con-ferencc (MUC-4), 1992.4 Zero fa i lure ra te  means  that  ti le hypotheses  generated  bya generator  a lways  conta ined  tile cor rec t  antecedent .SCarbone l l  and  Brown's  sys tem hand les  on ly  in tersentent ia l3rd  person  pronotms and  some defi l f i te NPs ,  and  Rich andLuperFoy 's  sys tem hand les  only  3rd  person  pronouns .162\[3\] Damaris Ayuso.
Discourse Entities in JANUS.In Proceedings of 27th Annual Meeting of theACL, 1989.\[4\] Ezra Black, John Lafferty, and Salim Roukos.Development and Evaluation of a Broad-(:',overage Probablistic Grammar of English-Language Computer Manuals.
In Proceedings of30lh Annual Meeting of the ACL, 1992.\[5\] Susan Brennan, Marilyn Friedman, and CarlPollard.
A Centering Approach to Pronouns.
InProceedings of 25th Annual Meeting of the A(,'L,1987.\[6\] Jairne G. Carbonell and Ralf D. Brown.Anaphora Resolution: A Multi-Strategy Ap-/)roach.
In Proceedings of the 12lh InternationalConference on Computational Linguistics, 1988.\[7\] Ido Dagan and Alon Itai.
Automatic Acquisitionof Constraints for the Resolution of AnaphoraReferences and Syntactic Ambiguities.
In Pro-ceedings of the 13th International Conference onComputational Linguistics, 1990.\[8\] Barbara Crosz and Candace L. Sidner.
Atten-tions, Intentions and the Structure of Discourse.Computational Linguistics, 12, 1986.\[9\] Barbara J. Grosz, Aravind K. Joshi, and ScottWeinstein.
Providing a Unified Account of Def-inite Noun Phrases in Discourse.
In Proceedingsof 21st Annual Meeting of the ACL, 1983.\[10\] Raymonde Guindon, Paul Stadky, Hans Brun-net, and Joyce Conner.
The Structure of User-Adviser Dialogues: Is there Method in theirMadness?
In Proceedings of 24th Annual Meet-ing of the ACL, 1986.\[11\] Irene Helm.
The Semantics of Definite and In-definite Noun Phrases.
PhD thesis, University ofMassachusetts, 1982.\[12\] Jerry R. Hohbs.
Pronoun Resolution.
TechnicalReport 76-1, Department of Computer Science,City College, City University of New York, 1976.\[13\] Robert Ingria and David Stallard.
A Computa-tional Mechanism for Pronominal Reference.
InProceedings of 27th Annual Meeting of the ACL,1989.\[14\] Hans Kamp.
A Theory of Truth and SemanticRepresentation.
In J. Groenendijk et al, edi-tors, Formal Methods in the Study of Language.Mathematical Centre, Amsterdam, 1981.\[15\] Lauri Karttunen.
Discourse Referents.
In J. Mc-Cawley, editor, Syntax and Semantics 7.
Aca-demic Press, New York, 1976.\[16\] Elaine Rich and Susan LuperFoy.
An Architec-ture for Anaphora Resolution.
In Proceedings ofthe Second Conference on Applied Natural Lan-guage Processing, 1988.\[17\] Mort Rimon, Michael C. McCord, UlrikeSchwall, and Pilar Mart~nez.
Advances in Ma-chine Translation Research in IBM.
In Proceed-zngs of Machine Translation Summit IIl, 1991.\[18\] Marilyn A. Walker.
Evaluating Discourse Pro-cessing Algorithms.
In Proceedings of 27th An-nual Meeting of the ACL, 1989.\[19\] Bonnie Webber.
A Formal Approach to Dis-course Anaphora.
Technical report, Bolt, Be-ranek, and Newman, 1978.163
