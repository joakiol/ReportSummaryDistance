Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 891?898, Vancouver, October 2005. c?2005 Association for Computational LinguisticsA Semantic Scattering Model for the Automatic Interpretation of GenitivesDan MoldovanLanguage Computer CorporationRichardson, TX 75080moldovan@languagecomputer.comAdriana BadulescuLanguage Computer CorporationRichardson, TX 75080adriana@languagecomputer.comAbstractThis paper addresses the automatic clas-sification of the semantic relations ex-pressed by the English genitives.
A learn-ing model is introduced based on the sta-tistical analysis of the distribution of gen-itives?
semantic relations on a large cor-pus.
The semantic and contextual fea-tures of the genitive?s noun phrase con-stituents play a key role in the identifica-tion of the semantic relation.
The algo-rithm was tested on a corpus of approx-imately 2,000 sentences and achieved anaccuracy of 79% , far better than 44% ac-curacy obtained with C5.0, or 43% ob-tained with a Naive Bayes algorithm, or27% accuracy with a Support Vector Ma-chines learner on the same corpus.1 Introduction1.1 Problem DescriptionThe identification of semantic relations in open textis at the core of Natural Language Processing andmany of its applications.
Detecting semantic rela-tions is useful for syntactic and semantic analysis oftext and thus plays an important role in automatictext understanding and generation.
Furthermore, se-mantic relations represent the core elements in theorganization of lexical semantic knowledge basesused for inferences.
Recently, there has been a re-newed interest in text semantics fueled in part bythe complexity of some major research initiativesin Question Answering, Text Summarization, TextUnderstanding and others, launched in the UnitedStates and abroad.Two of the most frequently used linguistic con-structions that encode a large set of semantic rela-tions are the s-genitives, e.g.
?man?s brother?, andthe of-genitives, e.g.
?dress of silk?.
The interpreta-tion of these phrase-level constructions is paramountfor various applications that make use of lexical se-mantics.Example: ?The child?s mother had moved the childfrom a car safety seat to an area near the openpassenger-side door of the car.?
(The Desert Sun,Monday, October 18th, 2004).There are two semantic relations expressed bygenitives: (1) ?child?s mother?
is an s-genitive en-coding a KINSHIP relation, and (2) ?passenger-sidedoor of the car?
is an of-genitive encoding a PART-WHOLE relation.This paper provides a detailed corpus analysis ofgenitive constructions and a model for their auto-matic interpretation in English texts.1.2 Semantics of GenitivesIn English there are two kinds of genitives.
In gen-eral, in one, the modifier is morphologically linkedto the possessive clitic ?s and precedes the head noun(s-genitive, i.e.
NPmodif ?s NPhead), and in thesecond one the modifier is syntactically marked bythe preposition of and follows the head noun (of-genitive, i.e.
NPhead of NPmodif ).Although the genitive constructions have beenstudied for a long time in cognitive linguistics, theirsemantic investigation proved to be very difficult, as891the meanings of the two constructions are difficult topin down.
There are many factors that contribute tothe genitives?
semantic behavior, such as the type ofthe genitive, the semantics of the constituent nouns,the surrounding context, and others.A characteristic of genitives is that they are veryproductive, as the construction can be given varioussemantic interpretations.
However, in some situa-tions, the number of interpretations can be reducedby employing world knowledge.
Consider the ex-amples, ?Mary?s book?
and ?Shakespeare?s book?.
?Mary?s book?
can mean the book Mary owns, thebook Mary wrote, the book Mary is reading, or thebook Mary is very fond of.
Each of these interpre-tations is possible in the right context.
In ?Shake-speare?s book?, however, the preferred interpreta-tion, provided by a world knowledge dictionary, isthe book written by Shakespeare.1.3 Previous WorkThere has been much interest recently on the discov-ery of semantic relations from open-text using sym-bolic and statistical techniques.
This includes theseminal paper of (Gildea and Jurafsky, 2002), Sense-val 3 and coNLL competitions on automatic labelingof semantic roles detection of noun compound se-mantics (Lapata, 2000), (Rosario and Hearst, 2001)and many others.
However, not much work hasbeen done to automatically interpret the genitiveconstructions.In 1999, Berland and Charniak (Berland andCharniak, 1999) applied statistical methods on avery large corpus to find PART-WHOLE relations.Following Hearst?s method for the automatic ac-quisition of hypernymy relations (Hearst, 1998),they used the genitive construction to detect PART-WHOLE relations based on a list of six seeds repre-senting whole objects, (i.e.
book, building, car, hos-pital, plant, and school).
Their system?s output wasan ordered list of possible parts according to somestatistical metrics (Dunning?s log-likelihood metricand Johnson?s significant-difference metric).
Theypresented the results for two specific patterns (?NN?sNN?
and ?NN of DT NN?).
The accuracy obtainedfor the first 50 parts was 55% and for the first 20parts was 70%.In 2003, Girju, Badulescu, and Moldovan (Girju,Badulescu, and Moldovan, 2003) detected the PART-WHOLE relations for some of the most frequentpatterns (including the genitives) using the Itera-tive Semantic Specialization, a learning model thatsearches for constraints in the WordNet noun hierar-chies.
They obtained an f-measure of 93.62% for s-genitives and 91.12% for of-genitives for the PART-WHOLE relation.Given the importance of the semantic relations en-coded by the genitive, the disambiguation of theserelations has long been studied in cognitive linguis-tics (Nikiforidou, 1991), (Barker, 1995), (Taylor,1996), (Vikner and Jensen, 1999), (Stefanowitsch,2001), and others.2 Genitives?
Corpus Analysis2.1 The DataIn order to provide a general model of the genitives,we analyzed the syntactic and semantic behavior ofboth constructions on a large corpus of examples se-lected randomly from an open domain text collec-tion, LA Times articles from TREC-9.
This analy-sis is justified by our desire to answer the followingquestions: ?What are the semantic relations encodedby the genitives??
and ?What is their distribution ona large corpus?
?A set of 20,000 sentences were randomly selectedfrom the LA Times collection.
In these 20,000 sen-tences, there were 3,255 genitive instances (2,249of-constructions and 1,006 s-constructions).
Fromthese, 80% were used for training and 20% for test-ing.Each genitive instance was tagged with the cor-responding semantic relations by two annotators,based on a list of 35 most frequently used semanticrelations proposed by (Moldovan et al, 2004) andshown in Table 1.
The genitives?
noun componentswere manually disambiguated with the correspond-ing WordNet 2.0 senses or the named entities if theyare not in WordNet (e.g.
names of persons, namesof locations, etc).2.2 Inter-annotator AgreementThe annotators, two graduate students in Computa-tional Semantics, were given the genitives and thesentences in which they occurred.
Whenever the an-notators found an example encoding a semantic re-lation other than those provided, they had to tag itas ?OTHER?.
Besides the type of relation, the an-892notators were asked to provide the correct WordNet2.0 senses of the two nouns and information aboutthe order of the modifier and the head nouns in thegenitive construction.
For example, although in of-constructions the head is followed by the modifiermost of the time, this is not always true.
For in-stance, in ?owner of car[POSSESSION]?
the headowner is followed by the modifier car, while in?John?s car[POSSESSION/R]?
the order is reversed.Approximately one third of the training exampleshad the nouns in reverse order.Most of the time, one genitive instance was taggedwith one semantic relation, but there were also sit-uations in which an example could belong to morethan one relation in the same context.
For example,the genitive ?city of USA?
was tagged as a PART-WHOLE relation and as a LOCATION relation.
Therewere 21 such cases in the training corpus.The judges?
agreement was measured using theKappa statistics (Siegel and Castelan, 1988), oneof the most frequently used measure of inter-annotator agreement for classification tasks: K =Pr(A)?Pr(E)1?Pr(E) , where Pr(A) is the proportion oftimes the raters agree and Pr(E) is the probabilityof agreement by chance.The K coefficient is 1 if there is a total agreementamong the annotators, and 0 if there is no agreementother than that expected to occur by chance.On average, the K coefficient is close to 0.82 forboth of and s-genitives, showing a good level ofagreement for the training and testing data on theset of 35 relations, taking into consideration the taskdifficulty.
This can be explained by the instructionsthe annotators received prior to annotation and bytheir expertise in lexical semantics.2.3 Distribution of Semantic RelationsTable 1 shows the distribution of the semantic rela-tions in the annotated corpus.In the case of of-genitives, there were 19 relationsfound from the total of 35 relations considered.
Themost frequently occurring relations were POSSES-SION, KINSHIP, PROPERTY, PART-WHOLE, LOCA-TION, SOURCE, THEME, and MEASURE.There were other relations (107 for of-genitives)that do not belong to the predefined list of 35 rela-tions, such as ?state of emergency?.
These exampleswere clustered in different undefined subsets basedNo.
Freq.
Semantic Relations ExamplesOf S1 36 220 POSSESSION ?Mary?s book?2 25 61 KINSHIP ?Mary?s brother?3 109 75 PROPERTY ?John?s coldness?4 11 123 AGENT ?investigation of the crew?5 5 109 TIME-EVENT ?last year?s exhibition?6 30 7 DEPICTION-DEPICTED ?a picture of my nice?7 328 114 PART-WHOLE ?the girl?s mouth?8 0 0 HYPERNYMY (IS-A) ?city of Dallas?9 0 0 ENTAILMENT N/A10 10 3 CAUSE ?death of cancer?11 11 62 MAKE/PRODUCE ?maker of computer?12 0 0 INSTRUMENT N/A13 32 46 LOCATION/SPACE ?university of Texas?14 0 0 PURPOSE N/A15 56 33 SOURCE/FROM ?president of Bolivia?16 70 5 TOPIC ?museum of art?17 0 0 MANNER N/A18 0 0 MEANS ?service of bus?19 10 4 ACCOMPANIMENT ?solution of the problem?20 1 2 EXPERIENCER ?victim of lung disease?21 49 41 RECIPIENT ?Josephine?s reward?22 0 0 FREQUENCY N/A23 0 0 INFLUENCE N/A24 5 2 ASSOCIATED WITH ?contractors of shipyard?25 115 1 MEASURE ?hundred of dollars?26 0 0 SYNONYMY N/A27 0 0 ANTONYMY N/A28 0 0 PROB.
OF EXISTENCE N/A29 0 0 POSSIBILITY N/A30 0 0 CERTAINTY N/A31 120 50 THEME ?acquisition of the holding?32 8 2 RESULT ?result of the review?33 0 0 STIMULUS N/A34 0 0 EXTENT N/A35 0 0 PREDICATE N/A36 107 49 OTHER ?state of emergency?Table 1: The distribution of the semantic relations inthe annotated corpus of 20,000 sentences.on their semantics.
The largest subsets did not covermore than 3% of the OTHER set of examples.
Thisobservation shows that the set of 35 semantic rela-tions from Table 1 is representative for genitives.Table 1 also shows the semantic preferences ofeach genitive form.
For example, POSSESSION,KINSHIP, and some kinds of PART-WHOLE relationsare most of the time encoded by the s-genitive, whilesome specific PART-WHOLE relations, such as ?dressof silk?
and ?array of flowers?, cannot be encodedbut only by the of-genitive.
This simple analysisleads to the important conclusion that the two con-structions must be treated separately as their seman-tic content is different.
This observation is also con-sistent with other recent work in linguistics on thegrammatical variation of the English genitives (Ste-fanowitsch, 2001).3 The Model3.1 Problem FormulationGiven a genitive, the goal is to develop a procedurefor the automatic labeling of the semantic relationit conveys.
The semantic relation derives from the893semantics of the noun phrases participating in geni-tives as well as the surrounding context.Semantic classification of syntactic patterns ingeneral can be formulated as a learning problem.This is a multi-class classification problem since theoutput can be one of the semantic relations in the set.We cast this as a supervised learning problem whereinput/ output pairs are available as training data.An important first step is to map the characteris-tics of each genitive construction into a feature vec-tor.
Let?s define with xi the feature vector of an in-stance i and let X be the space of all instances; i.e.xi ?
X .
The multi-class classification is performedby a function that maps the feature space X into asemantic space SF : X ?
S, where S is the set of semantic rela-tions from Table 1, i.e.
rk ?
S.Let T be the training set of examples or instancesT = (x1r1,x2r2, ...,xnrn) ?
(X ?
S)n where n isthe number of examples x each accompanied by itssemantic relation label r. The problem is to decidewhich semantic relation r to assign to a new, unseenexample xn+1.
In order to classify a given set ofexamples (members of X), one needs some kind ofmeasure of the similarity (or the difference) betweenany two given members of X .3.2 Feature SpaceAn essential aspect of our approach below is theword sense disambiguation (WSD) of the noun.
Us-ing a state-of-the-art open-text WSD system with70% accuracy for nouns (Novischi et al, 2004), eachword is mapped into its corresponding WordNet 2.0sense.
The disambiguation process takes into ac-count surrounding words, and it is through this pro-cess that context gets to play a role in labeling thegenitives?
semantics.So far, we have identified and experimented withthe following NP features:1.
Semantic class of head noun specifies the Word-Net sense (synset) of the head noun and implic-itly points to all its hypernyms.
It is extracted au-tomatically via a word sense disambiguation mod-ule.
The genitive semantics is influenced heavily bythe meaning of the noun constituents.
For exam-ple: ?child?s mother?
is a KINSHIP relation whereas ?child?s toy?
is a POSSESSION relation.2.
Semantic class of modifier noun specifies theWordNet synset of the modifier noun.
The follow-ing examples show that the semantic of a genitiveis also influenced by the semantic of the modifiernoun; ?Mary?s apartment?
is a POSSESSION rela-tion, and ?apartment of New York?
is a LOCATIONrelation.The positive and negative genitive examples of thetraining corpus are pairs of concepts of the format:<modifier semclass#WNsense;head semclass#WNsense; target>,where target is a set of at least one of the 36 se-mantic relations.
The modifier semclass andhead semclass concepts are WordNet semanticclasses tagged with their corresponding WordNetsenses.3.3 Semantic Scattering Learning ModelFor every pair of <modifier - head> noun genitives,let us define with fmi and fhj the WordNet 2.0 sensesof the modifier and head respectively.
For conve-nience we replace the tuple < fmi , fhj > with fij .The Semantic Scattering Model is based on the fol-lowing observations:Observation 1. fmi and fhj can be regarded as nodeson some paths that link the senses of the most spe-cific noun concepts with the top of the noun hierar-chies.Observation 2.
The closer the pair of noun sensesfij is to the bottom of noun hierarchies the fewer thesemantic relations associated with it; the more gen-eral fij is the more semantic relations.The probability of a semantic relation r given fea-ture pair fijP (r|fij) =n(r, fij)n(fij), (1)is defined as the ratio between the number of occur-rences of a relation r in the presence of feature pairfij over the number of occurrences of feature pairfij in the corpus.
The most probable relation r?
isr?
= argmaxr?RP (r|fij) (2)From the training corpus, one can measure the quan-tities n(r, fij) and n(fij).
Depending on the level ofabstraction of fij two cases are possible:Case 1.
The feature pair fij is specific enough suchthat there is only one semantic relation r for which894P (r|fij) = 1 and 0 for all the other semantic rela-tions.Case 2.
The feature pair fij is general enough suchthat there are at least two semantic relations forwhich P (r|fij) 6= 0.
In this case equation (2) isused to find the most appropriate r?.Definition.
A boundary G?
in the WordNet noun hi-erarchies is a set of synset pairs such that :a) for any feature pair on the boundary, denotedfG?ij ?
G?, fG?ij maps uniquely into only one rela-tion r, andb) for any fuij  fG?ij , fuij maps into more than onerelation r, andc) for any f lij ?
fG?ij , f lij maps uniquely into a se-mantic relation r. Here relations  and ?
mean ?se-mantically more general?
and ?semantically morespecific?
respectively.
This is illustrated in Figure1.Observation 3.
We have noticed that there are moreconcept pairs under the boundary G?
than above, i.e.| {f lij} || {fuij} |.fijG1G 2G3G*G4fijlfuijG*fijG*(b)(a)Figure 1: (a) Conceptual view of the noun hierar-chies separated by the boundary G?
; (b) BoundaryG?
is found through an iterative process called ?se-mantic scattering?.3.4 Boundary Detection AlgorithmAn approximation to boundary G?
is found usingthe training set through an iterative process calledsemantic scattering.
We start with the most generalboundary corresponding to the nine noun WordNethierarchies and then specialize it based on the train-ing data until a good approximation is reached.Step 1.
Create an initial boundaryThe initial boundary denoted G1 is formedfrom combinations of the nine WordNet hierar-chies: abstraction#6, act#2, entity#1, event#1,group#1, possession#2, phenomenon#1, psycholog-ical feature#1, state#4.
To each training exam-ple a corresponding feature fij =< fmi , fhj >is first determined, after which is replaced withthe most general corresponding feature consistingof top WordNet hierarchy concepts denoted withf1ij .
For instance, to the example ?apartment of thewoman?
it corresponds the general feature entity#1-entity#1 and POSSESSION relation, to ?husband ofthe woman?
it corresponds entity#1-entity#1 andKINSHIP relation, and to ?hand of the woman?
itcorresponds entity#1-entity#1 and PART-WHOLE re-lation.
At this high level G1, to each feature pair f 1ijit corresponds a number of semantic relations.
Foreach feature, one can determine the most probablerelation using equation (2).
For instance, to featureentity#1-entity#1 there correspond 13 relations andthe most probable one is the PART-WHOLE relationas indicated by Table 2.Step 2.
Specialize the boundary2.1 Constructing a lower boundaryThis step consists of specializing the semanticclasses of the ambiguous features.
A feature f kijon boundary Gk is ambiguous if it corresponds tomore then one relation and its most relevant rela-tion has a conditional probability less then 0.9.
Toeliminate non-important specializations, we special-ize only the ambiguous classes that occurs in morethan 1% of the training examples.The specialization procedure consists of firstidentifying features f kij to which correspond morethan one semantic relation, then replace these fea-tures with their hyponyms synsets.
Thus one fea-ture breaks into several new specialized features.The net effect is that the semantic relations thatwere attached to f kij will be ?scattered?
across thenew specialized features.
This process continues tilleach feature will have only one semantic relation at-tached.
Each iteration creates a new boundary, asshown in Figure 1.
Table 3 shows statistics of se-mantic features f kij for each level of specializationGk.
Note the average number of relations per fea-ture decreasing asymptotically to 1 as k increases.2.2 Testing the new boundary895R 1 2 3 6 7 11 13 15 16 19 21 24 25 OthersP (r|entity ?
entity) 0.048 0.120 0.006 0.032 0.430 0.016 0.035 0.285 0.012 0.004 0.010 0.001 0.001 0Table 2: Sample row from the conditional probability table where the feature pair is entity-entity.
Thenumbers in the top row identify the semantic relations (as in Table 1).Of-genitives S-genitivesBoundary G1 G2 G3 G1 G2 G3Number of modifier 9 31 74 9 37 91featuresNumber head 9 34 66 9 24 36featuresNo.
of feature pairs 63 out of 81 216 out of 1054 314 out of 4884 41 of 81 157 out of 888 247 out of 3276Number of features 26 153 281 14 99 200with only one relationAverage number of 3 1.46 1.14 3.59 1.78 1.36relations per featureTable 3: Statistics for the semantic class features by level of specialization.The new boundary is more specific then the previ-ous boundary and it is closer to the ideal boundary.However, we do not know how well it behaves onunseen examples and we are looking for a boundarythat classifies with a high accuracy the unseen exam-ples.
We test the boundary on unseen examples.
Forthat we used 10% of the annotated examples (differ-ent from the 10% of the examples used for testing)and compute the accuracy (f-measure) of the newboundary on them.If the accuracy is larger than the previous bound-ary?s accuracy, we are converging toward the bestapproximation of the boundary and thus we shouldrepeat Step 2 for the new boundary.If the accuracy is lower than the previous bound-ary?s accuracy, the new boundary is too specific andthe previous boundary is a better approximation ofthe ideal boundary.For the automatic detection of the semantic re-lations encoded by genitives, the boundary con-structed by the Semantic Scattering model is moreapppropriate than a ?tree cut?, like the ones used forverb disambiguation (McCarthy, 1997) (Li and Abe,1998) and constructed using the Minimum Descrip-tion Length model (Rissanen, 1978).
The develope-ment of a ?tree cut?
model for the detection of thesemantic relations encoded by genitives involves theconstruction of a different ?tree cut?
for each headnoun and threfore the usage of these cuts is restrictedto those head nouns.
On the other hand, SemanticScattering constructs only one boundary that, unlikethe ?tree cut?
model, is general enough to classifyany genitive construction, including the ones withconstituents unseen during training.4 Semantic Relations ClassificationAlgorithmThe ideal boundary G?
is used for classifying thesemantic relations encoded by genitives.
The algo-rithm consists of:Step 1.
Process the sentence.
Perform Word SenseDisambiguation and syntactic parsing of the sen-tence containing the genitive.Step 2.
Identify the head and modifier noun con-cepts.Step 3.
Identify the feature pair.
Using the resultsfrom WSD and WordNet noun hierarchies, map thehead and modifier concepts into the correspondingclasses from the boundary and identify a feature pairfij that has the closest euclidean distance to the twoclasses.Step 4.
Find the semantic relation.
Using the featurefij , determine the semantic relation that correspondsto that feature on the boundary.
If there is no suchrelation, mark it as OTHER.5 ResultsFor testing, we considered 20% of the annotated ex-amples.
We used half of the examples for detectingthe boundary G?
and half for testing the system.G?
Boundary DetectionThe algorithm ran iteratively performing boundary896Of-genitives S-genitivesResults Baseline1 Baseline2 Results Baseline1 Baseline2 ResultsNumber of correctly 49 59 81 15 27 71retrieved relationsNumber of relations 73 75 99 63 66 85retrievedNumber of correct 104 104 104 96 96 96relationsPrecision 67.12% 76.62% 81.82% 23.81% 40.91% 83.53%Recall 47.12% 56.73% 77.88% 15.63% 28.13% 73.96%F-measure 55.37% 65.92% 79.80% 18.87% 33.34% 78.45%Table 4: Overall results for the semantic interpretation of genitivesspecializations on the WordNet IS-A noun hierar-chies in order to eliminate the ambiguities of thetraining examples.
Boundary G1 corresponds to thesemantic classes of the nine WordNet noun hier-archies and boundaries G2 and G3 to their subse-quent immediate hyponyms.
For both s-genitivesand of-genitives, boundary G2 was more accuratethen boundary G1 and therefore we repeated Step2.
However, boundary G3 was less accurate thenboundary G2 and thus boundary G2 is the best ap-proximation of the ideal boundary.Semantic Relations ClassificationTable 4 shows the results obtained when classify-ing the 36 relations (the 36th relation being OTHER)for of-genitives and s-genitives.
The results are pre-sented for the Semantic Scattering system that usesG2 as the best approximation of the G?
together withtwo baselines.
Baseline1 system obtained the re-sults without any word sense disambiguation (WSD)feature, i.e.
using only the default sense number 1for the concept pairs, and without any specializa-tion.
Baseline2 system applied two iterations of theboundary detection algorithm but without any wordsense disambiguation.Overall, the Semantic Scattering System achievesan 81.82% precision and 77.88% recall for of-genitives and an 83.53% precision and 73.96% re-call for s-genitives.Both the WSD and the specialization are impor-tant for our system as indicated by the Baselinesystems performance.
The impact of specializa-tion on the f-measure (Baseline2 minus Baseline1) is10.55% for of-genitives and 14.47% for s-genitives,while the impact of WSD (final result minus Base-line2) is 14% for of-genitives and 45.11% for s-genitives.Error AnalysisAn important way of improving the performance ofa system is to perform a detailed error analysis of theresults.
We have analyzed the various error sourcesencountered in our experiments and summarized theresults in Table 5.Error Type Of-genitives S-genitives%Error %ErrorMissing feature 28.57 29.17General semantic classes 28.57 20.83WSD System 19.05 29.17Reversed order of constituents 14.29 12.5Named Entity Recognizer 4.76 8.33Missing WordNet sense 4.76 0Table 5: The error types encountered on the testingcorpus.6 Comparison with other ModelsTo evaluate our model, we have conducted ex-periments with other frequently used machinelearning models, on the same dataset, using thesame features.
Table 6 shows a comparisonbetween the results obtained with the SemanticScattering algorithm and the decision trees (C5.0,http://www.rulequest.com/see5-info.html), thenaive Bayes model (jBNC, Bayesian NetworkClassifier Toolbox, http://jbnc.sourceforge.net),and Support Vector Machine (libSVM, Chih-Chung Chang and Chih-Jen Lin.
2004.
LIB-SVM: a Library for Support Vector Machines,http://www.csie.ntu.edu.tw/ cjlin/papers/libsvm.pdf).The reason for the superior performance of Se-mantic Scattering is because the classificationof genitives is feature poor relying only on thesemantics of the noun components, and the other897three models normally work better with a larger setof features.Accuracy Of-genitives S-genitivesSemantic Scattering 79.85% 78.75%Decision Trees (C5.0) 40.60% 47.0%Naive Bayes (JBNC) 42.31% 43.7%SVM (LibSVM) 31.45 % 23.51%Table 6: Accuracy performance of four learningmodels on the same testing corpus.7 Discussion and ConclusionsThe classification of genitives is an example of alearning problem where a tailored model outper-forms other generally applicable models.This paper presents a model for the semantic clas-sification of genitives.
A set of 35 semantic relationswas identified, and we provided statistical evidencethat when it comes to genitives, some relations aremore frequent than others, while some are absent.The model relies on the semantic classes of nounconstituents.
The algorithm was trained and testedon 20,000 sentences containing 2,249 of-genitivesand 1006 s-genitives and achieved an average preci-sion of 82%, a recall of 76%, and an f-measure of79%.
For comparison, we ran a C5.0 learning sys-tem on the same corpus and obtained 40.60% accu-racy for of-genitives and 47% for s-genitives.
A sim-ilar experiment with a Naive Bayes learning systemled to 42.31% accuracy for of-genitives and 43.7%for s-genitives.
The performance with a SupportVector Machines learner was the worst, achievingonly a 31.45% accuracy for of-genitives and 23.51%accuracy for s-genitives.
We have also identified thesources of errors which when addressed may bringfurther improvements.ReferencesBarker, Chris.
1995.
Possessive Descriptions.
CSLIPublications, Standford, CA.Berland, Matthew and Eugene Charniak.
1999.
Findingparts in very large.
In Proceeding of ACL 1999.Fellbaum, Christiane.
1998.
WordNet - An ElectronicLexical Databases.
Cambridge MA: MIT Press.Girju, Roxana, Adriana Badulescu, and Dan Moldovan.2003.
Learning semantic constraints for the automaticdiscovery of part-whole relations.
In Proceedings ofthe HLT-NAACL 2003.Gildea, Daniel and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
Computational Linguis-tics, 28(3):277-295.Hearst, Marti.
1998.
Automated Discovery of Word-Net relations.
In An Electronic Lexical Database andSome of its Applications.
MIT Press, Cambridge MA.Lapata, Maria.
2000.
Automatic Interpretation of Nomi-nalizations.
In Proceedings of AAAI 2000, 716-721.Li, Hang and Naoki Abe.
1998.
Generalizing caseframes using a thesaurus and the mdl principle.
Com-putational Linguistics, 24(2):217?224.McCarthy, Diana.
1997.
Word sense disambiguation foracquisition of selectional preferences.
In Proceedingsof the ACL/EACL 97.Moldovan, Dan, Adriana Badulescu, Marta Tatu, DanielAntohe, and Roxana Girju.
2004.
Models for the Se-mantic Classification of Noun Phrases.
In Proceed-ings of the HLT-NAACL 2004, Computational LexicalSemantics Workshop.Nikiforidou, Kiki.
1991.
The meanings of the genitive:A case study in the semantic structure and semanticchange.
Cognitive Linguistics, 2:149?205.Novischi, Adrian, Dan Moldovan, Paul Parker, AdrianaBadulescu, and Bob Hauser.
2004.
LCC?s WSD sys-tems for Senseval 3.
In Proceedings of Senseval 3.Rissanen, Jorma.
1978.
Modeling by shortest data de-scription.
Automatic, 14.Rosario, Barbara and Marti Hearst.
2001.
Classify-ing the Semantic Relations in Noun Compounds viaa Domain-Specific Lexical Hierarchy.
In Proceedingof EMNLP 2001.Siegel, S. and N.J. Castellan.
1988.
Non Paramet-ric Statistics for the behavioral sciences.
New York:McGraw-Hill.Stefanowitsch, Anatol.
2001.
Constructional semanticsas a limit to grammatical alternation: Two genitivesof English.
Determinants of Grammatical Variation inEnglish.Taylor, John.
1996.
Possessives in English.
An ex-ploration in cognitive grammar.
Oxford, ClarendonPress.Vikner, Carl and Per Anker Jensen.
1999.
A semanticanalysis of the English genitive: interaction of lexicaland formal semantics.898
