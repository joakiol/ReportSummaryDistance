Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 218?228,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsNegative Training Data can be Harmful to Text ClassificationXiao-Li  Li Bing Liu See-Kiong  NgInstitute for Infocomm Research University of Illinois at Chicago Institute for Infocomm Research1 Fusionopolis Way #21-01,Connexis Singapore 138632851 South Morgan Street,Chicago, IL 60607-7053, USA1 Fusionopolis Way #21-01,Connexis Singapore 138632xlli@i2r.a-star.edu.sg liub@cs.uic.edu skng@i2r.a-star.edu.sgAbstractThis paper studies the effects of training dataon binary text classification and postulatesthat negative training data is not needed andmay even be harmful for the task.
Traditionalbinary classification involves building a clas-sifier using labeled positive and negativetraining examples.
The classifier is then ap-plied to classify test instances into positiveand negative classes.
A fundamental assump-tion is that the training and test data are iden-tically distributed.
However, this assumptionmay not hold in practice.
In this paper, westudy a particular problem where the positivedata is identically distributed but the negativedata may or may not be so.
Many practicaltext classification and retrieval applications fitthis model.
We argue that in this setting nega-tive training data should not be used, and thatPU learning can be employed to solve theproblem.
Empirical evaluation has been con-ducted to support our claim.
This result is im-portant as it may fundamentally change thecurrent binary classification paradigm.1 IntroductionText classification is a well-studied problem inmachine learning, natural language processing, andinformation retrieval.
To build a text classifier, aset of training documents is first labeled with pre-defined classes.
Then, a supervised machine learn-ing algorithm (e.g., Support Vector Machines(SVM), na?ve Bayesian classifier (NB)) is appliedto the training examples to build a classifier that issubsequently employed to assign class labels to theinstances in the test set.
In this paper, we focus onbinary text classification with two classes (i.e.
pos-itive and negative classes).Most learning methods assume that the trainingand test data have identical distributions.
However,this assumption may not hold in practice, i.e., thetraining and the test distributions can be different.The problem is called covariate shift or sampleselection bias (Heckman 1979; Shimodaira 2000;Zadrozny 2004; Huang et al 2007; Sugiyama et al2008; Bickel et al 2009).
In general, this problemis not solvable because the two distributions can bearbitrarily far apart from each other.
Various as-sumptions were made to solve special cases of theproblem.
One main assumption was that the condi-tional distribution of the class given an instance isthe same over the training and test sets (Shimodai-ra 2000; Huang et al 2007; Bickel et al 2009).In this paper, we study another special case ofthe problem in which the positive training and testsamples have identical distributions, but the nega-tive training and test samples may have differentdistributions.
We believe this scenario is more ap-plicable for binary text classification.
As the focusin many applications is on identifying positive in-stances correctly, it is important that the positivetraining and the positive test data have the samedistribution.
The distributions of the negative train-ing and negative test data can be different.
We be-lieve that this special case of the sample selectionbias problem is also more applicable for machinelearning.
We will show that a partially supervisedlearning model, called PU learning (learning fromPositive and Unlabeled examples) fits this specialcase quite well (Liu et al 2002).Following the notations in (Bickel et al 2009),our special case of the sample selection bias prob-lem can be formulated as follows: We are given atraining sample matrix XL with row vectors x1, ?,xk.
The positive and negative training instances aregoverned by different unknown distributions p(x|?
)218and p(x|?)
respectively.
The element yi of vector y= (y1,  y2, ?, yk) is the class label for training in-stance xi (yi ?
{+1, -1}, where +1 and -1 denotepositive and negative classes respectively) and isdrawn based on an unknown target concept p(y|x).In addition, we are also given an unlabeled test setin matrix XT with rows xk+1, ?, xk+m.
The (hidden)positive test instances in XT are also governed bythe unknown distribution p(x|?
), but the (hidden)negative test instances in XT are governed by anunknown distribution, p(x|?
), where ?
may or maynot be the same as ?.
p(x|?)
and p(x|?)
can differarbitrarily, but there is only one unknown targetconditional class distribution p(y|x).This problem setting is common in many appli-cations, especially in those applications where theuser is interested in identifying a particular type ofdocuments (i.e.
binary text classification).
For ex-ample, we want to find sentiment analysis papersin the literature.
For training a text classifier, wemay label the papers in some EMNLP proceedingsas sentiment analysis (positive) and non-sentimentanalysis (negative) papers.
A classifier can then bebuilt to find sentiment analysis papers from ACLand other EMNLP proceedings.
However, this la-beled training set will not be appropriate for identi-fying sentiment analysis papers from the WWW,KDD and SIGIR conference proceedings.
This isbecause although the sentiment analysis papers inthese proceedings are similar to those in the train-ing data, the non-sentiment analysis papers in theseconferences can be quite different.
Another exam-ple is email spam detection.
A spam classificationsystem built using the training data of spam andnon-spam emails from a university may not per-form well in a company.
The reason is that al-though the spam emails (e.g., unsolicitedcommercial ads) are similar in both environments,the non-spam emails in them can be quite different.One can consider labeling the negative data ineach environment individually so that only thenegative instances relevant to the testing environ-ment are used to train the classifier.
However, it isoften impractical (if not impossible) to do so.
Forexample, given a large blog hosting site, we wantto classify its blogs into those that discuss stockmarkets (positive), and those that do not (nega-tive).
In this case, the negative data covers an arbi-trary range of topics.
It is clearly impractical tolabel all the negative data.Most existing methods for addressing the sam-ple selection bias problem work as follows.
First,they estimate the bias of the training data based onthe given test data using statistical methods.
Then,a classifier is trained on a weighted version of theoriginal training set based on the estimated bias.
Inthis paper, we show that our special case of thesample selection bias problem can be solved in amuch simpler and somewhat radical manner?bysimply discarding the negative training data alto-gether.
We can use the positive training data andthe unlabeled test data to build the classifier usingthe PU learning model  (Liu et al 2002).PU learning was originally proposed to solve thelearning problem where no labeled negative train-ing data exist.
Several algorithms have been devel-oped in the past few years that can learn from a setof labeled positive examples augmented with a setof unlabeled examples.
That is, given a set P ofpositive examples of a particular class (called thepositive class) and a set U of unlabeled examples(which contains both hidden positive and hiddennegative examples), a classifier is built using P andU to classify the data in U as well as future testdata into two classes, i.e., those belonging to P(positive) and those not belonging to P (negative).In this paper, we also propose a new PU learningmethod which gives more consistently accurateresults than the current methods.Our experimental evaluation shows that whenthe distributions of the negative training and testsamples are different, PU learning is much moreaccurate than traditional supervised learning fromthe positive and negative training samples.
Thismeans that the negative training data actuallyharms classification in this case.
In addition, whenthe distributions of the negative training and testsamples are identical, PU learning is shown to per-form equally well as supervised learning, whichmeans that the negative training data is not needed.This paper thus makes three contributions.
First,it formulates a new special case of the sample se-lection bias problem, and proposes to solve theproblem using PU learning by discarding the nega-tive training data.
Second, it proposes a new PUlearning method which is more accurate than theexisting methods.
Third, it experimentally demon-strates the effectiveness of the proposed methodand shows that negative training data is not neededand can even be harmful.
This result is importantas it may fundamentally change the way that manypractical classification problems should be solved.2192 Related WorkA key assumption made by most machine learningalgorithms is that the training and test samplesmust be drawn from the same distribution.
Asmentioned, this assumption can be violated in prac-tice.
Some researchers have addressed this problemunder covariate shift or sample selection bias.Sample selection bias was first introduced in theeconometrics by Heckman (1979).
It came into thefield of machine learning through the work of Za-drozny (2004).
The main approach in machinelearning is to first estimate the distribution bias ofthe training data based on the test data, and thenlearn using weighted training examples to compen-sate for the bias (Bickel et al 2009).Shimodaira (2000) and Sugiyama and Muller(2005) proposed to estimate the training and testdata distributions using kernel density estimation.The estimated density ratio could then be used togenerate weighted training examples.
Dudik et al(2005) and Bickel and Scheffer (2007) used maxi-mum entropy density estimation, while Huang etal.
(2007) proposed kernel mean matching.
Su-giyama et al (2008) and Tsuboi et al (2008) esti-mated the weights for the training instances byminimizing the Kullback-Leibler divergence be-tween the test and the weighted training distribu-tions.
Bickel et al (2009) proposed an integratedmodel.
In this paper, we adopt an entirely differentapproach by dropping the negative training dataaltogether in learning.
Without the negative train-ing data, we use PU learning to solve the problem(Liu et al 2002; Yu et al 2002; Denis et al 2002;Li et al 2003; Lee and Liu, 2003; Liu et al 2003;Denis et al 2003; Li et al 2007; Elkan and Noto,2008; Li et al 2009; Li et al 2010).
We will dis-cuss this learning model further in Section 3.Another related work to ours is transfer learningor domain adaptation.
Unlike our problem setting,transfer learning addresses the scenario where onehas little or no training data for the target domain,but has ample training data in a related domainwhere the data could be in a different feature spaceand follow a different distribution.
A survey oftransfer learning can be found in (Pan and Yang2009).
Several NLP researchers have studied trans-fer learning for different applications (Wu et al2009a; Yang et al 2009; Agirre & Lacalle 2009;Wu et al 2009b; Sagae & Tsujii 2008; Goldwasser& Roth 2008; Li and Zong 2008; Andrew et al2008; Chan and Ng 2007; Jiang and Zhai 2007;Zhou et al 2006), but none of them addresses theproblem studied here.3 PU Learning TechniquesIn traditional supervised learning, ideally, there is alarge number of labeled positive and negative ex-amples for learning.
In practice, the negative ex-amples can often be limited or unavailable.
Thishas motivated the development of the model oflearning from positive and unlabeled examples, orPU learning, where P denotes a set of positive ex-amples, and U a set of unlabeled examples (whichcontains both hidden positive and hidden negativeinstances).
The PU learning problem is to build aclassifier using P and U in the absence of negativeexamples to classify the data in U or a future testdata T. In our setting, the test set T will also act asthe unlabeled set U.PU learning has been investigated by several re-searchers in the past decade.
A study of PAC learn-ing for the setting under the statistical query modelwas given in (Denis, 1998).
Liu et al reported thesample complexity result and showed how theproblem may be solved (Liu et al, 2002).
Subse-quently, a number of practical algorithms (e.g., Liuet al, 2002; Yu et al, 2002; Li and Liu, 2003)were proposed.
They generally follow a two-stepstrategy: (i) identifying a set of reliable negativedocuments RN from the unlabeled set; and then (ii)building a classifier using P (positive set), RN (re-liable negative set) and U-RN (unlabelled set) byapplying an existing learning algorithm (such asnaive Bayesian classifier or SVM) iteratively.There are also some other approaches based onunbalanced errors (e.g., Liu et al 2003; Lee andLiu, 2003; Elkan and Noto, 2008).In this section, we first introduce a representa-tive PU learning technique S-EM, and then presenta new technique called CR-SVM.3.1 S-EM AlgorithmS-EM (Liu et al 2002) is based on na?ve Bayesianclassification (NB) (Lewis, 1995; Nigam et al,2000) and the EM algorithm (Dempster et al1977).
It has two steps.
The first step uses a spytechnique to identify some reliable negatives (RN)from the unlabeled set U and the second step usesthe EM algorithm to learn a Bayesian classifierfrom P, RN and U?RN.220Step 1: Extracting reliable negatives RN from Uusing a spy techniqueThe spy technique in S-EM works as follows (Fig-ure 1): First, a small set of positive examples (de-noted by SP) called ?spies?
is randomly sampledfrom P (line 2).
The default sampling ratio in S-EM is s = 15%.
Then, an NB classifier is built us-ing P?SP as the positive set and U?SP as the neg-ative set (lines 3-5).
The NB classifier is applied toclassify each u ?
U?SP, i.e., to assign a probabil-istic class label p(+|u) (+ means positive) to u. Theidea of the spy technique is as follows.
Since thespy examples were from P and were put into U asnegatives in building the NB classifier, they shouldbehave similarly to the hidden positive instances inU.
We thus can use them to find the reliable nega-tive set RN from U.
Using the probabilistic labelsof spies in SP and an input parameter l (noise lev-el), a probability threshold t is determined.
Due tospace constraints, we are unable to explain l. De-tails can be found in (Liu et al 2002).
t is then usedto find RN from U (lines 8-10).1.
RN ?
?
;                  // Reliable negative set2.
SP ?
Sample(P, s%);          // spy set3.
Assign each example in P ?
SP the class label +1;4.
Assign each example in U ?SP the class label -1;5.
C ?NB(P ?
SP, U?SP);   // Produce a NB classifier6.
Classify each u ?U?SP using C;7.
Decide a probability threshold t using SP and l;8.
For each u ?U do9.
If its probability p(+|u) < t then10.
RN ?
RN ?
{u};Figure 1.
Spy technique for extracting RN from UStep 2: Learning using the EM algorithmGiven the positive set P, the reliable negative setRN, and the remaining unlabeled set U?RN, we runEM using NB as the base learning algorithm.The naive Bayesian (NB) method is an effectivetext classification algorithm.
There are two differ-ent NB models, namely, the multinomial NB andthe multi-variate Bernoulli NB.
In this paper, weuse the multinomial NB since it has been observedto perform consistently better than the multi-variate Bernoulli NB (Nigam et al, 2000).Given a set of training documents D, each doc-ument di ?
D is an ordered list of words.
We usewdi,k to denote the word in position k of di, whereeach word is from the vocabulary V = {w1, ?
, w|v|},which is the set of all words considered in classifi-cation.
We also have a set of classes C = {c1, c2}representing positive and negative classes.
Forclassification, we compute the posterior probabilityPr(cj|di).
Based on the Bayes rule and multinomialmodel, we have(1)and with Laplacian smoothing,(2)where N(wt,di) is the number of times that the wordwt occurs in document di, and Pr(cj|di) {0,1} de-pending on the class label of the document.
As-suming that probabilities of words are independentgiven the class, we have the NB classifier:(3)EM (Dempster et al 1977) is a popular class ofiterative algorithms for maximum likelihood esti-mation in problems with incomplete data.
It is of-ten used to address missing values in the data bycomputing expected values using the existing val-ues.
The EM algorithm consists of two steps, theE-step and the M-step.
The E-step fills in the miss-ing data, and M-step re-estimated the parameters.This process is iterated till satisfaction (i.e.
con-vergence).
For NB, the steps used by EM are iden-tical to those used to build the classifier (equations(3) for the E-step, and equations (1) and (2) for the?
?
?= ==++= |V|s|D|i ijis|D|i ijitjtd|cd,wN|V|d|cd,wNc|w1 11))Pr(())Pr((1)?r(?1.
Each document in P is assigned the class label 1;2.
Each document in RN is assigned the class label ?1;3.
Learn an initial NB classifier f from P and RN, us-ing Equations (1) and (2);4.
Repeat5.
For each document di in U-RN do     // E-Step6.
Using the current classifier f  computePr(cj|di) using Equation (3);7.
Learn a new NB classifier f from P, RN and U-RN by computing Pr(cj) and Pr(wt|cj), usingEquations (1) and (2);                       // M-Step8.
Until the classifier parameters stabilize9.
The last iteration of EM gives the final classifier f ;10.
For each document di in U do11.
If its probability Pr(+|di) ?
0.5 then12.
Output di as a positive document;13. else Output di as a negative documentFigure 2.
EM algorithm with the NB classifier||)|(r)(r||1DdccDi ijj?
= ?=??
?
?= ==???
?=?||1||1 ,||1 ,)|(r)(r)|(r)(r)|(rCrdk rkdrdk jkdjij iiiicwccwcdc221M-step).
In EM, Pr(cj|di) takes the value in [0, 1]instead of {0, 1} in all the three equations.The algorithm for the second step of S-EM isgiven in Figure 2.
Lines 1-3 build a NB classifier fusing P and RN.
Lines 4-8 run EM until conver-gence.
Finally, the converged classifier is used toclassify the unlabeled set U (lines 10-13).3.2 Proposed CR-SVMAs we will see in the experiment section, the per-formance of S-EM can be weak in some cases.This is due to the mixture model assumption of itsNB classifier (Nigam et al 2000), which requiresthat the mixture components and classes be of one-to-one correspondence.
Intuitively, this means thateach class should come from a distinctive distribu-tion rather than a mixture of multiple distributions.In our setting, however, the negative class oftenhas documents of mixed topics, e.g., representingthe broad class of everything else except the top-ic(s) represented by the positive class.There are some existing PU learning methodsbased on SVM which can deal with this problem,e.g., Roc-SVM (Li and Liu, 2003).
Like S-EM,Roc-SVM also has two steps.
The first step usesRocchio classification (Rocchio, 1971) to find a setof reliable negatives RN from U.
In particular, thismethod treats the entire unlabeled set U as negativedocuments and then uses the positive set P and theunlabeled set U as the training data to build a Roc-chio classifier.
The classifier is subsequently ap-plied to classify the unlabeled set U. Thosedocuments that are classified as negative are thenconsidered as reliable negative examples RN.
Thesecond step of Roc-SVM runs SVM iteratively(instead of EM).
Unlike NB, SVM does not makeany distributional assumption.However, Roc-SVM does not do well due to theweakness of its first step in finding a good set ofreliable negatives RN.
This motivates us to proposea new SVM based method CR-SVM to detect abetter quality RN set.
The second step of CR-SVMis similar to that in Roc-SVM.Step 1: Extracting reliable negatives RN from Uusing Cosine and RocchioThe first step of the proposed CR-SVM algorithmfor finding a RN set consists of two sub-steps:Sub-step 1 (extracting the potential negative setPN using the cosine similarity): Given the positiveset P and the unlabeled set U, we extract a set ofpotential negatives PN from U by computing thesimilarities of the unlabeled documents in U andthe positive documents in P. The idea is that thosedocuments in U that are very dissimilar to the doc-uments in P are likely to be negative.1.
PN = ?;2.
Represent each document in P and U as vectors us-ing the TF-IDF representation;3.
For each dj ?
P do4.5.
;6.
For each dj ?
P  do7.
compute cos(pr, dj) using Equation (4);8.
Sort all the documents dj?P according to cos(pr, dj)in decreasing order;9. ?
= cos(pr, dp) where dp is ranked in the position of(1- l)*|P|;10.
For each di ?
U  do11.
If cos(pr, di)< ?
then12.
PN = PN ?
{di}Figure 3.
Extracting potential negatives PN from UThe detailed algorithm is given in Figure 3.Each document in P and U is first represented as avector d = (q1, q2, ?, qn) using the TF-IDF scheme(Salton 1986).
Each element qi (i=1, 2, ?, n) in drepresents a word feature wi.
A positive representa-tive vector (pr) is built by summing up the docu-ments in P and normalizing it (lines 3-5).
Lines 6-7compute the similarities of each document dj in Pwith pr using the cosine similarity, cos(pr, dj).Line 8 sorts the documents in P according totheir cos(pr, dj) values.
We want to filter away asmany as possible hidden positive documents fromU so that we can obtain a very pure negative set.Since the hidden positives in U should have thesame behaviors as the positives in P in terms oftheir similarities to pr, we set their minimum simi-larity as the threshold value ?
which is the mini-mum similarity before a document is considered asa potential negative document:PjjPj?= = ddpr  ),,(cosmin||1?
(4)In a noiseless scenario, using the minimum simi-larity is acceptable.
However, most real-life appli-cations contain outliers and noisy artifacts.
Usingthe absolute minimum similarity may be unrelia-ble; the similarity cos(pr, dj) of an outlier docu-2||1 ||||*||1 ?==Pj jjP ddpr2 ||||/ prprpr =222ment dj in P could be near 0 or smaller than most(or even all) negative documents.
It would there-fore be prudent to ignore a small percentage l ofthe documents in P most dissimilar to the repre-sentative positive (pr) and assume them as noise oroutliers.
Since we do not know the noise level ofthe data, to be safe, we use a noise level l = 5% asthe default.
The final classification result is notsensitive to l as long as it is not too big.
In line 9,we use the noise level l to decide on a suitable ?.Then, for each document di in U, if its cosine simi-larity cos(pr, di) < ?, we regard it as a potentialnegative and store it in PN (lines 10-12).Our experiment results showed that PN is stillnot sufficient or big enough for accurate PU learn-ing.
Thus, we need to do a bit more work to findthe final RN.Sub-step 2 (extracting the final reliable negativeset RN from U using Rocchio with PN): At thispoint, we have a positive set P and a potential neg-ative set PN where PN is a purer negative set thanU.
To extract the final reliable negatives, we em-ploy the Rocchio classification to build a classifierRC using P and PN (We do not use SVM here as itis very sensitive to the noise in PN).
Those docu-ments in U that are classified as negatives by RCwill then be regarded as reliable negatives, andstored in set RN.The algorithm for this sub-step is given in Fig-ure 4.
Following the Rocchio formula, a positiveand a negative prototype vectors p and n are built(lines 3 and 4), which are used to classify the doc-uments in U (lines 5-7).
?
and ?
are parameters foradjusting the relative impact of the positive andnegative examples.
In this work, we use ?
= 16 and?
= 4 as recommended in (Buckley et al 1994).Step 2:  Learning by running SVM iterativelyThis step is similar to that in Roc-SVM, buildingthe final classifier by running SVM iteratively withthe sets P, RN and the remaining unlabeled set Q(Q = U ?
RN).The algorithm is given in Figure 5.
We runSVM classifiers Si (line 3) iteratively to extractmore and more negative documents from Q. Theiteration stops when no more negative documentscan be extracted from Q (line 5).
There is, howev-er, a danger in running SVM iteratively, as SVM isquite sensitive to noise.
It is possible that duringsome iteration, SVM is misled by noisy data toextract many positive documents from Q and putthem in the negative set RN.
If this happens, thefinal SVM classifier will be inferior.
As such, weemploy a test to decide whether to keep the firstSVM classifier or the final one.
To do so, we usethe final SVM classifier obtained at convergence(called Slast, line 9) to classify the positive set P tosee if many positive documents in P are classifiedas negatives.
Roc-SVM chooses 5% as the thre-shold, so CR-SVM also uses this threshold.
If thereare 5% of positive documents (5%*|P|) in P thatare classified as negative, it indicates that SVM hasgone wrong and we should use the first SVM clas-sifier (S1).
In our experience, the first classifier isalways quite strong; good results can therefore beachieved even without catching the last (possiblybetter) classifier.The main difference between Roc-SVM andCR-SVM is that Roc-SVM does not produce PN.
Itsimply treats the unlabeled set U as negatives forextracting RN.
Since PN is clearly a purer negativeset than U, the use of PN by CR-SVM helps ex-tract a better quality reliable negative set RN whichsubsequently allows the final classifier of CR-SVM to give better results than Roc-SVM.Note that the methods (S-EM and CR-SVM) areall two-step algorithms in which the first step andthe second step are independent of each other.
Thealgorithm for the second step basically needs agood set of reliable negatives RN extracted from U.This means that one can pick any algorithm for thefirst step to work with any algorithm for the secondstep.
For example, we can also have CR-EM whichuses the algorithm (shown in Figures 3 and 4) ofthe first step of CR-SVM to combine with the al-gorithm of the second step of S-EM.
CR-EM ac-tually works quite well as it is also able to exploitthe more accurate reliable negative set RN ex-tracted using cosine and Rocchio.1.
RN = ?;2.
Represent each document in P, PN and U as vectorsusing the TF-IDF representation;3. ;4. ;5.
For each di ?
U  do6.
If  cos(di, n)> cos(di, p) then7.
RN  = RN ?
{di}Figure 4.
Identifying RN using the Rocchio classifier????
?=PN iiP jjijPNP dd ddddp||||||1||||||1 ??????
?=P jjPN iijiPPN dd ddddn||||||1||||||1 ?
?2234 Empirical EvaluationWe now present the experimental results to supportour claim that negative training data is not neededand can even harm text classification.
We alsoshow the effectiveness of the proposed PU learningmethods CR-SVM and CR-EM.
The followingmethods are compared: (1) traditional supervisedlearning methods SVM and NB which use bothpositive and negative training data; (2) PU learningmethods, including two existing methods S-EMand Roc-SVM and two new methods CR-SVM andCR-EM, and (3) one-class SVM (Sch?lkop et al,1999) where only positive training data is used inlearning (the unlabeled set is not used at all).We used LIBSVM 1  for SVM and one-classSVM, and two publicly available 2  PU learningtechniques S-EM and Roc-SVM.
Note that we donot compare with some other PU learning methodssuch as those in (Liu et al 2003, Lee and Liu, 2003and Elkan and Noto, 2008) as the purpose of thispaper is not to find the best PU learning methodbut to show that PU learning can address our spe-cial sample selection bias problem.
Our currentmethods already do very well for this purpose.4.1 Datasets and Experimental SettingsWe used two well-known benchmark data collec-tions for text classification, the Reuters-21578 col-lection 3  and the 20 Newsgroup collection 4 .Reuters-21578 contains 21578 documents.
Weused the most populous 10 out of the 135 catego-ries following the common practice of other re-searchers.
20 Newsgroup has 11997 documentsfrom 20 discussion groups.
The 20 groups werealso categorized into 4 main categories.We have performed two sets of experiments,and just used bag-of-words as features since ourobjective in this paper is not feature engineering.
(1) Test set has other topic documents.
This setof experiments simulates the scenario in which thenegative training and test samples have differentdistributions.
We select positive, negative and oth-er topic documents for Reuters and 20 Newsgroup,and produce various data sets.
Using these datasets, we want to show that PU learning can do bet-1 http://www.csie.ntu.edu.tw/~cjlin/libsvm/2 http://www.cs.uic.edu/~liub/LPU/LPU-download.html3 http://www.research.att.com/~lewis/reuters21578.html4 http://people.csail.mit.edu/jrennie/20Newsgroups/ter than traditional learning that uses both positiveand negative training data.For the Reuters collection, each of the 10 cate-gories is used as a positive class.
We randomlyselect one or two of the remaining categories as thenegative class (denoted by Neg 1 or Neg 2), andthen we randomly choose some documents fromthe rest of the categories as other topic documents.These other topic documents are regarded as nega-tives and added to the test set but not to the nega-tive training data.
They thus introduce a differentdistribution to the negative test data.
We generated20 data sets (10*2) for our experiments this way.The 20 Newsgroup collection has 4 main cate-gories with sub-categories5; the sub-categories inthe same main category are relatively similar toeach other.
We are able to simulate two scenarios:(1) the other topic documents are similar to thenegative class documents (similar case), and (2)the other topic documents are quite different fromthe negative class documents (different case).
Thisallows us to investigate whether the classificationresults will be affected when the other topic docu-ments are somewhat similar or vastly differentfrom the negative training set.
To create the train-ing and test data for our experiments, we randomlyselect one sub-category from a main category (cat1) as the positive class, and one (or two) sub-category from another category (cat 2) as the nega-tive class (again denoted by Neg 1 or Neg 2).
Forthe other topics, we randomly choose some docu-5  The four main categories and their corresponding sub-categories are: computer (graphics, os, ibmpc.hardware,mac.hardware, windows.x), recreation (autos, motorcycles,baseball, hockey), science (crypt, electronics, med, space), andtalk (politics.misc, politics.guns, politics.mideast, religion).1.
Every document in P is assigned the class label +1;2.
Every document in RN is assigned the label ?1;3.
Use P and RN  to train a SVM classifier Si, with i =1 initially and i = i+1 with each iteration (line 3-7);4.
Classify Q using Si.
Let the set of documents in Qthat are classified as negative be W;5.
If (W = ?)
then  stop;6. else Q = Q ?
W;7.
RN = RN ?W8.
goto (3);9.
Use the last SVM classifier Slast to classify P;10.
If more than 5% positives are classified as negative11.
then use S1 as the final classifier;12. else use Slast as the final classifier;Figure 5.
Constructing the final classifier using SVM224ments from the remaining sub-categories of cat 2for the similar case, and some documents from arandomly chosen different category (cat 3) (as theother topic documents) for the different case.
Wegenerated 8 data sets (4*2) for the similar case,and 8 data sets (4*2) for the different case.The training and test sets are then constructed asfollows: we partition the positive (and similarly forthe negative) class documents into two standardsubsets: 70% for training and 30% for testing.
Inorder to create different experimental settings, wevary the number of the other topic documents thatare added to the test set as negatives, controlled bya parameter ?, which is a percentage of |TN|, where|TN| is the size of the negative test set without theother topic documents.
That is, the number of oth-er topic documents added is ?
?
|TN|.
(2) Test set has no other topic documents.
Thisset of experiments is the traditional classificationin which the training and test data have the samedistribution.
We employ the same data sets as in(1) but without having any other topic documentsin the test set.
Here we want to show that PU learn-ing can do equally well without using the negativetraining data even in the traditional setting.4.2 Results with Other Topic Documents inTest SetWe show the results for experiment set (1), i.e.
thedistributions of the negative training and test dataare different (caused by the inclusion of other topicdocuments in the test set, or the addition of othertopic documents to complement existing negativesin the test set).
The evaluation metric is the F-scoreon the positive class (Bollmann and Cherniavsky,1981), which is commonly used for evaluating textclassification.4.2.1  Results on the Reuters dataFigure 6 shows the comparison results when thenegative class contains only one category of doc-uments (Neg 1), while Figure 7 shows the resultswhen the negative class contains documents fromtwo categories (Neg 2) in the Reuters collection.The data points in the figures are the averages ofthe results from the corresponding datasets.Our proposed method CR-SVM is shown to per-form consistently better than the other techniques.When the size of the other topic documents (x-axis) in the test set increases, the F-scores of thetwo traditional learning methods SVM and NBdecreased much more dramatically as comparedwith the PU learning techniques.
The traditionallearning models were clearly unable to handle dif-ferent distributions for training and test data.Among the PU learning techniques, the proposedCR-SVM gave the best results consistently.
Roc-SVM did not do consistently well as it did notmanage to find high quality reliable negatives RNsometimes.
The EM based methods (CR-EM andS-EM) performed well in the case when we hadonly one negative class (Figure 6).
However, it didnot do well in the situation where there were twonegative classes (Figure 7) due to the underlyingmixture model assumption of the na?ve Bayesianclassifier.
One-class SVM (OSVM) performedpoorly because it did not exploit the useful infor-mation in the unlabeled set at all.Figure 6.
Results of Neg 1 using the Reuter dataFigure 7.
Results of Neg 2 using the Reuter data4.2.2  Results on 20 Newsgroup dataRecall that for the 20 Newsgroup data, we havetwo settings: similar case and different case.Similar case: Here, the other topic documents are0.60.70.80.9110% 20% 30% 40% 50% 60% 70% 80% 90% 100%a%*|TN | of other topic documentsF-scoreSVM NB OSVM S-EMRoc-SVM CR-EM CR-SVM0.50.60.70.80.9110% 20% 30% 40% 50% 60% 70% 80% 90% 100%a%*|TN | of other topic documentsF-scoreSVM NB OSVM S-EMRoc-SVM CR-EM CR-SVM225similar to the negative class documents, as theybelong to the same main category.The comparison results are given in Figure 8(Neg 1) and Figure 9 (Neg 2).
We observe thatCR-EM, S-EM and CR-SVM all performed well.EM based methods (CR-EM and S-EM) have aslight edge over CR-SVM.
Again, the F-scores ofthe traditional supervised learning (SVM and NB)deteriorated when more other topic documentswere added to the test set, while CR-EM, S-EMand CR-SVM were able to remain unaffected andmaintained roughly constant F-scores.
When thenegative class contained documents from two cate-gories (Neg 2), the F-scores of the traditionallearning dropped even more rapidly.
Both Roc-SVM and One-class SVM (OSVM) performedpoorly, due to the same reasons given previously.Figure 8.
Results of Neg 1, similar case ?
using the 20Newsgroup dataFigure 9.
Results of Neg 2, similar case ?
using the 20Newsgroup dataDifferent case: In this case, the other topic docu-ments are quite different from the negative classdocuments, since they are originated from differentmain categories.The results are shown in Figures 10 (Neg 1) and11 (Neg 2).
The trends are similar to those for thesimilar case, except that the performance of thetraditional supervised learning methods (SVM andNB) dropped even more rapidly with more othertopic documents.
As the other topic documentshave very different distributions from the negativesin the training set in this case, they really confusedthe traditional classifiers.
In contrast, the three PUlearning techniques were still able to perform con-sistently well, regardless of the number of othertopic documents added to the test data.Figure 10.
Results of Neg 1, different case ?
using the20 Newsgroup dataFigure 11.
Results of Neg 2, different case ?
using the20 Newsgroup dataIn summary, the results showed that learningwith negative training data based on the traditionalparadigm actually harms classification when theidentical distribution assumption does not hold.4.3 Results without Other Topic Documents inTest SetGiven an application, one may not know whether0.60.70.80.9110% 20% 30% 40% 50% 60% 70% 80% 90% 100%a%*|TN | of other topic documentsF-scoreSVM NB OSVM S-EMRoc-SVM CR-EM CR-SVM0.60.70.80.9110% 20% 30% 40% 50% 60% 70% 80% 90% 100%a%*|TN | of other topic documentsF-scoreSVM NB OSVM S-EMRoc-SVM CR-EM CR-SVM0.60.70.80.9110% 20% 30% 40% 50% 60% 70% 80% 90% 100%a%*|TN| of other topic documentsF-scoreSVM NB OSVM S-EMRoc-SVM CR-EM CR-SVM0.60.70.80.9110% 20% 30% 40% 50% 60% 70% 80% 90% 100%a%*|TN| of other topic documentsF-scoreSVM NB OSVM S-EMRoc-SVM CR-EM CR-SVM226the identical distribution assumption holds.
Theabove results showed that PU learning is betterwhen it does not hold.
How about when the as-sumption does hold?
To find out, we compared theresults of SVM, NB, and three PU learning me-thods using the datasets without any other topicdocuments added to the test set.
In this case, thetraining and test data distributions are the same.Table 1 shows the results for this scenario.
Notethat for PU learning, the negative training datawere not used.
The traditional supervised learningtechniques (SVM and NB), which made full use ofthe positive and negative training data, only per-formed just about 1-2% better than the PU learningmethod CR-SVM (which is not statistically signifi-cant based on paired t-test).
This suggests that wecan do away with negative training data, since PUlearning can perform equally well without them.This has practical importance since the full cover-age of negative training data is hard to find and tolabel in many applications.From the results in Figures 6?11 and Table 1,we can conclude that PU learning can be used forbinary text classification without the negativetraining data (which can be harmful for the task).CR-SVM is our recommended PU learning methodbased on its generally consistent performance.Table 1.
Comparison of methods without other docu-ments in test setMethodsReuters(Neg 1)Reuters(Neg 2)20News(Neg 1)20News(Neg 2)SVM 0.971 0.964 0.988 0.990NB 0.972 0.947 0.988 0.992S-EM 0.952 0.921 0.974 0.975CR-EM 0.955 0.897 0.983 0.986CR-SVM 0.960 0.959 0.967 0.9745 ConclusionsThis paper studied a special case of the sample se-lection bias problem in which the positive trainingand test distributions are the same, but the negativetraining and test distributions may be different.
Weshowed that in this case, the negative training datashould not be used in learning, and PU learningcan be applied to this setting.
A new PU learningalgorithm (called CR-SVM) was also proposed toovercome the weaknesses of the current two-stepalgorithms.Our experiments showed that the traditionalclassification methods suffered greatly when thedistributions are different for the negative trainingand test data, but PU learning does not.
We alsoshowed that PU learning performed equally well inthe ideal case where the training and test data haveidentical distributions.
As such, it can be advanta-geous to discard the potentially harmful negativetraining data and use PU learning for classification.In our future work, we plan to do more compre-hensive experiments to compare the classic super-vised learning and PU learning techniques withdifferent kinds of settings, for example, by varyingthe ratio between positive and negative examples,as well as their sizes.
It is also important to explorehow to catch the best iteration of the SVM/NBclassifier in the iterative running process of thealgorithms.
Finally, we would like to point out thatit is conceivable that negative training data couldstill be useful in many cases.
An interesting direc-tion to explore is to somehow combine the ex-tracted reliable negative data from the unlabeledset and the existing negative training data to furtherenhance learning algorithms.ReferencesAgirre E., Lacalle L.O.
2009.
Supervised Domain Adap-tion for WSD.
Proceedings of the 12th Conference ofthe European Chapter for Computational Linguistics(EACL09), pp 42-50.Andrew A., Nallapati R., Cohen W., 2008.
ExploitingFeature Hierarchy for Transfer Learning in NamedEntity Recognition, ACL.Bickel, S., Bruckner, M., and Scheffer.
2009.
T. Dis-criminative learning under covariate shift.
Journal ofMachine Learning Research.Bickel S. and Scheffer T. 2007.
Dirichlet-enhancedspam filtering based on biased samples.
In Advancesin Neural Information Processing Systems.Bollmann, P.,& Cherniavsky, V. 1981.
Measurement-theoretical investigation of the mz-metric.
Informa-tion Retrieval Research.Buckley, C., Salton, G., & Allan, J.
1994.
The effect ofadding relevance information in a relevance feed-back environment, SIGIR.Blum, A. and Mitchell, T. 1998.
Combining labeled andunlabeled data with co-training.
In Proc.
of Compu-tational Learning Theory, pp.
92?10.Chan Y. S., Ng H. T. 2007.
Domain Adaptation withActive Learning for Word Sense Disambiguation,ACL.Dempster A., Laird N. and Rubin D.. 1977.
Maximumlikelihood from incomplete data via the EM algorithm,Journal of the Royal Statistical Society.Denis F., PAC learning from positive statistical queries.ALT, 1998.227Denis F., Laurent A., R?mi G., Marc T. 2003.
Text clas-sification and co-training from positive and unlabeledexamples.
ICML.Denis, F, R?mi G, and Marc T. 2002.
Text Classifica-tion from Positive and Unlabeled Examples.
In Pro-ceedings of the 9th International Conference onInformation Processing and Management of Uncer-tainty in Knowledge-Based Systems.Downey, D., Broadhead, M. and Etzioni, O.
2007.
Lo-cating complex named entities in Web Text.
IJCAI.Dudik M., Schapire R., and Phillips S. 2005.
Correctingsample selection bias in maximum entropy densityestimation.
In Advances in Neural Information Proc-essing Systems.Elkan, C. and Noto, K. 2008.
Learning classifiers fromonly positive and unlabeled data.
KDD, 213-220.Goldwasser, D., Roth D. 2008.
Active Sample Selectionfor Named Entity Transliteration, ACL.Heckman J.
1979.
Sample selection bias as a specifica-tion error.
Econometrica, 47:153?161.Huang J., Smola A., Gretton A., Borgwardt K., andScholkopf B.
2007.
Correcting sample selection biasby unlabeled data.
In Advances in Neural Informa-tion Processing Systems.Jiang J. and Zhai C. X.
2007.
Instance Weighting forDomain Adaptation in NLP, ACL.Lee, W. S. and Liu, B.
2003.
Learning with Positive andUnlabeled Examples Using Weighted Logistic Re-gression.
ICML.Lewis D. 1995.
A sequential algorithm for training textclassifiers: corrigendum and additional data.
SIGIRForum, 13-19.Li, S., Zong C., 2008.
Multi-Domain Sentiment Classifi-cation, ACL.Li, X., Liu, B.
2003.
Learning to classify texts usingpositive and unlabeled data, IJCAI.Li, X., Liu, B., 2005.
Learning from Positive and Unla-beled Examples with Different Data Distributions.ECML.Li, X., Liu, B., 2007.
Learning to Identify UnexpectedInstances in the Test Set.
IJCAI.Li, X., Yu, P. S., Liu B., and Ng, S. 2009.
PositiveUnlabeled Learning for Data Stream Classification,SDM.Li, X., Zhang L., Liu B., and Ng, S. 2010.
Distribution-al Similarity vs. PU Learning for Entity Set Expan-sion, ACL.Liu, B, Dai, Y., Li, X., Lee, W-S., and Yu.
P. 2003.Building text classifiers using positive and unlabeledexamples.
ICDM, 179-188.Liu, B, Lee, W-S, Yu, P. S, and Li, X.
2002.
Partiallysupervised text classification.
ICML, 387-394.Nigam, K., McCallum, A., Thrun, S. and Mitchell, T.2000.
Text classification from labeled and unlabeleddocuments using EM.
Machine Learning, 39(2/3),103?134.Pan, S. J. and Yang, Q.
2009.
A survey on transferlearning.
IEEE Transactions on Knowledge and Da-ta Engineering, Vol.
99, No.
1.Rocchio, J.
1971.
Relevant feedback in informationretrieval.
In G. Salton (ed.).
The smart retrieval sys-tem: experiments in automatic document processing,Englewood Cliffs, NJ, 1971.Sagae K., Tsujii J.
2008.Online Methods for Multi-Domain Learning andAdaptation, EMNLP.Salton G. and McGill M. J.
1986.
Introduction to Mod-ern Information Retrieval.Sch?lkop f B., Platt J.C., Shawe-Taylor J., Smola A.J.,and Williamson R.C.
1999.
Estimating the supportof a high-dimensional distribution.
Technical report,Microsoft Research, MSR-TR-99-87.Shimodaira H. 2000.
Improving predictive inferenceunder covariate shift by weighting the log-likelihoodfunction.
Journal of Statistical Planning and Infer-ence, 90:227?244.Sugiyama M. and Muller K.-R. 2005.
Input-dependentestimation of generalization error under covariateshift.
Statistics and Decision, 23(4):249?279.Sugiyama M., Nakajima S., Kashima H., von Bunau P.,and Kawanabe M. 2008.
Direct importance estima-tion with model selection and its application to co-variate shift adaptation.
In Advances in NeuralInformation Processing Systems.Tsuboi J., Kashima H., Hido S., Bickel S., and Sugi-yama M. 2008.
Direct density ratio estimation forlarge-scale covariate shift adaptation.
In Proceed-ings of the SIAM International Conference on DataMining, 2008.Wu D., Lee W.S., Ye N. and Chieu H. L. 2009.
Domainadaptive bootstrapping for named entity recognition,ACL.Wu Q., Tan S. and Cheng X.
2009.
Graph Ranking forSentiment Transfer, ACL.Yang Q., Chen Y., Xue G., Dai W., Yu Y.
2009.
Hete-rogeneous Transfer Learning for Image Clusteringvia the SocialWeb, ACLYu, H., Han, J., K. Chang.
2002.
PEBL: Positive exam-ple based learning for Web page classification usingSVM.
KDD, 239-248.Zadrozny B.
2004.
Learning and evaluating classifiersunder s ample selection bias, ICML.Zhou Z., Gao J., Soong F., Meng H. 2006.
A Compara-tive Study of Discriminative Methods for RerankingLVCSR N-best Hypotheses in Domain Adaptationand Generalization.
ICASSP.228
