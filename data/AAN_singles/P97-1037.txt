A DP based Search Using MonotoneAl ignments in Statistical TranslationC.
Tillmann, S. Vogel, H. Ney, A. Zub iagaLehrstuh l  f/Jr In forma,t ik  VI ,  RWTH AachenD-52056 Aachen,  Germany{t illmann, ney}?informatik, rwth-aachen, deAbst rac tIn this paper, we describe a Dynamic Pro-gramming (DP) based search algorithmfor statistical translation and present ex-perimental results.
The statistical trans-lation uses two sources of information: atranslation model and a language mod-el.
The language model used is a stan-dard bigram model.
For the transla-tion lnodel, the alignment probabilities aremade dependent on the differences in thealignment positions rather than on theabsolute positions.
Thus, the approachamounts to a first-order Hidden Markovmodel (HMM) as they are used successful-ly in speech recognition for the time align-ment problem.
Under the assumption thatthe alignment is monotone with respect othe word order in both languages, an ef-ficient search strategy for translation canbe formulated.
The details of the searchalgorithm are described.
Experiments onthe EuTrans corpus produced a word errorrate of 5.1(/~..1 Overv iew:  The  Sta t i s t i ca lApproach  to  T rans la t ionThe goal is the translation of a text given in somesource language into a target language.
We are giveno J a source ('French') string fl = fl...fj...f.l, whichis to be translated into a target ('English') stringc~ = el...ei...el.
Among all possible target strings,we will choose the one with the highest probabilitywhich is given by Bayes' decision rule (Brown et  al..1993):,~ = argmax{P,'(e\]~lfg~)}= argmax {P,'(ef).
Pr(.f/lef)}Pr(e{) is the language model of the target language.whereas Pr(j'lale{) is the string translation model.The argmax operation denotes the search problem.In this paper, we address?
the problem of introducing structures into theprobabilistic dependencies in order to modelthe string translation probability Pr( f \ ]  \[e~).?
the search procedure, i.e.
an algorithm to per-form the argmax operation in an efficient way.?
transformation steps for both the source andthe target languages in order to improve thetranslation process.The transformations are very much dependent onthe language pair and the specific translation taskand are therefore discussed in the context of the taskdescription.
We have to keep in mind that in thesearch procedure both the language and the transla-tion model are applied after the text transformationsteps.
However, to keep the notation simple we willnot make this explicit distinction in the subsequentexposition.
The overall architecture of the statisticaltranslation approach is summarized in Figure 1.2 A l igmnent  Mode lsA key issue in modeling the string translation prob-ability Pr(f( le I) is the question of how we definethe correspondence b tween the words of the targetsentence and the words of the source sentence.
Intypical cases, we can assume a sort of pairwise de-pendence by considering all word pairs (fj,ei) fora given sentence pair \[f(; el\].
We further constrainthis model by assigning each source word to exact-ly one target word.
Models describing these typesof dependencies are referred to as alignrnen.t models(Brown et  al., 1993), (Dagan eta\].
.
1993).
(Kay &R6scheisen, 1993).
(Fung & Church.
1994), (Vogelet al, 1996).In this section, we introduce a monotoue HMMbased alignment and an associated DP based searchalgorithm for translation.
Another approach to sta-tistical machine translation using DP was presentedin (Wu, 1996).
The notational convention will be a,sfollows.
We use the symbol Pr(.)
to denote general289Source Language Text1I Transformation 1?~Global Search: j~  Lexicon Modelmaximize Pr(el).
pr(f~lell} I I AllgnmentModelovor j. pc(e~) \[ Language Mode l ,\[;....,!...,,on\]1Target Language TextFigure I: Architecture of the translation approachbased on Bayes decision rule.probability distributions with (nearly) no specific as-snmptions.
In contrast, for model-based probabilitydistributions, we use the generic symbol p(.
).2.1 A l ignment  wi th  HMMWhen aligning the words in parallel texts (forIndo-European language pairs like Spanish-English,German-English, halian-German ....), we typicallyobserve a strong localization effect..
Figure 2 illus-trates this effect, for the language pair Spanish-to-English.
In many cases, although not always, thereis an even stronger estriction: the difference in theposition index is smaller than 3 and the alignment.is essentially monotone.
To be more precise, thesentences can be partitioned into a small numberof segments, within each of which the alignment ismonotone with respect to word order in both lan-gaages.To describe these word-by-word alignments, weintroduce the mapping j - -  o j, which assigns a po-sition j (with source word .fj ) to the position i = aj(with target word ei).
The concept of these align-ments is similar to the ones introduced by (Brownet al, 1993), but we will use another type of de-pendence in the probability distributions.
Lookingat.
such alignments produced by a human expert, it,is evident that the mathematical model should tryto capture the strong dependence of aj on the pre-ceding alignment aj-1.
Therefore the probability ofalignment aj for position j should have a dependenceon the previous alignment position O j _ l :P((/j \[(/j-1 )A similar approach has been chosen by (Dagan etal., 1993) and (Vogel et al.
1996).
Thus the problemformulation is similar t.o that of/,he time alignmentproblem in speech recognition, where the so-calledHidden Markov models have been successfully usedfor a long time (Jelinek.
1976).
Using the same basicprinciples, we can rewrite the probability by intro-ducing the 'hidden" aligmnents a~ := a l...aj...aa fora sentence pair \[f~; c/\]:P,,(s 'lcI  =J~i' j=1To avoid any confnsion with the term 'hidden'incomparison with speech recognition, we observe thatthe model states as such (representing words) are nothidden but the actual alignments, i.e.
the sequenceof position index pairs (j. i = aj ).So far there has been no basic restriction of theapproach.
We now assume a first-order dependenceon the alignments aj only:Pr ( f j ,a j l f~- l ,a{ -1 .e{)  = p( f j , ( / j la j - l ,e{)= p(a j la j _ l ) .p ( f j lea , ) ,where, in addition, we have assumed that the lexiconprobability p(fle) depends only on aj and not.
ona j  _ 1 ?To reduce the number of alignment parameters,we assume that the HMM alignment probabilitiesp(i\[i') depend only on the jump width (i - i').
Themonotony condition can than be formulated as:p(i \ [ i ' )=O for i ?
i '+O.
i '+ l , i '+2 .This monotony requirement limits the applicabili-ty of our approach.
However, by performing simpleword reorderings, it.
is possible to approach this re-quirement (see Section 4.2).
Additional countermea-sures will be discussed later.
Figure 3 gives an illus-tration of the possible alignments for the monotonehidden Markov model.
To draw the analogy withspeech recognition, we have to identify the states(along the vertical axis) with the positions i of thetarget words ei and the time (along the horizont.alaxis) with the positions j of the source words J).2.2  T ra in ingTo train the alignment and the lexicon model, weuse the maximum likelihood criterion in the so-calledmaximum approximation, i.e.
the likelihood criteri-on covers only the most likely alignment rather thanthe set of all alignments:JPr(.f( leI)  = ~ 1-i \[P(aJlaJ-l'.
I)" P(fJle?.i )\]"i' j=iJ-'= max1- ~\[p(ajla.o_~, I).
p(.l)leo,)\]jal j= l290days  otwo  ofo r  oroom odoub le  oa oi s  omuchhow IoI .
.
.
.
L___L___L___L  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.c v u h d p d dU a n a o a o '' I a b b r s ia e i i a an t e st ao ci0nroomJ ,  othe  J. oin  Joco ld \ [ ,  otoo  I .
oi s  I.i t  J. oJ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.e I h h d fn a a a e rb c m 'i e a it s oa iC ai d00nn ightafo rtvaandsa feate lephone la Jwith  Jroom Ja Ibooked Ihavewe000000 00oIIoI .
.
.
.
- - - - ' - - - - - - - - -  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.t r u h c t c f y t p u ne e n a o e a un s a b n 1 j ee e i ' a rm r t e to v a f es a c od i na ) o0ne a n o1 r a ce a hv eiSi0nFigure 2: Word aligmnents for Spanish-English sentence pairs.291o*"Zr.~ ?L5 iv,<F~I I I I \[ I1 2 3 4 5 6SOURCE POSITIONFigure 3: Illustrat ion of alignments for the n lonotoneHMM.To find the optimal alignment, we use dynamicprogramming for which we have the following typicalrecursion formula:Q(i, j )  = p( f j  \]ei)max \[p(ili') .
Q(i ' ,  j - 1)1i 'Here.
Q(i. j )  is a sort of partial probability as in t.imealignment for speech recognit.ion (aelinek, 1976).
Asa result, the training procedure amounts to a se-quence of iterat.ions, each of which consists of twosteps:?
posilion alignm~TH: Given the model parame-t.ers, det.ermine the most likely position align-n-lent.?
parame*e-r eslimalion: Given the position align-ment.
i.e.
going along the alignment paths forall sentence pairs, perform maximum likelihoodestimation of the model parameters; for model-free distributions, these estimates result in rel-a.tive fi'equencies.The IBM model 1 (Brown et al, 1993) is used to findan initial estimate of the translation probabilities.3 Search  Algor i thm for  T rans la t ionFor the translation operat.ion, we use a bigram lan-guage model, which is given in terms of the con-dit.ional probability of observing word ei given thepredecessor word e.i- 1:p(~ilei-:)Using the conditional probability of the bigram lan-guage model, we have the overall search criterion inthe maxinmm approximation:max p(ei le;_:) lnax l ' I  \[p(aj la~-:)P(f J lea,)\]  ",,' t i=:  ~i ~=:Here and in the following, we omit a special treat-ment of the start and end conditions like j = 1 orj = J in order to simplify the presentation and avoidconfusing details.
Having the above criterion inmind, we try t.o associate the language model prob-abilities with the aligmnents j ~ i - aj.
To thispurpose, we exploit the monotony property of ouralignment model which allows only transitions froma j - i  tO aj if the difference 6 = o j -a j -1  is 0,1,2.We define a modified probability p~(el#) for the lan-guage model depending on the alignment differencet~.
We consider each of the three cases 5 = 0, 1,2separately:?
~ = 0 (horizontal transition = alignment repe-tition): This case corresponds to a target wordwith two or more aligned source words andtherefore requires ~ = # so that there is nocontribution fl'om the language model:1 for e=e 'P~=?
(ele') = 0 for e ee '?
6 = 1 (forward transition = regular alignment.
):This case is the regular one, and we can usedirectly the probability of the bigram languagemodel:p~=:(ele')  = p(ele')?
~ = 2 (skip transition = non-aligned word):This case corresponds to skipping a word.
i.e,there is a word in the target string with noaligned word in the source string.
We have tofind the highest probability of placing a non-aligned word e_- between a predecessor word e'and a successor word e. Thus we optimize thefollowing product, over the non-aligned word g:p~=~(eJe') = maxb~(elg).p(gIe')\] iThis maximization is done beforehand and theresult is stored in a table.Using this modified probability p~(ele'), we canrewrite the overall search criterion:aTl-I )\].The problem now is to find the unknown mapping:j - -  (aj, ca.,)which defines a path through a network with a uni-form trellis structure.
For this trellis, we can stilluse Figure 3.
However.
in each position i along the292Table h DP based search algorithm for the monotone translation model.
!nput: source str ing/ l .
.
.
f j .
.
.
f Jinitializationfor each position j = 1,2 ..... d in source sel'ltence dofor each position i = 1,2, ...,/maz in target sentence dofor each target word e doV Q(i, j, e) = p(f j  le)' ma;x{p(i\[i - 6).
p~(e\[e').
Q( i  - 6. j - 1, e')}6,etraceback:- find best end hypothesis: max Q(i, J, e)- recover optimal word sequencevertical axis.
we have to allow all possible words eof the target vocabulary.
Due to the monotony ofour alignnaent model and the bigraln language mod-el.
we have only first-order type dependencies suchthat the local probabilities (or costs when using thenegative logarithms of the probabilities) depend on-I.q on the arcs (or transitions) in the lattice.
Eachpossible index triple ( i .
j .e)  defines a grid point inthe lattice, and we have the following set of possi-ble transitions fi'om one grid point to another gridpoint :~fi {0.1.2} : ( i -6 .
j - l .
e ' ) - - ( i , j , e )Each of these transitions is assigned a local proba-bility:p(ili - 6).
p,,(ele') .
p(f j  le)Using this formulation of the search task, we cannow use the method of dynamic programming(DP)to find the best path through the lattice.
To thispurpose, we introduce the auxiliary quantity:Q( i .
j .e ) :  probability of the best.
partial pathwhich ends in the grid point (i, j, e).Since we have only first-order dependencies in ourmodel, it is easy to see that the auxiliary quantitynmst satisfy the following DP recursion equation:Q( i .
j .e )  = p(f j le) .max {p( i l i -  ~).
maxp,,(ele').
Q( i -  6, j - 1,e')}.To explicitly construct he unknown word sequence~.
it is convenient o make use of so-called back-pointers which store for each grid point ( i .
j ,e )  thebest predecessor grid point (Ney et al.
1992).The DP equation is evaluated recursively to findthe best partial path to each grid point (i, j, e).
Theresuhing algorithm is depicted in Table 1.
The com-plexity of the algorithm is J .
I,,,.,.
?
E'-'.
where E isthe size of t.he target language vocabulary and I,,,,~.is the n~aximum leng{'h of the target sentence con-sidered.
It is possible to reduce this COml)utationalcomplexity by using so-called pruning methods (Neyet al.
1992): due to space limitatiol~s, they are notdiscussed here.4 Exper imenta l  Resu l t s4.1 The  Task and the CorpusThe search algorithln proposed in this paper wastested on a subtask of the "'Traveler Task" (Vidal,1997).
The general domain of the task comprisestypical situations a visitor to a foreign country isfaced with.
The chosen subtask corresponds to a sce-nario of the hulnan-to-human communication situ-ations at the registration desk in a hotel (see Table4).The corpus was generated in a semi-automaticway.
On the basis of examples from traveller book-lets, a prol)abilistic gralmnar for different languagepairs has been constructed from which a large cor-pus of sentence pairs was generated.
The vocabularyconsisted of 692 Spanish and 518 English words (in-eluding punctuatioll marks).
For the experiments, atrailfing corpus of 80,000 sentence pairs with 628,117Spanish and 684.777 English words was used.
In ad-dition, a test corpus with 2.730 sentence pairs differ-ent froln the training sentence pairs was construct-ed.
This test corpus contained 28.642 Spanish a.nd24.927 English words.
For the English sentences,we used a bigram language model whose perplexityon the test corpus varied between 4.7 for the orig-inal text.
and 3.5 when all transformation steps asdescribed below had been applied.Table 2: Effect of the transformation steps on thevocabulary sizes in both languages.Transformation Step Spanish EnglishOriginal (with punctuation) 692 518+ C.ategorization 416 227+ 'por_~avor' 417+ V~'ol'd Splkt.ing 374+ Word Joining 237+ 'Word Reordering2934.2  Text  Tl-ansformationsThe purpose of the text transformations is to makethe two languages resenable ach other as closely aspossible with respect, to sentence l ngth and word or-der.
In addition, the size of both vocabularies i re-duced by exploiting evident regularities; e.g.
propernames and numbers are replaced by category mark-ers.
We used different, preprocessing steps whichwere applied consecutively:?
Or ig inal  Corpus:  Punctuation marks aretreated like regular words.?
Categor izat ion:  Some particular words orword groups are replaced by word categories.Seven non-overlapping categories are used:three categories for names (surnames, name andfemale names), two categories for numbers (reg-ular numbers and room numbers) and two cat-egories for date and time of day.?
'D_'eatment of 'pot  : favor' :  The word 'pot:favor' is always moved to the end of thesentence and replaced by the one-word token' pot_ favor  ' .?
Word  Spl i t t ing:  In Spanish, the personalpronouns (in subject case and in object, case)can be part of the inflected verb form.
To coun-teract this phenomenon, we split the verb intoa verb part and pronoun part, such as 'darnos"- -  "dar _nos' and "pienso" - -  '_yo pienso'.?
Word  Jo in ing:  Phrases in the English lan-guage such as "Would yogi mind doing .
.
. '
and'1 would like you to do ..." are difficult to han-dle by our alignment model.
Therefore, weapply some word joining, such as 'would yo~tmi71d" - -  'wo~dd_yo',_mind" and ~would like ' - -"wotdd_like '.?
Word  Reorder ing:  This step is applied tothe Spanish text to take into account, cases likethe position of the adjective in noun-adjectivephrases and the position of object, pronouns.E.g.
"habitacidT~ dobh ' - -  'doble habitaci6~'.By this reordering, our assumption about themonotony of the alignment model is more oftensatisfied.The effect of these transformation steps on the sizesof both vocabularies i shown in Table 2.
In addi-tion to all preprocessing steps, we removed the punc-t.uation marks before translation and resubstitutedt.hena by rule into the target sentence.4.3  Trans lat ion ResultsFor each of the transformation steps describedabove, all probability models were trained anew, i.e,the lexicon probabilities p(f le) ,  the alignment prob-abilities p(i l i  - 6) and the bigram language proba-bilities p(ele').
To produce the translated sentencein normal anguage, the transformation steps in thetarget language were inverted.The translation results are summarized in Table3.
As an aut.omatic and easy-to-use measure of thetranslation errors, the Levenshtein distance betweenthe automatic translation and the reference transla-tion was calculated.
Errors are reported at the wordlevel and at.
the sentence level:?
word leveh insertions (INS).
deletions (DEL),and total lmmber of word errors (\VER).?
sentence level: a sentence is counted as correctonly if it is identical to the reference sentence.Admittedly, this is not a perfect measure.
In par-ticular, the effect of word ordering is not taken intoaccount appropriately.
Actually, the figures for sen-tence error rate are overly pessimistic.
Many sen-tences are acceptable and semantically correct rans-lations (see the example translations in Table 4),Table 3: Word error rates (INS/DEL, WER) andsentence rror rates (SER) for different ransforma-tion steps.Transformation StepOriginal CorPora+ Categorization+ ' por2 favor  '+ Word SplittingTranslation Errors \[~.\]423/11.2 21.2 85.52.5/?.6 16.1 81.02.6/8.3 14.3 75.62.5/7.4 12.3 65.4i.3/4.9 44.6 + Word Joining 7.3+ Word Reordering 0.9/3.4 5.1 30.1As can be seen in Table 3. the translation er-rors can be reduced systen~at.ically by applying alltransformation steps.
The word error rate is re-duced from 21.2{,} t.o 5.1{2~: the sentence rror rateis reduced from 85.55~, to 30.1%.
The two most ina-portant ransformation steps are categorization andword joining.
What is striking, is the large fi'actionof deletion errors.
These deletion errors are oftencaused by the omission of word groups like 'for meplease "and "could you ".
Table 4 shows some exampletranslations (for the best translation results).
It canbe seen that the semantic meaning of the sentence inthe source language may be preserved even if thereare three word errors according t.o our performancecriterion.
To study the dependence on the amountof training data, we also performed a training wit.laonly 5 000 sentences out of the training corpus.
Forthis training condition, the word error rate went uponly slightly, namely from 5.15}.
(for 80,000 trainingsentences) to 5.3% (for 5 000 training sentences).To study the effect of the language model, we test-ed a zerogram, a unigram and a bigram languagemodel using the standard set of 80 000 training sen-tences.
The results are shown in Table 5.
The294Table 4: Examples from tile EuTrans task: O= original sentence, R= reference translation.
A= automatict.ranslatiol~.O: He hecho la reserva de una habitacidn con televisidn y t.el~fono a hombre del sefior Morales.R: I have made a reservation for a room with TV and telephone for Mr. Morales.A: I have made a reservation for a room with TV and telephone for Mr. Morales.O: Sfibanme las maletas a mi habitacidn, pot favor.R: Send up my suitcases to my room, please.A: Send up my suitcases to my room, please.O: Pot favor, querr{a qua nos diese las llaves de la habitacidn.R: I would like you to give us the keys to the room, please.A: I would like you to give us the keys to the room, please.O: Pot favor, me pide mi taxi para la habitacidn tres veintidds?R: Could you ask for nay taxi for room number three two two for me.
please'?A: Could you ask for my taxi for room number three two two.
please?O: Por favor, reservamos dos habitaciones dobles con euarto de bafio.R: We booked two double rooms with a bathroom.A: We booked two double rooms with a bathroom, please.O: Quisiera qua nos despertaran mafiana las dos y cuarto, pot favor.R: l would like you to wake us up tomorrow at.
a quarter past two.
please.A: I want you to wake us up tomorrow at a quarter past two.
please.O: Rep/seme la cuenta de la l~abitacidn ochocientos veintiuno.R: Could .you check the bill for room number eight two one for me, please'?A: Check the bill for room lmmber eight two one.WER decreases from 31.1c/c for the zerogram modelto 5.1% for the bigram model.The results presented here can be compared withthe results obtained by the finite-state transducerapproach described in (Vidal, 1996: Vidal, 1997),where the same training and test conditions wereused.
However the only preprocessing step was cat-egorization.
In that work.
a WER of 7.1c)~.
was ob-tained as opposed to 5.1(7c presented in this paper.For smaller amounts of training data (say 5 000 sen-tence pairs), the DP based search seems to be evenlnore superior.Table 5: Language model perplexity (PP), word er-ror rates ( INS/DEL.
WER) and sentence rror rates(SER) for different language models.Model Language PP INS/DEL Translation WER Errors \ [SER \[%\]Zerogram 237.0 0.6/18.6 31.1 98.1Unigram 74.4 0.9/12.4 20.4 94.8Bigram 4.1 0.9/3.4 5.1 30.14.4 Effect of the  Word  Reorder ingIn more general cases and applications, there willahvays be sentence pairs with word alignments forwhich the monotony constraint is \]lot satisfied.
How-ever even then, the nlonotouy constraint is satisfiedlocally for the lion's share of all word alignments insuch sentences.
Therefore.
we expect t.o extend theapproach presented by the following methods:?
more systelnatic approaches to local and globalword reorderiugs that try to produce the sameword order in both languages.?
a multli-level approach that allows a small (say4) number of large forward and backward tran-sitions.
Within each level, the monotone align-ment model can still be applied, and only whenmoving from one level to the next, we have tohandle the problem of different word orders.To show the usefulness of global word reorder-ing.
we changed the word order of some sentencesby hand.
Table 6 shows the effect of the global re-ordering for two sentences.
In the first example, wechanged the order of two groups of consecutive wordsand placed an a.dditional copy of the Spanish word"euest, a'" into the source sentence.
In the secondexample, the personal pronoun "'me" was placed atthe end of the source sentence.
In both cases, weobtained a correct translation.5 Conc lus ionIn this paper, we have presented an HMM based ap-proach to handling word alignlnents and an associat-ed search algorithm for autonaatic translation.
Thecharacteristic feature of this approach is to make thealigmnent probabilities explicitly dependent on theMignment position of the previous word and t.o as-sume a monotony constraint for the word order inboth languages.
Due t.o this mOllOtony constraint.we are able to apply an efficient DP based search al-gorithln.
We have tested the model successfully onthe EuTrans traveller task, a limited domain taskwith a vocabulary of 200 to 500 words.
The result-295Table 6: Effect of the global word reordering: O= original sentence, R= reference translation, A= automatictranslation, O '= original sentence reordered, A '= aut, omatic translation after reordering.O: Cu?nto cuesta una habitacidn doble para cinco noches incluyendo servicio de habitaciones ?R: How much does a double room including room service cost for five nights ?A: How much does a double room including room service ?O': Cu~into cuesta una habitacidn doble incluyendo servicio' de habitaciones cuesta para cinco noches ?A': How much does a double room hlcluding room service cost for five nights ?O:.
Expli'que _me la factura de la habitacidn tres dos cuatro.R: Explain the bill for room number three two four for me.A: Explain the bill for room number three two four.O': Explique la faclura de la habitaci6n tres dos cuatro .ane.A :  Explain tile bill for rooln number three two four for me.ing word error rate was only 5.1V(.
To mitigate themonotony constraint, we plan to reorder the wordsin the source sentences to produce the same wordorder in both languages.Ack lmwledgementThis work has been supported partly by t.he Ger-man Federal Ministry of Education.
Science.
Re-search and Technology under the contract number01 IV 601 A (Verbmobil) and by the European Com-munity under the ESPRIT project number 20268(EuTrans).ReferencesA.
L. Berger.
P. F. Brown.
S. A. Della Pietra, V. J.Della Pietra.
,\].
R. Gillett.
J. D. Lafferty.
R. L.Mercer.
H. Printz.
and L. Ures.
1994.
"The Call-dide System for Machine Translation".
In Proc.
ofARPA Huma~ La,guage Technology Workshop.pp.
152-157.
Plainsboro.
NJ.
Morgan KaufinannPublishers.
San Mateo.
CA, March.P.
F. Brown, V. J. Della Pietra.
S. A. Della Pietra,and R. L. Mercer.
1993.
"'The Mathematics ofStatistical Machine Translation: Parameter Esti-mat.ion".
Comp,fational Linguistics, Vol.
19, No.2.
pp.
263-311.I.
Dagan.
K. W. Church.
and W. A. Gale.
1993.
"'Robust Bilingual Word Alignment for MachineAided Translation".
In Proc.
of the Workshop onI.<ry Large Corpora.
pp.
1-8.
Columbus, OH.P.
Fung.
and K. W. Church.
1994.
"'K-vec: A NewApproach for Aligning Parallel Texts", In Proc.
oflhe 15th In i. Conf.
on ('ompulalim~al Linguistics,pp.
10.(.
)6-1102, Kyoto.F..lelinek.
1.(.t76.
"'Speech Recognition by StatisticalMethods".
Proc.
of lhe IEEE.
Vol.
64. pp.
532-556.
April.M.
Kay.
and M. R6scheisen.
1993.
"Text-Translation Alignlnent".
Comp~talional Lin.gu~s-lie.s.
Vol.
19.
No.
2. pp.
121-142.H.
Ney, D. Mergel, A. Noll, A. Paeseler.
1992.
"Da-t.a Driven Search Organization for ContinuonsSpeech Recognition".
IEEE Trans.
on Signal Pro-cessing, Vol.
SP-40.
No.
2. pp.
272-281.
February.E.
Vidal.
1996.
"Final report of Esprit ResearchProject.
20268 (EuTrans): Example-Based Under-standing and Translation Systelns".
UniversidadPolit~cnica de Valencia, Instituto Tecnol6gio deInformgtica, October.E.
Vidal.
1997.
"Finite-State Speech-to-SpeechTranslation".
In Proc.
of lhe Int.
Co,,f.
on Acous-fits, Speech and Signal Processing.
Munich.
April.S.
Vogel, H. Ney, and C. Tillmmm.
1996.
"HMMBased Word Alignment in Statistical Transla-tion".
In Proc.
of the 16~h Inf.
Conf.
on Com-putational Linguistics.
pp.
836-841.
Copenhagen,August.D.
Wu.
1996.
"'A Polynomial-Time Algorithm forStatistical Machine Translation".
In Proc.
of the34th Annual Conf.
of the Associalio~ for Comp~l-talional Linguistics, pp.
152-158.
Santa Cruz, CA.Julle,296
