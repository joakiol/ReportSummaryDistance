Learning and Evaluation of DialogueStrategies for New Applications:Empirical Methods for Optimizationfrom Small Data SetsVerena Rieser?School of GeoSciences/University ofEdinburghOliver Lemon?
?School of Mathematical and ComputerSciences/Heriot-Watt UniversityWe present a new data-driven methodology for simulation-based dialogue strategy learning,which allows us to address several problems in the field of automatic optimization of dialoguestrategies: learning effective dialogue strategies when no initial data or system exists, anddetermining a data-driven reward function.
In addition, we evaluate the result with real users,and explore how results transfer between simulated and real interactions.
We use Reinforce-ment Learning (RL) to learn multimodal dialogue strategies by interaction with a simulatedenvironment which is ?bootstrapped?
from small amounts of Wizard-of-Oz (WOZ) data.
Thisuse of WOZ data allows data-driven development of optimal strategies for domains where noworking prototype is available.
Using simulation-based RL allows us to find optimal policieswhich are not (necessarily) present in the original data.
Our results show that simulation-basedRL significantly outperforms the average (human wizard) strategy as learned from the data byusing Supervised Learning.
The bootstrapped RL-based policy gains on average 50 times morereward when tested in simulation, and almost 18 times more reward when interacting with realusers.
Users also subjectively rate the RL-based policy on average 10% higher.
We also show thatresults from simulated interaction do transfer to interaction with real users, and we explicitlyevaluate the stability of the data-driven reward function.1.
IntroductionStatistical learning approaches, such as Reinforcement Learning (RL), for Spoken Dia-logue Systems offer several potential advantages over the standard rule-based hand-coding approach to dialogue systems development: a data-driven development cycle,?
Centre for Environmental Change and Sustainability, School of GeoSciences, Drummond Street,Edinburgh EH89XP, UK.
E-mail: verena.rieser@ed.ac.uk.??
The Interaction Lab, School of Mathematical and Computer Sciences (MACS), Heriot-Watt University,Edinburgh EH14 4AS, UK.
E-mail: o.lemon@hw.ac.uk.Submission received: 23 January 2009; revised submission received: 13 August 2010; accepted for publication:13 September 2010.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 1provably optimal action policies, a precise mathematical model for action selection, pos-sibilities for generalization to unseen states, and automatic optimization of competingtrade-offs in the objective function.
See Young (2000), Lemon and Pietquin (2007), andFrampton and Lemon (2009) for an introduction to dialogue strategy learning.One of the major limitations of this approach is that it relies on a large quantity ofdata being available.
In cases when a fixed data set is used for learning (e.g., Walker2000; Singh et al 2002; Henderson, Lemon, and Georgila 2008), the optimal policy canonly be discovered when it is present in the data set.
(Note, by a policy being ?presentin a data set?
we mean that the set of state-action mappings which define the policyis contained in that data set.
When a policy is not present in a data set, either somestates covered by the policy are not seen at all in that data, or the actions chosen by thepolicy in some states are different to those seen in the data.)
To overcome this problem,simulated learning environments are being used to explore optimal policies which werepreviously unseen in the data (e.g., Eckert, Levin, and Pieraccini 1997; Ai, Tetreault,and Litman 2007; Young et al 2009).
However, several aspects of the components ofthis simulated environment are usually hand-crafted, and thus limit the scope of policylearning.
In particular, the optimization (or reward) function is often manually set (Paek2006).
In order to build simulation components from real data, annotated in-domaindialogue corpora have to be available which explore a range of dialogue managementdecisions.
Collecting dialogue data without a working prototype is problematic, leavingthe developer with a classic ?chicken-or-egg?
problem.We therefore propose to learn dialogue strategies using simulation-based RL, wherethe simulated environment is learned from small amounts of Wizard-of-Oz (WOZ) data.In a WOZ experiment, a hidden human operator, the so-called ?wizard,?
simulates(partly or completely) the behavior of the application, while subjects are left in the beliefthat they are interacting with a real system (Fraser and Gilbert 1991).In contrast to preceding work, our approach enables strategy learning in domainswhere no prior system is available.
Optimized learned strategies are then availablefrom the first moment of on-line operation, and handcrafting of dialogue strategiesis avoided.
This independence from large amounts of in-domain dialogue data allowsresearchers to apply RL to new application areas beyond the scope of existing dialoguesystems.
We call this method ?bootstrapping.
?In addition, our work is the first using a data-driven simulated environment.
Previ-ous approaches to simulation-based dialogue strategy learning usually handcraft someof their components.Of course, some human effort is needed in developing the WOZ environment andannotating the collected data, although automatic dialogue annotation could be ap-plied (Georgila et al 2009).
The alternative?collecting data using hand-coded dialoguestrategies?would still require annotation of the user actions, and has the disadvantageof constraining the system policies explored in the collected data.
Therefore, WOZ dataallows exploration of a range of possible strategies, as intuitively generated by thewizards, in contrast to using an initial system which can only explore a pre-definedrange of options.However, WOZ experiments usually only produce a limited amount of data, andthe optimal policy is not likely to be present in the original small data set.
Our methodshows how to use these data to build a simulated environment in which optimalpolicies can be discovered.
We show this advantage by comparing RL-based strategyagainst a supervised strategy which captures average human wizard performance onthe dialogue task.
This comparison allows us to measure relative improvement over thetraining data.154Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsThe use of WOZ data has earlier been proposed in the context of RL.
Williams andYoung (2004) use WOZ data to discover the state and action space for the design ofa Markov Decision Process (MDP).
Prommer, Holzapfel, and Waibel (2006) use WOZdata to build a simulated user and noise model for simulation-based RL.
Althoughboth studies show promising first results, their simulated environments still containmany hand-crafted aspects, which makes it hard to evaluate whether the success of thelearned strategy indeed originates from the WOZ data.
Schatzmann et al (2007) proposeto ?bootstrap?
with a simulated user which is entirely hand-crafted.
In the following wepropose what is currently the most strongly data-driven approach to these problems.
Wealso show that the resulting policy performs well for real users.
In particular we proposea five-step procedure (see Figure 1):1.
We start by collecting data in a WOZ experiment, as described in Section 2.2.
From these data we train and test different components of our simulatedenvironment using Supervised Learning techniques (Section 3).
InFigure 1Data-driven methodology for simulation-based dialogue strategy learning for new applications.155Computational Linguistics Volume 37, Number 1particular, we extract a supervised policy, reflecting human (wizard)performance on this task (see Section 3.3).
We build a noise simulation(Section 3.4), and two different user simulations (Section 3.5), as well as adata-driven reward function (Section 3.6).3.
We then train and evaluate dialogue policies by interacting with thesimulated environment (Section 4).4.
Once the learned policies are ?good enough?
in simulation, we test themwith real users (Section 5).5.
In addition, we introduce a final phase where we meta-evaluate the wholeframework (Section 6).
This final step is necessary because WOZexperiments only simulate human?computer interaction (HCI).
Wetherefore need to show that a strategy bootstrapped from WOZ dataindeed transfers to real HCI.
We first show that the results betweensimulated and real interaction are compatible (Section 6.1).
We alsometa-evaluate the reward function, showing that it is a stable, accurateestimate for real user?s preferences (Section 6.2).Note that RL is fundamentally different to Supervised Learning (SL): RL is a statisti-cal planning approach which allows us to find an optimal policy (sequences of actions)with respect to an overall goal (Sutton and Barto 1998); SL, in contrast, is concerned withdeducing a function from training data for predicting/classifying events.
This articleis not concerned with showing differences between SL and RL on a small amount ofdata, but we use SL methods to capture the average human wizard strategy in theoriginal data, and show that simulation-based RL is able to find new policies that werepreviously unseen.We apply this framework to optimize multimodal information-seeking dialoguestrategies for an in-car digital music player.
Dialogue Management and multimodaloutput generation are two closely interrelated problems for information seeking dia-logues: the decision of when to present information depends on how many pieces ofinformation to present and the available options for how to present them, and vice versa.We therefore formulate the problem as a hierarchy of joint learning decisions which areoptimized together.
We see this as a first step towards an integrated statistical model ofDialogue Management and more advanced output planning/Natural Language Gener-ation (Lemon 2008; Rieser and Lemon 2009b; Lemon 2011; Rieser, Lemon, and Liu 2010;Janarthanam and Lemon 2010).In the following, Section 2 describes the Wizard-of-Oz data collection (i.e., howto collect appropriate data when no initial data or system exists), Section 3 explainsthe construction of the simulated learning environment (including how to determine adata-driven reward function), Section 4 presents training and evaluation of the learnedpolicies in simulation (i.e., how to learn effective dialogue strategies), Section 5 presentsthe results of the tests with real users, and Section 6 presents a meta-evaluation of theframework, including transfer results.2.
Wizard-of-Oz Data CollectionThe corpus used for learning was collected in a multimodal study of German task-oriented dialogues for an in-car music player application.
The corpus was created156Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applicationsin the larger context of the TALK project1 and is also known as the SAMMIE corpus(Kruijff-Korbayova?
et al 2006).
In contrast to conventional WOZ trials we were not onlyinterested in the users?
behavior, but also in the behavior of our human wizards.
Thisstudy provides insights into natural strategies of information presentation as performedby human wizards.2.1 Experimental SetupSix people played the role of an intelligent interface (the ?wizards?).
The wizardswere able to speak freely and display search results on the screen by clicking on pre-computed templates.
Wizards?
outputs were not restricted, in order to explore thedifferent ways they intuitively chose to present search results.
Wizard?s utterances wereimmediately transcribed and played back to the user with Text-To-Speech.
Twenty-onesubjects (11 women, 10 men) were given a set of predefined tasks to perform, as well asa primary driving task, using a driving simulator (Mattes 2003).
The users were able tospeak, as well as make selections on the screen.The experiment proceeded as follows.
First the wizards were trained to use thedatabase interface and they were also given general instructions about how to interactwith the user.
Training took 45 minutes, including five example tasks.After the user arrived s/he was introduced to the driving simulator and had toperform a short test drive.
The users solved two sets of tasks with two tasks in each.After each task the user filled out a task-specific questionnaire, in which they indicatedperceived task success and satisfaction on a five-point Likert scale.
Finally, the user wasinterviewed by the experiment leader following a questionnaire containing questionssimilar to the PARADISE study (Walker, Kamm, and Litman 2000), including questionson task ease, timing, multimodal and verbal presentation, as well as future use of suchsystems.
All subjects reported that they were convinced that they were interacting witha real system.To approximate speech recognition errors we used a tool that randomly deletesparts of the transcribed utterances.
Due to the fact that humans are able to make senseof even heavily corrupted input, this method not only covers non-understandings, butwizards also built up their own hypotheses about what the user really said, which canlead to misunderstandings.
The word deletion rate varied: 20% of the utterances wereweakly corrupted (= deletion rate of 20%), and 20% were strongly corrupted (= deletionrate of 50%).
In 60% of the cases the wizard saw the transcribed speech uncorrupted.Example (1) illustrates the kind of corrupted utterances the wizard had to deal with.
(1) uncorrupted: ?Zu dieser Liste bitte Track ?Tonight?
hinzufu?gen.?
[ ?Add track ?Tonight?
to this list.?
]weakly corrupted: ?Zu dieser Liste bitte Track ?Tonight?
.
.
.
.
?[?.
.
.
track ?Tonight?
to this list.?
]strongly corrupted: ?Zu .
.
.
Track ?Tonight?
.
.
.
.[?.
.
.
track ?Tonight?
to .
.
.
?
]There are some shortcomings of this technique, which we discuss in Rieser andLemon (2009a).
However, the data are useful for our purposes because our main interest1 TALK (Talk and Look: Tools for Ambient Linguistic Knowledge; www.talk-project.org) was funded bythe EU as project no.
IST-507802 within the 6th Framework program.157Computational Linguistics Volume 37, Number 1here is in multimodal presentation strategies (in the presence of some input noise).Other studies have specifically targeted the Dialogue Management question of how tohandle ASR input noise (e.g., Stuttle, Williams, and Young 2004; Skantze 2005).2.2 Data CollectedThe corpus gathered with this set-up comprises 21 sessions and over 1,600 turns.
Someexample dialogues can be found in Appendix B.
Example (2) shows a typical multi-modal presentation sub-dialogue from the corpus (translated from German).
Note thatthe wizard displays quite a long list of possible candidates on an (average sized) com-puter screen, while the user is driving.
This example illustrates that even for humans itis difficult to find an ?optimal?
solution to the problem we are trying to solve.
(2) User: ?Please search for music by Bjo?rk.
?Wizard: ?I found 43 items.
The items are displayed on the screen.?
[displays list]User: ?Please select Human Behaviour.
?Information was logged for each session, for example, the transcriptions of thespoken utterances, the wizard?s database query and the number of results, and thescreen option chosen by the wizard.
A rich set of contextual dialogue features wasalso annotated, as listed in Section 3.1.
Also see Rieser, Kruijff-Korbayova?, and Lemon(2005).Of the 793 wizard turns 22.3% were annotated as presentation strategies, re-sulting in 177 instances for learning, where the six wizards contributed about equalproportions.A ?2 test on presentation strategy (comparing whether wizards chose to presentin multimodal or verbal modality) showed significant differences between wizards(?2(1) = 34.21, p < .001).
On the other hand, a Kruskal-Wallis test comparing userpreferences for the multimodal output showed no significant difference across wizards(H(5)=10.94, p > .05).2 Mean performance ratings for the wizards?
multimodal behaviorranged from 1.67 to 3.5 on a five-point Likert scale.
We also performed an analysis ofwhether wizards improved their performance over time (learning effects).
The resultsshow that the wizard?s average user satisfaction scores in general slightly decreasedwith the number of sessions that they performed, however.Observing significantly different strategies that are not significantly different interms of user satisfaction, we conjecture that the wizards converged on strategies whichwere appropriate in certain contexts.
To strengthen this hypothesis we split the data bywizard and performed a Kruskal-Wallis test on multimodal behavior per session.
Onlythe two wizards with the lowest performance score showed no significant variationacross session, whereas the wizards with the highest scores showed the most varyingbehavior.
These results again indicate a context-dependent strategy.In Section 3.1 we test this hypothesis (that good multimodal clarification strate-gies are context-dependent) by using feature selection techniques in order to find the2 The Kruskal-Wallis test is the non-parametric equivalent to a one-way ANOVA.
Because the usersindicated their satisfaction on a five-point likert scale, an ANOVA which assumes normality would beinvalid.158Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applicationsfeatures which are most predictive for the wizards?
behavior.
The dialogues show thatcommon ?mistakes?
were that the wizards either displayed too much information onthe screen, see Example (1) in Appendix B, or the wizards fail to present results earlyenough, see Example (2) in Appendix B.
In general, users report that they get distractedfrom driving if too much information is presented.
On the other hand, users prefershorter dialogues (most of the user ratings are negatively correlated with dialoguelength).These results indicate that we need to find a strategy given the competing trade-offs between the number of results (large lists are difficult for users to process), thelength of the dialogue (long dialogues are tiring, but collecting more information canresult in more precise results), and the noise in the speech recognition environment(in high noise conditions accurate information is difficult to obtain).
In the followingwe utilize the ratings from the user questionnaires to optimize a presentation strategyusing simulation-based RL.3.
Simulated Learning EnvironmentSimulation-based RL learns by interaction with a simulated environment (Sutton andBarto 1998).
We obtain the simulated components from the WOZ corpus using data-driven methods.
Although this requires quite a large effort, the exercise is important asa case study for exploring the proposed methodology.The employed database contains 438 items and is similar in retrieval ambiguity andstructure to the one used in the WOZ experiment.
The dialogue system used for learningimplements a multimodal information presentation strategy which is untrained, butcomprises some obvious constraints reflecting the system logic (e.g., that only filledslots can be confirmed), implemented as Information State Update (ISU) rules (see alsoHeeman 2007; Henderson, Lemon, and Georgila 2008).Other behavior which is hand-coded in the system is to greet the user in thebeginning of a dialogue and to provide help if the user requests help.
The help functionprovides the user with some examples of what to say next (see system prompt s6 in theExample Dialogue in Table 1 in Appendix D).
All other actions are left for optimization.3.1 Feature SpaceA state or context in our system is a dialogue ?information state?
as defined in (Lemonet al, 2005).
We divide the types of information represented in the dialogue informationstate into local features (constituting low-level and dialogue features), dialogue historyfeatures, and user model features.
We also defined features reflecting the applicationenvironment (e.g., driving).
The information state features are shown in Tables 1, 2,and 3, and further described below.
All features are automatically extracted from theWOZ log-files (as described in Section 2.2), and are available at runtime in ISU-baseddialogue systems.Local features.
First, we extracted features present in the ?local?
context of a wizardaction, as shown in Table 1, such as the number of matches returned from the databasequery (DB), whether any words were deleted by the corruption algorithm (see Sec-tion 2.1), and the previous user speech act (user-act) of the antecedent utterance.
The159Computational Linguistics Volume 37, Number 1Table 1Contextual/information-state features: Local features.Local featuresDB: database matches (integer)deletion: words deleted (yes/no)user-act: add, repeat, y/n, change, othersuser actions are annotated manually by two annotators (?
= .632).
Please see Table 1 inAppendix A for detailed results on inter-annotator agreement.Also, note that the deletion feature (and later delHist, and delUser) counts thenumber of words deleted by the corruption tool (see Section 2.1) and serves as anapproximation to automatic speech recognition (ASR) confidence scores as observed bythe system.
Equally, the human wizard will be able to infer when words in a sentencewere deleted and hence has a certain confidence that the input is complete.Dialogue history features.
The history features account for events in the whole dialogueso far, that is, all information gathered before entering the presentation phase, asshown in Table 2.
We include features such as the number of questions that the wizardasked so far (questHist), how often the screen output was already used (screenHist),the average corruption rate so far (delHist), the dialogue length measured in turns(dialogueLength), the dialogue duration in seconds (dialogueDuration), and whetherthe user reacted to the screen output, either by verbally referencing (refHist), forexample, using expressions such as It?s item number 4, or by clicking (clickHist).User model features.
Under ?user model features?
we consider features reflecting the wiz-ards?
responsiveness to the behavior and situation of the user.
Each session comprisesfour dialogues with one wizard.
The user model features average the user?s behavior inthese dialogues so far, as shown in Table 3, such as how responsive the user is towardsthe screen output, namely, how often this user clicks (clickUser) and how frequentlys/he used verbal references so far (refUser); how often the wizard had already shown ascreen output (screenUser) and how many questions were already asked (questUser);how much the user?s speech was corrupted on average so far (delUser), that is, anapproximation of how well this user is recognized; and whether this user is currentlydriving or not (driving).
This information was available to the wizards.Table 2Contextual/information-state features: History features.Dialogue History FeaturesquestHist: number of questions (integer)screenHist: number screen outputs (integer)delHist: average corruption rate; no.wordsDeletedInDialogueSoFarno.utterancesInDialogueSoFar (real)dialogueLength: length in turns (integer)dialogueDuration: time in sec (real)refHist: number of verbal user references to screen output (integer)clickHist: number of click events (integer)160Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsTable 3Contextual/information-state features: User model features.User model featuresclickUser: average number of clicks (real)refUser: average number of verbal references (real)delUser: average corruption rate for that user; no.wordsDeletedForUserSoFarno.utterancesForUserSoFar (real)screenUser: average number of screens shown to that user (real)questUser: average number of questions asked to user (real)driving: user driving (yes/no)Note that all these features are generic over information-seeking dialogues wheredatabase results can be displayed on a screen; except for driving which only appliesto hands-and-eyes-busy situations.
This potential feature space comprises 16 features,many of them taking numerical attributes as values.
Including them all in the state spacefor learning would make the RL problem unnecessarily complex.
In the next section wedescribe automatic feature selection techniques, which help to reduce the feature spaceto a subset which is most predictive of when and how to present search results.3.1.1 Feature Selection.
We use feature selection techniques to identify the context featureswhich are most predictable for the wizards choosing a specific action.
We choose toapply forward selection for all our experiments in order to not include redundantfeatures, given our large feature set.
We use the following feature filtering methods:correlation-based subset evaluation (CFS; Hall 2000) and a decision tree algorithm (rule-based SL).
We also apply a correlation-based ?2 ranking technique.
Filtering techniquesaccount for inter-feature relations, selecting subsets of predictive features at the expenseof saying less about individual feature performance itself.
Ranking techniques evaluateeach feature individually.
For our experiments we use implementations of selectiontechniques provided by the WEKA toolkit (Witten and Frank 2005).First, we investigated the wizards?
information acquisition strategy, namely, whichfeatures are related to the wizards?
decision when to present a list (presentInfo)?thatis, the task is to predict presentInfo vs. all other possible dialogue acts.
None of thefeature selection techniques were able to identify any predictive feature for this task.Next, we investigated the wizards?
information presentation strategy, that is, whichfeatures are related to the wizards?
decision to present a list verbally (presentInfo-verbal) or multi-modally (presentInfo-multimodal).
All the feature selection tech-niques consistently choose the feature DB (number of retrieved items from the database).This result is maybe not very surprising, but it supports the claim that using featureselection on WOZ data delivers valid results.
Relevant features for other domains maybe less obvious.
For example, Levin and Passonneau (2006) suggest the use of WOZdata in order to discover the state space for error recovery strategies.
For this task manyother contextual features may come into play, as shown by Gabsdil and Lemon (2004)and Lemon and Konstas (2009) for automatic ASR re-ranking.We use this information to construct the state space for RL, as described in thefollowing section, as well as using these feature selection methods to construct thewizard strategy as described in Section 3.3.161Computational Linguistics Volume 37, Number 1Figure 2State-action space for hierarchical Reinforcement Learning.3.2 MDP and Problem RepresentationThe structure of an information-seeking dialogue system consists of an informationacquisition phase, and an information presentation phase.
For information acquisitionthe task of the dialogue manager is to gather ?enough?
search constraints from the user,and then, ?at the right time,?
to start the information presentation phase, where the pre-sentation task is to present ?the right amount?
of information in the right way?eitheron the screen or listing the items verbally.
What ?the right amount?
actually meansdepends on the application, the dialogue context, and the preferences of users.
Foroptimizing dialogue strategies information acquisition and presentation are two closelyinterrelated problems and need to be optimized jointly: When to present informationdepends on the available options for how to present them, and vice versa.We therefore formulate the problem as an MDP, relating states to actions in a hierar-chical manner (see Figure 2): Four actions are available for the information acquisitionphase; once the action presentInfo is chosen, the information presentation phase isentered, where two different actions for output realization are available.The state space is constructed semi-automatically.
We manually enumerate the task-related features needed to learn about the dialogue task.
For example, we manuallyspecified the number of slots, and information about the ?grounded-ness?
of the slots,needed to learn confirmation strategies.3 We also added the features which were auto-matically discovered by the feature selection techniques defined in Section 3.1.1.The state-space comprises eight binary features representing the task for a four-slot problem: filledSlot indicates whether a slot is filled, confirmedSlot indicateswhether a slot is confirmed.
We also add the number of retrieved items (DB).
We foundthat human wizards especially pay attention to this feature, using the feature selectiontechniques of Rieser and Lemon (2006b).
The feature DB takes integer values between1 and 438, resulting in 28 ?
438 = 112, 128 distinct dialogue states for the state space.In total there are 4112,128 theoretically possible policies for information acquisition.4 For3 Note that we simplified the notion of a slot being grounded as a binary feature, following Henderson,Lemon, and Georgila (2008).
More recent work uses more fine-grained notions of confidence inuser-provided information (e.g., Roque and Traum 2008), or the notion of ?belief states?
in PartiallyObservable Markov Decision Processes (e.g., Williams and Young 2007).
This does lead to new policies ininformation acquisition, but is not the focus of this article.4 In practice, the policy space is smaller, as some combinations are not possible (e.g., a slot cannot beconfirmed before being filled).
Furthermore, some incoherent action choices are excluded by the basicsystem logic.162Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applicationsthe presentation phase the DB feature is discretized, as we will further discuss in Section3.7.
For the information presentation phase there are 223= 256 theoretically possiblepolicies.3.3 Wizard BehaviorOur hypothesis is that simulation-based RL allows us to find optimal policies which aresuperior to those present in the original data.
Therefore we create a policy which mimicsthe average wizard behavior, and this allows us to measure the relative improvementsover the training data (cf.
Henderson, Lemon, and Georgila 2008).
We create this base-line by applying SL.
For these experiments we use the WEKA toolkit (Witten and Frank2005).
We learn with the decision tree J4.8 classifier, WEKA?s implementation of the C4.5system (Quinlan 1993), and rule induction JRIP, the WEKA implementation of RIPPER(Cohen 1995).
In particular, we learn models which predict the following wizard actions: Presentation timing: when the ?average?
wizard starts the presentationphase on a turn level (binary decision). Presentation modality: in which modality the list is presented (multimodalvs.
verbal).We use annotated dialogue context features as input, as described in Section 3.1,with feature selection techniques as described in Section 3.1.1.
Both models are trainedusing 10-fold cross validation, comparing the predicted labels against the true labels ina hold-out test set.
Table 4 presents the results for comparing the accuracy of the learnedclassifiers against the majority baseline.A data analysis shows that all of the wizards are more likely to show a graphicon the screen when the number of database hits is ?
4.
However, none of the wizardsstrictly follows that strategy.For presentation timing, none of the classifiers produces significantly improvedresults.
Hence, we conclude that there is no distinctive pattern observable by the SLalgorithms for when to present information.
For strategy implementation we thereforeuse a frequency-based approach following the distribution in the WOZ data: In 48% ofcases the baseline policy decides to present the retrieved items; for the rest of the timethe system follows a hand-coded strategy.For learning presentation modality, both classifiers significantly outperform the ma-jority baseline.
The learned models both learn the same rule set, which can be rewrittenas in Listing 1.
Note that this rather simple algorithm is meant to represent the averagestrategy as learned by SL from the initial data (which then allows us to measure therelative improvements of the RL-based strategy).Table 4Predicted accuracy for presentation timing and modality (with standard deviation ?
).majority baseline JRip J48timing 52.0(?
2.2) 50.2(?
9.7) 53.5(?
11.7)modality 51.0(?
7.0) 93.5(?11.5)* 94.6(?
10.0)** Statistically significant improvement at p < .05.163Computational Linguistics Volume 37, Number 1Listing 1Supervised Strategyi f ( db<4){return present InfoVerba l ; }else {return presentInfoMM ;}3.4 Noise SimulationOne of the fundamental characteristics of HCI is an error-prone communication chan-nel.
Therefore, the simulation of channel noise is an important aspect of the learn-ing environment.
Previous work uses data-intensive simulations of ASR errors (e.g.,Pietquin and Dutoit 2006; Schatzmann, Thomson, and Young 2007a).
Because we onlyhave limited data available, we use a simple model simulating the effects of non- andmisunderstanding on the interaction, rather than the noise itself.
This method is espe-cially suited to learning from small data sets.
From our data we estimate a 30% chanceof user utterances to be misunderstood, and 4% to be complete non-understandings,which is a realistic estimation for deployed dialogue systems (cf.
Litman and Pan 1999;Carpenter et al 2001; Hirschberg, Litman, and Swerts 2001; Georgila, Henderson, andLemon 2005).We simulate the effects that noise has on the user behavior, as well as for the taskaccuracy.5 For the user side, the noise model defines the likelihood of the user acceptingor rejecting the system?s hypothesis (e.g., when the system utters a confirmation), thatis, in 30% of the cases the user rejects, in 70% the user agrees.
These probabilities arecombined with the probabilities for user actions from the user simulation, as describedin the next section.
For non-understandings we have the user simulation generatingOut-of-Vocabulary utterances with a chance of 4%.
Furthermore, the noise model deter-mines the likelihood of task accuracy as calculated in the reward function for learning.A filled slot which is not confirmed by the user has a 30% chance of having been mis-recognized, see Task Completion as defined in Section 3.6.3.5 User SimulationA user simulation is a predictive model of real user behavior used for automatic di-alogue strategy development and testing.
See Schatzmann et al (2006) for a compre-hensive survey.
Simulations on the intention/dialogue act level are most popular forRL-based strategy learning, as they outperform the lower level approaches in terms ofrobustness, portability, and scalability.
For our domain, the user can either add newinformation (add), repeat or paraphrase information which was already provided at anearlier stage (repeat), give a simple yes/no answer (y/n), or change to a different topicby providing a different slot value than the one asked for (change).
Examples from thecorpus are given in Table 5 and in the dialogues listed in Appendix B.
These actions areannotated manually by two annotators (?
= .632, see Appendix A).5 Simulating the effects of noise, rather than the noise itself, is sufficient to learn presentation strategies inthe presence of noise (e.g., whether a slot has to be confirmed before a result can be presented).
Note thatother work has focused on learning dialogue strategies under different noise conditions (e.g., Bohus et al2006; Williams and Young 2007).164Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsTable 5User action types and frequencies as annotated in the data.# action type freq % example (original) translation1 add 54 30.5 a?h, Ella Fitzgerald.
er, Ella Fitzgerald.3 repeat 57 32.2 ja, Smile ja.
yes, Smile yes.2 y/n 14 7.9 ja, in Ordnung.
yes, that?s OK.4 change 17 9.6 dann machen wir was anderesund zwar ha?tte ich gern einePlaylist mit drei Liedern.Let?s try something else then.
Iwould like a playlist with threesongs.others 35 19.8 ?
no answer, comment, asideIn this work, we are challenged to learn user simulations from a small data set.
Wefirst construct a simple bigram model in order to explore the quality of the data.
Bigram(or more general n-gram) models for user simulations were first introduced by Eckert,Levin, and Pieraccini (1997, 1998).
An n-gram?based user simulation predicts the useraction ?au,t at time t that is most probable given the dialogue history of system and useractions, see Equation (1) where as,t denotes the system action at time t.?au,t = argmaxau,tP(au,t|as,t, as,t?1, au,t?1, ..., au,t?n+1, as,t?n+1) (1)The bigram model obtained from our WOZ data and the observed frequencies areshown in Figure 3.
When examining the distributions of user replies per system turn forthe bigram model, we can see that 25% of the state?action pairs have zero frequencies.However, user simulations should allow the learner to also find strategies which arenot in the data.
Especially when learning from small data sets, user simulations forFigure 3User action frequencies following a system act (bigram model): 25% zero frequencies forstate-action pairs due to data sparsity.165Computational Linguistics Volume 37, Number 1automatic strategy training should cover the whole variety of possible user action foreach state in order to produce robust strategies.
Ai, Tetreault, and Litman (2007), forexample, show that random models outperform more accurate ones if the latter fail toprovide enough coverage.
On the other hand, user simulations used for testing shouldbe more accurate with respect to the data in order to test under realistic conditions (e.g.,Mo?ller et al 2006).We therefore apply two learning methods to deal with data sparsity (for n-grammodels): First, we develop a user simulation which is based on a new clustering tech-nique; second, we apply smoothing (which is the standard technique applied to accountfor zero frequencies in n-gram models).3.5.1 Cluster-Based User Simulation.
We introduce a cluster-based technique for buildinguser simulations from small amounts of data (see also Rieser and Lemon 2006a).
A sim-ilar approach has later been suggested by Schatzmann, Thomson, and Young (2007b),called the ?summary-space mapping technique,?
where similar states are summarized,and a distribution of possible user behavior is assigned to a set of states, which we call?clusters.?
This method allows one to generate the full range of possible user behaviorin every state.Cluster-based user simulations generate explorative user behavior which is similarbut not identical to user behavior observed in the original data.
In contrast to the bigrammodel, where the likelihood of the next user act is conditioned on the previous systemaction, the likelihood for the cluster-based model is conditioned on a cluster of similarsystem states (see Equation (2)).
?au,t ?
argmaxau,tP(au,t|clusters,t?1) (2)The underlying idea is that, with sparse training data, we want user simulations to be?similar to real users in similar situation.?
This user simulation should generate anykind of observed user behavior in a context (as opposed to the zero frequencies forsparse data), while still generating behavior which is pragmatically plausible in this sit-uation.
That is, we want our user simulation to generate behavior which is complete andconsistent with respect to the observed actions in the data.
We also want our model togenerate actions which show some variability with respect to the observed behavior, thatis, a controlled degree of randomness.
This variance will help us to explore situationswhich are not observed in the data, which is especially valuable when building a modelfrom sparse training data (cf.
Ai, Tetreault, and Litman 2007).Clustering is applied in order to identify more general situations than the previ-ously annotated system speech acts by grouping them according to their similarity.
Forbuilding such clusters we apply the Expectation-Maximization (EM) algorithm.
The EMalgorithm is an incremental approach to clustering (Dempster, Laird, and Rubin 1977),which fits parameters of Gaussian density distributions to the data.
In order to definesimilarity between system actions, we need to describe their (semantic) properties.We therefore annotate the system acts using a fine-grained scheme by Rodriguez andSchlangen (2004) and Rieser and Moore (2005), which allows classification of dialogueacts in terms of different forms and functions.We use a slightly modified version of the scheme, where we only use a subset of thesuggested annotation tags, while adding another level describing the output modality,as summarized in Figure 4.
In particular, the annotation scheme describes wizardactions in terms of their communication level, which describes the linguistic targetafter Clark (1996).
We distinguish between utterances which aim to elicit acoustic166Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsFigure 4Annotation scheme of discourse functions for wizard?s actions.information (e.g., Sorry, can you please repeat?
and utterances which aim to elicit fur-ther information to uniquely identify the user?s reference (e.g., By which artist?).
Aswell as utterances trying to establish contact (e.g., Can you hear me?
), and utterancesabout the user?s intention (e.g., What do you want me to do?).
The problem severitydescribes which type of feedback the system requests from the user, namely, asking forconfirmation, for repetition, or for elaboration.
The modality of the dialogue actcan either be verbal or multimodal.Table 6 shows a mapping between system speech acts as described in Figure 2 andthe annotated discourse functions.
We use these features for clustering the speech actsaccording to their similarity in discourse function and form.The EM algorithm generates three state clusters: The system acts askAQuestion andimplConfirm are summarized into cluster 1; explConf and reject are in cluster 2; andpresentListVerbal and presentListMM are in cluster 3.
For every cluster we assign theobserved frequencies of user actions (i.e., all the user actions which occur with one ofthe states belonging to that cluster), as shown in Figure 5.3.5.2 Smoothed Bigram User Simulation.
For our second user simulation model we applysmoothing to a bigram model.
We implement a simple smoothing technique called?add-one smoothing?
(Jurafsky and Martin 2000).
This technique discounts some non-zero counts in order to obtain probability mass that will be assigned to the zero counts.We apply this technique to the original frequency-based bigram model.
The resultingmodel is shown in Figure 6.In general, the smoothed model is closer to the original data than the cluster-basedone (thus being more realistic at the expense of allowing less exploratory behavior).
Inthe next section we introduce an evaluation metric which allows us to assess the level ofexploratory versus realistic user behavior as exhibited by the different user simulations.Table 6System speech acts and corresponding discourse functions.speech act level severity modalityreject acoustic repetition verbalexplicitConfirm acoustic confirmation verbalaskAQuestion goal elaboration verbalimplicitConfirm goal confirmation+elaboration verbalpresentVerbal goal confirmation verbalpresentMM goal confirmation multimodal167Computational Linguistics Volume 37, Number 1Figure 5User action frequencies from the cluster-based user simulation.Figure 6User action frequencies from the smoothed bigram user simulation.3.5.3 Evaluation of User Simulations.
Several metrics have been proposed to evaluateuser simulations (e.g., Scheffler and Young 2001; Schatzmann, Georgila, and Young2005; Ai and Litman 2006; Georgila, Henderson, and Lemon 2006; Williams 2007).A good measure of dialogue similarity is based on the Kullback?Leibler (KL) diver-168Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsTable 7Kullback?Leibler divergence scores for the different user simulations.User simulations Baselinessmoothed cluster random majority0.087 0.095 0.43 0.48gence6 (as also used by Cuaya?huitl et al 2005; Jung et al 2009), which is defined asfollows:DKL(P||Q) =M?i=1P(i) ?
log P(i)Q(i)(3)This metric measures the divergence between distributions P and Q in a contextwith M responses.
Ideally, the KL divergence between two similar distributions is closeto zero.KL allows us to compare the raw probabilities as observed in the original data withthe probabilities generated by our user simulation models.
We then compare the KLresults of the cluster-based and the smoothed user simulation against a random modeland a majority baseline (see Table 7).
The random model is constructed by assigningequal frequency to all four actions, whereas the majority baseline always predicts themost frequent action in one context.
The comparison against the random baseline teststhe hypothesis that our user simulations are more consistent with the observed datathan random behavior.
The majority baseline represents the hypothesis that our usersimulation explores a significantly wider range of behavior than the most frequent useraction.The user simulation models have a small divergence from the original data suggest-ing that they are good simulations for training and testing policies.
The smoothed andthe cluster-based model gain on average five times lower KL scores than the baselines.We therefore conclude that both simulations show consistent (i.e., better than random)as well as varying (i.e., better than the majority decision) behavior.As mentioned previously, we want user simulations for policy training to allowmore exploration, whereas for testing we want user simulations which are more realis-tic.
We therefore choose to test with the smoothed model because its low KL score showsthat it is closest to the data, and we use the cluster-based simulation for training.Note that the KL divergence only measures consistency with respect to specificdialogue contexts.
However, user simulations also need to be coherent with respectto the dialogue history and the current user goal.
We therefore model the user?s goal(i.e., the song s/he is looking for) similar to ?agenda-based user models?
(Schatzmannet al 2007; Schatzmann, Thomson, and Young 2007b).
The user goal corresponds toa database entry, which is randomly chosen in the beginning of each dialogue.
Everytime the user simulation generates a speech act, the corresponding value is chosen fromthe goal record, dependent on the slot value the system was asking for.6 Also known as information divergence, information gain, or relative entropy.169Computational Linguistics Volume 37, Number 13.6 Data-Driven Reward ModelingThe reward function defines the goal of the overall dialogue.
For example, if it is mostimportant for the dialogue to be efficient, the function penalizes dialogue length, whilerewarding task success.
In most previous work the reward function is manually set,which makes it ?the most hand-crafted aspect?
of RL (Paek 2006).
For example, Williamsand Young (2007) use +10 points for task completion and ?1 point per turn, but thereis no empirical basis for this specific ratio.
In contrast, we learn the reward modelfrom data, using a modified version of the PARADISE framework (Walker, Kamm, andLitman 2000), following pioneering work by Walker, Fromer, and Narayanan (1998).
InPARADISE multiple linear regression is used to build a predictive model of subjectiveuser ratings (from questionnaires) from objective dialogue performance measures (suchas dialogue length).
The subjective measure that we wish to optimize for our applicationis Task Ease, a variable obtained by taking the average of two questions in the question-naire.7 We use PARADISE to predict Task Ease from various input variables, via stepwiseregression.
The chosen model comprises dialogue length in turns, task completion (asmanually annotated in the WOZ data), and the multimodal user score from the userquestionnaire, as shown in Equation (4) (R2 = .144,R2adjusted = .123).TaskEase = ?
20.2 ?
dialogueLength +11.8 ?
taskCompletion + 8.7 ?
multimodalScore; (4)This equation is used to calculate the overall reward for the information acquisitionphase.
Task completion is calculated on-line during learning, penalizing all slots whichare filled but not confirmed.
Slots that are filled but not confirmed have a 30% chanceof being incorrect according to the noise model (see Section 3.4).
For the informationpresentation phase, we compute a local reward.
We relate the multimodal score (avariable obtained by taking the average of four questions)8 to the number of itemspresented (DB) for each modality, using curve fitting.
In contrast to linear regression,curve fitting does not assume a linear inductive bias, but it selects the most likely model(given the data points) by function interpolation.
The resulting models are shown inFigure 7.
The reward for multimodal presentation is a quadratic function that assigns amaximal score to a strategy displaying 14.8 items (curve inflection point).
The rewardfor verbal presentation is a linear function assigning negative scores to all presenteditems ?
4.
The reward functions for information presentation intersect at no.
items = 3.A comprehensive evaluation of this reward function can be found in Section 6.2.3.7 State Space DiscretizationWe use linear function approximation in order to learn with large state-action spaces.Linear function approximation learns linear estimates for expected reward values ofactions in states represented as feature vectors.
This is inconsistent with the idea of non-linear reward functions (as introduced in the previous section).
We therefore quantizethe state space for information presentation.
We partition the database feature into three7 ?The task was easy to solve?, ?I had no problems finding the information I wanted.
?8 ?I liked the combination of information being displayed on the screen and presented verbally?,?Switching between modes did not distract me?, ?The displayed lists and tables contained on average theright amount of information?, ?The information presented verbally was easy to remember.
?170Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsFigure 7Evaluation functions relating number of items presented in different modalities to multimodalscore.bins, taking the first intersection point between verbal and multimodal reward and theturning point of the multimodal function as discretization boundaries.
Previous workon learning with large databases commonly quantizes the database feature in orderto learn with large state spaces using manual heuristics (e.g., Levin, Pieraccini, andEckert 2000; Heeman 2007).
Our quantization technique is more principled as it reflectsuser preferences for multi-modal output.
Furthermore, in previous work database itemswere not only quantized in the state-space, but also in the reward function, resulting in adirect mapping between quantized retrieved items and discrete reward values, whereasour reward function still operates on the continuous values.
In addition, the decision ofwhen to present a list (information acquisition phase) is still based on continuous DBvalues.
In future work we plan to engineer new state features in order to learn withnon-linear rewards while the state space is still continuous.
A continuous representationof the state space allows learning of more fine-grained local trade-offs between theparameters, as demonstrated by Rieser and Lemon (2008a).4.
Training and Testing the Learned Policies in SimulationWe now train and test the multimodal presentation strategies by interacting with thesimulated learning environment.
For the following RL experiments we used the REALL-DUDE toolkit of Lemon et al (2006).
The SHARSHA algorithm is employed for train-ing, which adds hierarchical structure to the well known SARSA algorithm (Shapiro andLangley 2002).
The policy is trained with the cluster-based user simulation over 180ksystem cycles, which results in about 20k simulated dialogues.
In total, the learned strat-egy has 371 distinct state-action pairs as presented in the look-up table in Appendix E.We test the RL-based and the SL wizard baseline policy, as listed in Listing 1,which allows us to measure relative improvement over the training data.
We run 500171Computational Linguistics Volume 37, Number 1test dialogues with the smoothed user simulation, as described in Section 3.5.2, so thatwe are not training and testing on the same simulation.
We then compare quantitativedialogue measures by performing a paired t-test.
In particular, we compare meanvalues of the final rewards, number of filled and confirmed slots, dialogue length,and items presented multimodally (MM items) and items presented verbally (verbalitems).
RL performs significantly better (p < .001) than the baseline strategy.
The onlynon-significant difference is the number of items presented verbally, where both the RLand the average wizard strategy settled on a threshold of fewer than four items.
Themean performance measures for simulation-based testing are shown in Table 8.The major strength of the RL policy is that it learns to keep the dialogues reasonablyshort (on average 5.9 system turns for RL versus 8.4 turns for SL wizard) by presentinglists as soon as the number of retrieved items is within tolerance range for the respec-tive modality (as reflected in the reward function).
The SL strategy in contrast hasnot learned the right timing nor an upper bound for displaying items on the screen(note that the distribution for MM items is highly positively skewed with a maximum of283 items being displayed).
See example dialogues in Appendix C.The results show that simulation-based RL with an environment bootstrappedfrom WOZ data allows learning of robust strategies which significantly outperform thestrategies learned by SL from the original data set.
This confirms our hypothesis thatsimulation-based RL allows us to find optimal policies which are not easily discoverable(by Supervised Learning) in the original data.Furthermore, RL allows us to provide additional information about user prefer-ences in the reward function, whereas SL simply mimics the data.
In addition, RLis based on delayed rewards, namely, the optimization of a final goal.
For dialoguesystems we often have measures indicating how successful and/or satisfying the overallperformance of a strategy was, but it is hard to tell how exactly things should have beendone in a specific situation.
This is what makes RL specifically attractive for dialoguestrategy learning.
In the next section we test the learned strategy with real users.5.
Tests with Real Users5.1 Experimental DesignFor the user tests the RL policy is ported to a working ISU-based dialogue system viatable look-up (see table in Appendix E) , which indicates the action with the highestexpected reward for each state (cf.
Singh et al 2002).
The supervised average wizardbaseline is implemented using standard threshold-based update rules.
The experimen-Table 8Comparison of results for SL wizard and RL-based strategies in simulation.Measure SL wizard baseline RL Strategyavg.
turns 8.42(?3.04) 5.9(?2.4)***avg.
speech items 1.04(?.2) 1.1(?.3)avg.
MM items 61.37(?82.5) 11.2(?2.4)***avg.
reward ?1,741.3(?566.2) 44.06(?51.5)****** Significant difference between SL and RL at p < .001 (with standard deviation ?
).172Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsTable 9Comparison of mean user ratings for SL wizard baseline and RL policies (with standarddeviation ?
).Measure Wizard Strategy RL StrategyTask Ease 4.78 (?1.84) 5.51 (?1.44)***Timing 4.42 (?1.84) 5.36 (?1.46)***MM Presentation 4.57 (?1.87) 5.32 (?1.62)***Verbal Presentation 4.94 (?1.52) 5.55 (?1.38)***Future Use 3.86 (?1.44) 4.68 (?1.39)****** Statistical significance at p < .001.tal conditions are similar to the WOZ study, that is, we ask the users to solve similartasks, and use similar questionnaires.9 Furthermore, we decided to use typed userinput rather than ASR.
The use of text input allows us to target the experiments to thedialogue management decisions on presentation strategies, and prevents ASR qualityfrom interfering with the experimental results, especially because subjective user scoresare highly sensitive to ASR noise (Hajdinjak and Mihelic 2006).
Both RL and SL wizardpolicies are trained to handle noisy conditions, so that they usually confirm user input,which makes dialogues longer but more reliable.
The lack of noise in this experimentmeans that confirmation happens more than is strictly required (although there arestill text input spelling mistakes), but the information presentation decisions are notaffected.Seventeen subjects (8 women, 9 men) are given a set of 12 (6?2) predefined, orderedtasks, which they solve by interaction with the RL-based and the SL-based averagewizard system in a cyclic order.
As a secondary task users are asked to count certainobjects in a driving simulation.
In total, 204 dialogues with 1,115 turns are gathered inthis set-up.
See also Rieser (2008).5.2 ResultsIn general, the users rate the RL-based policy significantly higher (p < .001) than theSL-based average wizard policy.
The results from a Wilcoxon Signed Ranks Test onthe user questionnaire data (see Table 9) show significantly improved Task Ease, betterpresentation timing, more agreeable verbal and multimodal presentation, and that moreusers would use the RL-based system in the future (Future Use).
All the observeddifferences have a medium effects size (r ?
|.3|).We also observe that female participants clearly favor the RL-based strategy,whereas the ratings by male participants are more indifferent.
Similar gender effectsare also reported by other studies on multimodal output presentation (e.g., Foster andOberlander 2006; Jokinen and Hurtig 2006).Furthermore, we compare objective dialogue performance measures.
The dialoguesof the RL strategy are significantly shorter (p < .005), while fewer items are dis-played (p < .001), and the help function is used significantly less (p < .003).
The mean9 The WOZ study was performed in German, whereas the user tests are performed in English.
Therefore, adifferent database had to be used and task sets and user questionnaires had to be translated.173Computational Linguistics Volume 37, Number 1performance measures for testing with real users are shown in Table 10.
Also seeexample dialogues in Appendix D. However, there is no significant difference for theperformance of the secondary driving task.6.
Meta EvaluationWe introduce a final phase where we meta-evaluate the whole framework.
This finalstep is necessary because WOZ experiments only simulate HCI.
We therefore need toshow that a strategy bootstrapped from WOZ data indeed transfers to real HCI.
We firstshow that the results for simulated and real interaction are compatible (Section 6.1).
Wealso meta-evaluate the reward function, showing that it is a stable, accurate estimate forreal users?
preferences (Section 6.2).6.1 Transfer Between Simulated and Real EnvironmentsWe first test whether the results obtained in simulation transfer to tests with real users,following Lemon, Georgila, and Henderson (2006).
We evaluate the quality of the simu-lated learning environment by directly comparing the dialogue performance measuresbetween simulated and real interaction.
This comparison enables us to make claimsregarding whether a policy which is ?bootstrapped?
from WOZ data is transferable toreal HCI.
We first evaluate whether objective dialogue measures are transferable, usinga paired t-test, comparing overall mean performance.For the RL policy there is no statistical difference in overall performance (reward),dialogue length (turns), and the number of presented items (verbal and multimodalitems) between simulated and real interaction (see Figure 8).
This fact (that the per-formances are not different) indicates that the learned strategy transfers well to realsettings.
For the SL wizard policy the dialogue length for real users is significantly(t(101) = 5.5, p < .001, r = .48) shorter than in simulation.
We conclude from an er-ror analysis that this length difference is mainly due to the fact that real users tendto provide the most ?informative?
slot value (i.e., the most specific value from theexperimental task description) right at the beginning of the task (and therefore moreefficiently contribute to solve the task), whereas simulated users use a default orderingof slot values and most of the time they provide the slot value that the system was askingfor (provide info).
This difference becomes more prominent for the SL wizard policythan for the RL-based policy, as the SL wizard policy in general asks more questionsbefore presenting the information.
In future work the user simulation therefore shouldlearn optimal slot ordering.Table 10Comparison of results for SL average wizard and RL-based strategies with real users.Measure Wizard Strategy RL Strategyavg.
turns 5.86(?3.2) 5.07(?2.9)***avg.
speech items 1.29(?.4) 1.2(?.4)avg.
MM items 52.2(?68.5) 8.73(?4.4)***avg.
reward ?628.2(?178.6) 37.62(?60.7)****** Significant difference between SL and RL at p < .001 (with standard deviation ?
).174Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsFigure 8Graph comparison of objective measures.
SLs = SL policy in simulation; SLr = SL policy withreal users; RLs = RL policy in simulation; RLr = RL policy with real users.6.2 Evaluation of the Learned Reward FunctionWe propose a new method for meta-evaluation of the reward (or ?objective?)
function.One major advantage of RL-based dialogue strategy development is that the dialoguestrategy can be automatically trained and evaluated using the same objective function(Walker 2005).
Despite its central aspect for RL, quality assurance for objective functionshas received little attention so far.
In fact, as noted in Section 3.6, the reward function isone of the most hand-coded aspects of RL (Paek 2006).Here, we bring together two strands of research for evaluating the reward function:One strand uses Reinforcement Learning to automatically optimize dialogue strategies(e.g., Singh et al 2002; Henderson, Lemon, and Georgila 2008; Rieser and Lemon 2008a,2008b); the other focuses on automatic evaluation of dialogue strategies (e.g., the PAR-ADISE framework [Walker et al 1997]), and meta-evaluation of dialogue metrics (e.g.,Engelbrecht and Mo?ler 2007; Paek 2007).
Clearly, automatic optimization and evaluationof dialogue policies, as well as quality control of the objective function, are closely inter-related problems: How can we make sure that we optimize a system according to realusers?
preferences?In Section 3.6 we constructed a data-driven objective function using the PARADISEframework, and used it for automatic dialogue strategy optimization, following workby Walker, Former, and Narayanan (1998).
However, it is not clear how reliable sucha predictive model is, that is, whether it indeed estimates real user preferences.
The175Computational Linguistics Volume 37, Number 1models obtained with PARADISE often fit the data poorly (Engelbrecht and Mo?ller 2007).It is also not clear how general they are across different systems and user groups(Walker, Kamm, and Litman 2000; Paek 2007).
Furthermore, it is not clear how they per-form when being used for automatic strategy optimization within the RL framework.In the following we evaluate different aspects of the reward function.
In Sec-tion 6.2.1 we test the model stability in a test?retest comparison across different userpopulations and data sets.
In Section 6.2.2 we measure its prediction accuracy.6.2.1 Reward Model Stability.
We first test the reward model?s stability by re-constructingit from the data gathered in the real user tests (see Section 5) and comparing it to theoriginal model constructed from the WOZ data.
By replicating the regression modelon different data sets we test whether the automatic estimate of Task Ease general-izes beyond the conditions and assumptions of a particular experimental design.
Theresulting models are shown in Equations (5)?
(7), where TaskEaseWOZ is the regressionmodel obtained from the WOZ data,10 TaskEaseSL is obtained from the user test datarunning the supervised average wizard policy, and TaskEaseRL is obtained from theuser test data running the RL-based policy.
They all reflect the same trends: Longerdialogues (measured in turns) result in a lower Task Ease, whereas a good performancein the multimodal information presentation phase (multimodal score) will positivelyinfluence Task Ease.
For the user tests almost all the tasks were completed; thereforetask completion was only chosen to be a predictive factor for the WOZ model.TaskEaseWOZ = 1.58 + .12?
taskCompl + .09 ?
mmScore ?
.20?
dialogueLength (5)TaskEaseSL = 3.50 + .54?
mmScore ?
.34 ?
dialogueLength (6)TaskEaseRL = 3.80 + .49 ?
mmScore ?
.36?
dialogueLength (7)To evaluate the obtained regression models we use two measures: how well theyfit the data and how close the functions are to each other (model replicability).
Bothare measured using goodness-of-fit R2.
For the WOZ model the data fit was ratherlow (R2WOZ = .123),11 whereas for the models obtained from the user tests the fit hasimproved (R2RL = .48, and R2SL = .55).Next, we compare how well the models from different data sets fit each other.Although the models obtained from the user test data show almost perfect overlap(R2 = .98), the (reduced) WOZ model differs (R2 = .22) in the sense that it assigns lessweight to dialogue length and the multimodal presentation score, and more weightis assigned to task completion.
Task completion did not play a role for the user tests,as mentioned earlier.
This shows that multimodal presentation and dialogue lengthbecome even more important once the tasks are being completed.
Overall, then, thedata-driven reward model is relatively stable across the different data sets (WOZ, realusers with the SL policy, and real users using the RL policy).6.2.2 Reward Model Performance: Prediction Accuracy.
We now investigate how well thesereward models generalize by testing their prediction accuracy.
Previous research evalu-10 In contrast to the model in Equation (4) we now include the constant in the regression.11 For R2 we use the adjusted values.176Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsTable 11Prediction accuracy for models within (1?3) and across (4?5) data sets.ID train test RMSE % error1 WOZ WOZ 0.82 16.422 SL SL 1.27 18.143 RL RL 1.06 15.144 RL SL 1.23 17.575 SL RL 1.03 14.71ated two aspects: how well a given objective function/reward model is able to predictunseen scores from the original system (Engelbrecht and Mo?ller 2007), and how well itis able to predict unseen scores of a new/different system (Walker, Kamm, and Litman2000).
We evaluate these two aspects as well, the only difference is that we use theRoot Mean Standard Error (RMSE) instead of R2 for measuring the model?s predictionaccuracy.
The RMSE is a frequently used measure of the differences between valuespredicted by a model or an estimator and the values actually observed.
It is defined over[0,?
], where 0 indicates perfect overlap.
The maximum RMSE possible (= worst case)in our set-up is 7 for SL/RL and 5 for WOZ.
In order to present results from differentscales we also report the percentage of the RMSE of the maximum error (% error).
RMSEis (we argue) more robust for small data sets.12R2 = 1?
?ni=1(yi ?
?yi)2?ni=1(y ?
y)(8)RMSE =???
?1nn?i=1(yi ?
?yi)2 (9)First, we measure the predictive power of our models within the same data set using10-fold cross validation, and then across the different systems by testing models trainedon one system to predict perceived Task Ease scores for another system, following amethod introduced by Walker, Kamm, and Litman (2000).The results for comparing the RMSE for training and testing within data sets (ID1-3) and across data sets (ID 4?5) are shown in Table 11.
RMSE measures the average ofthe square of the ?error.?
As such, lower RMSE values are better.
The contrary is truefor R2, where ?1?
indicates perfect overlap between two functions.The results show that predictions according to PARADISE can lead to accurate testresults despite the low data fit.
Whereas for the regression model obtained from theWOZ data the fit was 10 times lower than for SL/RL, the prediction performance iscomparably good (see Table 11, ID 1?3).
The models also generalize well across systems(see Table 11, ID 4?5).Table 12 visualizes the results (ID 1?3): Mean values for predicted and for trueratings are plotted per turn (see Engelbrecht and Mo?ller 2007).
The top two graphs in the12 In particular, we argue that, by correcting for variance, R2 can lead to artificially good results when usingsmall tests sets (which typically vary more) and is sensitive to outliers (see Equation (8)).
RMSE insteadmeasures the (root) mean difference between actual and predicted values (see Equation (9)).177Computational Linguistics Volume 37, Number 1Table 12Average Task Ease ratings for dialogues of different length (in turns); the solid lines are the trueratings and the dashed line the predicted values; from top: RL, SL wizard, WOZ data.178Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applicationstable show that the predicted mean values are fairly accurate for the SL and RL objectivefunctions.
The graph at the bottom indicates that the predictions are less accurate for theWOZ data, especially for low numbers of turns.
This seems to contradict the previousresults in Table 11, which show low error rates for the WOZ data.
However, this is dueto the fact that most of the observations in the WOZ data set are in the region where thepredictions are accurate (i.e., most of the dialogues in the WOZ data are over 14 turnslong, where the curves converge).We conclude that, according to our measures, an objective function obtained fromWOZ data is a valid first estimate of real users?
preferences.
Despite a low fit to the initialdata, the objective function obtained from WOZ data makes accurate and useful predic-tions for automatic dialogue evaluation/reward.
The models obtained from the testswith a real system follow the same trends, but can be seen as more reliable estimatesof the objective function in this domain.
In future work we will explore incrementallytraining a system according to improved representations of real user preferences, forexample, gathered on-line from a deployed spoken dialogue system.7.
ConclusionWe have presented a new data-driven methodology for simulation-based dialoguestrategy learning.
It allows us to address several problems in the field of automaticoptimization of dialogue strategies: learning effective dialogue strategies when no ini-tial data or system exists, and determining a data-driven reward function.
We learnedoptimal strategies by interaction with a simulated environment which is bootstrappedfrom a small amount of Wizard-of-Oz data, and we evaluated the result with real users.The use of WOZ data allows us to develop optimal strategies for domains where noworking prototype is available.
In contrast to previous work, the developed simulationsare largely data-driven and the reward function reflects real user preferences.We compare the Reinforcement Learning?based strategy against a supervised strat-egy which mimics the (human) wizards?
policies from the original data.
This compari-son allows us to measure relative improvement over the training data.
Our results showthat RL significantly outperforms the average wizard strategy in simulation as well as ininteractions with real users.
The RL-based policy gains on average 50 times more rewardwhen tested in simulation, and almost 18 times more reward when interacting withreal users.
The human users also subjectively rate the RL-based policy on average 10%higher, and 49% higher for Task Ease.
We also show that results obtained in simulationare comparable to results for real users and we also evaluate the reward function.
Weconclude that a strategy trained from WOZ data via our bootstrapping method workswell for real HCI.Regarding scaling up such methods to larger databases, we would still quantizethe number of database hits into High, Medium, and Low (more hits would be addedto the ?High?
bin).
In fact, wizards seemed to treat everything over 60 database hitsequivalently (see Section 3.7).
For learning, the state space grows linearly with thenumber of possible database hits.
Techniques such as linear function approximation areuseful for handling the resulting larger state spaces (Henderson, Lemon, and Georgila2008).In ongoing work we apply similar techniques to statistical planning for NaturalLanguage Generation in spoken dialogue (Lemon 2011, 2008; Janarthanam and Lemon2008; Rieser and Lemon 2009b; Rieser, Lemon, and Liu 2010; Janarthanam and Lemon2010), see the EC FP7 CLASSiC project: www.classic-project.org.179Computational Linguistics Volume 37, Number 1Appendix A: Measuring Inter-Annotator Agreement on User Act Using the?
StatisticsCohen?s Kappa (often simply called kappa, ?)
is a measure of agreement between thetwo individuals.
We use this measure to assess inter-annotator agreement on user act,following Carletta et al (1997).
Detailed results are in the table.Table 1Inter-annotator agreement on user act with ?
= .632.annotator1 annotator2user reply add change others repeat y/n totaladd 58 5 2 1 0 66change 4 11 0 0 0 15others 9 5 21 1 0 36repeat 9 0 7 9 0 25y/n 2 0 2 1 30 35total 82 21 32 12 30 177180Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsAppendix B: Wizard-of-Oz Example DialoguesTable 1Example dialogue from the SAMMIE WOZ corpus (mixed wizard strategy).s1 User: Bitte suche das Lied ,Layla?.Transl.
: Search for the song ?Layla?
please.SA* user: add -> song-title(Layla)Wizard: Wir haben das Lied gefunden.
Es existiert 142 mal in verschiedenenAusfu?hrungenTransl.
: The song is retrieved.
There are 142 different versions.mmAction: [ Table with 142 entries is shown on the screen.
]SAwizard:presentMM -> song-title (db:142)s2 User: Bitte suche das Lied on dem Komponisten ,Eric Clapton?.Transl.
: Search for the song by the composer ?Eric Clapton?
please.mmAction:[ User does not react to what is displayed.
]SA user: change -> artist(?Eric Clapton?
)Wizard: Es gibt das Lied von ,Eric Clapton?
oder von ?Derek & the Dominos?.Transl.
: The song exists by ?Eric Clapton?
or by ?Derek & the Dominos?.SAwizard:presentVerbal -> song-title, artist (db:2)s3 User: Bitte spiele das Lied von ,Eric Clapton Unplugged?
ab.Transl.
: Play the song from ?Eric Clapton Unplugged?
please.SA user: repeat ->album(?Eric Clapton Unplugged?
)Wizard: Ich spiele das Lied jetzt ab.Transl.
: OK I am going to play the song now.SAwizard:feedback/othersmmAction: [ music ]userratingstaskEase: 2, timing:2, mmPresent.
: 1 , verbalPresent: 3, futureUse:3* SA = Speech ActComments:In this example the wizard uses a mixed strategy presenting information multimodally in turn 1and verbally in turn 2.
The first time 142 items are presented multimodally by verbally reportingthe number of retrieved items while displaying them on the screen.
The database used in the WOZexperiments contained over a million songs and is searched with partial string match.
Thereforethe retrieved items for ?Layla?
can be as high as 142.
The user does not directly react to what isshown on the screen (either by referring to it or clicking), but continues providing informationuntil the wizard (verbally) presents two alternatives, where the user indirectly chooses oneby providing an album name which also disambiguates the artist.
The user rankings from thequestionnaire are about average.181Computational Linguistics Volume 37, Number 1Table 2Example dialogue from the SAMMIE WOZ corpus (wizard not starting the presentation phaseearly enough).s1 User: Also, ich suche das Lied ,Smile?.Transl.
: Well, I am searching for the song ?Smile?.SA user: add -> song-title(Smile)Wizard: Welches Lied suchst du?Transl.
: What song are you searching for?comment: [ Song name was deleted ]SAwizard:reject(song-title)s2 User: Smile.Transl.
: Smile.SA user: repeat -> song-title(Smile)Wizard: Du suchst das Lied ,Smile??Transl.
: Are you searching for the song ?Smile?
?SAwizard:explConf(song-title)s3 User: Ja, Smile ja.Transl.
: Yes, Smile yes.SA user: y-answer -> song-title(Smile)Wizard: Ich suche es... Ich habe vier Lieder mit dem einfachen Titel ,Smile?.Transl.
: I am searching...
I found four songs with only ?Smile?
as the title.SAwizard:presentVerbal(song-title)s4 User: Ja denn, das Lied bitte anzeigen.Transl.
: Very well then, why don?t you show it?SA user: othersWizard: OK.Transl.
: OK.SAwizard:presentMM(song-title)userratingstaskEase: 2, timing: 2, mmPresent.
: 5, verbalPresent: 3, futureUse:3Comments:In this example the user is asking to see a short list on the screen (after resolving some under-standing errors induced by the deletion tool), which is indicating that (a) the wizard did not enterthe presentation phase early enough, and (b) that for the user four items seems to be a reasonablenumber to display.
The user scores reflect this fact: The presentation mode got a maximal score(which will influence learning towards showing short lists), whereas the task ease got a lowerscore (task ease is used as an overall reward, i.e., this ?tells?
the learner to enter the presentationmode earlier).182Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsAppendix C: Example Dialogues from Simulated InteractionTable 1Example dialogue with the SL policy operating in the simulated environment.s1 SL policy: greet (db:438)sim.User: add->genre(Rock)state: filled [slot1], confirmed [], db:142s2 SL policy: implConfAskASlot -> genre, artist)sim.User: repeat->genre(Rock)state: filled [slot1], confirmed[slot1], db:142s3 SL policy: present[mm] ->genresim.User: change->artist(Jimi Hendrix)state: filled [slot1,slot2], confirmed [slot1], db:4s4 SL policy: implConfAskASlot->artist, albumsim.User: add->album(Experience Hendrix: The Best of Jimi Hendrix)state: filled [slot1,slot2,slot3], confirmed [slot1,slot2], db:4s5 SL policy: implConfAskASlot->album, song-titlesim.User: add->song-title(Voodoo Child)state: filled [slot1,slot2,slot3,slot4], confirmed[slot1,slot2,slot3], db:1s6 SL policy: explicitConfirm->song-titlesim.User: repeat->song-title(Voodoo Child)state: filled [slot1,slot2,slot3,slot4], confirmed[slot1,slot2,slot3,slot4], db:1s7 SL policy: present[verbal] ->genre, artist, album, song-titlesim.User: yes-answer->yes(yes)state filled [slot1,slot2,slot3,slot4], confirmed[slot1,slot2,slot3,slot4], db:1reward: lengthPenalty : ?141.4, complVal : 118, presentVal : 8.7?
(4.33?
2151.23)finalReward:-18701.43Comments:This example shows a dialogue where the SL policy is operating in the simulated environment.Presentation timing and amount of presented information are not ideal (and therefore leading toquite a long dialogue): In turn 2 the system presents a list of 238 items, whereas when only havingfour alternatives (from turn 3 onwards) it fails to show a list.183Computational Linguistics Volume 37, Number 1Table 2Example dialogue with the RL policy operating in the simulated environment (mixed strategy).s1 RL policy: greet (db: 438)sim.User: add->artist(Nirvana)state: filled [slot2], confirmed [], db:26s2 RL policy: implConfAskASlot -> artist, album)sim.User: add->album(MTV Unplugged)state: filled [slot2,slot3], confirmed [slot2], db:14s3 RL policy: present[mm] ->artist, albumsim.User: click->song-title(On a Plain),genre(Alternative)*state: filled [slot1,slot2,slot3,slot4], confirmed[slot2,slot3], db:1s4 RL policy: present[verbal] ->artist, album, song-titlesim.User: yes-answer->yes(yes)state: filled [slot1,slot2,slot3,slot4], confirmed[slot1,slot2,slot3,slot4], db:1reward: lengthPenalty : ?80.8, complVal : 118, presentVal : 8.7?
(4.33 + 7.23)finalReward:140.3Comments:This example shows the RL policy interacting in the simulated environment.
The system followsa mixed strategy: first presenting a list of 14 items on the screen and then presenting a uniqueitem verbally.
* the user acts add after a multimodal presentation are equivalent to clickingTable 3Example dialogue with the RL policy operating in the simulated environment: presentingalternatives.s1 RL policy: greet (db: 438)sim.User: add->artist(Bob Dylan)state: filled [slot2], confirmed [], db:56s2 RL policy: implConf(artist)AskASlot(genre)sim.User: change->song-title(Hurricane)state: filled [slot2,slot4], confirmed [slot2], db:2s3 RL policy: present[verbal] ->album1,album2sim.User: add->album(Desire)state: filled [slot2, slot3, slot4], confirmed [slot2,slot4], db:1s4 RL policy: present[verbal] ->artist, album, song-titlesim.User: yes-answer->yes(yes)state: filled [slot2, slot3, slot4], confirmed[slot2, slot3, slot4], db:1reward: lengthPenalty : ?80.8, complVal : 118, presentVal : 8.7?
(4.327 + 3.395)finalReward: 104.38Comments:This example shows the RL policy interacting in the simulated environment.
The system presentstwo alternatives and a single item using only verbal output.184Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsAppendix D: Example Dialogues from User TestingTable 1Example dialogue with the SL policy operating with real users.s1 SL system: Hi, this is iTalk your talkative music player.
How may I help you today?SA system: greet (db: 438)User: i would like to listen to a blues songSA user: add->genre(Blues)state: filled [slot1], confirmed [], db:175s2 SL system: There are 175 items matching your query.
A list of possible candidates areshown on the screen.SA system: presentMM-> genre (db:175.0)User: ?SA user: nothing (time out)state: filled [slot1], confirmed [], db:175s3 SL system: A Blues song.
By which artist?SA system: implConf(genre), askASlot(artist)User: eric claptonSA user: add->artist(?Eric Clapton) (db:25.0)state: filled [slot1, slot2], confirmed [slot1], db:25s4 SL system: Searching for music by Eric Clapton.
Which album?SA system: implConf(?Eric Clapton?
), askASlot(album)User: mtv unpluggedSA user: add->album(?MTV unplugged?
)state: filled [slot1, slot2, slot3], confirmed [slot1, slot2], db:13s5 SL system: A song from the album MTV unplugged.
What is the title of the song?SA system: implConf(album), askASlot(song-title)User: what do you haveSA user: request-helpstate: filled [slot1, slot2, slot3], confirmed [slot1, slot2], db:13s6 SL system: You have several options.
For example: Layla , Nobody Knows You WhenYou Are Down and Out , Lonely Stranger , Old Love , and San Francisco BayBlues.
Do you want to hear more?SA system: give-helpUser: no layla pleaseSA user: add->song-title(Layla)state: filled [slot1, slot2, slot3, slot4], confirmed[slot1, slot2, slot3], db:1s7 SL system: You will now hear Layla by Eric Clapton from the album MTV unplugged.Are you happy with this option?SA system: presentVerbal->genre,artist,album,song-titleUser: yesSA user: yes-answerstate: filled [slot1, slot2, slot3, slot4], confirmed[slot1, slot2, slot3, slot4], db:1reward: lengthPenalty : ?141.4, complVal : 118, presentVal : 8.7?
(4.33?
7.63)finalReward: -6577.03user ratings taskEase:2, timing:2, mmPresent:1, verbalPresent:2, futureUse:2Comments:This example shows the SL policy applied with real users.
In turn 2 the user ignores the 175 items displayed,then the system does not display the 13 retrieved items.
The user asks for help.185Computational Linguistics Volume 37, Number 1Table 2Example dialogue with the RL policy operating with real users: multimodal and verbalpresentation.s1 RL system: Hi.
this is iTalk your talkative music player.
How may I help you today?SA system greet (db:438)User: play me some bluesSA user: add->genre(Blues) (db:175)state: filled [slot1], confirmed [], db:175s2 RL system: Blues for type of music.
By which artist?SA system: implConf(genre),askASlot(artist)User: from the album desireSA user: change->album(Desire)(db:10)state: filled [slot1, slot3], confirmed [slot1], db:10s3 RL system: There are 10 items matching your query.
A list of possible candidates areshown on the screen.SA system: presentMM->genre,albumUser: [clicks]SA user: click->genre,song-titlestate: filled [slot1, slot2, slot3, slot4], confirmed [slot1, slot3], db:1s4 RL system: You selected an item on the screen.
Please wait a few moments.
[.
.
. ]
You willnow hear Mozambique by Bob Dylan from the album Desire.
Are you happywith this optionSA presentVerbal->genre,album,artist,song-titleUser: yesSA yes-answerstate: filled [slot1, slot2, slot3, slot4], confirmed[slot1, slot2, slot3, slot4], db:1reward: lengthPenalty : ?60.6, complVal : 118, presentVal : 8.7?
(4.33 + 6.57)finalReward: 152.16user ratings taskEase:6, timing:7, mmPresent.
:7 verbalPresent: 6 futureUse:6Comments:This example shows the RL policy interacting with real users.
The system uses a mixed strategy, showingmultimodal output as soon as the number of retrieved items is below 15 (and therefore the reward will bepositive), and presenting one item verbally.186Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsTable 3Example dialogue with the RL policy operating with real users: Verbal presentation of options.s1 RL system: Hi.
this is iTalk your talkative music player.
How may I help you today?SA system: greet (db:438)User: i want to hear pollySA user: add->song-title(Polly)state: filled [slot4], confirmed [], db:2s2 RL system: There are 2 results matching your query all by the artist Nirvana all from thegenre Alternative all having the song title Polly, but a different album.
Youhave the following options: Nevermind and MTV Unplugged.
Which albumwould you like?SA system: presentVerbal->song-title,artist,genrecomment: systems fills unique slots.User: mtv unpluggedSA user: add->album(?MTV unplugged?)
(db:1)state: filled [slot3, slot4], confirmed [song4], db:1s3 RL system: There is only one result matching your query.
Please wait a few moments.[.
.
. ]
You will now hear Polly by Nirvana from the album MTV unplugged.Are you happy with this option ?SA system: presentVerbal->song-title,artist,genre,albumUser: yesSA user: yes-answerstate: filled [slot1, slot2, slot3, slot4], confirmed[slot1, slot2, slot3, slot4], db:1reward: lengthPenalty : ?60.6, complVal : 118, presentVal : 8.7?
(4.33 + 3.4)finalReward: 124.6user ratings taskEase:7, timing:7, mmPresent.
:7 verbalPresent: 7 futureUse:6Comments:This example shows the RL policy interacting with real users.
Two alternatives are presented verbally.187Computational Linguistics Volume 37, Number 1Appendix E: Learned State Action MappingsThe table in this section presents the learned state action mappings, and is to be readas follows.
The first two columns constitute the state space.
The first column shows theslots that have been filled and/or confirmed.
The slots are:slot 1: genreslot 2: artistslot 3: albumslot 4: song titleThe second column represents possible numbers of database hits.
Note that the possiblenumber of items returned from the database is constrained by the structure of the task(i.e., how combinations of different slots values constrain the search space).The third column is the optimal action for that state.
The ?x?s in the second columndenote the numbers of database hits that share the same optimal action (given the setof filled and confirmed slots).
Horizontal lines are drawn between sets of states withdifferent filled slots.188Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsLearnedStateActionMappingsstatespacesystemactionslotvaluesno.DBhits123456789101112131416182025263440545692142175438filled[],confirmed[]xaskASlotfilled[slot1],confirmed[]xxximplConffilled[slot1],confirmed[slot1]xxxaskASlotfilled[slot2],confirmed[]xxxxxximplConfxxxpresentMMfilled[slot2],confirmed[slot2]xxxxxxaskASlotxxxpresentMMfilled[slot3],confirmed[]xxxximplConfxxxxxxxxxxxpresentMMfilled[slot3],confirmed[slot3]xxxxaskASlotxxxxxxxxxxxpresentMMfilled[slot4],confirmed[]xexplConfxxxpresentVerbalfilled[slot4],confirmed[slot4]xaskASlotxxxpresentVerbalfilled[slot1,slot2],confirmed[]xxxxxxxximplConffilled[slot1,slot2],confirmed[slot1,slot2]xxxxxxaskASlotxxpresentMMfilled[slot1,slot2],confirmed[slot1]xxxxxximplConfxxpresentMMfilled[slot1,slot2],confirmed[slot2]xxxxxximplConfxxpresentMMfilled[slot1,slot3],confirmed[]xxxximplConfxxxxxxxxxxpresentMMfilled[slot1,slot3],confirmed[slot1,slot3]xxxxaskASlotxxxxxxxxxxpresentMMfilled[slot1,slot3],confirmed[slot1]xxxximplConfxxxxxxxxxxpresentMMfilled[slot1,slot3],confirmed[slot3]xxxximplConfxxxxxxxxxxpresentMMfilled[slot1,slot4],confirmed[]xximplConfxxpresentVerbalfilled[slot1,slot4],confirmed[slot1,slot4]xaskASlotxxxpresentVerbalfilled[slot1,slot4],confirmed[slot1]ximplConfxxxpresentVerbalfilled[slot1,slot4],confirmed[slot4]ximplConfxxxpresentVerbal189Computational Linguistics Volume 37, Number 1LearnedStateActionMappingsstatespacesystemactionslotvaluesno.DBhits123456789101112131416182025263440545692142175438filled[slot2,slot3],confirmed[]xxxxxxxxxximplConffilled[slot2,slot3],confirmed[slot2,slot3]xxaskASlotxxxxxxxxxxpresentMMfilled[slot2,slot3],confirmed[slot2]xximplConfxxxxxxxxxxpresentMMfilled[slot2,slot3],confirmed[slot3]xximplConfxxxxxxxxxxpresentMMfilled[slot2,slot4],confirmed[]xxxexplConfxpresentVerbalfilled[slot2,slot4],confirmed[slot2,slot4]xxxpresentVerbalfilled[slot2,slot4],confirmed[slot2]xpresentMMxxxpresentVerbalfilled[slot2,slot4],confirmed[slot4]xpresentMMxxxpresentVerbalfilled[slot3,slot4],confirmed[]xpresentVerbalfilled[slot3,slot4],confirmed[slot3,slot4]xpresentVerbalfilled[slot3,slot4],confirmed[slot3]xpresentVerbalfilled[slot3,slot4],confirmed[slot4]xpresentVerbalfilled[slot1,slot2,slot3],confirmed[]xxximplConfxxxxxxxxxxxpresentMMfilled[slot1,slot2,slot3],confirmed[slot1,slot2,slot3]xxxaskASlotxxxxxxxxxxxxxxpresentMMfilled[slot1,slot2,slot3],confirmed[slot1,slot2]xxximplConfxxxxxxxxxxxxxxpresentMMfilled[slot1,slot2,slot3],confirmed[slot1,slot3]xxximplConfxxxxxxxxxxxpresentMMfilled[slot1,slot2,slot3],confirmed[slot1]xxxxxxxxxxxxxximplConffilled[slot1,slot2,slot3],confirmed[slot2,slot3]xxximplConfxxxxxxxxxxxpresentMMfilled[slot1,slot2,slot3],confirmed[slot2]xxxxxxxxxxxxxximplConffilled[slot1,slot2,slot3],confirmed[slot3]xxxxxxxxxxxxxximplConffilled[slot1,slot2,slot4],confirmed[]xximplConfxxpresentVerbalfilled[slot1,slot2,slot4],confirmed[slot1,slot2,slot4]xpresentMMxxxpresentVerbalfilled[slot1,slot2,slot4],confirmed[slot1,slot2]xpresentMMxxxpresentVerbalfilled[slot1,slot2,slot4],confirmed[slot1,slot4]xpresentMMxxpresentVerbalfilled[slot1,slot2,slot4],confirmed[slot1]xpresentMMxxxpresentVerbalfilled[slot1,slot2,slot4],confirmed[slot2,slot4]xpresentMMxxxpresentVerbalfilled[slot1,slot2,slot4],confirmed[slot2]xpresentMMxxxpresentVerbalfilled[slot1,slot2,slot4],confirmed[slot4]xpresentMMxxxpresentVerbal190Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsLearnedStateActionMappingsstatespacesystemactionslotvaluesno.DBhits123456789101112131416182025263440545692142175438filled[slot2,slot3,slot4],confirmed[]xexplConffilled[slot2,slot3,slot4],confirmed[slot2,slot3,slot4]xpresentVerbalfilled[slot2,slot3,slot4],confirmed[slot2,slot3]xpresentVerbalfilled[slot2,slot3,slot4],confirmed[slot2,slot4]xpresentVerbalfilled[slot2,slot3,slot4],confirmed[slot2]xpresentVerbalfilled[slot2,slot3,slot4],confirmed[slot3,slot4]xpresentVerbalfilled[slot2,slot3,slot4],confirmed[slot3]xpresentVerbalfilled[slot2,slot3,slot4],confirmed[slot4]xpresentVerbalfilled[slot1,slot2,slot3,slot4],confirmed[]xexplConffilled[slot1,slot2,slot3,slot4],confirmed[slot1,slot2,slot3]xpresentVerbalfilled[slot1,slot2,slot3,slot4],confirmed[slot1,slot2,slot4]xpresentVerbalfilled[slot1,slot2,slot3,slot4],confirmed[slot1,slot2]xpresentVerbalfilled[slot1,slot2,slot3,slot4],confirmed[slot1,slot3,slot4]xpresentVerbalfilled[slot1,slot2,slot3,slot4],confirmed[slot1,slot3]xpresentVerbalfilled[slot1,slot2,slot3,slot4],confirmed[slot1,slot4]xpresentVerbalfilled[slot1,slot2,slot3,slot4],confirmed[slot1]xexplConffilled[slot1,slot2,slot3,slot4],confirmed[slot2,slot3,slot4]xpresentVerbalfilled[slot1,slot2,slot3,slot4],confirmed[slot2,slot3]xpresentVerbalfilled[slot1,slot2,slot3,slot4],confirmed[slot2,slot4]xpresentVerbalfilled[slot1,slot2,slot3,slot4],confirmed[slot2]xexplConffilled[slot1,slot2,slot3,slot4],confirmed[slot3,slot4]xpresentVerbalfilled[slot1,slot2,slot3,slot4],confirmed[slot3]xexplConffilled[slot1,slot2,slot3,slot4],confirmed[slot4]xexplConf191Computational Linguistics Volume 37, Number 1AcknowledgmentsWe would like to thank Professor ManfredPinkal for discussing this work.
We wouldalso like to thank the anonymous reviewersfor their useful comments.
The researchleading to these results has received fundingfrom the European Community?s SeventhFramework Programme (FP7, 2007?2013)under grant agreement number 216594(?Computational Learning in AdaptiveSystems for Spoken Conversation,?
CLASSiCproject: www.classic-project.org), the ECFP6 project ?TALK: Talk and Look, Tools forAmbient Linguistic Knowledge?
(IST 507802,www.talk-project.org), from EPSRC grantnumbers EP/E019501/1 and EP/G069840/1,and from the International Research TrainingGroup in Language Technology andCognitive Systems, Saarland University.ReferencesAi, Hua and Diane Litman.
2006.
Comparingreal-real, simulated-simulated, andsimulated-real spoken dialogue corpora.In Proceedings of the AAAI Workshop onStatistical and Empirical Approaches forSpoken Dialogue Systems, Boston, MA.Ai, Hua, Joel Tetreault, and Diane Litman.2007.
Comparing user simulation modelsfor dialog strategy learning.
In Proceedingsof the North American Meeting of theAssociation of Computational Linguistics(NAACL), pages 1?4, Rochester, NY.Bohus, Dan, Brian Langner, Antoine Raux,Alan W. Black, Maxine Eskenazi, and AlexRudnicky.
2006.
Online supervisedlearning of non-understanding recoverypolicies.
In Proceedings of the IEEE/ACLworkshop on Spoken Language Technology(SLT), Aruba, pages 170?173.Carletta, Jean, Amy Isard, Stephen Isard,Jacqueline C. Kowtko, GwynethDoherty-Sneddon, and Anne H. Anderson.1997.
The reliability of a dialogue structurecoding scheme.
Computational Linguistics,23(1):13?31.Carpenter, Paul, Chun Jin, Daniel Wilson,Rong Zhang, Dan Bohus, and Alexander I.Rudnicky.
2001.
Is this conversation ontrack?
In Proceedings of the EuropeanConference on Speech Communication andTechnology (Eurospeech), page 2121,Aalborg.Clark, Herbert.
1996.
Using Language.Cambridge University Press.Cohen, William W. 1995.
Fast effective ruleinduction.
In Proceedings of the 12thInternational Conference on MachineLearning (ICML), pages 115?123, TahoeCity, CA.Cuaya?huitl, Heriberto, Steve Renals, OliverLemon, and Hiroshi Shimodaira.
2005.Human?Computer dialogue simulationusing Hidden Markov Models.
InProceedings of the IEEE workshop onAutomatic Speech Recognition andUnderstanding (ASRU), pages 290?295,San Juan.Dempster, A., N. Laird, and D. Rubin.
1977.Maximum likelihood from incompletedata via the EM algorithm.
Journal ofRoyal Statistical Society B, 39:1?38.Eckert, W., E. Levin, and R. Pieraccini.
1997.User modeling for spoken dialoguesystem evaluation.
In Proceedings of theIEEE workshop on Automatic SpeechRecognition and Understanding (ASRU),pages 80?87, Santa Barbara, CA.Eckert, W., E. Levin, and R. Pieraccini.1998.
Automatic evaluation of spokendialogue systems.
In Proceedings of Formalsemantics and pragmatics of dialogue(TWLT13), pages 99?110, Twente, NL.Engelbrecht, Klaus-Peter and SebastianMo?ller.
2007.
Pragmatic usage of linearregression models for the predictions ofuser judgements.
In Proceedings of the8th SIGdial workshop on Discourse andDialogue, pages 291?294, Antwerp.Foster, Mary Ellen and Jon Oberlander.2006.
Data-driven generation ofemphatic facial displays.
In Proceedingsof the 11th Conference of the EuropeanChapter of the Association forComputational Linguistics (EACL),pages 353?360, Trento.Frampton, Matthew and Oliver Lemon.2009.
Recent research advances inReinforcement Learning in SpokenDialogue Systems.
Knowledge EngineeringReview, 24(4):375?408.Fraser, Norman M. and G. Nigel Gilbert.1991.
Simulating speech systems.
ComputerSpeech and Language, 5:81?99.Gabsdil, Malte and Oliver Lemon.
2004.Combining acoustic and pragmaticfeatures to predict recognitionperformance in spoken dialogue systems.In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics(ACL), pages 343?350, Barcelona.Georgila, K., J. Henderson, and O. Lemon.2006.
User simulation for spoken dialoguesystems: Learning and evaluation.
InProceedings of the International Conferenceof Spoken Language Processing (Interspeech/ICSLP), Pittsburgh, PA, pages 1065?1068.192Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New ApplicationsGeorgila, Kallirroi, James Henderson, andOliver Lemon.
2005.
Learning usersimulations for information state updatedialogue systems.
In Proceedings of theInternational Conference of Spoken LanguageProcessing (Interspeech/ICSLP),pages 893?896, Lisbon, Portugal.Georgila, Kallirroi, Oliver Lemon, JamesHenderson, and Johanna Moore.
2009.Automatic annotation of context andspeech acts for dialogue corpora.Natural Language Engineering,15(3):315?353.Hajdinjak, Melita and France Mihelic.
2006.The PARADISE evaluation framework:Issues and findings.
ComputationalLinguistics, 32(2):263?272.Hall, Mark.
2000.
Correlation-based featureselection for discrete and numeric classmachine learning.
In Proceedings of the17th International Conference on MachineLearning (ICML), pages 359?366,San Francisco, CA.Heeman, Peter.
2007.
CombiningReinforcement Learning withinformation-state update rules.
InProceedings of the North American Chapter ofthe Association for Computational LinguisticsAnnual Meeting (NAACL), pages 268?275,Rochester, NY.Henderson, James, Oliver Lemon, andKallirroi Georgila.
2008.
HybridReinforcement/Supervised Learning ofdialogue policies from fixed datasets.Computational Linguistics, 34(4):487?513.Hirschberg, Julia, Diane Litman, and MarcSwerts.
2001.
Identifying user correctionsautomatically in spoken dialogue systems.In Proceedings of the North American Meetingof the Association of ComputationalLinguistics (NAACL), pages 1?8,Pittsburgh, PA.Janarthanam, Srini and Oliver Lemon.
2008.User simulations for online adaptation andknowledge-alignment in troubleshootingdialogue systems.
In Proceedings of the 12thSEMdial Workshop on the Semantics andPragmatics of Dialogues, pages 149?156,London, UK.Janarthanam, Srini and Oliver Lemon.
2010.Learning to adapt to unknown users:Referring expression generation in SpokenDialogue Systems.
In Proceedings of theAnnual Meeting of the Association forComputational Linguistics (ACL),pages 69?78, Uppsala.Jokinen, Kristiina and Topi Hurtig.
2006.User expectations and real experienceon a multimodal interactive system.In Proceedings of the International Conferenceof Spoken Language Processing (Interspeech/ICSLP), pages 1049?1055.Jung, Sangkeun, Cheongjae Lee, KyungdukKim, Minwoo Jeong, and Gary GeunbaeLee.
2009.
Data-driven user simulation forautomated evaluation of spoken dialogsystems.
Computer Speech & Language,23:479?509.Jurafsky, Daniel and James H. Martin.
2000.Speech and Language Processing.
AnIntroduction to Natural Language Processing,Computational Linguistics, and SpeechRecognition.
Prentice-Hall: New Jersey, US.Kruijff-Korbayova?, Ivana, Tilmann Becker,Nate Blaylock, Ciprian Gerstenberger,Michael Kaisser, Peter Poller, VerenaRieser, and Jan Schehl.
2006.
The SAMMIEcorpus of multimodal dialogues with anMP3 player.
In Proceedings of the 5thInternational Conference on LanguageResources and Evaluation (LREC),pages 2018?2023, Genoa, Italy.Lemon, Oliver.
2008.
Adaptive naturallanguage generation in dialogue usingReinforcement Learning.
In Proceedings ofthe 12th SEMdial Workshop on the Semanticsand Pragmatics of Dialogues, pages 149?156,London, UK.Lemon, Oliver.
(2011).
Learning whatto say and how to say it: joint optimizationof spoken dialogue management andNatural Language Generation.
ComputerSpeech and Language, 2(2):210?221.Lemon, Oliver, Kallirroi Georgila, and JamesHenderson.
2006.
Evaluating effectivenessand portability of Reinforcement Learneddialogue strategies with real users: TheTALK TownInfo evaluation.
In Proceedingsof the IEEE/ACL Workshop on SpokenLanguage Technology (SLT), Aruba,pages 170?173.Lemon, Oliver, Kallirroi Georgila, JamesHenderson, Malte Gabsdil, IvanMeza-Ruiz, and Steve Young.
2005.Deliverable D4.1: Integration of learningand adaptivity with the ISU approach.Technical report, TALK Project.Lemon, Oliver and Ioannis Konstas.
2009.User simulations for context-sensitivespeech recognition in spoken dialoguesystems.
In European Conference of theAssociation for Computational Linguistics,pages 505?513, Athens.Lemon, Oliver, Xingkun Liu, Daniel Shapiro,and Carl Tollander.
2006.
HierarchicalReinforcement Learning of dialoguepolicies in a development environment fordialogue systems: REALL-DUDE.
In193Computational Linguistics Volume 37, Number 1Proceedings of the 10th SEMdial Workshop onthe Semantics and Pragmatics of Dialogues,pages 185?186, Potsdam, Germany.Lemon, Oliver and Olivier Pietquin.
2007.Machine Learning for spoken dialoguesystems.
In Proceedings of the InternationalConference of Spoken Language Processing(Interspeech/ICSLP), pages 2685?2688,Antwerp, Belgium.Levin, E., R. Pieraccini, and W. Eckert.
2000.A stochastic model of human?machineinteraction for learning dialog strategies.IEEE Transactions on Speech and AudioProcessing, 8(1), pages 11?23.Levin, Esther and Rebecca Passonneau.
2006.A WOz variant with contrastiveconditions.
In Proceedings Dialog-on-DialogWorkshop, Interspeech, Pittsburgh, PA.Litman, D. and S. Pan.
1999.
Empiricallyevaluating an adaptable spokendialogue system.
In Proceedings of the7th International Conference on UserModeling, pages 55?64, Banff.Mattes, Stefan.
2003.
The Lane-Change-taskas a tool for driver distraction evaluation.In H. Strasser, K. Kluth, H. Rausch, andH.
Bubb, editors, Quality of Work andProducts in Enterprises of the Future.Ergonomica Verlag, Stuttgart, Germany,pages 57?60.Mo?ller, Sebastian, Roman Englert, KlausEngelbrecht, Verena Hafner, AnthonyJameson, Antti Oulasvirta, AlexanderRaake, and Norbert Reithinger.
2006.MeMo: Towards automatic usabilityevaluation of spoken dialogue servicesby user error simulations.
In Proceedings ofthe International Conference of SpokenLanguage Processing (Interspeech/ICSLP),Pittsburgh, PA, pages 1786?1789.Paek, Tim.
2006.
Reinforcement Learning forspoken dialogue systems: Comparingstrengths and weaknesses for practicaldeployment.
In Proceedings Dialog-on-Dialog Workshop, Interspeech, Pittsburgh, PA.Paek, Tim.
2007.
Toward evaluation thatleads to best practices: Reconcilingdialogue evaluation in research andindustry.
In Proceedings of the NAACL-HLTWorkshop on Bridging the Gap: Academic andIndustrial Research in Dialog Technologies,Rochester, NY, pages 40?47.Pietquin, O. and T. Dutoit.
2006.
Aprobabilistic framework for dialogsimulation and optimal strategy learning.IEEE Transactions on Audio, Speech andLanguage Processing, 14(2):589?599.Prommer, Thomas, Hartwig Holzapfel, andAlex Waibel.
2006.
Rapid simulation-driven Reinforcement Learning ofmultimodal dialog strategies in human-robot interaction.
In Proceedings of theInternational Conference of Spoken LanguageProcessing (Interspeech/ICSLP), Pittsburgh,PA, pages 1918?1924.Quinlan, Ross.
1993.
C4.5: Programs forMachine Learning.
Morgan Kaufmann,San Francisco.Rieser, Verena.
2008.
BootstrappingReinforcement Learning-based DialogueStrategies from Wizard-of-Oz data.
Ph.D.thesis, Saarbrueken Dissertations inComputational Linguistics and LanguageTechnology, Vol 28.Rieser, Verena, Ivana Kruijff-Korbayova?, andOliver Lemon.
2005.
A corpus collectionand annotation framework for learningmultimodal clarification strategies.
InProceedings of the 6th SIGdial Workshop onDiscourse and Dialogue, pages 97?106,Lisbon.Rieser, Verena and Oliver Lemon.
2006a.Cluster-based user simulations forlearning dialogue strategies.
In Proceedingsof the 9th International Conference of SpokenLanguage Processing (Interspeech/ICSLP),Pittsburgh, PA.Rieser, Verena and Oliver Lemon.
2006b.Using Machine Learning to explore humanmultimodal clarification strategies.
InProceedings of the 21st InternationalConference on Computational Linguistics and44th Annual Meeting of the Association forComputational Linguistics (COLING/ACL),pages 659?666, Sydney.Rieser, Verena and Oliver Lemon.
2008a.Does this list contain what you weresearching for?
Learning adaptive dialoguestrategies for interactive questionanswering.
Natural Language Engineering,15(1):55?72.Rieser, Verena and Oliver Lemon.
2008b.Learning effective multimodal dialoguestrategies from Wizard-of-Oz data:Bootstrapping and evaluation.
InProceedings of the 21st InternationalConference on Computational Linguistics and46th Annual Meeting of the Association forComputational Linguistics (ACL/HLT),pages 638?646, Columbus, OH.Rieser, Verena and Oliver Lemon.
2009a.Learning human multimodal dialoguestrategies.
Natural Language Engineering,16(1):3?23.Rieser, Verena and Oliver Lemon.
2009b.Natural Language Generation as planningunder uncertainty for Spoken DialogueSystems.
In Proceedings of the Conference of194Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applicationsthe European Chapter of the ACL (EACL),pages 683?691, Athens.Rieser, Verena, Oliver Lemon, and XingkunLiu.
2010.
Optimising informationpresentation for Spoken DialogueSystems.
In Proceedings of the AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 1009?1018,Uppsala.Rieser, Verena and Johanna Moore.
2005.Implications for generating clarificationrequests in task-oriented dialogues.
InProceedings of the 43rd Annual Meetingof the Association for ComputationalLinguistics (ACL), pages 239?246,Ann Arbor, MI.Rodriguez, Kepa J. and David Schlangen.2004.
Form, intonation and functionof clarification requests in Germantask-orientated spoken dialogues.
InProceedings of the 8th SEMdial Workshop onthe Semantics and Pragmatics of Dialogues,pages 101?108, Barcelona.Roque, Antonio and David Traum.
2008.Degrees of grounding based onevidence of understanding.
InProceedings of SIGdial, pages 54?63,Columbus, OH.Schatzmann, J., K. Weilhammer, M. Stuttle,and S. Young.
2006.
A survey of statisticaluser simulation techniques forreinforcement-learning of dialoguemanagement strategies.
KnowledgeEngineering Review, 21(2):97?126.Schatzmann, Jost, Kallirroi Georgila, andSteve Young.
2005.
Quantitative evaluationof user simulation techniques for SpokenDialogue Systems.
In Proceedings of the 6thSIGdial Workshop on Discourse and Dialogue,pages 45?54, Lisbon.Schatzmann, Jost, Blaise Thomson, KarlWeilhammer, Hui Ye, and Steve Young.2007.
Agenda-based user simulation forbootstrapping a POMDP dialogue system.In Proceedings of the North American Meetingof the Association of ComputationalLinguistics (NAACL), pages 149?152,Rochester, NY.Schatzmann, Jost, Blaise Thomson, and SteveYoung.
2007a.
Error simulation for trainingstatistical dialogue systems.
In Proceedingsof the IEEE workshop on Automatic SpeechRecognition and Understanding (ASRU),pages 526?531, Kyoto.Schatzmann, Jost, Blaise Thomson, and SteveYoung.
2007b.
Statistical user simulationwith a hidden agenda.
In Proceedings of the8th SIGdial Workshop on Discourse andDialogue, pages 273?282, Antwerp.Scheffler, Konrad and Steve Young.
2001.Corpus-based dialogue simulation forautomatic strategy learning andevaluation.
In Proceedings NAACLWorkshop on Adaptation in Dialogue Systems,Pittsburgh, PA, pages 64?70.Shapiro, Dan and P. Langley.
2002.Separating skills from preference: Usinglearning to program by reward.
InProceedings of the 19th InternationalConference on Machine Learning (ICML),pages 570?577, Sydney.Singh, S., D. Litman, M. Kearns, andM.
Walker.
2002.
Optimizing dialoguemanagement with ReinforcementLearning: Experiments with the NJFunsystem.
JAIR, 16:105?133.Skantze, Gabriel.
2005.
Exploring humanerror recovery strategies: Implications forspoken dialogue systems.
SpeechCommunication, 43(3):325?341.Stuttle, Matthew N., Jason D. Williams, andSteve Young.
2004.
A framework fordialogue data collection with a simulatedASR channel.
In Proceedings of theInternational Conference of Spoken LanguageProcessing (Interspeech/ICSLP), Jeju.Sutton, R. and A. Barto.
1998.
ReinforcementLearning.
MIT Press, Cambridge, MA.Walker, M., C. Kamm, and D. Litman.2000.
Towards developing generalmodels of usability with PARADISE.Natural Language Engineering, 6(3),pages 363?377.Walker, Marilyn.
2000.
An application forReinforcement Learning to dialoguestrategy selection in a spoken dialoguesystem for email.
Artificial IntelligenceResearch, 12:387?416.Walker, Marilyn.
2005.
Can we talk?
Methodsfor evaluation and training of spokendialogue system.
Language Resources andEvaluation, 39(1):65?75.Walker, Marilyn, Jeanne Fromer, andShrikanth Narayanan.
1998.
Learningoptimal dialogue strategies: A case studyof a spoken dialogue agent for email.In Proceedings of the 36th Annual Meetingof the Association for ComputationalLinguistics and 17th InternationalConference on Computational Linguistics(ACL/COLING), pages 1345?1351,Montreal.Walker, Marylin, Diane Litman, CandaceKamm, and Alicia Abella.
1997.PARADISE: A general framework forevaluating spoken dialogue agents.
InProceedings of the 35th Annual GeneralMeeting of the Association for195Computational Linguistics Volume 37, Number 1Computational Linguistics (ACL),pages 271?280, Madrid.Williams, J. and S. Young.
2007.
Partiallyobservable Markov decision processes forSpoken Dialog Systems.
Computer Speechand Language, 21(2):231?422.Williams, Jason.
2007.
A method forevaluating and comparing usersimulations: The Cramer?von Misesdivergence.
In Proceedings of the IEEEWorkshop on Automatic Speech Recognitionand Understanding (ASRU), pages 508?513,Kyoto, Japan.Williams, Jason and Steve Young.
2004.
UsingWizard-of-Oz simulations to bootstrapReinforcement-Learning-based dialogmanagement systems.
In Proceedings of the4th SIGDIAL Workshop on Discourse andDialogue, pages 135?139, Sapporo, Japan.Witten, Ian H. and Eibe Frank.
2005.
DataMining: Practical Machine Learning Tools andTechniques (2nd Edition).
Morgan Kaufmann,San Francisco.Young, Steve.
2000.
Probabilistic methods inspoken dialogue systems.
PhilosophicalTrans Royal Society (Series A),358(1769):1389?1402.Young, Steve, M. Gasic, S. Keizer, F. Mairesse,J.
Schatzmann, B. Thomson, and K. Yu.2009.
The Hidden Information StateModel: A practical framework forPOMDP-based spoken dialoguemanagement.
Computer Speech andLanguage, 24(2):150?174.196
