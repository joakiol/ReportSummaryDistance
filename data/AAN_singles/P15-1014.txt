Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 136?145,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Word Representations by Jointly ModelingSyntagmatic and Paradigmatic RelationsFei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu, and Xueqi ChengCAS Key Lab of Network Data Science and TechnologyInstitute of Computing TechnologyChinese Academy of Sciences, Chinaofey.sunfei@gmail.com{guojiafeng,lanyanyan,junxu,cxq}@ict.ac.cnAbstractVector space representation of words hasbeen widely used to capture fine-grainedlinguistic regularities, and proven to besuccessful in various natural language pro-cessing tasks in recent years.
However,existing models for learning word repre-sentations focus on either syntagmatic orparadigmatic relations alone.
In this pa-per, we argue that it is beneficial to jointlymodeling both relations so that we can notonly encode different types of linguisticproperties in a unified way, but also boostthe representation learning due to the mu-tual enhancement between these two typesof relations.
We propose two novel dis-tributional models for word representationusing both syntagmatic and paradigmaticrelations via a joint training objective.
Theproposed models are trained on a publicWikipedia corpus, and the learned rep-resentations are evaluated on word anal-ogy and word similarity tasks.
The re-sults demonstrate that the proposed mod-els can perform significantly better thanall the state-of-the-art baseline methods onboth tasks.1 IntroductionVector space models of language represent eachword with a real-valued vector that captures bothsemantic and syntactic information of the word.The representations can be used as basic featuresin a variety of applications, such as information re-trieval (Manning et al, 2008), named entity recog-nition (Collobert et al, 2011), question answer-ing (Tellex et al, 2003), disambiguation (Sch?utze,1998), and parsing (Socher et al, 2011).A common paradigm for acquiring such repre-sentations is based on the distributional hypothe-sis (Harris, 1954; Firth, 1957), which states thatiswolfTheafierceanimal.istigerTheafierceanimal.syntagmaticsyntagmaticparadigmaticFigure 1: Example for syntagmatic and paradig-matic relations.words occurring in similar contexts tend to havesimilar meanings.
Based on this hypothesis, vari-ous models on learning word representations havebeen proposed during the last two decades.According to the leveraged distributional infor-mation, existing models can be grouped into twocategories (Sahlgren, 2008).
The first categorymainly concerns the syntagmatic relations amongthe words, which relate the words that co-occurin the same text region.
For example, ?wolf?
isclose to ?fierce?
since they often co-occur in a sen-tence, as shown in Figure 1.
This type of modelslearn the distributional representations of wordsbased on the text region that the words occur in, asexemplified by Latent Semantic Analysis (LSA)model (Deerwester et al, 1990) and Non-negativeMatrix Factorization (NMF) model (Lee and Se-ung, 1999).
The second category mainly cap-tures paradigmatic relations, which relate wordsthat occur with similar contexts but may not co-occur in the text.
For example, ?wolf?
is closeto ?tiger?
since they often have similar contextwords.
This type of models learn the word rep-resentations based on the surrounding words, asexemplified by the Hyperspace Analogue to Lan-guage (HAL) model (Lund et al, 1995), Con-tinuous Bag-of-Words (CBOW) model and Skip-Gram (SG) model (Mikolov et al, 2013a).In this work, we argue that it is important to136take both syntagmatic and paradigmatic relationsinto account to build a good distributional model.Firstly, in distributional meaning acquisition, itis expected that a good representation should beable to encode a bunch of linguistic properties.For example, it can put semantically related wordsclose (e.g., ?microsoft?
and ?office?
), and also beable to capture syntactic regularities like ?big isto bigger as deep is to deeper?.
Obviously, theselinguistic properties are related to both syntag-matic and paradigmatic relations, and cannot bewell modeled by either alone.
Secondly, syntag-matic and paradigmatic relations are complimen-tary rather than conflicted in representation learn-ing.
That is relating the words that co-occur withinthe same text region (e.g., ?wolf?
and ?fierce?
aswell as ?tiger?
and ?fierce?)
can better relate wordsthat occur with similar contexts (e.g., ?wolf?
and?tiger?
), and vice versa.Based on the above analysis, we propose twonew distributional models for word representa-tion using both syntagmatic and paradigmatic re-lations.
Specifically, we learn the distributionalrepresentations of words based on the text region(i.e., the document) that the words occur in as wellas the surrounding words (i.e., word sequenceswithin some window size).
By combining thesetwo types of relations either in a parallel or a hier-archical way, we obtain two different joint trainingobjectives for word representation learning.
Weevaluate our new models in two tasks, i.e., wordanalogy and word similarity.
The experimentalresults demonstrate that the proposed models canperform significantly better than all of the state-of-the-art baseline methods in both of the tasks.2 Related WorkThe distributional hypothesis has provided thefoundation for a class of statistical methodsfor word representation learning.
According tothe leveraged distributional information, existingmodels can be grouped into two categories, i.e.,syntagmatic models and paradigmatic models.Syntagmatic models concern combinatorial re-lations between words (i.e., syntagmatic rela-tions), which relate words that co-occur within thesame text region (e.g., sentence, paragraph or doc-ument).For example, sentences have been used as thetext region to acquire co-occurrence informationby (Rubenstein and Goodenough, 1965; Millerand Charles, 1991).
However, as pointed our byPicard (1999), the smaller the context regions arethat we use to collect syntagmatic information,the worse the sparse-data problem will be for theresulting representation.
Therefore, syntagmaticmodels tend to favor the use of larger text regionsas context.
Specifically, a document is often takenas a natural context of a word following the liter-ature of information retrieval.
In these methods, awords-by-documents co-occurrence matrix is builtto collect the distributional information, where theentry indicates the (normalized) frequency of aword in a document.
A low-rank decompositionis then conducted to learn the distributional wordrepresentations.
For example, LSA (Deerwester etal., 1990) employs singular value decompositionby assuming the decomposed matrices to be or-thogonal.
In (Lee and Seung, 1999), non-negativematrix factorization is conducted over the words-by-documents matrix to learn the word represen-tations.Paradigmatic models concern substitutionalrelations between words (i.e., paradigmatic rela-tions), which relate words that occur in the samecontext but may not at the same time.
Unlikesyntagmatic model, paradigmatic models typicallycollect distributional information in a words-by-words co-occurrence matrix, where entries indi-cate how many times words occur together withina context window of some size.For example, the Hyperspace Analogue to Lan-guage (HAL) model (Lund et al, 1995) con-structed a high-dimensional vector for wordsbased on the word co-occurrence matrix from alarge corpus of text.
However, a major problemwith HAL is that the similarity measure will bedominated by the most frequent words due to itsweight scheme.
Various methods have been pro-posed to address the drawback of HAL.
For exam-ple, the Correlated Occurrence Analogue to Lexi-cal Semantic (COALS) (Rohde et al, 2006) trans-formed the co-occurrence matrix by an entropy orcorrelation based normalization.
Bullinaria andLevy (2007), and Levy and Goldberg (2014b) sug-gested that positive pointwise mutual information(PPMI) is a good transformation.
More recently,Lebret and Collobert (2014) obtained the wordrepresentations through a Hellinger PCA (HPCA)of the words-by-words co-occurrence matrix.
Pen-nington et al (2014) explicitly factorizes thewords-by-words co-occurrence matrix to obtain137the Global Vectors (GloVe) for word representa-tion.Alternatively, neural probabilistic languagemodels (NPLMs) (Bengio et al, 2003) learn wordrepresentations by predicting the next word givenpreviously seen words.
Unfortunately, the trainingof NPLMs is quite time consuming, since com-puting probabilities in such model requires nor-malizing over the entire vocabulary.
Recently,Mnih and Teh (2012) applied Noise ContrastiveEstimation (NCE) to approximately maximize theprobability of the softmax in NPLM.
Mikolovet al (2013a) further proposed continuous bag-of-words (CBOW) and skip-gram (SG) models,which use a simple single-layer architecture basedon inner product between two word vectors.
Bothmodels can be learned efficiently via a simple vari-ant of Noise Contrastive Estimation, i.e., Negativesampling (NS) (Mikolov et al, 2013b).3 Our ModelsIn this paper, we argue that it is important to jointlymodel both syntagmatic and paradigmatic rela-tions to learn good word representations.
In thisway, we not only encode different types of linguis-tic properties in a unified way, but also boost therepresentation learning due to the mutual enhance-ment between these two types of relations.We propose two joint models that learn the dis-tributional representations of words based on boththe text region that the words occur in (i.e., syntag-matic relations) and the surrounding words (i.e.,paradigmatic relations).
To model syntagmatic re-lations, we follow the previous work (Deerwesteret al, 1990; Lee and Seung, 1999) to take docu-ment as a nature text region of a word.
To modelparadigmatic relations, we are inspired by the re-cent work from Mikolov et al (Mikolov et al,2013a; Mikolov et al, 2013b), where simple mod-els over word sequences are introduced for effi-cient and effective word representation learning.In the following, we introduce the notationsused in this paper, followed by detailed model de-scriptions, ending with some discussions of theproposed models.3.1 NotationBefore presenting our models, we first list the no-tations used in this paper.
Let D={d1, .
.
.
, dN}denote a corpus of N documents over theword vocabulary W .
The contexts for wordsat.
.
.the catsaton the.
.
.catonthethecni?1cni+1cni+2cni?2dnwni......ProjectionFigure 2: The framework for PDC model.
Fourwords (?the?, ?cat?, ?on?
and ?the?)
are used topredict the center word (?sat?).
Besides, the doc-ument in which the word sequence occurs is alsoused to predict the center word (?sat?
).wni?W (i.e.
i-th word in document dn) arethe words surrounding it in an L-sized window(cni?L, .
.
.
, cni?1, cni+1, .
.
.
, cni+L) ?
H , where cnj?W, j?
{i?L, .
.
.
, i?1, i+1, .
.
.
, i+L}.
Each doc-ument d ?
D, each word w ?
W and each con-text c ?
W is associated with a vector?d ?
RK,w?
?
RKand c?
?
RK, respectively, where K isthe embedding dimensionality.
The entries in thevectors are treated as parameters to be learned.3.2 Parallel Document Context ModelThe first proposed model architecture is shown inFigure 2.
In this model, a target word is predictedby its surrounding context, as well as the docu-ment it occurs in.
The former prediction task cap-tures the paradigmatic relations, since words withsimilar context will tend to have similar represen-tations.
While the latter prediction task models thesyntagmatic relations, since words co-occur in thesame document will tend to have similar represen-tations.
More detailed analysis on this will be pre-sented in Section 3.4.
The model can be viewedas an extension of CBOW model (Mikolov etal., 2013a), by adding an extra document branch.Since both the context and document are parallelin predicting the target word, we call this modelthe Parallel Document Context (PDC) model.More formally, the objective function of PDC138model is the log likelihood of all words?
=N?n=1?wni?dn(log p(wni|hni)+ log p(wni|dn))where hnidenotes the projection of wni?s contexts,defined ashni= f(cni?L, .
.
.
, cni?1, cni+1, .
.
.
, cni+L)where f(?)
can be sum, average, concatenate ormax pooling of context vectors1.
In this paper, weuse average, as that of word2vec tool.We use softmax function to define the probabil-ities p(wni|hni) and p(wni|dn) as follows:p(wni|hni) =exp(?wni??hni)?w?Wexp(w?
?
?hni)(1)p(wni|dn) =exp(?wni??dn)?w?Wexp(w?
?
?dn)(2)where?hnidenotes projected vector of wni?s con-texts.To learn the model, we adopt the negative sam-pling technique (Mikolov et al, 2013b) for effi-cient learning since the original objective is in-tractable for direct optimization.
The negativesampling actually defines an alternate training ob-jective function as follows?=N?n=1?wni?dn(log ?(?wni?
?hni)+ log ?(?wni?
?dn)+ k ?
Ew?
?Pnwlog ?(?w??
?hni)+ k ?
Ew?
?Pnwlog ?(?w??
?dn))(3)where ?
(x) = 1/(1 + exp(?x)), k is the num-ber of ?negative?
samples, w?denotes the sampledword, and Pnwdenotes the distribution of negativeword samples.
We use stochastic gradient descent(SGD) for optimization, and the gradient is calcu-lated via back-propagation algorithm.3.3 Hierarchical Document Context ModelSince the above PDC model can be viewed as anextension of CBOW model, it is natural to in-troduce the same document-word prediction layerinto the SG model.
This becomes our second1Note that the context window size L can be a function ofthe target word wni.
In this paper, we use the same strategyas word2vec tools which uniformly samples from the set{1, 2, ?
?
?
, L}.. .
.the catsaton the.
.
.satcatonthethedncni?1cni+1cni+2cni?2?
?
?
?
?
?ProjectionProjectionwniFigure 3: The framework for HDC model.
Thedocument is used to predict the target word (?sat?
).Then, the word (?sat?)
is used to predict the sur-rounding words (?the?, ?cat?, ?on?
and ?the?
).model architecture as shown in Figure 3.
Specif-ically, the document is used to predict a targetword, and the target word is further used to pre-dict its surrounding context words.
Since the pre-diction is conducted in a hierarchical manner, wename this model the Hierarchical Document Con-text (HDC) model.
Similar as the PDC model,the syntagmatic relation in HDC is modeled bythe document-word prediction layer and the word-context prediction layer models the paradigmaticrelation.Formally, the objective function of HDC modelis the log likelihood of all words:?=N?n=1?wni?dn(i+L?j=i?Lj ?=ilog p(cnj|wni)+ log p(wni|dn))where p(wni|dn) is defined the same as in Equa-tion (2), and p(cnj|wni) is also defined by a softmaxfunction as follows:p(cnj|wni) =exp(?cnj??wni)?c?Wexp(c?
?
?wni)Similarly, we adopt the negative sampling tech-nique for learning, which defines the following139training objective function?
=N?n=1?wni?dn(i+L?j=i?Lj ?=i(log ?(?cnj?
?wni)+ k ?
Ec?
?Pnclog ?(?c??
?wni))+ log ?(?wni?
?dn) + k?Ew?
?Pnwlog ?(?w??
?dn))where k is the number of the negative samples, c?and w?denotes the sampled context and word re-spectively, and Pncand Pnwdenotes the distribu-tion of negative context and word samples respec-tively2.
We also employ SGD for optimization,and calculate the gradient via back-propagation al-gorithm.3.4 DiscussionsIn this section we first show how PDC and HDCmodels capture the syntagmatic and paradigmaticrelations from the viewpoint of matrix factoriza-tion.
We then talk about the relationship of ourmodels with previous work.As pointed out in (Sahlgren, 2008), to capturesyntagmatic relations, the implementational basisis to collect text data in a words-by-documents co-occurrence matrix in which the entry indicates the(normalized) frequency of occurrence of a wordin a document (or, some other type of text region,e.g., a sentence).
While the implementational ba-sis for paradigmatic relations is to collect text datain a words-by-words co-occurrence matrix that ispopulated by counting how many times words oc-cur together within the context window.
We nowtake the proposed PDC model as an example toshow how it achieves these goals, and similar re-sults can be shown for HDC model.The objective function of PDC with negativesampling in Equation (3) can be decomposed intothe following two parts:?1=?w?W?h?H(#(w, h)?
log ?(w?
?
?h)+k?#(h)?pnw(w)log ?(?w??
?h))(4)?2=?d?D?w?W(#(w, d)?
log ?(w?
?
?d)+k?|d|?pnw(w)log ?(?w??
?d))(5)where #(?, ?)
denotes the number of times the pair(?, ?)
appears in D, #(h)=?w?W#(w, h), |d|2Pncis not necessary to be the same as Pnw.denotes the length of document d, the objectivefunction ?1corresponds to the context-word pre-diction task and ?2corresponds to the document-word prediction task.Following the idea introduced by (Levy andGoldberg, 2014a), it is easy to show that the so-lution of the objective function ?1follows thatw?
?
?h = log(#(w, h)#(h) ?
pnw(w)) ?
log kand the solution of the objective function ?2fol-lows thatw?
?
?d = log(#(w, d)|d| ?
pnw(w)) ?
log kIt reveals that the PDC model with negative sam-pling is actually factorizing both a words-by-contexts co-occurrence matrix and a words-by-documents co-occurrence matrix simultaneously.In this way, we can see that the implementationalbasis of the PDC model is consistent with that ofsyntagmatic and paradigmatic models.
In otherwords, PDC can indeed capture both syntagmaticand paradigmatic relations by processing the rightdistributional information.
Please notice that thePDC model is not equivalent to direct combina-tion of existing matrix factorization methods, dueto the fact that the matrix entries defined in PDCmodel are more complicated than the simple co-occurrence frequency (Lee and Seung, 1999).When considering existing models, one mayconnect our models to the Distributed Memorymodel of Paragraph Vectors (PV-DM) and the Dis-tributed Bag of Words version of Paragraph Vec-tors (PV-DBOW) (Le and Mikolov, 2014).
How-ever, both of them are quite different from ourmodels.
In PV-DM, the paragraph vector and con-text vectors are averaged or concatenated to pre-dict the next word.
Therefore, the objective func-tion of PV-DM can no longer decomposed as thePDC model as shown in Equation (4) and (5).In other words, although PV-DM leverages bothparagraph and context information, it is unclearhow these information is collected and used inthis model.
As for PV-DBOW, it simply lever-ages paragraph vector to predict words in the para-graph.
It is easy to show that it only uses thewords-by-documents co-occurrence matrix, andthus only captures syntagmatic relations.Another close work is the Global Context-Aware Neural Language Model (GCANLM for140short) (Huang et al, 2012).
The model definestwo scoring components that contribute to the fi-nal score of a (word sequence, document) pair.The architecture of GCANLM seems similar toour PDC model, but exhibits lots of differencesas follows: (1) GCANLM employs neural net-works as components while PDC resorts to simplemodel structure without non-linear hidden layers;(2) GCANLM uses weighted average of all wordvectors to represent the document, which turnsout to model words-by-words co-occurrence (i.e.,paradigmatic relations) again rather than words-by-documents co-occurrence (i.e., syntagmatic re-lations); (3) GCANLM is a language model whichpredicts the next word given the preceding words,while PDC model leverages both preceding andsucceeding contexts for prediction.4 ExperimentsIn this section, we first describe our experimen-tal settings including the corpus, hyper-parameterselections, and baseline methods.
Then we com-pare our models with baseline methods on twotasks, i.e., word analogy and word similarity.
Af-ter that, we conduct some case studies to showthat our model can better capture both syntagmaticand paradigmatic relations and how it improvesthe performances on semantic tasks.4.1 Experimental SettingsWe select Wikipedia, the largest online knowl-edge base, to train our models.
We adopt thepublicly available April 2010 dump3(Shaoul andWestbury, 2010), which is also used by (Huang etal., 2012; Luong et al, 2013; Neelakantan et al,2014).
The corpus in total has 3, 035, 070 articlesand about 1 billion tokens.
In preprocessing, welowercase the corpus, remove pure digit words andnon-English characters4.Following the practice in (Pennington et al,2014), we set context window size as 10 and use10 negative samples.
The noise distributions forcontext and words are set as the same as usedin (Mikolov et al, 2013a), pnw(w) ?
#(w)0.75.We also adopt the same linear learning rate strat-egy described in (Mikolov et al, 2013a), wherethe initial learning rate of PDC model is 0.05, and3http://www.psych.ualberta.ca/?westburylab/downloads/westburylab.wikicorp.download.html4We ignore the words less than 20 occurrences duringtraining.Table 1: Corpora used in baseline models.model corpus sizeC&W Wikipedia 2007 + Reuters RCV1 0.85BHPCA Wikipedia 2012 1.6BGloVe Wikipedia 2014+ Gigaword5 6BGCANLM, CBOW, SGWikipedia 2010 1BPV-DBOW, PV-DMHDC is 0.025.
No additional regularization is usedin our models5.We compare our models with various state-of-the-art models including C&W (Collobert et al,2011), GCANLM (Huang et al, 2012), CBOW,SG (Mikolov et al, 2013a), GloVe (Pennington etal., 2014), PV-DM, PV-DBOW (Le and Mikolov,2014) and HPCA (Lebret and Collobert, 2014).For C&W, GCANLM6, GloVe and HPCA, we usethe word embeddings they provided.
For CBOWand SG model, we reimplement these two mod-els since the original word2vec tool uses SGDbut cannot shuffle the data.
Besides, we also im-plement PV-DM and PV-DBOW models due to(Le and Mikolov, 2014) has not released sourcecodes.
We train these four models on the samedataset with the same hyper-parameter settings asour models for fair comparison.
The statistics ofthe corpora used in baseline models are shownin Table 1.
Moreover, since different papers re-port different dimensionality, to be fair, we con-duct evaluations on three dimensions (i.e., 50, 100,300) to cover the publicly available results7.4.2 Word AnalogyThe word analogy task is introduced byMikolov etal.
(2013a) to quantitatively evaluate the linguisticregularities between pairs of word representations.The task consists of questions like ?a is to b as c isto ?, where is missing and must be guessedfrom the entire vocabulary.
To answer such ques-tions, we need to find a word vector x?, which isthe closest to?b ?
a?
+ c?
according to the cosinesimilarity:arg maxx?W,x ?=ax ?=b, x ?=c(?b + c??
a?)
?
x?The question is judged as correctly answered onlyif x is exactly the answer word in the evaluation5Codes avaiable at http://www.bigdatalab.ac.cn/benchmark/bm/bd?code=PDC, http://www.bigdatalab.ac.cn/benchmark/bm/bd?code=HDC.6Here, we use GCANLM?s single-prototype embedding.7C&W and GCANLM only released the vectors with 50dimensions, and HPCA released vectors with 50 and 100 di-mensions.141Table 2: Results on the word analogy task.
Un-derlined scores are the best within groups of thesame dimensionality, while bold scores are thebest overall.model size dim semantic syntactic totalC&W 0.85B 50 9.33 11.33 10.98GCANLM 1B 50 2.6 10.7 7.34HPCA 1.6B 50 3.36 9.89 7.2GloVe 6B 50 48.46 45.24 46.22CBOW 1B 50 54.38 49.64 52.01SG 1B 50 53.73 46.12 49.04PV-DBOW 1B 50 55.02 44.17 49.34PV-DM 1B 50 45.08 43.22 44.25PDC 1B 50 61.21 54.55 57.88HDC 1B 50 57.8 49.74 53.41HPCA 1.6B 100 4.16 15.73 10.79GloVe 6B 100 65.34 61.51 63.11CBOW 1B 100 70.73 63.01 66.87SG 1B 100 67.66 59.72 63.45PV-DBOW 1B 100 67.49 56.29 61.51PV-DM 1B 100 57.72 58.81 58.45PDC 1B 100 72.77 67.68 70.35HDC 1B 100 69.57 63.75 66.67GloVe 6B 300 77.44 67.75 71.7CBOW 1B 300 76.2 68.44 72.39SG 1B 300 78.9 65.72 71.88PV-DBOW 1B 300 66.85 58.5 62.08PV-DM 1B 300 56.88 68.35 63.39PDC 1B 300 79.55 69.71 74.76HDC 1B 300 79.67 67.1 73.13set.
The evaluation metric for this task is the per-centage of questions answered correctly.The dataset contains 5 types of semantic analo-gies and 9 types of syntactic analogies8.
The se-mantic analogy contains 8, 869 questions, typi-cally about people and place like ?Beijing is toChina as Paris is to France?, while the syntac-tic analogy contains 10, 675 questions, mostly onforms of adjectives or verb tense, such as ?good isto better as bad to worse?.Result Table 2 shows the results on wordanalogy task.
As we can see that CBOW, SGand GloVe are much stronger baselines as com-pare with C&W, GCANLM and HPCA.
Even so,our PDC model still performs significantly bet-ter than these state-of-the-art methods (p-value< 0.01), especially with smaller vector dimen-sionality.
More interestingly, by only trainingon 1 billion words, our models can outperformthe GloVe model which is trained on 6 billion8http://code.google.com/p/word2vec/source/browse/trunk/questions-words.txtwords.
The results demonstrate that by model-ing both syntagmatic and paradigmatic relations,we can learn better word representations capturinglinguistic regularities.Besides, CBOW, SG and PV-DBOW can beviewed as sub-models of our proposed models,since they use either context (i.e., paradigmatic re-lations) or document (i.e., syntagmatic relations)alone to predict the target word.
By comparingwith these sub-models, we can see that the PDCand HDC models can perform significantly betteron both syntactic and semantic subtasks.
It showsthat by jointly modeling the two relations, one canboost the representation learning and better cap-ture both semantic and syntactic regularities.4.3 Word SimilarityBesides the word analogy task, we also evalu-ate our models on three different word similar-ity tasks, including WordSim-353 (Finkelstein etal., 2002), Stanford?s Contextual Word Similari-ties (SCWS) (Huang et al, 2012) and rare word(RW) (Luong et al, 2013).
These datasets containword paris together with human assigned similar-ity scores.
We compute the Spearman rank corre-lation between similarity scores based on learnedword representations and the human judgements.In all experiments, we removed the word pairs thatcannot be found in the vocabulary.Results Figure 4 shows results on three differ-ent word similarity datasets.
First of all, our pro-posed PDC model always achieves the best per-formances on the three tasks.
Besides, if we com-pare the PDC and HDC models with their cor-responding sub-models (i.e., CBOW and SG) re-spectively, we can see performance gain by addingsyntagmatic information via document.
This gainbecomes even larger for rare words with low di-mensionality as shown on RW dataset.
More-over, on the SCWS dataset, our PDC model us-ing the single-prototype representations under di-mensionality 50 can achieve a comparable result(65.63) to the state-of-the-art GCANLM (65.7 asthe best performance reported in (Huang et al,2012)) which uses multi-prototype vectors9.4.4 Case StudyHere we conduct some case studies to (1) gainsome intuition on how these two relations affect9Note, in Figure 4, the performance of GCANLM is com-puted based on their released single-prototype vectors.142C&W GCANLM HPCA GloVe PV-DM PV-DBOW SG HDC CBOW PDC50 100 30020406080WordSim 353?
?10050 100 30040506070SCWS?
?10050 100 3000204060RW?
?100Figure 4: Spearman rank correlation on three datasets.
Results are grouped by dimensionality.Table 3: Target words and their 5 most similarwords under different representations.
Words initalic often co-occur with the target words, whilewords in bold are substitutable to the target words.feynmanCBOWeinstein, schwinger, bohm, betherelativitySGschwinger, quantum, bethe, einsteinsemiclassicalPDCgeometrodynamics, bethe, semiclassicalschwinger, perturbativeHDCschwinger, electrodynamics, bethesemiclassical, quantumPV-DBOWphysicists, spacetime, geometrodynamicstachyons, einsteinmoonCBOW earth, moons, pluto, sun, nebulaSG earth, sun, mars, planet, aquariusPDC sun, moons, lunar, heavens, earthHDC earth, sun, mars, planet, heavensPV-DBOW lunar, moons, celestial, sun, eclipticthe representation learning, and (2) analyze whythe joint model can perform better.To show how syntagmatic and paradigmaticrelations affect the learned representations, wepresent the 5 most similar words (by cosine simi-larity with 50-dimensional vectors) to a given tar-get word under the PDC and HDC models, as wellas three sub-models, i.e., CBOW, SG, and PV-DBOW.
The results are shown in table 3, wherewords in italic are those often co-occurred withthe target word (i.e., syntagmatic relations), whilewords in bold are whose substitutable to the targetword (i.e., paradigmatic relation).Clearly, top words from CBOW and SG mod-els are more under paradigmatic relations, whilethose from PV-DBOW model are more under syn-000deepdeepercrevassesCBOW000deepdeepercrevassesPDCFigure 5: The 3-D embedding of learned wordvectors of ?deep?, ?deeper?
and ?crevasses?
underCBOW and PDC models.tagmatic relations, which is quite consistent withthe model design.
By modeling both relations, thetop words from PDC and HDC models becomemore diverse, i.e., more syntagmatic relations thanCBOW and SGmodels, and more paradigmatic re-lations than PV-DBOW model.
The results revealthat the word representations learned by PDC andHDCmodels are more balanced with respect to thetwo relations as compared with sub-models.The next question is why learning a joint modelcan work better on previous tasks?
We first takeone example from the word analogy task, which isthe question ?big is to bigger as deep is to ?with the correct answer as ?deeper?.
Our PDCmodel produce the right answer but the CBOWmodel fails with the answer ?shallower?.
We thusembedding the learned word vectors from the twomodels into a 3-D space to illustrate and analyzethe reason.As shown in Figure 5, we can see that by jointlymodeling two relations, PDC model not only re-quires that ?deep?
to be close to ?deeper?
(in co-sine similarity), but also requires that ?deep?
and?deeper?
to be close to ?crevasses?.
The additional143requirements further drag these three words closeras compared with those from the CBOW model,and this make our model outperform the CBOWmodel on this question.
As for the word similaritytasks, we find that the word pairs are either syntag-matic (e.g., ?bank?
and ?money?)
or paradigmatic(e.g., ?left?
and ?abandon?).
It is, therefore, notsurprising to see that a more balanced representa-tion can achieve much better performance than abiased representation.5 ConclusionExisting work on word representations models ei-ther syntagmatic or paradigmatic relations.
In thispaper, we propose two novel distributional modelsfor word representation, using both syntagmaticand paradigmatic relations via a joint training ob-jective.
The experimental results on both wordanalogy and word similarity tasks show that theproposed joint models can learn much better wordrepresentations than the state-of-the-art methods.Several directions remain to be explored.
Inthis paper, the syntagmatic and paradigmatic rela-tions are equivalently important in both PDC andHDC models.
An interesting question would thenbe whether and how we can add different weightsfor syntagmatic and paradigmatic relations.
Be-sides, we may also try to learn the multi-prototypeword representations for polysemous words basedon our proposed models.AcknowledgmentsThis work was funded by 973 Program ofChina under Grants No.
2014CB340401 and2012CB316303, and the National Natural Sci-ence Foundation of China (NSFC) under GrantsNo.
61232010, 61433014, 61425016, 61472401and 61203298.
We thank Ronan Collobert, EricH.
Huang, R?emi Lebret, Jeffrey Pennington andTomas Mikolov for their kindness in sharing codesand word vectors.
We also thank the anonymousreviewers for their helpful comments.ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
J. Mach.
Learn.
Res., 3:1137?1155,March.John A. Bullinaria and Joseph P. Levy.
2007.
Ex-tracting semantic representations from word co-occurrence statistics: A computational study.
Be-havior Research Methods, 39(3):510?526.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
J. Mach.
Learn.
Res., 12:2493?2537,November.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan andGadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Trans.
Inf.
Syst., 20(1):116?131, January.J.
R. Firth.
1957.
A synopsis of linguistic theory 1930-55.
Studies in Linguistic Analysis (special volume ofthe Philological Society), 1952-59:1?32.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers - Volume 1, ACL ?12, pages 873?882, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Quoc Le and Tomas Mikolov.
2014.
Distributed rep-resentations of sentences and documents.
In TonyJebara and Eric P. Xing, editors, Proceedings of the31st International Conference on Machine Learning(ICML-14), pages 1188?1196.
JMLR Workshop andConference Proceedings.R?emi Lebret and Ronan Collobert.
2014.
Word em-beddings through hellinger pca.
In Proceedings ofthe 14th Conference of the European Chapter of theAssociation for Computational Linguistics, pages482?490.
Association for Computational Linguis-tics.Daniel D. Lee and H. Sebastian Seung.
1999.
Learningthe parts of objects by non-negative matrix factoriza-tion.
Nature, 401(6755):788?791, october.Omer Levy and Yoav Goldberg.
2014a.
Neural wordembedding as implicit matrix factorization.
In Ad-vances in Neural Information Processing Systems27, pages 2177?2185.
Curran Associates, Inc., Mon-treal, Quebec, Canada.Omer Levy and Yoav Goldberg, 2014b.
Proceedings ofthe Eighteenth Conference on Computational Natu-ral Language Learning, chapter Linguistic Regular-ities in Sparse and Explicit Word Representations,pages 171?180.
Association for Computational Lin-guistics.144Kevin Lund, Curt Burgess, and Ruth Ann Atchley.1995.
Semantic and associative priming in a high-dimensional semantic space.
In Proceedings of the17th Annual Conference of the Cognitive ScienceSociety, pages 660?665.Minh-Thang Luong, Richard Socher, and Christo-pher D. Manning.
2013.
Better word representa-tions with recursive neural networks for morphol-ogy.
In Proceedings of the Seventeenth Confer-ence on Computational Natural Language Learning,pages 104?113.
Association for Computational Lin-guistics.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, New York,NY, USA.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
In Proceedings of Workshopof ICLR.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed repre-sentations of words and phrases and their compo-sitionality.
In C.J.C.
Burges, L. Bottou, M. Welling,Z.
Ghahramani, and K.Q.
Weinberger, editors, Ad-vances in Neural Information Processing Systems26, pages 3111?3119.
Curran Associates, Inc.George A Miller and Walter G Charles.
1991.
Contex-tual correlates of semantic similarity.
Language &Cognitive Processes, 6(1):1?28.Andriy Mnih and Yee Whye Teh.
2012.
A fast andsimple algorithm for training neural probabilisticlanguage models.
In Proceedings of the 29th In-ternational Conference on Machine Learning, pages1751?1758.Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-sos, and Andrew McCallum.
2014.
Efficientnon-parametric estimation of multiple embeddingsper word in vector space.
In Proceedings of the2014 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 1059?1069, Doha, Qatar, October.
Association for Com-putational Linguistics.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
Glove: Global vectorsfor word representation.
In Proceedings of the 2014Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP 2014, October 25-29,2014, Doha, Qatar, A meeting of SIGDAT, a SpecialInterest Group of the ACL, pages 1532?1543.Justin Picard.
1999.
Finding content-bearing terms us-ing term similarities.
In Proceedings of the NinthConference on European Chapter of the Associationfor Computational Linguistics, EACL ?99, pages241?244, Stroudsburg, PA, USA.
Association forComputational Linguistics.Douglas L. T. Rohde, Laura M. Gonnerman, andDavid C. Plaut.
2006.
An improved model ofsemantic similarity based on lexical co-occurence.Communications of the ACM, 8:627?633.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Commun.
ACM,8(10):627?633, October.Magnus Sahlgren.
2008.
The distributional hypothe-sis.
Italian Journal of Linguistics, 20(1):33?54.Hinrich Sch?utze.
1998.
Automatic word sensediscrimination.
Comput.
Linguist., 24(1):97?123,March.Cyrus Shaoul and ChrisWestbury.
2010.
The westburylab wikipedia corpus.
Edmonton, AB: University ofAlberta.Richard Socher, Cliff C. Lin, Chris Manning, and An-drew Y. Ng.
2011.
Parsing natural scenes and nat-ural language with recursive neural networks.
InLise Getoor and Tobias Scheffer, editors, Proceed-ings of the 28th International Conference on Ma-chine Learning (ICML-11), pages 129?136, NewYork, NY, USA.
ACM.Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernan-des, and Gregory Marton.
2003.
Quantitative eval-uation of passage retrieval algorithms for questionanswering.
In Proceedings of the 26th Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Informaion Retrieval, SIGIR ?03,pages 41?47, New York, NY, USA.
ACM.145
