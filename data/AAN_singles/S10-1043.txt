Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 198?201,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsUNITN: Part-Of-Speech Counting in Relation ExtractionFabio CelliUniversity of TrentoItalyfabio.celli@unitn.itAbstractThis report describes the UNITN system, aPart-Of-Speech Context Counter, that par-ticipated at Semeval 2010 Task 8: Multi-Way Classification of Semantic RelationsBetween Pairs of Nominals.
Given a textannotated with Part-of-Speech, the systemoutputs a vector representation of a sen-tence containing 20 features in total.
Thereare three steps in the system?s pipeline:first the system produces an estimation ofthe entities?
position in the relation, thenan estimation of the semantic relation typeby means of decision trees and finally itgives a predicition of semantic relationplus entities?
position.
The system ob-tained good results in the estimation of en-tities?
position (F1=98.3%) but a criticallypoor performance in relation classification(F1=26.6%), indicating that lexical and se-mantic information is essential in relationextraction.
The system can be used as anintegration for other systems or for pur-poses different from relation extraction.1 Introduction and BackgroundThis technical report describes the UNITN system(a Part-Of-Speech Context Counter) that partici-pated to Semeval 2010 Task 8: Multi-Way Clas-sification of Semantic Relations Between Pairs ofNominals (see Hendrickx et al, 2009).
A differentversion of this system based on Part-Of-Speechcounting has been previously used for the auto-matic annotation of three general and separable se-mantic relation classes (taxonomy, location, asso-ciation) obtaining an average F1-measure of 0.789for english and 0.781 for italian, see Celli 2010for details.
The organizers of Semeval 2010 Task8 provided ten different semantic relation types incontext, namely:?
Cause-Effect (CE).
An event or object leadsto an effect.
Example: Smoking causes can-cer.?
Instrument-Agency (IA).
An agent uses aninstrument.
Example: Laser printer.?
Product-Producer (PP).
A producer causesa product to exist.
Example: The growth hor-mone produced by the pituitary gland.?
Content-Container (CC).
An object is phys-ically stored in a delineated area of space,the container.
Example: The boxes containedbooks.?
Entity-Origin (EO).
An entity is coming oris derived from an origin (e.g., position ormaterial).
Example: Letters from foreigncountries.?
Entity-Destination (ED).
An entity is mov-ing towards a destination.
Example: The boywent to bed.?
Component-Whole (CW).
An object is acomponent of a larger whole.
Example: Myapartment has a large kitchen.?
Member-Collection (MC).
A member formsa nonfunctional part of a collection.
Exam-ple: There are many trees in the forest.?
Message-Topic (CT).
An act of communica-tion, whether written or spoken, is about atopic.
Example: The lecture was about se-mantics.?
Other.
The entities are related in a way thatdo not fall under any of the previous men-tioned classes.
Example: Batteries stored ina discharged state are susceptible to freezing.198The task was to predict, given a sentence and twomarked-up entities, which one of the relation la-bels to apply and the position of the entities in therelation (except from ?Other?).
An example is re-ported below:?
?The <e1>bag</e1>contained <e2>books</e2>,a cell phone and notepads,but no explosives.?
?Content-Container(e2,e1)The task organizers also provided 8000 sentencesfor training and 2717 sentences for testing.
Partof the task was to discover whether it is better topredict entities?
position before semantic relationor viceversa.In the next section there is a description of theUNITN system, in section 3 are reported the re-sults of the system on the dataset provided for Se-meval Task 8, in section 4 there is the discussion,then some conclusions follow in section 5.2 System DescriptionUNITN is a Part-Of-Speech Context Counter.Given as input a plain text with Part-Of-Speechand end-of-sentence markers annotated it outputsa numerical feature vector that gives a representa-tion of a sentence.
For Part-Of-Speech and end-of-sentence annotation I used Textpro, a tool for NLPthat showed state-of-the-art performance for POStagging (see Pianta et al, 2008).
The POS tagsetis the one used in the BNC, described at http://pie.usna.edu/POScodes.html.Features in the vector can be tailored for specifictasks, in this case 20 features were used in total.They are:1.
Number of prepositions in sentence.2.
Number of nouns and proper names in sen-tence.3.
Number of lexical verbs in sentence.4.
Number of ?be?
verbs in sentence.5.
Number of ?have?
verbs in sentence.6.
Number of ?do?
verbs in sentence.7.
Number of modal verbs in sentence.8.
Number of conjunctions in sentence.9.
Number of adjectives in sentence.10.
Number of determiners in sentence.11.
Number of pronouns in sentence.12.
Number of punctuations in sentence.13.
Number of negative particles in sentence.14.
Number of words in the context between thefirst and the second entity.15.
Number of verbs in the context between thefirst and the second entity.16.
patterns (from, in, on, by, of, to).17.
POS of entity 1 (noun, adjective, other).18.
POS of entity 2 (noun, adjective, other).19.
Estimate of entities?
position in the relation(e1-e2, e2-e1, 00).20.
Estimate of semantic relation (relations de-scribed in section 1 above).Prepositional patterns in feature 16 were chosenfor their high cooccurrence frequency with a se-mantic relation type and their low cooccurrencewith the other ones.The system works in three steps: in the first onefeatures 1-18 are used for predicting feature 19,in the second one features 1-19 are used for pre-dicting feature 20.
In the third step, after the ap-plication of Hall 1998?s attribute selection filter(that evaluates the worth of a subset of attributesby considering the individual predictive ability ofeach feature along with the degree of redundancybetween them) features 12, 14, 16, 19 and 20 areused for the prediction of semantic relation plusentities?
position (19 relations in total).For all the steps I used C4.5 decision trees (seeQuinlan 1993) and Cohen 1995?s RIPPER algo-rithm (Repeated Incremental Pruning to ProduceError Reduction).
Evaluation for steps 1, 2 and 3have been run on the training set, with a 10-foldcross-validation, since the test set was relased ina second time.
Results of evaluation of step 1, 2and 3 are reported in table 1 below, chance values(100/number of classes) are taken as baselines, allexperiments have been run in Weka (see Wittenand Frank, 2005).I also inverted step 1 and 2 for predicting seman-199Prediction Baseline average F1step 1 33.33% 98.3%step 2 10% 29.8%step 3 5.26% 28.1%Table 1: Evaluation for steps 1, 2 and 3.tic relation estimate before entities?
position esti-mate and the average F1-measure is even worse(0.271), demonstrating that entities?
position esti-mate has a positive weight on semantic relation es-timate.
There are instead some problems with step2, and I will return on this later in the discussion(section 4).3 ResultsAs it was requested by the task, the system hasbeen run 4 times in the testing phase: the first time(r1) using 1000 examples from the training set forbuilding the model, the second time (r2) 2000 ex-amples, the third (r3) 4000 example and the lastone (r4) using the entire training set.The results obtained by UNITN in the competi-tion are not good, overall performance is poor, es-pecially for some relations, in particular Product-Producer and Message-Topic.
The best perfor-mance is achieved by the Member-Collection re-lation (47.30% ), that changed from 0% in the firstrun to 42.71% in the second one.
Scores are re-ported, relation by relation, in table 2 below, thediscussion follows in section 4.Rel F1 (r1) F1 (r2) F1 (r3) F1 (r4)CE 23.08% 17.24% 22.37% 26.86%CW 13.64% 0.00% 13.85% 25.23%CC 26.43% 25.36% 26.72% 28.39%ED 37.26% 37.25% 46.27% 46.35%EO 36.60% 36.49% 37.61% 41.79%IA 10.68% 7.95% 5.59% 17.32%MC 0.00% 42.71% 43.08% 47.30%CT 1.48% 0.00% 4.93% 6.81%PP 0.00% 0.00% 1.67% 0.00%Other 27.14% 26.15% 25.80% 20.64%avg* 16.57% 18.56% 22.45% 26.67%Table 2: Results.
*Macro average excuding?Other?.4 DiscussionOn the one hand the POSCo system showed anhigh performance in step 1 (entities?
positiondetection), indicating that the numerical sentencerepresentation obtained by means of Part-Of-Speech can be a good way for extracting syntacticinformation.On the other hand the POSCo system provednot to be good for the classification of semanticrelations.
This clearly indicates that lexical andsemantic information is essential in relationextraction.
This fact is highlighted also by theattribute selection filter algorithm that choosed,among others, feature 16 (prepositional patterns),which was the only attribute providing lexicalinformation in the system.It is interesting to note that it chose feature12 (punctuation) and 14 (number of words inthe context between the first and the secondentity).
Punctuation can be used to provide, toa certain level, information about how muchthe sentence is complex (the higher the numberof the punctuation, the higher the subordinatedphrases), while feature 14 provides informationabout the distance between the related entities andthis could be useful for the classification betweenpresence or absence of a semantic relation (thelonger the distance, the lower the probability tohave a relation between entities) but it is uselessfor a multi-way classification with many semanticrelations, like in this case.5 ConclusionsIn this report we have seen that Part-Of-SpeechCounting does not yield good performances in re-lation extraction.
Despite this it provides someinformation about the complexity of the sentenceand this can be useful for predicting the positionof the entities in the relation.
The results confirmthe fact that lexical and semantic information isessential in relation extraction, but also that thereare some useful non-lexical features, like the com-plexity of the sentence and the distance betweenthe first and the second related entities, that can beused as a complement for systems based on lexicaland semantic resources.200ReferencesFabio Celli.
2010.
Automatic Semantic RelationAnnotation for Italian and English.
(technical reportavailable at http://clic.cimec.unitn.it/fabio).William W. Cohen.
1995.
Fast effective rule induction.In Proceedings of the 12th International Conferenceon Machine Learning.
Lake Tahoe, CA.Mark A.
Hall.
1998.
Correlation-based FeatureSelection for Discrete and Numeric Class Ma-chine Learning.
Technical report availableat http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.148.6025&rep=rep1&type=pdf.Iris Hendrickx and Su Nam Kim and Zornitsa Kozarevaand Preslav Nakov and Diarmuid?O S?eaghdhaand Sebastian Pad?o and Marco Pennacchiotti andLorenza Romano and Stan Szpakowicz.
2010.SemEval-2010 Task 8: Multi-Way Classification ofSemantic Relations Between Pairs of Nominals.
InProceedings of the 5th SIGLEX Workshop on Se-mantic Evaluation, Uppsala, Sweden.Emanuele Pianta and Christian Girardi and RobertoZanoli.
2008.
The TextPro tool suite.
In Proceedingsof LREC, Marrakech, Morocco.John Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann Publishers, San Ma-teo, CA.Ian H. Witten and Eibe Frank.
2005.
Data Mining.Practical Machine Learning Tools and Techniqueswith Java implementations.
Morgan and Kaufman,San Francisco, CA.201
