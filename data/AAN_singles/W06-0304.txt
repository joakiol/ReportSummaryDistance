Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 23?30,Sydney, July 2006. c?2006 Association for Computational LinguisticsUser-directed Sentiment Analysis: Visualizing the Affective Content ofDocumentsMichelle L. GregoryPNNL902 Battelle Blvd.Richland Wa.
99354michelle.gregory@pnl.govNancy ChinchorConsultantchinchor@earthlink.netPaul WhitneyPNNL902 Battelle Blvd.Richland Wa.
99354paul.whitney@pnl.govRichard CarterPNNL902 Battelle Blvd.Richland Wa.
99354richard.carter@pnl.govElizabeth HetzlerPNNL902 Battelle Blvd.Richland Wa.
99354beth.hetzler@pnl.govAlan TurnerPNNL902 Battelle Blvd.Richland Wa.
99354alan.turner@pnl.govAbstractRecent advances in text analysis have ledto finer-grained semantic analysis, in-cluding automatic sentiment analysis?the task of measuring documents, orchunks of text, based on emotive catego-ries, such as positive or negative.
How-ever, considerably less progress has beenmade on efficient ways of exploringthese measurements.
This paper discussesapproaches for visualizing the affectivecontent of documents and describes aninteractive capability for exploring emo-tion in a large document collection.1 IntroductionRecent advances in text analysis have led tofiner-grained semantic classification, which en-ables the automatic exploration of subtle areas ofmeaning.
One area that has received a lot of at-tention is automatic sentiment analysis?the taskof classifying documents, or chunks of text, intoemotive categories, such as positive or negative.Sentiment analysis is generally used for trackingpeople?s attitudes about particular individuals oritems.
For example, corporations use sentimentanalysis to determine employee attitude and cus-tomer satisfaction with their products.
Given theplethora of data in digital form, the ability to ac-curately and efficiently measure the emotionalcontent of documents is paramount.The focus of much of the automatic sentimentanalysis research is on identifying the affectbearing words (words with emotional content)and on measurement approaches for sentiment(Turney & Littman, 2003; Pang & Lee, 2004;Wilson et al, 2005).
While identifying relatedcontent is an essential component for automaticsentiment analysis, it only provides half thestory.
A useful area of research that has receivedmuch less attention is how these measurementsmight be presented to the users for explorationand added value.This paper discusses approaches for visualiz-ing affect and describes an interactive capabilityfor exploring emotion in a large document col-lection.
In Section 2 we review current ap-proaches to identifying the affective content ofdocuments, as well as possible ways of visualiz-ing it.
In Section 3 we describe our approach:The combination of a lexical scoring method todetermine the affective content of documents anda visual analytics tool for visualizing it.
We pro-vide a detailed case study in Section 4, followedby a discussion of possible evaluations.2 BackgroundAt the AAAI Symposium on Attitude and Affectheld at Stanford in 2004 (Qu et al, 2005), it wasclear that the lexical approach to capturing affectwas adequate for broad brush results, but therewere no production quality visualizations forpresenting those results analytically.
Thus, webegan exploring methods and tools for the visu-alization of lexically-based approaches for meas-uring affect which could facilitate the explorationof affect within a text collection.2.1 Affect ExtractionFollowing the general methodology of informa-tional retrieval, there are two pre-dominantmethods for identifying sentiment in text: Textclassification models and lexical approaches.Classification models require that a set of docu-ments are hand labeled for affect, and a system is23trained on the feature vectors associated withlabels.
New text is automatically classified bycomparing the feature vectors with the trainingset.
(Pang & Lee, 2004; Aue & Gamon, 2005).This methodology generally requires a largeamount of training data and is domain dependent.In the lexical approach, documents (Turney &Littman, 2003), phrases (see Wilson et al, 2005),or sentences (Weibe & Riloff, 2005) are catego-rized as positive or negative, for example, basedon the number of words in them that match alexicon of sentiment bearing terms.
Major draw-backs of this approach include the contextualvariability of sentiment (what is positive in onedomain may not be in another) and incompletecoverage of the lexicon.
This latter drawback isoften circumvented by employing bootstrapping(Turney & Littman, 2003; Weibe & Riloff, 2005)which allows one to create a larger lexicon froma small number of seed words, and potentiallyone specific to a particular domain.2.2 Affect VisualizationThe uses of automatic sentiment classificationare clear (public opinion, customer reviews,product analysis, etc.).
However, there has notbeen a great deal of research into ways of visual-izing affective content in ways that might aiddata exploration and the analytic process.There are a number of visualizations designedto reveal the emotional content of text, in par-ticular, text that is thought to be highly emotivelycharged such as conversational transcripts andchat room transcripts (see DiMicco et al, 2002;Tat & Carpendale, 2002; Lieberman et al, 2004;Wang et al, 2004, for example).
Aside fromusing color and emoticons to explore individualdocuments (Liu et al, 2003) or email inboxes(Mandic & Kerne, 2004), there are very fewvisualizations suitable for exploring the affect oflarge collections of text.
One exception is thework of Liu et al (2005) in which they provide avisualization tool to compare reviews of prod-ucts,using a bar graph metaphor.
Their systemautomatically extracts product features (with as-sociated affect) through parsing and pos tagging,having to handle exceptional cases individually.Their Opinion Observer is a powerful tool de-signed for a single purpose: comparing customerreviews.In this paper, we introduce a visual analytictool designed to explore the emotional content oflarge collections of open domain documents.
Thetools described here work with document collec-tions of all sizes, structures (html, xml, .doc,email, etc), sources (private collections, web,etc.
), and types of document collections.
Thevisualization tool is a mature tool that supportsthe analytical process by enabling users to ex-plore the thematic content of the collection, usenatural language to query the collection, makegroups, view documents by time, etc.
The abilityto explore the emotional content of an entire col-lection of documents not only enables users tocompare the range of affect in documents withinthe collection, but also allows them to relate af-fect to other dimensions in the collection, such asmajor topics and themes, time, and source.3 The ApproachOur methodology combines a traditional lexicalapproach to scoring documents for affect with amature visualization tool.
We first automaticallyidentify affect by comparing each documentagainst a lexicon of affect-bearing words andobtain an affect score for each document.
Weprovide a number of visual metaphors to repre-sent the affect in the collection and a number oftools that can be used to interactively explore theaffective content of the data.3.1 Lexicon and MeasurementWe use a lexicon of affect-bearing words to iden-tify the distribution of affect in the documents.Our lexicon authoring system allows affect-bearing terms, and their associated strengths, tobe bulk loaded, declared manually, or algo-rithmically suggested.
In this paper, we use alexicon derived from the General Inquirer (GI)and supplemented with lexical items derivedfrom a semi-supervised bootstrapping task.
TheGI tool is a computer-assisted approach for con-tent analyses of textual data (Stone, 1977).
It in-cludes an extensive lexicon of over 11,000 hand-coded word stems and 182 categories.We used this lexicon, specifically the positiveand negative axes, to create a larger lexicon bybootstrapping.
Lexical bootstrapping is a methodused to help expand dictionaries of semanticcategories (Riloff & Jones, 1999) in the contextof a document set of interest.
The approach wehave adopted begins with a lexicon of affectbearing words (POS and NEG) and a corpus.Each document in the corpus receives an affectscore by counting the number of words from theseed lexicon that occur in the document; a sepa-rate score is given for each affect axis.
Words inthe corpus are scored for affect potential bycomparing their distribution (using an L1 Distri-24bution metric) of occurrence over the set ifdocuments to the distribution of affect bearingwords.
Words that compare favorably with affectare hypothesized as affect bearing words.
Resultsare then manually culled to determine if in factthey should be included in the lexicon.Here we report on results using a lexicon builtfrom 8 affect categories, comprising 4 conceptpairs:?
Positive (n=2236)-Negative (n=2708)?
Virtue (n=638)-Vice (n=649)?
Pleasure (n=151)-Pain (n=220)?
Power Cooperative (n=103)-Power Con-flict (n=194)Each document in the collection is comparedagainst all 8 affect categories and receives ascore for each.
Scores are based on the summa-tion of each affect axis in the document, normal-ized by the number of words in the documents.This provides an overall proportion of positivewords, for example, per document.
Scores canalso be calculated as the summation of each axis,normalized by the total number of affect wordsfor all axes.
This allows one to quickly estimatethe balance of affect in the documents.
For ex-ample, using this measurement, one could seethat a particular document contains as manypositive as negative terms, or if it is heavilyskewed towards one or the other.While the results reported here are based on apredefined lexicon, our system does include aLexicon Editor in which a user can manually en-ter their own lexicon or add strengths to lexicalitems.
Included in the editor is a Lexicon Boot-strapping Utility which the user can use to helpcreate a specialized lexicon of their own.
Thisutility runs as described above.
Note that whilewe enable the capability of strength, we have notexperimented with that variable here.
All wordsfor all axes have a default strength of .5.3.2 VisualizationTo visualize the affective content of a collectionof documents, we combined a variety of visualmetaphors with a tool designed for visual ana-lytics of documents, IN-SPIRE.3.2.1 The IN-SPIRE SystemIN-SPIRE (Hetzler and Turner, 2004) is a visualanalytics tool designed to facilitate rapid under-standing of large textual corpora.
IN-SPIRE gen-erates a compiled document set from mathemati-cal signatures for each document in a set.Document signatures are clustered according tocommon themes to enable information explora-tion and visualizations.
Information is presentedto the user using several visual metaphors to ex-pose different facets of the textual data.
The cen-tral visual metaphor is a Galaxy view of the cor-pus that allows users to intuitively interact withthousands of documents, examining them bytheme (see Figure 4, below).
IN-SPIRE leveragesthe use of context vectors such as LSA (Deer-wester et al, 1990) for document clustering andprojection.
Additional analytic tools allow explo-ration of temporal trends, thematic distributionby source or other metadata, and query relation-ships and overlaps.
IN-SPIRE was recently en-hanced to support visual analysis of sentiment.3.2.2 Visual MetaphorsIn selecting metaphors to represent the affectscores of documents, we started by identifyingthe kinds of questions that users would want toexplore.
Consider, as a guiding example, a set ofcustomer reviews for several commercial prod-ucts (Hu & Liu, 2004).
A user reviewing thisdata might be interested in a number of ques-tions, such as:?
What is the range of affect overall??
Which products are viewed most posi-tively?
Most negatively??
What is the range of affect for a particularproduct??
How does the affect in the reviews deviatefrom the norm?
Which are more negativeor positive than would be expected fromthe averages??
How does the feedback of one productcompare to that of another??
Can we isolate the affect as it pertains todifferent features of the products?In selecting a base metaphor for affect, wewanted to be able to address these kinds of ques-tions.
We wanted a metaphor that would supportviewing affect axes individually as well as inpairs.
In addition to representing the most com-mon axes, negative and positive, we wanted toprovide more flexibility by incorporating theability to portray multiple pairs because we sus-pect that additional axes will help the user ex-plore nuances of emotion in the data.
For ourcurrent metaphor, we drew inspiration from theRose plot used by Florence Nightingale (Wainer,1997).
This metaphor is appealing in that it iseasily interpreted, that larger scores draw more25attention, and that measures are shown in consis-tent relative location, making it easier to comparemeasures across document groups.
We use amodified version of this metaphor in which eachaxis is represented individually but is also pairedwith its opposite to aid in direct comparisons.
Tothis end, we vary the spacing between the rosepetals to reinforce the pairing.
We also use color;each pair has a common hue, with the more posi-tive of the pair shown in a lighter shade and themore negative one in a darker shade (see Figure1).To address how much the range of affect var-ies across a set of documents, we adapted theconcept of a box plot to the rose petal.
For eachaxis, we show the median and quartile values asshown in the figure below.
The dark line indi-cates the median value and the color band por-trays the quartiles.
In the plot in Figure 1, forexample, the scores vary quite a bit.Figure 1.
Rose plot adapted to show median andquartile variation.Another variation we made on the base meta-phor was to address a more subtle set of ques-tions.
It may happen that the affect scores withina dataset are largely driven by document mem-bership in particular groups.
For example, in ourcustomer data, it may be that all documentsabout Product A are relatively positive whilethose about Product B are relatively negative.
Auser wanting to understand customer complaintsmay have a subtle need.
It is not sufficient to justlook at the most negative documents in the data-set, because none of the Product A documentsmay pass this threshold.
What may also help is tolook at all documents that are more negative thanone would expect, given the product they dis-cuss.
To carry out this calculation, we use a sta-tistical technique to calculate the Main (or ex-pected) affect value for each group and the Re-sidual (or deviation) affect value for each docu-ment with respect to its group (Scheffe, 1999).To convey the Residual concept, we needed arepresentation of deviation from expected value.We also wanted this portrayal to be similar to thebase metaphor.
We use a unit circle to portraythe expected value and show deviation by draw-ing the appropriate rose petals either outside(larger than expected) or inside (smaller thanexpected) the unit circle, with the color amountshowing the amount of deviation from expected.In the figures below, the dotted circle representsexpected value.
The glyph on the left shows acluster with scores slightly higher than expectedfor Positive and for Cooperation affect.
Theglyph on the right shows a cluster with scoresslightly higher than expected for the Negativeand Vice affect axes (Figure 2).Figure 2.
Rose plot adapted to show deviationfrom expected values.3.2.3 Visual InteractionIN-SPIRE includes a variety of analytic toolsthat allow exploration of temporal trends, the-matic distribution by source or other metadata,and query relationships and overlaps.
We haveincorporated several interaction capabilities forfurther exploration of the affect.
Our analysissystem allows users to group documents in nu-merous ways, such as by query results, by meta-data (such as the product), by time frame, and bysimilarity in themes.
A user can select one ormore of these groups and see a summary of af-fect and its variation in those groups.
In addition,the group members are clustered by their affectscores and glyphs of the residual, or variationfrom expected value, are shown for each of thesesub-group clusters.Below each rose we display a small histogramshowing the number of documents representedby that glyph (see Figure 3).
These allow com-parison of affect to cluster or group size.
For ex-ample, we find that extreme affect scores aretypically found in the smaller clusters, while lar-ger ones often show more mid-range scores.
Asthe user selects document groups or clusters, weshow the proportion of documents selected.26Figure 3.
Clusters by affect score, with one roseplot per cluster.The interaction may also be driven from theaffect size.
If a given clustering of affect charac-teristics is selected, the user can see the themesthey represent, how they correlate to metadata, orthe time distribution.
We illustrate how the affectvisualization and interaction fit into a largeranalysis with a brief case study.4 Case studyThe IN-SPIRE visualization tool is a non-dataspecific tool, designed to explore large amountsof textual data for a variety of genres and docu-ment types (doc, xml,  etc).
Many users of thesystem have their own data sets they wish to ex-plore (company internal documents), or data canbe harvested directly from the web, either in asingle web harvest, or dynamically.
The casestudy and dataset presented here is intended as anexample only, it does not represent the full rangeof exploration capabilities of the affective con-tent of datasets.We explore a set of customer reviews, com-prising a collection of Amazon reviews for fiveproducts (Hu & Liu, 2004).
While a customermay not want to explore reviews for 5 differentproduct types at once, the dataset is realistic inthat a web harvest of one review site will containreviews of multiple products.
This allows us todemonstrate how the tool enables users to focuson the data and comparisons that they are inter-ested in exploring.
The 5 products in this datasetare:?
Canon G3; digital camera?
Nikon coolpix 4300; digital camera?
Nokia 6610; cell phone?
Creative Labs Nomad Jukebox Zen Xtra40GB; mp3 player?
Apex AD2600 Progressive-scan DVDplayerWe begin by clustering the reviews, based onoverall thematic content.
The labels are auto-matically generated and indicate some of thestronger theme combinations in this dataset.These clusters are driven largely by product vo-cabulary.
The two cameras cluster in the lowerportion; the Zen shows up in the upper right clus-ters, with the phone in the middle and the ApexDVD player in the upper left and upper middle.In this image, the pink dots are the Apex DVDreviews.Figure 4.
Thematic clustering of product reviewThe affect measurements on these documentsgenerate five clusters in our system, each ofwhich is summarized with a rose plot showingaffect variation.
This gives us information on therange and distribution of affect overall in thisdata.
We can select one of these plots, either toreview the documents or to interact further.
Se-lection is indicated with a green border, as shownin the upper middle plot of Figure 5.Figure 5.
Clusters by affect, with one clusterglyph selected.The selected documents are relatively positive;they have higher scores in the Positive and Vir-tue axes and lower scores in the Negative axis.We may want to see how the documents in this27affect cluster distribute over the five products.This question is answered by the correlation tool,shown in Figure 6; the positive affect clustercontains more reviews on the Zen MP3 playerthan any of the other products.Figure 6.
Products represented in one of the posi-tive affect clusters.Alternatively we could get a summary of af-fect per product.
Figure 7 shows the affect forthe Apex DVD player and the Nokia cell phone.While both are positive, the Apex has strongernegative ratings than the Nokia.Figure 7.
Comparison of Affect Scores of Nokiato ApexMore detail is apparent by looking at the clus-ters within one or more groups and examiningthe deviations.
Figure 8 shows the sub-clusterswithin the Apex group.
We include the summaryfor the group as a whole (directly beneath theApex label), and then show the four sub-clustersby illustrating how they deviate from expectedvalue.
We see that two of these tend to be morepositive than expected and two are more negativethan expected.Figure 8.
Summary of Apex products with sub-clusters showing deviations.Figure 9.
Thematic distribution of reviews forone product (Apex).Looking at the thematic distribution amongthe Apex documents shows topics that dominateits reviews (Figure 9).We can examine the affect across these vari-ous clusters.
Figure 10 shows the comparison ofthe ?service?
cluster to the ?dvd player picture?cluster.
This graphic demonstrates that docu-ments with ?service?
as a main theme tend to bemuch more negative, while documents with ?pic-ture?
as a main theme are much more positive.28Figure 10.
Affect summary and variation for?service?
cluster and ?picture?
cluster.The visualization tool includes a documentviewer so that any selection of documents can bereviewed.
For example, a user may be interestedin why the ?service?
documents tend to be nega-tive, in which case they can review the originalreviews.
The doc viewer, shown in Figure 11,can be used at any stage in the process with anynumber of documents selected.
Individual docu-ments can be viewed by clicking on a documenttitle in the upper portion of the doc viewer.Figure 11: The Doc Viewer.In this case study, we have illustrated the use-fulness of visualizing the emotional content of adocument collection.
Using the tools presentedhere, we can summarize the dataset by sayingthat in general, the customer reviews are positive(Figure 5), but reviews for some products aremore positive than others (Figures 6 and 7).
Inaddition to the general content of the reviews, wecan narrow our focus to the features contained inthe reviews.
We saw that while reviews for Apexare generally positive (Figure 8), reviews aboutApex ?service?
tend to be much more negativethan reviews about Apex ?picture?
(Figure 10).5 EvaluationIN-SPIRE is a document visualization tool that isdesigned to explore the thematic content of alarge collection of documents.
In this paper, wehave described the added functionality of explor-ing affect as one of the possible dimensions.
Asan exploratory system, it is difficult to defineappropriate evaluation metric.
Because the goalof our system is not to discretely bin the docu-ments into affect categories, traditional metricssuch as precision are not applicable.
However, toget a sense of the coverage of our lexicon, we didcompare our measurements to the hand annota-tions provided for the customer review dataset.The dataset had hand scores (-3-3) for eachfeature contained in each review.
We summedthese scores to discretely bin them into positive(>0) or negative (<0).
We did this both at thefeature level and the review level (by looking atthe cumulative score for all the features in thereview).
We compared these categorizations tothe scores output by our measurement tool.
If adocument had a higher proportion of positivewords than negative, we classified it as positive,and negative if it had a higher proportion ofnegative words.
Using a chi-square, we foundthat the categorizations from our system wererelated with the hand annotations for both thewhole reviews (chi-square=33.02, df=4,p<0.0001) and the individual features (chi-square=150.6, df=4, p<0.0001), with actualagreement around 71% for both datasets.
Whilethis number is not in itself impressive, recall thatour lexicon was built independently of the datafor which is was applied.
W also expect someagreement to be lost by conflating all scores intodiscrete bins, we expect that if we compared thenumeric values of the hand annotations and ourscores, we would have stronger correlations.These scores only provide an indication thatthe lexicon we used correlates with the hand an-notations for the same data.
As an exploratorysystem, however, a better evaluation metricwould be a user study in which we get feedbackon the usefulness of this capability in accom-plishing a variety of analytical tasks.
IN-SPIREis currently deployed in a number of settings,both commercial and government.
The addedcapabilities for interactively exploring affecthave recently been deployed.
We plan to conducta variety of user evaluations in-situ that focus onits utility in a number of different tasks.
Resultsof these studies will help steer the further devel-opment of this methodology.296 ConclusionWe have developed a measurement and visuali-zation approach to affect that we expect to beuseful in the context of the IN-SPIRE text analy-sis toolkit.
Our innovations include the flexibilityof the lexicons used, the measurement options,the bootstrapping method and utility for lexicondevelopment, and the visualization of affect us-ing rose plots and interactive exploration in thecontext of an established text analysis toolkit.While the case study presented here was con-ducted in English, all tools described are lan-guage independent and we have begun exploringand creating lexicons of affect bearing words inmultiple languages.ReferencesA.
Aue.
& M. Gamon.
2005.
Customizing Senti-ment Classifiers to New Domains: a Case Study.Submitted  RANLP.S.
Deerwester, S.T.
Dumais, T.K.
Landauer, G.W.Furnas, and R.A. Harshman.
1990.
Indexing by La-tent Semantic Analysis.
Journal of the Society forInformation Science, 41(6):391?407.J.
M. DiMicco, V. Lakshmipathy, A. T. Fiore.
2002.Conductive Chat: Instant Messaging With a SkinConductivity Channel.
In Proceedings of Confer-ence on  Computer Supported Cooperative Work.D.
G. Feitelson.
2003.
Comparing Partitions with SpieCharts.
Technical Report 2003-87, School of Com-puter Science and Engineering, The Hebrew Uni-versity of Jerusalem.E.
Hetzler and A. Turner.
2004.
Analysis ExperiencesUsing Information Visualization.
IEEE ComputerGraphics and Applications, 24(5):22-26, 2004.M.
Hu and B. Liu.
2004.
Mining Opinion Features inCustomer Reviews.
In Proceedings of NineteenthNational Conference on Artificial Intelligence(AAAI-2004).H.
Lieberman, H. Liu, P. Singh and B. Barry.
2004.Beating Common Sense into Interactive Applica-tions.
AI Magazine 25(4): Winter 2004, 63-76.B.
Liu, M. Hu and J. Cheng.
2005.
Opinion Observer:Analyzing and Comparing Opinions on the Web.Proceedings of the 14th international World WideWeb conference (WWW-2005), May 10-14, 2005:Chiba, Japan.H.
Liu, T. Selker, H. Lieberman.
2003.
Visualizingthe Affective Structure of a Text Document.
Com-puter Human Interaction, April 5-10, 2003: FortLauderdale.M.
Mandic and A. Kerne.
2004. faMailiar?Intimacy-based Email Visualization.
In Proceedings of IEEEInformation Visualization 2004, Austin Texas, 31-32.B.
Pang and L. Lee.
2004.
A Sentimental Education:Sentiment Analysis Using Subjectivity Summariza-tion Based on Minimum Cuts.
In Proceedings ofthe 42nd ACL, pp.
271-278, 2004.Y.
Qu,, J. Shanahan, and J. Weibe.
2004.
ExploringAttitude and Affect in Text: Theories and Applica-tions.
Technical Report SS-04-07.E.
Riloff and R. Jones.
1999.
Learning Dictionariesfor Information Extraction by Multi-Level Boot-strapping.
Proceedings of the Sixteenth NationalConference on Artificial Intelligence (AAAI-99)pp.
474-479.H.
Scheff?.
1999.
The Analysis of Variance, Wiley-Interscience.P.
Stone.
1977.
Thematic Text Analysis: New Agen-das for Analyzing Text Content.
In Text Analysisfor the Social Sciences, ed.
Carl Roberts, LawrenceErlbaum Associates.A.
Tat and S. Carpendale.
2002.
Visualizing HumanDialog.
In Proceedings of IEEE Conference on In-formation Visualization, IV'02, p.16-24, London,UK.P.
Turney and M. Littman.
2003.
Measuring Praiseand Criticism: Inference of Semantic Orientationfrom Association.
ACM Transactions on Informa-tion Systems (TOIS) 21:315-346.H.
Wainer.
1997.
A Rose by Another Name.?
VisualRevelations, Copernicus Books, New York.H.
Wang, H. Prendinger, and T. Igarashi.
2004.
Com-municating Emotions in Online Chat Using Physio-logical Sensors and Animated Text.?
In Proceed-ings of ACM SIGCHI Conference on Human Fac-tors in Computing Systems (CHI'04), Vienna, Aus-tria, April 24-29.J.
Wiebe and Ellen Riloff.
2005.
Creating Subjectiveand Objective Sentence Classifiers from Unanno-tated Texts.?
In Proceedings of Sixth InternationalConference on Intelligent Text Processing andComputational Linguistics.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recog-nizing Contextual Polarity in Phrase-Level Senti-ment Analysis.?
In Proceeding of HLT-EMNLP-2005.30
