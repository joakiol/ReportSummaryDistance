Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 229?232,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPOptimizing Word Alignment Combination For Phrase Table TrainingYonggang Deng and Bowen ZhouIBM T.J. Watson Research CenterYorktown Heights, NY 10598, USA{ydeng,zhou}@us.ibm.comAbstractCombining word alignments trained intwo translation directions has mostly re-lied on heuristics that are not directlymotivated by intended applications.
Wepropose a novel method that performscombination as an optimization process.Our algorithm explicitly maximizes the ef-fectiveness function with greedy searchfor phrase table training or synchronizedgrammar extraction.
Experimental resultsshow that the proposed method leads tosignificantly better translation quality thanexisting methods.
Analysis suggests thatthis simple approach is able to maintainaccuracy while maximizing coverage.1 IntroductionWord alignment is the process of identifyingword-to-word links between parallel sentences.
Itis a fundamental and often a necessary step beforelinguistic knowledge acquisitions, such as train-ing a phrase translation table in phrasal machinetranslation (MT) system (Koehn et al, 2003), orextracting hierarchial phrase rules or synchronizedgrammars in syntax-based translation framework.Most word alignment models distinguish trans-lation direction in deriving word alignment matrix.Given a parallel sentence, word alignments in twodirections are established first, and then they arecombined as knowledge source for phrase train-ing or rule extraction.
This process is also calledsymmetrization.
It is a common practice in moststate of the art MT systems.
Widely used align-ment models, such as IBM Model serial (Brownet al, 1993) and HMM , all assume one-to-manyalignments.
Since many-to-many links are com-monly observed in natural language, symmetriza-tion is able to make up for this modeling limita-tion.
On the other hand, combining two direc-tional alignments practically can lead to improvedperformance.
Symmetrization can also be real-ized during alignment model training (Liang et al,2006; Zens et al, 2004).Given two sets of word alignments trained intwo translation directions, two extreme combina-tion are intersection and union.
While intersec-tion achieves high precision with low recall, unionis the opposite.
A right balance of these two ex-treme cases would offer a good coverage with rea-sonable accuracy.
So starting from intersection,gradually adding elements in the union by heuris-tics is typically used.
Koehn et al (2003) growthe set of word links by appending neighboringpoints, while Och and Hey (2003) try to avoid bothhorizontal and vertical neighbors.
These heuristic-based combination methods are not driven explic-itly by the intended application of the resultingoutput.
Ayan (2005) exploits many advanced ma-chine learning techniques for general word align-ment combination problem.
However, humanannotation is required for supervised training inthose techniques.We propose a new combination method.
Likeheuristics, we aim to find a balance between in-tersection and union.
But unlike heuristics, com-bination is carried out as an optimization processdriven by an effectiveness function.
We evaluatethe impact of each alignment pair w.r.t.
the targetapplication, say phrase table training, and gradu-ally add or remove the word link that currentlycan maximize the predicted benefit measured bythe effectiveness function.
More specifically, weconsider the goal of word alignment combinationis for phrase table training, and we directly moti-vate word alignment combination as a process ofmaximizing the number of phrase translations thatcan be extracted within a sentence pair.2 Combination As Optimization ProcessGiven a parallel sentence (e = eI1, f = fJ1), aword link is represented by a pair of indices (i, j),229which means that Foreign word fjis aligned withEnglish word ei.
The direction of word alignmentsis ignored.
Since the goal of word alignment com-bination is for phrase table training, we first for-mally define a phrase translation.
Provided witha set of static word alignments A, a phrase pair(ei2i1, fj2j1) is considered translation of each other ifand only if there exists at least one word link be-tween them and no cross phrase boundary links ex-ist in A, i.e., for all (i, j) ?
A, i ?
[i1, i2] iff j ?
[j1, j2].
Notice that by this definition, it does notmatter whether boundary words of the phrase pairsshould be aligned or not.
Let PPn(A) denote theset of phrase pairs that can be extracted with Awhere up to n boundary words are allowed to benot-aligned, i.e., aligned to empty word NULL.
Ascan be imagined, increasing n would improve re-call of phrase table but likely to hurt precision.
Forword alignment combination, we focus on the setwith high accuracy where n = 0.Let A1, A2denote two sets of word alignmentsto be combined for the given sentence pair.
Forinstance, A1could be word alignments from En-glish to foreign while A2the other direction.
Ondifferent setup, A1could be Model-4 alignments,while A2is from HMM.
In the first combinationmethod we presented in Algorithm 1, we start withintersection AI.
Acis the candidate link set to beevaluated and appended to the combined set A. Itsinitial value is the difference between union andintersection.
We assume that there is an effective-ness function g(?)
which quantitatively measuresthe ?goodness?
of a alignment set for the intendedapplication.
A higher number indicates a betteralignment set.
We use the function g to drive theprocess.
Each time, we identify the best word link(?i,?j) in the candidate set that can maximize thefunction g and append it to the current set A. Thisprocess is repeated until the candidate set is emptyor adding any link in the set would lead to degra-dation.
Finally (line 15 to 21), we pickup wordlinks in the candidate set to align those uncov-ered words.
This is applied to maximize cover-age, which is similar as the ?final?
in (Koehn et al,2003).
Again, we use the function g(?)
to rank theword links in Acand sequentially append them toA depending on current word coverage.The algorithm clearly is a greedy search pro-cedure that maximizes the function g. Since weplan to take the combined word alignments forphrase translation training, a natural choice forg is the number of phrase pairs that can be ex-tracted with the given alignment set.
We chooseg(A) = |PP0(A)|, where we only count phrasepairs that all boundary words are aligned.
Thereason of putting a tight constraint is to maintainphrase table accuracy while improving the cover-age.
By keeping track of the span of currentlyaligned words, we can have efficient implemen-tation of the function g.Algorithm 1 Combination ofA1andA2as an OptimizedExpanding Process1: AI= A1?A2, AU= A1?A22: A = AI, Ac= AU?AI3: total = g(A)4: while Ac6= ?
do5: curMax = max(i,j)?Acg(A ?
{(i, j)})6: if curMax ?
total then7: (?i,?j) = argmax(i,j)?Acg(A ?
{(i, j)})8: A = A ?
{(?i,?j)}9: Ac= Ac?
{(?i,?j)}10: total = curMax11: else {adding any link will make it worse}12: break13: end if14: end while15: while Ac6= ?
do16: (?i,?j) = argmax(i,j)?Acg(A ?
{(i, j)})17: if e?iis not aligned or f?jis not aligned then18: A = A ?
{(?i,?j)}19: end if20: Ac= Ac?
{(?i,?j)}21: end while22: return AAlternatively, the optimization can go in oppo-site direction.
We start with the union A = AU,and gradually remove the worse word link (?i,?j) =argmax(i,j)?Acg(A ?
{(i, j)}) that could max-imize the effectiveness function.
Similarly, thisshrinking process is repeated until either candidateset is empty or removing any link in the candidateset would reduce the value of function g.Other choice of ?goodness?
function g is pos-sible.
For instance, one could consider syntacticconstraints, or weight phrase pairs differently ac-cording to their global co-occurrence.
The basicidea is to implement the combination as an itera-tive customized optimization process that is drivenby the application.3 Experimental ResultsWe test the proposed new idea on Persian Farsi toEnglish translation.
The task is to translate spokenFarsi into English.
We decode reference transcrip-tion so recognition is not an issue.
The training230data was provided by the DARPA TransTac pro-gram.
It consists of around 110K sentence pairswith 850K English words in the military forceprotection domain.
We train IBM Model-4 usingGIZA++ toolkit (Och and Ney, 2003) in two trans-lation directions and perform different word align-ment combination.
The resulting alignment set isused to train a phrase translation table, where Farsiphrases are limited to up to 6 words.The quality of resulting phrase translation tableis measured by translation results.
Our decoderis a phrase-based multi-stack implementation ofthe log-linear model similar to Pharaoh (Koehn etal., 2003).
Like other log-linear model based de-coders, active features in our translation engine in-clude translation models in two directions, lexiconweights in two directions, language model, lexi-calized reordering models, sentence length penaltyand other heuristics.
These feature weights aretuned on the dev set to achieve optimal transla-tion performance evaluated by automatic metric.The language model is a statistical 4-gram modelestimated with Modified Kneser-Ney smoothing(Chen and Goodman, 1996) using only Englishsentences in the parallel training data.3.1 Phrase Table ComparisonWe first study the impact of different word align-ment combination methods on phrase translationtable, and compare our approaches to heuristicbased methods.
The same English to Farsi andFarsi to English Model-4 word alignments areused, but we try different combination methodsand analysis the final alignment set and the result-ing phase translation table.
Table 1 presents somestatistics.
Each row corresponds to a particularcombination.
The first two are intersection (I) andunion (U).
The next two methods are heuristic (H)in (Och and Ney, 2003) and grow-diagonal (GD)proposed in (Koehn et al, 2003).
Our proposedmethods are presented in the following two rows:one is optimization as an expanding process (OE),the other is optimization as an shrinking process(OS).
In the last four rows, we add ?final?
opera-tion (line 15 to 21 in Algorithm 1).For each method, we calculate the output align-ment set size as a percentage of the union (the2nd column) and resulting phrase table (PPn(A))size (in thousand) with different constrain on themaximum number of unaligned boundary wordsn = 0, 1, 2 (the next 3 columns).
As we cansee, the intersection has less than half of all wordlinks in the pool.
This implies the underlying wordalignment quality leaves much room for improve-ments, mainly due to data sparseness.
Not sur-prisingly, when relaxing unaligned boundary wordnumber from 0 to 2, the phrase table size increasesmore than 7 times.
This is the result of very lowrecall of word alignments, consequently the esti-mated phrase table PP2(A) has very low accu-racy.
Union suffers from the opposite problem:many incorrect word links prevent good phrasepairs from being extracted.The two heuristic methods and our proposedoptimization approaches achieve somewhat a bal-ance between I and U.
By comparing size ofPP0(A) (3rd column), optimization methods areable to identify much more phrase pairs with sim-ilar size of alignment set.
This confirms that thenew method is indeed moving to the desired di-rection of extracting as many accurate (all bound-ary words should be aligned) phrase pairs as pos-sible.
We still notice that ratio of |PP2(A)| and|PP0(A)| (the last column) is high.
We suspectthat the ratio of this two phrase table size mightsomewhat be indicative of the phrase table accu-racy, which is hard to estimate without manual an-notation though.Method|A||AU||PP0| |PP1| |PP2||PP2||PP0|I 45% 424 2047 3658 8.63U 100% 354 555 578 1.63H 78% 538 1225 1519 2.82GD 82% 499 1081 1484 2.97OS 84% 592 1110 1210 2.04OE 78% 659 1359 1615 2.45HF 95% 427 670 697 1.63GDF 97% 412 647 673 1.63OSF 89% 484 752 781 1.61OEF 89% 476 739 768 1.61Table 1: Statistics of word alignment set and theresulting phrase table size (number of entries inthousand (K)) with different combination methods3.2 Translation ResultsThe ultimate goal of word alignment combinationis for building translation system.
The quality ofresulting phrase tables is measured by automatictranslation metric.
We have one dev set (1430 sen-tences with 11483 running words), test set 1 (1390sentences with 10334 running words) and test set2 (417 sentences with 4239 running words).
Thedev set and test set 1 are part of all available Farsi-231English parallel corpus.
They are holdout fromtraining data as tuning and testing.
The test set 2is the standard NIST offline evaluation set, where4 references are available for each sentence.
Thedev and test set 1 are much closer to the trainingset than the standard test set 2.
We tune all fea-ture weights automatically (Och, 2003) to maxi-mize the BLEU (Papineni et al, 2002) score onthe dev set.Table 2 shows BLEU score of different com-bination methods on all three sets.
Union per-forms much worse on the dev and test1 than inter-section, while intersection achieved the same per-formance on test2 as union but with more than 6times of phrase table size.
Grow-diagonal (GD)has more than 1 bleu point on test2 than intersec-tion but with less than half of phrase table size.The proposed new method OE is consistently bet-ter than both heuristic methods GD and H, withmore than 1 point on dev/teset1 and 0.7 point ontest2.
Comparing the last group to the middle one,we can see the effect of the ?final?
operation onall four methods.
Tabel 1 shows that after apply-ing the final operation, phrase table size is cut intohalf.
When evaluated with automatic translationmetric, all four methods generally perform muchworse on dev and test1 that are close to trainingdata, but better on NIST standard test2.
We ob-serve half BLEU point improvement for optimiza-tion method but marginal gain for heuristic-basedapproaches.
This suggest that the phrase table ac-curacy get improved with the final operation.
Op-timization method directly tries to maximize thenumber of phrase pairs that can be extracted.
Weobserve that it (OEF) is able to find more than14% more phrase pairs than heuristic methods andachieve 1 BLEU point gain than the best heuristicmethod (GDF).Method dev test1 test2I 0.396 0.308 0.348U 0.341 0.294 0.348H 0.400 0.314 0.341GD 0.391 0.314 0.360OS 0.383 0.316 0.356OE 0.410 0.329 0.367HF 0.361 0.297 0.343GDF 0.361 0.301 0.362OSF 0.372 0.305 0.361OEF 0.370 0.306 0.372Table 2: Translation results (BLEU score) withphrase tables trained with different word align-ment combination methods4 ConclusionsWe presented a simple yet effective method forword alignment symmetrization and combinationin general.
The problem is formulated as an opti-mization with greedy search driven by an effec-tiveness function, which can be customized di-rectly to maximum benefit for intended applica-tions such as phrase table training or synchronizedgrammar extraction in machine translation.
Ex-perimental results demonstrated consistent betterBLEU scores than the best heuristic method.
Theoptimization process can better maintain accuracywhile improving coverage.The algorithm is generic and leaves much spacefor variations.
For instance, designing a better ef-fectiveness function g, or considering a soft linkwith some probability rather than binary 0/1 con-nection would potentially be opportunities for fur-ther improvement.
On the other hand, the searchspace of current algorithm is limited by the poolof candidate set, it is possible to suggest new linkswhile driven by the target function.Acknowledgments We thank the DARPATransTac program for funding and the anonymousreviewers for their constructive suggestions.ReferencesN.
F. Ayan.
2005.
Combining Linguistic andMachine Learn-ing Techniques for Word Alignment Improvement.
Ph.D.thesis, University of Maryland, College Park, November.P.
Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.1993.
The mathematics of machine translation: Parameterestimation.
Computational Linguistics, 19:263?312.S.
F. Chen and J. Goodman.
1996.
An empirical study ofsmoothing techniques for language modeling.
In Proc.
ofACL, pages 310?318.P.
Koehn, F. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In Proc.
of HLT-NAACL, pages 48?54.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment by agree-ment.
In Proc.
of HLT-NAACL, pages 104?111.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
Computational Lin-guistics, 29(1):19?51.F.
J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
of ACL, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.
Bleu:a method for automatic evaluation of machine translation.In Proc.
of ACL, pages 311?318.R.
Zens, E. Matusov, and H. Ney.
2004.
Improved wordalignment using a symmetric lexicon model.
In Proc.
ofCOLING, pages 36?42.232
