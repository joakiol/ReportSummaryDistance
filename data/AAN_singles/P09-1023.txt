Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 199?207,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPSummarizing Definition from WikipediaShiren Ye and Tat-Seng Chua and Jie LuLab of Media SearchNational University of Singapore{yesr|chuats|luj}@comp.nus.edu.sgAbstractWikipedia provides a wealth of knowl-edge, where the first sentence, infobox(and relevant sentences), and even the en-tire document of a wiki article could beconsidered as diverse versions of sum-maries (definitions) of the target topic.We explore how to generate a series ofsummaries with various lengths based onthem.
To obtain more reliable associationsbetween sentences, we introduce wiki con-cepts according to the internal links inWikipedia.
In addition, we develop anextended document concept lattice modelto combine wiki concepts and non-textualfeatures such as the outline and infobox.The model can concatenate representativesentences from non-overlapping salient lo-cal topics for summary generation.
We testour model based on our annotated wiki ar-ticles which topics come from TREC-QA2004-2006 evaluations.
The results showthat the model is effective in summariza-tion and definition QA.1 IntroductionNowadays, ?ask Wikipedia?
has become as pop-ular as ?Google it?
during Internet surfing, asWikipedia is able to provide reliable informationabout the concept (entity) that the users want.
Asthe largest online encyclopedia, Wikipedia assem-bles immense human knowledge from thousands ofvolunteer editors, and exhibits significant contribu-tions to NLP problems such as semantic related-ness, word sense disambiguation and question an-swering (QA).For a given definition query, many search en-gines (e.g., specified by ?define:?
in Google) oftenplace the first sentence of the corresponding wiki1article at the top of the returned list.
The use of1 For readability, we follow the upper/lower case ruleon web (say, ?web pages?
and ?on the Web?
), and utilizeone-sentence snippets provides a brief and concisedescription of the query.
However, users often needmore information beyond such a one-sentence de-finition, while feeling that the corresponding wikiarticle is too long.
Thus, there is a strong demandto summarize wiki articles as definitions with vari-ous lengths to suite different user needs.The initial motivation of this investigation is tofind better definition answer for TREC-QA taskusing Wikipedia (Kor and Chua, 2007).
Accord-ing to past results on TREC-QA (Voorhees, 2004;Voorhees and Dang, 2005), definition queries areusually recognized as being more difficult than fac-toid and list queries.
Wikipedia could help toimprove the quality of answer finding and evenprovide the answers directly.
Its results are bet-ter than other external resources such as WordNet,Gazetteers and Google?s define operator, especiallyfor definition QA (Lita et al, 2004).Different from the free text used in QA and sum-marization, a wiki article usually contains valuableinformation like infobox and wiki link.
Infoboxtabulates the key properties about the target, suchas birth place/date and spouse for a person as wellas type, founder and products for a company.
In-fobox, as a form of thumbnail biography, can beconsidered as a mini version of a wiki article?s sum-mary.
In addition, the relevant concepts existing ina wiki article usually refer to other wiki pages bywiki internal links, which will form a close set ofreference relations.
The current Wikipedia recur-sively defines over 2 million concepts (in English)via wiki links.
Most of these concepts are multi-word terms, whereas WordNet has only 50,000 plusmulti-word terms.
Any term could appear in thedefinition of a concept if necessary, while the totalvocabulary existing in WordNet?s glossary defini-tion is less than 2000.
Wikipedia addresses explicitsemantics for numerous concepts.
These specialknowledge representations will provide additionalinformation for analysis and summarization.
Wethus need to extend existing summarization tech-nologies to take advantage of the knowledge repre-sentations in Wikipedia.
?wiki(pedia) articles?
and ?on (the) Wikipedia?, the latter re-ferring to the entire Wikipedia.199The goal of this investigation is to explore sum-maries with different lengths in Wikipedia.
Ourmain contribution lies in developing a summariza-tion method that can (i) explore more reliable asso-ciations between passages (sentences) in huge fea-ture space represented by wiki concepts; and (ii) ef-fectively combine textual and non-textual featuressuch as infobox and outline in Wikipedia to gener-ate summaries as definition.The rest of this paper is organized as follows: Inthe next section, we discuss the background of sum-marization using both textual and structural fea-tures.
Section 3 presents the extended documentconcept lattice model for summarizing wiki arti-cles.
Section 4 describes corpus construction andexperiments are described; while Section 5 con-cludes the paper.2 BackgroundBesides some heuristic rules such as sentence po-sition and cue words, typical summarization sys-tems measure the associations (links) between sen-tences by term repetitions (e.g., LexRank (Erkanand Radev, 2004)).
However, sophisticated authorsusually utilize synonyms and paraphrases in vari-ous forms rather than simple term repetitions.
Fur-nas et al (1987) reported that two people choosethe same main key word for a single well-knownobject less than 20% of the time.
A case study byYe et al (2007) showed that 61 different words ex-isting in 8 relevant sentences could be mapped into16 distinctive concepts by means of grouping termswith close semantic (such as [British, Britain, UK]and [war, fought, conflict, military]).
However,most existing summarization systems only considerthe repeated words between sentences, where latentassociations in terms of inter-word synonyms andparaphrases are ignored.
The incomplete data likelylead to unreliable sentence ranking and selection forsummary generation.To recover the hidden associations between sen-tences, Ye et al (2007) compute the semantic simi-larity using WordNet.
The term pairs with semanticsimilarity higher than a predefined threshold will begrouped together.
They demonstrated that collect-ing more links between sentences will lead to bet-ter summarization as measured by ROUGE scores,and such systems were rated among the top systemsin DUC (document understanding conference) in2005 and 2006.
This WordNet-based approach hasseveral shortcomings due to the problems of datadeficiency and word sense ambiguity, etc.Wikipedia already defined millions of multi-word concepts in separate articles.
Its definition ismuch larger than that of WordNet.
For instance,more than 20 kinds of songs and movies called But-terfly , such as Butterfly (Kumi Koda song), Butter-fly (1999 film) and Butterfly (2004 film), are listedin Wikipedia.
When people say something aboutbutterfly in Wikipedia, usually, a link is assignedto refer to a particular butterfly.
Following thislink, we can acquire its explicit and exact seman-tic (Gabrilovich and Markovitch, 2007), especiallyfor multi-word concepts.
Phrases are more im-portant than individual words for document re-trieval (Liu et al, 2004).
We hope that the wiki con-cepts are appropriate text representation for sum-marization.Generally, wiki articles have little redundancyin their contents as they utilize encyclopedia style.Their authors tend to use wiki links and ?See Also?links to refer to the involved concepts rather thanexpand these concepts.
In general, the guidelinefor composing wiki articles is to avoid overlongand over-complicated styles.
Thus, the strategy of?split it?
into a series of articles is recommended;so wiki articles are usually not too long and containlimited number of sentences.
These factors lead tofewer links between sentences within a wiki article,as compared to normal documents.
However, theprinciple of typical extractive summarization ap-proaches is that the sentences whose contents arerepeatedly emphasized by the authors are most im-portant and should be included (Silber and McCoy,2002).
Therefore, it is challenging to summarizewiki articles due to low redundancy (and links)between sentences.
To overcome this problem,we seek (i) more reliable links between passages,(ii) appropriate weighting metric to emphasize thesalient concepts about the topic, and (iii) additionalguideline on utilizing non-textual features such asoutline and infobox.
Thus, we develop wiki con-cepts to replace ?bag-of-words?
approach for betterlink measurements between sentences, and extendan existing summarization model on free text to in-tegrate structural information.By analyzing rhetorical discourse structure ofaim, background, solution, etc.
or citation context,we can obtain appropriate abstracts and the mostinfluential contents from scientific articles (Teufeland Moens, 2002; Mei and Zhai, 2008).
Similarly,we believe that the structural information such asinfobox and outline is able to improve summariza-tion as well.
The outline of a wiki article using in-ner links will render the structure of its definition.In addition, infobox could be considered as topicsignature (Lin and Hovy, 2000) or keywords aboutthe topic.
Since keywords and summary of a doc-ument can be mutually boosted (Wan et al, 2007),infobox is capable of summarization instruction.When Ahn (2004) and Kor (2007) utilizeWikipedia for TREC-QA definition, they treat theWikipedia as the Web and perform normal searchon it.
High-frequency terms in the query snippetsreturned from wiki index are used to extend queryand rank (re-rank) passages.
These snippets usually200come from multiple wiki articles.
Here the use-ful information may be beyond these snippets butexisting terms are possibly irrelevant to the topic.On the contrary, our approach concentrates on thewiki article having the exact topic only.
We as-sume that every sentence in the article is used to de-fine the query topic, no matter whether it containsthe term(s) of the topic or not.
In order to extractsome salient sentences from the article as definitionsummaries, we will build a summarization modelthat describes the relations between the sentences,where both textual and structural features are con-sidered.3 Our Approach3.1 Wiki ConceptsIn this subsection, we address how to find rea-sonable and reliable links between sentences usingwiki concepts.Consider a sentence: ?After graduating fromBoston University in 1988, she went to work at aCalvin Klein store in Boston.?
from a wiki article?Carolyn Bessette Kennedy?2, we can find 11 dis-tinctive terms, such as after, graduate, Boston, Uni-versity,1988, go, work, Calvin, Klein, store, Boston,if stop words are ignored.However, multi-word terms such as BostonUniversity and Calvin Klein are linked to thecorresponding wiki articles, where their definitionsare given.
Clearly, considering the anchor texts astwo wiki concepts rather than four words is morereasonable.
Their granularity are closer to semanticcontent units in a summarization evaluation methodPyramid (Nenkova et al, 2007) and nuggets inTREC-QA .
When the text is represented bywiki concepts, whose granularity is similar to theevaluation units, it is possibly easy to detect thematching output using a model.
Here,?
Two separate words, Calvin and Klein, aremeaningless and should be discarded; oth-erwise, spurious links between sentences arelikely to occur.?
Boston University and Boston are processedseparately, as they are different named entities.No link between them is appropriate3.?
Terms such as ?John F. Kennedy, Jr.?
and?John F. Kennedy?
will be considered as twodiverse wiki concepts, but we do not accounton how many repeated words there are.?
Different anchor texts, such as U.S.A. andUnited States of America, are recognized as2All sample sentences in this paper come from this articleif not specified.3Consider new pseudo sentence: ?After graduating fromStanford in 1988, she went to work ... in Boston.?
We do notneed assign link between Stanford and Boston as well.an identical concept since they refer to thesame wiki article.?
Two concepts, such as money and cash, willbe merged into an identical concept when theirsemantics are similar.In wiki articles, the first occurrence of a wikiconcept is tagged by a wiki link, but there is nosuch a link to its subsequent occurrences in the re-maining parts of the text in most cases.
To allevi-ate this problem, a set of heuristic rules is proposedto unify the subsequent occurrences of concepts innormal text with previous wiki concepts in the an-chor text.
These heuristic rules include: (i) edit dis-tance between linked wiki concept and candidatesin normal text is larger than a predefined threshold;and (ii) partially overlapping words beginning withcapital letter, etc.After filtering out wiki concepts, the words re-maining in wiki articles could be grouped into twosets: close-class terms like pronouns and preposi-tions as well as open-class terms like nouns andverbs.
For example, in the sentence ?She died at age33, along with her husband and sister?, the open-class terms include die, age, 33, husband and sister.Even though most open-class terms are defined inWikipedia as well, the authors of the article do notconsider it necessary to present their references us-ing wiki links.
Hence, we need to extend wiki con-cepts by concatenating them with these open-classterms to form an extended vector.
In addition, weignore all close-class terms, since we cannot findefficient method to infer reliable links across them.As a result, texts are represented as a vector of wikiconcepts.Once we introduce wiki concepts to replace typ-ical ?bag-of-words?
approach, the dimensions ofconcept space will reach six order of magnitudes.We cannot ignore the data spareness issue and com-putation cost when the concept space is so huge.Actually, for a wiki article and a set of relevant arti-cles, the involved concepts are limited, and we needto explore them in a small sub-space.
For instance,59 articles about Kennedy family in Wikipedia have10,399 distinctive wiki concepts only, where 5,157wiki concepts exist twice and more.
Computing theoverlapping among them is feasible.Furthermore, we need to merge the wiki conceptswith identical or close semantic (namely, buildinglinks between these synonyms and paraphrases).We measure the semantic similarity between twoconcepts by using cosine distance between theirwiki articles, which are represented as the vectorsof wiki concepts as well.
For computation effi-ciency, we calculate semantic similarities betweenall promising concept pairs beforehand, and thenretrieve the value in a Hash table directly.
We spentCPU time of about 12.5 days preprocessing the se-201mantic calculation.
Details are available at our tech-nical report (Lu et al, 2008).Following the principle of TFIDF, we definethe weighing metric for the vector representedby wiki concepts using the entire Wikipedia asthe observation collection.
We define the CFIDFweight of wiki concept i in article j as:wi,j = cfi,j?
idfi = ni,j?k nk,j?
log |D||dj : ti ?
dj| ,(1)where cfi,j is the frequency of concept i in arti-cle j; idfi is the inverse frequency of concept iin Wikipedia; and D is the number of articles inWikipedia.
Here, sparse wiki concepts will havemore contribution.In brief, we represent articles in terms of wikiconcepts using the steps below.1.
Extract the wiki concepts marked by wikilinks in context.2.
Detect the remaining open-class terms as wikiconcepts as well.3.
Merge concepts whose semantic similarity islarger than predefined threshold (0.35 in ourexperiments) into the one with largest idf .4.
Weight all concepts according to Eqn (1).3.2 Document Concept Lattice ModelNext, we build the document concept lattice (DCL)for articles represented by wiki concepts.
For il-lustration on how DCL is built, we consider 8 sen-tences from DUC 2005 Cluster d324e (Ye et al,2007) as case study.
8 sentences, represented by 16distinctive concepts A-P, are considered as the basenodes 1-8 as shown in Figure 1.
Once we groupnodes by means of the maximal common conceptsamong base nodes hierarchically, we can obtain thederived nodes 11-41, which form a DCL.
A derivednode will annotate a local topic through a set ofshared concepts, and define a sub concept space thatcontains the covered base nodes under proper pro-jection.
The derived node, accompanied with itsbase nodes, is apt to interpret a particular argument(or statement) about the involved concepts.
Further-more, one base node among them, coupled with thecorresponding sentence, is capable of this interpre-tation and could represent the other base nodes tosome degree.In order to Extract a set of sentences to coverkey distinctive local topics (arguments) as much aspossible, we need to select a set of important non-overlapping derived nodes.
We measure the impor-tance of node N in DCL of article j in term of rep-resentative power (RP) as:RP (N) =?ci?N(|ci|?wi,j)/ log(|N |), (2)                                                  ff   fi                                                                                    fi   fl                 Figure 1: A sample of concept latticewhere concept ci in node N is weighted by wi,jaccording to Eqn (1), and |N | denotes the conceptnumber in N (if N is a base node) or the numberof distinct concepts in |N | (if N is a derived node),respectively.
Here, |ci| represents the c?s frequencyin N , and log(|N |) reflects N ?s cost if N is selected(namely, how many concepts are used in N ).
Forexample, 7 concepts in sentence 1 lead to the total|c| of 34 if their weights are set to 1 equally.
ItsRP is RP (1) = 34/log(7) = 40.23.
Similarly,RP (31) = 6 ?
3/log(3) = 37.73.By selecting a set of non-overlapping derivednodes with maximal RP, we are able to obtain a setof local topics with highest representativeness anddiversity.
Next, a representative sentence with max-imal RP in each of such derived nodes is chosen torepresent the local topics in observation.
When thelength of the required summary changes, the num-ber of the local topics needed will also be modi-fied.
Consequently, we are able to select the sets ofappropriate derived nodes in diverse generalizationlevels, and obtain various versions of summariescontaining the local topics with appropriate gran-ularities.In the DCL example shown in Figure 1, if we ex-pect to have a summary with two sentences, we willselect the derived nodes 31 and 32 with highest RP.Nodes 31 and 32 will infer sentences 4 and 2, andthey will be concatenated to form a summary.
If thesummary is increased to three sentences, then threederived nodes 31, 23 and 33 with maximal RP willrender representative sentences 4, 5 and 6.
Hence,the different number of actual sentences (4+5+6 vs.4+2) will be selected depending on the length ofthe required summary.
The uniqueness of DCL isthat the sentences used in a shorter summary maynot appear in a longer summary for the same sourcetext.
According to the distinctive derived nodes indiverse levels, the sentences with different general-ization abilities are chosen to generate various sum-maries.202Figure 2: Properties in infobox and their supportsentences3.3 Model of Extended Document ConceptLattice (EDCL)Different from free text and general web docu-ments, wiki articles contain structural features, suchas infoboxes and outlines, which correlate stronglyto nuggets in definition TREC-QA.
By integratingthese structural features, we will generate better RPmeasures in derived topics which facilitates betterpriority assignment in local topics.3.3.1 Outline: Wiki Macro StructureA long wiki article usually has a hierarchical out-line using inner links to organize its contents.
Forexample, wiki article Cat consists of a set of hier-archical sections under the outline of mouth, legs,Metabolism, genetics, etc.
This outline providesa hierarchical clustering of sub-topics assigned byits author(s), which implies that selecting sentencesfrom diverse sections of outline is apt to obtain abalanced summary.
Actually, DCL could be con-sidered as the composite of many kinds of clus-terings (Ye et al, 2007).
Importing the clusteringfrom outline into DCL will be helpful for the gen-eration of a balanced summary.
We thus incorpo-rate the structure of outline into DCL as follows:(i) treat section titles as concepts in the pseudo de-rived nodes; (ii) link these pseudo nodes and thebase nodes in this section if they share concepts;and (iii) revise base nodes?
RP in Eqn (2) (see Sec-tion 3.3.3).3.3.2 Infobox: a Mini Version of SummaryInfobox tabulates the key properties about the topicconcept of a wiki article.
It could be consideredas a mini summary, where many nuggets in TREC-QA are included.
As properties in infobox are notcomplete sentences and do not present relevant ar-guments, it is inappropriate to concatenate themas a summary.
However, they are good indicatorsfor summary generation.
Following the terms in aproperty (e.g., spouse name and graduation school),                           ff  fi  fl ffi    !
fl  "# $ % $ & ' ( & ) * + ,- .
+ / $ ( )0 ( 1 ' 2 ' 3456785769:; < = > ?
> @Figure 3: Extend document concept lattice by out-line and infobox in Wikipediawe can find the corresponding sentences in the bodyof the text that contains such terms4.
It describes thedetails about the involved property and provides therelevant arguments.
We call it support sentence.Now, again, we have a hierarchy: Infobox +properties + support sentences.
This hierarchy canbe used to render a summary by concatenating thesupport sentences.
This summary is inferred fromhand-crafted infobox directly and is a full versionof infobox; so its quality is guaranteed.
However, itis possibly inapplicable due to its improper length.Following the iterative reinforcement approach forsummarization and keyword extraction (Wan et al,2007), it could be used to refine other versions ofsummaries.
Hence, we utilize infobox and its sup-port sentences to modify nodes?
RPs in DCL so thatthe priority of local topics has bias to infobox.
Toachieve it, we extend DCL by inserting a hierarchyfrom infobox: (i) generate a pseudo derived nodefor each property; (ii) link every derived node toits support sentences; and (iii) cover these pseudonodes by a virtual derived node called infobox.3.3.3 Summary Generation from EDCLIn DCL, sentences with common concepts form lo-cal topics by autonomous approach, where sharedconcepts are depicted in derived nodes.
Now weintroduce two additional hierarchies derived fromoutline and infobox into DCL to refine RPs ofsalient local topics for summarization, which willrender a model named extended document con-cept lattice (EDCL).
As shown in Figure 3, basenodes in EDCL covered by pseudo derived nodeswill increase their RPs when they receive influencefrom outline and infobox.
Also, if RPs of their cov-ered base nodes changes, the original derived nodeswill modify their RPs as well.
Therefore, the new4Sometimes, we can find more than one appropriate sen-tence for a property.
In our investigation, we select top twosentences with the occurrence of the particular term if avail-able.203RPs in derived nodes and based nodes will lead tobetter priority of ranking derived nodes, which islikely to result in a better summary.
One importantdirect consequence of introducing the extra hierar-chies is to increase the RP of nodes relevant to out-line and infobox so that the summaries from EDCLare likely to follow human-crafted ones.The influence of human effects are transmittedin a ?V?
curve approach.
We utilize the followingsteps to generate a summary with a given length(say m sentences) from EDCL.1.
Build a normal DCL, and compute RP foreach node according to Eqn 2.2.
Generate pseudo derived nodes (denoted byP ) based on outline and infobox, and link thepseudo derived nodes to their relevant basenodes (denoted by B0).3.
Update RP in B0 by magnifying the contri-bution of shared concepts between P and N05.4.
Update RP in derived nodes that cover B0 onaccount of the new RP in B0.5.
Select m non-overlapping derived nodes withmaximal RP as the current observation.6.
Concatenate representative sentences withtop RP from each derived node in the currentobservation as output.7.
If one representative sentence is covered bymore than one derived node in step 5, theoutput will be less than m sentences.
In thiscase, we need to increase m and repeat step5-6 until m sentences are selected.4 ExperimentsThe purposes of our experiment are two-fold: (i)evaluate the effects of wiki definition to the TREC-QA task; and (ii) examine the characteristics andsummarization performance of EDCL.4.1 Corpus ConstructionWe adopt the tasks of TREC-QA in 2004-2006(TREC 12-14) as test scope.
We retrieve arti-cles with identical topic names from Wikipedia6.Non-letter transformations are permitted (e.g., from?Carolyn Bessette-Kennedy?
to ?Carolyn Bessette-Kennedy?).
Because our focus is summariza-tion evaluation, we ignore the cases in TREC-QA where the exact topics do not exist inWikipedia, even though relevant topics are avail-able (e.g., ?France wins World Cup in soccer?in TREC-QA vs. ?France national football team?5We magnify it by adding |c0| ?
wc ?
?.
Here, c0 is theshared concepts between P and N0, and ?
is the influencefactor and set to 2-5 in our experiments.6The dump is available athttp://download.wikimedia.org/.
Our dumpwas downloaded in Sept 2007.and ?2006 FIFA World Cup?
in Wikipedia).
Fi-nally, among the 215 topics in TREC 12-14, we ob-tain 180 wiki articles with the same topics.We ask 15 undergraduate and graduate studentsfrom the Department of English Literature in Na-tional University of Singapore to choose 7-14 sen-tences in the above wiki articles as extractive sum-maries.
Each wiki article is annotated by 3 per-sons separately.
In order for the volunteers to avoidthe bias from TREC-QA corpus, we do not providequeries and nuggets used in TREC-QA.
Similar toTREC nuggets, we call the selected sentences wikinuggets.
Wiki nuggets provides the ground truthof the performance evaluation, since some TRECnuggets are possibly unavailable in Wikipedia.Here, we did not ask the volunteers to createsnippets (like TREC-QA) or compose an abstrac-tive summary (like DUC).
This is because of thespecial style of wiki articles: the entire documentis a long summary without trivial stuff.
Usually, wedo not need to concatenate key phrases from diversesentences to form a recapitulative sentence.
Mean-while, selecting a set of salient sentences to form aconcise version is a relatively less time-consumingbut applicable approach.
Snippets, by and large,lead to bad readability, and therefore we do not em-ploy this approach.In addition, the volunteers also annotate 7-10pairs of question/answer for each article for fur-ther research on QA using Wikipedia.
The cor-pus, called TREC-Wiki collection, is available atour site (http://nuscu.ddns.comp.nus.edu.sg).
Thesystem of Wikipedia summarization using EDCL islaunched on the Web as well.4.2 Corpus Exploration4.2.1 Answer availabilityThe availability of answers in Wikipedia for TREC-QA could be measured in two aspects: (i) howmany TREC-QA topics have been covered byWikipedia?
and (ii) how many nuggets could befound in the corresponding wiki article?
We findthat (i) over 80% of topics (180/215) in the TREC12-14 are available in Wikipedia, and (ii) about47% TREC nuggets could be detected directly fromWikipedia (examining applet modified from Pour-pre (Lin and Demner-Fushman, 2006)).
In contrast,6,463 nuggets existing in TREC-QA 12-14 are dis-tributed in 4,175 articles from AQUAINT corpus.We can say that Wikipedia is the answer goldminefor TREC-QA questions.When we look into these TREC nuggets in wikiarticles closely, we find that most of them are em-bedded in wiki links or relevant to infobox.
It sug-gests that they are indicators for sentences havingnuggets.2044.2.2 Correlation between TREC nuggetsand non-text featuresAnalyzing the features used could let us understandsummarization better (Nenkova and Louis, 2008).Here, we focus on the statistical analysis betweenTREC/wiki nuggets and non-textual features suchas wiki links, infobox and outline.
The featuresused are introduced in Table 1.
The correlation co-efficients are listed in Table 2.Observation: (1) On the whole, wiki nuggetsexhibit higher correlation to non-textual featuresthan TREC nuggets do.
The possible reason is thatTREC nuggets are extracted from AQUAINT ratherthan Wikipedia.
(2) As compared to other features,infobox and wiki links strongly relate to nuggets.They are thus reliable features beyond text for sum-marization.
(3) Sentence positions exhibit weakcorrelation to nuggets, even though the first sen-tence of an article is a good one-sentence definition.Feature DescriptionLink Does the sentence have link?Topic rel.
Does the sentence contain anyword in topic concept?Outline rel.
Does the sentence hold word inits section title(s) (outline)?Infobox rel.
Is it a support sentence?Position First sentence of the article, firstsentence and last sentence of aparagraph, or others?Table 1: Features for correlation measurementFeature TREC nuggets Wiki nuggetsLink 0.087 0.120Topic rel.
0.038 0.058Outline rel.
0.078 0.076Infobox rel.
0.089 0.170Position -0.047 0.021Table 2: Correlation Coefficients between non-textual features in Wiki and TREC/wiki nuggets4.3 Statistical Characteristics of EDCLWe design four runs with various configurations asshown in Table 3.
We implement a sentence re-ranking program using MMR (maximal marginalrelevance) (Carbonell and Goldstein, 1998) in Run1, which is considered as the test baseline.
We ap-ply standard DCL in Run 2, where concepts aredetermined according to their definitions in Word-Net (Ye et al, 2007).
We introduce wiki conceptsfor standard DCL in Run 3.
Run 4 is the full ver-sion of EDCL, which considers both outline and in-fobox.Observations: (1) In Run 1, the average num-ber of distinctive words per article is near to 1200after stop words are filtered out.
When we mergediverse words having similar semantic according toWordNet concepts , we obtain 873 concepts per ar-ticle on average in Run 2.
The word number de-creases by about 28% as a result of the omissionof close-class terms and the merging of synonymsand paraphrases.
(2) When wiki concepts are intro-duced in Run 3, the number of concepts continuesto decrease.
Here, some adjacent single-word termsare merged into wiki concepts if they are annotatedby wiki links.
Even though the reduction of totalconcepts is limited, these new wiki concepts willgroup the terms that cannot be detected by Word-Net.
(3) DCL based on WordNet concepts has lessderived nodes (Run 3) than DCL based on wiki con-cepts does, although the former has more concepts.It implies that wiki concepts lead to higher link den-sity in DCL as more links between concepts can bedetected.
(4) Outline and infobox will bring addi-tional 54 derived nodes (from 1695 to 1741).
Ad-ditional computation cost is limited when they areintroduced into EDCL.Run 1 Word co-occurrence + MMRRun 2 Basic DCL model (WordNet concepts)Run 3 DCL + wiki conceptsRun 4 EDCL (DCL + wiki concepts + outline+ infobox)Table 3: Test configurationsConcepts Base nodes Derived nodesRun 1 1173 (number of words)Run 2 873 259 1517Run 3 826 259 1695Run 4 831 259 1741Table 4: Average node/concept numbers in DCLand EDCL4.4 Summarization Performance of EDCLWe evaluate the performance of EDCL from two as-pects such as contribution to TREC-QA definitiontask and accuracy of summarization in our TREC-Wiki collection.Since factoid/list questions are about the most es-sential information of the target as well, like Cui?sapproach (2005), we treat factoid/list answers asessential nuggets and add them to the gold stan-dard list of definition nuggets.
We set the sentencenumber of summaries generated by the system to20512.
We examine the definition quality by nugget re-call (NR) and an approximation to nugget precision(NP) on answer length.
These scores are combinedusing the F1 and F3 measures.
The recall in F3is weighted three times as important as precision.The evaluation is automatically conducted by Pour-pre v1.1 (Lin and Demner-Fushman, 2006).Based on the performance of EDCL for TREC-QA definition task listed in Table 5, we observethat: (i) When EDCL considers wiki concepts andstructural features such as outline and infobox, itsF-scores increase significantly (Run 3 and Run 4).
(ii) Table 5 also lists the results of Cui?s system(marked by asterisk) using bigram soft patterns(Cui et al, 2005), which is trained by TREC-12and tested on TREC 13.
Our EDCL can achievecomparable or better F-scores on the 180 topics inTREC 12-14.
It suggests that Wikipedia could pro-vide high-quality definition directly even though wedo not use AQUAINT.
(iii) The precision of EDCLin Run 4 outperforms that of soft-pattern approachremarkably (from 0.34 to 0.497).
One possible rea-son is that all sentences in a wiki article are orientedto its topic, and the sentence irrelevant to its topichardly occurs.NR NP F1 F3Run 1 0.247 0.304 0.273 0.252Run 2 0.262 0.325 0.290 0.267Run 3 0.443 0.431 0.431 0.442Run 4 0.538 0.497 0.517 0.534Bigram SP* 0.552 0.340 0.421 0.510Table 5: EDCL evaluated by TREC-QA nuggets                           Figure 4: Performance of summarizing Wikipediausing EDCL with different configurationsWe also test the performance of EDCL using ex-tractive summaries in TREC-Wiki collection.
Bymeans of comparing to each set of sentences se-lected by a volunteer, we examine how many ex-act annotated sentences are selected by the systemusing different configurations.
The average recallsand precisions as well as their F-scores are shownin Figure 4.Observations: (i) The structural information ofWikipeida has significant contribution to EDCL forsummarization.
We manually examine some sum-maries and find that the sentences containing morewiki links are apt to be chosen when wiki conceptsare introduced in EDCL.
Most sentences in outputsummaries in Run 4 usually have 1-3 links and rel-evant to infobox or outline.
(ii) When using wikiconcepts, infobox and outline to enrich DCL, wefind that the precision of sentence selection has im-proved more than the recall.
It reaffirms the con-clusion in the previous TREC-QA test in this sub-section.
(iii) In addition, we manually examine thesummaries on some wiki articles with common top-ics, such as car, house, money, etc.
We find that thesummaries generated by EDCL could effectivelygrasp the key information about the topics when thesentence number of summaries exceeds 10.5 Conclusion and Future WorkWikipedia recursively defines enormous conceptsin huge vector space of wiki concepts.
The explicitsemantic representation via wiki concepts allowsus to obtain more reliable links between passages.Wikipedia?s special structural features, such as wikilinks, infobox and outline, reflect the hidden humanknowledge.
The first sentence of a wiki article, in-fobox (and its support sentences), outline (and itsrelevant sentences), as well as the entire documentcould be considered as diverse summaries with var-ious lengths.
In our proposed model, local topicsare autonomously organized in a lattice structureaccording to their overlapping relations.
The hier-archies derived from infobox and outline are im-ported to refine the representative powers of localtopics by emphasizing the concepts relevant to in-fobox and outline.
Experiments indicate that ourproposed model exhibits promising performance insummarization and QA definition tasks.Of course, there are rooms to further improve themodel.
Possible improvements includes: (a) usingadvanced semantic and parsing technologies to de-tect the support and relevant sentences for infoboxand outline; (b) summarizing multiple articles in awiki category; and (c) exploring the mapping fromclose-class terms to open-class terms for more linksbetween passages is likely to forward some interest-ing results.More generally, the knowledge hidden in non-textual features of Wikipedia allow the model toharvest better definition summaries.
It is challeng-ing but possibly fruitful to recast the normal docu-ments with wiki styles so as to adopt EDCL for freetext and enrich the research efforts on other NLPtasks.206References[Ahn et al2004] David Ahn, Valentin Jijkoun, et al2004.
Using Wikipedia at the TREC QA Track.
InText REtrieval Conference.
[Carbonell and Goldstein1998] J. Carbonell andJ.
Goldstein.
1998.
The use of mmr, diversity-basedre-ranking for reordering documents and producingsummaries.
In SIGIR, pages 335?336.
[Cui et al2005] Hang Cui, Min-Yen Kan, and Tat-SengChua.
2005.
Generic soft pattern models for de-finitional question answering.
In Proceedings ofthe 28th annual international ACM SIGIR confer-ence on research and development in information re-trieval, pages 384?391, New York, NY, USA.
ACM.
[Erkan and Radev2004] Gu?nes?
Erkan and Dragomir R.Radev.
2004.
LexRank: Graph-based Lexical Cen-trality as Salience in Text Summarization.
ArtificialIntelligence Research, 22:457?479.
[Furnas et al1987] George W. Furnas, Thomas K. Lan-dauer, Louis M. Gomez, and Susan T. Dumais.1987.
The vocabulary problem in human-systemcommunication.
Communications of the ACM,30(11):964?971.
[Gabrilovich and Markovitch2007] EvgeniyGabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using wikipedia-basedexplicit semantic analysis.
In Proceedings of TheTwentieth International Joint Conference for Arti-ficial Intelligence, pages 1606?1611, Hyderabad,India.
[Kor and Chua2007] Kian-Wei Kor and Tat-Seng Chua.2007.
Interesting nuggets and their impact on defin-itional question answering.
In Proceedings of the30th annual international ACM SIGIR conferenceon Research and development in information re-trieval, pages 335?342, New York, NY, USA.
ACM.
[Lin and Demner-Fushman2006] Jimmy J. Lin andDina Demner-Fushman.
2006.
Methods for auto-matically evaluating answers to complex questions.Information Retrieval, 9(5):565?587.
[Lin and Hovy2000] Chin-Yew Lin and Eduard Hovy.2000.
The automated acquisition of topic signa-tures for text summarization.
In Proceedings of the18th conference on Computational linguistics, pages495?501, Morristown, NJ, USA.
ACL.
[Lita et al2004] Lucian Vlad Lita, Warren A.
Hunt, andEric Nyberg.
2004.
Resource analysis for ques-tion answering.
In Proceedings of the ACL 2004on Interactive poster and demonstration sessions,page 18, Morristown, NJ, USA.
ACL.
[Liu et al2004] Shuang Liu, Fang Liu, Clement Yu, andWeiyi Meng.
2004.
An effective approach to docu-ment retrieval via utilizing wordnet and recognizingphrases.
In Proceedings of the 27th annual interna-tional ACM SIGIR conference on Research and de-velopment in information retrieval, pages 266?272,New York, NY, USA.
ACM.
[Lu et al2008] Jie Lu, Shiren Ye, and Tat-SengChua.
2008.
Explore semantic similarityand semantic relatedness via wikipedia.
Tech-nical report, National Univeristy of Singapore,http://nuscu.ddns.comp.nus.edu.sg.
[Mei and Zhai2008] Qiaozhu Mei and ChengXiangZhai.
2008.
Generating impact-based summariesfor scientific literature.
In Proceedings of ACL-08:HLT, pages 816?824, Columbus, Ohio, June.
ACL.
[Nenkova and Louis2008] Ani Nenkova and AnnieLouis.
2008.
Can you summarize this?
identify-ing correlates of input difficulty for multi-documentsummarization.
In Proceedings of ACL-08: HLT,pages 825?833, Columbus, Ohio, June.
ACL.
[Nenkova et al2007] Ani Nenkova, Rebecca Passon-neau, and Kathleen McKeown.
2007.
The pyra-mid method: Incorporating human content selectionvariation in summarization evaluation.
ACM Trans-actions on Speech and Language Processing, 4(2):4.
[Silber and McCoy2002] H. Grogory Silber and Kath-leen F. McCoy.
2002.
Efficiently computed lexicalchains as an intermediate representation for auto-matic text summarization.
Computational Linguis-tics, 28(4):487?496.
[Teufel and Moens2002] Simone Teufel and MarcMoens.
2002.
Summarizing scientific articles:experiments with relevance and rhetorical sta-tus.
Computational Linguistics, 28(4):409?445,December.
[Voorhees and Dang2005] Ellen M. Voorhees andHoa Trang Dang.
2005.
Overview of the trec2005 question answering track.
In Text REtrievalConference.
[Voorhees2004] Ellen M. Voorhees.
2004.
Overviewof the trec 2004 question answering track.
In TextREtrieval Conference.
[Wan et al2007] Xiaojun Wan, Jianwu Yang, and Jian-guo Xiao.
2007.
Towards an iterative reinforcementapproach for simultaneous document summarizationand keyword extraction.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, pages 552?559, Prague, Czech Repub-lic, June.
ACL.
[Ye et al2007] Shiren Ye, Tat-Seng Chua, Min-YenKan, and Long Qiu.
2007.
Document con-cept lattice for text understanding and summariza-tion.
Information Processing and Management,43(6):1643?1662.207
