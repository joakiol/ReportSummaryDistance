Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171?180,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsLinguistic Regularities in Sparse and Explicit Word RepresentationsOmer Levy?and Yoav GoldbergComputer Science DepartmentBar-Ilan UniversityRamat-Gan, Israel{omerlevy,yoav.goldberg}@gmail.comAbstractRecent work has shown that neural-embedded word representations capturemany relational similarities, which can berecovered by means of vector arithmeticin the embedded space.
We show thatMikolov et al.
?s method of first addingand subtracting word vectors, and thensearching for a word similar to the re-sult, is equivalent to searching for a wordthat maximizes a linear combination ofthree pairwise word similarities.
Based onthis observation, we suggest an improvedmethod of recovering relational similar-ities, improving the state-of-the-art re-sults on two recent word-analogy datasets.Moreover, we demonstrate that analogyrecovery is not restricted to neural wordembeddings, and that a similar amountof relational similarities can be recoveredfrom traditional distributional word repre-sentations.1 IntroductionDeep learning methods for language processingowe much of their success to neural network lan-guage models, in which words are represented asdense real-valued vectors in Rd.
Such representa-tions are referred to as distributed word represen-tations or word embeddings, as they embed an en-tire vocabulary into a relatively low-dimensionallinear space, whose dimensions are latent contin-uous features.
The embedded word vectors aretrained over large collections of text using vari-ants of neural networks (Bengio et al., 2003; Col-lobert and Weston, 2008; Mnih and Hinton, 2008;Mikolov et al., 2011; Mikolov et al., 2013b).
The?Supported by the European Community?s SeventhFramework Programme (FP7/2007-2013) under grant agree-ment no.
287923 (EXCITEMENT).word embeddings are designed to capture whatTurney (2006) calls attributional similarities be-tween vocabulary items: words that appear in sim-ilar contexts will be close to each other in theprojected space.
The effect is grouping of wordsthat share semantic (?dog cat cow?, ?eat devour?
)or syntactic (?cars hats days?, ?emptied carrieddanced?)
properties, and are shown to be effectiveas features for various NLP tasks (Turian et al.,2010; Collobert et al., 2011; Socher et al., 2011;Al-Rfou et al., 2013).
We refer to such word rep-resentations as neural embeddings or just embed-dings.Recently, Mikolov et al.
(2013c) demonstratedthat the embeddings created by a recursive neu-ral network (RNN) encode not only attributionalsimilarities between words, but also similaritiesbetween pairs of words.
Such similarities arereferred to as linguistic regularities by Mikolovet al.
and as relational similarities by Turney(2006).
They capture, for example, the gen-der relation exhibited by the pairs ?man:woman?,?king:queen?, the language-spoken-in relation in?france:french?, ?mexico:spanish?
and the past-tense relation in ?capture:captured?, ?go:went?.Remarkably, Mikolov et al.
showed that such rela-tions are reflected in vector offsets between wordpairs (apples ?
apple ?
cars ?
car), andthat by using simple vector arithmetic one couldapply the relation and solve analogy questions ofthe form ?a is to a?as b is to ??
in which thenature of the relation is hidden.
Perhaps the mostfamous example is that the embedded representa-tion of the word queen can be roughly recoveredfrom the representations of king, man and woman:queen ?
king ?man+ womanThe recovery of relational similarities using vectorarithmetic on RNN-embedded vectors was evalu-ated on many relations, achieving state-of-the-artresults in relational similarity identification tasks171(Mikolov et al., 2013c; Zhila et al., 2013).
It waslater demonstrated that relational similarities canbe recovered in a similar fashion also from embed-dings trained with different architectures (Mikolovet al., 2013a; Mikolov et al., 2013b).This fascinating result raises a question: to whatextent are the relational semantic properties a re-sult of the embedding process?
Experiments in(Mikolov et al., 2013c) show that the RNN-basedembeddings are superior to other dense represen-tations, but how crucial is it for a representation tobe dense and low-dimensional at all?An alternative approach to representing wordsas vectors is the distributional similarity repre-sentation, or bag of contexts.
In this representa-tion, each word is associated with a very high-dimensional but sparse vector capturing the con-texts in which the word occurs.
We call such vec-tor representations explicit, as each dimension di-rectly corresponds to a particular context.
Theseexplicit vector-space representations have beenextensively studied in the NLP literature (see (Tur-ney and Pantel, 2010; Baroni and Lenci, 2010) andthe references therein), and are known to exhibita large extent of attributional similarity (Pereiraet al., 1993; Lin, 1998; Lin and Pantel, 2001;Sahlgren, 2006; Kotlerman et al., 2010).In this study, we show that similarly to theneural embedding space, the explicit vector spacealso encodes a vast amount of relational similar-ity which can be recovered in a similar fashion,suggesting the explicit vector space representationas a competitive baseline for further work on neu-ral embeddings.
Moreover, this result implies thatthe neural embedding process is not discoveringnovel patterns, but rather is doing a remarkablejob at preserving the patterns inherent in the word-context co-occurrence matrix.A key insight of this work is that the vectorarithmetic method can be decomposed into a linearcombination of three pairwise similarities (Section3).
While mathematically equivalent, we find thatthinking about the method in terms of the decom-posed formulation is much less puzzling, and pro-vides a better intuition on why we would expectthe method to perform well on the analogy re-covery task.
Furthermore, the decomposed formleads us to suggest a modified optimization objec-tive (Section 6), which outperforms the state-of-the-art at recovering relational similarities underboth representations.2 Explicit Vector Space RepresentationWe adopt the traditional word representation usedin the distributional similarity literature (Turneyand Pantel, 2010).
Each word is associated witha sparse vector capturing the contexts in which itoccurs.
We call this representation explicit, as eachdimension corresponds to a particular context.For a vocabulary V and a set of contexts C,the result is a |V |?|C| sparse matrix S in whichSijcorresponds to the strength of the associationbetween word i and context j.
The associationstrength between a word w ?
V and a contextc ?
C can take many forms.
We chose to usethe popular positive pointwise mutual information(PPMI) metric:Sij= PPMI(wi, cj)PPMI(w, c) ={0 PMI(w, c) < 0PMI(w, c) otherwisePMI(w, c) = logP (w,c)P (w)P (c)= logfreq(w,c)|corpus|freq(w)freq(c)where |corpus| is the number of items in the cor-pus, freq(w, c) is the number of times word wappeared in context c in the corpus, and freq(w),freq(c) are the corpus frequencies of the wordand the context respectively.The use of PMI in distributional similarity mod-els was introduced by Church and Hanks (1990)and widely adopted (Dagan et al., 1994; Turney,2001).
The PPMI variant dates back to at least(Niwa and Nitta, 1994), and was demonstrated toperform very well in Bullinaria and Levy (2007).In this work, we take the linear contexts inwhich words appear.
We consider each word sur-rounding the target word w in a window of 2 toeach side as a context, distinguishing between dif-ferent sequential positions.
For example, in thesentence a b c d e the contexts of the word care a?2, b?1, d+1and e+2.
Each vector?s dimen-stion is thus |C| ?
4 |V |.
Empirically, the num-ber of non-zero dimensions for vocabulary itemsin our corpus ranges between 3 (for some rare to-kens) and 474,234 (for the word ?and?
), with amean of 1595 and a median of 415.Another popular choice of context is the syntac-tic relations the word participates in (Lin, 1998;Pad?o and Lapata, 2007; Levy and Goldberg,2014).
In this paper, we chose the sequentialcontext as it is compatible with the informationavailable to the state-of-the-art neural embeddingmethod we are comparing against.1723 Analogies and Vector ArithmeticMikolov et al.
demonstrated that vector space rep-resentations encode various relational similarities,which can be recovered using vector arithmeticand used to solve word-analogy tasks.3.1 Analogy QuestionsIn a word-analogy task we are given two pairs ofwords that share a relation (e.g.
?man:woman?,?king:queen?).
The identity of the fourth word(?queen?)
is hidden, and we need to infer it basedon the other three (e.g.
answering the question:?man is to woman as king is to ?
??).
In the restof this paper, we will refer to the four words asa:a?, b:b?.
Note that the type of the relation isnot explicitly provided in the question, and solv-ing the question correctly (by a human) involvesfirst inferring the relation, and then applying it tothe third word (b).3.2 Vector ArithmeticMikolov et al.
showed that relations betweenwords are reflected to a large extent in theoffsets between their vector embeddings(queen ?
king ?
woman ?
man),and thus the vector of the hidden word b?will besimilar to the vector b ?
a + a?, suggesting thatthe analogy question can be solved by optimizing:argmaxb?
?V(sim (b?, b?
a+ a?
))where V is the vocabulary excluding the questionwords b, a and a?, and sim is a similarity mea-sure.
Specifically, they used the cosine similaritymeasure, defined as:cos (u, v) =u ?
v?u?
?v?resulting in:argmaxb?
?V(cos (b?, b?
a+ a?))
(1)Since cosine is inverse to the angle, high cosinesimilarity (close to 1) means that the vectors sharea very similar direction.
Note that this metric nor-malizes (and thus ignores) the vectors?
lengths,unlike the Euclidean distance between them.
Forreasons that will be clear later, we refer to (1) asthe 3COSADD method.An alternative to 3COSADD is to require thatthe direction of transformation be conserved:argmaxb?
?V(cos (b??
b, a??
a)) (2)This basically means that b??
b shares the samedirection with a??
a, ignoring the distances.
Werefer to this method as PAIRDIRECTION.
Thoughit was not mentioned in the paper, Mikolovet al.
(2013c) used PAIRDIRECTION for solvingthe semantic analogies of the SemEval task, and3COSADD for solving the syntactic analogies.13.3 Reinterpreting Vector ArithmeticIn Mikolov et al.
?s experiments, all word-vectorswere normalized to unit length.
Under such nor-malization, the argmax in (1) is mathematicallyequivalent to (derived using basic algebra):argmaxb?
?V(cos (b?, b)?
cos (b?, a) + cos (b?, a?
))(3)This means that solving analogy questions withvector arithmetic is mathematically equivalent toseeking a word (b?)
which is similar to b and a?but is different from a. Relational similarity isthus expressed as a sum of attributional similari-ties.
While (1) and (3) are equal, we find the intu-ition as to why (3) ought to find analogies clearer.4 Empirical SetupWe derive explicit and neural-embedded vec-tor representations, and compare their capacitiesto recover relational similarities using objectives3COSADD (eq.
3) and PAIRDIRECTION (eq.
2).Underlying Corpus and Preprocessing Previ-ous reported results on the word analogy tasks us-ing vector arithmetics were obtained using propri-etary corpora.
To make our experiments repro-ducible, we selected an open and widely accessi-ble corpus ?
the English Wikipedia.
We extractedall sentences from article bodies (excluding ti-tles, infoboxes, captions, etc) and filtered non-alphanumeric tokens, allowing mid-token symbolsas apostrophes, hyphens, commas, and periods.All the text was lowercased.
Duplicates and sen-tences with less than 5 tokens were then removed.Overall, we retained a corpus of about 1.5 billiontokens, in 77.5 million sentences.Word Representations To create contexts forboth embedding and sparse representation, weused a window of two tokens to each side (5-grams, in total), ignoring words that appeared less1This was confirmed both by our independent trials andby corresponding with the authors.173than 100 times in the corpus.
The filtered vocabu-lary contained 189,533 terms.2The explicit vector representations were createdas described in Section 2.
The neural embeddingswere created using the word2vec software3ac-companying (Mikolov et al., 2013b).
We embed-ded the vocabulary into a 600 dimensional space,using the state-of-the-art skip-gram architecture,the negative-training approach with 15 negativesamples (NEG-15), and sub-sampling of frequentwords with a parameter of 10?5.
The parametersettings follow (Mikolov et al., 2013b).4.1 Evaluation ConditionsWe evaluate the different word representations us-ing the three datasets used in previous work.
Twoof them (MSR and GOOGLE) contain analogyquestions, while the third (SEMEVAL) requiresranking of candidate word pairs according to theirrelational similarity to a set of supplied word pairs.Open Vocabulary The open vocabularydatasets (MSR and GOOGLE) present questionsof the form ?a is to a?as b is to b?
?, where b?is hidden, and must be guessed from the entirevocabulary.
Performance on these datasets ismeasured by micro-averaged accuracy.The MSR dataset4(Mikolov et al., 2013c) con-tains 8000 analogy questions.
The relations por-trayed by these questions are morpho-syntactic,and can be categorized according to parts ofspeech ?
adjectives, nouns and verbs.
Adjec-tive relations include comparative and superlative(good is to best as smart is to smartest).
Nounrelations include single and plural, possessive andnon-possessive (dog is to dog?s as cat is to cat?s).Verb relations are tense modifications (work is toworked as accept is to accepted).The GOOGLE dataset5(Mikolov et al., 2013a)contains 19544 questions.
It covers 14 relationtypes, 7 of which are semantic in nature and 7are morpho-syntactic (enumerated in Section 8).The dataset was created by manually constructingexample word-pairs of each relation, and provid-ing all the pairs of word-pairs (within each relationtype) as analogy questions.2Initial experiments with different window-sizes and cut-offs showed similar trends.3http://code.google.com/p/word2vec4research.microsoft.com/en-us/projects/rnn/5code.google.com/p/word2vec/source/browse/trunk/questions-words.txtOut-of-vocabulary words6were removed fromboth test sets.Closed Vocabulary The SEMEVAL dataset con-tains the collection of 79 semantic relations thatappeared in SemEval 2012 Task 2: Measuring Re-lation Similarity (Jurgens et al., 2012).
Each rela-tion is exemplified by a few (usually 3) character-istic word-pairs.
Given a set of several dozen tar-get word pairs, which supposedly have the samerelation, the task is to rank the target pairs ac-cording to the degree in which this relation holds.This can be cast as an analogy question in thefollowing manner: For example, take the Recipi-ent:Instrument relation with the prototypical wordpairs king:crown and police:badge.
To measurethe degree that a target word pair wife:ring has thesame relation, we form the two analogy questions?king is to crown as wife is to ring?
and ?police isto badge as wife is to ring?.
We calculate the scoreof each analogy, and average the results.
Note thatas opposed to the first two test sets, this one doesnot require searching the entire vocabulary for themost suitable word in the corpus, but rather to ranka list of existing word pairs.Following previous work, performance on SE-MEVAL was measured using accuracy, macro-averaged across all the relations.5 Preliminary ResultsOur first experiment uses 3COSADD (method (3)in Section 3) to measure the prevalence of linguis-tic regularities within each representation.Representation MSR GOOGLE SEMEVALEmbedding 53.98% 62.70% 38.49%Explicit 29.04% 45.05% 38.54%Table 1: Performance of 3COSADD on different tasks withthe explicit and neural embedding representations.The results in Table 1 show that a large amountof relational similarities can be recovered withboth representations.
In fact, both representationsachieve the same accuracy on the SEMEVAL task.However, there is a large performance gap in favorof the neural embedding in the open-vocabularyMSR and GOOGLE tasks.Next, we run the same experiment withPAIRDIRECTION (method (2) in Section 3).6i.e.
words that appeared in English Wikipedia lessthan 100 times.
This removed 882 instances from theMSR dataset and 286 instances from GOOGLE.174Representation MSR GOOGLE SEMEVALEmbedding 9.26% 14.51% 44.77%Explicit 0.66% 0.75% 45.19%Table 2: Performance of PAIRDIRECTION on different taskswith the explicit and neural embedding representations.The results in Table 2 show that the PAIRDI-RECTION method is better than 3COSADD onthe restricted-vocabulary SEMEVAL task (accu-racy jumps from 38% to 45%), but fails at theopen-vocabulary questions in GOOGLE and MSR.When the method does work, the numbers for theexplicit and embedded representations are againcomparable to one another.Why is PAIRDIRECTION performing so wellon the SEMEVAL task, yet so poorly on the oth-ers?
Recall that the PAIRDIRECTION objectivefocuses on the similarity of b??
b and a??
a,but does not take into account the spatial distancesbetween the individual vectors.
Relying on di-rection alone, while ignoring spatial distance, isproblematic when considering the entire vocabu-lary as candidates (as is required in the MSR andGOOGLE tasks).
We are likely to find candidatesb?that have the same relation to b as reflected bya ?
a?but are not necessarily similar to b.
As aconcrete example, in man:woman, king:?, we arelikely to recover feminine entities, but not neces-sarily royal ones.
The SEMEVAL test set, on theother hand, already provides related (and thereforegeometrically close) candidates, leaving mainlythe direction to reason about.6 Refining the Objective FunctionThe 3COSADD objective, as expressed in (3), re-veals a ?balancing act?
between two attractors andone repeller, i.e.
two terms that we wish to maxi-mize and one that needs to be minimized:argmaxb?
?V(cos (b?, b)?
cos (b?, a) + cos (b?, a?
))A known property of such linear objectives is thatthey exhibit a ?soft-or?
behavior and allow onesufficiently large term to dominate the expression.This behavior is problematic in our setup, becauseeach term reflects a different aspect of similarity,and the different aspects have different scales.
Forexample, king is more royal than it is masculine,and will therefore overshadow the gender aspectof the analogy.
It is especially true in the case ofexplicit vector representations, as each aspect ofthe similarity is manifested by a different set offeatures with varying sizes and weights.A case in point is the analogy question ?Londonis to England as Baghdad is to ?
?
?, which weanswer using:argmaxx?V(cos (x, en)?
cos (x, lo) + cos (x, ba))We seek a word (Iraq) which is similar to Eng-land (both are countries), is similar to Baghdad(similar geography/culture) and is dissimilar toLondon (different geography/culture).
Maximiz-ing the sum yields an incorrect answer (under bothrepresentations): Mosul, a large Iraqi city.
Look-ing at the computed similarities in the explicit vec-tor representation, we see that both Mosul and Iraqare very close to Baghdad, and are quite far fromEngland and London:(EXP) ?
England ?
London ?
Baghdad SumMosul 0.031 0.031 0.244 0.244Iraq 0.049 0.038 0.206 0.217The same trends appear in the neural embeddingvectors, though with different similarity scores:(EMB) ?
England ?
London ?
Baghdad SumMosul 0.130 0.141 0.755 0.748Iraq 0.153 0.130 0.631 0.655While Iraq is much more similar to England thanMosul is (both being countries), both similarities(0.049 and 0.031 in explicit, 0.130 and 0.153 inembedded) are small and the sums are dominatedby the geographic and cultural aspect of the anal-ogy: Mosul and Iraq?s similarity to Baghdad (0.24and 0.20 in explicit, 0.75 and 0.63 in embedded).To achieve better balance among the differentaspects of similarity, we propose switching froman additive to a multiplicative combination:argmaxb?
?Vcos (b?, b) cos (b?, a?
)cos (b?, a) + ?(4)(?
= 0.001 is used to prevent division by zero)This is equivalent to taking the logarithm of eachterm before summation, thus amplifying the dif-ferences between small quantities and reducingthe differences between larger ones.
Using this ob-jective, Iraq is scored higher than Mosul (0.259 vs0.236, 0.736 vs 0.691).
We refer to objective (4)as 3COSMUL.773COSMUL requires that all similarities be non-negative,which trivially holds for explicit representations.
With em-beddings, we transform cosine similarities to [0, 1] using(x+ 1)/2 before calculating (4).1757 Main ResultsWe repeated the experiments, this time using the3COSMUL method.
Table 3 presents the results,showing that the multiplicative objective recov-ers more relational similarities in both representa-tions.
The improvements achieved in the explicitrepresentation are especially dramatic, with an ab-solute increase of over 20% correctly identified re-lations in the MSR and GOOGLE datasets.Objective Representation MSR GOOGLE3COSADDEmbedding 53.98% 62.70%Explicit 29.04% 45.05%3COSMULEmbedding 59.09% 66.72%Explicit 56.83% 68.24%Table 3: Comparison of 3COSADD and 3COSMUL.3COSMUL outperforms the state-of-the-art(3COSADD) on these two datasets.
Moreover, theresults illustrate that a comparable amount of rela-tional similarities can be recovered with both rep-resentations.
This suggests that the linguistic reg-ularities apparent in neural embeddings are not aconsequence of the embedding process, but ratherare well preserved by it.On SEMEVAL, 3COSMUL preformed on parwith 3COSADD , recovering a similar amount ofanalogies with both explicit and neural representa-tions (38.37% and 38.67%, respectively).8 Error AnalysisWith 3COSMUL, both the explicit vectors andthe neural embeddings recover similar amounts ofanalogies, but are these the same patterns, or per-haps different types of relational similarities?8.1 Agreement between RepresentationsConsidering the open-vocabulary tasks (MSR andGOOGLE), we count the number of times both rep-resentations guessed correctly, both guessed in-correctly, and when one representations leads tothe right answer while the other does not (Ta-ble 4).
While there is a large amount of agreementbetween the representations, there is also a non-negligible amount of cases in which they comple-ment each other.
If we were to run in an ora-cle setup, in which an answer is considered cor-rect if it is correct in either representation, wewould have achieved an accuracy of 71.9% on theMSR dataset and 77.8% on GOOGLE.Both Both Embedding ExplicitCorrect Wrong Correct CorrectMSR 43.97% 28.06% 15.12% 12.85%GOOGLE 57.12% 22.17% 9.59% 11.12%ALL 53.58% 23.76% 11.08% 11.59%Table 4: Agreement between the representations on open-vocabulary tasks.Relation Embedding ExplicitGOOGLEcapital-common-countries 90.51% 99.41%capital-world 77.61% 92.73%city-in-state 56.95% 64.69%currency 14.55% 10.53%family (gender inflections) 76.48% 60.08%gram1-adjective-to-adverb 24.29% 14.01%gram2-opposite 37.07% 28.94%gram3-comparative 86.11% 77.85%gram4-superlative 56.72% 63.45%gram5-present-participle 63.35% 65.06%gram6-nationality-adjective 89.37% 90.56%gram7-past-tense 65.83% 48.85%gram8-plural (nouns) 72.15% 76.05%gram9-plural-verbs 71.15% 55.75%MSRadjectives 45.88% 56.46%nouns 56.96% 63.07%verbs 69.90% 52.97%Table 5: Breakdown of relational similarities in each repre-sentation by relation type, using 3COSMUL.8.2 Breakdown by Relation TypeTable 5 presents the amount of analogies dis-covered in each representation, broken down byrelation type.
Some trends emerge: the ex-plicit representation is superior in some of themore semantic tasks, especially geography re-lated ones, as well as the ones superlatives andnouns.
The neural embedding, however, has theupper hand on most verb inflections, compara-tives, and family (gender) relations.
Some rela-tions (currency, adjectives-to-adverbs, opposites)pose a challenge to both representations, thoughare somewhat better handled by the embeddedrepresentations.
Finally, the nationality-adjectivesand present-participles are equally handled byboth representations.8.3 Default-Behavior ErrorsThe most common error pattern under both repre-sentations is that of a ?default behavior?, in whichone central representative word is provided as ananswer to many questions of the same type.
Forexample, the word ?Fresno?
is returned 82 timesas an incorrect answer in the city-in-state rela-tion in the embedded representation, and the word?daughter?
is returned 47 times as an incorrect an-swer in the family relation in the explicit represen-176RELATION WORD EMB EXPgram7-past-tense who 0 138city-in-state fresno 82 24gram6-nationality-adjective slovak 39 39gram6-nationality-adjective argentine 37 39gram6-nationality-adjective belarusian 37 39gram8-plural (nouns) colour 36 35gram3-comparative higher 34 35city-in-state smith 1 61gram7-past-tense and 0 49gram1-adjective-to-adverb be 0 47family (gender inflections) daughter 8 47city-in-state illinois 3 40currency currency 5 40gram1-adjective-to-adverb and 0 39gram7-past-tense enhance 39 20Table 6: Common default-behavior errors under both repre-sentations.
EMB / EXP: the number of time the word wasreturned as an incorrect answer for the given relation underthe embedded or explicit representation.tation.
Loosely, ?Fresno?
is identified by the em-bedded representation as a prototypical location,while ?daughter?
is identified by the explicit rep-resentation as a prototypical female.
Under a def-inition in which a default behavior error is one inwhich the same incorrect answer is returned for aparticular relation 10 or more times, such errorsaccount for 49% of the errors in the explicit repre-sentation, and for 39% of the errors in the embed-ded representation.Table 6 lists the 15 most common default er-rors under both representations.
In most default er-rors the category of the default word is closely re-lated to the analogy question, sharing the categoryof either the correct answer, or (as in the case of?Fresno?)
the question word.
Notable exceptionsare the words ?who?, ?and?, ?be?
and ?smith?
thatare returned as default answers in the explicit rep-resentation, and which are very far from the in-tended relation.
It seems that in the explicit repre-sentation, some very frequent function words actas ?hubs?
and confuse the model.
In fact, theperformance gap between the representations inthe past-tense and plural-verb relations can be at-tributed specifically to such function-word errors:23.4% of the mistakes in past-tense relation aredue to the explicit representation?s default answerof ?who?
or ?and?, while 19% of the mistakes inthe plural-verb relations are due to default answersof ?is/and/that/who?.8.4 Verb-inflection ErrorsA correct solution to the morphological anal-ogy task requires recovering both the correct in-flection (requiring syntactic similarity) and thecorrect base word (requiring semantic similar-ity).
We observe that linguistically, the mor-phological distinctions and similarities tend torely on a few common word forms (for exam-ple, the ?walk:walking?
relation is characterizedby modals such as ?will?
appearing before ?walk?and never before ?walking?, and be verbs ap-pearing before walking and never before ?walk?
),while the support for the semantic relations isspread out over many more items.
We hypothe-size that the morphological distinctions in verbsare much harder to capture than the semantics.
In-deed, under both representations, errors in whichthe selected word has a correct form with an incor-rect inflection are over ten times more likely thanerrors in which the selected word has the correctinflection but an incorrect base form.9 Interpreting Relational SimilaritiesThe ability to capture relational similarities byperforming vector (or similarity) arithmetic is re-markable.
In this section, we try and provide intu-ition as to why it works.Consider the word ?king?
; it has several aspects,high-level properties that it implies, such as roy-alty or (male) gender, and its attributional simi-larity with another word is based on a mixture ofthose aspects; e.g.
king is related to queen on theroyalty and the human axes, and shares the genderand the human aspect with man.
Relational simi-larities can be viewed as a composition of attribu-tional similarities, each one reflecting a differentaspect.
In ?man is to woman as king is to queen?,the two main aspects are gender and royalty.
Solv-ing the analogy question involves identifying therelevant aspects, and trying to change one of themwhile preserving the other.How are concepts such as gender, royalty, or?cityness?
represented in the vector space?
Whilethe neural embeddings are mostly opaque, one ofthe appealing properties of explicit vector repre-sentations is our ability to read and understand thevectors?
features.
For example, king is representedin our explicit vector space by 51,409 contexts, ofwhich the top 3 are tut+1, jeongjo+1, adulyadej+2?
all names of monarchs.
The explicit representa-tion allows us to glimpse at the way different as-pects are represented.
To do so, we choose a repre-sentative pair of words that share an aspect, inter-sect their vectors, and inspect the highest scoring177Aspect Examples Top FeaturesFemale woman queen estrid+1ketevan+1adeliza+1nzinga+1gunnhild+1impregnate?2hippolyta+1Royalty queen king savang+1uncrowned?1pmare+1sisowath+1nzinga+1tupou+1uvea+2majesty?1Currency yen ruble devalue?2banknote+1denominated+1billion?1banknotes+1pegged+2coin+1Country germany  australia emigrates?21943-45+2pentathletes?2emigrated?2emigrate?2hong-kong?1Capital berlin canberra hotshots?1embassy?21925-26+2consulate-general+2meetups?2nunciature?2Superlative sweetest tallest freshest+2asia?s?1cleveland?s?2smartest+1world?s?1city?s?1america?s?1Height taller  tallest regnans?2skyscraper+1skyscrapers+16?4+2windsor?s?1smokestacks+1burj+2Table 7: The top features of each aspect, recovered by pointwise multiplication of words that share that aspect.
The result ofpointwise multiplication is an ?aspect vector?
in which the features common to both words, characterizing the relation, receivethe highest scores.
The feature scores (not shown) correspond to the weight the feature contributes to the cosine similaritybetween the vectors.
The superscript marks the position of the feature relative to the target word.features in the intersection.
Table 7 presents thetop (most influential) features of each aspect.Many of these features are names of people orplaces, which appear rarely in our corpus (e.g.Adeliza, a historical queen, and Nzinga, a royalfamily) but are nonetheless highly indicative ofthe shared concept.
The prevalence of rare wordsstems from PMI, which gives them more weight,and from the fact that words like woman and queenare closely related (a queen is a woman), and thushave many features in common.
Ordering the fea-tures of woman  queen by prevalence revealsfemale pronouns (?she?, ?her?)
and a long list ofcommon feminine names, reflecting the expectedaspect shared by woman and queen.
Word pairsthat share more specific aspects, such as capitalcities or countries, show features that are charac-teristic of their shared aspect (e.g.
capital citieshave embassies and meetups, while immigrationis associated with countries).
It is also interestingto observe how the relatively syntactic ?superlativ-ity?
aspect is captured with many regional posses-sives (?america?s?, ?asia?s?, ?world?s?
).10 Related WorkRelational similarity (and answering analogyquestions) was previously tackled using explicitrepresentations.
Previous approaches use task-specific information, by either relying on a(word-pair, connectives) matrix rather than thestandard (word, context) matrix (Turney andLittman, 2005; Turney, 2006), or by treating anal-ogy detection as a supervised learning task (Ba-roni and Lenci, 2009; Jurgens et al., 2012; Turney,2013).
In contrast, the vector arithmetic approachfollowed here is unsupervised, and works on ageneric single-word representation.
Even thoughthe training process is oblivious to the task of anal-ogy detection, the resulting representation is ableto detect them quite accurately.
Turney (2012) as-sumes a similar setting but with two types of wordsimilarities, and combines them with products andratios (similar to 3COSMUL) to recover a varietyof semantic relations, including analogies.Arithmetic combination of explicit word vec-tors is extensively studied in the context of com-positional semantics (Mitchell and Lapata, 2010),where a phrase composed of two or more wordsis represented by a single vector, computed by afunction of its component word vectors.
Blacoeand Lapata (2012) compare different arithmeticfunctions across multiple representations (includ-ing embeddings) on a range of compositionalitybenchmarks.
To the best of our knowledge suchmethods of word vector arithmetic have not beenexplored for recovering relational similarities inexplicit representations.11 DiscussionMikolov et al.
showed how an unsupervised neuralnetwork can represent words in a space that ?nat-urally?
encodes relational similarities in the formof vector offsets.
This study shows that findinganalogies through vector arithmetic is actually aform of balancing word similarities, and that, con-trary to the recent findings of Baroni et al.
(2014),under certain conditions traditional word similar-ities induced by explicit representations can per-form just as well as neural embeddings on thistask.Learning to represent words is a fascinating andimportant challenge with implications to most cur-rent NLP efforts, and neural embeddings in par-ticular are a promising research direction.
Webelieve that to improve these representations weshould understand how they work, and hope thatthe methods and insights provided in this workwill help to deepen our grasp of current and futureinvestigations of word representations.178ReferencesRami Al-Rfou, Bryan Perozzi, and Steven Skiena.2013.
Polyglot: Distributed word representationsfor multilingual nlp.
In Proc.
of CoNLL 2013.Marco Baroni and Alessandro Lenci.
2009.
One dis-tributional memory, many semantic spaces.
In Pro-ceedings of the Workshop on Geometrical Modelsof Natural Language Semantics, pages 1?8, Athens,Greece, March.
Association for Computational Lin-guistics.Marco Baroni and Alessandro Lenci.
2010.
Dis-tributional memory: A general framework forcorpus-based semantics.
Computational Linguis-tics, 36(4):673?721.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Dont count, predict!
asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proceedingsof the 52nd Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Pa-pers), Baltimore, Maryland, USA, June.
Associationfor Computational Linguistics.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for seman-tic composition.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 546?556, Jeju Island, Korea,July.
Association for Computational Linguistics.John A. Bullinaria and Joseph P. Levy.
2007.
Ex-tracting semantic representations from word co-occurrence statistics: A computational study.
Be-havior Research Methods, 39(3):510?526.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational linguistics, 16(1):22?29.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th International Conference onMachine Learning, pages 160?167.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Ido Dagan, Fernando Pereira, and Lillian Lee.
1994.Similarity-based estimation of word cooccurrenceprobabilities.
In Proceedings of the 32nd annualmeeting on Association for Computational Linguis-tics, pages 272?278.
Association for ComputationalLinguistics.David A Jurgens, Peter D Turney, Saif M Mohammad,and Keith J Holyoak.
2012.
Semeval-2012 task 2:Measuring degrees of relational similarity.
In Pro-ceedings of the First Joint Conference on Lexicaland Computational Semantics, pages 356?364.
As-sociation for Computational Linguistics.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distribu-tional similarity for lexical inference.
Natural Lan-guage Engineering, 16(4):359?389.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics (Volume 2: Short Papers), Balti-more, Maryland, USA, June.
Association for Com-putational Linguistics.Dekang Lin and Patrick Pantel.
2001.
Dirt: discoveryof inference rules from text.
In KDD, pages 323?328.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and 17th International Conference on Com-putational Linguistics - Volume 2, ACL ?98, pages768?774, Stroudsburg, PA, USA.
Association forComputational Linguistics.Tomas Mikolov, Stefan Kombrink, Lukas Burget,JH Cernocky, and Sanjeev Khudanpur.
2011.Extensions of recurrent neural network languagemodel.
In Acoustics, Speech and Signal Processing(ICASSP), 2011 IEEE International Conference on,pages 5528?5531.
IEEE.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
CoRR, abs/1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
2013b.
Distributed rep-resentations of words and phrases and their com-positionality.
In Advances in Neural InformationProcessing Systems 26: 27th Annual Conference onNeural Information Processing Systems 2013.
Pro-ceedings of a meeting held December 5-8, 2013,Lake Tahoe, Nevada, United States, pages 3111?3119.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013c.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 746?751, Atlanta,Georgia, June.
Association for Computational Lin-guistics.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1439.179Andriy Mnih and Geoffrey E Hinton.
2008.
A scal-able hierarchical distributed language model.
In Ad-vances in Neural Information Processing Systems,pages 1081?1088.Yoshiki Niwa and Yoshihiko Nitta.
1994.
Co-occurrence vectors from corpora vs. distance vec-tors from dictionaries.
In Proceedings of the 15thconference on Computational linguistics-Volume 1,pages 304?309.
Association for Computational Lin-guistics.Sebastian Pad?o and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of english words.
InProceedings of the 31st annual meeting on Associa-tion for Computational Linguistics, pages 183?190.Association for Computational Linguistics.Magnus Sahlgren.
2006.
The Word-Space Model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, Stock-holm.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 151?161.
Association forComputational Linguistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
Association forComputational Linguistics.Peter D. Turney and Michael L. Littman.
2005.Corpus-based learning of analogies and semantic re-lations.
Machine Learning, 60(1-3):251?278.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188.Peter D. Turney.
2001.
Mining the web for synonyms:Pmi-ir versus lsa on toefl.
In Proceedings of the 12thEuropean Conference on Machine Learning, pages491?502.
Springer-Verlag.Peter D. Turney.
2006.
Similarity of semantic rela-tions.
Computational Linguistics, 32(3):379?416.Peter D. Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.Journal of Artificial Intelligence Research, 44:533?585.Peter D. Turney.
2013.
Distributional semantics be-yond words: Supervised learning of analogy andparaphrase.
CoRR, abs/1310.5042.Alisa Zhila, Wen-tau Yih, Christopher Meek, Geof-frey Zweig, and Tomas Mikolov.
2013.
Combiningheterogeneous models for measuring relational sim-ilarity.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 1000?1009, Atlanta, Georgia, June.Association for Computational Linguistics.180
