Cross-lingual Information Retrieval using Hidden Markov ModelsJinxi XuBBN Technologies70 Fawcett St.Cambridge, MA, USA 02138jxu@bbn.comRalph WeischedelBBN Technologies70 Fawcett St.Cambridge, MA, USA 02138weischedel @bbn.comAbstractThis paper presents empirical results incross-lingual information retrieval usingEnglish queries to access Chinesedocuments (TREC-5 and TREC-6) andSpanish documents (TREC-4).
Since ourinterest is in languages where resourcesmay be minimal, we use an integratedprobabilistic model that requires only abilingual dictionary as a resource.
Weexplore how a combined probabilitymodel of term translation and retrieval canreduce the effect of translation ambiguity.In addition, we estimate an upper boundon performance, if translation ambiguitywere a solved problem.
We also measureperformance as a function of bilingualdictionary size.1 IntroductionCross-language information retrieval (CLIR) canserve both those users with a smattering ofknowledge of other languages and also thosefluent in them.
For those with limitedknowledge of the other language(s), CLIR offersa wide pool of documents, even though the userdoes not have the skill to prepare ahigh qualityquery in the other language(s).
Once documentsare retrieved, machine translation or humantranslation, if desired, can make the documentsusable.
For the user who is fluent in two ormore languages, even though e/she may be ableto formulate good queries in each of the sourcelanguages, CLIR relieves the user from havingto do so.Most CLIR studies have been based on a variantof tf-idf; our experiments instead use a hiddenMarkov model (HMM) to estimate theprobability that a document is relevant given thequery.
We integrated two simple estimates ofterm translation probability into the mono-lingual HMM model, giving an estimate of theprobability that a document is relevant given aquery in another language.In this paper we address the following questions:?
How can a combined probability model ofterm translation and retrieval minimize theeffect of translation ambiguity?
(Sections 3,5, 6, 7, and 10)?
What is the upper bound performance usingbilingual dictionary lookup for termtranslation?
(Section 8)?
How much does performance d grade due toomissions from the bilingual dictionary andhow does performance vary with size ofsuch a dictionary?
(Sections 8-9)All experiments were performed using acommon baseline, an HMM-based (mono-lingual) indexing and retrieval engine.
In orderto design controlled experiments for thequestions above, the IR system was run withoutsophisticated query expansion techniques.Our experiments are based on the Chinesematerials of TREC-5 and TREC-6 and theSpanish materials of TREC-4.2 HMM for Mono-Lingual RetrievalFollowing Miller et al, 1999, the IR systemranks documents according to the probabilitythat a document D is relevant given the query Q,P(D is R IQ).
Using Bayes Rule, and the factthat P(Q) is constant for a given query, and ourinitial assumption of a uniform a priori95probability that a document is relevant, rankingdocuments according to P(Q\[D is R) is the sameas ranking them according to P(D is RIQ).
Theapproach therefore estimates the probability thata query Q is generated, given the document D isrelevant.
(A glossary of symbols used appearsbelow.
)We use x to represent the language (e.g.English) for which retrieval is carried out.According to that model of monolingualretrieval, it can be shown thatp(Q \[ D is R) = I I  (aP(W \[ Gx) + (1- a)e(w ID)),W inQwhere W's are query words in Q. Miller et alestimated probabilities as follows:* The transition probability a is 0.7 using theEM algorithm (Rabiner, 1989) on the TREC4ad-hoc query set.number of occurrences of W in C x?
e0e  IGx)= length of Cxwhich is the general language probability forword W in language x.number of occurrences of W in D?
e (WlD)  =length of DIn principle, any large corpus Cx that isrepresentative of language x can be used incomputing the general language probabilities.In practice, the collection to be searched isused for that purpose.
The length of aQ a queryEnglish querya documenta document in foreign language ydocument is relevanta wordan English corpusa corpus in language xQXDDrD isRWGxCxWxBLan English wordforeign language yWy a word ina bilingual dictionaryA Glossary of Notation used in Formulascollection is the sum of the documentlengths.3 HMM for Cross-lingual IRFor CLIR we extend the query generationprocess o that a document Dy written inlanguage y can generate a query Qx in languagex.
We use Wx to denote aword in x and Wy todenote aword in y.
As before, to model generalquery words from language x, we estimate P(Wx\]Gx) by using a large corpus Cx in language x.Also as before, we estimate P(WyIDy) to be thesample distribution of Wy in Dy.We use P(Wx\[Wy) to denote the probability thatWy is translated as Wx.
Though terms oftenshould not be translated independent of theircontext, we make that simplifying assumptionhere.
We assume that the possible translationsare specified by a bilingual lexicon BL.
Sincethe event spaces for Wy's in P(WyIDy) aremutually exclusive, we can compute the outputprobability P(WxIDy):P(WxIDy)= ~P(WylDy)P(WxIWy)W inBL yWe compute P(Q~IDy is R) as below:P(Qx IDr /sR) = I~I(aetwx IG,)+O-a)P(W~ IDy))w.~,o.The above model generates queries fromdocuments, that is, it attempts o determine howlikely a particular query is given a relevantdocument.
The retrieval system, however, canuse either query translation or documenttranslation.
We chose query translation overdocument translation for its flexibility, since itallowed us to experiment with a new method ofestimating the translation probabilities withoutchanging the index structure.4 Experimental Set-upFor retrieval using English queries to searchChinese documents, we used the TREC5 andTREC6 Chinese data which consists of 164,789documents from the Xinhua News Agency andPeople's Daily, averaging 450 Chinesecharacters/document.
Each of the TREC topicshas three Chinese fields: title, description and96narrative, plus manually translated, Englishversions of each.
We corrected some of theEnglish queries that contained errors, such as"Dali Lama" instead of the correct "Dalai Lama"and "Medina" instead of "Medellin."
Stopwords and stop phrases were removed.
Wecreated three versions of Chinese queries andthree versions of English queries: short (titleonly), medium (title and description), and long(all three fields).For retrieval using English queries to searchSpanish documents, we used the TREC4Spanish data, which has 57,868 documents.
Ithas 25 queries in Spanish with manualtranslations toEnglish.
We will denote theChinese data sets as Trec5C and Trec6C and theSpanish data set as Trec4S.We used a Chinese-English lexicon from theLinguistic Data Consortium (LDC).
We pre-processed the dictionary as follows:1.
Stem Chinese words via a simple algorithmto remove common suffixes and prefixes.2.
Use the Porter stemmer on English words.3.
Split English phrases into words.
If anEnglish phrase is a translation for a Chineseword, each word in the phrase is taken as aseparate translation for the Chinese word.
~4.
Estimate the translation probabilities.
(Wefirst report results assuming a uniformdistribution on a word's translations.
If aChinese word c has n translations el, e2, ...en.each of them will be assigned equal probability,i.e., P(ei lc)=l/n.
Section 10 supplements thiswith a corpus-based distribution.)5.
Invert he lexicon to make it an English-Chinese lexicon.
That is, for each English worde, we associate it with a list of Chinese words cl,c2, ... Cm together with non-zero translationprobabilities P( elc~).The resulting English-Chinese l xicon has80,000 English words.
On average, eachEnglish word has 2.3 Chinese translations.For Spanish, we downloaded a bilingualEnglish-Spanish lexicon from the Internet(http://www.activa.arrakis.es) containing around22,000 English words (16,000 English stems)and processed it similarly.
Each English wordhas around 1.5 translations on average.
A co-occurrence based stemmer (Xu and Croft, 1998)was used to stem Spanish words.
Onedifference from the treatment of Chinese is toinclude the English word as one of its owntranslations in addition to its Spanishtranslations in the lexicon.
This is useful fortranslating proper nouns, which often haveidentical spellings in English and Spanish butare routinely excluded from a lexicon.One problem is the segmentation f Chinesetext, since Chinese has no spaces betweenwords.
In these initial experiments, we relied ona simple sub-string matching algorithm toextract words from Chinese text.
To extractwords from a string of Chinese characters, thealgorithm examines any sub-string of length 2 orgreater and recognizes it as a Chinese word if itis in a predefined dictionary (the LDC lexicon inour case).
In addition, any single characterwhich is not part of any recognized Chinesewords in the first step is taken as a Chineseword.
Note that this algorithm can extract acompound Chinese word as well as itscomponents.
For example, the Chinese word for"particle physics" as well as the Chinese wordsfor "particle" and "physics" will be extracted.This seems desirable because it ensures theretrieval algorithm will match both thecompound words as well as their components.The above algorithm was used in processingChinese documents and Chinese queries.English data from the 2 GB of TREC disks l&2was used to estimate P(WlG,..ngti~h), the generallanguage probabilities for English words.
Theevaluation metric used in this study is theaverage precision using the trec_eval program(Voorhees and Harman, 1997).
Mono-lingualretrieval results (using the Chinese and Spanishqueries) provided our baseline, with the HMMretrieval system (Miller et al 1999).1 Clearly, this is not correct; however, itsimplified implementation.975 Retrieval ResultsTable 2 reports average precision for mono-lingual retrieval, average precision for cross-lingual, and the relative performance ratio ofcross-lingual retrieval to mono-lingual.Relative performance of cross-lingual IR variesbetween 67% and 84% of mono-lingual IR.Trec6 Chinese queries have a somewhat higherrelative performance than Trec5 Chinesequeries.
Longer queries have higher elativeperformance than short queries in general.Overall, cross-lingual performance using ourHMM retrieval model is around 76% of mono-lingual retrieval.
A comparison of our mono-lingual results with Trec5 Chinese and Trec6Chinese results published in the TRECproceedings (Voorhees and Harman, 1997,1998) shows that our mono-lingual results areclose to the top performers in the TRECconferences.
Our Spanish mono-lingualperformance is also comparable tothe topautomatic runs of the TREC4 Spanish task(Harrnan, 1996).
Since these mono-lingualresults were obtained without usingsophisticated query processing techniques suchas query expansion, we believe the mono-lingualresults form a valid baseline.Query sets Mono- Cross- % oflingual lingual Mono-lingualTrec5C-short 0.2830 0.1889 67%Trec5C-medium 0.3427 0.2449 72%Trec5C-long 0.3750 0.2735 73%Trec6C-short 0.3423 0.2617 77%Trec6C-medium 0.4606 0.3872 84%Trec6C-long 0.5104 0.4206 82%Trec4S 0.2252 0.1729 77%Table 2: Comparing mono-lingual and cross-lingual retrieval performance.
The scores onthe monolingual and cross-lingual columns areaverage precision.6 Comparison with other MethodsIn this section we compare our approach withtwo other approaches.
One approach is "simplesubstitution", i.e., replacing a query term withall its translations and treating the translatedquery as a bag of words in mono-lingualretrieval.
Suppose we have a simple queryQ=(a, b), the translations for a are al, a2, a3, andthe translations for b are bl, b2.
The translatedquery would be (at, a2, a3, b~, b2).
Since all termsare treated as equal in the translated query, thisgives terms with more translations (potentiallythe more common terms) more credit inretrieval, even though such terms houldpotentially be given less credit if they are morecommon.
Also, a document matching differenttranslations ofone term in the original querymay be ranked higher than a document thatmatches translations ofdifferent terms in theoriginal query.
That is, a document thatcontains terms at, a2 and a3 may be rankedhigher than a document which contains terms atand bl.
However, the second ocument is morelikely to be relevant since correct translations ofthe query terms are more likely to co-occur(Ballesteros and Croft, 1998).A second method is to structure the translatedquery, separating the translations for one termfrom translations for other terms.
This approachlimits how much credit he retrieval algorithmcan give to a single term in the original queryand prevents the translations ofone or a fewterms from swamping the whole query.
Thereare several variations of such a method(Ballesteros and Croft, 1998; Pirkola, 1998; Hull1997).
One such method is to treat differenttranslations ofthe same term as synonyms.Ballesteros, for example, used the INQUERY(Callan et al 1995) synonym operator to grouptranslations ofdifferent query terms.
However,if a term has two translations inthe targetlanguage, it will treat hem as equal even thoughone of them is more likely to be the correcttranslation than the other.
By contrast, ourHMM approach supports translationprobabilities.
The synonym approach isequivalent to changing all non-zero translationprobabilities P(W~\[ Wy)'s to 1 in our retrieyalfunction.
Even estimating uniform translationprobabilities gives higher weights tounambiguous translations and lower weights tohighly ambiguous translations.98These intuitions are supported empirically by theresults in Table 3.
We can see that the HMMperforms best for every query set.
Simplesubstitution performs worst.
The synonymapproach is significantly better than substitution,but is consistently worse than the HMMtranslations were kept in disambiguation, theimprovement would be 4% for Trec6C-medium.The results of this manual disambiguationsuggest that there are limits to automaticdisambiguation.Substi- Synonym HMMtutionTrec5C-long 0.0391 0.2306 0.2735Trec6C-long 0.0941 0.3842 0.4206Trec4S 0.0935 0.1594 0.1729Table 3: Comparing different methods ofquery translation.
All numbers are averageprecision.7 Impact of Translation AmbiguityTo get an upper bound on performance ofanydisambiguation technique, we manuallydisambiguated the Trec5C-medium, Trec6C-medium and Trec4S queries.
That is, for eachEnglish query term, a native Chinese or Spanishspeaker scanned the list of translations in thebilingual exicon and kept one translationdeemed to be the best for the English term anddiscarded the rest.
If none of the translationswas correct, the first one was chosen.The results in Table 4 show that manualdisambiguation improves performance by 17%on Trec5C, 4% on Trec4S, but not at all onTrec6C.
Furthermore, the improvement onTrec5C appears to be caused by bigimprovements for a small number of queries.The one-sided t-test (Hull, 1993) at significancelevel 0.05 indicated that the improvement onTrec5C is not statistically significant.It seems urprising that disambiguation does nothelp at all for Trec6C.
We found that manyterms have more than one valid translation.
Forexample, the word "flood" (as in "floodcontrol") has 4 valid Chinese translations.
Usingall of them achieves the desirable ffect of queryexpansion.
It appears that for Trec6C, the benefitof disambiguation is cancelled by choosing onlyone of several alternatives, discarding thoseother good translations.
If multiple correctQuery setsTrec5C-mediumTrec6C-mediumTrec4S(+4%)Degree of DisambiguationNone Manual % ofMono-lingual0.2449 0.2873 84%(+17%)0.3872 0.3830 83%(-1%)0.1729 0.1799 80%Table 4: The effect of disambiguation onretrieval performance.
The scores reportedare average precision.8 Impact of Missing TranslationsResults in the previous ection showed thatmanual disambiguation can bring performanceof cross-lingual IR to around 82% of mono-lingual IR.
The remaining performance gapbetween mono-lingual nd cross-lingual IR islikely to be caused by the incompleteness of thebilingual exicon used for query translation, i.e.,missing translations for some query terms.
Thismay be a more serious problem for cross-lingualIR than ambiguity.
To test the conjecture, foreach English query term, a native speaker inChinese or Spanish manually checked whetherthe bilingual exicon contains acorrecttranslation for the term in the context of thequery.
If it does not, a correct ranslation for theterm was added to the lexicon.
For the querysets Trec5C-medium and Trec6C-medium, thereare 100 query terms for which the lexicon doesnot have a correct ranslation.
This represents19% of the 520 query terms (a term is countedonly once in one query).
For the query setTrec4S, the percentage is 12%.The results in Table 5 show that with augmentedlexicons, performance of cross-lingual IR is91%, 99% and 95% of mono-lingual IR onTrec5C-mediurn, Trec6C-medium and Trec4S.99The improvement over using the original exiconis 28%, 18% and 23% respectively.
The resultsdemonstrate the importance cff a completelexicon.
Compared with the results in section 7,the results here suggest that missing translationshave a much larger impact on cross-lingual IRthan translation ambiguity does.Query sets Original Augmented % o flexicon lexicon Mono-lingualTrec5C- 0.2449 0.3131 91%medium (+28%)Trec6C- 0.3872 0.4589 99%medium (+18%)Trec4S 0.1729 0.2128 95%(+23%)Table 5: The impact of missing the righttranslations on retrieval performance.
Allscores are average precision.9 Impact of  Lexicon SizeIn this section we measure CLIR performance asa function of lexicon size.
We sorted theEnglish words from TREC disks l&2 in order ofdecreasing frequency.
For a lexicon of size n,we keep only the n most frequent English words.The upper graph in Figure 1 shows the curve ofcross-lingual IR performance asa function of thesize of the lexicon based on the Chinese shortand medium-length queries.
Retrievalperformance was averaged over Trec5C andTrec6C.
Initially retrieval performance increasessharply with lexicon size.
After the dictionaryexceeds 20,000, performance l vels off.
Anexamination of the translated queries hows thatwords not appearing in the 20,000-word lexiconusually do not appear in the larger lexiconseither.
Thus, increases in the general lexiconbeyond 20,000 words did not result in asubstantial increase in the coverage of the queryterms.The lower graph in Figure 1 plots the retrievalperformance asa function of the percent of thefull lexicon.
The figure shows that short queriesare more susceptible toincompleteness of thelexicon than longer queries.
Using a 7,000-wordlexicon, the short queries only achieve 75% oftheir performance with the full lexicon.
Incomparison, the medium-length queries achieve87% of their performance.\[--*- Short Query 4-- Medium Query J0.350.3o.25== o.20.15~.
0.1O.O500 10000 20000 30000 40000 50000 60000Lexicon Size\[ -*-- Short + Medium \]_-- 120o lO0I~g 000 o o_  60,f.
o0O,,10000 20000 30000 40000 5(X)O0 60000Lexicon SizeFigure 1 Impact of lexicon size on cross-lingual IRperformanceWe categorized the missing terms and found thatmost of them are proper nouns (especiallylocations and person ames), highly technicalterms, or numbers.
Such words understandablydo not normally appear in traditional lexicons.Translation of numbers can be solved usingsimple rules.
Transliteration, a technique thatguesses the likely translations of a word basedon pronunciation, can be readily used intranslating proper nouns.Another technique is automatic discovery oftranslations from parallel or non-parallel corpora(Fung and Mckeown, 1997).
Since traditionallexicons are more or less static repositories ofknowledge, techniques that discover translationfrom newly published materials can supplementthem with corpus-specific vocabularies.10010 Using a Parallel CorpusIn this section we estimate translationprobabilities from a parallel corpus rather thanassuming uniform likelihood as in section 4.
AHong Kong News corpus obtained from theLinguistic Data Consortium has 9,769 newsstories in Chinese with English translations.
Ithas 3.4 million English words.
Since thedocuments are not exact ranslations of eachother, occasionally having extra or missingsentences, we used document-level co-occurrence toestimate translation probabilities.The Chinese documents were "segmented" usingthe technique discussed in section 4.
Let co(e,c)be the number of parallel documents where anEnglish word e and a Chinese word c co-occur,and df(c) be the document frequency of c. If aChinese word c has n possible translations el toen in the bilingual exicon, we estimate thecorpus translation probability as:co(e i , c)P_  corpus(ell c) =i=nMAX(df (c ) ,  ~ co(e i, c))i=1Since several translations for c may co-occur ina document, ~co(e~ c) can be greater than df(c).Using the maximum of the two ensures thatE P_corpus(eilc)_<l.Instead of relying solely on corpus-basedestimates from a small parallel corpus, weemploy a mixture model as follows:P( e I c) = ~ P _ corpus( eI c) + (1- #)P_ lexicon( e\[ c)The retrieval results in Table 6 show thatcombining the probability estimates from thelexicon and the parallel corpus does improveretrieval performance.
The best results areobtained when 13=0.7; this is better than usinguniform probabilities by 9% on Trec5C-mediumand 4% on Trec6C-medium.
Using the corpusprobability estimates alone results in asignificant drop in performance, the parallelcorpus is not large enough nor diverse noughfor reliable stimation of the translationprobabilities.
In fact, many words do not appearin the corpus at all.
With a larger and betterparallel corpus, more weight should be given tothe probability estimates from the corpus.Trec5 - Trec6-medium mediumP_lexicon 0.2449 0.387213=0.3 0.2557 0.398013=0.5 0.2605 0.402113=0.7 0.2658 0.4035P_corpus 0.2293 0.2971Table 6: Performance with different valuesof 13.
All scores are average precision.11 Related WorkOther studies which view IR as a querygeneration process include Maron and Kuhns,1960; Hiemstra nd Kraaij, 1999; Ponte andCroft, 1998; Miller et al 1999.
Our work hasfocused on cross-lingual retrieval.Many approaches tocross-lingual IR have beenpublished.
One common approach is usingMachine Translation (MT) to translate thequeries to the language of the documents ortranslate documents othe language of thequeries (Gey et al 1999; Oard, 1998).
For mostlanguages, there are no MT systems at all.
Ourfocus is on languages where no MT exists, but abilingual dictionary may exist or may bederived.Another common approach is term translation,e.g., via a bilingual exicon.
(Davis and Ogden,1997; Ballesteros and Croft, 1997; Hull andGrefenstette, 1996).
While word sensedisambiguation has been a central topic inprevious tudies for cross-lingual IR, our studysuggests that using multiple weightedtranslations and compensating for theincompleteness of the lexicon may be morevaluable.
Other studies on the value ofdisambiguation for cross-lingual IR includeHiernstra nd de Jong, 1999; Hull, 1997.Sanderson, 1994 studied the issue ofdisarnbiguation for mono-lingual IR.101The third approach to cross-lingual retrieval is tomap queries and documents o someintermediate r presentation, e.g latent semanticindexing (LSI) (Littman et al 1998), or theGeneral Vector space model (GVSM),(Carbonell et al 1997).
We believe ourapproach is computationally ess costly than(LSI and GVSM) and assumes less resources(WordNet in Diekema et al, 1999).12 Conclusions and Future WorkWe proposed an approach to cross-lingual IRbased on hidden Markov models, where thesystem estimates the probability that a query inone language could be generated from adocument in another language.
Experimentsusing the TREC5 and TREC6 Chinese test setsand the TREC4 Spanish test set show thefollowing:?
Our retrieval model can reduce theperformance d gradation due to translationambiguity This had been a major limitingfactor for other query-translationapproaches.?
Some earlier studies uggested that querytranslation is not an effective approach tocross-lingual IR (Carbonell et al 1997).However, our results uggest that querytranslation can be effective particularly if abilingual dictionary is the primary bilingualresource available.?
Manual selection from the translations in thebilingual dictionary improves performancelittle over the HMM.?
We believe an algorithm cannot rule out apossible translation with absoluteconfidence; it is more effective to rely onprobability estimation/re-estimation todifferentiate likely translations and unlikelytranslations.?
Rather than translation ambiguity, a moreserious limitation to effective cross-lingualIR is incompleteness of the bilingual exiconused for query translation.?
Cross-lingual IR performance is typically75% that of mono-lingual for our HMM onthe Chinese and Spanish collections.Future improvements in cross-lingual IR willcome by attacking the incompleteness ofbilingual dictionaries and by improved queryexpansion and context-dependent translation.Our current model assumes that query terms aregenerated one at time.
We would like to extendthe model to allow phrase generation i thequery generation process.
We also wish toexplore techniques to extend bilingual exicons.ReferencesL.
Ballesteros and W.B.
Croft 1997.
"Phrasaltranslation and query expansion techniques forcross-language information retrieval."
Proceedingsof the 20th ACM SIGIR International Conferenceon Research and Development in InformationRetrieval 1997, pp.
84-91.L.
Ballesteros and W.B.
Croft, 1998.
"Resolvingambiguity for cross-language retrieval.
"Proceedings of the 21st ACM SIGIR Conference onResearch and Development in InformationRetrieval, 1998, pp.
64-71.J.P.
Callan, W.B.
Croft and J. Broglio.
1995.
"TRECand TIPSTER Experiments with INQUERY".Information Processing and Management, pages327-343, 1995.J.
Carbonell, Y. Yang, R. Frederking, R. Brown, Y.Geng and D. Lee, 1997.
"Translingual informationretrieval: a comparative evaluation."
InProceedings of the 15th International JointConference on Artificial Intelligence, 1997.M.
Davis and W. Ogden, 1997.
"QUILT:Implementing a Large Scale Cross-language TextRetrieval System."
Proceedings of ACM SIGIRConference, 1997.A.
Diekema, F. Oroumchain, P. Sheridan and E.Liddy, 1999.
"TREC-7 Evaluation of ConceptualInterlingual Document Retrieval (CINDOR) inEnglish and French."
TREC7 Proceedings, NISTspecial publication.P.
Fung and K. Mckeown.
"Finding TerminologyTranslations from Non-parallel Corpora."
The 5 'hAnnual Workshop on Very Large Corpora, HongKong: August 1997, 192n202F.
Gey, J.
He and A. Chen, 1999.
"Manual queriesand Machine Translation in cross-languageretrieval at TREC-7".
In TREC7 Proceedings,NIST Special Publication, 1999.102Harman, 1996.
The TREC-4 Proceedings.
NISTSpecial publication, 1996.D.
Hiemstra nd F. de Jong, 1999.
"Disambiguafionstrategies for Cross-language InformationRetrieval."
Proceedings of the third EuropeanConference on Research and Advanced Technologyfor Digital Libraries, pp.
274-293, 1999.D.
Hiemstra and W. Kraaij, 1999.
"Twenty-One atTREC-7: ad-hoc and cross-language track."
InTREC-7 Proceedings, NIST Special Publication,1999.D.
Hull, 1993.
"Using Statistical Testing in theEvaluation of Retrieval Experiments."
Proceedingsof the 16th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 329-338, 1993.D.
A.
Hull and G. Grefenstette, 1996.
"A dictionary-based approach to multilingual informationretrieval".
Proceedings of ACM SIGIR Conference,1996.D.
A.
Hull, 1997.
"Using structured queries fordisambiguation in cross-language informationretrieval."
In AAAI Symposium on Cross-LanguageText and Speech Retrieval.
AAAI, 1997.M.
E. Maron and K. L. Kuhns, 1960.
"OnRelevance, Probabilistic Indexing and InformationRetrieval."
Journal of the Association for": Computing Machinery, 1960, pp 216-244.D.
Miller, T. Leek and R. Schwartz, 1999.
"AHidden Markov Model Information RetrievalSystem."
Proceedings of the 22nd AnnualInternational ACM S1GIR Conference on Researchand Development in Information Retrieval, pages214-221, 1999.D.W.
Oard, 1998.
"A comparative study of query anddocument translation for cross-languageinformation retrieval."
In Proceedings of the ThirdConference of the Association for MachineTranslation in America (AMTA ), 1998.Ari Pirkola, 1998.
"The effects of query structureand dictionary setups in dictionary-based cross-language information retrieval."
Proceedings ofACM SIGIR Conference, 1998, pp 55-63.J.
Ponte and W.B.
Croft, 1998.
"A LanguageModeling Approach to Information Retrieval.
"Proceedings of the 21st Annual International ACMS1GIR Conference on Research and Developmentin Information Retrieval, pages 275-281, 1998.L.
Rabiner, 1989.
"A tutorial on hidden Markovmodels and selected applications in speechrecognition."
Proc.
IEEE 77, pp.
257-286, 1989.M.
Sanderson.
"Word sense disambiguation andinformation retrieval."
Proceedings of ACM SIGIRConference, 1994, pp 142-15 I.Voorhees and Harman, 1997.
TREC-5 Proceedings.E.
Voorhees and D. Harman, Editors.
NISTspecial publication.Voorhees and Harman, 1998.
TREC-6 Proceedings.E.
Voorhees and D. Harrnan, Editors.
NISTspecial publication.J.
Xu and W.B.
Croft, 1998.
"Corpus-basedstemming using co-occurrence of word variants".ACM Transactions on Information Systems,January 1998, vol 16, no.
1.103
