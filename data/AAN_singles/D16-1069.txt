Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 721?730,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsGenerating Abbreviations for Chinese Named Entities Using RecurrentNeural Network with Dynamic DictionaryQi Zhang, Jin Qian, Ya Guo, Yaqian Zhou, Xuanjing HuangShanghai Key Laboratory of Data ScienceSchool of Computer Science, Fudan UniversityShanghai, P.R.
China{qz, jqian12, yguo13, zhouyaqian, xjhuang}@fudan.edu.cnAbstractChinese named entities occur frequently informal and informal environments.
Variousapproaches have been formalized the problemas a sequence labelling task and utilize acharacter-based methodology, in which char-acter is treated as the basic classification unit.One of the main drawbacks of these methodsis that some of the generated abbreviationsmay not follow the conventional wisdom ofChinese.
To address this problem, we pro-pose a novel neural network architecture toperform task.
It combines recurrent neuralnetwork (RNN) with an architecture determin-ing whether a given sequence of characterscan be a word or not.
For demonstratingthe effectiveness of the proposed method, weevaluate it on Chinese named entity generationand opinion target extraction tasks.
Experi-mental results show that the proposed methodcan achieve better performance than state-of-the-art methods.1 IntroductionAbbreviations of Chinese named entities are fre-quently used on different kinds of environments.Along with the development of social media, thiskinds of circumstance occurs more frequently.
Un-like western languages such as English, Chinesedoes not insert spaces between words or word formsthat undergo morphological alternations.
Hence,most of the Chinese natural language processingmethods assume a Chinese word segmenter is usedin a pre-processing step to produce word-segmentedChinese sentences as input.
However, if the Chineseword segmenter produces erroneous output, thequality of these methods will be degraded as a directresult.
Moreover, since the word segmenter maysplit the targets into two individual words, manymethods adopted character-based methodologies,such as methods for named entity recognition (Wuet al, 2005), aspect-based opinion mining (Xu et al,2014), and so on.Through character-based methodology, most ofthe previous abbreviation generation approacheshave been formalized as sequence labelling prob-lem.
Chinese characters are treated as the basicclassification unit and are classified one by one.In these methods, dictionaries play important effectin constructing features and avoiding meaninglessoutputs.
Various previous works have demonstratedthe significant positive effectiveness of the externaldictionary (Zhang et al, 2010).
However, becausethese external dictionaries are usually static and pre-constructed, one of the main drawbacks of thesemethods is that the words which are not includedin the dictionaries cannot be well processed.
Thisissue has also been mentioned by numerous previousworks (Peng et al, 2004; Liu et al, 2012).Hence, understanding how Chinese words areconstructed can benefit a variety of Chinese NLPtasks to avoid meaningless output.
For example,to generate the abbreviation for a named entity, wecan use a binary classifier to determine whether acharacter should be removed or retained.
Both ????
and ??????
are appropriate abbreviationsfor ?????????
(Air China)?.
However?????
is not a Chinese word and cannot beunderstood by humans.721tt-1 t+1ytstctmtWVU FsFyOutputLayerHiddenLayerFeatureLayerInputLayermit mi+1tmi?1tRADD RADDRADD... ...tt-1 t+1... ...mt generationFigure 1: The architecture of RNN with dynamic dictionary.Thus we are motivated to study the task of?dynamic dictionary?
and integrating it with se-quence labelling model to perform the abbreviationgeneration task.
Dynamic dictionary denotes abinary classification problem which tries to deter-mine whether or not a given sequence of charactersis a word.
Although human can use implicitknowledge to easily recognize whether an unseentext segment is a word or not at first glance,the task is not as easy as it may seem.
First,Chinese has a different morphological system fromEnglish.
Each Chinese character represents botha syllable and a morpheme (McBride-Chang etal., 2003).
Hence, Chinese script is sometimesdescribed as being morphosyllabic.
Second, thereare many homophones in Chinese.
This means thatcharacters that have very different written formsmay sound identical.
Third, there are a huge numberof Chinese words.
Without taking the implicitknowledge of morphology into consideration, anarbitrary sequence of characters can be used as aname.
In Mandarin, there are approximately 7,000characters in daily use.
Hence, determining whethera given sequence of characters is a word or not is anchallenging task.Since the length of Chinese words is variable, inthis paper, we propose a modified recurrent architec-ture to model the dynamic dictionary constructiontask.
For processing sequence labelling tasks,we also combine the proposed method with RNN.Since the proposed dynamic dictionary model canbe pre-trained independently with extensive domainindependent dictionaries, the combined model canbe easily used in different domains.
The proposedmodel can take advantage of both the sequence-level discrimination ability of RNN and the abilityof external dictionary.The main contributions of this work can besummarized as follows:?
We define the dynamic dictionary problem andconstruct a large dataset, which consists ofmore than 20 million words for training andevaluation.?
We integrate RNN with a deep feedforwardnetwork based dynamic dictionary learningmethod for processing Chinese NLP taskswhich are formalized as sequence labellingtasks.?
Experimental results demonstrate that the accu-racy of the proposed method can achieve betterresults than current state-of-the-arts methodson two different tasks.2 Model Architecture2.1 Dynamic DictionaryThe task of dynamic dictionary is to predict whethera given sequence of characters can be a word ornot.
The input is a text segment, which containsa variable number of characters.
The output is an722......nn-1n-2...1ysncnWVUFigure 2: The recurrent architecture used in this work formodelling dynamic dictionary (RADD).binary value.
It is different from the traditionalsequence classification tasks, whose the number ofoutputs are usually same as the input.
However, theinformation of the whole sequence is an importantfactor and should be incorporated.
Hence, in thiswork, we use a modified recurrent architecture(RADD is used to represent the network in thefollowing literature for short), which is shown inFig.2.In Fig.2, n represents the number of characters ofthe input text segment.
ck represents input characterat time k encoded using embeddings of charactersthrough table lookup.
The hidden layer sk maintainsthe past and current information.
The hiddenactivations of the last step sn could be consideredas the representation of the whole text segment.
snis used as the input to the classification layer.
yproduces a probability distribution over the binarylabels.
Each layer is also represents a set of neurons.Layers are also connected with weights denoted bythe matrices U, W, and V. The values in the hiddenand output layers are calculated as follows:sk = f(Uck + Wsk?1)y = f(Vsn) (1)where f(?)
is sigmoid activation function f(z) =11+exp?z .
The architecture can be unfolded as a deepfeedforward network.We define all the parameters for the stage ofmodelling dynamic dictionary to be trained as ?
=(W,U, V ).
Given an input text segment, thenetwork with parameter ?
outputs the probability,p(1|x, ?
), of the given text segment can be aword or not.
Cross entropy criterion is used asthe loss function O of the binary classificationproblem.
The network is trained by stochasticgradient descent using backpropagation throughtime (BPTT) (Werbos, 1990).
The hidden layeractivation of position i at time t, sit, is:sit = f(ait), (2)ait =?juijcjt +?lwilslt?1.
(3)The error firstly propagates from output layer tohidden layer of last time step N .
The derivativeswith respect to the hidden active of position i at thelast time step N can be calculated as follows:?iN = f ?
(aiN )?Oy vi, (4)where vi represents the weight of hidden-outputconnection and the activation of the output layer y.The gradients of hidden layer of previous time stepscan be recursively computed as:?it = f ?
(ait)?j?jt+1wij .
(5)Given all (suppose the number is T) the trainingexamples (xi, yi), we can then define the objectivefunction as follows:J(?)
=T?i=1logp(y(i)|x(i), ?).
(6)To compute the network parameter ?, we max-imize the log likelihood J(?)
through stochasticgradient decent over shuffled mini-batches with theAdadelta(Zeiler, 2012) update rule.2.2 RNN-RADDAs mentioned in the previous section, featuresextracted from external dictionary have been empir-ically proved to be useful for Chinese NLP varioustasks.
However, since these external dictionariesare usually pre-constructed, the out-of-vocabularyproblem may impact the performance.
Hence, we inthis work propose to use RNN to determine whethera given sequence of characters is a word or not.Then the proposed RADD is incorporated into RNN(RNN-RADD is used as the abbreviation of thecombined model).7232-gram ct?2ct?1, ct?2ct?1, ct?1ctctct+i, ct+1ct+23-gram ct?2ct?1ct, ct?1ctct+1, ctct+1ct+24-gram ct?2ct?1ctct+1, ct?1ctct+1ct+2Table 1: Illustration of the templates used to generate mt.RNN-RADD also follows the character basedmethodology.
Hence, the basic units of RNN-RADD are Chinese characters.
The architectureis illustrated in Fig.
1, where ct denotes the inputcharacter at time t encoded using dense distributedrepresentation.
The hidden layer st also maintainsthe history of the character sequence.
yt denotes theprobability distribution over labels.
mt representsthe features generated through RADD.
Followingprevious works, we construct a number of textsegments from the contexts of the character basedon pre-defined templates.
The templates used inthis work is shown in the Table.
For an input textsegment, RADD generates a binary value to indicatewhether or not the text segment is a word a not.
mtjrepresents the value of the output corresponding tothe jth template for the tth character.
Each layerrepresents a set of neurons.
Layers are connectedwith weights denoted by the matrices U, W, V, Fs,and Fy.The values in the hidden and output layers in theRNN-RADD can be expressed as follows:st = f(Uct + Fsmt + Wst?1), (7)yt = g(Vst + Fymt).Since RAD is trained separately with large scaledomain independent dictionaries.
In this work, theweight matrices of the RNN-RADD are updatedwith the similar way as RNN.
The error loss functionis computed via cross entropy criterion.
Theparameters are trained by stochastic gradient descentusing BPTT.
In order to speed up training process,the mt and character embeddings are keep statistic,during the training procedure.2.3 Learning MethodBased on the Eq.
(3) and Eq.
(4), Log-scale objectivefunctionsQ(?)
of RNN-RADD can be calculated as:Q(?)
=T?t=1(?ay?t?1y?t + zy?tt ?
logZR?CRF ).To update the label transition weights, we computegradients as follows:?Q(?
)?aji= ??t?
(yt?1 = j, yt = i)?
?
?t(?jt?1?itexp(?aji + zti)?j ?jt?jt),where ?it?1 is the sum of partial path scores endingat position t?1, with label i, which can be computedas follows:?it?1 = exp(zit?1)?j?jt?2exp(?aji).
?jt is the sum of partial path scores starting atposition t, with label j and exclusive of observationt, which can be computed as follows:?jt =?q?qt+1exp(?ajq + zjt+1).The model parameters ?
are updated using stochasticgradient ascent (SGA) over the training data multi-ple passes.3 ExperimentsTo demonstrate the effectiveness of the proposedmethod, we first compared the proposed RNN-baseddynamic dictionary construction method againstseveral baseline methods on the task.
Then, weevaluated the performance of the proposed methodon two Chinese natural language processing tasks:Chinese word segmentation, and opinion targetextraction.3.1 Experimental SettingsTo generate the distributed representations for Chi-nese characters, we use the method similar to Skip-ngram (Mikolov et al, 2013), which has been suc-cessfully employed in comparable tasks.
However,724in this work, characters were considered the basicunits of data, and the toolkit was provided by theauthors1.
We used Sogou news corpus (SogouCA2),which consists of news articles belonging to 18different domains published from June 2012 to July2012, as the training data to optimize the distributedrepresentations of Chinese characters.
After severalexperiments on development, we decided to setthe dimension of the character embedding to 200.Through several evaluations on the validation set, inboth RNN-RAD and RAD, the hidden layer size isset to 50.3.2 Learning Chinese Dynamic DictionaryFor training and testing the proposed dynamicdictionary method, we constructed a dataset bycollecting words and names from publicly availableresources belonging to different domains, includinga Chinese dictionary3, an English-Chinese bilin-gual wordlist4, Baidu Baike5, the Chinese DomainDictionary6, and the Chinese person names list7.After removing duplicates, the dataset contains11,406,995 words in total.
Based on the statics ofthe dictionary we used, about 80.6% of Chineseperson names are three characters, and words withtwo characters comprise the majority of the normalChinese dictionary.
Since some sources containcorporation and organization names, there are alsoa number of words whose lengths are longer thanten characters.
However, in all sources, most of thewords are less than five characters.We randomly selected 50,000 items for use astest data and an additional 50,000 items for use asdevelopment data for tuning parameters.
In additionto these positive examples, for training and testing,we also needed negative examples, so we extractedbigrams, trigrams, 4-grams, and 5-grams from theSogouCA Then, we randomly extracted a numberof n-grams which were not included in the collectedword lists described above as negative training data.We treat these n-grams as negative results.
For1https://code.google.com/p/word2vec/2http://www.sogou.com/labs/dl/ca.html3http://download.csdn.net/detail/logken/35753764https://catalog.ldc.upenn.edu/LDC2002L275http://baike.baidu.com6http://www.datatang.com/data/44250/7http://www.datatang.com/data/13482training, testing, and development, we randomlyselected 20 million, 50,000, and 50,000 n-gramsrespectively.Besides the proposed RADD method, we alsoevaluated some state-of-the-art supervised methods,including:Support Vector Machine (SVM) is one of the mostcommon supervised methods and has been success-fully used for various tasks (Hearst et al, 1998).Hence, in this work, we also evaluated its perfor-mance on the same task.
We used the characters asfeatures to construct the vector representation.
Sincethe number of Chinese characters is limited, we usedall of the characters existing in the training data.
Weused LIBSVM to implement (Chang and Lin, 2011).Conditional Random Fields (CRFs) were pro-posed by Lafferty et al (2001) to model sequencelabeling tasks.
According to the description given in?2.2, an NLP task can be converted into a sequencelabeling problem.
Hence, we used CRF to modelcharacters as basic features and several combinationtemplates of them.
Compared to SVM, CRF takesboth richer features and the labeling sequence intoconsideration.
CRF++ 0.588 was used to do theexperiments.Dynamic Convolutional Neural Network(DCNN), defined by Kalchbrenner et al (2014), isused to model sentence semantics.
The proposedmethod can handle input sequences of varyinglength, so we adopted their method by using theembeddings of characters as input.
The toolkit weused in this work is provided by the authors9.Recursive Autoencoder (RAE) (Socher et al,2011), is a machine learning framework forrepresenting variable sized words with a fixedlength vector.
In this work, we used greedyunsupervised RAE for modeling sequences ofChinese characters.
The toolkit was provided by theauthors 10.
Then, SVM was used to do the binaryclassification based on the generated vectors.Table 3.2 illustrates the results of the differentmethods on this task.
From the results, we seethat the proposed method obtains the best perfor-8http://crfpp.googlecode.com/svn/trunk/doc/index.html9http://nal.co/DCNN10http://www.socher.org/725Methods P R F1SVM 82.27% 84.74% 83.49%CRF 80.81% 86.82% 83.71%DCNN 86.86% 86.55% 86.71%RAE 84.77% 85.45% 85.11%RADD 89.74% 91.00% 90.39%Table 2: Comparison of different methods on the dynamicdictionary construction task.mance among all of the approaches.
DCNN, RAE,and RADD outperform SVM and CRF, which usecharacters as features.
One possible reason is thatthe character representations are more powerful incapturing morphology than characters only.
Anotheradvantage of the deep learning framework is that itcan be easily trained and makes feature engineeringefforts unnecessary.We also note that although DCNN can captureword relations of varying size in modelling sen-tences, RADD achieves better performance on thetask of learning the morphology of Chinese.
Onepossible interpretation is that although the relationsbetween words in a given sentence can be wellcaptured by DCNN, relations usually exist betweennearby characters hence the recurrent network ismore appropriate for the task.
Moreover, RADD ismuch easier to implement and is more efficient thanDCNN.Fig.
3 shows the performance of RADD with dif-ferent character embedding dimensions and hiddenlayer sizes.
From the figure, we see that RADDachieves the best result when the hidden layer sizeis larger than 200.
We also observe that RNN canachieve the highest performance with many differentparameters.
This means that we can easily findoptimal hyper parameters.3.3 Experimental Results3.3.1 Abbreviation GenerationThe task of generating entity abbreviations in-volves producing abbreviated equivalents of theoriginal entities.
For example, ??
is anabbreviation of ????
(Peking University).Previous methods usually formulate the task asa sequence labeling problem and model it usingcharacter features (Yang et al, 2009; Xie et al,5010020086.0%87.0%88.0%89.0%90.0%91.0%50100200300F1 ScoreFigure 3: The results of RAD with different characterembedding dimension and hidden layer size.2011).
Although Chen et al (2013) proposed touse Markov logic networks (MLN) (Richardsonand Domingos, 2006) to combine local and globalconstraints, the morphology of Chinese was rarelyconsidered.In this work, we report the performance of?RNN-RADD?, which takes the dynamic Chinesedictionary into consideration, on the dataset con-structed by Chen et al (2013).
The dataset contains50,232 entity abbreviation pairs.
They also reportedthe performance achieved by their method on thedataset.
We follow the strategy used by Chen etal.
(2013) to generate training and test data.
75% ofrandomly selected pairs are used for training data,5% for development, and the other 20% are used fortesting purposes.For comparison, we also report results achievedby the state-of-the-art methods.
Yang et al (2009)transferred the abbreviation generation method intoa sequence labeling problem and proposed to useCRF to model it with several linguistic features.Chen et al (2013) introduced local and positionfeatures and proposed to use MLN to achieve thetask.
We directly reference and report the resultsachieved by these methods on the dataset.Table 3.3.1 shows the relative performances ofthe different methods.
?SVM?
and ?RNN?
denotethe results of SVM and RNN on the sequencelabeling problem, respectively.
From the results,we see that RNN-RADD achieves the best resultamong all the methods.
The relative improvement726Methods AccuracyCRFs-Yang (Yang et al, 2009) 39.70%CRFs-LF+DPF (Chen et al, 2013) 40.60%MLN (Chen et al, 2013) 56.80%SVM 40.00%RNN 60.65%RNN-RADD 65.98%Table 3: Performance of different methods on abbreviationgeneration task.
CRFs-Yang represents the method and featuresets proposed by Yang et al (2009).
CRF-LF+DPF denotes thelocal and position features introduced by Chen et al (2013).MLN represents the method incorporating local and globalconstraints with MLN.of it over the previous best result achieved byMLN is about 16.2%.
Comparing the performanceof RNN-RADD with RNN, we also observe thatthe dynamic dictionary of Chinese can benefitthe abbreviation generation task.
The relativeimprovement is approximately 7.3%.Fig.
4 shows the values of log-scale objectivefunction of RNN and RNN-RADD during trainingon the data set.
From this figure, we can concludethat the RNN based dynamic dictionary can ben-efit the task.
Although additional feature vectormi is included, the absolutely value of objectivefunction is lower than its of RNN.
It can in somedegree demonstrate the effectiveness of the proposedmethod.RNN RNN-RADDLoss102103104105Epoch1 2 3 4 5 6 7 8 9 10 11 12 13 14 15Figure 4: Comparison of RNN and RNN-RADD duringtraining on the abbreviation data set.
The vertical axis is thevalue of log-scale objective functions.
Horizontal axis is thenumber of epochs during training.Sentences TargetsTraining 59,786 40,227Test 11,829 8,034Development 4,061 2,673Table 4: Statistics of the dataset used for the opinion targetextraction task.3.3.2 Opinion Target ExtractionOpinion target extraction is a key subtask in thefine-grained opinion mining problem.
The goal of itis to identify the items which opinions are expressedon from the given sentences.
For example:The image quality of the camera is amazing.The ?image quality?
is the opinion target of thesentence.
Previous methods studied the problemfrom different perspectives using supervised and un-supervised methods.
Syntactic structure constituentis one of the most common assumptions used byprevious works (Popescu and Etzioni, 2007; Qiu etal., 2009; Wu et al, 2009; Xu et al, 2014).
Sincethese works usually use character level features,meaningless text segments are one of the major errortypes.
Therefore, we integrate the dynamic Chinesedictionary into this method to detect and discardmeaningless text segments.To evaluate the proposed method, we used adataset containing more than 6,000 reviews, whichcontains 75,676 sentences, about vehicles.
Theopinion target and opinion words were manuallylabeled.
About 80% of the whole dataset israndomly selected for training.
15% and 5% reviewsare selected as the test and development datasetsrespectively.
Details of the data are listed inTable 3.3.2.The task can also be modelled by sequencelabelling problem.
Hence, besides the proposedRNN-RADD method, we also evaluated some state-of-the-art supervised methods, including: CRF,SVM, and RNN.
We used SVM and CRF underthe character-based methodology for comparison.RNN is based on the character level embeddings.Table 3.3.2 shows the results of the different meth-ods on the opinion target extraction task.
Fromthe results, we can see that, the proposed methodRNN-RADD achieve the best performance in F1score.
Comparing the results of RNN with RNN-RADD, we see that the proposed dynamic dictionary727Methods P R F1CRF 71.1% 77.5% 74.2%CRF+D 72.5% 74.3% 73.4%SVM 77.2% 74.9% 76.0%SVM+D 78.1% 74.3% 76.2%RNN 79.5% 81.7% 80.6%RNN-RADD 85.5% 81.5% 83.4%Table 5: Results of different methods on the opinion targetextraction task.method can benefit the RNN based method.
Theerror reduction achieved by its incorporation is about11.4%.
From the results of CRF and CRF+D, wecan observe that dictionary is not always usefulness.We think that the main reason that the dictionarymay bring too much conflict.
From the resultsof CRF and RNN, we can see that similar to theChinese word segmentation task, methods usingcharacter dense representations can usually achievebetter performance than character based methods.4 Related WorkAlthough dictionary can be manually constructed, itis a time-consuming work.
Moreover, these man-ually constructed dictionaries are usually updatedonly occasionally.
It would take months beforeit could be updated.
Hence, automatic dictionaryconstruction methods have also been investigatedin recent years.
Chang and Su (1997) proposedan unsupervised iterative approach for extractingout-of-vocabulary words from Chinese text corpora.Khoo (Khoo et al, 2002) introduced a method basedon stepwise logistic regression to identify two-andthree-character words in Chinese text.
Jin andWong (2002) incorporated local statistical informa-tion, global statistical information and contextualconstraints to identify Chinese words.
For collectingThai unknown words, Haruechaiyasak et al (2006)proposes a collaborative framework for achievingthe task based on Web pages over the Internet.Except these unsupervised methods, there havebeen other approaches requiring additional infor-mation or selective input.
Yarowsky and Wicen-towski (2000) proposed to use labeled corpus totrain a supervised method for transforming past-tense in English.
Rogati et al (2003) introduceda stemming model based on statistical machinetranslation for Arabic.
They used a parallel corpusto train the model.
Luong et al (2013) studiedthe problem of word representations for rare andcomplex words.
They proposed to combine recur-sive neural networks and neural language models tobuild representations for morphologically complexwords from their morphemes.
Since English isusually considered limited in terms of morphology,their method can handle unseen words, whoserepresentations could be constructed from vectors ofknown morphemes.However, most of the existing Chinese dictio-nary construction methods focused on find out-of-vocabulary words from corpus.
In this paper,we propose to transfer the dictionary constructionproblem to classification task and use a modifiedrecurrent neutral network to directly model whethera given sequences of characters is a word or not.5 ConclusionIn this work, we studied the problem of dynamic dic-tionary which tries to determine whether a sequenceof Chinese characters is a word or not.
We proposeda deep feed forward network architecture (RADD)to model the problem and integrated it into RNNmethod.
To train the model and evaluate the ef-fectiveness of the proposed method, we constructeda dataset containing more than 11 million words.By applying the proposed combined method to twodifferent Chinese NLP tasks, we can see that itcan achieve better performance than state-of-the-artmethods.
Comparing to the previous methods, thenumber of hyper parameters of the proposed methodRNN-RADD is small and less feature engineeringworks are needed.
In the future, we plan to integratethe dynamic dictionary into the term constructionmodel in information retrieval.6 AcknowledgementThe authors wish to thank the anonymous reviewersfor their helpful comments.
This work was partiallyfunded by National Natural Science Foundation ofChina (No.
61532011, 61473092, and 61472088),the National High Technology Research and Devel-opment Program of China (No.
2015AA015408).728ReferencesChih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27.Jing-Shin Chang and Keh-Yih Su.
1997.
Anunsupervised iterative method for chinese new lexiconextraction.
Computational Linguistics and ChineseLanguage Processing, 2(2):97?148.Huan Chen, Qi Zhang, Jin Qian, and Xuanjing Huang.2013.
Chinese named entity abbreviation generationusing first-order logic.
In Proceedings of the SixthInternational Joint Conference on Natural LanguageProcessing.Choochart Haruechaiyasak, ChatchawalSangkeettrakarn, Pornpimon Palingoon, SarawootKongyoung, and Chaianun Damrongrat.
2006.A collaborative framework for collecting thaiunknown words from the web.
In Proceedings of theCOLING/ACL.Marti A. Hearst, ST Dumais, E Osman, John Platt,and Bernhard Scholkopf.
1998.
Support vectormachines.
Intelligent Systems and their Applications,IEEE, 13(4):18?28.Honglan Jin and Kam-Fai Wong.
2002.
A chinesedictionary construction algorithm for informationretrieval.
ACM Transactions on Asian LanguageInformation Processing (TALIP).Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
In Proceedings of ACL.Christopher SG Khoo, Yubin Dai, and Teck Ee Loh.2002.
Using statistical and contextual informationto identify two-and three-character words in chinesetext.
Journal of the American Society for InformationScience and Technology, 53(5):365?377.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
In Proceedings of ICML.Xiaohua Liu, Ming Zhou, Furu Wei, Zhongyang Fu, andXiangyang Zhou.
2012.
Joint inference of namedentity recognition and normalization for tweets.
InProceedings of ACL.Minh-Thang Luong, Richard Socher, and Christopher D.Manning.
2013.
Better word representations withrecursive neural networks for morphology.
In CoNLL,Sofia, Bulgaria.Catherine McBride-Chang, Hua Shu, Aibao Zhou,Chun Pong Wat, and Richard K Wagner.
2003.Morphological awareness uniquely predicts youngchildren?s chinese character recognition.
Journal ofEducational Psychology, 95(4).Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InAdvances in Neural Information Processing Systems,pages 3111?3119.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detectionusing conditional random fields.
In Proceedings ofthe 20th international conference on ComputationalLinguistics.Ana-Maria Popescu and Orena Etzioni.
2007.
Extractingproduct features and opinions from reviews.
InNatural language processing and text mining, pages9?28.
Springer.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009.Expanding domain sentiment lexicon through doublepropagation.
In IJCAI, volume 9, pages 1199?1204.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning, 62(1-2):107?136.Monica Rogati, Scott McCarley, and Yiming Yang.
2003.Unsupervised learning of arabic stemming using aparallel corpus.
In Proceedings of ACL 2003.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-supervised recursive autoencoders for predictingsentiment distributions.
In Proceedings of the EMNLP?11.Paul J Werbos.
1990.
Backpropagation through time:what it does and how to do it.
Proceedings of theIEEE.Youzheng Wu, Jun Zhao, Bo Xu, and Hao Yu.
2005.Chinese named entity recognition based on multiplefeatures.
In Proceedings of HLT/EMNLP.Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu.2009.
Phrase dependency parsing for opinion mining.In Proceedings of EMNLP.Li-Xing Xie, Ya-Bin Zheng, Zhi-Yuan Liu, Mao-SongSun, and Can-Hui Wang.
2011.
Extracting chineseabbreviation-definition pairs from anchor texts.
InMachine Learning and Cybernetics (ICMLC).Liheng Xu, Kang Liu, Siwei Lai, and Jun Zhao.2014.
Product feature mining: Semantic clues versussyntactic constituents.
In Proceedings of the 52ndACL.Dong Yang, Yi-cheng Pan, and Sadaoki Furui.
2009.Automatic chinese abbreviation generation usingconditional random field.
In Proceedings of NAACL2009.David Yarowsky and Richard Wicentowski.
2000.Minimally supervised morphological analysis bymultimodal alignment.
In Proceedings of ACL.Matthew D Zeiler.
2012.
Adadelta: An adaptive learningrate method.
arXiv preprint arXiv:1212.5701.729Lei Zhang, Bing Liu, Suk Hwan Lim, and EamonnO?Brien-Strain.
2010.
Extracting and ranking productfeatures in opinion documents.
In Proceedings ofthe 23rd international conference on computationallinguistics: Posters.730
