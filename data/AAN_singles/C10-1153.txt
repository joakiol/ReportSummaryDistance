Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1362?1370,Beijing, August 2010A Minimum Error Weighting Combination Strategy for ChineseSemantic Role LabelingTao Zhuang and Chengqing ZongNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences{tzhuang, cqzong}@nlpr.ia.ac.cnAbstractMany Semantic Role Labeling (SRL)combination strategies have been pro-posed and tested on English SRL task.But little is known about how much Chi-nese SRL can benefit from system combi-nation.
And existing combination strate-gies trust each individual system?s outputwith the same confidence when mergingthem into a pool of candidates.
In our ap-proach, we assign different weights to dif-ferent system outputs, and add a weightedmerging stage to the conventional SRLcombination architecture.
We also pro-pose a method to obtain an appropriateweight for each system?s output by min-imizing some error function on the devel-opment set.
We have evaluated our strat-egy on Chinese Proposition Bank data set.With our minimum error weighting strat-egy, the F1 score of the combined resultachieves 80.45%, which is 1.12% higherthan baseline combination method?s re-sult, and 4.90% higher than the best in-dividual system?s result.1 IntroductionIn recent years, Chinese Semantic Role Labelinghas received much research effort (Sun and Juraf-sky, 2004; Xue, 2008; Che et al, 2008; Ding andChang, 2008; Sun et al, 2009; Li et al, 2009).And Chinese SRL is also included in CoNLL-2009 shared task (Hajic?
et al, 2009).
On the dataset used in (Xue, 2008), the F1 score of the SRLresults using automatic syntactic analysis is stillin low 70s (Xue, 2008; Che et al, 2008; Sun etal., 2009).
As pointed out by Xue (Xue, 2008),the SRL errors are mainly caused by the errorsin automatic syntactic analysis.
In fact, ChineseSRL suffers from parsing errors even more thanEnglish SRL, because the state-of-the-art parserfor Chinese is still not as good as that for En-glish.
And previous research on English SRLshows that combination is a robust and effectivemethod to alleviate SRL?s dependency on pars-ing results (Ma`rquez et al, 2005; Koomen etal., 2005; Pradhan et al, 2005; Surdeanu et al,2007; Toutanova et al, 2008).
However, the ef-fect of combination for Chinese SRL task is stillunknown.
This raises two questions at least: (1)How much can Chinese SRL benefit from combi-nation?
(2) Can existing combination strategiesbe improved?
All existing combination strate-gies trust each individual system?s output with thesame confidence when putting them into a poolof candidates.
But according to our intuition, dif-ferent systems have different performance.
Andthe system that have better performance shouldbe trusted with more confidence.
We can use ourprior knowledge about the combined systems todo a better combination.The observations above motivated the work inthis paper.
Instead of directly merging outputswith equal weights, different outputs are assigneddifferent weights in our approach.
An output?sweight stands for the confidence we have in thatoutput.
We acquire these weights by minimizingan error function on the development set.
Andwe use these weights to merge the outputs.
Inthis paper, outputs are generated by a full parsingbased Chinese SRL system and a shallow parsingbased SRL system.
The full parsing based system1362use multiple parse trees to generate multiple SRLoutputs.
Whereas the shallow parsing based sys-tem only produce one SRL output.
After mergingall SRL outputs, we use greedy and integer lin-ear programming combination methods to com-bine the merged outputs.We have evaluated our combination strategy onChinese Propbank data set used in (Xue, 2008)and get encouraging results.
With our minimumerror weighting (MEW) strategy, the F1 scoreof the combined result achieves 80.45%.
Thisis a significant improvement over the best re-ported SRL performance on this data set, whichis 74.12% in the literature (Sun et al, 2009).2 Related workA lot of research has been done on SRL combina-tion.
Most of them focused on English SRL task.But the combination methods are general.
Andthey are closely related to the work in this paper.Punyakanok et al (2004) formulated an IntegerLinear Programming (ILP) model for SRL.
Basedon that work, Koomen et al (2005) combined sev-eral SRL outputs using ILP method.
Ma`rquez etal.
(2005) proposed a combination strategy thatdoes not require the individual system to give ascore for each argument.
They used a binary clas-sifier to filter different systems?
outputs.
Thenthey used a greedy method to combine the can-didates that pass the filtering process.
Pradhanet al (2005) combined systems that are based onphrase-structure parsing, dependency parsing, andshallow parsing.
They also used greedy methodwhen combining different outputs.
Surdeanu etal.
(2007) did a complete research on a variety ofcombination strategies.
All these research showsthat combination can improve English SRL per-formance by 2?5 points on F1 score.
However,little is known about how much Chinese SRL canbenefit from combination.
And, as we will show,existing combination strategies can still be im-proved.3 Individual SRL Systems3.1 Full Parsing Based SystemThe full parsing based system utilize full syn-tactic analysis to perform semantic role labeling.We implemented a Chinese semantic role label-ing system similar to the one described in (Xue,2008).
Our system consists of an argument identi-fication stage and an argument classification stage.In the argument identification stage, a number ofargument locations are identified in a sentence.In the argument classification stage, each locationidentified in the first stage is assigned a semanticrole label.
The features used in this paper are thesame with those used in (Xue, 2008).Maximum entropy classifier is employed forboth the argument identification and classificationtasks.
And Zhang Le?s MaxEnt toolkit1 is used forimplementation.3.2 Shallow Parsing Based SystemThe shallow parsing based system utilize shal-low syntactic information at the level of phrasechunks to perform semantic role labeling.
Sunet al (2009) proposed such a system on ChineseSRL and reported encouraging results.
The sys-tem used in this paper is based on their approach.For Chinese chunking, we adopted the methodused in (Chen et al, 2006), in which chunking isregarded as a sequence labeling task with IBO2representation.
The features used for chunkingare the uni-gram and bi-gram word/POS tags witha window of size 2.
The SRL task is also re-garded as a sequence labeling problem.
For anargument with label ARG*, we assign the labelB-ARG* to its first chunk, and the label I-ARG*to its rest chunks.
The chunks outside of any argu-ment are assigned the label O.
The features usedfor SRL are the same with those used in the one-stage method in (Sun et al, 2009).In this paper, we employ Tiny SVM along withYamcha (Kudo and Matsumoto, 2001) for Chi-nese chunking, and CRF++2 for SRL.3.3 Individual systems?
outputsThe maximum entropy classifier used in full pars-ing based system and the CRF model used in shal-low paring based system can both output classi-fication probabilities.
For the full parsing basedsystem, the classification probability of the ar-1http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html2http://crfpp.sourceforge.net/1363gument classification stage is used as the argu-ment?s probability.
Whereas for the shallow pars-ing based system, an argument is usually com-prised of multiple chunks.
For example, an argu-ment with label ARG0 may contain three chunkslabeled as: B-ARG0, I-ARG0, I-ARG0.
And eachchunk has a label probability.
Thus we have threeprobabilities p1, p2, p3 for one argument.
In thiscase, we use the geometric mean of individualchunks?
probabilities (p1 ?
p2 ?
p3)1/3 as the ar-gument?s probability.As illustrated in Figure 1, in an individual sys-tem?s output, each argument has three attributes:its location in sentence loc, represented by thenumber of its first word and last word; its semanticrole label l; and its probability p.Sent: ????????????????
?Args: [ ARG0 ] [pred] [ ARG1 ]loc: (0, 2) (4, 7)l: ARG0 ARG1p: 0.94 0.92Figure 1: Three attributes of an output argument:location loc, label l, and probability p.So each argument outputted by a system is atriple (loc, l, p).
For example, the ARG0 in Fig-ure 1 is ((0, 2),ARG0, 0.94).
Because the outputsof baseline systems are to be combined, we callsuch triple a candidate for combination.4 Approach OverviewAs illustrated in Figure 2, the architecture of oursystem consists of a candidates generation stage, aweighted merging stage, and a combination stage.In the candidates generation stage, the baselinesystems are run individually and their outputs arecollected.
We use 2-best parse trees of Berkeleyparser (Petrov and Klein, 2007) and 1-best parsetree of Bikel parser (Bikel, 2004) and Stanfordparser (Klein and Manning, 2003) as inputs to thefull parsing based system.
The second best parsetree of Berkeley parser is used here for its goodquality.
So together we have four different out-puts from the full parsing based system.
From theshallow parsing based system, we have only oneoutput.SentenceWeightedmergingFull parsing based SRL system Shallow parsing based SRL systemBerkeleyparserBikelparserStanfordparser ChunkerOutput1 Output4Output3Output2 Output5Candidates poolCombinationFinal resultsCandidatesGenerationStageWeigthedMergingStageCombinationStageFigure 2: The overall architecture of our system.In the weighted merging stage, each systemoutput is assigned a weight according to our priorknowledge obtained on the development set.
De-tails about how to obtain appropriate weights willbe explained in Section 6.
Then all candidateswith the same loc and l are merged to one byweighted summing their probabilities.
Specifi-cally, suppose that there are n system outputs tobe combined, with the i-th output?s weight to bewi.
And the candidate in the i-th output with locand l is (loc, l, pi) (If there is no candidate with locand l in the i-th output, pi is 0.).
Then the mergedcandidate is (loc, l, p), where p = ?ni=1 wipi.After the merging stage, a pool of merged can-didates is obtained.
In the combination stage,candidates in the pool are combined to form aconsistent SRL result.
Greedy and integer lin-ear programming combination methods are exper-imented in this paper.13645 Combination Methods5.1 Global constraintsWhen combining the outputs, two global con-straints are enforced to resolve the conflict be-tween outputs.
These two constraints are:1.
No duplication: There is no duplication forkey arguments: ARG0 ?
ARG5.2.
No overlapping: Arguments cannot overlapwith each other.We say two argument candidates conflict witheach other if they do not satisfy the two constraintsabove.5.2 Two combination methodsUnder these constraints, two methods are exploredto combine the outputs.
The first one is a greedymethod.
In this method, candidates with probabil-ity below a threshold are deleted at first.
Then theremaining candidates are inspected in descendingorder according to their probabilities.
And eachcandidate will be put into a solution set if it doesnot conflict with candidates already in the set.This greedy combination method is very simpleand has been adopted in previous research (Prad-han et al, 2005; Ma`rquez et al, 2005).The second combination method is integer lin-ear programming (ILP) method.
ILP method wasfirst applied to SRL in (Punyakanok et al, 2004).Here we formulate an ILP model whose form isdifferent from the model in (Punyakanok et al,2004; Koomen et al, 2005).
For convenience, wedenote the whole label set as {l1, l2, .
.
.
, ln}.
Andlet l1 ?
l6 stand for the key argument labels ARG0?
ARG5 respectively.
Suppose there are m differ-ent locations, denoted as loc1, .
.
.
, locm, amongall candidates in the pool.
And the probability ofassigning lj to loci is pij .
A binary variable xij isdefined as:xij ={1 if loci is assigned label lj ,0 otherwise.The objective of the ILP model is to maximize thesum of arguments?
probabilities:maxm?i=1n?j=1(pij ?
T )xij (1)where T is a threshold to prevent including toomany candidates in solution.
T is similar to thethreshold in greedy combination method.
In thispaper, both thresholds are empirically tuned ondevelopment data, and both are set to be 0.2.The inequalities in equation (2) make sure thateach loc is assigned at most one label.
?1 ?
i ?
m :n?j=1xij ?
1 (2)The inequalities in equation (3) satisfy the Noduplication constraint.
?1 ?
j ?
6 :m?i=1xij ?
1 (3)For any location loci, let Ci denote the indexset of the locations that overlap with it.
Thenthe No overlapping constraint means that if lociis assigned a label, i.e., ?nj=1 xij = 1, then forany k ?
Ci, lock cannot be assigned any label,i.e., ?nj=1 xkj = 0.
A common technique in ILPmodeling to form such a constraint is to use a suf-ficiently large auxiliary constant M .
And the con-straint is formulated as:?1 ?
i ?
m :?k?Cin?j=1xkj ?
(1?n?j=1xij)M(4)In this case, M only needs to be larger than thenumber of candidates to be combined.
In this pa-per, M = 500 is large enough.
And we employlpsolve3 to solve the ILP model.Note that the form of the ILP model in thispaper is different from that in (Punyakanok etal., 2004; Koomen et al, 2005) in three as-pects: (1) A special label class null, which meansno label is assigned, was added to the label setin (Punyakanok et al, 2004; Koomen et al, 2005).Whereas no such special class is needed in ourmodel, because if no label is assigned to loci,?nj=1 xij = 0 would simply indicate this case.This makes our model contain fewer variables.
(2) Without null class in our model, we need touse a different technique to formulate the No-overlapping constraint.
(3) In order to compare3http://lpsolve.sourceforge.net/1365with the greedy combination method, the ILPmodel in this paper conforms to exactly the sameconstraints as the greedy method.
Whereas manymore global constraints were taken into accountin (Punyakanok et al, 2004; Koomen et al, 2005).6 Train Minimum Error WeightsThe idea of minimum error weighting is straight-forward.
Individual outputs O1, O2, .
.
.
, Onare assigned weights w1, w2, .
.
.
, wn respectively.These weights are normalized, i.e., ?ni=1 wi = 1.An output?s weight can be seen as the confidencewe have in that output.
It is a kind of prior knowl-edge we have about that output.
We can gain thisprior knowledge on the development set.
As longas the data of the development set and the test setare similar, this prior knowledge should be ableto help to guide SRL combination on test set.
Inthis section, we discuss how to obtain appropriateweights.6.1 Training modelSuppose the golden answer and SRL result on de-velopment set are d and r respectively.
An errorfunction Er(r, d) is a function that measures theerror contained in r in reference to d. An errorfunction can be defined as the number of wrongarguments in r. It can also be defined using preci-sion, recall, or F1 score.
For example, Er(r, d) =1?
Precision(r, d), or Er(r, d) = 1?
F1(r, d).Smaller value of error function means less error inr.The combination process can also be seen asa function, which maps the outputs and weightsto the combined result r: r = Comb(On1 , wn1 ).Therefore, the error function of our system on de-velopment set is:Er(r, d) = Er(Comb(On1 , wn1 ), d) (5)From equation (5), it can be seen that: Given de-velopment set d, if the outputs to be combined On1and the combination method Comb are fixed, theerror function is just a function of the weights.
Sowe can obtain appropriate weights by minimizingthe error function:w?n1 = argminwn1Er(Comb(On1 , wn1 ), d) (6)6.2 Training algorithmAlgorithm 1 Powell Training Algorithm.1: Input : Error function Er(w).2: Initialize n directions d1, .
.
.
,dn, anda start point w in Rn.3: Set termination threshold ?.4: do:5: w1 ?
w6: for i ?
1, .
.
.
, n:7: ?i ?
argmin?
f(wi + ?di)8: wi+1 ?
wi + ?idi9: dn+1 ?
wn+1 ?w10: ??
?
argmin?f(w + ?dn+1)11: w?
?
w + ?
?dn+112: ?Er ?
Er(w)?
Er(w?
)13: i ?
arg max1?j?nEr(wj)?
Er(wj+1)14: if (??
)2 ?
?ErEr(wi)?
Er(wi+1) :15: for j ?
i, .
.
.
, n:16: dj ?
dj+117: w ?
w?18: while ?Er > ?19: Output: The minimum error weights w.There are two difficulties to solve the optimiza-tion problem in equation 6.
The first one is thatthe error function cannot be written to an analyt-ical form.
This is because the Comb function,which stands for the combination process, cannotbe written as an analytical formula.
So the prob-lem cannot be solved using canonical gradient-based optimization algorithms, because the gradi-ent function cannot be derived.
The second diffi-culty is that, according to our experience, the er-ror function has many local optima, which makesit difficult to find a global optima.To resolve the first difficulty, Modified Powell?smethod (Yuan, 1993) is employed to solve the op-timization problem.
Powell?s method is a heuris-tic search method that does not require the objec-tive function to have an explicit analytical form.The training algorithm is presented in Algorithm1.
In Algorithm 1, the line search problem in steps7 and 10 is solved using Brent?s method (Yuan,1993).
And the temination threshold ?
is empiri-cally set to be 0.001 in this paper.1366To resolve the second difficulty, we performmultiple searches using different start points, andthen choose the best solution found.7 Experiments7.1 Experimental setupWe use Chinese Proposition Bank (CPB) 1.0 andChinese Tree Bank (CTB) 5.0 of Linguistic DataConsortium corpus in our experiments.
The train-ing set is comprised of 648 files(chtb 081.fid tochtb 885.fid).
The development set is comprisedof 40 files(chtb 041.fid to chtb 080.fid).
Thetest set is comprised of 72 files(chtb 001.fid tochtb 040.fid and chtb 900.fid to chtb 931.fid).The same data setting has been used in (Xue,2008; Ding and Chang, 2008; Sun et al, 2009).Sun et al (2009) used sentences with golden seg-mentation and POS tags as input to their SRLsystem.
However, we use sentences with onlygolden segmentation as input.
Then we performautomatic POS tagging using Stanford POS tag-ger (Toutanova et al, 2003).
In (Xue, 2008), theparser used by the SRL system is trained on thetraining and development set plus 275K words ofbroadcast news.
In this paper, all parsers usedby the full parsing based system are trained onthe training set plus the broadcast news portionof CTB6.0.
And the chunker used in the shallowparsing based system is trained just on the trainingset.7.2 Individual outputs?
performanceIn this paper the four outputs of the full parsingbased system are represented by FO1 ?
FO4 re-spectively.
Among them, FO1 and FO2 are theoutputs using the first and second best parse treesof Berkeley parser, FO3 and FO4 are the outputsusing the best parse trees of Stanford parser andBikel parser respectively.
The output of the shal-low parsing based system is represented by SO.The individual outputs?
performance on develop-ment and test set are listed in Table 1.From Table 1 we can see that the performanceof individual outputs are similar on developmentset and test set.
On both sets, the F1 scores ofindividual outputs are in the same order: FO1 >FO2 > SO > FO3 > FO4.Data set Outputs P (%) R(%) F1FO1 79.17 72.09 75.47FO2 77.89 70.56 74.04development FO3 72.57 67.02 69.68FO4 75.60 63.45 69.00SO 73.72 67.35 70.39FO1 80.75 70.98 75.55FO2 79.44 69.37 74.06test FO3 73.95 66.37 70.00FO4 75.89 63.26 69.00SO 75.69 67.90 71.59Table 1: The results of individual systems on de-velopment and test set.7.3 Combining outputs of full parsing basedsystemIn order to investigate the benefit that the fullparsing based system can get from using multi-ple parsers, we combine the four outputs FO1 ?FO4.
The combination results are listed in Ta-ble 2.
In tables of this paper, ?Grd?
and ?ILP?stand for greedy and ILP combination methods re-spectively, and ?+MEW?
means the combinationis performed with MEW strategy.P (%) R(%) F1Grd 82.68 73.36 77.74ILP 82.21 73.93 77.85Grd+MEW 81.30 75.38 78.23ILP+MEW 81.27 75.74 78.41Table 2: The results of combining outputs of fullparsing based system on test set.Er FO1 FO2 FO3 FO4Grd 1?
F1 0.31 0.16 0.30 0.23ILP 1?
F1 0.33 0.10 0.27 0.30Table 3: The minimum error weights for the re-sults in Table 2.From Table 2 and Table 1, we can see that, with-out MEW strategy, the F1 score of combinationresult is about 2.3% higher than the best individ-ual output.
With MEW strategy, the F1 score isimproved about 0.5% further.
That is to say, withMEW strategy, the benefit of combination is im-proved by about 20%.
Therefore, the effect ofMEW is very encouraging.Here the error function for MEW training ischosen to be 1 ?
F1.
And the trained weightsfor greedy and ILP methods are listed in Table 31367separately.
In tables of this paper, the column Ercorresponds to the error function used for MEWstrategy.7.4 Combining all outputsWe have also combined all five outputs.
The re-sults are listed in Table 4.
Compared with the re-sults in Table 2, we can see that the combinationresults is largely improved, especially the recall.P (%) R(%) F1Grd 83.64 75.32 79.26ILP 83.31 75.71 79.33Grd+MEW 83.34 77.47 80.30ILP+MEW 83.02 78.03 80.45Table 4: The results of combining all outputs ontest set.From Table 4 and Table 1 we can see that with-out MEW strategy, the F1 score of combinationresult is about 3.8% higher than the best individ-ual output.
With MEW, the F1 score is improvedfurther by more than 1%.
That means the bene-fit of combination is improved by over 25% withMEW strategy.Here the error function for MEW training is still1 ?
F1, and the trained weights are listed in Ta-ble 5.Er FO1 FO2 FO3 FO4 SOGrd 1?
F1 0.23 0.12 0.23 0.20 0.22ILP 1?
F1 0.24 0.08 0.22 0.21 0.25Table 5: The minimum error weights for the re-sults in Table 4.7.5 Using alternative error functions forminimum error weights trainingIn previous experiments, we use 1 ?
F1 as errorfunction.
As pointed out in Section 6, the def-inition of error function is very general.
So wehave experimented with two other error functions,which are 1 ?
Precision, and 1 ?
Recall.
Ob-viously, these two error functions favor precisionand recall separately.
The results of combiningall five outputs using these two error functions arelisted in Table 6, and the trained weights are listedin Table 7.From Table 6 and Table 4, we can see that when1 ?
Precison is used as error function, the pre-Er P (%) R(%) F1Grd+MEW 1?
P 85.31 73.42 78.92ILP+MEW 1?
P 85.62 72.76 78.67Grd+MEW 1?R 81.94 77.55 79.68ILP+MEW 1?R 79.74 78.34 79.03Table 6: The results of combining all outputs withalternative error functions.Er FO1 FO2 FO3 FO4 SOGrd 1?
P 0.25 0.24 0.22 0.22 0.07ILP 1?
P 0.30 0.26 0.20 0.15 0.09Grd 1?R 0.21 0.10 0.17 0.15 0.37ILP 1?R 0.24 0.04 0.10 0.22 0.39Table 7: The minimum error weights for the re-sults in Table 6.cision of combination result is largely improved.But the recall decreases a lot.
Similar effect of theerror function 1?Recall is also observed.The results of this subsection reflect the flex-ibility of MEW strategy.
This flexibility comesfrom the generality of the definition of error func-tion.
The choice of error function gives us somecontrol over the results we want to get.
We candefine different error functions to favor precision,or recall, or some error counts such as the numberof misclassified arguments.7.6 DiscussionIn this paper, the greedy and ILP combinationmethods conform to the same simple constraintsspecified in Section 5.
From the experimentresults, we can see that ILP method generatesslightly better results than greedy method.In Subsection 7.4, we see that combining alloutputs using ILP method with MEW strategyyields 4.90% improvement on F1 score over thebest individual output FO1.
In order to under-stand each output?s contribution to the improve-ment over FO1.
We compare the differences be-tween outputs.Let CO denote the set of correct arguments inan output O.
Then we get the following statisticswhen comparing two outputs A and B: (1) thenumber of common correct arguments in A andB, i.e., |CA ?
CB| ; (2) the number of correct ar-guments in A and not in B, i.e., |CA \CB|; (3) thenumber of correct arguments in B and not in A,i.e., |CB \ CA|.
The comparison results between1368some outputs on test set are listed in Table 8.
Inthis table, UF stands for the union of the 4 outputsFO1 ?
FO4.A B |CA ?
CB | |CA \ CB | |CB \ CA|FO2 5498 508 372FO3 5044 962 552FO4 4815 1191 512FO1SO 4826 1180 920UF SO 5311 1550 435Table 8: Comparison between outputs on test set.From Table 8 we can see that the output SOhas 4826 common correct arguments with FO1,which is relatively small.
And, more importantly,SO contains 920 correct arguments not in FO1,which is much more than any other output con-tains.
Therefore, SO is more complementary toFO1 than other outputs.
On the contrary, FO2 isleast complementary to FO1.
Even compared withthe union of FO1 ?
FO4, SO still contains 435correct arguments not in the union.
This showsthat the output of shallow parsing based system isa good complement to the outputs of full parsingbased system.
This explains why recall is largelyimproved when SO is combined in Subsection 7.4.From the analysis above we can also see that theweights in Table 5 are quite reasonable.
In Ta-ble 5, SO is assigned the largest weight and FO2is assigned the smallest weight.In Subsection 7.3, the MEW strategy improvesthe benefit of combination by about 20%.
And inSubsection 7.4, the MEW strategy improves thebenefit of combination by over 25%.
This showsthat the MEW strategy is very effective for Chi-nese SRL combination.To our best knowledge, no results on ChineseSRL combination has been reported in the litera-ture.
Therefore, to compare with previous results,the top two results of single SRL system in theliterature and the result of our combination sys-tem on this data set are listed in Table 9.
For theresults in Table 9, the system of Sun et al usessentences with golden POS tags as input.
Xue?ssystem and our system both use sentences withautomatic POS tags as input.
The result of Sunet al (2009) is the best reported result on this dataset in the literature.POS P (%) R(%) F1(Xue, 2008) auto 76.8 62.5 68.9(Sun et al, 2009) gold 79.25 69.61 74.12Ours auto 83.02 78.03 80.45Table 9: Previous best single system?s results andour combination system?s result on this data set.8 ConclusionsIn this paper, we propose a minimum errorweighting strategy for SRL combination and in-vestigate the benefit that Chinese SRL can getfrom combination.
We assign different weights todifferent system outputs and add a weighted merg-ing stage to conventional SRL combination sys-tem architecture.
And we also propose a methodto train these weights on development set.
Weevaluate the MEW strategy on Chinese Propbankdata set with greedy and ILP combination meth-ods.Our experiments have shown that the MEWstrategy is very effective for Chinese SRL combi-nation, and the benefit of combination can be im-proved over 25% with this strategy.
And also, theMEW strategy is very flexible.
With different def-initions of error function, this strategy can favorprecision, or recall, or F1 score.
The experimentshave also shown that Chinese SRL can benefit alot from combination, especially when systemsbased on different syntactic views are combined.The SRL result with the highest F1 score in thispaper is generated by ILP combination togetherwith MEW strategy.
In fact, the MEW strategy iseasy to incorporate with other combination meth-ods, just like incorporating with the greedy andILP combination methods in this paper.AcknowledgmentThe research work has been partially funded bythe Natural Science Foundation of China underGrant No.
60975053, 90820303 and 60736014,the National Key Technology R&D Program un-der Grant No.
2006BAH03B02, the Hi-Tech Re-search and Development Program (?863?
Pro-gram) of China under Grant No.
2006AA010108-4, and also supported by the China-Singapore In-stitute of Digital Media (CSIDM) project undergrant No.
CSIDM-200804.1369ReferencesDaniel Bikel.
2004.
Intricacies of Collins ParsingModel.
Computational Linguistics, 30(4):480-511.Wanxiang Che, Min Zhang, Ai Ti Aw, Chew Lim Tan,Ting Liu, and Sheng Li.
2008.
Using a Hybrid Con-volution Tree Kernel for Semantic Role Labeling.ACM Transactions on Asian Language InformationProcessing, 2008, 7(4).Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.2006.
An empirical study of Chinese chunking.
InProceedings of COLING/ACL-2006.Weiwei Ding and Baobao Chang.
2008.
ImprovingChinese Semantic Role Classification with Hierar-chical Feature Selection Strategy.
In Proceedings ofEMNLP-2008.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue and Yi Zhang.
2009.
The CoNLL-2009 Shared Task: Syntactic and Semantic Depen-dencies in Multiple Languages.
In Proceedings ofCoNLL-2009.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of ACL-2003.Peter Koomen, Vasin Punyakanok, Dan Roth, andWen-tau Yih.
2005.
Generalized Inference withMultiple Semantic Role Labeling Systems.
In Pro-ceedings of CoNLL-2005 shared task.Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith Support Vector Machines.
In Proceedings ofNAACL-2001.Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu,and Peide Qian.
2009.
Improving Nominal SRLin Chinese Language with Verbal SRL Informationand Automatic Predicate Recognition.
In Proceed-ings of EMNLP-2009.Llu?
?s Ma`rquez, Mihai Surdeanu, Pere Comas, andJordi Turmo.
2005.
A Robust Combination Strat-egy for Semantic Role Labeling.
In Proceedings ofEMNLP-2005.Slav Petrov and Dan Klein.
2007.
Improved Inferencefor Unlexicalized parsing.
In Proceedings of ACL-2007.Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,James H. Martin, and Daniel Jurafsky.
2005.
Se-mantic Role Labeling Using Different SyntacticViews.
In Proceedings of ACL-2005.Vasin Punyakanok, Dan Roth, Wen-tau Yih, and DavZimak.
2004.
Semantic Role Labeling via IntegerLinear Programming Inference.
In Proceedings ofCOLING-2004.Honglin Sun and Daniel Jurafsky.
2004.
Shallowsemantic parsing of Chinese.
In Proceedings ofNAACL-2004.Weiwei Sun, Zhifang Sui, Meng Wang, and Xin Wang.2009.
Chinese Semantic Role Labeling with Shal-low Parsing.
In Proceedings of EMNLP-2009.Mihai Surdeanu, Llu?
?s Ma`rquez, Xavier Carreras, andPere R. Comas.
2007.
Combination Strategies forSemantic Role Labeling.
Journal of Artificial Intel-ligence Research (JAIR), 29:105-151.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A Global Joint Model for Se-mantic Role Labeling.
Computational Linguistics,34(2): 145-159.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.
In Proceedings of HLT-NAACL-2003.Nianwen Xue.
2008.
Labeling Chinese Predicateswith Semantic Roles.
Computational Linguistics,34(2): 225-255.Yaxiang Yuan.
1993.
Numerical Methods for Nonlin-ear Programming.
Shanghai Scientific and Techni-cal Pulishers, Shanghai.1370
