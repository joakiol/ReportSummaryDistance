AN INTEGRATED HEURIST IC  SCHEMEFOR PART IAL  PARSE EVALUATIONAlon LavieSchool of Computer  ScienceCarnegie Mellon University5000 Forbes Ave., Pittsburgh, PA 15213email : lavie@cs.cmu.eduAbstractGLR* is a recently developed robust version of theGeneralized LR Parser \[Tomita, 1986\], that can parsealmost any input sentence by ignoring unrecognizableparts of the sentence.
On a given input sentence, theparser eturns a collection of parses that correspond tomaximal, or close to maximal, parsable subsets of theoriginal input.
This paper describes recent work on de-veloping an integrated heuristic scheme for selecting theparse that is deemed "best" from such a collection.
Wedescribe the heuristic measures used and their combi-nation scheme.
Preliminary results from experimentsconducted on parsing speech recognized spontaneousspeech are also reported.The  GLR*  ParserThe  GLR Pars ing  Algor i thmThe Generalized LR Parser, developed by Tomita\[Tomita, 1986\], extended the original Lit parsing al-gorithm to the case of non-LR languages, where theparsing tables contain entries with multiple parsing ac-tions.
Tomita's algorithm uses a Graph StructuredStack (GSS) in order to efficiently pursue in parallelthe different parsing options that arise as a result ofthe multiple ntries in the parsing tables.
A second atastructure uses pointers to keep track of all possible parsetrees throughout the parsing of the input, while sharingcommon subtrees of these different parses.
A process oflocal ambiguity packing allows the parser to pack sub-parses that are rooted in the same non-terminal into asingle structure that represents hem all.The GLR parser is the syntactic engine of the Univer-sal Parser Architecture developed at CMU \[Tomita etal., 1988\].
The architecture supports grammatical spec-ification in an LFG framework; that consists of context-free grammar ules augmented with feature bundlesthat are associated with the non-terminals of the rules.Feature structure computation is, for the most part,specified and implemented via unification operations.This allows the grammar to constrain the applicabilityof context-free rules.
The result of parsing an input sen-tence consists of both a parse tree and the computedfeature structure associated with the non-terminal tthe root of the tree.The GLR*  ParserGLR* is a recently developed robust version of the Gen-eralized LR Parser, that allows the skipping of unrecog-nizable parts of the input sentence \[Lavie and Tomita,1993\].
It is designed to enhance the parsability of do-mains such as spontaneous speech, where the input islikely to contain deviations from the grammar, due toeither extra-grammaticalities or limited grammar cov-erage.
In cases where the complete input sentence isnotcovered by the grammar, the parser attempts to find amaximal subset of the input that is parsable.
In manycases, such a parse can serve as a good approximationto the true parse of the sentence.The parser accommodates the skipping of words ofthe input string by allowing shift operations to be per-formed from inactive state nodes in the Graph Struc-tured Stack (GSS).
Shifting an input symbol from aninactive state is equivalent to skipping the words of theinput that were encountered after the parser reachedthe inactive state and prior to the current word thatis being shifted.
Since the parser is LR(0), previousreduce operations remain valid even when words fur-ther along in the input are skipped.
Information aboutskipped words is maintained in the symbol nodes thatrepresent parse sub-trees.To guarantee runtime feasibility, the GLR* parser iscoupled with a "beam" search heuristic, that dynami-cally restricts the skipping capability of the parser, so asto focus on parses of maximal and close to maximal sub-strings of the input.
The efficiency of the parser is alsoincreased by an enhanced process of local ambiguitypacking and pruning.
Locally ambiguous symbol nodesare compared in terms of the words skipped withinthem.
In cases where one phrase has more skippedwords than the other, the phrase with more skippedwords is discarded in favor of the more complete parsedphrase.
This operation significantly reduces the numberof parses being pursued by the parser.316The Parse  Eva luat ion  Heur i s t i csAt the end of the process of parsing a sentence, theGLR* parser returns with a set of possible parses, eachcorresponding to some grammatical subset of words ofthe input sentence.
Due to the beam search heuristicand the ambiguity packing scheme, this set of parsesis limited to maximal or close to maximal grammaticalsubsets.
The principle goal is then to find the maximalparsable subset of the input string (and its parse).
How-ever, in many cases there are several distinct maximalparses, each consisting of a different subset of words ofthe original sentence.
Furthermore, our experience hasshown that in many cases, ignoring an additional oneor two input words may result in a parse that is syn-tactically and/or semantically more coherent.
We havethus developed an evaluation heuristic that combinesseveral different measures, in order to select the parsethat is deemed overall "best".Our heuristic uses a set of features by which each ofthe parse candidates can be evaluated and compared.We use features of both the candidate parse and theignored parts of the original input sentence.
The fea-tures are designed to be general and, for the most part,grammar and domain independent.
For each parse, theheuristic computes a penalty score for each of the fea-tures.
The penalties of the different features are thencombined into a single score using a linear combination.The weights used in this scheme are adjustable, and canbe optimized for a particular domain and/or grammar.The parser then selects the parse ranked best (i.e.
theparse of lowest overall score).
1The  Parse  Eva luat ion  FeaturesSo far, we have experimented with the following set ofevaluation features:1.
The number and position of skipped words2.
The number of substituted words3.
The fragmentation of the parse analysis4.
The statistical score of the disambiguated parse treeThe penalty scheme for skipped words is designed toprefer parses that correspond to fewer skipped words.It assigns a penalty in the range of (0.95 - 1.05) foreach word of the original sentence that was skipped.The scheme is such that words that are skipped laterin the sentence receive the slightly higher penalty.
Thispreference was designed to handle the phenomena offalse starts, which is common in spontaneous speech.The GLR* parser has a capability for handling com-mon word substitutions when the parser's input stringis the output of a speech recognition system.
Whenthe input contains a pre-determined commonly substi-tuted word, the parser attempts to continue with both1The system can display the n best parses found, wherethe parameter n is controlled by the user at runtime.
Bydefault, we set n to one, and the parse with the lowest scoreis displayed.the original input word and a specified "correct" word.The number of substituted words is used as an eval-uation feature, so as to prefer an analysis with fewersubstituted words.The grammars we have been working with allow a sin-gle input sentence to be analyzed as several grammat-ical "sentences" or fragments.
Our experiments haveindicated that, in most cases, a less fragmented analy-sis is more desirable.
We therefore use the sum of thenumber of fragments in the analysis as an additionalfeature.We have recently augmented the parser with a statis-tical disambiguation module.
We use a framework simi-lar to the one proposed by Briscoe and Carroll \[Briscoeand Carroll, 1993\], in which the shift and reduce ac-tions of the LR parsing tables are directly augmentedwith probabilities.
Training of the probabilities i per-formed on a set of disambiguated parses.
The proba-bilities of the parse actions induce statistical scores onalternative parse trees, which are used for disambigua-tion.
However, additionally, we use the statistical scoreof the disambiguated parse as an additional evaluationfeature across parses.
The statistical score value is firstconverted into a confidence measure, such that more"common" parse trees receive a lower penalty score.This is done using the following formula:penalty = (0.1 * (-loglo(pscore)))The penalty scores of the features are then combinedby a linear combination.
The weights assigned to thefeatures determine the way they interact.
In our exper-iments so far, we have fined tuned these weights manu-ally, so as to try and optimize the results on a trainingset of data.
However, we plan on investigating the pos-sibility of using some known optimization techniquesfor this task.The Parse  Qua l i ty  Heur i s t i cThe uti l i~ of a parser such as GLR* obviously dependson the semantic oherency of the parse results that itreturns.
Since the parser is designed to succeed in pars-ing almost any input, parsing success by itself can nolonger provide a likely guarantee of such coherency.
Al-though we believe this task would ultimately be betterhandled by a domain dependent semantic analyzer thatwould follow the parser, we have attempted to partiallyhandle this problem using a simple filtering scheme.The filtering scheme's task is to classify the parsechosen as best by the parser into one of two categories:"good" or "bad".
Our heuristic takes into account boththe actual value of the parse's combined penalty scoreand a measure relative to the length of the input sen-tence.
Similar to the penalty score scheme, the precisethresholds are currently fine tuned to try and optimizethe classification results on a training set of data.317GLRGLR*/1)GLR* 2)Unparsablenumber percent58 48.3%5 4.2%5 4.2%Parsablenumber percent62 51.7%115 95.8%115 95.8%Good/CloseParsesnumber percent60 50.0%84 70.0%90 75.0%Table I: Performance Results of the GLR*  Parser(I) = simple heuristic, (2) = full heuristicsBadParsesnumber l~ercent2 1.7%31 25.8%25 20.8%Parsing of Spontaneous Speech Us ingGLR*We have recently conducted some new experiments otest the utility of the GLR* parser and our parse evalu-ation heuristics when parsing speech recognized sponta-neous peech in the ATIS domain.
We modified an ex-isting partial coverage syntactic grammar into a gram-mar for the ATIS domain, using a development set ofsome 300 sentences.
The resulting grammar has 458rules, which translate into a parsing table of almost700 states.A list of common appearing substitutions was con-structed from the development set.
The correct parsesof 250 grammatical sentences were used to train theparse table statistics that are used for disambiguationand parse evaluation.
After some experimentation, theevaluation feature weights were set in the following way.As previously described, the penalty for a skipped wordranges between 0.95 and 1.05, depending on the word'sposition in the sentence.
The penalty for a substitutedword was set to 0.9, so that substituting a word wouldbe preferable to skipping the word.
The fragmentationfeature was given a weight of 1.1, to prefer skipping aword if it reduces the fragmentation count by at leastone.
The three penalties are then summed, togetherwith the converted statistical score of the parse.We then used a set of 120 new sentences as a test set.Our goal was three-fold.
First, we wanted to comparethe parsing capability of the GLR* parser with thatof the original GLR parser.
Second, we wished to testthe effectiveness of our evaluation heuristics in select-ing the best parse.
Third, we wanted to evaluate theability of the parse quality heuristic to correctly classifyGLR* parses as "good" or "bad".
We ran the parserthree times on the test set.
The first run was withskipping disabled.
This is equivalent to running theoriginal GLR parser.
The second run was conductedwith skipping enabled and full heuristics.
The thirdrun was conducted with skipping enabled, and with asimple heuristic that prefers parses based only on thenumber of words skipped.
In all three runs, the sin-gle selected parse result for each sentence was manuallyevaluated to determine if the parser returned with a"correct" parse.The results of the experiment can be seen in Table 1.The results indicate that using the GLR* parser esultsin a significant improvement in performance.
Whenusing the full heuristics, the percentage of sentences,for which the parser returned a parse that matchedor almost matched the "correct" parse increased from50% to 75%.
As a result of its skipping capabilities,GLR* succeeds to parse 58 sentences (48%) that werenot parsable by the original GLR parser.
Fully 96%of the test sentences (all but 5) are parsable by GLR*.However, a significant portion of these sentences (23 outof the 58) return with bad parses, due to the skippingof essential words of the input.
We looked at the effec-tiveness of our parse quality heuristic in identifying suchbad parses.
The heuristic is successful in labeling 21 ofthe 25 bad parses as "bad".
67 of the 90 good/closeparses are labeled as "good" by the heuristic.
Thus,although somewhat overly harsh, the heuristic is quiteeffective in identifying bad parses.Our results indicate that our full integrated heuris-tic scheme for selecting the best parse out-performsthe simple heuristic, that considers only the number ofwords skipped.
With the simple heuristic, good/closeparses were returned in 24 out of the 53 sentences thatinvolved some degree of skipping.
With our integratedheuristic scheme, good/close parses were returned in30 sentences (6 additional sentences).
Further analy-sis showed that only 2 sentences had parses that werebetter than those selected by our integrated parse eval-uation heuristic.Re ferences\[Briscoe and Carroll, 1993\] T. Briscoe and J. Carroll.Generalized Probabilistic LR Parsing of Natural Lan-guage (Corpora) with Unification-Based Grammars.Computational Linguistics, 19(1):25-59, 1993.\[Lavie and Tomita, 1993\] A. Lavie and M. Tomita.GLR* - An Efficient Noise-skipping Parsing Algo-rithm for Context-free Grammars.
In Proceedings ofThird International Workshop on Parsing Technolo-gies, pages 123-134, 1993.\[Tomita et al, 1988\] M. Tomita, T. Mitamura,H.
Musha, and M. Kee.
The Generalized LRParser/Compiler- Version 8.1: User's Guide.
Tech-nical Report CMU-CMT-88-MEMO, 1988.\[Tomita, 1986\] M. Tomita.
Efficient Parsing for Nat.nral Language.
Kluwer Academic Publishers, Hing-ham, Ma., 1986.318
