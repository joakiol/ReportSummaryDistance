Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 115?122,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsA Hybrid Model for Grammatical Error CorrectionYang Xiang, Bo Yuan, Yaoyun Zhang*, Xiaolong Wang?,Wen Zheng, Chongqiang WeiIntelligent Computing Research Center, Key Laboratory of Network Oriented IntelligentComputation, Computer Science and technology Department,Harbin Institute of Technology Shenzhen Graduate School,Shenzhen, Guangdong, 518055, P.R.
China{windseedxy, yuanbo.hitsz, xiaoni5122, zhengwen379, weichongqiang}@gmail.comwangxl@insun.hit.edu.cn?AbstractThis paper presents a hybrid model for theCoNLL-2013 shared task which focuses on theproblem of grammatical error correction.
Thisyear?s task includes determiner, preposition,noun number, verb form, and subject-verbagreement errors which is more comprehen-sive than previous error correction tasks.
Wecorrect these five types of errors in differentmodules where either machine learning basedor rule-based methods are applied.
Pre-processing and post-processing procedures areemployed to keep idiomatic phrases from be-ing corrected.
We achieved precision of35.65%, recall of 16.56%, F1 of 22.61% in theofficial evaluation and precision of 41.75%,recall of 20.29%, F1 of 27.3% in the revisedversion.
Some further comparisons employingdifferent strategies are made in our experi-ments.1 IntroductionAutomatic Grammatical Error Correction (GEC)for non-native English language learners has at-tracted more and more attention with the devel-opment of natural language processing, machinelearning and big-data techniques.
?The CoNLL-2013 shared task focuses on the problem of GECin five different error types including determiner,preposition, noun number, verb form, and sub-ject-verb agreement which is more complicatedand challenging than previous correction tasks.Other than most previous works which concen-trate most on determiner and preposition errors,more error types introduces the possibility ofcorrecting multiple interacting errors such as de-?
Corresponding authorterminer vs. noun number and preposition vs.verb form.Generally, for GEC on annotated data such asthe NUCLE corpus (Dahlmeier et al 2013) inthis year?s shared task which contains both origi-nal errors and human annotations, there are twomain types of approaches.
One of them is theemployment of external language materials.
Alt-hough there are minor differences on strategies,the main idea of this approach is to use frequen-cies as a filter, such as n-gram counts, and takethose phrases that have relatively high frequen-cies as the correct ones.
Typical works are shownin (Yi et al 2008) and (Bergsma et al 2009).Similar methods also exist in HOO shared tasks1such as the web 1TB n-gram features used byDahlmeier and Ng (2012a) and the large-scale n-gram model described by Heilman et al(2012).The other type is machine learning based ap-proach which considers most on local contextincluding syntactic and semantic features.
Han etal.
(2006) take maximum entropy as their classi-fier and apply some simple parameter tuningmethods.
Felice and Pulman (2008) present theirclassifier-based models together with a few rep-resentative features.
Seo et al(2012) invite ameta-learning approach and show its effective-ness.
Dahlmeier and Ng (2011) introduce an al-ternating structure optimization based approach.Most of the works mentioned above focus ondeterminer and preposition errors.
Besides, Leeand Seneff (2008) propose a method to correctverb form errors through combining the featuresof parse trees and n-gram counts.
To ourknowledge, no one focused on noun form errorsin specific researches.In this paper, we propose a hybrid model tosolve the problem of GEC for five error types.1 http://clt.mq.edu.au/research/projects/hoo/hoo2012115Machine learning based methods are applied tosolve determiner (ArtOrDet), preposition (Prep)and noun form (Nn) problems while rule-basedmethods are proposed for subject-verb agreement(SVA) and verb form (Vform) problems.
Wetreat corrections of errors in each type as indi-vidual sub problems the results of which arecombined through a result combination module.Solutions on interacting error corrections wereconsidered originally but dropped at last becauseof the bad effects brought about by them such asthe accumulation of errors which lead to a verylow performance.
We perform feature selectionand confidence tuning in machine learning basedmodules which contribute a lot to our perfor-mance.
Also, pre-processing and post-processingprocedures are employed to keep idiomaticphrases from being corrected.Through experiments, we found that the resultof the system was affected by many factors suchas the selection of training samples and features,and the settings of confidence parameters in clas-sifiers.
Some of the factors make the whole sys-tem too sensitive that it can easily be trapped intoa local optimum.
Some comparisons are shownin our experiments section.No other external language materials are in-cluded in our model except for several NLP toolswhich will be introduced in ?5.2.
We achievedprecision of 35.65%, recall of 16.56% and F1 of22.61% in the official score of our submitted re-sult.
However, it was far from satisfactory main-ly due to the ill settings of confidence parameters.Trying to find out a set of optimal confidenceparameters, our model is able to reach an upperbound of precision of 34.23%, recall of 25.56%and F1 of 29.27% on the official test set.
For therevised version, we achieved precision of41.75%, recall of 20.29%, and F1 of 27.3%.The remainder of this paper is arranged as fol-lows.
The next section introduces our systemarchitecture.
Section 3 describes machine learn-ing based modules.
Section 4 shows rule basedmodules.
Experiments and analysis are arrangedin Section 5.
Finally, we give our discussion andconclusion in Section 6 and 7.2 System ArchitectureInitially, we treat errors of each type as individu-al sub problems.
Machine learning based meth-ods are applied to solve ArtOrDet, Prep and Nnproblems where similar problem solving stepsare shared: sample generation, feature extraction,training, confidence tuning in development data,and testing.
We apply some hand-crafted heuris-tic rules in solving subject-verb agreement (SVA)and verb form (Vform) problems.
Finally, resultsfrom different modules are combined together.The whole architecture of this GEC system isdescribed in Figure 1.A pre-processing and a post-processing filterare utilized which include filters for some idio-matic phrases extracted from the training dataset.The Frequent Pattern Growth Algorithm (FP-Growth) is widely used for frequent pattern min-ing in machine learning.
In pre-processing, wefirstly apply FP-Growth to gather the frequentitems in the training set.
Through some manualrefinements, a few idiomatic phrases are re-moved from the candidate set to be corrected.
Inpost-processing, the idiomatic phrase list is usedto check whether a certain collocation is stillgrammatical after several corrections are per-formed.
There are 996 idiomatic phrases in ourlist which is composed by mainly patterns fromthe training set and a series of hand-crafted ones.Typical phrases we extracted are in general,have/need to be done, on the other hand, alarge/big number/amount of, at the same time, inpublic, etc.Figure 1.
Architecture of our GEC system.3 Machine Learning Based ModulesFor the error types ArtOrDet, Prep and Nn, wechoose machine learning based methods becausewe consider there is not enough evidence to di-rectly determine which word or form to be used.Moreover, it is impossible to transfer all the cas-es we encounter into rules.
In this section, wedescribe our processing ideas for each error typerespectively and then specifically introduce ourfeature selection and confidence tuning approach.3.1 DeterminersDeterminers in the error type ?ArtOrDet?
containarticles a/an, the and other determiners such asOriginal textsPre-processingMachine learningbased modulesRule based mod-ulesPost-processingCorrected texts Result combination116this, those, etc.
This type of error accounts for alarge proportion which is of great impact on thefinal result.
We consider only articles since theother determiners are rarely used and the usagesof them are sometimes ambiguous.
Like ap-proaches described in some previous works(Dahlmeier and Ng, 2012a; Felice and Pulman,2008), we assign three types a/an, the and emptyfor each article position and build a multi-classclassifier.For training, developing and testing, all nounphrases (NPs) are chosen as candidate samples tobe corrected.
For NPs whose articles have beenannotated in the corpus, the correct ones are theirtarget categories, and for those haven?t been an-notated, the target categories are their observedarticle types.
Samples we make use of can bedivided into two basic types in each category:with and without a wrong article.
Two examplesare shown below:with: a/empty big apples ~ empty categorywithout: the United States ~ the categoryFor each category in a, the, and empty, we usethe whole with data and take samples of withoutones from the set of correct NPs to make uptraining instances of one category.
The reasonwhy we make samples of the without ones is forthe consideration that the classifier would alwayspredicts the observed article and never proposesany corrections if given too many without sam-ples, the case of which is mentioned in (Dahl-meier and Ng, 2012a).
However, we found thatthe ratio of with-without shows little effect in ourmodel.
The article a is regulated to a or an ac-cording to pronunciation.Syntactic and semantic features are consideredin feature extraction with the help of WordNetand the ?.conll?
file provided.
We adopt syntac-tic features such as the surface word, phrase,part-of-speech, n-grams, constituent parse tree,dependency parse tree and headword of an NP;semantic features like noun category and hyper-nym.
Some expand operations are also donebased on them (reference to Dahlmeier and Ng,2012a; Felice and Pulman, 2008).
After featureextraction, we apply a genetic algorithm to dofeature subset selection in order to reduce dimen-sionality and filter out noisy features which is tobe described in ?3.4.Maximum Entropy (ME) has been proven tobehave well for heterogeneous features in naturallanguage processing tasks and we adopt it totrain our model.
We have also tried several otherclassifiers including SVM, decision tree, Na?veBayes, and RankSVM but finally find ME per-forms well and stably.
It provides confidencescores for each category which we will make useof downstream.3.2 PrepositionsPreposition error correction task is similar to theprevious one except the different categories andcorresponding features.
Since there are 36 com-mon prepositions listed by the shared task, origi-nally, we assign 37 types including 36 preposi-tions and empty for each preposition position andbuild a multi-class classifier.
For training, devel-oping and testing, each preposition as well as theempty position directly after a verb is consideredas a candidate.
Syntactic and semantic featuresextracted are similar to those in article error cor-rection except for some specific cases for prepo-sitions such as the verbs related to prepositionsand the dependency relations.
Similarly, we treatthose preposition phrases with and without a cer-tain preposition as the two types of samples intraining (as described in ?3.1).
Two examples arelisted below:with: on/in the 1860s~ in categorywithout: have to be done ~ to categoryThrough statistics on the training data, wefound that most prepositions have very few sam-ples which may not contribute to the perfor-mance at all and even bring about noise whenassigned to wrong categories.
After severalrounds of experiments, we finally adopt a classi-fier with seven prepositions which are frequentlyused in the whole corpus.
They are on, of, in, at,to, with and for.
As to the classifier, ME alsooutperforms the others.3.3 Noun FormNoun form may be interacting with determinersand verbs which may also have errors in the orig-inal text.
So errors may occur in the context fea-tures extracted from the original text.
However,if we use the context features that have been cor-rected, more errors would be employed due tothe low performance of the previous steps.Through statistics, we found that co-occurrencebetween two types of errors such as SVA andArtOrDet only accounts for a small proportion.After a few experiments, we decided to give upinteracting errors so as to avoid accumulated er-rors.This is a binary classification problem.
Allhead nouns in NPs are considered as candidates.Each category contains with and without samplessimilar to the cases in ?3.1 and ?3.2.
Features arehighly related to the deterministic factors for the117head noun form such as the countability, Word-Net type, name entity and whether there somespecific dependency relations including det,amod etc.ME also outperforms other classifiers.3.4 Feature Selection Using Genetic Algo-rithmFeatures we extracted are excessive and sparseafter binarization.
They bring noise in quality aswell as complexity in computation and need tobe selected a priori.
In our work, it is a wrapperfeature selection task.
That is, we have to select acombination of features that perform well to-gether rather than make sure each of them be-haves well.
This GEC task is interesting in fea-ture selection because word surface features thatare observed only once are also effective whilewe think that they overfit.
Genetic algorithm(GA) has been proven to be useful in selectingwrapper features in classification (ElAlami, 2009;Anba-rasi et al2010).
We used GA to select fea-tures as well as reduce feature dimensionality.We convert the features into a binary sequencein which each character represents one dimen-sion.
Let ?1?
indicates that we keep this dimen-sion while ?0?
means that we drop it, we use abinary sequence such as ?0111000?100?
to de-note a combination of feature dimensions.
GAfunctions on the feature sequences and finallydecides which features should be kept.
The fit-ness function we used is the evaluation measureF1 described in ?5.3.3.5 Confidence TuningThe Maximum Entropy classifier returns a confi-dence score for each category given a testingsample.
However, for different samples, the dis-tribution of predicted scores varies a lot.
Forsome samples, the classifier may have a veryhigh predicted score for a certain category whichmeans the classifier is confident enough to per-form this prediction.
But for some other samples,two or more categories may share close scores,the case of which means the classifier hesitateswhen telling them apart.We introduce a confidence tuning approach onthe predicted results through a comparison be-tween the observed category and the predictedcategory which is similar to the ?thresholding?approach described in Tetreault and Chodorow(2008).
The main idea of the confidence tuningalgorithm is: the choice between keep and drop isbased on the difference between the confidencescores of the predicted category and the observedcategory.
If this difference goes beyond a thresh-old t, the prediction is kept while if it is under t,we won?t do any corrections.
We believe thistuning strategy is especially appropriate in thistask since to distinguish whether the observedcategory is correct or not affects a lot to the pre-dicted result.The confidence threshold for each category isgenerated through a hill climbing algorithm inthe development data aimed at maximizing F1-meaure of the result.4 Rule-based ModulesA few hand-crafted rules are applied to solve theverb related corrections including SVA andVform.
In these cases, the verb form is only re-lated to some specific features as described byLee and Seneff (2008).4.1 SVASVA (Subject-verb-agreement) is particularlyrelated to the noun subject that a verb determines.In the dependency tree, the number of the nounwhich has a relation nsubj with the verb deter-mines the form of this verb.
Through observation,we find that the verbs to be considered in SVAcontain only bes (including am, is, are, was,were) and the verbs in simple present tensewhose POSs are labeled with VBZ (singular) orVBP(plural).To pick out the noun subject is easy except forthe verb that contained in a subordinate clause.We use semantic role labeling (SRL) to helpsolve this problem in which the coordinated canbe extracted through a trace with the label ?R-Argument?.
The following Figure is an examplegenerated by the SRL toolkit mate-tools (BerndBohnet, 2010)2.Figure 2.
SRL for the demo sentence ?Jack, whowill show me the way, is very tall.?
The subject ofthe verb show can be traced through R-A0 -> A0.However, the performance of this part is partlycorrelated with the noun form that may have er-rors in the original text and the wrong SRL resultbrought about because of wrong sentence gram-mars.2 http://code.google.com/p/mate-tools/1184.2 Verb FormThe cases are more complicated in the verb formerror correction task.
Modal, aspect and voice areall forms that should be considered for a verb.And sometimes, two or more forms are com-bined together to perform its role in a sentence.For example, in the sentence:He has been working in this position for along time.The bolded verb has been working is a com-bination of the active voice work, the progressiveaspect be+VBG and the perfect aspect has+VBN.It is a bit difficult for us to take all cases intoconsideration, so we just apply several simplerules and solve a subset of problems for this type.Some typical rules are listed below:1.
The verb that has a dependency relation auxto preposition to is modified to its base form.2.
The verb that has a dependency relationpcomp to preposition by is modified to its pastform.3.
The verb related to other prepositions (ex-cept to and by) is modified to ~ing form.4.
The verb depends on auxiliary do and mod-al verb (including its inflections and negativeform) is modified to its base form.We have also tried to use SRL and transitivityof a verb to determine the active and passivevoice but it didn?t work well.5 Experiments and Analysis5.1 Data DescriptionThe NUCLE corpus introduced by NUS (Nation-al University of Singapore) contains 1414 essayswritten by L2 students with relatively high profi-ciency of English in which grammatical errorshave been well annotated by native tutors.
It hasa small proportion of annotated errors which ismuch lower than other similar corpora (Dahl-meier et al 2013).
In our experiments, we dividethe whole corpus into 80%, 10% and 10% fortraining, developing and testing.
And we use 90%and 10% for training and developing for the finaltest.5.2 External tools and corporaExternal tools we used include WordNet (Fell-baum, 1998) for word base form and noun cate-gory generation, Morphg (Minnen et al 2000)3to generate inflections of nouns and verbs, mate-tools (Bohnet, 2010) for SRL, Stanford-ner3 http://www.informatics.sussex.ac.uk/research/groups/nlp/carroll/morph.html(Finkel et al 2005)4 for name entity extractionand Longman online dictionary5  for generationof noun countability and verb transitivity.We didn?t employ any external corpora in oursystem.5.3 ExperimentsThe performance of each machine learning mod-ule is affected by the selection of training sam-ples, features and confidence tuning for the max-imum entropy classifier.
All these factors con-tribute more or less to the final performance andneed to be carefully developed.
In our experi-ments, we focus on machine learning basedmodules and make comparisons on sample selec-tion, confidence tuning and feature selection andlist a series of results before and after applyingour strategies.In our experiment, the performance is meas-ured with precision, recall and F1-measure where12 precision recallF precision recall?
??
?Precision is the amount of predicted correc-tions that are also corrected by the manual anno-tators divided by the whole amount of predictedcorrections.
Recall has the same numerator asprecision while its denominator is the amount ofmanually corrected errors.
They are in accord-ance with those measurements generated by theofficial m2scorer (Dahlmeier and Ng, 2012c) to agreat extent and easily to be integrated in ourprogram.As we have mentioned in Section 3, we don?temploy all samples but make use of all with(with errors and annotations) instances and sam-ple the without ones (without errors) for training.And the sampling for without type is totally ran-dom without loss of generality.
We apply thesame strategy in all of these three error types(ArtOrDet, Prep and Nn) and try several ratios ofwith-without to find out whether this ratio hasgreat impact on the final result and which ratioperforms best.
We use the 80%-10%-10% data(mentioned in ?5.1) for our experiments andmake comparisons of different ratios on develop-ing data.
The experimental results are describedin detail in Figure 3.Confidence tuning is applied in all these threeerror types which contributes most to the finalperformance in our model.
We compare the re-sults before and after tuning in all sample ratios4 http://www-nlp.stanford.edu/software/CRF-NER.shtml5 http://www.ldoceonline.com/119that we designed and they are also depicted inFigure 3.Sample with:without1:1 1:2 1:3 1:6 1:8 1:10 1:allPRF0.0.2.4.6.8precision before and after tuningrecall before and after tuningF1 before and after tuningFigure 3-1.
Comparisons before and after tuningin ArtOrDet.
1:all means to use the whole withoutsamples.Sample with:without1:1 1:2 1:3 1:6 1:8 1:10 1:allPRF0.0.1.2.3.4.5.6presision before and after tuningrecall before and after tuningF1 before and after tuningFigure 3-2.
Comparisons before and after tuning inPrep.1:1 1:2 1:3 1:6 1:8 1:10 1:allPRF0.0.2.4.6.81.0precision before and after tuningrecall before and after tuningF1before and after tuningSample with:withoutFigure 3-3.
Comparisons before and after tuningin Nn.From the three groups of data in Figure 3, wenotice that the ratio of samples has little impacton F1.
This phenomenon shows that our conclu-sion goes against the previous work by Dahl-meier and Ng (2012a).
We believe it is mainlydue to our confidence tuning which makes theparameters vary much under different sampleratios, that is, if given the same parameters, theeffect of sample ratio selection may become ob-vious.
Unfortunately, we didn?t do such a sys-tematic comparison in our work.
The improve-ment under confidence tuning can be seen clearlyin all ratios of with-without samples.
The confi-dence tuning algorithm employed in our work isbetter than the traditional tuning methods thatassign a fixed threshold for each category or forall categories (about 1%~2% better measured byF1).However, although we are able to pick out thetraining data with a high F1 through confidencetuning for the developing data, it is difficult forus to choose a set of confidence parameters thatalso fits the test data well.
Given several closeF1s, the numerical values of denominators andnumerators which determine the precision andrecall can vary a lot.
For example, one set thathas a high precision and low recall may share thesimilar F1 with another set that has a low preci-sion and high recall.
Our work lacked of the de-velopment on how to control the number of pro-posed errors to make leverage on the perfor-mance between developing set and testing set.
Itresulted in that the developing set and the testingset were not balanced at all, and our model wasnot able to keep the sample distribution as thetraining set.
This is the main factor that leads to alow performance in our submitted result whichcan be clearly seen in Table 1.
The upper boundperformance of our system achieves precision of34.23%, recall of 25.56% and F1 of 29.27%, inwhich the F1 goes 7% beyond our submitted sys-tem.
We notice that results of all metrics of thethree error types where machine learning algo-rithms are applied improve with the simultaneousincrease of numerators and denominators.
This isespecially noticeable in Prep.For the other two types SVA and Vform, wejust apply several heuristic rules to solve a subsetof problems and the case of Vform has not beensolved well such as tense and voice.Genetic Algorithm (GA) is applied to processfeature reduction and subset selection.
This isdone in ArtOrDet type in which we extract asmany as 350,000 binary features.
For error typePrep and Nn, the feature dimensionalities weconstructed were not as high as that in ArtOrDet,and the improvements under GA were not obvi-ous which we would not discuss in this work.Through experiments on a few sample ratios, wenotice that feature selection using genetic algo-rithm is able to reduce the feature dimensionalityto about 170,000 which greatly lowers down the120downstream computational complexity.
However,the improvement contributed by GA after confi-dence tuning is not obvious as that before confi-dence tuning.
We think it is partly because of thebad initialization of GA which is to be improvedin our future work.
The unfixed parameters mayalso lead to such a result which we didn?t discussenough in our work.
The comparison before andafter GA is described in Figure 4.Our submission% Upper bound%P(Det) 41.38(168/406) 36.44(254/697)R(Det) 24.35(168/690) 36.81(254/690)F1(Det) 30.66 36.63P(Prep) 13.79(4/29) 26.12(35/134)R(Prep) 1.29(4/311) 11.25(35/311)F1(Prep) 2.35 15.73P(Nn) 24.81(65/262) 27.27(102/374)R(Nn) 16.41(65/396) 25.76(102/396)F1(Nn) 19.76 26.49P(SVA)R(SVA)F1(SVA)24.42(21/86)16.94(21/124)20.0024.42(21/86)16.94(21/124)20.00P(Vform)R(Vform)F1(Vform)19.35(6/31)4.92(6/122)7.8419.35(6/31)4.92(6/122)7.84P(all) 35.65(272/763) 34.23(420/1227)R(all) 16.56(272/1643) 25.56(420/1643)F1(all) 22.61 29.27Table 1.
Different performances according to dif-ferent confidence parameters.
Det stands for Ar-tOrDet.Pre-processing and post-processing we pro-pose also contribute to some extent which wecould see from Table 2.
Some idiomatic phrasesare excluded from being corrected in pre-processing which enhances precision while someare being modified in post-processing to improverecall.Without pre-processingand post-processing%Final%PRF133.72(265/768)16.13(265/1643)21.8235.65(272/763)16.56(272/1643)22.61Table 2.
Comparison with and without pre-processing and post-processing.We didn?t do much on the interacting errorsproblem since we didn?t work out perfect plansto solve it.
So, in the result combination module,we just simply combine the result of each parttogether.Sample positive:negative1:1 1:2 1:3 1:6F10.00.05.10.15.20.25.30MEME+GAME+TuningME+GA+TuningFigure 4.
Comparisons before and after Genet-ic Algorithm on ArtOrDet error type.
ME, GA,and Tuning stand for Maximum Entropy, Ge-netic Algorithm and confidence tuning.In the revised version, under further correc-tions for the gold annotations, our modelachieves precision of 41.75%, recall of 20.19%and F1 of 27.3%.6  DiscussionWhich factor contributes most to the final resultin the problem of grammatical error correction?Since we didn?t include any external corpora, wediscuss it here only according to the local classi-fiers and context features.Based on our experiments, we find that, in ourmachine learning based modules, a tiny modifi-cation of confidence parameter setting for eachcategory, no matter which type of error, can havegreat impact on the final result.
It results in thatour model is much too sensitive to parameterswhich may easily lead to a poor behavior.
Per-haps a sufficient consideration of how to keepthe distribution of samples, such as cross-validation, may be helpful.
In addition, the selec-tion of classifiers, features and training samplesall have effect on the result more or less, but notas obvious as that of the confidence thresholdsetting.7 ConclusionIn this paper, we propose a hybrid modelcombining machine learning based modules andrule-based modules to solve the grammatical er-ror correction task.
We are able to solve a subsetof the correction problems in which ArtOrDetand Nn perform better.
However, our result inthe testing data shows that our model is sensitive121to parameters.
How to keep the distribution oftraining samples needs to be further developed.AcknowledgementThis work is supported in part by the NationalNatural Science Foundation of China (No.
612-72383 and 61173075).ReferencesBernd Bohnet.
Top Accuracy and Fast Depend-ency Parsing is not a Contradiction.
In Pro-ceedings of COLING, 2010.C.
Fellbaum.
WordNet: An Electronic LexicalData-base.
MIT Press.
1998.Daniel Dahlmeier, and Hwee Tou Ng.
Grammat-ical error correction with alternating structureoptimization.
In Proceedings of ACL.
Associa-tion for Computational Linguistics, 2011.Daniel Dahlmeier, Hwee Tou Ng, and Eric JunFeng Ng.
NUS at the HOO 2012 Shared Task.In Proceedings of the Seventh Workshop onBuilding Educational Applications Using NLP.Association for Computational Linguistics,2012a.Daniel Dahlmeier and Hwee Tou Ng.
A beam-search decoder for grammatical error correc-tion.
In Proceedings of the EMNLP.
Associa-tion for Computational Linguistics, 2012b.Daniel Dahlmeier and Hwee Tou Ng.
BetterEvaluation for Grammatical Error Correction.In Proceedings of NAACL, Association forComputational Linguistics, 2012c.Daniel Dahlmeier, Hwee Tou Ng and Siew MeiWu.
Building a Large Annotated Corpus ofLearner English: The NUS Corpus of LearnerEnglish.
In Proceedings of the 8th Workshopon Innovative Use of NLP for Building Educa-tional Applications (BEA), 2013.De Felice, Rachele and Stephen G. Pulman.
Aclassifier-based approach to preposition anddeterminer error correction in L2 English.
InProceedings of COLING.
Association forComputational Linguistics, 2008.G.
Minnen, J. Carroll and D. Pearce.
Robust,applied morphological generation.
In Proceed-ings of the 1st International Natural LanguageGeneration Conference, 2000.Jenny Rose Finkel, Trond Grenager, and Chris-topher Manning.
Incorporating Non-local In-formation into Information Extraction Systemsby Gibbs Sampling.
In Proceedings of ACL ,2005.Joel R. Tetreault and Martin Chodorow.
The upsand downs of preposition error detection inESL writing.
In Proceedings of COLING, As-sociation for Computational Linguistics, 2008.John Lee and Stephanie Seneff.
Correcting mis-use of verb forms.
In Proceedings of ACL:HLT, 2008.M Anbarasi, E Anupriya, and NC Iyengar.
En-hanced prediction of heart disease with featuresubset selection using genetic algorithm.
In-ternational Journal of Engineering Scienceand Technology,Vol.2(10),2010: 5370-5376.ME ElAlami.
A filter model for feature subsetselection based on genetic algorithm.Knowledge-Based Systems,Vol.22(5), 2009:356-362.Michael Heilman, Aoife Cahill, and JoelTetreault.
Precision isn't everything: a hybridapproach to grammatical error detection.
InProceedings of the Seventh Workshop onBuilding Educational Applications Using NLP.Association for Computational Linguistics,2012.Hongsuck Seo et alA meta learning approach togrammatical error correction.
In Proceedingsof ACL.
Association for Computational Lin-guistics, 2012.N.R.
Han, M. Chodorow, and C. Leacock.
2006.Detecting errors in English article usage bynon-native speakers.
Natural Language Engi-neering, Vol.12(02):115-129.S.
Bergsma, D. Lin, and R. Goebel.
2009.
Web-scale ngram models for lexical disambiguation.In Proceedings of IJCAI.2009.X.
Yi, J. Gao, and W.B.
Dolan.
2008.
A web-based English proofing system for English as asecond language users.
In Proceedings ofIJCNLP.2008.122
