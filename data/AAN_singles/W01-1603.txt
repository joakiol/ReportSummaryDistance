Development of a machine learnable discourse tagging toolMasahiro Araki?, Yukihiko Kimura?, Takuya Nishimoto?, and Yasuhisa Niimi?
?Department of Electronics and Information ScienceKyoto Institute of TechnologyMatsugasaki Sakyo-ku Kyoto 606-8585, Japan{araki,kimu,nishi,niimi}@dj.kit.ac.jpAbstractWe have developed a discourse leveltagging tool for spoken dialogue cor-pus using machine learning meth-ods.
As discourse level informa-tion, we focused on dialogue act, rel-evance and discourse segment.
Indialogue act tagging, we have im-plemented a transformation-basedlearning procedure and resulted in70% accuracy in open test.
Inrelevance and discourse segmenttagging, we have implemented adecision-tree based learning proce-dure and resulted in about 75% and72% accuracy respectively.1 IntroductionIn dialogue research communities, the need ofdialogue corpora with various level of anno-tation is recognized.
However, creating an-notated dialogue corpora needs considerablecost in recording, transcribing, annotating,and checking the consistency and reliabilityof the annotated data.Considering such situation, we focused onannotation step and developed discourse leveltagging tool for spoken dialogue corpus usingmachine learning methods.
In this paper, weexplain the detail of tagging scheme and de-scribe machine learning algorithm suitable foreach level of tagging.2 Multiple level tagging schemefor Japanese dialogueIt is widely recognized that making annotatedspoken dialogue corpora is labor-intensive ef-fort.
To this end, the Discourse ResearchInitiative (DRI) was set up in March of1996 by US, European, and Japanese re-searchers to develop standard discourse anno-tation schemes (Carletta et al, 1997; Core etal., 1998).
In line with the effort of this initia-tive, Japanese Discourse Research Initiativehas started and created annotation scheme forvarious level of information of dialogue, thatis JDTAG (Japanese Dialogue TAG) (JDRI,2000).
Our aim is to develop tagging tools inline with the JDTAG.In the following of this section, we explainthe part of tagging scheme which are relevantto our tools.2.1 Dialogue actIn JDTAG, slash unit is defined followingMeteer and Taylor (Meteer and Taylor, 1995).Dialogue act tagging scheme is a set of rules toidentify the function of each slash unit fromthe viewpoint of speech act theory (Searle,1969) and discourse analysis (Coulthhard,1992; Stenstrom, 1994).
These dialogue acttag reflect a local structure of the dialogue.To improve the agreement score among theannotators, we assume basic structure of dia-logue shown in Figure 1.Typical exchange pattern is shown in Fig-ure 2.In this scheme, the tags (Figure 3) need tobe an element of exchange structure exceptfor those of dialogue management.2.2 RelevanceDialogue act tag can be regarded as a functionof utterance.
Therefore, we can see the se-quence of dialogue act tag as the flat structureof the dialogue.
It is insufficient to express?
Task-orientedDialogue ?
(Opening) ProblemSolving (Closing)?
ProblemSolving ?
Exchange+?
Exchange ?
Initiate (Response)/Initiate* (Response)* (FollowUp) (FollowUp)?*?
:repeat more than 0 time!$?+?
:repeat more than 1 time, ( ): the element can be omitted.Figure 1: Exchange structure-------------------------------------------(I) 0041 A: chikatetsu no ekimei ha?
(What?s the name of the subway station?
)(R) 0042 B: chikatetsy no teramachieki ni narimasu.
(The subway station is Teramachi.
)(F) 0043 A: hai.
(I see.)
-------------------------------------------I: Initiate, R: Response, F:Follow-upFigure 2: Typical exchange pattern?
Dialogue managementOpen, Close?
InitiateRequest, Suggest, Persuade, Propose,Confirm, Yes-no question, Wh-question,Promise, Demand, Inform,Other assert, Other initiate.?
ResponsePositive, Negative, Answer, Other response.?
Follow upUnderstand?
Response with InitiateThe element of this category is represented asResponse Type / Initiate Type.Figure 3: Tag set of dialogue acttree-like structure, such as embedded subdia-logue.
In order to represent such higher levelinformation, we use a relevance tag.There are two types of relevance betweenslash units.
The one is the relevance of theinside of exchange.
The other is the relevanceof between neighboring exchanges.
We callthe former one as relevance type 1, and thelatter one as relevance type 2.Relevance type 1 represents the relation ofinitiate utterance and its response utteranceby showing the ID number of the initiate ut-terance at the response utterance.
By us-ing this tag, the initiate-response pair whichstrides over embedded subdialogue can begrasped.Relevance type 2 represents the meso-structure of the dialogue such as chaining,coupling, elliptical coupling as introduced in(Stenstrom, 1994).
Chaining is a pattern of[A:I B:R] [A:I B:R] (speaker A initiates theexchange and speaker B responds it).
Cou-pling is a pattern of [A:I B:R] [B:I A:R].Elliptical coupling is a pattern of [A:I] [B:IA:R] which omits the response in the first ex-change.
Relevance type 2 tag is attached tothe each initiate response in showing whethersuch meso-level dialogue structure can be ob-served (yes) or not (no).The follow-up utterance has no relevancetag.
It is because follow-up necessarily has arelevance to the preceded response utterance.The example of dialogue act tagging (firstelement of tag) and relevance tagging (secondelement) is shown Figure 4.----------------------------------------[<Yes-no question> <relevance no>]0027 A: hatsuka no jyuuji karaha aite irun de syou ka(Is it available from 10 at 20th?
)[<Yes-no question> <relevance yes>]0028 B: kousyuu shitsu desu ka?
(Are you mentioning the seminar room?
)[<Positive> <0028>]0029 A: hai(Yes.
)[<Negative> <0027>]0030 B: hatsuka ha aite orimasen(It is not available in 20th.
)[<Understand>]0031 A: soudesu ka(OK.)----------------------------------------Figure 4: An example dialogue with the dia-logue act and relevance tags----------------------------------------[2: room for a lecture: ]38 A: {F e} heya wa dou simashou ka?
(How about meeting room?
)[1: small-sized meeting room: clarification]39 B: heya wa shou-kaigishitsu wa aite masu ka?
(Can I use the small-sized meeting room?
)40 A: {F to} kayoubi no {F e} 14 ji han karawa {F e} shou-kaigisitsu wa aite imasen(The small meeting room is not availablefrom 14:30 on Tuesday.
)[1:the large-sized meeting room: ]41 A: dai-kaigishitsu ga tukae masu(You can use the large meeting room.
)[1: room for a lecture: return]42 B: {D soreja} dai-kaigishitsu de onegaishimasu(Ok.
Please book the large meeting room.)
----------------------------------------[TBI:topic name:segment relation]Figure 5: An example dialogue with the dia-logue segment tags2.3 Dialogue segmentDialogue segment of JDTAG indicates bound-ary of discourse segment introduced in (Groszand Sidner, 1986).
A dialogue segment isidentified based on the exchange structure ex-plained above.
A dialogue segment tag is firstinserted before each initiating utterance.
Af-ter that, a topic break index, a topic name,and a segment relation are identified.Topic break index (TBI) takes the value of1 or 2: the boundary with TBI=2 is less con-tinuous than the one with TBI=1 with regardto the topic.
The topic name is labeled by an-notators?
subjective judgment for the topicsof that segment.
The segment relation indi-cates the one between the preceding and thefollowing segments, which is classified as clar-ification, interruption, and return.Figure 5 shows an example dialogue withthe dialogue segment tags.3 Dialogue act taggerConsidering the limitation of amount of cor-pus with dialogue level annotations, a promis-ing dialogue act tagger is based on ma-chine learning method with limited amount oftraining data rather than statistical method,which needs large amount of training data.Rule-based and example-based learning algo-rithms are suitable to this purpose.
In thissection, we compare our implementation oftransformation-based rule learning algorithmand example-based tagging algorithm.3.1 Transformation-based learningTransformation-based learning is a simplerule-based learning algorithm.
Figure 6 illus-trates the learning process.???????????????????????????????????????
???????????????????
????????????
??????
????????
??????
????????
???????
?
??????
????????
? ?
?????????????????????
??????????????
???????????????????
????????
????????
??????????
????
?????????????
??
???
???????
??????????
?????????????
???????
?Figure 6: Learning procedure of dialogue acttagging rule by TBLFirst, initial tagged data was made fromunannotated corpus by using bootstrappingmethod.
In our implementation, we use de-fault rule which assigns the most frequenttag to all the utterance as a bootstrappingmethod.
All the possible rules are constructedfrom annotated corpus by combining condi-tional parts and their consequence.
All thepossible rule are applied to the data andthe rule whose transformation results in thegreatest improvement is selected.
This rule isadded to the current rule set and this itera-tion is continued until no improvement is ob-served.
In the previous research, TBL showedsuccessful performance in many annotationtask, e.g.
(Brill, 1995), (Samuel et al, 1998).In our experiment, the selected features inthe conditional part of the rule are words(the notation in the rule is include), sentencelength (length) and previous dialogue act tag(prev).
Although each feature is not enoughto use as a clue in determining dialogue act,the combination of these features works well.We used four types of combinations, that is,include + include, include + length, include+ prev and length + prev.The result of the learning process is a se-quence of rules.
For example, in dialogue acttagging, acquired rules in scheduling domainare shown in Figure 7.#1 condition: default,new_tag: wh-question#2 condition: include="yoroshii(good)"& include="ka(*1)",new_tag: yes-no-question#3 condition: include="hai(yes)"& prev=yes-no-question,new_tag: affirmative#4 condition: include="understand"& length < 4,new_tag: follow-up...(*1 "ka" is a functional wordfor interrogative sentence)Figure 7: Acquired dialogue act tagging rules3.2 Example-based learningExample-based learning is suitable for classi-fication task.
It stores up example of inputand corresponding class, calculates the dis-tance between these examples and new input,and classifies it to the nearest class.In our dialogue act tagging, the exampleis consists of word sequence (partitioned byslash unit tag) and part of speech information.Corresponding dialogue act tag is attached toall the example.The distance between example and new in-put is calculated using the weighted agree-ment of elements shown in Table 1.Table 1: Elements for calculating a distance.element weightdialogue act of before two sentence 1dialogue act of previous sentence 3postpositional word of end of sentence 3clue word for dialogue act 3another word 23.3 Experimental resultsWe have compared above two dialogue acttagging algorithms in two different tasks: aroute direction task and a car trouble shoot-ing task.
We used 4 dialogues for each task(268 and 184 sentences) as a training dataand 2 dialogues as a test data (113 and 63sentences).
The results are shown in Table 2.Table 2: Comparison of TBL and example-based method.algorithm task closed openroute direction 85.1 72.6TBL car trouble shooting 90.2 66.7average 87.7 69.7route direction 93.8 62.6ex-based car trouble shooting 89.7 52.4average 91.8 57.5We got similar average score for closed test.Therefore, we regard the tuning level of pa-rameter of each algorithm as a comparablelevel.
In open test in the same task, we got69.7% in TBL and 57.5 % in example-basedmethod.
As a result, we can conclude TBL ismore suitable method for dialogue act tagginglearning in limited amount of training data.4 Relevance tagger using decisiontree4.1 Decision tree learningDecision tree learning algorithm is one of clas-sification rule learning algorithm.
The train-ing data is a list of attribute-value pair.
Theoutput of this algorithm is a decision treewhose nodes are regarded as set of rules.
Eachrule tests the value of an attribute and indi-cates the next node.A basic algorithm is as follows:1. create root node.2.
if all the data belong to the same class,create a class node and exit.otherwise,?
choose one attribute which hasthe maximum mutual informa-tion and create nodes correspond-ing values.?
divide and assign the trainingdata according to the values andcreate link to the new node.3.
apply this algorithm to all the new nodes recur-sivelyWe also used post-pruning rule hired inC4.5 (Quinlan, 1992).4.2 Relevance tagging algorithm andresultsRelevance type 1Relevance type 1 tag is automatically an-notated according to the exchange structurewhich is identified in dialogue act taggingstage.
The accuracy of the relevance type1 tag is depend on whether a given dialogueor task domain is follow the assumption ofexchange structure explained above.
In wellformed dialogue, the accuracy is above 95%.However, in ill-formed case, it is around 70%.Relevance type 2We have used decision tree method inidentifying relevance type 2, which identi-fies whether neighboring exchange structureshave a certain kind of relevance.
The at-tributes of training data are as follows.1.
relevance type 2 tag of previous exchange2.
initiative dialogue act tag of previous exchange3.
response dialogue act tag of previous exchange4.
initiative dialogue act tag of current exchangeWe used 9 dialogue (hotel reservation, routedirection, scheduling, and telephone shop-ping) as training and test data.
The resultsare shown in Table 3.
We got this results after10 cross validation.
In cross domain experi-ment, we got 84% accuracy in closed test (av-erage 47 nodes) and 75% in open test.
Usingpost-pruning method, we got 82% of accuracy(average 22 nodes; estimated accuracy 76%)in closed test and 77% in open test.5 Dialogue segment tagger5.1 TBI taggerWe used decision tree method in identifyingthe value of topic break index because the tar-get attribute have only two values; 1 (smalltopic change) or 2 (large topic change).
Incase of target attribute has small number ofvalues, decision tree method can be estimatedto outperform transformation-based learning.The attributes of training data are as fol-lows.1.
relevance type 2 tag of previous exchange2.
relevance type 2 tag of current exchange3.
topic break index tag of previous exchange4.
dialogue act tag of previous slash unit5.
dialogue act tag of current slash unitWe used same data set with the experimentof dialogue act tagging.
We got 87% accuracyin closed test (average 61 nodes) and 80% inopen test.
Using post-pruning method, we got82% of accuracy (average 12 nodes; estimatedaccuracy 76%) in closed test and 78% in opentest (see Table 4).5.2 Topic name taggerIn JDTAG topic name tagging scheme, anno-tators can assign a topic name subjectivelyto the given dialogue segment.
Certainly itis an appropriate method for this scheme touse for the dialogue of any task domain.
But,even in the almost same pattern of exchange,different annotators might annotate differenttopic names.
It prevent the data from a prac-tical usage, e.g.
extracting exchange patternin asking a route to certain place.We prepare a candidate topic name list andassign to dialogue segment as a topic name.Because candidate topic name is around 10to 30 according to the task domain, we usetransformation-based learning method for ac-quiring a topic name tagging rule set.The selected features in the conditionalpart of the rule are words of current segment(up to 2), dialogue act tag of the first slashunit of the segment, topic name tag of previ-ous segment.As a result, in the above data set, the candi-date rules are 5588.
And we got 98% accuracyin the closed test and 56% in open test.5.3 Segment relation taggerThe number of segment relation types are 4(clarification, interruption, return, and none).Therefore, we used decision tree for acquiringrules for identifying segment relation types.In making decision tree, we did not usepost-pruning because a great many of seg-ment relation tag is none (about 85%).
Post-pruning makes a tree too general (only onetop node which identifies none or else).Table 3: Results of relevance type 2 taggingnot pruned pruned# of nodes accuracy # of nodes accuracy estimated error rateTraining 47.3 83.7% 22.0 81.9% 23.7%Test 47.3 75.4% 22.0 77.2% 23.7%Table 4: Results of topic break index taggingnot pruned pruned# of nodes accuracy # of nodes accuracy estimated error rateTrainingtrouble shooting 53.0 90.0% 10.2 82.4% 22.6%Testtrouble shooting 53.0 85.2% 10.2 78.2% 22.6%Trainingroute direction 69.0 84.1% 14.5 82.4% 25.5%Testroute direction 69.0 73.8% 14.5 77.5% 25.5%As a result, also in the same data set, wegot 92% accuracy in the closed test.6 ConclusionWe have developed a discourse level taggingtool for spoken dialogue corpus using machinelearning methods.
We use transformation-based learning method in case of many targetvalues, and decision tree method otherwise.Our future work is to develop an environ-ment in which annotators can easily browseand post-edit the output of the tool.AcknowledgementThis work has been supported by the NEDOIndustrial Technology Research Grant Pro-gram (No.
00A18004b)ReferencesE.
Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: Acase study in part-of-speech tagging.
Compu-tational Linguistics, 21(4):543?566.J.
Carletta, N. Dahlback, N. Reithinger, andM.
A. Walker.
1997.
Standards for di-alogue coding in natural language pro-cessing.
Dagstuhl-Seminar-Report:167(ftp://ftp.cs.uni-sb.de/pub/dagstuhl/ re-porte/97/9706.ps.gz).M.
Core, M. Ishizaki, J. Moore, C. Nakatani,N.
Reithinger, D. Traum, and S. Tutiya.
1998.The Report of the Third Workshop of theDiscourse Research Initiative.
Chiba CorpusProject.
Technical Report 3, Chiba University.M.
Coulthhard, editor.
1992.
Advances in SpokenDiscourse Analysis.
Routledge.B.
J. Grosz and C. L. Sidner.
1986.
Attention,intention and the structure of discourse.
Com-putational Linguistics, 12:175?204.The Japanese Discourse Research Initiative JDRI.2000.
Japanese dialogue corpus of multi-levelannotation.
In Proc.
of the 1st SIGDIAL Work-shop on discourse and dialogue.M.
Meteer and A. Taylor.
1995.
Dysflu-ency annotation stylebook for the switch-board corpus.
Linguistic Data Consor-tium (ftp://ftp.cis.upenn.edu/pub/treebank/swbd/doc/DFL-book.ps.gz).J.
R. Quinlan.
1992.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.K.
Samuel, S. Carberry, and K. Vijay-Shanker.
1998.
Dialogue act tagging withtransformation-based learning.
In Proc.
ofCOLING-ACL 98, pages 1150?1156.J.
R. Searle.
1969.
Speech Acts.
Cambridge Uni-versity Press.A.
B. Stenstrom.
1994.
An Introduction to SpokenInteraction.
Addison-Wesley.
