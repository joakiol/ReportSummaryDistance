Proceedings of the Workshop on Information Extraction Beyond The Document, pages 66?73,Sydney, July 2006. c?2006 Association for Computational LinguisticsLearning Domain-Specific Information Extraction Patterns from the WebSiddharth Patwardhan and Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112{sidd,riloff}@cs.utah.eduAbstractMany information extraction (IE) systemsrely on manually annotated training datato learn patterns or rules for extracting in-formation about events.
Manually anno-tating data is expensive, however, and anew data set must be annotated for eachdomain.
So most IE training sets are rel-atively small.
Consequently, IE patternslearned from annotated training sets of-ten have limited coverage.
In this paper,we explore the idea of using the Web toautomatically identify domain-specific IEpatterns that were not seen in the trainingdata.
We use IE patterns learned from theMUC-4 training set as anchors to identifydomain-specific web pages and then learnnew IE patterns from them.
We computethe semantic affinity of each new patternto automatically infer the type of informa-tion that it will extract.
Experiments onthe MUC-4 test set show that these new IEpatterns improved recall with only a smallprecision loss.1 IntroductionInformation Extraction (IE) is the task of identi-fying event descriptions in natural language textand extracting information related to those events.Many IE systems use extraction patterns or rulesto identify the relevant information (Soderland etal., 1995; Riloff, 1996; Califf and Mooney, 1999;Soderland, 1999; Yangarber et al, 2000).
Most ofthese systems use annotated training data to learnpattern matching rules based on lexical, syntactic,and/or semantic information.
The learned patternsare then used to locate relevant information in newtexts.IE systems typically focus on information aboutevents that are relevant to a specific domain, suchas terrorism (Sundheim, 1992; Soderland et al,1995; Riloff, 1996; Chieu et al, 2003), man-agement succession (Sundheim, 1995; Yangarberet al, 2000), or job announcements (Califf andMooney, 1999; Freitag and McCallum, 2000).Supervised learning systems for IE depend ondomain-specific training data, which consists oftexts associated with the domain that have beenmanually annotated with event information.The need for domain-specific training data hasseveral disadvantages.
Because of the manual la-bor involved in annotating a corpus, and because anew corpus must be annotated for each domain,most annotated IE corpora are relatively small.Language is so expressive that it is practicallyimpossible for the patterns learned from a rela-tively small training set to cover all the differentways of describing events.
Consequently, the IEpatterns learned from manually annotated train-ing sets typically represent only a subset of the IEpatterns that could be useful for the task.
Manyrecent approaches in natural language processing(Yarowsky, 1995; Collins and Singer, 1999; Riloffand Jones, 1999; Nigam et al, 2000; Wiebe andRiloff, 2005) have recognized the need to useunannotated data to improve performance.While the Web provides a vast repository ofunannotated texts, it is non-trivial to identify textsthat belong to a particular domain.
The difficultyis that web pages are not specifically annotatedwith tags categorizing their content.
Nevertheless,in this paper we look to the Web as a vast dynamicresource for domain-specific IE learning.
Our ap-proach exploits an existing set of IE patterns thatwere learned from annotated training data to auto-matically identify new, domain-specific texts from66the Web.
These web pages are then used for ad-ditional IE training, yielding a new set of domain-specific IE patterns.
Experiments on the MUC-4test set show that the new IE patterns improve cov-erage for the domain.This paper is organized as follows.
Section 2presents the MUC-4 IE task and data that we use inour experiments.
Section 3 describes how we cre-ate a baseline IE system from the MUC-4 trainingdata.
Section 4 describes the collection and pre-processing of potentially relevant web pages.
Sec-tion 5 then explains how we use the IE patternslearned from the MUC-4 training set as anchors tolearn new IE patterns from the web pages.
We alsocompute the semantic affinity of each new patternto automatically infer the type of information thatit will extract.
Section 6 shows experimental re-sults for two types of extractions, victims and tar-gets, on the MUC-4 test set.
Finally, Section 7compares our approach to related research, andSection 8 concludes with ideas for future work.2 The MUC-4 IE Task and DataThe focus of our research is on the MUC-4 infor-mation extraction task (Sundheim, 1992), which isto extract information about terrorist events.
TheMUC-4 corpus contains 1700 stories, mainly newsarticles related to Latin American terrorism, andassociated answer key templates containing the in-formation that should be extracted from each story.We focused our efforts on two of the MUC-4string slots, which require textual extractions: hu-man targets (victims) and physical targets.
TheMUC-4 data has proven to be an especially dif-ficult IE task for a variety of reasons, includingthe fact that the texts are entirely in upper case,roughly 50% of the texts are irrelevant (i.e., theydo not describe a relevant terrorist event), andmany of the stories that are relevant describe mul-tiple terrorist events that need to be teased apart.The best results reported across all string slotsin MUC-4 were in the 50%-70% range for re-call and precision (Sundheim, 1992), with mostof the MUC-4 systems relying on heavily hand-engineered components.
Chieu et al (2003) re-cently developed a fully automatic template gen-erator for the MUC-4 IE task.
Their best systemproduced recall scores of 41%-44% with precisionscores of 49%-51% on the TST3 and TST4 testsets.3 Learning IE Patterns from a FixedTraining SetAs our baseline system, we created an IEsystem for the MUC-4 terrorism domain us-ing the AutoSlog-TS extraction pattern learn-ing system (Riloff, 1996; Riloff and Phillips,2004), which is freely available for research use.AutoSlog-TS is a weakly supervised learner thatrequires two sets of texts for training: texts thatare relevant to the domain and texts that are irrel-evant to the domain.
The MUC-4 data includesrelevance judgments (implicit in the answer keys),which we used to partition our training set into rel-evant and irrelevant subsets.AutoSlog-TS?
learning process has two phases.In the first phase, syntactic patterns are appliedto the training corpus in an exhaustive fashion,so that extraction patterns are generated for (lit-erally) every lexical instantiation of the patternsthat appears in the corpus.
For example, the syn-tactic pattern ?<subj> PassVP?
would generateextraction patterns for all verbs that appear in thecorpus in a passive voice construction.
The sub-ject of the verb will be extracted.
In the terrorismdomain, some of these extraction patterns mightbe: ?<subj> PassVP(murdered)?
and ?<subj>PassVP(bombed).?
These would match sentencessuch as: ?the mayor was murdered?, and ?the em-bassy and hotel were bombed?.
Figure 1 showsthe 17 types of extraction patterns that AutoSlog-TS currently generates.
PassVP refers to passivevoice verb phrases (VPs), ActVP refers to activevoice VPs, InfVP refers to infinitive VPs, andAuxVP refers to VPs where the main verb is aform of ?to be?
or ?to have?.
Subjects (subj), di-rect objects (dobj), PP objects (np), and posses-sives can be extracted by the patterns.In the second phase, AutoSlog-TS applies allof the generated extraction patterns to the trainingcorpus and gathers statistics for how often eachpattern occurs in relevant versus irrelevant texts.The extraction patterns are subsequently rankedbased on their association with the domain, andthen a person manually reviews the patterns, de-ciding which ones to keep1 and assigning thematicroles to them.
We manually defined selectionalrestrictions for each slot type (victim and target)1Typically, many patterns are strongly associated with thedomain but will not extract information that is relevant to theIE task.
For example, in this work we only care about patternsthat will extract victims and targets.
Patterns that extract othertypes of information are not of interest.67Pattern Type Example Pattern<subj> PassVP <victim> was murdered<subj> ActVP <perp> murdered<subj> ActVP Dobj <weapon> caused damage<subj> ActInfVP <perp> tried to kill<subj> PassInfVP <weapon> was intended to kill<subj> AuxVP Dobj <victim> was casualty<subj> AuxVP Adj <victim> is deadActVP <dobj> bombed <target>InfVP <dobj> to kill <victim>ActInfVP <dobj> planned to bomb <target>PassInfVP <dobj> was planned to kill <victim>Subj AuxVP <dobj> fatality is <victim>NP Prep <np> attack against <target>ActVP Prep <np> killed with <weapon>PassVP Prep <np> was killed with <weapon>InfVP Prep <np> to destroy with <weapon><possessive> NP <victim>?s murderFigure 1: AutoSlog-TS?
pattern types and sampleIE patternsand then automatically added these to each patternwhen the role was assigned.On our training set, AutoSlog-TS generated40,553 distinct extraction patterns.
A person man-ually reviewed all of the extraction patterns thathad a score ?
0.951 and frequency ?
3.
Thisscore corresponds to AutoSlog-TS?
RlogF metric,described in (Riloff, 1996).
The lowest ranked pat-terns that passed our thresholds had at least 3 rel-evant extractions out of 5 total extractions.
In all,2,808 patterns passed the thresholds.
The reviewerultimately decided that 396 of the patterns wereuseful for the MUC-4 IE task, of which 291 wereuseful for extracting victims and targets.4 Data CollectionIn this research, our goal is to automatically learnIE patterns from a large, domain-independent textcollection, such as the Web.
The billions of freelyavailable documents on the World Wide Web andits ever-growing size make the Web a potentialsource of data for many corpus-based natural lan-guage processing tasks.
Indeed, many researchershave recently tapped the Web as a data-sourcefor improving performance on NLP tasks (e.g.,Resnik (1999), Ravichandran and Hovy (2002),Keller and Lapata (2003)).
Despite these suc-cesses, numerous problems exist with collectingdata from the Web, such as web pages contain-ing information that is not free text, including ad-vertisements, embedded scripts, tables, captions,etc.
Also, the documents cover many genres, andit is not easy to identify documents of a particulargenre or domain.
Additionally, most of the doc-uments are in HTML, and some amount of pro-cessing is required to extract the free text.
In thefollowing subsections we describe the process ofcollecting a corpus of terrorism-related CNN newsarticles from the Web.4.1 Collecting Domain-Specific TextsOur goal was to automatically identify and collecta set of documents that are similar in domain to theMUC-4 terrorism text collection.
To create sucha corpus, we used hand-crafted queries given toa search engine.
The queries to the search enginewere manually created to try to ensure that the ma-jority of the documents returned by the search en-gine would be terrorism-related.
Each query con-sisted of two parts: (1) the name of a terrorist or-ganization, and (2) a word or phrase describing aterrorist action (such as bombed, kidnapped, etc.
).The following lists of 5 terrorist organizations and16 terrorist actions were used to create search en-gine queries:Terrorist organizations: Al Qaeda,ELN, FARC, HAMAS, IRATerrorist actions: assassinated, assas-sination, blew up, bombed, bombing,bombs, explosion, hijacked, hijacking,injured, kidnapped, kidnapping, killed,murder, suicide bomber, wounded.We created a total of 80 different queries repre-senting each possible combination of a terrorist or-ganization and a terrorist action.We used the Google2 search engine with thehelp of the freely available Google API3 to lo-cate the texts on the Web.
To ensure that we re-trieved only CNN news articles, we restricted thesearch to the domain ?cnn.com?
by adding the?site:?
option to each of the queries.
We alsorestricted the search to English language docu-ments by initializing the API with the lang enoption.
We deleted documents whose URLs con-tained the word ?transcript?
because most of thesewere transcriptions of CNN?s TV shows and werestylistically very different from written text.
Weran the 80 queries twice, once in December 2005and once in April 2005, which produced 3,496documents and 3,309 documents, respectively.After removing duplicate articles, we were left2http://www.google.com3http://www.google.com/apis68with a total of 6,182 potentially relevant terrorismarticles.4.2 Processing the TextsThe downloaded documents were all HTML doc-uments containing HTML tags and JavaScript in-termingled with the news text.
The CNN web-pages typically also contained advertisements, textfor navigating the website, headlines and links toother stories.
All of these things could be problem-atic for our information extraction system, whichwas designed to process narrative text using a shal-low parser.
Thus, simply deleting all HTML tagson the page would not have given us natural lan-guage sentences.
Instead, we took advantage ofthe uniformity of the CNN web pages to ?clean?them and extract just the sentences correspondingto the news story.We used a tool called HTMLParser4 to parsethe HTML code, and then deleted all nodes in theHTML parse trees corresponding to tables, com-ments, and embedded scripts (such as JavaScriptor VBScript).
The system automatically extractednews text starting from the headline (embeddedin an H1 HTML element) and inferred the end ofthe article text using a set of textual clues such as?Feedback:?, ?Copyright 2005?, ?contributed tothis report?, etc.
In case of any ambiguity, all ofthe text on the web page was extracted.The size of the text documents ranged from 0bytes to 255 kilobytes.
The empty documentswere due to dead links that the search engine hadindexed at an earlier time, but which no longer ex-isted.
Some extremely small documents also re-sulted from web pages that had virtually no freetext on them, so only a few words remained af-ter the HTML had been stripped.
Consequently,we removed all documents less than 10 bytes insize.
Upon inspection, we found that many of thelargest documents were political articles, such aspolitical party platforms and transcriptions of po-litical speeches, which contained only brief refer-ences to terrorist events.
To prevent the large doc-uments from skewing the corpus, we also deletedall documents over 10 kilobytes in size.
At the endof this process we were left with a CNN terrorismnews corpus of 5,618 documents, each with an av-erage size of about 648 words.
In the rest of thepaper we will refer to these texts as ?the CNN ter-rorism web pages?.4http://htmlparser.sourceforge.net5 Learning Domain-Specific IE Patternsfrom Web PagesHaving created a large domain-specific corpusfrom the Web, we are faced with the problemof identifying the useful extraction patterns fromthese new texts.
Our basic approach is to use thepatterns learned from the fixed training set as seedpatterns to identify sentences in the CNN terror-ism web pages that describe a terrorist event.
Wehypothesized that extraction patterns occurring inthe same sentence as a seed pattern are likely to beassociated with terrorism.Our process for learning new domain-specificIE patterns has two phases, which are described inthe following sections.
Section 5.1 describes howwe produce a ranked list of candidate extractionpatterns from the CNN terrorism web pages.
Sec-tion 5.2 explains how we filter these patterns basedon the semantic affinity of their extractions, whichis a measure of the tendency of the pattern to ex-tract entities of a desired semantic category.5.1 Identifying Candidate PatternsThe first goal was to identify extraction patternsthat were relevant to our domain: terrorist events.We began by exhaustively generating every pos-sible extraction pattern that occurred in our CNNterrorism web pages.
We applied the AutoSlog-TSsystem (Riloff, 1996) to the web pages to automat-ically generate all lexical instantiations of patternsin the corpus.
Collectively, the resulting patternswere capable of extracting every noun phrase inthe CNN collection.
In all, 147,712 unique extrac-tion patterns were created as a result of this pro-cess.Next, we computed the statistical correlationof each extraction pattern with the seed patternsbased on the frequency of their occurrence in thesame sentence.
IE patterns that never occurredin the same sentence as a seed pattern were dis-carded.
We used Pointwise Mutual Information(PMI) (Manning and Schu?tze, 1999; Banerjee andPedersen, 2003) as the measure of statistical corre-lation.
Intuitively, an extraction pattern that occursmore often than chance in the same sentence as aseed pattern will have a high PMI score.The 147,712 extraction patterns acquired fromthe CNN terrorism web pages were then rankedby their PMI correlation to the seed patterns.
Ta-ble 1 lists the most highly ranked patterns.
Manyof these patterns do seem to be related to terrorism,69<subj> killed sgt <subj> destroyed factories<subj> burned flag explode after <np>sympathizers of <np> <subj> killed heir<subj> kills bystanders <subj> shattered roofrescued within <np> fled behind <np>Table 1: Examples of candidate patterns that arehighly correlated with the terrorism seed patternsbut many of them are not useful to our IE task (forthis paper, identifying the victims and physical tar-gets of a terrorist attack).
For example, the pattern?explode after <np>?
will not extract victims orphysical targets, while the pattern ?sympathizersof <np>?
may extract people but they would notbe the victims of an attack.
In the next section, weexplain how we filter and re-rank these candidatepatterns to identify the ones that are directly usefulto our IE task.5.2 Filtering Patterns based upon theirSemantic AffinityOur next goal is to filter out the patterns that arenot useful for our IE task, and to automaticallyassign the correct slot type (victim or target) tothe ones that are relevant.
To automatically deter-mine the mapping between extractions and slots,we define a measure called semantic affinity.
Thesemantic affinity of an extraction pattern to a se-mantic category is a measure of its tendency toextract NPs belonging to that semantic category.This measure serves two purposes:(a) It allows us to filter out candidate patternsthat do not have a strong semantic affinity toour categories of interest.
(b) It allows us to define a mapping between theextractions of the candidate patterns and thedesired slot types.We computed the semantic affinity of each can-didate extraction pattern with respect to six seman-tic categories: target, victim, perpetrator, organi-zation, weapon and other.
Targets and victims areour categories of interest.
Perpetrators, organiza-tions, and weapons are common semantic classesin this domain which could be ?distractors?.
Theother category is a catch-all to represent all othersemantic classes.
To identify the semantic class ofeach noun phrase, we used the Sundance package(Riloff and Phillips, 2004), which is a freely avail-able shallow parser that uses dictionaries to assignsemantic classes to words and phrases.We counted the frequencies of the semantic cat-egories extracted by each candidate pattern andapplied the RLogF measure used by AutoSlog-TS(Riloff, 1996) to rank the patterns based on theiraffinity for the target and victim semantic classes.For example, the semantic affinity of an extractionpattern for the target semantic class would be cal-culated as:affinitypattern =ftargetfall?
log2ftarget (1)where ftarget is the number of target semanticclass extractions and fall = ftarget + fvictim +fperp +forg +fweapon +fother.
This is essentiallya probability P (target) weighted by the log of thefrequency.We then used two criteria to remove patternsthat are not strongly associated with a desired se-mantic category.
If the semantic affinity of a pat-tern for category C was (1) greater than a thresh-old, and (2) greater than its affinity for the othercategory, then the pattern was deemed to have asemantic affinity for category C. Note that weintentionally allow for a pattern to have an affin-ity for more than one semantic category (exceptfor the catch-all other class) because this is fairlycommon in practice.
For example, the pattern ?at-tack on <np>?
frequently extracts both targets(e.g., ?an attack on the U.S.
embassy?)
and vic-tims (e.g., ?an attack on the mayor of Bogota?
).Our hope is that such a pattern would receive ahigh semantic affinity ranking for both categories.Table 2 shows the top 10 high frequency(freq ?
50) patterns that were judged to have astrong semantic affinity for the target and victimcategories.
There are clearly some incorrect en-tries (e.g., ?<subj> fired missiles?
is more likelyto identify perpetrators than targets), but most ofthe patterns are indeed good extractors for the de-sired categories.
For example, ?fired into <np>?,?went off in <np>?, and ?car bomb near <np>?are all good patterns for identifying targets of aterrorist attack.
In general, the semantic affinitymeasure seemed to do a reasonably good job offiltering patterns that are not relevant to our task,and identifying patterns that are useful for extract-ing victims and targets.6 Experiments and ResultsOur goal has been to use IE patterns learned froma fixed, domain-specific training set to automat-ically learn additional IE patterns from a large,70Target Patterns Victim Patterns<subj> fired missiles wounded in <np>missiles at <np> <subj> was identifiedbomb near <np> wounding <dobj>fired into <np> <subj> woundingdied on <np> identified <dobj>went off in <np> <subj> identifiedcar bomb near <np> including <dobj>exploded outside <np> <subj> ahmedgunmen on <np> <subj> lyingkilled near <np> <subj> includingTable 2: Top 10 high-frequency target and victimpatterns learned from the Webdomain-independent text collection, such as theWeb.
Although many of the patterns learnedfrom the CNN terrorism web pages look like goodextractors, an open question was whether theywould actually be useful for the original IE task.For example, some of the patterns learned fromthe CNN web pages have to do with behead-ings (e.g., ?beheading of <np>?
and ?beheaded<np>?
), which are undeniably good victim ex-tractors.
But the MUC-4 corpus primarily con-cerns Latin American terrorism that does not in-volve beheading incidents.
In general, the ques-tion is whether IE patterns learned from a large, di-verse text collection can be valuable for a specificIE task above and beyond the patterns that werelearned from the domain-specific training set, orwhether the newly learned patterns will simply notbe applicable.
To answer this question, we evalu-ated the newly learned IE patterns on the MUC-4test set.The MUC-4 data set is divided into 1300 devel-opment (DEV) texts, and four test sets of 100 textseach (TST1, TST2, TST3, and TST4).5 All ofthese texts have associated answer key templates.We used 1500 texts (DEV+TST1+TST2) as ourtraining set, and 200 texts (TST3+TST4) as ourtest set.The IE process typically involves extractinginformation from individual sentences and thenmapping that information into answer key tem-plates, one template for each terrorist event de-scribed in the story.
The process of template gen-eration requires discourse processing to determinehow many events took place and which facts cor-respond to which event.
Discourse processing and5The DEV texts were used for development in MUC-3and MUC-4.
The TST1 and TST2 texts were used as test setsfor MUC-3 and then as development texts for MUC-4.
TheTST3 and TST4 texts were used as the test sets for MUC-4.template generation are not the focus of this paper.Our research aims to produce a larger set of extrac-tion patterns so that more information will be ex-tracted from the sentences, before discourse anal-ysis would begin.
Consequently, we evaluate theperformance of our IE system at that stage: afterextracting information from sentences, but beforetemplate generation takes place.
This approach di-rectly measures how well we are able to improvethe coverage of our extraction patterns for the do-main.6.1 Baseline Results on the MUC-4 IE TaskThe AutoSlog-TS system described in Section 3used the MUC-4 training set to learn 291 targetand victim IE patterns.
These patterns produced64% recall with 43% precision on the targets, and50% recall with 52% precision on the victims.6These numbers are not directly comparable tothe official MUC-4 scores, which evaluate tem-plate generation, but our recall is in the same ball-park.
Our precision is lower, but this is to be ex-pected because we do not perform discourse anal-ysis.7 These 291 IE patterns represent our base-line IE system that was created from the MUC-4training data.6.2 Evaluating the Newly Learned PatternsWe used all 396 terrorism extraction patternslearned from the MUC-4 training set8 as seeds toidentify relevant text regions in the CNN terrorismweb pages.
We then produced a ranked list of newterrorism IE patterns using a semantic affinity cut-off of 3.0.
We selected the top N patterns from theranked list, with N ranging from 50 to 300, andadded these N patterns to the baseline system.Table 3 lists the recall, precision and F-measurefor the increasingly larger pattern sets.
For the tar-6We used a head noun scoring scheme, where we scoredan extraction as correct if its head noun matched the headnoun in the answer key.
This approach allows for differentleading modifiers in an NP as long as the head noun is thesame.
For example, ?armed men?
will successfully match?5 armed men?.
We also discarded pronouns (they were notscored at all) because our system does not perform corefer-ence resolution.7Among other things, discourse processing merges seem-ingly disparate extractions based on coreference resolution(e.g., ?the guerrillas?
may refer to the same people as ?thearmed men?)
and applies task-specific constraints (e.g., theMUC-4 task definition has detailed rules about exactly whattypes of people are considered to be terrorists).8This included not only the 291 target and victim patterns,but also 105 patterns associated with other types of terrorisminformation.71Targets VictimsPrecision Recall F-measure Precision Recall F-measurebaseline 0.425 0.642 0.511 0.498 0.517 0.50750+baseline 0.420 0.642 0.508 0.498 0.517 0.507100+baseline 0.419 0.650 0.510 0.496 0.521 0.508150+baseline 0.415 0.650 0.507 0.480 0.521 0.500200+baseline 0.412 0.667 0.509 0.478 0.521 0.499250+baseline 0.401 0.691 0.507 0.478 0.521 0.499300+baseline 0.394 0.691 0.502 0.471 0.542 0.504Table 3: Performance of new IE patterns on MUC-4 test setget slot, the recall increases from 64.2% to 69.1%with a small drop in precision.
The F-measuredrops by about 1% because recall and precisionare less balanced.
But we gain more in recall(+5%) than we lose in precision (-3%).
For thevictim patterns, the recall increases from 51.7% to54.2% with a similar small drop in precision.
Theoverall drop in the F-measure in this case is neg-ligible.
These results show that our approach forlearning IE patterns from a large, diverse text col-lection (the Web) can indeed improve coverage ona domain-specific IE task, with a small decrease inprecision.7 Related WorkUnannotated texts have been used successfully fora variety of NLP tasks, including named entityrecognition (Collins and Singer, 1999), subjectiv-ity classification (Wiebe and Riloff, 2005), textclassification (Nigam et al, 2000), and word sensedisambiguation (Yarowsky, 1995).
The Web hasbecome a popular choice as a resource for largequantities of unannotated data.
Many researchideas have exploited the Web in unsupervised orweakly supervised algorithms for natural languageprocessing (e.g., Resnik (1999), Ravichandran andHovy (2002), Keller and Lapata (2003)).The use of unannotated data to improve in-formation extraction is not new.
Unannotatedtexts have been used for weakly supervised train-ing of IE systems (Riloff, 1996) and in boot-strapping methods that begin with seed wordsor patterns (Riloff and Jones, 1999; Yangarberet al, 2000).
However, those previous sys-tems rely on pre-existing domain-specific cor-pora.
For example, EXDISCO (Yangarber etal., 2000) used Wall Street Journal articles fortraining.
AutoSlog-TS (Riloff, 1996) and Meta-bootstrapping (Riloff and Jones, 1999) used theMUC-4 training texts.
Meta-bootstrapping wasalso trained on web pages, but the ?domain?
wascorporate relationships so domain-specific webpages were easily identified simply by gatheringcorporate web pages.The KNOWITALL system (Popescu et al, 2004)also uses unannotated web pages for informationextraction.
However, this work is quite differ-ent from ours because KNOWITALL focuses onextracting domain-independent relationships withthe aim of extending an ontology.
In contrast,our work focuses on using the Web to augmenta domain-specific, event-oriented IE system withnew, automatically generated domain-specific IEpatterns acquired from the Web.8 Conclusions and Future WorkWe have shown that it is possible to learn newextraction patterns for a domain-specific IE taskby automatically identifying domain-specific webpages using seed patterns.
Our approach produceda 5% increase in recall for extracting targets and a3% increase in recall for extracting victims of ter-rorist events.
Both increases in recall were at thecost of a small loss in precision.In future work, we plan to develop improvedranking methods and more sophisticated seman-tic affinity measures to further improve coverageand minimize precision loss.
Another possible av-enue for future work is to embed this approach in abootstrapping mechanism so that the most reliablenew IE patterns can be used to collect additionalweb pages, which can then be used to learn moreIE patterns in an iterative fashion.
Also, whilemost of this process is automated, some human in-tervention is required to create the search queriesfor the document collection process, and to gener-ate the seed patterns.
We plan to look into tech-niques to automate these manual tasks as well.72AcknowledgmentsThis research was supported by NSF Grant IIS-0208985 and the Institute for Scientific Comput-ing Research and the Center for Applied ScientificComputing within Lawrence Livermore NationalLaboratory.ReferencesS.
Banerjee and T. Pedersen.
2003.
The Design, Im-plementation, and Use of the Ngram Statistics Pack-age.
In Proceedings of the Fourth InternationalConference on Intelligent Text Processing and Com-putational Linguistics, pages 370?381, Mexico City,Mexico, February.M.
Califf and R. Mooney.
1999.
Relational Learningof Pattern-matching Rules for Information Extrac-tion.
In Proceedings of the Sixteenth National Con-ference on Artificial Intelligence, pages 328?334,Orlando, FL, July.H.
Chieu, H. Ng, and Y. Lee.
2003.
Closing theGap: Learning-Based Information Extraction Rival-ing Knowledge-Engineering Methods.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 216?223, Sap-poro, Japan, July.M.
Collins and Y.
Singer.
1999.
Unsupervised Modelsfor Named Entity Classification.
In Proceedings ofJoint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Cor-pora, pages 100?110, College Park, MD, June.D.
Freitag and A. McCallum.
2000.
Informa-tion Extraction with HMM Structures Learned byStochastic Optimization.
In Proceedings of the Sev-enteenth National Conference on Artificial Intelli-gence, pages 584?589, Austin, TX, August.F.
Keller and M. Lapata.
2003.
Using the Web toObtain Frequencies for Unseen Bigrams.
Compu-tational Linguistics, 29(3):459?484, September.C.
Manning and H. Schu?tze.
1999.
Foundations ofStatistical Natural Language Processing.
The MITPress, Cambridge, MA.K.
Nigam, A. McCallum, S. Thrun, and T. Mitchell.2000.
Text Classification from Labeled and Un-labeled Documents using EM.
Machine Learning,39(2-3):103?134, May.A.
Popescu, A. Yates, and O. Etzioni.
2004.
Class Ex-traction from the World Wide Web.
In Ion Muslea,editor, Adaptive Text Extraction and Mining: Papersfrom the 2004 AAAI Workshop, pages 68?73, SanJose, CA, July.D.
Ravichandran and E. Hovy.
2002.
Learning SurfaceText Patterns for a Question Answering System.
InProceedings of the 40th Annual Meeting on Associ-ation for Computational Linguistics, pages 41?47,Philadelphia, PA, July.P.
Resnik.
1999.
Mining the Web for Bilingual Text.In Proceedings of the 37th meeting of the Associa-tion for Computational Linguistics, pages 527?534,College Park, MD, June.E.
Riloff and R. Jones.
1999.
Learning Dictionar-ies for Information Extraction by Multi-Level Boot-strapping.
In Proceedings of the Sixteenth NationalConference on Artificial Intelligence, pages 474?479, Orlando, FL, July.E.
Riloff and W. Phillips.
2004.
An Introduction to theSundance and AutoSlog Systems.
Technical ReportUUCS-04-015, School of Computing, University ofUtah.E.
Riloff.
1996.
Automatically Generating ExtractionPatterns from Untagged Text.
In Proceedings of theThirteenth National Conference on Articial Intelli-gence, pages 1044?1049, Portland, OR, August.S.
Soderland, D. Fisher, J. Aseltine, and W. Lehnert.1995.
CRYSTAL: Inducing a Conceptual Dictio-nary.
In Proceedings of the Fourteenth InternationalJoint Conference on Artificial Intelligence, pages1314?1319, Montreal, Canada, August.S.
Soderland.
1999.
Learning Information ExtractionRules for Semi-Structured and Free Text.
MachineLearning, 34(1-3):233?272, February.B.
Sundheim.
1992.
Overview of the Fourth MessageUnderstanding Evaluation and Conference.
In Pro-ceedings of the Fourth Message Understanding Con-ference (MUC-4), pages 3?21, McLean, VA, June.B.
Sundheim.
1995.
Overview of the Results ofthe MUC-6 Evaluation.
In Proceedings of theSixth Message Understanding Conference (MUC-6),pages 13?31, Columbia, MD, November.J.
Wiebe and E. Riloff.
2005.
Creating Subjectiveand Objective Sentence Classifiers from Unanno-tated Texts.
In Proceedings of the 6th InternationalConference on Computational Linguistics and Intel-ligent Text Processing, pages 486?497, Mexico City,Mexico, February.R.
Yangarber, R. Grishman, P. Tapanainen, and S. Hut-tunen.
2000.
Automatic Acquisition of DomainKnowledge for Information Extraction.
In Proceed-ings of the 18th International Conference on Com-putational Linguistics, pages 940?946, Saarbru?cken,Germany, August.D.
Yarowsky.
1995.
Unsupervised Word Sense Dis-ambiguation Rivaling Supervised Methods.
In Pro-ceedings of the 33rd Annual Meeting of the Associa-tion for Computational Linguistics, pages 189?196,Cambridge, MA, June.73
