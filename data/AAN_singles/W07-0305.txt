Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 32?39,NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsOlympus: an open-source frameworkfor conversational spoken language interface researchDan Bohus, Antoine Raux, Thomas K. Harris,Maxine Eskenazi, Alexander I. RudnickySchool of Computer ScienceCarnegie Mellon University{dbohus, antoine, tkharris, max, air}@cs.cmu.eduAbstractWe introduce Olympus, a freely availableframework for research in conversationalinterfaces.
Olympus?
open, transparent,flexible, modular and scalable nature fa-cilitates the development of large-scale,real-world systems, and enables researchleading to technological and scientific ad-vances in conversational spoken languageinterfaces.
In this paper, we describe theoverall architecture, several systemsspanning different domains, and a numberof current research efforts supported byOlympus.1 IntroductionSpoken language interfaces developed in industrialand academic settings differ in terms of goals, thetypes of tasks and research questions addressed,and the kinds of resources available.In order to be economically viable, most indus-try groups need to develop real-world applicationsthat serve large and varied customer populations.As a result, they gain insight into the researchquestions that are truly significant for current-generation technologies.
When needed, they areable to focus large resources (typically unavailablein academia) on addressing these questions.
Toprotect their investments, companies do not gener-ally disseminate new technologies and results.In contrast, academia pursues long-term scien-tific research goals, which are not tied to immedi-ate economic returns or customer populations.
As aresult, academic groups are free to explore a largervariety of research questions, even with a high riskof failure or a lack of immediate payoff.
Academicgroups also engage in a more open exchange ofideas and results.
However, building spoken lan-guage interfaces requires significant investmentsthat are sometimes beyond the reach of academicresearchers.
As a consequence, research in acade-mia is oftentimes conducted with toy systems andskewed user populations.
In turn, this raises ques-tions about the validity of the results and hindersthe research impact.In an effort to address this problem and facilitateresearch on relevant, real-world questions, we havedeveloped Olympus, a freely available frameworkfor building and studying conversational spokenlanguage interfaces.
The Olympus architecture,described in Section 3, has its roots in the CMUCommunicator project (Rudnicky et al, 1999).Based on that experience and subsequent projects,we have engineered Olympus into an open, trans-parent, flexible, modular, and scalable architecture.To date, Olympus has been used to develop anddeploy a number of spoken language interfacesspanning different domains and interaction types;these systems are presented in Section 4.
They arecurrently supporting research on diverse aspects ofspoken language interaction.
Section 5 discussesthree such efforts: error handling, multi-participantconversation, and turn-taking.We believe that Olympus and other similar tool-kits, discussed in Section 6, are essential in orderto bridge the gap between industry and academia.Such frameworks lower the cost of entry for re-32search on practical conversational interfaces.
Theyalso promote technology transfer through the reuseof components, and support direct comparisonsbetween systems and technologies.2 Desired characteristicsWhile developing Olympus, we identified a num-ber of characteristics that in our opinion are neces-sary to effectively support and foster research.
Theframework should be open, transparent, flexible,modular, and scalable.Open.
Complete source code should be avail-able for all the components so that researchers andengineers can inspect and modify it towards theirends.
Ideally, source code should be free for bothresearch and commercial purposes and growthrough contributions from the user community.Transparent / Analytic.
Open source codepromotes transparency, but beyond that researchersmust be able to analyze the system?s behavior.
Tothis end, every component should provide detailedaccounts of their internal state.
Furthermore, toolsfor data visualization and analysis should be anintegral part of the framework.Flexible.
The framework should be able to ac-commodate a wide range of applications and re-search interests, and allow easy integration of newtechnologies.Modular / Reusable.
Specific functions (e.g.speech recognition, parsing) should be encapsu-lated in components with rich and well-definedinterfaces, and an application-independent design.This will promote reusability, and will lessen thesystem development effort.Scalable.
While frameworks that rely on sim-ple, well established approaches (e.g.
finite-statedialogs in VoiceXML) allow the development oflarge-scale systems, this is usually not the case forframeworks that provide the flexibility and trans-parency needed for research.
However, some re-search questions are not apparent until one movesfrom toy systems into large-scale applications.
Theframework should strive to not compromise scal-ability for the sake of flexibility or transparency.3 ArchitectureAt the high level, a typical Olympus applicationconsists of a series of components connected in aclassical, pipeline architecture, as illustrated by thebold components in Figure 1.
The audio signal forthe user utterance is captured and passed through aspeech recognition module that produces a recog-nition hypothesis (e.g., two p.m.).
The recognitionhypothesis is then forwarded to a language under-standing component that extracts the relevant con-cepts (e.g., [time=2p.m.
]), and then through aconfidence annotation module that assigns a confi-dence score.
Next, a dialog manager integrates thissemantic input into the current context, and pro-duces the next action to be taken by the system inthe form of the semantic output (e.g., {requestend_time}).
A language generation module pro-duces the corresponding surface form, which issubsequently passed to a speech synthesis moduleand rendered as audio.Galaxy communication infrastructure.
Whilethe pipeline shown in bold in Figure 1 captures thelogical flow of information in the system, in prac-tice the system components communicate via acentralized message-passing infrastructure ?
Gal-axy (Seneff et al, 1998).
Each component is im-plemented as a separate process that connects to atraffic router ?
the Galaxy hub.
The messages aresent through the hub, which forwards them to theappropriate destination.
The routing logic is de-scribed via a configuration script.Speech recognition.
Olympus uses the Sphinxdecoding engine (Huang et al, 1992).
A recogni-tion server captures the audio stream, forwards it toa set of parallel recognition engines, and collectsthe corresponding recognition results.
The set ofbest hypotheses (one from each engine) is thenforwarded to the language understanding compo-nent.
The recognition engines can also generate n-best lists, but that process significantly slows downthe systems and has not been used live.
Interfacesto connect Sphinx-II and Sphinx-III engines, aswell as a DTMF (touch-tone) decoder to the recog-nition server are currently available.
The individualrecognition engines can use either n-gram- orgrammar-based language models.
Dialog-statespecific as well as class-based language models aresupported, and tools for constructing language andacoustic models from data are readily available.Most of the Olympus systems described in the nextsection use two gender-specific Sphinx-II recog-nizers in parallel.
Other parallel decoder configura-tions can also be created and used.Language understanding is performed byPhoenix, a robust semantic parser (Ward and Issar,331994).
Phoenix uses a semantic grammar to parsethe incoming set of recognition hypotheses.
Thisgrammar is assembled by concatenating a set ofreusable grammar rules that capture domain-independent constructs like [Yes], [No], [Help],[Repeat], and [Number], with a set of domain-specific grammar rules authored by the system de-veloper.
For each recognition hypothesis the outputof the parser consists of a sequence of slots con-taining the concepts extracted from the utterance.Confidence annotation.
From Phoenix, the setof parsed hypotheses is passed to Helios, the con-fidence annotation component.
Helios uses featuresfrom different knowledge sources in the system(e.g., recognition, understanding, dialog) to com-pute a confidence score for each hypothesis.
Thisscore reflects the probability of correct understand-ing, i.e.
how much the system trusts that the cur-rent semantic interpretation corresponds to theuser?s intention.
The hypothesis with the highestscore is forwarded to the dialog manager.Dialog management.
Olympus uses the Raven-Claw dialog management framework (Bohus andRudnicky, 2003).
In a RavenClaw-based dialogmanager, the domain-specific dialog task is repre-sented as a tree whose internal nodes capture thehierarchical structure of the dialog, and whoseleaves encapsulate atomic dialog actions (e.g., ask-ing a question, providing an answer, accessing adatabase).
A domain-independent dialog engineexecutes this dialog task, interprets the input in thecurrent dialog context and decides which action toengage next.
In the process, the dialog managermay exchange information with other domain-specific agents (e.g., application back-end, data-base access, temporal reference resolution agent).Language generation.
The semantic output ofthe dialog manager is sent to the Rosetta template-based language generation component, which pro-duces the corresponding surface form.
Like thePhoenix grammar, the language generation tem-plates are assembled by concatenating a set of pre-defined, domain-independent templates, withmanually authored task-specific templates.Speech synthesis.
The prompts are synthesizedby the Kalliope speech synthesis module.
Kalliopecan be currently configured to use Festival (Blackand Lenzo, 2000), which is an open-source speechsynthesis system, or Cepstral Swift (Cepstral2005), a commercial engine.
Finally, Kalliope alsosupports the SSML markup language.Other components.
The various componentsbriefly described above form the core of the Olym-pus dialog system framework.
Additional compo-nents have been created throughout thedevelopment of various systems, and, given themodularity of the architecture, can be easily re-used.
These include a telephony component, a textParsingPHOENIXRecognitionServerLang.
GenROSETTASynthesisKALLIOPE?SPHINX SPHINXSPHINXConfidenceHELIOSHUBText I/OTTYSERVERApplicationBack-endDialog.
Mgr.RAVENCLAWDate-TimeresolutionProcessMonitorUntil what timewould you likethe room?
{request end_time}Figure 1.
The Olympus dialog system reference architecture (a typical system)two p.m. [time=2pm] [time=2pm]/0.6534input-and-output interface, and a temporal refer-ence resolution agent that translates complex date-time expressions (including relative references,holidays, etc.)
into a canonical form.
Recently, aJabber interface was implemented to support inter-actions via the popular GoogleTalk internet mes-saging system.
A Skype speech client componentis also available.Data Analysis.
Last but not least, a variety oftools for logging, data processing and data ana-lytics are also available as part of the framework.These tools have been used for a wide variety oftasks ranging from system monitoring, to trendsanalysis, to training of internal models.A key characteristic shared by all the Olympuscomponents is the clear separation between do-main-independent programs and domain-specificresources.
This decoupling promotes reuse andlessens the system development effort.
To build anew system, one can focus simply on developingresources (e.g., language model, grammar, dialogtask specification, generation templates) withouthaving to do any programming.
On the other hand,since all components are open-source, any part ofthe system can be modified, for example to testnew algorithms or compare approaches.4 SystemsTo date, the Olympus framework has been used tosuccessfully build and deploy several spoken dia-log systems spanning different domains and inter-action types (see Table 1).
Given the limited space,we discuss only three of these systems in a bitmore detail: Let?s Go!, LARRI, and TeamTalk.More information about the other systems can befound in (RavenClaw-Olympus, 2007).4.1 Let?s Go!The Let?s Go!
Bus Information System (Raux et al2005; 2006) is a telephone-based spoken dialogsystem that provides access to bus schedules.
In-teraction with the system starts with an openprompt, followed by a system-directed phasewhere the user is asked the missing information.Each of the three or four pieces of informationprovided (origin, destination, time of travel, andoptional bus route) is explicitly confirmed.
Thesystem knows 12 bus routes, and about 1800 placenames.Originally developed as an in-lab research sys-tem, Let?s Go!
has been open to the general publicsince March, 2005.
Outside of business hours, callsto the bus company are transferred to Let?s Go!,providing a constant flow of genuine dialogs(about 40 calls per weeknight and 70 per weekendnight).
As of March, 2007, a corpus of about30,000 calls to the system has been collected andpartially transcribed and annotated.
In itself, thispublicly available corpus constitutes a unique re-source for the community.
In addition, the systemitself has been modified for research experiments(e.g., Raux et al, 2005, Bohus et al, 2006).
Be-tween-system studies have been conducted by run-ning several versions of the system in parallel andpicking one at random for every call.
We have re-cently opened this system to researchers from othergroups who wish to conduct their own experi-ments.4.2 LARRILARRI (Bohus and Rudnicky, 2002a) is a multi-modal system for support of maintenance and re-pair activities for F/A-18 aircraft mechanics.
Thesystem implements an Interactive Electronic Tech-nical Manual.LARRI integrates a graphical user interface foreasy visualization of dense technical information(e.g., instructions, schematics, video-streams) witha spoken dialog system that facilitates informationaccess and offers assistance throughout the execu-tion of procedural tasks.
The GUI is accessible viaa translucent head-worn display connected to awearable client computer.
A rotary mouse (dial)provides direct access to the GUI elements.After an initial log-in phase, LARRI guides theuser through the selected task, which consists of asequence of steps containing instructions, option-ally followed by verification questions.
Basic stepscan include animations or short video sequencesthat can be accessed by the user through the GUIor through spoken commands.
The user can alsotake the initiative and access the documentation,either via the GUI or by simple commands such as?go to step 15?
or ?show me the figure?.The Olympus architecture was easily adaptedfor this mobile and multi-modal setting.
The wear-able computer hosts audio input and output clients,as well as the graphical user interface.
The Galaxyhub architecture allows us to easily connect these35components to the rest of the system, which runson a separate server computer.
The rotary-mouseevents from the GUI are rendered as semantic in-puts and are sent to Helios which in turn forwardsthe corresponding messages to the dialog manager.4.3 TeamTalkTeamTalk (Harris et al, 2005) is a multi-modalinterface that facilitates communication between ahuman operator and a team of heterogeneous ro-bots, and is designed for a multi-robot-assistedtreasure-hunt domain.
The human operator usesspoken language in concert with pen-gestures on ashared live map to elicit support from teams of ro-bots.
This support comes in the forms of mappingunexplored areas, searching explored areas for ob-jects of interest, and leading the human to said ob-jects.
TeamTalk has been built as a fully functionalinterface to real robots, including the PioneerP2DX and the Segway RMP.
In addition, it caninterface with virtual robots within the high-fidelity USARSim (Balakirsky et al, 2006) simula-tion environment.
TeamTalk constitutes an excel-lent platform for multi-agent dialog research.To build TeamTalk, we had to address two chal-lenges to current architecture.
The multi-participant nature of the interaction required multi-ple dialog managers; the live map with pen-gestured references required a multi-modal integra-tion.
Again, the flexibility and transparency of theOlympus framework allowed for relatively simplesolutions to both of these challenges.
To accom-modate multi-participant dialog, each robot in thedomain is associated with its own RavenClaw-based dialog manager, but all robots share theother Olympus components: speech recognition,language understanding, language generation andspeech synthesis.
To accommodate the live mapGUI, a Galaxy server was built in Java that couldsend the user?s inputs to Helios and receive outputsfrom RavenClaw.5 ResearchThe Olympus framework, along with the systemsdeveloped using it, provides a robust basis for re-search in spoken language interfaces.
In this sec-tion, we briefly outline three current researchefforts supported by this architecture.
Informationabout other supported research can be found in(RavenClaw-Olympus, 2007).5.1 Error handlingA persistent and important problem in today?s spo-ken language interfaces is their lack of robustnesswhen faced with understanding errors.
This prob-lem stems from current limitations in speech rec-ognition, and appears across most domains andinteraction types.
In the last three years, we con-ducted research aimed at improving robustness inspoken language interfaces by: (1) endowing themwith the ability to accurately detect errors, (2) de-System name Domain / Description GenreRoomLine(Bohus and Rudnicky 2005)telephone-based system that provides support for conferenceroom reservation and scheduling within the School of Com-puter Science at CMU.information access (mixedinitiative)Let?s Go!
Public(Raux et al2005)telephone-based system that provides access to bus scheduleinformation in the greater Pittsburgh area.information access(system initiative)LARRI(Bohus and Rudnicky 2002)multi-modal system that provides assistance to F/A-18 aircraftpersonnel during maintenance tasks.multi-modal task guidanceand procedure browsingIntelligent ProcedureAssistant(Aist et al2002)early prototype for a multi-modal system aimed at providingguidance and support to the astronauts on the InternationalSpace Station during the execution of procedural tasks andchecklists.multi-modal task guidanceand procedure browsingTeamTalk(Harris et al2005)multi-participant spoken language command-and-control inter-face for a team of robots in the treasure-hunt domain.multi-participant command-and-controlVERA telephone-based taskable agent that can be instructed to de-liver messages to a third party and make wake-up calls.voice mail / message deliv-eryMadeleine text-based dialog system for medical diagnosis.
diagnosisConQuest(Bohus et al2007)telephone-based spoken dialog system that provides confer-ence schedule information.information access(mixed-initiative)RavenCalendar(Stenchikova et al2007).multimodal dialog system for managing personal calendarinformation, such as meetings, classes, appointments andreminders (uses Google Calendar as a back-end)information access andschedulingTable 1.
Olympus-based spoken dialog systems (shaded cells indicate deployed systems)36veloping a rich repertoire of error recovery strate-gies and (3) developing scalable, data-driven ap-proaches for building error recovery policies1.
Twoof the dialog systems from Table 1 (Let?s Go!
andRoomLine) have provided a realistic experimentalplatform for investigating these issues and evaluat-ing the proposed solutions.With respect to error detection, we have devel-oped tools for learning confidence annotationmodels by integrating information from multipleknowledge sources in the system (Bohus and Rud-nicky, 2002b).
Additionally, Bohus and Rudnicky(2006) proposed a data-driven approach for con-structing more accurate beliefs in spoken languageinterfaces by integrating information across multi-ple turns in the conversation.
Experiments with theRoomLine system showed that the proposed beliefupdating models led to significant improvements(equivalent with a 13.5% absolute reduction inWER) in both the effectiveness and the efficiencyof the interaction.With respect to error recovery strategies, wehave developed and evaluated a large set of strate-gies for handling misunderstandings and non-understandings (Bohus and Rudnicky, 2005).
Thestrategies are implemented in a task-decoupledmanner in the RavenClaw dialog managementframework.Finally, in (Bohus et al, 2006) we have pro-posed a novel online-learning based approach forbuilding error recovery policies over a large setof non-understanding recovery strategies.
An em-pirical evaluation conducted in the context of theLet?s Go!
system showed that the proposed ap-proach led to a 12.5% increase in the non-understanding recovery rate; this improvement wasattained in a relatively short (10-day) time period.The models, tools and strategies developedthroughout this research can and have been easilyreused in other Olympus-based systems.5.2 Multi-participant conversationConversational interfaces are generally built forone-on-one conversation.
This has been a workableassumption for telephone-based systems, and auseful one for many single-purpose applications.However this assumption will soon becomestrained as a growing collection of always-1A policy specifies how the system should choose betweendifferent recovery strategies at runtime.available agents (e.g., personal trainers, pedestrianguides, or calendar systems) and embodied agents(e.g., appliances and robots) feature spoken lan-guage interfaces.
When there are multiple activeagents that wish to engage in spoken dialog, newissues arise.
On the input side, the agents need tobe able to identify the addressee of any given userutterance.
On the output side, the agents need toaddress the problem of channel contention, i.e.,multiple participants speaking over each other.Two architectural solutions can be envisioned:(1) the agents share a single interface that under-stands multi-agent requirements, or (2) each agentuses its own interface and handles multi-participantbehavior.
Agents that provide different serviceshave different dialog requirements, and we believethis makes a centralized interface problematic.
Fur-thermore, the second solution better fits humancommunication behavior and therefore is likely tobe more natural and habitable.TeamTalk is a conversational system that fol-lows the second approach: each robot has its owndialog manager.
Processed user inputs are sent toall dialog managers in the system; each dialogmanager decides based on a simple algorithm(Harris et al, 2004) whether or not the current in-put is addressed to it.
If so, an action is taken.
Oth-erwise the input is ignored; it will be processed andresponded to by another robot.
On the output side,to address the channel contention problem, eachRavenClaw output message is augmented with in-formation about the identity of the robot that gen-erated it.
The shared synthesis component queuesthe messages and plays them back sequentiallywith the corresponding voice.We are currently looking into two additionalchallenges related to multi-participant dialog.
Weare interested in how to address groups and sub-groups in addition to individuals of a group, andwe are also interested in how to cope with multiplehumans in addition to multiple agents.
Some ex-periments investigating solutions to both of theseissues have been conducted.
Analysis of the resultsand refinements of these methods are ongoing.5.3 Timing and turn-takingWhile a lot of research has focused on higher lev-els of conversation such as natural language under-standing and dialog planning, low-level inter-actional phenomena such as turn-taking have not37received as much attention.
As a result, currentsystems either constrain the interaction to a rigidone-speaker-at-a-time style or expose themselvesto interactional problems such as inappropriatedelays, spurious interruptions, or turn over-taking(Raux et al, 2006).
To a large extent, these issuesstem from the fact that in common dialog architec-tures, including Olympus, the dialog managerworks asynchronously from the real world (i.e.,utterances and actions that are planned are as-sumed to be executed instantaneously).
This meansthat user barge-ins and backchannels are often in-terpreted in an incorrect context, which leads toconfusion, unexpected user behavior and potentialdialog breakdowns.
Additionally, dialog systems?low-level interactional behavior is generally theresult of ad-hoc rules encoded in different compo-nents that are not precisely coordinated.In order to investigate and resolve these is-sues, we are currently developing version 2 of theOlympus framework.
In addition to all the compo-nents described in this paper, Olympus 2 featuresan Interaction Manager which handles the precisetiming of events perceived from the real world(e.g., user utterances) and of system actions (e.g.,prompts).
By providing an interface between theactual conversation and the asynchronous dialogmanager, Olympus 2 allows a more reactive behav-ior without sacrificing the powerful dialog man-agement features offered by RavenClaw.
Olympus2 is designed so that current Olympus-based sys-tems can be upgraded with minimal effort.This novel architecture, initially deployed inthe Let?s Go system, will enable research on turn-taking and other low-level conversational phenom-ena.
Investigations within the context of other ex-isting systems, such as LARRI and TeamTalk, willuncover novel challenges and research directions.6 Discussion and conclusionThe primary goal of the Olympus framework is toenable research that leads to technological and sci-entific advances in spoken language interfaces.Olympus is however by no means a singular ef-fort.
Several other toolkits for research and devel-opment are available to the community.
Theydiffer on a number of dimensions, such as objec-tives, scientific underpinnings, as well as techno-logical and implementation aspects.
Severaltoolkits, both commercial, e.g., TellMe, BeVocal,and academic, e.g., Ariadne (2007), SpeechBuilder(Glass et al, 2004), and the CSLU toolkit (Cole,1999), are used for rapid development.
Some, e.g.,CSLU and SpeechBuilder, have also been used foreducational purposes.
And yet others, such asOlympus, GALATEEA (Kawamoto et al, 2002)and DIPPER (Bos et al, 2003) are primarily usedfor research.
Different toolkits rely on differenttheories and dialog representations: finite-state,slot-filling, plan-based, information state-update.Each toolkit balances tradeoffs between complex-ity, ease-of-use, control, robustness, flexibility, etc.We believe the strengths of the Olympusframework lie not only in its current components,but also in its open, transparent, and flexible na-ture.
As we have seen in the previous sections,these characteristics have allowed us to developand deploy practical, real-world systems operatingin a broad spectrum of domains.
Through thesesystems, Olympus provides an excellent basis forresearch on a wide variety of spoken dialog issues.The modular construction promotes the transferand reuse of research contributions across systems.While desirable, an in-depth understanding ofthe differences between all these toolkits remainsan open question.
We believe that an open ex-change of experiences and resources across toolkitswill create a better understanding of the currentstate-of-the-art, generate new ideas, and lead tobetter systems for everyone.
Towards this end, weare making the Olympus framework, as well as anumber of systems and dialog corpora, freelyavailable to the community.AcknowledgementsWe would like to thank all those who have broughtcontributions to the components underlying theOlympus dialog system framework.
Neither Olym-pus nor the dialog systems discussed in this paperwould have been possible without their help.
Weparticularly wish to thank Alan W Black for hiscontinued support and advice.
Work on Olympuscomponents and systems was supported in part byDARPA, under contract NBCH-D-03-0010, Boe-ing, under contract CMU-BA-GTA-1, and the USNational Science Foundation under grant number0208835.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe views of the funding agencies.38ReferencesAist, G., Dowding, J., Hockey, B.A., Rayner, M.,Hieronymus, J., Bohus, D., Boven, B., Blaylock, N.,Campana, E., Early, S., Gorrell, G., and Phan, S.,2003.
Talking through procedures: An intelligentSpace Station procedure assistant, in Proc.
of EACL-2003, Budapest, HungaryAriadne, 2007, The Ariadne web-site, as of January2007, http://www.opendialog.org/.Balakirsky, S., Scrapper, C., Carpin, S., and Lewis, M.2006.
UsarSim: providing a framework for multi-robot performance evaluation, in Proc.
of PerMIS.Black, A. and Lenzo, K., 2000.
Building Voices in theFestival Speech System, http://festvox.org/bsv/, 2000.Bohus, D., Grau Puerto, S., Huggins-Daines, D., Keri,V., Krishna, G., Kumar, K., Raux, A., Tomko, S.,2007.
Conquest ?
an Open-Source Dialog System forConferences, in Proc.
of HLT 2007, Rochester, USA.Bohus, D., Langner, B., Raux, A., Black, A., Eskenazi,M., Rudnicky, A.
2006.
Online Supervised Learningof Non-understanding Recovery Policies, in Proc.
ofSLT-2006, Aruba.Bohus, D., and Rudnicky, A.
2006.
A K-hypotheses +Other Belief Updating Model, in Proc.
of the AAAIWorkshop on Statistical and Empirical Methods inSpoken Dialogue Systems, 2006.Bohus, D., and Rudnicky, A.,  2005.
Sorry I didn?tCatch That: An Investigation of Non-understandingErrors and Recovery Strategies, in Proc.
of SIGdial-2005, Lisbon, Portugal.Bohus, D., and Rudnicky, A., 2003.
RavenClaw: DialogManagement Using Hierarchical Task Decomposi-tion and an Expectation Agenda, in Proc.
of Eu-rospeech 2003, Geneva, Switzerland.Bohus, D., and Rudnicky, A., 2002a.
LARRI: A Lan-guage-based Maintenance and Repair Assistant, inProc.
of IDS-2002, Kloster Irsee, Germany.Bohus, D., and Rudnicky, A., 2002b.
Integrating Multi-ple Knowledge Sources in the CMU CommunicatorDialog System, Technical Report CMU-CS-02-190.Bos, J., Klein, E., Lemon, O., and Oka, T., 2003.DIPPER: Description and Formalisation of an In-formation-State Update Dialogue System Architec-ture, in Proc.
of SIGdial-2003, Sapporo, JapanCepstral, LLC, 2005.
SwiftTM: Small Footprint Text-to-Speech Synthesizer, http://www.cepstral.com.Cole, R., 1999.
Tools for Research and Education inSpeech Science, in Proc.
of the International Confer-ence of Phonetic Sciences, San Francisco, USA.Glass, J., Weinstein, E., Cyphers, S., Polifroni, J., 2004.A Framework for Developing Conversational Inter-faces, in Proc.
of CADUI, Funchal, Portugal.Harris, T. K., Banerjee, S., Rudnicky, A., Sison, J.Bodine, K., and Black, A.
2004.
A Research Platformfor Multi-Agent Dialogue Dynamics, in Proc.
of TheIEEE International Workshop on Robotics and Hu-man Interactive Communications, Kurashiki, Japan.Harris, T. K., Banerjee, S., Rudnicky, A.
2005.
Hetero-genous Multi-Robot Dialogues for Search Tasks, inAAAI Spring Symposium: Dialogical Robots, PaloAlto, California.Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee,K.-F. and Rosenfeld, R., 1992.
The SPHINX-IISpeech Recognition System: an overview, in Com-puter Speech and Language, 7(2), pp 137-148, 1992.Kawamoto, S.,  Shimodaira, H., Nitta, T., Nishimoto,T., Nakamura, S., Itou, K., Morishima, S., Yotsukura,T., Kai, A., Lee, A., Yamashita, Y., Kobayashi, T.,Tokuda, K., Hirose, K., Minematsu, N., Yamada, A.,Den, Y., Utsuro, T., and Sagayama, S., 2002.
Open-source software for developing anthropomorphicspoken dialog agent, in Proc.
of PRICAI-02, Interna-tional Workshop on Lifelike Animated Agents.Raux, A., Langner, B., Bohus, D., Black, A., and Eske-nazi, M.  2005, Let's Go Public!
Taking a SpokenDialog System to the Real World, in Proc.
of Inter-speech 2005, Lisbon, Portugal.Raux, A., Bohus, D., Langner, B., Black, A., and Eske-nazi, M. 2006 Doing Research on a Deployed SpokenDialogue System: One Year of Let's Go!
Experience,in Proc.
of Interspeech 2006, Pittsburgh, USA.RavenClaw-Olympus web page, as of January 2007:http://www.ravenclaw-olympus.org/.Rudnicky, A., Thayer, E., Constantinides, P., Tchou, C.,Shern, R., Lenzo, K., Xu W., and Oh, A., 1999.
Cre-ating natural dialogs in the Carnegie Mellon Com-municator system, in Proc.
of Eurospeech 1999.Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., andZue V. 1998 Galaxy-II: A reference architecture forconversational system development, in Proc.
ofICSLP98, Sydney, Australia.Stenchikova, S., Mucha, B., Hoffman, S., Stent, A.,2007.
RavenCalendar: A Multimodal Dialog Systemfor Managing A Personal Calendar, in Proc.
of HLT2007, Rochester, USA.Ward, W., and Issar, S., 1994.
Recent improvements inthe CMU spoken language understanding system, inProc.
of the ARPA Human Language TechnologyWorkshop, pages 213?216, Plainsboro, NJ.39
