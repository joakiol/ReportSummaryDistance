Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861?870,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAdaptive HTER Estimation for Document-Specific MT Post-EditingFei Huang?Facebook Inc.Menlo Park, CAfeihuang@fb.comJian-Ming Xu Abraham IttycheriahIBM T.J. Watson Research CenterYorktown Heights, NY{jianxu, abei, roukos}@us.ibm.comSalim RoukosAbstractWe present an adaptive translation qual-ity estimation (QE) method to predictthe human-targeted translation error rate(HTER) for a document-specific machinetranslation model.
We first introduce fea-tures derived internal to the translation de-coding process as well as externally fromthe source sentence analysis.
We showthe effectiveness of such features in bothclassification and regression of MT qual-ity.
By dynamically training the QE modelfor the document-specific MT model, weare able to achieve consistency and pre-diction quality across multiple documents,demonstrated by the higher correlation co-efficient and F-scores in finding Good sen-tences.
Additionally, the proposed methodis applied to IBM English-to-Japanese MTpost editing field study and we observestrong correlation with human preference,with a 10% increase in human translators?productivity.1 IntroductionMachine translation (MT) systems suffer from aninconsistent and unstable translation quality.
De-pending on the difficulty of the input sentences(sentence length, OOV words, complex sentencestructures and the coverage of the MT system?straining data), some translation outputs can be per-fect, while others are ungrammatical, missing im-portant words or even totally garbled.
As a result,users do not know whether they can trust the trans-lation output unless they spend time to analyze?This work was done when the author was with IBM Re-search.the MT output.
This shortcoming is one of themain obstacles for the adoption of MT systems,especially in machine assisted human translation:MT post-editing, where human translators havean option to edit MT proposals or translate fromscratch.
It has been observed that human trans-lators often discard MT proposals even if someare very accurate.
If MT proposals are used prop-erly, post-editing can increase translators produc-tivity and lead to significant cost savings.
There-fore, it is beneficial to provide MT confidence es-timation, to help the translators to decide whetherto accept MT proposals, making minor modifica-tions on MT proposals when the quality is highor translating from scratching when the quality islow.
This will save the time of reading and parsinglow quality MT and improve user experience.In this paper we propose an adaptive qual-ity estimation that predicts sentence-level human-targeted translation error rate (HTER) (Snover etal., 2006) for a document-specific MT post-editingsystem.
HTER is an ideal quality measurementfor MT post editing since the reference is ob-tained from human correction of the MT output.Document-specific MT model is an MT model thatis specifically built for the given input document.It is demonstrated in (Roukos et al, 2012) thatdocument-specific MT models significantly im-prove the translation quality.
However, this raisestwo issues for quality estimation.
First, existingapproaches to MT quality estimation rely on lex-ical and syntactical features defined over parallelsentence pairs, which includes source sentences,MT outputs and references, and translation models(Blatz et al, 2004; Ueffing and Ney, 2007; Spe-cia et al, 2009a; Xiong et al, 2010; Soricut andEchihabi, 2010a; Bach et al, 2011).
Therefore,when the MT quality estimation model is trained,861it can not be adapted to provide accurate estimateson the outputs of document-specific MT models.Second, the MT quality estimation might be in-consistent across different document-specific MTmodels, thus the confidence score is unreliable andnot very helpful to users.In contrast to traditional static MT quality es-timation methods, our approach not only trainsthe MT quality estimator dynamically for eachdocument-specific MT model to obtain higher pre-diction accuracy, but also achieves consistencyover different document-specific MT models.
Theexperiments show that our MT quality estima-tion is highly correlated with human judgmentand helps translators to increase the MT proposaladoption rate in post-editing.We will review related work on MT quality es-timation in section 2.
In section 3 we will intro-duce the document-specific MT system built forpost-editing.
We describe the static quality estima-tion method in section 4, and propose the adaptivequality estimation method in section 5.
In section6 we demonstrate the improvement of MT qualityestimation with our method, followed by discus-sion and conclusion in section 7.2 Related WorkThere has been a long history of study in con-fidence estimation of machine translation.
Thework of (Blatz et al, 2004) is among the bestknown study of sentence and word level featuresfor translation error prediction.
Along this line ofresearch, improvements can be obtained by incor-porating more features as shown in (Quirk, 2004;Sanchis et al, 2007; Raybaud et al, 2009; Speciaet al, 2009b).
Soricut and Echihabi (2010b) pro-posed various regression models to predict the ex-pected BLEU score of a given sentence translationhypothesis.
Ueffing and Hey (2007) introducedword posterior probabilities (WPP) features andapplied them in the n-best list reranking.
Targetpart-of-speech and null dependency link are ex-ploited in a MaxEnt classifier to improve the MTquality estimation (Xiong et al, 2010).Quality estimation focusing on MT post-editinghas been an active research topic, especially afterthe WMT 2012 (Callison-Burch et al, 2012) andWMT2013 (Bojar et al, 2013) workshops withthe ?Quality Estimation?
shared task.
Bic?ici etal.
(2013) proposes a number of features mea-suring the similarity of the source sentence to thesource side of the MT training corpus, which,combined with features from translation output,achieved significantly superior performance in theMT QE evaluation.
Felice and Specia (2012) in-vestigates the impact of a large set of linguisti-cally inspired features on quality estimation accu-racy, which are not able to outperform the shal-lower features based on word statistics.
Gonz?alez-Rubio et al (2013) proposed a principled methodfor performing regression for quality estimationusing dimensionality reduction techniques basedon partial least squares regression.
Given the fea-ture redundancy in MT QE, their approach is ableto improve prediction accuracy while significantlyreducing the size of the feature sets.3 Document-specific MT SystemIn our MT post-editing setup, we are given docu-ments in the domain of software manuals, techni-cal outlook or customer support materials.
Eachtranslation request comes as a document with sev-eral thousand sentences, focusing on a specifictopic, such as the user manual of some software.The input documents are automatically seg-mented into sentences, which are also called seg-ments.
Thus in the rest of the paper we will usesentences and segments interchangeably.
Our par-allel corpora includes tens of millions of sentencepairs covering a wide range of topics.
Buildinga general MT system using all the parallel datanot only produces a huge translation model (unlesswith very aggressive pruning), the performance onthe given input document is suboptimal due to theunwanted dominance of out-of-domain data.
Pastresearch suggests using weighted sentences or cor-pora for domain adaptation (Lu et al, 2007; Mat-soukas et al, 2009; Foster et al, 2010).
Herewe adopt the same strategy, building a document-specific translation model for each input docu-ment.The document-specific system is built based onsub-sampling: from the parallel corpora we se-lect sentence pairs that are the most similar tothe sentences from the input document, then buildthe MT system with the sub-sampled sentencepairs.
The similarity is defined as the number ofn-grams that appear in both source sentences, di-vided by the input sentence?s length, with higherweights assigned to longer n-grams.
From theextracted sentence pairs, we utilize the standardpipeline in SMT system building: word align-862Figure 1: Adaptive QE for document-specific MT system.ment (HMM (Vogel et al, 1996) and MaxEnt (It-tycheriah and Roukos, 2005) alignment models,phrase pair extraction, MT model training (Itty-cheriah and Roukos, 2007) and LM model train-ing.
The top region within the dashed line in Fig-ure 1 shows the overall system built pipeline.3.1 MT DecoderThe MT decoder (Ittycheriah and Roukos, 2007)employed in our study extracts various features(source words, morphemes and POS tags, targetwords and POS tags, etc.)
with their weightstrained in a maximum entropy framework.
Thesefeatures are combined with other features used ina typical phrase-based translation system.
Alto-gether the decoder incorporates 17 features withweights estimated by PRO (Hopkins and May,2011) in the decoding process, and achievesstate-of-the-art translation performance in vari-ous Arabic-English translation evaluations (NISTMT2008, GALE and BOLT projects).4 Static MT Quality EstimationMT quality estimation is typically formulated asa prediction problem: estimating the confidencescore or translation error rate of the translated sen-tences or documents based on a set of features.
Inthis work, we adopt HTER in (Snover et al, 2006)as our prediction output.
HTER measures the per-centage of insertions, deletions, substitutions andshifts needed to correct the MT outputs.
In therest of the paper, we use TER and HTER inter-changably.In this section we will first introduce the set offeatures, and then discuss MT QE problem fromclassification and regression point of views.4.1 Features for MT QEThe features for quality estimation should reflectthe complexity of the source sentence and the de-coding process.
Therefore we conduct syntacticanalysis on the source sentences, extract featuresfrom the decoding process and select the follow-ing 26 features:?
17 decoding features, including phrasetranslation probabilities (source-to-target andtarget-to-source), word translation probabil-ities (also in both directions), maxent prob-abilities1, word count, phrase count, distor-1The maxent probability is the translation probability863tion probabilities, as well as a set of languagemodel scores.?
Sentence length, i.e., the number of words inthe source sentence.?
Source sentence syntactic features, includingthe number of noun phrases, verb phrases,adjective phrases, adverb phrases, as in-spired by (Green et al, 2013).?
The length of verb phrases, because verbs aretypically the roots in dependency structureand they have more varieties during transla-tion.?
The maximum length of source phrases inthe final translation, since longer matchingsource phrase indicates better coverage of theinput sentence with possibly better transla-tions.?
The number of phrase pairs with high fuzzymatch (FM) score.
The high FM phrases areselected from sentence pairs which are clos-est in terms of n-gram overlap to the inputsentence.
These sentences are often found inprevious translations of the software manual,and thus are very helpful for translating thecurrent sentence.?
The average translation probability of thephrase translation pairs in the final transla-tion, which provides the overall translationquality on the phrase level.The first 17 features come from the decod-ing process, which are called ?decoding features?.The remaining 9 features not related to the de-coder are called ?external features?.
To evaluatethe effectiveness of the proposed features, we trainvarious classifiers with different feature configura-tions to predict whether a translation output is use-ful (with lower TER) as described in the followingsection.4.2 MT QE as ClassificationPredicting TER with various input features canbe treated as a regression problem.
However forthe post-editing task, we argue that it could alsobe cast as a classification problem: MT systemderived from a Maximum Entropy translation model (Itty-cheriah and Roukos, 2005).Configuration Training set Test setBaseline (All negative) 80% 77%17 decoding features only 89% 79%9 external features only 85% 81%total 26 features 92% 83%Table 1: QE classification accuracy with differentfeature configurationsusers (including the translators) are often inter-ested to know whether a given translation is rea-sonably good or not.
If useful, they can quicklylook through the translation and make minor mod-ifications.
On the other hand, they will just skipreading and parsing the bad translation, and preferto translate by themselves from scratch.
Thereforewe also develop algorithms that classify the trans-lation at different levels, depending on whether theTER is less than a given threshold.
In our experi-ments, we set TER=0.1 as the threshold.We randomly select one input document with2067 sentences for the experiment.
We builda document-specific MT system to translate thisdocument, then ask human translator to correctthe translation output.
We compute TER for eachsentence using the human correction as the refer-ence.
The TER of the whole document is 0.31,which means about 30% errors should be cor-rected.
In the classification task, our goal is to pre-dict whether a sentence is a Good translation (withTER ?
0.1), and label them for human correction.We adopt a decision tree-based classifier, experi-menting with different feature configurations.
Weselect the top 1867 sentences for training and thebottom 200 sentences for test.
In the test set, thereare 46 sentences with TER ?
0.1.
Table 1 showsthe classification accuracy.First we can see that as the overall TER isaround 0.3, predicting all the sentences being neg-ative already has a strong baseline: 77%.
How-ever this is not helpful for the human translators,because that means they have to translate everysentence from scratch, and consequently there isno productivity gain from MT post-editing.
If weonly use the 17 decoding features, it improves theclassification accuracy by 9% on the training set,but only 2% on the test set.
This is probably due tothe overfitting when training the decision tree clas-sifier.
While using the 7 external features, the gainon training set is less but the gain on the test set864is greater (4% improvement), because the trans-lation output is generated based on the log-linearcombination of these decoding features, which arebiased towards the final translations.
The exter-nal features capture the syntactic structure of thesource sentence, as well as the coverage of thetraining data with regard to the input sentence,which are good indicators of the translation qual-ity.
Combining both the decoding features and theexternal features, we observed the best accuracyon both the training and test set.
We will use thecombined 26 features in the following work.4.3 MT QE as RegressionFor the QE regression task, we predict the TER foreach sentence translation using the above 26 fea-tures.
We experiment with several classifiers: lin-ear regression model, decision tree based regres-sion model and SVM model.
With the same train-ing and test data set up, we predict the TER foreach sentence in the test set, and compute the cor-relation coefficient (r) and root mean square error(RMSE).
Our experiments show that the decisiontree-based regression model obtains the highestcorrelation coefficients (0.53) and lowest RMSE(0.23) in both the training and test sets.
We willuse this model for the adaptive MT QE in the fol-lowing work.5 Adaptive MT Quality EstimationThe above QE regression model is trained on aportion of the sentences from the input document,and evaluated on the remaining sentences from thesame document.
One would like to know whetherthe trained model can achieve consistent TER pre-diction accuracy on other documents.
When weuse the cross-document models for prediction, thecorrelation is significantly worse (the details arediscussed in section 6.1).
Therefore it is neces-sary to build a QE regression model that?s robustto different document-specific translation models.To deal with this problem, we propose this adap-tive MT QE method described below.Our proposed method is as follows: we select afixed set of sentence pairs (Sq, Rq) to train the QEmodel.
The source side of the QE training dataSqis combined with the input document SdforMT system training data subsampling.
Once thedocument-specific MT system is trained, we use itto translate both the input document and the sourceQE training data, obtaining the translation TdandFigure 2: Correlation coefficient r between pre-dicted TER (x-axis) and true TER (y-axis) for QEmodels trained from the same document (top fig-ure) or different document (bottom figure).Tq.
We compute the TER of Tqusing Rqas thereference, and train a QE regression model withthe 26 features proposed in section 4.1.
Then weuse this document-specific QE model to predict theTER of the document translation Td.
As the QEmodel is adaptively re-trained for each document-specific MT system, its prediction is more accurateand consistent.
Figure 1 shows the flow of our MTsystem with the adaptive QE training integrated aspart of the built.6 ExperimentsIn this section, we first discuss experiments thatcompare adaptive QE method and static QEmethod on a few documents, and then presentresults we obtained after deploying the adaptiveQE method in an English-to-Japanese MT Post-Editing project.
As mentioned before, the mainmotivation for us to develop MT QE classificationscheme is that translators often discard good MTproposals and translate the segments from scratch.We would like to provide translators with someguidance on reasonably good MT proposals?thesentences with low TERs?to help them increasethe leverage on MT proposals to achieve improvedproductivity.8656.1 Evaluation on Test SetOur experiment and evaluation is conducted overthree documents, each with about 2000 segments.We first build document-specific MT model foreach document, then ask human translators to cor-rect the MT outputs and obtain the reference trans-lation.
In a typical MT QE scenario, the QE modelis pre-trained and applied to various MT outputs,even though the QE training data and MT out-puts are generated from different translation mod-els.
To evaluate whether such model mismatchmatters, we compare the cross-model QE with thesame-model QE, where the QE training data andthe MT outputs are generated from the same MTmodel.We select one document LZA with 2067 sen-tences.
We use the first 1867 sentences to train thestatic QE model and the remaining 200 sentencesare used as test set for TER prediction.
We com-pute the correlation coefficient (r) between eachpredicted TER and true TER, as shown in Figure2.
We find that the TER predictions are reason-ably correct when the training and test sentencesare from the same MT model (the top figure), withcorrelation coefficients around 0.5.
For the cross-model QE, we train a static QE model with 1867sentences from another document RTW, and use itto predict the TER of the same 200 sentences fromdocument LZA (the bottom figure).
We observesignificant degradation of correlation coefficient,dropping from 0.5 to 0.1.
This degradation andunstable nature is the prime motivation to developa more robust MT quality estimation model.We select 1700 sentences from multiple pre-viously translated documents as the QE trainingdata, which are independent of the test documents.We train the static QE model with this training set,including the source sentences, references and MToutputs (from multiple translation models).
Totrain the adaptive QE model for each test docu-ment, we build a translation model whose subsam-pling data includes source sentences from both thetest document and the QE training data.
We trans-late the QE source sentences with this newly builtMT model, and the translation output is used totrain the QE model specific to each test document.We compare these two QE models on three doc-uments, LZA, RTW and WC7, measuring r andRMSE for each QE model.
The result is shownin Table 2.
We find that the adaptive QE modeldemonstrates higher r and lower RMSE than thestatic QE model for all the test documents.Besides the general correlation with humanjudgment, we particularly focus on those reason-ably good translations, i.e., the sentences with lowTERs which can help improve the translator?s pro-ductivity most.
Here we report the precision, re-call and F-score of finding such ?Good?
sentences(with TER ?
0.1) on the three documents in Ta-ble 3.
Again, the adaptive QE model produceshigher recall, mostly higher precision, and signif-icantly improved F-score.
The overall F-score ofthe adaptive QE model is 0.282.
Compared withthe static QE model?s 0.17 F-score, this is rela-tively 64% improvement.In the adaptive QE model, the source side QEtraining data is included in the subsampling pro-cess to build the document-specific MT model.
Itwould be interesting to know whether this processwill negatively affect the MT quality.
We evaluatethe TER of MT outputs with and without the adap-tive QE training on the same three documents.
Asseen in Table 4, we do not notice translation qual-ity degradation.
Instead, we observe slightly im-provement on two document, with TERs reductionby 0.1-0.4 pt.
As our MT model training data in-clude proprietary data, the MT performance is sig-nificantly better than publicly available MT soft-ware.6.2 Impact on Human TranslatorsWe apply the proposed adaptive QE model tolarge scale English-to-Japanese MT Post-Editingproject on 36 documents with 562K words.
EachEnglish sentence can be categorized into 3 classes:?
Exact Match (EM): the source sentence iscompletely covered in the bilingual trainingcorpora thus the corresponding target sen-tence is returned as the translation;?
Fuzzy Match (FM): the source sentence issimilar to some sentence in the training data(similarity measured by string editing dis-tance), the corresponding fuzzy match targetsentence (FM proposal) as well as the MTtranslation output (MT proposal) are returnedfor human translators to select and correct;?
No Proposal (NP): there is no close matchsource sentences in the training data (the FM2The adaptive QE model obtains much higher F-score(80%) on the rest of the sentences (with TER > 0.1).866Document LZA RTW WC7Num.
of Sents 2067 2003 2405r ?
RMSE ?
r ?
RMSE ?
r ?
RMSE ?Static QE 0.10 0.38 0.40 0.32 0.13 0.36Adaptive QE 0.58 0.23 0.61 0.22 0.47 0.20Table 2: QE regression with static and adaptive modelsDocument LZA RTW WC7Num.
of Sents 2067 2003 2405P/R/F-score P/R/F-score P/R/F-scoreStatic QE 0.73/0.08/0.14 0.69/ 0.11/ 0.19 0.74/ 0.10/ 0.18Adaptive QE 0.69/0.14/0.24 0.84/ 0.16/ 0.26 0.80/ 0.23/ 0.35Table 3: Performance on predicting Good sentences with static and adaptive modelssimilarity score of 70% is used as the thresh-old), therefore only the MT output is re-turned.EM sentences are excluded from the study be-cause in general they do not require editing.
Wefocus on the FM and NP sentences3.
In Table 5we present the precision, recall and F-score of the?Good?
sentences in the FM and NP categories,similar to those shown in Table 3.
We consistentlyobserve higher performance on the FM sentences,in terms of precision, recall and F-score.
This isexpected because these sentences are well coveredin the training data.
The overall F-score is in linewith the test set results shown in Table 3.We are also interested to know whether the pro-posed adaptive QE method is helpful to humantranslators in the MT post-editing task.
Based onthe TERs predicted by the adaptive QE model, weassign each MT proposal with a confidence label:High (0 ?
TER ?
0.2), Medium (0.2 < TER ?0.3), or Low (TER > 0.3).
We present the MT pro-posals with confidence labels to human translators,then measure the percentage of sentences whoseMT proposals are used.
From Table 6 and 7,we can see that sentences with High and Mediumconfidence labels are more frequently used by thetranslators than those with Low labels, for both theFM and NP categories.
The MT usage for the FMcategory is less than that for the NP category be-cause translators can choose FM proposals insteadof the MT proposals for correction.We also measure the translator?s productivitygain for MT proposals with different confidence3The word count distribution of EM, FM and NP is 21%,38% and 41%, respectively.Document LZA RTW WC7TER-Baseline 30.81 30.74 29.96TER-with Adaptive QE 30.69 30.78 29.56Table 4: MT Quality with and without AdaptiveQE measured by TERlabels.
The productivity of a translator is definedas the number of source words translated per unittime.
The post editing tool, IBM TranslationMan-ager, records the time that a translator spends ona segment and computes the number of charactersthat a translator types on the segment so that wecan compute how many words the translator hasfinished in a given time.We choose the overall productivity of NP0 asthe base unit 1, where there is no proposal presentsand the translator has to translate the segmentsfrom scratch.
Measured with this unit, for exam-ple, the overall productivity of FM0 being 1.14implies a relative gain of 14% over that of NP0,which demonstrates the effectiveness of FM pro-posals.Table 6 and 7 also show the productivity gainon sentences with High, Medium and Low labelsfrom FM and NP categories.
Again, the produc-tivity gain is consistent with the confidence labelsfrom the adaptive QE model?s prediction.
Theoverall productivity gain with confidence-labeledMT proposals is about 10% (comparing FM1 vs.FM0 and NP1 vs. NP0).
These results clearlydemonstrate the effectiveness of the adaptive QEmodel in aiding the translators to make use of MTproposals and improve productivity.867Category Class FM usage MT usage ProductivityHigh 33% 34% 1.35FM1 Medium 47% 18% 1.21Low 60% 8% 1.20Overall 45% 21% 1.26High 53% - 1.12FM0 Medium 64% - 1.14Low 67% - 1.16Overall 59% - 1.14Table 6: MT proposal usage and productivity gain in FM category.In FM1, both Fuzzy Match and MT proposals present.
In control class FM0, only Fuzzy Match proposalspresent, and therefore, MT usage is not available for FM0.
Strong correlation is observed betweenpredicted ?High?
, ?Medium?
and ?Low?
sentences with MT usage and post editing productivity.Category Class MT usage ProductivityHigh 50% 1.25NP1 Medium 42% 1.08Low 27% 1.00Overall 38% 1.09High - 1.08NP0 Medium - 1.00Low - 0.96Overall - 1.00Table 7: MT proposal usage and productivity gain in NP category.In NP1, MT is the only proposal available, while in control NP0, there presents no proposal at all andthe translator has to translate from scratch.
Strong correlation is observed between predicted ?High?
,?Medium?
and ?Low?
sentences with MT usage and post editing productivity868Type Precision Recall F-scoreFM 0.71 0.23 0.35NP 0.67 0.18 0.29Overall 0.69 0.21 0.32Table 5: Performance on predicting Good sen-tences (TER ?
0.1) by adaptive QE model7 Discussion and ConclusionIn this paper we proposed a method to adaptivelytrain a quality estimation model for document-specific MT post editing.
With the 26 pro-posed features derived from decoding process andsource sentence syntactic analysis, the proposedQE model achieved better TER prediction, highercorrelation with human correction of MT outputand higher F-score in finding good translations.The proposed adaptive QE model is deployed toa large scale English-to-Japanese MT post edit-ing project, showing strong correlation with hu-man preference and leading to about 10% gain inhuman translator productivity.The training data for QE model can be selectedindependent of the input document.
With suchfixed QE training data, it is possible to measure theconsistency of the trained QE models, and to al-low the sanity check of the document-specific MTmodels.
However, adding such data in the sub-sampling process extracts more bilingual data forbuilding the MT models, which slightly increasethe model building time but increased the transla-tion quality.
Another option is to select the sen-tence pairs from the MT system subsampled train-ing data, which is more similar to the input docu-ment thus the trained QE model could be a bettermatch to the input document.
However, the QEmodel training data is no longer constant.
Themodel consistency is no longer guaranteed, andthe QE training data must be removed from theMT system training data to avoid data contamina-tion.ReferencesNguyen Bach, Fei Huang, and Yaser Al-Onaizan.2011.
Goodness: A method for measuring machinetranslation confidence.
In ACL, pages 211?219.Ergun Bic?ici, Declan Groves, and Josef van Genabith.2013.
Predicting sentence translation quality usingextrinsic and language independent features.
Ma-chine Translation.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2004.
Confidence es-timation for machine translation.
In Proceedings ofthe 20th international conference on ComputationalLinguistics, COLING ?04, Stroudsburg, PA, USA.Association for Computational Linguistics.Ondrej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut,and Lucia Specia.
2013.
Findings of the 2013Workshop on Statistical Machine Translation.
InEighth Workshop on Statistical Machine Transla-tion, WMT-2013, pages 1?44, Sofia, Bulgaria.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Seventh Workshop on Statis-tical Machine Translation, pages 10?51, Montr?eal,Canada.Mariano Felice and Lucia Specia.
2012.
Linguisticfeatures for quality estimation.
In Seventh Workshopon Statistical Machine Translation, pages 96?103,Montr?eal, Canada.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In Proceed-ings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?10,pages 451?459, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Jes?us Gonz?alez-Rubio, Jose Ram?on Navarro-Cerd?an,and Francisco Casacuberta.
2013.
Dimensionalityreduction methods for machine translation qualityestimation.
Machine Translation, 27(3-4):281?301.Spence Green, Jeffrey Heer, and Christopher D. Man-ning.
2013.
The efficacy of human post-editing forlanguage translation.
In Proceedings of the SIGCHIConference on Human Factors in Computing Sys-tems, CHI ?13, pages 439?448, New York, NY,USA.
ACM.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1352?1362, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Abraham Ittycheriah and Salim Roukos.
2005.
Amaximum entropy word aligner for arabic-englishmachine translation.
In In Proceedings of HLT-EMNLP, pages 89?96.Abraham Ittycheriah and Salim Roukos.
2007.
Directtranslation model 2.
In In HLT-NAACL 2007: MainConference, pages 57?64.Yajuan Lu, Jin Huang, and Qun Liu.
2007.
Improv-ing statistical machine translation performance by869training data selection and optimization.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 343?350, Prague, Czech Republic,June.
Association for Computational Linguistics.Spyros Matsoukas, Antti-Veikko I. Rosti, and BingZhang.
2009.
Discriminative corpus weight es-timation for machine translation.
In Proceedingsof the 2009 Conference on Empirical Methods inNatural Language Processing: Volume 2 - Volume2, EMNLP ?09, pages 708?717, Stroudsburg, PA,USA.
Association for Computational Linguistics.Christopher B. Quirk.
2004.
Training a sentence-levelmachine translation confidence measure.
In In Pro-ceedings of LREC.Sylvain Raybaud, Caroline Lavecchia, David Langlois,and Kamel Sma??li.
2009.
New confidence mea-sures for statistical machine translation.
CoRR,abs/0902.1033.Salim Roukos, Abraham Ittycheriah, and Jian-MingXu.
2012.
Document-specific statistical machinetranslation for improving human translation produc-tivity.
In Proceedings of the 13th international con-ference on Computational Linguistics and Intelli-gent Text Processing - Volume Part II, CICLing?12,pages 25?39, Berlin, Heidelberg.
Springer-Verlag.Alberto Sanchis, Alfons Juan, Enrique Vidal, and De-partament De Sistemes Informtics.
2007.
Estima-tion of confidence measures for machine translation.In In Procedings of Machine Translation Summit XI.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof translation edit rate with targeted human annota-tion.
In In Proceedings of Association for MachineTranslation in the Americas, pages 223?231.Radu Soricut and Abdessamad Echihabi.
2010a.Trustrank: inducing trust in automatic translationsvia ranking.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, ACL ?10, pages 612?621, Stroudsburg, PA,USA.
Association for Computational Linguistics.Radu Soricut and Abdessamad Echihabi.
2010b.Trustrank: Inducing trust in automatic translationsvia ranking.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 612?621.
Association for Computa-tional Linguistics.Lucia Specia, Craig Saunders, Marco Turchi, ZhuoranWang, and John Shawe-taylor.
2009a.
Improvingthe confidence of machine translation quality esti-mates.
In In Proceedings of MT Summit XII.Lucia Specia, Marco Turchi, Zhuoran Wang, JohnShawe-Taylor, and Craig Saunders.
2009b.
Improv-ing the confidence of machine translation quality es-timates.Nicola Ueffing and Hermann Ney.
2007.
Word-level confidence estimation for machine translation.Computational Linguistics, 33(1):9?40.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based word alignment in statisticaltranslation.
In Proceedings of the 16th Conferenceon Computational Linguistics - Volume 2, COLING?96, pages 836?841, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Deyi Xiong, Min Zhang, and Haizhou Li.
2010.
Er-ror detection for statistical machine translation usinglinguistic features.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 604?611, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.870
