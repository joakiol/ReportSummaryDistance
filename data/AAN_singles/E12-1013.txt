Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120?129,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsComputing Lattice BLEU Oracle Scores for Machine TranslationArtem Sokolov Guillaume WisniewskiLIMSI-CNRS & Univ.
Paris SudBP-133, 91 403 Orsay, France{firstname.lastname}@limsi.frFranc?ois YvonAbstractThe search space of Phrase-Based Statisti-cal Machine Translation (PBSMT) systemscan be represented under the form of a di-rected acyclic graph (lattice).
The qualityof this search space can thus be evaluatedby computing the best achievable hypoth-esis in the lattice, the so-called oracle hy-pothesis.
For common SMT metrics, thisproblem is however NP-hard and can onlybe solved using heuristics.
In this work,we present two new methods for efficientlycomputing BLEU oracles on lattices: thefirst one is based on a linear approximationof the corpus BLEU score and is solved us-ing the FST formalism; the second one re-lies on integer linear programming formu-lation and is solved directly and using theLagrangian relaxation framework.
Thesenew decoders are positively evaluated andcompared with several alternatives from theliterature for three language pairs, using lat-tices produced by two PBSMT systems.1 IntroductionThe search space of Phrase-Based Statistical Ma-chine Translation (PBSMT) systems has the formof a very large directed acyclic graph.
In severalsoftwares, an approximation of this search spacecan be outputted, either as a n-best list contain-ing the n top hypotheses found by the decoder, oras a phrase or word graph (lattice) which com-pactly encodes those hypotheses that have sur-vived search space pruning.
Lattices usually con-tain much more hypotheses than n-best lists andbetter approximate the search space.Exploring the PBSMT search space is one ofthe few means to perform diagnostic analysis andto better understand the behavior of the system(Turchi et al 2008; Auli et al 2009).
Usefuldiagnostics are, for instance, provided by look-ing at the best (oracle) hypotheses contained inthe search space, i.e, those hypotheses that havethe highest quality score with respect to one orseveral references.
Such oracle hypotheses canbe used for failure analysis and to better under-stand the bottlenecks of existing translation sys-tems (Wisniewski et al 2010).
Indeed, the in-ability to faithfully reproduce reference transla-tions can have many causes, such as scantinessof the translation table, insufficient expressivenessof reordering models, inadequate scoring func-tion, non-literal references, over-pruned lattices,etc.
Oracle decoding has several other applica-tions: for instance, in (Liang et al 2006; Chi-ang et al 2008) it is used as a work-around tothe problem of non-reachability of the referencein discriminative training of MT systems.
Latticereranking (Li and Khudanpur, 2009), a promisingway to improve MT systems, also relies on oracledecoding to build the training data for a rerankingalgorithm.For sentence level metrics, finding oracle hy-potheses in n-best lists is a simple issue; how-ever, solving this problem on lattices proves muchmore challenging, due to the number of embed-ded hypotheses, which prevents the use of brute-force approaches.
When using BLEU, or rathersentence-level approximations thereof, the prob-lem is in fact known to be NP-hard (Leusch etal., 2008).
This complexity stems from the factthat the contribution of a given edge to the totalmodified n-gram precision can not be computedwithout looking at all other edges on the path.Similar (or worse) complexity result are expected120for other metrics such as METEOR (Banerjee andLavie, 2005) or TER (Snover et al 2006).
Theexact computation of oracles under corpus levelmetrics, such as BLEU, poses supplementary com-binatorial problems that will not be addressed inthis work.In this paper, we present two original methodsfor finding approximate oracle hypotheses on lat-tices.
The first one is based on a linear approxima-tion of the corpus BLEU, that was originally de-signed for efficient Minimum Bayesian Risk de-coding on lattices (Tromble et al 2008).
The sec-ond one, based on Integer Linear Programming, isan extension to lattices of a recent work on failureanalysis for phrase-based decoders (Wisniewskiet al 2010).
In this framework, we study twodecoding strategies: one based on a generic ILPsolver, and one, based on Lagrangian relaxation.Our contribution is also experimental as wecompare the quality of the BLEU approxima-tions and the time performance of these new ap-proaches with several existing methods, for differ-ent language pairs and using the lattice generationcapacities of two publicly-available state-of-the-art phrase-based decoders: Moses1 and N-code2.The rest of this paper is organized as follows.In Section 2, we formally define the oracle decod-ing task and recall the formalism of finite stateautomata on semirings.
We then describe (Sec-tion 3) two existing approaches for solving thistask, before detailing our new proposals in sec-tions 4 and 5.
We then report evaluations of theexisting and new oracles on machine translationtasks.2 Preliminaries2.1 Oracle Decoding TaskWe assume that a phrase-based decoder is ableto produce, for each source sentence f , a latticeLf = ?Q,?
?, with # {Q} vertices (states) and# {?}
edges.
Each edge carries a source phrasefi, an associated output phrase ei as well as a fea-ture vector h?i, the components of which encodevarious compatibility measures between fi and ei.We further assume that Lf is a word lattice,meaning that each ei carries a single word3 and1http://www.statmt.org/moses/2http://ncode.limsi.fr/3Converting a phrase lattice to a word lattice is a simplematter of redistributing a compound input or output over athat it contains a unique initial state q0 and aunique final state qF .
Let ?f denote the set of allpaths from q0 to qF in Lf .
Each path pi ?
?f cor-responds to a possible translation epi.
The job ofa (conventional) decoder is to find the best path(s)in Lf using scores that combine the edges?
fea-ture vectors with the parameters ??
learned duringtuning.In oracle decoding, the decoder?s job is quitedifferent, as we assume that at least a referencerf is provided to evaluate the quality of each indi-vidual hypothesis.
The decoder therefore aims atfinding the path pi?
that generates the hypothesisthat best matches rf .
For this task, only the outputlabels ei will matter, the other informations can beleft aside.4Oracle decoding assumes the definition of ameasure of the similarity between a referenceand a hypothesis.
In this paper we will con-sider sentence-level approximations of the popu-lar BLEU score (Papineni et al 2002).
BLEU isformally defined for two parallel corpora, E ={ej}Jj=1 and R = {rj}Jj=1, each containing Jsentences as:n-BLEU(E ,R) = BP ?
( n?m=1pm)1/n, (1)where BP = min(1, e1?c1(R)/c1(E)) is thebrevity penalty and pm = cm(E ,R)/cm(E) areclipped or modified m-gram precisions: cm(E) isthe total number of wordm-grams in E ; cm(E ,R)accumulates over sentences the number of m-grams in ej that also belong to rj .
These countsare clipped, meaning that a m-gram that appearsk times in E and l times in R, with k > l, is onlycounted l times.
As it is well known, BLEU per-forms a compromise between precision, which isdirectly appears in Equation (1), and recall, whichis indirectly taken into account via the brevitypenalty.
In most cases, Equation (1) is computedwith n = 4 and we use BLEU as a synonym for4-BLEU.BLEU is defined for a pair of corpora, but, as anoracle decoder is working at the sentence-level, itshould rely on an approximation of BLEU that canlinear chain of arcs.4The algorithms described below can be straightfor-wardly generalized to compute oracle hypotheses undercombined metrics mixing model scores and quality measures(Chiang et al 2008), by weighting each edge with its modelscore and by using these weights down the pipe.121evaluate the similarity between a single hypoth-esis and its reference.
This approximation intro-duces a discrepancy as gathering sentences withthe highest (local) approximation may not resultin the highest possible (corpus-level) BLEU score.Let BLEU?
be such a sentence-level approximationof BLEU.
Then lattice oracle decoding is the taskof finding an optimal path pi?
(f) among all paths?f for a given f , and amounts to the followingoptimization problem:pi?
(f) = arg maxpi??fBLEU?
(epi, rf ).
(2)2.2 Compromises of Oracle DecodingAs proved by Leusch et al(2008), even withbrevity penalty dropped, the problem of decidingwhether a confusion network contains a hypoth-esis with clipped uni- and bigram precisions allequal to 1.0 is NP-complete (and so is the asso-ciated optimization problem of oracle decodingfor 2-BLEU).
The case of more general word andphrase lattices and 4-BLEU score is consequentlyalso NP-complete.
This complexity stems fromchaining up of local unigram decisions that, dueto the clipping constraints, have non-local effecton the bigram precision scores.
It is consequentlynecessary to keep a possibly exponential num-ber of non-recombinable hypotheses (character-ized by counts for each n-gram in the reference)until very late states in the lattice.These complexity results imply that any oracledecoder has to waive either the form of the objec-tive function, replacing BLEU with better-behavedscoring functions, or the exactness of the solu-tion, relying on approximate heuristic search al-gorithms.In Table 1, we summarize different compro-mises that the existing (section 3), as well asour novel (sections 4 and 5) oracle decoders,have to make.
The ?target?
and ?target level?columns specify the targeted score.
None ofthe decoders optimizes it directly: their objec-tive function is rather the approximation of BLEUgiven in the ?target replacement?
column.
Col-umn ?search?
details the accuracy of the target re-placement optimization.
Finally, columns ?clip-ping?
and ?brevity?
indicate whether the corre-sponding properties of BLEU score are consideredin the target substitute and in the search algorithm.2.3 Finite State AcceptorsThe implementations of the oracles described inthe first part of this work (sections 3 and 4) use thecommon formalism of finite state acceptors (FSA)over different semirings and are implemented us-ing the generic OpenFST toolbox (Allauzen et al2007).A (?,?
)-semiring K over a set K is a system?K,?,?, 0?, 1?
?, where ?K,?, 0??
is a commutativemonoid with identity element 0?, and ?K,?, 1??
isa monoid with identity element 1?.
?
distributesover ?, so that a ?
(b ?
c) = (a ?
b) ?
(a ?
c)and (b?
c)?
a = (b?
a)?
(c?
a) and element0?
annihilates K (a?
0?
= 0??
a = 0?
).Let A = (?, Q, I, F,E) be a weighted finite-state acceptor with labels in ?
and weights in K,meaning that the transitions (q, ?, q?)
in A carry aweight w ?
K. Formally, E is a mapping from(Q ?
?
?
Q) into K; likewise, initial I and fi-nal weight F functions are mappings from Q intoK.
We borrow the notations of Mohri (2009):if ?
= (q, a, q?)
is a transition in domain(E),p(?)
= q (resp.
n(?)
= q?)
denotes its origin(resp.
destination) state, w(?)
= ?
its label andE(?)
its weight.
These notations extend to paths:if pi is a path in A, p(pi) (resp.
n(pi)) is its initial(resp.
ending) state and w(pi) is the label alongthe path.
A finite state transducer (FST) is an FSAwith output alphabet, so that each transition car-ries a pair of input/output symbols.As discussed in Sections 3 and 4, several oracledecoding algorithms can be expressed as shortest-path problems, provided a suitable definition ofthe underlying acceptor and associated semiring.In particular, quantities such as:?pi??
(A)E(pi), (3)where the total weight of a successful path pi =?1 .
.
.
?l in A is computed as:E(pi) =I(p(?1))?[l?i=1E(?i)]?
F (n(?l))can be efficiently found by generic shortest dis-tance algorithms over acyclic graphs (Mohri,2002).
For FSA-based implementations oversemirings where ?
= max, the optimizationproblem (2) is thus reduced to Equation (3), whilethe oracle-specific details can be incorporated intoin the definition of ?.122oracle target target level target replacement search clipping brevityexisting LM-2g/4g 2/4-BLEU sentence P2(e; r) or P4(e; r) exact no noPB 4-BLEU sentence partial log BLEU (4) appr.
no noPB` 4-BLEU sentence partial log BLEU (4) appr.
no yesthispaper LB-2g/4g 2/4-BLEU corpus linear appr.
lin BLEU (5) exact no yesSP 1-BLEU sentence unigram count exact no yesILP 2-BLEU sentence uni/bi-gram counts (7) appr.
yes yesRLX 2-BLEU sentence uni/bi-gram counts (8) exact yes yesTable 1: Recapitulative overview of oracle decoders.3 Existing AlgorithmsIn this section, we describe our reimplementationof two approximate search algorithms that havebeen proposed in the literature to solve the oracledecoding problem for BLEU.
In addition to theirapproximate nature, none of them accounts for thefact that the count of each matching word has tobe clipped.3.1 Language Model Oracle (LM)The simplest approach we consider is introducedin (Li and Khudanpur, 2009), where oracle decod-ing is reduced to the problem of finding the mostlikely hypothesis under a n-gram language modeltrained with the sole reference translation.Let us suppose we have a n-gram languagemodel that gives a probability P (en|e1 .
.
.
en?1)of word en given the n?
1 previous words.The probability of a hypothesis e is thenPn(e|r) =?i=1 P (ei+n|ei .
.
.
ei+n?1).
The lan-guage model can conveniently be represented as aFSA ALM , with each arc carrying a negative log-probability weight and with additional ?-type fail-ure transitions to accommodate for back-off arcs.If we train, for each source sentence f , a sepa-rate language model ALM (rf ) using only the ref-erence rf , oracle decoding amounts to finding ashortest (most probable) path in the weighted FSAresulting from the composition L ?ALM (rf ) overthe (min,+)-semiring:pi?LM (f) = ShortestPath(L ?ALM (rf )).This approach replaces the optimization of n-BLEU with a search for the most probable pathunder a simplistic n-gram language model.
Onemay expect the most probable path to select fre-quent n-gram from the reference, thus augment-ing n-BLEU.3.2 Partial BLEU Oracle (PB)Another approach is put forward in (Dreyer etal., 2007) and used in (Li and Khudanpur, 2009):oracle translations are shortest paths in a latticeL, where the weight of each path pi is the sen-tence level log BLEU(pi) score of the correspond-ing complete or partial hypothesis:log BLEU(pi) =14?m=1...4log pm.
(4)Here, the brevity penalty is ignored and n-gram precisions are offset to avoid null counts:pm = (cm(epi, r) + 0.1)/(cm(epi) + 0.1).This approach has been reimplemented usingthe FST formalism by defining a suitable semir-ing.
Let each weight of the semiring keep a setof tuples accumulated up to the current state ofthe lattice.
Each tuple contains three words of re-cent history, a partial hypothesis as well as currentvalues of the length of the partial hypothesis, n-gram counts (4 numbers) and the sentence-levellog BLEU score defined by Equation (4).
In thebeginning each arc is initialized with a singletonset containing one tuple with a single word as thepartial hypothesis.
For the semiring operations wedefine one common?-operation and two versionsof the ?-operation:?
L1 ?PB L2 ?
appends a word on the edge ofL2 to L1?s hypotheses, shifts their recent historiesand updates n-gram counts, lengths, and currentscore; ?
L1 ?PB L2 ?
merges all sets from L1and L2 and recombinates those having the samerecent history; ?
L1 ?PB` L2 ?
merges all setsfrom L1 and L2 and recombinates those havingthe same recent history and the same hypothesislength.If several hypotheses have the same recenthistory (and length in the case of ?PB`), re-combination removes all of them, but the one123q?0:0/01:1/0(a) ?1q?00:/10 :/100:0010:0100:010:10(b) ?2q?00:/10:/100:/10 000:/1000:/10 :/10:0100:0010:0100:00100:010:10:00100:00010(c) ?3Figure 1: Examples of the ?n automata for ?
= {0, 1} and n = 1 .
.
.
3.
Initial and final states are marked,respectively, with bold and with double borders.
Note that arcs between final states are weighted with 0, while inreality they will have this weight only if the corresponding n-gram does not appear in the reference.with the largest current BLEU score.
Optimalpath is then found by launching the genericShortestDistance(L) algorithm over one ofthe semirings above.The (?PB`,?PB)-semiring, in which theequal length requirement also implies equalbrevity penalties, is more conservative in recom-bining hypotheses and should achieve final BLEUthat is least as good as that obtained with the(?PB,?PB)-semiring5.4 Linear BLEU Oracle (LB)In this section, we propose a new oracle based onthe linear approximation of the corpus BLEU in-troduced in (Tromble et al 2008).
While this ap-proximation was earlier used for Minimum BayesRisk decoding in lattices (Tromble et al 2008;Blackwood et al 2010), we show here how it canalso be used to approximately compute an oracletranslation.Given five real parameters ?0...4 and a word vo-cabulary ?, Tromble et al(2008) showed that onecan approximate the corpus-BLEU with its first-order (linear) Taylor expansion:lin BLEU(pi) = ?0 |epi|+4?n=1?n?u?
?ncu(epi)?u(r),(5)where cu(e) is the number of times the n-gramu appears in e, and ?u(r) is an indicator variabletesting the presence of u in r.To exploit this approximation for oracle decod-ing, we construct four weighted FSTs ?n con-taining a (final) state for each possible (n ?
1)-5See, however, experiments in Section 6.gram, and all weighted transitions of the kind(?n?11 , ?n : ?n1 /?n ?
?
?n1 (r), ?n2 ), where ?s arein ?, input word sequence ?n?11 and output se-quence ?n2 , are, respectively, the maximal prefixand suffix of an n-gram ?n1 .In supplement, we add auxiliary states corre-sponding to m-grams (m < n ?
1), whose func-tional purpose is to help reach one of the main(n ?
1)-gram states.
There are |?|n?1?1|?|?1 , n > 1,such supplementary states and their transitions are(?k1 , ?k+1 : ?k+11 /0, ?k+11 ), k = 1 .
.
.
n?2.
Apartfrom these auxiliary states, the rest of the graph(i.e., all final states) reproduces the structure ofthe well-known de Bruijn graphB(?, n) (see Fig-ure 1).To actually compute the best hypothesis, wefirst weight all arcs in the input FSA L with ?0 toobtain ?0.
This makes each word?s weight equalin a hypothesis path, and the total weight of thepath in ?0 is proportional to the number of wordsin it.
Then, by sequentially composing ?0 withother ?ns, we discount arcs whose output n-gramcorresponds to a matching n-gram.
The amountof discount is regulated by the ratio between ?n?sfor n > 0.With all operations performed over the(min,+)-semiring, the oracle translation is thengiven by:pi?LB = ShortestPath(?0??1??2??3?
?4).We set parameters ?n as in (Tromble et al2008): ?0 = 1, roughly corresponding to thebrevity penalty (each word in a hypothesis addsup equally to the final path length) and ?n =?
(4p ?
rn?1)?1, which are increasing discounts1240 0.20.4 0.60.8 1p00.20.40.60.81r2224262830323436BLEU2224262830323436Figure 2: Performance of the LB-4g oracle for differ-ent combinations of p and r on WMT11 de2en task.for matching n-grams.
The values of p and r werefound by grid search with a 0.05 step value.
Atypical result of the grid evaluation of the LB or-acle for German to English WMT?11 task is dis-played on Figure 2.
The optimal values for theother pairs of languages were roughly in the sameballpark, with p ?
0.3 and r ?
0.2.5 Oracles with n-gram ClippingIn this section, we describe two new oracle de-coders that take n-gram clipping into account.These oracles leverage on the well-known factthat the shortest path problem, at the heart ofall the oracles described so far, can be reducedstraightforwardly to an Integer Linear Program-ming (ILP) problem (Wolsey, 1998).
Once oracledecoding is formulated as an ILP problem, it isrelatively easy to introduce additional constraints,for instance to enforce n-gram clipping.
We willfirst describe the optimization problem of oracledecoding and then present several ways to effi-ciently solve it.5.1 Problem DescriptionThroughout this section, abusing the notations,we will also think of an edge ?i as a binary vari-able describing whether the edge is ?selected?
ornot.
The set {0, 1}#{?}
of all possible edge as-signments will be denoted by P .
Note that ?, theset of all paths in the lattice is a subset of P: byenforcing some constraints on an assignment ?
inP , it can be guaranteed that it will represent a pathin the lattice.
For the sake of presentation, we as-sume that each edge ?i generates a single wordw(?i) and we focus first on finding the optimalhypothesis with respect to the sentence approxi-mation of the 1-BLEU score.As 1-BLEU is decomposable, it is possible todefine, for every edge ?i, an associated reward, ?ithat describes the edge?s local contribution to thehypothesis score.
For instance, for the sentenceapproximation of the 1-BLEU score, the rewardsare defined as:?i ={?1 if w(?i) is in the reference,?
?2 otherwise,where ?1 and ?2 are two positive constants cho-sen to maximize the corpus BLEU score6.
Con-stant ?1 (resp.
?2) is a reward (resp.
a penalty)for generating a word in the reference (resp.
not inthe reference).
The score of an assignment ?
?
Pis then defined as: score(?)
=?#{?
}i=1 ?i ?
?i.
Thisscore can be seen as a compromise between thenumber of common words in the hypothesis andthe reference (accounting for recall) and the num-ber of words of the hypothesis that do not appearin the reference (accounting for precision).As explained in Section 2.3, finding the or-acle hypothesis amounts to solving the shortestdistance (or path) problem (3), which can be re-formulated by a constrained optimization prob-lem (Wolsey, 1998):arg max??P#{?
}?i=1?i ?
?i (6)s.t.?????
(qF )?
= 1,????+(q0)?
= 1????+(q)?
??????(q)?
= 0, q ?
Q\{q0, qF }where q0 (resp.
qF ) is the initial (resp.
final) stateof the lattice and ??
(q) (resp.
?+(q)) denotes theset of incoming (resp.
outgoing) edges of state q.These path constraints ensure that the solution ofthe problem is a valid path in the lattice.The optimization problem in Equation (6) canbe further extended to take clipping into account.Let us introduce, for each word w, a variable ?wthat denotes the number of times w appears in thehypothesis clipped to the number of times, it ap-pears in the reference.
Formally, ?w is defined by:?w = min???????
(w)?, cw(r)??
?6We tried several combinations of ?1 and ?2 and keptthe one that had the highest corpus 4-BLEU score.125where ?
(w) is the subset of edges generating w,and????
(w) ?
is the number of occurrences ofw in the solution and cw(r) is the number of oc-currences of w in the reference r. Using the ?variables, we define a ?clipped?
approximation of1-BLEU:?1 ?
?w?w ?
?2 ???#{?
}?i=1?i ??w?w?
?Indeed, the clipped number of words in the hy-pothesis that appear in the reference is given by?w ?w, and?#{?
}i=1 ?i ?
?w ?w corresponds tothe number of words in the hypothesis that do notappear in the reference or that are surplus to theclipped count.Finally, the clipped lattice oracle is defined bythe following optimization problem:arg max?
?P,?w(?1 + ?2) ?
?w?w ?
?2 ?#{?}?i=1?i(7)s.t.
?w ?
0, ?w ?
cw(r), ?w ?????(w)??????
(qF )?
= 1,????+(q0)?
= 1????+(q)?
??????(q)?
= 0, q ?
Q \ {q0, qF }where the first three sets of constraints are the lin-earization of the definition of ?w, made possibleby the positivity of ?1 and ?2, and the last threesets of constraints are the path constraints.In our implementation we generalized this op-timization problem to bigram lattices, in whicheach edge is labeled by the bigram it generates.Such bigram FSAs can be produced by compos-ing the word lattice with ?2 from Section 4.
Inthis case, the reward of an edge will be defined asa combination of the (clipped) number of unigrammatches and bigram matches, and solving the op-timization problem yields a 2-BLEU optimal hy-pothesis.
The approach can be further generalizedto higher-order BLEU or other metrics, as long asthe reward of an edge can be computed locally.The constrained optimization problem (7) canbe solved efficiently using off-the-shelf ILPsolvers7.7In our experiments we used Gurobi (Optimization,2010) a commercial ILP solver that offers free academic li-cense.5.2 Shortest Path Oracle (SP)As a trivial special class of the above formula-tion, we also define a Shortest Path Oracle (SP)that solves the optimization problem in (6).
Asno clipping constraints apply, it can be solved ef-ficiently using the standard Bellman algorithm.5.3 Oracle Decoding through LagrangianRelaxation (RLX)In this section, we introduce another method tosolve problem (7) without relying on an exter-nal ILP solver.
Following (Rush et al 2010;Chang and Collins, 2011), we propose an originalmethod for oracle decoding based on Lagrangianrelaxation.
This method relies on the idea of re-laxing the clipping constraints: starting from anunconstrained problem, the counts clipping is en-forced by incrementally strengthening the weightof paths satisfying the constraints.The oracle decoding problem with clippingconstraints amounts to solving:arg min????#{?
}?i=1?i ?
?i (8)s.t.????(w)?
?
cw(r), w ?
rwhere, by abusing the notations, r also denotesthe set of words in the reference.
For sake of clar-ity, the path constraints are incorporated into thedomain (the arg min runs over ?
and not over P).To solve this optimization problem we consider itsdual form and use Lagrangian relaxation to dealwith clipping constraints.Let ?
= {?w}w?r be positive Lagrange mul-tipliers, one for each different word of the refer-ence, then the Lagrangian of the problem (8) is:L(?, ?)
= ?#{?}?i=1?i?i+?w?r?w??????(w)?
?
cw(r)?
?The dual objective is L(?)
= min?
L(?, ?
)and the dual problem is: max?,?0 L(?).
Tosolve the latter, we first need to work out the dualobjective:??
= arg min??
?L(?, ?
)= arg min???#{?
}?i=1?i(?w(?i) ?
?i)126where we assume that ?w(?i) is 0 when wordw(?i) is not in the reference.
In the same wayas in Section 5.2, the solution of this problem canbe efficiently retrieved with a shortest path algo-rithm.It is possible to optimize L(?)
by noticing thatit is a concave function.
It can be shown (Changand Collins, 2011) that, at convergence, the clip-ping constraints will be enforced in the optimalsolution.
In this work, we chose to use a simplegradient descent to solve the dual problem.
A sub-gradient of the dual objective is:?L(?)??w=????(w)????
?
cw(r).Each component of the gradient corresponds tothe difference between the number of times theword w appears in the hypothesis and the num-ber of times it appears in the reference.
The algo-rithm below sums up the optimization of task (8).In the algorithm ?
(t) corresponds to the step sizeat the tth iteration.
In our experiments we used aconstant step size of 0.1.
Compared to the usualgradient descent algorithm, there is an additionalprojection step of ?
on the positive orthant, whichenforces the constraint ?
 0.?w, ?
(0)w ?
0for t = 1?
T do??
(t) = arg min?
?i ?i ?
(?w(?i) ?
?i)if all clipping constraints are enforcedthen optimal solution foundelse for w ?
r donw ?
n. of occurrences of w in ??(t)?
(t)w ?
?
(t)w + ?
(t) ?
(nw ?
cw(r))?
(t)w ?
max(0, ?
(t)w )6 ExperimentsFor the proposed new oracles and the existing ap-proaches, we compare the quality of oracle trans-lations and the average time per sentence neededto compute them8 on several datasets for 3 lan-guage pairs, using lattices generated by two open-source decoders: N-code and Moses9 (Figures 38Experiments were run in parallel on a server with 64Gof RAM and 2 Xeon CPUs with 4 cores at 2.3 GHz.9As the ILP (and RLX) oracle were implemented inPython, we pruned Moses lattices to accelerate task prepa-ration for it.decoder fr2en de2en en2detest N-code 27.88 22.05 15.83Moses 27.68 21.85 15.89oracle N-code 36.36 29.22 21.18Moses 35.25 29.13 22.03Table 2: Test BLEU scores and oracle scores on100-best lists for the evaluated systems.and 4).
Systems were trained on the data providedfor the WMT?11 Evaluation task10, tuned on theWMT?09 test data and evaluated on WMT?10 testset11 to produce lattices.
The BLEU test scoresand oracle scores on 100-best lists with the ap-proximation (4) for N-code and Moses are givenin Table 2.
It is not until considering 10,000-bestlists that n-best oracles achieve performance com-parable to the (mediocre) SP oracle.To make a fair comparison with the ILP andRLX oracles which optimize 2-BLEU, we in-cluded 2-BLEU versions of the LB and LM ora-cles, identified below with the ?-2g?
suffix.
Thetwo versions of the PB oracle are respectivelydenoted as PB and PB`, by the type of the ?-operation they consider (Section 3.2).
Parame-ters p and r for the LB-4g oracle for N-code werefound with grid search and reused for Moses:p = 0.25, r = 0.15 (fr2en); p = 0.175, r = 0.575(en2de) and p = 0.35, r = 0.425 (de2en).
Cor-respondingly, for the LB-2g oracle: p = 0.3, r =0.15; p = 0.3, r = 0.175 and p = 0.575, r = 0.1.The proposed LB, ILP and RLX oracles werethe best performing oracles, with the ILP andRLX oracles being considerably faster, sufferingonly a negligible decrease in BLEU, compared tothe 4-BLEU-optimized LB oracle.
We stoppedRLX oracle after 20 iterations, as letting it con-verge had a small negative effect (?1 point of thecorpus BLEU), because of the sentence/corpus dis-crepancy ushered by the BLEU score approxima-tion.Experiments showed consistently inferior per-formance of the LM-oracle resulting from the op-timization of the sentence probability rather thanBLEU.
The PB oracle often performed compara-bly to our new oracles, however, with sporadicresource-consumption bursts, that are difficult to10http://www.statmt.org/wmt201111All BLEU scores are reported using the multi-bleu.plscript.127253035404550RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0123456BLEUavg.
time, sBLEU47.8248.1248.2247.7146.7646.4841.2338.9138.75avg.
time(a) fr2en253035RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  00.511.5BLEUavg.
time, sBLEU34.7934.70 35.4935.0934.8534.7630.7829.5329.53avg.
time(b) de2en15202530RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  00.51BLEUavg.
time, sBLEU24.7524.66 25.3424.8524.7824.7322.1920.7820.74avg.
time(c) en2deFigure 3: Oracles performance for N-code lattices.253035404550RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0123BLEUavg.
time, sBLEU43.8244.0844.4443.8243.4243.2041.0336.3436.25avg.
time(a) fr2en253035RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  01234BLEUavg.
time, sBLEU36.43 36.91 37.7336.5236.7536.6230.5229.5129.45avg.
time(b) de2en15202530RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g  0123456789BLEUavg.
time, sBLEU28.6828.64 29.9428.9428.7628.6526.4821.2921.23avg.
time(c) en2deFigure 4: Oracles performance for Moses lattices pruned with parameter -b 0.5.avoid without more cursory hypotheses recom-bination strategies and the induced effect on thetranslations quality.
The length-aware PB` oraclehas unexpectedly poorer scores compared to itslength-agnostic PB counterpart, while it should,at least, stay even, as it takes the brevity penaltyinto account.
We attribute this fact to the com-plex effect of clipping coupled with the lack ofcontrol of the process of selecting one hypothe-sis among several having the same BLEU score,length and recent history.
Anyhow, BLEU scoresof both of PB oracles are only marginally differ-ent, so the PB`?s conservative policy of pruningand, consequently, much heavier memory con-sumption makes it an unwanted choice.7 ConclusionWe proposed two methods for finding oracletranslations in lattices, based, respectively, on alinear approximation to the corpus-level BLEUand on integer linear programming techniques.We also proposed a variant of the latter approachbased on Lagrangian relaxation that does not relyon a third-party ILP solver.
All these oracles havesuperior performance to existing approaches, interms of the quality of the found translations, re-source consumption and, for the LB-2g oracles,in terms of speed.
It is thus possible to use bet-ter approximations of BLEU than was previouslydone, taking the corpus-based nature of BLEU, orclipping constrainst into account, delivering betteroracles without compromising speed.Using 2-BLEU and 4-BLEU oracles yields com-parable performance, which confirms the intuitionthat hypotheses sharing many 2-grams, wouldlikely have many common 3- and 4-grams as well.Taking into consideration the exceptional speed ofthe LB-2g oracle, in practice one can safely opti-mize for 2-BLEU instead of 4-BLEU, saving largeamounts of time for oracle decoding on long sen-tences.Overall, these experiments accentuate theacuteness of scoring problems that plague moderndecoders: very good hypotheses exist for most in-put sentences, but are poorly evaluated by a linearcombination of standard features functions.
Eventhough the tuning procedure can be held respon-sible for part of the problem, the comparison be-tween lattice and n-best oracles shows that thebeam search leaves good hypotheses out of the n-best list until very high value of n, that are neverused in practice.AcknowledgmentsThis work has been partially funded by OSEO un-der the Quaero program.128ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst:A general and efficient weighted finite-state trans-ducer library.
In Proc.
of the Int.
Conf.
on Imple-mentation and Application of Automata, pages 11?23.Michael Auli, Adam Lopez, Hieu Hoang, and PhilippKoehn.
2009.
A systematic analysis of translationmodel search spaces.
In Proc.
of WMT, pages 224?232, Athens, Greece.Satanjeev Banerjee and Alon Lavie.
2005.
ME-TEOR: An automatic metric for MT evaluation withimproved correlation with human judgments.
InProc.
of the ACL Workshop on Intrinsic and Extrin-sic Evaluation Measures for Machine Translation,pages 65?72, Ann Arbor, MI, USA.Graeme Blackwood, Adria` de Gispert, and WilliamByrne.
2010.
Efficient path counting transducersfor minimum bayes-risk decoding of statistical ma-chine translation lattices.
In Proc.
of the ACL 2010Conference Short Papers, pages 27?32, Strouds-burg, PA, USA.Yin-Wen Chang and Michael Collins.
2011.
Exact de-coding of phrase-based translation models throughlagrangian relaxation.
In Proc.
of the 2011 Conf.
onEMNLP, pages 26?37, Edinburgh, UK.David Chiang, Yuval Marton, and Philip Resnik.2008.
Online large-margin training of syntacticand structural translation features.
In Proc.
of the2008 Conf.
on EMNLP, pages 224?233, Honolulu,Hawaii.Markus Dreyer, Keith B.
Hall, and Sanjeev P. Khu-danpur.
2007.
Comparing reordering constraintsfor SMT using efficient BLEU oracle computation.In Proc.
of the Workshop on Syntax and Structurein Statistical Translation, pages 103?110, Morris-town, NJ, USA.Gregor Leusch, Evgeny Matusov, and Hermann Ney.2008.
Complexity of finding the BLEU-optimal hy-pothesis in a confusion network.
In Proc.
of the2008 Conf.
on EMNLP, pages 839?847, Honolulu,Hawaii.Zhifei Li and Sanjeev Khudanpur.
2009.
Efficientextraction of oracle-best translations from hyper-graphs.
In Proc.
of Human Language Technolo-gies: The 2009 Annual Conf.
of the North Ameri-can Chapter of the ACL, Companion Volume: ShortPapers, pages 9?12, Morristown, NJ, USA.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,and Ben Taskar.
2006.
An end-to-end discrim-inative approach to machine translation.
In Proc.of the 21st Int.
Conf.
on Computational Linguisticsand the 44th annual meeting of the ACL, pages 761?768, Morristown, NJ, USA.Mehryar Mohri.
2002.
Semiring frameworks and al-gorithms for shortest-distance problems.
J. Autom.Lang.
Comb., 7:321?350.Mehryar Mohri.
2009.
Weighted automata algo-rithms.
In Manfred Droste, Werner Kuich, andHeiko Vogler, editors, Handbook of Weighted Au-tomata, chapter 6, pages 213?254.Gurobi Optimization.
2010.
Gurobi optimizer, April.Version 3.0.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Proc.
ofthe Annual Meeting of the ACL, pages 311?318.Alexander M. Rush, David Sontag, Michael Collins,and Tommi Jaakkola.
2010.
On dual decomposi-tion and linear programming relaxations for naturallanguage processing.
In Proc.
of the 2010 Conf.
onEMNLP, pages 1?11, Stroudsburg, PA, USA.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof translation edit rate with targeted human anno-tation.
In Proc.
of the Conf.
of the Association forMachine Translation in the America (AMTA), pages223?231.Roy W. Tromble, Shankar Kumar, Franz Och, andWolfgang Macherey.
2008.
Lattice minimumbayes-risk decoding for statistical machine transla-tion.
In Proc.
of the Conf.
on EMNLP, pages 620?629, Stroudsburg, PA, USA.Marco Turchi, Tijl De Bie, and Nello Cristianini.2008.
Learning performance of a machine trans-lation system: a statistical and computational anal-ysis.
In Proc.
of WMT, pages 35?43, Columbus,Ohio.Guillaume Wisniewski, Alexandre Allauzen, andFranc?ois Yvon.
2010.
Assessing phrase-basedtranslation models with oracle decoding.
In Proc.of the 2010 Conf.
on EMNLP, pages 933?943,Stroudsburg, PA, USA.L.
Wolsey.
1998.
Integer Programming.
John Wiley& Sons, Inc.129
