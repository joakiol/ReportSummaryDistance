Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34?42,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsRule-based Syntactic Preprocessingfor Syntax-based Machine TranslationYuto Hatakoshi, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi NakamuraNara Institute of Science and TechnologyGraduate School of Information ScienceTakayama, Ikoma, Nara 630-0192, Japan{hatakoshi.yuto.hq8,neubig,ssakti,tomoki,s-nakamura}@is.naist.jpAbstractSeveral preprocessing techniques usingsyntactic information and linguisticallymotivated rules have been proposed to im-prove the quality of phrase-based machinetranslation (PBMT) output.
On the otherhand, there has been little work on similartechniques in the context of other trans-lation formalisms such as syntax-basedSMT.
In this paper, we examine whetherthe sort of rule-based syntactic preprocess-ing approaches that have proved beneficialfor PBMT can contribute to syntax-basedSMT.
Specifically, we tailor a highly suc-cessful preprocessing method for English-Japanese PBMT to syntax-based SMT,and find that while the gains achievable aresmaller than those for PBMT, significantimprovements in accuracy can be realized.1 IntroductionIn the widely-studied framework of phrase-basedmachine translation (PBMT) (Koehn et al., 2003),translation probabilities between phrases consist-ing of multiple words are calculated, and trans-lated phrases are rearranged by the reorderingmodel in the appropriate target language order.While PBMT provides a light-weight frameworkto learn translation models and achieves hightranslation quality in many language pairs, it doesnot directly incorporate morphological or syntac-tic information.
Thus, many preprocessing meth-ods for PBMT using these types of informationhave been proposed.
Methods include preprocess-ing to obtain accurate word alignments by the divi-sion of the prefix of verbs (Nie?en and Ney, 2000),preprocessing to reduce the errors in verb conju-gation and noun case agreement (Avramidis andKoehn, 2008), and many others.
The effectivenessof the syntactic preprocessing for PBMT has beensupported by these and various related works.In particular, much attention has been paid topreordering (Xia and McCord, 2004; Collins etal., 2005), a class of preprocessing methods forPBMT.
PBMT has well-known problems with lan-guage pairs that have very different word order,due to the fact that the reordering model has dif-ficulty estimating the probability of long distancereorderings.
Therefore, preordering methods at-tempt to improve the translation quality of PBMTby rearranging source language sentences into anorder closer to that of the target language.
It?s of-ten the case that preordering methods are basedon rule-based approaches, and these methods haveachieved great success in ameliorating the wordordering problems faced by PBMT (Collins et al.,2005; Xu et al., 2009; Isozaki et al., 2010b).One particularly successful example of rule-based syntactic preprocessing is Head Finalization(Isozaki et al., 2010b), a method of syntactic pre-processing for English to Japanese translation thathas significantly improved translation quality ofEnglish-Japanese PBMT using simple rules basedon the syntactic structure of the two languages.The most central part of the method, as indicatedby its name, is a reordering rule that moves theEnglish head word to the end of the correspondingsyntactic constituents to match the head-final syn-tactic structure of Japanese sentences.
Head Final-ization also contains some additional preprocess-ing steps such as determiner elimination, parti-cle insertion and singularization to generate a sen-tence that is closer to Japanese grammatical struc-ture.In addition to PBMT, there has also recentlybeen interest in syntax-based SMT (Yamada andKnight, 2001; Liu et al., 2006), which translatesusing syntactic information.
However, few at-tempts have been made at syntactic preprocessingfor syntax-based SMT, as the syntactic informa-tion given by the parser is already incorporateddirectly in the translation model.
Notable excep-34tions include methods to perform tree transforma-tions improving correspondence between the sen-tence structure and word alignment (Burkett andKlein, 2012), methods for binarizing parse trees tomatch word alignments (Zhang et al., 2006), andmethods for adjusting label sets to be more ap-propriate for syntax-based SMT (Hanneman andLavie, 2011; Tamura et al., 2013).
It should benoted that these methods of syntactic preprocess-ing for syntax-based SMT are all based on auto-matically learned rules, and there has been little in-vestigation of the manually-created linguistically-motivated rules that have proved useful in prepro-cessing for PBMT.In this paper, we examine whether rule-basedsyntactic preprocessing methods designed forPBMT can contribute anything to syntax-basedmachine translation.
Specifically, we examinewhether the reordering and lexical processing ofHead Finalization contributes to the improvementof syntax-based machine translation as it did forPBMT.
Additionally, we examine whether it ispossible to incorporate the intuitions behind theHead Finalization reordering rules as soft con-straints by incorporating them as a decoder fea-ture.
As a result of our experiments, we demon-strate that rule-based lexical processing can con-tribute to improvement of translation quality ofsyntax-based machine translation.2 Head FinalizationHead Finalization is a syntactic preprocessingmethod for English to Japanese PBMT, reducinggrammatical errors through reordering and lexi-cal processing.
Isozaki et al.
(2010b) have re-ported that translation quality of English-JapanesePBMT is significantly improved using a transla-tion model learned by English sentences prepro-cessed by Head Finalization and Japanese sen-tences.
In fact, this method achieved the highestresults in the large scale NTCIR 2011 evaluation(Sudoh et al., 2011), the first time a statistical ma-chine translation (SMT) surpassed rule-based sys-tems for this very difficult language pair, demon-strating the utility of these simple syntactic trans-formations from the point of view of PBMT.2.1 ReorderingThe reordering process of Head Finalization usesa simple rule based on the features of Japanesegrammar.
To convert English sentence intoJohn hit a ballJohn hita ballNN VBD DT NNNPVPNPSVBDVPNPSDT NNNPNNOriginal EnglishHead Final EnglishAdd Japanese ParticlesJohn hita ballva0 va2Singularize,Eliminate DeterminersJohn hita ballva0 va2ReorderingFigure 1: Head FinalizationJapanese word order, the English sentence is firstparsed using a syntactic parser, and then headwords are moved to the end of the correspondingsyntactic constituents in each non-terminal nodeof the English syntax tree.
This helps replicatethe ordering of words in Japanese grammar, wheresyntactic head words come after non-head (depen-dent) words.Figure 1 shows an example of the applicationof Head Finalization to an English sentence.
Thehead node of the English syntax tree is connectedto the parent node by a bold line.
When this nodeis the first child node, we move it behind the de-pendent node in order to convert the English sen-tence into head final order.
In this case, movingthe head node VBD of black node VP to the end ofthis node, we can obtain the sentence ?John a ballhit?
which is in a word order similar to Japanese.2.2 Lexical ProcessingIn addition to reordering, Head Finalization con-ducts the following three steps that do not affectword order.
These steps do not change the word35ordering, but still result in an improvement oftranslation quality, and it can be assumed that theeffect of this variety of syntactic preprocessing isnot only applicable to PBMT but also other trans-lation methods that do not share PBMT?s problemsof reordering such as syntax-based SMT.
The threesteps included are as follows:1.
Pseudo-particle insertion2.
Determiner (?a?, ?an?, ?the?)
elimination3.
SingularizationThe motivation for the first step is that in con-trast to English, which has relatively rigid wordorder and marks grammatical cases of many nounphrases according to their position relative to theverb, Japanese marks the topic, subject, and objectusing case marking particles.
As Japanese parti-cles are not found in English, Head Finalizationinserts ?pseudo-particles?
to prevent a mistransla-tion or lack of particles in the translation process.In the pseudo-particle insertion process (1), we in-sert the following three types of pseudo-particlesequivalent to Japanese case markers ?wa?
(topic),?ga?
(subject) or ?wo?
(object).?
va0: Subject particle of the main verb?
va1: Subject particle of other verbs?
va2: Object particle of any verbIn the example of Figure 1, we insert the topic par-ticle va0 behind of ?John?, which is a subject of averb ?hit?
and object particle va2 at the back ofobject ?ball.
?Another source of divergence between the twolanguages stems from the fact that Japanese doesnot contain determiners or makes distinctions be-tween singular and plural by inflection of nouns.Thus, to generate a sentence that is closer toJapanese, Head Finalization eliminates determin-ers (2) and singularizes plural nouns (3) in addi-tion to the pseudo-particle insertion.In Figure 1, we can see that applying thesethree processes to the source English sentence re-sults in the sentence ?John va0 (wa) ball va2 (wo)hit?
which closely resembles the structure of theJapanese translation ?jon wa bo-ru wo utta.
?3 Syntax-based Statistical MachineTranslationSyntax-based SMT is a method for statisticaltranslation using syntactic information of the sen-tence (Yamada and Knight, 2001; Liu et al., 2006).By using translation patterns following the struc-ture of linguistic syntax trees, syntax-based trans-lations often makes it possible to achieve moregrammatical translations and reorderings com-pared with PBMT.
In this section, we describetree-to-string (T2S) machine translation based onsynchronous tree substitution grammars (STSG)(Graehl et al., 2008), the variety of syntax-basedSMT that we use in our experiments.T2S captures the syntactic relationship betweentwo languages by using the syntactic structure ofparsing results of the source sentence.
Each trans-lation pattern is expressed as a source sentencesubtree using rules including variables.
The fol-lowing example of a translation pattern includetwo noun phrases NP0and NP1, which are trans-lated and inserted into the target placeholders X0and X1respectively.
The decoder generates thetranslated sentence in consideration of the proba-bility of translation pattern itself and translationsof the subtrees of NP0and NP1.S((NP0) (VP(VBD hit) (NP1)))?
X0wa X1wo uttaT2S has several advantages over PBMT.
First,because the space of translation candidates is re-duced using the source sentence subtree, it is oftenpossible to generate translations that are more ac-curate, particularly with regards to long-distancereordering, as long as the source parse is correct.Second, the time to generate translation results isalso reduced because the search space is smallerthan PBMT.
On the other hand, because T2S gen-erates translation results using the result of auto-matic parsing, translation quality highly dependson the accuracy of the parser.4 Applying Syntactic Preprocessing toSyntax-based Machine TranslationIn this section, we describe our proposed methodto apply Head Finalization to T2S translation.Specifically, we examine two methods for incor-porating the Head Finalization rules into syntax-based SMT: through applying them as preprocess-ing step to the trees used in T2S translation, and36through adding reordering information as a featureof the translation patterns.4.1 Syntactic Preprocessing for T2SWe applied the two types of processing shown inTable 1 as preprocessing for T2S.
This is similarto preprocessing for PBMTwith the exception thatpreprocessing for PBMT results in a transformedstring, and preprocessing for T2S results in a trans-formed tree.
In the following sections, we elabo-rate on methods for applying these preprocessingsteps to T2S and some effects expected therefrom.Table 1: Syntactic preprocessing applied to T2SPreprocessing DescriptionReordering Reordering based on Japanesetypical head-final grammaticalstructureLexical Processing Pseudo-particle insertion, deter-miner elimination, singulariza-tion4.1.1 Reordering for T2SIn the case of PBMT, reordering is used to changethe source sentence word order to be closer tothat of the target, reducing the burden on the rel-atively weak PBMT reordering models.
On theother hand, because translation patterns of T2Sare expressed by using source sentence subtrees,the effect of reordering problems are relativelysmall, and the majority of reordering rules spec-ified by hand can be automatically learned in awell-trained T2S model.
Therefore, preorderingis not expected to cause large gains, unlike in thecase of PBMT.However, it can also be thought that preorderingcan still have a positive influence on the translationmodel training process, particularly by increasingalignment accuracy.
For example, training meth-ods for word alignment such as the IBM or HMMmodels (Och and Ney, 2003) are affected by wordorder, and word alignment may be improved bymoving word order closer between the two lan-guages.
As alignment accuracy plays a importantrole in T2S translation (Neubig and Duh, 2014), itis reasonable to hypothesize that reordering mayalso have a positive effect on T2S.
In terms of theactual incorporation with the T2S system, we sim-ply follow the process in Figure 1, but output thereordered tree instead of only the reordered termi-nal nodes as is done for PBMT.John hit a ballNN VBD DT NNNPVPNPSOriginal EnglishNN VBD NN VANPVPNPSVAJohn hit ball va2va0Lexical ProcessingFigure 2: A method of applying Lexical Process-ing4.1.2 Lexical Processing for T2SIn comparison to reordering, Lexical Processingmay be expected to have a larger effect on T2S,as it will both have the potential to increase align-ment accuracy, and remove the burden of learningrules to perform simple systematic changes thatcan be written by hand.
Figure 2 shows an ex-ample of the application of Lexical Processing totransform not strings, but trees.In the pseudo-particle insertion component,three pseudo particles ?va0,?
?va1,?
and ?va2?
(asshown in Section 2.2) are added in the source En-glish syntax tree as terminal nodes with the non-terminal node ?VA?.
As illustrated in Figure 2, par-ticles are inserted as children at the end of the cor-responding NP node.
For example, in the figurethe topic particle ?va0?
is inserted after ?John,?subject of the verb ?hit,?
and the object particle?va2?
is inserted at the end of the NP for ?ball,?the object.In the determiner elimination process, terminalnodes ?a,?
?an,?
and ?the?
are eliminated alongwith non-terminal node DT.
Determiner ?a?
andits corresponding non-terminal DT are eliminatedin the Figure 2 example.Singularization, like in the processing forPBMT, simply changes plural noun terminals totheir base form.4.2 Reordering Information as SoftConstraintsAs described in section 4.1.1, T2S work well onlanguage pairs that have very different word order,but is sensitive to alignment accuracy.
On the otherhand, we know that in most cases Japanese wordorder tends to be head final, and thus any rules thatdo not obey head final order may be the result ofbad alignments.
On the other hand, there are somecases where head final word order is not applica-ble (such as sentences that contain the determiner37?no,?
or situations where non-literal translationsare necessary) and a hard constraint to obey head-final word order could be detrimental.In order to incorporate this intuition, we adda feature (HF-feature) to translation patterns thatconform to the reordering rules of Head Final-ization.
This gives the decoder ability to discerntranslation patterns that follow the canonical re-ordering patterns in English-Japanese translation,and has the potential to improve translation qualityin the T2S translation model.We use the log-linear approach (Och, 2003) toadd the Head Finalization feature (HF-feature).
Asin the standard log-linear model, a source sen-tence f is translated into a target language sen-tence e, by searching for the sentence maximizingthe score:?e = arg maxewT?
h(f ,e).
(1)where h(f , e) is a feature function vector.
w isa weight vector that scales the contribution fromeach feature.
Each feature can take any real valuewhich is useful to improve translation quality, suchas the log of the n-gram language model proba-bility to represent fluency, or lexical/phrase trans-lation probability to capture the word or phrase-wise correspondence.
Thus, if we can incorporatethe information about reordering expressed by theHead Finalization reordering rule as a features inthis model, we can learn weights to inform the de-coder that it should generally follow this canonicalordering.Figure 3 shows a procedure of Head Finaliza-tion feature (HF-feature) addition.
To add theHF-feature to translation patterns, we examinethe translation rules, along with the alignmentsbetween target and source terminals and non-terminals.
First, we apply the Reordering to thesource side of the translation pattern subtree ac-cording to the canonical head-final reordering rule.Second, we examine whether the word order of thereordered translation pattern matches with that ofthe target translation pattern for which the wordalignment is non-crossing, indicating that the tar-get string is also in head-final word order.
Finally,we set a binary feature (hHF(f , e) = 1) if the tar-get word order obeys the head final order.
Thisfeature is only applied to translation patterns forwhich the number of target side words is greaterthan or equal to two.VPVBD NPhit x0:NPx0 woSource side oftranslation patternTarget side oftranslation patternVPNP VBDhitx0:NP1.
Apply Reordering tosource translation pattern2.
Add HF-featureif word alignment isnon-crossinguttaWord alignmentx0 woTarget side oftranslation patternuttaReorderedtranslation patternFigure 3: Procedure of HF-feature additionTable 2: The details of NTCIR7Dataset Lang Words SentencesAveragelengthtrainEn 99.0M 3.08M 32.13Ja 117M 3.08M 37.99devEn 28.6k 0.82k 34.83Ja 33.5k 0.82k 40.77testEn 44.3k 1.38k 32.11Ja 52.4k 1.38k 37.995 ExperimentIn our experiment, we examined how much eachof the preprocessing steps (Reordering, LexicalProcessing) contribute to improve the translationquality of PBMT and T2S.
We also examined theimprovement in translation quality of T2S by theintroduction of the Head Finalization feature.5.1 Experimental EnvironmentFor our English to Japanese translation experi-ments, we used NTCIR7 PATENT-MT?s Patentcorpus (Fujii et al., 2008).
Table 2 shows thedetails of training data (train), development data(dev), and test data (test).As the PBMT and T2S engines, we used theMoses (Koehn et al., 2007) and Travatar (Neubig,2013) translation toolkits with the default settings.38Enju (Miyao and Tsujii, 2002) is used to parse En-glish sentences and KyTea (Neubig et al., 2011) isused as a Japanese tokenizer.
We generated wordalignments using GIZA++ (Och and Ney, 2003)and trained a Kneser-Ney smoothed 5-gram LMusing SRILM (Stolcke et al., 2011).
MinimumError Rate Training (MERT) (Och, 2003) is usedfor tuning to optimize BLEU.
MERT is replicatedthree times to provide performance stability on testset evaluation (Clark et al., 2011).We used BLEU (Papineni et al., 2002) andRIBES (Isozaki et al., 2010a) as evaluation mea-sures of translation quality.
RIBES is an eval-uation method that focuses on word reorderinginformation, and is known to have high correla-tion with human judgement for language pairs thathave very different word order such as English-Japanese.5.2 ResultTable 3 shows translation quality for each com-bination of HF-feature, Reordering, and LexicalProcessing.
Scores in boldface indicate no sig-nificant difference in comparison with the con-dition that has highest translation quality usingthe bootstrap resampling method (Koehn, 2004)(p < 0.05).For PBMT, we can see that reordering plays anextremely important role, with the highest BLEUand RIBES scores being achieved when using Re-ordering preprocessing (line 3, 4).
Lexical Pro-cessing also provided a slight performance gainfor PBMT.When we applied Lexical Processing toPBMT, BLEU and RIBES scores were improved(line 1 vs 2), although this gain was not significantwhen Reordering was performed as well.Overall T2S without any preprocessingachieved better translation quality than all con-ditions of PBMT (line 1 of T2S vs line 1-4 ofPBMT).
In addition, BLEU and RIBES score ofT2S were clearly improved by Lexical Processing(line 2, 4, 6, 8 vs line 1, 3, 5, 7), and these scoresare the highest of all conditions.
On the otherhand, Reordering and HF-Feature addition had nopositive effect, and actually tended to slightly hurttranslation accuracy.5.3 Analysis of PreprocessingWith regards to PBMT, as previous works onpreordering have already indicated, BLEU andRIBES scores were significantly improved by Re-ordering.
In addition, Lexical Processing also con-Table 5: Optimized weight of HF-feature in eachconditionHF-feature ReorderingWord Weight ofProcessing HF-feature+ - - -0.00707078+ - + 0.00524676+ + - 0.156724+ + + -0.121326tributed to improve translation quality of PBMTslightly.
We also investigated the influencethat each element of Lexical Processing (pseudo-particle insertion, determiner elimination, singu-larization) had on translation quality, and foundthat the gains were mainly provided by particleinsertion, with little effect from determiner elim-ination or singularization.Although Reordering was effective for PBMT,it did not provide any benefit for T2S.
This in-dicates that T2S can already conduct long dis-tance word reordering relatively correctly, andword alignment quality was not improved as muchas expected by closing the gap in word order be-tween the two languages.
This was verified by asubjective evaluation of the data, finding very fewmajor reordering issues in the sentences translatedby T2S.On the other hand, Lexical Processing func-tioned effectively for not only PBMT but also T2S.When added to the baseline, lexical processing onits own resulted in a gain of 0.57 BLEU, and 0.99RIBES points, a significant improvement, withsimilar gains being seen in other settings as well.Table 4 demonstrates a typical example of theimprovement of the translation result due to Lex-ical Processing.
It can be seen that translationperformance of particles (indicated by underlinedwords) was improved.
The underlined particle isin the direct object position of the verb that corre-sponds to ?comprises?
in English, and thus shouldbe given the object particle ??
wo?
as in the refer-ence and the system using Lexical Processing.
Onthe other hand, in the baseline system the genitive??
to?
is generated instead due to misaligned par-ticles being inserted in an incorrect position in thetranslation rules.5.4 Analysis of Feature AdditionOur experimental results indicated that translationquality is not improved by HF-feature addition(line 1-4 vs line 5-8).
We conjecture that the rea-son why HF-feature did not contribute to an im-39Table 3: Translation quality by combination of HF-feature, Reordering, and Lexical Processing.
Boldindicates results that are not statistically significantly different from the best result (39.60 BLEU in line4 and 79.47 RIBES in line 2).IDPBMT T2SHF-feature Reordering Lexical Processing BLEU RIBES BLEU RIBES1 - - - 32.11 69.06 38.94 78.482 - - + 33.16 70.19 39.51 79.473 - + - 37.62 77.56 38.44 78.484 - + + 37.77 77.71 39.60 79.265 + - - ?
?
38.74 78.336 + - + ?
?
39.29 79.237 + + - ?
?
38.48 78.448 + + + ?
?
39.38 79.21Table 4: Improvement of translation results due to Lexical ProcessingSource another connector 96 , which is matable with this cable connector 90 , comprises a plurality ofmale contacts 98 aligned in a row in an electrically insulative housing 97 as shown in the figure .Reference ????????????????????????????????????????????????????????????????????
?- Lexical Processing ????????????????????????????????????????????????????????????????????????
?+ Lexical Processing ???????????????????????????????????????????????????????????????????????
?provement in translation quality is that the reorder-ing quality achieved by T2S translation was al-ready sufficiently high, and the initial feature ledto confusion in MERT optimization.Table 5 shows the optimized weight of the HFfeature in each condition.
From this table, we cansee that in two of the conditions positive weightsare learned, and in two of the conditions negativeweights are learned.
This indicates that there is noconsistent pattern of learning weights that corre-spond to our intuition that head-final rules shouldreceive higher preference.It is possible that other optimization methods,or a more sophisticated way of inserting these fea-tures into the translation rules could help alleviatethese problems.6 ConclusionIn this paper, we analyzed the effect of applyingsyntactic preprocessing methods to syntax-basedSMT.
Additionally, we have adapted reorderingrules as a decoder feature.
The results showedthat lexical processing, specifically insertion ofpseudo-particles, contributed to improving trans-lation quality, and it was effective as preprocessingfor T2S.It should be noted that this paper, while demon-strating that the simple rule-based syntactic pro-cessing methods that have been useful for PBMTcan also contribute to T2S in English-Japanesetranslation, more work is required to ensure thatthis will generalize to other settings.
A next step inour inquiry is the generalization of these results toother proposed preprocessing techniques and otherlanguage pairs.
In addition, we would like to trytwo ways described below.
First, it is likely thatother tree transformations, for example changingthe internal structure of the tree by moving chil-dren to different nodes, would help in cases whereit is common to translate into highly divergent syn-tactic structures between the source and target lan-guages.
Second, we plan to investigate other waysof incorporating the preprocessing rules as a softconstraints, such as using n-best lists or forests toenode many possible sentence interpretations.ReferencesEleftherios Avramidis and Philipp Koehn.
2008.
En-riching morphologically poor languages for statisti-cal machine translation.
In Annual Meeting of the40Association for Computational Linguistics (ACL),pages 763?770.David Burkett and Dan Klein.
2012.
Transformingtrees to improve syntactic convergence.
In Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 863?872.Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah ASmith.
2011.
Better hypothesis testing for statisti-cal machine translation: Controlling for optimizerinstability.
In Annual Meeting of the Association forComputational Linguistics (ACL), pages 176?181.Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.2005.
Clause restructuring for statistical machinetranslation.
In Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 531?540.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and Sayori Shimohata.
2008.
Overview of thepatent translation task at the NTCIR-7 workshop.
InProceedings of the 7th NTCIR Workshop Meeting,pages 389?400.Jonathan Graehl, Kevin Knight, and Jonathan May.2008.
Training tree transducers.
ComputationalLinguistics, pages 391?427.Greg Hanneman and Alon Lavie.
2011.
Automaticcategory label coarsening for syntax-based machinetranslation.
In Workshop on Syntax and Structure inStatistical Translation, pages 98?106.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010a.
Automaticevaluation of translation quality for distant languagepairs.
In Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 944?952.Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, andKevin Duh.
2010b.
Head finalization: A simplereordering rule for SOV languages.
In Proceedingsof the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 244?251.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In AnnualMeeting of the Association for Computational Lin-guistics (ACL), pages 177?180.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 388?395.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 609?616.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximumentropy estimation for feature forests.
In Proceed-ings of the second international conference on Hu-man Language Technology Research, pages 292?297.Graham Neubig and Kevin Duh.
2014.
On the ele-ments of an accurate tree-to-string machine transla-tion system.
In Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 143?149.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.2011.
Pointwise prediction for robust, adaptablejapanese morphological analysis.
In Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 529?533.Graham Neubig.
2013.
Travatar: A forest-to-stringmachine translation engine based on tree transduc-ers.
Annual Meeting of the Association for Compu-tational Linguistics (ACL), page 91.Sonja Nie?en and Hermann Ney.
2000.
ImprovingSMT quality with morpho-syntactic analysis.
InProceedings of the 18th conference on Computa-tional linguistics-Volume 2, pages 1081?1085.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, pages 19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 311?318.Andreas Stolcke, Jing Zheng, Wen Wang, and VictorAbrash.
2011.
SRILM at sixteen: Update and out-look.
In IEEE Automatic Speech Recognition andUnderstanding Workshop (ASRU), page 5.Katsuhito Sudoh, Kevin Duh, Hajime Tsukada,Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki,and Jun ?ichi Tsujii.
2011.
NTT-UT statistical ma-chine translation in NTCIR-9 PatentMT.
In Pro-ceedings of NTCIR, pages 585?592.Akihiro Tamura, Taro Watanabe, Eiichiro Sumita, Hi-roya Takamura, and Manabu Okumura.
2013.
Part-of-speech induction in dependency trees for statisti-cal machine translation.
In Annual Meeting of theAssociation for Computational Linguistics (ACL),pages 841?851.41Fei Xia and Michael McCord.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In International Conference on Computa-tional Linguistics (COLING), page 508.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improvesmt for subject-object-verb languages.
In NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 245?253.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 523?530.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for ma-chine translation.
In North American Chapter of theAssociation for Computational Linguistics, pages256?263.42
