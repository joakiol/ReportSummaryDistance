Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206?214,Honolulu, October 2008. c?2008 Association for Computational LinguisticsForest-based Translation Rule ExtractionHaitao Mi11Key Lab.
of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, Chinahtmi@ict.ac.cnLiang Huang2,12Dept.
of Computer & Information ScienceUniversity of Pennsylvania3330 Walnut St., Levine HallPhiladelphia, PA 19104, USAlhuang3@cis.upenn.eduAbstractTranslation rule extraction is a fundamentalproblem in machine translation, especially forlinguistically syntax-based systems that needparse trees from either or both sides of the bi-text.
The current dominant practice only uses1-best trees, which adversely affects the ruleset quality due to parsing errors.
So we pro-pose a novel approach which extracts rulesfrom a packed forest that compactly encodesexponentially many parses.
Experiments showthat this method improves translation qualityby over 1 BLEU point on a state-of-the-arttree-to-string system, and is 0.5 points betterthan (and twice as fast as) extracting on 30-best parses.
When combined with our previouswork on forest-based decoding, it achieves a2.5 BLEU points improvement over the base-line, and even outperforms the hierarchicalsystem of Hiero by 0.7 points.1 IntroductionAutomatic extraction of translation rules is a funda-mental problem in statistical machine translation, es-pecially for many syntax-based models where trans-lation rules directly encode linguistic knowledge.Typically, these models extract rules using parsetrees from both or either side(s) of the bitext.
Theformer case, with trees on both sides, is often calledtree-to-tree models; while the latter case, with treeson either source or target side, include both tree-to-string and string-to-tree models (see Table 1).Leveraging from structural and linguistic informa-tion from parse trees, these models are believedto be better than their phrase-based counterparts insource target examples (partial)tree-to-tree Ding and Palmer (2005)tree-to-string Liu et al (2006); Huang et al (2006)string-to-tree Galley et al (2006)string-to-string Chiang (2005)Table 1: A classification of syntax-based MT.
The firstthree use linguistic syntax, while the last one only formalsyntax.
Our experiments cover the second type using apacked forest in place of the tree for rule-extraction.handling non-local reorderings, and have achievedpromising translation results.1However, these systems suffer from a major limi-tation, that the rule extractor only uses 1-best parsetree(s), which adversely affects the rule set qualitydue to parsing errors.
To make things worse, mod-ern statistical parsers are often trained on domainsquite different from those used in MT.
By contrast,formally syntax-based models (Chiang, 2005) do notrely on parse trees, yet usually perform better thanthese linguistically sophisticated counterparts.To alleviate this problem, an obvious idea is toextract rules from k-best parses instead.
However, ak-best list, with its limited scope, has too few vari-ations and too many redundancies (Huang, 2008).This situation worsens with longer sentences as thenumber of possible parses grows exponentially withthe sentence length and a k-best list will only capturea tiny fraction of the whole space.
In addition, manysubtrees are repeated across different parses, so it is1For example, in recent NIST Evaluations, some of thesemodels (Galley et al, 2006; Quirk et al, 2005; Liu et al, 2006)ranked among top 10.
See http://www.nist.gov/speech/tests/mt/.206IPNPx1:NPB CCyu?x2:NPBx3:VPB ?
x1 x3 with x2Figure 1: Example translation rule r1.
The Chinese con-junction yu?
?and?
is translated into English prep.
?with?.also inefficient to extract rules separately from eachof these very similar trees (or from the cross-productof k2 similar tree-pairs in tree-to-tree models).We instead propose a novel approach that ex-tracts rules from packed forests (Section 3), whichcompactly encodes many more alternatives than k-best lists.
Experiments (Section 5) show that forest-based extraction improves BLEU score by over 1point on a state-of-the-art tree-to-string system (Liuet al, 2006; Mi et al, 2008), which is also 0.5points better than (and twice as fast as) extractingon 30-best parses.
When combined with our previ-ous orthogonal work on forest-based decoding (Miet al, 2008), the forest-forest approach achieves a2.5 BLEU points improvement over the baseline,and even outperforms the hierarchical system of Hi-ero, one of the best-performing systems to date.Besides tree-to-string systems, our method is alsoapplicable to other paradigms such as the string-to-tree models (Galley et al, 2006) where the rules arein the reverse order, and easily generalizable to pairsof forests in tree-to-tree models.2 Tree-based TranslationWe review in this section the tree-based approach tomachine translation (Liu et al, 2006; Huang et al,2006), and its rule extraction algorithm (Galley etal., 2004; Galley et al, 2006).2.1 Tree-to-String SystemCurrent tree-based systems perform translation intwo separate steps: parsing and decoding.
The inputstring is first parsed by a parser into a 1-best tree,which will then be converted to a target languagestring by applying a set of tree-to-string transforma-tion rules.
For example, consider the following ex-ample translating from Chinese to English:(a) Bu`sh??
yu?
Sha?lo?ng ju?x?
?ng le hu?`ta?n?
1-best parser(b) IPNPNPBBu`sh??CCyu?NPBSha?lo?ngVPBVVju?x??ngASleNPBhu?`ta?nr1?
(c) NPBBu`sh??VPBVVju?x?
?ngASleNPBhu?`ta?nwith NPBSha?lo?ngr2 ?
r3 ?
(d) Bush held NPBhu?`ta?nwith NPBSha?lo?ngr4 ?
r5 ?
(e) Bush held a meeting with Sharonr2 NPB(Bu`sh??)?
Bushr3 VPB(VV(ju?x?
?ng) AS(le) x1:NPB)?
held x1r4 NPB(Sha?lo?ng)?
Sharonr5 NPB(hu?`ta?n)?
a meetingFigure 2: Example derivation of tree-to-string translation,with rules used.
Each shaded region denotes a tree frag-ment that is pattern-matched with the rule being applied.
(1) Bu`sh??Bushyu?and/withSha?lo?ngSharon1ju?x?
?ngholdlepast.hu?`ta?nmeeting2?Bush held a meeting2 with Sharon1?Figure 2 shows how this process works.
The Chi-nese sentence (a) is first parsed into a parse tree (b),which will be converted into an English string in 5steps.
First, at the root node, we apply rule r1 shownin Figure 1, which translates the Chinese coordina-tion construction (?...
and ...?)
into an English prepo-sitional phrase.
Then, from step (c) we continue ap-plying rules to untranslated Chinese subtrees, untilwe get the complete English translation in (e).22We swap the 1-best and 2-best parses of the example sen-tence from our earlier paper (Mi et al, 2008), since the current1-best parse is easier to illustrate the rule extraction algorithm.207IP?Bush .. Sharon?NP?Bush ?
with Sharon?NPB?Bush?Bu`sh?
?CC?with?yu?NPB?Sharon?Sha?lo?ngVPB?held ..
meeting?VV?held?ju?x?
?ngAS?held?leNPB?a meeting?hu?`ta?n(minimal) rules extractedIP (NP(x1:NPB x2:CC x3:NPB) x4:VPB)?
x1 x4 x2 x3CC (yu?)?
withNPB (Bu`sh??)?
BushNPB (Sha?lo?ng)?
SharonVPB (VV(ju?x?
?ng) AS(le) x1:NPB)?
held x1NPB (hu?`ta?n)?
a meetingBush held a meeting with SharonFigure 3: Tree-based rule extraction (Galley et al, 2004).
Each non-leaf node in the tree is annotated with its targetspan (below the node), where ?
denotes a gap, and non-faithful spans are crossed out.
Shadowed nodes are admissible,with contiguous and faithful spans.
The first two rules can be ?composed?
to form rule r1 in Figure 1.IP0, 6?Bush .. Sharon?e2NP0, 3?Bush ?
with Sharon?e3NPB0, 1?Bush?Bu`sh?
?CC1, 2?with?yu?VP1, 6?held .. Sharon?PP1, 3?with Sharon?P1, 2?with?NPB2, 3?Sharon?Sha?lo?ngVPB3, 6?held .. meeting?VV3, 4?held?ju?x?
?ngAS4, 5?held?leNPB5, 6?a meeting?hu?`ta?ne1extra (minimal) rules extractedIP (x1:NPB x2:VP)?
x1 x2VP (x1:PP x2:VPB)?
x2 x1PP (x1:P x2:NPB)?
x1 x2P (yu?)?
withBush held a meeting with SharonFigure 4: Forest-based rule extraction.
Solid hyperedges correspond to the 1-best tree in Figure 3, while dashed hyper-edges denote the alternative parse interpreting yu?
as a preposition in Figure 5.More formally, a (tree-to-string) translation rule(Galley et al, 2004; Huang et al, 2006) is a tuple?lhs(r), rhs(r), ?
(r)?, where lhs(r) is the source-side tree fragment, whose internal nodes are la-beled by nonterminal symbols (like NP and VP),and whose frontier nodes are labeled by source-language words (like ?yu??)
or variables from a setX = {x1, x2, .
.
.
}; rhs(r) is the target-side stringexpressed in target-language words (like ?with?)
andvariables; and ?
(r) is a mapping from X to nonter-minals.
Each variable xi ?
X occurs exactly once inlhs(r) and exactly once in rhs(r).
For example, forrule r1 in Figure 1,lhs(r1) = IP ( NP(x1 CC(yu?)
x2) x3),rhs(r1) = x1 x3 with x2,?
(r1) = {x1: NPB, x2: NPB, x3: VPB}.These rules are being used in the reverse direction ofthe string-to-tree transducers in Galley et al (2004).2082.2 Tree-to-String Rule ExtractionWe now briefly explain the algorithm of Galley et al(2004) that can extract these translation rules from aword-aligned bitext with source-side parses.Consider the example in Figure 3.
The basic ideais to decompose the source (Chinese) parse into a se-ries of tree fragments, each of which will form a rulewith its corresponding English translation.
However,not every fragmentation can be used for rule extrac-tion, since it may or may not respect the alignmentand reordering between the two languages.
So wesay a fragmentation is well-formed with respect toan alignment if the root node of every tree fragmentcorresponds to a contiguous span on the target side;the intuition is that there is a ?translational equiva-lence?
between the subtree rooted at the node andthe corresponding target span.
For example, in Fig-ure 3, each node is annotated with its correspondingEnglish span, where the NP node maps to a non-contiguous one ?Bush ?
with Sharon?.More formally, we need a precise formulationto handle the cases of one-to-many, many-to-one,and many-to-many alignment links.
Given a source-target sentence pair (?, ?)
with alignment a, the (tar-get) span of node v is the set of target words alignedto leaf nodes yield(v) under node v:span(v) , {?i ?
?
| ?
?j ?
yield(v), (?j , ?i) ?
a}.For example, in Figure 3, every node in the parse treeis annotated with its corresponding span below thenode, where most nodes have contiguous spans ex-cept for the NP node which maps to a gapped phrase?Bush ?
with Sharon?.
But contiguity alone is notenough to ensure well-formedness, since there mightbe words within the span aligned to source wordsuncovered by the node.
So we also define a span sto be faithful to node v if every word in it is onlyaligned to nodes dominated by v, i.e.:?
?i ?
s, (?j , ?i) ?
a?
?j ?
yield(v).For example, sibling nodes VV and AS in the treehave non-faithful spans (crossed out in the Figure),because they both map to ?held?, thus neither ofthem can be translated to ?held?
alone.
In this case,a larger tree fragment rooted at VPB has to beextracted.
Nodes with non-empty, contiguous, andfaithful spans form the admissible set (shaded nodesIP0,6NPB0,1Bu`sh??VP1,6PP1,3P1,2yu?NPB2,3Sha?lo?ngVPB3,6ju?x?
?ng le hu?`ta?nFigure 5: An alternative parse of the Chinese sentence,with yu?
as a preposition instead of a conjunction; com-mon parts shared with 1-best parse in Fig.
3 are elided.in the figure), which serve as potential cut-points forrule extraction.3With the admissible set computed, rule extractionis as simple as a depth-first traversal from the root:we ?cut?
the tree at all admissible nodes to form treefragments and extract a rule for each fragment, withvariables matching the admissible descendant nodes.For example, the tree in Figure 3 is cut into 6 pieces,each of which corresponds to a rule on the right.These extracted rules are called minimal rules,which can be glued together to form composed ruleswith larger tree fragments (e.g.
r1 in Fig.
1) (Galleyet al, 2006).
Our experiments use composed rules.3 Forest-based Rule ExtractionWe now extend tree-based extraction algorithm fromthe previous section to work with a packed forestrepresenting exponentially many parse trees.3.1 Packed ForestInformally, a packed parse forest, or forest inshort, is a compact representation of all the deriva-tions (i.e., parse trees) for a given sentence undera context-free grammar (Earley, 1970; Billot andLang, 1989).
For example, consider again the Chi-nese sentence in Example (1) above, which has(at least) two readings depending on the part-of-speech of the word yu?
: it can be either a conjunction(CC ?and?)
as shown in Figure 3, or a preposition(P ?with?)
as shown in Figure 5, with only PP andVPB swapped from the English word order.3Admissible set (Wang et al, 2007) is also known as ?fron-tier set?
(Galley et al, 2004).
For simplicity of presentation, weassume every target word is aligned to at least one source word;see Galley et al (2006) for handling unaligned target words.209These two parse trees can be represented as asingle forest by sharing common subtrees such asNPB0, 1 and VPB3, 6, as shown in Figure 4.
Such aforest has a structure of a hypergraph (Huang andChiang, 2005), where items like NP0, 3 are callednodes, whose indices denote the source span, andcombinations likee1 : IP0, 6 ?
NPB0, 3 VP3, 6we call hyperedges.
We denote head(e) and tails(e)to be the consequent and antecedant items of hyper-edge e, respectively.
For example,head(e1) = IP0, 6, tails(e1) = {NPB0, 3,VP3, 6}.We also denote BS (v) to be the set of incoming hy-peredges of node v, being different ways of derivingit.
For example, in Figure 4, BS (IP0, 6) = {e1, e2}.3.2 Forest-based Rule Extraction AlgorithmLike in tree-based extraction, we extract rules froma packed forest F in two steps:(1) admissible set computation (where to cut), and(2) fragmentation (how to cut).It turns out that the exact formulation developedfor admissible set in the tree-based case can be ap-plied to a forest without any change.
The fragmen-tation step, however, becomes much more involvedsince we now face a choice of multiple parse hyper-edges at each node.
In other words, it becomes non-deterministic how to ?cut?
a forest into tree frag-ments, which is analogous to the non-deterministicpattern-match in forest-based decoding (Mi et al,2008).
For example there are two parse hyperedgese1 and e2 at the root node in Figure 4.
When we fol-low one of them to grow a fragment, there again willbe multiple choices at each of its tail nodes.
Like intree-based case, a fragment is said to be completeif all its leaf nodes are admissible.
Otherwise, an in-complete fragment can grow at any non-admissiblefrontier node v, where following each parse hyper-edge at v will split off a new fragment.
For example,following e2 at the root node will immediately leadus to two admissible nodes, NPB0, 1 and VP1, 6(we will highlight admissible nodes by gray shadesAlgorithm 1 Forest-based Rule Extraction.Input: forest F , target sentence ?
, and alignment aOutput: minimal rule setR1: admset ?
ADMISSIBLE(F, ?, a) ?
admissible set2: for each v ?
admset do3: open ?
?
?
queue of active fragments4: for each e ?
BS (v) do ?
incoming hyperedges5: front ?
tails(e) \ admset ?
initial frontier6: open .append(?
{e}, front?
)7: while open 6= ?
do8: ?frag , front?
?
open .pop() ?
active fragment9: if front = ?
then10: generate a rule r using fragment frag11: R.append(r)12: else ?
incomplete: further expand13: u?
front .pop() ?
a frontier node14: for each e ?
BS (u) do15: front ?
?
front ?
(tails(e) \ admset)16: open .append(?frag ?
{e}, front ??
)in this section like in Figures 3 and 4).
So this frag-ment, frag1 = {e2}, is now complete and we canextract a rule,IP (x1:NPB x2:VP)?
x1 x2.However, following the other hyperedge e1IP0, 6 ?
NP0, 3 VPB3, 6will leave the new fragment frag2 = {e1} incom-plete with one non-admissible node NP0, 3.
We thengrow frag2 at this node by choosing hyperedge e3NP0, 3 ?
NPB0, 1 CC1, 2 NPB2, 3 ,and spin off a new fragment frag3 = {e1, e3}, whichis now complete since all its four leaf nodes are ad-missible.
We then extract a rule with four variables:IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB)?
x1 x4 x2 x3.This procedure is formalized by a breadth-firstsearch (BFS) in Pseudocode 1.
The basic idea is tovisit each frontier node v, and keep a queue openof actively growing fragments rooted at v. We keepexpanding incomplete fragments from open , and ex-tract a rule if a complete fragment is found (line 10).Each fragment is associated with a frontier (variable210front in the Pseudocode), being the subset of non-admissible leaf nodes (recall that expansion stops atadmissible nodes).
So each initial fragment alonghyperedge e is associated with an initial frontier(line 5), front = tails(e) \ admset .A fragment is complete if its frontier is empty(line 9), otherwise we pop one frontier node u toexpand, spin off new fragments by following hyper-edges of u, and update the frontier (lines 14-16), un-til all active fragments are complete and open queueis empty (line 7).A single parse tree can also be viewed as a triv-ial forest, where each node has only one incominghyperedge.
So the Galley et al (2004) algorithm fortree-based rule extraction (Sec.
2.2) can be consid-ered a special case of our algorithm, where the queueopen always contains one single active fragment.3.3 Fractional Counts and Rule ProbabilitiesIn tree-based extraction, for each sentence pair, eachrule extracted naturally has a count of one, whichwill be used in maximum-likelihood estimation ofrule probabilities.
However, a forest is an implicitcollection of many more trees, each of which, whenenumerated, has its own probability accumulatedfrom of the parse hyperedges involved.
In otherwords, a forest can be viewed as a virtual weightedk-best list with a huge k. So a rule extracted from anon 1-best parse, i.e., using non 1-best hyperedges,should be penalized accordingly and should have afractional count instead of a unit one, similar to theE-step in EM algorithms.Inspired by the parsing literature on pruning(Charniak and Johnson, 2005; Huang, 2008) we pe-nalize a rule r by the posterior probability of its treefragment frag = lhs(r).
This posterior probability,notated ??
(frag), can be computed in an Inside-Outside fashion as the product of three parts: the out-side probability of its root node, the probabilities ofparse hyperedges involved in the fragment, and theinside probabilities of its leaf nodes,??
(frag) =?(root(frag))?
?e ?
fragP(e)?
?v ?
yield(frag)?
(v)(2)where ?(?)
and ?(?)
denote the outside and insideprobabilities of tree nodes, respectively.
For examplein Figure 4,??
({e2, e3}) = ?
(IP0, 6) ?
P(e2) ?
P(e3)?
?
(NPB0, 1)?
(CC1, 2)?
(NPB2, 3)?
(VPB3, 6).Now the fractional count of rule r is simplyc(r) = ??(lhs(r))??
(TOP) (3)where TOP denotes the root node of the forest.Like in the M-step in EM algorithm, we nowextend the maximum likelihood estimation to frac-tional counts for three conditional probabilities re-garding a rule, which will be used in the experi-ments:P(r | lhs(r)) = c(r)?r?:lhs(r?
)=lhs(r) c(r?
), (4)P(r | rhs(r)) = c(r)?r?:rhs(r?
)=rhs(r) c(r?
), (5)P(r |root(lhs(r)))=c(r)?r?:root(lhs(r?
))=root(lhs(r)) c(r?).
(6)4 Related WorkThe concept of packed forest has been previouslyused in translation rule extraction, for example inrule composition (Galley et al, 2006) and tree bina-rization (Wang et al, 2007).
However, both of theseefforts only use 1-best parses, with the second onepacking different binarizations of the same tree in aforest.
Nevertheless we suspect that their extractionalgorithm is in principle similar to ours, althoughthey do not provide details of forest-based fragmen-tation (Algorithm 1) which we think is non-trivial.The forest concept is also used in machine transla-tion decoding, for example to characterize the searchspace of decoding with integrated language models(Huang and Chiang, 2007).
The first direct appli-cation of parse forest in translation is our previouswork (Mi et al, 2008) which translates a packed for-est from a parser; it is also the base system in ourexperiments (see below).
This work, on the otherhand, is in the orthogonal direction, where we uti-lize forests in rule extraction instead of decoding.211Our experiments will use both default 1-best decod-ing and forest-based decoding.
As we will see in thenext section, the best result comes when we combinethe merits of both, i.e., using forests in both rule ex-traction and decoding.There is also a parallel work on extracting rulesfrom k-best parses and k-best alignments (Venu-gopal et al, 2008), but both their experiments andour own below confirm that extraction on k-bestparses is neither efficient nor effective.5 Experiments5.1 SystemOur experiments are on Chinese-to-English trans-lation based on a tree-to-string system similar to(Huang et al, 2006; Liu et al, 2006).
Given a 1-best tree T , the decoder searches for the best deriva-tion d?
among the set of all possible derivations D:d?
= arg maxd?D?0 log P(d | T ) + ?1 log Plm(?
(d))+ ?2|d|+ ?3|?
(d)|(7)where the first two terms are translation and lan-guage model probabilities, ?
(d) is the target string(English sentence) for derivation d, and the last twoterms are derivation and translation length penalties,respectively.
The conditional probability P(d | T )decomposes into the product of rule probabilities:P(d | T ) =?r?dP(r).
(8)Each P(r) is in turn a product of five probabilities:P(r) =P(r | lhs(r))?4 ?
P(r | rhs(r))?5?
P(r | root(lhs(r)))?6?
Plex(lhs(r) | rhs(r))?7?
Plex(rhs(r) | lhs(r))?8(9)where the first three are conditional probabilitiesbased on fractional counts of rules defined in Sec-tion 3.3, and the last two are lexical probabilities.These parameters ?1 .
.
.
?8 are tuned by minimumerror rate training (Och, 2003) on the dev sets.
Werefer readers to Mi et al (2008) for details of thedecoding algorithm.0.2400.2420.2440.2460.2480.2500.2520.2540  1  2  3  4  5  6BLEUscoreaverage extracting time (secs/1000 sentences)1-bestpe=2pe=5pe=8k=30forest extractionk-best extractionFigure 6: Comparison of extraction time and BLEUscore: forest-based vs.1-best and 30-best.rules from... extraction decoding BLEU1-best trees 0.24 1.74 0.243030-best trees 5.56 3.31 0.2488forest: pe=8 2.36 3.40 0.2533Pharaoh - - 0.2297Table 2: Results with different rule extraction methods.Extraction and decoding columns are running times insecs per 1000 sentences and per sentence, respectively.We use the Chinese parser of Xiong et al (2005)to parse the source side of the bitext.
FollowingHuang (2008), we also modify this parser to out-put a packed forest for each sentence, which canbe pruned by the marginal probability-based inside-outside algorithm (Charniak and Johnson, 2005;Huang, 2008).
We will first report results trainedon a small-scaled dataset with detailed analysis, andthen scale to a larger one, where we also combine thetechnique of forest-based decoding (Mi et al, 2008).5.2 Results and Analysis on Small DataTo test the effect of forest-based rule extraction, weparse the training set into parse forests and use threelevels of pruning thresholds: pe = 2, 5, 8.Figure 6 plots the extraction speed and transla-tion quality of forest-based extraction with variouspruning thresholds, compared to 1-best and 30-bestbaselines.
Using more than one parse tree apparentlyimproves the BLEU score, but at the cost of muchslower extraction, since each of the top-k trees has tobe processed individually although they share many212rules from ... total # on dev new rules used1-best trees 440k 90k -30-best trees 1.2M 130k 8.71%forest: pe=8 3.3M 188k 16.3%Table 3: Statistics of rules extracted from small data.
Thelast column shows the ratio of new rules introduced bynon 1-best parses being used in 1-best derivations.common subtrees.
Forest extraction, by contrast, ismuch faster thanks to packing and produces consis-tently better BLEU scores.
With pruning thresholdpe = 8, forest-based extraction achieves a (case in-sensitive) BLEU score of 0.2533, which is an ab-solute improvement of 1.0% points over the 1-bestbaseline, and is statistically significant using thesign-test of Collins et al (2005) (p < 0.01).
Thisis also 0.5 points better than (and twice as fast as)extracting on 30-best parses.
These BLEU score re-sults are summarized in Table 2, which also showsthat decoding with forest-extracted rules is less thantwice as slow as with 1-best rules, and only fraction-ally slower than with 30-best rules.We also investigate the question of how oftenrules extracted from non 1-best parses are used bythe decoder.
Table 3 shows the numbers of rulesextracted from 1-best, 30-best and forest-based ex-tractions, and the numbers that survive after filter-ing on the dev set.
Basically in the forest-based casewe can use about twice as many rules as in the 1-best case, or about 1.5 times of 30-best extraction.But the real question is, are these extra rules reallyuseful in generating the final (1-best) translation?The last row shows that 16.3% of the rules usedin 1-best derivations are indeed only extracted fromnon 1-best parses in the forests.
Note that this is astronger condition than changing the distribution ofrules by considering more parses; here we introducenew rules never seen on any 1-best parses.5.3 Final Results on Large DataWe also conduct experiments on a larger trainingdataset, FBIS, which contains 239K sentence pairswith about 6.9M/8.9M words in Chinese/English,respectively.
We also use a bigger trigram modeltrained on the first 1/3 of the Xinhua portion of Gi-gaword corpus.
To integrate with forest-based de-coding, we use both 1-best trees and packed forestsextract.
\ decoding 1-best tree forest: pd=101-best trees 0.2560 0.267430-best trees 0.2634 0.2767forest: pe=5 0.2679 0.2816Hiero 0.2738Table 4: BLEU score results trained on large data.during both rule extraction and decoding phases.Since the data scale is larger than the small data, weare forced to use harsher pruning thresholds, withpe = 5 for extraction and pd = 10 for decoding.The final BLEU score results are shown in Ta-ble 4.
With both tree-based and forest-based decod-ing, rules extracted from forests significantly outper-form those extracted from 1-best trees (p < 0.01).The final result with both forest-based extractionand forest-based decoding reaches a BLEU score of0.2816, outperforming that of Hiero (Chiang, 2005),one of the best performing systems to date.
These re-sults confirm that our novel forest-based rule extrac-tion approach is a promising direction for syntax-based machine translation.6 Conclusion and Future WorkIn this paper, we have presented a novel approachthat extracts translation rules from a packed forestencoding exponentially many trees, rather than from1-best or k-best parses.
Experiments on a state-of-the-art tree-to-string system show that this methodimproves BLEU score significantly, with reasonableextraction speed.
When combined with our previ-ous work on forest-based decoding, the final resultis even better than the hierarchical system Hiero.For future work we would like to apply this ap-proach to other types of syntax-based translationsystems, namely the string-to-tree systems (Galleyet al, 2006) and tree-to-tree systems.AcknowledgementThis work was funded by National Natural Sci-ence Foundation of China, Contracts 60736014and 60573188, and 863 State Key Project No.2006AA010108 (H. M.), and by NSF ITR EIA-0205456 (L. H.).
We would also like to thank QunLiu for supporting this work, and the three anony-mous reviewers for improving the earlier version.213ReferencesSylvie Billot and Bernard Lang.
1989.
The structure ofshared forests in ambiguous parsing.
In Proceedingsof ACL ?89, pages 143?151.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine-grained n-best parsing and discriminative rerank-ing.
In Proceedings of the 43rd ACL, Ann Arbor, MI.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd ACL, Ann Arbor, MI.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL, pages 531?540,Ann Arbor, Michigan, June.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probablisitic synchronous dependency in-sertion grammars.
In Proceedings of the 43rd ACL,Ann Arbor, MI.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 13(2):94?102.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of HLT-NAACL, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of COLING-ACL.Liang Huang and David Chiang.
2005.
Better k-bestParsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technologies (IWPT-2005).Liang Huang and David Chiang.
2007.
Forest rescor-ing: Fast decoding with integrated language models.In Proceedings of ACL, Prague, Czech Rep., June.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA, Boston,MA, August.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings of theACL: HLT, Columbus, OH, June.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of COLING-ACL, pages 609?616.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL: HLT,Columbus, OH.Franz Joseph Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL,pages 160?167.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically informedphrasal smt.
In Proceedings of the 43rd ACL, Ann Ar-bor, MI.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2008.
Wider pipelines: N-bestalignments and parses in mt training.
In Proceedingsof AMTA, Honolulu, Hawaii.Wei Wang, Kevin Knight, and Daniel Marcu.
2007.Binarizing syntax trees to improve syntax-based ma-chine translation accuracy.
In Proceedings of EMNLP,Prague, Czech Rep., July.Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin.2005.
Parsing the penn chinese treebank with seman-tic knowledge.
In Proceedings of IJCNLP 2005, pages70?81.214
