Proceedings of the EACL 2012 Student Research Workshop, pages 64?73,Avignon, France, 26 April 2012. c?2012 Association for Computational LinguisticsHierarchical Bayesian Language Modellingfor the Linguistically InformedJan A. BothaDepartment of Computer ScienceUniversity of Oxford, UKjan.botha@cs.ox.ac.ukAbstractIn this work I address the challenge of aug-menting n-gram language models accord-ing to prior linguistic intuitions.
I arguethat the family of hierarchical Pitman-Yorlanguage models is an attractive vehiclethrough which to address the problem, anddemonstrate the approach by proposing amodel for German compounds.
In an em-pirical evaluation, the model outperformsthe Kneser-Ney model in terms of perplex-ity, and achieves preliminary improvementsin English-German translation.1 IntroductionThe importance of effective language models inmachine translation (MT) and automatic speechrecognition (ASR) is widely recognised.
n-grammodels, in particular ones using Kneser-Ney(KN) smoothing, have become the standardworkhorse for these tasks.
These models are notideal for languages that have relatively free wordorder and/or complex morphology.
The ability toencode additional linguistic intuitions into modelsthat already have certain attractive properties is animportant piece of the puzzle of improving ma-chine translation quality for those languages.
Butdespite their widespread use, KN n-gram mod-els are not easily extensible with additional modelcomponents that target particular linguistic phe-nomena.I argue in this paper that the family of hierarchi-cal Pitman-Yor language models (HPYLM) (Teh,2006; Goldwater et al, 2006) are suitable forinvestigations into more linguistically-informedn-gram language models.
Firstly, the flexibilityto specify arbitrary back-off distributions makes iteasy to incorporate multiple models into a largern-gram model.
Secondly, the Pitman-Yor processprior (Pitman and Yor, 1997) generates distribu-tions that are well-suited to a variety of power-law behaviours, as is often observed in language.Catering for a variety of those is important sincethe frequency distributions of, say, suffixes, couldbe quite different from that of words.
KN smooth-ing is less flexibility in this regard.
And thirdly,the basic inference algorithms have been paral-lelised (Huang and Renals, 2009), which shouldin principle allow the approach to still scale tolarge data sizes.As a test bed, I consider compounding in Ger-man, a common phenomenon that creates chal-lenges for machine translation into German.2 Background and Related Workn-gram language models assign probabilities toword sequences.
Their key approximation is thata word is assumed to be fully determined by n?1words preceding it, which keeps the number of in-dependent probabilities to estimate in a range thatis computationally attractive.
This basic modelstructure, largely devoid of syntactic insight, issurprisingly effective at biasing MT and ASR sys-tems toward more fluent output, given a suitablechoice of target language.But the real challenge in constructing n-grammodels, as in many other probabilistic settings, ishow to do smoothing, since the vast majority oflinguistically plausible n-grams will occur rarelyor be absent altogether from a training corpus,which often renders empirical model estimatesmisleading.
The general picture is that probabilitymass must be shifted away from some events andredistributed across others.The method of Kneser and Ney (1995) and64its later modified version (Chen and Goodman,1998) generally perform best at this smoothing,and are based on the idea that the number ofdistinct contexts a word appears in is an impor-tant factor in determining the probability of thatword.
Part of this smoothing involves discount-ing the counts of n-grams in the training data;the modified version uses different levels of dis-counting depending on the frequency of the count.These methods were designed with surface worddistributions, and are not necessarily suitable forsmoothing distributions of other kinds of surfaceunits.Bilmes and Kirchhoff (2003) proposed a moregeneral framework for n-gram language mod-elling.
Their Factored Language Model (FLM)views a word as a vector of features, such that aparticular feature value is generated conditionalon some history of preceding feature values.
Thisallowed the inclusion of n-gram models over se-quences of elements like PoS tags and semanticclasses.
In tandem, they proposed more compli-cated back-off paths; for example, trigrams canback-off to two underlying bigram distributions,one dropping the left-most context word and theother the right-most.
With the right combina-tion of features and back-off structure they gotgood perplexity reductions, and obtained someimprovements in translation quality by applyingthese ideas to the smoothing of the bilingualphrase table (Yang and Kirchhoff, 2006).My approach has some similarity to the FLM:both decompose surface word forms into elementsthat are generated from unrelated conditional dis-tributions.
They differ predominantly along twodimensions: the types of decompositions and con-ditioning possible, and my use of a particularBayesian prior for handling smoothing.In addition to the HPYLM for n-gram lan-guage modelling (Teh, 2006), models based onthe Pitman-Yor process prior have also been ap-plied to good effect in word segmentation (Gold-water et al, 2006; Mochihashi et al, 2009) andspeech recognition (Huang and Renals, 2007;Neubig et al, 2010).
The Graphical Pitman-Yorprocess enables branching back-off paths, whichI briefly revisit in ?7, and have proved effectivein language model domain-adaptation (Wood andTeh, 2009).
Here, I extend this general line ofinquiry by considering how one might incorpo-rate linguistically informed sub-models into theHPYLM framework.3 Compound NounsI focus on compound nouns in this work for tworeasons: Firstly, compounding is in general a veryproductive process, and in some languages (in-cluding German, Swedish and Dutch) they arewritten as single orthographic units.
This in-creases data sparsity and creates significant chal-lenges for NLP systems that use whitespace toidentify their elementary modelling units.
Aproper account of compounds in terms of theircomponent words therefore holds the potential ofimproving the performance of such systems.Secondly, there is a clear linguistic intuition toexploit: the morphosyntactic properties of thesecompounds are often fully determined by the headcomponent within the compound.
For example,in ?Geburtstagskind?
(birthday kid), it is ?Kind?that establishes this compound noun as singularneuter, which determine how it would need toagree with verbs, articles and adjectives.
In thenext section, I propose a model in the suggestedframework that encodes this intuition.The basic structure of German compoundscomprises a head component, preceded by one ormore modifier components, with optional linkerelements between consecutive components (Gold-smith and Reutter, 1998).Examples?
The basic form is just the concatenation of twonounsAuto + Unfall = Autounfall (car crash)?
Linker elements are sometimes added be-tween componentsKu?che + Tisch = Ku?chentisch (kitchen table)?
Components can undergo stemming duringcompositionSchule + Hof = Schulhof (schoolyard)?
The process is potentially recursive(Geburt + Tag) + Kind = Geburtstag + Kind= Geburtstagskind (birthday kid)The process is not limited to using nouns ascomponents, for example, the numeral in Zwei-Euro-Mu?nze (two Euro coin) or the verb ?fahren?
(to drive) in Fahrzeug (vehicle).
I will treat allthese cases the same.653.1 Fluency amid sparsityConsider the following example from the trainingcorpus used in the subsequent evaluations:de: Die Neuinfektionen u?bersteigen weiterhin dieBehandlungsbemu?hungen.en: New infections continue to outpace treatment ef-forts.The corpus contains numerous other compoundsending in ?infektionen?
(16) or ?bemu?hungen?(117).
A standard word-based n-gram modeldiscriminates among those alternatives using asmany independent parameters.However, we could gauge the approximate syn-tactic fluency of the sentence almost as well if weignore the compound modifiers.
Collapsing allthe variants in this way reduces sparsity and yieldsbetter n-gram probability estimates.To account for the compound modifiers, a sim-ple approach is to use a reverse n-gram languagemodel over compound components, without con-ditioning on the sentential context.
Such a modelessentially answers the question, ?Given that theword ends in ?infektionen?, what modifier(s), ifany, are likely to precede it??
The vast majority ofnouns will never occur in that position, meaningthat the conditional distributions will be sharplypeaked.mit der Draht?seil?bahnFigure 1: Intuition for the proposed generative pro-cess of a compound word: The context generates thehead component, which generates a modifier compo-nent, which in turn generates another modifier.
(Trans-lation: ?with the cable car?
)3.2 Related Work on CompoundsIn machine translation and speech recognition,one approach has been to split compounds as apreprocessing step and merge them back togetherduring postprocessing, while using otherwise un-modified NLP systems.
Frequency-based meth-ods have been used for determining how aggres-sively to split (Koehn and Knight, 2003), sincethe maximal, linguistically correct segmentationis not necessarily optimal for translation.
Thisgave rise to slight improvements in machine trans-lation evaluations (Koehn et al, 2008), with fine-tuning explored in (Stymne, 2009).
Similar ideashave also been employed for speech recognition(Berton et al, 1996) and predictive-text input(Baroni and Matiasek, 2002), where single-tokencompounds also pose challenges.4 Model Description4.1 HPYLMFormally speaking, an n-gram model is an(n?
1)-th order Markov model that approxi-mates the joint probability of a sequence ofwords w asP (w) ?|w|?i=1P (wi|wi?n+1, .
.
.
, wi?1),for which I will occasionally abbreviate a con-text [wi, .
.
.
, wj ] as u.
In the HPYLM, the condi-tional distributions P (w|u) are smoothed by plac-ing Pitman-Yor process priors (PYP) over them.The PYP is defined through its base distribution,and a strength (?)
and discount (d) hyperparame-ter that control its deviation away from its mean(which equals the base distribution).LetG[u,v] be the PYP-distributed trigram distri-bution P (w|u, v).
The hierarchy arises by usingas base distribution for the prior of G[u,v] anotherPYP-distributedG[v], i.e.
the distributionP (w|v).The recursion bottoms out at the unigram distri-bution G?, which is drawn from a PYP with basedistribution equal to the uniform distribution overthe vocabularyW .
The hyperparameters are tiedacross all priors with the same context length |u|,and estimated during training.G0 = Uniform(|W|)G?
?
PY (d0, ?0, G0)...Gpi(u) ?
PY (d|pi(u)|, ?|pi(u)|, Gpi(pi(u)))Gu ?
PY (d|u|, ?|u|, Gpi(u))w ?
Gu,where pi(u) truncates the context u by droppingthe left-most word in it.4.2 HPYLM+cDefine a compound word w?
as a sequence ofcomponents [c1, .
.
.
, ck], plus a sentinel symbol $marking either the left or the right boundary of theword, depending on the direction of the model.
Tomaintain generality over this choice of direction,66let ?
be an index set over the positions, such thatc?1 always designates the head component.Following the motivation in ?3.1, I set up themodel to generate the head component c?1 condi-tioned on the word context u, while the remainingcomponents w?
\ c?1 are generated by some modelF , independently of u.To encode this, I modify the HPYLM in twoways: 1) Replace the support with the reduced vo-cabularyM, the set of unique components c ob-tained when segmenting the items in W .
2) Addan additional level of conditional distributionsHu(with |u| = n?
1) where items fromM combineto form the observed surface words:Gu .
.
.
(as before, except G0 =Uniform(|M|))Hu ?
PY (d|u|, ?|u|, Gu ?
F )w?
?
HuSo the base distribution for the prior of the wordn-gram distribution Hu is the product of a distri-bution Gu over compound heads, given the samecontext u, and another (n?-gram) language modelF over compound modifiers, conditioned on thehead component.Choosing F to be a bigram model (n?=2) yieldsthe following procedure for generating a word:c?1 ?
Gufor i = 2 to kc?i ?
F (?|c?i?1)The linguistically motivated choice for condi-tioning in F is ?ling = [k, k ?
1, .
.
.
, 1] such thatc?1 is the true head component; $ is drawn fromF (?|c1) and marks the left word boundary.In order to see if the correct linguistic intuitionhas any bearing on the model?s extrinsic perfor-mance, we will also consider the reverse, sup-posing that the left-most component were actu-ally more important in this task, and letting theremaining components be generated left-to-right.This is expressed by ?inv = [1, .
.
.
, k], where $this time marks the right word boundary and isdrawn from F (?|ck).To test whether Kneser-Ney smoothing is in-deed sometimes less appropriate, as conjecturedearlier, I will also compare the case whereF = FKN , a KN-smoothed model, with the casewhere F = FHPY LM , another HPYLM.Linker Elements In the preceding definition ofcompound segmentation, the linker elements donot form part of the vocabulary M. Regardinglinker elements as components in their own rightwould sacrifice important contextual informationand disrupt the conditionals F (?|c?i?1).
That is,given Ku?che?n?tisch, we want P (Ku?che|Tisch) inthe model, but not P (Ku?che|n).But linker elements need to be accountedfor somehow to have a well-defined generativemodel.
I follow the pragmatic option of merg-ing any linkers onto the adjacent component ?
for?ling merging happens onto the preceding compo-nent, while for ?inv it is onto the succeeding one.This keeps the ?head?
component c?1 in tact.More involved strategies could be considered,and it is worth noting that for German the pres-ence and identity of linker elements between ciand ci+1 are in fact governed by the precedingcomponent ci (Goldsmith and Reutter, 1998).5 TrainingFor ease of exposition I describe inference withreference to the trigram HPYLM+c model witha bigram HPYLM for F , but the general caseshould be clear.The model is specified by the latent vari-ables (G[?
], G[v], G[u,v], H[u,v], F?, Fc), whereu, v ?
W , c ?M, and hyperparameters ?
={di, ?i} ?
{d?j , ?
?j} ?
{d?
?2, ??
?2}, where i = 0, 1, 2,j = 0, 1, single primes designate the hyperpa-rameters in FHPY LM and double primes those ofH[u,v].
We can construct a collapsed Gibbs sam-pler by marginalising out these latent variables,giving rise to a variant of the hierarchical ChineseRestaurant Process in which it is straightforwardto do inference.Chinese Restaurant Process A direct repre-sentation of a random variable G drawn from aPYP can be obtained from the so-called stick-breaking construction.
But the more indirect rep-resentation by means of the Chinese RestaurantProcess (CRP) (Pitman, 2002) is more suitablehere since it relates to distributions over itemsdrawn from such a G. This fits the current set-ting, where wordsw are being drawn from a PYP-distributed G.Imagine that a corpus is created in two phases:Firstly, a sequence of blank tokens xi is instanti-ated, and in a second phase lexical identities wiare assigned to these tokens, giving rise to the67observed corpus.
In the CRP metaphor , the se-quence of tokens xi are equated with a sequenceof customers that enter a restaurant one-by-one tobe seated at one of an infinite number of tables.When a customer sits at an unoccupied table k,they order a dish ?k for the table, but customersjoining an occupied table have to dine on the dishalready served there.
The dish ?i that each cus-tomer eats is equated to the lexical identity wi ofthe corresponding token, and the way in which ta-bles and dishes are chosen give rise to the charac-teristic properties of the CRP:More formally, let x1, x2, .
.
.
be draws fromG,while t is the number of occupied tables, c thenumber of customers in the restaurant, and ck thenumber of customers at the k-th table.
Condi-tioned on preceding customers x1, .
.
.
, xi?1 andtheir arrangement, the i-th customer sits at tablek = k?
according to the following probabilities:Pr(k?| .
.
. )
?{ck?
?
d occupied table k??
+ dt unoccupied table t+ 1Ordering a dish for a new table corresponds todrawing a value ?k from the base distribution G0,and it is perfectly acceptable to serve the samekind of dish at multiple tables.Some characteristic behaviour of the CRP canbe observed easily from this description: 1) Asmore customers join a table, that table becomesa more likely choice for future customers too.2) Regardless of how many customers there are,there is always a non-zero probability of joiningan unoccupied table, and this probability also de-pends on the number of total tables.The dish draws can be seen as backing off tothe underlying base distribution G0, an importantconsideration in the context of the hierarchicalvariant of the process explained shortly.
Note thatthe strength and discount parameters control theextent to which new dishes are drawn, and thusthe extent of reliance on the base distribution.The predictive probability of a word w given aseating arrangement is given byPr(w| .
.
. )
?
cw ?
dtw + (?
+ dt)G0(w)In smoothing terminology, the first term can beinterpreted as applying a discount of dtw to theobserved count cw of w; the amount of dis-count therefore depends on the prevalence of theword (via tw).
This is one significant way inwhich the PYP/CRP gives more nuanced smooth-ing than modified Kneser-Ney, which only usesfour different discount levels (Chen and Good-man, 1998).
Similarly, if the seating dynamicsare constrained such that each dish is only servedonce (tw = 1 for any w), a single discount levelis affected, establishing direct correspondence tooriginal interpolated Kneser-Ney smoothing (Teh,2006).Hierarchical CRP When the prior of Gu hasa base distribution Gpi(u) that is itself PYP-distributed, as in the HPYLM, the restaurantmetaphor changes slightly.
In general, each nodein the hierarchy has an associated restaurant.Whenever a new table is opened in some restau-rantR, another customer is plucked out of thin airand sent to join the parent restaurant pa(R).
Thisinduces a consistency constraint over the hierar-chy: the number of tables tw in restaurant R mustequal the number of customers cw in its parentpa(R).In the proposed HPYLM+c model usingFHPY LM , there is a further constraint of a simi-lar nature: When a new table is opened and servesdish ?
= w?
in the trigram restaurant for H[u,v],a customer c?1 is sent to the corresponding bi-gram restaurant for G[u,v], and customers c?2:k ,$are sent to the restaurants for Fc?
, for contextsc?
= c?1:k?1 .
This latter requirement is novel herecompared to the hierarchical CRP used to realisethe original HPYLM.Sampling Although the CRP allows us to re-place the priors with seating arrangements S,those seating arrangements are simply latent vari-ables that need to be integrated out to get a truepredictive probability of a word:p(w|D) =?S,?p(w|S,?
)p(S,?|D),where D is the training data and, as before, ?
arethe parameters.
This integral can be approximatedby averaging over m posterior samples (S,?
)generated using Markov chain Monte Carlo meth-ods.
The simple form of the conditionals in theCRP allows us to do a Gibbs update whereby thetable index k of a customer is resampled condi-tioned on all the other variables.
Sampling a newseating arrangement S for the trigram HPYLM+cthus corresponds to visiting each customer in therestaurants for H[u,v], removing them while cas-cading as necessary to observe the consistency68across the hierarchy, and seating them anew atsome table k?.In the absence of any strong intuitions about ap-propriate values for the hyperparameters, I placevague priors over them and use slice sampling1(Neal, 2003) to update their values during gener-ation of the posterior samples:d ?
Beta(1, 1) ?
?
Gamma(1, 1)Lastly, I make the further approximation ofm = 1, i.e.
predictive probabilities are informedby a single posterior sample (S,?
).6 ExperimentsThe aim of the experiments reported here is to testwhether the richer account of compounds in theproposed language models has positive effects onthe predictability of unseen text and the genera-tion of better translations.6.1 MethodsData and Tools Standard data preprocessingsteps included normalising punctuation, tokenis-ing and lowercasing all words.
All data sets arefrom the WMT11 shared-task.2.
The full English-German bitext was filtered to exclude sentenceslonger than 50, resulting in 1.7 million parallelsentences; word alignments were inferred fromthis using the Berkeley Aligner (Liang et al,2006) and used as basis from which to extract aHiero-style synchronous CFG (Chiang, 2007).The weights of the log-linear translation mod-els were tuned towards the BLEU metric ondevelopment data using cdec?s (Dyer et al,2010) implementation of MERT (Och, 2003).For this, the set news-test2008 (2051 sen-tences) was used, while final case-insensitiveBLEU scores are measured on the official test setnewstest2011 (3003 sentences).All language models were trained on the targetside of the preprocessed bitext containing 38 mil-lion tokens, and tested on all the German devel-opment data (i.e.
news-test2008,9,10).Compound segmentation To construct a seg-mentation dictionary, I used the 1-best segmenta-tions from a supervised MaxEnt compound split-ter (Dyer, 2009) run on all token types in bitext.
Inaddition, word-internal hyphens were also taken1Mark Johnson?s implementation, http://www.cog.brown.edu/?mj/Software.htm2http://www.statmt.org/wmt11/as segmentation points.
Finally, linker elementswere merged onto components as discussed in?4.2.
Any token that is split into more than onepart by this procedure is regarded as a compound.The effect of the individual steps is summarisedin Table 1.# Types ExampleNone 350998 Geburtstagskindpre-merge 201328 Geburtstag?kindmerge, ?ling 150980 Geburtstags?kindmerge, ?inv 162722 Geburtstag?skindTable 1: Effect of segmentation on vocabulary size.Metrics For intrinsic evaluation of languagemodels, perplexity is a common metric.
Given atrained model q, the perplexity over the words ?in unseen test set T is exp(?
1|T |??
ln(q(?
))).One convenience of this per-word perplexity isthat it can be compared consistently across dif-ferent test sets regardless of their lengths; its neatinterpretation is another: a model that achieves aperplexity of ?
on a test set is on average ?-waysconfused about each word.
Less confusion andtherefore lower test set perplexity is indicative ofa better model.
This allows different models to becompared relative to the same test set.The exponent above can be regarded as anapproximation of the cross-entropy between themodel q and a hypothetical model p from whichboth the training and test set were putatively gen-erated.
It is sometimes convenient to use this asan alternative measure.But a language model only really becomes use-ful when it allows some extrinsic task to be exe-cuted better.
When that extrinsic task is machinetranslation, the translation quality can be assessedto see if one language model aids it more than an-other.
The obligatory metric for evaluating ma-chine translation quality is BLEU (Papineni et al,2001), a precision based metric that measures howclose the machine output is to a known correcttranslation (the reference sentences in the test set).Higher precision means the translation system isgetting more phrases right.Better language model perplexities sometimeslead to improvements in translation quality, butit is not guaranteed.
Moreover, even when realtranslation improvements are obtained, they are69PPL c-Cross-ent.mKN 441.32 0.1981HPYLM 429.17 0.1994FKN ?ling 432.95 0.2028FKN ?inv 446.84 0.2125FHPY LM ?ling 421.63 0.1987FHPY LM ?inv 435.79 0.2079Table 2: Monolingual evaluation results.
The secondcolumn shows perplexity measured all WMT11 Ger-man development data (7065 sentences).
At the wordlevel, all are trigram models, while F are bigram mod-els using the specified segmentation scheme.
The thirdcolumn has test cross-entropies measured only on the6099 compounds in the test set (given their contexts ).not guaranteed to be noticeable in the BLEUscore, especially when targeting an arguably nar-row phenomenon like compounding.BLEUmKN 13.11HPYLM 13.20FHPY LM , ?ling 13.24FHPY LM , ?inv 13.32Table 3: Translation results, BLEU (1-ref), 3003 testsentences.
Trigram language models, no count prun-ing, no ?unknown word?
token.P / R / FmKN 22.0 / 17.3 / 19.4HPYLM 21.0 / 17.8 / 19.3FHPY LM , ?ling 23.6 / 17.3 / 19.9FHPY LM , ?inv 24.1 / 16.5 / 19.6Table 4: Precision, Recall and F-score of compoundtranslations, relative to reference set (72661 tokens, ofwhich 2649 are compounds).6.2 Main ResultsFor the monolingual evaluation, I used an interpo-lated, modified Kneser-Ney model (mKN) and anHPYLM as baselines.
It has been shown for otherlanguages that HPYLM tends to outperform mKN(Okita and Way, 2010), but I am not aware of thisresult being demonstrated on German before, as Ido in Table 2.The main model of interest is HPYLM+c us-ing the ?ling segmentation and a model FHPY LMover modifiers; this model achieves the lowestperplexity, 4.4% lower than the mKN baseline.Next, note that using FKN to handle the modi-fiers does worse than FHPY LM , confirming ourexpectation that KN is less appropriate for thattask, although it still does better than the originalmKN baseline.The models that use the linguistically im-plausible segmentation scheme ?inv both fareworse than their counterparts that use the sensiblescheme, but of all tested models only FKN & ?invfails to beat the mKN baseline.
This suggests thatin some sense having any account whatsoever ofcompound formation tends to have a beneficial ef-fect on this test set ?
the richer statistics due to asmaller vocabulary could be sufficient to explainthis ?
but to get the most out of it one needs thesuperior smoothing over modifiers (provided byFHPY LM ) and adherence to linguistic intuition(via ?ling).As for the translation experiments, the rela-tive qualitative performance of the two baselinelanguage models carries over to the BLEU score(HPYLM does 0.09 points better than KN), and isfurther improved upon slightly by using two vari-ants of HPYLM+c (Table 3).6.3 AnalysisTo get a better idea of how the extended mod-els employ the increased expressiveness, I calcu-lated the cross-entropy over only the compoundwords in the monolingual test set (second columnof Table 2).
Among the HPYLM+c variants, wesee that their performance on compounds only isconsistent with their performance (relative to eachother) on the whole corpus.
This implies thatthe differences in whole-corpus perplexities are atleast in part due to their different levels of adept-ness at handling compounds, as opposed to somefluke event.It is, however, somewhat surprising to observethat HPYLM+c do not achieve a lower com-pound cross-entropy than the mKN baseline, as itsuggests that HPYLM+c?s perplexity reductionscompared to mKN arise in part from somethingother than compound handling, which is theirwhole point.This discrepancy could be related to the fair-ness of this direct comparison of models that ul-70timately model different sets of things: Accord-ing to the generative process of HPYLM+c (?4),there is no limit on the number of components ina compound: in theory, an arbitrary number ofcomponents c ?
M can combine to form a word.HPYLM+c is thus defined over a countably infi-nite set of words, thereby reserving some prob-ability mass for items that will never be realisedin any corpus, whereas the baseline models aredefined only over the finite set W .
These directcomparisons are thus lightly skewed in favour ofthe baselines.
This bolsters confidence in the per-plexity reductions presented in the previous sec-tion, but the skew may afflict compounds morestarkly, leading to the slight discrepancy observedin the compound cross-entropies.
What mattersmore is the performance among the HPYLM+cvariants, since they are directly comparable.To home in still further on the compound mod-elling, I selected those compounds for whichHPYLM+c (FHPY LM ,?ling) does best/worst interms of the probabilities assigned, compared tothe mKN baseline (see Table 5).
One pattern thatemerges is that the ?top?
compounds mostly con-sist of components that are likely to be quite com-mon, and that this improves estimates both for n-grams that are very rare (the singleton ?senkun-gen der treibhausgasemmissionen?
= decreases ingreen house gas emissions) or relatively common(158, ?der hauptstadt?
= of the capital).n-gram ?
Cgesichts?punkten 0.064 335700 milliarden us-?dollar 0.021 2s.
der treibhausgas?emissionen 0.018 1r.
der treibhausgas?emissionen 0.011 3ministerium fu?r land?wirtschaft 0.009 11bildungs?niveaus 0.009 14newt ging?rich* -0.257 2nouri al-?maliki* -0.257 3klerikers moqtada al-?sadr* -0.258 1nuri al-?maliki* -0.337 3sankt peters?burg* -0.413 35na?chtlichem flug?la?rm -0.454 2Table 5: Compound n-grams in the test set for whichthe absolute difference ?
= PHPYLM+c?PmKN is great-est.
C is n-gram count in the training data.
Asterisksdenote words that are not compounds, linguisticallyspeaking.
Abbrevs: r. = reduktionen, s.= senkungenOn the other hand, the ?bottom?
compoundsare mostly ones whose components will be un-common; in fact, many of them are not truly com-pounds but artefacts of the somewhat greedy seg-mentation procedure I used.
Alternative proce-dures will be tested in future work.Since the BLEU scores do not reveal muchabout the new language models?
effect on com-pound translation, I also calculated compound-specific accuracies, using precision, recall andF-score (Table 4).
Here, the precision for asingle sentence would be 100% if all the com-pounds in the output sentence occur in the ref-erence translation.
Compared to the baselines,the compound precision goes up noticeably underthe HPYLM+c models used in translation, with-out sacrificing on recall.
This suggests that thesemodels are helping to weed out incorrectly hy-pothesised compounds.6.4 CaveatsAll results are based on single runs and are there-fore not entirely robust.
In particular, MERTtuning of the translation model is known to in-troduce significant variance in translation perfor-mance across different runs, and the small differ-ences in BLEU scores reported in Table 3 are verylikely to lie in that region.Markov chain convergence also needs furtherattention.
In absence of complex latent struc-ture (for the dishes), the chain should mix fairlyquickly, and as attested by Figure 2 it ?converges?with respect to the test metric after about 20 sam-ples, although the log posterior (not shown) hadnot converged after 40.
The use of a single poste-rior sample could also be having a negative effecton results.7 Future DirectionsThe first goal will be to get more robust ex-perimental results, and to scale up to 4-grammodels estimated on all the available monolin-gual training data.
If good performance can bedemonstrated under those conditions, this gen-eral approach could pass as a viable alternative tothe current Kneser-Ney dominated state-of-the artsetup in MT.Much of the power of the HPYLM+c modelhas not been exploited in this evaluation, in par-ticular its ability to score unseen compounds con-sisting of known components.
This feature was710 10 20 30 40Iteration420440460480500520540560PerplexityKN ?lingHPYLMHPYLM+c ?lingmKN-baselineFigure 2: Convergence of test set perplexities.not active in these evaluations, mostly due to thecurrent phase of implementation.
A second areaof focus is thus to modify the decoder to gen-erate such unseen compounds in translation hy-potheses.
Given the current low compound recallrates, this could greatly benefit translation quality.An informal analysis of the reference translationsin the bilingual test set showed that 991 of the1406 out-of-vocabulary compounds (out of 2692OOVs in total) fall into this category of unseen-but-recognisable compounds.Ultimately the idea is to apply this modellingapproach to other linguistic phenomena as well.In particular, the objective is to model instancesof concatenative morphology beyond compound-ing, with the aim of improving translation intomorphologically rich languages.
Complex agree-ment patterns could be captured by condition-ing functional morphemes in the target word onmorphemes in the n-gram context, or by stem-ming context words during back-off.
Such ad-ditional back-off paths can be readily encoded inthe Graphical Pitman-Yor process (Wood and Teh,2009).These more complex models may requirelonger to train.
To this end, I intend to use thesingle table per dish approximation (?5) to reducetraining to a single deterministic pass through thedata, conjecturing that this will have little effecton extrinsic performance.8 SummaryI have argued for further explorations into theuse of a family of hierarchical Bayesian modelsfor targeting linguistic phenomena that may notbe captured well by standard n-gram languagemodels.
To ground this investigation, I focusedon German compounds and showed how thesemodels are an appropriate vehicle for encodingprior linguistic intuitions about such compounds.The proposed generative model beats the popu-lar modified Kneser-Ney model in monolingualevaluations, and preliminarily achieves small im-provements in translation from English into Ger-man.
In this translation task, single-token Ger-man compounds traditionally pose challenges totranslation systems, and preliminary results showa small increase in the F-score accuracy of com-pounds in the translation output.
Finally, I haveoutlined the intended steps for expanding this lineof inquiry into other related linguistic phenomenaand for adapting a translation system to get opti-mal value out of such improved language models.AcknowledgementsThanks goes to my supervisor, Phil Blunsom, forcontinued support and advice; to Chris Dyer forsuggesting the focus on German compounds andsupplying a freshly trained compound splitter; tothe Rhodes Trust for financial support; and to theanonymous reviewers for their helpful feedback.ReferencesMarco Baroni and Johannes Matiasek.
2002.
Pre-dicting the components of German nominal com-pounds.
In ECAI, pages 470?474.Andre Berton, Pablo Fetter, and Peter Regel-Brietzmann.
1996.
Compound Words in Large-Vocabulary German Speech Recognition Systems.In Proceedings of Fourth International Conferenceon Spoken Language Processing.
ICSLP ?96, vol-ume 2, pages 1165?1168.
IEEE.Jeff A Bilmes and Katrin Kirchhoff.
2003.
Factoredlanguage models and generalized parallel back-off.
In Proceedings of NAACL-HLT (short papers),pages 4?6, Stroudsburg, PA, USA.
Association forComputational Linguistics.Stanley F Chen and Joshua Goodman.
1998.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
Technical report.David Chiang.
2007.
Hierarchical Phrase-Based Translation.
Computational Linguistics,33(2):201?228, June.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Se-tiawan, Vladimir Eidelman, and Philip Resnik.2010.
cdec: A Decoder, Alignment, and Learningframework for finite-state and context-free trans-lation models.
In Proceedings of the Association72for Computational Linguistics (Demonstration ses-sion), pages 7?12, Uppsala, Sweden.
Associationfor Computational Linguistics.Chris Dyer.
2009.
Using a maximum entropy modelto build segmentation lattices for MT.
In Proceed-ings of NAACL, pages 406?414.
Association forComputational Linguistics.John Goldsmith and Tom Reutter.
1998.
AutomaticCollection and Analysis of German Compounds.
InF.
Busa F. et al, editor, The Computational Treat-ment of Nominals, pages 61?69.
Universite de Mon-treal, Canada.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2006.
Interpolating Between Types andTokens by Estimating Power-Law Generators.
InAdvances in Neural Information Processing Sys-tems, Volume 18.Songfang Huang and Steve Renals.
2007.
Hierarchi-cal Pitman-Yor Language Models For ASR in Meet-ings.
IEEE ASRU, pages 124?129.Songfang Huang and Steve Renals.
2009.
A paral-lel training algorithm for hierarchical Pitman-Yorprocess language models.
In Proceedings of Inter-speech, volume 9, pages 2695?2698.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modelling.
InProceedings of the IEEE International Conferenceon Acoustics, Speech and SIgnal Processing, pages181?184.Philipp Koehn and Kevin Knight.
2003.
EmpiricalMethods for Compound Splitting.
In Proceedingsof EACL, pages 187?193.
Association for Compu-tational Linguistics.Philipp Koehn, Abhishek Arun, and Hieu Hoang.2008.
Towards better Machine Translation Qual-ity for the German ?
English Language Pairs.
InThird Workshop on Statistical Machine Translation,number June, pages 139?142.
Association for Com-putational Linguistics.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by Agreement.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,Main Conference, pages 104?111, New York City,USA, June.
Association for Computational Linguis-tics.Daichi Mochihashi, Takeshi Yamada, and NaonoriUeda.
2009.
Bayesian unsupervised word seg-mentation with nested Pitman-Yor language mod-eling.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 1 - ACL-IJCNLP?09, pages 100?108, Suntec, Singapore.
Associa-tion for Computational Linguistics.Radford M Neal.
2003.
Slice Sampling.
The Annalsof Statistics, 31(3):705?741.Graham Neubig, Masato Mimura, Shinsuke Mori, andTatsuya Kawahara.
2010.
Learning a LanguageModel from Continuous Speech.
In Interspeech,pages 1053?1056, Chiba, Japan.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof ACL, pages 160?167.Tsuyoshi Okita and Andy Way.
2010.
HierarchicalPitman-Yor Language Model for Machine Transla-tion.
Proceedings of the International Conferenceon Asian Language Processing, pages 245?248.Kishore Papineni, Salim Roukos, Todd Ward, Wei-jing Zhu, Thomas J Watson, and Yorktown Heights.2001.
Bleu: A Method for Automatic Evaluation ofMachine Translation.
Technical report, IBM.J Pitman and M. Yor.
1997.
The two-parameterPoisson-Dirichlet distribution derived from a sta-ble subordinator.
The Annals of Probability,25:855?900.J.
Pitman.
2002.
Combinatorial stochastic processes.Technical report, Department of Statistics, Univer-sity of California at Berkeley.Sara Stymne.
2009.
A comparison of merging strate-gies for translation of German compounds.
Pro-ceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Lin-guistics: Student Research Workshop, pages 61?69.Yee Whye Teh.
2006.
A hierarchical Bayesian lan-guage model based on Pitman-Yor processes.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the ACL, pages 985?992.
Association forComputational Linguistics.Frank Wood and Yee Whye Teh.
2009.
A Hierarchi-cal Nonparametric Bayesian Approach to StatisticalLanguage Model Domain Adaptation.
In Proceed-ings of the 12th International Conference on Arti-ficial Intelligence and Statistics (AISTATS), pages607?614, Clearwater Beach, Florida, USA.Mei Yang and Katrin Kirchhoff.
2006.
Phrase-basedBackoff Models for Machine Translation of HighlyInflected Languages.
In Proceedings of the EACL,pages 41?48.73
