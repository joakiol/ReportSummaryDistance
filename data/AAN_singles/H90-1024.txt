DARPA ATIS Test ResultsJune 1990D.
S. Pallett, W. M. Fisher, J. G. Fiscus, and J. S. GarofoloRoom A 216 Technology BuildingNational Institute of Standards and Technology (NIST)Gaithersburg, MD 20899IntroductionThe first Spoken Language System tests to beconducted in the DARPA Air TravelInformation System (ATIS) domain took placeduring the period June 15 - 20, 1989.
Thispaper presents a brief description of the testprotocol, comparator software used for scoringresults at NIST, test material selection process,and preliminary tabulation of the scoredresults for seven SLS systems from five sites:BBN, CMU, MIT/LCS, SRI and Unisys.
Onesystem, designated cmu-spi(r) in this paper,made use of digitized speech as input (.wavfiles), and generated CAS-format answers.Other systems made use of SNORtranscriptions (.snr files) as input.Test ProtocolThe test protocol for these tests was modelledafter precedents established over the pastseveral years in the DARPA ResourceManagement speech recognition benchmarktests.
On June 11, 1990, participating siteswere notified of the availability of SNOR-format transcriptions for a designated set of93 "Class A" test utterances.
Data wereavailable using an FTP protocol that had beenused earlier for access to system trainingmaterial.
Copies of the speech waveform fileswere distributed to three sites using Exabytetapes.
Responses were provided (in mostcases) to NIST on June 15.
In return, NISTprovided a key to an encrypted version of thecomplete ATIS session data from which thetest material was selected, and sites were freeto access that data in scoring their systemresults locally.
A preliminary summary of thetest results was distributed by NIST toparticipants on June 18th.Availability of the ATIS DataAccess to all of the ATIS data released byNIST (except for the speech waveform data)has been available via anonymous FTP.
Thespeech waveform data has been madeavailable to three sites to date on 8ramExabyte tape: AT&T, CMU and SRI.Production of the entire Pilot ATIS Corpus isplanned for release by NIST on CD-ROMmedia after completion of the corpuscollection effort at TI.Comparator SoftwareBoth REF (reference) and HYP (hypothesized)answers were to be written in a CAS(Common Answer Specification) format thatwas a slight adaptation of the CAS originallydeveloped by BBN, and which had beenagreed on by the CAS/Comparator TaskGroup.
Two programs were available to aidin the evaluation by automatically comparingmatching REF and HYP answers: one in LISP,contributed by BBN, and one in C, developedat NIST.
Final responsibility for decisions on114whether or not an answer is correct restedwith human judges at NIST.When the comparator programs ran, therewere a few disagreements in two areas: (1)some answers were scored "correct" by themore forgiving NIST code even though theREF and HYP answers disagreed in the use ofquotation marks delimiting an answer; and (2)some HYP answers consisting of tables ofnumeric codes were incorrectly counted asmatching the REF answers by the NIST code.The first area is a trivial matter of formats andhow forgiving a program should be.
Ourjudgement in these cases was that the contentof the answer was correct.The second area raises some interesting logicalquestions.
In a typical case, the requiredcolumn in the table was a code that lookedlike a number, such as flight_code; becauseintegers and floating point numbers were tobe treated as the same type, the tolerance forfloating point comparison was used in decidingequality.
Because the key fields of the extraerroneous rows were "close enough" to thecorrect ones, they were ignored.
An ad hoccode change was subsequently made in theNIST code so that the tolerance was used onlyin equality tests when at least one of thenumbers was floating point.
But we thinkthat in principle there is nothing wrong aboutusing a tolerance in comparing two integers;the real wrongness is treating a pointer (orname) as a number.
One principled way toclear this up would be to consistently useenclosing quotation marks to indicate tokensthat are not to be treated numerically.We had to increase considerably the spaceallotted to input buffers in the NIST Csoftware, since one answer that was submittedtook more than 175 K bytes.As a result of seeing some particular answers,one more change was made in the NISTComparator code to make it more forgiving:leading and trailing whitespace in a string isnow ignored.
This made several answers fromone of the sites count as correct, in agreementwith our judgement hat they had the rightcontent.Several examples came up in the test answersto illustrate the trouble with looking formatches of only values, without constrainingthe values to be of the same variable, inconjunction with allowing extra values in atuple.
For instance, query bdO0cls, "WHAT ISCLASS Y", has the REF answer (CY")), andone of the HYP answers supplied is:(('Y' '~'  "COACH" "NO" "YES" "NO" "NO""NONE" "1234567"))Our CAS specification counts this is a correctmatch, although it is indeterminate which ofthe tuple's two "Y" fields was matched.
AHYP answer with '~'  value in any field wouldcount as equally correct.
In tabular answersintended for human consumption, this problemis solved by supplying column headings.
Itwould be easy to incorporate a similar systeminto our computerized scoring methods.Test  Mater ia l  Select ion ProcessTime was available to do only cursory studyof material in sorting it into training and testbins.
A vague, intuitive sense of "plainvanilla" vs. "weird" was used.
Several sessionswere ruled out as test material because theywere unusual: one had an extremely lowfrequency of "Class A" queries; in another,almost all the queries were just NP's, withoutverbs.
In the sessions accepted as testmaterial, all "Class A" queries were used.It was strongly suggested that we partition theresults into "new word" and "old word" sets,the "new word" set being those queriescontaining words not in the training material.This motivated us to think some about the"new word" problem.
Probably the principlebeing implicitly addressed here is that testqueries are unfair ff they are not answerableby the logical generalization of thei15conjunction of training material and the initialstate of the language model.
(In a spellingbee, it is probably unfair to expect acontestant to come up with the "k" in "knight"if that word -- or a related word -- had neverbeen seen in training.)
This is the other sideof the usual constraint between testing andtraining: that they be statistically independent,for a valid test.Violation of the "fair generalization" constraintbetween training set and test set does notmake a test "invalid", or necessarily biased, butonly inefficient and "unfair" if only the bottomline is paid attention to.Since some words are understandable eventhough one has not previously heard them,and polysemous words are not necessarilyunderstandable after limited exposure, "words"are not the right unit to look at in deciding ifa test case is implied by the training set.
Thereal constraint is that all sound-to-meaningmappings that are required in order to answerthe test question be learnable from thetraining material (assuming an initial "tabularasa" language model).
This points to idioms:sound-meaning pairs that are not predictableby general rule from a knowledge of thesounds and meanings of their constituentparts.
Knowing the meaning of "time" and"table" does not make "time table"understandable.
And morphemes (roughlynon-complex words) qualify as idioms, sincethey have no sound-meaning constituent parts.New syntactic constructions mediatingbetween sound and meaning would also makethe sound-to-meaning mapping of a queryunhandleable.Here are the qualitatively new elements thatwe found in the test material (including non-Class A) utterances :1.
Morphemes: EARLY, EQUAL, ITS,LOCKHEED, NIGHT, \[STAYING\] OVER,,and PART.2.
Words: \[U\] A'S, LEAVES,MEANINGS,  MORNINGS,NINETEENTH, PRICES, SEATINGS,SERVICING, \[TO\] SERVICE, SPECIALS,STAYING \[OVER\], and THREE'S.3.
Multi-word Idioms: TIME TABLE(only in Class X)With one exception, none of the five Class Aqueries with new morphemes were answeredcorrectly.
Query bp00kls, with "NIGHT' in it,was successfully answered by only the MITsystem.
Perhaps because "NIGHT' is in theknowledge database, it should have beencounted as an "old" morpheme.Several of the ten queries with new complexwords in the test set were answered correctly;primarily ones with new words that areregular morphological variants of other wordsthat are in the training set (or the assumedpre-existing language model), e.g.
"meanings","times", or "nineteenth".A table showing the "new phenomena" subsetof results is provided at the end of this paper(Table 2).It seems to us that a promising research topicwould be further study of such training-test"fair generalization" or "learnability"constraints, with an eye to automatingdetection of their violation in the design ofbetter tests.Preliminary ResultsResults were reported to NIST for a total ofseven systems by June 19th: two systems fromBBN, two from CMU and one each from MIT,SRI and Unisys.
The system designated ascmu-spi Cspi" = > speech input) was the onlyone for which the input consisted of thespeech waveform.
For the other systems theinput consisted of the SNOR transcriptions.Subsequently, reformatted results for threesystems were accepted: "cmu-r", "cmu-spir",and "mit-r".116The C/S-format input provided for an answerof the form NO ANSWER to indicate that thesystem failed to provide an answer for any ofseveral reasons (e.g., failure to recognize thewords, failure to parse, failure to produce avalid database query, etc).
Some sites madeconsiderable use of this option, others (e.g.,MIT) initially did not, partially due tomiscommunication about the validity of thisoption.Some trivial fixes in the format used forsubmissions of results from some of the siteswere made.
One site initially omitted ananswer for one of the queries, throwingsubsequent REF-HYP alignment off; weinserted for them a NO,ANSWER response.Since there was miscommunication about theuse of the "NOjkNSWER" response, we alsochanged one system's tock response meaning"system can't handle it" to "NO_ANSWER" forthem, and allowed another site to submitrevised results with "NOANSWER" in place ofsome of their responses.
In the table ofresults, the revised systems are "cmu-r", "cmu-spir", and "mit-r".Responding to several complaints from sitesabout specific items in the test referencematerial, we corrected one reference answer(bd0071s) and changed the classification ofthree queries (bm0011s, bp0081s, andbw00sls) from Class A to Class X (in effectdeleting these from the test set, reducing thetest set size to 90 valid Class A queries).
Theclassification disputes all centered onambiguity, one of the hardest calls to make.If similar limitations on what is evaluable aremade for the next round, we would like tohave both an explicit principle for decidingwhen ambiguity is present and a procedure foradjudicating disputes agreed on early.
Thedetailed results are given in Table l a for ClassA queries with only lexical items that appearat least once in the training data, and in Table2a for Class A queries with "new" morphemes,words, or idioms.
Table 3 presents a completesummary of the results for the entire 90sentence-utterance test set.Since the Class A test queries are not context-dependent, he ordering of these queries is notsignificant.
As an aid in analysis, for theresults presented in Tables la  and 2a, querieshave been (roughly) rank ordered in order ofincreasing apparent difficulty.
Note thatqueries toward the top of both parts of thetable resulted in more "T" answers than "F" orNA", while queries toward the bottom of thetable resulted in more "F" and "NA" answers.Not surprisingly, there appears to be a generaltrend toward increasing apparent difficultywith increased length of the utterance(number of words).Table 3 shows that the number of correctanswers from the various systems ranged from25 to 58.
Note also that for the system forwhich speech waveform data was used asinput, (cmu-spir), 35 of the queries wereanswered correctly.
Comparing results fromsimilar systems for the two subsets of the data(Tables lb  and 2b), note that the ratios of thenumbers of correctly recognized queries in thetwo subsets vary from 1.9 to 4.6, with betterperformance on the subset for which alllexical items occurred at least once in thetraining data, of course.Comparisons uch as these are complicated,however, by the fact that different systemsreturned NO ANSWER for from 0 to 60 of thequeries.
Perhaps a more appropriatedenominator to be used in computing thepercentage of correct responses would havebeen the number of responses for which ananswer was provided.SummaryThis paper has presented results of the firstSpoken Language System tests conducted inthe DARPA Air Travel Information Systemdomain.117.Su~I:ID0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0~ ~ ~ ~ ~ ~ ~ ~ ~ .~ ~ ~ .~ .~ .~ .~ ~ .~ .~ .
.
.
.
.
.
~ ~ ~ ~ .~  .
~~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ,~ ~ ~ ~ ~ ~ ~ ~ ~ ~~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ Z Z Z ~ Z Z Z Z ZI I I I I t I I I I I I I I I I I I I I I I I I I~ .~.~ .~ .~ .~ .~ .~ .~ .~ .~ -~ .~ .~ .~ .~ .~ .~.~om p~ ~p p p p p ~ ~NpNp pNp~ ,o~ooo ~ ~~.u mo ~ g ~ ~ ~ t~ tt ~ t ~ ~ o ~ ~ o o o ~ ~ o~ , ~ , ~ , ~ , ~ , ~ ,  , , ~ , ~ ,  , ~ ,  ,~o~o~o~o~o~o~N ~ X ~IN  ~ N ~ N, - - I  t , -~  I ,--1 I , - I  I ,--~ I , -~  I , -- I  I ,--~ I , -~  I , -- I  I ,--~ I , - I  1 , -~  I , -- I  I ,--I , - - I  I ~--~ I.q o .q o ,.q o ,,q 0 .,q o .q o .Q o .Q o o,.q118~ ~ ~ ~ ~ .~ .~ ~ .~ ~ ~ ~ .~ ~ ~ .~ .~ .~ ~ .~ ~ .~ .~ ~ ~ ~ .~ .~ ~ .~ .~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Z Z Z ~ Z ~ Z Z ~ ~ ~ ~ Z Z Z Z Z Z Z Z ~ ~ Z Z Z Z ~ Z Z Z-C -~ .~ -~ -~ -~ -: -: -~ -~ -~ -C = -~ -~ -~ -: -~ -~ -C -~ -~ -~ -~ -~ -~ -~ .~ -~ -~ -~~ ~ ~ ~ ~ ~ ~ e ~ ~ ~ ~ ~ ~ z ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ zk k k k k k ~ n ~ k  k k ~ k k k ~ k ~ k  k k k ~ k~qk  k k k*,4 ",4 Z ",~ -,4 -,-4 -~40-~4 ~-~ ",'4 ~ -,~ -,~ "'~ "'~ O "'-I O-~4 -,~ -~ ~-,-4 -,'4 -,-4 -,'4 -,~ O-~4 -~ ~ -,~ .,.4 .~ -,"4 ",4 -,~ -,"4 ImO~0Z ~ ~ m o m  ~O inca mZ ~ ~ m m ~z m~ ~0 m ~ ~o ,n m ~ ~o~ ~ m ~n~ ~ ~ ~ ~  ~ ~ X : ~ ~  ~ ~ ~ ~ ~.-, ~ ~ 8 ~ ~Z ~ Z ~Z ~= = = = =  = = = ~ = ~ =  =z= = ~ = o =  = z = ; = ~ =  = = = =~= = = = u =Z Z OE EZE OE~ ~ ~nEOE~E< E rnE  EZEO H ~ ~ ~ O ~ ~ ~ Z Z 0 ~ 0 ~ ~ ~ ~ ,.a ~ ~ ~ ~ 0o oI ~ I ~ I ~I~ I I ~I~ I ~.~ I ~;~ I I ~.~ I ~ I ~ I ,~ I ~ I ~.~ I ~ I ;-~ I ~ I I~ I ~ I ~ I~I~ I I ~ =~.~I  l i -~ I~  I I I I ~ l~gz~,~g~oz~o~o~o~o~z~,~o~,~~ ~ = z  ~ z ~ ~ ~ o ~m M m X m X ~ M m M m M ~ M m X m X ~ M m X ~ M m X ~ M m M ~ X m X m X ~ X ~ M ~ M ~ M m M ~ M ~ M ~ H ~ M m X m M m M ~ XI ~--I I ~--I I ,-I I ,--I I ~--I I ,-I I ,-.I I ,-,I I ,-"I I ,--I t ~--I I ,--I I ,-4 1 f-~ I ~--I I ,-I I ,--I I ~--I I ,"i I ,-'I I ~-'I I ,-I I ,-"~ I ,--I I ,-.~ I m-I I ,-I l ,-~ I ,-I.~ 0 ~ 0 .~i 0 ,,~ 0 ~ 0 ,,~ 0 ,.~ 0 .Q 0 ~ 0 .,~I 0 .Q 0 ,,Q 0 .Q 0 ~ 0 ,.Ct 0 ,,0 0 ,.~ 0 .el 0 ~ 0 .Q 0 ,.~ 0 ~ 0 .QI 0 .0  0 ,.el 0 ,.0 0 ,.~I 0 .~i 0 ,,~I 0 ,.CI 0 .~ 0l l9~ ~ ~ ~ ~ ~ ~ ~ ~ ~ o o~ o o o o o o o oZ~q Z ~2 mZ~ Z~ Z Z Z ~ Z Z ~ Z Z ZZ ~ Z ~ Z ~ Z  Z Z Z Z~ ~ 8 ~ ~ ~ ~ ~ o4J - ~ ?
- - ~-  O~ .
.
.
.
.
.
.
~ - ~-  u~-  - -~ ~..~-~ -~ - ~= ~ ~ ~  ~ ~ ~ ~ ~ o ~  ~z~8~~ ~ ~ z ~  ~ z  ~ ~ z ~ z ~  z"~ ~E .-.I ~ -~ -~ ~E~ -~ ~-~ ~ -~ -~ ~ -~ -~ 0 -~ -~ ~ -~ ~ -~ 0 "~ ~ -~ Z -~ ~-,.-t ~ -,-I ~ -~ I~ ~ ~ ~ ?q ~ ~,-~; ~ Z ~ 0 ~ ~ ~ ~ ~ ~.~.
~ ,..~ ~ t~ ~ ~..~ ~ ~ ~ Z ~E.~ ~ ~ Z 0 ~ ~ ~ u~0 ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 0 ~ ~ OI ~ I 0 I I ~-,~1 ~ OI  ~1 I I~1  ~1~-~0 ,~ ,~  ~ 0 ~ Z ~ ~~ ~ ~ ~ ~ ~ 0,~ ,~o ,~o ~ , ~ o ~  o ~z~o~,~o,~o,~o~o~=.~ ~ z ~.
~ ~Z ~ Z  ~Z ~ Z ~ Z  Z ~ O ~ Z  Z ~I ~1~ I ,tT.~ I I :~  I I1~ I I ~1~ I I I ~,~ I ~,-.a I I I 1 I :~  I I ~ ,  I ICI Iz ~ z >  zo~ z~z~zo~z~zozzz~z~z~z~zoz~zoz~z,.~ o Z .~ ~ ~ ,,q o ,q o , .~  O.Q 0 ,c~ o ,.~ O, ,Q  O, .Q  ~, .
t~ o , ,~  o ~.Q o ,O O, .Q  o ,.Q,-~ ,,Q o ~ o ,Q  o ,,~,J:~ o ~ ,,~,1 ~ ~ ,.q o ,Q o ,q o .Q o ,Q o .q o .Q o ,,Q o ,q o .Q o ~ ,,Q o ,,Q 0 ,~ o ,.O o .Q o ~ 0 ,.O o ,QuN *' iI I 1 I I I ~~ ~ ~ ~ ~ ~,~-,d-,d 1m i,9, ~.,~1208 ]!DZm.~ ~ .~ ~ ~ ~ .~ ~ .~ ~ ~ .~ ~ .~~ ~ Z Z Z Z~X Z X Z Z Z Zt~mZ",-~ ~ -,-~ ~ -,-I ~ -~ ",'~ ~ -,"~ -,-~ .
'4 ",~ ~ ",'~ ",'4 -~ -,-4 .-i~Z ~X:  0 .~ ~ ~:L~ ~.~ ~.~Z ~ ~.~ ~.~ ~X:  ~ ~ ~.~Z~ ~ ~,~ ~ ~0 ~ I Z I !
:Z:~ I I~:~ I ~.~ 0 I ~ I Z I Z I I ~ I ~ ~,2 ~o ~,-.
~ 0 ~ w Z  w' ' '88~,8~8~8o8o 8~8~8~8z z~~ 0 ~  ~i ' ~ ~,~ ~ ~ ~ ~o ~ ~ ~ ~ ~o ~ ~ ~~ #, '~ ~ ~E-~ ~ ~ = ~ ~,,:~ ~ Z ~ ?, , ,~ = I.q  ,.~ ~1~ ,.~ ~ ,.q ,q  t.9 ..q ,e,3 ,.q .q  ~ ~'-~ ,,q I:) .q  .q  I# = ~ = ~ = ~ = ~ = ~  I8~ ~Nz~z~z~zz~z~zo ,.~ ~ ..~ o m o ~ o , .~  ~ o m o ,~ o m ~ mi~ ~ i ~ ~i!
~ ~ ~ ~0~ I I  I I I  I~ ~ ' ~ ' ~  ~'~1OOOZ0 ,--I e-I U'I ('q ~I~ O~ ~"-.
,-.I ,-.Im - -  - -  ~.l-J - i~/ I I I I I~ ~ ~ ~ ~, ,U~-~-~ 0-, '~ I .~8~-~-~,~ ?~,~=000-~ .-4 -,..~121
