Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 237?240,Prague, June 2007. c?2007 Association for Computational LinguisticsMELB-MKB: Lexical Substitution System based on Relatives in ContextDavid Martinez, Su Nam Kim and Timothy BaldwinLT Group, CSSEUniversity of MelbourneVictoria 3010 Australia{davidm,snkim,tim}@csse.unimelb.edu.auAbstractIn this paper we describe the MELB-MKBsystem, as entered in the SemEval-2007 lex-ical substitution task.
The core of our sys-tem was the ?Relatives in Context?
unsuper-vised approach, which ranked the candidatesubstitutes by web-lookup of the word se-quences built combining the target contextand each substitute.
Our system ranked thirdin the final evaluation, performing close tothe top-ranked system.1 IntroductionThis paper describes the system we developed forthe SemEval lexical substitution task, a new task inSemEval-2007.
Although we tested different con-figurations on the trial data, our basic system reliedon WordNet relatives (Fellbaum, 1998) and Googlequeries in order to identify the most plausible sub-stitutes in the context.The main goal when building our system was tostudy the following factors: (i) substitution candi-date set, (ii) settings of the relative-based algorithm,and (iii) syntactic filtering.
We analysed these fac-tors over the trial data provided by the organisation,and used the BEST metric to tune our system.
Thismetric accepts multiple answers, and averages thescore across the answers.
We did not experimentwith the OOT (top 10 answers) and MULTIWORDmetrics.In the remainder of this paper we briefly intro-duce the basic Relatives in Context algorithm in Sec-tion 2.
Next we describe our experiments on the trialdata in Section 3.
Our final system and its results aredescribed in Section 4.
Finally, our conclusions areoutlined in Section 5.2 AlgorithmOur basic algorithm is an unsupervised method pre-sented in Martinez et al (2006).
This techniquemakes use of the WordNet relatives of the targetword for disambiguation, by way of the followingsteps: (i) obtain a set of close relatives from Word-Net for each sense of the target word; (ii) for eachtest instance define all possible word sequences thatinclude the target word; (iii) for each word sequence,substitute the target word with each relative, andthen query Google; (iv) rank queries according tothe following factors: length of the query, distanceof the relative to the target word, and number of hits;and (v) select the relative from the highest rankedquery.1For the querying step, first we tokenise each tar-get sentence, and then we apply sliding windows ofdifferent sizes (up to 6 tokens) that include the tar-get word.
For each window and each relative in thepool, we substitute the target word for the relative,and query Google.
The algorithm stops augment-ing the window for the relative when one of its sub-strings returns zero hits.
The length of the query ismeasured as the number of words, and the distanceof the relative to the target words gives preferenceto synonyms over hypernyms, and immediate hyper-nyms over further ones.One important parameter in this method is thecandidate set.
We performed different experimentsto measure the expected score we could achieve1In the case of WSD we would use the relative to chose thesense it relates to.237from WordNet relatives, and the contribution of dif-ferent types of filters (syntactic, frequency-based,etc.)
to the overall result.
We also explored othersettings of the algorithm, such as the ranking crite-ria, and the number of answers to return.
These ex-periments and some other modifications of the basicalgorithm are covered in Section 3.3 Development on Trial dataIn this section we analyse the coverage of WordNetover the data, the basic parameter exploration pro-cess, a syntactic filter, and finally the extra experi-ments we carried out before submission.
The trialdata consisted on 300 instances of 34 words withgold-standard annotations.3.1 WordNet coverageThe most obvious resource for selecting substitutioncandidates was WordNet, due to its size and avail-ability.
We used version 2.0 throughout this work.In our first experiment, we tried to determine whichkind of relationships to use, and the coverage of thegold-standard annotations that we could expect fromWordNet relations only.
As a basic set of relations,we used the following: SYNONYMY, SIMILAR-TO,ENTAILMENT, CAUSE, ALSO-SEE, and INSTANCE.We created two extended candidate sets using im-mediate and 2-step hypernyms (hype and hype2, re-spectively, in Table 1).Given that we are committed to using Word-Net, we set out to measure the percentage of gold-standard substitutes that were ?reachable?
using dif-ferent WordNet relations.
Table 1 shows the cov-erage for the three sets of candidates.
Instance-coverage indicates the percentage of instances thathave at least one of the gold-standard instances cov-ered from the candidate set.
We can see that the per-centage is surprisingly low.Any shortcoming in coverage will have a directimpact on performance, suggesting the need for al-ternate means to obtain substitution candidates.
Onepossibility is to extend the candidates from Word-Net by following links from the relatives (e.g.
col-lect all synonyms of the synonymous words), butthis could add many noisy candidates.
We can alsouse other lexical repositories built by hand or auto-matically, such as the distributional theusauri builtCandidate Set Subs.
Cov.
Inst.
Cov.basic 344/1152 (30%) 197 / 300 (66%)hype 404/1152 (35%) 229/300 (76%)hype2 419/1152 (36%) 229/300 (76%)Table 1: WordNet coverage for different candidatesets, based on substitute (Subs.)
and instance (Inst.
)coverage.in Lin (1998).
A different approach that we are test-ing for future work is to adapt the algorithm to workwith wildcards instead of explicit candidates.
Due totime constraints, we only relied on WordNet for oursubmission.3.2 Parameter TuningIn this experiment we tuned different parameters ofthe basic algorithm.
First, we observed the data inorder to identify the most relevant variables for thistask.
We tried to avoid including too many parame-ters and overfitting the system to the trial dataset.
Atthis point, we separated the instances by PoS, andstudied the following parameters:Candidate set: From WordNet, we tested fourpossible datasets for each target word: basic-set, 1st-sense (basic relations from the first sense only), hype(basic set and immediate hypernyms), and hype2(basic set and up to two-step hypernyms).Semcor-based filters: Semcor provides frequencyinformation for WordNet senses, and can be usedto identify rare senses.
As each candidate is ob-tained via WordNet semantic relations with the tar-get word, we can filter out those candidates that arerelated with unfrequent senses in Semcor.
We testedthree configurations: (1) no filter, (2) filter out candi-dates when the candidate-sense in the relation doesnot occur in Semcor, (3) and filter out candidateswhen the target-sense in the relation does not oc-cur in Semcor.
The filters can potentially lead to theremoval of all candidates, in which case a back-offis applied (see below).Relative-ranking criteria: Our algorithm ranksrelatives according to the length in words of theircontext-match.
In the case of ties, the number of re-turned hits from Google is applied.
The length canbe different depending on whether we count punc-tuation marks as separate tokens, and whether theword-length of substitute multiwords is included.238We tested three options: including the target word,not including the target word (multiwords count as asingle word), and not counting punctuation marks.Back-off: We need a back-off method in case thebasic algorithm does not find any matches.
Wetested the following: sense-ordered synonyms fromWordNet (highest sense first, randomly breakingties), and most frequent synonyms from the first sys-tem (using two corpora: Semcor and BNC).Number of answers: We also measured the per-formance for different numbers of system outputs(1, 2, or 3).All in all, we performed 324 (4x3x3x3x3) runsfor each PoS, based on the different combinations.The best scores for each PoS are shown in Table 2,together with the baselines.
We can see that the pre-cision is above the official WordNet baseline, but isstill very low.
The results illustrate the difficulty ofthe task.
In error analysis, we observed that the per-formance and settings varied greatly depending onthe PoS of the target word.
Adverbs produced thebest performance, followed by nouns.
The scoreswere very low for adjectives and verbs (the baselinescore for verbs was only 2%).We will now explain the main conclusions ex-tracted from the parameter analysis.
Regarding thecandidate set, we observed that using synonyms onlywas the best approach for all PoS, except for verbs,where hypernyms helped.
The option of limiting thecandidates to the first sense only helped for adjec-tives, but not for other PoS.For the Semcor-based filter, our results showedthat the target-sense filter improved the performancefor verbs and adverbs.
For nouns and adjectives, thecandidate-sense filter worked best.
All in all, apply-ing the Semcor filters was effective in removing raresenses and improving performance.The length criteria did not affect the results signif-icantly, and only made a difference in some extremecases.
Not counting the length of the target wordhelped slightly for nouns and adverbs, and removingpunctuation improved results for adjectives.
Regard-ing the back-off method, we observed that the countof frequencies in Semcor was the best approach forall PoS except verbs, which reached their best per-formance with BNC frequencies.PoS Relatives in Context WordNet BaselineNouns 18.4 14.9Verbs 6.7 2.0Adjectives 9.6 7.5Adverbs 31.1 29.9Overall 14.4 10.4Table 2: Experiments to tune parameters on the trialdata, based on the BEST metric.
Scores correspondto precision (which is the same as recall).Finally, we observed that the performance for theBEST score decreased significantly when more thanone answer was returned, probably due to the diffi-culty of the task.3.3 Syntactic FilterAfter the basic parameter analysis, we studied thecontribution of a syntactic filter to remove those can-didates that, when substituted, generate an ungram-matical sentence.
Intuitively, we would expect thisto have a high impact for verbs, which vary consid-erably in their subcategorisation properties.
For ex-ample, in the case of the (reduced) target If we orderour lives well ..., the syntactic filter should ideallydisallow candidates such as If we range our liveswell ...In order to apply this filter, we require a parserwhich has an explicit notion of grammaticality, rul-ing out the standard treebank parsers.
We experi-mented briefly with RASP, but found that the En-glish Resource Grammar (ERG: Flickinger (2002)),combined with the PET run-time engine, was thebest fit for out needs.
Unfortunately we could not getunknown word handling working within the ERGfor our submission, such that we get a meaningfuloutput for a given input string only in the case thatthe ERG has full lexical coverage over that string(we will never get a spanning parse for an inputwhere we are missing lexical entries).
As such, thesyntactic filter is limited in coverage only to stringswhere the ERG has lexical coverage.Ideally, we would have tested this filter on trialdata, but unfortunately we ran out of time.
Thus, wesimply eyeballed a sample of examples, and we de-cided to include this filter in our final submission.
Aswe will see in Section 4, its effect was minimal.
Weplan to perform a complete evaluation of this modulein the near future.2393.4 Extra experimentsOne of the limitations of the ?Relatives in Context?algorithm is that it only relies on the local con-text.
We wanted to explore the contribution of otherwords in the context for the task, and we performedan experiment including the Topical Signatures re-source (Agirre and Lopez de Lacalle, 2004).
Wesimply counted the overlapping of words shared be-tween the context and the different candidates.
Weonly tested this for nouns, for which the results werebelow baseline.
We then tried to integrate the topic-signature scores with the ?Relatives in Context?
al-gorithm, but we did not improve our basic system?sresults on the trial data.
Thus, this approach was notincluded in our final submission.Another problem we observed in error analysiswas that the Semcor-based filters were too strict insome cases, and it was desirable to have a way ofpenalising low frequency senses without removingthem completely.
Thus, we weighted senses by theinverse of their sense-rank.
As we did not have timeto test this intuition properly, we opted for applyingthe sense-weighting only when the candidates hadthe same context-match length, instead of using thenumber of hits.
We will see the effect of this methodin the next section.4 Final systemThe test data consisted of 1,710 instances.
For ourfinal system we applied the best configuration foreach PoS as observed in the development experi-ments, and the syntactic filter.
We also incorpo-rated the sense-weighting to solve ties.
The resultsof our system, the best competing system, and thebest baseline (WordNet) are shown in Table 3 for theBEST metric.
Precision and recall are provided forall the instances, and also for the ?Mode?
instances(those that have a single preferred candidate).Our method outperforms the baseline in all cases,and performs very close to the top system, rankingthird out of eight systems.
This result is consistentin the ?further analysis?
tables provided by the taskorganisers for subsets of data, where our system al-ways performs close to the top score.
The overallscores are below 13% recall for all systems whentargeting all instances.
This illustrates the difficultyof the task, and the similarity of the top-3 scores sug-All instances ModeSystem P R P RBest 12.90 12.90 20.65 20.65Relat.
in Context 12.68 12.68 20.41 20.41WordNet baseline 9.95 9.95 15.28 15.28Table 3: Official results based on the BEST metric.gests that similar resources (i.e.
WordNet) have beenused in the development of the systems.After the release of the gold-standard data, wetested two extra settings to measure the effect of thesyntactic filter and the sense-weighting in the finalscore.
We observed that our application of the syn-tactic filter had almost no effect in the performance,but sense-weighting increased the overall recall by0.4% (from 12.3% to 12.7%).5 ConclusionsAlthough the task was difficult and the scores werelow, we showed that by using WordNet and the lo-cal context we are able to outperform the baselinesand achieve close to top performance.
For futurework, we would like to integrate a parser with un-known word handling in our system.
We also aim toadapt the algorithm to match the target context withwildcards, in order to avoid explicitly defining thecandidate set.AcknowledgmentsThis research was carried out with support from Australian Re-search Council grant no.
DP0663879.ReferencesEneko Agirre and Oier Lopez de Lacalle.
2004.
Publicly avail-able topic signatures for all WordNet nominal senses.
InProc.
of the 4rd International Conference on Languages Re-sources and Evaluations (LREC 2004), pages 1123?6, Lis-bon, Portugal.Christiane Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, USA.Dan Flickinger.
2002.
On building a more efficient grammar byexploiting types.
In Stephan Oepen, Dan Flickinger, Jun?ichiTsujii, and Hans Uszkoreit, editors, Collaborative LanguageEngineering.
CSLI Publications, Stanford, USA.Dekang Lin.
1998.
Automatic retrieval and clustering of simi-lar words.
In Proceedings of COLING-ACL, pages 768?74,Montreal, Canada.David Martinez, Eneko Agirre, and Xinglong Wang.
2006.Word relatives in context for word sense disambiguation.
InProc.
of the 2006 Australasian Language Technology Work-shop, pages 42?50, Sydney, Australia.240
