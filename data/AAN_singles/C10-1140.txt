Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1245?1253,Beijing, August 2010Kernel Slicing: Scalable Online Training with Conjunctive FeaturesNaoki YoshinagaInstitute of Industrial Science,the University of Tokyoynaga@tkl.iis.u-tokyo.ac.jpMasaru KitsuregawaInstitute of Industrial Science,the University of Tokyokitsure@tkl.iis.u-tokyo.ac.jpAbstractThis paper proposes an efficient onlinemethod that trains a classifier with manyconjunctive features.
We employ kernelcomputation called kernel slicing, whichexplicitly considers conjunctions amongfrequent features in computing the poly-nomial kernel, to combine the merits oflinear and kernel-based training.
To im-prove the scalability of this training, wereuse the temporal margins of partial fea-ture vectors and terminate unnecessarymargin computations.
Experiments on de-pendency parsing and hyponymy-relationextraction demonstrated that our methodcould train a classifier orders of magni-tude faster than kernel-based online learn-ing, while retaining its space efficiency.1 IntroductionThe past twenty years have witnessed a growinguse of machine-learning classifiers in the field ofNLP.
Since the classification target of complexNLP tasks (e.g., dependency parsing and relationextraction) consists of more than one constituent(e.g., a head and a dependent in dependency pars-ing), we need to consider conjunctive features,i.e., conjunctions of primitive features that fo-cus on the particular clues of each constituent, toachieve a high degree of accuracy in those tasks.Training with conjunctive features involves aspace-time trade-off in the way conjunctive fea-tures are handled.
Linear models, such as log-linear models, explicitly estimate the weights ofconjunctive features, and training thus requires agreat deal of memory when we take higher-orderconjunctive features into consideration.
Kernel-based models such as support vector machines, onthe other hand, ensure space efficiency by usingthe kernel trick to implicitly consider conjunctivefeatures.
However, training takes quadratic timein the number of examples, even with online algo-rithms such as the (kernel) perceptron (Freund andSchapire, 1999), and we cannot fully exploit am-ple ?labeled?
data obtained with semi-supervisedalgorithms (Ando and Zhang, 2005; Bellare et al,2007; Liang et al, 2008; Daume?
III, 2008).We aim at resolving this dilemma in train-ing with conjunctive features, and propose onlinelearning that combines the time efficiency of lin-ear training and the space efficiency of kernel-based training.
Following the work by Goldbergand Elhadad (2008), we explicitly take conjunc-tive features into account that frequently appear inthe training data, and implicitly consider the otherconjunctive features by using the polynomial ker-nel.
We then improve the scalability of this train-ing by a method called kernel slicing, which al-lows us to reuse the temporal margins of partialfeature vectors and to terminate computations thatdo not contribute to parameter updates.We evaluate our method in two NLP tasks: de-pendency parsing and hyponymy-relation extrac-tion.
We demonstrate that our method is orders ofmagnitude faster than kernel-based online learn-ing while retaining its space efficiency.The remainder of this paper is organized as fol-lows.
Section 2 introduces preliminaries and no-tations.
Section 3 proposes our training method.Section 4 evaluates the proposed method.
Sec-tion 5 discusses related studies.
Section 6 con-cludes this paper and addresses future work.1245Algorithm 1 BASE LEARNER: KERNEL PA-IINPUT: T = {(x, y)t}|T |t=1, k : Rn ?
Rn 7?
R, C ?
R+OUTPUT: (S|T |,?|T |)1: initialize: S0 ?
?, ?0 ?
?2: for t = 1 to |T | do3: receive example (x, y)t : x ?
Rn, y ?
{?1,+1}4: compute margin: mt(x) =?si?St?1?ik(si,x)5: if `t = max {0, 1?
ymt(x)} > 0 then6: ?t ?
min?C, `t?x?2ff7: ?t ?
?t?1 ?
{?ty}, St ?
St?1 ?
{x}8: else9: ?t ?
?t?1, St ?
St?110: end if11: end for12: return (S|T |,?|T |)2 PreliminariesThis section first introduces a passive-aggressivealgorithm (Crammer et al, 2006), which we useas a base learner.
We then explain fast methods ofcomputing the polynomial kernel.Each example x in a classification problem isrepresented by a feature vector whose element xjis a value of a feature function, fj ?
F .
Here, weassume a binary feature function, fj(x) ?
{0, 1},which returns one if particular context data appearin the example.
We say that feature fj is active inexample x when xj = fj(x) = 1.
We denote abinary feature vector, x, as a set of active featuresx = {fj | fj ?
F , fj(x) = 1} for brevity; fj ?
xmeans that fj is active in x, and |x| represents thenumber of active features in x.2.1 Kernel Passive-Aggressive AlgorithmA passive-aggressive algorithm (PA) (Crammer etal., 2006) represents online learning that updatesparameters for given labeled example (x, y)t ?T in each round t. We assume a binary label,y ?
{?1,+1}, here for clarity.
Algorithm 1is a variant of PA (PA-I) that incorporates a ker-nel function, k. In round t, PA-I first computesa (signed) margin mt(x) of x by using the ker-nel function with support set St?1 and coefficients?t?1 (Line 4).
PA-I then suffers a hinge-loss,`t = max {0, 1?
ymt(x)} (Line 5).
If `t > 0,PA-I adds x to St?1 (Line 7).
Hyperparameter Ccontrols the aggressiveness of parameter updates.The kernel function computes a dot product inRH space without mapping x ?
Rn to ?
(x) ?RH (k(x,x?)
= ?(x)T?(x?)).
We can implic-itly consider (weighted) d or less order conjunc-tions of primitive features by using polynomialkernel function kd(s,x) = (sTx + 1)d. For ex-ample, given support vector s = (s1, s2)T andinput example x = (x1, x2)T, the second-orderpolynomial kernel returns k2(s,x) = (s1x1 +s2x2 +1)2 = 1+3s1x1 +3s2x2 +2s1x1s2x2 (?si, xi ?
{0, 1}).
This function thus implies map-ping ?2(x) = (1,?3x1,?3x2,?2x1x2)T.Although online learning is generally efficient,the kernel spoils its efficiency (Dekel et al, 2008).This is because the kernel evaluation (Line 4)takes O(|St?1||x|) time and |St?1| increases astraining continues.
The learner thus takes the mostamount of time in this margin computation.2.2 Kernel Computation for ClassificationThis section explains fast, exact methods of com-puting the polynomial kernel, which are meant totest the trained model, (S,?
), and involve sub-stantial computational cost in preparation.2.2.1 Kernel InvertedKudo and Matsumoto (2003) proposed polyno-mial kernel inverted (PKI), which builds invertedindices h(fj) ?
{s | s ?
S, fj ?
s} from eachfeature fj to support vector s ?
S to only con-sider support vector s relevant to given x suchthat sTx 6= 0.
The time complexity of PKI isO(B ?
|x| + |S|) where B ?
1|x|?fj?x |h(fj)|,which is smaller than O(|S||x|) if x has manyrare features fj such that |h(fj)|  |S|.To the best of our knowledge, this is the onlyexact method that has been used to speed up mar-gin computation in the context of kernel-based on-line learning (Okanohara and Tsujii, 2007).2.2.2 Kernel ExpansionIsozaki and Kazawa (2002) and Kudo and Mat-sumoto (2003) proposed kernel expansion, whichexplicitly maps both support set S and given ex-ample x ?
Rn into RH by mapping ?d imposedby kd:m(x) =(?si?S?i?d(si))T?d(x) =?fi?xdwi,1246where xd ?
{0, 1}H is a binary feature vectorin which xdi = 1 for (?d(x))i 6= 0, and w is aweight vector in the expanded feature space, Fd.The weight vector w is computed from S and ?
:w =?si?S?id?k=0ckdIk(sdi ), (1)where ckd is a squared coefficient of k-th order con-junctive features for d-th order polynomial kernel(e.g., c02 = 1, c12 = 3, and c22 = 2)1 and Ik(sdi ) issdi ?
{0, 1}H whose dimensions other than thoseof k-th order conjunctive features are set to zero.The time complexity of kernel expansion isO(|xd|) where |xd| = ?dk=0(|x|k)?
|x|d, whichcan be smaller than O(|S||x|) in usual NLP tasks(|x|  |S| and d ?
4).2.2.3 Kernel SplittingSince kernel expansion demands a huge mem-ory volume to store the weight vector, w, in RH(H =?dk=0(|F|k)), Goldberg and Elhadad (2008)only explicitly considered conjunctions amongfeatures fC ?
FC that commonly appear in sup-port set S, and handled the other conjunctive fea-tures relevant to rare features fR ?
F \ FC byusing the polynomial kernel:m(x) = m(x?)
+ m(x)?m(x?
)=?fi?x?dw?i +?si?SR?ik?d(si,x, x?
), (2)where x?
is x whose dimensions of rare featuresare set to zero, w?
is a weight vector computedwith Eq.
1 for FdC , and k?d(s,x, x?)
is defined as:k?d(s,x, x?)
?
kd(s,x)?
kd(s, x?
).We can space-efficiently compute the first termof Eq.
2 since |w?|  |w|, while we canquickly compute the second term of Eq.
2 sincek?d(si,x, x?)
= 0 when sTi x = sTi x?
; we onlyneed to consider a small subset of the support set,SR =?fR?x\x?
h(fR), that has at least one of therare features, fR, appearing in x\x?
(|SR|  |S|).Counting the number of features examined, thetime complexity of Eq.
2 is O(|x?d|+ |SR||x?|).1Following Lemma 1 in Kudo and Matsumoto (2003),ckd =?dl=k`dl?
`?km=0(?1)k?m ?ml` km?
?.3 AlgorithmThis section first describes the way kernel splittingis integrated into PA-I (Section 3.1).
We then pro-pose kernel slicing (Section 3.2), which enablesus to reuse the temporal margins computed in thepast rounds (Section 3.2.1) and to skip unneces-sary margin computations (Section 3.2.2).In what follows, we use PA-I as a base learner.Note that an analogous argument can be appliedto other perceptron-like online learners with theadditive weight update (Line 7 in Algorithm 1).3.1 Base Learner with Kernel SplittingA problem in integrating kernel splitting into thebase learner presented in Algorithm 1 is how todetermine FC , features among which we explic-itly consider conjunctions, without knowing thefinal support set, S|T |.
We heuristically solvethis by ranking feature f according to their fre-quency in the training data and by using the top-N frequent features in the training data as FC(= {f | f ?
F , RANK(f) ?
N}).2 Since S|T |is a subset of the examples, this approximates theselection from S|T |.
We empirically demonstratethe validity of this approach in the experiments.We then useFC to construct a base learner withkernel splitting; we replace the kernel computa-tion (Line 4 in Algorithm 1) with Eq.
2 where(S,?)
= (St?1,?t?1).
To compute mt(x?)
byusing kernel expansion, we need to additionallymaintain the weight vector w?
for the conjunctionsof common features that appear in St?1.The additive parameter update of PA-I enablesus to keep w?
to correspond to (St?1,?t?1).When we add x to support set St?1 (Line 7 inAlgorithm 1), we also update w?
with Eq.
1:w?
?
w?
+ ?tyd?k=0ckdIk(x?d).Following (Kudo and Matsumoto, 2003), weuse a trie (hereafter, weight trie) to maintain con-junctive features.
Each edge in the weight trie islabeled with a primitive feature, while each path2The overhead of counting features is negligible com-pared to the total training time.
If we want to run the learnerin a purely online manner, we can alternatively choose firstN features that appear in the processed examples as FC .1247represents a conjunctive feature that combines allthe primitive features on the path.
The weightsof conjunctive features are retrieved by travers-ing nodes in the trie.
We carry out an analogoustraversal in updating the parameters of conjunc-tive features, while registering a new conjunctivefeature by adding an edge to the trie.The base learner with kernel splitting combinesthe virtues of linear training and kernel-basedtraining.
It reduces to linear training when we in-crease N to |F|, while it reduces to kernel-basedtraining when we decrease N to 0.
The outputis support set S|T | and coefficients ?|T | (option-ally, w?
), to which the efficient classification tech-niques discussed in Section 2.2 and the one pro-posed by Yoshinaga and Kitsuregawa (2009) canbe applied.Note on weight trie construction The time andspace efficiency of this learner strongly dependson the way the weight trie is constructed.
Weneed to address two practical issues that greatlyaffect efficiency.
First, we traverse the trie fromthe rarest feature that constitutes a conjunctivefeature.
This rare-to-frequent mining helps us toavoid enumerating higher-order conjunctive fea-tures that have not been registered in the trie, whencomputing margin.
Second, we use RANK(f)encoded into a dlog128 RANK(f)e-byte string byusing variable-byte coding (Williams and Zobel,1999) as f ?s representation in the trie.
This en-coding reduces the trie size, since features withsmall RANK(f) will appear frequently in the trie.3.2 Base Learner with Kernel SlicingAlthough a base learner with kernel splitting canenjoy the merits of linear and kernel-based train-ing, it can simultaneously suffer from their demer-its.
Because the training takes polynomial timein the number of common features in x (|x?d| =?dk=0(|x?|k)?
|x?|d) at each round, we need to setN to a smaller value when we take higher-orderconjunctive features into consideration.
However,since the margin computation takes linear time inthe number of support vectors |SR| relevant to rarefeatures fR ?
F\FC , we need to setN to a largervalue when we handle a larger number of trainingexamples.
The training thereby slows down whenwe train a classifier with high-order conjunctivefeatures and a large number of training examples.We then attempt to improve the scalability ofthe training by exploiting a characteristic of la-beled data in NLP.
Because examples in NLP tasksare likely to be redundant (Yoshinaga and Kitsure-gawa, 2009), the learner computes margins of ex-amples that have many features in common.
If wecan reuse the ?temporal?
margins of partial featurevectors computed in past rounds, this will speedup the computation of margins.We propose kernel slicing, which generalizeskernel splitting in a purely feature-wise mannerand enables us to reuse the temporal partial mar-gins.
Starting from the most frequent feature f1 inx (f1 = argminf?x RANK(f)), we incrementallycompute mt(x) by accumulating a partial mar-gin, mjt (x) ?
mt(xj)?mt(xj?1), when we addthe j-th frequent feature fj in x:mt(x) = m0t +|x|?j=1mjt (x), (3)where m0t =?si?St?1 ?ikd(si,?)
=?i ?i, andxj has the j most frequent features in x (x0 = ?,xj =?j?1k=0{argminf?x\xk RANK(f)}).Partial margin mjt (x) can be computed by us-ing the polynomial kernel:mjt (x) =?si?St?1?ik?d(si,xj ,xj?1), (4)or by using kernel expansion:mjt (x) =?fi?xdj \xdj?1w?i.
(5)Kernel splitting is a special case of kernel slicing,which uses Eq.
5 for fj ?
FC and Eq.
4 for fj ?F \ FC .3.2.1 Reuse of Temporal Partial MarginsWe can speed up both Eqs.
4 and 5 by reusinga temporal partial margin, ?jt?
= mjt?
(x) that hadbeen computed in past round t?
(< t):mjt (x) = ?jt?
+?si?Sj?ik?d(si,xj ,xj?1), (6)where Sj = {s | s ?
St?1 \ St?
?1, fj ?
s}.1248Algorithm 2 KERNEL SLICINGINPUT: x ?
2F , St?1, ?t?1, FC ?
F , ?
: 2F 7?
N?
ROUTPUT: mt(x)1: initialize: x0 ?
?, j ?
1, mt(x)?
m0t2: repeat3: xj ?
xj?1 unionsq {argminf?x\xj?1 RANK(f)}4: retrieve partial margin: (t?, ?jt?)?
?
(xj)5: if fj ?
F \ FC or Eq.
7 is true then6: compute mjt(x) using Eq.
6 with ?jt?7: else8: compute mjt(x) using Eq.
59: end if10: update partial margin: ?(xj)?
(t,mjt(x))11: mt(x)?
mt(x) + mjt(x)12: until xj 6= x13: return mt(x)Eq.
6 is faster than Eq.
4,3 and can even befaster than Eq.
5.4 When RANK(fj) is high, xj ap-pears frequently in the training examples and |Sj |becomes small since t?
will be close to t. WhenRANK(fj) is low, xj rarely appears in the trainingexamples but we can still expect |Sj | to be smallsince the number of support vectors in St?1\St?
?1that have rare feature fj will be small.To compute Eq.
3, we now have the choice tochoose Eq.
5 or 6 for fj ?
FC .
Counting thenumber of features to be examined in computingmjt (x), we have the following criteria to deter-mine whether we can use Eq.
6 instead of Eq.
5:1 + |Sj ||xj?1| ?
|xdj \ xdj?1| =d?k=1(j ?
1k ?
1),where the left- and right-hand sides indicate thenumber of features examined in Eq.
6 for the for-mer and Eq.
5 for the latter.
Expanding the right-hand side for d = 2, 3 and dividing both sides with|xj?1| = j ?
1, we have:|Sj | ?
{1 (d = 2)j2 (d = 3).
(7)If this condition is met after retrieving the tem-poral partial margin, ?jt?
, we can compute partialmargin mjt (x) with Eq.
6.
This analysis reveals3When a margin of xj has not been computed, we regardt?
= 0 and ?jt?
= 0, which reduces Eq.
6 to Eq.
4.4We associate partial margins with partial feature se-quences whose features are sorted by frequent-to-rare order,and store them in a trie (partial margin trie).
This enables usto retrieve partial margin ?jt?
for given xj in O(1) time.that we can expect little speed-up for the second-order polynomial kernel; we will only use Eq.
6with third or higher-order polynomial kernel.Algorithm 2 summarizes the margin computa-tion with kernel slicing.
It processes each featurefj ?
x in frequent-to-rare order, and accumulatespartial margin mjt (x) to have mt(x).
Intuitivelyspeaking, when the algorithm uses the partial mar-gin, it only considers support vectors on each fea-ture that have been added since the last evaluationof the partial feature vector, to avoid the repetitionin kernel evaluation as much as possible.3.2.2 Termination of Margin ComputationKernel slicing enables another optimization thatexploits a characteristic of online learning.
Be-cause we need an exact margin, mt(x), only whenhinge-loss `t = 1?ymt(x) is positive, we can fin-ish margin computation as soon as we find that thelower-bound of ymt(x) is larger than one.When ymt(x) is larger than one after pro-cessing feature fj in Eq.
3, we quickly examinewhether this will hold even after we process theremaining features.
We can compute a possiblerange of partial margin mkt (x) with Eq.
4, hav-ing the upper- and lower-bounds, k?
?d and k?
?d, ofk?d(si,xk,xk?1) (= kd(si,xk)?
kd(si,xk?1)):mkt (x) ?
k?
?d?si?S+k?i + k?
?d?si?S?k?i (8)mkt (x) ?
k?
?d?si?S+k?i + k?
?d?si?S?k?i, (9)where S+k = {si | si ?
St?1, fk ?
si, ?i > 0},S?k = {si | si ?
St?1, fk ?
si, ?i < 0}, k?
?d =(k+1)d?
kd and k?
?d = 2d?
1 (?
0 ?
sTi xk?1 ?|xk?1| = k ?
1, sTi xk = sTi xk?1 + 1 for allsi ?
S+k ?
S?k ).We accumulate Eqs.
8 and 9 from rare to fre-quent features, and use the intermediate resultsto estimate the possible range of mt(x) beforeLine 3 in Algorithm 2.
If the lower bound ofymt(x) turns out to be larger than one, we ter-minate the computation of mt(x).As training continues, the model becomes dis-criminative and given x is likely to have a largermargin.
The impact of this termination will in-crease as the amount of training data expands.12494 EvaluationWe evaluated the proposed method in two NLPtasks: dependency parsing (Sassano, 2004) andhyponymy-relation extraction (Sumida et al,2008).
We used labeled data included in open-source softwares to promote the reproducibility ofour results.5 All the experiments were conductedon a server with an Intel R?
XeonTM 3.2 GHz CPU.We used a double-array trie (Aoe, 1989; Yata etal., 2009) as an implementation of the weight trieand the partial margin trie.4.1 Task DescriptionsJapanese Dependency Parsing A parser inputsa sentence segmented by a bunsetsu (base phrasein Japanese), and selects a particular pair of bun-setsus (dependent and head candidates); the clas-sifier then outputs label y = +1 (dependent) or?1 (independent) for the pair.
The features con-sist of the surface form, POS, POS-subcategoryand the inflection form of each bunsetsu, and sur-rounding contexts such as the positional distance,punctuations and brackets.
See (Yoshinaga andKitsuregawa, 2009) for details on the features.Hyponymy-Relation Extraction A hyponymyrelation extractor (Sumida et al, 2008) first ex-tracts a pair of entities from hierarchical listingstructures in Wikipedia articles (hypernym andhyponym candidates); a classifier then outputs la-bel y = +1 (correct) or ?1 (incorrect) for thepair.
The features include a surface form, mor-phemes, POS and the listing type for each entity,and surrounding contexts such as the hierarchicaldistance between the entities.
See (Sumida et al,2008) for details on the features.4.2 SettingsTable 1 summarizes the training data for the twotasks.
The examples for the Japanese dependencyparsing task were generated for a transition-basedparser (Sassano, 2004) from a standard data set.6We used the dependency accuracy of the parser5The labeled data for dependency parsing is availablefrom: http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/pecco/, andthe labeled data for hyponymy-relation extraction is avail-able from: http://nlpwww.nict.go.jp/hyponymy/.6Kyoto Text Corpus Version 4.0:http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus-e.html.DATA SET DEP REL|T | 296,776 201,664(y = +1) 150,064 152,199(y = ?1) 146,712 49,465Ave.
of |x| 27.6 15.4Ave.
of |x2| 396.1 136.9Ave.
of |x3| 3558.3 798.7|F| 64,493 306,036|F2| 3,093,768 6,688,886|F3| 58,361,669 64,249,234Table 1: Training data for dependency parsing(DEP) and hyponymy-relation extraction (REL).as model accuracy in this task.
In the hyponymy-relation extraction task, we randomly chosen twosets of 10,000 examples from the labeled data fordevelopment and testing, and used the remainingexamples for training.
Note that the number ofactive features, |Fd|, dramatically grows when weconsider higher-order conjunctive features.We compared the proposed method, PA-I SL(Algorithm 1 with Algorithm 2), to PA-I KER-NEL (Algorithm 1 with PKI; Okanohara and Tsu-jii (2007)), PA-I KE (Algorithm 1 with kernel ex-pansion; viz., kernel splitting with N = |F|),SVM (batch training of support vector machines),7and `1-LLM (stochastic gradient descent trainingof the `1-regularized log-linear model: Tsuruokaet al (2009)).
We refer to PA-I SL that does notreuse temporal partial margins as PA-I SL?.
Todemonstrate the impact of conjunctive features onmodel accuracy, we also trained PA-I without con-junctive features.
The number of iterations in PA-Iwas set to 20, and the parameters of PA-I were av-eraged in an efficient manner (Daume?
III, 2006).We explicitly considered conjunctions among top-N (N = 125 ?
2n;n ?
0) features in PA-I SLand PA-I SL?.
The hyperparameters were tuned tomaximize accuracy on the development set.4.3 ResultsTables 2 and 3 list the experimental results forthe two tasks (due to space limitations, Tables 2and 3 list PA-I SL with parameter N that achievedthe fastest speed).
The accuracy of the modelstrained with the proposed method was better than`1-LLMs and was comparable to SVMs.
The infe-7http://chasen.org/?taku/software/TinySVM/1250METHOD d ACC.
TIME MEMORYPA-I 1 88.56% 3s 55MB`1-LLM 2 90.55% 340s 1656MBSVM 2 90.76% 29863s 245MBPA-I KERNEL 2 90.68% 8361s 84MBPA-I KE 2 90.67% 41s 155MBPA-I SL?N=4000 2 90.71% 33s 95MB`1-LLM 3 90.76% 4057s 21,499MBSVM 3 90.93% 25912s 243MBPA-I KERNEL 3 90.90% 8704s 83MBPA-I KE 3 90.90% 465s 993MBPA-I SLN=250 3 90.89% 262s 175MBTable 2: Training time for classifiers used in de-pendency parsing task.030060090012001500102 103 104 105Trainingtime[s]N: # of expanded primitive featuresPA-I SPPA-I SL?PA-I SLFigure 1: Training time for PA-I variants as a func-tion of the number of expanded primitive featuresin dependency parsing task (d = 3).rior accuracy of PA-I (d = 1) confirmed the ne-cessity of conjunctive features in these tasks.
Theminor difference among the model accuracy of thethree PA-I variants was due to rounding errors.PA-I SL was the fastest of the training meth-ods with the same feature set, and its space effi-ciency was comparable to the kernel-based learn-ers.
PA-I SL could reduce the memory footprintfrom 993MB8 to 175MB for d = 3 in the depen-dency parsing task, while speeding up training.Although linear training (`1-LLM and PA-I KE)dramatically slowed down when we took higher-order conjunctive features into account, kernelslicing alleviated deterioration in speed.
Espe-cially in the hyponymy-relation extraction task,PA-I SL took almost the same time regardless ofthe order of conjunctive features.8`1-LLM took much more memory than PA-I KE mainlybecause `1-LLM expands conjunctive features in the exam-ples prior to training, while PA-I KE expands conjunctive fea-tures in each example on the fly during training.
Interestedreaders may refer to (Chang et al, 2010) for this issue.METHOD d ACC.
TIME MEMORYPA-I 1 91.75% 2s 28MB`1-LLM 2 92.67% 136s 1683MBSVM 2 92.85% 12306s 139MBPA-I KERNEL 2 92.91% 1251s 54MBPA-I KE 2 92.96% 27s 143MBPA-I SL?N=8000 2 92.88% 17s 77MB`1-LLM 3 92.86% 779s 14,089MBSVM 3 93.09% 17354s 140MBPA-I KERNEL 3 93.14% 1074s 49MBPA-I KE 3 93.11% 103s 751MBPA-I SLN=125 3 93.05% 17s 131MBTable 3: Training time for classifiers used inhyponymy-relation extraction task.0306090120150102 103 104 105 106Trainingtime[s]N: # of expanded primitive featuresPA-I SPPA-I SL?PA-I SLFigure 2: Training time for PA-I variants as a func-tion of the number of expanded primitive featuresin hyponymy-relation extraction task (d = 3).Figures 1 and 2 plot the trade-off between thenumber of expanded primitive features and train-ing time with PA-I variants (d = 3) in the twotasks.
Here, PA-I SP is PA-I with kernel slicingwithout the techniques described in Sections 3.2.1and 3.2.2, viz., kernel splitting.
The early termi-nation of margin computation reduces the train-ing time when N is large.
The reuse of temporalmargins makes the training time stable regardlessof parameter N .
This suggests a simple, effec-tive strategy for calibrating N ; we start the train-ing with N = |F|, and when the learner reachesthe allowed memory size, we shrink N to N/2by pruning sub-trees rooted by rarer features withRANK(f) > N/2 in the weight trie.Figures 3 and 4 plot training time with PA-Ivariants (d = 3) for the two tasks as a functionof the training data size.
PA-I SP inherited the de-merit of PA-I KERNEL which takes quadratic timein the number of examples, while PA-I SL took al-most linear time in the number of examples.125101002003004005006000 50000 100000 150000 200000 250000 300000Trainingtime[s]|T |: # of training examplesPA-I KERNELPA-I SPN=250PA-I SLN=250Figure 3: Training time for PA-I variants as a func-tion of the number of training examples in depen-dency parsing task (d = 3).5 Related WorkThere are several methods that learn ?simpler?models with fewer variables (features or supportvectors), to ensure scalability in training.Researchers have employed feature selectionto assure space-efficiency in linear training.
Wuet al (2007) used frequent-pattern mining to se-lect effective conjunctive features prior to train-ing.
Okanohara and Tsujii (2009) revised graft-ing for `1-LLM (Perkins et al, 2003) to prune use-less conjunctive features during training.
Iwakuraand Okamoto (2008) proposed a boosting-basedmethod that repeats the learning of rules repre-sented by feature conjunctions.
These methods,however, require us to tune the hyperparameter totrade model accuracy and the number of conjunc-tive features (memory footprint and training time);note that an accurate model may need many con-junctive features (in the hyponymy-relation ex-traction task, `1-LLM needed 15,828,122 featuresto obtain the best accuracy, 92.86%).
Our method,on the other hand, takes all conjunctive featuresinto consideration regardless of parameter N .Dekel et al (2008) and Cavallanti et al (2007)improved the scalability of the (kernel) percep-tron, by exploiting redundancy in the training datato bound the size of the support set to given thresh-old B (?
|St|).
However, Orabona et al (2009)reported that the models trained with these meth-ods were just as accurate as a naive method thatceases training when |St| reaches the same thresh-old, B.
They then proposed budget online learn-ing based on PA-I, and it reduced the size of thesupport set to a tenth with a tolerable loss of accu-0501001502000 50000 100000 150000 200000Trainingtime[s]|T |: # of training examplesPA-I KERNELPA-I SPN=125PA-I SLN=125Figure 4: Training time for PA-I variants as afunction of the number of training examples inhyponymy-relation extraction task (d = 3).racy.
Their method, however, requiresO(|St?1|2)time in updating the parameters in round t, whichdisables efficient training.
We have proposed anorthogonal approach that exploits the data redun-dancy in evaluating the kernel to train the samemodel as the base learner.6 ConclusionIn this paper, we proposed online learning withkernel slicing, aiming at resolving the space-timetrade-off in training a classifier with many con-junctive features.
The kernel slicing generalizeskernel splitting (Goldberg and Elhadad, 2008) ina purely feature-wise manner, to truly combine themerits of linear and kernel-based training.
To im-prove the scalability of the training with redundantdata in NLP, we reuse the temporal partial marginscomputed in past rounds and terminate unneces-sary margin computations.
Experiments on de-pendency parsing and hyponymy-relation extrac-tion demonstrated that our method could train aclassifier orders of magnitude faster than kernel-based learners, while retaining its space efficiency.We will evaluate our method with ample la-beled data obtained by the semi-supervised meth-ods.
The implementation of the proposed algo-rithm for kernel-based online learners is availablefrom http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/.Acknowledgment We thank Susumu Yata forproviding us practical lessons on the double-arraytrie, and thank Yoshimasa Tsuruoka for makinghis `1-LLM code available to us.
We are also in-debted to Nobuhiro Kaji and the anonymous re-viewers for their valuable comments.1252ReferencesAndo, Rie Kubota and Tong Zhang.
2005.
A frame-work for learning predictive structures from multi-ple tasks and unlabeled data.
Journal of MachineLearning Research, 6:1817?1853.Aoe, Jun?ichi.
1989.
An efficient digital search al-gorithm by using a double-array structure.
IEEETransactions on Software Engineering, 15(9):1066?1077.Bellare, Kedar, Partha Pratim Talukdar, Giridhar Ku-maran, Fernando Pereira, Mark Liberman, AndrewMcCallum, and Mark Dredze.
2007.
Lightly-supervised attribute extraction.
In Proc.
NIPS 2007Workshop on Machine Learning for Web Search.Cavallanti, Giovanni, Nicolo` Cesa-Bianchi, and Clau-dio Gentile.
2007.
Tracking the best hyperplanewith a simple budget perceptron.
Machine Learn-ing, 69(2-3):143?167.Chang, Yin-Wen, Cho-Jui Hsieh, Kai-Wei Chang,Michael Ringgaard, and Chih-Jen Lin.
2010.
Train-ing and testing low-degree polynomial data map-pings via linear SVM.
Journal of Machine LearningResearch, 11:1471?1490.Crammer, Koby, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 7:551?585.Daume?
III, Hal.
2006.
Practical Structured Learn-ing Techniques for Natural Language Processing.Ph.D.
thesis, University of Southern California.Daume?
III, Hal.
2008.
Cross-task knowledge-constrained self training.
In Proc.
EMNLP 2008,pages 680?688.Dekel, Ofer, Shai Shalev-Shwartz, and Yoram Singer.2008.
The forgetron: A kernel-based percep-tron on a budget.
SIAM Journal on Computing,37(5):1342?1372.Freund, Yoav and Robert E. Schapire.
1999.
Largemargin classification using the perceptron algo-rithm.
Machine Learning, 37(3):277?296.Goldberg, Yoav and Michael Elhadad.
2008.splitSVM: fast, space-efficient, non-heuristic, poly-nomial kernel computation for NLP applications.
InProc.
ACL-08: HLT, Short Papers, pages 237?240.Isozaki, Hideki and Hideto Kazawa.
2002.
Efficientsupport vector classifiers for named entity recogni-tion.
In Proc.
COLING 2002, pages 1?7.Iwakura, Tomoya and Seishi Okamoto.
2008.
A fastboosting-based learner for feature-rich tagging andchunking.
In Proc.
CoNLL 2008, pages 17?24.Kudo, Taku and Yuji Matsumoto.
2003.
Fast methodsfor kernel-based text analysis.
In Proc.
ACL 2003,pages 24?31.Liang, Percy, Hal Daume?
III, and Dan Klein.
2008.Structure compilation: trading structure for fea-tures.
In Proc.
ICML 2008, pages 592?599.Okanohara, Daisuke and Jun?ichi Tsujii.
2007.
A dis-criminative language model with pseudo-negativesamples.
In Proc.
ACL 2007, pages 73?80.Okanohara, Daisuke and Jun?ichi Tsujii.
2009.
Learn-ing combination features with L1 regularization.
InProc.
NAACL HLT 2009, Short Papers, pages 97?100.Orabona, Francesco, Joseph Keshet, and Barbara Ca-puto.
2009.
Bounded kernel-based online learning.Journal of Machine Learning Research, 10:2643?2666.Perkins, Simon, Kevin Lacker, and James Theiler.2003.
Grafting: fast, incremental feature selectionby gradient descent in function space.
Journal ofMachine Learning Research, 3:1333?1356.Sassano, Manabu.
2004.
Linear-time dependencyanalysis for Japanese.
In Proc.
COLING 2004,pages 8?14.Sumida, Asuka, Naoki Yoshinaga, and Kentaro Tori-sawa.
2008.
Boosting precision and recall of hy-ponymy relation acquisition from hierarchical lay-outs in Wikipedia.
In Proc.
LREC 2008, pages2462?2469.Tsuruoka, Yoshimasa, Jun?ichi Tsujii, and SophiaAnaniadou.
2009.
Stochastic gradient descenttraining for L1-regularized log-linear models withcumulative penalty.
In Proc.
ACL-IJCNLP 2009,pages 477?485.Williams, Hugh E. and Justin Zobel.
1999.
Compress-ing integers for fast file access.
The Computer Jour-nal, 42(3):193?201.Wu, Yu-Chieh, Jie-Chi Yang, and Yue-Shi Lee.
2007.An approximate approach for training polynomialkernel SVMs in linear time.
In Proc.
ACL 2007, In-teractive Poster and Demonstration Sessions, pages65?68.Yata, Susumu, Masahiro Tamura, Kazuhiro Morita,Masao Fuketa, and Jun?ichi Aoe.
2009.
Sequentialinsertions and performance evaluations for double-arrays.
In Proc.
the 71st National Convention ofIPSJ, pages 1263?1264.
(In Japanese).Yoshinaga, Naoki and Masaru Kitsuregawa.
2009.Polynomial to linear: efficient classification withconjunctive features.
In Proc.
EMNLP 2009, pages1542?1551.1253
