Proceedings of NAACL-HLT 2015, pages 36?40,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsAMRICA: an AMR Inspector for Cross-language AlignmentsNaomi SaphraCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21211, USAnsaphra@jhu.eduAdam LopezSchool of InformaticsUniversity of EdinburghEdinburgh, United Kingdomalopez@inf.ed.ac.ukAbstractAbstract Meaning Representation (AMR), anannotation scheme for natural language se-mantics, has drawn attention for its simplic-ity and representational power.
Because AMRannotations are not designed for human read-ability, we present AMRICA, a visual aid forexploration of AMR annotations.
AMRICAcan visualize an AMR or the difference be-tween two AMRs to help users diagnose in-terannotator disagreement or errors from anAMR parser.
AMRICA can also automati-cally align and visualize the AMRs of a sen-tence and its translation in a parallel text.
Webelieve AMRICA will simplify and streamlineexploratory research on cross-lingual AMRcorpora.1 IntroductionResearch in statistical machine translation has be-gun to turn to semantics.
Effective semantics-basedtranslation systems pose a crucial need for a practi-cal cross-lingual semantic representation.
One suchschema, Abstract Meaning Representation (AMR;Banarescu et al, 2013), has attracted attention for itssimplicity and expressive power.
AMR representsthe meaning of a sentence as a directed graph overconcepts representing entities, events, and propertieslike names or quantities.
Concepts are representedby nodes and are connected by edges representingrelations?roles or attributes.
Figure 1 shows an ex-ample of the AMR annotation format, which is opti-mized for text entry rather than human comprehen-sion.For human analysis, we believe it is easier to visu-alize the AMR graph.
We present AMRICA, a sys-(b / be-located-at-91 :li 4:ARG1 (i / i):ARG2 (c / country:name (n / name:op1 "New" :op2 "Zealand")):time (w / week :quant 2:time (p / past)))Figure 1: AMR for ?I?ve been in New Zealand the pasttwo weeks.?
(Linguistic Data Consortium, 2013)tem for visualizing AMRs in three conditions.
First,AMRICA can display AMRs as in Figure 2.
Sec-ond, AMRICA can visualize differences betweenaligned AMRs of a sentence, enabling users to diag-nose differences in multiple annotations or betweenan annotation and an automatic AMR parse (Sec-tion 2).
Finally, to aid researchers studying cross-lingual semantics, AMRICA can visualize differ-ences between the AMR of a sentence and that ofits translation (Section 3) using a novel cross-lingualextension to Smatch (Cai and Knight, 2013).
TheAMRICA code and a tutorial are publicly available.12 Interannotator AgreementAMR annotators and researchers are still exploringhow to achieve high interannotator agreement (Caiand Knight, 2013).
So it is useful to visualize apair of AMRs in a way that highlights their disagree-ment, as in Figure 3.
AMRICA shows in black thosenodes and edges which are shared between the anno-tations.
Elements that differ are red if they appear inone AMR and blue if they appear in the other.
Thisfeature can also be used to explore output from an1http://github.com/nsaphra/AMRICA36Figure 2: AMRICA visualization of AMR in Figure 1.Figure 3: AMRICA visualization of the disagreementbetween two independent annotations of the sentence inFigure 1.automatic AMR parser in order to diagnose errors.To align AMRs, we use the public implementationof Smatch (Cai and Knight, 2013).2Since it alsoforms the basis for our cross-lingual visualization,we briefly review it here.AMR distinguishes between variable and con-stant nodes.
Variable nodes, like i in Figure 1, rep-resent entities and events, and may have multiple in-coming and outgoing edges.
Constant nodes, like 2in Figure 1, participate in exactly one relation, mak-ing them leaves of a single parent variable.
Smatchcompares a pair of AMRs that have each been de-composed into three kinds of relationships:2http://amr.isi.edu/download/smatch-v2.0.tar.gz1.
The set V of instance-of relations describe theconceptual class of each variable.
In Figure 1,(c / country) specifies that c is an in-stance of a country.
If node v is an instanceof concept c, then (v, c) ?
V .2.
The set E of variable-to-variable relations likeARG2(b, c) describe relationships betweenentities and/or events.
If r is a relation fromvariable v1to variable v2, then (r, v1, v2) ?
E.3.
The set C of variable-to-constant relations likequant(w, 2) describe properties of entitiesor events.
If r is a relation from variable v toconstant x, then (r, v, x) ?
C.Smatch seeks the bijective alignment?b : V ?
V?between an AMR G = (V,E,C) and a larger AMRG?= (V?, E?, C?)
satisfying Equation 1, where I isan indicator function returning 1 if its argument istrue, 0 otherwise.
?b = argmaxb?
(v,c)?VI((b(v), c) ?
V?
)+ (1)?
(r,v1,v2)?EI((r, b(v1), b(v2)) ?
E?)+?
(r,v,c)?CI((r, b(v), c) ?
C?
)Cai and Knight (2013) conjecture that this opti-mization can be shown to be NP-complete by reduc-tion to the subgraph isomorphism problem.
Smatchapproximates the solution with a hill-climbing algo-rithm.
It first creates an alignment b0in which eachnode of G is aligned to a node in G?with the sameconcept if such a node exists, or else to a randomnode.
It then iteratively produces an alignment biby greedily choosing the best alignment that can beobtained from bi?1by swapping two alignments oraligning a node in G to an unaligned node, stoppingwhen the objective no longer improves and returningthe final alignment.
It uses random restarts since thegreedy algorithm may only find a local optimum.3 Aligning Cross-Language AMRsAMRICA offers the novel ability to align AMR an-notations of bitext.
This is useful for analyzing37AMR annotation differences across languages, andfor analyzing translation systems that use AMR asan intermediate representation.
The alignment ismore difficult than in the monolingual case, sincenodes in AMRs are labeled in the language ofthe sentence they annotate.
AMRICA extends theSmatch alignment algorithm to account for this dif-ficulty.AMRICA does not distinguish between constantsand variables, since their labels tend to be groundedin the words of the sentence, which it uses for align-ment.
Instead, it treats all nodes as variables andcomputes the similarities of their node labels.
Sincenode labels are in their language of origin, exactstring match no longer works as a criterion for as-signing credit to a pair of aligned nodes.
There-fore AMRICA uses a function L : V ?
V ?
Rindicating the likelihood that the nodes align.
Thesechanges yield the new objective shown in Equation 2for AMRs G = (V,E) and G?= (V?, E?
), where Vand V?are now sets of nodes, and E and E?are de-fined as before.
?b = argmaxb?v?VL(v, b(v))+ (2)?
(r,v1,v2)?EI((r, b(v1), b(v2)) ?
E?
)If the labels of nodes v and v?match, thenL(v, v?)
= 1.
If they do not match, then L de-composes over source-node-to-word alignment as,source-word-to-target-word alignment a, and target-word-to-node at, as illustrated in Figure 5.
Moreprecisely, if the source and target sentences containn and n?words, respectively, then L is defined byEquation 3.
AMRICA takes a parameter ?
to con-trol how it weights these estimated likelihoods rela-tive to exact matches of relation and concept labels.L(v, v?)
= ?n?i=1Pr(as(v) = i)?
(3)n?
?j=1Pr(ai= j) ?
Pr(at(v?)
= j)Node-to-word probabilities Pr(as(v) = i) andPr(as(v?)
= j) are computed as described in Sec-tion 3.1.
Word-to-word probabilities Pr(ai= j)are computed as described in Section 3.2.
AM-RICA uses the Smatch hill-climbing algorithm toyield alignments like that in Figure 4.3.1 Node-to-word and word-to-node alignmentAMRICA can accept node-to-word alignments asoutput by the heuristic aligner of Flanigan et al(2014).3In this case, the tokens in the aligned spanreceive uniform probabilities over all nodes in theiraligned subgraph, while all other token-node align-ments receive probability 0.
If no such alignmentsare provided, AMRICA aligns concept nodes to to-kens matching the node?s label, if they exist.
A to-ken can align to multiple nodes, and a node to multi-ple tokens.
Otherwise, alignment probability is uni-formly distributed across unaligned nodes or tokens.3.2 Word-to-word AlignmentAMRICA computes the posterior probability of thealignment between the ith word of the source and jthword of the target as an equal mixture between theposterior probabilities of source-to-target and target-to-source alignments from GIZA++ (Och and Ney,2003).4To obtain an approximation of the pos-terior probability in each direction, it uses the m-best alignments a(1).
.
.
a(m), where a(k)i= j indi-cates that the ith source word aligns to the jth targetword in the kth best alignment, and Pr(a(k)) is theprobability of the kth best alignment according toGIZA++.
We then approximate the posterior proba-bility as follows.Pr(ai= j) =?mk=1Pr(a(k))I[a(k)i= j]?mk=1Pr(a(k))4 Demonstration ScriptAMRICA makes AMRs accessible for data explo-ration.
We will demonstrate all three capabilitiesoutlined above, allowing participants to visually ex-plore AMRs using graphics much like those in Fig-ures 2, 3, and 4, which were produced by AMRICA.We will then demonstrate how AMRICA can beused to generate a preliminary alignment for bitext3Another option for aligning AMR graphs to sentences isthe statistical aligner of Pourdamghani et al (2014)4In experiments, this method was more reliable than usingeither alignment alone.38Figure 5: Cross-lingual AMR example from Nianwen Xue et al (2014).
The node-to-node alignment of the high-lighted nodes is computed using the node-to-word, word-to-word, and node-to-word alignments indicated by greendashed lines.Figure 4: AMRICA visualization of the example in Fig-ure 5.
Chinese concept labels are first in shared nodes.AMRs, which can be corrected by hand to providetraining data or a gold standard alignment.Information to get started with AMRICA is avail-able in the README for our publicly availablecode.AcknowledgmentsThis research was supported in part by the NationalScience Foundation (USA) under awards 1349902and 0530118.
We thank the organizers of the2014 Frederick Jelinek Memorial Workshop and themembers of the workshop team on Cross-LingualAbstract Meaning Representations (CLAMR), whotested AMRICA and provided vital feedback.ReferencesL.
Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Grif-fitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer,and N. Schneider.
2013.
Abstract meaning represen-tation for sembanking.
In Proc.
of the 7th Linguistic39Annotation Workshop and Interoperability with Dis-course.S.
Cai and K. Knight.
2013.
Smatch: an evaluation met-ric for semantic feature structures.
In Proc.
of ACL.J.
Flanigan, S. Thomson, C. Dyer, J. Carbonell, and N. A.Smith.
2014.
A discriminative graph-based parser forthe abstract meaning representation.
In Proc.
of ACL.Nianwen Xue, Ondrej Bojar, Jan Hajic, Martha Palmer,Zdenka Uresova, and Xiuhong Zhang.
2014.
Not aninterlingua, but close: Comparison of English AMRsto Chinese and Czech.
In Proc.
of LREC.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51, Mar.N.
Pourdamghani, Y. Gao, U. Hermjakob, and K. Knight.2014.
Aligning english strings with abstract meaningrepresentation graphs.Linguistic Data Consortium.
2013.
DEFT phase 1 AMRannotation R3 LDC2013E117.40
