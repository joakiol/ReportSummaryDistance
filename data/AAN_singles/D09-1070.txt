Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 668?677,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPUnsupervised morphological segmentation and clustering with documentboundariesTaesun Moon, Katrin Erk, and Jason BaldridgeDepartment of LinguisticsUniversity of Texas at Austin1 University Station B5100Austin, TX 78712-0198 USA{tsmoon,katrin.erk,jbaldrid}@mail.utexas.eduAbstractMany approaches to unsupervised mor-phology acquisition incorporate the fre-quency of character sequences with re-spect to each other to identify word stemsand affixes.
This typically involves heuris-tic search procedures and calibrating mul-tiple arbitrary thresholds.
We present asimple approach that uses no thresholdsother than those involved in standard ap-plication of ?2 significance testing.
Akey part of our approach is using docu-ment boundaries to constrain generation ofcandidate stems and affixes and clusteringmorphological variants of a given wordstem.
We evaluate our model on Englishand the Mayan language Uspanteko; itcompares favorably to two benchmark sys-tems which use considerably more com-plex strategies and rely more on experi-mentally chosen threshold values.1 IntroductionUnsupervised morphology acquisition attempts tolearn from raw corpora one or more of the follow-ing about the written morphology of a language:(1) the segmentation of the set of word types in acorpus (Creutz and Lagus, 2007), (2) the cluster-ing of word types in a corpus based on some notionof morphological relatedness (Schone and Juraf-sky, 2000), (3) the generation of out-of-vocabularyitems which are morphologically related to otherword types in the corpus (Yarowsky et al, 2001).We take a novel approach to segmenting wordsand clustering morphologically related words.The approach uses no parameters that need tobe tuned on data.
The two main ideas of theapproach are (a) the filtering of affixes by sig-nificant co-occurrence, and (b) the integration ofknowledge of document boundaries when gener-ating candidate stems and affixes and when clus-tering morphologically related words.
The mainapplication that we envision for our approach isto produce interlinearized glossed texts for under-resourced/endangered languages (Palmer et al,2009).
Thus, we strive to eliminate hand-tunedparameters to enable documentary linguists to useour model as a preprocessing step for their manualanalysis of stems and affixes.
To require a docu-mentary linguist?who is likely to have little to noknowledge of NLP methods?to tune parameters isunfeasible.
Additionally, data-driven explorationof parameter settings is unlikely to be reliable inlanguage documentation since datasets typicallyare quite small.
To be relevant in this context, amodel needs to produce useful results out of thebox.Constraining learning by using documentboundaries has been used quite effectively in un-supervised word sense disambiguation (Yarowsky,1995).
Many applications in information retrievalare built on the statistical correlation between doc-uments and terms.
However, we are unaware ofcases where knowledge of document boundarieshas been used for unsupervised learning for mor-phology.
The intuition behind our approach is verysimple: if two words in a single document arevery similar in terms of orthography, then the twowords are likely to be related morphologically.
Wemeasure how integrating these assumptions intoour model at different stages affects performance.We define a simple pipeline model.
After gen-erating candidate stems and affixes (possibly con-strained by document boundaries), a ?2 test basedon global corpus counts filters out unlikely affixes.Mutually consistent affix pairs are then clusteredto form affix groups.
These in turn are used tobuild morphologically related word clusters, pos-sibly constrained by evidence from co-occurenceof word forms in documents.
Following Schoneand Jurafsky (2000), clusters are evaluated for668whether they capture inflectional paradigms usingCELEX (Baayen et al, 1993).We are unaware of other work on morphologyusing ?2 tests despite its wide application acrossmany disciplines.1 This may be due to the largedegree of noise found in the candidate affix setsinduced through other candidate generation meth-ods.
The ?2 test has two standard thresholds?asignificance threshold and a lower bound on ob-served counts.
These are the only manually setparameters we require?and we in fact use thewidely accepted standard values for these thresh-olds without varying them in our experiments.This is a significant improvement over other ap-proaches that typically require a number of arbi-trary thresholds and parameters yet provide littleintuitive justification for them.
(We give examplesof these in ?3.
)We evaluate our approach on two languages,English and Uspanteko, and compare its per-formance to two benchmark systems, Morfessor(Creutz and Lagus, 2007) and Linguistica (Gold-smith, 2001).
English is commonly used in otherstudies and permits the use of CELEX as a goldstandard for evaluation.
Uspanteko is an endan-gered Mayan language for which we have a set ofinterlinearized glossed texts (IGT) (Pixabaj et al,2007; Palmer et al, 2009).
IGT provides word-by-word morpheme segmenation, which we useto create a synthetic gold standard.
In additionto evaluation against this standard, Telma KaanPixabaj?a Mayan linguist who helped create theannotated corpus?reviewed by hand 100 wordclusters produced by our system, Morfessor andLinguistica.
Note that because English is suffixaland Uspanteko is both prefixal and suffixal, we usea slightly modified model for Uspanteko.The approach introduced in this paper comparesfavorably to Linguistica and Morfessor, two mod-els that employ much more complex strategies andrely on experimentally-tuned language/corpus-specific parameters.
In our evaluation, documentboundary awareness greatly benefits precision forsmall datasets, blocking acquisition of spurious af-fixes.
For large datasets, global candidate genera-tion outperforms document-aware candidate gen-eration at the task of filtering out spurious stems,but document-aware clustering improves preci-sion.
These findings are promising for the applica-tion of this approach to under-resourced languages1Monson (2004) suggests, but does not actually use, ?2.like Uspanteko.2 Unsupervised morphology acquisitionUnsupervised morphology acquisition aims tomodel one or more of three properties of writ-ten morphology: segmentation, clustering arounda common stem, and generation of new wordforms with productive affixes.
Intuitively, there arestraightforward, but non-trivial, challenges thatarise when evaluating a model.
One large chal-lenge is distinguishing derivational from inflec-tional morphology.
Most approaches deal with to-kens without considering context.
Since inflec-tional morphology is virtually always driven bysyntax and word context, such approaches are un-able to learn only inflectional morphology or onlyderivational morphology.
Even approaches whichtake context into consideration (Schone and Juraf-sky, 2000; Baroni et al, 2002; Freitag, 2005) can-not learn specifically for one or the other.In addition, the evaluation of both segmentationand clustering involves arbitrary judgment calls.Concerning segmentation, should altimeter andaltitude be one morpheme or two?
(The sam-ple English gold standard for MorphoChallenge2009 provides alti+meter but altitude.)
Similar is-sues arise when evaluating clusters of related wordforms if inflection and derivation are not distin-guished.
Does atheism belong to the same clusteras theism?
Where is the frequency cutoff point be-tween a productive derivational morpheme and anunproductive one?
Yet, many studies have eval-uated their segmentations and clusters by goingover their results word by word, cluster by clusterand judging by sight whether some segmentationor clustering is good (e.g., Goldsmith (2001)).Like Schone and Jurafsky (2001), we build clus-ters that will have both inflectionally and deriva-tionally related stems and evaluate them with re-spect to a gold standard of only inflectionally re-lated stems.3 Related workThere is a diverse body of existing work on unsu-pervised morphology acquisition.
We summarizeprevious work, emphasizing some of its more ar-bitrary and ad hoc aspects.Letter successor variety.
Letter successor va-riety (LSV) models (Hafer and Weiss, 1974;Gaussier, 1999; Bernhard, 2005; Bordag, 2005;669Keshava and Pitler, 2005; Hammarstro?m, 2006;Dasgupta and Ng, 2007; Demberg, 2007) use thehypothesis that there is less certainty when pre-dicting the next character at morpheme bound-aries.
LSV has several issues that require fine pa-rameter tuning.
For example, Hafer and Weiss(1974) counts how many types of characters ap-pear after some initial string (the successor count)and how many types of characters appear beforesome final string (the predecessor count).
A suc-cessful criterion for segmenting a word was if thepredecessor count for the second part was greaterthan 17 and the successor count for the first partwas greater than 5.
Other studies have similar dataspecific parameters and restrictions.MDL and Bayesian models.
Minimum descrip-tion length (MDL) models (Goldsmith, 2001;Creutz and Lagus, 2002; Creutz and Lagus, 2004;Goldsmith, 2006; Creutz and Lagus, 2007) try tosegment words by maximizing the probability ofa training corpus subject to a penalty based onthe size of hypothesized morpheme lexicons theybuild on the basis of the segmentations.
While the-oretically elegant, a pure implementation on realdata results in descriptions that do not reflect ac-tual morphology.
Creutz and Lagus (2005) re-port that, ?frequent word forms remain unsplit,whereas rare word forms are excessively split.?
Inthe end, every MDL approach uses probabilisti-cally motivated refinements that restrict the ten-dency of raw MDL to generate descriptions thatdo not fit linguistic notions of morphology.
De-spite the sophistication of the models in this group,there are many parameters that need to be set, andheuristic search procedures are crucial for theirsuccess (Goldwater, 2007).
Snover et al (2002)present a Bayesian model that uses a prior distribu-tion to refine disjoint clusters of morphologicallyrelated words.
It disposes with parameter settingby selecting the highest ranking hypothesis.Context aware approaches.
A word?s mor-phology is strongly influenced by its syntactic andsemantic context.
Schone and Jurafsky (2000) at-tempts to cluster morphologically related wordsstarting with an unrefined trie search (but with aparameter of minimum possible stem length andan upper bound on potential affix candidates) thatis constrained by semantic similarity in a wordcontext vector space.
Schone and Jurafsky (2001)builds on this approach, but adds more ad hocparameters to handle circumfixation.
Baroni etal.
(2002) takes a similar approach but uses editdistance to cluster words that are similar but donot necessarily share a long, contiguous substring.They remove noise by constraining cluster mem-bership with mutual information derived semanticsimilarity.
Freitag (2005) uses a mutual informa-tion derived measure to learn the syntactic simi-larity between words and clusters them.
Then hederives finite state machines across words in dif-ferent clusters and refines them through a graphwalk algorithm.
This group is the only one to eval-uate against CELEX (Schone and Jurafsky, 2000;Schone and Jurafsky, 2001; Freitag, 2005).Others.
Some other models require input suchas POS tables and lexicons and use a wider rangeof information about the corpus (Yarowsky andWicentowski, 2000; Yarowsky et al, 2001; Chan,2006).
Because of the knowledge dependence ofthese models, they are able to properly induceinflectional morphology, as opposed to the stud-ies cited above.
Snyder and Barzilay (2008) usesa set of aligned phrases across related languagesto learn how to segment words with a Bayesianmodel and is otherwise fully unsupervised.4 Model2Our goal is to generate conflation sets: sets ofword types that are related through either inflec-tional or derivational morphology (Schone and Ju-rafsky, 2000).
Solving this task requires learninghow individual types are segmented (though thesegmentation itself is not evaluated).
For presentpurposes, we assume that the affixal pattern of thelanguage is known: whether it is prefixal, suffixal,or both.
To simplify presentation, we discuss amodel that captures suffixes only.
Our approach isa four stage process:1.
Candidate Generation: generate candidatestems and affixes using an orthographicallydefined data structure (a trie)2.
Candidate Filtering: filter candidate affixesusing the statistical significance for pairs ofaffixes based on their co-occurence countswith shared stems3.
Affix Clustering: cluster significant affix pairsinto affix groups2The code implementing the model is available fromhttp://comp.ling.utexas.edu/earl6704.
Word Clustering: form conflation sets basedon affix clustersThe first and last stages are particularly prone tonoise, which has necessitated many of the thresh-olds and heuristics employed in previous work.We hypothesize that naturally occuring documentboundaries provide a strong constraint that shouldreduce this noise, and we test that hypothesis byusing it in those stages.Our intuition comes from an observation byYarowsky (1995) regarding multiple tokens ofwords in documents.
He tabulates the applicabil-ity of using document boundaries to disambiguateword senses, which measures how often a givenword occurs more than twice in the same docu-ment.
For ten potentially ambiguous words, hecounts how often they occur more than once insome document and finds that if the words do oc-cur, they do so multiple times in 50.1% of thesedocuments, on average.
His counts ignored mor-phological variation, and it is likely the applica-bility measure would have increased considerably:if a content word is used more than once in sometext, it is likely to be repeated in different syntacticcontexts, requiring the word to be inflected or to bederived for a different part-of-speech category.
3For stage one, we build separate tries for eachdocument rather than a trie for the entire corpus.This should reduce the chance that orthographi-cally similar but morphologically unrelated wordpairs lead to bad candidates by reducing the searchspace for words which share a stem to a local doc-ument.
For example, assuage and assume are bothlikely to occur in a large corpus and suggest thatthere is a stem assu with affixes -age and -me.They are less likely to occur together in many dif-ferent documents that form the corpus, whereasassume, assumed, and assuming are.
We refer tothis document constrained candidate generation asCandGen-D, and to the unconstrained generation(a single trie for all documents) as CandGen-G.For stage four, documents are used to constrainpotential membership of words in clusters: allpairs of words in a cluster must have occured to-gether in some document.
We refer to document-constrained clustering as Clust-D and the uncon-strained global clustering as Clust-G.3For example, in just this one paragraph we have{document,documents}, {measure, measures}, {occur, oc-curs, occuring}, and {word, words}.4.1 Candidate generationGiven a document or collection of documents, weuse tries (prefix trees) to identify potential stemsand affixes and collect statistics for co-occurrencesbetween affixes and between affixes and stems.ab cd $Figure 1A trie G, like the exampleon the right, can be iden-tified with the set of allwords on paths from theroot to any leaf, in the caseof the example figure theset G = {abd, ab$, ac}.
(We use $ to denote anempty affix.)
Given a trieG over alphabet L, we de-fine the set of trunks of Gas all paths from the root to a branching point:Tr(G) = {w ?
L+ |?a, b ?
L, x1, x2?
L?
:a 6= b ?
wax1, wbx2?
G}Also, we define the set of branches of a trunk t ?Tr(G) as the paths from its branching points to theleaves:Br(t,G) = {x ?
L+ | tx ?
G}In our example, {a, ab} are the trunks, withBr(a, G) = {bd, b$, c} and Br(ab, G) = {d, $}.When we use a trie to induce stems and affixes,all induced stems will be trunks, and all inducedaffixes will be branches.From a given trie, we induce a set of stem can-didates and affix candidates.
A simple criterion isused: if a trunk is longer than all of its branches,the trunk is a stem candidate and its branches areaffix candidates.
So, the set of stem candidates fora trie G, CStem(G), is the set of trunks t ?
Tr(G)such that |t| > |b| for all b ?
Br(t,G).Given a stem candidate s ?
CStem(G), its set ofaffix candidates CAff(s,G) is identical to its set ofbranches.
(To talk about the sets of stem and affixcandidates for a whole trie G or a set of tries, wewrite CAff(G), StC(G), CAff, and CStem.)
Thecount of an affix candidate b ?
CAff is the numberof stem candidates with which it occurs:count(b) =?G|{s ?
CStem(G) | b ?
CAff(s,G)}|For Fig.
1, the set of stem candidates is {ab} (sincesome branches of the trunk a are longer than the671trunk itself).
The matching set of affix candidatesis CAff(ab, G) = {d, $}, each with a count of one.An affix rule candidate is an unordered pair ofaffix candidates {b1, b2}.
It states that any stemoccurring with b1can also occur with b2.
Affixrules implement the assumption that all produc-tive affixes will cooccur with other productive af-fixes and that these will form a coherent group.The rule candidates for a given stem candidates ?
CStem(G) are:CRule(s,G) ={{b1, b2} ?
CAff(s,G) | b16= b2}For example, the single stem candidate ab inFig.
1 has one rule candidate, {d, $}.
We also useCRule(G) for the rule candidates of a trie G acrossall stems, and CRule for the union of rule candi-dates in a set of tries.The count of a rule candidate r={b1, b2} in atrie is the number of stem candidates it appearswith:count(r) =?G|{s ?
CStem(G) | r ?
CRule(s,G)}|We also use CAff(s) for the set of affix candidatesof stem s across several tries, and CRule(s) for theset of rule candidates of a stem s across severaltries.Document-specific versus global candidate gen-eration.
CandGen-D defines separate tries forevery document in the corpus and induces stem,affix and rule candidates for each document.CandGen-G instead induces these candidates fora global trie over all the words in the corpus.From the perspective of the formalism laid outabove, the only difference is that CandGen-Dhas as many tries Gias there are documents iand CandGen-G has only one G. This simpledifference leads to different candidate sets andcounts over their occurrences.
For example, saytwo documents contain the pair putt/putts andanother contains bogey/bogeys.
With CandGen-D, count($)=3, count(s)=3, and count($, s)=2.For the same documents, CandGen-G would pro-duce count($)=2 and count(s)=2 since putt/puttswould have occurred only once in the global trie.Also, consider a rare pair such as aard-vark/aardvarks where each word is found in a dif-ferent document.
The pair would be identifiedby CandGen-G but not by CandGen-D.
The pairwould contribute a count of one to count($, s) inCandGen-G but not in CandGen-D.
So, CandGen-G can provide better coverage, but it is also morelikely to identify noisy candidates, such as as-suage/assumed, than CandGen-D.4.2 Candidate filteringThe sets of candidates CStem,CAff,CRule is ex-pected to be noisy since the only basis for gener-ating them was strings that share a large portion oftheir substrings.
One way of filtering candidates isto find affix candidates whose co-occurence withother candidates is not statistically significant.We measure correlation between candidate af-fixes b1, b2in a candidate rule with the paired?2 test.
By using ?2, we only consider pairwisecorrelation between affixes, rather than attemptingglobal inference.
Global consistency of affix setsis not ensured, and as such the approach is sus-ceptible to the multiple comparisons problem.
Westill opt for this approach for its simplicity and be-cause global inference is problematic due to datasparseness.Correlation between b1and b2is determined bythe following contingency table:4b1?
b1b2O11O12?
b2O21O22Based on the significance testing, we define the setof valid rules PairRule as those for which the ?2test is significant at p < 0.05.
Thus, affix can-didates not significantly correlated with any otheraffix in CAff are discarded.4.3 Affix clusteringThe previous stage produces a set of pairs of af-fixes that are significantly correlated.
However,inflectional paradigms rarely contain just two af-fixes, so we would like to group together affixpairs into larger affix sets to improve generaliza-tion.
We use a bottom up, minimum distance clus-tering for valid affix pairs (rules).
We do not as-sume that cluster membership is exclusive.
Forexample, it would not make sense to determinethat the null affix -$ can belong to only one cluster.Therefore, we produce non-disjoint affix clusters.A valid cluster of affixes is a maximal set of af-fixes forming pairwise valid rules: Aff ?
CAff is avalid cluster of affixes iff4where O11= count({b1, b2}), O12= count(b2) ?O11, O21= count(b1)?O11, O22= N?O11?O12?O21and N =Pb?CAff count(b).
See table (1) for examples.672ed ?eding 10273 21853?ing 27120 4119332(a) ?2 = 352678le ?les 122 132945?s 936 4044575(b) ?2 = 239.132ed ?eding 2651 1310?ing 1490 150848(c) ?2 = 65101.6le ?les 20 12073?s 198 144008(d) ?2 = 0.631, p = 0.427Table 1: Affix counts in contingency tables for the valid pair ed/ing and spurious pair le/s according toCandGen-D in (a) and (b) and according to CandGen-G in (c) and (d).
?2 test values are given undereach table.
Data is from NYT.
Total affix token counts induced through CandGen-D and CandGen-Gare N=4178578 and N=156299, respectively.
A total of 2054 and 3739 affix types were induced forCandGen-D and CandGen-G, respectively showing that CandGen-G does have better coverage thoughit might have more noise.1.
?b1, b2?
Aff : {b1, b2} ?
PairRule, and2.
If b ?
CAff with ?b?
?
Aff : {b, b?}
?PairRule, then b ?
Aff.The set of all valid affix clusters is GroupRule.This formulation does not rule out the existenceof clusters with affixes in common.4.4 Word clusteringWe next cluster word forms into morphologicallyrelated groups.
Our model assumes two wordforms to be morphologically related iff (1) they oc-curred in the same trie G, (2) they have a trunk s incommon that is a stem in Stem(G), and (3) their af-fixes under this stem s are members in a commonvalid affix cluster in GroupRule.
Hence a singlestem s can be involved in at most |GroupRule| con-flation sets, one for each valid affix cluster.
Again,the only distinction between clustering with aglobal trie (Clust-G) and clustering with severaltries from the documents in a corpus (Clust-D) isthat the former has only one trie.We define the conflation set for a given stem s ?Stem and valid affix cluster Aff ?
GroupRule asWd(s,Aff) = {sb1, sb2| b1, b2?
Aff ?
?G.s ?
Stem(G) ?
b1, b2?
CAff(s,G)}One issue that needs clarification is when thecandidate generation and clustering stages use dif-ferent strategies, i.e.
the models CandGen-D+Clust-G and CandGen-G +Clust-D.
This sim-ply means that the statistics, and thus the validGroupRule, are derived from either CandGen-D orCandGen-G.4.5 Induction for languages that are bothprefixal and affixalThe above approach would not fit a language thatis prefixal and suffixal.
Assuming we have in-duced separate conflation sets over a prefix trie anda suffix trie, we merge clusters between the two ifthey have at least one word form in common.
For-mally, given a set of prefix conflation sets PCS anda set of suffix conflation sets SCS, the final set ofconflation sets CS is:CS = {p ?
s |p ?
PCS, s ?
SCS ?
p ?
s 6= ?
}5 DataWe apply our method on English and Uspanteko,an endangered Mayan language.Learning corpora.
For English, we use twosubsets of the NYTimes portion in the Gigawordcorpus which we will call NYT and MINI-NYT.NYT in the current study is the complete collec-tion of articles in the New York Times from June,2002.
NYT has 10K articles, 88K types and 9Mtokens.
MINI-NYT is a subset of NYT with 190articles, 15K types and 187K tokens.The Uspanteko text, USP has 29 distinct texts,7K types, and 50K tokens.
The texts are fromOKMA (Pixabaj et al, 2007) and the segmenta-tion and labels of the interlinear glossed text anno-tations were checked for consistency and cleanedup (Palmer et al, 2009).
All counts are for lower-cased, punctuation-removed word forms.CELEX.
The CELEX lexical database (Baayenet al, 1993) has been built for Dutch, English andGerman and provides detailed entries that list andanalyze the morphological properties of words,among other information.
Using CELEX, we eval-uate on types rather than tokens.
The performanceof the model is based on how many of the words itjudges to be morphologically related overlap withthe entries in CELEX.
Following previous work(Schone and Jurafsky, 2000; Schone and Jurafsky,6732001; Freitag, 2005), we evaluate on inflectionalclusters only, using the CELEX file listing clustersof inflectional variants.
56 Experiments and evaluationWe outline our evaluation methodology, baselines,benchmarks and results, and discuss the results.6.1 Evaluation metricSchone and Jurafsky (2000) give definitions forcorrect (C), inserted (I), and deleted (D) wordsin model-derived conflation sets in relation to agold standard.
Their formulation does not allowfor multiple cluster membership of words.
We ex-tend the definition to incorporate this fact about thedata.
Let w be a word form.
We write Xwfor theclusters induced by the model that contain w, andYwfor gold standard clusters containing w. Xwand Ywonly count words which occurred in bothmodel and gold standard clusters.
ThenC =?w?Xw?Yw(|Xw?
Yw|/|Yw|)I =?w?Xw?Yw(|Xw?
(Xw?
Yw)|/|Yw|)D =?w?Xw?Yw(|Yw?
(Xw?
Yw)|/|Yw|)Based on these definitions, we formulate preci-sion (P ), recall (R), and the f -score (F ) as: P =C/(C+I), R = C/(C+D), F = (2PR)/(P+R).USP evaluation We use two different means toevaluate the performance on USP.
One is thef -score derived from the above section with re-spect to a standard that was automatically gen-erated from the morpheme segment tiers of theOKMA IGT.
We generated the standard by takingnon-hyphenated segments as the stem and cluster-ing words with shared stems.We also had an expert in Uspanteko manuallyevaluate a random subset (N = 100) of the modeloutput to compensate for any failings in the stan-dard.
The evaluator determined a dominant stemfor a cluster and identified words which were notrelated to that stem.
We measured accuracy and5CELEX does have a second file listing words and theirbreakup into constituent morphemes for both derivation andinflection, but its use would have required additional process-ing that could introduce errors.0 10 20 30 40 50 60 70 80 90 100405060708090100precisionrecallmini?NYTNYTUsp?SUsp?PFigure 2: Precision/recall graph for baseline ex-periments on English, prefix USP (Usp-P) and suf-fix USP (Usp-S).full cluster accuracy6 for the expert evaluations(table 4).We experimented on Uspanteko with three dif-ferent assumptions: (1) it is only prefixal; (2) it isonly suffixal; (3) it is both prefixal and suffixal.We applied the assumptions of only prefixal oronly suffixal to LINGUISTICA as well.
The rele-vant results are given row headers in tables with acorresponding +P(prefix) or +S(suffix).6.2 Baselines and benchmarksIn a set of baselines, we put words which sharethe first k characters into the same cluster.
Wedo this for NYT, MINI-NYT, and USP in a pre-fix tree, and for USP in suffix tree (using the last kcharacters).
We set the values of 0 < k < max,where max is the length of the longest string, andplot the results in a precision-recall graph (Fig.
2).Low k corresponds to high recall and low preci-sion while high k shows the opposite.
The contrastin morphological patterns for each language canalso be seen.
Because Uspanteko is morpholog-ically complex with suffixes and prefixes, a verysimple strategy cannot achieve high recall as op-posed to English where it is possible to retrieve allvariants with a simple prefix tree.We use Linguistica (Goldsmith, 2001) and Mor-fessor (Creutz and Lagus, 2007) as benchmarks.We used the default settings for these programs.Note that comparison with these tools is not com-6Given a model cluster Ciand the ?misses?
for each clus-ter Mi, accuracy is measured as 1/NPi(|Ci|?|Mi|)/(|Ci|)where N is the sample size.
Full cluster accuracy is the num-ber of clusters that did not have any misses over N .674MINI-NYT NYTP R F P R FLINGUISTICA 64.30 93.34 76.15 47.50 88.33 61.77MORFESSOR 45.2 87.8 59.7 63.6 69.2 66.3CandGen-D + Clust-G 69.41 91.42 78.91 46.00 79.81 58.36CandGen-D + Clust-D 83.47 80.36 81.89 59.02 74.50 65.86CandGen-G + Clust-G 73.44 88.72 80.36 61.81 82.98 70.85CandGen-G + Clust-D 88.34 77.95 82.82 77.71 70.24 73.79Table 2: Results on English for all models in precision(P), recall(R), f -score(F) for each data set.pletely fair.
Morfessor only generates segmenta-tions.
We therefore processed Morfessor outputby clustering words by assuming that the longestsegment in any segmentation is the stem and eval-uated this instead.
Linguistica produces stems andassociated suffixes so the clusters naturally followfrom this output.
However, Linguistica only inferseither prefix or suffix patterns.6.3 Results and discussionThe results on English are in table 2 with ?2 testcriteria of p<0.05 and each cell in the contingencytable >5.
CandGen-G +Clust-D had the best f -score, and easily beats the benchmarks.This is different from our expectation thatawareness of document boundaries at all stages(i.e., CandGen-D +Clust-D) would show the bestresults.
The discrepancy is especially marked forthe larger NYT.
One important reason for this isthe affix criterion itself: trunks must be longer thanbranches.
Consider again the sample contingencytables in Table 1 that were derived from NYTthrough CandGen-D and CandGen-G. We had as-sumed at the outset that CandGen-D would be bet-ter able to filter out noise and would be sparser, butresults show the opposite.
The reason is that thatshort words in a global lexicon are more likely toshare trunks with longer, unrelated words.
Thisensures that short word forms rarely generate can-didate affixes.
Longer words which are less likelyto have spurious long branches generate the bulkof candidate suffixes and stems.
This is born outby the stems that were associated with the spuri-ous suffix pair le/s: CandGen-G has cliente, cripp,crumb, daniel, ender, label, mccord, nag, oval,sear, stubb, whipp.
CandGen-D has crumb, hand,need, sing, tab, trick, trip.
The word forms thatare associated with le/s through the CandGen-Dstrategy are crumble/crumbs, handle/hands, .
.
.
.Compare this with the word forms associated withthe search strategy CandGen-G such as clien-tele/clientes, cripple/crips, .
.
.
.
The majority ofthem are not common English words; they aremost probably proper names such as LaBelle andSearle.
Furthermore, there is no item among thestems from the CandGen-G search where concate-nating the stems le and s would result in both wordforms being a common noun or verb as is thecase with the stems from the CandGen-D searchwhere all concatenated word forms are commonEnglish words.
Though CandGen-G finds spuri-ous stems, the counts for the spurious affix pair aresuppressed (see table 1) because it is a type countrather than a token count.
This results in le/s be-ing properly excluded as a rule.
This explains whyCandGen-D has worse precision in general thanCandGen-G.The affix criterion has other minor issues.
Oneis that it ignores the few cases where stems areshorter than affixes, such as the very commonwords be, do, go.7 Assuming that the longestproductive inflectional suffix in English is -ing8,the criterion would correctly find stem candidatesfor -ing only when the stem is longer than 3 or4 letters.
Another is that the criterion, whencombined with CandGen-D, generates candidatesfrom the/them/then/their/these which cooccur fre-quently in documents.
This is not an issue whenthe criterion is applied in CandGen-G.Nonetheless, results show that when data sizesare small, as with USP (Table 3) and MINI-NYT,awareness of document boundaries at the candi-date generation stage is beneficial to precision.7The exclusion of such words in a token based evaluationas opposed to a type based evaluation would heavily penalizeour approach.
We are not aware, however, of any prior workin unsupervised morphology that evaluates over tokens.8with occasional gemination of final consonant such asoccur ?
occurring675P R FCa-D + Cl-D 70.51 44.35 54.45Ca-G + Cl-G 70.00 46.87 56.15Ca-D + Cl-D + S 88.58 45.21 59.86Ca-D + Cl-G + S 85.03 44.75 58.64Ca-G + Cl-D + S 90.34 45.48 60.50Ca-G + Cl-G + S 84.54 46.03 59.60Ca-D + Cl-D + P 93.84 47.90 63.42Ca-D + Cl-G + P 89.94 47.38 62.06Ca-G + Cl-D + P 95.42 47.89 63.78Ca-G + Cl-G + P 92.03 50.01 64.80LINGUISTICA + S 81.14 47.60 60.00LINGUISTICA + P 84.15 52.00 64.28MORFESSOR 28.12 62.28 38.75Table 3: Performance of models on automaticallygenerated USP evaluation set.
P: Prefix only, S:Suffix only.
If there is no indication of S or P, itmeans model attempted to learn bothAcc.
FAcc.
Avg.
Sz.Ca-G + Cl-G 98.5 79.0 2.94LINGUISTICA 96.0 85.0 2.64MORFESSOR 85.3 55.0 4.8Table 4: Human expert evaluated accuracy (Acc.
)and full cluster accuracy (FAcc.)
of models onUSP and average cluster size in words (Avg.
Sz.
)However, it seems that CandGen-G has better cov-erage no matter the size of the corpus, whichexplains why coupling it with Clust-D producesoverall better scores.
Clust-D does provide a use-ful added constraint to mere orthographic similar-ity (i.e.
shared trunks in a trie).A worrisome aspect of the results is that perfor-mance degrades for large data sets (this is also truefor Linguistica).
However, it also hints that thismethod might work well for under-resourced lan-guages.
We surmise that since productive suffixesdo not suffer from sparsity, even a small data setprovides sufficient evidence to reach reliable con-clusions about the productive morphology of somelanguage.
Increasing the size of the data merelyincreases the counts of spurious affixes and posesproblems for a relative simple measure such asthe ?2 test.
A similar result was shown in Creutzand Lagus (2005) where f -score performance oftheir segmentation method improved as more datawas provided then decreased as the input exceeded250K tokens in English.
Their method showedcontinued improvement with increased data forFinnish.
This hints that more data is beneficialfor morphologically complex languages but notfor morphologically impoverished languages.Finally, it is also encouraging that the manualevaluation (Table 4) shows very high accuracy, asjudged by a documentary linguist.
Both our modeland Linguistica perform very well under this eval-uation.7 ConclusionWe have presented a novel approach to unsuper-vised morphology acquisition that uses a verysimple pipeline and does not use any thresholdsother than standard ones associated with the ?2test.
The model relies on document boundariesand correlation tests for filtering spurious stemsand affixes.
The model compares favorably toLinguistica and Morfessor, two models that em-ploy much more complex strategies and rely onfine-tuned parameters.
We found that the use ofdocument boundaries is especially beneficial withsmall datasets, which is promising for the applica-tion of this model to under-resourced languages.For large datasets, global candidate generationoutperformed document-aware candidate genera-tion at the task of filtering out spurious stems,but document-aware clustering does improve pre-cision and overall performance.In this paper we have addressed one aspect ofmorphology acquisition, segmentation and clus-tering.
Extending the approach is straightforward,for example, substituting more sophisticated datastructures or statistical tests for the current ones.In particular, we will move from the use of doc-ument boundaries to a flexible notion of textualdistance to estimate likelihood of morphologicalrelatedness.AcknowledgmentsThis work is funded by NSF grant BCS 06651988?Reducing Annotation Effort in the Documenta-tion of Languages using Machine Learning andActive Learning.?
Thanks to Alexis Palmer, TelmaKaan Pixabaj, Elias Ponvert, and the anonymousreviewers.676ReferencesR.
H. Baayen, R. Piepenbrock, and H. van Rijn.
1993.The CELEX lexical database on CD-ROM.
Linguis-tic Data Consortium, Philadelphia, PA.M.
Baroni, J. Matiasek, and H. Trost.
2002.
Unsu-pervised discovery of morphologically related wordsbased on orthographic and semantic similarity.
InACL ?02 workshop on Morphological and phonolog-ical learning, pages 48?57.D.
Bernhard.
2005.
Unsupervised morphological seg-mentation based on segment predictability and wordsegments alignment.
In Proceedings of MorphoChallenge 2005, pages 18?27.S.
Bordag.
2005.
Two-step approach to unsupervisedmorpheme segmentation.
In Proceedings of MorphoChallenge 2005, pages 23?27.E.
Chan.
2006.
Learning Probabilistic Paradigms forMorphology in a Latent Class Model.
In ACL SIG-PHON ?06, pages 69?78.M.
Creutz and K. Lagus.
2002.
Unsupervised dis-covery of morphemes.
In ACL ?02 workshop onMorphological and phonological learning-Volume6, pages 21?30.M.
Creutz and K. Lagus.
2004.
Induction of a simplemorphology for highly-inflecting languages.
In ACLSIGPHON ?04, pages 43?51.M.
Creutz and K. Lagus.
2005.
Inducing the morpho-logical lexicon of a natural language from unanno-tated text.
In AKRR ?05, pages 106?113.M.
Creutz and K. Lagus.
2007.
Unsupervised modelsfor morpheme segmentation and morphology learn-ing.
ACM Trans.
Speech Lang.
Process., 4(1):3.S.
Dasgupta and V. Ng.
2007.
High-performance,language-independent morphological segmentation.In NAACL-HLT, pages 155?163.V.
Demberg.
2007.
A language-independent unsu-pervised model for morphological segmentation.
InACL ?07, volume 45, page 920.D.
Freitag.
2005.
Morphology induction from termclusters.
In CoNLL ?05.E.
Gaussier.
1999.
Unsupervised learning of deriva-tional morphology from inflectional lexicons.
InACL workshop on Unsupervised Methods in Natu-ral Language Learning.J.
Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
Comp.
Ling.,27(2):153?198.J.
Goldsmith.
2006.
An algorithm for the unsupervisedlearning of morphology.
Natural Language Engi-neering, 12(04):353?371.S.J.
Goldwater.
2007.
Nonparametric Bayesian mod-els of lexical acquisition.
Ph.D. thesis, Brown Uni-versity.M.A.
Hafer and S.F.
Weiss.
1974.
Word Segmentationby Letter Successor Varieties.
Information Storageand Retrieval, 10:371?385.H.
Hammarstro?m.
2006.
A naive theory of affixationand an algorithm for extraction.
In ACL SIGPHON?06, pages 79?88, June.S.
Keshava and E. Pitler.
2005.
A simpler, intuitiveapproach to morpheme induction.
In Proceedings ofMorpho Challenge 2005, pages 28?32.C.
Monson.
2004.
A framework for unsupervised nat-ural language morphology induction.
In Proceed-ings of the Student Workshop at ACL, volume 4.Alexis Palmer, Taesun Moon, and Jason Baldridge.2009.
Evaluating automation strategies in languagedocumentation.
In Proceedings of the NAACL HLT2009 Workshop on Active Learning for Natural Lan-guage Processing, pages 36?44, Boulder, CO.T.C.
Pixabaj, M.A.
Vicente Me?ndez, M. VicenteMe?ndez, and O.A.
Damia?n.
2007.
Text collectionsin Four Mayan Languages.
Archived in The Archiveof the Indigenous Languages of Latin America.P.
Schone and D. Jurafsky.
2000.
Knowledge-free in-duction of morphology using latent sematic analysis.In CoNLL-2000 and LLL-2000.P.
Schone and D. Jurafsky.
2001.
Knowledge-freeinduction of inflectional morphologies.
In NAACL?01, pages 1?9.M.G.
Snover, G.E.
Jarosz, and M.R.
Brent.
2002.
Un-supervised learning of morphology using a novel di-rected search algorithm: taking the first step.
In ACL?02 workshop on Morphological and phonologicallearning, pages 11?20.B.
Snyder and R. Barzilay.
2008.
Unsupervised multi-lingual learning for morphological segmentation.
InACL ?08.D.
Yarowsky and R. Wicentowski.
2000.
Minimallysupervised morphological analysis by multimodalalignment.
In ACL ?00, pages 207?216.D.
Yarowsky, G. Ngai, and R. Wicentowski.
2001.Inducing multilingual text analysis tools via robustprojection across aligned corpora.
In HLT ?01.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In ACL ?95,pages 189?196.677
