OntoLearn Reloaded: A Graph-BasedAlgorithm for Taxonomy InductionPaola Velardi?Sapienza University of RomeStefano Faralli?Sapienza University of RomeRoberto Navigli?Sapienza University of RomeIn 2004 we published in this journal an article describing OntoLearn, one of the first systemsto automatically induce a taxonomy from documents and Web sites.
Since then, OntoLearn hascontinued to be an active area of research in our group and has become a reference work withinthe community.
In this paper we describe our next-generation taxonomy learning methodol-ogy, which we name OntoLearn Reloaded.
Unlike many taxonomy learning approaches in theliterature, our novel algorithm learns both concepts and relations entirely from scratch via theautomated extraction of terms, definitions, and hypernyms.
This results in a very dense, cyclicand potentially disconnected hypernym graph.
The algorithm then induces a taxonomy fromthis graph via optimal branching and a novel weighting policy.
Our experiments show that weobtain high-quality results, both when building brand-new taxonomies and when reconstructingsub-hierarchies of existing taxonomies.1.
IntroductionOntologies have proven useful for different applications, such as heterogeneous dataintegration, information search and retrieval, question answering, and, in general, forfostering interoperability between systems.
Ontologies can be classified into three maintypes (Sowa 2000), namely: i) formal ontologies, that is, conceptualizations whose cat-egories are distinguished by axioms and formal definitions, stated in logic to supportcomplex inferences and computations; ii) prototype-based ontologies, which are basedon typical instances or prototypes rather than axioms and definitions in logic; iii) lexical-ized (or terminological) ontologies, which are specified by subtype-supertype relationsand describe concepts by labels or synonyms rather than by prototypical instances.Here we focus on lexicalized ontologies because, in order to enable naturallanguage applications such as semantically enhanced information retrieval and ques-tion answering, we need a clear connection between our formal representation of the?
Dipartimento di Informatica, Sapienza Universita` di Roma, Via Salaria, 113, 00198 Roma Italy.E-mail: {velardi,faralli,navigli}@di.uniroma1.it.Submission received: 17 December 2011; revised submission received: 28 July 2012; accepted for publication:10 October 2012.doi:10.1162/COLI a 00146?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 3domain and the language used to express domain meanings within text.
And, in turn,this connection can be established by producing full-fledged lexicalized ontologies forthe domain of interest.
Manually constructing ontologies is a very demanding task,however, requiring a large amount of time and effort, even when principled solutionsare used (De Nicola, Missikoff, and Navigli 2009).
A quite recent challenge, referredto as ontology learning, consists of automatically or semi-automatically creating alexicalized ontology using textual data from corpora or the Web (Gomez-Perez andManzano-Mancho 2003; Biemann 2005; Maedche and Staab 2009; Petasis et al2011).
Asa result of ontology learning, the heavy requirements of manual ontology constructioncan be drastically reduced.In this paper we deal with the problem of learning a taxonomy (i.e., the backboneof an ontology) entirely from scratch.
Very few systems in the literature address thistask.
OntoLearn (Navigli and Velardi 2004) was one of the earliest contributions in thisarea.
In OntoLearn taxonomy learning was accomplished in four steps: terminologyextraction, derivation of term sub-trees via string inclusion, disambiguation of domainterms using a novel Word Sense Disambiguation algorithm, and combining the sub-trees into a taxonomy.
The use of a static, general-purpose repository of semanticknowledge, namely, WordNet (Miller et al1990; Fellbaum 1998), prevented the systemfrom learning taxonomies in technical domains, however.In this paper we present OntoLearn Reloaded, a graph-based algorithm for learninga taxonomy from the ground up.
OntoLearn Reloaded preserves the initial step ofour 2004 pioneering work (Navigli and Velardi 2004), that is, automated terminologyextraction from a domain corpus, but it drops the requirement for WordNet (therebyavoiding dependence on the English language).
It also drops the term compositionalityassumption that previously led to us having to use a Word Sense Disambiguationalgorithm?namely, SSI (Navigli and Velardi 2005)?to structure the taxonomy.
Instead,we now exploit textual definitions, extracted from a corpus and the Web in an iterativefashion, to automatically create a highly dense, cyclic, potentially disconnected hyper-nym graph.
An optimal branching algorithm is then used to induce a full-fledged tree-like taxonomy.
Further graph-based processing augments the taxonomy with additionalhypernyms, thus producing a Directed Acyclic Graph (DAG).Our system provides a considerable advancement over the state of the art intaxonomy learning: First, excepting for the manual selection of just a few upper nodes, thisis the first algorithm that has been experimentally shown to build fromscratch a new taxonomy (i.e., both concepts and hypernym relations)for arbitrary domains, including very technical ones for whichgold-standard taxonomies do not exist. Second, we tackle the problem with no simplifying assumptions: We copewith issues such as term ambiguity, complexity of hypernymy patterns,and multiple hypernyms. Third, we propose a novel algorithm to extract an optimal branchingfrom the resulting hypernym graph, which?after some recoverysteps?becomes our final taxonomy.
Taxonomy induction is themain theoretical contribution of the paper. Fourth, the evaluation is not limited, as it is in most papers, to the numberof retrieved hypernymy relations that are found in a reference taxonomy.666Velardi, Faralli, and Navigli OntoLearn ReloadedInstead, we also analyze the extracted taxonomy in its entirety;furthermore, we acquire two ?brand new?
taxonomies in thedomains of ARTIFICIAL INTELLIGENCE and FINANCE. Finally, our taxonomy-building workflow is fully implemented andthe software components are either freely available from our Website,1 or reproducible.In this paper we extend our recent work on the topic (Navigli, Velardi, and Faralli2011) as follows: i) we describe in full detail the taxonomy induction algorithm; ii) weenhance our methodology with a final step aimed at creating a DAG, rather than a stricttree-like taxonomical structure; iii) we perform a large-scale multi-faceted evaluationof the taxonomy learning algorithm on six domains; and iv) we contribute a novelmethodology for evaluating an automatically learned taxonomy against a referencegold standard.In Section 2 we illustrate the related work.
We then describe our taxonomy-induction algorithm in Section 3.
In Section 4 we present our experiments, and discussthe results.
Evaluation is both qualitative (on new ARTIFICIAL INTELLIGENCE andFINANCE taxonomies), and quantitative (on WordNet and MeSH sub-hierarchies).
Sec-tion 5 is dedicated to concluding remarks.2.
Related WorkTwo main approaches are used to learn an ontology from text: rule-based and distri-butional approaches.
Rule-based approaches use predefined rules or heuristic patternsto extract terms and relations.
These approaches are typically based on lexico-syntacticpatterns, first introduced by Hearst (1992).
Instances of relations are harvested from textby applying patterns aimed at capturing a certain type of relation (e.g., X is a kind of Y).Such lexico-syntactic patterns can be defined manually (Berland and Charniak 1999;Kozareva, Riloff, and Hovy 2008) or obtained by means of bootstrapping techniques(Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006).
In the latter case,a number of term pairs in the wanted relation are manually picked and the relation issought within text corpora or the Web.
Other rule-based approaches learn a taxonomyby applying heuristics to collaborative resources such as Wikipedia (Suchanek, Kasneci,and Weikum 2008; Ponzetto and Strube 2011), also with the supportive aid of computa-tional lexicons such as WordNet (Ponzetto and Navigli 2009).Distributional approaches, instead, model ontology learning as a clustering orclassification task, and draw primarily on the notions of distributional similarity (Padoand Lapata 2007; Cohen and Widdows 2009), clustering of formalized statements (Poonand Domingos 2010), or hierarchical random graphs (Fountain and Lapata 2012).
Suchapproaches are based on the assumption that paradigmatically-related concepts2 appearin similar contexts and their main advantage is that they are able to discover relationsthat do not explicitly appear in the text.
They are typically less accurate, however, andthe selection of feature types, notion of context, and similarity metrics vary considerablydepending on the specific approach used.1 http://lcl.uniroma1.it/ontolearn reloaded and http://ontolearn.org.2 Because we are concerned with lexical taxonomies, in this paper we use the words concepts and termsinterchangeably.667Computational Linguistics Volume 39, Number 3Recently, Yang and Callan (2009) presented a semi-supervised taxonomy induc-tion framework that integrates contextual, co-occurrence, and syntactic dependencies,lexico-syntactic patterns, and other features to learn an ontology metric, calculatedin terms of the semantic distance for each pair of terms in a taxonomy.
Terms areincrementally clustered on the basis of their ontology metric scores.
In their work, theauthors assume that the set of ontological concepts C is known, therefore taxonomylearning is limited to finding relations between given pairs in C. In the experiments,they only use the word senses within a particular WordNet sub-hierarchy so as to avoidany lexical ambiguity.
Their best experiment obtains a 0.85 precision rate and 0.32 recallrate in replicating is-a links on 12 focused WordNet sub-hierarchies, such as PEOPLE,BUILDING, PLACE, MILK, MEAL, and so on.Snow, Jurafsky, and Ng (2006) propose the incremental construction of taxonomiesusing a probabilistic model.
In their work they combine evidence from multiplesupervised classifiers trained on very large training data sets of hyponymy and cousinrelations.
Given the body of evidence obtained from all the relevant word pairs ina lexico-syntactic relation, the taxonomy learning task is defined probabilistically asthe problem of finding the taxonomy that maximizes the probability of having thatevidence (a supervised logistic regression model is used for this).
Rather than learninga new taxonomy from scratch, however, this approach aims at attaching new conceptsunder the appropriate nodes of an existing taxonomy (i.e., WordNet).
The approach isevaluated by manually assessing the quality of the single hypernymy edges connectingleaf concepts to existing ones in WordNet, with no evaluation of a full-fledged struc-tured taxonomy and no restriction to a specific domain.
A related, weakly supervisedapproach aimed at categorizing named entities, and attaching them to WordNet leaves,was proposed by Pasca (2004).
Other approaches use formal concept analysis (Cimiano,Hotho, and Staab 2005), probabilistic and information-theoretic measures to learn tax-onomies from a folksonomy (Tang et al2009), and Markov logic networks and syntacticparsing applied to domain text (Poon and Domingos 2010).The work closest to ours is that presented by Kozareva and Hovy (2010).
From aninitial given set of root concepts and basic level terms, the authors first use Hearst-likelexico-syntactic patterns iteratively to harvest new terms from the Web.
As a result aset of hyponym?hypernym relations is obtained.
Next, in order to induce taxonomicrelations between intermediate concepts, the Web is searched again with surface pat-terns.
Finally, nodes from the resulting graph are removed if the out-degree is belowa threshold, and edges are pruned by removing cycles and selecting the longest pathin the case of multiple paths between concept pairs.
Kozareva and Hovy?s method hassome limitations, which we discuss later in this paper.
Here we note that, in evalu-ating their methodology, the authors discard any retrieved nodes not belonging to aWordNet sub-hierarchy (they experiment on PLANTS, VEHICLES, and ANIMALS), thusit all comes down to Yang and Callan?s (2009) experiment of finding relations between apre-assigned set of nodes.In practice, none of the algorithms described in the literature was actually appliedto the task of creating a new taxonomy for an arbitrary domain of interest truly fromscratch.
Instead, what is typically measured is the ability of a system to reproduce asfar as possible the relations of an already existing taxonomy (a common test is WordNetor the Open Directory Project3), when given the set of domain concepts.
Evaluatingagainst a gold standard is, indeed, a reasonable validation methodology.
The claim to be3 http://www.dmoz.org/.668Velardi, Faralli, and Navigli OntoLearn ReloadedFigure 1The OntoLearn Reloaded taxonomy learning workflow.
?automatically building?
a taxonomy needs also to be demonstrated on new domainsfor which no a priori knowledge is available, however.
In an unknown domain, tax-onomy induction requires the solution of several further problems, such as identifyingdomain-appropriate concepts, extracting appropriate hypernym relations, and detect-ing lexical ambiguity, whereas some of these problems can be ignored when evaluatingagainst a gold standard (we will return to this issue in detail in Section 4).
In fact,the predecessor of OntoLearn Reloaded, that is, OntoLearn (Navigli and Velardi 2004),suffers from a similar problem, in that it relies on the WordNet taxonomy to establishparadigmatic connections between concepts.3.
The Taxonomy Learning WorkflowOntoLearn Reloaded starts from an initially empty directed graph and a corpus for thedomain of interest (e.g., an archive of artificial intelligence papers).
We also assumethat a small set of upper terms (entity, abstraction, etc.
), which we take as the endpoints of our algorithm, has been manually defined (e.g., from a general purpose taxon-omy like WordNet) or is available for the domain.4 Our taxonomy-learning workflow,summarized in Figure 1, consists of five steps:1.
Initial Terminology Extraction (Section 3.1): The first step applies a termextraction algorithm to the input domain corpus in order to produce aninitial domain terminology as output.2.
Definition & Hypernym Extraction (Section 3.2): Candidate definitionsentences are then sought for the extracted domain terminology.
For eachterm t, a domain-independent classifier is used to select well-formeddefinitions from the candidate sentences and extract the correspondinghypernyms of t.4 Although very few domain taxonomies are available, upper (core) concepts have been defined in severaldomains, such as MEDICINE, ART, ECONOMY, and so forth.669Computational Linguistics Volume 39, Number 33.
Domain Filtering (Section 3.3): A domain filtering technique is appliedto filter out those definitions that do not pertain to the domain of interest.The resulting domain definitions are used to populate the directed graphwith hypernymy relations connecting t to the extracted hypernym h.Steps (2) and (3) are then iterated on the newly acquired hypernyms,until a termination condition occurs.4.
Graph Pruning (Section 3.4): As a result of the iterative phase we obtaina dense hypernym graph that potentially contains cycles and multiplehypernyms for most nodes.
In this step we combine a novel weightingstrategy with the Chu-Liu/Edmonds algorithm (Chu and Liu 1965;Edmonds 1967) to produce an optimal branching (i.e., a tree-liketaxonomy) of the initial noisy graph.5.
Edge Recovery (Section 3.5): Finally, we optionally apply a recoverystrategy to reattach some of the hypernym edges deleted during theprevious step, so as to produce a full-fledged taxonomy in the formof a DAG.We now describe in full detail the five steps of OntoLearn Reloaded.53.1 Initial Terminology ExtractionDomain terms are the building blocks of a taxonomy.
Even though in many cases aninitial domain terminology is available, new terms emerge continuously, especiallyin novel or scientific domains.
Therefore, in this work we aim at fully automatizingthe taxonomy induction process.
Thus, we start from a text corpus for the domainof interest and extract domain terms from the corpus by means of a terminologyextraction algorithm.
For this we use our term extraction tool, TermExtractor,6 thatimplements measures of domain consensus and relevance to harvest the most relevantterms for the domain from the input corpus.7 As a result, an initial domain terminol-ogy T(0) is produced that includes both single- and multi-word expressions (such as,respectively, graph and flow network).
We add one node to our initially empty graphGnoisy = (Vnoisy, Enoisy) for each term in T(0)?that is, we set Vnoisy := T(0) and Enoisy := ?.In Table 1 we show an excerpt of our ARTIFICIAL INTELLIGENCE and FINANCEterminologies (cf.
Section 4 for more details).
Note that our initial set of domain terms(and, consequently, nodes) will be enriched with the new hypernyms acquired duringthe subsequent iterative phase, described in the next section.3.2 Definition and Hypernym ExtractionThe aim of our taxonomy induction algorithm is to learn a hypernym graph by means ofseveral iterations, starting from T(0) and stopping at very general terms U, that we takeas the end point of our algorithm.
The upper terms are chosen from WordNet topmost5 A video of the first four steps of OntoLearn Reloaded is available athttp://www.youtube.com/watch?v=-k3cOEoI Dk.6 http://lcl.uniroma1.it/termextractor.7 TermExtractor has already been described in Sclano and Velardi (2007) and in Navigli and Velardi (2004);therefore the interested reader is referred to these papers for additional details.670Velardi, Faralli, and Navigli OntoLearn ReloadedTable 1An excerpt of the terminology extracted for the ARTIFICIAL INTELLIGENCE and FINANCEdomains.ARTIFICIAL INTELLIGENCEacyclic graph parallel corpus flow networkadjacency matrix parse tree pattern matchingartificial intelligence partitioned semantic network pageranktree data structure pathfinder taxonomic hierarchyFINANCEinvestor shareholder open economybid-ask spread profit maximization speculationlong term debt shadow price risk managementoptimal financing policy ratings profit marginsynsets.
In other words, U contains all the terms in the selected topmost synsets.
InTable 2 we show representative synonyms of the upper-level synsets that we used forthe ARTIFICIAL INTELLIGENCE and FINANCE domains.
Seeing that we use high-levelconcepts, the set U can be considered domain-independent.
Other choices are of coursepossible, especially if an upper ontology for a given domain is already available.For each term t ?
T(i) (initially, i = 0), we first check whether t is an upper term (i.e.,t ?
U).
If it is, we just skip it (because we do not aim at extending the taxonomy beyondan upper term).
Otherwise, definition sentences are sought for t in the domain corpusand in a portion of the Web.
To do so we use Word-Class Lattices (WCLs) (Navigli andVelardi 2010, introduced hereafter), which is a domain-independent machine-learnedclassifier that identifies definition sentences for the given term t, together with thecorresponding hypernym (i.e., lexical generalization) in each sentence.For each term in our set T(i), we then automatically extract definition candidatesfrom the domain corpus, Web documents, and Web glossaries, by harvesting all thesentences that contain t. To obtain on-line glossaries we use a Web glossary extractionsystem (Velardi, Navigli, and D?Amadio 2008).
Definitions can also be obtained via alightweight bootstrapping process (De Benedictis, Faralli, Navigli 2013).Finally, we apply WCLs and collect all those sentences that are classified as defini-tional.
We show some terms with their definitions in Table 3 (first and second column,respectively).
The extracted hypernym is shown in italics.Table 2The set of upper concepts used in OntoLearn Reloaded for AI and FINANCE (only representativesynonyms from the corresponding WordNet synsets are shown).ability#n#1 abstraction#n#6 act#n#2 code#n#2communication#n#2 concept#n#1 data#n#1 device#n#1discipline#n#1 entity#n#1 event#n#1 expression#n#6research#n#1 instrumentality#n#1 knowledge#n#1 knowledge domain#n#1language#n#1 methodology#n#2 model#n#1 organization#n#1person#n#1 phenomenon#n#1 process#n#1 property#n#2quality#n#1 quantity#n#1 relation#n#1 representation#n#2science#n#1 system#n#2 technique#n#1 theory#n#1671Computational Linguistics Volume 39, Number 3Table 3Some definitions for the ARTIFICIAL INTELLIGENCE domain (defined term in bold, extractedhypernym in italics).Term Definition Weight Domain?adjacency matrix an adjacency matrix is a zero-one matrix 1.00 flow network in graph theory, a flow network is a directed graph 0.57 flow network global cash flow network is an online company thatspecializes in education and training courses inteaching the entrepreneurship0.14 ?Table 4Example definitions (defined terms are marked in bold face, their hypernyms in italics).
[In arts, a chiaroscuro]DF [is]VF [a monochrome picture]GF.
[In mathematics, a graph]DF [is]VF [a data structure]GF [that consists of .
.
.
]REST.
[In computer science, a pixel]DF [is]VF [a dot]GF [that is part of a computer image]REST.
[Myrtales]DF [are an order of]VF [ flowering plants]GF [placed as a basal group .
.
.
]REST.3.2.1 Word-Class Lattices.
We now describe our WCL algorithm for the classification ofdefinitional sentences and hypernym extraction.
Our model is based on a formal notionof textual definition.
Specifically, we assume a definition contains the following fields(Storrer and Wellinghoff 2006): The DEFINIENDUM field (DF): this part of the definition includes thedefiniendum (that is, the word being defined) and its modifiers(e.g., ?In computer science, a pixel?
); The DEFINITOR field (VF): which includes the verb phrase used tointroduce the definition (e.g., ?is?
); The DEFINIENS field (GF): which includes the genus phrase (usuallyincluding the hypernym, e.g., ?a dot?
); The REST field (RF): which includes additional clauses that furtherspecify the differentia of the definiendum with respect to its genus(e.g., ?that is part of a computer image?
).To train our definition extraction algorithm, a data set of textual definitions wasmanually annotated with these fields, as shown in Table 4.8 Furthermore, the single-or multi-word expression denoting the hypernym was also tagged.
In Table 4, for eachsentence the definiendum and its hypernym are marked in bold and italics, respectively.Unlike other work in the literature dealing with definition extraction (Hovy et al2003;Fahmi and Bouma 2006; Westerhout 2009; Zhang and Jiang 2009), we covered not onlya variety of definition styles in our training set, in addition to the classic X is a Y pattern,but also a variety of domains.
Therefore, our WCL algorithm requires no re-trainingwhen changing the application domain, as experimentally demonstrated by Navigli andVelardi (2010).
Table 5 shows some non-trivial patterns for the VF field.8 Available on-line at: http://lcl.uniroma1.it/wcl.672Velardi, Faralli, and Navigli OntoLearn ReloadedTable 5Some nontrivial patterns for the VF field.is a term used to describe is a specialized form ofis the genus of was coined to describeis a term that refers to a kind of is a special class ofcan denote is the extension of the concept ofis commonly used to refer to is defined both asStarting from the training set, the WCL algorithm learns generalized definitionalmodels as detailed hereafter.Generalized sentences.
First, training and test sentences are part-of-speech tagged with theTreeTagger system, a part-of-speech tagger available for many languages (Schmid 1995).The first step in obtaining a definitional pattern is word generalization.
Depending onits frequency we define a word class as either a word itself or its part of speech.
Formally,let T be the set of training sentences.
We first determine the set F of words in T whosefrequency is above a threshold ?
(e.g., the, a, an, of ).
In our training sentences, we replacethe defined term with the token ?TARGET?
(note that ?TARGET?
?
F).Given a new sentence s = t1, t2, .
.
.
, tn, where ti is the i-th token of s, we generalizeits words ti to word classes t?i as follows:t?i ={ti if ti ?
FPOS(ti) otherwisethat is, a word ti is left unchanged if it occurs frequently in the training corpus (i.e.,ti ?
F); otherwise it is replaced with its part of speech (POS(ti)).
As a result we obtain ageneralized sentence s?.
For instance, given the first sentence in Table 4, we obtain thecorresponding generalized sentence: ?In NNS, a ?TARGET?
is a JJ NN,?
where NN andJJ indicate the noun and adjective classes, respectively.
Generalized sentences are dou-bly beneficial: First, they help reduce the annotation burden, in that many differentlylexicalized sentences can be caught by a single generalized sentence; second, thanksto their reduction of the definition variability, they allow for a higher-recall definitionmodel.Star patterns.
Let T again be the set of training sentences.
In this step we associate astar pattern ?
(s) with each sentence s ?
T .
To do so, let s ?
T be a sentence such thats = t1, t2, .
.
.
, tn, where ti is its i-th token.
Given the set F of most frequent words in T ,the star pattern ?
(s) associated with s is obtained by replacing with * all the tokens ti?
F,that is, all the tokens that are non-frequent words.
For instance, given the sentence ?Inarts, a chiaroscuro is a monochrome picture,?
the corresponding star pattern is ?In *, a?TARGET?
is a *,?
where ?TARGET?
is the defined term.Sentence clustering.
We then cluster the sentences in our training set T on the basis oftheir star pattern.
Formally, let ?
= (?1, .
.
.
,?m) be the set of star patterns associatedwith the sentences in T .
We create a clustering C = (C1, .
.
.
, Cm) such that Ci = {s ?
T :?
(s) = ?i}, that is, Ci contains all the sentences whose star pattern is ?i.As an example, assume ?3 = ?In *, a ?TARGET?
is a *.?
The first three sentencesreported in Table 4 are all grouped into cluster C3.
We note that each cluster Ci contains673Computational Linguistics Volume 39, Number 3sentences whose degree of variability is generally much lower than for any pair ofsentences in T belonging to two different clusters.Word-class lattice construction.
The final step consists of the construction of a WCL foreach sentence cluster, using the corresponding generalized sentences.
Given such acluster Ci ?
C, we apply a greedy algorithm that iteratively constructs the WCL.Let Ci = {s1, s2, .
.
.
, s|Ci|} and consider its first sentence s1 = t1, t2, .
.
.
, tn.
Initially, wecreate a directed graph G = (V, E) such that V = {t1, .
.
.
, tn} and E = {(t1, t2), (t2, t3), .
.
.
,(tn?1, tn)}.
Next, for each j = 2, .
.
.
, |Ci|, we determine the alignment between sentence sjand each sentence sk ?
Ci such that k < j according to the following dynamic program-ming formulation (Cormen, Leiserson, and Rivest 1990, pages 314?319):Ma,b = max {Ma?1,b?1 + Sa,b, Ma,b?1, Ma?1,b}, (1)where a ?
{0, .
.
.
, |sk|} and b ?
{0, .
.
.
, |sj|}, Sa,b is a score of the matching between thea-th token of sk and the b-th token of sj, and M0,0, M0,b and Ma,0 are initially set to 0 forall values of a and b.The matching score Sa,b is calculated on the generalized sentences s?k and s?j asfollows:Sa,b ={1 if t?k,a = t?j,b0 otherwisewhere t?k,a and t?j,b are the a-th and b-th tokens of s?k and s?j , respectively.
In other words, thematching score equals 1 if the a-th and the b-th tokens of the two generalized sentenceshave the same word class.Finally, the alignment score between sk and sj is given by M|sk|,|sj|, which calculatesthe minimal number of misalignments between the two token sequences.
We repeat thiscalculation for each sentence sk (k = 1, .
.
.
, j ?
1) and choose the one that maximizes itsalignment score with sj.
We then use the best alignment to add sj to the graph G: We addto the set of nodes V the tokens of s?j for which there is no alignment to s?k and we add toE the edges (t?1, t?2), .
.
.
, (t?|sj|?1, t?|sj|).Example.
Consider the first three definitions in Table 4.
Their star pattern is ?In *,a ?TARGET?
is a *.?
The corresponding WCL is built as follows: The first part-of-speech tagged sentence, ?In/IN arts/NN , a/DT ?TARGET?/NN is/VBZ a/DTmonochrome/JJ picture/NN,?
is considered.
The corresponding generalized sentence is?In NN1 , a ?TARGET?
is a JJ NN2.?
The initially empty graph is thus populated with onenode for each word class and one edge for each pair of consecutive tokens, as shown inFigure 2a.
Note that we use a rectangle to denote the hypernym token NN2 .
We also addto the graph a start node and an end node ?, and connect them to the correspondinginitial and final sentence tokens.
Next, the second sentence, ?In mathematics, a graphis a data structure that consists of...,?
is aligned to the first sentence.
The alignmentis perfect, apart from the NN3 node corresponding to ?data.?
The node is added tothe graph together with the edges ?a??
NN3 and NN3 ?
NN2 (Figure 2b, node andedges in bold).
Finally, the third sentence in Table 4, ?In computer science, a pixel is adot that is part of a computer image,?
is generalized as ?In NN4 NN1 , a ?TARGET?is a NN2.?
Thus, a new node NN4 is added, corresponding to ?computer?
and new674Velardi, Faralli, and Navigli OntoLearn ReloadedFigure 2The Word-Class Lattice construction steps on the first three sentences in Table 4.
We show inbold the nodes and edges added to the lattice graph as a result of each sentence alignment step.The support of each word class is reported beside the corresponding node.edges are added that connect node ?In?
to NN4 and NN4 to NN1.
Figure 2c shows theresulting lattice.Variants of the WCL model.
So far we have assumed that our WCL model learns latticesfrom the training sentences in their entirety (we call this model WCL-1).
We also consid-ered a second model that, given a star pattern, learns three separate WCLs, one for eachof the three main fields of the definition, namely: definiendum (DF), definitor (VF), anddefiniens (GF).
We refer to this latter model as WCL-3.
Note that our model does nottake into account the REST field, so this fragment of the training sentences is discarded.The reason for introducing the WCL-3 model is that, whereas definitional patterns arehighly variable, DF, VF, and GF individually exhibit a lower variability, thus WCL-3improves the generalization power.Once the learning process is over, a set of WCLs is produced.
Given a test sentences, the classification phase for the WCL-1 model consists of determining whether thereexists a lattice that matches s. In the case of WCL-3, we consider any combination ofdefiniendum, definitor, and definiens lattices.
Given that different combinations mightmatch, for each combination of three WCLs we calculate a confidence score as follows:score(s, lDF, lVF, lGF ) = coverage ?
log2(support + 1) (2)where s is the candidate sentence, lDF, lVF, and lGF are three lattices (one foreach definition field), coverage is the fraction of sentence tokens covered by the675Computational Linguistics Volume 39, Number 3third lattice, and support is the total number of sentences in the corresponding starpattern.WCL-3 selects, if any, the combination of the three WCLs that best fits the sentencein terms of coverage and support from the training set.
In fact, choosing the mostappropriate combination of lattices impacts the performance of hypernym extraction.Given its higher performance (Navigli and Velardi 2010), in OntoLearn Reloaded weuse WCL-3 for definition classification and hypernym extraction.3.3 Domain Filtering and Creation of the Hypernym GraphThe WCLs described in the previous section are used to identify definitional sentencesand harvest hypernyms for the terms obtained as a result of the terminology extractionphase.
In this section we describe how to filter out non-domain definitions and create adense hypernym graph for the domain of interest.Given a term t, the common case is that several definitions are found for it (e.g.,the flow network example provided at the beginning of this section).
Many of thesewill not pertain to the domain of interest, however, especially if they are obtainedfrom the Web or if they define ambiguous terms.
For instance, in the COMPUTERSCIENCE domain, the cash flow definition of flow network shown in Table 3 was notpertinent.
To discard these non-domain sentences, we weight each definition candidated(t) according to the domain terms that are contained therein using the followingformula:DomainWeight(d(t)) =|Bd(t) ?
D||Bd(t)|(3)where Bd(t) is the bag of content words in the definition candidate d(t) and D is givenby the union of the initial terminology T(0) and the set of single words of the terms inT(0) that can be found as nouns in WordNet.
For example, given T(0) = { greedy algo-rithm, information retrieval, minimum spanning tree }, our domain terminology D = T(0) ?
{ algorithm, information, retrieval, tree }.
According to Equation (3), the domain weightof a definition is normalized by the total number of content words in the definition, soas to penalize longer definitions.
Domain filtering is performed by keeping only thosedefinitions d(t) whose DomainWeight(d(t)) ?
?, where ?
is an empirically tuned thresh-old.9 In Table 3 (third column), we show some values calculated for the correspondingdefinitions (the fourth column reports a check mark if the domain weight is abovethe threshold, an ?
otherwise).
Domain filtering performs some implicit form of WordSense Disambiguation (Navigli 2009), as it aims at discarding senses of hypernymswhich do not pertain to the domain.Let Ht be the set of hypernyms extracted with WCLs from the definitions of term twhich survived this filtering phase.
For each t ?
T(i), we add Ht to our graph Gnoisy =(Vnoisy, Enoisy), that is, we set Vnoisy := Vnoisy ?
Ht.
For each t, we also add a directededge (h, t)10 for each hypernym h ?
Ht, that is, we set Enoisy := Enoisy ?
{(h, t)}.
As a result9 Empirically set to 0.38, as a result of tuning on several data sets of manually annotated definitions indifferent domains.10 In what follows, (h, t) or h ?
t reads ?t is-a h.?676Velardi, Faralli, and Navigli OntoLearn Reloadedof this step, the graph contains our domain terms and their hypernyms obtained fromdomain-filtered definitions.
We now set:T(i+1) :=?t?T(i)Ht \i?j=1T(j) (4)that is, the new set of terms T(i+1) is given by the hypernyms of the current set of termsT(i) excluding those terms that were already processed during previous iterations ofthe algorithm.
Next, we move to iteration i + 1 and repeat the last two steps, namely,we perform definition/hypernym extraction and domain filtering on T(i+1).
As a resultof subsequent iterations, the initially empty graph is increasingly populated with newnodes (i.e., domain terms) and edges (i.e., hypernymy relations).After a given number of iterations K, we obtain a dense hypernym graph Gnoisythat potentially contains more than one connected component.
Finally, we connect allthe upper term nodes in Gnoisy to a single top node .
As a result of this connectingstep, only one connected component of the noisy hypernym graph?which we callthe backbone component?will contain an upper taxonomy consisting of upperterms in U.The resulting graph Gnoisy potentially contains cycles and multiple hypernyms forthe vast majority of nodes.
In order to eliminate noise and obtain a full-fledged taxon-omy, we perform a step of graph pruning, as described in the next section.3.4 Graph PruningAt the end of the iterative hypernym harvesting phase, described in Sections 3.2 and 3.3,the result is a highly dense, potentially disconnected, hypernymy graph (see Section 4for statistics concerning the experiments that we performed).
Wrong nodes and edgesmight stem from errors in any of the definition/hypernym extraction and domain filter-ing steps.
Furthermore, for each node, multiple ?good?
hypernyms can be harvested.Rather than using heuristic rules, we devised a novel graph pruning algorithm, basedon the Chu-Liu/Edmonds optimal branching algorithm (Chu and Liu 1965; Edmonds1967), that exploits the topological graph properties to produce a full-fledged taxonomy.The algorithm consists of four phases (i.e., graph trimming, edge weighting, optimalbranching, and pruning recovery) that we describe hereafter with the help of the noisygraph in Figure 3a, whose grey nodes belong to the initial terminology T(0) and whosebold node is the only upper term.3.4.1 Graph Trimming.
We first perform two trimming steps.
First, we disconnect ?false?roots, i.e., nodes which are not in the set of upper terms and with no incoming edges(e.g., image in Figure 3a).
Second, we disconnect ?false?
leaves, namely, leaf nodes whichare not in the initial terminology and with no outgoing edges (e.g., output in Figure 3a).We show the disconnected components in Figure 3b.3.4.2 Edge Weighting.
Next, we weight the edges in our noisy graph Gnoisy.
A policy basedonly on graph connectivity (e.g., in-degree or betweenness, see Newman [2010] for acomplete survey) is not sufficient for taxonomy learning.11 Consider again the graph in11 As also remarked by Kozareva and Hovy (2010), who experimented with in-degree.677Computational Linguistics Volume 39, Number 3Figure 3A noisy graph excerpt (a), its trimmed version (b), and the final taxonomy resulting frompruning (c).Figure 3: In choosing the best hypernym for the term token sequence, a connectivity-basedmeasure might select collection rather than list, because the former reaches more nodes.In taxonomy learning, however, longer hypernymy paths should be preferred (e.g., datastructure ?
collection ?
list ?
token sequence is better than data structure ?
collection ?token sequence).We thus developed a novel weighting policy aimed at finding the best trade-offbetween path length and the connectivity of traversed nodes.
It consists of three steps:i) Weight each node v by the number of nodes belonging to the initialterminology that can be reached from v (potentially including v itself).12Let w(v) denote the weight of v (e.g., in Figure 3b, node collection reacheslist and token sequence, thus w(collection) = 2, whereas w(graph) = 3).All weights are shown in the corresponding nodes in Figure 3b.ii) For each node v, consider all the paths from an upper root r to v.Let ?
(r, v) be the set of such paths.
Each path p ?
?
(r, v) is weightedby the cumulative weight of the nodes in the path, namely:?
(p) =?v??pw(v?)
(5)iii) Assign the following weight to each incoming edge (h, v) of v (i.e., h is oneof the direct hypernyms of v):w(h, v) = maxr?Umaxp??(r,h)?
(p) (6)This formula assigns to edge (h, v) the value ?
(p) of the highest-weightingpath p from h to any upper root ?
U.
For example, in Figure 3b, w(list) = 2,w(collection) = 2, w(data structure) = 5.
Therefore, the set of paths ?
(datastructure, list) = { data structure ?
list, data structure ?
collection ?
list },whose weights are 7 (w(data structure) + w(list)) and 9 (w(data structure) +w(collection) + w(list)), respectively.
Hence, according to Formula 6, w(list,token sequence) = 9.
We show all edge weights in Figure 3b.12 Nodes in a cycle are visited only once.678Velardi, Faralli, and Navigli OntoLearn Reloaded3.4.3 Optimal Branching.
Next, our goal is to move from a noisy graph to a tree-liketaxonomy on the basis of our edge weighting strategy.
A maximum spanning treealgorithm cannot be applied, however, because our graph is directed.
Instead, we needto find an optimal branching, that is, a rooted tree with an orientation such that everynode but the root has in-degree 1, and whose overall weight is maximum.
To this end,we first apply a pre-processing step: For each (weakly) connected component in thenoisy graph, we consider a number of cases, aimed at identifying a single ?reasonable?root node to enable the optimal branching to be calculated.
Let R be the set of candidateroots, that is, nodes with no incoming edges.
We perform the following steps:i) If |R| = 1 then we select the only candidate as root.ii) Else if |R| > 1, if an upper term is in R, we select it as root, else we choosethe root r ?
R with the highest weight w according to the weightingstrategy described in Section 3.4.2.
We also disconnect all the unselectedroots, that is, those in R \ {r}.iii) Else (i.e., if |R| = 0), we proceed as for step (ii), but we search candidateswithin the entire connected component and select the highest weightingnode.
In contrast to step (ii), we remove all the edges incoming to theselected node.This procedure guarantees not only the selection but also the existence of a singleroot node for each component, from which the optimal branching algorithm can start.We then apply the Chu-Liu/Edmonds algorithm (Chu and Liu 1965; Edmonds 1967) toeach component Gi = (Vi, Ei) of our directed weighted graph Gnoisy in order to find anoptimal branching.
The algorithm consists of two phases: a contraction phase and anexpansion phase.
The contraction phase is as follows:1.
For each node which is not a root, we select the entering edge with thehighest weight.
Let S be the set of such |Vi| ?
1 edges;2.
If no cycles are formed in S, go to the expansion phase.
Otherwise,continue;3.
Given a cycle in S, contract the nodes in the cycle into a pseudo-node k,and modify the weight of each edge entering any node v in the cycle fromsome node h outside the cycle, according to the following equation:w(h, k) = w(h, v) + (w(x(v), v) ?
minv(w(x(v), v))) (7)where x(v) is the predecessor of v in the cycle and w(x(v), v) is the weightof the edge in the cycle which enters v;4.
Select the edge entering the cycle which has the highest modified weightand replace the edge which enters the same real node in S by the newselected edge;5.
Go to step 2 with the contracted graph.The expansion phase is applied if pseudo-nodes have been created during step 3.Otherwise, this phase is skipped and Ti = (Vi, S) is the optimal branching of component679Computational Linguistics Volume 39, Number 3Gi (i.e., the i-th component of Gnoisy).
During the expansion phase, pseudo-nodes arereplaced with the original cycles.
To break the cycle, we select the real node v into whichthe edge selected in step 4 enters, and remove the edge entering v belonging to thecycle.
Finally, the weights on the edges are restored.
For example, consider the cyclein Figure 4a.
Nodes pagerank, map, and rank are contracted into a pseudo-node, andthe edges entering the cycle from outside are re-weighted according to Equation (7).According to the modified weights (Figure 4b), the selected edge, that is, (table, map),is the one with weight w = 13.
During the expansion phase, the edge (pagerank, map) iseliminated, thus breaking the cycle (Figure 4c).The tree-like taxonomy resulting from the application of the Chu-Liu/Edmondsalgorithm to our example in Figure 3b is shown in Figure 3c.3.4.4 Pruning Recovery.
The weighted directed graph Gnoisy input to the Chu-Liu/Edmonds algorithm might contain many (weakly) connected components.
In this case,an optimal branching is found for each component, resulting in a forest of taxonomytrees.
Although some of these components are actually noisy, others provide an impor-tant contribution to the final tree-like taxonomy.
The objective of this phase is to recoverfrom excessive pruning, and re-attach some of the components that were disconnectedduring the optimal branching step.
Recall from Section 3.3 that, by construction, wehave only one backbone component, that is, a component which includes an upper tax-onomy.
Our aim is thus to re-attach meaningful components to the backbone taxonomy.To this end, we apply Algorithm 1.
The algorithm iteratively merges non-backbone treesto the backbone taxonomy tree T0 in three main steps: Semantic reconnection step (lines 7?9 in Algorithm 1): In this step wereuse a previously removed ?noisy?
edge, if one is available, to reattach anon-backbone component to the backbone.
Given a root node rTi of anon-backbone tree Ti (i > 0), if an edge (v, rTi ) existed in the noisy graphGnoisy (i.e., the one obtained before the optimal branching phase), withv ?
T0, then we connect the entire tree Ti to T0 by means of this edge.Figure 4A graph excerpt containing a cycle (a); Edmonds?
contraction phase: a pseudo-node enclosingthe cycle with updated weights on incoming edges (b); and Edmonds?
expansion phase: thecycle is broken and weights are restored (c).680Velardi, Faralli, and Navigli OntoLearn ReloadedAlgorithm 1 PruningRecovery(G, Gnoisy)Require: G is a forest1: repeat2: Let F := {T0, T1, .
.
.
, T|F|} be the forest of trees in G = (V, E)3: Let T0 ?
F be the backbone taxonomy4: E?
?
E5: for all T in F \ {T0} do6: rT ?
rootOf (T)7: if ?v ?
T0 s.t.
(v, rT ) ?
Gnoisy then8: E ?
E ?
{(v, rT )}9: break10: else11: if out-degree(rT ) = 0 then12: if ?v ?
T0 s.t.
v is the longest right substring of rT then13: E := E ?
{(v, rT )}14: break15: else16: E ?
E \ {(rT, v) : v ?
V}17: break18: until E = E? Reconnection step by lexical inclusion (lines 11?14): Otherwise, if Ti is asingleton (the out-degree of rTi is 0) and there exists a node v ?
T0 suchthat v is the longest right substring of rTi by lexical inclusion,13 we connectTi to the backbone tree T0 by means of the edge (v, rTi ). Decomposition step (lines 15?17): Otherwise, if the component Ti is not asingleton (i.e., if the out-degree of the root node rTi is > 0) we disconnectrTi from Ti.
At first glance, it might seem counterintuitive to remove edgesduring pruning recovery.
Reconnecting by lexical inclusion within adomain has already been shown to perform well in the literature (Vossen2001; Navigli and Velardi 2004), but we want to prevent any cascadingerrors on the descendants of the root node, and at the same time free upother pre-existing ?noisy?
edges incident to the descendants.These three steps are iterated on the newly created components, until no changeis made to the graph (line 18).
As a result of our pruning recovery phase we return theenriched backbone taxonomy.
We show in Figure 5 an example of pruning recovery thatstarts from a forest of three components (including the backbone taxonomy tree on top,Figure 5a).
The application of the algorithm leads to the disconnection of a tree root,that is, ordered structure (Figure 5a, lines 15?17 of Algorithm 1), the linking of the treesrooted at token list and binary search tree to nodes in the backbone taxonomy (Figures 5band 5d, lines 7?9), and the linking of balanced binary tree to binary tree thanks to lexicalinclusion (Figure 5c, lines 11?14 of the algorithm).13 Similarly to our original OntoLearn approach (Navigli and Velardi 2004), we define a node?s stringv = wnwn?1 .
.
.w2w1 to be lexically included in that of a node v?
= w?mw?m?1 .
.
.w?2w?1 if m > n andwj = w?j for each j ?
{1, .
.
.
, n}.681Computational Linguistics Volume 39, Number 3Figure 5An example starting with three components, including the backbone taxonomy tree on thetop and two other trees on the bottom (a).
As a result of pruning recovery, we disconnect orderedstructure (a); we connect token sequence to token list by means of a ?noisy?
edge (b); we connectbinary tree to balanced binary tree by lexical inclusion (c); and finally binary tree to binary searchtree by means of another ?noisy?
edge (d).3.5 Edge RecoveryThe goal of the last phase was to recover from the excessive pruning of the optimalbranching phase.
Another issue of optimal branching is that we obtain a tree-like tax-onomic structure, namely, one in which each node has only one hypernym.
This is notfully appropriate in taxonomy learning, because systematic ambiguity and polysemyoften require a concept to be paradigmatically related to more than one hypernym.
Infact, a more appropriate structure for a conceptual hierarchy is a DAG, as in WordNet.For example, two equally valid hypernyms for backpropagation are gradient descent search682Velardi, Faralli, and Navigli OntoLearn Reloadedprocedure and training algorithm, so two hypernym edges should correctly be incident tothe backpropagation node.We start from our backbone taxonomy T0 obtained after the pruning recoveryphase described in Section 3.4.4.
In order to obtain a DAG-like taxonomy we applythe following step: for each ?noisy?
edge (v, v?)
?
Enoisy such that v, v?
are nodes in T0but the edge (v, v?)
does not belong to the tree, we add (v, v?)
to T0 if:i) it does not create a cycle in T0;ii) the absolute difference between the length of the shortest path from v tothe root rT0 and that of the shortest path from v?
to rT0 is within an interval[m, M].
The aim of this constraint is to maintain a balance between theheight of a concept in the tree-like taxonomy and that of the hypernymconsidered for addition.
In other words, we want to avoid the connectionof an overly abstract concept with an overly specific one.In Section 4, we experiment with three versions of our OntoLearn Reloaded algo-rithm, namely: one version that does not perform edge recovery (i.e., which learns atree-like taxonomy [TREE], and two versions that apply edge recovery (i.e., which learna DAG) with different intervals for constraint (ii) above (DAG[1, 3] and DAG[0, 99]; notethat the latter version virtually removes constraint (ii)).
Examples of recovered edgeswill be presented and discussed in the evaluation section.3.6 ComplexityWe now perform a complexity analysis of the main steps of OntoLearn Reloaded.
Giventhe large number of steps and variables involved we provide a separate discussion ofthe main costs for each individual step, and we omit details about commonly used datastructures for access and storage, unless otherwise specified.
Let Gnoisy = (Vnoisy, Enoisy)be our noisy graph, and let n = |Vnoisy| and m = |Enoisy|.1.
Terminology extraction: Assuming a part-of-speech tagged corpus asinput, the cost of extracting candidate terms by scanning the corpus with amaximum-size window is in the order of the word size of the inputcorpus.
Thus, the application of statistical measures to our set of candidateterms has a computational cost that is on the order of the square of thenumber of term candidates (i.e., the cost of calculating statistics for eachpair of terms).2.
Definition and hypernym extraction: In the second step, we first retrievecandidate definitions from the input corpus, which costs on the order ofthe corpus size.14 Each application of a WCL classifier to an inputcandidate sentence s containing a term t costs on the order of the wordlength of the sentence, and we have a constant number of such classifiers.So the cost of this step is given by the sum of the lengths of the candidatesentences in the corpus, which is lower than the word size of the corpus.14 Note that this corpus consists of both free text and Web glossaries (cf.
Section 3.2).683Computational Linguistics Volume 39, Number 33.
Domain filtering and creation of the graph: The cost of domain filteringfor a single definition is in the order of its word length, so the running timeof domain filtering is in the order of the sum of the word size of theacquired definitions.
As for the hypernym graph creation, using anadjacency-list representation of the graph Gnoisy, the dynamic addition of anewly acquired hypernymy edge costs O(n), an operation which has to berepeated for each (hypernymy, term) pair.4.
Graph pruning, consisting of the following steps: Graph trimming: This step requires O(n) time in order to identifyfalse leaves and false roots by iterating over the entire set of nodes. Edge weighting: i) We perform a DFS (O(n + m)) to weight all thenodes in the graph; ii) we collect all paths from upper roots to anygiven node, totalizing O(n!)
paths in the worst case (i.e., in acomplete graph).
In real domains, however, the computational costof this step will be much lower.
In fact, over our six domains, theaverage number of paths per node ranges from 4.3 (n = 2107,ANIMALS) to 3175.1 (n = 2616, FINANCE domain): In the latter,worst case, in practice, the number of paths is in the order of n, thusthe cost of this step, performed for each node, can be estimated byO(n2) running time; iii) assigning maximum weights to edges costsO(m) if in the previous step we keep track of the maximum valueof paths ending in each node h (see Equation (6)). Optimal branching: Identifying the connected components of ourgraph costs O(n + m) time, identifying root candidates andselecting one root per component costs O(n), and finally applyingthe Chu-Liu/Edmonds algorithm costs O(m ?
log2n) for sparsegraphs, O(n2) for dense ones, using Tarjan?s implementation(Tarjan 1977).5.
Pruning recovery: In the worst case, m iterations of Algorithm 1 will beperformed, each costing O(n) time, thus having a total worst-case cost ofO(mn).6.
Edge recovery: For each pair of nodes in T0 we perform i) theidentification of cycles (O(n + m)) and ii) the calculation of the shortestpaths to the root (O(n + m)).
By precomputing the shortest path for eachnode, the cost of this step is O(n(n + m)) time.Therefore, in practice, the computational complexity of OntoLearn Reloaded ispolynomial in the main variables of the problem, namely, the number of words in thecorpus and nodes in the noisy graph.4.
EvaluationOntology evaluation is a hard task that is difficult even for humans, mainly becausethere is no unique way of modeling the domain of interest.
Indeed several differenttaxonomies might model a particular domain of interest equally well.
Despite thisdifficulty, various evaluation methods have been proposed in the literature for assessing684Velardi, Faralli, and Navigli OntoLearn Reloadedthe quality of a taxonomy.
These include Brank, Mladenic, and Grobelnik (2006) andMaedche, Pekar, and Staab (2002):a) automatic evaluation against a gold standard;b) manual evaluation performed by domain experts;c) structural evaluation of the taxonomy;d) application-driven evaluation, in which a taxonomy is assessed on thebasis of the improvement its use generates within an application.Other quality indicators have been analyzed in the literature, such as accuracy,completeness, consistency (Vo?lker et al2008), and more theoretical features (Guarinoand Welty 2002) like essentiality, rigidity, and unity.
Methods (a) and (b) are by far themost popular ones.
In this section, we will discuss in some detail the pros and cons ofthese two approaches.Gold standard evaluation.
The most popular approach for the evaluation of lexicalizedtaxonomies (adopted, e.g., in Snow, Jurafsky, and Ng 2006; Yang and Callan 2009;and Kozareva and Hovy 2010) is to attempt to reconstruct an existing gold standard(Maedche, Pekar, and Staab 2002), such as WordNet or the Open Directory Project.This method is applicable when the set of taxonomy concepts are given, and theevaluation task is restricted to measuring the ability to reproduce hypernymy linksbetween concept pairs.
The evaluation is far more complex when learning a specializedtaxonomy entirely from scratch, that is, when both terms and relations are unknown.In reference taxonomies, even in the same domain, the granularity and cotopy15 of anabstract concept might vary according to the scope of the taxonomy and the expertiseof the team who created it (Maedche, Pekar, and Staab 2002).
For example, both theterms chiaroscuro and collage are classified under picture, image, icon in WordNet, but inthe Art & Architecture Thesaurus (AA&T)16 chiaroscuro is categorized under perspectiveand shading techniques whereas collage is classified under image-making processes andtechniques.
As long as common-sense, non-specialist knowledge is considered, it is stillfeasible for an automated system to replicate an existing classification, because theWeb will provide abundant evidence for it.
For example, Kozareva and Hovy (2010,K&H hereafter) are very successful at reproducing the WordNet sub-taxonomy forANIMALS, because dozens of definitional patterns are found on the Web that classify,for example, lion as a carnivorous feline mammal, or carnivorous, or feline.
As we showlater in this section, however, and as also suggested by the previous AA&T example,finding hypernymy patterns in more specialized domains is far more complex.
Even insimpler domains, however, it is not clear how to evaluate the concepts and relations notfound in the reference taxonomy.
Concerning this issue, Zornitsa Kozareva commentsthat: ?When we gave sets of terms to annotators and asked them to produce a taxonomy,people struggled with the domain terminology and produced quite messy organization.Therefore, we decided to go with WordNet and use it as a gold truth?
(personalcommunication).
Accordingly, K&H do not provide an evaluation of the nodes andrelations other than those for which the ground truth is known.
This is further clarifiedin a personal communication: ?Currently we do not have a full list of all is-a outside15 The cotopy of a concept is the set of its hypernyms and hyponyms.16 http://www.getty.edu/vow/AATHierarchy.685Computational Linguistics Volume 39, Number 3WordNet.
[...] In the experiments, we work only with the terms present in WordNet[...] The evaluation is based only on the WordNet relations.
However, the harvestingalgorithm extracts much more.
Currently, we do not know how to evaluate the Webtaxonomization.
?To conclude, gold standard evaluation has some evident drawbacks: When both concepts and relations are unknown, it is almost impossible toreplicate a reference taxonomy accurately. In principle, concepts not in the reference taxonomy can be either wrongor correct; therefore the evaluation is in any case incomplete.Another issue in gold standard evaluation is the definition of an adequate evalu-ation metric.
The most common measure used in the literature to compare a learnedwith a gold-standard taxonomy is the overlapping factor (Maedche, Pekar, and Staab2002).
Given the set of is-a relations in the two taxonomies, the overlapping factorsimply computes the ratio between the intersection and union of these sets.
Thereforethe overlapping factor gives a useful global measure of the similarity between thetwo taxonomies.
It provides no structural comparison, however: Errors or differencesin grouping concepts in progressively more general classes are not evidenced by thismeasure.Comparison against a gold standard has been analyzed in a more systematic wayby Zavitsanos, Paliouras, and Vouros (2011) and Brank, Mladenic, and Grobelnik (2006).They propose two different strategies for escaping the ?naming?
problem that we haveoutlined.
Zavitsanos, Paliouras, and Vouros (2011) propose transforming the ontologyconcepts and their properties into distributions over the term space of the source datafrom which the ontology has been learned.
These distributions are used to computepairwise concept similarity between gold standard and learned ontologies.Brank, Mladenic, and Grobelnik (2006) exploit the analogy between ontology learn-ing and unsupervised clustering, and propose OntoRand, a modified version of theRand Index (Rand 1971) for computing the similarity between ontologies.
Morey andAgresti (1984) and Carpineto and Romano (2012), however, demonstrated a high de-pendency of the Rand Index (and consequently of OntoRand itself) upon the number ofclusters, and Fowlkes and Mallows (1983) show that the Rand Index has the undesirableproperty of converging to 1 as the number of clusters increases, even in the unrealisticcase of independent clusterings.
These undesired outcomes have also been experiencedby Brank, Mladenic, and Grobelnik (2006, page 5), who note that ?the similarity of anontology to the original one is still as high as 0.74 even if only the top three levels ofthe ontology have been kept.?
Another problem with the OntoRand formula, as alsoremarked in Zavitsanos, Paliouras, and Vouros (2011), is the requirement of comparingontologies with the same set of instances.Manual evaluation.
Comparison against a gold standard is important because it repre-sents a sort of objective evaluation of an automated taxonomy learning method.
Aswe have already remarked, however, learning an existing taxonomy is not particularlyinteresting in itself.
Taxonomies are mostly needed in novel, often highly technical do-mains for which there are no gold standards.
For a system to claim to be able to acquirea taxonomy from the ground up, manual evaluation seems indispensable.
Nevertheless,none of the taxonomy learning systems surveyed in Section 2 performs such evaluation.Furthermore, manual evaluation should not be limited to an assessment of the acquired686Velardi, Faralli, and Navigli OntoLearn Reloadedhypernymy relations ?in isolation,?
but must also provide a structural assessmentaimed at identifying common phenomena and the overall quality of the taxonomicstructure.
Unfortunately, as already pointed out, manual evaluation is a hard task.Deciding whether or not a concept belongs to a given domain is more or less feasiblefor a domain expert, but assessing the quality of a hypernymy link is far more complex.On the other hand, asking a team of experts to blindly reconstruct a hierarchy, given aset of terms, may result in the ?messy organization?
reported by Zornitsa Kozareva.
Incontrast to previous approaches to taxonomy induction, OntoLearn Reloaded providesa natural solution to this problem, because is-a links in the taxonomy are supported byone or more definition sentences from which the hypernymy relation was extracted.
Asshown later in this section, definitions proved to be a very helpful feature in supportingmanual analysis, both for hypernym evaluation and structural assessment.The rest of this section is organized as follows.
We first describe the experimen-tal set-up (Section 4.1): OntoLearn Reloaded is applied to the task of acquiring sixtaxonomies, four of which attempt to replicate already existing gold standard sub-hierarchies in WordNet17 and in the MeSH medical ontology,18 and the other two arenew taxonomies acquired from scratch.
Next, we present a large-scale multi-facetedevaluation of OntoLearn Reloaded focused on three of the previously described eval-uation methods, namely: comparison against a gold standard, manual evaluation, andstructural evaluation.
In Section 4.2 we introduce a novel measure for comparing aninduced taxonomy against a gold standard one.
Finally, Section 4.3 is dedicated to amanual evaluation of the six taxonomies.4.1 Experimental Set-upWe now provide details on the set-up of our experiments.4.1.1 Domains.
We applied OntoLearn Reloaded to the task of acquiring six taxonomies:ANIMALS, VEHICLES, PLANTS, VIRUSES, ARTIFICIAL INTELLIGENCE, and FINANCE.The first four taxonomies were used for comparison against three WordNet sub-hierarchies and the viruses sub-hierarchy of MeSH.
The ANIMALS, VEHICLES, andPLANTS domains were selected to allow for comparison with K&H, who experimentedon the same domains.
The ARTIFICIAL INTELLIGENCE and FINANCE domains are ex-amples of taxonomies truly built from the ground up, for which we provide a thoroughmanual evaluation.
These domains were selected because they are large, interdisci-plinary, and continuously evolving fields, thus representing complex and specializeduse cases.4.1.2 Definition Harvesting.
For each domain, definitions were sought in Wikipedia andin Web glossaries automatically obtained by means of a Web glossary extraction system(Velardi, Navigli, and D?Amadio 2008).
For the ARTIFICIAL INTELLIGENCE domain wealso used a collection consisting of the entire IJCAI proceedings from 1969 to 2011 andthe ACL archive from 1979 to 2010.
In what follows we refer to this collection as the ?AIcorpus.?
For FINANCE we used a combined corpus from the freely available collectionof Journal of Financial Economics from 1995 to 2012 and from Review Of Finance from 1997to 2012 for a total of 1,575 papers.17 http://wordnet.princeton.edu.18 http://www.nlm.nih.gov/mesh/.687Computational Linguistics Volume 39, Number 34.1.3 Terminology.
For the ANIMALS, VEHICLES, PLANTS, and VIRUSES domains, theinitial terminology was a fragment of the nodes of the reference taxonomies,19 sim-ilarly to, and to provide a fair comparison with, K&H.
For the AI domain instead,the initial terminology was selected using our TermExtractor tool20 on the AI corpus.TermExtractor extracted over 5,000 terms from the AI corpus, ranked according to acombination of relevance indicators related to the (direct) document frequency, domainpertinence, lexical cohesion, and other indicators (Sclano and Velardi 2007).
We manu-ally selected 2,218 terms from the initial set, with the aim of eliminating compoundslike order of magnitude, empirical study, international journal, that are frequent but notdomain relevant.
For similar reasons a manual selection of terms was also applied to theterminology automatically extracted for the FINANCE domain, obtaining 2,348 terms21from those extracted by TermExtractor.
An excerpt of extracted terms was provided inTable 1.4.1.4 Upper Terms.
Concerning the selection of upper terms U (cf.
Section 3.2), againsimilarly to K&H, we used just one concept for each of the four domains focusedupon: ANIMALS, VEHICLES, PLANTS, and VIRUSES.
For the AI and FINANCE domains,which are more general and complex, we selected from WordNet a core taxonomy of32 upper concepts U (resulting in 52 terms) that we used as a stopping criterion forour iterative definition/hypernym extraction and filtering procedure (cf.
Section 3.2).The complete list of upper concepts was given in Table 2.
WordNet upper concepts aregeneral enough to fit most domains, and in fact we used the same set U for AI andFINANCE.
Nothing, however, would have prevented us from using a domain-specificcore ontology, such as the CRM-CIDOC core ontology for the domain of ART ANDARCHITECTURE.224.1.5 Algorithm Versions and Structural Statistics.
For each of the six domains we ran thethree versions of our algorithm: without pruning recovery (TREE), with [1, 3] recovery(DAG[1, 3]), and with [0, 99] recovery (DAG[0, 99]), for a total of 18 experiments.
Weremind the reader that the purpose of the recovery process was to reattach some of theedges deleted during the optimal branching step (cf.
Section 3.5).Figure 6 shows an excerpt of the AI tree-like taxonomy under the node data structure.Notice that, even though the taxonomy looks good overall, there are still a few errors,such as ?neuron is a neural network?
and overspecializations like ?network is a digraph.
?Figure 7 shows a sub-hierarchy of the FINANCE tree-like taxonomy under the conceptvalue.In Table 6 we give the structural details of the 18 taxonomies extracted for our sixdomains.
In the table, edge and node compression refers to the number of survivingnodes and edges after the application of optimal branching and recovery steps to thenoisy hypernymy graph.
To clarify the table, consider the case of VIRUSES, DAG[1, 3]:we started with 281 initial terms, obtaining a noisy graph with 1,174 nodes and 1,859edges.
These were reduced to 297 nodes (i.e., 1,174?877) and 339 edges (i.e., 1,859?1,520)after pruning and recovery.
Out of the 297 surviving nodes, 222 belonged to the initial19 For ANIMALS, VEHICLES, and PLANTS we used precisely the same seeds as K&H.20 http://lcl.uniroma1.it/termextractor.21 These dimensions are quite reasonable for large technical domains: as an example, The Economist?sglossary of economic terms includes on the order of 500 terms (http://www.economist.com/economics-a-to-z/).22 http://cidoc.mediahost.org/standard crm(en)(E1).xml.688Velardi, Faralli, and Navigli OntoLearn ReloadedFigure 6An excerpt of the ARTIFICIAL INTELLIGENCE taxonomy.terminology; therefore the coverage over the initial terms is 0.79 (222/281).
This meansthat, for some of the initial terms, either no definitions were found, or the definitionwas rejected in some of the processing steps.
The table also shows, as expected, that theterm coverage is much higher for ?common-sense?
domains like ANIMALS, VEHICLES,and PLANTS, is still over 0.75 for VIRUSES and AI, and is a bit lower for FINANCE(0.65).
The maximum and average depth of the taxonomies appears to be quite variable,with VIRUSES and FINANCE at the two extremes.
Finally, Table 6 reports in the lastcolumn the number of glosses (i.e., domain definitional sentences) obtained in eachrun.
We would like to point out that providing textual glosses for the retrieved domainhypernyms is a novel feature that has been lacking in all previous approaches toontology learning, and which can also provide key support to much-needed manualvalidation and enrichment of existing semantic networks (Navigli and Ponzetto 2012).4.2 Evaluation Against a Gold StandardIn this section we propose a novel, general measure for the evaluation of a learnedtaxonomy against a gold standard.
We borrow the Brank, Mladenic, and Grobelnik689Computational Linguistics Volume 39, Number 3Figure 7An excerpt of the FINANCE taxonomy.
(2006) idea of exploiting the analogy with unsupervised clustering but, rather thanrepresenting the two taxonomies as flat clusterings, we propose a measure that takesinto account the hierarchical structure of the two analyzed taxonomies.
Under thisperspective, a taxonomy can be transformed into a hierarchical clustering by replacingeach label of a non-leaf node (e.g., perspective and shading techniques) with the transitiveclosure of its hyponyms (e.g., cangiatismo, chiaroscuro, foreshortening, hatching).4.2.1 Evaluation Model.
Techniques for comparing clustering results have been surveyedin Wagner and Wagner (2007), although the only method for comparing hierarchicalclusters, to the best of our knowledge, is that proposed by Fowlkes and Mallows (1983).Suppose that we have two hierarchical clusterings H1 and H2, with an identical set of nobjects.
Let k be the maximum depth of both H1 and H2, and Hij a cut of the hierarchy,where i ?
{0, .
.
.
, k} is the cut level and j ?
{1, 2} selects the clustering of interest.
Then,for each cut i, the two hierarchies can be seen as two flat clusterings Ci1 and Ci2 of the nconcepts.
When i = 0 the cut is a single cluster incorporating all the objects, and wheni = k we obtain n singleton clusters.
Now let: n11 be the number of object pairs that are in the same cluster in both Ci1and Ci2; n00 be the number of object pairs that are in different clusters in both Ci1and Ci2; n10 be the number of object pairs that are in the same cluster in Ci1 butnot in Ci2;690Velardi, Faralli, and Navigli OntoLearn ReloadedTable 6Structural evaluation of three versions of our taxonomy-learning algorithm on six differentdomains.Experiment Term Coverage Depth |V| |E| V Compress.
E Compress.
GlossesAITREE 75.51% 12 max 2,387 2,386 43.00% 67.31% 1,249(1,675/2,218) 6.00 avg (1,801/4,188) (4,915/7,301)DAG [1,3] 75.51% 19 max 2,387 3,554 43.00% 51.32% 2,081(1,675/2,218) 8.27 avg (1,801/4,188) (3,747/7,301)DAG [0,99] 75.51% 20 max 2,387 3,994 43.00% 45.29% 2,439(1,675/2,218) 8.74 avg (1,801/4,188) (3,307/7,301)FINANCETREE 65.20% 14 max 2,038 2,037 22.09% 47.99% 1,064(1,533/2,348) 6.83 avg (578/2,616) (1,880/3,917)DAG [1,3] 65.20% 38 max 2,038 2,524 22.09% 35.56% 1,523(1,533/2,348) 18.82 avg (578/2,616) (1,393/3,917)DAG [0,99] 65.20% 65 max 2,038 2,690 22.09% 31.32% 1,677(1,533/2,348) 33.54 avg (578/2,616) (1,227/3,917)VIRUSESTREE 79.00% 5 max 297 296 74.70% 84.07% 172(222/281) 2.13 avg (877/1,174) (1,563/1,859)DAG [1,3] 79.00% 5 max 297 339 74.70% 81.76% 212(222/281) 2.20 avg (877/1,174) (1,520/1,859)DAG [0,99] 79.00% 5 max 297 360 74.70% 80.63% 233(222/281) 2.32 avg (877/1,174) (1,563/1,859)ANIMALSTREE 93.56% 10 max 900 899 57.28% 66.96% 724(640/684) 4.35 avg (1,207/2,107) (1,822/2,721)DAG [1,3] 93.56% 16 max 900 1,049 57.28% 61.44% 872(640/684) 5.21 avg (1,207/2,107) (1,672/2,721)DAG [0,99] 93.56% 16 max 900 1,116 57.28% 58.98% 939(640/684) 5.39 avg (1,207/2,107) (1,605/2,721)PLANTSTREE 96.57% 19 max 710 709 72.69% 84.53% 638(535/554) 5.85 avg (1,890/2,600) (3,877/4,586)DAG [1,3] 96.57% 19 max 710 922 72.69% 79.89% 851(535/554) 6.65 avg (1,890/2,600) (3,664/4,586)DAG [0,99] 96.57% 19 max 710 1,242 72.69% 72.91% 1,171(535/554) 6.54 avg (1,890/2,600) (3,344/4,586)VEHICLESTREE 95.72% 8 max 169 168 71.50% 80.48% 150(112/117) 3.44 avg (424/593) (693/861)DAG [1,3] 95.72% 8 max 169 200 71.50% 76.77% 182(112/117) 3.94 avg (424/593) (661/861)DAG [0,99] 95.72% 10 max 169 231 71.50% 73.17% 213(112/117) 4.48 avg (424/593) (630/861) n01 be the number of object pairs that are in the same cluster in Ci2 but notin Ci1;The generalized Fowlkes and Mallows (F&M) measure of cluster similarity for thecut i (i ?
{0, .
.
.
, k}), as reformulated by Wagner and Wagner (2007), is defined as:Bi1,2 =ni11?
(ni11 + ni10) ?
(ni11 + ni01).
(8)Note that the formula can be interpreted as the geometric mean of precision andrecall of an automated method in clustering the same concept pairs as in a gold-standard691Computational Linguistics Volume 39, Number 3clustering.
This formula has a few undesirable properties: first, the value of Bi1,2 getsclose to its maximum 1.0 as we approach the root of the hierarchy (i = 0); second, thetwo hierarchies need to have the same maximum depth k; third, the hierarchies need tohave the same number of initial objects and a crisp classification.In order to apply the F&M measure to the task of comparing a learned and a gold-standard taxonomy, we need to mitigate these problems.
Equation (8) copes with thethird problem without modifications.
In fact, if the sets of objects in H1 and H2 aredifferent, the integers n10 and n01 can be considered as also including objects that belongto one hierarchy and not to the other.
In this case, the value of B01,2 will provide a measureof the overlapping objects in the learned taxonomy and the gold standard one.
In orderto take into account multiple (rather than crisp) classifications, again, there is no needto change the formula, which is still meaningful if an object is allowed to belong tomore than one cluster.
As before, mismatches between H1 and H2 would result in highervalues of n10 and n01 and lower Bi1,2.A more serious problem with Equation (8) is that the lower the value of i, the higherthe value of the formula, whereas, ideally, we would like to reward similar clusteringswhen the clustering task is more difficult and fine-grained, that is, for cuts that are closeto the leaf nodes.
To assign a reward to ?early?
similarity values, we weight the valuesof Bi1,2 with a coefficienti+1k .
We can then compute a cumulative measure of similaritywith the following formula:B1,2 =?k?1i=0i+1k Bi1,2?k?1i=0i+1k=?k?1i=0i+1k Bi1,2k+12.
(9)Finally, to solve the problem of different depths of the two hierarchies, we define apolicy that penalizes a learned taxonomy that is less structured than the gold standardone, and rewards?or at least does not penalize?the opposite case.As an example, consider Figure 8, which shows two taxonomies H1 and H2, withnon-identical sets of objects {a, b, c, d, e, f} and {a, b, c, d, e, g}.
In the figure each edge islabeled by its distance from the root node (the value i in the F&M formula).
Notice thatH1 and H2 have multiple classifications (i.e., multiple hypernyms in our case) for theobject e, thus modeling the common problem of lexical ambiguity and polysemy.
Letus suppose that H1 is the learned taxonomy, and H2 the gold standard one.
We startcomparing the clusterings at cut 0 and stop at cut kr ?
1, where kr is the depth of theFigure 8Two hierarchical clusters of n non-identical objects.692Velardi, Faralli, and Navigli OntoLearn Reloadedgold standard taxonomy.
This means that if the learned taxonomy is less structuredwe replicate the cut kl ?
1 for kr ?
kl times (where kl is the maximum depth of thelearned taxonomy), whereas if it is more structured we stop at cut kr ?
1.
In contrast toprevious evaluation models, our aim is to reward (instead of penalize) more structuredtaxonomies provided they still match the gold standard one.Table 7 shows the cuts from 0 to 3 of H1 and H2 and the values of Bi1,2.
For i = 2 theB value is 0, if H2 is the learned taxonomy, and is not defined, if H2 is the gold standard.Therefore, when computing the cumulative Equation (9), we obtain the desired effect ofpenalizing less the structured learned taxonomies.
Note that, when the two hierarchieshave different depths, the value k ?
1 in Equation (9) is replaced by kr ?
1.Finally, we briefly compare our evaluation approach with the OntoRand index,introduced by Brank, Mladenic, and Grobelnik (2006).
The Rand Index measures thesimilarity between two clusterings Cl and Cr by the formula:R(Cl, Cr) =2(n11 + n00)n(n ?
1) (10)where n11, n00, and n have the same meaning described earlier.
In Brank, Mladenic,and Grobelnik (2006), a clustering is obtained from an ontology by associating eachontology instance to its concept.
The set of clusters is hence represented by the set ofleaf concepts in the hierarchy, namely, according to our notation, the clustering Ck?1i .
Inorder to take into account the hierarchical structure, they define the OntoRand formula.This measure, rather than summing up to 1 or 0, depending on whether or not twogiven instances i and j belong to the same cluster in the compared ontologies, returns areal number in [0, 1] depending upon the distance between i and j in terms of commonancestors.
In other terms, if i and j do not belong to the same concept but have a veryclose common ancestor, the OntoRand measure returns a value still close to 1.Our measure has several advantages over the OntoRand index:i) It allows for a comparison at different levels of depth of the hierarchy,and the cumulative similarity measure penalizes the contribution of thehighest cuts of the hierarchy.ii) It does not require that the two hierarchies have the same depth, nor thatthey have the same number of leaf nodes.iii) The measure can be extended to lattices (e.g., it is not required that eachobject belongs precisely to one cluster).Table 7Application of the evaluation method to the hierarchies of Figure 8.
The values of Bi1,2 are shownboth when H1 and H2 are the learned taxonomy (penultimate and last column, respectively).i C1 C2 n11 n10 n01 H1 H2Bi1,20 {a,b,c,d,e,f} {a,b,c,d,e,g} 10 5 5 0.67 0.671 {a,b,c,d,e},{e,f} {a,b,c,d,e},{e},{g} 10 1 0 0.95 0.952 {a,b},{c,d},{e},{f} {a},{b},{c},{d},{e},{g} 0 2 0 n.a.
03 {a},{b},{c},{d},{e},{f} {a},{b},{c},{d},{e},{g} 0 0 0 n.a.
n.a.693Computational Linguistics Volume 39, Number 3iv) It is not dependent, as the Rand Index is, on the number n00, the value ofwhich has the undesirable effect of producing an ever higher similarity asthe number of singleton clusters grows (Morey and Agresti 1984).4.2.2 Results.
This section presents the results of the F&M evaluation model for goldstandard evaluation, therefore we focus on four domains and do not consider AI andFINANCE.
The three WordNet sub-hierarchies are also compared with the taxonomiesautomatically created by Kozareva and Hovy (2010) in the same domains, kindly madeavailable by the authors.
It is once more to be noted that Kozareva and Hovy, during hy-pernym extraction, reject all the nodes not belonging to WordNet, whereas we assumeno a-priori knowledge of the domain, apart from adopting the same set of seed termsused by K&H.Figure 9 shows, for each domain (ANIMALS, PLANTS, VEHICLES, and VIRUSES), andfor each cut level of the hierarchy, a plot of Bi1,2 values multiplied by the penalty factor.As far as the comparison with K&H is concerned, we notice that, though K&H obtainbetter performance in general, OntoLearn has higher coverage over the domain, as isshown by the highest values for i = 0, and has a higher depth of the derived hierarchy,especially with DAG[0, 99].
Another recurrent phenomenon is that K&H curves grace-fully degrade from the root to the leaf nodes, possibly with a peak in the intermediatelevels, whereas OntoLearn has a hollow in the mid-high region (see the region 4?6 forANIMALS and 1?2 for the other three hierarchies) and often a relative peak in the lowestFigure 9Gold standard evaluation of our three versions of OntoLearn Reloaded against WordNet(ANIMALS, PLANTS, and VEHICLES) and MeSH (VIRUSES).
A comparison with K&H is alsoshown for the first three domains.694Velardi, Faralli, and Navigli OntoLearn Reloadedlevels.
In the manual evaluation section we explain this phenomenon, which also occursin the ARTIFICIAL INTELLIGENCE taxonomy.The generally decreasing values of Bi1,2 in Figure 9 show that, as expected, mim-icking the clustering criteria of a taxonomy created by a team of experts proves verydifficult at the lowest levels, while performance grows at the highest levels.
At thelowest taxonomy levels there are two opposing phenomena: overgeneralization andoverspecialization.
For example, macaque has monkey as a direct hypernym in WordNet,and we find short-tailed monkey as a direct hypernym of macaque.
An opposite case isganoid, which is a taleostan in WordNet and simply a fish in our taxonomy.
The firstcase does not reward the learned taxonomy (though, unlike for the overlapping factor[Maedche, Pekar, and Staab 2002], it does not cause a penalty), whereas the second isquite penalizing.
More of these examples will be provided in Section 4.3.Finally, in Table 8 we show the cumulative B1,2 values for the four domains, ac-cording to Equation (9).
Here, except for the VEHICLES domain, the unconstrainedDAG[0, 99] performs best.4.3 Manual EvaluationThis section is dedicated to the manual assessment of the learned ontologies.
Thesection is divided in three parts: Section 4.3.1 is concerned with the human validation ofhypernymy relations, Section 4.3.2 examines the global learned taxonomic structure inthe search for common phenomena across the six domains, and finally Section 4.3.3 in-vestigates the possibility of enhancing our hypernymy harvesting method with K&H?sHearst-like patterns, applying their method to the AI domain and manually evaluatingthe extracted hypernyms.4.3.1 Hypernym Evaluation.
To reduce subjectivity in taxonomy evaluation, we askedthree annotators, only one of whom was a co-author, to validate, for each of the threeexperiments of each of the six domains, a random sample of hypernymy relations.
Foreach relation the definition(s) supporting the relation were also provided.
This wasespecially helpful for domains like VIRUSES, but also PLANTS and ANIMALS, in whichthe annotators were not expert.
The size of each random sample was 300 for the (larger)AI and FINANCE domains and 100 for the others.Each evaluator was asked to tag incorrect relations, regardless of whether the errorwas due to the selection of non-domain definitions (e.g., for VEHICLES: ?a driver is agolf club with a near vertical face that is used for hitting long shots from the tee?
), toa poor definition (e.g., for AI: ?a principle is a fundamental essence, particularly oneproducing a given quality?)
or to a wrong selection of the hypernym.
As an example ofthe latter, in the PLANTS domain, we extracted the hypernym species from the sentence:?geranium is a genus of 422 species of flowering annual, biennial, and perennial plantsTable 8Values of B1,2 for the domains of VIRUSES, ANIMALS, PLANTS, and VEHICLES.Experiment VIRUSES ANIMALS PLANTS VEHICLESTREE 0.093 0.064 0.059 0.065DAG [1,3] 0.101 0.062 0.072 0.069DAG [0,99] 0.115 0.097 0.080 0.103K&H n.a.
0.067 0.068 0.158695Computational Linguistics Volume 39, Number 3Table 9Precision of hypernym edges on six domains (calculated on a majority basis) and inter-annotatoragreement on the corresponding sample of relations.Experiment # of Sample Precision ?AITREE 300 80.3% [241/300] 0.45DAG [1,3] 300 73.6% [221/300] 0.42DAG [0,99] 300 73.0% [219/300] 0.41FINANCETREE 300 93.6% [281/300] 0.40DAG [1,3] 300 93.0% [279/300] 0.43DAG [0,99] 300 92.6% [278/300] 0.41VIRUSESTREE 100 99.0% [99/100] 0.49DAG [1,3] 100 99.0% [99/100] 0.39DAG [0,99] 100 99.0% [99/100] 0.32ANIMALSTREE 100 92.0% [92/100] 0.53DAG [1,3] 100 92.0% [92/100] 0.36DAG [0,99] 100 90.0% [90/100] 0.56PLANTSTREE 100 89.0% [89/100] 0.49DAG [1,3] 100 85.0% [85/100] 0.53DAG [0,99] 100 97.0% [97/100] 0.26VEHICLESTREE 100 92.0% [92/100] 0.64DAG [1,3] 100 92.0% [92/100] 0.49DAG [0,99] 100 91.0% [91/100] 0.44?
Interpretation< 0 Poor agreement0.01?0.20 Slight agreement0.21?0.40 Fair agreement0.41?0.60 Moderate agreement0.61?0.80 Substantial agreement0.81?1.00 Almost perfect agreementthat are commonly known as the cranesbills?
since, in the WCL verb set, we have ?isa * species of?
and ?is a * genus of?, but not the concatenation of these two patterns.Annotators could mark with ?
a hyponym?hypernym pair for which they felt uncertain.Though it would have been useful to distinguish between the different types of error,we found that regarding many error types there was, anyway, very low inter-annotatoragreement.
Indeed the annotation task would appear to be intrinsically complex andcontroversial.
In any case, an assessment of the definition and hypernym extractiontasks in isolation was already provided by Navigli and Velardi (2010).Table 9 summarizes the results.
Precision of each classification was computed on amajority basis, and we used Fleiss?
kappa statistics (Fleiss 1971) to measure the inter-annotator agreement.
In general, the precision is rather good, though it is lower for theAI domain, probably due to its high ?vitality?
(many new terms continuously arise, andfor some of them it is difficult to find good quality definitions).
In general, precision ishigher in focused domains (VIRUSES, ANIMALS, PLANTS, and VEHICLES) than in wide-range domains (AI and FINANCE).
The former domains, however, have just one quite696Velardi, Faralli, and Navigli OntoLearn Reloaded?narrow?
upper concept (virus for VIRUSES, etc.
), whereas AI and FINANCE have severalupper concepts (e.g., person or abstraction), and furthermore they are less focused.
Thismeans that there is an inherently higher ambiguity and this may be seen as justifyingthe lower performance.
In Table 9 we also note that TREE structures achieve in general ahigher precision, except for PLANTS, whereas the DAG has the advantage of improvingrecall (see also Section 4.2.2).Note that high precision here does not contradict the results shown in Section 4.2.2:In this case, each single relation is evaluated in isolation, therefore overgenerality oroverspecificity do not imply a penalty, provided the relation is judged to be correct.Furthermore, global consistency is not considered here: for example, distance metriclearning ?
parametric technique, and eventually ends up in technique, whereas beliefnetwork learning ?
machine learning algorithm ends up in algorithm and then in procedure.In isolation, these hypernymy patterns are acceptable, but within a taxonomic structureone would like to see a category node grouping all terms denoting machine learningalgorithms.
This behavior should be favored by the node weighting strategy describedin Section 3.4, aimed at attracting nodes with multiple hypernyms towards the mostpopulated category nodes.
As in the previous example, however, there are categorynodes that are almost equally ?attractive?
(e.g., algorithm and technique), and, further-more, the taxonomy induction algorithm can only select among the set of hypernymsextracted during the harvesting phase.
Consequently, when no definition suggests thatdistance metric learning is a hyponym of machine learning algorithm, or of any otherconcept connected to machine learning algorithm, there is no way of grouping distancemetric learning and belief network learning in the desired way.
This task must be postponedto manual post-editing.Concerning the kappa statistics, we note that the values range from moderate tosubstantial in most cases.
These numbers are apparently low, but the task of evaluatinghypernymy relations is quite a complex one.
Similar kappa values were obtained inYang and Callan (2008) in a human-guided ontology learning task.4.3.2 Structural Assessment.
In addition to the manual evaluation summarized inTable 9, a structural assessment was performed to identify the main sources of error.To this end, one of the authors analyzed the full AI and FINANCE taxonomies and asample of the other four domains in search of recurring errors.
In general, our optimalbranching algorithm and weighting schema avoids many of the problems highlighted inwell-known studies on taxonomy acquisition from dictionaries (Ide and Ve?ronis 1993),like circularity, over-generalization, and so forth.
There are new problems to be faced,however.The main sources of error are the following: Ambiguity of terms, especially at the intermediate levels Low quality of definitions Hypernyms described by a clause rather than by a single- or multi-wordexpression Lack of an appropriate WCL to analyze the definition Difficulty of extracting the correct hypernym string from phrases withidentical syntactic structureWe now provide examples for each of these cases.697Computational Linguistics Volume 39, Number 3Figure 10Error distribution of the TREE version of our algorithm on the ARTIFICIAL INTELLIGENCEdomain.Ambiguity.
Concerning ambiguity of terms, consider Figures 10 and 11, which show thedistribution of errors at the different levels of the learned AI and FINANCE taxonomiesfor the TREE experiment.
The figures provide strong evidence that most errors arelocated in the intermediate levels of the taxonomy.
As we move from leaf nodes tothe upper ontology, the extracted terms become progressively more general and con-sequently more ambiguous.
For these terms the domain heuristics may turn out to beinadequate, especially if the definition is a short sentence.But why are these errors frequent at the intermediate levels and not at the highestlevels?
To understand this, consider the following example from the AI domain: Forthe term classifier the wrong hypernym is selected from the sentence ?classifier is aperson who creates classifications.?
In many cases, wrong hypernyms do not accumulatesufficient weight and create ?dead-end?
hypernymy chains, which are pruned duringthe optimal branching step.
But, unfortunately, a domain appropriate definition isFigure 11Error distribution of the TREE version of our algorithm on the FINANCE domain.698Velardi, Faralli, and Navigli OntoLearn Reloadedfound for person: ?person is the more general category of an individual,?
due to thepresence of the domain word category.
On the other hand, this new sentence producesan attachment that, in a sense, recovers the error, because category is a ?good?
domainconcept that eventually ends up in subsequent iterations to the upper node abstraction.Therefore, what happens is that the upper taxonomy nodes, with the help of the domainheuristic, mitigate the ?semantic drift?
caused by out-of-domain ambiguity, recoveringthe ambiguity errors of the intermediate levels.
This phenomenon is consistently foundin all domains, as shown by the hollow that we noticed in the graphs of Section 4.2.2.An example in the ANIMALS domain is represented by the hypernymy sequencefawn ?
color ?
race ?
breed ?
domestic animal, where the wrong hypernym color wasoriginated by the sentence ?fawn is a light yellowish brown color that is usually used inreference to a dog?s coat color.?
Only in VIRUSES is the phenomenon mitigated by thehighly specific and very focused nature of the domain.In addition to out-of-domain ambiguity, we have two other phenomena: in-domainambiguity and polysemy.
In-domain ambiguity is rare, but not absent (Agirre et al2010; Faralli and Navigli 2012).
Consider the example of Figure 12a, from the VEHICLESdomain: tractor has two definitions corresponding to two meanings, which are bothcorrect.
The airplane meaning is ?tractor is an airplane where the propeller is located infront of the fuselage,?
whereas the truck meaning is ?tractor is a truck for pulling a semi-trailer or trailer.?
Here the three hyponyms of tractor (see the figure) all belong to thetruck sense.
We leave to future developments the task of splitting in-domain ambiguousnodes in the appropriate way.Another case is systematic polysemy, which is shown in Figure 13.
The graph inthe figure, from the AI domain, captures the fact that a semantic network, as well asits hyponyms, are both a methodology and a representation.
Another example is shownin Figure 12b for the PLANTS domain, where systematic polysemy can be observedfor terms like olive, orange, and breadfruit, which are classified as evergreen tree andfruit.
Polysemy, however, does not cause errors, as it does for in-domain ambiguity,because hyponyms of polysemous concepts inherit the polysemy: In the two graphsof Figures 13 and 12b, both partitioned semantic network and tangerine preserve thepolysemy of their ancestors.
Note that in-domain ambiguity and polysemy are onlycaptured by the DAG structure; therefore this can be seen as a further advantage (inaddition to higher recall) of the DAG model over and against the more precise TREEstructure.Figure 12An example of in-domain ambiguity (a) and an example of systematic polysemy (b).
Dashededges were added to the graph as a result of the edge recovery phase (see Section 3.5).699Computational Linguistics Volume 39, Number 3Figure 13An example of systematic polysemy.
Dashed edges were added to the graph as a result of theedge recovery phase (see Section 3.5).Low quality of definitions.
Often textual definitions, especially if extracted from theWeb, do not have a high quality.
Examples are: ?artificial intelligence is the next bigdevelopment in computing?
or ?aspectual classification is also a necessary prerequi-site for interpreting certain adverbial adjuncts.?
These sentences are definitions on asyntactic ground, but not on a semantic ground.
As will be shown in Section 4.3.3,this problem is much less pervasive than for Hearst-like lexico-syntactic patterns,although, neither domain heuristics nor the graph pruning could completely eliminatethe problem.
We can also include overgeneralization in this category of problems: Ouralgorithm prefers specific hypernyms to general hypernyms, but for certain terms nospecific definitions are found.
The elective way to solve this problem would be to assigna quality confidence score to the definition source (document or Web page), for example,by performing an accurate and stable classification of its genre (Petrenz and Webber2011).Hypernym is a clause.
There are cases in which, although very descriptive and goodquality definitions are found, it is not possible to summarize the hypernym with aterm or multi-word expression.
For example ?anaphora resolution is the process ofdetermining whether two expressions in natural language refer to the same real worldentity.?
OntoLearn extracts process of determining which ends up in procedure, process.This is not completely wrong, however, and in some case is even fully acceptable, asfor ?summarizing is a process of condensing or expressing in short something youhave read, watched or heard?
: here, process of condensing is an acceptable hypernym.An example for FINANCE is: ?market-to-book ratio is book value of assets minus bookvalue of equity plus market value of equity,?
where we extracted book value, rather thanthe complete formula.
Another example is: ?roa is defined as a ratio of operating incometo book value of assets,?
from which we extracted ratio, which is, instead, acceptable.Lack of an appropriate definitional pattern.
Though we acquired hundreds of differentdefinitional patterns, there are still definitions that are not correctly parsed.
We already700Velardi, Faralli, and Navigli OntoLearn Reloadedmentioned the geranium example in the PLANTS domain.
An example in the AI domainis ?execution monitoring is the robot?s process of observing the world for discrepanciesbetween the actual world and its internal representation of it,?
where the extractedhypernym is robot because we have no WCL with a Saxon genitive.Wrong hypernym string.
This is the case in which the hypernym is a superstring orsubstring of the correct one, like: ?latent semantic analysis is a machine learning proce-dure.?
Here, the correct hypernym is machine learning procedure, but OntoLearn extractsmachine because learning is POS tagged as a verb.
In general, it is not possible to evaluatethe extent of the hypernym phrase except case-by-case.
The lattice learner acquired avariety of hypernymy patterns, but the precision of certain patterns might be quite low.For example, the hypernymy pattern ?
* of *?
is acceptable for ?In grammar, a lexicalcategory is a linguistic category of words?
or ?page rank is a measure of site popularity?but not for ?page rank is only a factor of the amount of incoming and outgoing linksto your site?
nor for ?pattern recognition is an artificial intelligence area of considerableimportance.?
The same applies to the hypernymy pattern ADJ NN: important algorithm iswrong, although greedy algorithm is correct.4.3.3 Evaluation of Lexico-Syntactic Patterns.
As previously remarked, Kozareva and Hovy(2010) do not actually apply their algorithm to the task of creating a new taxonomy, butrather they try to reproduce three WordNet taxonomies, under the assumption that thetaxonomy nodes are known (cf.
Section 4).
Therefore, there is no evidence of the preci-sion of their method on new domains, where the category nodes are unknown.
On theother hand, if Hearst?s patterns, which are at the basis of K&H?s hypernymy harvestingalgorithm, could show adequate precision, we would use them in combination withour definitional patterns.
This section investigates the matter.As briefly summarized in Section 2, K&H create a hypernym graph in three steps.Given a few root concepts (e.g., animal) and basic level concepts or instances (e.g.,lion), they:1) harvest new basic and intermediate concepts from the Web in an iterativefashion, using doubly anchored patterns (DAP) like ??root?
such as ?seed?and ??
and inverse DAP (i.e., DAP?1) like ??
such as ?term1?
and ?term2?
?.The procedure is iterated until no new terms can be found;2) rank the nodes extracted with DAP by out-degree and those extractedwith inverse DAP by in-degree, so as to prune out less promising terms;3) induce the final taxonomic structure by positioning the intermediate nodesbetween basic level and root terms using a concept positioning procedurebased on a variety of Hearst-like surface patterns.
Finally, they eliminatecycles, as well as nodes with no predecessor or no successor, and theyselect the longest path in the case of multiple paths between node pairs.In this section we apply their method23 to the domain of AI in order to manuallyanalyze the quality of the extracted relations.
To replicate the first two steps of K&Halgorithm we fed the algorithm with a growing set of seed terms randomly selectedfrom our validated terminology, together with their hierarchically related root terms23 We followed the exact procedure described in Figure 2 of Kozareva & Hovy (2010).701Computational Linguistics Volume 39, Number 3Table 10K&H performance on the AI domain.number of root/seed pairs 1 10 100 1,000# new concepts 131 163 227 247# extracted is-a relations 114 146 217 237correct and in-domain 21.05% 24.65% 18.89% 18.56%(24/114) (36/146) (41/217) (44/237)in the upper taxonomy (e.g., unsupervised learning is a method or maximum entropy is ameasure).
We then performed the DAP and DAP?1 steps iteratively until no more termscould be retrieved, and we manually evaluated the quality of the harvested conceptsand taxonomic relations using the same thresholding formula described in K&H.24 Wegive the results in Table 10.As we said earlier, our purpose here is mainly to evaluate the quality of Hearstpatterns in more technical domains, and the efficacy of DAP and DAP?1 steps inretrieving domain concepts and relations.
Therefore, replicating step (3) above is notuseful in this case since, rather than adding new nodes, step (3) is aimed, as in ouroptimal branching and pruning recovery steps, at reorganizing and trimming the finalgraph.Table 10 should be compared with the first three rows (AI) of Table 9: It shows thatin the absence of a priori knowledge on the domain concepts the quantity and qualityof the is-a links extracted by the K&H algorithm is much lower than those extracted byOntoLearn Reloaded.
First, the number of new nodes found by the K&H algorithm isquite low: For the same domain of ARTIFICIAL INTELLIGENCE, our method, as shown inTable 9, is able to extract from scratch 2,387 ?
52 = 2,335 nodes,25 in comparison with the247 new nodes of Table 10, obtained with 1,000 seeds.
Second, many nodes extracted bythe K&H algorithm, like fan speed, guidelines, chemical engineering, and so on, are out-of-domain and many hypernym relations are incorrect irrespective of their direction, likecomputer program is a slow and data mining is a contemporary computing problem.
Third, thevast majority of the retrieved hypernyms are overgeneral, like discipline, method, area,problem, technique, topic, and so forth, resulting in an almost flat hypernymy structure.
Ahigh in-degree threshold and a very high number of seeds do not mitigate the problem,demonstrating that Hearst-like patterns are not very good at harvesting many validhypernym relations in specialized domains.26Following this evaluation, we can outline several advantages of our method overK&H?s work (and, as a consequence, over Hearst?s patterns):i) We obtain higher precision and recall when no a priori knowledge isavailable on the taxonomy concepts, because hypernyms are extractedfrom expert knowledge on the Web (i.e., technical definitions rather thanpatterns reflecting everyday language).24 The technique is based on the in-degree and out-degree of the graph nodes.25 Remember that the 52 domain-independent upper terms are manually defined (cf.
Section 4.1.4).26 This result is in line with previous findings in a larger, domain-balanced experiment (Navigli and Velardi2010) in which we have shown that WCLs outperform Hearst patterns and other methods in the task ofhypernym extraction.702Velardi, Faralli, and Navigli OntoLearn Reloadedii) We cope better with sense ambiguity via the domain filtering step.27iii) We use a principled algorithmic approach to graph pruning and cycleremoval.28iv) Thanks to the support provided by textual definitions, we are able tocope with the problem of manually evaluating the retrieved conceptsand relations, even in the absence of a reference taxonomy.4.3.4 Summary of Findings.
We here summarize the main findings of our manifoldevaluation experiments:i) With regard to the two versions of our graph pruning algorithm, we foundthat TREE structures are more precise, whereas DAGs have a higher recall.ii) Errors are mostly concentrated in the mid-level of the hierarchy, whereconcepts are more ambiguous and the ?attractive?
power of top nodes isless influential.
This was highlighted by our quantitative (F&M) modeland justified by means of manual analysis.iii) The quality and number of definitions is critical for high performance.Less-focused domains in which new terms continuously emerge are themost complex ones, because it is more difficult to retrieve high-qualitydefinitions for them.iv) Definitions, on the other hand, are a much more precise and high-coveragesource of knowledge for hypernym extraction than (Hearst-like) patternsor contexts, because they explicitly represent expert knowledge on agiven domain.
Furthermore, they are a very useful support for manualvalidation and structural analysis.5.
ConclusionsIn this paper we presented OntoLearn Reloaded, a graph-based algorithm for learninga taxonomy from scratch using highly dense, potentially disconnected, hypernymygraphs.
The algorithm performs the task of eliminating noise from the initial graphremarkably well on arbitrary, possibly specialized, domains, using a weighting schemethat draws both on the topological properties of the graph and on some general prin-ciples of taxonomic structures.
OntoLearn Reloaded provides a considerable advance-ment over the state of the art in taxonomy learning.
First, it is the first algorithm thatexperimentally demonstrates its ability to build a new taxonomy from the ground up,without any a priori assumption on the domain except for a corpus and a set of (possiblygeneral) upper terms.
The majority of existing systems start from a set of conceptsand induce hypernymy links between these concepts.
Instead, we automatically learnboth concepts and relations via term extraction and iterative definition and hypernym27 In the authors?
words (Kozareva and Hovy 2010, page 1,115): ?we found that the learned terms in themiddle ranking do not refer to the meaning of vehicle as a transportation device, but to the meaning ofvehicle as media (i.e., seminar, newspapers), communication and marketing.
?28 Again in the authors?
words (Kozareva and Hovy 2010, page 1,115): ?we found that in-degree is notsufficient by itself.
For example, highly frequent but irrelevant hypernyms such as meats and others areranked at the top of the list, while low frequent but relevant ones such as protochordates, hooved-mammals,homeotherms are discarded.
?703Computational Linguistics Volume 39, Number 3extraction.
Second, we cope with issues such as term ambiguity, complexity, andmultiplicity of hypernymy patterns.
Third, we contribute a multi-faceted evaluation,which includes a comparison against gold standards, plus a structural and a manualevaluation.
Taxonomy induction was applied to the task of creating new ARTIFICIALINTELLIGENCE and FINANCE taxonomies and four taxonomies for gold-standardcomparison against WordNet and MeSH.29Our experimental analysis shows that OntoLearn Reloaded greatly simplifies thetask of acquiring a taxonomy from scratch: Using a taxonomy validation tool,30 a teamof experts can correct the errors and create a much more acceptable taxonomy in amatter of hours, rather than man-months, also thanks to the automatic acquisition oftextual definitions for our concepts.
As with any automated and unsupervised learningtool, however, OntoLearn does make errors, as we discussed in Section 4.
The accuracyof the resulting taxonomy is clearly related to the number and quality of discovereddefinitional patterns, which is in turn related to the maturity and generality of a domain.Even with good definitions, problems might arise due to in- and out-domain ambiguity,the latter being probably the major source of errors, together with complex definitionalstructures.
Although we believe that there is still room for improvement to OntoLearnReloaded, certain errors would appear unavoidable, especially for less focused andrelatively dynamic domains like ARTIFICIAL INTELLIGENCE and FINANCE, in whichnew terms arise continuously and have very few, or no definitions on the Web.Future work includes the addition of non-taxonomical relations along the lines ofReVerb (Etzioni et al2011) and WiSeNet (Moro and Navigli 2012), and a more sophis-ticated rank-based method for scoring textual definitions.
Finally, we plan to tacklethe issue of automatically discriminating between in-domain ambiguity and systematicpolysemy (as discussed in Section 4.3.2).AcknowledgmentsStefano Faralli and Roberto Navigligratefully acknowledge the support of theERC Starting Grant MultiJEDI No.
259234.The authors wish to thank Jim McManus forhis valuable comments on the paper, andZornitsa Kozareva and Eduard Hovy formaking their data available.ReferencesAgirre, Eneko, Oier Lo?pez de Lacalle,Christiane Fellbaum, Shu-Kai Hsieh,Maurizio Tesconi, Monica Monachini,Piek Vossen, and Roxanne Segers.2010.
SemEval-2010 Task 17: All-wordsWord Sense Disambiguation on aspecific domain.
In Proceedings of the5th International Workshop on SemanticEvaluation (SemEval-2010), pages 75?80,Uppsala.Berland, Matthew and Eugene Charniak.1999.
Finding parts in very largecorpora.
In Proceedings of the 27th AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 57?64, CollegePark, MD.Biemann, Chris.
2005.
Ontology learningfrom text?A survey of methods.LDV-Forum, 20(2):75?93.Brank, Janez, Dunja Mladenic, andMarko Grobelnik.
2006.
Gold standardbased ontology evaluation using instanceassignment.
In Proceedings of 4th WorkshopEvaluating Ontologies for the Web (EON),Edinburgh.Carpineto, Claudio and Giovanni Romano.2012.
Consensus Clustering Based ona New Probabilistic Rand Index withApplication to Subtopic Retrieval.IEEE Transactions on Pattern Analysis andMachine Intelligence, 34(12):2315?2326.Chu, Yoeng-Jin and Tseng-Hong Liu.1965.
On the shortest arborescenceof a directed graph.
Science Sinica,14:1396?1400.Cimiano, Philipp, Andreas Hotho, andSteffen Staab.
2005.
Learning concept29 Data sets are available at: http://lcl.uniroma1.it/ontolearn reloaded.30 For example, http://lcl.uniroma1.it/tav/.704Velardi, Faralli, and Navigli OntoLearn Reloadedhierarchies from text corpora usingformal concept analysis.
Journal ofArtificial Intelligence Research,24(1):305?339.Cohen, Trevor and Dominic Widdows.2009.
Empirical distributional semantics:Methods and biomedical applications.Journal of Biomedical Informatics,42(2):390?405.Cormen, Thomas H., Charles E. Leiserson,and Ronald L. Rivest.
1990.
Introductionto Algorithms.
MIT Electrical Engineeringand Computer Science.
MIT Press,Cambridge, MA.De Benedictis, Flavio, Stefano Faralli, andRoberto Navigli.
2013.
GlossBoot:Bootstrapping Multilingual DomainGlossaries from the Web.
In Proceedingsof the 51st Annual Meeting of theAssociation for Computational Linguistics(ACL), Sofia.De Nicola, Antonio, Michele Missikoff,and Roberto Navigli.
2009.
A softwareengineering approach to ontologybuilding.
Information Systems,34(2):258?275.Edmonds, Jack.
1967.
Optimum branchings.Journal of Research of the National Bureau ofStandards, 71B:233?240.Etzioni, Oren, Anthony Fader, JanaraChristensen, Stephen Soderland, andMausam.
2011.
Open informationextraction: The second generation.
InProceedings of the 22nd International JointConference on Artificial Intelligence (IJCAI),pages 3?10, Barcelona.Fahmi, Ismail and Gosse Bouma.
2006.Learning to identify definitions usingsyntactic features.
In Proceedings ofthe EACL 2006 workshop on LearningStructured Information in NaturalLanguage Applications, pages 64?71,Trento.Faralli, Stefano and Roberto Navigli.2012.
A new minimally supervisedframework for domain Word SenseDisambiguation.
In Proceedings ofthe 2012 Joint Conference on EmpiricalMethods in Natural Language Processingand Computational Natural LanguageLearning (EMNLP-CoNLL),pages 1,411?1,422, Jeju.Fellbaum, Christiane, editor.
1998.
WordNet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.Fleiss, Joseph L. 1971.
Measuringnominal scale agreement amongmany raters.
Psychological Bulletin,76(5):378?382.Fountain, Trevor and Mirella Lapata.
2012.Taxonomy induction using hierarchicalrandom graphs.
In Proceedings of the NorthAmerican Chapter of the Association forComputational Linguistics: Human LanguageTechnologies (HLT-NAACL), pages 466?476,Montre?al.Fowlkes, Edward B. and Colin L. Mallows.1983.
A method for comparing twohierarchical clusterings.
Journal of theAmerican Statistical Association,78(383):553?569.Girju, Roxana, Adriana Badulescu, andDan Moldovan.
2006.
Automatic discoveryof part-whole relations.
ComputationalLinguistics, 32(1):83?135.Gomez-Perez, Asuncio?n and DavidManzano-Mancho.
2003.
A survey ofontology learning methods andtechniques.
OntoWeb Delieverable 1.5.Universidad Polite?cnica de Madrid.Guarino, Nicola and Chris Welty.
2002.Evaluating ontological decisions withOntoClean.
Communications of the ACM,45(2):61?65.Hearst, Marti A.
1992.
Automatic acquisitionof hyponyms from large text corpora.In Proceedings of the 14th InternationalConference on Computational Linguistics(COLING), pages 539?545, Nantes.Hovy, Eduard, Andrew Philpot,Judith Klavans, Ulrich Germann, andPeter T. Davis.
2003.
Extending metadatadefinitions by automatically extractingand organizing glossary definitions.
InProceedings of the 2003 Annual NationalConference on Digital Government Research,pages 1?6, Boston, MA.Ide, Nancy and Jean Ve?ronis.
1993.Extracting knowledge bases frommachine-readable dictionaries: Havewe wasted our time?
In Proceedingsof the Workshop on Knowledge Bases andKnowledge Structures, pages 257?266,Tokyo.Kozareva, Zornitsa and Eduard Hovy.
2010.A semi-supervised method to learn andconstruct taxonomies using the Web.In Proceedings of the 2010 Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 1,110?1,118,Cambridge, MA.Kozareva, Zornitsa, Ellen Riloff, andEduard Hovy.
2008.
Semantic classlearning from the Web with hyponympattern linkage graphs.
In Proceedingsof the 46th Annual Meeting of theAssociation for Computational Linguistics(ACL), pages 1,048?1,056, Columbus, OH.705Computational Linguistics Volume 39, Number 3Maedche, Alexander, Viktor Pekar, andSteffen Staab.
2002.
Ontology learningpart one?on discovering taxonomicrelations from the Web.
In N. Zhong,J.
Liu, and Y. Y. Yao, editors, WebIntelligence.
Springer Verlag, Berlin,pages 301?322.Maedche, Alexander and Steffen Staab.2009.
Ontology learning.
In Steffen Staaband Rudi Studer, editors, Handbook onOntologies.
Springer, Berlin, pages 245?268.Miller, George A., R. T. Beckwith,Christiane D. Fellbaum, D. Gross, andK.
Miller.
1990.
WordNet: An onlinelexical database.
International Journal ofLexicography, 3(4):235?244.Morey, Leslie C. and Alan Agresti.
1984.
Themeasurement of classification agreement:An adjustment to the Rand statistic forchance agreement.
Educational andPsychological Measurement, 44:33?37.Moro, Andrea and Roberto Navigli.
2012.WiSeNet: Building a Wikipedia-basedsemantic network with ontologizedrelations.
In Proceedings of the 21stACM Conference on Information andKnowledge Management (CIKM 2012),pages 1,672?1,676, Maui, HI.Navigli, Roberto.
2009.
Word SenseDisambiguation: A survey.
ACMComputing Surveys, 41(2):1?69.Navigli, Roberto, and Simone PaoloPonzetto.
2012.
BabelNet: The automaticconstruction, evaluation and application ofa wide-coverage multilingual semanticnetwork.
Artificial Intelligence 193,pp.
217?250.Navigli, Roberto and Paola Velardi.
2004.Learning domain ontologies fromdocument warehouses and dedicatedwebsites.
Computational Linguistics,30(2):151?179.Navigli, Roberto and Paola Velardi.
2005.Structural semantic interconnections:A knowledge-based approach to WordSense Disambiguation.
IEEE Transactionson Pattern Analysis and Machine Intelligence,27(7):1075?1088.Navigli, Roberto and Paola Velardi.
2010.Learning Word-Class Lattices fordefinition and hypernym extraction.In Proceedings of the 48th Annual Meetingof the Association for ComputationalLinguistics (ACL), pages 1,318?1,327,Uppsala.Navigli, Roberto, Paola Velardi, and StefanoFaralli.
2011.
A graph-based algorithm forinducing lexical taxonomies from scratch.In Proceedings of the 22nd International JointConference on Artificial Intelligence (IJCAI),pages 1,872?1,877, Barcelona.Newman, Mark E. J.
2010.
Networks: AnIntroduction.
Oxford University Press.Pado, Sebastian and Mirella Lapata.
2007.Dependency-based construction ofsemantic space models.
ComputationalLinguistics, 33(2):161?199.Pantel, Patrick and Marco Pennacchiotti.2006.
Espresso: Leveraging genericpatterns for automatically harvestingsemantic relations.
In Proceedings of44th Annual Meeting of the Association forComputational Linguistics joint with 21stConference on Computational Linguistics(COLING-ACL), pages 113?120, Sydney.Pasca, Marius.
2004.
Acquisition ofcategorized named entities for web search.In Proceedings of the 13th ACM InternationalConference on Information and KnowledgeManagement (CIKM), pages 137?145,Washington, DC.Petasis, Georgios, Vangelis Karkaletsis,Georgios Paliouras, Anastasia Krithara,and Elias Zavitsanos.
2011.
Ontologypopulation and enrichment: State of theart.
In Georgios Paliouras, ConstantineSpyropoulos, and George Tsatsaronis,editors, Knowledge-Driven MultimediaInformation Extraction and OntologyEvolution, volume 6050 of Lecture Notesin Computer Science.
Springer, Berlin /Heidelberg, pages 134?166.Petrenz, Philipp and Bonnie L. Webber.2011.
Stable classification of text genres.Computational Linguistics, 37(2):385?393.Ponzetto, Simone Paolo and Roberto Navigli.2009.
Large-scale taxonomy mapping forrestructuring and integrating Wikipedia.In Proceedings of the 21st International JointConference on Artificial Intelligence (IJCAI),pages 2,083?2,088, Pasadena, CA.Ponzetto, Simone Paolo and Michael Strube.2011.
Taxonomy induction based on acollaboratively built knowledge repository.Artificial Intelligence, 175:1737?1756.Poon, Hoifung and Pedro Domingos.
2010.Unsupervised ontology induction fromtext.
In Proceedings of the 48th AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 296?305, Uppsala.Rand, William M. 1971.
Objective criteria forthe evaluation of clustering methods.Journal of the American StatisticalAssociation, 66(336):846?850.Schmid, Helmut.
1995.
Improvements inpart-of-speech tagging with an applicationto German.
In Proceedings of the ACLSIGDAT-Workshop, pages 47?50, Dublin.706Velardi, Faralli, and Navigli OntoLearn ReloadedSclano, Francesco and Paola Velardi.
2007.TermExtractor: A Web application tolearn the shared terminology of emergentWeb communities.
In Proceedings of the 3thInternational Conference on Interoperabilityfor Enterprise Software and Applications(I-ESA), pages 287?290, Funchal.Snow, Rion, Dan Jurafsky, and Andrew Ng.2006.
Semantic taxonomy induction fromheterogeneous evidence.
In Proceedings of44th Annual Meeting of the Association forComputational Linguistics joint with 21stConference on Computational Linguistics(COLING-ACL), pages 801?808, Sydney.Sowa, John F. 2000.
Knowledge Representation:Logical, Philosophical, and ComputationalFoundations.
Brooks Cole Publishing Co.,Pacific Grove, CA.Storrer, Angelika and Sandra Wellinghoff.2006.
Automated detection and annotationof term definitions in German text corpora.In Proceedings of the 5th InternationalConference on Language Resources andEvaluation (LREC), pages 2,373?2,376,Genova.Suchanek, Fabian M., Gjergji Kasneci,and Gerhard Weikum.
2008.
YAGO:A large ontology from Wikipedia andWordNet.
Journal of Web Semantics,6(3):203?217.Tang, Jie, Ho Fung Leung, Qiong Luo,Dewei Chen, and Jibin Gong.
2009.Towards ontology learning fromfolksonomies.
In Proceedings of the21st International Joint Conference onArtificial Intelligence (IJCAI),pages 2,089?2,094, Pasadena, CA.Tarjan, Robert Endre.
1977.
Finding optimumbranchings.
Networks, 7(1):25?35.Velardi, Paola, Roberto Navigli, and PierluigiD?Amadio.
2008.
Mining the Web to createspecialized glossaries.
IEEE IntelligentSystems, 23(5):18?25.Vo?lker, Johanna, Denny Vrandec?ic?,York Sure, and Andreas Hotho.
2008.AEON?An approach to the automaticevaluation of ontologies.
Journal ofApplied Ontology, 3(1-2):41?62.Vossen, Piek.
2001.
Extending, trimmingand fusing WordNet for technicaldocuments.
In Proceedings of the NorthAmerican Chapter of the Associationfor Computational Linguistics Workshopon WordNet and Other LexicalResources: Applications, Extensionsand Customizations (NAACL),pages 125?131, Pittsburgh, PA.Wagner, Silke and Dorothea Wagner.
2007.Comparing clusterings: An overview.Technical Report 2006-04, Faculty ofInformatics, Universita?t Karlsruhe (TH).Westerhout, Eline.
2009.
Definition extractionusing linguistic and structural features.In Proceedings of the RANLP Workshopon Definition Extraction, pages 61?67,Borovets.Yang, Hui and Jamie Callan.
2008.Human-guided ontology learning.In Proceedings of Human-ComputerInteraction and Information Retrieval(HCIR), pages 26?29, Redmond, WA.Yang, Hui and Jamie Callan.
2009.
Ametric-based framework for automatictaxonomy induction.
In Proceedings ofthe 47th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 271?279, Suntec.Zavitsanos, Elias, Georgios Paliouras,and George A. Vouros.
2011.
Goldstandard evaluation of ontology learningmethods through ontology transformationand alignment.
IEEE Transactions onKnowledge and Data Engineering,23(11):1635?1648.Zhang, Chunxia and Peng Jiang.
2009.Automatic extraction of definitions.In Proceedings of 2nd IEEE InternationalConference on Computer Science andInformation Technology (ICCSIT),pages 364?368, Beijing.707
