Proceedings of the 12th European Workshop on Natural Language Generation, pages 110?113,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsA Japanese corpus of referring expressions used in a situatedcollaboration taskPhilipp Spanger Yasuhara Masaaki Iida Ryu Tokunaga TakenobuDepartment of Computer ScienceTokyo Institute of Technology{philipp, yasuhara, ryu-i, take}@cl.cs.titech.ac.jpAbstractIn order to pursue research on generatingreferring expressions in a situated collab-oration task, we set up a data-collectionexperiment based on the Tangram puzzle.For a pair of participants we recorded ev-ery utterance in synchronisation with thecurrent state of the puzzle as well as alloperations by the participants.
Referringexpressions were annotated with their ref-erents in order to build a referring expres-sion corpus in Japanese.
We provide pre-liminary results on the analysis of the cor-pus from various standpoints, focussing onaction-mentioning expressions.1 IntroductionReferring expressions are a linguistic device to re-fer to a certain object, enabling smooth collabo-ration between humans and agents where physicaloperations are involved.
Previous research ofteneither selectively focussed only on a limited num-ber of expression-types or set up overly controlledexperiments.
In contrast, we intend to work to-wards analysing the whole breadth of referring ex-pressions in a situated domain.
For this purposewe created a corpus (in Japanese) and analysed itfrom various standpoints.From very early on in referring expression re-search, there has been some interest in the col-laborative aspect of the reference process (Clarkand Wilkes-Gibbs, 1986).
This has more recentlydeveloped into creating situated corpora in orderto analyse the referring expressions occurring insituated collaborative tasks.
The COCONUT cor-pus (Di Eugenio et al, 2000) is collected fromkeyboard-input dialogues between two partici-pants who are collaboratively working on a sim-ple 2-D design task (buying and arranging furni-ture for two rooms).
In contrast, the QUAKE cor-pus (Byron et al, 2005) ?
as well as the more re-cent SCARE corpus (Stoia et al, 2008), which isan extension of QUAKE ?
is based on an interac-tion captured in a 3-D virtual reality (VR) worldwhere two participants collaboratively carry outa treasure hunting task.
There has been ongoingwork to exploit these two resources for research ondifferent aspects of referring expressions (PamelaW.
Jordan, 2005; Byron, 2005).However, while these resources have inspirednew research into different aspects of referring ex-pressions, at the same time they have clear limi-tations.
The COCONUT corpus is collected fromdialogues in which participants refer to symbol-like objects in a 2-D world.
It thus resem-bles the more recent (non-collaborative) TUNA-corpus (van Deemter, 2007) in tending to en-courage very simple types of expressions.
Fur-thermore, limiting participants?
interaction to key-board input makes the dialogue less natural.
Whilethe QUAKE corpus deals with a more complex do-main (3-D virtual world), the participating sub-jects were only able to carry out limited kinds ofactions (pushing buttons, picking up or droppingobjects) as compared with the complexity of thethree-dimensional target domain.In contrast to these two corpora, we set up acomparatively simple collaborative task (TangramPuzzle) allowing participants to freely communi-cate via speech and to perform actions variousenough to accomplish the given task, e.g.
pick-ing, moving, turning and rotating pieces.
All ut-terances by participants were recorded in synchro-nisation with operations on objects and the objectarrangement.
The utterances were transcribed andall referring expressions found were annotated to-gether with their referents.
Thus, this corpus al-lows us to study in detail human-human interac-tion, particularly referring expressions in a situ-ated setting.
In what follows, we first describe de-tails of the building of the corpus and then provide110results of our preliminary analysis.
This analysisreveals a novel type of referring expression men-tioning an action on objects, which we call action-mentioning expressions.2 Building the corpusFigure 1: Screenshot of the Tangram simulator2.1 Experimental settingWe recruited 12 Japanese graduate students (4 fe-males, 8 males) and split them into 6 pairs.
Eachpair was instructed to solve the Tangram puzzle(an ancient Chinese geometrical puzzle) coopera-tively.
The goal of Tangram is to construct a givenshape by arranging seven pieces of simple figuresas shown in Figure 1.In order to record detailed information of theinteraction (position of pieces, participants?
ac-tions), we implemented a Tangram simulator inwhich the pieces on the computer display can bemoved, rotated and flipped with simple mouse op-erations.
Figure 1 shows the simulator interface inwhich the left shows the goal shape area and theright the working area.
We assigned two differ-ent roles to participants, a solver and an operator;the solver thinks of the arrangement of the piecesto make the goal shape and gives instructions tothe operator, while the operator manipulates thepieces with the mouse according to the solver?s in-structions.A solver and an operator sit side by side in frontof their own computer display.
Both participantsshare the same working area of the simulator.
Theoperator can manipulate the pieces, but cannot seethe goal shape.
In contrast, the solver sees the goalshape but cannot move pieces.
A shield screen wasset between the participants in order to preventthem from peeking at their partner?s display.
Inthis asymmetrical interaction, we can expect manyreferring expressions during the interaction.Each pair is assigned four exercises and the par-ticipants exchanged roles after two exercises.
Weset a time limit of 15 minutes for an exercise.Utterances by the participants are recorded sep-arately in stereo through headset microphones insynchronisation with the position of the pieces andthe mouse actions.
In total, we collected 24 dia-logues of about four hours.
The average length ofa dialogue was 10 minutes 43 seconds.2.2 AnnotationRecorded dialogues were transcribed with a timecode attached to each utterance.
Since our mainconcern is collecting referring expressions, we de-fined an utterance to be a complete sentence toprevent a referring expression being split into sev-eral utterances.
Referring expressions were an-notated together with their referents by using themulti-purpose annotation tool SLAT (Noguchi etal., 2008).
Two annotators (two of the authors) an-notated four dialogue texts separately.
We anno-tated all 24 dialogue texts and corrected discrep-ancies by discussion between the annotators.3 Analysis of the corpusWe collected a total of 1,509 tokens and 449 typesof referring expressions in 24 dialogues.
Ourasymmetric experimental setting tended to encour-age referring expressions from the solver, whilethe operator was constrained to confirming his un-derstanding of the solver?s instructions.
This is re-flected in the number of referring expressions bythe solver (1,287) largely outnumbering those ofthe operator (222).
There are a number of expres-sions (215 expressions; 15% of the total) referringto multiple objects (referring to 2 or more pieces)and we excluded them from our current analysis.We exclusively deal here with expressions refer-ring to a specific single piece or indefinite expres-sions, i.e.
those that have no definite referent (intotal 1,294 tokens).We found the following syntactic/semantic fea-tures used in the expressions: i) demonstratives(adjectives and pronouns), ii) object attribute-values, iii) spatial relations, iv) actions on an ob-ject and v) others.
The number of these features issummarised in Table 1.
(Note that multiple fea-tures can be used in a single expression.)
Theright-most column shows an example with its En-111Table 1: Features of referring expressionsFeature types tokens Examplei) demonstrative 118 745adjective 100 196 ?ano migigawa no sankakkei (that triangle at the right side)?pronoun 19 551 ?kore (this)?ii) attribute 303 641size 165 267 ?tittyai sankakkei (the small triangle)?shape 271 605 ?o?kii sankakkei (the large triangle)?direction 6 6 ?ano sita muiteru dekai sankakkei (that large triangle facing to the bottom)?iii) spatial relations 129 148projective 125 144 ?hidari no okkii sankakkei (the small triangle on the left)?topological 2 2 ?o?kii hanareteiru yatu (the big distant one)?overlapping 2 2 ?sono sita ni aru sankakkei (the triangle underneath it)?iv) action-mentioning 78 85 ?migi ue ni doketa sankakkei (the triangle you put away to the top right)?v) others 29 30remaining 15 15 ?nokotteiru o?kii sankakkei (the remaining large triangle)?similarity 14 15 ?sore to onazi katati no (the one of the same shape as that one)?glish translation.
The identified feature in the re-ferring expression is underlined.We note here a tendency to employ object at-tributes, particularly the attribute ?shape?
as wellas use of demonstratives, particularly demonstra-tive pronouns.
These kinds of referring expres-sions are quite general and appear in a variety ofother non-situated settings as well.
In addition,we found another kind of expression not usuallyemployed by humans outside of situated collabo-ration tasks; referring expressions mentioning anaction on an object.
We have 85 expressions (over6% of the total) of this type in our corpus.4 Action-mentioning expressionsWe further analysed those expressions that men-tion an action on an object, which we call action-mentioning expressions hereafter.
Although therewas significant variation in usage of action-mentioning expressions among the pairs, all 6pairs of participants used at least one action-mentioning expression, indicating that it is a fun-damental type of expression for this task set-ting.
Action-mentioning expressions are differentfrom haptic-ostensive referring expressions (Fos-ter et al, 2008) since action-mentioning expres-sions are not necessarily accompanied by simulta-neous physical operation on an object.Action-mentioning expressions can be again di-vided into three categories: i) combination of atemporal adverbial with a verb indicating an ac-tion (?turned?, ?put?, ?moved?, etc) (55 tokens orabout 65% of action-mentioning expression), ii)use of temporal adverbials without a verb, i.e.
verbellipsis (22 tokens or about 26%) and iii) expres-sions with a verb without temporal adverbials (8tokens or about 9%).
The second category includ-ing verb ellipsis would be rare in English, but it isquite natural in Japanese.Only less than 10% of this kind of expressiondid not include any temporal adverbial, indicatingthat humans tend to describe the temporal aspectof an action.
This needs to be integrated into anygeneration algorithm for this task domain.
Thetemporal adverbials used by the participants werethe Japanese ?sakki no NP (the NP [verb-ed] justbefore)?
or ?ima no NP (the current NP/the NP[you are verb-ing] now/the NP [verb-ed] just be-fore)?.
?Ima?
generally refers to the current timepoint (?now?).
It can, however, refer to a past timepoint as well, thus it is ambiguous.Participants tended to use ?ima?
largely in itsperfect meaning (completed action).
The fre-quency of use of ?ima?
in its perfect meaning incomparison to its progressive meaning was about2:1.
The distribution of the two types of tempo-ral adverbials ?sakki?
and ?ima?
was about 2:3.The slight preference here for ?ima?
might be ex-plained by its dual meanings (progressive and per-fect) in contrast to the exclusive use of ?sakki?
forpast actions.Figure 2 shows the distribution of ?sakki (justbefore)?
and past-cases of ?ima (now)?
dependenton the time-distance to the action they refer to.
Foractions occurring within a timeframe of about 10seconds previous to uttering an expression, partic-ipants had an overwhelming preference for ?ima?.The frequency of ?ima?
decreases quickly for ac-tions that occurred 10-20 seconds prior to the ut-terance.
In contrast, after 20 seconds from the ac-11204812160?1010?2020?3030?4040?5050?6060?7070?8080?9090?100100?frequencytime (sec)"sakki (just before)""ima (now)"Figure 2: Frequency of ?sakki?
and ?ima?
over thetime-distance to referenced actiontion, participants prefered ?sakki?.In addition, we investigated what actions oc-curred in between the utterance and the actionmentioned.
The actions we take into account hereare basic manipulations of an object like ?move?,?flip?, ?click?
and so on.
Referring to an immedi-ately preceding action, participants had a strongpreference for using ?ima?.
Interestingly, withonly one other action in between, the participants?preference becomes opposite (i.e.
?sakki?
is pre-ferred.).
For referring to actions further in the past(i.e.
more actions in between), there was a con-tinous preference for ?sakki?
over ?ima?.
Furtheranalysis should also investigate the phenomenonof the difference in use of temporal adverbials forother languages and whether this is related to char-acteristics of the Japanese language or rather an in-herent property of the use of temporal adverbialsin natural language.5 Conclusion and future workWe collected a corpus of Japanese referring ex-pressions as a first step towards developing algo-rithms for generating referring expressions in a sit-uated collaboration.
We carried out an initial anal-ysis of the collected expressions, focussing on ex-pressions that include a mention of an action onan object.
We noted that they are often combinedwith temporal adverbials with participants seek-ing to make a temporal ordering of actions.
Inaddition, we intend to further analyse other typesof expressisons (demonstratives, etc) and work ondeveloping generation algorithms for this domain.In future work, we intend to generalise this exper-iment in the Tangram-domain to other domains.Furthermore, information such as gestures and eyemovements should be incorporated in data collec-tion.
This will lay the basis for the development ofmore general models for the generation of refer-ring expressions in a situated collaborative task.ReferencesDonna Byron, Thomas Mampilly, Vinay Sharma, andTianfang Xu.
2005.
Utilizing visual attention forcross-modal coreference interpretation.
In CON-TEXT 2005, pages 83?96.Donna K. Byron.
2005.
The OSU Quake 2004 cor-pus of two-party situated problem-solving dialogs.Technical report, Department of Computer Scienceand Enginerring, The Ohio State University.H.
H. Clark and D. Wilkes-Gibbs.
1986.
Referring asa collaborative process.
Cognition, 22:1?39.B.
Di Eugenio, P. W. Jordan, R. H. Thomason, and J. DMoore.
2000.
The agreement process: An empiricalinvestigation of human-human computer-mediatedcollaborative dialogues.
International Journal ofHuman-Computer Studies, 53(6):1017?1076.Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,Robin L. Hill, Jon Oberlander, and Alois Knoll.2008.
The roles of haptic-ostensive referring expres-sions in cooperative, task-based human-robot dia-logue.
In Proceedings of 3rd Human-Robot Inter-action, pages 295?302.Masaki Noguchi, Kenta Miyoshi, Takenobu Tokunaga,Ryu Iida, Mamoru Komachi, and Kentaro Inui.2008.
Multiple purpose annotation using SLAT ?Segment and link-based annotation tool.
In Pro-ceedings of 2nd Linguistic Annotation Workshop,pages 61?64.Marilyn A. Walker Pamela W. Jordan.
2005.
Learningcontent selection rules for generating object descrip-tions in dialogue.
Journal of Artificial IntelligenceResearch, 24:157?194.Laura Stoia, Darla Magdalene Shockley, Donna K. By-ron, and Eric Fosler-Lussier.
2008.
SCARE: A sit-uated corpus with annotated referring expressions.In Proceedings of the Sixth International Confer-ence on Language Resources and Evaluation (LREC2008).Kees van Deemter.
2007.
TUNA: Towards a UNifiedAlgorithm for the generation of referring expres-sions.
Technical report, Aberdeen University.www.csd.abdn.ac.uk/research/tuna/pubs/TUNA-final-report.pdf.113
