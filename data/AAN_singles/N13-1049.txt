Proceedings of NAACL-HLT 2013, pages 460?470,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsAutomatic Morphological Enrichmentof a Morphologically Underspecified TreebankSarah Alkuhlani, Nizar Habash and Ryan RothCenter for Computational Learning SystemsColumbia University{salkuhlani,habash,ryanr}@ccls.columbia.eduAbstractIn this paper, we study the problem of auto-matic enrichment of a morphologically under-specified treebank for Arabic, a morpholog-ically rich language.
We show that we canmap from a tagset of size six to one with 485tags at an accuracy rate of 94%-95%.
Wecan also identify the unspecified lemmas inthe treebank with an accuracy over 97%.
Fur-thermore, we demonstrate that using our au-tomatic annotations improves the performanceof a state-of-the-art Arabic morphological tag-ger.
Our approach combines a variety of tech-niques from corpus-based statistical models tolinguistic rules that target specific phenomena.These results suggest that the cost of treebank-ing can be reduced by designing underspec-ified treebanks that can be subsequently en-riched automatically.1 IntroductionCollections of manually-annotated morphologicaland syntactic analyses of sentences, or treebanks,are an important resource for building statisticalparsing models or for syntax-aware approaches toapplications such as machine translation.
Rich tree-bank annotations have also been used for a varietyof natural language processing (NLP) applicationssuch as tokenization, diacritization, part-of-speech(POS) tagging, morphological disambiguation, basephrase chunking, and semantic role labeling.The development of a treebank with rich annota-tions is demanding in time and money, especially formorphologically complex languages.
Consequently,the richer the annotation, the slower the annotationprocess and the smaller the size of the treebank.
Assuch, a tradeoff is usually made between the size ofthe treebank and the richness of its annotations.In this paper, we investigate the possibility ofautomatically enriching the morphologically un-derspecified Columbia Arabic Treebank (CATiB)(Habash and Roth, 2009; Habash et al 2009) withthe more complex POS tags and lemmas used inthe Penn Arabic Treebank (PATB) (Maamouri et al2004).
We employ a variety of techniques that rangefrom corpus-based statistical models to handwrit-ten rules based on linguistic observations.
Our bestmethod reaches accuracy rates of 94%-95% on fullPOS tag identification.
We can also identify the un-specified lemmas in CATiB with an accuracy over97%.
37% of our POS tag errors are due to gold treeor gold POS errors.
A learning curve experiment toevaluate the dependence of our method on annotateddata shows that while the quality of some compo-nents may reduce sharply with less data (12% abso-lute reduction in accuracy when using 132 of the dataor some 10K annotated words), the overall effect isa lot smaller (2% absolute drop).
These results sug-gest that the cost of treebanking can be reduced bydesigning underspecified treebanks that can be sub-sequently enriched automatically.The rest of this paper is structured as follows:Section 2 presents related work; Section 3 detailsvarious language background facts about Arabic andits treebanking; Section 4 explains our approach;and Section 5 presents and discusses our results.2 Related WorkArabic Treebanking There has been a lot workon building treebanks for different languages.
Inthe case of Modern Standard Arabic (MSA), thereare three efforts that vary in terms of richness andrepresentation choice.
The Penn Arabic Treebank(PATB) (Maamouri et al 2004; Maamouri et al2009b; Maamouri et al 2009a), the Prague Ara-bic Dependency Treebank (PADT) (Smr?
and Ha-jic?, 2006; Smr?
et al 2008) and the ColumbiaArabic Treebank (CATiB) (Habash and Roth, 2009;Habash et al 2009) .
The PATB uses phrase struc-ture representation, while the other two use two460different dependency representations.
The PATBand PADT representations are quite detailed.
ThePATB not only provides tokenization, complex POStags (485 tags in our data set), and syntactic struc-ture; it also provides empty categories, diacritiza-tion, lemma choices, glosses and some semantictags.
In comparison CATiB only provides tokeniza-tion, six POS tags and eight dependency relations.The tradeoff is speed: CATiB?s complete POS andsyntax annotation rate is 540 tokens/hour (and an-notator training takes two months), a much higherspeed than reported for complete (POS and syntax)annotation in PATB (around 250-300 tokens/hourand 6-12 months for annotator training) and PADT(around 75 tokens/hour) (Habash and Roth, 2009).An important recent addition to the family of Arabictreebanks is the Quran Treebank, which targets theClassical Arabic language of the Quran, not MSA(Dukes and Buckwalter, 2010).Treebank Enrichment There has been a numberof efforts on developing treebanks with rich rep-resentations and on treebank enrichment for manylanguages, such Danish, English, German, Italianand Spanish (Oepen et al 2002; Hinrichs et al2004; M?ller, 2010).
Additionally, there has beensome work on Arabic treebank enrichment that builton the PATB by manually extending its alreadyrich annotations or automatically converting themto new formalisms.
The Arabic Propbank (Proposi-tional Bank) (Palmer et al 2008) and the OntoNotesproject (Hovy et al 2006) both annotate for Ara-bic semantic information.
Alkuhlani and Habash(2011) add annotations marking functional genderand number, and rationality; and Abdul-Mageed andDiab (2012) annotate the sentence level with sen-timent labels.
Tounsi et al(2009) automaticallyconverted the PATB to a lexical functional gram-mar (LFG) representation.
Similarly, Habash andRoth (2009) used a similar technique to build aninitial version of CATiB.
We use this CATiB ver-sion of PATB to evaluate our approach in this pa-per.
Also related to this is the work on automaticenrichment of specific features, e.g., Habash et al(2007a) demonstrated that nominal case, can be de-termined for gold syntactic analyses at high accu-racy.
We replicate their results and improve uponthem.
And unlike them, we handle all the morpho-logical features in the PATB, not just case.Morphological Disambiguation There has beena lot of work on Arabic POS tagging and morpho-logical disambiguation (Diab et al 2004; Habashand Rambow, 2005; Smith et al 2005; Hajic?
et al2005; Roth et al 2008; Habash et al 2013).
Theseapproaches are intended to apply to raw text anddetermine the appropriate in-context morphologicalreading for each word.
In contrast, in this paper,we are starting from a partially disambiguated andrelatively rich representation: we have tokenization,general POS tags and syntactic dependency infor-mation.Finally, morphological information (beyond tok-enization) has been shown to be useful for manyNLP applications.
Marton et al(2011) demon-strated that morphology helps Arabic parsing.
Us-ing morphological features such as case has alsoimproved parsing for Russian, Turkish and Hindi(Nivre et al 2008; Eryigit et al 2008; Nivre, 2009).Other work has shown value for morphology in thecontext of Arabic named entity recognition (Bena-jiba et al 2009).
These results support the value ofour goal of enriching resources with morphologicalinformation, which then can be used to improve dif-ferent NLP applications.3 Linguistic BackgroundIn this section, we present some relevant generallinguistic facts about Arabic and then discuss thespecifics of the tagsets we work with in this paper.Arabic Linguistic Facts The Arabic languageposes many challenges for NLP.
Arabic is a mor-phologically complex language which includes richinflectional and cliticizational morphology, e.g., theword A?
E?J.
J?J??
w+s+y-ktb-wn+hA1 ?and they willwrite it?
has two proclitics, one prefix, one suffix andone pronominal enclitic.
Additionally, Arabic has ahigh degree of ambigiouty due to the absence of thediacritics and inconsistent spelling of letters such asAlif,@ ?
and Ya ?y.
The Buckwalter Arabic Mor-phological Analyzer (BAMA) (Buckwalter, 2004),which is used in the PATB, produces an average of12 analyses per word.In this paper, we work with gold tokenized Ara-bic as it appears in the PATB and CATiB treebanks.1Arabic transliteration is presented in the Habash-Soudi-Buckwalter scheme (Habash et al 2007b): (in alphabetical or-der) Abt?jHxd?rzs?SDTD??
?fqklmnhwy and the additional sym-bols: ?
Z, ?
@, A?
@, A?@, w??
', y?
Z?
', h?
?, ?
?.461As such, the words are partially disambiguated withregards to possible tokenizable clitics and Alif/Yaspelling forms.
That said, there is still a lot of am-biguity remaining especially because diacritics arenot marked.
Words in the treebank may be ambigu-ous in terms of their POS, lemmas and inflectionalfeatures.
The inflectional features include gender,number, person, case, state, mood, voice, aspect andthe presence of the determiner +?
@ Al+ ?the?, whichis not tokenized off in the treebanks.Arabic has a well known discrepancy in form andfunction that appears most commonly in the formof irregular plurals, called Broken Plurals, whichalthough functionally are plural, have singular suf-fixes.
We will not discuss form and function dis-crepancy in this paper except as needed.
For moreon this see Habash (2010).The Buckwalter Tagset The Buckwalter POStagset is perhaps one of the most commonly usedtagsets for Arabic NLP research.
The tagset?s pop-ularity is in part due to its use in the PATB.
Buck-walter tags can be used for tokenized and untok-enized text.
The untokenized tags are produced byBAMA (Buckwalter, 2004) and consist of 485 tags.The tokenized tags, which are used in the PATB,are derived from the untokenized tags and can reachthousands of tags.
Both variants use the same ba-sic 70 or so sub-tag symbols (such as DET ?deter-miner?, NSUFF ?nominal suffix?, ADJ ?adjective?and ACC ?accusative?)
(Maamouri et al 2009a).These sub-tags are combined to form around 170morpheme tags such as NSUFF_FEM_SG ?femi-nine singular nominal suffix?
and CASE_DEF_ACC?accusative definite?.
The word tags are con-structed out of one or more morpheme tags,e.g.
DET+NOUN_PROP+CASE_DEF_NOM forthe word 	???
@ Al+Siyn+u ?China?.CATiB Trees and POS Tags CATiB uses thesame basic tokenization scheme used by PATB andPADT.
However, the CATiB POS tagset is muchsmaller.
Whereas in practice PATB uses 485 Buck-walter tags specifying every aspect of Arabic wordmorphology such as definiteness, gender, number,person, mood, voice and case, CATiB uses 6 POStags: NOM (non-proper nominals including nouns,pronouns, adjectives and adverbs), PROP (propernouns), VRB (verbs), VRB-PASS (passive-voiceverbs), PRT (particles such as prepositions or con-VRB?????
trsl???IV3FS+IV+IVSUFF_MOOD:I?????
>arsal?send?MODPRT+?
s+???FUT_PART+?
sa+?will?SBJPROP???????
AlSyn???DET+NOUN_PROP+CASE_DEF_NOM?????
Siyn?China?OBJNOM?
???
qmrA???NOUN+CASE_INDEF_ACC????
qamar?moon?MODNOM?????
??????
<STnAEyA???ADJ+CASE_INDEF_ACC????
??????
<iSTinAEy??artificial?MODPRT????
<lY???PREP????
<lY?to?OBJPROP???????
?
Almryx???DET+NOUN_PROP+CASE_DEF_GEN??????
mar?iyx?Mars?Figure 1: An example dependency tree for the sentence ???????
?
????
?????
??????
?
???
?
????
?
??????
s+trsl Alhnd qmrA<STnAEyA <lY Almryx ?India will send a satellite to Mars [in 2013]?.
In every tree node, the terms above the line arepart of the CATiB annotations: the word, POS (VRB = verb, PRT = particle, PROP = proper noun, NOM = nominal)and relation (MOD = modifier, SBJ = subject, OBJ = object).
The terms under the line are the Buckwalter POS tag, thelemma and the gloss, respectively.analyses as a constraint on the space from which wewill select the appropriate in context tag.
The ap-proach is quite similar to how MADA (morphologi-cal analysis and disambiguation for Arabic) (Habashand Rambow, 2005) works except that we are usingthe CATiB tree as the context in which we disam-biguate.
Given the degree of richness of the tree, weexpect to outperform basic disambiguation on text.In the rest of this section, we discuss our generalstrategy, followed by a detailed presentation of ourapproach: morphological disambiguation and mor-phological filtering.4.1 From CATiB to Buckwalter: Devising aStrategyIn CATiB trees, different pieces of information canbe relevant to different disambiguation tasks.
We an-alyzed the data we have and obtained the followingobservations which we use to devise our strategy forhow to address different types of ambiguity:?
Words in CATiB treebank are tokenized.
Thisresolves many ambiguous cases: analyses in-volving cliticized prepositions or conjunctionsare dismissed.
Further more, separated cliticsare marked, which restricts their reading.?
The CATiB POS tag, although two orders ofmagnitude smaller than the Buckwalter tag set,provides a lot of information.
It resolves ambi-guity amongst verbs (active or passive), nomi-nals, particles, proper nouns and punctuation.?
The CATiB tags NOM or PRT are the most am-biguous.
They are challenging because thereare both lexical and morphosyntactic featuresat play.
We rely on our training data to learnmodels of how to disambiguate them.Figure 1: An example dependency tree for the sentencet'Q??
@ ??
@AJ?A 	J??@@Q??
???
@ ?
?Q?
s+trsl AlSyn qmrAA?STnA?yA A?l?
Almryx ?China will send a satellite toMars?.
In every tree node, the terms above the line arepart of the CATiB annotations: the word, POS (VRB,PRT, PROP, NOM) and relation (MOD, SBJ, OBJ).
Theterms under the line are t e Buckwalter POS tag, thelemma and the gloss, respectively.junctions) and PNX (punctuation).
CATiB uses adependency representation that mod ls predicate-argument structure (subject, object, etc.)
and Ara-bic nominal structure (idafa, tamyiz, modification).The eight CATiB relation labels are: SBJ (sub-ject of verb or topic of simple nominal sentence),OBJ (object of verb, preposition, or deverbal noun),TPC (topic i complex ominal s ntences contain-ing an explicit pronominal referent), PRD (pred-icate marking the complement in some copularconstructions), IDF (relation between the posses-sor [dependent] and the possessed [head] in theidafa/possessive nominal construction), TMZ (re-lation of the specifier [dependent] to the specified[head] in the tamyiz/specification nominal construc-tions), MOD (general modifier of verbs or nouns),and ?
(marking flatness inside constructions suchas first-last proper name sequences).
This relationlabel set is much smaller than the twenty or so dash-tags used in PATB to mark syntactic and semanticfunctions.
Furthermore, no empty categories, coref-erence, or semantic relations (e.g., TMP or LOC) areprovided (Habash and Roth, 2009).
A detailed dis-462cussion of CATiB guidelines and further comparisonwith PATB appears in (Habash et al 2009).In this paper, we target the enrichment of CATiBwith the morphological information used in thePATB: Buckwalter POS tags and lemmas.
We donot address other kinds of rich information.
Figure 1presents an example of a CATiB tree with the exten-sions we predict automatically for each word.4 ApproachWe define our task as assigning a Buckwalter POStag and lemma to each word in a CATiB syntac-tic tree, i.e., disambiguating the CATiB POS tag interms of the finer grained Buckwalter tag in context.Our approach utilizes a variety of corpus-based andrule-based techniques.
We use corpus-based tech-niques that exploit available training data in the formof portions of the PATB that are automatically con-verted to CATiB style trees.
We also use rule-basedsolutions that allow us to apply linguistic knowl-edge and insights.
An important tool that we useis a morphological analyzer which generates for ev-ery word all possible out-of-context analyses.
Weuse these analyses as a constraint on the space fromwhich we will select the appropriate in-context tag.The approach is quite similar to how MADA (Mor-phological Analysis and Disambiguation for Arabic)(Habash and Rambow, 2005) works except that weare using the CATiB tree as the context in which wedisambiguate.
Given the degree of richness of thetree, we expect to outperform basic disambiguationon text.In the rest of this section, we discuss our generalstrategy, followed by a detailed presentation of ourapproach: morphological disambiguation and mor-phological filtering.4.1 From CATiB to Buckwalter: Devising aStrategyIn CATiB trees, different pieces of information canbe relevant to different disambiguation tasks.
We an-alyzed a sample of the data we have and obtained thefollowing observations which we use to devise ourstrategy for how to address different types of ambi-guity:?
Words in the CATiB treebank are tokenized.This resolves many ambiguous cases: anal-yses involving cliticized prepositions or con-junctions are dismissed.
Further more, sepa-rated clitics are marked, which restricts theirreading.
The ambiguity in terms of the numberof lemmas per word reduces from 2.7 for unto-kenized words to just 1.1 for tokenized words.?
The CATiB POS tagset, although two orders ofmagnitude smaller than the Buckwalter tagset,provides a lot of information.
It resolves ambi-guity amongst verbs (active or passive), nomi-nals, particles, proper nouns and punctuation.?
The CATiB tags NOM and PRT are the mostambiguous.
They are challenging because thereare both lexical and morphosyntactic featuresat play.
We rely on our training data to learnmodels of how to disambiguate them.
TheCATiB treebank annotation does not determin-istically allow us to identify the finer grainedtag using the POS and relations alone: e.g., theNOM child of an NOM parent (with the rela-tion MOD) can be an ADJ (67% probability)or a NOUN (21%).?
Case, state, mood and to a lesser degree aspectare syntactically dependent features, for whichwe use the CATiB tree and linguistic rules todisambiguate the correct value in context.?
Gender, number and person are expressed usingaffixes that highly limit the feature-value possi-bilities, e.g., the suffix ?+ +h?
deterministicallyselects for +NSUFF_FEM_SG suffix tag.4.2 Morphological Analysis & DisambiguationWe use the morphological analyzer BAMA to get alist of all possible analyses for a word.
BAMA re-turns unranked analyses for untokenized text only.Since we know that the input is already tokenized,we built an extension to BAMA that handles cliticsand accepts analyses that are consistent with the tok-enization of the input, discarding all other analyses.We use both the morphological analyzer BAMAand a training set from the PATB to predict the Buck-walter tag for a given word.
We use BAMA to get alist of all possible Buckwalter tag and lemma pairsfor each word.
We then rank these choices using oneof the following two methods: a maximum likeli-hood estimate model (MLE) conditioned on specificfeatures in the CATiB tree or a MADA-like suite ofclassifiers that select for specific POS tag featuressuch as gender or number.4634.2.1 Maximum Likelihood ModelThe MLE model ranks the set of choices fromBAMA returning the most probable analysis.
Weconsider two models:?
MLE Baseline 1 selects the Buckwalter tagwith the highest unconditioned probability inthe training data, P(BW), among the set ofBAMA choices for the word whose tag wewant to determine.?
MLE Baseline 2 selects the Buckwalter tagwith the highest probability conditioned onthe word and CATiB tag: P(BW|word,CATiB).This model backs off to the Buckwalter tagwith the highest probability conditioned on theCATiB tag, i.e., P(BW|CATiB), and then backsoff to MLE Baseline 1.4.2.2 Analysis and Disambiguation ofTokenized ArabicWe retrained the MADA system (Habash andRambow, 2005) using a tokenized version of thePATB.
We call the new version TADA: TokenizedAnalysis and Disambiguation of Arabic.
TADAtakes tokenized text, and returns a ranked list ofanalyses for each tokenized word and clitic.
Justlike MADA, TADA uses BAMA to identify possibleanalyses of the word.
It then uses a suite of classi-fiers to predict inflectional and lexical features thatare used to rank the possible analyses.As expected, TADA outperforms the simple MLEmodels described earlier; however, its performanceis not high enough since it makes no use of tree fea-tures.
The results are presented in Section 5.
How-ever, we will present here a preliminary error analy-sis of TADA?s output to motivate the morphologicalfilters presented next (Section 4.3).TADA Preliminary Error Analysis We consid-ered the first 100 errors in the Buckwalter tags in ourdevelopment set.
About half of the errors involved aproblem in case (42%), state (13%) or mood (3%).Case and state errors had many overlaps.
All ofthese errors are syntactically determinable using thetree representation in a manner similar to Habashet al(2007a).
In 17% of the cases, a POS errorcan be resolved using the CATiB tag, (e.g., propernoun vs adjective or verb).
In 2% of the cases, theerror involved an orthographic normalization (Alif-form) that led to an undesirable solution (e.g., ?
J??@?lsnh?
?tongues?
vs ?
J??
@ Alsnh?
?the-year?).
Thesecases should be resolved by enforcing the CATiBtree word form.
Ambiguity in CATiB tags was aproblem for nominal forms 15% of the time (e.g.,NOUN vs ADJ), particles 10% of the time (e.g., +?w+ ?and?
can be CONJ or SUB_CONJ), and pro-nouns 5% of the time (e.g., ?
?+ +hm ?them?
can beIVSUFF_DO:3MP or PVSUFF_DO:3MP [attachedto an imperfective or perfective verb]).2 In 1% ofthe cases, there was an error involving ambiguityin number (dual/plural).
And finally, in 3% of thecases, we determined that the gold POS tag was ac-tually incorrect.
Within the same set of sentencesstudied, we found 18 lemma choice errors.
Almostall, except for three cases, involve a nominal formambiguity resulting from diacrtic absence, e.g., XY?
?muhad?id ?threatening?
or muhad?ad ?threatened?.Eight of the 18 cases (or 44%) happened without anaccompanying POS error.
Overall, the accuracy oflemma choice is highly dependent on the correct-ness of the chosen core Buckwalter tag; lemma ac-curacy when the tag is correct is 97.9%, but it dropsto 71.3% when the tag is wrong.4.3 Morphological FiltersWe implemented a set of filters that take the list ofranked analyses produced by TADA and discard anyanalyses that are inconsistent with the filters?
deci-sions in the tree context.
TADA ranking is preservedamong the remaining analyses.4.3.1 CATiB FilterTADA returns all analyses for word, includingdifferent forms of the word (i.e., different Alif/Yaforms as part of BAMA?s back-off mode).
For ex-ample, when given the word ???
?l?, TADA returnsanalyses for both the words ???
?l?
?on?
and ???
?ly?Ali?.
Since the input to our system is the gold wordform from CATiB trees, the CATiB filter will discardanalyses that do not match the given word form.The CATiB filter also resolves some POS ambigu-ity given information in the CATiB POS tag.
For ex-ample, the CATiB POS tags NOM or VRB can eas-ily decide whether the ambiguous word I.
KA?
kAtbis a noun (kAtib ?writer?)
or a verb (kAtab ?to corre-spond?
).2This is a peculiarity of the tagset used in PATB.
The dis-tinction does not seem to be necessary to our knowledge, butwe still consider it the gold goal.4644.3.2 Pronominal FilterThe pronominal filter (PRON) selects the pro-nouns that are consistent with the verbs they are at-tached to.
A pronoun attached to a verb could eitherbe IVSUFF_DO, PVSUFF_DO or CVSUFF_DOdepending on whether the verb is imperfective (IV),perfective (PV), or imperative (CV).4.3.3 Noun/Adjective FilterThe noun/adjective (NOUN/ADJ) filter is ap-plied to words with the CATiB tag NOM.It uses a nominal classifier, which classifiesCATiB NOM words into one of the follow-ing Buckwalter noun/adjective classes: NOUN,NOUN.VN, NOUN_QUANT, NOUN_NUM, ADJ,ADJ.VN, ADV_COMP, ADV_NUM.
The NA (not-applicable) tag is assigned to all other words.
For ex-ample, the classifier will decide whether ?QJ.?
kbyrh?is a noun ?abomination?
or an adjective ?great [fem-inine singular]?
based on the context.To build the nominal classifier, we use Yam-cha (Kudo and Matsumoto, 2003), a support-vector-machine-based sequence tagger trained on our PATBtraining data.
We use the following set of fea-tures: the word form, CATiB POS tag, parent fea-tures (word form, CATiB POS tag), dependency re-lation, order of appearance (the word comes beforeor after its parent), the distance between the wordand its parent, and different types of relation-childPOS (REL-CTB) features.
The REL-CTB featuresstate whether a word has a child with a CATiB POS(CTB) under a dependency relation (REL).
A wordcan have 0 or more children.
We have six CATiBPOS tags and eight dependency relations and thus upto 48 different REL-CTB binary learning features.An example of this feature is a PRT that has a childNOM under a dependency relation OBJ.
In this case,the value of the feature OBJ-NOM is 1.
We also adda window of two words before and two words afterthe word being tagged as static features, and the tagof the previous two words as dynamic features.
Thenominal classifier predicts the correct nominal classwith an accuracy of 97.70%.4.3.4 Particle FilterThe particle filter (PRT) selects the specific Buck-walter POS for a particle.
For example, the particleA?
mA can be the negative particle ?not?, the relativepronoun ?that?
or the interrogative pronoun ?what?
?.The PRT filter uses a particle classifier that uses thesame learning features and training data as the nomi-nal classifier.
The particle classifier predicts the cor-rect particle class with an accuracy of 99.54%.4.3.5 Verbal Mood and Aspect FilterThe Buckwalter POS tags for verbs have threemarkers for aspect: imperfective (IV), perfective(PV), and imperative (CV); and three markers formood: jussive (J), subjunctive (S) and indicative(I).
We apply our rule-based mood-and-aspect fil-ter (MOOD/ASPECT) to words that have the CATiBtag VRB or VRB-PASS.If the verb is preceded by a jussive, subjunctive orfuture particle then it is imperfective in aspect andits mood is determined by the particle.
The mood isindicative if the verb is preceded by a future particlesuch as 	???
swf ?will?
; it is jussive if the verb is pre-ceded by a jussive particle such as ??
lm ?not+past?,+?
l+ ?for?
; and it is subjunctive if the verb is pre-ceded by a subjunctive particle such as 	?
@ ?n ?that?,??
ln ?not+future?, ??
ky ?so as to?, and ?
?k Ht?
?un-til?.
This is also valid when a negating B lA inter-venes between the subjunctive particle and the verb.If the verb is proceeded with the particle Y??
lqd ?al-ready?, then the verb is perfective.
Otherwise, theverb could be either imperfective (with an indicativemood), perfective or imperative (all allowed throughthe filter).4.3.6 Nominal State FilterThe Buckwalter POS tags have three nominalstate markers: INDEF, DEF and POSS.3 The nomi-nal state filter (STATE) applies the following rules:If the word is head of an idafa (IDF), then we ex-clude the INDEF analyses.
Otherwise, we excludethe POSS and the non-Al/DET determined DEFanalysis (which are only used for IDF heads).4.3.7 Nominal Case FilterThe nominal case filter (CASE) assigns the val-ues nom (nominative), acc (accusative) or gen (gen-itive) to each NOM/PROP word primarily based onthe CATiB dependency relation label that describesthe type of relation between the word and its parent.The nominal case filter extends the case predictor inHabash et al(2007a).
The following four rules areapplied in sequence.3These values do not exactly match the functional values forstate in Arabic (Smr?, 2007).465?
RULE 1: Assign acc to all NOM/PROP wordsas a default.?
RULE 2: Assign nom to NOM/PROP wordsthat (a) head the tree, (b) have the label TPC,(c) have the label SBJ but are not headed by aparticle from the closed class of Inna and itssisters (Habash et al 2007a), or (d) have thelabel PRD but is not headed by a verb or dever-bal noun.
Exempt words in the closed class ofadverb-like nouns such as ??
?
fwq ?over?, ?J.
?qbl ?before?, and ?
?k Hwl ?around?.?
RULE 3: Assign gen to NOM/PROP words thathave the label OBJ under a preposition, or thathave the label IDF.?
RULE 4: All children of NOM/PROP parentswhose label is MOD, and NOM/PROP chil-dren of conjunctions whose label is OBJ, copythe case of their parent.
Conjunctions carry thecase temporarily to pass on agreement.4.3.8 MLE OverrideWe added an MLE-based component to overrideanswers that are provided by our final system.
Weused a no-BAMA version of the MLE Baseline 2.The difference between the MLE override compo-nent and MLE Baseline 2 is that it takes into accountall possible Buckwalter POS tags that appear in thetraining set for a specific word regardless of whetherthey are provided by BAMA or not.
This MLE over-ride component is trained on the same training setand returns the most common Buckwalter tag andlemma pair for a given word form and CATiB POStag pair.
BAMA is not used here since the reasonbehind this additional step is to overcome any limi-tation caused by using BAMA to start with.
Theselimitations include primarily cases of BAMA failureto produce analyses (OOV) or minor version differ-ences between BAMA and the PATB.
If a Buckwal-ter tag and lemma pair appear above a threshold ofn times and always with the same word-lemma pair,then we override our answer with the new answerfrom the MLE.
When we override, we only overridethe core part of the Buckwalter tag.
We do not over-ride the state, case, and mood features since they aresyntactic features.
We tried different values for thethreshold and got the best results when n = 4.4.4 Putting it All TogetherTADA provides an initial list of ranked analyses.Then, the morphological filters discard analyses thatare not consistent with the CATiB tree information.The analysis with the highest TADA rank among theremaining analyses is selected as the answer.We apply our filters in the following order.
Wefirst apply the CATiB filter.
After that, we applythe pronominal, noun/adjective and particle filters.These three filters can be applied in any order sincethey are applied on disjoint sets of words.
The nextfilter is the mood/aspect filter which has to be ap-plied after the particle filter since it depends on theparticle choice in predicting the mood of the follow-ing verb.
At this point, we freeze the lemma choicefor the word.
The next two filters, state and case,look at syntactic features and should not affect thechoice of the lemma.We use two back-off mechanisms.
The first oneis with the application of each filter.
If the effect ofapplying a filter results in an empty set (no matchfound) then we undo the effect of the filter and passthe list of analyses as is to the next filter.
The sec-ond mechanism is using the MLE override at the endof the pipeline.
TADA, the noun/adjective and par-ticle filters, and the MLE override use corpus-basedcomponents while all other filters are rule-based.5 Evaluation5.1 Experimental SettingsWe use a CATiB version of the PATB part 3v3.1 andpart 2v3.0 released by the Linguistic Data Consor-tium (LDC) (Maamouri et al 2004).
We use thetrain/development/test (80/10/10) splits of Marton etal.
(2010) for PATB part 3v3.1 (16.6K sentences;400K tokens): we use their train as our training data,their development as the tuning data for TADA andtheir test as our development set.
For our blind test,we use the first 1000 sentences in PATB part 2v3.0(38K tokens).We report all results in terms of token accuracyon the full Buckwalter tag, reduced Buckwalter tagand the lemma.
The reduced Buckwalter tag is theBuckwalter tag without case, state, and mood.
Thenumber of tags is reduced to 220 tags (compared to485 tags for the full Buckwalter tagset).In cases of gold full Buckwalter tags that are un-derspecified for case, state or mood, we do not pe-nalize our systems if our more specific predicted466Full ReducedBW BW Diff LemmaMLE Baseline 1 57.19 73.44 16.25 90.87MLE Baseline 2 77.69 93.27 15.58 94.31TADA 86.15 94.04 7.89 96.97++ CATiB 87.33 95.50 8.17 97.72++ PRON 88.16 96.32 8.16 97.72++ NOUN/ADJ 88.93 97.26 8.33 97.72++ PRT 89.24 97.57 8.33 97.72++ MOOD/ASPECT 89.46 97.60 8.14 97.74++ STATE 89.92 97.61 7.69 97.74++ CASE 94.90 97.61 2.71 97.74++ MLE override 95.27 98.00 2.73 97.81Table 1: Accuracy of enriching CATiB trees with Buck-walter (BW) tags and lemmas on the development set.Reduced Buckwalter is similar to Buckwalter, but ignorescase, mood and state.
The Difference between the twometrics highlights the errors from case, mood and state.tag otherwise matches the gold tag.
Words whoselemmas are unknown (nolemma, TBupdate) or hasthe lemma DEFAULT (including digits and punc-tuation) are excluded from the evaluation, but nottraining: in the development set, 4,498 out of 25,446words were excluded (?18%).5.2 ResultsTable 1 shows the results of our experiments onthe development set.
Considering the baseline sys-tems, we see that using both the CATiB POS tagand the word form in MLE Baseline 2 gives us a20.5% absolute increase above MLE Baseline 1.
Us-ing TADA improves the performance significantly(adding 8.46% absolute over MLE Baseline 2).
Ev-ery additional morphological filter has a positive im-pact and the improvement of the accuracy for fullBuckwalter with each new filter ranged between0.22% and 1.18% absolute except for the case filter,which adds almost 5%.
Adding the MLE overridehas a positive impact on the accuracy of the full andreduced Buckwalter tags and the lemma.We apply our baselines, TADA, TADA+filters andTADA+filters+MLE to the blind test set (see Ta-ble 2).
The test set is a bit harder than the devel-opment set, but the results are consistent with thoseseen for the development set.5.3 Error AnalysisWe conducted an analysis of the errors in the outputof the final system TADA+filters+MLE on the devel-opment set.
We considered 100 randomly selectederror cases and examined them in the CATiB treesFull ReducedBW BW Diff LemmaMLE Baseline 1 55.96 71.88 15.92 90.77MLE Baseline 2 77.15 92.88 15.73 94.03TADA 86.49 94.42 7.93 96.63++ All filters 93.44 97.25 3.81 97.13++ MLE override 93.61 97.43 3.82 97.17Table 2: Accuracy of enriching CATiB trees with Buck-walter (BW) tags and lemmas on the blind test set.to assess the source of the error.
About 37% of allerrors are due to gold treebank errors: 21% are goldtree structure/relation errors and 16% are gold POSerrors.
The rest of the errors result from failures inour system.
The most common error is in NOM dis-ambiguation: NOUN/NOUN_NUM, NOUN/ADJ,NOUN/NOUN_QUANT, etc.
The NOM errors ac-counted for 33% of all errors.
Case comes sec-ond with 12% errors, then PRT and gender-number-person errors with 5% each.
State errors contributeto 3% of total errors.5.4 Learning Curve StudyThe non-rule-based components of our approach,namely TADA, NOUN/ADJ and PRT filters, andMLE override depend on the existence of an anno-tated treebank in rich format.
To understand the de-gree of dependence, we ran a series of experimentson different sizes of the training data: 12 , 14 , 18 , 116 ,and 132 of the full training set (341.1K words).
Thesedata sets were used to train new versions of TADA,the NOUN/ADJ and PRT filters, and the MLE over-ride.
The results of running TADA and the final sys-tem on the development set using the different datasets are summarized in Tables 3 and 4, respectively.As expected, when the training data size goes downthe accuracy goes down.
Our final system, whichadds filters on top of TADA, had a significant ef-fect on the performance as shown in Table 4.
Usingonly 10.6K of annotated words, the quality of TADAreduces sharply (12.12% absolute reduction in accu-racy) while the overall effect on our full system isa lot smaller (2.03% absolute drop).
Similarly, theperformance of the nominal and particle classifiersdegrade when trained on less data.
When we use132 of the training data, the correct nominal class ispredicted at an accuracy of 90.49% (7.21% absolutedrop), while the correct particle class is predicted atan accuracy of 96.59% (2.95% absolute drop).
Weused the MLE override threshold determined based467Size Full BW Reduced BW Diff Lemma1/32 10.6K 74.03 88.78 14.75 93.411/16 21.3K 77.16 90.30 13.14 94.371/8 42.6K 79.76 91.63 11.87 95.561/4 85.3K 81.91 92.80 10.89 96.221/2 170.7K 84.12 93.62 9.50 96.741 341.1K 86.15 94.04 7.89 96.97Table 3: Accuracy of enriching CATiB trees with Buck-walter (BW) tags and lemmas using TADA only for dif-ferent training sizes on the development set.Size Full BW Reduced BW Diff Lemma1/32 10.6K 93.24 95.81 2.57 95.681/16 21.3K 93.67 96.28 2.61 96.271/8 42.6K 94.14 96.79 2.65 96.941/4 85.3K 94.56 97.26 2.70 97.221/2 170.7K 94.96 97.66 2.70 97.611 341.1K 95.27 98.00 2.73 97.81Table 4: Accuracy of enriching CATiB trees with Buck-walter (BW) tags and lemmas using our best performingsystem for different training sizes on the development set.on the full training data, which may not be optimalfor smaller data sets.The contribution of our full system over TADAwhen using 132 of the full training data is over19% absolute (on full Buckwalter tag determination)compared to 9% when using the full training data.The morph analysis (out of context) is the same forall experiments and that this provides a lot of stabil-ity to the results.
The high lemma accuracy overallis a result of disambiguating tokenized words, wherethe average numbers of lemmas per word is only 1.1as mentioned above.
These results suggest that ourapproach is usable even in the early stages of devel-oping new richly annotated treebanks.5.5 Extrinsic EvaluationWe applied our automatic enrichment to the under-specified CATiB treebank (as opposed to the parts ofPATB, which we used throughout the paper to sim-ulate CATiB).
We evaluate the added value of theseannotations by using them to extend the training datafor the morphological tagger MADA (Habash andRambow, 2005), which is used on untokenized text.We train a new set of MADA classifier models usinga combination of the original MADA (v 3.2) train-ing data (578K words taken from PATBs 1, 2 and 3)and the enriched CATiB data (218K words).
We ap-ply the new MADA system to our development setand evaluate on several metrics.
As a baseline, weprocess the same development set using MADA (v3.2).
Other than the training data used to constructthe classifier models, there are no differences be-tween the two systems.
The CATiB-enriched systemresults in a Buckwalter POS tag accuracy of 85.6%(a 2.2% error reduction over the baseline).
Whenevaluating on the set of 14 MADA morphologicalfeatures, the new system results in a 85.7% accuracy(2.4% error reduction).
The new system also im-proves PATB segmentation accuracy (99.2%, a 5.4%error reduction).
In the future, we will evaluate thecontribution of the additional annotations in the con-text of other applications, such as syntactic parsing.6 Conclusion and Future WorkWe have demonstrated that an underspecified ver-sion of an Arabic treebank can be fully specified forArabic?s rich morphology automatically at an accu-racy rate of 94%-95% for POS tags and 97% forlemmas.
Our approach combines a variety of tech-niques from corpus-based statistical models (whichrequire some rich annotations) to linguistic rules thattarget specific phenomena.
Since the underspecifiedtreebank is much faster to manually annotate than itsfully specified version, these results suggest that thecost of treebanking can be reduced by designing un-derspecified treebanks that can be subsequently en-riched automatically.In the future, we plan to extend the automaticenrichment effort to include more complex featuressuch as empty nodes and semantic labels.
We alsoplan to take the insights from this effort and applythem to treebanks of other languages.
A small por-tion of a treebank that is fully annotated in rich for-mat will of course be needed before we can applythese insights to other languages.AcknowledgmentsThe first author was funded by a scholarship fromthe Saudi Arabian Ministry of Higher Education.The rest of the work was funded under DARPAprojects number HR0011-08-C-0004 and HR0011-12-C-0014.
Any opinions, findings and conclu-sions or recommendations expressed in this paperare those of the authors and do not necessarily re-flect the views of DARPA.468ReferencesM.
Abdul-Mageed and M. Diab.
2012.
AWATIF: AMulti-Genre Corpus for Modern Standard Arabic Sub-jectivity and Sentiment Analysis.
The 8th Interna-tional Conference on Language Resources and Eval-uation (LREC2012).Sarah Alkuhlani and Nizar Habash.
2011.
A Corpusfor Modeling Morpho-Syntactic Agreement in Ara-bic: Gender, Number and Rationality.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics (ACL?11), Portland, Ore-gon, USA.Yassine Benajiba, Mona Diab, and Paolo Rosso.
2009.Arabic Named Entity Recognition: A Feature-drivenStudy.
IEEE Transactions on Audio, Speech & Lan-guage Processing, 17(5):926?934.Tim Buckwalter.
2004.
Buckwalter Arabic Morpho-logical Analyzer Version 2.0.
LDC catalog numberLDC2004L02, ISBN 1-58563-324-0.Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004.Automatic tagging of Arabic text: From raw text tobase phrase chunks.
In Proceedings of the 5th Meet-ing of the North American Chapter of the Associa-tion for Computational Linguistics/Human LanguageTechnologies Conference (HLT-NAACL04), Boston,MA.Kais Dukes and Tim Buckwalter.
2010.
A Depen-dency Treebank of the Quran using Traditional Ara-bic Grammar.
In Proceedings of the 7th internationalconference on Informatics and Systems (INFOS 2010),Cairo, Egypt.G?lsen Eryigit, Joakim Nivre, and Kemal Oflazer.
2008.Dependency Parsing of Turkish.
Computational Lin-guistics, 34(3):357?389.Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and MorphologicalDisambiguation in One Fell Swoop.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 573?580, AnnArbor, Michigan.Nizar Habash and Ryan Roth.
2009.
CATiB: TheColumbia Arabic Treebank.
In Proceedings of theACL-IJCNLP 2009 Conference Short Papers, pages221?224, Suntec, Singapore.Nizar Habash, Ryan Gabbard, Owen Rambow, SethKulick, and Mitch Marcus.
2007a.
Determining Casein Arabic: Learning Complex Linguistic Behavior Re-quires Complex Linguistic Features.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages1084?1092.Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.2007b.
On Arabic Transliteration.
In A. van denBosch and A. Soudi, editors, Arabic ComputationalMorphology: Knowledge-based and Empirical Meth-ods.
Springer.Nizar Habash, Reem Faraj, and Ryan Roth.
2009.
Syn-tactic Annotation in the Columbia Arabic Treebank.
InProceedings of MEDAR International Conference onArabic Language Resources and Tools, Cairo, Egypt.Nizar Habash, Ryan Roth, Owen Rambow, Ramy Eskan-der, and Nadi Tomeh.
2013.
Morphological Analysisand Disambiguation for Dialectal Arabic.
In Proceed-ings of the 2013 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies (NAACL-HLT),Atlanta, GA.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publish-ers.Jan Hajic?, Otakar Smr?, Tim Buckwalter, and HubertJin.
2005.
Feature-based tagger of approximationsof functional Arabic morphology.
In Proceedings ofthe Workshop on Treebanks and Linguistic Theories(TLT), Barcelona, Spain.Erhard Hinrichs, Sandra K?bler, Karin Naumann, HeikeTelljohann, and Julia Trushkina.
2004.
Recent Devel-opments in Linguistic Annotations of the T?Ba-D/ZTreebank.
In Proceedings of the Third Workshop onTreebanks and Linguistic Theories.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:The 90% Solution.
In NAACL ?06: Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers on XX,pages 57?60, Morristown, NJ, USA.Taku Kudo and Yuji Matsumoto.
2003.
Fast Methods forKernel-Based Text Analysis.
In Erhard Hinrichs andDan Roth, editors, Proceedings of ACL, pages 24?31.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic Treebank :Building a Large-Scale Annotated Arabic Corpus.Mohamed Maamouri, Ann Bies, Sondos Krouna, FatmaGaddeche, and Basma Bouziri, 2009a.
Penn ArabicTreebank Guidelines.
Linguistic Data Consortium.Mohamed Maamouri, Ann Bies, and Seth Kulick.
2009b.Creating a Methodology for Large-Scale Correction ofTreebank Annotation: The Case of the Arabic Tree-bank.
In Proceedings of MEDAR International Con-ference on Arabic Language Resources and Tools,Cairo, Egypt.Yuval Marton, Nizar Habash, and Owen Rambow.
2010.Improving Arabic Dependency Parsing with Lexicaland Inflectional Morphological Features.
In Proceed-ings of the NAACL HLT 2010 First Workshop on Sta-tistical Parsing of Morphologically-Rich Languages,pages 13?21, Los Angeles, CA, USA, June.469Yuval Marton, Nizar Habash, and Owen Rambow.
2011.Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics (ACL?11), Port-land, Oregon, USA.Henrik M?ller.
2010.
Annotation of Morphology andNP Structure in the Copenhagen Dependency Tree-banks (CDT).
In Proceedings of the Ninth Interna-tional Workshop on Treebanks and Linguistic Theo-ries, pages 151?162.Joakim Nivre, Igor M. Boguslavsky, and Leonid L.Iomdin.
2008.
Parsing the SynTagRus Treebank ofRussian.
In COLING ?08: Proceedings of the 22ndInternational Conference on Computational Linguis-tics, pages 641?648, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Joakim Nivre.
2009.
Parsing Indian Languages withMaltParser.
Proceedings of the ICON09 NLP ToolsContest: Indian Language Dependency Parsing, pages12?18.Stephan Oepen, Dan Flickinger, Kristina Toutanova, andChristoper D. Manning.
2002.
LinGO Redwoods -A Rich and Dynamic Treebank for HPSG.
In LRECworkshop on parsing evaluation, Las Palmas, Spain.Martha Palmer, Olga Babko-Malaya, Ann Bies, MonaDiab, Mohamed Maamouri, Aous Mansouri, and Wa-jdi Zaghouani.
2008.
A Pilot Arabic Propbank.
InProceedings of LREC, Marrakech, Morocco, May.Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,and Cynthia Rudin.
2008.
Arabic Morphological Tag-ging, Diacritization, and Lemmatization Using Lex-eme Models and Feature Ranking.
In Proceedings ofACL-08: HLT, Short Papers, pages 117?120, Colum-bus, Ohio.Noah Smith, David Smith, and Roy Tromble.
2005.Context-Based Morphological Disambiguation withRandom Fields.
In Proceedings of the 2005 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP05), pages 475?482, Vancouver,Canada.Otakar Smr?
and Jan Hajic?.
2006.
The Other ArabicTreebank: Prague Dependencies and Functions.
InAli Farghaly, editor, Arabic Computational Linguis-tics: Current Implementations.
CSLI Publications.Otakar Smr?, Viktor Bielick?, Iveta Kour?ilov?, JakubKr?c?mar, Jan Hajic?, and Petr Zem?nek.
2008.
PragueArabic Dependency Treebank: A Word on the Mil-lion Words.
In Proceedings of the Workshop on Ara-bic and Local Languages (LREC 2008), pages 16?23,Marrakech, Morocco.Otakar Smr?.
2007.
Functional Arabic Morphology.
For-mal System and Implementation.
Ph.D. thesis, CharlesUniversity in Prague, Prague, Czech Republic.Lamia Tounsi, Mohammed Attia, and Josef van Gen-abith.
2009.
Automatic Treebank-Based Acquisi-tion of Arabic LFG Dependency Structures.
In Pro-ceedings of the EACL 2009 Workshop on Computa-tional Approaches to Semitic Languages, pages 45?52,Athens, Greece.470
