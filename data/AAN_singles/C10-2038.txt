Coling 2010: Poster Volume, pages 329?337,Beijing, August 2010Utilizing User-input Contextual Terms for Query DisambiguationByron J. GaoTexas State Universitybgao@txstate.eduDavid C. AnastasiuTexas State Universityda1143@txstate.eduXing JiangNanyang Technological Universityjian0008@ntu.edu.sgAbstractPrecision-oriented search results such asthose typically returned by the majorsearch engines are vulnerable to issues ofpolysemy.
When the same term refers todifferent things, the dominant sense is pre-ferred in the rankings of search results.In this paper, we propose a novel tech-nique in the context of web search that uti-lizes contextual terms provided by usersfor query disambiguation, making it pos-sible to prefer other senses without alter-ing the original query.1 IntroductionWorld Wide Web and search engines have becomean indispensable part of everyone?s everyday life.While web search has come a long way over thepast 10 years, it still has a long way to go to re-spond to the ever-increasing size of the web andneeds of web surfers.
Today, web search is un-der intensive and active research, drawing unpar-alleled attention from both industry and academia.Need of disambiguation.
One of the majorchallenges in web search lies in unsatisfactory rel-evance of results caused by ambiguity.
Queryterms are inherently ambiguous due to polysemy,and most queries are short containing 1 to 3 termsonly (Jansen et al, 2000).
Thus queries are in gen-eral prone to ambiguity of user intent or informa-tion needs, resulting in retrieval of enormous irrel-evant pages.
As the web increases in size at an in-creasing rate, ambiguity becomes ubiquitous andusers are in increasing need of effective means ofdisambiguation.
The ambiguity issue and its con-sequences are demonstrated in Example 1.Example 1 There are 17 entries in Wikipedia fordifferent renown individuals under the same nameof ?Jim Gray?, including a computer scientist, asportscaster, a zoologist, a politician, a film di-rector, a cricketer, and so on.
Suppose we intendto find information about Jim Gray, the Turingaward winner, we can issue a query of ?Jim Gray?in Yahoo!For this extremely famous name in com-puter science, only 3 are relevant in the top 10results.
They are his Wikipedia entry, homepageat Microsoft Research, and DBLP entry.Straightforward approach.
One intuitive wayof disambiguation would be to apply available do-main knowledge and refine the query by addingsome confining contextual terms.
This would gen-erally improve precision.
However, there are sev-eral inevitable problems in this approach.
First,the improvement on precision is at the sacrificeof recall.
For example, many Jim Gray pagesmay not contain the added contextual terms andare thus excluded from the search results.Second, the query is altered, leading to unfavor-able ranking of results.
Term proximity matterssignificantly in ranking (Manning et al, 2008).Some good pages w.r.t.
the original query maybe ranked low in the new search results becauseof worsened term proximity and relevance w.r.t.the new query.
Thus, with this straightforwardapproach only limited success can be expected atbest, as demonstrated in Example 2.Example 2 Suppose we know that Jim Gray isa computer scientist, we can issue a query of ?JimGray computer?.
All the top 10 results are aboutJim Gray and relevant.
However, many of themare trivial pages, failing to include 2 of the 3 mostimportant ones.
His DBLP entry appears as the32927th result, and his homepage at Microsoft Re-search appears as the 51st result.This limited success is achieved by using acarefully selected contextual term.
?Computer?is a very general term appearing on most of theJim Gray pages.
Also, there are no other com-petitively known computer people with the samename.
Most other contextual terms would performmuch worse.
Thus a third problem of this straight-forward query refinement approach is that onlyfew contextual terms, which may not be avail-able to users, would possibly achieve the limitedsuccess.
Often, much of our domain knowledgewould cause more damage than repair and is prac-tically unusable, as demonstrated in Example 3.Example 3 Suppose we know that Jim Grayhas David DeWitt as a close friend and colleague,we can issue a query of ?Jim Gray David De-Witt?.
Again, all the top 10 results are aboutJim Gray and relevant.
However, the theme ofthe query is almost completely altered.
Evidently,the 1st result ?Database Pioneer Joins Microsoftto Start New Database Research Lab?, amongmany others, talks about David DeWitt.
It is rele-vant to Jim Gray only because the lab is named?Jim Gray Systems Lab?
in honor of him.The Bobo approach.
Can we freely apply ourdomain knowledge to effectively disambiguatesearch intent and improve relevance of resultswithout altering the original query?
For this pur-pose, we propose and implement Bobo.1For conceptual clarity, the Bobo interface fea-tures two boxes.
Besides a regular query box, anadditional box is used to take contextual termsfrom users that capture helpful domain knowl-edge.
Contextual terms are used for disambigua-tion purposes.
They do not alter the original querydefined by query terms.
Particularly, unlike inthe straightforward approach, positive contextualterms are not required to be included in searchresults and negative contextual terms are not re-quired to be excluded from search results.
Con-textual terms help estimate relevance of search re-sults, routing them towards a user intended do-1Bobo has been implemented using Yahoo!
web searchAPI and maintained at http://dmlab.cs.txstate.edu/bobo/.main, filtering out those not-in-domain, or irrel-evant, results.Bobo works in two rounds.
In round I, aquery is issued using by default the combinationof query terms and contextual terms, or just thecontextual terms if the query returns too few re-sults.
Then from the results, some top-rankedhigh-quality pages are (automatically) selected asseeds.
In round II, a query is issued using thequery terms.
Then the results are compared withthe seeds and their similarities are computed.
Thesimilarity values reflect the degree of relevance ofsearch results to the user intent, based on whichthe results are re-ranked.Example 4 reports the Bobo experiment usingthe same contextual terms as in Example 3.Example 4 As in Example 3, suppose we knowJim Gray has David DeWitt as a colleague.Then with Bobo, we can enter ?Jim Gray?
in thequery box and ?David DeWitt?
in the auxiliarybox.
As a result with default preferences, all thetop 10 results are relevant including all the top3 important Jim Gray pages.
From the top 10,only 1 page, the DBLP entry, contains ?David De-Witt?
as they coauthored papers.
The theme of thequery is not altered whereas in Example 3, all thetop 10 results contain ?David DeWitt?.In Example 4, the selected seeds are relevant toJim Gray.
Observe that seeds can be useful ifthey are relevant to the user-intended domain, notonly the user-intended query.
Bobo works effec-tively with such seeds and thus can utilize a muchexpanded range of domain knowledge.
Helpfulcontextual terms do not even need to co-occurwith query terms on any page.
They only need tooccur, possibly separately, on some pages of thesame domain, as demonstrated in Example 5.Example 5 Using the criteria of being in thesame community as Jim Gray but co-occuringon no web pages, we randomly chose a studentname, Flavia Moser.
In Bobo, we entered ?JimGray?
in the query box, ?Flavia Moser?
in theauxiliary box, and used only the contextual termsfor the round I query.
As a result, 11 of the top 12results were relevant including all the top 3 im-portant Jim Gray pages.
Of course, none of thereturned pages contains ?Flavia Moser?.3302 Related WorkDisambiguating search intent, capturing informa-tion needs and improving search performancehave been a fundamental research objective ininformation retrieval and studied from differentperspectives.
Voorhees (1993) shows that dis-ambiguation cannot be easily resolved using the-sauruses.
The filtering problem (Baeza-Yates andRibeiro-Neto, 1999; Schapire et al, 1998) viewsdisambiguation as a binary text classification taskassigning documents into one of the two cate-gories, relevant and irrelevant.
The routing prob-lem (Schutze et al, 1995; Singhal et al, 1997)differs from text classification in that search re-sults need to be ranked instead of just classified(Gkanogiannis and Kalamboukis, 2008).Contextual search (Lawrence, 2000; Finkel-stein et al, 2002; Kraft et al, 2006), personalizedsearch (Haveliwala, 2002; Teevan et al, 2005;Zhu et al, 2008), and implicit relevance feed-back (Kelly and Teevan, 2003; Joachims et al,2005; White et al, 2004) generally utilize long-term search history to build user profiles.
Theseprofiles are used on a regular basis to guide manyqueries.
Such approaches entail little extra userinvolvement in search, but need to manage pro-files, face the privacy issue, and swallow the in-flexibility in context switch.Explicit and pseudo relevance feedback (RF)techniques (Ruthven and Lalmas, 2003; Baeza-Yates and Ribeiro-Neto, 1999; Manning et al,2008) are more related to Bobo in the sense thatthey do not build long-term profiles.
Instead, theyconstruct a one-time search context that are usedonly once to guide a single query each time.
Suchapproaches enjoy the flexibility to switch sponta-neously from one domain to another in responseto different information needs.RF is regarded as the most popular query ref-ormation strategy (Baeza-Yates and Ribeiro-Neto,1999).
It iterates in multiple rounds, typicallytwo, to modify a query step by step.
Explicit RFasks explicit feedback from users, whereas pseudo(or blind) RF assumes relevance of top-ranked re-sults.
The problem of explicit RF is that it requirestoo much user involvement.
Users are often reluc-tant to provide explicit feedback, or do not wishto prolong the search interaction.
Web search en-gines of today do not provide this facility.
Ex-cite.com initially included but dropped it due tothe lack of use (Manning et al, 2008).Pseudo RF, first suggested by Croft and Harper(1979) and since widely investigated, automatesthe manual part of RF, so that users get im-proved search performance without extended in-teractions.
Psuedo RF has been found to improveperformance in the TREC ad hoc task and CornellSMART system at TREC 4 (Buckley et al, 1995).Unfortunately, pseudo RF suffers from a majorflaw, the so-called query drift problem.
Querydrift occurs when the feedback documents containfew or no relevant ones.
In this case, search resultswill be routed farther away from the search intent,resulting in even worse performance.
Differentapproaches (Mitra et al, 1998; Yu et al, 2003;Lee et al, 2008)have been proposed to alleviatequery drift but with little success.
Some querieswill be improved, others will be harmed (Ruthvenand Lalmas, 2003).Similarly to RF, Bobo works in two rounds.Similarly to pseudo RF, it makes use of top-rankedround I results.
However, Bobo and RF differfundamentally in various aspects.Firstly, Bobo is not a query reformation tech-nique as RF.
In RF, the automatically generatedadditional terms become part of the reformedquery to be issued in round II, while in Bobo, theuser-input contextual terms are not used in roundII.
The terms generated by RF may work wellas contextual terms for Bobo but not the otherway around.
In general, effective contextual termsform a much larger set.In query reformation, it is often hard to under-stand why a particular document was retrieved af-ter applying the technique (Manning et al, 2008).In Bobo, the original query is kept intact and onlythe ranking of search results is changed.Secondly, in RF, only query terms are used inround I queries.
In Bobo, by default the combi-nation of query terms and contextual terms, bothentered by users, is used, leading to much morerelevant seeds that are comparable to explicit RF.In this sense, Bobo provides a novel and effectiveremedy for query drift.Beyond that, Bobo can use contextual terms331only to obtain seeds that are relevant to the user-intended domain and not necessarily the user-intended query, leading to effective utilization ofa largely expanded range of domain knowledge.Thirdly, RF can have practical problems.
Thetypically long queries (usually more than 20terms) generated by RF techniques are inefficientfor IR systems, resulting in high computing costand long response time (Manning et al, 2008).
InBobo, however, both query terms (1 to 3) andcontextual terms (1 to 2) are short.
A round Iquery combining the two would typically contain2 to 5 terms only.3 OverviewBobo uses the vector space model, where bothdocuments and queries are represented as vectorsin a discretized vector space.
Documents used insimilarity comparison can be in the form of ei-ther full pages or snippets.
Documents are pre-processed and transformed into vectors based ona chosen term weighting scheme, e.g., TF-IDF.The architecture of Bobo is shown in Figure 1.Without input of contextual terms, Bobo worksexactly like a mainstream search engine and thedashed modules will not be executed.
Input ofcontextual terms is optional in need of disam-biguation of user intent.
Domain knowledge, di-rectly or indirectly associated with the query, canbe used as ?pilot light?
to guide the search towardsa user-intended domain.With input of contextual terms, Bobo worksin two rounds.
In round I, a query is issued us-ing by default the combination of query terms andcontextual terms, or just the contextual terms ifthey are unlikely to co-occur much with the queryterms.
Then from the results, the top k documents(full pages or snippets) satisfying certain qual-ity conditions, e.g., number of terms containedin each seed, are selected as seeds.
Optionally,seeds can be cleaned by removing the containedquery terms to reduce background noise of indi-vidual seeds, or purified by removing possibly ir-relevant seeds to improve overall concentration.Contextual terms themselves can be used as an elfseed, which is a special document allowing nega-tive terms, functioning as an explicit feedback.In round II, a query is issued using the queryround Iround IIsearchengineseed qualitycontrolquery terms+ contextual termsorcontextual termsround Iresults seedssearch engine similarity computation query terms round IIresultsre-ranked round II resultsFigure 1: Architecture of Bobo.terms.
Then, each returned result (full page orsnippet) is compared to the seeds to compute asimilarity using a designated similarity measure,Jaccard coefficient or Cosine coefficient.
In thecomputation, seeds can be combined to form aprototype as in Rocchio, or not combined us-ing none generalization as in instance-based lazylearning to better capture locality and handle poly-morphic domains.
Based on the assumption thatseeds are highly relevant, the similarity values es-timate the closeness of search results to the userintent, based on which the results are re-ranked.Bobo was implemented using Yahoo!
websearch API.
For each query, Bobo retrieves 30HTML pages from the API.
If snippets are usedfor seeding and comparison, the response timeof Bobo is sufficiently fast.
If full pages areused, page downloading and preprocessing areprohibitively time-consuming.
However, the goalof Bobo is to illustrate the promise of the noveldisambiguation approach.
If Bobo were imple-mented at the server (search engine) side, re-sponse time would not be an issue.4 Principles and PreferencesIn this section, we introduce in detail the designprinciples and preferences of Bobo regarding thevarious key issues.
We also discuss possible im-provements in these aspects.4.1 Use of Contextual TermsHow to use contextual terms has a fundamentalimpact on the behavior and performance of Bobo.In round I.
By default, the combination ofquery terms and contextual terms are used inround I queries.
This produces seeds that are rel-evant to the user-intended query.
For instance, inExample 4, the seeds are relevant to Jim Gray.This usage of contextual terms actually provides anovel and effective remedy for query drift, thanksto the input of domain knowledge.332Generally, a large portion of domain knowledgecannot be utilized in a straightforward manner,due to the fact that contextual terms may co-occurwith query terms in very few or none web pages.However, as shown in Example 5, Bobo allowsusing only contextual terms for round I queries,enabling utilization of indirectly associated do-main knowledge.As elf seed.
Contextual terms can be consid-ered forming a pseudo document, which can beoptionally used as a seed.
We call such a seed elfseed as it is actually a piece of explicit relevancefeedback.
Unlike normal seeds, an elf seed maycontain positive as well as negative terms, provid-ing a way of collecting positive as well as negativeexplicit feedback.Discussion.
The option of combing queryterms and contextual terms in round I queries canbe automated.
The idea is to combine the termsfirst, then test the kth result to see whether it con-tains all the terms.
If not, only the contextualterms should be used in the query.4.2 Quality of SeedsAs in pseudo relevance feedback, quality of seedsplays an critical role in search performance.
Thedifference is that in Bobo, input of contextualterms is largely responsible for the much im-proved relevance of seeds.
To provide furtherquality control, Bobo accepts several user-inputthresholds, e.g., number of seeds and number ofterms contained in each seed.
Beyond that, Boboalso provides the following options.Removing query terms.
By default, Bobouses a combination of contextual terms and queryterms in round I queries.
Thus usually all theseeds contain the query terms.
Round II resultscontain the query terms as well.
Then, in simi-larity computation against the seeds, those queryterms contribute almost equally to each round IIresult.
This amount of contribution then becomesbackground noise, reducing the sensitivity in dif-ferentiating round II results.By default, Bobo removes query terms fromseeds.
Although a simple step, this option signifi-cantly improves performance in our experiments.Purifying seeds.
Different approaches havebeen proposed to alleviate query drift by improv-ing relevance of pseudo feedback, but with limitedsuccess (Ruthven and Lalmas, 2003).
In Bobo,due to the input of domain knowledge, we canwell assume that the majority of seeds are rele-vant, based on which, we can design simple mech-anisms to purify seeds.
Briefly, we first calculatethe centroid of seeds.
Then, we compute the sim-ilarity of each seed against the centroid, and re-move those outlying seeds with poor similarities.Discussion.
Current search engines take intoaccount link-based popularity scores in rankingsearch results.
In Bobo, round I search resultsare not used to directly meet information needs ofusers.
They are never browsed by users.
Thus,different search engines with alternative rankingschemes may be used to better fulfill the purposeof round I queries.Round I queries do not need to be issued to thesame region as round II queries either.
Working ina more quality region may help avoid spammingand retrieve better candidates for seed selection.4.3 Term WeightingBobo uses two term weighting schemes.
The de-fault one is the conventional TF-IDF.
The otherscheme, TF-IDF-TAI, uses term association tofavor terms that show high co-occurrence withquery terms.
It is tailored to Bobo, where doc-uments are not compared in isolation, but being?watched?
by a query.
While TF-IDF can be con-sidered global weighting independent of queries,TF-IDF-TAI can be considered local weighting.Here we omit the details due to the page limit.IDF estimation.
To estimate the IDF valuesof terms, Bobo used 664, 103 documents in theAd-hoc track of TREC dataset.2 These documentscan produce a reasonable approximation as theycover various domains such as newspapers, U.S.patents, financial reports, congressional records,federal registers, and computer related contents.In particular, for a term A, IDF (A) =log2 nDF (A) , where DF (A) is the document fre-quency of A in the TREC data set and n =664,103.2http://trec.nist.gov/data/docs eng.html.3334.4 Similarity ComputationBy computing similarities between round II re-sults and seeds, Bobo estimates how close differ-ent results are to the search intent.Document type.
Seeds can either be in typeof snippets (including titles) or full pages.
So itis with round II results.
White et al (2007) re-ported that snippets performed even better thanfull texts for the task of pseudo RF.
In our experi-ments, snippets also performed comparably to fullpages.
Thus, Bobo uses ?snippet?
as the defaultoption for fast response time.Similarity measure.
Bobo uses two standardsimilarity measures, Cosine coefficient (default)and Jaccard coefficient.
Both performed verywell in our experiments, with the default optionslightly better.Prototype-based similarity.
Bobo imple-ments two types of similarity computation meth-ods, prototype-based or instance-based, with thelatter as the default option.The prototype-based method is actually a formof the well-known Rocchio algorithm (Rocchio,1971; Salton and Buckley, 1997), which is effi-cient but would perform poorly in the presence ofpolymorphic domains.
In this method, the seedsare combined and the centroid of seeds is used insimilarity computation.
Given a set S of seeds, thecentroid ~u is calculated as ~u = 1|S|?s?S ~s, where~s is the vector space representation of seed s ?
S.Recall that the original Rocchio algorithm forquery reformation is defined as follows,~qe = ?~q + ?1|Dr|?~dj?Dr~dj ?
?1|Dir|?~dj?Dir~djwhere q is the original query vector, qe is themodified query vector, and Dr and Dir repre-sent the sets of known relevant and irrelevantdocument vectors respectively.
?, ?, and ?
areempirically-chosen tuning parameters.If we assign ?
= 0 and ?
= 0, the Rocchioformula agrees with our definition of centroid ofseeds.
We assign ?
= 0 because Bobo does nottarget query reformation.
We assign ?
= 0 notbecause of the lack of negative feedback, whichis not hard to identify from low-ranked round Isearch results.
The reason is that even in explicit+ +++++++ ++?
?Figure 2: A Polymorphic Domain.RF, there is no evidence that negative feedbackimproves performance (Schutze et al, 1995).Instance-based similarity.
Rocchio is simpleand efficient.
However, it over-generalizes train-ing data and is inaccurate in the presence of poly-morphic, or disjunctive, domains.
In Figure 2, the10 seeds labeled by ?+?
are split into two sepa-rate and rather distant sub-domains.
The centroidof seeds labeled by ???
is not local to any sub-domain.
Search result 1 is close to the centroidwhereas result 2 is not.
Rocchio would give highrelevance score to result 1 and poor score to result2.
However, result 2 actually belongs to one of thetwo sub-domains whereas result 1 does not.To handle polymorphic domains and capturelocality, Bobo uses an instance-based approach,where the similarity of a document against eachindividual seed is computed, weighted, and ag-gregated.
Let sim(d, S) denote the similarity be-tween a document d and a set S of seeds, then,sim(d, S) =?s?Ssim(d, s)?
sim(d, s)Using this approach, result 2 will receive muchhigher relevance score than result 1 in Figure 2.Note that, this approach resembles instance-based lazy learning such as k-nearest neighborclassification.
Lazy learning generally has supe-rior performance but would suffer from poor clas-sification efficiency.
This, however, is not a crit-ical issue in our application because we do nothave many seeds.
The default number of seedsin Bobo is set to 10.Discussion.
While Bobo adopts rather stan-dard approaches, we are aware of the many otherapproaches proposed in the literature for pairwiseweb page similarity computation.
An interestingdirection to investigate would be a link-based orhybrid approach.
For example, Vassilvitskii andBrill (2006) uses web-graph distance for relevancefeedback in web search.3345 Empirical EvaluationWe evaluated Bobo in comparison with regularYahoo!
search with and without using contextualterms.
Results returned from Yahoo!
may varywith time.
This, however, will not change the gen-eral trends revealed by our empirical study.
Fromthese trends we conclude that, Bobo is a sim-ple yet effective paradigm for query intent disam-biguation without altering the original query andwith maximized utilization of domain knowledge.5.1 Experiment Setting and MethodologyParameter setting.
To emphasize the Bobo idea,unless otherwise specified, we used default op-tions in the experiments that implement conven-tional approaches, e.g., TF-IDF for term weight-ing and Cosine coefficient for similarity compu-tation.
By default, number of seeds was set to10 with each seed having at least 10 terms.
Thenumber of layers was set such that round II resultswere re-ranked in decreasing order of similarity.Cleaning seeds was set to yes.
Purifying seeds,elf seed and weighting seeds were set to no.Dataset.
Finding information about people isone of the most common search activities.
Around30% of web queries include person names (Artileset al, 2005).
Person names, however, are highlyambiguous, e.g., only 90,000 different names areshared by 100 million people according to theU.S.
Census Bureau (Guha and Garg, 2004).To test the disambiguation effectiveness ofBobo, we constructed 60 ambiguous namequeries and 180 test cases from the Wikipedia dis-ambiguation pages.3In Wikipedia, articles about two or more differ-ent topics could have the same natural page title.Disambiguation pages are then used to solve theconflicts.
From the various categories, we usedthe human name category, containing disambigua-tion pages for multiple people of the same name.For each name, the disambiguation page lists allthe different people together with their brief in-troductions.
For example, an Alan Jackson is in-troduced as ?born 1958, American country musicsinger and songwriter?.3en.wikipedia.org/wiki/Category:Disambiguation pages.Person names were chosen from the most com-mon English first and last names for the year 2000published on Wikipedia.
The first 10 male andfirst 10 female given names were combined withthe first 10 most common last names to make a listof 200 possible names.
From this list, names werechosen based on the following criteria.
For eachname, there are at least 2 distinct people with thesame name, each having at least 3 relevant pagesin the returned 30 results.In total 60 names were chosen as ambiguousqueries.
For each query, the actual informationneed was predetermined in a random manner.Then, for this predetermined person, 3 contextualterms were selected from her brief introduction, orher Wikipedia page in case the introduction wastoo short.
For example, for the above Alan Jack-son example, ?music?, ?singer?, or ?songwriter?can be selected as contextual terms.
Contextualterms were used one at a time, thus there are 3 testcases for each ambiguous query.The identification of relevance of search resultswas done manually.
For each query, let R30 be theset of relevant pages w.r.t.
the information needcontained in the 30 retrieved results.
R30 can beconsidered containing the most important relevantpages for the original query.Comparison partners and evaluation mea-sures.
To compare with Bobo, two types ofregular search methods were used.
The Yahoo!method uses the original query and performs thesimplest Yahoo!
web search, returning the sameset of results as Bobo but without re-ranking.To demonstrate the relevance improvement ofBobo over the Yahoo!
method, we used a coupleof standard ranking-aware evaluation measures,which were 11-point precision-recall graph, pre-cision at k graph, Mean Average Precision (MAP)and R-precision.The Yahoo!-refined method is the straightfor-ward query refinement approach we previouslydiscussed.
It refines the original query by addingsome contextual terms.
The refined query altersthe original query, leading to unfavorable rankingof results and failing to include many importantrelevant pages, i.e., R30 pages, in the top results.To demonstrate this point, we used the recall atk evaluation measure, which measures the frac-33500.20.40.60.810 0.2 0.4 0.6 0.8 1PrecisionRecallBoBo - Cosine BoBo - Jaccard Yahoo!Figure 3: Bobo vs. Yahoo!
on Averaged 11-pointPrecision-Recall.00.20.40.60.815 10 15 20 25 30PrecisionkBoBo - Cosine BoBo - Jaccard Yahoo!Figure 4: Bobo vs. Yahoo!
on Averaged Preci-sion at k.tion of relevant pages (here, ones in R30) con-tained in the top k results.In the entire empirical study, the Yahoo!
resultswere averaged over 60 queries, whereas all otherresults were averaged over 180 test cases.5.2 Evaluation ResultsIn Figures 3, 4 and 5, Bobo results using both Co-sine similarity and Jaccard coefficient are shown.The two performed similarly, with the former (de-fault) slightly better.Bobo vs. Yahoo!.
The 11-point precision-recall graphs and precision at k graphs are pre-sented in Figure 3 and Figure 4 respectively.Web search users would typically browse a fewtop-ranked results.
From Figure 4 we can see thatfor k =15, 10 and 5, the precision improvementof Bobo over Yahoo!
is roughly 20% ?
40%.In addition, the MAP and R-precision val-ues for Bobo are 0.812 and 0.740 respectively,whereas they are 0.479 and 0.405 for Yahoo!
re-00.20.40.60.815 10 15 20 25 30RecallkBoBo - Cosine BoBo - Jaccard Yahoo!-refinedFigure 5: Bobo vs. Yahoo!-refined on AveragedRecall at k.spectively.
The improvement of Bobo over Ya-hoo!
is about 33% for both measures.Bobo vs. Yahoo!-refined.
The recall at kgraphs are presented in Figure 5.
From the fig-ure we can see that for k = 15, k = 10 and k = 5,the recall (of important R30 pages) improvementof Bobo over Yahoo!
is roughly 30%.The results demonstrated that, although thestraightforward query refinement approach can ef-fectively improve relevance, it fails to rank thoseimportant relevant pages high, as it alters the orig-inal query and changes the query themes.
Bobo,on the contrary, overcomes this problem by us-ing the contextual terms ?in the backstage?, ef-fectively improving relevance while keeping theoriginal query intact.Due to the page limit, here we omit other se-ries of experiments that evaluated the flexibility ofBobo in choosing effective contextual terms andhow the varied user preferences affect its perfor-mance.
A user study was also conducted to testthe usability and performance of Bobo.6 ConclusionsAs the web increases in size at an increasing rate,ambiguity becomes ubiquitous.
In this paper,we introduced a novel Bobo approach to achievesimple yet effective search intent disambiguationwithout altering the original query and with max-imized domain knowledge utilization.Although we introduce Bobo in the context ofweb search, the idea can be applied to the set-tings of traditional archival information retrievalor multimedia information retrieval.336ReferencesArtiles, Javier, Julio Gonzalo, and Felisa Verdejo.2005.
A testbed for people searching strategies inthe WWW.
In SIGIR.Baeza-Yates, R. and B. Ribeiro-Neto.
1999.
ModernInformation Retrieval.
Addison-Wesley.Buckley, Chris, Singhal Amit, , and Mitra Mandar.1995.
New retrieval approaches using smart: Trec4.
In TREC.Croft, W. and D. Harper.
1979.
Using probabilisticmodels of information retrieval without relevanceinformation.
Journal of Documentation, 35(4):285?295.Finkelstein, Lev, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing search in context: the con-cept revisited.
ACM Transactions on InformationSystems, 20(1):116?131.Gkanogiannis, Anestis and Theodore Kalamboukis.2008.
An algorithm for text categorization.
In SI-GIR.Guha, R. V. and A. Garg.
2004.
Disambiguating peo-ple in search.
In WWW.Haveliwala, Taher H. 2002.
Topic-sensitive pagerank.In WWW.Jansen, B. J., A. Spink, , and T. Saracevic.
2000.
Reallife, real users and real needs: A study and analysisof users queries on the web.
Information Processingand Management, 36(2):207?227.Joachims, Thorsten, Laura Granka, Bing Pan, HeleneHembrooke, and Geri Gay.
2005.
Accurately inter-preting clickthrough data as implicit feedback.
InSIGIR.Kelly, Diane and Jaime Teevan.
2003.
Implicit feed-back for inferring user preference: a bibliography.SIGIR Forum, 37(2):18?28.Kraft, Reiner, Chi Chao Chang, Farzin Maghoul, andRavi Kumar.
2006.
Searching with context.
InWWW.Lawrence, Steve.
2000.
Context in web search.
IEEEData Engineering Bulletin, 23(3):25?32.Lee, Kyung Soon, W. Bruce Croft, and James Al-lan.
2008.
A cluster-based resampling method forpseudo-relevance feedback.
In SIGIR.Manning, Christopher D., Prabhakar Raghavan, andHinrich Schutze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press.Mitra, M., A. Singhal, and C. Buckley.
1998.
Improv-ing automatic query expansion.
In SIGIR.Rocchio, J.
1971.
Relevance Feedback in Informa-tion Retrieval.
In The SMART Retrieval System?
Experiments in Automatic Document Processing.Prentice-Hall.Ruthven, Ian and Mounia Lalmas.
2003.
A survey onthe use of relevance feedback for information accesssystems.
Knowledge Engineering Review, 18(1).Salton, Gerard and Chris Buckley.
1997.
Improvingretrieval performance by relevance feedback.
Mor-gan Kaufmann.Schapire, Robert E., Yoram Singer, and Amit Singhal.1998.
Boosting and rocchio applied to text filtering.In SIGIR.Schutze, Hinrich, David A.
Hull, and Jan O. Peder-sen. 1995.
A comparison of classifiers and doc-ument representations for the routing problem.
InSIGIR.Singhal, Amit, Mandar Mitra, and Chris Buckley.1997.
Learning routing queries in a query zone.
InSIGIR.Teevan, Jaime, Susan T. Dumais, and Eric Horvitz.2005.
Personalizing search via automated analysisof interests and activities.
In SIGIR.Vassilvitskii, Sergei and Eric Brill.
2006.
Using web-graph for relevance feedback in web search.
In SI-GIR.Voorhees, Ellen M. 1993.
Using wordnet to disam-biguate word senses for text retrieval.
In SIGIR.White, Ryen W., Joemon M. Jose, C. J.
Van Rijsber-gen, and Ian Ruthven.
2004.
A simulated study ofimplicit feedback models.
In ECIR.White, Ryen W., Charles L.A. Clarke, and SilviuCucerzan.
2007.
Comparing query logs andpseudo-relevance feedback for web search query re-finement.
In SIGIR.Yu, Shipeng, Deng Cai, Ji-Rong Wen, and Wei-YingMa.
2003.
Improving pseudo-relevance feedbackin web information retrieval using web page seg-mentation.
In WWW.Zhu, Yangbo, Jamie Callan, and Jaime Carbonell.2008.
The impact of history length on personalizedsearch.
In SIGIR.337
