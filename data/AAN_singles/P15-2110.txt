Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 668?673,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsOne Tense per Scene: Predicting Tense in Chinese ConversationsTao Ge1,2, Heng Ji3, Baobao Chang1,2, Zhifang Sui1,21Key Laboratory of Computational Linguistics, Ministry of Education,School of EECS, Peking University, Beijing, 100871, China2Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, 221009, China3Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180, USAgetao@pku.edu.cn, jih@rpi.edu,chbb@pku.edu.cn, szf@pku.edu.cnAbstractWe study the problem of predicting tensein Chinese conversations.
The uniquechallenges include: (1) Chinese verbs donot have explicit lexical or grammaticalforms to indicate tense; (2) Tense in-formation is often implicitly hidden out-side of the target sentence.
To tacklethese challenges, we first propose a setof novel sentence-level (local) features us-ing rich linguistic resources and then pro-pose a new hypothesis of ?One tense perscene?
to incorporate scene-level (global)evidence to enhance the performance.
Ex-perimental results demonstrate the powerof this hybrid approach, which can serveas a new and promising benchmark.1 IntroductionIn natural languages, tense is important to indicatethe time at which an action or event takes place.In some languages such as Chinese, verbs do nothave explicit morphological or grammatical formsto indicate their tense information.
Therefore, au-tomatic tense prediction is important for both hu-man?s deep understanding of these languages aswell as downstream natural language processingtasks (e.g., machine translation (Liu et al., 2011)).In this paper, we concern ?semantic?
tense (timeof the event relative to speech time) as opposedto morphosyntactic tense systems found in manylanguages.
Our goal is to predict the tense (past,present or future) of the main predicate1of eachsentence in a Chinese conversation, which hasnever been thoroughly studied before but is ex-tremely important for conversation understanding.Some recent work (Ye et al., 2006; Xue andZhang, 2014; Zhang and Xue, 2014) on Chinese1The main predicate of a sentence can be considered equalto the root of a dependency parsetense prediction found that tense in written lan-guage can be effectively predicted by some fea-tures in local contexts such as aspectual markers(e.g.
?
(zhe), ?
(le), ?
(guo)) and time ex-pressions (e.g., ??
(yesterday)).
However, it ismuch more challenging to predict tense in Chineseconversations and there has not been an effectiveset of rules to predict Chinese tense so far due tothe complexity of language-specific phenomena.Let?s look at the examples shown in Table 1.In general, there are three unique challenges fortense prediction in Chinese conversations:(1) Informal verbal expressions: sentences ina conversation are often grammatically incorrect,which makes aspectual marker based evidence un-reliable.
Moreover, sentences in a conversationoften omit important sentence components.
Forexample, in conversation 1 in Table 1, ???
(if)?which is a very important cue to predict tense ofverb ??(destroy)?
is omitted.
(2) Effects of interactions on tense: In contrast toother genres, conversations are interactive, whichmay have an effect on tense: in some cases, tensecan only be inferred by understanding the interac-tions.
For example, we can see from conversations2, 3 and 4 in Table 1 that when the second person(?
(you)) is used as the object of the predicate ???
(tell)?, the predicate describes the action duringthe conversation and thus its tense is present.
Incontrast, when the third person is used in a sen-tence, it is unlikely that the tense of the predicateis present because it does not describe an actionduring the conversation.
This challenge is uniqueto Chinese conversations.
(3) Tense ambiguity in a single sentence:Sentence-level analysis is often inadequate to dis-ambiguate tense.
For example, it is impossible todetermine whether ???(tell)?
in conversations 3and 4 in Table 1 is a past action (the speaker al-ready told) or a future action (the speaker hasn?ttold yet) only based on sentence-level contexts.6681a: [??(if)]?(you)?(touch)?(my)??(son)??(once)??(I)?(first)?(destroy)??(you)?
(If you touch myson, I?ll destroy you.
)2 b: ?(I)??(tell)?(you)?????(flight)??(cancel)??
(I?m telling you: the flight is canceled.)3c:?
(you)??
(just now)??
(to him)?(say)??(what)??
(What did you say to him just now?
)d: ?(I)[??
(just now)]??(tell)?(him)?????(flight)??(cancel)??
(I told him the flight is canceled.
)4e: ?(you)?(will)?(do)?(what)?(go)?
(What are you going to do?
)f: ?(I)[?(will)]??(tell)?(him)?????(fight)??(cancel)??
(I?ll tell him the flight is canceled.
)5a: ??(happen)???(what)??(event)?
(What happened?
)b: ?(I)???
(Wu Qing)??
(with) (I was with Wu Qing)b: ??(We)?(keep)??(surveilance)???
(a cargo) (We were keeping surveillance on a cargo...)b: ??(We)??(suspect)??(thoses)?(are)???(stolen)??
(antiques) (We suspected those were stolen an-tiques)b: ???
(those guys)???(suddenly)??(walk)??(out)?(beat)??
(us) (Suddenly, all those guys walked outto beat us up!
)b: ?(I)?(want)??
(call the police)??(they)???
(stop) (They stopped only when I tried to call the police)Table 1: Five sample conversations that show the challenges in tense prediction in Chinese conversations.a,b,c,d at the beginning of each sentence denote various speakers.
The words in square brackets areomitted content in the original sentences and the underlined words are main predicates.In fact, the sentence in conversation 3 omits ???
(just now)?
which indicates past tense and thesentence in the conversation 4 omits ??
(will)?which indicates future tense.
If we add the omittedword back to the original sentence, there will notbe tense ambiguity.To tackle the above challenges, we proposeto predict tense in Chinese conversations fromtwo views ?
sentence-level (local) and scene-level (global).
We first develop a local classifierwith linguistic knowledge and new conversation-specific features (Section 2.1).
Then we proposea novel framework to exploit the global contextsof the entire scene to infer tense, based on a new?One tense per scene?
hypothesis (Section 2.2).We created a new a benchmark data set2, whichcontains 294 conversations (1,857 sentences) anddemonstrated the effectiveness of our approach.ble2 Method2.1 Local PredictorWe develop a Maximum Entropy (MaxEnt) clas-sifier (Zhang, 2004) as the local predictor.Basic features: The unigrams, bigrams and tri-grams of a sentence.Dependency parsing features: We use the Stan-ford parser (Chen and Manning, 2014) to conductdependency parsing3on the target sentences anduse dependency paths associated with the mainpredicate of a sentence as well as their dependencytypes as features.
By using the parsing features,2http://nlp.cs.rpi.edu/data/chinesetense.zip3We use CCProcessed dependencies.we can not only find aspectual markers (e.g., ???
)but also capture the effect of sentence structures onthe tense.Linguistic knowledge features: We also ex-ploit the following linguistic knowledge from theGrammatical Knowledge-base of ContemporaryChinese (Yu et al., 1998) (also known as GKB):?
Tense of time expressions: GKB lists allcommon time expressions and their associ-ated tense.
For example, GKB can tell us ???
(previous years)?
and ????
(MiddleAges)?
can only be associated with the pasttense.?
Function of conjunction words: Some con-junction words may have an effect on tense.For example, the conjunction word ???(if)?
indicates a conditional clause and themain predicate of this sentence is likely to befuture tense.
GKB can tell us the function ofcommon Chinese conjunction words.Conversation-specific features: As mentioned inSection 1, different person roles being the subjector the object of a predicate may have an effect onthe tense in a conversation.
We analyze the personroles of the subject and the object of the main pred-icate and encode them as features, which helps ourmodel understand effects of interactions on tense.2.2 Global PredictorAs we discussed before, tense ambiguity in a sen-tence arises from the omissions of sentence com-ponents.
According to the principle of efficientinformation transmission (Jaeger and Levy, 2006;669Jaeger, 2010) and Gricean Maxims (Grice et al.,1975) in cooperative theory, the omitted elementscan be predicted by considering contextual infor-mation and the tense can be further disambiguated.In order to better predict tense, we propose a newhypothesis:One tense per scene: Within a scene, tense in sen-tences tends to be consistent and coherent.During a conversation, a speaker/listener canknow the tense of a predicate by either a tense in-dicator in the target sentence or scene-level tenseanalysis.
A scene is a subdivision of a conversa-tion in which the time is continuous and the topicis highly coherent and which does not usually in-volve a change of tense.
For example, for the con-versation 3 in Table 1, we can learn the scene isabout the past from the word ???
(just now)?
inthe first sentence.
Therefore, we can exploit thisclue to determine the tense of ???(tell)?
as past.Therefore, when we are not sure which tenseof the main predicate in a sentence should be,we can consider the tense of the entire scene.For example, the conversation 5 in Table 1 isabout a past scene because the whole conver-sation is about a past event.
For the sen-tence ???(We)?(keep)??(surveillance)???
(a cargo)?
where the tense of the predicate isambiguous (past tense and present tense are bothreasonable), we can exploit the tense of the scene(past) to determine its tense as past.Global tense predictionInspired by the burst detection algorithm proposedby Kleinberg (2003), we use a 3-state automatonsequence model to globally predict tense based onthe above hypothesis.
In a conversation with nsentences, each sentence is one element in the se-quence.
The sentence?s tense can be seen as thehidden state and the sentence?s features are the ob-servation.
Formally, we define the tense in the ithsentence as tiand the observations (i.e., features)in the sentence as oi.
The goal of this model is tooutput an optimal sequence t?= {t?1, t?2, ..., t?n}that minimizes the cost function defined as fol-lows:Cost(t,o) = ?n?i=1?lnP (ti|oi)+(1??
)n?1?i=11(ti+16= ti)(1)where 1(?)
is an indicator function.As we can see in (1), the cost function consistsof two parts.
The first part is the negative log like-lihood of the local prediction, allowing the modelto incorporate the results from the local predic-tor.
The second part is the cost of tense inconsis-tency between adjacent sentences, which enablesthe model to take into account tense consistencyin a scene.
Finding the optimal sequence is a de-coding process, which can be done using Viterbialgorithm in O(n) time.
The parameter ?
is usedfor adjusting weights of these two parts.
If ?
= 1,the predictor will not consider global tense consis-tency and thus the optimal sequence t?will be thesame as the output of the local predictor.Figure 1 shows how the global predictor worksfor predicting the tense in the conversation 5 inTable 1.
The global predictor can correct wronglocal predictions, especially less confident ones.p ppp ppp p c p p pcorrect tenselocal predictionglobal prediction p p p p p psentencesFigure 1: Global tense prediction for the conver-sation 5 in Table 1.3 Experiments3.1 Data and Scoring MetricTo the best of our knowledge, tense prediction inChinese conversations has never been studied be-fore and there is no existing benchmark for evalu-ation.
We collected 294 conversations (including1,857 sentences) from 25 popular Chinese movies,dramas and TV shows.
Each conversation con-tains 2-18 sentences.
We manually annotate themain predicate and its tense in each sentence.
Weuse ICTCLAS (Zhang et al., 2003) to do word seg-mentation as preprocessing.Since tense prediction can be seen as a multi-class classification problem, we use accuracy asthe metric to evaluate the performance.
We ran-domly split our dataset into three sets: training set(244 conversations), development set (25 conver-sations) and test set (25 conversations) for eval-uation.
In evaluation, we ignore imperative sen-tences and sentences without predicates.3.2 Experimental ResultsWe compare our approach with the followingbaselines:?
Majority: We label every instance with themajority tense (present tense).670?
Local predictor with basic features (Local(b))?
Local predictor with basic features + depen-dency parsing features (Local(b+p))?
Local predictor with basic features + depen-dency parsing features + linguistic knowl-edge features (Local(b+p+l))?
Local predictor + all features introduced inSection 2.1 (Local(all))?
Conditional Random Fields (CRFs): Wemodel a conversation as a sequence of sen-tences and predict tense using CRFs (Laf-ferty et al., 2001).
We implement CRFs usingCRFsuite (Okazaki, 2007) with all featuresintroduced in Section 2.1.Among the baselines, Local(b+p) is the mostsimilar model to the approaches in previous workon Chinese tense prediction in written languages(Ye et al., 2006; Xue, 2008; Liu et al., 2011).
Re-cent work (Zhang and Xue, 2014) used eventualityand modality labels as features that derived froma classifier trained on an annotated corpus.
How-ever, the annotated corpus for training the eventu-ality and modality classifier is not publicly avail-able, we cannot duplicate their approaches.Dev TestMajority 65.13% 54.01%Local(b) 69.74% 66.42%Local(b+p) 70.39% 67.15%Local(b+p+l) 71.05% 69.34%Local(all) 71.05% 69.34%CRFs 69.74% 64.96%Global 72.37% 72.26%Table 2: Tense prediction accuracy.Table 2 shows the results of various models.
Forour global predictor, the optimal ?
(0.4) is tunedon the development set and used on the test set.According to Table 2, n-grams and depen-dency parsing features4are useful to predicttense, and linguistic knowledge can further im-prove the accuracy of tense prediction.
However,adding conversation-specific features (interactionfeatures) does not benefit Local(b+p+l).
The first4We also tried adding POS tags to dependency paths butdidn?t see improvements because POS information has beenimplicitly indicated by dependency types and thus becomesredundant.reason is that the subject and the object of thepredicates in many sentences are omitted, whichis common in Chinese conversations.
The otherreason, also the main reason, is that simply usingthe person roles of the subject and the object isnot sufficient to depict the interaction.
For exam-ple, the subject and the object of the following sen-tences have the same person role but have differenttenses because ???(warn)?
is the current actionof the speaker but ??(teach)?
is not.
Therefore,to exploit the interaction features of a conversa-tion, we must deeply understand the meanings ofaction verbs.?(I)??(warn)?(you)?
(I?m warn-ing you.)?(I)?(teach)?(you)?
(I?ll teachyou.
)The global predictor significantly improves thelocal predictor?s performance (at 95% confidencelevel according to Wilcoxon Signed-Rank Test),which verifies the effectiveness of ?One tense perscene?
hypothesis for tense prediction.
It is no-table that CRFs do not work well on our dataset.The reason is that the transition pattern of tensesin a sequence of sentences is not easy to learn, es-pecially when the size of training data is not verylarge.
In many cases, the tense of a verb in asentence is determined by features within the sen-tence, which has nothing to do with tense tran-sition.
In these cases, learning tense transitionpatterns will mislead the model and accordinglyaffect the performance.
In contrast, our globalmodel is more robust because it is based on our?One tense per scene?
hypothesis which can beseen as prior linguistic knowledge, thus achievesgood performance even when the training data isnot sufficient.3.3 DiscussionThere are still many remaining challenges fortense prediction in Chinese conversations:Omission detection: The biggest challenge forthis task is the omission of sentence components.As shown in Table 1, if omitted words can be re-covered, it will be less likely to make a wrong pre-diction.Word Sense Disambiguation: Some functionwords which can indicate tense are ambiguous.For example, the function word ???
has manysenses.
It can mean??(will),??
(want) and?671?
(need), and also it is sometimes used to presentan option.
It is difficult for a system to correctlypredict tense unless it can disambiguate the senseof such function words:?
???(later)?(he)?
(will)??
(come)?
(He?ll come here later.)?
?
(I)?
(want) ?
(eat) ??
(apples)?
(Iwant to eat apples)?
?(you)?(need)??(much)??
(exercise)(You need to take more exercises.)?
???(why)?(you)?(opt)?(save)?(me)?
(Why did you save me?
)Verb Tense Preference: Different verbs may havedifferent tense preferences.
For example, ???(think)?
is often used in the past tense while ???(think)?
is usually in the present tense:?
?(I)?
?(think)?(he)?
?(won?t)?
(co-me) (I thought he would not come.)?
?(I)?
?(think)?(he)?
?(won?t)?
(co-me) (I think he won?t come.
)Generic and specific subject/object: Whetherthe subject/object is generic or specific has an ef-fect on tense.
For example, in the sentence ???(that)?
?(war)?(very)?
?(brutal)?
?, thepredicate ???(brutal)?
is in the past tensewhile in the sentence ??
?(war)?(very)??(brutal)?
?, the predicate ???(brutal)?
is inthe present tense.4 Related WorkEarly work on Chinese tense prediction (Ye etal., 2006; Xue, 2008) modeled this task as amulti-class classification problem and used ma-chine learning approaches to solve the problem.Recent work (Liu et al., 2011; Xue and Zhang,2014; Zhang and Xue, 2014) studied distant an-notation of tense from a bilingual parallel cor-pus.
Among them, Xue and Zhang (2014) andZhang and Xue (2014) improved tense predictionby using eventuality and modality labels.
How-ever, none of the previous work focused on thespecific challenge of the tense prediction in orallanguages although the dataset used by Liu et al.
(2011) includes conversations.
In contrast, thispaper presents the unique challenges and corre-sponding solutions to tense prediction in conver-sations.5 Conclusions and Future WorkThis paper presents the importance and challengesof tense prediction in Chinese conversations andproposes a novel solution to the challenges.In the future, we plan to further study thisproblem by focusing on omission detection, verbtense preference from the view of pragmatics, andjointly learning the local and global predictors.
Inaddition, we will study predicting the tense of mul-tiple predicates in a sentence and identifying im-perative sentences in a conversation, which is alsoa challenge of tense prediction.AcknowledgmentsWe thank all the anonymous reviewers for theirconstructive suggestions.
We thank Prof. Shi-wen Yu and Xun Wang for providing insights fromChinese linguistics.
This work is supported byNational Key Basic Research Program of China2014CB340504, NSFC project 61375074, U.S.DARPA Award No.
FA8750-13-2-0045 and ChinaScholarship Council (CSC, No.
201406010174).The contact author of this paper is Zhifang Sui.ReferencesDanqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In EMNLP.H Paul Grice, Peter Cole, and Jerry L Morgan.
1975.Syntax and semantics.
Logic and conversation,3:41?58.TF Jaeger and Roger P Levy.
2006.
Speakers optimizeinformation density through syntactic reduction.
InAdvances in neural information processing systems.T Florian Jaeger.
2010.
Redundancy and reduc-tion: Speakers manage syntactic information den-sity.
Cognitive psychology, 61(1):23?62.Jon Kleinberg.
2003.
Bursty and hierarchical structurein streams.
Data Mining and Knowledge Discovery,7(4):373?397.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.Feifan Liu, Fei Liu, and Yang Liu.
2011.
Learningfrom Chinese-English parallel data for Chinese tenseprediction.
In IJCNLP.Naoaki Okazaki.
2007.
CRFsuite: a fast implementa-tion of conditional random fields (CRFs).672Nianwen Xue and Yuchen Zhang.
2014.
Buy one getone free: Distant annotation of Chinese tense, eventtype, and modality.
In LREC.Nianwen Xue.
2008.
Automatic inference of thetemporal location of situations in Chinese text.
InEMNLP.Yang Ye, Victoria Li Fossum, and Steven Abney.
2006.Latent features in automatic tense translation be-tween Chinese and English.
In SIGHAN workshop.Shiwen Yu, Xuefeng Zhu, Hui Wang, and YunyunZhang.
1998.
The grammatical knowledge-base ofcontemporary Chinese?a complete specification.Yucheng Zhang and Nianwen Xue.
2014.
Automaticinference of the tense of Chinese events using im-plicit information.
In EMNLP.Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and QunLiu.
2003.
Hhmm-based Chinese lexical analyzerictclas.
In SIGHAN workshop.Le Zhang.
2004.
Maximum entropy modeling toolkitfor Python and C++.673
