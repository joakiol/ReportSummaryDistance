Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 61?69,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsEmpirical Studies in Learning to ReadMarjorie Freedman, Edward Loper, Elizabeth Boschee, Ralph WeischedelBBN Raytheon Technologies10 Moulton StCambridge, MA 02139{mfreedma, eloper, eboschee, weischedel}@bbn.comAbstractIn this paper, we present empirical results onthe challenge of learning to read.
That is, giv-en a handful of examples of the concepts andrelations in an ontology and a large corpus,the system should learn to map from text tothe concepts/relations of the ontology.
In thispaper, we report contrastive experiments onthe recall, precision, and F-measure (F) of themapping in the following conditions: (1) em-ploying word-based patterns, employing se-mantic structure, and combining the two; and(2) fully automatic learning versus allowingminimal questions of a human informant.1 IntroductionThis paper reports empirical results with an algo-rithm that ?learns to read?
text and map that textinto concepts and relations in an ontology specifiedby the user.
Our approach uses unsupervised andsemi-supervised algorithms to harness the diversityand redundancy of the ways concepts and relationsare expressed in document collections.
Diversitycan be used to automatically generate patterns andparaphrases for new concepts and relations toboost recall.
Redundancy can be exploited to au-tomatically check and improve the accuracy ofthose patterns, allowing for system learning with-out human supervision.For example, the system learns how to recog-nize a new relation (e.g.
invent), starting from 5-20instances (e.g.
Thomas Edison + the light bulb).The system iteratively searches a collection ofdocuments to find sentences where those instancesare expressed (e.g.
?Thomas Edison?s patent forthe light bulb?
), induces patterns over textual fea-tures found in those instances (e.g.
pa-tent(possessive:A, for:B)), and repeats the cycle byapplying the generated patterns to find additionalinstances followed by inducing more patterns fromthose instances.
Unsupervised measures of redun-dancy and coverage are used to estimate the relia-bility of the induced patterns and learned instances;only the most reliable are added, which minimizesthe amount of noise introduced at each step.There have been two approaches to evaluationof mapping text to concepts and relations: Auto-matic Content Extraction (ACE)1 and KnowledgeBase Population (KBP)2.
In ACE, complete ma-nual annotation for a small corpus (~25k words)was possible; thus, both recall and precision couldbe measured across every instance in the test set.This evaluation can be termed micro reading inthat it evaluates every concept/relation mention inthe corpus.
In ACE, learning algorithms hadroughly 300k words of training data.By contrast, in KBP, the corpus of documentsin the test set was too large for a complete answerkey.
Rather than a complete answer key, relationswere extracted for a list of entities; system outputwas pooled and judged manually.
This type ofreading has been termed macro reading3, sincefinding any instance of the relation in the 1.3Mdocument corpus is measured success, rather thanfinding every instance.
Only 118 queries were pro-vided, though several hundred were created anddistributed by participants.In the study in this paper, recall, precision, andF are measured for 11 relations under the followingcontrastive conditions1 http://www.nist.gov/speech/tests/ace/2 http://apl.jhu.edu/~paulmac/kbp.html3 See http://rtw.ml.cmu.edu/papers/mitchell-iswc09.pdf611.
Patterns based on words vs. predicate-argument structure vs. combining both.2.
Fully automatic vs. a few periodic res-ponses by humans to specific queries.Though many prior studies have focused onprecision, e.g., to find any text justification to an-swer a question, we focus equally on recall andreport recall performance as well as precision.
Thisaddresses the challenge of finding information onrarely mentioned entities (no matter how challeng-ing the expression).
We believe the effect will beimproved technology overall.
We evaluate our sys-tem in a micro-reading context on 11 relations.
In afully automatic configuration, the system achievesan F of .48 (Recall=.37, Precision=.68).
With li-mited human intervention, F rises to .58 (Re-call=.49, Precision=.70).
We see that patternsbased on predicate-argument structure (textgraphs) outperform patterns based on surfacestrings with respect to both precision and recall.Section 2 describes our approach; section 3,some challenges; section 4, the implementation;section 5, evaluation; section 6, empirical resultson extraction type; section 7, the effect of periodic,limited human feedback; section 8, related work;and section 9, lessons learned and conclusions.2 ApproachOur approach for learning patterns that can be usedto detect relations is depicted in Figure 1.
Initially,a few instances of the relation tuples are provided,along with a massive corpus, e.g., the web or thegigaword corpus from the Linguistic Data Consor-tium (LDC).
The diagram shows three inventor-invention pairs, beginning with Thomas Edi-son?light bulb.
From these, we find candidatesentences in the massive corpus, e.g., Thomas Edi-son invented the light bulb.
Features extracted fromthe sentences retrieved, for example features of thetext-graph (the predicate-argument structure con-necting the two arguments), provide a training in-stance for pattern induction.
The induced patternsare added to the collection (database) of patterns.Running the extended pattern collection over thecorpus finds new, previously unseen relationtuples.
From these new tuples, additional sentenceswhich express those tuples can be retrieved, andthe cycle of learning can continue.There is an analogous cycle of learning con-cepts from instances and the large corpus; the ex-periments in this paper do not report on that paral-lel learning cycle.Figure 1: Approach to Learning RelationsAt the ith iteration, the steps are1.
Given the set of hypothesized instances of therelation (triples HTi), find instances of suchtriples in the corpus.
(On the first iteration,?hypothesized?
triples are manually-generatedseed examples.)2.
Induce possible patterns.
For each proposedpattern P:a.
Apply pattern P to the corpus to generate aset of triples TPb.
Estimate precision as the confidence-weighted average of the scores of thetriples in TP.
Reduce precision score by thepercentage of triples in TP that violate us-er-specified relation constraints (e.g.
arityconstraints described in 4.3)c. Estimate recall as the confidence-weightedpercentage of triples in HTi found by thepattern3.
Identify a set of high-confidence patterns HPiusing cutoffs automatically derived from rank-based curves for precision, recall, and F-measure (?=0.7)4.
Apply high-confidence patterns to a Web-scalecorpus to hypothesize new triples.
For eachproposed triple Ta.
Estimate score(T) as the expected proba-bility that T is correct, calculated by com-bining the respective precision and recallscores of all of the patterns that did or didnot return it (using the Na?ve Bayes as-sumption that all patterns are independent)b.
Estimate confidence(T) as the percentageof patterns in HPi by which T was found5.
Identify a set of high-confidence triples HTi+1					 Edison invented the light bulbBell built the first telephoneEdison was granted a U.S. patentfor the light bulbFranklin invented the lightning rod	62using cutoffs automatically derived from rankbased curves; use these triples to begin thenext iteration3 ChallengesThe iterative cycle of learning we describe abovehas most frequently been applied forreading tasks.
However, here we are interested inmeasuring performance for micro-are several reasons for wanting to measure perfomance in a micro-reading task:?
For information that is rare (e.g.
relationsabout infrequently mentioned entities), a mcro-reading paradigm may more accuratelypredict results.?
For some tasks or domains microbe all that we can do-- the actual corpus of iterest may not be macro-scaled.?
Macro-reading frequently utilizes statistics ofextraction targets from the whole corpus toimprove its precision.
Therefore, imicro-reading could improve the precision ofmacro-reading.?
Because we are measuring performance in amicro-reading context, recall at the instancelevel is as important as precision.ly, our learning system must learn to predictpatterns that incorporate nominal and pronminal mentions.?
Furthermore, while the approach we describemakes use of corpus-wide statistics during thelearning process, during pattern application welimit ourselves to only information from withina single document (and in practice primarilywithin a single sentence).
Our evaluationmeasures performance at the instance4 Implementation4.1 Pattern TypesBoschee et al(2008) describes two types of paterns: patterns that rely on surface strings and paterns that rely only on two types of syntacticstructure.
We diverge from that early work by a4Our evaluation measures only performance in extractirelation: that is if the text of sentence implies to an annotthat the relation is present, then the annotator hastructed to mark the sentence as correct (regardlesor not outside knowledge contradicts this fact).-macro-reading.
Therer-i--reading mayn-mprovingConsequent-o--level4.t-t--l-ng theators been in-s of whetherlowing more expressive surface-string patterns: oursurface-string patterns can include wildlowing the system to make matchomitting words).
For example fortim), the system learns the patternassassinated <VICTIM>, which correctly matchBooth, with hopes of a resurgent Confederacy inmind, cruelly assassinated LincolnWe also diverge from the earliermaking use of patterns based on thepredicate-argument structure and not dependencyparses.
The normalized predicatetures (text-graphs) are built by performing a set ofrule-based transformations on the syntactic parseof a sentence.
These transformations include fiing the logical subject and object for each verb,resolving some traces, identifying temporal argments, and attaching other verb arguments withlexicalized roles (e.g.
?of?
in Figureing graphs allow both noun and verb predicates.Manually created patterns usinghave been successfully used for event detectionand template-based question answering.graph structures have also served as useful featurein supervised, discriminative models for relationand event extraction.
Whilethe experiments describedhere do not include depen-dency tree paths, we doallow arbitrarily large textgraph patterns.4.2 Co-ReferenceNon-named mentions are essential for ahigh instance-level recall.
In certain cases, ation is most clearly and frequently epronouns and descriptions (e.grelation child).5 Because non-named instances apear in different constructions than named istances, we need to learn patterns that will appearin non-named contexts.
Thus, cmation is used during pattern induction to extractpatterns from sentences where the hypothesizedtriples are not explicitly mentioned.
In particularany mention that is co-referent with the desiredentity can be used to induce a pattern.for 7 types of entities is produced by SERIF, a5 The structure of our noun-predicates allows us to learn lexcalized patterns in cases like this.
For her fatherinduce the pattern n:father:<ref>PARENT,Figure 2:-cards (al-es which requirekill(agent, vic-<AGENT> <...>es.work by onlynormalized-argument struc-nd-u-2).
The result-this structureThe textsllowingrela-xpressed withher father for thep-n-o-reference infor-,Co-referencei-we would<pos>CHILD.Text Graph Pattern63state of the art information extraction engine.manually determined confidence threshold is usedto discard mentions where co-reference certainty istoo low.During pattern matching, co-reference is usedto find the ?best string?
for each element of thematched triple.
In particular, pronouns and descritors are replaced by their referents; and abbreviatnames are replaced by full names.cannot be resolved to a description or a name, or ithe co-reference threshold falls below a manuallydetermined threshold, then the match is discarded.Pattern scoring requires that we compare istances of triples across the whole corpus.
If theinstances were compared purely on the basis ofstrings, in many cases the same entity would apear as distinct (e.g.
US, United States)would interfere with the arity constraints describebelow.
To alleviate this challenge, we ubase of name strings that have been shown to beequivalent with a combination of editextraction statistics (Baron & FreedmanThus, for triple(tP) and hypothesized triples (HTif tP?HTi, but can be mapped via the equivalentnames database to some triple tP?score and confidence are adjustedtP?, weighted by the confidence of the equivalence.4.3 Relation Set and ConstraintsWe ran experiments using 11 relation types.
Therelation types were selected as a subset of the reltions that have been proposed for DARPA?s mchine reading program.
In addition to seedexamples, we provided the learning system withthree types of constraints for each relation:Symmetry: For relations where R(X,Y)R(Y,X), the learning (and scoring process), normlizes instances of the relation so that R(X,YR(Y,X) are equivalent.Arity: For each argument of the relation, prvide an expected maximum number of instancesper unique instance of the other argument.numbers are intentionally higher than the expectedtrue value to account for co-referenPatterns that violate the arity constraint (e.gv:accompanied(<obj>=<X>, <sub>=<Y>pattern for spouse) are penalized.
This is one wayof providing negative feedback during the unspervised training process.Argument Type: For each argument, specifyAp-edIf any pronounfn-sep-.
Thisdse a data--distance and, 2008).i),?HTi, then itstowards that ofa-a-=a-) ando-Thesece mistakes.as au-the expected class of entities for this argument.Entity types are one of the 7 ACE typesOrganization, Geo-political entity, Location, Faciity, Weapon, Vehicle) or Date.tem only allows instance proposals when the typesare correct.
Potentially, the system could use patern matches that violate type constraints as an aditional type of negative example.implementation would need to account for the factthat in some cases, potentially too general paare quite effective when the type constraints aremet.
For example, for the relationployed(PERSON, ORGANIZATION)<PER> is a fairly precise pattern, despite clearlybeing overly general without the type constraints.In our relation set, only two relations (and spouse) are symmetric.
Tablethe other constraints.
ACE types/dates are in coumns labeled with the first letter of ttype (A is arity).
We have only included thosetypes that are an argument for some relationTable 1: Argument types of the test relations4.4 Corpus and Seed ExamplesWhile many other experiments using this approachhave used web-scale corpora, we chose to includeWikipedia articles as well as Gigawordvide additional instances of information (e.g.birthDate and sibling) that is uncommon in news.For each relation type, 20 seedselected randomly from the corpus by using acombination of keyword search and an ACE etraction system to identify passages that were likly to contain the relations of interest.seed example was guaranteed to occur at least oncein a context that indicated the relation was present.5 Evaluation FrameworkTo evaluate system performance, we ran two sep(Person,l-Currently the sys-t-d-Anytternsem-, <ORG>?ssibling1 below includesl-he name of the.-3 to pro--examples werex-e-As such, eacha-64rate annotation-based evaluations, the first meaured precision, and the second measureTo measure overall precision, we ran each sytem?s patterns over the web-scale corpora, andrandomly sampled 200 of the instances it foundThese instances were then manuallywhether they conveyed the intended relation or not.The system precision is simply the percentage ofinstances that were judged to be correct.To measure recall, we began by randomly slecting 20 test-examples from the corpus, using thesame process that we used to select the trainingseeds (but guaranteed to be distinct from the traiing seeds).
We then searched the webfor sentences that might possibly link these testexamples together (whether directly or via coreference).
We randomly sampled this set of setences, choosing 10 sentences for each testexample, to form a collection of 200 sentenceswhich were likely to convey the desired relation.These sentences were then manually annotated toindicate which sentences actually convey the dsired relation; this set of sentences forms thetest set.
Once a recall set had been created for eachrelation, a system?s recall could berunning that system?s patterns over the documentsin the recall set, and checking what percentage ofthe recall test sentences it correctly identified.We intentionally chose to sample 10 sentencesfrom each test example, rather than sampling fromthe set of all sentences found for any of the testexamples, in order to prevent one or two verycommon instances from dominating the recall set.As a result, the recall test set is somewhat biasedaway from ?true?
recall, since it places a higherweight on the ?long tail?
of instances.we believe that this gives a more accurate indiction of the system?s ability to find novel instanceof a relation (as opposed to novel ways of talkingabout known instances).6 Effect of Pattern TypeAs described in 4.1, our system is capable of learing two classes of patterns: surfacetext-graphs.
We measured our system?s perfomance on each of the relation types after twenty6 While the system provides estimated precision for each patern, we do not evaluate over the n-best matches.
All patternswith estimated confidence above 50%  are treated eqsample from the set of matches produced by these pas-d recall.s-6.assessed as toe-n--scale corpus-n--e-recallevaluated byHowever,a-sn--strings andr-t-ually.
Wetterns.iterations.
In each iteration, the system can learnmultiple patterns of either type.no penalty for learning overlapping pattern types.For example, in the first iteration for the relationkilled(), the system learns both the surfacepattern <AGENT> killed <VICTIMgraph pattern: v:killed(<sub>=<AGENT>,<obj>=<VICTIM>).
During decoding, if multiplepatterns match the same relation instance, the sytem accepts the relation instance, but does notmake use of the additional information that therewere multiple supporting patterns.Figure 3: Precision of Pattern Types by RelationFigure 4: Recall of Pattern Type byFigure 5: F-Score of Pattern Type by RelationFigure 3, Figure 4, and Figure 5 plot precision,recall, and F-score for each of the 11 relationsshowing performance of all patterns vs. only textgraph patterns vs. only surface-string patterns.?
For most relations, the text-graph patterns prvide both higher precision and higher recallthan the surface-string patterns.cision of the text-graph patterns forthe result of the system learning a number ofoverly general patterns that correlate with atacks, but do not themselves indicate the preThere is currently-string> and the text-s-Relation-o-The lower pre-attackOn ist-s-65ence of an attack.
For instance, the systemlearns patterns with predicates saidCertainly, governments often make statementson the date of an attack and troops arrive in alocation before attacking, but both patterns willproduce a large number of spurious instances.?
While text-graph patterns typically haveprecision than the combined pattern set, suface-string patterns provide enough improvment in recall that typically the allscore is higher than the text-graph FFigure 6: Text-Graph and Surface-StringA partial explanation for the higher recall andprecision of the text-graph patterns is illustrated inFigure 6 which presents a simple surfacepattern and a simple text-graph pattern that appearto represent the same information.
On the right ofthe figure are three sentences.
The texttern correctly identifies the agenteach sentence.
However, the surfacemisses the killed() relation in the first sentence andmisidentifies the victim in the second sentence.
Thefalse-alarm in the second sentence would havebeen avoided by a system that restricted itself tomatching named instances, but as described abovein section 4.2, for the micro-reading task describedhere, detecting relations with pronouns is critical.While we allowed the creation of textpatterns with arbitrarily long paths between thearguments, in practice, the system rarely learnedsuch patterns.
For the relation killed(Agent, Vitim), we learned 8 patterns that have more than onepredicate (compared to 22 that only have a singlepredicate).
For the relationtion(Victim, Location), the system learned 28 paterns with more than 1 predicate (compared with20 containing only 1 predicate).
Inprecision of the longer patterns was higher, buttheir recall was significantly lower.killedInLocation, none of the longer path patternsmatched any of the examples in our recall set.One strength of text-graph patterns isfor intelligent omission of overly specific textexample, ignoring ?during a buglary?and arrive.higherr-e--pattern F--score.Patterns-string-graph pat-and victim in-string pattern,-graphc-killedInLoca-t-both cases, theIn the case ofallowing, forin Figure 6.Surface string patterns can includefor surface string patterns, the omissiontactically defined.
Approximately 30% of surfacestring patterns included one wildtional 17% included two.
Figureaged precision and recall for textsurface-string patterns.
The final three columnsbreak the surface-string patterns down by the nuber of wild-cards.
It appears that with one, the paterns remain reasonably precise, but the addition oa second wild-card drops precision by more than50%.
The presence of wild-cardrecall, but surface-string patterns do not reach thelevel of recall of text-graph patterns.Text Graph Surface StringAll NoPrecision 0.75 0.61 0.72Recall 0.32 0.22 0.16Figure 7: Performance by Number of7 Effect of Human ReviewIn addition to allowing the system to selfcompletely unsupervised manner, we ran a parallelset of experiments where the system was givenlimited human guidance.
At the end of5, 10, and 20, a person providedof feedback (on average 5 minutes)was presented with five patternsmatched instances for each pattern.able to provide two types of feedback:?
The pattern is correct/incorrect (e.g.<EMPLOYEE> said <ORGANIZATION> isan incorrect pattern for employ(?
The matched instances are correct/incorrect(e.g.
?Bob received a diploma fromcorrect instance, even if the pattern that prduced it is debatable (e.g.
v:<received>subj:PERSON, from:ORGANIZATIONrect instance can also produce a newto-be correct seed.Pattern judgments are stored in the database andincorporated as absolute truth.
Instance judgmentsprovide useful input into pattern scoring.were selected for annotation usingcombines their estimated f-measure; thecy; and their dissimilarity to patterns thatpreviously chosen for annotation.instances for each pattern are randomly sampled, toensure that the resulting annotationderive an unbiased precision estimawild-cards, butis not syn--card.
An addi-7 presents aver--graph andm-t-fpatterns improves-* 1-* 2-*0.69 0.300.10 0.09WildCards (*)-train in aiterations 1,under 10 minutes.
The person, and five sampleThe person wasX,Y))MIT?
is ao-).
A cor-known-Patternsa score thatir frequen-wereThe matchedcan be used tote.66Figure 8, Figure 9, and Figure 10recall, and F-score at iterations 5 and 20 for thesystem running in a fully unsupervised manner andone allowing human intervention.Figure 8: Precision at Iterations 5 and 20 for the Unspervised System and the System with InterventionFigure 9: Recall at Iterations 5 and 20 forvised System and the System with InterventionFigure 10: F-Score at Iterations 5 and 20 for the Unspervised System and the System with InterventionFor two relations: child and siblingproved dramatically with human intervention.
Byinspecting the patterns produced by the system, wesee that in case of sibling without intervention, thesystem only learned the relation ?brother?the relation ?sister.?
The limited feedback from aperson was enough to allow the system to learnpatterns for sister as well, causing the significantlyimproved recall.
We see smaller, but frequentlysignificant improvements in recall in a number ofother relations.
Interestingly, for different relatthe recall improvements are seen at different itertions.
For sibling, the jump in recall appears withinthe first five iterations.
Contrastingly, forSchool, there is a minor improvement in recall aplot precision,u-the Unsuper-u-, recall im-and notions,a-attend-f-ter iteration 5, but a much larger improvement afteiteration 20.
For child, there is actually a small dcrease in recall after 5 iterations, but after 20 itertions, the system has dramatically improved.The effect on precision is similarly varied.
For9 of the 11, human intervention improves precsion; but the improvement is never as dramatic asthe improvement in recall.
For precision, thestrongest improvements in performance appear inthe early iterations.
It is unclear whether this merly reflects that bootstrapping is likely to becomeless precise over time (as it learnsor if early feedback is truly better for improvingprecision.In the case of attackOn, even withvention, after iteration 10, the system begins tolearn very general patterns of the type described inthe previous section (e.g.
<said in:LOCATIONon:DATE> as a pattern indicating an attackpatterns may be correlated with experiencing anattack but are not themselves evidence of an attackBecause the overly general patterns do in fact corelate with the presence of an attack, the positiveexamples provided by human intervention may infact produce more such patterns.There is an interaction between improved precsion and improved recall.
If a system is very iprecise at iteration n, the additional instances that itproposes may not reflect the relation and be so diferent from each other that the system becomesunable to produce good patterns that improve rcall.
Conversely, if recall at iterationproduce a sufficiently diverse set of instances, itwill be difficult for the system to generatestances that are used to estimate pattern precision.8 Related WorkMuch research has been done on concept andrelation detection using large amounts of supevised training.
This is the typical approach in prgrams like Automatic Content Extraction (ACE),which evaluates system performancefixed set of concepts and relations in text.
In ACEall participating researchers are given access to asubstantial amount of supervised training, e.g.,250k words of annotated data.
Researchers havetypically used this data to incorporate a great dealof structural syntactic information in their models(e.g.
Ramshaw 2001), but the obvious weakness ofthese approaches is the resulting reliance on there-a-i-e-more patterns),human inter-.
These.r-i-m-f-e-n does notin-r-o-in detecting a,67manually annotated examples, which are expensiveand time-consuming to create.Co-training circumvents this weakness by play-ing off two sufficiently different views of a data setto leverage large quantities of unlabeled data(along with a few examples of labeled data), inorder to improve the performance of a learningalgorithm (Mitchell and Blum, 1998).
Co-trainingwill offer our approach to simultaneously learn thepatterns of expressing a relation and its arguments.Other researchers have also previously exploredautomatic pattern generation from unsupervisedtext, classically in (Riloff & Jones 1999).
Ravi-chandran and Hovy (2002) reported experimentalresults for automatically generating surface pat-terns for relation identification; others have ex-plored similar approaches (e.g.
Agichtein &Gravano 2000 or Pantel & Pennacchiotti, 2006).More recently (Mitchell et al, 2009) has shownthat for macro-reading, precision and recall can beimproved by learning a large set of interconnectedrelations and concepts simultaneously.We depart from this work by learning patternsthat use the structural features of text-graph pat-terns and our particular approach to pattern andpair scoring and selection.Most approaches to automatic pattern genera-tion have focused on precision, e.g., Ravichandranand Hovy report results in the Text Retrieval Con-ference (TREC) Question Answering track, whereextracting one instance of a relation can be suffi-cient, rather than detecting all instances.
This studyhas also emphasized recall.
Information about anentity may only be mentioned once, especially forrarely mentioned entities.
A primary focus on pre-cision allows one to ignore many instances thatrequire co-reference or long-distance dependen-cies; one primary goal of our work is to measuresystem performance in exactly those areas.9 ConclusionWe have shown that bootstrapping approaches canbe successfully applied to micro-reading tasks.Most prior work with this approach has focused onmacro-reading, and thus emphasized precision.Clearly, the task becomes much more challengingwhen the system must detect every instance.
De-spite the challenge, with very limited human inter-vention, we achieved F-scores of >.65 on 6 of the11 relations (average F on the relation set was .58).We have also replicated an earlier preliminaryresult (Boschee, 2008) showing that for a micro-reading task, patterns that utilize seman-tic/syntactic information outperform patterns thatmake use of only surface strings.
Our result coversa larger inventory of relation types and attempts toprovide a more precise measure of recall than theearlier preliminary study.Analysis of our system?s output provides in-sights into challenges that such a system may face.One challenge for bootstrapping systems is thatit is easy for the system to learn just a subset ofrelations.
We observed this in both sibling wherewe learned the relation brother and for employedwhere we only learned patterns for leaders of anorganization.
For sibling human intervention al-lowed us to correct for this mistake.
However foremployed even with human intervention, our recallremains low.
The difference between these tworelations may be that for sibling there are only twosub-relations to learn, while there is a rich hie-rarchy of potential sub-relations under the generalrelation employed.
The challenge is quite possiblyexacerbated by the fact that the distribution of em-ployment relations in the news is heavily biasedtowards top officials, but our recall test set inten-tionally does not reflect this skew.Another challenge for this approach is continu-ing to learn in successive iterations.
As we saw inthe figures in Section 7, for many relations perfor-mance at iteration 20 is not significantly greaterthan performance at iteration 5.
Note that seeingimprovements on the long tail of ways to express arelation may require a larger recall set than the testset used here.
This is exemplified by the existenceof the highly precise 2-predicate patterns which insome cases never fired in our recall test set.In future, we wish to address the subset prob-lem and the problem of stalled improvements.
Bothcould potentially be addressed by improved inter-nal rescoring.
For example, the system scoringcould try to guarantee coverage over the wholeseed-set thus promoting patterns with low recall,but high value for reflecting different information.A complementary set of improvements could ex-plore improved uses of human intervention.AcknowledgmentsThis work was supported, in part, by BBN underAFRL Contract FA8750-09-C-179.68ReferencesE.
Agichtein and L. Gravano.
Snowball: extracting rela-tions from large plain-text collections.
In Proceed-ings of the ACM Conference on Digital Libraries, pp.85-94, 2000.A.
Baron and M. Freedman, ?Who is Who and What isWhat: Experiments in Cross Document Co-Reference?.
Empirical Methods in Natural LanguageProcessing.
2008.A.
Blum and T. Mitchell.
Combining Labeled and Un-labeled Data with Co-Training.
In Proceedings of the1998 Conference on Computational LearningTheory, July 1998.E.
Boschee, V. Punyakanok, R. Weischedel.
An Explo-ratory Study Towards ?Machines that Learn to Read?.Proceedings of AAAI BICA Fall Symposium, No-vember 2008.M.
Collins and Y Singer.
Unsupervised Models forNamed Entity Classification.
EMNLP/VLC.
(1999).M Mintz, S Bills, R Snow, and D Jurafsky..
Distant su-pervision for relation extraction without labeled data.Proceedings of ACL-IJCNLP 200.
2009..T. Mitchell, J. Betteridge, A. Carlson, E. Hruschka, andR.
Wang.
?Populating the Semantic Web by Macro-Reading Internet Text.
Invited paper, Proceedings ofthe 8th International Semantic Web Conference(ISWC 2009).P.
Pantel and M. Pennacchiotti.
Espresso: LeveragingGeneric Patterns for Automatically Harvesting Se-mantic Relations.
In Proceedings of Conference onComputational Linguistics / Association for Compu-tational Linguistics (COLING/ACL-06).
pp.
113-120.Sydney, Australia, 2006.L.
Ramshaw , E. Boschee, S. Bratus, S. Miller, R.Stone, R. Weischedel, A. Zamanian, ?Experiments inmulti-modal automatic content extraction?, Proceed-ings of Human Technology Conference, March 2001.D.
Ravichandran and E. Hovy.
Learning surface textpatterns for a question answering system.
In Pro-ceedings of the 40th Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2002),pages 41?47, Philadelphia, PA, 2002.E.
Riloff.
Automatically generating extraction patternsfrom untagged text.
In Proceedings of the ThirteenthNational Conference on Artificial Intelligence, pages1044-1049, 1996.E.
Rilof and Jones, R  "Learning Dictionaries for In-formation Extraction by Multi-Level Bootstrapping",Proceedings of the Sixteenth National Conference onArtificial Intelligence (AAAI-99) , 1999, pp.
474-479.
1999.R Snow, D Jurafsky, and A Y. Ng.. Learning syntacticpatterns for automatic hypernym discovery .
Proceed-ings of NIPS 17.
2005.69
