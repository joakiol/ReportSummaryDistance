Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543?548,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsReducing Annotation Effort for Quality Estimation via Active LearningDaniel Beck and Lucia Specia and Trevor CohnDepartment of Computer ScienceUniversity of SheffieldSheffield, United Kingdom{debeck1,l.specia,t.cohn}@sheffield.ac.ukAbstractQuality estimation models provide feed-back on the quality of machine translatedtexts.
They are usually trained on human-annotated datasets, which are very costlydue to its task-specific nature.
We in-vestigate active learning techniques to re-duce the size of these datasets and thusannotation effort.
Experiments on a num-ber of datasets show that with as little as25% of the training instances it is possibleto obtain similar or superior performancecompared to that of the complete datasets.In other words, our active learning querystrategies can not only reduce annotationeffort but can also result in better qualitypredictors.1 IntroductionThe purpose of machine translation (MT) qual-ity estimation (QE) is to provide a quality pre-diction for new, unseen machine translated texts,without relying on reference translations (Blatz etal., 2004; Specia et al, 2009; Callison-Burch etal., 2012).
This task is usually addressed withmachine learning models trained on datasets com-posed of source sentences, their machine transla-tions, and a quality label assigned by humans.
Acommon use of quality predictions is the decisionbetween post-editing a given machine translatedsentence and translating its source from scratch,based on whether its post-editing effort is esti-mated to be lower than the effort of translating thesource sentence.Since quality scores for the training of QE mod-els are given by human experts, the annotation pro-cess is costly and subject to inconsistencies due tothe subjectivity of the task.
To avoid inconsisten-cies because of disagreements among annotators,it is often recommended that a QE model is trainedfor each translator, based on labels given by sucha translator (Specia, 2011).
This further increasesthe annotation costs because different datasets areneeded for different tasks.
Therefore, strategies toreduce the demand for annotated data are needed.Such strategies can also bring the possibility of se-lecting data that is less prone to inconsistent anno-tations, resulting in more robust and accurate pre-dictions.In this paper we investigate Active Learning(AL) techniques to reduce the size of the datasetwhile keeping the performance of the resultingQE models.
AL provides methods to select in-formative data points from a large pool which,if labelled, can potentially improve the perfor-mance of a machine learning algorithm (Settles,2010).
The rationale behind these methods is tohelp the learning algorithm achieve satisfactory re-sults from only on a subset of the available data,thus incurring less annotation effort.2 Related WorkMost research work on QE for machine transla-tion is focused on feature engineering and featureselection, with some recent work on devising morereliable and less subjective quality labels.
Blatz etal.
(2004) present the first comprehensive study onQE for MT: 91 features were proposed and usedto train predictors based on an automatic metric(e.g.
NIST (Doddington, 2002)) as the quality la-bel.
Quirk (2004) showed that small datasets man-ually annotated by humans for quality can resultin models that outperform those trained on muchlarger, automatically labelled sets.Since quality labels are subjective to the anno-tators?
judgements, Specia and Farzindar (2010)evaluated the performance of QE models usingHTER (Snover et al, 2006) as the quality score,i.e., the edit distance between the MT output andits post-edited version.
Specia (2011) comparedthe performance of models based on labels for543post-editing effort, post-editing time, and HTER.In terms of learning algorithms, by and largemost approaches use Support Vector Machines,particularly regression-based approaches.
For anoverview on various feature sets and machinelearning algorithms, we refer the reader to a re-cent shared task on the topic (Callison-Burch etal., 2012).Previous work use supervised learning methods(?passive learning?
following the AL terminol-ogy) to train QE models.
On the other hand, ALhas been successfully used in a number of naturallanguage applications such as text classification(Lewis and Gale, 1994), named entity recognition(Vlachos, 2006) and parsing (Baldridge and Os-borne, 2004).
See Olsson (2009) for an overviewon AL for natural language processing as well asa comprehensive list of previous work.3 Experimental Settings3.1 DatasetsWe perform experiments using four MT datasetsmanually annotated for quality:English-Spanish (en-es): 2, 254 sentencestranslated by Moses (Koehn et al, 2007), as pro-vided by the WMT12 Quality Estimation sharedtask (Callison-Burch et al, 2012).
Effort scoresrange from 1 (too bad to be post-edited) to 5 (nopost-editing needed).
Three expert post-editorsevaluated each sentence and the final score wasobtained by a weighted average between the threescores.
We use the default split given in the sharedtask: 1, 832 sentences for training and 432 fortest.French-English (fr-en): 2, 525 sentences trans-lated by Moses as provided in Specia (2011), an-notated by a single translator.
Human labels in-dicate post-editing effort ranging from 1 (too badto be post-edited) to 4 (little or no post-editingneeded).
We use a random split of 90% sentencesfor training and 10% for test.Arabic-English (ar-en): 2, 585 sentences trans-lated by two state-of-the-art SMT systems (de-noted ar-en-1 and ar-en-2), as provided in (Speciaet al, 2011).
A random split of 90% sentences fortraining and 10% for test is used.
Human labels in-dicate the adequacy of the translation ranging from1 (completely inadequate) to 4 (adequate).
Thesedatasets were annotated by two expert translators.3.2 Query MethodsThe core of an AL setting is how the learner willgather new instances to add to its training data.
Inour setting, we use a pool-based strategy, wherethe learner queries an instance pool and selectsthe best instance according to an informativenessmeasure.
The learner then asks an ?oracle?
(in thiscase, the human expert) for the true label of the in-stance and adds it to the training data.Query methods use different criteria to predicthow informative an instance is.
We experimentwith two of them: Uncertainty Sampling (US)(Lewis and Gale, 1994) and Information Density(ID) (Settles and Craven, 2008).
In the following,we denote M(x) the query score with respect tomethod M .According to the US method, the learner selectsthe instance that has the highest labelling varianceaccording to its model:US(x) = V ar(y|x)The ID method considers that more dense regionsof the query space bring more useful information,leveraging the instance uncertainty and its similar-ity to all the other instances in the pool:ID(x) = V ar(y|x)?
(1UU?u=1sim(x, x(u)))?The ?
parameter controls the relative importanceof the density term.
In our experiments, we set itto 1, giving equal weights to variance and density.The U term is the number of instances in the querypool.
As similarity measure sim(x, x(u)), we usethe cosine distance between the feature vectors.With each method, we choose the instance thatmaximises its respective equation.3.3 ExperimentsTo build our QE models, we extracted the 17 fea-tures used by the baseline approach in the WMT12QE shared task.1 These features were used with aSupport Vector Regressor (SVR) with radial basisfunction and fixed hyperparameters (C=5, ?=0.01,=0.5), using the Scikit-learn toolkit (Pedregosaet al, 2011).
For each dataset and each querymethod, we performed 20 active learning simu-lation experiments and averaged the results.
We1We refer the reader to (Callison-Burch et al, 2012) fora detailed description of the feature set, but this was a verystrong baseline, with only five out of 19 participating systemsoutperforming it.544started with 50 randomly selected sentences fromthe training set and used all the remaining train-ing sentences as our query pool, adding one newsentence to the training set at each iteration.Results were evaluated by measuring Mean Ab-solute Error (MAE) scores on the test set.
Wealso performed an ?oracle?
experiment: at each it-eration, it selects the instance that minimises theMAE on the test set.
The oracle results give anupper bound in performance for each test set.Since an SVR does not supply variance valuesfor its predictions, we employ a technique knownas query-by-bagging (Abe and Mamitsuka, 1998).The idea is to build an ensemble of N SVRstrained on sub-samples of the training data.
Whenselecting a new query, the ensemble is able to re-turnN predictions for each instance, from where avariance value can be inferred.
We used 20 SVRsas our ensemble and 20 as the size of each trainingsub-sample.2 The variance values are then usedas-is in the case of US strategy and combined withquery densities in case of the ID strategy.4 Results and DiscussionFigure 1 shows the learning curves for all querymethods and all datasets.
The ?random?
curvesare our baseline since they are equivalent to pas-sive learning (with various numbers of instances).We first evaluated our methods in terms of howmany instances they needed to achieve 99% of theMAE score on the full dataset.
For three datasets,the AL methods significantly outperformed therandom selection baseline, while no improvementwas observed on the ar-en-1 dataset.
Results aresummarised in Table 1.The learning curves in Figure 1 show an inter-esting behaviour for most AL methods: some ofthem were able to yield lower MAE scores thanmodels trained on the full dataset.
This is par-ticularly interesting in the fr-en case, where bothmethods were able to obtain better scores usingonly ?25% of the available instances, with theUS method resulting in 0.03 improvement.
Therandom selection strategy performs surprisinglywell (for some datasets it is better than the ALstrategies with certain number of instances), pro-viding extra evidence that much smaller annotated2We also tried sub-samples with the same size of the cur-rent training data but this had a large impact in the querymethods running time while not yielding significantly betterresults.Figure 1: Learning curves for different query se-lection strategies in the four datasets.
The horizon-tal axis shows the number of instances in the train-ing set and the vertical axis shows MAE scores.545US ID Random Full dataset#instances MAE #instances MAE #instances MAEen-es 959 (52%) 0.6818 549 (30%) 0.6816 1079 (59%) 0.6818 0.6750fr-en 79 (3%) 0.5072 134 (6%) 0.5077 325 (14%) 0.5070 0.5027ar-en-1 51 (2%) 0.6067 51 (2%) 0.6052 51 (2%) 0.6061 0.6058ar-en-2 209 (9%) 0.6288 148 (6%) 0.6289 532 (23%) 0.6288 0.6290Table 1: Number (proportion) of instances needed to achieve 99% of the performance of the full dataset.Bold-faced values indicate the best performing datasets.Best MAE US Best MAE ID Full dataset#instances MAE US MAE Random #instances MAE ID MAE Randomen-es 1832 (100%) 0.6750 0.6750 1122 (61%) 0.6722 0.6807 0.6750fr-en 559 (25%) 0.4708 0.5010 582 (26%) 0.4843 0.5008 0.5027ar-en-1 610 (26%) 0.5956 0.6042 351 (15%) 0.5987 0.6102 0.6058ar-en-2 1782 (77%) 0.6212 0.6242 190 (8%) 0.6170 0.6357 0.6227Table 2: Best MAE scores obtained in the AL experiments.
For each method, the first column shows thenumber (proportion) of instances used to obtain the best MAE, the second column shows the MAE scoreobtained and the third column shows the MAE score for random instance selection at the same numberof instances.
The last column shows the MAE obtained using the full dataset.
Best scores are shown inbold and are significantly better (paired t-test, p < 0.05) than both their randomly selected counterpartsand the full dataset MAE.datasets than those used currently can be sufficientfor machine translation QE.The best MAE scores achieved for each datasetare shown in Table 2.
The figures were tested forsignificance using pairwise t-test with 95% confi-dence,3 with bold-faced values in the table indicat-ing significantly better results.The lower bounds in MAE given by the ora-cle curves show that AL methods can indeed im-prove the performance of QE models: an idealquery method would achieve a very large improve-ment in MAE using fewer than 200 instances in alldatasets.
The fact that different datasets presentsimilar oracle curves suggests that this is not re-lated for a specific dataset but actually a commonbehaviour in QE.
Although some of this gain inMAE may be due to overfitting to the test set, theresults obtained with the fr-en and ar-en-2 datasetsare very promising, and therefore we believe thatit is possible to use AL to improve QE results inother cases, as long as more effective query tech-niques are designed.5 Further analysis on the oraclebehaviourBy analysing the oracle curves we can observe an-other interesting phenomenon which is the rapidincrease in error when reaching the last ?200 in-stances of the training data.
A possible explana-3We took the average of the MAE scores obtained fromthe 20 runs with each query method for that.tion for this behaviour is the existence of erro-neous, inconsistent or contradictory labels in thedatasets.
Quality annotation is a subjective task bynature, and it is thus subject to noise, e.g., due tomisinterpretations or disagreements.
Our hypothe-sis is that these last sentences are the most difficultto annotate and therefore more prone to disagree-ments.To investigate this phenomenon, we performedan additional experiment with the en-es dataset,the only dataset for which multiple annotationsare available (from three judges).
We measure theKappa agreement index (Cohen, 1960) between allpairs of judges in the subset containing the first300 instances (the 50 initial random instances plus250 instances chosen by the oracle).
We then mea-sured Kappa in windows of 300 instances until thelast instance of the training set is selected by theoracle method.
We also measure variances in sen-tence length using windows of 300 instances.
Theidea of this experiment is to test whether sentencesthat are more difficult to annotate (because of theirlength or subjectivity, generating more disagree-ment between the judges) add noise to the dataset.The resulting Kappa curves are shown in Fig-ure 2: the agreement between judges is high forthe initial set of sentences selected, tends to de-crease until it reaches ?1000 instances, and thenstarts to increase again.
Figure 3 shows the resultsfor source sentence length, which follow the sametrend (in a reversed manner).
Contrary to our hy-546Figure 2: Kappa curves for the en-es dataset.
Thehorizontal axis shows the number of instances andthe vertical axis shows the kappa values.
Eachpoint in the curves shows the kappa index for awindow containing the last 300 sentences chosenby the oracle.pothesis, these results suggest that the most diffi-cult sentences chosen by the oracle are those in themiddle range instead of the last ones.
If we com-pare this trend against the oracle curve in Figure 1,we can see that those middle instances are the onesthat do not change the performance of the oracle.The resulting trends are interesting because theygive evidence that sentences that are difficult to an-notate do not contribute much to QE performance(although not hurting it either).
However, they donot confirm our hypothesis about the oracle be-haviour.
Another possible source of disagreementis the feature set: the features may not be discrim-inative enough to distinguish among different in-stances, i.e., instances with very similar featuresbut different labels might be genuinely different,but the current features are not sufficient to indi-cate that.
In future work we plan to further inves-tigate this by hypothesis by using other feature setsand analysing their behaviour.6 Conclusions and Future WorkWe have presented the first known experiments us-ing active learning for the task of estimating ma-chine translation quality.
The results are promis-ing: we were able to reduce the number of in-stances needed to train the models in three of thefour datasets.
In addition, in some of the datasetsactive learning yielded significantly better modelsusing only a small subset of the training instances.Figure 3: Average source and target sentencelengths for the en-es dataset.
The horizontal axisshows the number of instances and the verticalaxis shows the length values.
Each point in thecurves shows the average length for a window con-taining the last 300 sentences chosen by the oracle.The oracle results give evidence that it is possi-ble to go beyond these encouraging results by em-ploying better selection strategies in active learn-ing.
In future work we will investigate moreadvanced query techniques that consider featuresother than variance and density of the data points.We also plan to further investigate the behaviourof the oracle curves using not only different fea-ture sets but also different quality scores such asHTER and post-editing time.
We believe that abetter understanding of this behaviour can guidefurther developments not only for instance selec-tion techniques but also for the design of betterquality features and quality annotation schemes.AcknowledgmentsThis work was supported by funding fromCNPq/Brazil (No.
237999/2012-9, Daniel Beck)and from the EU FP7-ICT QTLaunchPad project(No.
296347, Lucia Specia).ReferencesNaoki Abe and Hiroshi Mamitsuka.
1998.
Querylearning strategies using boosting and bagging.
InProceedings of the Fifteenth International Confer-ence on Machine Learning, pages 1?9.Jason Baldridge and Miles Osborne.
2004.
Activelearning and the total cost of annotation.
In Pro-ceedings of EMNLP, pages 9?16.547John Blatz, Erin Fitzgerald, and George Foster.
2004.Confidence estimation for machine translation.
InProceedings of the 20th Conference on Computa-tional Linguistics, pages 315?321.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of 7th Workshopon Statistical Machine Translation.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and psychologicalmeasurement, 20(1):37?46.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology Research, pages 128?132.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.David D. Lewis and Willian A. Gale.
1994.
A sequen-tial algorithm for training text classifiers.
In Pro-ceedings of the ACM SIGIR Conference on Researchand Development in Information Retrieval, pages 1?10.Fredrik Olsson.
2009.
A literature survey of activemachine learning in the context of natural languageprocessing.
Technical report.Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Duborg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and E?douard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Chris Quirk.
2004.
Training a sentence-level machinetranslation confidence measure.
In Proceedings ofLREC, pages 825?828.Burr Settles and Mark Craven.
2008.
An analysisof active learning strategies for sequence labelingtasks.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1070?1079.Burr Settles.
2010.
Active learning literature survey.Technical report.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas.Lucia Specia and Atefeh Farzindar.
2010.
Estimatingmachine translation post-editing effort with HTER.In Proceedings of AMTA Workshop Bringing MT tothe User: MT Research and the Translation Indus-try.Lucia Specia, M Turchi, Zhuoran Wang, and J Shawe-Taylor.
2009.
Improving the confidence of machinetranslation quality estimates.
In Proceedings of MTSummit XII.Lucia Specia, Najeh Hajlaoui, Catalina Hallett, andWilker Aziz.
2011.
Predicting machine translationadequacy.
In Proceedings of MT Summit XIII.Lucia Specia.
2011.
Exploiting objective annotationsfor measuring translation post-editing effort.
In Pro-ceedings of EAMT.Andreas Vlachos.
2006.
Active annotation.
In Pro-ceedings of the Workshop on Adaptive Text Extrac-tion and Mining at EACL.548
