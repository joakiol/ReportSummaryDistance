CRF tagging for head recognition based on Stanford parserYong Cheng, Chengjie Sun, Bingquan Liu, Lei LinHarbin Institute of Technology{ycheng, cjsun, linl,liubq}@insun.hit.edu.cnAbstractChinese parsing has received more andmore attention, and in this paper, we usetoolkit to perform parsing on the data ofTsinghua Chinese Treebank (TCT) used inCIPS, and we use Conditional RandomFields (CRFs) to train specific model for thehead recognition.
At last, we comparedifferent results on different POS results.1 IntroductionIn the past decade, Chinese parsing hasreceived more and more attention, it is thecore of Chinese information processingtechnology, and it is also the cornerstone fordeep understanding of Chinese.Parsing is to identify automaticallysyntactic units in the sentence and give therelationship between these units.
It is basedon a given grammar.
The results of parsingare usually structured syntax tree.
Forexample, the parsing result of sentence "????????"
is as following.
(ROOT(dj (nS ??
)(vp (v ?
)(np(np (m ?)
(n ??
))(n ??
)))))With the development of Chineseeconomy, Chinese information processinghas become a worldwide hot spot, andparsing is an essential task.
However,parsing is a recognized research problem,and it is so difficult to meet the urgent needsof industrial applications in accuracy,robustness, speed.
So the study of Chinesegrammar and syntax analysis algorithm arestill the focus of Chinese informationprocessing.In all the parsing technology research,English parsing research is the most in-depth,and there are three main aspects of researchin statistical parsing, they are  parsing model,parsing algorithm, and corpus construction.As for the parsing model, currently there arefour commonly used parsing models, PCFGmodel [1], the model based on historical,Hierarchical model of progressive, head-driven model [2].Since parsing is mostly a data drivenprocess, its performance is determined bythe amount of data in a Treebank on which aparser is trained.
Much more data forEnglish than for any other languages havebeen available so far.
Thus most researcheson parsing are concentrated on English.
It isunrealistic to directly apply any existingparser trained on an English Treebank forChinese sentences.
But the methodology is,without doubt, highly applicable.
Even forthose corpora with special format andinformation integrated some modificationand enhancement on a well-performed parserto fit the special structure for the data couldhelp to obtain a good performance.This paper presents our solution for theshared Task 2 of CIPS2010-Chinese Parsing.We exploit an existing powerful parser,Stanford parser, which has showed itseffectiveness on English, with necessarymodifications for parsing Chinese for theshared task.
Since the corpus used in CIPS isfrom TCT, and the sentence contains thehead-word information, but for the Stanfordparser, it can't recognize the headconstituents.
So we apply a sequence taggingmethod to label head constituents based onthe data extracted from the TCT corpus, Insection 2 and section 3, we will present theTable 1.
Training data with different formatsdetails of our approach, and In section 4, wepresent the details of experiment.2 ParsingSince English parsing has made manyachievements, so we investigated somestatistical parsing models designed forEnglish.
There are three open sourceconstituent parsers, Stanford parser [3],Berkeley parser [4] and Bikel's parser [5].Bikel's parser is an implementation ofCollins' head-driven statistical model [6].The Stanford parser is based on the factoredmodel described in [7].
Berkeley parser isbased on unlexicalized parsing model, asdescribed in [8].All the three parsers are claimed to bemultilingual parsers but only accept trainingdata in UPenn Treebank format.
To adaptthese parsers to Tsinghua Chinese Treebank(TCT) used in CIP, we firstly transform theTCT training data into UPenn format.
Then,some slight modifications have been made tothe three parsers.
So that they could fulfillthe needs in our task.In our work, we use Stanford parser totrain our model by change the training datato three parts with different formats, one fortraining parsing model, one for training POSmodel, and the last for training head-recognition model.
Table 1 shows the threedifferent forms.3 Head recognitionHead recognition is to find the headword in a clause, for example, 'np-1' expressthat in the clause, the word with index '1' isthe key word.To recognize the head constituents, andextra step is needed since Stanford parsingcould not provide a straight forward way forthis.
Consider that head constituents arealways determined by their syntactic symboland their neighbors, whose order andrelations strongly affects the head labeling.Like chunking [9], it is natural to apply asequence labeling strategy to tackle thisproblem.
We adopt the linear-chain CRF[10], one of the most successful sequencelabeling framework so far, for the headrecognition is this stage.4 Experiment4.1 DataThe training data is from TsinghuaChinese Treebank (TCT), and our task is toperform full parsing on them.
There are37218 lines in official released training data,As the Table 1 show; we change the datainto three parts for different models.The testing data doesn?t contain POSlabels, and there are 1000 lines in officialreleased testing data.Parsing model1.
(ROOT (np-0-2 (n ????)
(cC ??)
(np-0-1 (n ?
? )
(n ??)
) ) )2.
(ROOT (vp-1 (pp-1 (p?)
(np-0-2 (np-1 (n ??)
(n ??)
) (cC ?? )
(np-2 (a ?
?
)(uJDE ?)
(np-1 (n ??)
(np-1 (n ??)
(n ??)
) ) ) ) ) (vp-1 (d ??)
(vp-1 (d ??)
(v ??)
) ) ) )POS model1.
?
?/nS  ?
?/a  ??/n2.?
?/nS  ?/vC  ?/a?
?/n  ?
?/n  ?/wP?
?/nR  ?
?/n  ?/vC?
?/m  ?/m  ?/qN?
?/n  ?/uJDE  ?
?/n  ?/wEHead-recognitionmodela O n np 0n a O np 1nS O np np 0np nS O np 1Table 2.
Different POS tagging resultsoriginal newpos accuracy 80.40 94.824.2 Models training4.2.1 Parsing model trainingAs for training parsing model withStanford parser, since there are littleparameters need to set, so we directly use theStanford parser to train a model without anyparameter setting.4.2.2 POS model trainingIn this session of the evaluation, POStagging is no longer as a separate task, so wehave to train our own POS tagging model.
Inthe evaluation process, we didn't fullyconsider the POS tagging results' impact onthe overall results, so we didn't train the POSmodel specially, we directly use the POSfunction in Stanford parser toolkit.
This hasled to relatively poor results in POS tagging,and it also affects the overall parsing result.After the evaluation, we train a specificmodel to improve the POS tagging results.As the table 1 shows, we extract trainingdata from the original corpus and adopt thelinear-chain CRF to train a POS taggingmodel.
Table 2 shows the original POStagging results and new results.4.2.3 Head recognition model trainingAs the table 1 shows, we extract specifictraining data from original corpus.Table 3.
Training data formats for Head-recognitionoriginal corpus 1.
[vp-0 ?
?/v  [np-1?
?/n  ?
?/n  ] ]temp corpus 1.
[np-1 ?
?/n  ?
?/n  ]2.
[vp-0 ?
?/v  [np-1?
?/n  ?
?/n  ] ]final corpus n O n np 0n n O np 1v O np vp 1np v O vp 0Table 4.
Statistics the frequency of the words ineach clausenumber of word statistics number< 1 1602 508343 125924 565 664>5 360And for head-word recognition, since theadjacent clause has little effect on therecognition of head-word, so we set theclause as the smallest unit.
We chose CRF totrain our model.
However, for getting theproper format of data for training in CRF,We have to do further processing on the data.As the table 3 shows, the final data set wordas the unit.For example, the line 'n O np vp 1?, themeaning from beginning to end is POS orclause mark of current word or clause, POSor clause mark of previous word, POS orclause mark of latter word, the clause markof current word, and the last mean that ifcurrent word or clause is headword 1represents YES, 0 represents NO.4.4 Result and ConclusionAs we mention before, in evaluation, wedidn't train specific POS tagging model, Sowe re-train our pos model, and the newresults is shown in table 6, it can be seen that,with the increase of POS result, there is acorresponding increase in the overall results.Table 5.
Performance of head recognition andthe template for model trainingBoundary +Constituent 70.58Boundary +Constituent + Head 66.97templateU00:%x[0,0]U01:%x[-1,0]U02:%x[1,0]U04:%x[0,0]/%x[-1,0]U05:%x[0,0]/%x[1,0]U06:%x[-1,0]/%x[1,0]Table 6.
Overall results on different POS resultsPOS Boundary +Constituentoriginal 80.40 67.00new 94.82 74.28Through our evaluation results, we cansee that it is not appropriate to directly useEnglish parser toolkit to process Chinese.And it is urgent to development parsingmodel based on the characteristics ofChinese.References[1] T. L. Booth and R. A. Thompson.
ApplyingProbability Measures  to Abstract Languages.IEEE Transactions on Computers, 1973, C-22(5):422-450.
[2] M. Collins.
Three Generative, LexicalisedModels for Statistical Parsing.
In Proceedingsof the 35th annual meeting of the associationfor computational linguistics.
[3] http://nlp.stanford.edu/software/lex-parser.html[4] http://code.google.com/p/berkeleyparser[5] http://www.cis.upenn.edu/~dbikel/download[6] Michael Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis.University of Pennsylvania.
[7] Dan Klein and Christopher D. ManningAccurate unlixicalized parsing.
In Proceedingsof the 41st Annual Meeting on Association forComputational Linguistics.
[8] S Petrov and D Klein.
Improved inference forunlexicalized parsing.
In Proceedings ofNAACL HLT 2007.
[9] Fei Sha and Fernando Pereira.
2003.
Shallowparsing with conditional random fields.
InProceedings of HLT-NAACL 2003, pages213-220, Edmonton.
Canada.
[10] John Lafferty.
Andrew McCallum.
AndFernando Pereira.
2001.
Conditional randomfields: Probabilistic models for segmenting andlabeling sequence data.
In Proceedings ofICML 2001, pages 282-289, Williams College,Williamstown, MA, USA.
