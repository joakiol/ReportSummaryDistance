Proceedings of NAACL HLT 2009: Short Papers, pages 249?252,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsActive Zipfian Sampling for Statistical Parser Training?Onur C?obanog?luDepartment of Computer ScienceSennott SquareUniversity of PittsburghPittsburgh, PA 15260, USAonc3@pitt.eduAbstractActive learning has proven to be a successfulstrategy in quick development of corpora to beused in training of statistical natural languageparsers.
A vast majority of studies in thisfield has focused on estimating informative-ness of samples; however, representativenessof samples is another important criterion to beconsidered in active learning.
We present anovel metric for estimating representativenessof sentences, based on a modification of Zipf?sPrinciple of Least Effort.
Experiments onWSJ corpus with a wide-coverage parser showthat our method performs always at least asgood as and generally significantly better thanalternative representativeness-based methods.1 IntroductionWide coverage statistical parsers (Collins, 1997;Charniak, 2000) have proven to require largeamounts of manually annotated data for training toachieve substantial performance.
However, build-ing such large annotated corpora is very expensivein terms of human effort, time and cost (Marcus etal., 1993).
Several alternatives of the standard super-vised learning setting have been proposed to reducethe annotation costs, one of which is active learning.Active learning setting allows the learner to select itsown samples to be labeled and added to the trainingdata iteratively.
The motive behind active learning?Vast majority of this work was done while the author was agraduate student in Middle East Technical University, under thefunding from T ?UB?ITAK-B?IDEB through 2210 National Schol-arship Programme for MSc Students.is that if the learner may select highly informativesamples, it can eliminate the redundancy generallyfound in random data; however, informative sam-ples can be very untypical (Tang et al, 2002).
Un-like random sampling, active learning has no guar-antee of selecting representative samples and untyp-ical training samples are expected to degrade testperformance of a classifier.To get around this problem, several methods ofestimating representativeness of a sample have beenintroduced.
In this study, we propose a novel rep-resentativeness estimator for a sentence, which isbased on a modification of Zipf?s Principle of LeastEffort (Zipf, 1949), theoretically sound and em-pirically validated on Brown corpus (Francis andKuc?era, 1967).
Experiments conducted with a widecoverage CCG parser (Clark and Curran, 2004;Clark and Curran, 2007) on CCGbank (Hocken-maier and Steedman, 2005) show that using our esti-mator as a representativeness metric never performsworse than and generally outperforms length bal-anced sampling (Becker and Osborne, 2005), whichis another representativeness based active learn-ing method, and pure informativeness based activelearning.2 Related WorkIn selective sampling setting, there are three criteriato be considered while choosing a sample to add tothe training data (Dan, 2004; Tang et al, 2002): In-formativeness (what will the expected contributionof this sample to the current model be?
), represen-tativeness (what is the estimated probability of see-ing this sample in the target population?)
and diver-249sity (how different are the samples in a batch fromeach other?).
The last criterion applies only to thebatch-mode setting, in which the training data is in-cremented by multiple samples at each step for prac-tical purposes.Most of the active learning research in statisticalparser training domain has focused on informative-ness measures developed for both single and multi-learner settings.
The informativeness measures forsingle-learners that have exhibited significant per-formance in well known experimental domains areas follow: Selecting the sentences unparsable by thecurrent model (and if the batch does not get filled,using a secondary method) (Thompson et al, 1999);selecting the sentences with the highest tree entropy,i.e.
the Shannon entropy of parses the probabilisticparser assigns to the sentence (Hwa, 2004); select-ing the sentences having lowest best probabilities,where best probability is the conditional probabilityof the most probable parse, given the sentence andthe current model (Osborne and Baldridge, 2004);primarily selecting the sentences that are expectedto include events observed with low frequency sofar with the help of bagging and filling the rest ofthe batch according to tree entropy, which is namedas two-stage active learning by Becker and Os-borne (2005).
Proposed informativeness measuresfor multiple learners and ensemble learners can befound in (Baldridge and Osborne, 2003; Osborneand Baldridge, 2004; Becker and Osborne, 2005;Baldridge and Osborne, 2008).As for representativeness measures, Tang et.al.
(2002) proposed using sample density, i.e.
theinverse of the average distance of the sample to theother samples in the pool, according to some dis-tance metric.
Becker and Osborne (2005) introducedlength balanced sampling, in which the length his-togram of the batch is kept equal to the length his-togram of a random sample of batch size drawn fromthe pool.3 Description Of The WorkWe introduce a novel representativeness measure forstatistical parser training domain.
Our measure is afunction proposed in (Sigurd et al, 2004), which es-timates the relative frequencies of sentence lengthsin a natural language.
Sigurd et.
al.
(2004) claimedthat the longer a sentence is, the less likely it will beuttered; in accordance with Zipf?s Principle of LeastEffort (Zipf, 1935).
However, too short sentenceswill appear infrequently as well, since the numberof different statements that may be expressed usingrelatively fewer words is relatively smaller.
Authorsconjectured that there is a clash of expressivity andeffort over the frequency of sentence length, whicheffort eventually wins.
They formulated this behav-ior with a Gamma distribution estimating the relativefrequencies of sentence lengths.
Authors conducteda parameter fit study for English using Brown cor-pus (Francis and Kuc?era, 1967) and reported that theformula f(L) = 1.1 ?
L1 ?
0.90L, where L is thesentence length, fits to the observations with veryhigh correlation.We propose using this fitted formula (namedfzipf?eng from now on) as the measure of repre-sentativeness of a sentence.
This metric has sev-eral nice features: It is model-independent (so it isnot affected from modeling errors), is both theoreti-cally sound and empirically validated, can be used inother NLP domains and is a numerical metric, pro-viding flexibility in combining it with informative-ness (and diversity) measures.4 Experiments4.1 Experimental SetupWe conducted experiments on CCGbank cor-pus (Hockenmaier and Steedman, 2005) with thewide coverage CCG parser of Clark and Cur-ran (2004; 2007)1.
C&C parser was fast enough toenable us to use the whole available training datapool for sample selection in experiments, but not fortraining (since training C&C parser is not that fast).Among the models implemented in the parser, thenormal-form model is used.
We used the default set-tings of the C&C parser distribution for fair evalu-ation.
WSJ Sections 02-21 (39604 sentences) areused for training and WSJ Section 23 (2407 sen-tences) is used for testing.
Following (Clark andCurran, 2007), we evaluated the parser performanceusing the labeled f-score of the predicate-argumentdependencies produced by the parser.1Following (Baldridge and Osborne, 2004), we claim thatthe performances of AL with C&C parser and other state-of-the-art wide coverage parsers will be similar25067686970717273747515000  20000  25000  30000  35000  40000  45000  50000Labeledf-scoreAnnotation cost (number of brackets)?random??entropy??entropy_lbs?
?entropy_zipf?67686970717273747515000  20000  25000  30000  35000  40000  45000  50000Labeledf-scoreAnnotation cost (number of brackets)?random??lbp??lbp_lbs?
?lbp_zipf?67686970717273747515000  20000  25000  30000  35000  40000  45000  50000Labeledf-scoreAnnotation cost (number of brackets)?random??twostage??twostage_lbs?
?twostage_zipf?Figure 1: Comparative performances of different representativeness measures.
The informativeness measure used istree entropy in the leftmost graph, lowest best probability in the central graph and two-stage AL in the rightmost graph.The line with the tag ?random?
always shows the random sampling baseline.none lbs zipf randomentropy 30.99%(74.24%) 20.63%(74.31%) 30.11%(74.36%) N/A (74.35%)lbp 22.34%(74.37%) 20.78%(74.49%) 30.19%(74.43%) N/A(74.50%)unparsed/entropy 19.98%(74.32%) 19.34%(74.43%) 26.27%(74.38%) N/A(74.35%)twostage 2.83%(73.94%) 11.13%(74.09%) 13.38%(74.05%) N/A(73.94%)Table 1: PRUD values of different AL schemes.
The row includes the informativeness measure and the columnincludes the representativeness measure used.
The column with the label random always includes the results forrandom sampling.
The numbers in parentheses are the labeled f-score values reached by the schemes.For each active learning scheme and random sam-pling, the size of the seed training set is 500 sen-tences, the batch size is 100 sentences and itera-tion stops after reaching 2000 sentences.2 For sta-tistical significance, each experiment is replicated 5times.
We evaluate the active learning performancein terms of Percentage Reduction in Utilized Data,i.e.
how many percents less data is used by ALcompared to random sampling, in order to reach acertain performance score.
Amount of used data ismeasured with the number of brackets in the data.
InCCGbank, a bracket alays corresponds to a parsedecision, so it is a reasonable approximation of theamount of annotator effort.Our measure is compared to length balanced sam-pling and using no representativeness measures.Since there is not a trivial distance metric betweenCCG parses and we do not know a proposed one, wecould not test it against sample density method.
Welimited the informativeness measures to be testedto the four single-learner measures we mentionedin Section 2.
Multi-learner and ensemble methodsare excluded, since the success of such methods re-2These values apply to the training of the parser and theCCG supertagger.
POS-tagger is trained with the whole avail-able pool of 39604 sentences due to sparse data problem.lies heavily on the diversity of the available mod-els (Baldridge and Osborne, 2004; Baldridge andOsborne, 2008).
The models in C&C parser arenot diverse enough and we left crafting such diversemodels to future work.We combined fzipf?eng with the informativenessmeasures as follow: With tree entropy, sentenceswith the highest fzipf?eng(s) ?
fnte(s,G) (namedfzipf?entropy(s,G)) values are selected.
fnte(s,G)is the tree entropy of the sentence s under the cur-rent model G, normalized by the binary logarithm ofthe number of parses, following (Hwa, 2004).
Withlowest best probability, sentences with the high-est fzipf?eng(s) ?
(1 ?
fbp(s,G)) values are se-lected, where fbp is the best probability function(see Section 2).
With unparsed/entropy, we primar-ily chose the unparsable sentences having highestfzipf?eng(s) values and filled the rest of the batchaccording to fzipf?entropy.
With two-stage activelearning, we primarily chose sentences that can beparsed by the full parser but not the bagged parserand have the highest fzipf?eng(s) values, we secon-darily chose sentences that cannot be parsed by bothparsers and have the highest fzipf?eng(s) values, thethird priority is given to sentences having highest251fzipf?entropy values.3 Combining length balancedsampling with all of these informativeness measuresis straightforward.
For statistical significance, a dif-ferent random sample is used for length histogramin each replication of experiment.4.2 ResultsResults can be seen in Figure 1 and Table 1.
Dueto lack of space and similarity of the graphs of un-parsed/entropy and LBP, we excluded the graph ofunparsed/entropy (but its results are included in Ta-ble 1).
Since observation points in different lines donot fall on the exactly same performance level (forexact PRUD measurement), we took the points on asclosest f-score levels as possible.
With tree entropy,Zipfian sampling performs almost as good as pureinformativeness based AL and with two-stage AL,length balanced sampling performs almost as goodas Zipfian sampling.
In all other comparisons, Zip-fian sampling outperforms its alternatives substan-tially.5 Conclusion and Future WorkWe introduced a representativeness measure for ac-tive learning in statistical parser training domain,based on an empirical sentence length frequencymodel of English.
Experiments on a wide cover-age CCG parser show that this measure outperformsthe alternative measures most of the time and neverhinders.
Our study can be extended via further ex-perimentation with the methods we excluded in Sec-tion 4.1, with other parsers, with other languagesand with other Zipfian cues of language (e.g.
Zipf?slaw on word frequencies (Zipf, 1949)).AcknowledgmentsWe specially thank to Jason Baldridge, CemBozs?ahin, Ruken C?ak?c?, Rebecca Hwa, Miles Os-borne and anonymous reviewers for their invaluablesupport, advices and feedback.ReferencesJason Baldridge and Miles Osborne.
2003.
Active learn-ing for HPSG parse selection.
In Proceedings ofCoNLL.3Note that our usage of two-stage AL is slightly differentfrom the original definition in (Becker and Osborne, 2005)Jason Baldridge and Miles Osborne.
2004.
Active learn-ing and the total cost of annotation.
In Proceedings ofEMNLP.Jason Baldridge and Miles Osborne.
2008.
Active learn-ing and logarithmic opinion pools for HPSG parse se-lection.
In Natural Language Engineering, volume 14,pages 199?222.
Cambridge, UK.Markus Becker and Miles Osborne.
2005.
A two-stagemethod for active learning of statistical grammars.
InProceedings of IJCAI.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL.Stephen Clark and James R. Curran.
2004.
Parsing theWSJ using CCG and log-linear models.
In Proceed-ings of ACL.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics,33(4):493?552.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of ACL.Shen Dan.
2004.
Multi-criteria based active learning fornamed entity recognition.
Master?s thesis, NationalUniversity of Singapore.W.
Nelson Francis and Henry Kuc?era.
1967.
Com-putational Analysis of Present-day American English.Brown University Press, Providence, RI.Julia Hockenmaier and Mark Steedman.
2005.
CCG-bank.
Linguistic Data Consortium, Philadelphia.Rebecca Hwa.
2004.
Sample selection for statisticalparsing.
Computational Linguistics, 30(3):253?276.Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English:The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Miles Osborne and Jason Baldridge.
2004.
Ensemble-based active learning for parse selection.
In Proceed-ings of HLT-NAACL.Bengt Sigurd, Mats Eeg-Olofsson, and Joost van Weijer.2004.
Word length, sentence length and frequency -Zipf revisited.
Studia Linguistica, 58(1):37?52.Min Tang, Xiaoqiang Luo, and Salim Roukos.
2002.
Ac-tive learning for statistical natural language parsing.
InProceedings of ACL.Cynthia A. Thompson, Mary E. Califf, and Raymond J.Mooney.
1999.
Active learning for natural languageparsing and information extraction.
In Proceedings ofICML.George K. Zipf.
1935.
The Psychobiology of Language.MIT Press, Cambridge, MA.
Reprinted in 1965.George K. Zipf.
1949.
Human Behavior and the Princi-ple of Least Effort.
Addison-Wesley, Cambridge, MA.252
