Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSemEval-2013 Task 2: Sentiment Analysis in TwitterPreslav NakovQCRI, Qatar Foundationpnakov@qf.org.qaZornitsa KozarevaUSC Information Sciences Institutekozareva@isi.eduAlan RitterUniversity of Washingtonaritter@cs.washington.eduSara RosenthalColumbia Universitysara@cs.columbia.eduVeselin StoyanovJHU HLTCOEves@cs.jhu.eduTheresa WilsonJHU HLTCOEtaw@jhu.eduAbstractIn recent years, sentiment analysis in socialmedia has attracted a lot of research interestand has been used for a number of applica-tions.
Unfortunately, research has been hin-dered by the lack of suitable datasets, com-plicating the comparison between approaches.To address this issue, we have proposedSemEval-2013 Task 2: Sentiment Analysis inTwitter, which included two subtasks: A, anexpression-level subtask, and B, a message-level subtask.
We used crowdsourcing onAmazon Mechanical Turk to label a largeTwitter training dataset alg with additionaltest sets of Twitter and SMS messages for bothsubtasks.
All datasets used in the evaluationare released to the research community.
Thetask attracted significant interest and a totalof 149 submissions from 44 teams.
The best-performing team achieved an F1 of 88.9% and69% for subtasks A and B, respectively.1 IntroductionIn the past decade, new forms of communication,such as microblogging and text messaging haveemerged and become ubiquitous.
Twitter messages(tweets) and cell phone messages (SMS) are oftenused to share opinions and sentiments about the sur-rounding world, and the availability of social con-tent generated on sites such as Twitter creates newopportunities to automatically study public opinion.Working with these informal text genres presentsnew challenges for natural language processing be-yond those encountered when working with moretraditional text genres such as newswire.Tweets and SMS messages are short in length: asentence or a headline rather than a document.
Thelanguage they use is very informal, with creativespelling and punctuation, misspellings, slang, newwords, URLs, and genre-specific terminology andabbreviations, e.g., RT for re-tweet and #hashtags.1How to handle such challenges so as to automati-cally mine and understand the opinions and senti-ments that people are communicating has only veryrecently been the subject of research (Jansen et al2009; Barbosa and Feng, 2010; Bifet et al 2011;Davidov et al 2010; O?Connor et al 2010; Pak andParoubek, 2010; Tumasjan et al 2010; Kouloumpiset al 2011).Another aspect of social media data, such as Twit-ter messages, is that they include rich structured in-formation about the individuals involved in the com-munication.
For example, Twitter maintains infor-mation about who follows whom.
Re-tweets (re-shares of a tweet) and tags inside of tweets providediscourse information.
Modeling such structured in-formation is important because it provides means forempirically studying social interactions where opin-ion is conveyed, e.g., we can study the properties ofpersuasive language or those associated with influ-ential users.Several corpora with detailed opinion and senti-ment annotation have been made freely available,e.g., the MPQA corpus (Wiebe et al 2005) ofnewswire text.
These corpora have proved veryvaluable as resources for learning about the lan-guage of sentiment in general, but they did not focuson social media.1Hashtags are a type of tagging for Twitter messages.312Twitter RT @tash jade: That?s really sad, Charlie RT ?Until tonight I never realised how fucked up I was?
-Charlie Sheen #sheenroastSMS Glad to hear you are coping fine in uni...
So, wat interview did you go to?
How did it go?Table 1: Examples of sentences from each corpus that contain subjective phrases.While some Twitter sentiment datasets have al-ready been created, they were either small and pro-prietary, such as the i-sieve corpus (Kouloumpiset al 2011), or they were created only for Span-ish like the TASS corpus2 (Villena-Roma?n et al2013), or they relied on noisy labels obtained fromemoticons and hashtags.
They further focused onmessage-level sentiment, and no Twitter or SMScorpus with expression-level sentiment annotationshas been made available so far.Thus, the primary goal of our SemEval-2013 task2 has been to promote research that will lead to abetter understanding of how sentiment is conveyedin Tweets and SMS messages.
Toward that goal,we created the SemEval Tweet corpus, which con-tains Tweets (for both training and testing) and SMSmessages (for testing only) with sentiment expres-sions annotated with contextual phrase-level polar-ity as well as an overall message-level polarity.
Weused this corpus as a testbed for the system evalua-tion at SemEval-2013 Task 2.In the remainder of this paper, we first describethe task, the dataset creation process, and the evalu-ation methodology.
We then summarize the charac-teristics of the approaches taken by the participatingsystems and we discuss their scores.2 Task DescriptionWe had two subtasks: an expression-level subtaskand a message-level subtask.
Participants couldchoose to participate in either or both subtasks.
Be-low we provide short descriptions of the objectivesof these two subtasks.Subtask A: Contextual Polarity DisambiguationGiven a message containing a marked instanceof a word or a phrase, determine whether thatinstance is positive, negative or neutral in thatcontext.
The boundaries for the marked in-stance were provided: this was a classificationtask, not an entity recognition task.2http://www.daedalus.es/TASS/corpus.phpSubtask B: Message Polarity ClassificationGiven a message, decide whether it is ofpositive, negative, or neutral sentiment.
Formessages conveying both a positive and anegative sentiment, whichever is the strongerone was to be chosen.Each participating team was allowed to submit re-sults for two different systems per subtask: one con-strained, and one unconstrained.
A constrained sys-tem could only use the provided data for training,but it could also use other resources such as lexi-cons obtained elsewhere.
An unconstrained systemcould use any additional data as part of the trainingprocess; this could be done in a supervised, semi-supervised, or unsupervised fashion.Note that constrained/unconstrained refers to thedata used to train a classifier.
For example, if otherdata (excluding the test data) was used to developa sentiment lexicon, and the lexicon was used togenerate features, the system would still be con-strained.
However, if other data (excluding the testdata) was used to develop a sentiment lexicon, andthis lexicon was used to automatically label addi-tional Tweet/SMS messages and then used with theoriginal data to train the classifier, then such a sys-tem would be unconstrained.3 Dataset CreationIn the following sections we describe the collectionand annotation of the Twitter and SMS datasets.3.1 Data CollectionTwitter is the most common micro-blogging site onthe Web, and we used it to gather tweets that expresssentiment about popular topics.
We first extractednamed entities using a Twitter-tuned NER system(Ritter et al 2011) from millions of tweets, whichwe collected over a one-year period spanning fromJanuary 2012 to January 2013; we used the publicstreaming Twitter API to download tweets.313Instructions: Subjective words are ones which convey an opinion.
Given a sentence, identify whether it is objective,positive, negative, or neutral.
Then, identify each subjective word or phrase in the context of the sentence and markthe position of its start and end in the text boxes below.
The number above each word indicates its position.
Theword/phrase will be generated in the adjacent textbox so that you can confirm that you chose the correct range.Choose the polarity of the word or phrase by selecting one of the radio buttons: positive, negative, or neutral.
If asentence is not subjective please select the checkbox indicating that ?There are no subjective words/phrases?.
Pleaseread the examples and invalid responses before beginning if this is your first time answering this hit.Figure 1: Instructions provided to workers on Mechanical Turk followed by a screenshot.Average # of Total Phrase Count VocabularyCorpus Words Characters Positive Negative Neutral SizeTwitter - Training 25.4 120.0 5,895 3,131 471 20,012Twitter - Dev 25.5 120.0 648 430 57 4,426Twitter - Test 25.4 121.2 2,734 1,541 160 11,736SMS - Test 24.5 95.6 1,071 1,104 159 3,562Table 2: Statistics for Subtask A.We then identified popular topics as those namedentities that are frequently mentioned in associationwith a specific date (Ritter et al 2012).
Given thisset of automatically identified topics, we gatheredtweets from the same time period which mentionedthe named entities.
The testing messages had differ-ent topics from training and spanned later periods.To identify messages that express sentiment to-wards these topics, we filtered the tweets us-ing SentiWordNet (Baccianella et al 2010).
Weremoved messages that contained no sentiment-bearing words, keeping only those with at least oneword with positive or negative sentiment score thatis greater than 0.3 in SentiWordNet for at least onesense of the words.
Without filtering, we found classimbalance to be too high.3Twitter messages are rich in social media features,including out-of-vocabulary (OOV) words, emoti-cons, and acronyms; see Table 1.
A large portion ofthe OOV words are hashtags (e.g., #sheenroast)and mentions (e.g., @tash jade).3Filtering based on an existing lexicon does bias the datasetto some degree; however, note that the text still contains senti-ment expressions outside those in the lexicon.Corpus Positive Negative Objective/ NeutralTwitter - Training 3,662 1,466 4,600Twitter - Dev 575 340 739Twitter - Test 1,573 601 1,640SMS - Test 492 394 1,208Table 3: Statistics for Subtask B.We annotated the same Twitter messages with an-notations for subtask A and subtask B. However,the final training and testing datasets overlap onlypartially between the two subtasks since we hadto throw away messages with low inter-annotatoragreement, and this differed between the subtasks.For testing, we also annotated SMS messages, takenfrom the NUS SMS corpus4 (Chen and Kan, 2012).Tables 2 and 3 show statistics about the corpora wecreated for subtasks A and B.4http://wing.comp.nus.edu.sg/SMSCorpus/314A BLower Avg.
Upper Avg.Twitter - Train 64.7 82.4 90.8 82.7Twitter - Dev 51.2 74.7 87.8 78.4Twitter - Test 68.8 83.6 90.9 76.9SMS - Test 66.5 88.5 81.2 77.6Table 4: Bounds for datasets in subtasks A and B.3.2 Annotation GuidelinesThe instructions provided to the annotators, alongwith an example, are shown in Figure 1.
We pro-vided several additional examples to the annotators,shown in Table 5.In addition, we filtered spammers by consideringthe following kinds of annotations invalid:?
containing overlapping subjective phrases;?
subjective but without a subjective phrase;?
marking every single word as subjective;?
not having the overall sentiment marked.3.3 Annotation ProcessOur datasets were annotated for sentiment on Me-chanical Turk.
Each sentence was annotated by fiveMechanical Turk workers (Turkers).
In order toqualify for the hits, the Turker had to have an ap-proval rate greater than 95% and have completed 50approved hits.
Each Turker was paid three centsper hit.
The Turker had to mark all the subjec-tive words/phrases in the sentence by indicating theirstart and end positions and say whether each subjec-tive word/phrase was positive, negative, or neutral(subtask A).
They also had to indicate the overallpolarity of the sentence (subtask B).Figure 1 shows the instructions and an exam-ple provided to the Turkers.
The first five rowsof Table 6 show an example of the subjectivewords/phrases marked by each of the workers.For subtask A, we combined the annotations ofeach of the workers using intersection as indicatedin the last row of Table 6.
A word had to appearin 2/3 of the annotations in order to be consideredsubjective.
Similarly, a word had to be labeled witha particular polarity (positive, negative, or neutral)2/3 of the time in order to receive that label.We also experimented with combining annota-tions by computing the union of the sentences, andtaking the sentence of the worker who annotated themost hits, but we found that these methods werenot as accurate.
Table 4 shows the lower, average,and upper bounds for all the hits by computing thebounds for each hit and averaging them together.This gives a good indication about how well we canexpect the systems to perform.
For example, even ifwe used the best annotator each time, it would stillnot be possible to get perfect accuracy.For subtask B, the polarity of the entire sentencewas determined based on the majority of the labels.If there was a tie, the sentence was discarded.
Inorder to reduce the number of sentences lost, wecombined the objective and the neutral labels, whichTurkers tended to mix up.
Table 4 shows the aver-age bound for subtask B by computing the boundsfor each hit and averaging them together.
Since thepolarity is chosen based on the majority, the upperbound is 100%.4 ScoringFor both subtasks, the participating systems wererequired to perform a three-way classification ?
aparticular marked phrase (for subtask A) or an en-tire message (for subtask B) was to be classified aspositive, negative, or objective.
For each system,we computed a score for predicting positive/negativephrases/messages vs. the other two classes.For instance, to compute positive precision, Ppos,we find the number of phrases/messages that a sys-tem correctly predicted to be positive, and we dividethat number by the total number of messages it pre-dicted to be positive.
To compute recall, for the pos-itive class, Rpos, we find the number of messagescorrectly predicted to be positive and we divide thatnumber by the total number of positive messages inthe gold standard.We then calculate F-score for the positive labels,the harmonic average of precision and recall as fol-lows Fpos = 2PposRposPpos+Rpos.
We carry out a similarcomputation to calculate Fneg, which is F1 for neg-ative messages.The overall score for each system run is thengiven by the average of the F1-scores for the posi-tive and negative classes: F = (Fpos + Fneg)/2.315Authorities are only too aware that Kashgar is 4,000 kilometres (2,500 miles) from Beijing but only a tenth ofthe distance from the Pakistani border, and are desperate to ensure instability or militancy does not leak over thefrontiers.Taiwan-made products stood a good chance of becoming even more competitive thanks to wider access to overseasmarkets and lower costs for material imports, he said.
?March appears to be a more reasonable estimate while earlier admission cannot be entirely ruled out,?
accordingto Chen, also Taiwan?s chief WTO negotiator.friday evening plans were great, but saturday?s plans didnt go as expected ?
i went dancing & it was an ok club,but terribly crowded :-(WHY THE HELL DO YOU GUYS ALL HAVE MRS. KENNEDY!
SHES A FUCKING DOUCHEAT&T was okay but whenever they do something nice in the name of customer service it seems like a favor, whileT-Mobile makes that a normal everyday thinobama should be impeached on TREASON charges.
Our Nuclear arsenal was TOP Secret.
Till HE told our enemieswhat we had.
#Coward #TraitorMy graduation speech: ?I?d like to thanks Google, Wikipedia and my computer!
:D #iThingteensTable 5: List of example sentences with annotations that were provided to the annotators.
All subjective phrases areitalicized.
Positive phrases are in green, negative phrases are in red, and neutral phrases are in blue.Worker 1 I would love to watch Vampire Diaries :) and some Heroes!
Great combination 9/13Worker 2 I would love to watch Vampire Diaries :) and some Heroes!
Great combination 11/13Worker 3 I would love to watch Vampire Diaries :) and some Heroes!
Great combination 10/13Worker 4 I would love to watch Vampire Diaries :) and some Heroes!
Great combination 13/13Worker 5 I would love to watch Vampire Diaries :) and some Heroes!
Great combination 11/13Intersection I would love to watch Vampire Diaries :) and some Heroes!
Great combinationTable 6: Example of a sentence annotated for subjectivity on Mechanical Turk.
Words and phrases that were marked assubjective are italicized and highlighted in bold.
The first five rows are annotations provided by Turkers, and the finalrow shows their intersection.
The final column shows the accuracy for each annotation compared to the intersection.Note that ignoring Fneutral does not reduce thetask to predicting positive vs. negative labels only(even though some participants have chosen to doso) since the gold standard still contains neutrallabels which are to be predicted: Fpos and Fnegwould suffer if these examples are labeled as posi-tive and/or negative instead of neutral.We provided participants with a scorer.
In addi-tion to outputting the overall F-score, it produceda confusion matrix for the three prediction classes(positive, negative, and objective), and it also vali-dated the data submission format.5 Participants and ResultsThe results for subtask A are shown in Tables 7 and8 for Twitter and for SMS messages, respectively;those for subtask B are shown in Table 9 for Twit-ter and in Table 10 for SMS messages.
Systems areranked by their scores for the constrained runs; theranking based on scores for unconstrained runs isshown as a subindex.For both subtasks, there were teams that only sub-mitted results for the Twitter test set.
Some teamssubmitted both a constrained and an unconstrainedversion (e.g., AVAYA and teragram).
As one wouldexpect, the results on the Twitter test set tended to bebetter than those on the SMS test set since the SMSdata was out-of-domain with respect to the training(Twitter) data.Moreover, the results for subtask A were signifi-cantly better than those for subtask B, which showsthat it is a much easier task, probably because thereis less ambiguity at the phrase-level.5.1 Subtask A: Contextual PolarityTable 7 shows that subtask A, Twitter, attracted 23teams, who submitted 21 constrained and 7 uncon-strained systems.
Five teams submitted both a con-strained and an unconstrained system, and two otherteams submitted constrained systems that are onthe boundary between being constrained and uncon-strained.316Run Const- Unconst- Use Super-rained rained Neut.?
vised?NRC-Canada 88.93 yes yesAVAYA 86.98 87.38(1) yes yesBOUNCE 86.79 yes yesLVIC-LIMSI 85.70 yes yesFBM 85.50 yes semiGU-MLT-LT 85.19 yes yesUNITOR 84.60 yes yesUSNA 81.31 yes yesSerendio 80.04 yes yesECNUCS 79.48 80.15(2) yes yesTJP 78.16 yes yes?columbia-nlp 74.94 yes yesteragram 74.89(3) yes yessielers 74.41 yes yesKLUE 73.74 yes yesOPTWIMA 69.17 36.91(6) yes yesswatcs 67.19 63.86(5) no yesKea 63.94 yes yessenti.ue-en 62.79 71.38(4) yes yesuottawa 60.20 yes yesIITB 54.80 yes yesSenselyticTeam 53.88 yes yesSU-sentilab 34.73(7) no yesMajority Baseline 38.10 N/A N/ATable 7: Results for subtask A on the Twitter dataset.
The?
marks a team that includes a task coorganizer, and the indicates a system submitted as constrained but whichused additional Tweets or additional sentiment-annotatedtext to collect statistics that were then used as a feature.One system was semi-supervised, and the restwere supervised.
The supervised systems used clas-sifiers such as SVM (8 systems), Naive Bayes (7 sys-tems), and Maximum Entropy (3 systems).
Otherapproaches used include an ensemble of classifiers,manual rules, and a linear classifier.
Two of the sys-tems chose not to predict neutral as a possible clas-sification label.The average F1-measure on the Twitter test setwas 74.1% for constrained systems and 60.5% forunconstrained ones; this does not mean that usingadditional data does not help, it just shows that thebest teams only participated with a constrained sys-tem.
NRC-Canada had the best constrained systemwith an F1-measure of 88.9%, and AVAYA had thebest unconstrained one with F1=87.4%.Run Const- Unconst- Use Super-rained rained Neut.?
vised?GU-MLT-LT 88.37 yes yesNRC-Canada 88.00 yes yes?AVAYA 83.94 85.79(1) yes yesUNITOR 82.49 yes yesTJP 81.23 yes yesLVIC-LIMSI 80.16 yes yesUSNA 79.82 yes yesECNUCS 76.69 77.34(2) yes yessielers 73.48 yes yesFBM 72.95 no semiteragram 72.83 72.83(4) yes yesKLUE 70.54 yes yes?columbia-nlp 70.30 yes yessenti.ue-en 66.09 74.13(3) yes yesswatcs 66.00 67.68(5) no yesKea 63.27 yes yesuottawa 55.89 yes yesSU-sentilab 55.38(6) no yesSenselyticTeam 51.13 yes yesOPTWIMA 37.32 36.38(7) yes yesMajority Baseline 31.50 N/A N/ATable 8: Results for subtask A on the SMS dataset.
The?
indicates a late submission, the ?
marks a team thatincludes a task co-organizer, and the  indicates a sys-tem submitted as constrained but which used additionalTweets or additional sentiment-annotated text to collectstatistics that were then used as a feature.Table 8 shows the results for the SMS test set,where 20 teams submitted 19 constrained and 7 un-constrained systems (again, this included two teamsthat submitted boundary systems, marked accord-ingly).
The average F-measure on this test setwas 70.8% for constrained systems and 65.7% forunconstrained systems.
The best constrained sys-tem was that of GU-MLT-LT with an F-measure of88.4%, and AVAYA had the best unconstrained sys-tem with an F1 of 85.8%.5.2 Subtask B: Message PolarityTable 9 shows that subtask B, Twitter, attracted 38teams, who submitted 36 constrained and 15 uncon-strained systems (and two boundary ones).The average F1-measure was 53.7% for the con-strained and 54.6% for the unconstrained systems.317Run Const- Unconst- Use Super-rained rained Neut.?
vised?NRC-Canada 69.02 yes yesGU-MLT-LT 65.27 yes yesteragram 64.86 64.86(1) yes yesBOUNCE 63.53 yes yesKLUE 63.06 yes yesAMI&ERIC 62.55 61.17(3) yes yes/semiFBM 61.17 yes yesAVAYA 60.84 64.06(2) yes yes/semiSAIL 60.14 61.03(4) yes yesUT-DB 59.87 yes yesFBK-irst 59.76 yes yesnlp.cs.aueb.gr 58.91 yes yesUNITOR 58.27 59.50(5) yes semiLVIC-LIMSI 57.14 yes yesUmigon 56.96 yes yesNILC USP 56.31 yes yesDataMining 55.52 yes semiECNUCS 55.05 58.42(6) yes yesnlp.cs.aueb.gr 54.73 yes yesASVUniOfLeipzig 54.56 yes yesSZTE-NLP 54.33 53.10(9) yes yesCodeX 53.89 yes yesOasis 53.84 yes yesNTNU 53.23 50.71(10) yes yesUoM 51.81 45.07(15) yes yesSSA-UO 50.17 yes noSenselyticTeam 50.10 yes yesUMCC DLSI (SA) 49.27 48.99(12) yes yesbwbaugh 48.83 54.37(8) yes yes/semisenti.ue-en 47.24 47.85(13) yes yesSU-sentilab 45.75(14) yes yesOPTWIMA 45.40 54.51(7) yes yesREACTION 45.01 yes yesuottawa 42.51 yes yesIITB 39.80 yes yesIIRG 34.44 yes yessinai 16.28 49.26(11) yes yesMajority Baseline 29.19 N/A N/ATable 9: Results for subtask B on the Twitter dataset.
The indicates a system submitted as constrained but whichused additional Tweets or additional sentiment-annotatedtext to collect statistics that were then used as a feature.These averages are much lower than those for sub-task A, which indicates that subtask B is harder,probably because a message can contain parts ex-pressing both positive and negative sentiment.Run Const- Unconst- Use Super-rained rained Neut.?
vised?NRC-Canada 68.46 yes yesGU-MLT-LT 62.15 yes yesKLUE 62.03 yes yesAVAYA 60.00 59.47(1) yes yes/semiteragram 59.10(2) yes yesNTNU 57.97 54.55(6) yes yesCodeX 56.70 yes yesFBK-irst 54.87 yes yesAMI&ERIC 53.63 52.62(7) yes yes/semiECNUCS 53.21 54.77(5) yes yesUT-DB 52.46 yes yesSAIL 51.84 51.98(8) yes yesUNITOR 51.22 48.88(10) yes semiSZTE-NLP 51.08 55.46(3) yes yesSenselyticTeam 51.07 yes yesNILC USP 50.12 yes yesREACTION 50.11 yes yesSU-sentilab 49.57(9) no yesnlp.cs.aueb.gr 49.41 55.28(4) yes yesLVIC-LIMSI 49.17 yes yesFBM 47.40 yes yesASVUniOfLeipzig 46.50 yes yessenti.ue-en 44.65 46.72(12) yes yesSSA UO 44.39 yes noUMCC DLSI (SA) 43.39 40.67(14) yes yesUoM 42.22 35.22(15) yes yesOPTWIMA 40.98 47.15(11) yes yesuottawa 40.51 yes yesbwbaugh 39.73 43.43(13) yes yes/semiIIRG 22.16 yes yesMajority Baseline 19.03 N/A N/ATable 10: Results for subtask B on the SMS dataset.
The indicates a system submitted as constrained but whichused additional Tweets or additional sentiment-annotatedtext to collect statistics that were then used as a feature.Once again, NRC-Canada had the best con-strained system with an F1-measure of 69%, fol-lowed by teragram, which had the best uncon-strained system with an F1-measure of 64.9%.As Table 10 shows, the average F1-measure onthe SMS test set was 50.2% for constrained and50.3% for unconstrained systems.
NRC-Canada hadthe best constrained system with an F1=68.5%, andAVAYA had the best unconstrained one with F1-measure of 59.5%.3185.3 OverallOverall, the results achieved by the best teams werevery strong, especially for the simpler subtask A:?
F1=88.93, NRC-Canada on subtask A, Twitter;?
F1=88.37, GU-MLT-LT on subtask A, SMS;?
F1=69.02, NRC-Canada on subtask B, Twitter;?
F1=68.46, NRC-Canada on subtask B, SMS.We can see that the strongest team overall was thatof NRC-Canada, which was ranked first on three ofthe four conditions; and it was second on subtask A,SMS.
There were two other teams that were strongacross both tasks and on both test sets: GU-MLT-LTand AVAYA.
Three other teams, namely teragram,BOUNCE and KLUE, were ranked in the top-3 in atleast one subtask and test set.6 DiscussionWe have seen that most participants restricted them-selves to the provided data and submitted con-strained systems.
Indeed, the best systems for eachof the two subtasks and for each of the two testingdatasets were constrained systems; of course, thisdoes not mean that additional data would not be use-ful.
Curiously, in some cases where a team submit-ted a constrained and unconstrained run, the uncon-strained run actually performed worse.Not surprisingly, most systems were supervised;there were only five semi-supervised systems, andthere was only one unsupervised system.
One ad-ditional team declared their system as unsupervisedsince it was not making use of the training data; westill classified it as supervised though since it did usesupervision ?
in the form of manual rules.Most participants predicted all three labels (posi-tive, negative and neutral), even though some partic-ipants opted for not predicting neutral, which madesome sense since the final F1-score was averagedover the positive and the negative predictions only.The most popular classifiers included SVM, Max-Ent, linear classifier, Naive Bayes; in some cases,manual rules or ensembles of classifiers were used.A variety of features were used, including word-related (e.g., words, stems, n-grams, word clus-ters), word-shape (e.g., punctuation, capitalization),syntactic (e.g., POS tags, dependency relations),Twitter-specific (e.g., repeated characters, emoti-cons, URLs, hashtags, slang, abbreviations), andsentiment-related (e.g., negation); one team alsoused discourse relations.
Almost all participants re-lied heavily of various sentiment lexicons, the mostpopular ones being MPQA and SentiWordNet, aswell as AFINN and Bing Liu?s Opinion Lexicon;some participants used their own lexicons ?
preex-isting or built from the provided data.Given that Twitter messages are noisy, most par-ticipants did some preprocessing, including tok-enization, stemming, lemmatization, stopword re-moval, normalization/removal of URLs, hashtags,users, slang, emoticons, repeated vowels, punctua-tion; some even did pronoun resolution.7 ConclusionWe have described a new task that entered SemEval-2013: task 2 on Sentiment Analysis on Twitter.
Thetask has attracted a very high number of participants:149 submissions from 44 teams.We believe that the datasets that we have createdas part of the task and which we have released to thecommunity5 under a Creative Commons Attribution3.0 Unported License,6 will be found useful by re-searchers beyond SemEval.AcknowledgmentsThe authors would like to thank Kathleen McKeownfor her insight in creating the Amazon MechanicalTurk annotation task.Funding for the Amazon Mechanical Turk anno-tations was provided by the JHU Human LanguageTechnology Center of Excellence and by the Of-fice of the Director of National Intelligence (ODNI),Intelligence Advanced Research Projects Activity(IARPA), through the U.S. Army Research Lab.
Allstatements of fact, opinion or conclusions containedherein are those of the authors and should not beconstrued as representing the official views or poli-cies of IARPA, the ODNI or the U.S. Government.5http://www.cs.york.ac.uk/semeval-2013/task2/6http://creativecommons.org/licenses/by/3.0/319ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An enhanced lexi-cal resource for sentiment analysis and opinion min-ing.
In Nicoletta Calzolari (chair), Khalid Choukri,Bente Maegaard, Joseph Mariani, Jan Odijk, SteliosPiperidis, Mike Rosner, and Daniel Tapias, editors,Proceedings of the Seventh International Conferenceon Language Resources and Evaluation, LREC ?10,pages 2200?2204, Valletta, Malta.Luciano Barbosa and Junlan Feng.
2010.
Robust senti-ment detection on Twitter from biased and noisy data.In Proceedings of the 23rd International Conferenceon Computational Linguistics: Posters, COLING ?10,pages 36?44, Beijing, China.Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer, andRicard Gavalda`.
2011.
Detecting sentiment change inTwitter streaming data.
Journal of Machine LearningResearch - Proceedings Track, 17:5?11.Tao Chen and Min-Yen Kan. 2012.
Creating a live, pub-lic short message service corpus: the NUS SMS cor-pus.
Language Resources and Evaluation, pages 1?37.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Semi-supervised recognition of sarcastic sentences inTwitter and Amazon.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning, CoNLL ?10, pages 107?116, Upp-sala, Sweden.Bernard J. Jansen, Mimi Zhang, Kate Sobel, and AbdurChowdury.
2009.
Twitter power: Tweets as elec-tronic word of mouth.
J.
Am.
Soc.
Inf.
Sci.
Technol.,60(11):2169?2188.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The Goodthe Bad and the OMG!
In Lada A. Adamic, Ricardo A.Baeza-Yates, and Scott Counts, editors, Proceedings ofthe Fifth International Conference on Weblogs and So-cial Media, ICWSM?
11, pages 538?541, Barcelona,Spain.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In William W. Cohen andSamuel Gosling, editors, Proceedings of the FourthInternational Conference on Weblogs and SocialMedia, ICWSM ?10, Washington, DC, USA.Alexander Pak and Patrick Paroubek.
2010.
Twitterbased system: Using Twitter for disambiguating senti-ment ambiguous adjectives.
In Proceedings of the 5thInternational Workshop on Semantic Evaluation, Se-mEval ?10, pages 436?439, Los Angeles, CA, USA.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: an exper-imental study.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,EMNLP ?11, pages 1524?1534, Edinburgh, UnitedKingdom.Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.2012.
Open domain event extraction from twitter.
InProceedings of the 18th ACM SIGKDD internationalconference on Knowledge discovery and data mining,KDD ?12, pages 1104?1112, Beijing, China.Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-ner, and Isabell M. Welpe.
2010.
Predicting electionswith Twitter: What 140 characters reveal about po-litical sentiment.
In William W. Cohen and SamuelGosling, editors, Proceedings of the Fourth Inter-national Conference on Weblogs and Social Media,ICWSM ?10, pages 178?185, Washington, DC, USA.The AAAI Press.Julio Villena-Roma?n, Sara Lana-Serrano, Euge-nio Mart?
?nez-Ca?mara, and Jose?
Carlos Gonza?lezCristo?bal.
2013.
TASS - Workshop on SentimentAnalysis at SEPLN.
Procesamiento del LenguajeNatural, 50:37?44.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation, 39(2-3):165?210.320
