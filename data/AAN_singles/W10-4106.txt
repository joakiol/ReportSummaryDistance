Semantic class induction and its application for a Chinese voicesearch systemYali LiThinkIT laboratory,Institute ofAcoustics, ChineseAcademy of Sciencesliyali@hccl.ioa.ac.cnWeiqun XuThinkIT laboratory,Institute of Acoustics,Chinese Academy ofSciencesxuweiqun@hccl.ioa.ac.cnYonghong YanThinkIT laboratory,Institute ofAcoustics, ChineseAcademy of Sciencesyyan@hccl.ioa.ac.cnAbstractIn this paper, we propose a novelsimilarity measure based onco-occurrence probabilities for inducingsemantic classes.
Clustering with the newsimilarity measure outperformed thatwith the widely used distance measurebased on Kullback-Leibler divergence inprecision, recall and F1 evaluation.
Wethen use the induced semantic classes andstructures by the new similarity measureto generate in-domain data.
At last, weuse the generated data to do languagemodel adaptation and improve the resultof character recognition from 85.2% to91%.1 IntroductionVoice search (e.g.
Wang et al, 2008) hasrecently become one of the major foci in spokendialogue system research and development.
Inmain stream large vocabulary ASR engines,statistical language models (n-grams inparticular), usually trained with plenty of data,are widely used and proved very effective.
Butfor a voice search system, we have to deal withthe case where there is no or very little relevantdata for language modeling.
One of theconventional solutions to this problem is tocollect and use some human-human orWizard-of-Oz (WOZ) dialogue data.
Once theinitial system is up running, the performance canbe further improved with human-computer datain a system-in-the-loop style.
Another practicalapproach is to handcraft some grammar rules andgenerate some artificial data.
But writinggrammars manually is tedious andtime-consuming and requires some linguisticexpertise.In this paper, we introduced a new similaritymeasure to induce semantic classes andstructures.
We then generated a large number ofdata using the induced semantic classes andstructures to make language model adaptation.At the end, we give the conclusion and impliedthe future work.2 Semantic Class InductionThe studies on semantic class induction in spokenlanguage (or spoken language acquisition ingeneral) have received some attention since themiddle 90's.
One of the earlier works is carriedout by Gorin (1995), who employed aninformation -theoretic connectionist networkembedded in a feedback control system to acquirespoken language.
Later on Arai et al (1999)further studied how to acquire grammarfragments in fluent speech through clusteringsimilar phrases using Kullback-Leibler distance.Meng and Siu (2002) proposed tosemi-automatically induce language structuresfrom unannotated corpora for spoken languageunderstanding, mainly using Kullback-Lieblerdivergence and mutual information.
Pargellis etal.
(2004) used similar measures (plus threeothers) to induce semantic classes for comparingdomain concept independence and portingconcepts across domains.
Potamianos (2005,2006, 2007) and colleagues conducted a series ofstudies to further improve semantic classinduction, including combining wide and narrowcontext similarity measures, and adopting asoft-clustering algorithm (via a probabilisticclass-membership function).2.1 ClusteringIn general, words and phrases which appear insimilar context usually share similar semantics.E.g., ????
(Tsinghua University) and ????
(Peking University) in the following twoutterances (literal translations are given inbrackets) are both names of place ororganisation.?
?
????
??
?
??
?Please/look for/Tsinghua University/near//bank(Please look for banks near TsinghuaUniversity.)?
?
????
??
?
???
?Please/look for/Peking University/nearby//gym(Please look for gyms near Peking University.
)To automatically discover that the above twowords have similar semantics from unannotatedcorpus, we try unsupervised clustering based onsome similarity measures to induce semanticclasses.
Further details about similarity measuresare given in section 2.2.Before clustering, the utterances aresegmented into phrases using a simple maximummatching against a lexicon.
Clustering areconducted on phrases, which may be of a singleword.2.2 Similarity MeasuresFor lexical distributional similarity, severalmeasures have been proposed and adopted, e.g.,Meng and Siu (2002), Lin(1998), Dagan et al(1999), Weeds et al (2004).We use two kinds of similarity measures inthe experiments.
One is similarity measure basedon distance, and the other is a new similaritymeasure directly using the co-occurrenceprobabilities.2.3 Distance based similarity measuresThe relative entropy between two probabilitymass functions )(xp  and )(xq  is defined by(Cover and Thomas, 2006) as:)()(log)()(log)()||(xqxpExqxpxpqpD pXx== ??
(1)The relative entropy, as an asymmetricdistance between two distributions, measures theinefficiency of assuming that the distribution isq  when the true distribution is p .It is commonly used as a statistical distanceand can be symmetry as follows:)||()||(),( pqDqpDqpdiv +=      (2)For two words in a similar context, e.g., inthe sequence { ,...,,..., 11 www ?
},where w  can be word a  or b , the rightbigram )||(1RR baD and )||(1RR abD  aredefined as:?
?= W b)|p(wa)|p(wa)|p(wbaDwRR1 1111 log)||((3)and?
?= W a)|p(wb)|p(wb)|p(wabDwRR1 1111 log)||( (4)where W  is the set of words or phrases.And the symmetric divergence is)||()||(),( 111RRRRRR abDbaDbadiv += (5)The left bigram symmetric divergence can besimilarly defined.Using both left and right symmetricdivergences, the distance between a  and bis),(),(),( 111RRLL badivbadivbad +=     (6)So the KL distance becomes:?????????
????
??
?W a)|p(wb)|p(wb)|p(w+W b)|p(wa)|p(wa)|p(w+W a)|p(wb)|p(wb)|p(w+W b)|p(wa)|p(wa)|p(w=)b,div(a+)b,div(a=b)KL(a,wwwwRRLL1 1111 1111 1111111loglogloglog(7)This is the widely used distance measure forlexical semantic similarity, e.g., Dagan et al(1999); Meng and Siu (2002); Pargellis et al(2004).
We can also see the IR distance and L1distance below:?????
+?
+?
+?
+?
?????
???
?W b)|p(wa)|p(wb)|p(wb)|p(w+W b)|p(wa)|p(wa)|p(wa)|p(w+W b)|p(wa)|p(wb)|p(wb)|p(w+W b)|p(wa)|p(wa)|p(wa)|p(w=b)IR(a,wwww1 11111 11111 11111 11112log2log2log2log(8)We can see from the IR metric that it issimilar to the KL distance.
Manhattan-norm (L1)distance :????????
?Wb)|p(wa)|p(w+Wb)|p(wa)|p(w=b)(a,Lww111111||||1(9)In Pargellis et al (2004), the lexical contextis further extended from bigrams to trigrams asfollows.
For the sequence:,...,,,,..., 2112 wwwww ?
?where w  can be word a  or b , the trigramKL between  a  and b is:??????????
????????
?????
?Ww, a)|wp(wb)|wp(wb)|wp(w+Ww, b)|wp(wa)|wp(wa)|wp(w+Ww, a)|wpwb)|wp(wb)|wp(w+Ww, b)|wp(wa)|wp(wa)|wp(w=b)(a,KLwww 2w221 2121212121212122 1121222121212loglogloglog(10)Since more information is taken into accountin b)(a,KL2 , more constraints are imposed on thesimilarity measure.
This is expected to improvethe precision of clustering but may lead to a lowerrecall.2.4 Co-occurrence Probability basedsimilarity measuresAfter a close investigation of the corpus, wecame up with an intuitive similarity measuredirectly based on the co-occurrence probability.The key idea is that the more commonneighbouring words or phrases any two words orphrases in question share, the more similar theyare to each other.
Therefore, for each left or rightneighboring word or phrase, we take the lowerconditional probability into account.Thus we have the following similaritymeasures:Similarity using the bigram context??????
?Wb))|p(wa),|(p(w+Wb))|p(wa),|(p(w=b)(a,Sww1111111minmin(11)Similarity using the trigram context?????????
?Ww,b))|wp(wa),|w(p(w+Wwb))|wp(wa),|w(p(w=b)(a,Sww21212112,12122minmin(12)Similarity extending b)(a,S1 , taking both leftand right contexts into account simultaneously???
?
?Ww, b))|wp(wa),|w(p(w+S=b)(a,Sw 11111113min (13)After pairs of words or phrases are clusteredabove, those pairs with common members arefurther merged.2.5 Comparison of measuresThe KL distances emphasize on the difference oftwo probability but the new measure take theprobability itself into account.
Take the rightbigram context the similarity measure forexample:)loglog(1111 111a)|p(wb)|p(wb)|p(w+W b)|p(wa)|p(wa)|p(w=b)(a,KLwR ??
(14)seeing  as )|( 1 awP x  and seeingas , the equation changed to:)|( 1 bwPy?
)xyy+yx(x=y)(x,KL R loglog  (15)and y)(x,SR  becomes to: ?= ),min(),( yxyxS R              (16)We can also get the y)(x,IRR  and |1 y)(x,L R?
++= )yx yy+yx x(xy)(x,IR R 2log2log   (17)and ||1 yxy)(x,L R ?=                   (18)We can see the space distribution in Figure.1.Figure 1.
Space distribution of different metrics0==zyx(19)zyx ==                            (20)We can see from the four figures (the spacedistribution of four bigram metrics) that fourcurve surface are all symmetric.
The curvesurface of the three distance (KL,IR, L1) allcontain the curve of (19), and curve surface ofthe minimum similarity contains the curve of(20).
We say that the KL distances, IR distancesand L1 distances all emphasize only on thedistances and don't take the probability itself intoaccount.We take the right context of two pairs),( 11 ba  and ),( 22 ba  for example.
If9.0)|(,1.0)|( 111 == awpawp9.0)|(,1.0)|( 121 == bwpbwp0.10.9, 232 =)a|p(w=)a|p(w'1.0)|(,9.0)|( 242' == bwpbwpThe calculation is shown as follows:00.10.1log*0.10.10.1log*0.1loglog11111111=+=)a|p(w)b|p(w)b|p(w+)b|p(w)a|p(w)a|p(w=)b,(aKL R00.90.9log*0.90.90.9log*0.9loglog22222222=+=)a|p(w)b|p(w)b|p(w+)b|p(w)a|p(w=),b(aKL'''''R)a|p(w '1.0)1.0,1.0min(1111==R )|()|(min )bwp,aw(p=),b(aS9.0)9.0,9.0min(2222==R )|()|(min )bwp,aw(p=),b(aS''The KL calculation result of two pairs is thesame but the new similarity calculated that),( 22 ba   is more similar than ),( 11 babecause they have more similar contextprobability 0.9.3 Experiments and Results3.1 DataIn our experiments, four types of corpora areexploited in different stages and different ways.z T: A large collection of text corpus is usedto train a general n-gram language model.z H: Some WOZ dialogues were collectedbefore the system is built, using a similarscenario where users talked in Chinese to aservice provider (human) via telephone tosearch for local information, or informationabout some local points of interest (POI).These dialogues were manually transcribedand used for language model training.
Thisis the best data we could get before thesystem is built though it is not the real butnear in-domain data.z C: After the initial system was up running,some real human-computer dialogues werecollected and transcribed.
These dialogueswere split into three sets.
One (C1) is usedfor semantic class and structure induction.One (C2) is used as test data.
The other (C3)is reserved.z A: Domain information (domain entities) isused in conjunction with the inducedsemantic classes and structures from C1 togenerate a large amount of in-domaincorpus for language model adaptation.
InTable 1,  we give some statistics in termsof the number of utterances(no.
u) andChinese characters(no.
c) for the abovecorpora.corpus no.u no.cT  38,636  8,706,340H  6,652  151,460C1  658  15,434C2  1,000  19,284C3  411  8,014A  14,205  365,576Table 1. statistics of different corpus3.2 Semantic ClusteringWe conducted clustering with the abovesimilarity measures on the data set C1.During the clustering, it is required that all theprobabilities involved in calculating similarity belarger than 0.
We have no threshold except thisconstraint.The outcomes are pairs of phrases.It is noticed that most of the clustered wordsand phrases are domain entities.In our experiments, we merged the inducedsimilar pairs into large clusters.
For example, ifa  is similar to b  and b  is similar to c , then( a , b , c ) are merged into one category.
In theend we use the categories to replace those wordsand phrases in corpus C1 and obtained templates.Examples of  the results are given below.$ask $toponym $near $wh-word $sevice[??]
$ask $toponym $near ?
$sevice ??
?
$toponym $ask ???
$poiwhere:$ask = ??
| ??
?| ????
| ...$toponym = ????
| ???
| ...$sevice = ??
| ???
| ???
|...$near = ??
| ??
| ...$wh-word = ???
| ???
| ???
| ...$poi = ????
| ?????
| ...To evaluate the induction performance, wecompare the induced word pairs against manualannotation.
We manually annotated each phrasewith a tag like $toponym, $poi and so on.
If aand b  are calculated as a pairs and theannotation is the same, we see that they arecorrectly induced which is referred to Pangos(2006).We compute the metrics of precision P ,recall R  and f-score 1F  as follows:%100?=MmP                        (21)where m  is the number of correctly inducedpairs, and M  is the number of induced pairs.%100?=NnR                        (22)where n  is the number of correctly inducedwords and phrases, and N  is the number ofwords and phrases in the annotation.%10021 ?+?
?=RPRPF                 (23)which is a harmonic mean of P  and R .Figure 2.
Induction processThe iterate process we adopted is as inPargellis et al (2004).
In the first iteration, wecalculated the similarity and use the largestsimilarity pairs to generate large classes whichcan be called semantic generalizer.
Then we usethese semantic classes to replace the corpus, andobtained new corpus just as the examplepresented above.
Then we duplicate this processfor the second iteration and so on.Figure 3.
Precision according to iterationsinduced by KL and S1 similarity measureFigure 4.
Recall according to iterations inducedby KL and S1 similarity measureFigure 5.
F1 according to iterations induced byKL and S1 similarity measureFigure 6.
F1 according to iterations induced byall bigram similarity measureFrom figures (Figure 3-6), we can see thatclustering with our new co-occurrenceprobability based similarity measuresoutperforms that with the widely used relativeentropy based distance measure consistently forboth bigram and trigram contexts.
This confirmsthe effectiveness of our new and simple measure.Regarding the context size, the results fromusing the bigram context outperforms that fromusing the trigram context in precision.
But recalland 1F  drops a lot.
This is due to that largercontexts bring more constraints.
The context sizeeffect holds for both types of similarity measures.And the best performance is achieved with thesimilarity measure 3S .
It is based on 1S  andtakes both left and right contexts into account atthe same time.3.3 Corpus GenerationSince the number of the domain entities(terminals) we can collect from the dialogues isvery limited, we have to expand those variables(non-terminals) in the induced templates withdomain information from the applicationdatabase and relevant web sites.
For example, weused all the words and phrases in the toponymcluster, e.g., ``????
| ???
| ...'', toreplace $toponym in the templates above.
Thenwe generated a large collection of artificial datawhich has a good coverage in both the utterancestructures (the way people speak) and the domainentities.
This resulted in the generated corpus Ain Table 1.
In generation we used the semanticclasses and structures  induced with 3S  andmanually corrected some obvious errors.
In thegenerated data, there are 14,205 utterances and365,576 Chinese characters.
:3.4 Language Model AdaptationThere are some language model adaptation(LMA) work oriented to the dialogue systems e.g.Wang et al2006), Hakkani-T?r et al(2006),Bellegarda(2004).
So far major effort has beenspent on adaptation for large vocabulary speechrecognition or transcription tasks.
But recentlythere have been a few studies that are orientedtoward dialogue systems, e.g.
Wang et al2006),Hakkani-T?r et al(2006).
In our experiments,three trigram language models were built, eachtrained separately on the large text collection (T),on the WOZ data (H) and on the artificiallygenerated data (A).
These trigram models werethen combined through model interpolation asfollows: We used the linear interpolation to adaptlanguage model.
The formula is shown as follows.T is the out-of-domain data, H is thehumane-to-humane dialogues, and A is thecorpus generated by grammars)ww|(wP?+)ww|(wP?+)ww|(wP?=)ww|P(wiiiAAiiiHHiiiTTiii21212121????????
(24)where 1,,0 << AHT ???
and 1=++ AHT ???
.The weights were determined empirically onthe held-out data (C3 in Table 1}).All the language models were built with theStolcke(2002)?s {SRILM} toolkit.Why we did not use the C corpus directly is that itdoes't have a good covering on thedomain-entities and other users usually sayutterances similar to C in structures but differentdomain entities.
So we use the good coveringgenerated data to make LMA.We evaluated the different language modelswith both intrinsic and extrinsic metrics.
Forintrinsic evaluation, we computed the perplexity.For extrinsic evaluation, we ran speechrecognition experiments on the test data C2 andcalculated the character error rate (CER).We can see that corpus A is useful to makemodel adaptation and it is closer to the in-domaindata than the human-human data forhuman-computer dialogues.
By using thesegenerated sentences, our domain-specificChinese speech recognition have a growth from85.2% to 91.4%.AHT?,?,?1,0,00.2,0.8,00.2,0,0.80.2,0.4,0.4PP  984  95.4  33.6  23.3CER(%)  32.3  14.8  10.7  9.0Table 2. perplexity and character error rateaccording to model interpolationThe optimized weights (0.2,0.4,0.4) isobtained from the develop sets C3.
From Table 2,we can see that language models built usingadditional dialogue related data, eitherhuman-human/WOZ  dialogues or datagenerated from human-computer dialogues,shows significant improvement in bothperplexity and speech recognition performanceover the one built with the general text data only.For the two dialogue related data, the generateddata is better than the WOZ data or closer to thetest data, since perplexity further drops from103.5 to 38.1 and CER drops from 14.8 to 10.7.This confirms our conjecture that human-humanWOZ dialogue data is near in-domain and notvery proper for human-computer dialogues.Therefore, to effectively improve languagemodeling for human-computer dialogues, weneed more in-domain data, even if it is generatedor artificial.
The best language model is obtainedthrough interpolation of both language modelsfrom dialogue related data with the one fromgeneral text data.
This may be because there isstill some mismatch between data sets C1 (forinduction and generation) and C2 (for test).And some of the missing bits in C1 appeared inthe WOZ data (corpus A).4 Related WorksThe most relevant work to ours is done by Wanget al (2006), who generated in-domain datathrough out-of-domain data transformation.
Firstsome artificial sentences are generated throughparsing and reconstructing out-of-domain dataand the illegal ones are filtered out.
Then thesynthetic corpus is sampled to achieve a desiredprobability distribution, based on eithersimulated dialogues or semantic informationextracted from development data.
But we used adifferent approach in producing more in-domaindata.
First semantic classes and structures areinduced from limited human-computer dialogues.Then large amount of artificial in-domain corpusis generated with the induced semantic classesand patterns augmented with domain entities.The main difference between the two works liesin how the data is generated and how thegenerated data helped.5 Conclusions and Future WorkIn this paper, we described our work ongenerating in-domain corpus using theauto-induced semantic classes and structures forlanguage model adaptation in a Chinese voicesearch dialogue system.
In inducing semanticclasses we proposed a novel co-occurrenceprobability based similarity measure.
Ourexperiments show that the simple co-occurrenceprobability based similarity measure is effectivefor semantic clustering which is used in ourexperiment.
For interpolation based languagemodel adaptation, the data generated using theinduced semantic classes and structuresenhanced with domain entities helped a lot forhuman-computer dialogues.
Despite that wedealt with the language of Chinese, we believethat that approaches we employed are languageindependent and can be applied to otherlanguages as well.In our experiment we noticed that theperformance of semantic clustering was affectedquite a lot by the noises in the data.
For futurework, we would like to investigate how tofurther improve the robustness of semanticclustering in noisy spoken language.
Thesemantic structures induced above are veryshallow.
We would like to investigate how tofind deep semantics and relations in the data.AcknowledgementThis work is partially supported by The NationalScience & Technology Pillar Program(2008BAI50B03), National Natural ScienceFoundation of China (No.
10925419, 90920302,10874203, 60875014).ReferencesArai, K. J. H., Wright, G. Riccardi, and Gorin, A. L.?Grammar fragment acquisition using syntacticand semantic clustering,?
Speech Communication,vol.
27, iss.
1, pp.
43?62, 1999Bellegarda, J. R. Statistical language modeladaptation: review and perspectives, SpeechCommunication, vol.
42, iss.
1, pp.
93?108, 2004Cover, T. M. and Thomas, J.
A., Elements ofInformation Theory.
Wiley-Interscience, 2006Dagan, I., Lee, L. and Pereira, F. C. N.?Similarity-Based Models of Word CooccurrenceProbabilities,?
Machine Learning, 1999Gorin, A. L. ?On automated language acquisition,?Acoustical Society of America Journal, vol.
97, pp.3441?3461, 1995Hakkani-T?r, D. Z., Riccardi, G. and Tur, G. Anactive approach to spoken language processing,ACM Transactions on Speech and LanguageProcessing (TSLP), vol.
3, iss.
3, pp.
1?31, 2006Lin, D. ?An information-theoretic definition ofsimilarity,?
in Proc.
ICML ?98: Proceedings of theFifteenth International Conference on MachineLearning, 1998Meng, H. M. and Siu, K.-C. ?SemiautomaticAcquisition of Semantic Structures forUnderstanding Domain-Specific Natural LanguageQueries,?
IEEE Trans.
Knowl.
Data Eng.
2002Pargellis, A. N., Fosler-Lussier, E., Fosler-Lussier,Lee, C.-H., Potamianos, A. and Tsai, A.?Auto-induced semantic classes,?
SpeechCommunication, vol.
43, iss.
3, pp.
183?203, 2004Pangos, A Combining statistical similarity measuresfor automatic induction of semantic classes, 2005Pangos, A., Iosif, E. and Tegos, A. Unsupervisedcombination of metrics for semantic classinduction, SLT 2006, 2006Pangos, A. and Iosif, E., A Soft-Clustering Algorithmfor Automatic Induction of Semantic Classes,interspeech07, 2007Stolcke, A. SRILM ?
an extensible languagemodeling toolkit, in Proc.
ICSLP, 2002Wang, C. Chung, G. and Seneff, S. Automaticinduction of language model data for a spokendialogue system, Language Resources andEvaluation, vol.
40, iss.
1, pp.
25?46, 2006Wang, Y.-Y.
and Dong Yu, E. A., An introduction tovoice search, Signal Processing Magazine, IEEE,vol.
25, iss.
3, pp.
28?38, 2008Weeds, J., Weir, D. and McCarthy, D.?Characterising measures of lexical distributionalsimilarity,?
in Proc.
in Proc.
COLING ?04, 2004,
