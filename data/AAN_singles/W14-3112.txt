Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 79?82,Baltimore, Maryland, USA, June 27, 2014.c?2014 Association for Computational LinguisticsConcurrent Visualization of Relationships between Words and Topics inTopic ModelsAlison Smith?, Jason Chuang?, Yuening Hu?, Jordan Boyd-Graber?, Leah Findlater?
?University of Maryland, College Park, MD?University of Washington, Seattle, WAamsmit@cs.umd.edu, jcchuang@cs.washington.edu, ynhu@cs.umd.edu, jbg@umiacs.umd.edu, leahkf@umd.eduAbstractAnalysis tools based on topic models areoften used as a means to explore largeamounts of unstructured data.
Users of-ten reason about the correctness of a modelusing relationships between words withinthe topics or topics within the model.
Wecompute this useful contextual informa-tion as term co-occurrence and topic co-variance and overlay it on top of stan-dard topic model output via an intuitiveinteractive visualization.
This is a workin progress with the end goal to combinethe visual representation with interactionsand online learning, so the users can di-rectly explore (a) why a model may notalign with their intuition and (b) modifythe model as needed.1 IntroductionTopic modeling is a popular technique for analyz-ing large text corpora.
A user is unlikely to havethe time required to understand and exploit the rawresults of topic modeling for analysis of a corpus.Therefore, an interesting and intuitive visualiza-tion is required for a topic model to provide addedvalue.
A common topic modeling technique is La-tent Dirichlet Allocation (LDA) (Blei et al., 2003),which is an unsupervised algorithm for perform-ing statistical topic modeling that uses a ?bag ofwords?
approach.
The resulting topic model repre-sents the corpus as an unrelated set of topics whereeach topic is a probability distribution over words.Experienced users who have worked with a textcorpus for an extended period of time often thinkof the thematic relationships in the corpus in termsof higher-level statistics such as (a) inter-topic cor-relations or (b) word correlations.
However, stan-dard topic models do not explicitly provide suchcontextual information to the users.Existing tools based on topic models, suchas Topical Guide (Gardner et al., 2010), Top-icViz (Eisenstein et al., 2012), and the topic vi-sualization of (Chaney and Blei, 2012) supporttopic-based corpus browsing and understanding.Visualizations of this type typically represent stan-dard topic models as a sea of word clouds; the in-dividual topics within the model are presented asan unordered set of word clouds ?
or somethingsimilar ?
of the top words for the topic1whereword size is proportional to the probability of theword for the topic.
A primary issue with wordclouds is that they can hinder understanding (Har-ris, 2011) due to the fact that they lack informationabout the relationships between words.
Addition-ally, topic model visualizations that display topicsin a random layout can lead to a huge, inefficientlyorganized search space, which is not always help-ful in providing a quick corpus overview or assist-ing the user to diagnose possible problems withthe model.The authors of Correlated Topic Models (CTM)(Lafferty and Blei, 2006) recognize the limitationof existing topic models to directly model the cor-relation between topics, and present an alterna-tive algorithm, CTM, which models the correla-tion between topics discovered for a corpus by us-ing a more flexible distribution for the topic pro-portions in the model.
Topical n-gram models(TNG) (Wang et al., 2007) discover phrases inaddition to topics.
TNG is a probabilistic modelwhich assigns words and n-grams based on sur-rounding context, instead of for all references inthe corpus.
These models independently accountfor the two limitations of statistical topic modelingdiscussed in this paper by modifying the underly-ing topic modeling algorithm.
Our work aims toprovide a low-cost method for incorporating this1This varies, but typically is either the top 10 to 20 wordsor the number of words which hold a specific portion of thedistribution weight.79information as well as visualizing it in an effec-tive way.
We compute summary statistics, termco-occurrence and topic covariance, which can beoverlaid on top of any traditional topic model.
Asa number of application-specific LDA implemen-tations exist, we propose a meta-technique whichcan be applied to any underlying algorithm.We present a relationship-enriched visualiza-tion to help users explore topic models throughword and topic correlations.
We propose inter-actions to support user understanding, validation,and refinement of the models.2 Group-in-a-box Layout for Visualizinga Relationship-Enriched Topic ModelExisting topic model visualizations do not eas-ily support displaying the relationships betweenwords in the topics and topics in the model.
In-stead, this requires a layout that supports intuitivevisualization of nested network graphs.
A group-in-a-box (GIB) layout (Rodrigues et al., 2011) is anetwork graph visualization that is ideal for ourscenario as it is typically used for representingclusters with emphasis on the edges within andbetween clusters.
The GIB layout visualizes sub-graphs within a graph using a Treemap (Shneider-man, 1998) space filling technique and layout al-gorithms for optimizing the layout of sub-graphswithin the space, such that related sub-graphs areplaced together spatially.
Figure 1 shows a samplegroup-in-a-box visualization.We use the GIB layout to visually separate top-ics of the model as groups.
We implement eachtopic as a force-directed network graph (Fruchter-man and Reingold, 1991) where the nodes of thegraph are the top words of the topic.
An edge ex-ists between two words in the network graph ifthe value of the term co-occurrence for the wordpair is above a certain threshold,2and the edge isweighted by this value.
Similarly, the edges be-tween the topic clusters represent the topic covari-ance metric.
Finally, the GIB layout optimizes thevisualization such that related topic clusters areplaced together spatially.
The result is a topic visu-alization where related words are clustered withinthe topics and related topics are clustered withinthe overall layout.2There are a variety of techniques for setting this thresh-old; currently, we aim to display fewer, stronger relationshipsto balance informativeness and complexity of the visualiza-tionFigure 1: A sample GIB layout from (Rodrigueset al., 2011).
The layout visualizes clusters dis-tributed in a treemap structure where the partitionsare based on the size of the clusters.3 Relationship MetricsWe compute the term and topic relationship in-formation required by the GIB layout as termco-occurrence and topic covariance, respectively.Term co-occurrence is a corpus-level statistic thatcan be computed independently from the LDA al-gorithm.
The results of the LDA algorithm are re-quired to compute the topic covariance.3.1 Corpus-Level Term Co-OccurrencePrior work has shown that Pointwise MutualInformation (PMI) is the most consistent scor-ing method for evaluating topic model coher-ence (Newman et al., 2010).
PMI is a statisticaltechnique for measuring the association betweentwo observations.
For our purposes, PMI is usedto measure the correlation between each term pairwithin each topic on the document level3.
ThePMI is calculated for every possible term pair inthe ingested data set using Equation 1.
The visu-alization uses only the PMI for the term pairs forthe top terms for each topic, which is a small sub-set of the calculated PMI values.
Computing thePMI is trivial compared to the LDA calculation,and computing the values for all pairs allows thejob to be run in parallel, as opposed to waiting forthe results of the LDA job to determine the topterm pairs.PMI(x, y) = logp(x, y)p(x)p(y)(1)The PMI measure represents the probability ofobserving x given y and vice-versa.
PMI can be3We use document here, but the PMI can be computed atvarious levels of granularity as required by the analyst intent.80positive or negative, where 0 represents indepen-dence, and PMI is at its maximum when x and yare perfectly associated.3.2 Topic CovarianceTo quantify the relationship between topics in themodel, we calculate the topic covariance metricfor each pair of topics.
To do this, we use thetheta vector from the LDA output.
The theta vec-tor describes which topics are used for which doc-uments in the model, where theta(d,i) representshow much the ith topic is expressed in documentd.
The equations for calculation the topic covari-ance are shown below.
?di=?di?j(?dj)(2)?i=1D?d(?di) (3)?
(i, j) =1D?d(?di?
?i)(?dj?
?j)) (4)4 VisualizationThe visualization represents the individual topicsas network graphs where nodes represent termsand edges represent frequent term co-occurrence,and the layout of the topics represents topic co-variance.
The most connected topic is placed inthe center of the layout, and the least connectedtopics are placed at the corners.
Figure 2 showsthe visualization for a topic model generated fora 1,000 document NSF dataset.
As demonstratedin Figure 3, a user can hover over a topic to seethe related topics4.
In this example, the user hashovered over the {visualization, visual, interac-tive} topic, which is related to {user, interfaces},{human, computer, interaction}, {design, tools},and {digital, data, web} among others.
Unlikeother topical similarity measures, such as cosinesimilarity or a count of shared words, the topic co-variance represents topics which are typically dis-cussed together in the same documents, helpingthe user to discover semantically similar topics.On the topic level, the size of the node in thetopic network graph represents the probability ofthe word given the topic.
By mapping word proba-bility to the area of the nodes instead of the height4we consider topics related if the topic co-occurrence isabove a certain pre-defined threshold.Figure 2: The visualization utilizes a group-in-a-box-inspired layout to represent the topic model asa nested network graph.of words, the resulting visual encoding is not af-fected by the length of the words, a well-knownissue with word cloud presentations that can visu-ally bias longer terms.
Furthermore, circles canoverlap without affecting a user?s ability to visu-ally separate them, and lead to more compact andless cluttered visual layout.
Hovering over a wordnode highlights the same word in other topics asshown in Figure 4.This visualization is an alternative interfacefor Interactive Topic Modeling (ITM) (Hu et al.,2013).
ITM presents users with topics that can bemodified as appropriate.
Our preliminary resultsshow that topics containing highly-weighted sub-clusters may be candidates for splitting, whereaspositively correlated topics are likely to be goodtopics, which do not need to be modified.
In fu-ture work, we intend to perform an evaluation toshow that this visualization enhances quality andefficiency of the ITM process.To support user interactions required by theITM algorithm, the visualization has an edit mode,which is shown in Figure 5.
Ongoing work in-cludes developing appropriate visual operations tosupport the following model-editing operations:1.
Adding words to a topic2.
Removing words from a topic3.
Requiring two words to be linked within atopic (must link)4.
Requiring two words to be forced into sepa-rate topics (cannot link)5 Conclusion and Future WorkThe visualization presented here provides a novelway to explore topic models with incorporated81Figure 3: The user has hovered over the most-central topic in the layout, which is the most con-nected topic.
The hovered topic is outlined, andthe topic name is highlighted in turquoise.
Thetopic names of the related topics are also high-lighted.Figure 4: The visualization where the user hashovered over a word of interest.
The same wordis highlighted turquoise in other topics.Figure 5: The edit mode for the visualization.From this mode, the user can add words, removewords, or rename the topic.term and topic correlation information.
This is awork in progress with the end goal to combine thevisual representation with interactive topic mod-eling to allow users to explore (a) why a modelmay not align with their intuition and (b) modifythe model as needed.
We plan to deploy the toolon real-world domain users to iteratively refine thevisualization and evaluate it in ecologically validsettings.ReferencesDavid M Blei, Andrew Y Ng, and Michael I Jordan.
2003.
Latent dirichletallocation.
Machine Learning Journal, 3:993?1022.Allison June-Barlow Chaney and David M Blei.
2012.
Visualizing topic mod-els.
In ICWSM.Jacob Eisenstein, Duen Horng Chau, Aniket Kittur, and Eric Xing.
2012.
Top-icviz: interactive topic exploration in document collections.
In CHI?12Extended Abstracts, pages 2177?2182.
ACM.Thomas MJ Fruchterman and Edward M Reingold.
1991.
Graph draw-ing by force-directed placement.
Software: Practice and experience,21(11):1129?1164.Matthew J Gardner, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, EricRingger, and Kevin Seppi.
2010.
The topic browser: An interactive toolfor browsing topic models.
In NIPS Workshop on Challenges of Data Vi-sualization.Jacon Harris.
2011.
Word clouds considered harm-ful.
http://www.niemanlab.org/2011/10/word-clouds-considered-harmful/.Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff, and Alison Smith.
2013.Interactive topic modeling.
Machine Learning, pages 1?47.JD Lafferty and MD Blei.
2006.
Correlated topic models.
In NIPS, Proceed-ings of the 2005 conference, pages 147?155.
Citeseer.David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin.
2010.
Au-tomatic evaluation of topic coherence.
In HLT, pages 100?108.
ACL.Eduarda Mendes Rodrigues, Natasa Milic-Frayling, Marc Smith, Ben Shnei-derman, and Derek Hansen.
2011.
Group-in-a-box layout for multi-faceted analysis of communities.
In ICSM, pages 354?361.
IEEE.Ben Shneiderman.
1998.
Treemaps for space-constrained visualization of hi-erarchies.Xuerui Wang, Andrew McCallum, and Xing Wei.
2007.
Topical n-grams:Phrase and topic discovery, with an application to information retrieval.
InICDM, pages 697?702.
IEEE.82
