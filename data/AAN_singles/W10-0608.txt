Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 61?69,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAcquiring Human-like Feature-BasedConceptual Representations from CorporaColin KellyComputer LaboratoryUniversity of CambridgeCambridge, CB3 0FD, UKcolin.kelly@cl.cam.ac.ukBarry DevereuxCentre for Speech,Language, and the BrainUniversity of CambridgeCambridge, CB2 3EB, UKbarry@csl.psychol.cam.ac.ukAnna KorhonenComputer LaboratoryUniversity of CambridgeCambridge, CB3 0FD, UKanna.korhonen@cl.cam.ac.ukAbstractThe automatic acquisition of feature-basedconceptual representations from text corporacan be challenging, given the unconstrainednature of human-generated features.
Weexamine large-scale extraction of concept-relation-feature triples and the utility of syn-tactic, semantic, and encyclopedic informa-tion in guiding this complex task.
Meth-ods traditionally employed do not investi-gate the full range of triples occurring inhuman-generated norms (e.g.
flute producesound), rather targeting concept-feature pairs(e.g.
flute ?
sound) or triples involving specificrelations (e.g.
is-a, part-of ).
We introducea novel method that extracts candidate triples(e.g.
deer have antlers, flute produce sound)from parsed data and re-ranks them using se-mantic information.
We apply this techniqueto Wikipedia and the British National Corpusand assess its accuracy in a variety of ways.Our work demonstrates the utility of externalknowledge in guiding feature extraction, andsuggests a number of avenues for future work.1 IntroductionIn the cognitive sciences, theories about how con-crete concepts such as ELEPHANT are represented inthe mind have often adopted a distributed, feature-based model of conceptual knowledge (e.g.
Ran-dall et al (2004), Tyler et al (2000)).
Accordingto such accounts, conceptual representations consistof patterns of activation over sets of interconnectedsemantic feature nodes (e.g.
has eyes, has ears,is large).
To test these theories empirically, cogni-tive psychologists require an accurate estimate of thekinds of knowledge that people are likely to repre-sent in such a system.
To date, the most importantsources of such knowledge are property-normingstudies, where a large number of participants writedown lists of features for concepts.
For example,McRae et al (2005) collected a set of norms list-ing features for 541 concrete concepts.
In that study,the features listed by different participants were nor-malised by mapping different feature descriptionswith identical meanings to the same feature label.1Table 1 gives the ten most frequent normed featuresfor two concepts in the norms.elephant bananaRelation Feature Relation Featureis large is yellowhas a trunk is a fruitis an animal is edibleis grey is softlives in Africa grows on treeshas ears eaten by peelinghas tusks - growshas legs eaten by monkeyshas four legs is longhas large ears tastes goodTable 1: Sample triples from McRae NormsHowever, property norm data have certain weak-nesses (these have been widely discussed; e.g.
Mur-phy (2002), McRae et al (2005)).
One issue isthat participants tend to under-report features thatare present in many of the concepts in a given cat-egory (McRae et al, 2005; Murphy, 2002).
For ex-ample, for the concept ELEPHANT, participants listsalient features like has trunk, but not less salientfeatures such as breathes air, even though presum-ably all McRae et al?s participants knew that ele-phants breathe air.
Although the largest collection1For example, for CAR, ?used for transportation?
and?people use it for transportation?
were mapped to the sameused for transportation feature.61of norms lists features for over 500 concepts, therelatively small size of property norm sets still givescause for concern.
Larger sets of norms would beuseful to psycholinguists; however, large-scale prop-erty norming studies are time-consuming and costly.In NLP, researchers have developed methods forextracting and classifying generic relationships fromdata, e.g.
Pantel and Pennacchiotti (2008), Davidovand Rappoport (2008a, 2008b).
In recent years,researchers have also begun to develop methodswhich can automatically extract feature norm-likerepresentations from corpora, e.g.
Almuhareb andPoesio (2005), Barbu (2008), Baroni et al (2009).The automatic approach is capable of gatheringlarge-scale distributional data, and furthermore it iscost-effective.
Corpora contain natural-language in-stances of words denoting concepts and their fea-tures, and therefore serve as ideal material for fea-ture generation tasks.
However, current methodsare restricted to specific relations between conceptsand their features, or target concept-feature pairsonly.
For example, Almuhareb and Poesio (2005)proposed a method based on manually developedlexico-syntactic patterns that extracts informationabout attributes and values of concepts.
They usedthese syntactic patterns and two grammatical rela-tions to create descriptions of nouns consisting ofvector entries and evaluated their approach basedon how well their vector descriptions clustered con-cepts.
This method performed well, but targetedis-a and part-of relations only.
Barbu (2008) com-bined manually defined linguistic patterns with a co-occurrence based method to extract features involv-ing six classes of relations.
He then split learningfor the property classes into two distinct paradigms.One used a pattern-based approach (four classes)with a seeded pattern-learning algorithm.
The othermeasured strength of association between the con-cept and referring adjectives and verbs (two classes).His pattern-based approach worked well for proper-ties in the superordinate class, had reasonable recallfor stuff and location classes, but zero recall for partclass.
His approach for the other two classes usedvarious association measures which he summed toestablish an overall score for potential properties.The recent Strudel model (Baroni et al, 2009) re-lies on more general linguistic patterns, ?connectorpatterns?, consisting of sequences of part-of-speech(POS) tags to look for candidate feature terms neara target concept.
The method assumes that ?the va-riety of patterns connecting a concept and a poten-tial property is a good indicator of the presence ofa true semantic link?.
Thus, properties are scoredbased on the count of distinct patterns connectingthem to a concept.
When evaluated against the ESS-LLI dataset (Baroni et al (2008); see section 3.1),Strudel yields a precision of 23.9% ?
this figure isthe best state-of-the-art result for unconstrained ac-quisition of concept-feature pairs.It seems unlikely that further development of theshallow connector patterns will significantly im-prove accuracy, as these already broadly cover mostPOS sequences that are concept-feature connectors.Because of the difficult nature of the task, we believethat extraction of more accurate representations ne-cessitates additional linguistic and world knowl-edge.
Furthermore, the utility of Strudel is limitedbecause it only produces concept-feature pairs, andnot concept-relation-feature triples similar to thosein human generated norms (although the distributionof the connector patterns for a extracted pair does of-fer clues about the broad class of semantic relationthat holds between concept and feature).In this paper, we explore issues of both method-ology and evaluation that arise when attemptingunconstrained, large-scale extraction of concept-relation-feature triples in corpus data.
Extractingsuch human-like features is difficult, and we do notanticipate a high level of accuracy in these early ex-periments.
We examine the utility of three typesof external knowledge in guiding feature extrac-tion: syntactic, semantic and encyclopedic.
Webuild three automatically parsed corpora, two fromWikipedia and one from the British National Cor-pus.
We introduce a method that (i) extracts concept-relation-feature triples from grammatical depen-dency paths produced by a parser and (ii) uses prob-abilistic information about semantic classes of fea-tures and concepts to re-rank the candidate triplesbefore filtering them.
We then assess the accuracyof our model using several different methods, anddemonstrate that external knowledge can help guidethe extraction of human-like features.
Finally, wehighlight issues in both methodology and evaluationthat are important for further progress in this area ofresearch.622 Extraction Method2.1 CorporaWe used Wikipedia to investigate the usefulness ofworld knowledge for our task.
Almost all con-cepts in the McRae norms have their own Wikipediaarticles, and the articles often include facts simi-lar to those elicited in norming studies.2 Extrane-ous data were removed from the articles (e.g.
in-foboxes, bibliographies) to create a plaintext versionof each article.
The 1.84 million articles were thencompiled into two subcorpora.
The first of these(Wiki500) consists of the Wikipedia articles corre-sponding to each of the McRae concepts.
It con-tains c. 500 articles (1.1 million words).
The sec-ond subcorpus is comprised of those articles wherethe title is fewer than five words long and containsone of the McRae concept words.3 This corpus,called Wiki110K, holds 109,648 plaintext articles(36.5 million words).We also employ the 100-million word British Na-tional Corpus (BNC) (Leech et al, 1994) which con-tains written (90%) and spoken (10%) English.
Itwas designed to represent a broad cross-section ofmodern British English.
This corpus provides an in-teresting contrast with Wikipedia, since we assumethat any features contained in such a wide-rangingcorpus would be presented in an incidental fashionrather than explicitly.
The BNC may contain use-ful features which are encoded in everyday speechand text but not in Wikipedia, perhaps due to theirambiguity for encyclopedic purposes, or due to theirnon-scientific but rather common-sense nature.
Forexample, eaten by monkeys is listed as a feature ofBANANA in the McRae norms, but the word monkeydoes not appear in the Wikipedia banana article.2.2 Candidate feature extractionUsing a modified, British English version of thepublished norms, we recoded them to a uniformconcept-relation-feature representation suitable forour experiments ?
it is triples of this form that weaim to extract.
Our method for extracting concept-2e.g.
The article Elephant describes how elephants are large,are mammals, and live in Africa.3This was done in order to avoid articles on very specifictopics which are unlikely to contain basic information about thetarget concept.relation-feature triples consists of two main stages.In the first stage, we extract large sets of candidateconcept-relation-feature triples for each target con-cept from parsed corpus data.
In the second stage,we re-rank and filter these triples with the intentionof retaining only those triples which are likely to betrue semantic features.In the first stage, the corpora are parsed using theRobust Accurate Statistical Parsing (RASP) system(Briscoe et al, 2006).
For each sentence in the cor-pora, this yields the most probable analysis returnedby the parser in the form of a set of grammaticalrelations (GRs).
The GR sets for each sentence con-taining the target concept noun are then retrievedfrom the corpus.
These GRs form an undirectedacyclic graph, whose nodes are labelled with wordsin the sentence and their POS, and whose edges arelabelled with the GR types linking the nodes to-gether.
Using this graph we generate all possiblepaths which are rooted at our target concept nodeusing a breadth-first search.We then examine whether any of these pathsmatch prototypical feature-relation GR structuresaccording to our manually-generated rules.
Therules were created by first extracting features fromthe McRae norms for a small subset of the conceptsand extracting those sentences from the Wiki500corpus which contained both concept and featureterms.
For each sentence, we then examined eachpath through the graph (containing the GRs and POStags) linking the concept, the feature, and all inter-mediate terms, and (providing no other rule alreadygenerated the concept-relation-feature triple) manu-ally generated a rule based on each path.For example, the sentence There are also apronsthat will cover the sleeves should yield the tripleapron cover sleeve.
We examine the tree structureof the sentence rooted at the concept (apron):apron+s:17_NN2cmod-that cover:34_VV0L--- dobj sleeve+s:44_NN2L--- det the:40_ATL--- aux will:29_VMcmod-that cover:34_VV0xcomp be+:8_VBRL--- ncmod also:12_RRL--- ncsubj There:2_EXHere, the relation is relatively simple ?
we merely63create a rule which requires that the relation is a verb(i.e.
has a V POS tag), the feature has an NN tag andthat there is a dobj GR linking the feature to theconcept.
Our rules are effectively a constraint on (a)which paths should be followed through the tree, and(b) which items in that path should be noted in ourconcept-relation-feature triple.
By creating severalsuch rules and applying them to a large number ofsentences, we extract potential features and relationsfor our concepts.We avoided specifying too many POS tags andGRs in rules since this could have resulted in toofew matching paths.
In the above example, we couldhave required also a cmod-that relation linking thefeature and concept ?
but this would have excludedsentences like the apron covered the sleeves.
Con-versely, we avoided making our rules too permis-sive.
For example, eliminating the dobj requirementwould have yielded the triple apron be steel from thesentence the apron hooks were steel.The application of this method to a number ofconcepts in the Wiki500 corpus yielded 15 ruleswhich we employed in our experiments.
We extracttriples using both singular and plural occurrences ofboth the concept term and the feature term.
We showthe first three of our rules in Table 2.
The first stageof our method uses the 15 rules to extract a verylarge number of candidate triples from corpus data.Rule: relation of concept has a VVN tag, featurehas a NN tag and they are linked by an xcompGRS: This is an anchor which relies solely on be-ing a heavy weight.T: anchor be weightRule: relation of concept is a verb, feature is an ad-jective and they are linked by an xcomp GRS: Sliced apples turn brown with exposure toair due to the conversion of natural pheno-lic substances into melanin upon exposure tooxygen.T: apple turn brownRule: feature of concept has a VV0 tag, relation isa verb and they are linked by an aux GRS: Grassy bottoms may be good holding, butonly if the anchor can penetrate the foliage.T: anchor can penetrateTable 2: Three sample rules for a given concept, withexample sentence (S) and corresponding triple (T).2.3 Re-ranking based on semantic informationThe second stage of our method evaluates the qualityof the extracted candidates using semantic informa-tion, with the aim of filtering out the poor qualityfeatures generated in the first stage.
We would ex-pect the number of times a triple is extracted for agiven concept to be proportional to the likelihoodthat the triple represents a true feature of that con-cept.
However, production frequency alone is not asufficient indicator of quality, because concept termscan produce unexpected candidate feature terms.4One may attempt to address this issue by intro-ducing semantic categories.
In other words, theprobability of a feature being part of a concept?srepresentation is dependent on the semantic cate-gory to which the concept belongs (for example,used for-cutting would be expected to have lowprobability for animal concepts).
We analysed thenorms to quantify this type of semantic informationwith the aim of identifying higher-order structure inthe distribution of semantic classes for features andconcepts.
The overarching goal was to determinewhether this information can indeed improve the ac-curacy of feature extraction.In formal terms, we assume that there is a 2-dimensional probability distribution over conceptand feature classes, P(C,F), where C is a conceptclass (e.g.
Apparel) and F is a feature class (e.g.Materials).
Knowing this distribution provides uswith a means of assessing how likely it is that a can-didate feature f is true for a concept c, assuming thatwe know that c ?
C and f ?
F .
The McRae normsmay be considered to be a sample drawn from thisdistribution, if the concept and feature terms appear-ing in the norms can be assigned to suitable conceptand feature classes.
These classes were identifiedby way of clustering.
The reranking step employedthe McRae norms so we could establish an upperbound for the semantic analysis, although we couldalso use other knowledge resources, e.g.
the OpenMind Common Sense database (Singh et al, 2002).2.3.1 ClusteringWe utilised Lin?s similarity measure (1998) forour similarity metric, employing WordNet (Fell-4For example, one of the extracted triples for TIGER is tigerhave squadron because of the RAF squadron called the Tigers.64k-meansbanjo biscuit blackbirdbat cup oxbeehive kettle peacockbirch sailboat prawnbookcase shoe pruneNMFashtray bouquet eelbayonet cabinet grapefruitcape card guppycat cellar moosecatfish chandelier otterHierarchicalFruit/Veg Apparel Instrumentsapple apron accordionavocado armour bagpipesbanana belt banjobeehive blouse celloblueberry boot clarinetTable 3: First five elements alphabetically from threesample clusters for the three clustering methods.baum, 1998) as the basis for calculating similarity.This metric is suitable for our task as we wouldlike to generate appropriate superordinate classes forwhich we can calculate distributional statistics.
Wecould merely cluster on the most frequent sense ofconcept and feature words in WordNet, but the mostfrequent sense in WordNet may not correspond tothe intended sense in our feature norm data.5 So weconsider also other senses of words in WordNet byemploying a manually-annotated list to choose thecorrect sense in WordNet.
This is only possible forconcept clustering since we don?t possess a manualWordNet sense annotation for the 7000 McRae fea-tures; for the feature clustering, we simply use themost frequent sense in WordNet.The concepts and feature-head terms appearingin the recoded norms were each clustered indepen-dently into 50 clusters using three methods: hi-erarchical clustering, k-means clustering and non-negative matrix factorization (NMF).
We show thefirst five alphabetical elements from three of theclusters produced by our clustering methods in Table3.
The hierarchical clustering seems to be producing5e.g.
the first and second most frequent definitions of kiterefer to a slang meaning for the word cheque ?
only the thirdmost frequent meaning refers to kite as a toy, which most peoplewould understand to be its predominant sense.Hierarchical ClusteringPlant Parts Materials Activitiesberry cotton annoyingbush fibre listeningcore nylon musicplant silk showingseed spandex lookingTable 4: Example members of feature clusters for hierar-chical clustering.Fruit/Veg Apparel InstrumentsPlant Parts 0.144 0.037 0.008Materials 0.006 0.148 0.008Activities 0.009 0.074 0.161Table 5: P(F |C) for C ?
{Fruit/Veg, Apparel, Instru-ments} and F ?
{Plant Parts, Materials, Activities}the most intuitive clusters.We calculated the conditional probability P(F |C)of a feature cluster given a concept cluster using thedata in the McRae norms.
Table 5 gives the condi-tional probability for each of the three feature clus-ters given each of the three concept clusters thatwere presented in Tables 3 and 4 for hierarchicalclustering.
For example, P(Materials|Apparel) ishigher than P(Materials|Fruit/Veg): given a conceptin the Apparel cluster the probability of a Materialsfeature is relatively high whereas given a concept inthe Fruit/Veg cluster the probability of a Materialsfeature is low.
The cluster analysis therefore sup-ports our hypothesis that the likelihood of a partic-ular feature for a particular concept is dependent onthe semantic categories that both belong to.2.3.2 RerankingWe investigated whether this distributional semanticinformation could be used to improve the quality ofthe candidate triples, by using the conditional prob-abilities of the appropriate feature cluster given theconcept cluster as a weighting factor.
To obtain theprobabilities for a triple, we first find the clusters thatthe concept and feature-head words belong to.
If thefeature-head word of the extracted triple appears inthe norms, its cluster membership is drawn directlyfrom there; if not, we assign the feature-head to thefeature cluster with which it has the highest averagesimilarity.6 Having determined the concept and fea-6We use average-linkage for hiearchical and k-means clus-tering, and mean cosine similarity for NMF.65ture clusters for the triple, we reweight its raw cor-pus occurrence frequency by multiplying it by theconditional probability.
In this way, incorrect triplesthat occur frequently in the data are downgraded andmore plausible triples have their ranking boosted.2.3.3 Baseline modelWe also implemented as a baseline a co-occurrence-based model, based on the ?SVD?
model de-scribed by Baroni and colleagues (Baroni and Lenci,2008; Baroni et al, 2009) ?
it is a simple, word-association method, not tailored to extracting fea-tures.
A context-word-by-target-word frequency co-occurrence matrix was constructed for both corpora,with a sentence-sized window.
Context words andtarget words were defined to be the 5,000 and 10,000most frequent content words in the corpus respec-tively.
The target words were supplemented withthe concept words from the recoded norms.
Theco-occurrence matrix was reduced to 150 dimen-sions by singular value decomposition, and cosinesimilarity between pairs of target words was calcu-lated.
The 200 most similar target words to eachconcept acted as the feature-head terms extracted bythis model.3 Experimental Evaluation3.1 Methods of EvaluationWe considered a number of methods for evaluatingthe quality of the extracted feature triples.
One pos-sibility would be to calculate precision and recallfor the extracted triples with respect to the McRaenorms ?gold standard?.
However, direct comparisonwith the recoded norms is problematic, since theremay be extracted features which are semanticallyequivalent to a triple in the norms but possessing adifferent lexical form.7Since semantically identical features can be lex-ically different, we followed the approach taken inthe ESSLLI 2008 Workshop on semantic models(Baroni et al, 2008).
The gold standard for the ESS-LLI task was the top 10 features for 44 of the McRaeconcepts.
For each concept-feature pair an expan-sion set was generated containing synonyms of the7For example, avocado have stone appears in the recodednorms whilst avocado contain pit is extracted by our method;direct comparison of these two triples results in avocado con-tain pit being incorrectly marked as an error.feature terms appearing in the norms.
For example,the feature lives on water was expanded to the set{aquatic, lake, ocean, river, sea, water}.We would expect to find in corpus data correctfeatures that do not appear in our ?gold standard?(e.g.
breathes air is listed for WHALE but for noother animal).
We therefore aim to attain high re-call when evaluating against the ESSLLI set (sinceideally all features in the norms should be extracted)but we are somewhat less concerned about achievinghigh precision (since extracted features that are notin the norms may still be correct, e.g.
breathes airfor TIGER).
To evaluate the ability of our modelto generate such novel features, we also conducteda manual evaluation of the highest-ranked extractedfeatures that did not appear in the norms.Extraction set Corpus Prec.
RecallSVD BaselineWiki500 0.0235 0.4712Wiki110K 0.0140 0.2798BNC 0.0131 0.2621Method -unfilteredWiki500 0.0242 0.6515Wiki110K 0.0039 0.8944BNC 0.0042 0.8813Method - top 20(unweighted)Wiki500 0.1159 0.2326Wiki110K 0.0761 0.1523BNC 0.0841 0.1692Method - top 20(hierarchicalclustering)Wiki500 0.1693 0.3394Wiki110K 0.1733 0.3553BNC 0.1943 0.3896Method - top 20(k-meansclustering)Wiki500 0.1159 0.2323Wiki110K 0.1000 0.2008BNC 0.1216 0.2442Method - top 20(NMFclustering)Wiki500 0.1375 0.2755Wiki110K 0.1409 0.2826BNC 0.1500 0.3010Table 6: Results when matching on features only.3.2 EvaluationPrevious large-scale models of feature extractionhave been evaluated on pairs rather than triples e.g.Baroni et al (2009).
Table 6 presents the resultsof our method when we evaluate using the feature-head term alone (i.e.
in calculating precision and re-call we disregard the relation verb and require onlya match between the feature-head terms in the ex-tracted triples and the recoded norms).
Results forsix sets of extractions are presented.
The first setis the set of features extracted by the SVD baseline.66The second set of extracted triples consists of thefull set of triples extracted by our method, prior tothe reweighting stage.
?Top 20 unweighted?
givesthe results when all but the top 20 most frequentlyextracted triples for each concept are filtered out.Note that the filtering criteria here is raw extractionfrequency, without reweighting by conditional prob-abilities.
?Top 20 (clustering type)?
are the corre-sponding results when the features are weighted bythe conditional probability factors (derived from ourthree clustering methods) prior to filtering; that is,using the top 20 reranked features.
The effective-ness of using the semantic class-based analysis datain our method can be assessed by comparing the fil-tered results with and without feature weighting.For the baseline implementation, the results arebetter when we use the smaller Wiki500 corpuscompared to the larger Wiki110K corpus.
This isnot surprising, since the smaller corpus containsonly those articles which correspond to the conceptsfound in the norms.
This smaller corpus thus min-imises noise due to phenomena such as word poly-semy which are more apparent in the larger corpus.The results for the baseline model and the unfil-tered method are quite similar for the Wiki500 cor-pus, whilst the results for the unfiltered method us-ing the Wiki110K corpus give the maximum recallachieved by our method; 89.4% of the features areextracted, although this figure is closely followed bythat of the BNC at 88.1%.
As the unfiltered methodis deliberately greedy, a large number of features arebeing extracted and therefore precision is low.Extraction set Corpus Prec.
RecallMethod - top 20(hierarchicalclustering)Wiki500 0.1011 0.2028Wiki110K 0.1102 0.2210BNC 0.0955 0.1917Table 7: Results for our best method when matching onfeatures and relations.For the results of the filtered method, where allbut the top 20 of features were discarded, we see thebenefit of reranking, with the reranked frequenciesfor all three clustering types yielding much higherprecision and recall scores than the unweightedmethod.
Our best performance is achieved using theBNC and hierarchical clustering, where we obtain19.4% precision and 38.9% recall.
Thus both gen-eral and encyclopedic corpus data prove useful forthe task.
An interesting question is whether thesetwo data types offer different, complementary fea-ture types for the task.
We discuss this point furtherin section 3.3.Using exactly the same gold standard, Baroni etal.
(2009) obtained precision of 23.9%.
However,this result is not directly comparable with ours, sincewe define precision over the whole set of extractedfeatures while Baroni et al considered the top 10extracted features only.The innovation of our method is that it uses infor-mation about the GR-graph of the sentence to alsoextract the relation which appears in the path link-ing the concept and feature terms in the sentence,which is not possible in a purely co-occurrence-based model.
We therefore also evaluated the ex-tracted triples using the full relation + feature-headpair (i.e.
both the feature and the relation verb haveto be correct).
The results for our best method areshown in Table 7.
Unsurprisingly, because this taskis more difficult, precision and recall are reduced.However, since we enforce no constraints on whatthe relation may be and since we do not have ex-panded synonym sets for our relations (as we do forour features) it is actually impressive to have boththe exact relation verb and feature matching with therecoded norms almost one in every five times.
To ourknowledge, our work is the first to try to compare ex-tracted features to the full relation and feature normparts of the triple.3.3 Qualitative analysisSince a key aim of our work is to learn novel featuresin corpus data, we also performed a qualitative eval-uation of the extracted features and relations.
Thisanalysis revealed that many of the errors were nottrue errors but potentially valid triples missing fromthe gold standard.
Table 8 shows the top 10 featuresfor two concepts extracted by our best method fromthe Wiki500 corpus and the BNC corpus.
We la-bel those features that are correct according to thenorms as Correct (C), those which do not appear inour norms but we believe to be plausible as Plausi-ble (P), and those that do not appear in the normsand are also implausible as Incorrect (I).
We can seethat our method has detected several plausible fea-tures not appearing in the norms (and thus our goldstandard), e.g.
swan have chick and screwdriver be67swanWiki500 BNCbe bird C have number Ibe black P have water Chave chick P have lake Chave plumage C be bird Chave feather C be white Crestrict water C have neck Cbe mute P be wild Peat grass P have duck Iturn elisa I have song Ihave neck C have pair IscrewdriverWiki500 BNCuse handle C have tool Chave blade P have end Puse tool C have blade Premedy problem P have hand Ihave size P be sharp Phave head C have bit Protate end P have arm Ihave plastic P be large Pachieve goal I be sonic Phave hand I have range PTable 8: Top 10 returned features and relations for swanand screwdriver.sharp.
Indeed, it could be argued that some ?incor-rect?
features (e.g.
screwdriver achieve goal) couldbe considered to be at least broadly accurate.
Werecognise that the ideal evaluation for our methodwould involve having human participants assess theextracted features for a diverse cross-section of ourconcepts, but this is beyond the scope of this paper.When considering the top 20 features extractedusing our best method applied to the Wiki500 cor-pus versus the BNC corpus, the overlap of featuresis relatively low at 22.73%.
When one also takes theextracted relations into account, this figure descendsto 6.45%.
It is clear that relatively distinct groups offeatures are being extracted from the encyclopedicand general corpus data.
Future work could investi-gate combining these for improved performance e.g.using the intersection of the best features from theBNC and Wiki110k corpora to improve precisionand the union to improve recall.4 DiscussionThis paper examined large-scale, unconstrained ac-quisition of human-like feature norms from corpusdata.
Our work was not limited to only a subsetof concepts, relation types or concept-feature pairs.Rather, we investigated concepts, features and rela-tions in conjunction, and extracted property norm-like concept-relation-feature triples.Our investigation shows that external knowledgeis highly useful in guiding this challenging task.
En-cyclopedic information proved useful for feature ex-traction: although our Wikipedia corpora are consid-erably smaller than the BNC, they performed almostequally well.
We also demonstrated the benefits ofemploying syntactic information in feature extrac-tion: our base extraction method operating on parseddata outperforms the co-occurrence-based baselineand permits us to extract relation verbs.
This un-derscores the usefulness of parsing for semanticallymeaningful feature extraction.
This is consistentwith recent work in the field of computational lex-ical semantics, although GR data has not previouslybeen successfully applied to feature extraction.We showed that semantic information about co-occurring concept and feature clusters can be usedto enhance feature acquisition.
We employed theMcRae norms for our analysis, however we couldalso employ other knowledge resources and clusterrelation verbs using recent methods, e.g.
Sun andKorhonen (2009), Vlachos et al (2009).Our paper has also investigated methods of eval-uation, which is a critical but difficult issue for fea-ture extraction.
Most recent approaches have beenevaluated against the ESSLLI sub-set of the McRaenorms which expands the set of features in the normswith their synonyms.
Yet even expansion sets likethe ESSLLI norms do not facilitate adequate eval-uation because they are not complete in the sensethat there are true features which are not includedin the norms.
Our qualitative analysis shows thatmany of the errors against the recoded norms arein fact correct or plausible features.
Future workcan aim for larger-scale qualitative evaluation usingmultiple judges as well as investigating other task-based evaluations.
For example, we have demon-strated that our automatically-acquired feature rep-resentations can make predictions about fMRI activ-ity associated with concept stimuli that are as pow-erful as those produced by a manually-selected setof features (Devereux et al, 2010).68AcknowledgmentsThis research was supported by EPSRC grantEP/F030061/1 and the Royal Society UniversityResearch Fellowship, UK.
We are grateful to McRae andcolleagues for making their norms publicly available,and to the anonymous reviewers for their input.ReferencesAbdulrahman Almuhareb and Massimo Poesio.
2005.Concept learning and categorization from the web.
InProceedings of the 27th Annual Meeting of the Cogni-tive Science Society, pages 103?108.Eduard Barbu.
2008.
Combining methods to learnfeature-norm-like concept descriptions.
In Proceed-ings of the ESSLLI Workshop on Distributional LexicalSemantics, pages 9?16.Marco Baroni and Alessandro Lenci.
2008.
Conceptsand properties in word spaces.
Italian Journal of Lin-guistics, 20(1):55?88.Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-tors.
2008.
ESSLLI 2008 Workshop on DistributionalLexical Semantics.Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-simo Poesio.
2009.
Strudel: A corpus-based semanticmodel based on properties and types.
Cognitive Sci-ence, pages 1?33.Edward J. Briscoe, John Carroll, and Rebecca Wat-son.
2006.
The second release of the RASP sys-tem.
In Proceedings of the Interactive Demo Sessionof COLING/ACL-06, pages 77?80.D.
Davidov and A. Rappoport.
2008a.
Classification ofsemantic relationships between nominals using patternclusters.
ACL.08.D.
Davidov and A. Rappoport.
2008b.
Unsuperviseddiscovery of generic relationships using pattern clus-ters and its evaluation by automatically generated SATanalogy questions.
ACL.08.Barry Devereux, Colin Kelly, and Anna Korhonen.
2010.Using fmri activation to conceptual stimuli to evalu-ate methods for extracting conceptual representationsfrom corpora.
In Proceedings of the NAACL-HLTWorkshop on Computational Neurolinguistics.Christiane Fellbaum, editor.
1998.
WordNet: An elec-tronic lexical database.
MIT Press.G.
Leech, R. Garside, and M. Bryant.
1994.
CLAWS4:the tagging of the British National Corpus.
In Pro-ceedings of the 15th conference on Computationallinguistics-Volume 1, pages 622?628.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proceedings of ICML?98, pages 296?304.Ken McRae, George S. Cree, Mark S. Seidenberg, andChris McNorgan.
2005.
Semantic feature productionnorms for a large set of living and nonliving things.Behavior Research Methods, 37:547?559.Gregory Murphy.
2002.
The big book of concepts.
TheMIT Press, Cambridge, MA.Patrick Pantel and Marco Pennacchiotti.
2008.
Automat-ically harvesting and ontologizing semantic relations.In Paul Buitelaar and Philipp Cimiano, editors, Ontol-ogy learning and population.
IOS press.Billi Randall, Helen E. Moss, Jennifer M. Rodd, MikeGreer, and Lorraine K. Tyler.
2004.
Distinctive-ness and correlation in conceptual structure: Behav-ioral and computational studies.
Journal of Experi-mental Psychology: Learning, Memory & Cognition,30(2):393?406.P.
Singh, T. Lin, E. Mueller, G. Lim, T. Perkins,and W. Li Zhu.
2002.
Open Mind CommonSense: Knowledge acquisition from the general pub-lic.
On the Move to Meaningful Internet Systems 2002:CoopIS, DOA, and ODBASE, pages 1223?1237.Lin Sun and Anna Korhonen.
2009.
Improving VerbClustering with Automatically Acquired SelectionalPreferences.
Empirical Methods on Natural LanguageProcessing.L.
K. Tyler, H. E. Moss, M. R. Durrant-Peatfield, and J. P.Levy.
2000.
Conceptual structure and the structure ofconcepts: A distributed account of category-specificdeficits.
Brain and Language, 75(2):195?231.Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-mani.
2009.
Unsupervised and constrained dirichletprocess mixture models for verb clustering.
In Pro-ceedings of the Workshop on Geometrical Models ofNatural Language Semantics, pages 74?82, Athens,Greece.69
