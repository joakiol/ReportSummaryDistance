Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 69?77,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsThe CW Corpus: A New Resource for Evaluatingthe Identification of Complex WordsMatthew ShardlowText Mining Research GroupSchool of Computer Science, University of ManchesterIT301, Kilburn Building, Manchester, M13 9PL, Englandm.shardlow@cs.man.ac.ukAbstractThe task of identifying complex words(CWs) is important for lexical simpli-fication, however it is often carried outwith no evaluation of success.
There isno basis for comparison of current tech-niques and, prior to this work, therehas been no standard corpus or eval-uation technique for the CW identi-fication task.
This paper addressesthese shortcomings with a new cor-pus for evaluating a system?s perfor-mance in identifying CWs.
SimpleWikipedia edit histories were mined forinstances of single word lexical simpli-fications.
The corpus contains 731 sen-tences, each with one annotated CW.This paper describes the method usedto produce the CW corpus and presentsthe results of evaluation, showing itsvalidity.1 IntroductionCW identification techniques are typically im-plemented as a preliminary step in a lexicalsimplification system.
The evaluation of theidentification of CWs is an often forgottentask.
Omitting this can cause a loss of accu-racy at this stage which will adversely affectthe following processes and hence the user?sunderstanding of the resulting text.Previous approaches to the CW identifica-tion task (see Section 5) have generally omit-ted an evaluation of their method.
This gapin the literature highlights the need for evalu-ation, for which gold standard data is needed.This research proposes the CW corpus, adataset of 731 examples of sentences with ex-actly one annotated CW per sentence.A CW is defined as one which causes a sen-tence to be more difficult for a user to read.For example, in the following sentence:?The cat reposed on the mat?The presence of the word ?reposed?
would re-duce the understandability for some readers.It would be difficult for some readers to workout the sentence?s meaning, and if the readeris unfamiliar with the word ?reposed?, they willhave to infer its meaning from the surroundingcontext.
Replacing this word with a more fa-miliar alternative, such as ?sat?, improves theunderstandability of the sentence, whilst re-taining the majority of the original semantics.Retention of meaning is an important fac-tor during lexical simplification.
If the word?reposed?
is changed to ?sat?, then the specificmeaning of the sentence will be modified (gen-erally speaking, reposed may indicate a stateof relaxation, whereas sat indicates a body po-sition) although the broad meaning is still thesame (a cat is on a mat in both scenarios).
Se-mantic shift should be kept to a minimum dur-ing lexical simplification.
Recent work (Biranet al 2011; Bott et al 2012) has employeddistributional semantics to ensure simplifica-tions are of sufficient semantic similarity.Word complexity is affected by many fac-tors such as familiarity, context, morphologyand length.
Furthermore, these factors changefrom person to person and context to context.The same word, in a different sentence, may beperceived as being of a different level of diffi-culty.
The same word in the same sentence,but read by a different person, may also beperceived as different in difficulty.
For exam-ple, a person who speaks English as a secondlanguage will struggle with unfamiliar wordsdepending on their native tongue.
Conversely,the reader who has a low reading ability willstruggle with long and obscure words.
Whilstthere will be some crossover in the language69these two groups find difficult, this will not beexactly the same.
This subjectivity makes theautomation and evaluation of CW identifica-tion difficult.Subjectivity makes the task of natural lan-guage generation difficult and rules out auto-matically generating annotated complex sen-tences.
Instead, our CW discovery process(presented in Section 2) mines simplificationsfrom Simple Wikipedia1 edit histories.
Sim-ple Wikipedia is well suited to this task as itis a website where language is collaborativelyand iteratively simplified by a team of editors.These editors follow a set of strict guidelinesand accountability is enforced by the self polic-ing community.
Simple Wikipedia is aimedat readers with a low English reading abilitysuch as children or people with English as asecond language.
The type of simplificationsfound in Wikipedia and thus mined for use inour corpus are therefore appropriate for peo-ple with low English proficiency.
By capturingthese simplifications, we produce a set of gen-uine examples of sentences which can be usedto evaluate the performance of CW identifi-cation systems.
It should be noted that al-though these simplifications are best suited tolow English proficiency users, the CW identifi-cation techniques that will be evaluated usingthe corpus can be trained and applied for avariety of user groups.The contributions of this paper are as fol-lows:?
A description of the method used to cre-ate the CW corpus.
Section 2.?
An analysis of the corpus combining re-sults from 6 human annotators.
Section3.?
A discussion on the practicalities sur-rounding the use of the CW corpus forthe evaluation of a CW identification sys-tem.
Section 4.Related and future work are also presented inSections 5 and 6 respectively.2 DesignOur corpus contains examples of simplifica-tions which have been made by human editors1http://simple.wikipedia.org/System ScoreSUBTLEX 0.3352Wikipedia Baseline 0.3270Kuc?era-Francis 0.3097Random Baseline 0.0157Table 1: The results of different experi-ments on the SemEval lexical simplifica-tion data (de Belder and Moens, 2012),showing the SUBTLEX data?s superiorperformance over several baselines.
Eachbaseline gave a familiarity value to a setof words based on their frequency of oc-currence.
These values were used to pro-duce a ranking over the data which wascompared with a gold standard rankingusing kappa agreement to give the scoresshown here.
A baseline using the GoogleWeb 1T dataset was shown to give ahigher score than SUBTLEX, howeverthis dataset was not available during thecourse of this research.during their revisions of Simple Wikipedia ar-ticles.
These are in the form of sentences withone word which has been identified as requir-ing simplification.2 These examples can beused to evaluate the output of a CW identi-fication system (see Section 6).
To make thediscovery and evaluation task easier, we limitthe discovered simplifications to one word persentence.
So, if an edited sentence differs fromits original by more than one word, we do notinclude it in our corpus.
This also promotesuniformity in the corpus, reducing the com-plexity of the evaluation task.2.1 PreliminariesSUBTLEXThe SUBTLEX dataset (Brysbaert and New,2009) is used as a familiarity dictionary.
Itsprimary function is to associate words withtheir frequencies of occurrence, assuming thatwords which occur more frequently are sim-pler.
SUBTLEX is also used as a dictionaryfor testing word existence: if a word does notoccur in the dataset, it is not considered forsimplification.
This may occur in the case ofvery infrequent words or proper nouns.
The2We also record the simplification suggested by theoriginal Simple Wikipedia editor.70SUBTLEX data is chosen over the more con-ventional Kuc?era-Francis frequency (Kuc?eraand Francis, 1967) and over a baseline pro-duced from Wikipedia frequencies due to aprevious experiment using a lexical simplifica-tion dataset from task 1 of SemEval 2012 (deBelder and Moens, 2012).
See Table 1.Word SenseHomonymy is the phenomenon of a wordformhaving 2 distinct meanings as in the clas-sic case: ?Bank of England?
vs. ?River bank ?.In each case, the word bank is referring toa different semantic entity.
This presents aproblem when calculating word frequency asthe frequencies for homonyms will be com-bined.
Word sense disambiguation is an un-solved problem and was not addressed whilstcreating the CW corpus.
The role of wordsense in lexical simplification will be investi-gated at a later stage of this research.Yatskar et al(2010)The CW corpus was built following the workof Yatskar et al(2010) in identifying para-phrases from Simple Wikipedia edit histo-ries.
Their method extracts lexical edits fromaligned sentences in adjacent revisions of aSimple Wikipedia article.
These lexical editsare then processed to determine their likeli-hood of being a true simplification.
Two meth-ods for determining this probability are pre-sented, the first uses conditional probabilityto determine whether a lexical edit representsa simplification and the second uses metadatafrom comments to generate a set of trustedrevisions, from which simplifications can bedetected using pointwise mutual information.Our method (further explained in Section 2.2)differs from their work in several ways.
Firstly,we seek to discover only single word lexical ed-its.
Secondly, we use both article metadataand a series of strict checks against a lexicon,a thesaurus and a simplification dictionary toensure that the extracted lexical edits are truesimplifications.
Thirdly, we retain the originalcontext of the simplification as lexical com-plexity is thought to be influenced by context(Biran et al 2011; Bott et al 2012).Automatically mining edit histories waschosen as it provides many instances quicklyand at a low cost.
The other method of cre-ating a similar corpus would have been toask several professionally trained annotatorsto produce hundreds of sets of sentences, andto mark up the CWs in these.
The use ofprofessionals would be expensive and annota-tors may not agree on the way in which wordsshould be simplified, leading to further prob-lems when combining annotations.2.2 MethodIn this section, we explain the procedure tocreate the corpus.
There are many process-ing stages as represented graphically in Figure1.
The stages in the diagram are further de-scribed in the sections below.
For simplicity,we view Simple Wikipedia as a set of pagesP, each with an associated set of revisions R.Every revision of every page is processed iter-atively until P is exhausted.Content ArticlesThe Simple Wikipedia edit histories were ob-tained.3 The entire database was very large,so only main content articles were considered.All user, talk and meta articles were discarded.Non-content articles are not intended to beread by typical users and so may not reflectthe same level of simplicity as the rest of thesite.Revisions which SimplifyWhen editing a Simple Wikipedia article, theauthor has the option to attach a comment totheir revision.
Following the work of Yatskaret al(2010), we only consider those revisionswhich have a comment containing some mor-phological equivalent of the lemma ?simple?,e.g.
simplify, simplifies, simplification, simpler,etc.
This allows us to search for commentswhere the author states that they are simpli-fying the article.Tf-idf MatrixEach revision is a set of sentences.
As changesfrom revision to revision are often small, therewill be many sentences which are the same inadjacent revisions.
Sentences which are likelyto contain a simplification will only have oneword difference and sentences which are un-related will have many different words.
Tf-idf(Salton and Yang, 1973) vectors are calculated3Database dump dated 4th February 2012.71Simple Wikipedia EditHistoriesFor every relevant pair of revisions ri and ri+1For every pageCalculate tf-idf matrix for sentences in ri and ri+1Threshold matrix to give likely candidatesSentence Pairs in the form <A,B> Where Ais a sentence from ri and B is from ri+1For every sentence pairSet of Pages P = p1, ?, piwhere each pi is the set ofrevisions R = r1, ?, ri andeach ri  is the set ofsentences S = s1, ?, si.Calculate Hamming distance between A and B,check it is equal to 1Extract the edited Words: ?
from A and ?
from BCheck ?
and ?
are real wordsCheck ?
is simpler than ?Stem ?
and ?, checking the stems are not equalIf all conditionsare metStore pair <A,B> in CW CorpusProcess next pairFalseTrueVerify CandidatesCW corpusCheck ?
and ?
are synonymousExtract Likely CandidatesFigure 1: A flow chart showing the process undertaken to extract lexical simplifications.Each part of this process is further explained in Section 2.2.
Every pair of revisionsfrom every relevant page is processed, although the appropriate recursion is omittedfrom the flow chart for simplicity.72for each sentence and the matrix containingthe dot product of every pair of sentence vec-tors from the first and second revision is cal-culated.
This allows us to easily see those vec-tors which are exactly the same ?
as thesewill have a score of one.4 It also allows us toeasily see which vectors are so different thatthey could not contain a one word edit.
Weempirically set a threshold at 0.9 <= X < 1to capture those sentences which were highlyrelated, but not exactly the same.Candidate PairsThe above process resulted in pairs of sen-tences which were very similar according tothe tf-idf metric.
These pairs were then sub-jected to a series of checks as detailed below.These were designed to ensure that as few falsepositives as possible would make it to the cor-pus.
This may have meant discarding sometrue positives too, however the cautious ap-proach was adopted to ensure a higher corpusaccuracy.Hamming DistanceWe are only interested in those sentences witha difference of one word, because sentenceswith more than one word difference may con-tain several simplifications or may be a re-wording.
It is more difficult to distinguishwhether these are true simplifications.
Wecalculate the Hamming distance between sen-tences (using wordforms as base units) to en-sure that only one word differs.
Any sentencepairs which do not have a Hamming distanceof 1 are discarded.Reality CheckThe first check is to ensure that both the wordsare a part of our lexicon, ensuring that thereis SUBTLEX frequency data for these wordsand also that they are valid words.
This stagemay involve removing some valid words, whichare not found in the lexicon, however this ispreferable to allowing words that are the resultof spam or vandalism.4As tf-idf treats a sentence as a bag of words it ispossible for two sentences to give a score of 1 if theycontain the same words, but in a different order.
Thisis not a problem as if the sentence order is different,there is a minimum of 2 lexical edits ?
meaning westill wish to discount this pair.Inequality CheckIt is possible that although a different wordis present, it is a morphological variant ofthe original word rather than a simplification.E.g., due to a change in tense, or a correc-tion.
To identify this, we stem both wordsand compare them to make sure they are notthe same.
If the word stems are equal thenthey are unlikely to be a simplification, so thispair is discarded.
Some valid simplificationsmay also be removed at this point, howeverthese are difficult to distinguish from the non-simplifications.Synonymy CheckTypically, lexical simplification involves the se-lection of a word?s synonym.
WordNet (Fell-baum, 1998) is used as a thesaurus to check ifthe second word is listed as a synonym of thefirst.
As previously discussed (Section 2.1),we do not take word sense into account at thispoint.
Some valid simplifications may not beidentified as synonyms in WordNet, howeverwe choose to take this risk ?
discarding allnon-synonym pairs.
Improving thesaurus cov-erage for complex words is left to future work.Stemming is favoured over lemmatisationfor two reasons.
Firstly, because lemmatisa-tion requires a lot of processing power andwould have terminally slowed the process-ing of the large revision histories.
Secondly,stemming is a dictionary-independent tech-nique, meaning it can handle any unknownwords.
Lemmatisation requires a large dic-tionary, which may not contain the rare CWswhich are identified.Simplicity CheckFinally, we check that the second word is sim-pler than the first using the SUBTLEX fre-quencies.
All these checks result in a pair ofsentences, with one word difference.
The dif-fering words are synonyms and the change hasbeen to a word which is simpler than the origi-nal.
Given these conditions have been met, westore the pair in our CW Corpus as an exampleof a lexical simplification.2.3 ExamplesThis process was used to mine the followingtwo examples:73Complex word: functions.Simple word: uses.A dictionary has been designed to have oneor more that can help the user in aparticular situation.Complex word: difficultSimple word: hardReadability tests give a prediction as to howreaders will find a particular text.3 Corpus Analysis3.1 Experimental DesignTo determine the validity of the CW corpus, aset of six mutually exclusive 50-instance ran-dom samples from the corpus were turned intoquestionnaires.
One was given to each of 6volunteer annotators who were asked to deter-mine, for each sentence, whether it was a trueexample of a simplification or not.
If so, theymarked the example as correct.
This binarychoice was employed to simplify the task forthe annotators.
A mixture of native and non-native English speakers was used, although nomarked difference was observed between thesegroups.
All the annotators are proficient inEnglish and currently engaged in further orhigher education.
In total, 300 instances oflexical simplification were evaluated, coveringover 40% of the CW corpus.A 20 instance sample was also created asa validation set.
The same 20 instanceswere randomly interspersed among each of the6 datasets and used to calculate the inter-annotator agreement.
The validation dataconsisted of 10 examples from the CW cor-pus and 10 examples that were filtered outduring the earlier stages of processing.
Thisprovided sufficient positive and negative datato show the annotator?s understanding of thetask.
These examples were hand picked to rep-resent positive and negative data and are usedas a gold standard.Agreement with the gold standard is cal-culated using Cohen?s kappa (Cohen, 1968).Inter-annotator agreement is calculated usingFleiss?
kappa (Fleiss, 1971), as in the evalua-tion of a similar task presented in de Belderand Moens (2012).
In total, each annotatorwas presented with 70 examples and asked toAnnotationIndexCohen?sKappaSampleAccuracy1 1 98%2 1 96%3 0.4 70%4 1 100%5 0.6 84%6 1 96%Table 2: The results of different annota-tions.
The kappa score is given againstthe gold standard set of 20 instances.
Thesample accuracy is the percentage of the50 instances seen by that annotator whichwere judged to be true examples of a lex-ical simplification.
Note that kappa isstrongly correlated with accuracy (Pear-son?s correlation: r = 0.980)label these.
A small sample size was used toreduce the effects of annotator fatigue.3.2 ResultsOf the six annotations, four show the exactsame results on the validation set.
These fouridentify each of the 10 examples from the CWcorpus as a valid simplification and each of the10 examples that were filtered out as an invalidsimplification.
This is expected as these twosets of data were selected as examples of posi-tive and negative data respectively.
The agree-ment of these four annotators further corrob-orates the validity of the gold standard.
An-notator agreement is shown in Table 2.The 2 other annotators did not stronglyagree on the validation sets.
Calculating Co-hen?s kappa between each of these annotatorsand the gold standard gives scores of 0.6 and0.4 respectively, indicating a moderate to lowlevel of agreement.
The value for Cohen?skappa between the two non-agreeing annota-tors is 0.2, indicating that they are in lowagreement with each other.Analysing the errors made by these 2 anno-tators on the validation set reveals some in-consistencies.
E.g., one sentence marked asincorrect changes the fragment ?education andteaching?
to ?learning and teaching?.
However,every other annotator marked the enclosingsentence as correct.
This level of inconsistencyand low agreement with the other annotators74shows that these annotators had difficulty withthe task.
They may not have read the instruc-tions carefully or may not have understood thetask fully.Corpus accuracy is defined as the percentageof instances that were marked as being trueinstances of simplification (not counting thosein the validation set).
This is out of 50 foreach annotator and can be combined linearlyacross all six annotators.Taking all six annotators into account, thecorpus accuracy is 90.67%.
Removing theworst performing annotator (kappa = 0.4) in-creases the corpus accuracy to 94.80%.
If wealso remove the next worst performing annota-tor (kappa = 0.6), leaving us with only the fourannotators who were in agreement on the val-idation set, then the accuracy increases againto 97.5%.There is a very strong Pearson?s correlation(r = 0.980) between an annotator?s agreementwith the gold standard and the accuracy whichthey give to the corpus.
Given that the loweraccuracy reported by the non-agreeing anno-tators is in direct proportion to their devia-tion from the gold standard, this implies thatthe reduction is a result of the lower qualityof those annotations.
Following this, the twonon-agreeing annotators should be discountedwhen evaluating the corpus accuracy ?
givinga final value of 97.5%.4 DiscussionThe necessity of this corpus developed from alack of similar resources.
CW identification isa hard task, made even more difficult if blindto its evaluation.
With this new resource, CWidentification becomes much easier to evaluate.The specific target application for this is lex-ical simplification systems as previously men-tioned.
By establishing and improving uponthe state of the art in CW identification, lexi-cal simplification systems will directly benefitby knowing which wordforms are problematicto a user.Methodologically, the corpus is simple to useand can be applied to evaluate many currentsystems (see Section 6).
Techniques using dis-tributional semantics (Bott et al 2012) mayrequire more context than is given by just thesentence.
This is a shortcoming of the corpusin its present form, although not many tech-niques currently require this level of context.If necessary, context vectors may be extractedby processing Simple Wikipedia edit histories(as presented in Section 2.2) and extractingthe required information at the appropriatepoint.There are 731 lexical edits in the corpus.Each one of these may be used as an exam-ple of a complex and a simple word, giving us1,462 points of data for evaluation.
This islarger than a comparable data set for a simi-lar task (de Belder and Moens, 2012).
Waysto further increase the number of instances arediscussed in Section 6.It would appear from the analysis of the val-idation sets (presented above in Section 3.2)that two of the annotators struggled with thetask of annotation, attaining a low agreementagainst the gold standard.
This is most likelydue to the annotators misunderstanding thetask.
The annotations were done at the indi-vidual?s own workstation and the main guid-ance was in the form of instructions on thequestionnaire.
These instructions should beupdated and clarified in further rounds of an-notation.
It may be useful to allow annotatorsdirect contact with the person administeringthe questionnaire.
This would allow clarifi-cation of the instructions where necessary, aswell as helping annotators to stay focussed onthe task.The corpus accuracy of 97.5% implies thatthere is a small error rate in the corpus.
Thisoccurs due to some non-simplifications slip-ping through the checks.
The error rate meansthat if a system were to identify CWs perfectly,it would only attain 97.5% accuracy on theCW corpus.
CW identification is a difficulttask and systems are unlikely to have such ahigh accuracy that this will be an issue.
If sys-tems do begin to attain this level of accuracythen a more rigorous corpus will be warrantedin future.There is significant interest in lexical sim-plification for languages which are not English(Bott et al 2012; Alu?
?sio and Gasperin, 2010;Dell?Orletta et al 2011; Keskisa?rkka?, 2012).The technique for discovering lexical simpli-fications presented here relies heavily on theexistence of Simple English Wikipedia.
As no75other simplified language Wikipedia exists, itwould be very difficult to create a CW corpusfor any language other than English.
However,the corpus can be used to evaluate CW identi-fication techniques which will be transferrableto other languages, given the existence of suf-ficient resources.5 Related WorkAs previously noted, there is a systemic lackof evaluation in the literature.
Notable excep-tions come from the medical domain and in-clude the work of Zeng et al(2005), Zeng-Treitler et al(2008) and Elhadad (2006).Zeng et al(2005) first look at word familiarityscoring correlated against user questionnairesand predictions made by a support vector ma-chine.
They show that they are able to predictthe complexity of medical terminology with arelative degree of accuracy.
This work is con-tinued in Zeng-Treitler et al(2008), where aword?s context is used to predict its familiar-ity.
This is similarly correlated against a usersurvey and used to show the importance ofcontext in predicting word familiarity.
Thework of Elhadad (2006) uses frequency andpsycholinguistic features to predict term famil-iarity.
They find that the size of their corpusgreatly affects their accuracy.
Whilst thesetechniques focus on the medical domain, theresearch presented in this paper is concernedwith the more general task of CW identifica-tion in natural language.There are two standard ways of identifyingCWs in lexical simplification systems.
Firstly,systems attempt to simplify every word (De-vlin and Tait, 1998; Thomas and Anderson,2012; Bott et al 2012), assuming that CWswill be modified, but for simple words, nosimpler alternative will exist.
The danger isthat too many simple words may be mod-ified unnecessarily, resulting in a change ofmeaning.
Secondly, systems use a thresholdover some word familiarity score (Biran et al2011; Elhadad, 2006; Zeng et al 2005).
Wordfrequency is typically used as the familiarityscore, although it may also be combined withword length (Biran et al 2011).
The adventof the CW corpus will allow these techniquesto be evaluated alongside each other on a com-mon data set.The CW corpus is similar in conceptionto the aforementioned lexical simplificationdataset (de Belder and Moens, 2012) whichwas produced for the SemEval 2012 Task 1 onlexical simplification.
This dataset alws syn-onym ranking systems to be evaluated on thesame platform and was highly useful duringthis research (see Table 1).6 Future WorkThe CW corpus is still relatively small at731 instances.
It may be grown by carryingout the same process with revision historiesfrom the main English Wikipedia.
Whilst theEnglish Wikipedia revision histories will havefewer valid simplifications per revision, theyare much more extensive and contain a lotmore data.
As well as growing the CW corpusin size, it would be worthwhile to look at waysto improve its accuracy.
One way would beto ask a team of annotators to evaluate everysingle instance in the corpus and to discard orkeep each according to their recommendation.Experiments using the corpus are presentedin Shardlow (2013), further details on the useof the corpus can be found by following thisreference.
Three common techniques for iden-tifying CWs are implemented and statisticallyevaluated.
The CW Corpus is available fromMETA-SHARE5 under a CC-BY-SA Licence.AcknowledgmentsThis research is supported by EPSRC grantEP/I028099/1.
Thanks go to the annota-tors and reviewers, who graciously volunteeredtheir time.ReferencesSandra Maria Alu?
?sio and Caroline Gasperin.2010.
Fostering digital inclusion and accessi-bility: the PorSimples project for simplifica-tion of Portuguese texts.
In Proceedings of theNAACL HLT 2010 Young Investigators Work-shop on Computational Approaches to Lan-guages of the Americas, YIWCALA ?10, pages46?53, Stroudsburg, PA, USA.
Association forComputational Linguistics.Or Biran, Samuel Brody, and Noe?mie Elhadad.2011.
Putting it simply: a context-aware ap-proach to lexical simplification.
In Proceed-5http://tinyurl.com/cwcorpus76ings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: HumanLanguage Technologies: short papers - Volume2, HLT ?11, pages 496?501, Stroudsburg, PA,USA.
Association for Computational Linguis-tics.Stefan Bott, Luz Rello, Biljana Drndarevic, andHoracio Saggion.
2012.
Can Spanish be sim-pler?
LexSiS: Lexical simplification for Spanish.In Coling 2012: The 24th International Confer-ence on Computational Linguistics., pages 357?374.Marc Brysbaert and Boris New.
2009.
Movingbeyond Kuc?era and Francis: A critical evalua-tion of current word frequency norms and theintroduction of a new and improved word fre-quency measure for American English.
BehaviorResearch Methods, 41(4):977?990.Jacob Cohen.
1968.
Weighted kappa: nominalscale agreement with provision for scaled dis-agreement or partial credit.
Psychological Bul-letin, 70(4):213?220.Jan de Belder and Marie-Francine Moens.
2012.A dataset for the evaluation of lexical simpli-fication.
In Computational Linguistics and In-telligent Text Processing, volume 7182 of Lec-ture Notes in Computer Science, pages 426?437.Springer, Berlin Heidelberg.Felice Dell?Orletta, Simonetta Montemagni, andGiulia Venturi.
2011.
Read-it: assessing read-ability of Italian texts with a view to text simpli-fication.
In Proceedings of the Second Workshopon Speech and Language Processing for AssistiveTechnologies, SLPAT ?11, pages 73?83, Strouds-burg, PA, USA.
Association for ComputationalLinguistics.Siobhan Devlin and John Tait.
1998.
The use of apsycholinguistic database in the simplification oftext for aphasic readers, volume 77.
CSLI Lec-ture Notes, Stanford, CA: Center for the Studyof Language and Information.Noemie Elhadad.
2006.
Comprehending technicaltexts: Predicting and defining unfamiliar terms.In AMIA Annual Symposium proceedings, page239.
American Medical Informatics Association.Christiane Fellbaum.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,MA.Joseph L. Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychologicalbulletin, 76:378?382, November.Robin Keskisa?rkka?.
2012.
Automatic text sim-plification via synonym replacement.
Master?sthesis, Linko?ping University.Henry Kuc?era and W. Nelson Francis.
1967.
Com-putational analysis of present-day American En-glish.
Brown University Press.Gerard Salton and Chung-Shu Yang.
1973.
On thespecification of term values in automatic index-ing.
Journal of Documentation, 29(4):351?372.Matthew Shardlow.
2013.
A comparison of tech-niques to automatically identify complex words.In Proceedings of the Student Research Work-shop at the 51st Annual Meeting of the Associa-tion for Computational Linguistics.
Associationfor Computational Linguistics.S.
Rebecca Thomas and Sven Anderson.
2012.WordNet-based lexical simplification of a docu-ment.
In Proceedings of KONVENS 2012, pages80?88.
O?GAI, September.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For thesake of simplicity: unsupervised extraction oflexical simplifications from Wikipedia.
In Hu-man Language Technologies: The 2010 AnnualConference of the North American Chapter ofthe Association for Computational Linguistics,HLT ?10, pages 365?368, Stroudsburg, PA, USA.Association for Computational Linguistics.Qing Zeng, Eunjung Kim, Jon Crowell, and TonyTse.
2005.
A text corpora-based estimation ofthe familiarity of health terminology.
In Biolog-ical and Medical Data Analysis, volume 3745 ofLecture Notes in Computer Science, pages 184?192.
Springer, Berlin Heidelberg.Qing Zeng-Treitler, Sergey Goryachev, Tony Tse,Alla Keselman, and Aziz Boxwala.
2008.
Esti-mating consumer familiarity with health termi-nology: a context-based approach.
Journal ofthe American Medical Informatics Association,15:349?356.77
