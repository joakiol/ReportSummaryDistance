A Constraint-Based Approach to Linguistic Performance*HASIDA, KSitiTokyo University7-3-1, Hongo, Bunkyo-ku, Tokyo 113, JapanElectrotechnical Laboratory1-1-4, Umezono, Tukuba, Ibaraki 103, JapanInstitute for New GenerationComputer Technology (ICOT)Mita-Kokusai Bldg.
21F, 1-4-28,Mira, Minato-ku, Tokyo 108, Japan-I-81-3-456-3194, hasida@icot.jpAbst ractThis paper investigates linguistic performance, fromthe viewpoint hat the information processing in cogni-tive systems hould be designed in terms of constraintsrather than procedures in order to deal with partialityof information.
In this perspective, the same gram-mar, the same belief and the same processing architec-ture should underly both sentence comprehension a dproduction.
A basic model of sentence processing, forboth comprehension a d production, is derived alongthis llne of reasoning.
This model is demonstrated toaccount for diverse linguistic phenomena pparentlyunrelated to each other, lending empirica!
support tothe constraint paradigm.1 In t roduct ionAll the cognitive agents, with limited capacity for infor-mation processing, face partiality of information: In-formation relevant o their activities is only partiallyaccessible, and also the distribution pattern of the ac-cessible information is too diverse to predict.
In sen-tence comprehension, for example, the phonological ormorphological information may or may not be partiallymissing due to some noise, the semantic informationmay or may not be abundant because of familiality orignorance on the topics, and so forth.
Thus the infor-mation distribution is very diverse and rather orthogo-nal to the underlying information structure consistingof the modules of morphological, syntactic, pragmatic,and other constraints.This diversity of information distribution gives riseto a very complex, non-modular flow of information incognitive processes, as information flows from placespossessing information to places lacking information.In order to deal with this complexity, a cognitive sys-tem must be designed to include two different logicallayers:'(1) Information represented in terms of constraints,*The work reported here started as the author's doctoral re-search at Tokyo University, and has developed filrther at Elec-trotechnical Laboratory and ICOT.
The author's current affil-iation is ICOT.
ltis thanks go to Prof. YAMADA Itisao, whowas the supervisor fthe doctoral program, and too many otherpeople to enu,nerate h re.by abstracting away information flow.
(2) A general processing mechanism to convey infor-mation across constraints, from places possessinginformation to places lacking it.>'on-modular flow of information may be captured onthe basis of modular design of cognitive architecture,only by separating the representation f underlying in-formation (as (1)) and flow of information (as (2)) fl'omeach other.Procedural approaches break down under partial-ity of information, because procedures stipulate, andhence restrict, information flow.
If one.
be it humanor nature, were to implement such diverse informationflow by procedural programming, the entire systemwould quickly become too complex to keep track of,failing to maintain the modularity of the system.
Thisis what has always happened, for example, in the de-velopment of natural anguage processing systems.The rest of the paper exemplifies the efficacy of theconstraint paradig,n with regard to natural language.We wil!
first discuss a general picture of language fac-ulty immediately obtained fl'om the constraint-basedview, and then derive a model of sentence processingneutral between comprehension a d production.
Thismodel will be shown to fit several inguistic phenom-ena.
Due to the generality of the perspective, the phe-nomena discussed below encompass apparently unre-lated aspects of natural anguage.2 Language and Const ra in tFrom tile constraint-based perspective immediatelyfollows a hypothesis tI'lat the same constraints (i.e.,lexical, syntactic, semantic, pragmatic, and whatever),corresponding to (1), and the same processing architec-ture, corresponding to (2), should underly both sen-tence comprehension a d production.
Other authorshave expressed less radical stances.
For instance, Kay\[11\] adopts two different grammars for parsing and gen-eration.
Our hypothesis also stronger than Shieber's\[16\]; Although he proposes to share not only one gram-mar but also one processing architecture between thetwo tasks, this 'common' architecture is, unlike ours,parameterized so as to adapt itself to parsing andgeneration i  accordance with different parameter set-tings.1149As a corollary of our strong uniformity hypothesis,we reject every approach postulating any procedurespecific to sentence comprehension or production.
Forinstance, we disagree upon the ways in which the De-terminism Hypothesis (DH) \[12\] has been instantiatedso far.
DH permits to assume only one partial struc-ture of a sentence at a time, and the approaches alongthis line \[2, 3, 12, 14\] has postulated, beyond necessity,specific ways of disambiguation for specific types ofambiguity in sentence comprehension and production.Instead we view sentence processing as parallel com-putation.
When a sentence is either comprehended orproduced, several partial structures of it, we assume,are simultaneously hypothesized.
The degree of par-allelism should be limited to fall within the small ca-pacity of the short-term memory (STM), so that weobtain the same sort of predictions as we do alongthe determinist account.
For instance, the difficultyin comprehending garden path sentences like (3) maybe attributed to the difficulty of keeping some struc-tural hypotheses in STM.
(3) The chocolate cakes are coated with tastes sweet.As discussed below, our approach quantitatively esti-mates the difficulty in processing embedded construc-tions like (4) also on the basis of the memory limita-tion.
(4) The cheese the rat the cat the dog chased caughtbit was rotten.Since DH does not account for such difficulty, inciden-tally, it seems superfluous to postulate DH.
We con-sider DH jnst~ as approximation of severe memory lim-itation, and avoid any stipulation of such a hypothesis.3 A Common Process  Mode lAmong the partial structures hypothesized duringcomprehension or production of a sentence, we pay at-tention to the maximal st~'uctures; the structures uchthat there is no larger structures.
Here we say onestructure is larger than another when the former in-eludes the latter.
For example, \[s \[NP Tom\] \[vp sleeps\]\]is larger than \[s \[NP Tom\] VP\].
Sentence processing,whether comprehension or production, is regarded asparallel construction of several maximM structures.Thus sentence processing as & whole is characterizedby specifying what a maximal structure is.We assume the grammatical structure of a sen-tence to be a binary tree.
Here we identify a wordwith its grammatical category, so that a local struc-ture, such as \[NP Tom\], is regarded as one node ratherthan a partial tree consisting of two distinct nodes.It is just for expository simplification that we as-sume binary trees.
Our account can be generalizedstraightforwardly to allow n-ary trees.
Further, theessence of our discussion below is neutral between theconstituency-based approaches and the dependency-based approaches.
Here we employ a representationscheme of the former type, without committing our-selves to the constituency-based framework.From the general speculation below, it follows that amaximal structure should be the left-hand half of (5).
(5) sThis maximal structure consists of the path form S toA and the part to the left of this path, except for Bi-1and the nodes between Bi-1 and Ai (those on tile slantdotted lines) for 1 < i < d+l ;A iandthenodesbe-tween Ai and Bi are included in the maximal structure.Here B0 and Ad+l stand for S and A, respectively.
Ai isa leftmost descendant (not necessarily the left daugh-ter) of Bi_l or they are identical for 1 _< i < d+l .Bi is a rightmost descendant (not necessarily the rightd&v.ghter) of Ai for 1 G i < d. Thus our model issimilar to left-corner parser \[1\], though our discussionis not restricted to parsing.This characterization of a maximal structure is ob-tained as follows.
First note that a maximal structureinvolves n words and n -  i nonterminal nodes, for somenatural number n; In the maximal structure in (5), theconnected substructure containing Ai (l <; i _< d)contains as many nonterminal nodes as words, so thatthe maximal structure also contains as many nonter-minal nodes as words, except for word A.
Note furtherthat the entire sentence structure, being a binary tree,also involves one less nonterminal nodes than words.Accordingly, postulating n - 1 nonterminM nodes ver-sus n words in a maximal structure amounts to postu-lating that the words and the nonterminal nodes areprocessed at approximately constant speed relative toeach other.
1 The number of words is a measure of lexi-cal information, and the number of nonterminal nodesis a measure of syntactic and semantic information,among others.
Hence if all the types of linguistic in-formation (lexical, syntactic, semantic, etc.)
are pro-cessed at approximately the same relative speed, then amaximal process should include nearly as many wordsas nonterminal nodes.This premise is justified, because if different typesof information were processed at different speeds, thentThe rate of n words versus n - 1 nonterminals does notprecisely represent the constant relative speed, but the dis-crepancy here is least possible and thus acceptable enough asapproximat ion.150 2there would arise imbalance of information distribu-tion across the corresponding different domains of in-formation.
Such imhalance should invoke informationflow from the domains with higher density to the do-mains with lower density of information distribution,when, as in the case of language, those domains of in-formation are tightly related with each other.
That is,information flow eliminates uch imbalance, resultingin approximately the same speed of processing acrossdifferent but closely related domains of information.Now that we have worked out how many nodes amaximal structure includes, what is left is which nodesit includes.
Let us refer to A in (5) as the current ac-tive word and the path from the root node S to thecurrent active word as the current active path.
It isnatural to consider that a maximal structure includesthe nodes to the left of the current active path, be-cause all the words they dominate have already beenproce,;sed.
Thus we come up with the above formula-tion of a maximal structure, if we notice that the nodeson the solid-line part (including Ai) of the current ac-tive path in (5) are adjacent o nodes to the left of thecurrent active path, whereas the other nodes on thecurrent active path (those on the dotted lines, includ-ing Bi) do not except for the mother of A, which willbe processed at the next moment.4 Immediate ProcessingAccording to this model, any word should be in,me-diately processed, particularly in parsing, in the sensethat corresponding amount of syntactic and semanticstructure is tailored with little delay.
The intrasen-tential status of a word is hence identified as soon asit is encountered.
This contrasts with the deterministaccounts which ,'assume lookahead to deal with localambiguity.Empirical evidences support our position.
InMarslen-Wilson's \[13\] experiment, for instance, thesubjects were asked to listen to a tape-recorded utter-ance and to say aloud what they hear with the short-est possible delay.
Some subjects performed this taskwith a lag of only about one syllable, and yet their er-ror reflected both syntactic and semantic ontext.
Forexample, one of such a subjects aid l ie had heard thatthe Brigade .
.
.
upon listening to He had heard at theBrigade .
.
.
.
Such a phenomenon cannot be accountedfor in terms of the determinist accounts with fixed pars-ing procedures.
In our model, it is explained by justassuming that only the most active maximal structuretailored by the subject survives the experimental situ-ation.5 ~.~ansient Memory LoadBy transient memory load (TML) we refer to tileamount of linguistic information temporarily storedin STM.
The measurements of TML during sentenceprocessing proposed so far include the depth of centerembedding (CE) \[5\] and that of self embedding (SE)\[15\].
A syntactic onstituent a is centeroembedded inanother syntactic constituent /3 when /3 = -rc~5 forsome non-null strings 7 and ?
We further say that c,is self-embedded in /3 when they are of the same sortof category, say NP.However, neither CE nor SE can explain why (6) ismuch easier to understand than (7).
(6) 2bm knows the story that a man who lived inHelsinki and his wife were poor but they werehappy.
(7) Tom knows that the story on the fact that therumor that Mary killed John was false is funny.Note that these sentences are of about the same length;The former consists of 20 words and the latter 19words.
Almost all my informants (including both na-tive and non-native speakers of English) reported that(6) is easier to understand than (7).
Those who feltcontrariwise ascribed the difficulty of (6) to the ambi-guity concerning the overall structure of the cornple-meI~t clause after that.The approach based on CE fails to account for thisdifference, because the maximum CE depth of (6) a:.dthat of (7) are both 3, as is shown below.
(8) \[0Tom knows the story that \[la man \[2 who\[3lived\] in Helsinki\] and his wife were poor\] butthey were happy\](9) \[0 Tom knows that \[~ the story on the fact that\[2 the rumor that Mary \[a killed\] John\] was false\]is funny\]The maximum SE depth cannot distinguish these sen-tences:(10) Tom knows \[NPo tile story that \[NP~ a man wholived in \[NP~ Helsinki\] and his wife\] were poor butthey were happy\](11) Tom knows that \[NP0 the story on the fact that\[NP, the rumor that \[NP2 Mary\] killed John\] wasfalse\] is funny.Our model provides a TML measure which accountsfor the contrast in question.
In order to plug a maximalstructure with the rest of the sentence in a grammati-cal manner, one must remember only the informationcontained in the categories o11 the border between themaximal structure and the remaining context; i.e., cat-egories Ai, the mother of Bi (1 ~ i _< d) and A in(5).
Thus the value of d in (5) could serve as a TMLmeasure.
As is illustrated in (12) and (13), in fact,the maximum of d is 2 and 3 for (6) and (7), respec-tively, explaining why (6)is easier.
In (12) and (13),enclosed in boxes are the nodes corresponding to A,,Bi(1 < i < d) and A when d is ttle maximum; i.e., 2in the former and 3 in tile latter.151 3(~-)NP\[TomVPoV NPoknows NP Sothe story Cothat S~ butNPt VPP were poorNP St his wifea man Cornp SawLoSthey were happylived P NP2I Iin Helsinki(13)NPtTomVPoV ~ g oAw+ ooi22PP is funnyp NPon N $1the fact Comptl PNP $2 was falsethe rumor Comp SaNP2Mary NPLkilled JohnNPthe story152 46 Language AcquisitionThe Dutch language xhibits a type of cross-serial de-pendency (CSD) in subordinate clauses:(14) .
.
.dat  Wolf de kinderen Marie... that Wolf the children Mariezag helpen zwemmensee-PAST help-INF swim-INF'.
.
.
that Wolf saw the children help Marie swim'Our .theory predicts that children learning Dutch cometo recognize the CSD constructions "as having the fol-lowing structure, which coincides with the structurefigured out by Bresnan et al \[4\] ~based on an analysisof adult language.
(15) SNP0 VPXo Zo//"...... ..t/./.........N P ~ '"... Vo ..... .."%.
.. %. '
.
%Xm_ ~ Z,,_ a/ ' - , ,NP,,-1 NP,, V,,:1 V,ttere Vo is a finite verb and V; is an infinite verb for1 < i < n. Vi is a causative verb or a perceptionverb for 1 < i < n. NPl is the subject of Vi for0 < i < n, and NPl is an object of V, forn < i <_ m(m > m).
Note that NP~, .
.
.NP , ,  and V0 , " 'V~constitute right-branching structures dominated by X0and Zo, respectively.Let us look at how a child regard a simple CSD con-struction (16) to be (17), which is an instance of (15)for m = n = 1.
(16) ,.. dat Wolf/vlarie zag zwemmen... that Wolf Marie see-PAST swim-INFL.. that Wolf saw Marie swim'(17) sNPo VPW!lf NPI Zo I / " " -Marie V0 VaI Izag zwemmenAccording to our model, the relevant part of the mostactive maximal structure would look like the following2(15) is slightly different from the structure proposed by Bres-nan et al, because we regard a sentence structure as a binarytree whereas their proposal involves tertiary branching obtainedby equating VP and X0 in (15).
This difference is irrelevant tothe essence ofthe following disc.ssion.when zag has just been acknowledged, provided thatthe child has already acquired the standard structureof a subordinate clause, in which the finite verb appearsat the end.
(18) SNPo VPoI 1Wol f V P 1NPtIMarieZ0VoIzagVPo, VP1, Zo and Vo correspond to B,~-t, Aa, Bd andA in (5), respectively (so that VPo and Zo are notincluded in the maximal structure here).
When zwem-men is encountered, category \[v, zwemmen\] must beinserted either between VPo and VPI or between Zoand Vo.
In the alleged subordinate clause construe r-tion, Zo (which might be identical to Vo) has a directaccess to \[NPj Marie\], which is the object of zag, the al-leged head of Zo.
On the other hand, VP1 lacks such anaccess, because the relationship between Marie and zagis established not through but under VP~.
It is hencemore preferable that \[v~ zwemmen\] attaches beneathZo, if the child has already perceived extralingulsti-eally the situation being described, in which Marie isswhnming.
Now the most active maximal structureshould look like this (Zo and Z1 are excluded from thismaximal structure if they are distinct from Yo and V1,respectively):(19) zoYoVo Ztzag VaIzwemmen(17) is ttms obtained by setting VPo = VPh Zo = Yo,attd Zl = Vl.Note that this reasoning essentially relies oil our for-mulation of a maximal process.
If a bottom-up modelwere assumed instead, for instance, there would be noimmediate reason to exclude a structure, say, as fol-lows.
(2o) SNPo VPIWolf U V1NP1 Vo zwemmenMarie zag5 153The above discussion can be extended to cover morecomplex cases (where m > 1 in (15)) in a ratherstraightforward manner, as is discussed by Hasida \[6\].The structure under Xo is tailored as a natural ex-tension of the way an ordinary subordinate clause isprocessed, then Vo is inserted beneath VP, followingthe ordinary structure of a subordinate clause togetherwith the semantic information about the situation de-scribed, and Vi attaches near to Vi-~ for 1 < i < ndue to the semantic information again.
The structureunder Z0 must be right-branching so that V0 be thehead of VP.Also by reference to the current model, Hasida \[7\]further gives an account of the unacceptability of someunbounded dependency onstructions in English whichis hard to explain in static terms of linguistics.7 Concluding RemarksWe have begun with a general constralnt-based per-spective about the cognitive mechanism, and shownthat a model of sentence processing derived thereof,neutral between comprehension and production, ac-counts for several linguistic phenomena seemingly un-related to each other.
It has thus been demonstratedthat the speculation to derive the model has empir-ical supports, lending justification for the constraintp~radigm.
In particular, our theory has been shownto be more adequate than the determinist approach,which must postulate a procedural design of the hu-man language faculty.A computational formalization of our model will bepossible in terms of constraint programming, as dis-cussed by Hasida et al \[8, 9, 17\].
Most of the time,a natural anguage processing system in terms of pro-cedural programming has been designed to be a seriesof a syntactic analysis procedure, a semantic analysisprocedure, a pragmatic analysis procedure, and so on,in order to reflect the modularity of the underlyingconstraints.
}towever, such a design imposes a stronglimitation on information flow, restricting the system'sability to a very narrow range of context.
One natu-rally attempts to remedy this so as to, say, enable thesyntactic analysis module to refer to semantic infor-mation, but this attempt must destroy the modularityof the entire design, ending up with a program toocomplicated to extend or even maintain.
Constraintparadigm seems to be the only way out of this diffi-culty.References\[1\] Aho, A. V. and Ullman, U. D. (1972) The Theoryof Parsing, Translation and Compiling, Prentice-Hall.\[2\] Berwick, R. C. and Weinberg, A.
(1984) TheGrammatical Basis of Linguistic Performance,MIT Press.\[3\] Berwick, R. (1985) The Acquisition of SyntacticKnowledge, MIT Press.\[4\] Bresnan, J. Kaplan, R. M., Peters, S. and Zaenen,A.
(1982) 'Cross-serlal Dependencies in Dutch,'Linguistic Inquiry, Vol.
13, pp.
613-635.\[5\] Church, K. W. (1980) On Memory Limitations inNatural Language Processing, MIT/LCS/TR-245,Laboratory for Computer Science, MassachusettsInstitute of Technology.\[6\] tIasida, K. (1985) Bounded Parallelism: A Theoryof Linguistic Performance, doctoral dissertation,University of Tokyo.\[7\] Haslda, K. (1988) 'A Cognitive Account of Un-bounded Dependency,' in Proceedings of COL-ING'88, pp.
231-236.\[8\] Hasida, K. (1989) A Constraint-Based View ofLanguage, presented at the F~rst Conference onSituation Theory and its Applications.\[9\] ttaslda, K. and Ishlzaki, S. (1987)'DependencyPropagation: A Unified Theory of Sentence Com-prehension and Generation,' Proceedings of IJ-CAI'87, pp.
664-670.\[10\] Kaplan, R. M. (1972) 'Augmented Transition Net-works as Psychological Models of Sentence Com-prehension,' Artificial Intelligence, Vol.
3, pp.
77-100.\[11\] Kay, M. (1985) 'Parsing in Functional Unifica-tion Grammar,' in Dowty, D., Karttunen, L.and Zwicky, A. M.
(eds.)
Natural Language Pars-ing: Psychological, Computational, and Theoreti-cal Perspectives, Cambridge University Press.\[12\] Marcus, M. P. (1980) A Theory of SyntacticRecognition for Natural Language, MIT Press.\[13\] Marslen-Wilson, W. D. (1975) 'Sentence Percep-tion as an Interactive Parallel Process,' Science,Vol.
189, pp.
226-228.\[14\] McDonald, D. (1980) Natural Language Pro-duction as a Process of Decision Making un-der Constraint, Doctoral Dissertation, Laboratoryof Computer Science, Massachusetts Institute ofTechnology.\[15\] Miller, G. A. and Chomsky, N. (1963) 'FinitaryModels of Language Users,' in Luee, R. D., Bush,R.
K., and Galanter, E. tlandbook of MathematicalPsychology, Vol.
lI, pp.
419-491, John Wiley andSons.\[16\] Shieber, S. M. (1988) 'A Uniform Architecture forParsing and Generation,' in Proceedings of COL-ING'SS, pp.
614-619.\[17\] Tuda, H., Hasida, K., and Sirai, H. (1989) 'JPSGParser on Constraint Logic Programming,' Pro-ceedings of the European Chapter of ACL'89.154 6
