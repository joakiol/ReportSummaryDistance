Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1416?1424,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsModels of Translation CompetitionsMark Hopkins and Jonathan MaySDL Research6060 Center Drive, Suite 150Los Angeles, CA 90045{mhopkins,jmay}@sdl.comAbstractWhat do we want to learn from a trans-lation competition and how do we learnit with confidence?
We argue that a dis-proportionate focus on ranking competi-tion participants has led to lots of differ-ent rankings, but little insight about whichrankings we should trust.
In response, weprovide the first framework that allows anempirical comparison of different analy-ses of competition results.
We then usethis framework to compare several analyt-ical models on data from the Workshop onMachine Translation (WMT).1 The WMT Translation CompetitionEvery year, the Workshop on Machine Transla-tion (WMT) conducts a competition between ma-chine translation systems.
The WMT organizersinvite research groups to submit translation sys-tems in eight different tracks: Czech to/from En-glish, French to/from English, German to/fromEnglish, and Spanish to/from English.For each track, the organizers also assemble apanel of judges, typically machine translation spe-cialists.1 The role of a judge is to repeatedly rankfive different translations of the same source text.Ties are permitted.
In Table 1, we show an ex-ample2 where a judge (we?ll call him ?jdoe?)
hasranked five translations of the French sentence ?Ilne va pas.
?Each such elicitation encodes ten pairwise com-parisons, as shown in Table 2.
For each compe-tition track, WMT typically elicits between 5000and 20000 comparisons.
Once the elicitation pro-cess is complete, WMT faces a large databaseof comparisons and a question that must be an-swered: whose system is the best?1Although in recent competitions, some of the judging hasalso been crowdsourced (Callison-Burch et al, 2010).2The example does not use actual system output.rank system translation1 bbn ?He does not go.
?2 (tie) uedin ?He goes not.
?2 (tie) jhu ?He did not go.
?4 cmu ?He go not.
?5 kit ?He not go.
?Table 1: WMT elicits preferences by askingjudges to simultaneously rank five translations,with ties permitted.
In this (fictional) example, thesource sentence is the French ?Il ne va pas.
?source text sys1 sys2 judge preference?Il ne va pas.?
bbn cmu jdoe 1?Il ne va pas.?
bbn jhu jdoe 1?Il ne va pas.?
bbn kit jdoe 1?Il ne va pas.?
bbn uedin jdoe 1?Il ne va pas.?
cmu jhu jdoe 2?Il ne va pas.?
cmu kit jdoe 1?Il ne va pas.?
cmu uedin jdoe 2?Il ne va pas.?
jhu kit jdoe 1?Il ne va pas.?
jhu uedin jdoe 0?Il ne va pas.?
kit uedin jdoe 2Table 2: Pairwise comparisons encoded by Ta-ble 1.
A preference of 0 means neither translationwas preferred.
Otherwise the preference specifiesthe preferred system.2 A Ranking ProblemFor several years, WMT used the following heuris-tic for ranking the translation systems:ORIGWMT(s) = win(s) + tie(s)win(s) + tie(s) + loss(s)For system s, win(s) is the number of pairwisecomparisons in which s was preferred, loss(s) isthe number of comparisons in which s was dispre-ferred, and tie(s) is the number of comparisons inwhich s participated but neither system was pre-ferred.Recently, (Bojar et al, 2011) questioned the ad-equacy of this heuristic through the following ar-1416gument.
Consider a competition with systems Aand B.
Suppose that the systems are different butequally good, such that one third of the time Ais judged better than B, one third of the time Bis judged better than A, and one third of the timethey are judged to be equal.
The expected valuesof ORIGWMT(A) and ORIGWMT(B) are both2/3, so the heuristic accurately judges the systemsto be equivalently good.
Suppose however thatwe had duplicated B and had submitted it to thecompetition a second time as system C. Since Band C produce identical translations, they shouldalways tie with one another.
The expected valueof ORIGWMT(A) would not change, but the ex-pected value of ORIGWMT(B) would increase to5/6, buoyed by its ties with system C.This vulnerability prompted (Bojar et al, 2011)to offer the following revision:BOJAR(s) = win(s)win(s) + loss(s)The following year, it was BOJAR?s turn to be crit-icized, this time by (Lopez, 2012):Superficially, this appears to be an im-provement....couldn?t a system still bepenalized simply by being comparedto [good systems] more frequently thanits competitors?
On the other hand,couldn?t a system be rewarded simplyby being compared against a bad systemmore frequently than its competitors?Lopez?s concern, while reasonable, is less obvi-ously damning than (Bojar et al, 2011)?s criti-cism of ORIGWMT.
It depends on whether thecollected set of comparisons is small enough orbiased enough to make the variance in competi-tion significant.
While this hypothesis is plausi-ble, Lopez makes no attempt to verify it.
Instead,he offers a ranking heuristic of his own, based ona Minimum Feedback Arc solver.The proliferation of ranking heuristics contin-ued from there.
The WMT 2012 organizers(Callison-Burch et al, 2012) took Lopez?s rankingscheme and provided a variant called Most Proba-ble Ranking.
Then, noting some potential pitfallswith that, they created two more, called MonteCarlo Playoffs and Expected Wins.
While onecould raise philosophical objections about each ofthese, where would it end?
Ultimately, the WMT2012 findings presented five different rankings forthe English-German competition track, with noguidance about which ranking we should pay at-tention to.
How can we know whether one rank-ing is better than other?
Or is this even the rightquestion to ask?3 A Problem with RankingsSuppose four systems participate in a translationcompetition.
Three of these systems are extremelyclose in quality.
We?ll call these close1, close2,and close3.
Nevertheless, close1 is very slightlybetter3 than close2, and close2 is very slightly bet-ter than close3.
The fourth system, called terrific,is a really terrific system that far exceeds the otherthree.Now which is the better ranking?terrific, close3, close1, close2 (1)close1, terrific, close2, close3 (2)Spearman?s rho4 would favor the second ranking,since it is a less disruptive permutation of the goldranking.
But intuition favors the first.
While itsmistakes are minor, the second ranking makes thehard-to-forgive mistake of placing close1 ahead ofthe terrific system.The problem is not with Spearman?s rho.
Theproblem is the disconnnect between the knowl-edge that we want a ranking to reflect and theknowledge that a ranking actually contains.
With-out this additional knowledge, we cannot deter-mine whether one ranking is better than another,even if we know the gold ranking.
We need todetermine what information they lack, and definemore rigorously what we hope to learn from atranslation competition.4 From Rankings to Relative AbilityOstensibly the purpose of a translation competi-tion is to determine the relative ability of a setof translation systems.
Let S be the space of alltranslation systems.
Hereafter, we will refer to Sas the space of students.
We choose this term toevoke the metaphor of a translation competition asa standardized test, which shares the same goal: toassess the relative abilities of a set of participants.But what exactly do we mean by ?ability??
Be-fore formally defining this term, first recognizethat it means little without context, namely:3What does ?better?
mean?
We?ll return to this question.4Or Pearson?s correlation coefficient.14171.
What kind of source text do we want thesystems to translate well?
Say system A isgreat at translating travel-related documents,but terrible at translating newswire.
Mean-while, system B is pretty good at both.
Thequestion ?which system is better??
requiresus to state how much we care about travelversus newswire documents ?
otherwise thequestion is underspecified.2.
Who are we trying to impress?
While it?stempting to think that translation quality isa universal notion, the 50-60% interannota-tor agreement in WMT evaluations (Callison-Burch et al, 2012) suggests otherwise.
It?salso easy to imagine reasons why one groupof judges might have different priorities thananother.
Think a Fortune 500 company ver-sus web forum users.
Lawyers versus lay-men.
Non-native versus native speakers.Posteditors versus Google Translate users.Different groups have different uses for trans-lation, and therefore different definitions ofwhat ?better?
means.With this in mind, let?s define some additional el-ements of a translation competition.
Let X be thespace of all possible segments of source text, J bethe space of all possible judges, and ?
= {0, 1, 2}be the space of pairwise preferences.5 We as-sume all spaces are countable.
Unless stated oth-erwise, variables s1 and s2 represent students fromS, variable x represents a segment from X , vari-able j represents a judge from J , and variable pirepresents a preference from ?.
Moreover, definethe negation p?i of preference pi such that p?i = 2 (ifpi = 1), p?i = 1 (if pi = 2), and p?i = 0 (if pi = 0).Now assume a joint distributionP (s1, s2, x, j, pi) specifying the probabilitythat we ask judge j to evaluate students s1 ands2?s respective translations of source text x, andthat judge j?s preference is pi.
We will furtherassume that the choice of student pair, sourcetext, and judge are marginally independent of oneanother.
In other words:P (s1, s2, x, j, pi)=P (pi|s1, s2, x, j) ?
P (x|s1, s2, j)?P (j|s1, s2) ?
P (s1, s2)= P (pi|s1, s2, x, j) ?
P (x) ?
P (j) ?
P (s1, s2)= PX (x) ?
PJ (j) ?
P (s1, s2) ?
P (pi|s1, s2, x, j)5As a reminder, 0 indicates no preference.It will be useful to reserve notation PX and PJfor the marginal distributions over source text andjudges.
We can marginalize over the source seg-ments and judges to obtain a useful quantity:P (pi|s1, s2)=?x?X?j?JPX (x) ?
PJ (j) ?
P (pi|s1, s2, x, j)We refer to this as the ?PX , PJ ?-relative ability ofstudents s1 and s2.
By using different marginaldistributions PX , we can specify what kinds ofsource text interest us (for instance, PX couldfocus most of its probability mass on Germantweets).
Similarly, by using different marginaldistributions PJ , we can specify what judges wewant to impress (for instance, PJ could focus allof its mass on one important corporate customeror evenly among all fluent bilingual speakers of alanguage pair).With this machinery, we can express the pur-pose of a translation competition more clearly:to estimate the ?PX , PJ ?-relative ability of a setof students.
In the case of WMT, PJ presum-ably6 defines a space of competent source-to-target bilingual speakers, while PX defines a spaceof newswire documents.We?ll refer to an estimate of P (pi|s1, s2) asa preference model.
In other words, a prefer-ence model is a distribution Q(pi|s1, s2).
Givena set of pairwise comparisons (e.g., Table 2),the challenge is to estimate a preference modelQ(pi|s1, s2) such that Q is ?close?
to P .
For mea-suring distributional proximity, a natural choice isKL-divergence (Kullback and Leibler, 1951), butwe cannot use it here because P is unknown.Fortunately, if we have i.i.d.
data drawn from P ,then we can do the next best thing and compute theperplexity of preference model Q on this heldouttest data.
LetD be a sequence of triples ?s1, s2, pi?where the preferences pi are i.i.d.
samples fromP (pi|s1, s2).
The perplexity of preference modelQ on test data D is:perplexity(Q|D) = 2???s1,s2,pi?
?D1|D| log2Q(pi|s1,s2)How do we obtain such a test set from competi-tion data?
Recall that a WMT competition pro-duces pairwise comparisons like those in Table 2.6One could argue that it specifies a space of machinetranslation specialists, but likely these individuals are thoughtto be a representative sample of a broader community.1418Let C be the set of comparisons ?s1, s2, x, j, pi?obtained from a translation competition.
Com-petition data C is not necessarily7 sampled i.i.d.from P (s1, s2, x, j, pi) because we may intention-ally8 bias data collection towards certain students,judges or source text.
Also, because WMT elicitsits data in batches (see Table 1), every segment xof source text appears in at least ten comparisons.To create an appropriately-sized test set thatclosely resembles i.i.d.
data, we isolate the sub-set C?
of comparisons whose source text appearsin at most k comparisons, where k is the smallestpositive integer such that |C?| >= 2000.
We thencreate the test set D from C?
:D = {?s1, s2, pi?|?s1, s2, x, j, pi?
?
C?
}We reserve the remaining comparisons for trainingpreference models.
Table 3 shows the resultingdataset sizes for each competition track.Unlike with raw rankings, the claim thatone preference model is better than another hastestable implications.
Given two competing mod-els, we can train them on the same comparisons,and compare their perplexities on the test set.
Thisgives us a quantitative9 answer to the question ofwhich is the better model.
We can then publisha system ranking based on the most trustworthypreference model.5 BaselinesLet?s begin then, and create some simple prefer-ence models to serve as baselines.5.1 UniformThe simplest preference model is a uniform distri-bution over preferences, for any choice of studentss1, s2:Q(pi|s1, s2) =13 ?pi ?
?This will be our only model that does not requiretraining data, and its perplexity on any test set willbe 3 (i.e.
equal to number of possible preferences).5.2 Adjusted UniformNow suppose we have a set C of comparisonsavailable for training.
Let Cpi ?
C denote thesubset of comparisons with preference pi, and let7In WMT, it certainly is not.8To collect judge agreement statistics, for instance.9As opposed to philosophical.C(s1, s2) denote the subset comparing students s1and s2.Perhaps the simplest thing we can do with thetraining data is to estimate the probability of ties(i.e.
preference 0).
We can then distribute theremaining probability mass uniformly among theother two preferences:Q(pi|s1, s2) =????????
?|C0||C| if pi = 01?
|C0||C|2 otherwise6 Simple Bayesian Models6.1 Independent PairsAnother simple model is the direct estimation ofeach relative ability P (pi|s1, s2) independently.
Inother words, for each pair of students s1 and s2, weestimate a separate preference distribution.
Themaximum likelihood estimate of each distributionwould be:Q(pi|s1, s2) =|Cpi(s1, s2)|+ |Cp?i(s2, s1)||C(s1, s2)|+ |C(s2, s1)|However the maximum likelihood estimate wouldtest poorly, since any zero probability estimatesfor test set preferences would result in infinite per-plexity.
To make this model practical, we assume asymmetric Dirichlet prior with strength ?
for eachpreference distribution.
This gives us the follow-ing Bayesian estimate:Q(pi|s1, s2) =?+ |Cpi(s1, s2)|+ |Cp?i(s2, s1)|3?+ |C(s1, s2)|+ |C(s2, s1)|We call this the Independent Pairs preferencemodel.6.2 Independent StudentsThe Independent Pairs model makes a strong inde-pendence assumption.
It assumes that even if weknow that student A is much better than student B,and that student B is much better than student C,we can infer nothing about how student A will fareversus student C. Instead of directly estimating therelative ability P (pi|s1, s2) of students s1 and s2,we could instead try to estimate the universal abil-ity P (pi|s1) = ?s2?S P (pi|s1, s2) ?
P (s2|s1) ofeach individual student s1 and then try to recon-struct the relative abilities from these estimates.For the same reasons as before, we assume asymmetric Dirichlet prior with strength ?
for each1419preference distribution, which gives us the follow-ing Bayesian estimate:Q(pi|s1) =?+?s2?S |Cpi(s1, s2)|+ |Cp?i(s2, s1)|3?+?s2?S |C(s1, s2)|+ |C(s2, s1)|The estimatesQ(pi|s1) do not yet constitute a pref-erence model.
A downside of this approach is thatthere is no principled way to reconstruct a pref-erence model from the universal ability estimates.We experiment with three ad-hoc reconstructions.The asymmetric reconstruction simply ignores anyinformation we have about student s2:Q(pi|s1, s2) = Q(pi|s1)The arithmetic and geometric reconstructionscompute an arithmetic/geometric average of thetwo universal abilities:Q(pi|s1, s2) =Q(pi|s1) +Q(p?i|s2)2Q(pi|s1, s2) = [Q(pi|s1) ?Q(p?i|s2)]12We respectively call these the (Asymmet-ric/Arithmetic/Geometric) Independent Studentspreference models.
Notice the similarities be-tween the universal ability estimates Q(pi|s1) andthe BOJAR ranking heuristic.
These three modelsare our attempt to render the BOJAR heuristic aspreference models.7 Item-Response Theoretic (IRT) ModelsLet?s revisit (Lopez, 2012)?s objection to the BO-JAR ranking heuristic: ?...couldn?t a system still bepenalized simply by being compared to [good sys-tems] more frequently than its competitors??
Theofficial WMT 2012 findings (Callison-Burch et al,2012) echoes this concern in justifying the exclu-sion of reference translations from the 2012 com-petition:[W]orkers have a very clear preferencefor reference translations, so includ-ing them unduly penalized systems that,through (un)luck of the draw, were pit-ted against the references more often.Presuming the students are paired uniformly atrandom, this issue diminishes as more compar-isons are elicited.
But preference elicitation is ex-pensive, so it makes sense to assess the relativeability of the students with as few elicitations aspossible.
Still, WMT 2012?s decision to eliminatereferences entirely is a bit of a draconian mea-sure, a treatment of the symptom rather than the(perceived) disease.
If our models cannot functionin the presence of training data variation, then weshould change the models, not the data.
A modelthat only works when the students are all about thesame level is not one we should rely on.We experiment with a simple model that relaxessome independence assumptions made by previ-ous models, in order to allow training data vari-ation (e.g.
who a student has been paired with)to influence the estimation of the student abili-ties.
Figure 1(left) shows plate notation (Kollerand Friedman, 2009) for the model?s indepen-dence structure.
First, each student?s ability dis-tribution is drawn from a common prior distribu-tion.
Then a number of translation items are gen-erated.
Each item is authored by a student and hasa quality drawn from the student?s ability distri-bution.
Then a number of pairwise comparisonsare generated.
Each comparison has two options,each a translation item.
The quality of each itemis observed by a judge (possibly noisily) and thenthe judge states a preference by comparing the twoobservations.We investigate two parameterizations of thismodel: Gaussian and categorical.
Figure 1(right)shows an example of the Gaussian parameteriza-tion.
The student ability distributions are Gaus-sians with a known standard deviation ?a, drawnfrom a zero-mean Gaussian prior with known stan-dard deviation ?0.
In the example, we showthe ability distributions for students 6 (an above-average student, whose mean is 0.4) and 14 (apoor student, whose mean is -0.6).
We also showan item authored by each student.
Item 43 hasa somewhat low quality of -0.3 (drawn from stu-dent 14?s ability distribution), while item 205 isnot student 6?s best work (he produces a meanquality of 0.4), but still has a decent quality at 0.2.Comparison 1 pits these items against one another.A judge draws noise from a zero-mean Gaussianwith known standard deviation ?obs, then adds thisto the item?s actual quality to get an observed qual-ity.
For the first option (item 43), the judge draws anoise of -0.12 to observe a quality of -0.42 (worsethan it actually is).
For the second option (item205), the judge draws a noise of 0.15 to observe aquality of 0.35 (better than it actually is).
Finally,the judge compares the two observed qualities.
Ifthe absolute difference is lower than his decision1420student.6.abilityGauss(0.4, ?a)item.43.author14item.43.quality-0.3comp.1.opt143comp.1.opt1.obs-0.42comp.1.pref2comp.1.opt2205comp.1.opt2.obs0.35student.priorGauss(0.0, ?0)decision.radius0.5obs.parametersGauss(0.0, ?obs)item.205.author6item.205.quality0.2student.14.abilityGauss(-0.6, ?a)student.s.ability item.i.authoritem.i.qualitycomp.c.opt1comp.c.opt1.obscomp.c.prefcomp.c.opt2comp.c.opt2.obsSICstudent.priordecision.radiusobs.parametersFigure 1: Plate notation (left) showing the independence structure of the IRT Models.
Example instan-tiated subnetwork (right) for the Gaussian parameterization.
Shaded rectangles are hyperparameters.Shaded ellipses are variables observable from a set of comparisons.radius (which here is 0.5), then he states no prefer-ence (i.e.
a preference of 0).
Otherwise he prefersthe item with the higher observed quality.The categorical parameterization is similar tothe Gaussian parameterization, with the followingdifferences.
Item quality is not continuous, butrather a member of the discrete set {1, 2, ...,?
}.The student ability distributions are categoricaldistributions over {1, 2, ...,?
}, and the studentability prior is a symmetric Dirichlet with strength?a.
Finally, the observed quality is the item qual-ity ?
plus an integer-valued noise ?
?
{1 ?
?, ...,??
?}.
Noise ?
is drawn from a discretizedzero-mean Gaussian with standard deviation ?obs.Specifically, Pr(?)
is proportional to the value ofthe probability density function of the zero-meanGaussian N (0, ?obs).We estimated the model parameters with Gibbssampling (Geman and Geman, 1984).
We foundthat Gibbs sampling converged quickly and con-sistently10 for both parameterizations.
Given theparameter estimates, we obtain a preference modelQ(pi|s1, s2) through the inference query:Pr(comp.c?.pref = pi | item.i?.author = s1,item.i?
?.author = s2,comp.c?.opt1 = i?,comp.c?.opt2 = i??
)10We ran 200 iterations with a burn-in of 50.where c?, i?, i??
are new comparison and item idsthat do not appear in the training data.We call these models Item-Response Theo-retic (IRT) models, to acknowledge their rootsin the psychometrics (Thurstone, 1927; Bradleyand Terry, 1952; Luce, 1959) and item-responsetheory (Hambleton, 1991; van der Linden andHambleton, 1996; Baker, 2001) literature.
Item-response theory is the basis of modern testingtheory and drives adaptive standardized tests likethe Graduate Record Exam (GRE).
In particular,the Gaussian parameterization of our IRT modelsstrongly resembles11 the Thurstone (Thurstone,1927) and Bradley-Terry-Luce (Bradley and Terry,1952; Luce, 1959) models of paired compari-son and the 1PL normal-ogive and Rasch (Rasch,1960) models of student testing.
From the test-ing perspective, we can view each comparison astwo students simultaneously posing a test questionto the other: ?Give me a translation of the sourcetext which is better than mine.?
The students cananswer the question correctly, incorrectly, or theycan provide a translation of analogous quality.
Anextra dimension of our models is judge noise, nota factor when modeling multiple-choice tests, forwhich the right answer is not subject to opinion.11These models are not traditionally expressed usinggraphical models, although it is not unprecedented (Mislevyand Almond, 1997; Mislevy et al, 1999).1421wmt10 wmt11 wmt12lp train test train test train testce 3166 2209 1706 3216 5969 6806fe 5918 2376 2556 4430 7982 5840ge 7422 3002 3708 5371 8106 6032se 8411 2896 1968 3684 3910 7376ec 10490 3048 8859 9016 13770 9112ef 5720 2242 3328 5758 7841 7508eg 10852 2842 5964 7032 10210 7191es 2962 2212 4768 6362 5664 8928Table 3: Dataset sizes for each competition track(number of comparisons).Figure 2: WMT10 model perplexities.
The per-plexity of the uniform preference model is 3.0 forall training sizes.8 ExperimentsWe organized the competition data as described atthe end of Section 4.
To compare the preferencemodels, we did the following:?
Randomly chose a subset of k compar-isons from the training set, for k ?
{100, 200, 400, 800, 1600, 3200}.12?
Trained the preference model on these com-parisons.?
Evaluated the perplexity of the trained modelon the test preferences, as described in Sec-tion 4.For each model and training size, we averagedthe perplexities from 5 trials of each competitiontrack.
We then plotted average perplexity as afunction of training size.
These graphs are shown12If k was greater than the total number of training com-parisons, then we took the entire set.Figure 3: WMT11 model perplexities.Figure 4: WMT12 model perplexities.in Figure 2 (WMT10)13, and Figure 4 (WMT12).For WMT10 and WMT11, the best models werethe IRT models, with the Gaussian parameteriza-tion converging the most rapidly and reaching thelowest perplexity.
For WMT12, in which refer-ence translations were excluded from the compe-tition, four models were nearly indistinguishable:the two IRT models and the two averaged Indepen-dent Student models.
This somewhat validates theorganizers?
decision to exclude the references, par-ticularly given WMT?s use of the BOJAR rankingheuristic (the nucleus of the Independent Studentmodels) for its official rankings.13Results for WMT10 exclude the German-English andEnglish-German tracks, since we used these to tune ourmodel hyperparameters.
These were set as follows.
TheDirichlet strength for each baseline was 1.
For IRT-Gaussian:?0 = 1.0, ?obs = 1.0, ?a = 0.5, and the decision radius was0.4.
For IRT-Categorical: ?
= 8, ?obs = 1.0, ?a = 0.5, andthe decision radius was 0.1422Figure 6: English-Czech WMT11 results (average of 5 trainings on 1600 comparisons).
Error bars(left) indicate one stddev of the estimated ability means.
In the heatmap (right), cell (s1, s2) is darker ifpreference model Q(pi|s1, s2) skews in favor of student s1, lighter if it skews in favor of student s2.Figure 5: WMT10 model perplexities (crowd-sourced versus expert training).The IRT models proved the most robust at han-dling judge noise.
We repeated the WMT10 ex-periment using the same test sets, but using theunfiltered crowdsourced comparisons (rather than?expert?14 comparisons) for training.
Figure 5shows the results.
Whereas the crowdsourcednoise considerably degraded the Geometric Inde-pendent Students model, the IRT models were re-markably robust.
IRT-Gaussian in particular cameclose to replicating the performance of GeometricIndependent Students trained on the much cleanerexpert data.
This is rather impressive, since thecrowdsourced judges agree only 46.6% of thetime, compared to a 65.8% agreement rate among14I.e., machine translation specialists.expert judges (Callison-Burch et al, 2010).Another nice property of the IRT models isthat they explicitly model student ability, so theyyield a natural ranking.
For training size 1600 ofthe WMT11 English-Czech track, Figure 6 (left)shows the mean student abilities learned by theIRT-Gaussian model.
The error bars show onestandard deviation of the ability means (recall thatwe performed 5 trials, each with a random trainingsubset of size 1600).
These results provide fur-ther insight into a case analyzed by (Lopez, 2012),which raised concern about the relative orderingof online-B, cu-bojar, and cu-marecek.
Accord-ing to IRT-Gaussian?s analysis of the data, thesethree students are so close in ability that any order-ing is essentially arbitrary.
Short of a full ranking,the analysis does suggest four strata.
Viewing oneof IRT-Gaussian?s induced preference models asa heatmap15 (Figure 6, right), four bands are dis-cernable.
First, the reference sentences are clearlythe darkest (best).
Next come students 2-7, fol-lowed by the slightly lighter (weaker) students 8-10, followed by the lightest (weakest) student 11.9 ConclusionWMT has faced a crisis of confidence lately, withresearchers raising (real and conjectured) issueswith its analytical methodology.
In this paper,we showed how WMT can restore confidence in15In the heatmap, cell (s1, s2) is darker if preference modelQ(pi|s1, s2) skews in favor of student s1, lighter if it skewsin favor of student s2.1423its conclusions ?
by shifting the focus from rank-ings to relative ability.
Estimates of relative ability(the expected head-to-head performance of systempairs over a probability space of judges and sourcetext) can be empirically compared, granting sub-stance to previously nebulous questions like:1.
Is my analysis better than your analysis?Rather than the current anecdotal approachto comparing competition analyses (e.g.
pre-senting example rankings that seem some-how wrong), we can empirically compare thepredictive power of the models on test data.2.
How much of an impact does judge noisehave on my conclusions?
We showedthat judge noise can have a significant im-pact on the quality of our conclusions, if weuse the wrong models.
However, the IRT-Gaussian appears to be quite noise-tolerant,giving similar-quality conclusions on bothexpert and crowdsourced comparisons.3.
How many comparisons should I elicit?Many of our preference models (includingIRT-Gaussian and Geometric IndependentStudents) are close to convergence at around1000 comparisons.
This suggests that we canelicit far fewer comparisons and still deriveconfident conclusions.
This is the first timea concrete answer to this question has beenprovided.ReferencesF.B.
Baker.
2001.
The basics of item response theory.ERIC.Ondej Bojar, Milos?
Ercegovc?evic?, Martin Popel, andOmar Zaidan.
2011.
A grain of salt for the wmtmanual evaluation.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages1?11, Edinburgh, Scotland, July.
Association forComputational Linguistics.Ralph Allan Bradley and Milton E Terry.
1952.
Rankanalysis of incomplete block designs: I. the methodof paired comparisons.
Biometrika, 39(3/4):324?345.C.
Callison-Burch, P. Koehn, C. Monz, K. Peterson,M.
Przybocki, and O.F.
Zaidan.
2010.
Findings ofthe 2010 joint workshop on statistical machine trans-lation and metrics for machine translation.
In Pro-ceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR, pages 17?53.
Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation.S.
Geman and D. Geman.
1984.
Stochastic relaxation,gibbs distributions, and the bayesian restoration ofimages.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 6(6):721?741.R.K.
Hambleton.
1991.
Fundamentals of item re-sponse theory, volume 2.
Sage Publications, Incor-porated.D.
Koller and N. Friedman.
2009.
Probabilistic graph-ical models: principles and techniques.
MIT press.S.
Kullback and R.A. Leibler.
1951.
On informationand sufficiency.
The Annals of Mathematical Statis-tics, 22(1):79?86.Adam Lopez.
2012.
Putting human assessments ofmachine translation systems in order.
In Proceed-ings of WMT.R.
Ducan Luce.
1959.
Individual Choice Behavior aTheoretical Analysis.
John Wiley and sons.R.J.
Mislevy and R.G.
Almond.
1997.
Graphical mod-els and computerized adaptive testing.
UCLA CSETechnical Report 434.R.J.
Mislevy, R.G.
Almond, D. Yan, and L.S.
Stein-berg.
1999.
Bayes nets in educational assessment:Where the numbers come from.
In Proceedingsof the fifteenth conference on uncertainty in artifi-cial intelligence, pages 437?446.
Morgan KaufmannPublishers Inc.G.
Rasch.
1960.
Studies in mathematical psychology:I. probabilistic models for some intelligence and at-tainment tests.Louis L Thurstone.
1927.
A law of comparative judg-ment.
Psychological review, 34(4):273?286.W.J.
van der Linden and R.K. Hambleton.
1996.Handbook of modern item response theory.Springer.1424
