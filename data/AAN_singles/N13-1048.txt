Proceedings of NAACL-HLT 2013, pages 450?459,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsTraining MRF-Based Phrase Translation Models using Gradient AscentJianfeng GaoMicrosoft ResearchRedmond, WA, USAjfgao@microsoft.comXiaodong HeMicrosoft ResearchRedmond, WA, USAxiaohe@microsoft.comAbstractThis paper presents a general, statisticalframework for modeling phrase translationvia Markov random fields.
The model al-lows for arbituary features extracted from aphrase pair to be incorporated as evidence.The parameters of the model are estimatedusing a large-scale discriminative trainingapproach that is based on stochastic gradi-ent ascent and an N-best list based expectedBLEU as the objective function.
The modelis easy to be incoporated into a standardphrase-based statistical machine translationsystem, requiring no code change in theruntime engine.
Evaluation is performed ontwo Europarl translation tasks, German-English and French-English.
Results showthat incoporating the Markov random fieldmodel significantly improves the perfor-mance of a state-of-the-art phrase-basedmachine translation system, leading to again of  0.8-1.3 BLEU points.1 IntroductionThe phrase translation model, also known as thephrase table, is one of the core components of aphrase-based statistical machine translation (SMT)system.
The most common method of constructingthe phrase table takes a two-phase approach.
First,the bilingual phrase pairs are extracted heuristical-ly from an automatically word-aligned training da-ta.
The second phase is parameter estimation,where each phrase pair is assigned with somescores that are estimated based on counting ofwords or phrases on the same word-aligned train-ing data.There has been a lot of research on improvingthe quality of the phrase table using more princi-pled methods for phrase extraction (e.g., Lamberand Banchs 2005), parameter estimation (e.g.,Wuebker et al2010; He and Deng 2012), or both(e.g., Marcu and Wong 2002; Denero et al2006).The focus of this paper is on the parameter estima-tion phase.
We revisit the problem of scoring aphrase translation pair by developing a new phrasetranslation model based on Markov random fields(MRFs) and large-scale discriminative training.We strive to address the following three primaryconcerns.First of all, instead of parameterizing a phrasetranslation pair using a set of scoring functions thatare learned independently (e.g., phrase translationprobabilities and lexical weights) we use a general,statistical framework in which arbitrary featuresextracted from a phrase pair can be incorporated tomodel the translation in a unified way.
To this end,we propose the use of a MRF model.Second, because the phrase model has to workwith other component models in an SMT system inorder to produce good translations and the qualityof translation is measured via BLEU score, it is de-sirable to optimize the parameters of the phrasemodel jointly with other component models withrespect to an objective function that is closely re-lated to the evaluation metric under consideration,i.e., BLEU in this paper.
To this end, we resort to alarge-scale discriminative training approach, fol-lowing the pioneering work of Liang et al(2006).Although there are established methods of tuning ahandful of features on small training sets, such asthe MERT method (Och 2003), the development ofdiscriminative training methods for millions of fea-tures on millions of sentence pairs is still an ongo-ing area of research.
A recent survey is due toKoehn (2010).
In this paper we show that by usingstochastic gradient ascent and an N-best list based450expected BLEU as the objective function, large-scale discriminative training can lead to significantimprovements.The third primary concern is the ease of adop-tion of the proposed method.
To this end, we use asimple and well-established learning method, en-suring that the results can be easily reproduced.We also develop the features for the MRF model insuch a way that the resulting model is of the sameformat as that of a traditional phrase table.
Thus,the model can be easily incorporated into a stand-ard phrase-based SMT system, requiring no codechange in the runtime engine.In the rest of the paper, Section 2 presents theMRF model for phrase translation.
Section 3 de-scribes the way the model parameters are estimated.Section 4 presents the experimental results on twoEuroparl translation tasks.
Section 5 reviews pre-vious work that lays the foundation of this study.Section 6 concludes the paper.2 ModelThe traditional translation models are directionalmodels that are based on conditional probabilities.As suggested by the noisy-channel model for SMT(Brown et al1993):?
= argmax| = argmax()| (1)The Bayes rule leads us to invert the conditioningof translation probability from a foreign (source)sentence  to an English (target) translation .However, in practice, the implementation ofstate-of-the-art phrase-based SMT systems uses aweighted log-linear combination of several models?
(,,)  including the logarithm of the phraseprobability (and the lexical weight) in source-to-target and target-to-source directions (Och and Ney2004)?
= argmax ?
?(,,)   (2)= argmax(,)where   in ?
(,,)  is a hidden structure thatbest derives  from , called the Viterbi derivationafterwards.
In phrase-based SMT,  consists of (1)the segmentation of the source sentence intophrases, (2) the segmentation of the target sentenceinto phrases, and (3) an alignment between thesource and target phrases.In this paper we use Markov random fields(MRFs) to model the joint distribution (, )over a source-target translation phrase pair (, ),parameterized by .
Different from the directionaltranslation models, as in Equation (1), the MRFmodel is undirected, which we believe upholds thespirit of the use of bi-directional translation proba-bilities under the log-linear framework.
That is, theagreement or the compatibility of a phrase pair ismore effective to score translation quality than adirectional translation probability which is mod-eled based on an imagined generative story does.2.1 MRFMRFs, also known as undirected graphical models,are widely used in modeling joint distributions ofspatial or contextual dependencies of physical phe-nomena (Bishop 2006).
A Markov random field isconstructed from a graph  .
The nodes of thegraph represent random variables, and edges definethe independence semantics between the randomvariables.
An MRF satisfies the Markov property,which states that a node is independent of all of itsnon-neighbors, defined by the clique configura-tions of .
In modeling a phrase translation pair,we define two types of nodes, (1) two phrase nodesand (2) a set of word nodes, each for a word in the-se phrases, such as the graph in Figure 1.
Let usdenote a clique by  and the set of variables in thatclique by ,  .
Then, the joint distribution overthe random variables in  is defined as(, ) = 	?
(, ;)() , (3)where  = , ?
, || ,  = , ?
, ||  and ()  isthe set of cliques in , and each (, ;) is anon-negative potential function defined over aclique  that measures the compatibility of the var-iables in ,  is a set of parameters that are usedwithin the potential function.
  in Equation (3),sometimes called the partition function, is a nor-malization constant and is given by = ?
?
?
(, ;)()   (4)= ?
?(, ) ,which ensures that the distribution (, ) givenby Equation (3) is correctly normalized.
The pres-451ence of  is one of the major limitations of MRFsbecause it is generally not feasible to compute dueto the exponential number of terms in the summa-tion.
However, we notice that   is a global con-stant which is independent of  and .
Therefore, inranking phrase translation hypotheses, as per-formed by the decoder in SMT systems, we candrop   and simply rank each hypothesis by itsunnormalized joint probability.
In our implementa-tion, we only store in the phrase table for eachtranslation pair ,  its unnormalized probability,i.e.,(, ) as defined in Equation (4).It is common to define MRF potential functionsof the exponential form as , ; =exp (), where  is a real-valued featurefunction over clique  and  is the weight of thefeature function.
In phrase-based SMT systems, thesentence-level translation probability from   to is decomposed as the product of a set of phrasetranslation probabilities.
By dropping the phrasesegmentation and distortion model components, wehave(|) ?
max(|,) (5)(|,) = ?
(|)(,)? ,where   is the Viterbi derivation.
Similarly, thejoint probability (,) can be decomposed as, ?
max(,,) (6),, = ?
(, )(,)??
?
log, ,??
?
?
()?
((,)),?= ?
 ?(, ),?which is essentially proportional to a weighted lin-ear combination of a set of features.To instantiate an MRF model, one needs to de-fine a graph structure representing the translationdependencies between source and target phrases,and a set of potential functions over the cliques ofthis graph.2.2 Cliques and Potential FunctionsThe MRF model studied in this paper is construct-ed from the graph  in Figure 1.
It contains twotypes of nodes, including two phrase nodes for thesource and target phrases respectively and wordnodes, each for a word in these phrases.
Thecliques and their corresponding potential functions(or features) attempt to abstract the idea behindthose translation models that have been proved ef-fective for machine translation in previous work.
Inthis study we focus on three types of cliques.First, we consider cliques that contain twophrase nodes.
A potential function over such aclique captures phrase-to-phrase translation de-pendencies similar to the use the bi-directionaltranslation models in phrase-based SMT systems.The potential is defined as ,  = (, ),where the feature (, ), called the phrase-pairfeature, is an indicator function whose value is 1 if  is target phrase and  is source phrase, and 0 oth-erwise.
While the conditional probabilities in a di-rectional translation model are estimated using rel-ative frequencies of phrase pairs extracted fromword-aligned parallel sentences, the parameter ofthe phrase-pair function  is learned discrimina-tively, as we will describe in Section 3.Second, we consider cliques that contain twoword nodes, one in source phrase and the other intarget phrase.
A potential over such a clique cap-tures word-to-word translation dependencies simi-lar to the use the IBM Model 1 for lexicalweighting in phrase-based SMT systems (Koehn etal.
2003).
The potential function is defined as ,  = (, ), where the feature (, ),called the word-pair feature, is an indicator func-tion whose value is 1 if  is a word in target phrase  and f is a word in source phrase , and 0 other-wise.The third type of cliques contains three wordnodes.
Two of them are in one language and thethird in the other language.
A potential over such aclique is intended to capture inter-word dependen-Figure 1: A Markov random field model for phrasetranslation of  = ,  and  = ,,.452cies for selecting word translations.
The potentialfunction is inspired by the triplet lexicon model(Hasan et al2008) which is based on lexicalizedtriplets (, , ?)
.
It can be understood as twosource (or target) words triggering one target (orsource) word.
The potential function is defined as , ,  = (, , ), where the feature (, , ), called the triplet feature, is an indica-tor function whose value is 1 if  is a word in tar-get phrase  and  and ?
are two different wordsin source phrase , and 0 otherwise.For any clique  that contains nodes in only onelanguage we assume that  = 1 for all settingof the clique, which has no impact on scoring aphrase pair.
One may wish to define a potentialover cliques containing a phrase node and wordnodes in target language, which could act as a formof target language model.
One may also add edgesin the graph so as to define potentials that capturemore sophisticated translation dependencies.
Theoptimal potential set could vary among differentlanguage pairs and depend to a large degree uponthe amount and quality of training data.
We leave acomprehensive study of features to future work.3 TrainingThis section describes the way the parameters ofthe MRF model are estimated.
Although MRFs areby nature generative models, it is not always ap-propriate to train the parameters using convention-al likelihood based approaches mainly for two rea-sons.
The first is due to the difficulty in computingthe partition function in Equation (4), especially ina task of our scale.
The second is due to the metricdivergence problem (Morgan et al2004).
That is,the maximum likelihood estimation is unlikely tobe optimal for the evaluation metric under consid-eration, as demonstrated on a variety of tasks in-cluding machine translation (Och 2003) and infor-mation retrieval (Metzler and Croft 2005; Gao etal.
2005).
Therefore, we propose a large-scale dis-criminative training approach that uses stochasticgradient ascent and an N-best list based expectedBLEU as the objective function.We cast machine translation as a structuredclassification task (Liang et al2006).
It maps aninput source sentence   to an output pair (,)where   is the output target sentence and   theViterbi derivation of  .
  is assumed to be con-structed during the translation process.
In phrase-based SMT,   consists of a segmentation of thesource and target sentences into phrases and analignment between source and target phrases.We also assume that translations are modeledusing a linear model parameterized by a vector .Given a vector (,,) of feature functions on(,,) , and assuming   contains a componentfor each feature, the output pair (,) for a giveninput  are selected using the argmax decision rule(?,?)
= argmax(,)(,,) (7)In phrase-based SMT, computing the argmax ex-actly is intractable, so it is performed approximate-ly by beam decoding.In a phrase-based SMT system equipped by aMRF-based phrase translation model, the parame-ters we need to learn are  = (,), where  is avector of a handful parameters used in the log-linear model of Equation (2), with one weight foreach component model; and  is a vector contain-ing millions of weights, each for one feature func-tion in the MRF model of Equation (3).
Our meth-od takes three steps to learn :1.
Given a baseline phrase-based SMT systemand a pre-set , we generate for each sourcesentence in training data an N-best list oftranslation hypotheses.2.
We fix , and optimize  with respect to anobjective function on training data.3.
We fix , and optimize  using MERT (Och2003) to maximize the BLEU score on de-velopment data.Now, we describe Steps 1 and 2 in detail.3.1 N-Best GenerationGiven a set of source-target sentence pairs as train-ing data  ,, = 1?
, we use the baselinephrase-based SMT system to generate for eachsource sentence   a list of 100-best candidatetranslations, each translation   coupled with itsViterbi derivation  , according to Equation (7).We denote the 100-best set by GEN().
Then, eachoutput pair ,  is labeled by a sentence-levelBLEU score, denoted by sBLEU, which is comput-ed according to Equation (8) (He and Deng 2012),sBLEU(,) =   ?
?
log! , (8)453where   is the reference translation, and ! , =1?4, are precisions of n-grams.
While precisionsof lower order n-grams, i.e., ! and ! , are com-puted directly without any smoothing, matchingcounts for higher order n-grams could be sparse atthe sentence level and need to be smoothed as! = #("#$?% &#") + '!#(&#") + ' , for  = 3,4where ' is a smoothing parameter and is set to 5,and !  is the prior value of ! , whose value iscomputed as ! = !/! for  = 3 and 4.
  in Equation (8) is the sentence-level brevitypenalty, computed as   = exp (1 ? )
*, whichdiffers from its corpus-level counterpart (Papineniet al2002) in two ways.
First, we use a non-clipped  , which leads to a better approximationto the corpus-level BLEU computation because theper-sentence   might effectively exceed unity incorpus-level BLEU computation, as discussed inChiang et al(2008).
Second, the ratio between thelength of reference sentence r and the length oftranslation hypothesis c is scaled by a factor ) suchthat the total length of the references on trainingdata equals that of the 1-best translation hypothe-ses produced by the baseline SMT system.
In ourexperiments, the value of ) is computed, on the N-best training data, as the ratio between the totallength of the references and that of the 1-besttranslation hypothesesIn our experiments we find that using sBLEUdefined above leads to a small but consistent im-provement over other variations of sentence-levelBLEU proposed previously (e.g., Liang et al2006).
In particular, the use of the scaling factor )in computing    makes    of the baseline?s 1-best output close to perfect on training data, andhas an effect of forcing the discriminative trainingto improve BLEU by improving n-gram precisionsrather than by improving brevity penalty.3.2 Parameter EstimationWe use an N-best list based expected BLEU, a var-iant of that in Rosti et al(2011), as the objectivefunction for parameter optimization.
Given the cur-rent model  , the expected BLEU, denoted byxBLEU(), over one training sample i.e., a labeledN-best list GEN() generated from a pair of sourceand target sentences (,), is defined asxBLEU= ?
|sBLEU(,)?
(!)
, (9)where sBLEU is the sentence-level BLEU, definedin Equation (8), and | is a normalized trans-lation probability from   to   computed usingsoftmax as| = "#$(%&'!,)?
"#$(%&'!,) , (10)where.
 is the translation score accordingto the current model , =  ?
,, (11)+?
 ?(, )(,)? .The right hand side of (11) contains two terms.
Thefirst term is the score produced by the baseline sys-tem, which is fixed during phrase model training.The second term is the translation score producedby the MRF model, which is updated after eachtraining sample during training.
Comparing Equa-tions (2) and (11), we can view the MRF model yetanother component model under the log linearmodel framework with its 	 being set to 1.Given the objective function, the parameters ofthe MRF model are optimized using stochasticgradient ascent.
As shown in Figure 2, we gothrough the training set + times, each time is con-sidered an epoch.
For each training sample, we up-date the model parameters as') = &*+ + , ?
-(&*+) (12)where , is the learning rate, and the gradient - iscomputed as = ,xBLEU(),(13)1 Initialize , assuming  is fixed during training2 For t = 1?T (T = the total number of iterations)3    For each training sample (labeled 100-best list)4 Compute | for each translation hypothe-sis  based on the current model 	 = (,)5Update the model via  =  +?
(),whereis the learning rate and  the gradientcomputed according to Equations (12) and (13)Figure 2: The algorithm of training a MRF-basedphrase translation model.454= ?
U(,)|(,,)(,) ,where U(,) = sBLEU,?
xBLEU.Two considerations regarding the developmentof the training method in Figure 2 are worth men-tioning.
They significantly simplify the trainingprocedure without sacrificing much the quality ofthe trained model.
First, we do not include a regu-larization term in the objective function becausewe find early stopping and cross valuation more ef-fective and simpler to implement.
In experimentswe produce a MRF model after each epoch, andtest its quality on a development set by first com-bining the MRF model with other baseline compo-nent models via MERT and then examining BLEUscore on the development set.
We performed train-ing for T epochs (+ = 100 in our experiments) andthen pick the model with the best BLEU score onthe development set.
Second, we do not use theleave-one-out method to generate the N-best lists(Wuebker et al2010).
Instead, the models used inthe baseline SMT system are trained on the sameparallel data on which the N-best lists are generat-ed.
One may argue that this could lead to over-fitting.
For example, comparing to the translationson unseen test data, the generated translation hy-potheses on the training set are of artificially highquality with the derivations containing artificiallylong phrase pairs.
The discrepancy between thetranslations on training and test sets could hurt thetraining performance.
However, we found in ourexperiments that the impact of over-fitting on thequality of the trained MRF models is negligible1.4 ExperimentsWe conducted our experiments on two Europarltranslation tasks, German-to-English (DE-EN) andFrench-to-English (FR-EN).
The data sets are pub-lished for the shared task in NAACL 2006 Work-shop on Statistical Machine Translation (WMT06)(Koehn and Monz 2006).For DE-EN, the training set contains 751K sen-tence pairs, with 21 words per sentence on average.The official development set used for the shared1As pointed out by one of the reviewers, the fact that ourtraining works fine without leave-one-out is probably due tothe small phrase length limit (i.e., 4) we used.
If a longerphrase limit (e.g., 7) is used the result might be different.
Weleave it to future work.task contains 2000 sentences.
In our experiments,we used the first 1000 sentences as a developmentset for MERT training and optimizing parametersfor discriminative training, such as learning rateand the number of iterations.
We used the rest1000 sentences as the first test set (TEST1).
Weused the WMT06 test data as the second test set(TEST2), which contains 2000 sentences.For FR-EN, the training set contains 688K sen-tence pairs, with 21 words per sentence on average.The development set contains 2000 sentences.
Weused 2000 sentences from the WMT05 shared taskas TEST1, and the 2000 sentences from theWMT06 shared task as TEST2.Two baseline phrase-based SMT systems, eachfor one language pair, are developed as follows.These baseline systems are used in our experi-ments both for comparison purpose and for gener-ating N-best lists for discriminative training.
First,we performed word alignment on the training setusing a hidden Markov model with lexicalized dis-tortion (He 2007), then extracted the phrase tablefrom the word aligned bilingual texts (Koehn et al2003).
The maximum phrase length is set to four.Other models used in a baseline system include alexicalized reordering model, word count andphrase count, and a trigram language model trainedon the English training data provided by theWMT06 shared task.
A fast beam-search phrase-based decoder (Moore and Quirk 2007) is used andthe distortion limit is set to four.
The decoder ismodified so as to output the Viterbi derivation foreach translation hypothesis.The metric used for evaluation is case insensi-tive BLEU score (Papineni et al2002).
We alsoperformed a significance test using the paired t-test.
Differences are considered statistically signif-icant when the p-value is less than 0.05.
Table 12The official results are accessible athttp://www.statmt.org/wmt06/shared-task/results.htmlSystems DE-EN (TEST2) FR-EN (TEST2)Rank-1 system 27.3 30.8Rank-2 system 26.0 30.7Rank-3 system 25.6 30.5Our baseline 26.0 31.4Table 1: Baseline results in BLEU.
The results oftop ranked systems are reported in Koehn andMonz (2006)2.455presents the baseline results.
The performance ofour phrase-based SMT systems compares favora-bly to the top-ranked systems, thus providing a fairbaseline for our research.4.1 ResultsTable 2 shows the main results measured in BLEUevaluated on TEST1 and TEST2.Row 1 is the baseline system.
Rows 2 to 5 arethe systems enhanced by integrating different ver-sions of the MRF-based phrase translation model.These versions, labeled as MRFf, are trained usingthe method described in Section 3, and differ in thefeature classes (which are specified by the sub-script f) incorporated in the MRF-based model.
Inthis study we focused on three classes of features,as described in Section 2, phrase-pair features (p),word-pair features (t) and triplet features (tp).
Thestatistics for these features are given in Table 3.Table 2 shows that all the MRF models lead to asubstantial improvement over the baseline systemacross all test sets, with a statistically significantmargin from 0.8 to 1.3 BLEU points.
As expected,the best phrase model incorporates all of the threeclasses of features (MRFp+t+tp in Row 2).
We alsofind that both MRFp and MRFt, although usingonly one class of features, perform quite well.
InTEST2 of DE-EN and TEST1 of FR-EN, they arein a near statistical tie with MRFp+t and MRFp+t+tp.The result suggests that while the MRF models arevery effective in modeling phrase translations, thefeatures we used in this study may not fully realizethe potential of the modeling technology.We also measured the sensitivity of the discrim-inative training method to different initializationsand training parameters.
Results show that ourmethod is very robust.
All the MRF models in Ta-ble 2 are trained by setting the initial feature vectorto zero, and the learning rate ,=0.01.
Figure 3 plotsthe BLEU score on development sets as a functionof the number of epochs t. The BLEU score im-proves quickly in the first 5 epochs, and then eitherremains flat, as on the DE-EN data, or keeps in-creasing but in a much slower pace, as on the FR-EN data.4.2  Comparing Objective FunctionsThis section compares different objective functionsfor discriminative training.
As shown in Table 4,xBLEU is compared to three widely used convexloss functions, i.e., hinge loss, logistic loss, and logloss.
The hinge loss and logistic loss take into ac-count only two hypotheses among an N-best listGEN: the one with the best sentence-level BLEUscore with respect to its reference translation, de-noted by (?,?)
, called the oracle candidatehenceforth, and the highest scored incorrect candi-date according to the current model, denoted by(,), defined as# Systems DE-EN FR-ENTEST1 TEST2 TEST1 TEST21 Baseline 26.0 26.0 31.3 31.42 MRFp+t+tp 27.3 ?
27.1 ?
32.4 ?
32.2 ?3 MRFp+t 27.2 ?
26.9 ?
32.3 ?
32.0 ?4 MRFp 26.8 ??
26.7 ??
32.2 ?
31.8 ?
?5 MRFt 26.8 ??
26.8 ?
32.1 ?
31.9 ?
?Table 2: Main results (BLEU scores) of MRF-based phrase translation models with differentfeature classes.
The superscripts ?
and ?
indicatestatistically significant difference (p < 0.05)from Baseline and  MRFp+t+tp, respectively.Feature classes # of features (weights)DE-EN FR-ENphrase-pair features (p) 2.5M 2.3Mword-pair features (t) 12.2M 9.7Mtriplet features (tp) 13.4M 13.8MTable 3: Statistics of the features used in build-ing MRF-based phrase translation models.Figure 3: BLEU score on development data (yaxis) for DE-EN (top) and FR-EN (bottom) as afunction of the number of epochs (x axis).25.826.026.226.426.626.827.00 20 40 60 80 10031.231.331.431.531.631.731.831.90 20 40 60 80 100456(,) =argmax,?
(!)\{(?,?)}-(,,),where-(. )
is defined in Equation (11).
Let .
= ,?,??
,, .
The hinge lossunder the N-best re-ranking framework is definedas max (0,1 ?
.)
.
It is easy to verify that totrain a model using this version of hinge loss, theupdate rule of Equation (12) can be rewritten as') = /&*+ ,                   if 0 = ?&*+ + ,., $?123 (14)where 0  is the highest scored candidate in GEN .Following Shalev-Shwartz (2012), by setting=1 , we reach the Perceptron-based training algo-rithm that has been widely used in previous studiesof discriminative training for SMT (e.g., Liang etal.
2006; Simianer et al2012).The logistic loss log(1 + exp(?.))
leads toan update rule similar to that of hinge loss') = /&*+ ,                              if 0 = ?&*+ + ,(.
)., $?123 (15)where  = 1/(1 + exp(	)).The log loss is widely used when a probabilisticinterpretation of the trained model is desired, as inconditional random fields (CRFs) (Lafferty et al2001).
Given a training sample, log loss is definedas log?|, where ?
is the oracle translationhypothesis with respect to its reference translation.
?| is computed as Equation (10).
So, unlikehinge loss and logistic loss, log loss takes into ac-count the distribution over all hypotheses in an N-best list.The results in Table 4 suggest that the objectivefunctions that take into account the distributionover all hypotheses in an N-best list (i.e., xBLEUand log loss) are more effective than the ones thatdo not.
xBLEU, although it is a non-concave func-tion, significantly outperforms the others because itis more closely coupled with the evaluation metricunder consideration (i.e., BLEU).5 Related WorkAmong the attempts to learning phrase translationprobabilities that go beyond pure counting ofphrases on word-aligned corpora, Wuebker et al(2010) and He and Deng (2012) are most related toour work.
The former find phrase alignment direct-ly on training data and update the translation prob-abilities based on this alignment.
The latter learnphrase translation probabilities discriminatively,which is similar to our approach.
But He andDeng?s method involves multiple stages, and is notstraightforward to implement3.
Our method differsfrom previous work in its use of a MRF model thatis simple and easy to understand, and a stochasticgradient ascent based training method that is effi-cient and easy to implement.A large portion of previous studies on discrimi-native training for SMT either use a handful of fea-tures or use small training sets of a few thousandsentences (e.g., Och 2003; Shen et al2004;Watanabe et al2007; Duh and Kirchhoff 2008;Chiang et al2008; Chiang et al2009).
Althoughthere is growing interest in large-scale discrimina-tive training (e.g., Liang et al2006; Tillmann andZhang 2006; Blunsom et al2008; Hopkins andMay 2011; Zhang et al2011), only recently doessome improvement start to be observed (e.g.,Simianer et al2012; He and Deng 2012).
It stillremains uncertain if the improvement is attributedto new features, new training algorithms, objectivefunctions, or simply large amounts of training data.We show empirically the importance of objectivefunctions.
Gimple and Smith (2012) also analyzeobjective functions, but more from a theoreticalviewpoint.The proposed MRF-based translation model isinspired by previous work of applying MRFs forinformation retrieval (Metzler and Croft 2005),query expansion (Metzler et al2007; Gao et al2012) and POS tagging (Haghighi and Klein 2006).3For comparison, the method of He and Deng (2012) alsoachieved very similar results to ours using the same experi-mental setting, as described in Section 4.# ObjectivefunctionsDE-EN FR-ENTEST1TEST2 TEST1 TEST21 xBLEU 27.2 26.9 32.3 32.02 hinge loss 26.4?
26.2?
31.8?
31.5?3 logistic loss 26.3?
26.2?
31.7?
31.5?4 log loss 26.5?
26.2?
32.1 31.7?Table 4: BLEU scores of MRF-based phrase trans-lation models trained using different objectivefunctions.
The MRF models use phrase-pair andword-pair features.
The superscript ?
indicatesstatistically significant difference (p < 0.05) fromxBLUE.457Another undirected graphical model that has beenmore widely used for NLP is a CRF (Lafferty et al2001).
An MRF differs from a CRF in that its par-tition function is no longer observation dependent.As a result, learning an MRF is harder than learn-ing a CRF using maximum likelihood estimation(Haghighi and Klein 2006).
Our work provides analternative learning method that is based on dis-criminative training.6 ConclusionsThe contributions of this paper are two-fold.
First,we present a general, statistical framework formodeling phrase translations via MRFs, where dif-ferent features can be incorporated in a unifiedmanner.
Second, we demonstrate empirically thatthe parameters of the MRF model can be learnedeffectively using a large-scale discriminative train-ing approach which is based on stochastic gradientascent and an N-best list based expected BLEU asthe objective function.In future work we strive to fully realize the po-tential of the MRF model by developing featuresthat can capture more sophisticated translation de-pendencies that those used in this study.
We willalso explore the use of MRF-based translationmodels for translation systems that go beyond sim-ple phrases, such as hierarchical phrase based sys-tems (Chiang 2005) and syntax-based systems(Galley et al2004).ReferencesBishop, C. M. 2006.
Patten recognition and ma-chine learning.
Springer.Blunsom, P., Cohn, T., and Osborne, M. 2008.
Adiscriminative latent variable models for statisti-cal machine translation.
In ACL-HLT.Brown, P. F., Della Pietra, S. A., Della Pietra, V. J.,and Mercer, R. L. 1993.
The mathematics of sta-tistical machine translation: parameter estimation.Computational Linguistics, 19(2): 263-311.Chiang, D. 2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In ACL,pp.
263-270.Chiang, D., Knight, K., and Wang, W. 2009.11,001 new features for statistical machine trans-lation.
In NAACL-HLT.Chiang, D., Marton, Y., and Resnik, P. 2008.Online large-margin training of syntactic andstructural translation features.
In EMNLP.DeNero, J., Gillick, D., Zhang, J., and Klein, D.2006.
Why generative phrase models underper-form surface heuristics.
In Workshop on Statisti-cal Machine Translation, pp.
31-38.Duh, K., and Kirchhoff, K. 2008.
Beyond log-linear models: boosted minimum error rate train-ing for n-best ranking.
In ACL.Galley, M., Hopkins, M., Knight, K., Marcu, D.2004.
What's in a translation rule?
In HLT-NAACL, pp.
273-280.Gao, J., Xie, S., He, X., and Ali, A.
2012.
Learninglexicon models from search logs for query ex-pansion.
In EMNLP-CoNLL, pp.
666-676.Gao, J., Qi, H., Xia, X., and Nie, J-Y.
2005.
Lineardiscriminant model for information retrieval.
InSIGIR, pp.
290-297.Gimpel, K., and Smith, N. A.
2012.
Structuredramp loss minimization for machine translation.In NAACL-HLT.Haghighi, A., and Klein, D. 2006.
Prototype-drivenlearning for sequence models.
In NAACL.Hasan, S., Ganitkevitch, J., Ney, H., and Andres-Fnerre, J.
2008.
Triplet lexicon models for statis-tical machine translation.
In EMNLP, pp.
372-381.He, X.
2007.
Using word-dependent transitionmodels in HMM based word alignment for sta-tistical machine translation.
In Proc.
of the Se-cond ACL Workshop on Statistical MachineTranslation.He, X., and Deng, L. 2012.
Maximum expectedbleu training of phrase and lexicon translationmodels.
In ACL, pp.
292-301.Hopkins, H., and May, J.
2011.
Tuning as ranking.In EMNLP.Koehn, P. 2010.
Statistical machine translation.Cambridge University Press.Koehn, P., and Monz, C. 2006.
Manual and auto-matic evaluation of machine translation betweenEuropean languages.
In Workshop on StatisticalMachine Translation, pp.
102-121.458Koehn, P., Och, F., and Marcu, D. 2003.
Statisticalphrase-based translation.
In HLT-NAACL, pp.127-133.Lafferty, J., McCallum, A., and Pereira, F. 2001.Conditional random fields: probablistic modelsfor segmenting and labeling sequence data.
InICML.Lambert, P., and Banchs, R.E.
2005.
Data inferredmulti-word expressions for statistical machinetranslation.
In MT Summit X, Phuket, Thailand.Liang, P., Bouchard-Cote, A. Klein, D., andTaskar, B.
2006.
An end-to-end discriminativeapproach to machine translation.
In COLING-ACL.Marcu, D., and Wong, W. 2002.
A phrase-based,joint probability model for statistical machinetranslation.
In EMNLP.Metzler, D., and Croft, B.
2005.
A markov randomfield model for term dependencies.
In SIGIR, pp.472-479.Metzler, D., and Croft, B.
2007.
Latent conceptexpansion using markov random fields.
InSIGIR, pp.
311-318.Morgan, W., Greiff, W., and Henderson, J.
2004.Direct maximization of average precision byhill-climbing with a comparison to a maximumentropy approach.
Technical report.
MITRE.Moore, R., and Quirk, C. 2007.
Faster beam-searchdecoding for phrasal statistical machinetranslation.
In MT Summit XI.Och, F., and Ney, H. 2004.
The alignment templateapproach to statistical machine translation.
Com-putational Linguistics, 29(1): 19-51.Och, F. 2003.
Minimum error rate training instatistical machine translation.
In ACL, pp.
160-167.Papinein, K., Roukos, S., Ward, T., and Zhu W-J.2002.
BLEU: a method for automatic evaluationof machine translation.
In ACL.Rosti, A-V., Hang, B., Matsoukas, S., andSchwartz, R. S. 2011.
Expected BLEU trainingfor graphs: bbn system description for WMTsystem combination task.
In Workshop onStatistical Machine Translation.Shalev-Shwartz, Shai.
2012.
Online learning andonline convex optimization.
Foundations andTrends in Machine Learning, 4(2):107-194.Shen, L., Sarkar, A., and Och, F. 2004.Discriminative reranking for machinetranslation.
In HLT/NAACL.Simianer, P., Riezler, S., and Dyer, C. 2012.
Jointfeature selection in distributed stochasic learningfor large-scale discriminative training in SMT.
InACL, pp.
11-21.Tillmann, C., and Zhang, T. 2006.
Adiscriminative global training algorithm forstatistical MT.
In COLING-ACL.Watanabe, T., Suzuki, J., Tsukada, H., and Isozaki,H.
2007.
Online large-margin training forstatistical machine translation.
In EMNLP.Wuebker, J., Mauser, A., and Ney, H. 2010.Training phrase translation models with leaving-one-out.
In ACL, pp.
475-484.Zhang, Y., Deng, L., He, X., and Acero, A., 2011.A Novel decision function and the associateddecision-feedback learning for speechtranslation, in ICASSP.459
