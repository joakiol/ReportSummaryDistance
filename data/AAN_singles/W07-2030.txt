Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 149?152,Prague, June 2007. c?2007 Association for Computational LinguisticsFICO: Web Person Disambiguation Via Weighted Similarityof Entity ContextsPaul KalmarFair Isaac Corporation3661 Valley Centre Dr.San Diego, CA 92130 USAPaulKalmar@FairIsaac.comMatthias BlumeFair Isaac Corporation3661 Valley Centre Dr.San Diego, CA 92130 USAMatthiasBlume@FairIsaac.comAbstractEntity disambiguation resolves the many-to-many correspondence between mentionsof entities in text and unique real-world en-tities.
Fair Isaac?s entity disambiguationuses language-independent entity contextto agglomeratively resolve mentions withsimilar names to unique entities.
This pa-per describes Fair Isaac?s automatic entitydisambiguation capability and assesses itsperformance on the SemEval 2007 WebPeople Search task.1 IntroductionWe use the term entity to mean a specific person orobject.
A mention is a reference to an entity suchas a word or phrase in a document.
Taken to-gether, all mentions that refer to the same real-world object model that entity (Mitchell et al2004).
Entity disambiguation inherently involvesresolving many-to-many relationships.
Multipledistinct strings may refer to the same entity.
Si-multaneously, multiple identical mentions refer todistinct entities (Bagga and Baldwin, 1998).Fair Isaac?s entity disambiguation software isbased largely on language-independent algorithmsthat resolve mentions in the context of the entirecorpus.
The system utilizes multiple types of con-text as evidence for determining whether two men-tions correspond to the same entity and it auto-matically learns the weight of evidence of eachcontext item via corpus statistics.The goal of the Web People Search task (Artileset al 2007) is to assign Web pages to groups,where each group contains all (and only those)pages that refer to one unique entity.
A page isassigned to multiple groups if it mentions multipleentities, for example ?John F. Kennedy?
and the?John F. Kennedy Library?.
The pages were se-lected via a set of keyword queries, and the disam-biguation is evaluated only on those query entities.This differs from Fair Isaac?s system in a few keyways: our system deals with mentions rather thandocuments, our system does not require a filter onmentions, and our system is generally used forlarge collections of documents containing verymany names rather than small sets of highly am-biguous documents dealing with one specificname.
Nevertheless, it was possible to run the FairIsaac entity disambiguation system on the WebPeople Search task data with almost no modifica-tions and achieve accurate results.The remaining sections of this paper describeFair Isaac?s automatic entity disambiguation meth-odology and report on the performance of the sys-tem on the WePS data.2 MethodologyIn unstructured text, each document provides anatural context for entity disambiguation.
Aftercleaning up extraneous markup we carry outwithin-document co-reference resolution, aggregat-ing information about each entity mentioned ineach document.
We then use these entity attributesas features in determining which documents dealwith the same entity.2.1 Dealing with Raw Web DataThe first challenge in dealing with data from theWeb is to decide which documents are useful and149what text from those documents contains relevantinformation.
As a first pass, the first HTML file ina folder which contained the query name was usedas the main page.
In retrospect, it might have beenbetter to combine all portions of the page, orchoose the longest page.
We copied the title ele-ment and converted all text chunks to paragraphs,eliminating all other HTML and script.
If noHTML was found in the directory for a page, thefirst text file which contained the query was usedinstead.2.2 Within-Document DisambiguationWhen dealing with unstructured text, a named en-tity recognition (NER) system provides the input tothe entity disambiguation.
Due to time constraintsand that Persons are the entity type of primary in-terest, any mention that matches one of the querystrings is automatically labeled as a Person, regard-less of its actual type.As described in Blume (2005), the system nextcarries out entity type-specific parsing in order toextract entity attributes such as titles, generatestandardized names (e.g.
p_abdul_khan_p for ?Dr.Abdul Q.
Khan?
), and populate the data structures(token hashes) that are used to perform the within-document entity disambiguation.We err on the side of not merging entities ratherthan incorrectly merging entities.
Looking at mul-tiple documents provides additional statistics.Thus, the cross-document disambiguation processdescribed in the next section will still merge someentities even within individual documents.2.3 Cross-Document DisambiguationOur cross-document entity disambiguation relieson one key insight: an entity can be distinguishedby the company it keeps.
If Abdul Khan 1 associ-ates with different people and organizations at dif-ferent locations than Abdul Khan 2, then he isprobably a different person.
Furthermore, if it ispossible to compare two entities based on one typeof context, it is possible to compare them based onevery type of context.Within each domain, we require a finite set ofcontext items.
In the domains of co-occurring lo-cations, organizations, and persons, these are thestandardized names derived in the entity informa-tion extraction phase of within-document disam-biguation.
We use the logarithm of the inversename frequency (the number of standard personnames with which this context item appears), INF,as a weight indicating the salience of each contextitem.
Co-occurrence with a common name pro-vides less indication that two mentions correspondto the same entity than co-occurrence with an un-common name.
To reduce noise, only entities thatoccur within a given window of entities are in-cluded in this vector.
In all test runs, this windowis set to 10 entities on either side.
Because of theeffects that small corpora have on statistics, weadded a large amount of newswire text to improvefrequency counts.
Many of the query names wouldhave low frequency in a text corpus that is notabout them specifically, but have high frequency inthis task because each document contains at leastone mention of them.
This would cause the INFweight to incorrectly estimate the importance ofany token; adding additional documents to the dis-ambiguation run reduces this effect and brings fre-quency counts to more realistic levels.We similarly count title tokens that occur withthe entity and compute INF weights for the titletokens.
Topic context, as described in Blume(2005), was used in some post-submission runs.We define a separate distance measure per con-text domain.
We are able to discount the co-occurrence with multiple items as well as quantifyan unexpected lack of shared co-occurrence byengineering each distance measure for each spe-cific domain.
The score produced by each distancemeasure may be loosely interpreted as the log ofthe likelihood of two randomly generated contextssharing the observed degree of similarity.In addition to the context-based distance meas-ures, we utilize a lexical (string) distance measurebased on exactly the same transformations as usedto compare strings for intra-document entity dis-ambiguation plus the Soundex algorithm (Knuth1998) to measure whether two name tokens soundthe same.
A large negative score indicates a greatdeal of similarity (log likelihood).The process of cross-document entity disam-biguation now boils down to repeatedly finding apair of entities, comparing them (computing thesum of the above distance measures), and mergingthem if the score exceeds some threshold.
Wecompute sets of keys based on lexical similarityand compare only entities that are likely to match.The WePS evaluation only deals with entities thatmatch a query.
Thus, we added a new step of keygeneration based on the query.1503 PerformanceWe have tested our entity disambiguation systemon several semi-structured and unstructured textdata sets.
Here, we report the performance on thetraining data provided for the Web People Searchtask.
This corpus consists of raw Web pages withsubstantial variation in capitalization, punctuation,grammar, and spelling ?
characteristics that makeNER challenging.
A few other issues also nega-tively impact our performance, including extrane-ous text, long lists of entities, and the issue of find-ing the correct document to parse.The NER process identified a ratio of approxi-mately 220 mentions per document across 3,359documents.
Within-document entity disambigua-tion reduced this to approximately 113 entities perdocument, which we refer to as document-levelentities.
Of these, 3,383 Persons (including thoseOrganizations and Locations which were relabeledas Persons) contained a query name.
Cross-document entity disambiguation reduced this to976 distinct persons with 721 distinct standardizednames.
Thus, 2,407 merge operations were per-formed in this step.
On average, there are 48 men-tions per query name.
Our system found an aver-age of 14 unique entities per query name.
In thegold standard, the average is 9 unique entities perquery name.Looking at the names that matched in the out-put, it is clear that NER is very important to theprocess.
Post submission of our initial run, weused proper tokenization of punctuation and anadditional NER system, which corrected manymistakes in the grouping of names.
Also, many ofthe names that were incorrectly merged would nothave been compared if not for the introduction ofthe additional key that compares all mentions thatmatch a query name.For the WePS evaluation submission, we con-verted our results to document-level entities bymapping each mention to the document that it waspart of and removing duplicates.
If we did not finda mention in a document, we labeled the documentas a singleton entity.We also used a number of standard metrics forour internal evaluation.
Most of these operate ondocument-level entities rather than on documents.To convert the ground truth provided for the taskto a form usable for these metrics, we assume thateach entity contains all mentions in the corre-sponding document group.
These metrics test thecross-document disambiguation rather than theNER and within-document disambiguation.
Thesemetrics should not be used to compare betweendifferent versions of NER and within-documentdisambiguation, since the ground truth used in theevaluation is generated by these processes.In Table 1, we compare a run with the additionalnewswire data and the comparison key (our WePSsubmission), leaving out the additional newswiredata and the additional comparison key, and leav-ing out only the additional comparison key.In Table 2, we compare runs based on the im-proved NER (available only after the WePS sub-mission deadline).
The first uses the same parame-ters as our submission, the second uses an in-creased threshold, and the third utilizes the wordvector-based clustering (document topics).Acc.
Prec.
Recall Harm.
PurityWithExtraKey 0.670 0.545 0.906 0.818NoAddedData 0.743 0.752 0.584 0.841NoExtraKey 0.770 0.767 0.624 0.861Table 1.
Results of pairwise comparisons and clus-terwise harmonic mean of purity and inverse purityon various disambiguation runs.
Each metric isaveraged across the individual results for everyquery name.Acc.
Prec.
Recall Harm.
PurityWithExtraKey 0.690 0.618 0.552 0.8151.25 Thresh 0.720 0.733 0.500 0.812Topic Info 0.719 0.645 0.545 0.818Table 2.
Results based on improved named entityrecognition.
These should not be directly com-pared against those in Table 1, since the differentNER yields different ground truth for these evalua-tion metrics.Most of our metrics are based on pairwise com-parisons ?
all document-level entities are comparedagainst all other document-level entities that matchthe same query name, noting whether the pair wascoreferent in the results and in the ground truth.With such comparison, we obtain measures includ-ing precision, recall, and accuracy.
In this trainingdata, depending on which NER is used, 35,000-50,000 pairwise comparisons are possible.We also define a clusterwise measure of theharmonic mean between purity and inverse puritywith respect to mentions.
This is different from themetric provided by WePS, purity and inverse pu-151rity at the document level.
Since some documentscontain multiple entities, the latter metric does notperform correctly.
Mentions, on the other hand,are always unique in our disambiguation.
How-ever, because the ground truth was specified at thedocument level, documents containing multipleentities that match a query yield ambiguous men-tions.
These decrease all purity-related scoresequally and do not vary between runs.The addition of the newswire data improved re-sults.
Inclusion of an extra comparison based onquery name matches allowed for comparison ofentities with names that do not match the format ofperson names, and only slightly reduced overallperformance.
The new NER run can only be com-pared on the last three runs.
to the system per-forms better with topic context than without it.In comparison, in the 2005 Knowledge Discov-ery and Dissemination (KD-D) Challenge TaskER-1a (the main entity disambiguation task), weachieved an accuracy of 94.5%.
The margin oferror in the evaluation was estimated at 3% due toerrors in the ?ground truth?.
This was a pure dis-ambiguation task with no NER or name standardi-zation required.
The evaluation set contained 100names, 9027 documents, and 583,152 pair-wiseassertions.4 ConclusionsAlthough the primary purposes of Fair Isaac?s en-tity disambiguation system differ from the goal ofthe Web People Search task, we found that withlittle modification it was possible to fairly accu-rately cluster Web pages with a given query nameaccording to the real-world entities mentioned onthe page.
Most of the errors that we encounteredare related to information extraction from unstruc-tured data as opposed to the cross-document entitydisambiguation itself.AcknowledgmentThis material is based upon work supported by theDefense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-06-C-0023.Any opinions, findings and conclusions or rec-ommendations expressed in this material are thoseof the author(s) and do not necessarily reflect theviews of the Defense Advanced Research ProjectsAgency (DARPA).ReferencesArtiles, J., Gonzalo, J. and Sekine, S. (2007).
The Se-mEval-2007 WePS Evaluation: Establishing a bench-mark for the Web People Search Task.
In Proceed-ings of Semeval 2007, Association for ComputationalLinguistics.Bagga, A. and Baldwin, B.
(1998).
Entity-based Cross-document Coreferencing Using the Vector SpaceModel.
17th International Conference on Computa-tional Linguistics (CoLing-ACL).
Montreal, Canada.10-14 August, 1998, 79-85.Blume, M. (2005).
Automatic Entity Disambiguation:Benefits to NER, Relation Extraction, Link Analysis,and Inference.
1st International Conference on Intel-ligence Analysis.
McLean, Virginia.
2-5 May, 2005.Gooi, C. H. and Allan, J.
(2004).
Cross-DocumentCoreference on a Large Scale Corpus.
Human Lan-guage Technology Conference (HLT-NAACL).Boston, Massachusetts.
2-7 May, 2004, 9-16.Kalashnikov, D. V. and Mehrotra, S. (2005).
A Prob-abilistic Model for Entity Disambiguation Using Re-lationships.
SIAM International Conference on DataMining (SDM).
Newport Beach, California.
21-23April, 2005.Knuth, D. E. (1998).
The Art of Computer Program-ming, Volume 3: Sorting and Searching.
Addison-Wesley Professional.Mann, G. S. and Yarowsky, D. (2003).
UnsupervisedPersonal Name Disambiguation.
Conference onComputational Natural Language Learning (CoNLL).Edmonton, Canada.
31 May - 1 June, 2003, 33-40.Mitchell, A.; Strassel, S.; Przybocki, P.; Davis, J. K.;Doddington, G.; Grishman, R.; Meyers, A.; Brun-stein, A.; Ferro, L. and Sundheim, B.
(2004).
Anno-tation Guidelines for Entity Detection and Tracking(EDT), Version 4.2.6.http://www.ldc.upenn.edu/Projects/ACE/.Ravin, Y. and Kazi, Z.
(1999).
Is Hillary Rodham Clin-ton the President?
Disambiguating Names acrossDocuments.
ACL 1999 Workshop on Coreferenceand Its Applications.
College Park, Maryland.
22June, 1999, 9-16.152
