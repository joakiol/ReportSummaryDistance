Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 232?240,Beijing, August 2010Local lexical adaptation in Machine Translation through triangulation:SMT helping SMTJosep Maria CregoLIMSI-CNRSjmcrego@limsi.frAur?lien MaxLIMSI-CNRSUniv.
Paris Sudamax@limsi.frFran?ois YvonLIMSI-CNRSUniv.
Paris Sudyvon@limsi.frAbstractWe present a framework where auxiliaryMT systems are used to provide lexicalpredictions to a main SMT system.
Inthis work, predictions are obtained bymeans of pivoting via auxiliary languages,and introduced into the main SMT sys-tem in the form of a low order languagemodel, which is estimated on a sentence-by-sentence basis.
The linear combinationof models implemented by the decoderis thus extended with this additional lan-guage model.
Experiments are carried outover three different translation tasks usingthe European Parliament corpus.
For eachtask, nine additional languages are usedas auxiliary languages to obtain the trian-gulated predictions.
Translation accuracyresults show that improvements in trans-lation quality are obtained, even for largedata conditions.1 IntroductionImportant improvements are yet to come regard-ing the performance of Statistical Machine Trans-lation systems.
Dependence on training data andlimited modelling expressiveness are the focus ofmany research efforts, such as using monolingualcorpora for the former and syntactic models forthe latter.Another promising approach consists in ex-ploiting complementary sources of informationin order to build better translations, as done byconsensus-based system combination (e.g.
(Ma-tusov et al, 2008)).
This, however, requires tohave several systems available for the same lan-guage pair.
Considering that the same trainingdata would be available to all systems, differencesin translation modelling are expected to produceredundant and complementary hypotheses.
Mul-tisource translation (e.g.
(Och and Ney, 2001;Schwartz, 2008)) is a variant, involving sourcetexts available in several languages which can betranslated by systems for different language pairsand whose outputs can be successfully combinedinto better translations (Schroeder et al, 2009).One theoretical expectation of multisource trans-lation is that it can successfully reduce ambiguityof the original source text, but does so under therare conditions of availability of existing (accu-rate) translations.
In contrast, pivot-based systemcombination (e.g.
(Utiyama and Isahara, 2007;Wu and Wang, 2007)) aims at compensating thelack of training data for a given language pair byproducing translation hypotheses obtained by piv-oting via an intermediary language for which bet-ter systems are available.These techniques generally produce a searchspace that differs from that of the direct transla-tion systems.
As such, they create a new transla-tion system out of various systems for which di-agnosis becomes more difficult.This paper instead focusses on improving a sin-gle system, which should be state-of-the-art asregards data and models.
We propose a frame-work in which information coming from externalsources is used to boost lexical choices and guidethe decoder into making more informed choices.11We performed initial experiments where the comple-mentary information was exploited during n-best list rerank-ing (Max et al, 2010), but except for the multisource condi-tion the list of hypotheses contained too little useful variation232Complementary sources can be of different na-ture: they can involve other automatic systems(for the same or different language pairs) and/orhuman knowledge.
Furthermore, complementaryinformation is injected at the lexical level, thusmaking targeted fine-grained lexical predictionsuseful.
Importantly, those predictions are ex-ploited at the sentence level2, so as to allow forefficient use of source contextual information.The second contribution of this paper is an in-stantiation of the proposed framework.
Auto-matically pivoting via auxiliary languages is usedto make complementary predictions that are ex-ploited through language model adaptation by thedecoder for a given language pair.
For this appar-ently difficult condition, where predictions resultfrom automatic translations involving two sys-tems, we manage to report significant improve-ments, measured with respect to the target and thesource text, under various configurations.This paper is organized as follows.
We first re-view related work in section 2.1, and describe thedistinctive characteristics of our approach in Sec-tion 2.2.
Section 2.3 presents our instantiation ofthe framework based on lexical boosting via aux-iliary language triangulation.
Experiments involv-ing three language pairs of various complexity anddifferent amounts of training data are described inSection 3.
We finally conclude by discussing theprospects offered by our proposed framework inSection 4.2 A framework for sentence-level lexicalboosting2.1 Related workThe idea of using more than one translation sys-tem to improve translation performance is not newand has been implemented in many different wayswhich we briefly review here.System combination An often used strategyconsists in combining the output of several sys-tems for a fixed language pair, and to rescore theresulting set of hypotheses taking into accountall the available translations and scores.
Variousto lead to measurable improvements.2We plan to experiment next on using predictions at thedocument level.proposals have been made to efficiently performsuch a combination, using auxiliary data struc-tures such as n-best lists, word lattices or con-sensus networks (see for instance (Kumar andByrne, 2004; Rosti et al, 2007; Matusov et al,2008; Hildebrand and Vogel, 2008; Tromble et al,2008)).
Theses techniques have proven extremelyeffective and have allowed to deliver very signifi-cant gains in several recent evaluation campaigns(Callison-Burch et al, 2008).Multisource translation A related, yet more re-sourceful approach, consists in trying to combineseveral systems providing translations from differ-ent sources into the same target, provided suchmultilingual sources are available.
(Och and Ney,2001) propose to select the most promising trans-lation amongst the hypotheses produced by sev-eral Foreign?English systems, where output se-lection is based on the translation scores.
Theintuition that if a system assigns a high figureof merits to the translation of a particular sen-tence, then this translation should be preferred,is implemented in the MAX combination heuris-tics, whose relative (lack of) success is discussedin (Schwartz, 2008).
A similar idea is explored in(Nomoto, 2004), where the sole target languagemodel score is used to rank competing outputs.
(Schroeder et al, 2009) propose to combine theavailable sources prior to translation, under theform of a multilingual lattice, which is decodedwith a multisource phrase table.
(Chen et al,2008) integrate the available auxiliary informationin a different manner, and discuss how to improvethe translation model of the primary system: theidea is to use the entries in the phrase table ofthe auxiliary system to filter out those acciden-tal correspondences that pollute the main transla-tion model.
The most effective implementation ofmultisource translation to date however consistsin using mono-source system combination tech-niques (Schroeder et al, 2009).Translation through pivoting The use of aux-iliary systems has also been proposed in anothercommon situation, as a possible remedy to thelack of parallel data for a particular language pair,or for a particular domain.
Assume, for instance,that one wishes to build a translation system for233the pair A ?
B, for which the parallel datais sparse; assuming further that such parallel re-sources exist for pairs A ?
C and for C ?
B,it is then tempting to perform the translation in-directly through pivoting, by first translating fromA to C, then from C to B.
Direct implementa-tions of this idea are discussed e.g.
in (Utiyamaand Isahara, 2007).
Pivoting can also interveneearlier in the process, for instance as a meansto automatically generate the missing parallel re-source, an idea that has also been considered toadapt an existing translation systems to new do-mains (Bertoldi and Federico, 2009).
Pivoting canfinally be used to fix or improve the translationmodel: (Cohn and Lapata, 2007) augments thephrase table for a baseline bilingual system withsupplementary phrases obtained by pivoting intoa third language.Triangulation in translation Triangulationtechniques are somewhat more general and onlyrequire the availabily of one auxiliary system (orone auxiliary parallel corpus).
For instance, theauthors of (Chen et al, 2008) propose to use thetranslation model of an auxiliary C ?
B systemto filter-out the phrase-table of a primary A ?
Bsystem.2.2 Our frameworkAs in other works, we propose to make use of sev-eral MT systems (of any type) to improve trans-lation performance, but contrarily to these workswe concentrate on improving one particular sys-tem.
Our framework is illustrated on Figure 1.The main system (henceforth, direct system), cor-responding to configuration 1, is a SMT system,translating from German to English in the exam-ple.
Auxiliary information may originate fromvarious sources (2-6) and enter into the decoder.A new model is dynamically built and is used toguide the exploration of the search space to thebest hypothesis.
Several auxiliary models can beused at once and can be weighted by standard op-timization techniques using development data, sothat bad sources are not used in practice, or byexploiting a priori information.
In the implemen-tation described in section 2.3, this information isupdated by the auxiliary source at each sentence.Figure 1: Lexical boosting framework with vari-ous configurations for auxiliary predictionsWe now briefly describe various possible con-figurations to make some links to previous worksexplicit.
Configuration 2 translates the samesource text by means of another system for thesame language pair, as would be done in systemcombination, except that here a new complete de-coding is performed by the direct system.
Con-figuration 3, which will be detailed in section 2.3,uses translations obtained by triangulating via anauxiliary language (Spanish in the example).
Us-ing this two-step translation is common to pivotapproaches, but our approach is different in thatthe result of the triangulation is only used as aux-iliary information for the decoding of the directsystem.
Configurations 4 and 5 are instances ofmultisource translation, where a paraphrase or atranslation of the source text is available.
Lastly,configuration 6 illustrates the case where a humantranslator, with knowledge of the target languageand at least of one of the available source lan-guages, could influence the decoding by provid-ing desired3 words (e.g.
only for source words orphrases that would be judged difficult to translate).This human supervision through a feedback text inreal time is similar to the proposal of (Dymetmanet al, 2003).Given this framework, several questions arise,3The proposal as it is limits the hypotheses produced bythe system to those that are attainable given its training data.It is conceivable, however, to find ways of introducing newknowledge in this framework.234the most important underlying this work beingwhether the performance of SMT systems can beimproved by using other SMT systems.
Anotherpoint of interest is whether improvements madeto auxiliary systems can yield improvement to thedirect system, without the latter undergoing anymodification.2.3 Lexical boosting via triangulationAuxiliary translations obtained by pivoting can beviewed as a source of adaptation data for the targetlanguage model of the direct system.
Assumingwe have computed n-best translation hypothesesof a sentence in the target language, we can thenboost the likeliness of the words and phrases oc-curring in these hypotheses by deriving an auxil-iary language model for each test sentence.
Thisallows us to integrate this auxiliary informationduring the search and thus provides a tighter in-tegration with the direct system.
This idea hassuccessfully been used in speech recognition, us-ing for instance close captions (Placeway and Laf-ferty, 1996) or an imperfect translation (Paulik etal., 2005) to provide auxiliary in-domain adap-tation data for the recognizer?s language model.
(Simard and Isabelle, 2009) proposed a similar ap-proach in Machine Translation in which they usethe target-side of an exact match in a translationmemory to build language models on a per sen-tence basis used in their decoder.This strategy can be implemented in a straight-forward manner, by simply training a languagemodel using the n-best list as an adaptation cor-pus.
Being automatically generated, hypothesesin the n-best list are not entirely reliable: in par-ticular, they may contain very unlikely target se-quences at the junction of two segments.
It is how-ever straightforward to filter these out using theavailable phrase alignment information.This configuration is illustrated on Figure 2: thedirect system (configuration 1) makes use of pre-dictions from pivoting through an auxiliary lan-guage (configuration 2), where n-best lists can beused to produce several hypotheses.
In order toget a upper bound on the potential gains of this ap-proach, we can run the artificial experiment (con-figuration 3) where a reference in the target lan-guage is used as a ?perfect?
source of information.Furthermore, we are interested in the performanceof the simple pivot system alone (configuration 4),as it gives an indication of the quality of the dataused for LM adaptation.Figure 2: Architecture of a German?English sys-tem for lexical boosting via triangulation throughSpanish3 Experiments and results3.1 Translation engineIn this study, we used our own machine trans-lation engine, which implements the n-gram-based approach to statistical machine translation(Mari?o et al, 2006).
The translation modelis implemented as a stochastic finite-state trans-ducer trained using a n-gram language model of(source,target) pairs.In addition to a bilingual n-gram model, ourSMT system uses six additional models whichare linearly combined following a discriminativemodeling framework: two lexicalized reorder-ing (Tillmann, 2004) models,a target-languagemodel, two lexicon models, a ?weak?
distance-based distortion model, a word bonus model anda translation unit bonus model.
Coefficients inthis linear combination are tuned over develop-ment data with the MERT optimization toolkit4,slightly modified to use our decoder?s n-best lists.For this study, we used 3-gram bilingual and3-gram target language models built using modi-fied Kneser-Ney smoothing (Chen and Goodman,1996); model estimation was performed with theSRI language modeling toolkit.5 Target language4http://www.statmt.org/moses5http://wwww.speech.sri.com/projects/srilm235models were trained on the target side of the bi-text corpora.After preprocessing the corpora with standardtokenization tools, word-to-word alignments areperformed in both directions, source-to-target andtarget-to-source.
In our system implementation,the GIZA++ toolkit6 is used to compute the wordalignments.
Then, the grow-diag-final-and heuris-tic is used to obtain the final alignments fromwhich translation units are extracted.
Convergentstudies have showed that systems built accord-ing to these principles typically achieve a per-formance comparable to that of the widely usedMOSES phrase-based system for the languagepairs under study.3.2 CorporaWe have used the Europarl corpus7 for our mainand auxiliary languages.
The eleven languagesare: Danish (da), German (de), English (en),Spanish (es), Finnish (fi), French (fr), Greek(el), Italian (it), Dutch (nl), Portuguese (pt) andSwedish (sv).We focussed on three translation tasks: onefor which translation accuracy, as measured byautomatic metrics, is rather high (fr ?
en),and two for which translation accuracy is lower(de ?
en) and (fr ?
de).
This will allow usto check whether the improvements provided byour method carry over even in situations where thebaseline is strong; conversely, it will allow us toassess whether the proposed techniques are appli-cable when the baseline is average or poor.In order to measure the contribution of each ofthe auxiliary languages we used a subset of thetraining corpus that is common to all languagepairs, hereinafter referred to as the intersectiondata condition.
We used the English side of alltraining language pairs to collect the same sen-tences in all languages, summing up to 320, 304sentence pairs.
Some statistics on the data used inthis study are reported in Table 1.
Finally, in orderto assess the impact of the training data size overthe results obtained, we also considered a muchmore challenging condition for the fr ?
de pair,where we used the entire Europarl data (V5) made6http://www.fjoch.com/GIZA++.html7http://www.statmt.org/europarlavailable for the fifth Workshop on Statistical Ma-chine Translation8 for training, and test our sys-tem on out-of-domain news data.
The trainingcorpus in this condition contains 43.6M Frenchwords and 37.2M German words.Development and test data for the first con-dition (intersection) were obtained by leavingout respectively 500 and 1000 sentences fromthe common subset (same sentences for all lan-guages), while the first 500 sentences of news-test2008 and the entire newstest2009 official testsets were used for the full data condition.Train Dev TestWords Voc.
Words Voc.
OOV Words Voc.
OOVda 8.5M 133.5k 13.4k 3.2k 104 25.9k 5.1k 226de 8.5M 145.3k 13.5k 3.5k 120 26.0k 5.5k 245en 8.9M 53.7k 14.0k 2.8k 39 27.2k 4.0k 63es 9.3M 85.3k 14.6k 3.3k 56 28.6k 5.0k 88fi 6.4M 274.9k 10.1k 4.3k 244 19.6k 7.1k 407fr 10.3M 67.8k 16.1k 3.2k 47 31.5k 4.8k 87el 8.9M 128.3k 14.1k 3.9k 72 27.2k 6.2k 159it 9.0M 78.9k 14.3k 3.4k 61 28.1k 5.1k 99nl 8.9M 105.0k 14.2k 3.1k 76 27.5k 4.8k 162pt 9.2M 87.3k 14.5k 3.4k 49 28.3k 5.2k 118sv 8.0M 140.8k 12.7k 3.3k 116 24.5k 5.2k 226Table 1: Statistics for the training, developmentand test sets of the intersection data condition3.3 ResultsIn this section, we report on the experiments car-ried out to assess the benefits of introducing anauxiliary language model to the linear combina-tion of models implemented in our SMT system.Table 2 reports translation accuracy (BLEU) re-sults for the main translation tasks considered inthis work (fr ?
de), (fr ?
en) and (de ?
en),as well as for multiple intermediate tasks neededfor pivoting via auxiliary systems.For each triplet of languages (src, aux, trg),columns 4th to 6th show BLEU scores for systemsperforming (src ?
aux), (aux ?
trg) and pivottranslations using aux as the bridge language.The last two columns display BLEU scores forthe main translation tasks (fr ?
de), (fr ?
en)and (de?
en).
Column src-trg refers to the base-line (direct) systems, for which no additional lan-8http://www.statmt.org/wmt10236src aux trg src-aux aux-trg pivot src-trg +auxLMIntersection data conditionfr - de - - - 18.02da 22.78 20.02 16.27 +0.44el 24.54 18.51 15.86 +0.76en 29.53 17.31 15.69 +0.50es 34.94 18.31 16.76 +0.96fi 10.71 14.15 11.39 +0.65it 31.60 16.86 16.54 -0.05nl 22.71 21.44 16.76 +0.55pt 33.61 17.47 16.34 -0.12sv 20.73 19.59 13.73 -0.14average +0.39- - ref - - - - +6.46fr - en - - - 29.53da 22.78 29.54 25.48 +0.02de 18.02 24.66 23.50 +0.05el 24.54 29.37 25.31 +0.07es 34.94 31.05 27.76 +0.61fi 10.71 20.56 19.15 +0.44it 31.60 25.75 25.79 +0.32nl 22.71 24.49 25.15 +0.01pt 33.61 29.44 27.27 +0.01sv 20.73 30.98 23.74 +0.50average +0.22- - ref - - - - +11.30de - en - - - 24.66da 24.59 29.54 22.73 +0.96el 19.72 29.37 20.88 +1.02es 25.48 31.05 21.23 +0.77fi 12.42 20.56 18.02 +0.94fr 25.93 29.53 21.55 +0.19it 18.82 25.75 18.05 +0.19nl 24.97 24.49 22.62 +0.64pt 23.15 29.44 21.93 +0.87sv 19.80 30.98 21.35 +0.69average +0.69- - ref - - - - +9.53Full data conditionfr - de - - - 19.94es 38.76 20.18 19.36 +0.61Table 2: Translation accuracy (BLEU) results.guage model is used; column +auxLM refers tothe same system augmented with the additionallanguage model.
Additional language models arebuilt from hypotheses obtained by means of pivottranslations, using aux as auxiliary language.
Thelast score is shown in the form of the difference(improvement) with respect to the score of thebaseline system.This table additionally displays the BLEU re-sults obtained when building the additional lan-guage models directly from the English referencetranslations (see last row of each translation task).These numbers provide an upper-bound of the ex-pected improvements.
Note finally that numbersin boldface correspond to the best numbers in theircolumn for a given language pair.As detailed above, the additional languagemodels are built using trg hypotheses obtained bypivoting via an auxiliary language: (src ?
aux)+ (aux ?
trg).
Hence, column pivot shows thequality (measured in terms of BLEU) of the hy-potheses used to estimate the additional model.Note that we did not limit the language model tobe estimated from the 1-best pivot hypotheses.
In-stead, we uses n-best translation hypotheses of the(src ?
aux) system and m-best hypotheses ofthe (aux ?
trg) system.
Hence, n ?
m targethypotheses were used as training data to estimatethe additional models.
Column +auxLM showsBLEU scores over the test set after performingfour system optimizations on the development setto select the best combination of values used for nand m among: (1, 1), (10, 1), (10, 1) and (10, 10).All hypotheses used to estimate a language modelare considered equally likely.
Language modelsare learnt using Witten-Bell discounting.
Approx-imately?1.0 point must be added to BLEU scoresshown in the last 2 columns for 95% confidencelevels.As expected, pivot translations yield lowerquality scores than the corresponding direct trans-lations hypotheses.
However, pivot hypothesesmay contain better lexical predictions, that the ad-ditional model helps transfer into the baseline sys-tem, yielding translations with a higher quality, asshown in many cases the +auxLM systems results.The case of using Finnish as an auxiliary languageis particularly remarkable.
Even though pivot hy-potheses obtained through Finnish have the low-est scores9, they help improve the baseline perfor-mance as additional language models.As expected, the translation results of the pair9Given the agglutinative nature of morphological pro-cesses in Finnish, reflected in a much lower number of wordsper sentence, and a higher number of types (see Table 1),BLEU scores for this language do not compare directly withthe ones obtained for other languages.237with a highest baseline (fr ?
en) were on av-erage less improved than those of the pairs withlower baselines.As can also be seen, the contribution of eachauxiliary language varies for each of the threetranslation tasks.
For instance, Danish (da) pro-vides a clear improvement to (de ?
en) transla-tions, while no gain is observed for (fr ?
en).No clear patterns seems to emerge, though, andthe correlation between the quality of the pivottranslation and the boost provided by using thesepivot hypotheses remains to be better analyzed.In order to assess whether the improvementsobtained carry over larger data conditions, wetrained our (fr ?
de), (fr ?
es) and (es?
de)systems over the entire EPPS data.
Results are re-ported in the bottom part of Table 2.
As can beseen, the (fr ?
de) system is still improved byusing the additional language model.
However,the absolute value of the gain under the full condi-tion (+0.61) is lower than that of the intersectiondata condition (+0.96).3.4 Contrastive evaluation of lexicaltranslationIn some cases, automatic metrics such as BLEUcannot show significant differences that can be re-vealed by fine-grained focussed human evaluation(e.g.
(Vilar et al, 2006)).
Furthermore, comput-ing some similarity between a system?s hypothe-ses and gold standard references puts a strongfocus on the target side of translation, and doesnot allow evaluating translation performance fromthe source words that were actually translated.We therefore use the evaluation methodology de-scribed in (Max et al, 2010) for a complementarymeasure of translation performance that focuseson the contrastive ability of two systems to ade-quately translate source words.Source words from the test corpus were firstaligned with target words in the reference, by au-tomatically aligning the union of the training andtest corpus using GIZA++.10 The test corpus wasanalyzed by the TREETAGGER11 so as to identify10The obtained alignments are thus strongly influenced byalignments from the training corpus.
It could be noted thatalignments could be manually corrected.11http://www.ims.uni-stuttgart.de/Source words?
part-of-speechaux ADJ ADV NOM PRO VER all +Bleuel - 27 21 114 25 99 286 +0.07+ 62 29 136 27 114 368es - 33 25 106 26 110 300 +0.61+ 64 38 136 22 117 377fi - 44 40 106 20 92 302 +0.44+ 49 31 120 23 106 329it - 55 39 128 35 119 376 +0.32+ 55 39 145 36 121 396sv - 40 30 138 29 109 346 +0.50+ 69 46 144 23 134 416Table 3: Contrastive lexical evaluation re-sults per part-of-speech between the baselineFrench?English system and our systems usingvarious auxiliary languages.
?-?
(resp.
?+?)
val-ues indicate numbers of words that only the base-line system (resp.
our system) correctly translatedwith respect to the reference translation.content words, which have a more direct impacton translation adequacy.
When source words arealigned to several target words, each target wordshould be individually searched for in the candi-date translation, and words from the reference canonly be matched once.Table 3 shows contrastive results per part-of-speech between the baseline fr?en system andsystems using various auxiliary languages.
Val-ues in the ?-?
row indicate the number of wordsthat only the baseline system translated as in thereference translation, and values in the ?+?
rowthe number of words that only our correspondingsystem translated as in the reference.
The moststriking result is the contribution of Greek, which,while giving no gain in terms of BLEU, improvedthe translation of 82 content words.
This couldbe explained, in addition to the lower Bleu3 andBleu4 precision, by the fact that the quality ofthe translation of grammatical words may havedecreased.
On the contrary, Italian brings littleimprovement for content words save for nouns.The mostly negative results on the translation ofpronouns were expected, because this depends ontheir antecedent in English and is not the object ofspecific modelling from the systems.
The trans-lation of nouns and adjectives benefits the mostfrom auxiliary translations.projekte/corplex/TreeTagger238Figure 3 illustrates this evaluation by means oftwo examples.
It should be noted that a recurrenttype of improvement was that of avoiding missingwords, which is here a direct result of their beingboosted in the auxiliary hypotheses.4 Conclusions and future workWe have presented a framework where auxiliaryMT systems are used to provide useful informa-tion to a main SMT system.
Our experimentson auxiliary language triangulation have demon-strated its validity on a difficult configuration andhave shown that improvements in translation qual-ity could be obtained even under large trainingdata conditions.The fact that low quality sources such as pivottranslation can provide useful complementary in-formation calls for a better understanding of thephenomena at play.
It is very likely that, look-ing at our results on the contribution of auxiliarylanguages, improving the quality of an auxiliarysource can also be achieved by identifying whata source is good for.
For example, in the stud-ied language configurations predictions of transla-tions for pronouns in the source text by auxiliarytriangulation does not give access to useful infor-mation.
On the contrary, triangulation with Greekwhen translating from French to English seems togive useful information regarding the translationof adjectives, a result which was quite unexpected.Also, it would be interesting to use richer pre-dictions than short n-grams, such as syntacticdependencies, but this would require significantchanges on the decoders used.
Using dynamicmodels at the discourse level rather than only atthe sentence level would also be a useful improve-ment.
Besides the improvements just mentioned,our future work includes working on several con-figurations of the framework described in sec-tion 2.2, in particular investigating the new typeof system combination.AcknowledgementsThis work has been partially funded by OSEO un-der the Quaero program.ReferencesBertoldi, Nicola and Marcello Federico.
2009.
Do-main adaptation for statistical machine translationwith monolingual resources.
In Proceedings ofWMT, Athens, Greece.Callison-Burch, Chris, Cameron Shaw Fordyce,Philipp Koehn, Christof Monz, and Josh Schroeder.2008.
Further meta-evaluation of machine transla-tion.
In Proceedings of WMT, Columbus, USA.Chen, Stanley F. and Joshua T. Goodman.
1996.
Anempirical study of smoothing techniques for lan-guage modeling.
In Proceedings of ACL, SantaCruz, USA.Chen, Yu, Andreas Eisele, and Martin Kay.
2008.
Im-proving statistical machine translation efficiency bytriangulation.
In Proceedings of LREC, Marrakech,Morocco.Cohn, Trevor and Mirella Lapata.
2007.
Machinetranslation by triangulation: Making effective useof multi-parallel corpora.
In Proceedings of ACL,Prague, Czech Republic.Dymetman, Marc, Aur?lien Max, and Kenji Yamada.2003.
Towards interactive text understanding.
InProceedings of ACL, short paper session, Sapporo,Japan.Hildebrand, Almut Silja and Stephan Vogel.
2008.Combination of machine translation systems via hy-pothesis selection from combined n-best lists.
InProceedings of AMTA, Honolulu, USA.Kumar, Shankar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In Proceedings of NAACL-HLT, Boston, USA.Mari?o, Jos?, Rafael E. Banchs, Josep Maria Crego,Adria de Gispert, Patrick Lambert, J.A.R.
Fonol-losa, and Martha Costa-juss?.
2006.
N-gram basedmachine translation.
Computational Linguistics,32(4):527?549.Matusov, Evgeny, Gregor Leusch, Rafael E. Banchs,Nicola Bertoldi, Daniel Dechelotte, Marcello Fed-erico, Muntsin Kolss, Young-Suk Lee, Jose Mari?o,Matthias Paulik, Salim Roukos, Holger Schwenk,and Hermann Ney.
2008.
System combination formachine translation of spoken and written language.IEEE Transactions on Audio, Speech and LanguageProcessing, 16(7):1222?1237, September.Max, Aur?lien, Josep M. Crego, and Fran?ois Yvon.2010.
Contrastive Lexical Evaluation of MachineTranslation.
In Proceedings of LREC, Valletta,Malta.239ref #357 this concession to the unions ignores the reality that all airlines have different safety procedures which even differbetween aircrafts within each airline .bas this concession unions ignores the fact that all airlines have different safety procedures which are even within eachof the companies in accordance with the types of equipment .w.r.t.
src cette concession aux syndicats ignore la r?alit?
selon laquelle toutes les compagnies a?riennes ont des proc?dures de s?curit?diff?rentes qui diff?rent m?me au sein de chacune des compagnies en fonction des types d ?
appareils .+aux this concession to the trade unions ignores the reality according to which all the airlines have different safety pro-cedures which differ even within each of the companies in accordance with the types of equipment .w.r.t.
src cette concession aux syndicats ignore la r?alit?
selon laquelle toutes les compagnies a?riennes ont des proc?dures de s?curit?diff?rentes qui diff?rent m?me au sein de chacune des compagnies en fonction des types d ?
appareils .Figure 3: Example of automatic translations from French to English for the baseline system and whenusing Spanish as the auxiliary language.
Bold marking indicates source/target words which were cor-rectly translated according to the reference translation.Nomoto, Tadashi.
2004.
Multi-engine machine trans-lation with voted language model.
In Proceedingsof ACL, Barcelona, Catalunya, Spain.Och, Franz Josef and Hermann Ney.
2001.
Statisti-cal multi-source translation.
In Proceedings of MTSummit, Santiago de Compostela, Spain.Paulik, Matthias, Christian F?gen, Thomas Schaaf,Tanja Schultz, Sebastian St?ker, and Alex Waibel.2005.
Document driven machine translation en-hanced automatic speech recognition.
In Proceed-ings of InterSpeech, Lisbon, Portugal.Placeway, Paul and John Lafferty.
1996.
Cheatingwith imperfect transcripts.
In Proceedings of IC-SLP, Philadelphia, USA.Rosti, Antti-Veikko, Necip Fazil Ayan, Bin Xiang,Spyros Matsoukas, Richard Schwatz, and Bonnie J.Dorr.
2007.
Combining outputs from multiplemachine translation systems.
In Proceedings ofNAACL-HTL, Rochester, USA.Schroeder, Josh, Trevor Cohn, and Philipp Koehn.2009.
Word lattices for multi-source translation.
InProceedings of EACL, Athens, Greece.Schwartz, Lane.
2008.
Multi-source translation meth-ods.
In Proceedings of AMTA, Honolulu, USA.Simard, Michel and Pierre Isabelle.
2009.
Phrase-based machine translation in a computer-assistedtranslation environment.
In Proceedings of MachineTranslation Summit XII, Ottawa, Canada.Tillmann, Christoph.
2004.
A unigram orientationmodel for statistical machine translation.
In Pro-ceedings of NAACL-HLT, Boston, USA.Tromble, Roy, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice Minimum Bayes-Risk decoding for statistical machine translation.
InProceedings of EMNLP, Honolulu, USA.Utiyama, Masao and Hitoshi Isahara.
2007.
A com-parison of pivot methods for phrase-based statisti-cal machine translation.
In Proceedings of NAACL-HLT, Rochester, USA.Vilar, David, Jia Xu, Luis Fernando d?Haro, and Her-mann Ney.
2006.
Error Analysis of Statistical Ma-chine Translation Output.
In Proceedings of LREC,Genoa, Italy.Wu, Hua and Haifeng Wang.
2007.
Pivot languageapproach for phrase-based statistical machine trans-lation.
In Proceedings of ACL, Prague, Czech Re-public.240
