Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 958?966,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPK-Best A?
ParsingAdam Pauls and Dan KleinComputer Science DivisionUniversity of California, Berkeley{adpauls,klein}@cs.berkeley.eduAbstractA?
parsing makes 1-best search efficient bysuppressing unlikely 1-best items.
Existing k-best extraction methods can efficiently searchfor top derivations, but only after an exhaus-tive 1-best pass.
We present a unified algo-rithm for k-best A?
parsing which preservesthe efficiency of k-best extraction while giv-ing the speed-ups of A?
methods.
Our algo-rithm produces optimal k-best parses under thesame conditions required for optimality in a1-best A?
parser.
Empirically, optimal k-bestlists can be extracted significantly faster thanwith other approaches, over a range of gram-mar types.1 IntroductionMany situations call for a parser to return the k-best parses rather than only the 1-best.
Uses fork-best lists include minimum Bayes risk decod-ing (Goodman, 1998; Kumar and Byrne, 2004),discriminative reranking (Collins, 2000; Char-niak and Johnson, 2005), and discriminative train-ing (Och, 2003; McClosky et al, 2006).
Themost efficient known algorithm for k-best parsing(Jime?nez and Marzal, 2000; Huang and Chiang,2005) performs an initial bottom-up dynamic pro-gramming pass before extracting the k-best parses.In that algorithm, the initial pass is, by far, the bot-tleneck (Huang and Chiang, 2005).In this paper, we propose an extension of A?parsing which integrates k-best search with an A?-based exploration of the 1-best chart.
A?
pars-ing can avoid significant amounts of computationby guiding 1-best search with heuristic estimatesof parse completion costs, and has been appliedsuccessfully in several domains (Klein and Man-ning, 2002; Klein and Manning, 2003c; Haghighiet al, 2007).
Our algorithm extends the speed-ups achieved in the 1-best case to the k-best caseand is optimal under the same conditions as a stan-dard A?
algorithm.
The amount of work done inthe k-best phase is no more than the amount ofwork done by the algorithm of Huang and Chiang(2005).
Our algorithm is also equivalent to stan-dard A?
parsing (up to ties) if it is terminated afterthe 1-best derivation is found.
Finally, our algo-rithm can be written down in terms of deductionrules, and thus falls into the well-understood viewof parsing as weighted deduction (Shieber et al,1995; Goodman, 1998; Nederhof, 2003).In addition to presenting the algorithm, weshow experiments in which we extract k-best listsfor three different kinds of grammars: the lexi-calized grammars of Klein and Manning (2003b),the state-split grammars of Petrov et al (2006),and the tree transducer grammars of Galley et al(2006).
We demonstrate that optimal k-best listscan be extracted significantly faster using our al-gorithm than with previous methods.2 A k-Best A?
Parsing AlgorithmWe build up to our full algorithm in several stages,beginning with standard 1-best A?
parsing andmaking incremental modifications.2.1 Parsing as Weighted DeductionOur algorithm can be formulated in terms ofprioritized weighted deduction rules (Shieber etal., 1995; Nederhof, 2003; Felzenszwalb andMcAllester, 2007).
A prioritized weighted deduc-tion rule has the form?1 : w1, .
.
.
, ?n : wnp(w1,...,wn)?????????
?0 : g(w1, .
.
.
, wn)where ?1, .
.
.
, ?n are the antecedent items of thededuction rule and ?0 is the conclusion item.
Adeduction rule states that, given the antecedents?1, .
.
.
, ?n with weights w1, .
.
.
, wn, the conclu-sion ?0 can be formed with weight g(w1, .
.
.
, wn)and priority p(w1, .
.
.
, wn).958These deduction rules are ?executed?
withina generic agenda-driven algorithm, which con-structs items in a prioritized fashion.
The algo-rithm maintains an agenda (a priority queue of un-processed items), as well as a chart of items al-ready processed.
The fundamental operation ofthe algorithm is to pop the highest priority item ?from the agenda, put it into the chart with its cur-rent weight, and form using deduction rules anyitems which can be built by combining ?
withitems already in the chart.
If new or improved,resulting items are put on the agenda with prioritygiven by p(?
).2.2 A?
ParsingThe A?
parsing algorithm of Klein and Manning(2003c) can be formulated in terms of weighteddeduction rules (Felzenszwalb and McAllester,2007).
We do so here both to introduce notationand to build to our final algorithm.First, we must formalize some notation.
As-sume we have a PCFG1 G and an input sentences1 .
.
.
sn of length n. The grammar G has a set ofsymbols ?, including a distinguished goal (root)symbol G. Without loss of generality, we assumeChomsky normal form, so each non-terminal ruler in G has the form r = A?
B C with weight wr(the negative log-probability of the rule).
Edgesare labeled spans e = (A, i, j).
Inside derivationsof an edge (A, i, j) are trees rooted at A and span-ning si+1 .
.
.
sj .
The total weight of the best (min-imum) inside derivation for an edge e is called theViterbi inside score ?(e).
The goal of the 1-bestA?
parsing algorithm is to compute the Viterbi in-side score of the edge (G, 0, n); backpointers al-low the reconstruction of a Viterbi parse in thestandard way.The basic A?
algorithm operates on deduc-tion items I(A, i, j) which represent in a col-lapsed way the possible inside derivations of edges(A, i, j).
We call these items inside edge items orsimply inside items where clear; a graphical rep-resentation of an inside item can be seen in Fig-ure 1(a).
The space whose items are inside edgesis called the edge space.These inside items are combined using the sin-gle IN deduction schema shown in Table 1.
Thisschema is instantiated for every grammar rule r1While we present the algorithm specialized to parsingwith a PCFG, it generalizes to a wide range of hypergraphsearch problems as shown in Klein and Manning (2001).VPs3s4s5s1s2... s6sn...VPVBZ NPDT NNs3s4s5VPG(a) (b)(c)VPVBZ1NP4DT NNs3s4s5(e)VP6s3s4s5VBZ NPDT NN(d)Figure 1: Representations of the different types ofitems used in parsing.
(a) An inside edge item:I(VP, 2, 5).
(b) An outside edge item: O(VP, 2, 5).
(c) An inside derivation item: D(TVP, 2, 5) for a treeTVP.
(d) A ranked derivation item: K(VP, 2, 5, 6).
(e) A modified inside derivation item (with back-pointers to ranked items): D(VP, 2, 5, 3,VP ?VBZ NP, 1, 4).in G. For IN, the function g(?)
simply sums theweights of the antecedent items and the gram-mar rule r, while the priority function p(?)
addsa heuristic to this sum.
The heuristic is a boundon the Viterbi outside score ?
(e) of an edge e;see Klein and Manning (2003c) for details.
Agood heuristic allows A?
to reach the goal itemI(G, 0, n) while constructing few inside items.If the heuristic is consistent, then A?
guaranteesthat whenever an inside item comes off the agenda,its weight is its true Viterbi inside score (Klein andManning, 2003c).
In particular, this guarantee im-plies that the goal item I(G, 0, n) will be poppedwith the score of the 1-best parse of the sentence.Consistency also implies that items are popped offthe agenda in increasing order of bounded Viterbiscores:?
(e) + h(e)We will refer to this monotonicity as the order-ing property of A?
(Felzenszwalb and McAllester,2007).
One final property implied by consistencyis admissibility, which states that the heuristicnever overestimates the true Viterbi outside scorefor an edge, i.e.
h(e) ?
?(e).
For the remain-der of this paper, we will assume our heuristicsare consistent.2.3 A Naive k-Best A?
AlgorithmDue to the optimal substructure of 1-best PCFGderivations, a 1-best parser searches over the spaceof edges; this is the essence of 1-best dynamicprogramming.
Although most edges can be built959Inside Edge Deductions (Used in A?
and KA?
)IN: I(B, i, l) : w1 I(C, l, j) : w2w1+w2+wr+h(A,i,j)??????????????
I(A, i, j) : w1 + w2 + wrTable 1: The deduction schema (IN) for building inside edge items, using a supplied heuristic.
This schema issufficient on its own for 1-best A?, and it is used in KA?.
Here, r is the rule A?
B C.Inside Derivation Deductions (Used in NAIVE)DERIV: D(TB , i, l) : w1 D(TC , l, j) : w2w1+w2+wr+h(A,i,j)??????????????
D(ATB TC, i, j): w1 + w2 + wrTable 2: The deduction schema for building derivations, using a supplied heuristic.
TB and TC denote full treestructures rooted at symbols B and C. This schema is the same as the IN deduction schema, but operates on thespace of fully specified inside derivations rather than dynamic programming edges.
This schema forms the NAIVEk-best algorithm.Outside Edge Deductions (Used in KA?
)OUT-B: I(G, 0, n) : w1w1???
O(G, 0, n) : 0OUT-L: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3w1+w3+wr+w2???????????
O(B, i, l) : w1 + w3 + wrOUT-R: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3w1+w2+wr+w3???????????
O(C, l, j) : w1 + w2 + wrTable 3: The deduction schemata for building ouside edge items.
The first schema is a base case that constructs anoutside item for the goal (G, 0, n) from the inside item I(G, 0, n).
The second two schemata build outside itemsin a top-down fashion.
Note that for outside items, the completion cost is the weight of an inside item rather thana value computed by a heuristic.Delayed Inside Derivation Deductions (Used in KA?
)DERIV: D(TB , i, l) : w1 D(TC , l, j) : w2 O(A, i, j) : w3w1+w2+wr+w3???????????
D(ATB TC, i, j): w1 + w2 + wrTable 4: The deduction schema for building derivations, using exact outside scores computed using OUT deduc-tions.
The dependency on the outside item O(A, i, j) delays building derivation items until exact Viterbi outsidescores have been computed.
This is the final search space for the KA?
algorithm.Ranked Inside Derivation Deductions (Lazy Version of NAIVE)BUILD: K(B, i, l, u) : w1 K(C, l, j, v) : w2w1+w2+wr+h(A,i,j)??????????????
D(A, i, j, l, r, u, v) : w1 + w2 + wrRANK: D1(A, i, j, ?)
: w1 .
.
.
Dk(A, i, j, ?)
: wkmaxm wm+h(A,i,j)?????????????
K(A, i, j, k) : maxm wmTable 5: The schemata for simultaneously building and ranking derivations, using a supplied heuristic, for the lazierform of the NAIVE algorithm.
BUILD builds larger derivations from smaller ones.
RANK numbers derivationsfor each edge.
Note that RANK requires distinct Di, so a rank k RANK rule will first apply (optimally) as soon asthe kth-best inside derivation item for a given edge is removed from the queue.
However, it will also still formallyapply (suboptimally) for all derivation items dequeued after the kth.
In practice, the RANK schema need not beimplemented explicitly ?
one can simply assign a rank to each inside derivation item when it is removed from theagenda, and directly add the appropriate ranked inside item to the chart.Delayed Ranked Inside Derivation Deductions (Lazy Version of KA?
)BUILD: K(B, i, l, u) : w1 K(C, l, j, v) : w2 O(A, i, j) : w3w1+w2+wr+w3???????????
D(A, i, j, l, r, u, v) : w1 + w2 + wrRANK: D1(A, i, j, ?)
: w1 .
.
.
Dk(A, i, j, ?)
: wk O(A, i, j) : wk+1maxm wm+wk+1????????????
K(A, i, j, k) : maxm wmTable 6: The deduction schemata for building and ranking derivations, using exact outside scores computed fromOUT deductions, used for the lazier form of the KA?
algorithm.960using many derivations, each inside edge itemwill be popped exactly once during parsing, witha score and backpointers representing its 1-bestderivation.However, k-best lists involve suboptimalderivations.
One way to compute k-best deriva-tions is therefore to abandon optimal substructureand dynamic programming entirely, and to searchover the derivation space, the much larger spaceof fully specified trees.
The items in this space arecalled inside derivation items, or derivation itemswhere clear, and are of the form D(TA, i, j), spec-ifying an entire tree TA rooted at symbol A andspanning si+1 .
.
.
sj (see Figure 1(c)).
Derivationitems are combined using the DERIV schema ofTable 2.
The goals in this space, representing rootparses, are any derivation items rooted at symbolG that span the entire input.In this expanded search space, each distinctparse has its own derivation item, derivable onlyin one way.
If we continue to search long enough,we will pop multiple goal items.
The first k whichcome off the agenda will be the k-best derivations.We refer to this approach as NAIVE.
It is very in-efficient on its own, but it leads to the full algo-rithm.The correctness of this k-best algorithm followsfrom the correctness of A?
parsing.
The derivationspace of full trees is simply the edge space of amuch larger grammar (see Section 2.5).Note that the DERIV schema?s priority includesa heuristic just like 1-best A?.
Because of thecontext freedom of the grammar, any consistentheuristic for inside edge items usable in 1-best A?is also consistent for inside derivation items (andvice versa).
In particular, the 1-best Viterbi out-side score for an edge is a ?perfect?
heuristic forany derivation of that edge.While correct, NAIVE is massively inefficient.In comparison with A?
parsing over G, where thereare O(n2) inside items, the size of the derivationspace is exponential in the sentence length.
Bythe ordering property, we know that NAIVE willprocess all derivation items d with?
(d) + h(d) ?
?
(gk)where gk is the kth-best root parse and ?(?)
is theinside score of a derivation item (analogous to ?for edges).2 Even for reasonable heuristics, this2The new symbol emphasizes that ?
scores a specificderivation rather than a minimum over a set of derivations.number can be very large; see Section 3 for empir-ical results.This naive algorithm is, of course, not novel, ei-ther in general approach or specific computation.Early k-best parsers functioned by abandoning dy-namic programming and performing beam searchon derivations (Ratnaparkhi, 1999; Collins, 2000).Huang (2005) proposes an extension of Knuth?salgorithm (Knuth, 1977) to produce k-best listsby searching in the space of derivations, whichis essentially this algorithm.
While Huang (2005)makes no explicit mention of a heuristic, it wouldbe easy to incorporate one into their formulation.2.4 A New k-Best A?
ParserWhile NAIVE suffers severe performance degra-dation for loose heuristics, it is in fact very effi-cient if h(?)
is ?perfect,?
i.e.
h(e) = ?
(e) ?e.
Inthis case, the ordering property of A?
guaranteesthat only inside derivation items d satisfying?
(d) + ?
(d) ?
?
(gk)will be placed in the chart.
The set of derivationitems d satisfying this inequality is exactly the setwhich appear in the k-best derivations of (G, 0, n)(as always, modulo ties).
We could therefore useNAIVE quite efficiently if we could obtain exactViterbi outside scores.One option is to compute outside scores withexhaustive dynamic programming over the orig-inal grammar.
In a certain sense, described ingreater detail below, this precomputation of exactheuristics is equivalent to the k-best extraction al-gorithm of Huang and Chiang (2005).
However,this exhaustive 1-best work is precisely what wewant to use A?
to avoid.Our algorithm solves this problem by integrat-ing three searches into a single agenda-driven pro-cess.
First, an A?
search in the space of insideedge items with an (imperfect) external heuristich(?)
finds exact inside scores.
Second, exact out-side scores are computed from inside and outsideitems.
Finally, these exact outside scores guide thesearch over derivations.
It can be useful to imag-ine these three operations as operating in phases,but they are all interleaved, progressing in order oftheir various priorities.In order to calculate outside scores, we intro-duce outside items O(A, i, j), which representbest derivations of G ?
s1 .
.
.
si A sj+1 .
.
.
sn;see Figure 1(b).
Where the weights of inside items961compute Viterbi inside scores, the weights of out-side items compute Viterbi outside scores.Table 3 shows deduction schemata for buildingoutside items.
These schemata are adapted fromthe schemata used in the general hierarchical A?algorithm of Felzenszwalb and McAllester (2007).In that work, it is shown that such schemata main-tain the property that the weight of an outside itemis the true Viterbi outside score when it is removedfrom the agenda.
They also show that outsideitems o follow an ordering property, namely thatthey are processed in increasing order of?
(o) + ?
(o)This quantity is the score of the best root deriva-tion which includes the edge corresponding to o.Felzenszwalb and McAllester (2007) also showthat both inside and outside items can be processedon the same queue and the ordering property holdsjointly for both types of items.If we delay the construction of a derivationitem until its corresponding outside item has beenpopped, then we can gain the benefits of using anexact heuristic h(?)
in the naive algorithm.
We re-alize this delay by modifying the DERIV deduc-tion schema as shown in Table 4 to trigger on andprioritize with the appropriate outside scores.We now have our final algorithm, which we callKA?.
It is the union of the IN, OUT, and new ?de-layed?
DERIV deduction schemata.
In words, ouralgorithm functions as follows: we initialize theagenda with I(si, i ?
1, i) and D(si, i ?
1, i) fori = 1 .
.
.
n. We compute inside scores in standardA?
fashion using the IN deduction rule, using anyheuristic we might provide to 1-best A?.
Once theinside item I(G, 0, n) is found, we automaticallybegin to compute outside scores via the OUT de-duction rules.
Once O(si, i ?
1, i) is found, wecan begin to also search in the space of deriva-tion items, using the perfect heuristics given bythe just-computed outside scores.
Note, however,that all computation is done with a single agenda,so the processing of all three types of items is in-terleaved, with the k-best search possibly termi-nating without a full inside computation.
As withNAIVE, the algorithm terminates when a k-th goalderivation is dequeued.2.5 CorrectnessWe prove the correctness of this algorithm by a re-duction to the hierarchical A?
(HA?)
algorithm ofFelzenszwalb and McAllester (2007).
The inputto HA?
is a target grammar Gm and a list of gram-mars G0 .
.
.Gm?1 in which Gt?1 is a relaxed pro-jection of Gt for all t = 1 .
.
.m.
A grammar Gt?1is a projection of Gt if there exists some onto func-tion pit : ?t 7?
?t?1 defined for all symbols in Gt.We use At?1 to represent pit(At).
A projection isrelaxed if, for every rule r = At ?
BtCt withweight wr there is a rule r?
= At?1 ?
Bt?1Ct?1in Gt?1 with weight wr?
?
wr.We assume that our external heuristic functionh(?)
is constructed by parsing our input sentencewith a relaxed projection of our target grammar.This assumption, though often true anyway, isto allow proof by reduction to Felzenszwalb andMcAllester (2007).3We construct an instance of HA?
as follows: LetG0 be the relaxed projection which computes theheuristic.
Let G1 be the input grammar G, and letG2, the target grammar of our HA?
instance, be thegrammar of derivations in G formed by expandingeach symbol A in G to all possible inside deriva-tions TA rooted atA.
The rules in G2 have the formTA ?
TB TC with weight given by the weight ofthe rule A ?
B C. By construction, G1 is a re-laxed projection of G2; by assumption G0 is a re-laxed projection of G1.
The deduction rules thatdescribe KA?
build the same items as HA?
withsame weights and priorities, and so the guaranteesfrom HA?
carry over to KA?.We can characterize the amount of work doneusing the ordering property.
Let gk be the kth-bestderivation item for the goal edge g. Our algorithmprocesses all derivation items d, outside items o,and inside items i satisfying?
(d) + ?
(d) ?
?(gk)?
(o) + ?
(o) ?
?(gk)?
(i) + h(i) ?
?
(gk)We have already argued that the set of deriva-tion items satisfying the first inequality is the set ofsubtrees that appear in the optimal k-best parses,modulo ties.
Similarly, it can be shown that thesecond inequality is satisfied only for edges thatappear in the optimal k-best parses.
The last in-equality characterizes the amount of work done inthe bottom-up pass.
We compare this to 1-best A?,which pops all inside items i satisfying?
(i) + h(i) ?
?
(g) = ?(g1)3KA?
is correct for any consistent heuristic but a non-reductive proof is not possible in the present space.962Thus, the ?extra?
inside items popped in thebottom-up pass during k-best parsing as comparedto 1-best parsing are those items i satisfying?
(g1) ?
?
(i) + h(i) ?
?
(gk)The question of how many items satisfy theseinequalities is empirical; we show in our experi-ments that it is small for reasonable heuristics.
Atworst, the bottom-up phase pops all inside itemsand reduces to exhaustive dynamic programming.Additionally, it is worth noting that our algo-rithm is naturally online in that it can be stoppedat any k without advance specification.2.6 Lazy Successor FunctionsThe global ordering property guarantees that wewill only dequeue derivation fragments of topparses.
However, we will enqueue all combina-tions of such items, which is wasteful.
By ex-ploiting a local ordering amongst derivations, wecan be more conservative about combination andgain the advantages of a lazy successor function(Huang and Chiang, 2005).To do so, we represent inside derivations notby explicitly specifying entire trees, but ratherby using ranked backpointers.
In this represen-tation, inside derivations are represented in twoways, shown in Figure 1(d) and (e).
The firstway (d) simply adds a rank u to an edge, givinga tuple (A, i, j, u).
The corresponding item is theranked derivation item K(A, i, j, u), which repre-sents the uth-best derivation of A over (i, j).
Thesecond representation (e) is a backpointer of theform (A, i, j, l, r, u, v), specifying the derivationformed by combining the uth-best derivation of(B, i, l) and the vth-best derivation of (C, l, j) us-ing rule r = A?
B C. The corresponding itemsD(A, i, j, l, r, u, v) are the new form of our insidederivation items.The modified deduction schemata for theNAIVE algorithm over these representations areshown in Table 5.
The BUILD schema pro-duces new inside derivation items from rankedderivation items, while the RANK schema as-signs each derivation item a rank; together theyfunction like DERIV.
We can find the k-best listby searching until K(G, 0, n, k) is removed fromthe agenda.
The k-best derivations can thenbe extracted by following the backpointers forK(G, 0, n, 1) .
.
.
K(G, 0, n, k).
The KA?
algo-rithm can be modified in the same way, shown inTable 6.1550500HeuristicDerivation items pushed (millions)5-split 4-split 3-split 2-split 1-split 0-splitNAIVEKA*Figure 2: Number of derivation items enqueued as afunction of heuristic.
Heuristics are shown in decreas-ing order of tightness.
The y-axis is on a log-scale.The actual laziness is provided by addition-ally delaying the combination of ranked items.When an item K(B, i, l, u) is popped off thequeue, a naive implementation would loop overitems K(C, l, j, v) for all v, C, and j (andsimilarly for left combinations).
Fortunately,little looping is actually necessary: there isa partial ordering of derivation items, namely,that D(A, i, j, l, r, u, v) will have a lower com-puted priority than D(A, i, j, l, r, u ?
1, v) andD(A, i, j, l, r, u, v ?
1) (Jime?nez and Marzal,2000).
So, we can wait until one of the latter twois built before ?triggering?
the construction of theformer.
This triggering is similar to the ?lazy fron-tier?
used by Huang and Chiang (2005).
All of ourexperiments use this lazy representation.3 Experiments3.1 State-Split GrammarsWe performed our first experiments with the gram-mars of Petrov et al (2006).
The training pro-cedure for these grammars produces a hierarchyof increasingly refined grammars through state-splitting.
We followed Pauls and Klein (2009) incomputing heuristics for the most refined grammarfrom outside scores for less-split grammars.We used the Berkeley Parser4 to learn suchgrammars from Sections 2-21 of the Penn Tree-bank (Marcus et al, 1993).
We trained with 6split-merge cycles, producing 7 grammars.
Wetested these grammars on 100 sentences of lengthat most 30 of Section 23 of the Treebank.
Our?target grammar?
was in all cases the most splitgrammar.4http://berkeleyparser.googlecode.com9630 2000 4000 6000 8000 10000050001500025000KA*kItemspushed(millions) K BestBottom-upHeuristic0 2000 4000 6000 8000 10000050001500025000EXHkItemspushed(millions) K BestBottom-upFigure 3: The cost of k-best extraction as a function of k for state-split grammars, for both KA?
and EXH.
Theamount of time spent in the k-best phase is negligible compared to the cost of the bottom-up phase in both cases.Heuristics computed from projections to suc-cessively smaller grammars in the hierarchy formsuccessively looser bounds on the outside scores.This allows us to examine the performance as afunction of the tightness of the heuristic.
We firstcompared our algorithm KA?
against the NAIVEalgorithm.
We extracted 1000-best lists using eachalgorithm, with heuristics computed using each ofthe 6 smaller grammars.In Figure 2, we evaluate only the k-best extrac-tion phase by plotting the number of derivationitems and outside items added to the agenda asa function of the heuristic used, for increasinglyloose heuristics.
We follow earlier work (Paulsand Klein, 2009) in using number of edges pushedas the primary, hardware-invariant metric for eval-uating performance of our algorithms.5 WhileKA?
scales roughly linearly with the looseness ofthe heuristic, NAIVE degrades very quickly as theheuristics get worse.
For heuristics given by gram-mars weaker than the 4-split grammar, NAIVE ranout of memory.Since the bottom-up pass of k-best parsing isthe bottleneck, we also examine the time spentin the 1-best phase of k-best parsing.
As a base-line, we compared KA?
to the approach of Huangand Chiang (2005), which we will call EXH (seebelow for more explanation) since it requires ex-haustive parsing in the bottom-up pass.
We per-formed the exhaustive parsing needed for EXHin our agenda-based parser to facilitate compar-ison.
For KA?, we included the cost of com-puting the heuristic, which was done by runningour agenda-based parser exhaustively on a smallergrammar to compute outside items; we chose the5We found that edges pushed was generally well corre-lated with parsing time.0 2000 4000 6000 8000 1000002006001000KA*kItemspushed(millions) K BestBottom-upHeuristicFigure 4: The performance of KA?
for lexicalizedgrammars.
The performance is dominated by the com-putation of the heuristic, so that both the bottom-upphase and the k-best phase are barely visible.3-split grammar for the heuristic since it gives thebest overall tradeoff of heuristic and bottom-upparsing time.
We separated the items enqueuedinto items enqueued while computing the heuris-tic (not strictly part of the algorithm), inside items(?bottom-up?
), and derivation and outside items(together ?k-best?).
The results are shown in Fig-ure 3.
The cost of k-best extraction is clearlydwarfed by the the 1-best computation in bothcases.
However, KA?
is significantly faster overthe bottom-up computations, even when the costof computing the heuristic is included.3.2 Lexicalized ParsingWe also experimented with the lexicalized parsingmodel described in Klein and Manning (2003b).This model is constructed as the product of adependency model and the unlexicalized PCFGmodel in Klein and Manning (2003a).
We9640 2000 4000 6000 8000 10000050015002500KA*kItemspushed(millions) K BestBottom-upHeuristic0 2000 4000 6000 8000 10000050015002500EXHkItemspushed(millions) K BestBottom-upFigure 5: k-best extraction as a function of k for tree transducer grammars, for both KA?
and EXH.constructed these grammars using the StanfordParser.6 The model was trained on Sections 2-20of the Penn Treebank and tested on 100 sentencesof Section 21 of length at most 30 words.For this grammar, Klein and Manning (2003b)showed that a very accurate heuristic can be con-structed by taking the sum of outside scores com-puted with the dependency model and the PCFGmodel individually.
We report performance as afunction of k for KA?
in Figure 4.
Both NAIVEand EXH are impractical on these grammars dueto memory limitations.
For KA?, computing theheuristic is the bottleneck, after which bottom-upparsing and k-best extraction are very fast.3.3 Tree Transducer GrammarsSyntactic machine translation (Galley et al, 2004)uses tree transducer grammars to translate sen-tences.
Transducer rules are synchronous context-free productions that have both a source and a tar-get side.
We examine the cost of k-best parsing inthe source side of such grammars with KA?, whichcan be a first step in translation.We extracted a grammar from 220 millionwords of Arabic-English bitext using the approachof Galley et al (2006), extracting rules with atmost 3 non-terminals.
These rules are highly lex-icalized.
About 300K rules are applicable for atypical 30-word sentence; we filter the rest.
Wetested on 100 sentences of length at most 40 fromthe NIST05 Arabic-English test set.We used a simple but effective heuristic forthese grammars, similar to the FILTER heuristicsuggested in Klein and Manning (2003c).
We pro-jected the source projection to a smaller grammarby collapsing all non-terminal symbols to X, and6http://nlp.stanford.edu/software/also collapsing pre-terminals into related clusters.For example, we collapsed the tags NN, NNS,NNP, and NNPS to N. This projection reducedthe number of grammar symbols from 149 to 36.Using it as a heuristic for the full grammar sup-pressed ?
60% of the total items (Figure 5).4 Related WorkWhile formulated very differently, one limitingcase of our algorithm relates closely to the EXHalgorithm of Huang and Chiang (2005).
In par-ticular, if all inside items are processed before anyderivation items, the subsequent number of deriva-tion items and outside items popped by KA?
isnearly identical to the number popped by EXH inour experiments (both algorithms have the sameordering bounds on which derivation items arepopped).
The only real difference between the al-gorithms in this limited case is that EXH placesk-best items on local priority queues per edge,while KA?
makes use of one global queue.
Thus,in addition to providing a method for speedingup k-best extraction with A?, our algorithm alsoprovides an alternate form of Huang and Chiang(2005)?s k-best extraction that can be phrased in aweighted deduction system.5 ConclusionsWe have presented KA?, an extension of A?
pars-ing that allows extraction of optimal k-best parseswithout the need for an exhaustive 1-best pass.
Wehave shown in several domains that, with an ap-propriate heuristic, our algorithm can extract k-best lists in a fraction of the time required by cur-rent approaches to k-best extraction, giving thebest of both A?
parsing and efficient k-best extrac-tion, in a unified procedure.965ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL).Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of theSeventeenth International Conference on MachineLearning (ICML).P.
Felzenszwalb and D. McAllester.
2007.
The gener-alized A* architecture.
Journal of Artificial Intelli-gence Research.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translationrule?
In Human Language Technologies: The An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics (HLT-ACL).Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In TheAnnual Conference of the Association for Compu-tational Linguistics (ACL).Joshua Goodman.
1998.
Parsing Inside-Out.
Ph.D.thesis, Harvard University.Aria Haghighi, John DeNero, and Dan Klein.
2007.Approximate factoring for A* search.
In Proceed-ings of HLT-NAACL.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the International Work-shop on Parsing Technologies (IWPT), pages 53?64.Liang Huang.
2005.
Unpublished manuscript.http://www.cis.upenn.edu/?lhuang3/knuth.pdf.V?
?ctor M. Jime?nez and Andre?s Marzal.
2000.
Com-putation of the n best parse trees for weighted andstochastic context-free grammars.
In Proceedingsof the Joint IAPR International Workshops on Ad-vances in Pattern Recognition, pages 183?192, Lon-don, UK.
Springer-Verlag.Dan Klein and Christopher D. Manning.
2001.
Parsingand hypergraphs.
In IWPT, pages 123?134.Dan Klein and Chris Manning.
2002.
Fast exact in-ference with a factored model for natural languageprocessing,.
In Proceedings of NIPS.Dan Klein and Chris Manning.
2003a.
Accurate unlex-icalized parsing.
In Proceedings of the North Amer-ican Chapter of the Association for ComputationalLinguistics (NAACL).Dan Klein and Chris Manning.
2003b.
Factored A*search for models over sequences and trees.
In Pro-ceedings of the International Joint Conference onArtificial Intelligence (IJCAI).Dan Klein and Christopher D. Manning.
2003c.
A*parsing: Fast exact Viterbi parse selection.
InIn Proceedings of the Human Language Technol-ogy Conference and the North American Associationfor Computational Linguistics (HLT-NAACL), pages119?126.Donald Knuth.
1977.
A generalization of Dijkstra?salgorithm.
Information Processing Letters, 6(1):1?5.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In Proceedings of The Annual Conference ofthe North American Chapter of the Association forComputational Linguistics (NAACL).M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: ThePenn Treebank.
In Computational Linguistics.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of The Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL), pages 152?159.Mark-Jan Nederhof.
2003.
Weighted deductive pars-ing and Knuth?s algorithm.
Computationl Linguis-tics, 29(1):135?143.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics (ACL), pages 160?167, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Adam Pauls and Dan Klein.
2009.
Hierarchical searchfor parsing.
In Proceedings of The Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics (NAACL).Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofCOLING-ACL 2006.Adwait Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
In Ma-chine Learning, volume 34, pages 151?5175.Stuart M. Shieber, Yves Schabes, and Fernando C. N.Pereira.
1995.
Principles and implementation ofdeductive parsing.
Journal of Logic Programming,24:3?36.966
