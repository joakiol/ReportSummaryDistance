Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 170?173,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsKX: A flexible system for Keyphrase eXtractionEmanuele Pianta Fondazione Bruno Kessler Trento, Italy.
pianta@fbk.euSara Tonelli Fondazione Bruno Kessler Trento, Italy.
satonelli@fbk.eu     AbstractIn this paper we present KX, a system for key-phrase extraction developed at FBK-IRST, which exploits basic linguistic annotation combined with simple statistical measures to select a list of weighted keywords from a document.
The system is flexible in that it of-fers to the user the possibility of setting pa-rameters such as frequency thresholds for col-location extraction and indicators for key-phrase relevance, as well as it allows for do-main adaptation exploiting a corpus of docu-ments in an unsupervised way.
KX is also eas-ily adaptable to new languages in that it re-quires only a PoS-Tagger to derive lexical pat-terns.
In the SemEval task 5 ?Automatic Key-phrase Extraction from Scientific Articles?, KX performance achieved satisfactory results both in finding reader-assigned keywords and in the combined keywords subtask.
1 Introduction Keyphrases are expressions, either single words or phrases, describing the most important con-cepts of a document.
As such, a list of key-phrases provides an approximate but useful char-acterization of the content of a text and can be used in a number of interesting ways both for human and automatic processing.
For example, keyphrases provide a sort of quick summary of a document.
This can be exploited not only in automatic summarization tasks, but also to en-able quick topic search over a number of docu-ments indexed according to their keywords, which is more precise and efficient than full-text search.
Once the keywords of a document collec-tion are known, they can also be used to calculate semantic similarity between documents and to cluster the texts according to such similarity (Ricca et al 2004).
Also, keyword extraction can be used as an intermediate step for automatic sense extraction (Jones et al 2002).For these reasons, the keyphrase extraction task proposed at SemEval 2010 raised much at-tention among NLP researchers, with 20 groups participating to the competition.
In this frame-work, we presented the KX system, specifically tuned to identify keyphrases in scientific articles.
In particular, the challenge comprised two sub-tasks: the extraction of reader-assigned and of author-assigned keyphrases in scientific articles from the ACM digital library.
The former are assigned to the articles by annotators, who can choose only keyphrases that occur in the docu-ment, while author-assigned keyphrases are not necessarily included in the text.
2 KX architecture A previous version of the KX system, named KXPat (Pianta, 2009), was developed to extract keyphrases from patent documents in the PatEx-pert project (www.patexpert.org).
The sys-tem employed in the SemEval task has additional parameters and has been tailored to identify key-phrases in scientific articles.
With KX, the identification of keyphrases can be accomplished with or without the help of a reference corpus, from which some statistical measures are computed in an unsupervised way.
We present here the general KX architecture, including the corpus-based pre-processing, even if in the SemEval task the information extracted from the corpus did not contribute as expected (see Section 3).
KX keyphrase extraction combines linguistic and statistical information, similar to (Frantzi et al, 2000) and is based on 4 steps.
The first three steps are carried out at corpus level, whereas the fourth one extracts information specific to each single document to be processed.
This means that the first three steps require a corpus C, preferably sharing the same domain of the document d from which the keyphrases should be extracted.
The fourth step, instead, is focused only on the170document d. The steps can be summarized as follows: Step 1: Extract from C the list NG-c of corpus n-grams, where an n-gram is any sequence of tokens in the text, for instance ?the sys-tem?, ?of the?, ?specifically built?.
Step 2: Select from the list NG-c a sub-list of multiword terms MW-c, that is combina-tions of words expressing a unitary con-cept, for instance ?light beam?
or ?access control?
Step 3: For each document in C, recognize and mark the multiword terms.
Calculate the inverse document frequency (IDF) for all words and multiword terms in the cor-pus.
Step 4: Given a document d from which a set of relevant keyphrases should be ex-tracted, count all words and multiword terms and rank them.
Step 1 is aimed at building a list of all possible n-grams in C. The maximum length of the selected n-grams can be set by the user.
For SemEval, beside one-token n-grams, we select 2-, 3- and 4-grams.
Since n-grams occurring only a few times are very unlikely to be useful for keyphrase recognition, they are cut off from the extracted list and excluded for further processing.
The fre-quency threshold can be set according to the ref-erence corpus dimensions.
For SemEval, we fixed the frequency threshold to 4.
In this step, a black-list was also used in order to exclude n-grams containing any of the stopwords in the list.
Such stopwords include for example ?every-thing?, ?exemplary?, ?preceding?, etc.
In Step 2, we select as multiword terms those n-grams that match certain lexical patterns.
To this purpose, we first analyze all n-grams with the MorphoPro morphological analyzer of the TextPro toolsuite (Pianta et al, 2006).
Then, we filter out the n-grams whose analysis does not correspond to a predefined set of lexical patterns.
For example, one of the patterns admitted for 4-grams is the following: [N]~[O]~[ASPGLU]~[NU].
This means that a 4-gram is a candidate multiword term if it is composed by a Noun, fol-lowed by ?of?
or ?for?
(defined as O), followed by either an Adjective, Singular noun, Past parti-ciple, Gerund, punctuation (L) or Unknown word, followed by either a Noun or Unknown word.
This is matched for example by the 4-gram ?subset [S] of [O] parent [S] peers [N]?.Both the lexical categories (e.g.
S for singular noun) and the admissible lexical patterns can be defined by the user.
In Step 3, multiword terms are recognized by combining local (document) and global (corpus) evidence.
To this purpose, we do not exploit as-sociation measures such as Log-Likelihood, or Mutual Information, but a simple frequency based criterion.
Two thresholds are defined: MinCorpus, which corresponds to the minimum number of occurrences of an n-gram in a refer-ence corpus, and MinDoc, i.e.
the minimum number of occurrences in the current document.
KX marks an n-gram in a document as a multiword term if it occurs at least MinCorpus times in the corpus or at least MinDoc times in the document.
The two parameters depend on the size of the corpus and the document respectively.
In SemEval, we found that the best thresholds are MinDoc=4 and MinCorpus=8.
A similar, fre-quency-based, strategy is used to solve ambigui-ties in how sequences of contiguous multiwords should be segmented.
For instance, given the sequence ?combined storage capability of sen-sors?
we need to decide whether we recognize ?combined storage capability?
or ?storage capa-bility of sensors?.
To this purpose, we calculate the strength of each alternative collocation as docFrequency * corpusFrequency, and then choose the stronger one.
To calculate IDF for each word and multiword term, we use the usual formula:   log( TotDocs / DocsContaningTerm ).
In step 4, we take into account a new docu-ment d, possibly not included in C, from which the keyphrases should be extracted.
First we rec-ognize and mark multiword terms, through the same algorithm used in Step 3.
Note that KX can recognize multiwords also in isolated documents, independently of any reference corpus, by acti-vating only the MinDoc parameter (see above).
Then, we count the frequency of words and multiword terms in d, obtaining a first list of keyphrases, ranked according to frequency.
Thus, frequency is the baseline ranking parame-ter, based on the assumption that important con-cepts are mentioned more frequently than less important ones.
After the creation of a frequency-based list of keyphrases, various techniques are used to re-rank it according to relevance.
In order to find the best ranking mechanism for the type of key-phrases we want to extract, different parameters can be set: ?
Inverse document frequency (IDF): this parameter takes into account the fact that a171concept that is mentioned in all documents is less relevant to our task than a concept occurring in few documents ?
Keyphrase length: number of tokens in a keyphrase.
Concepts expressed by longer phrases are expected to be more specific, and thus more relevant.
When this pa-rameter is activated, frequency is multi-plied by the keyphrase length.
?
Position of first occurrence: important concepts are expected to be mentioned be-fore less relevant ones.
If the parameter is activated, the frequency score will be mul-tiplied by the PosFact factor computed as (DistFromEnd / MaxIndex)pwr2, where MaxIndex is the length of the current document and DistFromEnd is MaxIndex minus the position of the first keyphrase occurrence in the text.
?
Shorter concept subsumption: In the key-phrase list, two concepts can occur, such that one is a specification of the other.
Concept subsumption and boosting are used to merge or re-rank such couples of concepts.
If a keyphrase is (stringwise) in-cluded in a longer keyphrase with a higher frequency, the frequency of the shorter keyphrase is transferred to the count of the longer one.
E.g.
?grid service discov-ery?=6 and ?grid service?=4 are re-ranked as ?grid service discovery?=10 and ?grid service?=0  ?
Longer concept boosting: If a keyphrase is included in a longer one with a lower frequency, the average score between the two keyphrase frequency is computed.
Such score is assigned to the less frequent keyphrase and subtracted from the fre-quency score of the higher ranked one.
For example, if ?grid service discovery?=4 and ?grid service?=6, the average fre-quency is 5, so that ?grid service discov-ery?=5 and ?grid service?
= 6?5=1.
This parameter can be activated alone or to-gether with another one that modifies the criterion for computing the boosting.
With this second option, the longer keyphrase is assigned the frequency of the shorter one.
For example, if ?grid service discovery?=4 and ?grid service?=6, the boosting gives ?grid service discovery?=6 and ?grid serv-ice?=6.
After the list of ranked keyphrases is extracted for each document, it is finally post-processed in two steps.
The post-processing phase has beenadded specifically for SemEval, because key-phrases do not usually need to be stemmed and acronym expansion is relevant only for the spe-cific genre of scientific articles.
For this reason, the two processes are not part of the official sys-tem architecture.
First, acronyms are replaced by the extended form, which is automatically extracted from the current document.
The algorithm for acronym detection scans for parenthetical expressions in the text and checks if a preceding text span can be considered a suitable correspondence (Nguyen and Kan, 2007).
The algorithm should detect cases in which the acronym appears after or before the extended form, like in ?Immediate Predecessors Tracking (IPT)?
and ?IPT (Imme-diate Predecessors Tracking)?.
If the acronym and the extended form appear both in the key-phrase list, only the extended form is kept and the acronym frequency is added.
The second step is stemming with the (Porter Stemmer).
Then, we check if the list of stemmed keyphrases contains duplicate entries.
If yes, we sum the frequencies of the double keyphrases and remove one of the two from the list.
3 Experimental Setup In the SemEval task, 144 training files were made available before the test data release.
We split them into a training/development set of 100 documents and a test set of 44 documents, in or-der to find the best parameter combination.
Key-phrase assignment is a subjective task and crite-ria for keyphrase identification depend on the domain and on the goal for which the keyphrases are needed.
For example in scientific articles longer keyphrases are often more informative than shorter ones, so the parameters for boosting longer concepts are particularly relevant.
We first tested all parameters in isolation to compute the improvement over the frequency-based baseline.
Results are reported in Table 1.
F1 is computed as the harmonic mean of preci-sion and recall over the 15 top-ranked key-phrases after stemming.
We report the combined F1, as computed by the task scorer in order to combine reader-assigned and author-assigned keyword sets.
Parameter F1 (combined) Baseline(MinDoc = 2) 13.63 Baseline(MinDoc = 4) 14.84 +CorpusColloc(small) 13.48     +CorpusColloc(big) 13.33 +IDF 17.98172+KeyphraseLength 16.78 +FirstPosition 16.18 +ShortConcSubsumption 16.03 +LongConcBoost(version1) 14.38 +LongConcBoost(version2) 13.93 MinDoc = 4, +FirstPosition,    +IDF,   +KeyphraseLength,   +ShortConcSubsumption,  +LongConcBoost(version1)25.62Table 1: Parameter performance over development set  The parameter scoring the highest improvement over the baseline is IDF.
Also the parameters boosting longer keyphrases and those that occur at the beginning of the text are effective.
Note that the LongConcBoost parameter achieves bet-ter results in the first version, which has a higher impact on the re-ranking.
Surprisingly, using a domain corpus to extract information about multiword terms, as described in Section 2, steps 1 - 3, does not achieve any improvement.
This means that KX can better recognize keyphrases in single documents without any corpus refer-ence.
Besides, the best setting for MinDoc, the minimum number of multiword occurrences in the current document (see Section 2) is 4.
We tested the CorpusColloc parameter using two different reference corpora: one contained the 100 articles of the training set (CorpusColloc small), while the other (CorpusColloc big) in-cluded both the 100 training articles and the 200 scientific publications of the NUS Keyphrase Corpus (Nguyen and Kan, 2007).
The perform-ance is worse using the larger corpus than the smaller one, and in both cases it is below the baseline obtained without any reference corpus.
In the bottom row of Table 1, the best pa-rameter combination is reported with the score obtained over the development set.
The im-provement over the baseline reaches 11.99 F1.
4 Evaluation In the SemEval task, the system was run on the test set (100 articles) with the best performing parameter combination described in the previous section.
The results obtained over the 15 top-ranked keyphrases are reported in Table 2.
Keyphrase type P R F1 Reader-assigned 20.33 25.33 22.56 Combined 23.60 24.15 23.87 Table 2: System performance over test set In the competition, the F1 score over reader-assigned keyphrases was ranked 3rd out of 20participants, while the combined measure  achieved the 7th best result out of 20.
5 Conclusions In this work we have described KX, a flexible system for keyphrase extraction, which achieved promising results in the SemEval task 5.
The good KX performance is due to its adaptable ar-chitecture, based on a set of parameters that can be tailored to the document type, the preferred keyphrase length, etc.
The system can also ex-ploit multiword lists (with frequency) extracted from a reference corpus, even if this feature did not improve KX performance in this specific task.
However, this proved to be relevant when applied to keyphrase extraction in the patent do-main, using a large domain-specific corpus of 10.000 very long documents (Pianta, 2009).
A limitation of KX in the task was that it ex-tracts only keyphrases already present in a given document, while the author-assigned subtask in the SemEval competition included also key-phrases that do not occur in the text.
Another improvement, which is now being implemented, is the extraction of the best parameter combina-tion using machine-learning techniques.
References  Jones, S., Lundy, S. and Paynter, G. W. 2002.
Interac-tive Document Summarization Using Automati-cally Extracted Keyphrases.
In Proc.
of the 35th Hawaii International Conference on System Sci-ences.
Frantzi, K., Ananiadou, S. and Mima, H. 2000.
Automatic recognition of multi-word terms: the C-value/NC-value method.
Journal on Digital Li-braries.
3 (2), pp.115-130.
Thuy Dung Nguyen and Min-Yen Kan. 2007.
Key-phrase Extraction in Scientific Documents.
In D.H.-L. Goh et al (eds.
): ICADL 2007, LNCS 4822, pp.
317-326.
Pianta, E., Girardi, C and Zanoli, R. 2006.
The TextPro tool suite.
In Proc.
of LREC.
Pianta, E. 2009.
Content Distillation from Patent Ma-terial, FBK Technical Report.
Ricca, F., Tonella, P., Girardi, C and Pianta, E. 2004.
An empirical study on Keyword-based Web Site Clustering.
In Proceedings of the 12th IWPC.
PorterStemmer: http://tartarus.org/~martin/PorterStemmer/perl.txt.173
