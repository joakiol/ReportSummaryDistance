Session 2: Language ModelingXuedong Huang, ChairMicrosoft ResearchMicrosoft CorporationOne Microsoft WayRedmond, WA 98052This session presented four interesting papers on statisticallanguage modeling aimed for improved large-vocabularyspeech recognition.
The basic problem in language modelingis to derive accurate underlying representations from a largeamount of training data, which shares the same fundamentalproblem as acoustic modeling.
As demonstrated in thissession, many techniques used for acoustic modeling can bewell extended to deal with problems in language modeling orvice versa.
One of the most important issues is how to makeeffective use of training data to characterize and exploitregularities innatural languages.
This is the common themeof four papers presented here.In the first paper, Ronald Rosenfeld from CMU described histhesis work on maximum entropy language modeling.
Themaximum entropy model is able to incorporate multipleconstraints consistently.
Although the maximum entropymodel is computationally expensive, itcould potentially helpspeech recognition significantly as the approach allows us toincorporate diverse linguistic phenomenon that can bedescribed in terms of statistics of the text.
With the maximumentropy approach, Ronald demonstrated that trigger-basedlanguage adaptation reduced the word error rate of CMU'sSphinx-II system by 10-14%.Rukmini Iyer from BU then presented her recent work onmixture language modeling.
The model is an m-componentmixture of conventional trigram models, which are derivedfrom clustered WSJ training data.
As we know, the mixtureacoustic model has significantly improved many state-of-the-art speech recognition systems.
Rukmini demonstrated herethat mixture language models also reduced the word error rateby 8% using the BU speech recognition system.Ryosuke Isotani from ATR described a very interestingmethod that integrates local and global anguage constraintsfor improved Japanese speech recognition.
The approachexploited the relationship of function words and contentwords, and used the combined language model for speechrecognition.
As a result, Ryosuke demonstrated that he worderror rate of the proposed language model was comparable tothat of the trigram language model, but the parameter sizewas significantly reduced.
It would be interesting to see if theproposed model can be applied to different languages, and ifit remains effective with a larger data base.Finally, Rich Schwartz from BBN presented a paper thataddresses three problems associated with language modeling.He first demonstrated that additional training datasubstantially improved the language model performance.Second, he introduced a method to minimize the differencebetween the language model training text and the way peoplespeak.
Third, he showed that by increasing the vocabularysize, the recognition accuracy did not degrade significantly.This somewhat alleviated problems associated with newwords in the test material.75
