Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 324?328,Dublin, Ireland, August 23-24, 2014.IIT Patna: Supervised Approach for Sentiment Analysis in TwitterRaja Selvarajan and Asif EkbalDepartment of Computer Science and EngineeringIndian Institute of Technology Patna, India{raja.cs10,asif}@iitp.ac.inAbstractIn this paper we report our works forSemEval-2014 Sentiment Analysis inTwitter evaluation challenge.
This is thefirst time we attempt for this task, andour submissions are based on supervisedmachine learning algorithm.
We use Sup-port Vector Machine for both the tasks,viz.
contextual polarity disambiguationand message polarity classification.
Weidentify and implement a small set offeatures for each the tasks, and did notmake use of any external resources and/ortools.
The systems are tuned on the devel-opment sets and finally blind evaluation isperformed on the respective test set, whichconsists of the datasets of five differentdomains.
Our submission for the firsttask shows the F-score values of 76.3%,77.04%, 70.91%, 72.25% and 66.32% forLiveJournal2014, SMS2013, Twitter2013,Twitter2014 and Twitter2014Sarcasmdatasets, respectively.
The system devel-oped for the second task yields the F-scorevalues of 54.68%, 40.56%, 50.32%,48.22% and 36.73%, respectively for thefive different test datasets.1 IntroductionDuring the past few years, the communications inthe forms of microblogging and text messaginghave emerged and become ubiquitous.
Opinionsand sentiments about the surrounding worlds arewidely expressed through the mediums of Twit-ter messages (Tweets) and Cell phone messages(SMS).
The availability of social content gener-ated on sites such as Twitter creates new opportu-This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/nities to automatically study public opinion.
Deal-ing with these informal text genres presents newchallenges for data mining and language process-ing techniques beyond those encountered whenworking with more traditional text genres such asnewswire.
Tweets and SMS messages are shortin length, usually a sentence or a headline ratherthan a document.
These texts are very informal innature and contains creative spellings and punctu-ation symbols (Nakov et al., 2013).
Text also con-tains lots of misspellings, slang, out-of-vocabularywords, URLs, and genre-specific terminology andabbreviations, e.g., RT for reTweet and #hash-tags.
The kind of these specific features pose greatchallenges for building various lexical and syntac-tic resources and/or tools, which are required forefficient processing of texts.
These aspects alsointroduce complexities to build the state-of-the-art data mining systems.
In recent times, therehas been a huge interest to mine and understandthe opinions and sentiments that people are com-municating in social media (Barbosa and Feng,2010; Bifet et al., 2011; Pak and Paroubek, 2010;Kouloumpis et al., 2011).
Recent studies showthe interests in sentiment analysis of Tweets acrossa variety of domains such as commerce (Jansenet al., 2009), health (Chew and Eysenbach, 2010;Salathe and Khandelwal, 2011) and disaster man-agement (Mandel et al., 2012).Another aspect of social media data, such astwitter messages, is that they include rich informa-tion about the individuals involved in the commu-nication.
For e.g., twitter maintains informationabout who follows whom.
ReTweets (reshares of aTweet) and tags inside of Tweets provide discourseinformation (Nakov et al., 2013).
Efficient mod-elling of such information is crucial in the sensethat it provides a mean to empirically study thesocial interactions where opinion is conveyed.Several corpora with detailed opinion and senti-ment annotation have been made freely available,324e.g., the MPQA corpus (Barbosa and Feng, 2005)of newswire text; i-sieve (Kouloumpis et al., 2011)and TASS corpus2 (Villena-Roman et al., 2013)for Twitter sentiment.
These resources were eitherin non-social media or they were small and propri-etary.
They further focused on message-level sen-timent.
The SemEval-2013 shared task (Nakov etal., 2013) on sentiment analysis in Twitter releasesSemEval Tweet corpus, which contains Tweetsand SMSmessages with sentiment expressions an-notated with contextual phrase-level polarity aswell as an overall message-level polarity.
Amongthe 44 submissions, the highest-performing sys-tem (Mohammad et al., 2013) made use of Sup-port Vector Machine (SVM) classifier.
It obtainedthe F-scores of 69.02% in the message-level taskand 88.93% in the term-level task.
Variety of fea-tures were implemented based on surface-forms,semantics, and sentiment features.
They generatedtwo large wordsentiment association lexicons, onefrom Tweets with sentiment-word hashtags, andone from Tweets with emoticons.
They showedthat in message-level task, the lexicon-based fea-tures gained 5 F-score points over all the others.SemEval-14 shared task1on sentiment analy-sis in Twitter is a continuing effort to promote theresearch in this direction.
Similar to the previ-ous year?s evaluation campaigns two primary taskswere addressed in this year challenge.
The firsttask (i.e.
Subtask A) deals with contextual polar-ity disambiguation and the second task (i.e.
Sub-task B) was about message polarity classification.For Subtask A, for a given message containing amarked instance of a word or phrase, the goal is todetermine whether that instance is positive, nega-tive or neutral in that context.
In Subtask B, for agiven message, the task is to classify whether themessage is of positive, negative, or neutral sen-timent.
For messages that convey both positiveand negative sentiments, the stronger one shouldbe chosen.In this paper we report on our submissions aspart of our first-time participation in this kind oftask (i.e.
sentiment classification).
We develop thesystems based on supervised machine learning al-gorithm, namely Support Vector Machine (SVM)(Joachims, 1999; Vapnik, 1995).
We identify andimplement a very small set of features that do notmake use of any external resources and/or tools.For each task the system is tuned on the devel-1http://alt.qcri.org/semeval2014/task9/opment data, and finally blind evaluation is per-formed on the test data.2 MethodsWe develop two systems, one for contextual polar-ity disambiguation and the other for message po-larity classification.
Each of the systems is basedon supervised machine learning algorithm, namelySVM.
Support vector machines (Joachims, 1999;Vapnik, 1995) have been shown to be highly ef-fective at traditional text categorization, generallyoutperforming many other classifiers such as naiveBayes (Joachims, 1999; Vapnik, 1995).
They arelarge-margin, rather than probabilistic, classifiers.For solving the two-class problem, the basic ideabehind the training procedure is to find a hyper-plane, represented by vector ~w, that not only sepa-rates the document vectors in one class from thosein the other, but for which the separation, or mar-gin, is as large as possible.
This search corre-sponds to a constrained optimization problem; let-ting cjin 1,-1 (corresponding to positive and neg-ative classes, respectively) be the correct class ofthe document dj, the solution could be written as:~w :=?jajcj~dj, aj>= 0where, the aj?s are obtained by solving a dual opti-mization problem.
Those~djsuch that ajis greaterthan zero are called support vectors, since they arethe only document vectors contributing to ~w.
Clas-sification of test instances consists simply of deter-mining which side of ~w?s hyperplane they fall on.2.1 PreprocessingWe pre-process Tweet to normalize it by replac-ing all ?URLs?
to ?http://url?
and all user-idsto ?
@usr?, and this is performed by the regularexpression based simple pattern matching tech-niques.
We remove punctuation markers from thestart and end positions of Tweets.
For e.g., ?theday is beautiful!?
is converted to ?the day is beauti-ful?.
Multiple whitespaces are replaced with singlewhitespace.
Stop-words are removed from eachreview.2.2 FeaturesIn this work we use same set of features for boththe tasks.
Each Tweet is represented as a vectorconsisting of the following features:1.
Local contexts: We extract the unigrams andbigrams from the training and test datasets.325A feature is defined that checks the occur-rences of these n-grams in a particular Tweetor phrase.2.
Upper case: This feature is binary valuedwith a value set to 1 if all the characters ofa phrase or Tweet are capitalized, and 0 oth-erwise.
This indicates that the target messageor context contains either positive or negativesentiment.3.
Elongated words: The feature checkswhether a word contains a character that re-peats more than twice.
This indicates thepresence of a positive sentiment word in thesurrounding.
This was defined in lines withthe one reported in (Mohammad et al., 2013).4.
Hash tags: This feature checks the numberof hash tags in the Tweet.
The value of thisfeature is set equal to the absolute number offeatures.5.
Repeated characters: This feature checkswhether the word(s) have at least threeconsecutive repeated characters (e.g.,happppppppy, hurrrrrey etc.).
In such cases,the words are normalized to contain onlyupto two repeated characters.
This helps tocapture the words having similar structures.6.
Negated contexts: A negated word can af-fect the polarity of the target word.
A negatedsegment is defined as a sequence of tokensthat starts with a negation word (e..g, no,couldn?t etc.)
and ends with a punctuationmarks (e.g.,,,., :, ;, !, ?).
All the words follow-ing the negation word are suffixed with NEG-ATIVE, and the polarity features are alsoconverted with NEGATIVE in line with (Mo-hammad et al., 2013).3 Experimental Results and AnalysisThe SemEval-2014 shared task datasets are basedon SemEval-2013 competition datasets.
It coversa range of topics, including a mixture of entities,products and events.
Keywords and Twitter hash-tags were used to identify messages relevant to theselected topic.
The selected test sets were takenfrom the five different domains.
We perform ex-periment with the python based NLTK toolki2.
We2http://www.nltk.org/Class precision recall F-scorePositive 72.02 90.45 80.19Negative 76.86 53.70 63.23Neutral 7.69 22.22 3.45Average 52.19 55.46 53.77Table 1: Results on development set for Task-A(%).Class precision recall F-scorePositive 49.92 63.75 55.99Negative 42.59 31.94 36.51Neutral 59.54 53.49 56.35Average 50.68 49.73 66.39Table 2: Results on development set for Task-B (in%).carried out experiments with the different classi-fiers.
However we report the results of SVM asit produced the highest accuracy with respect tothis particular feature set.
We use the default pa-rameters of SVM as implemented in this toolkit.We submitted two runs, one for each task.
Bothof our submissions were constrained in nature, i.e.we did not make use of any additional resourcesand/or tools to build our systems.We perform several experiments using the de-velopment set.
Best results are reported in Table 1and Table 2 for Task-A and Task-B, respectively.Evaluation shows that for message polarity dis-ambiguation we obtain the average precision, re-call and F-score values of 52.19%, 55.46% and53.77%, respectively.
For message polarity clas-sification we obtain the precision, recall and F-Score values of 50.68%, 49.73% and 66.39%, re-spectively.
It is evident from the evaluation thatthe first task suffers most due to the problems inclassifying the tweets having neutral sentiments,whereas the second task faces difficulties in clas-sifying the negative sentiments.
We report the con-fusion matrices in Table 3 and Table 4 for the firstgs\pred positive negative neutralpositive 502 50 3negative 160 196 9neutral 35 9 1Table 3: Confusion matrix for A.
Here, gs: Goldstandard; pred: Predicted class.326gs\pred positive negative neutralpositive 313 43 135negative 102 92 94neutral 212 81 337Table 4: Confusion matrix for B.
Here, gs: Goldstandard; pred: Predicted class.and second development sets, respectively.
Erroranalysis suggests that most miss-classifications arebecause of the less number of neutral instancescompared to the positive and negative instances inTask-A.
For the Task-B training set, the numberof instances of positive and neutral sentiments arevery low compared to the negative sentiment.After tuning the systems on the developmentsets, we perform blind evaluation on the testdatasets.
Evaluation results on the test sets arereported in Table 5 for both the tasks.
Theevaluation is carried out based on the evaluationscripts as provided by the organizers.
For mes-sage polarity disambiguation we obtain the high-est F-score of 77.04% for the SMS data typein Task-A.
The system shows the F-scores of76.03%, 70.91%, 72.25% and 66.35% for Live-Journal2014, Twitter2013, Twitter2014 and Twit-ter2014sarcasm, respectively.
For the second taskthe system attains the highest F-score value of54.68% for the LiveJournal2014 dataset.
For theother datasets, the system shows the F-scores of40.56%, 50.32%, 48.22% and 36.73% for theSMS2013, Twitter2013 and Twitter2014Sarcasm,respectively.
We followed a simple approach thatneeds fine-tuning in many places.
Currently oursystems lack behind the best reported systems bymargins of approximately 11-18% F-scores forTask-A, and 19-30% F-scores for Task-B.4 ConclusionIn this paper we report our works as part of ourparticipation to the SemEval-14 shared task onsentiment analysis for Twitter data.
Our systemswere developed based on SVM.
We use a smallset of features, and did not make use of any ex-ternal resources and/or tools in any of the tasks.Each of the systems is tuned on the developmentset, and blind evaluation is performed on the testset.
Evaluation shows that our system achieves theF-score values in the ranges of 66-76% for Task-Aand 36-55% for Task-B.It is to be noted that this is our first participa-Task Test-set AverageF-scoreA LiveJournal2014 76.03SMS2013 77.04Twitter2013 70.91Twitter2014 72.25Twitter2014Sarcasm 66.35B LiveJournal2014 54.68SMS2013 40.56Twitter2013 50.32Twitter2014 48.22Twitter2014Sarcasm 36.73Table 5: Results on the test set.tion, and there are many ways to improve the per-formance of the models.
Firstly we would like toidentify more features in order to improve the ac-curacies.
We also plan to come up with proper setsof features for the two task.
Efficient feature se-lection techniques will be implemented to identifythe most effective feature set for each of the tasks.We would like to apply evolutionary optimizationtechniques to optimize the different issues of ma-chine learning algorithm.ReferencesLuciano Barbosa and Junlan Feng.
2005.
Robust Sen-timent Detection on Twitter from Biased and NoisyData.
39:2-3.Luciano Barbosa and Junlan Feng.
2010.
Robust Sen-timent Detection on Twitter from Biased and NoisyData.
In Proceedings of the 23rd International Con-ference on Computational Linguistics (COLING),Beijing, China.Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,and Ricard Gavald?a.
2011.
Detecting SentimentChange in Twitter Streaming Data.
Journal of Ma-chine Learning Research - Proceedings Track, 17:5?11.Cynthia Chew and Gunther Eysenbach.
2010.
Pan-demics in the Age of Twitter: Content Analysisof Tweets during the 2009 H1N1 Outbreak.
PLoSONE, 5(11):e14118+.Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Ab-dur Chowdury.
2009.
Twitter Power: Tweets asElectronic Word of Mouth.
Journal of the Ameri-can Society for Information Science and Technology,60(11):2169?2188.Thorsten Joachims, 1999.
Making Large Scale SVMLearning Practical, pages 169?184.
MIT Press,Cambridge, MA, USA.327Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter Sentiment Analysis: TheGood the Bad and the OMG!
In Proceedings ofthe Fifth International Conference on Weblogs andSocial Media, ICWSM, pages 538?541, Barcelona,Spain.Benjamin Mandel, Aron Culotta, John Boulahanis,Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.2012.
A Demographic Analysis of Online Senti-ment during Hurricane Irene.
In Proceedings ofthe Second Workshop on Language in Social Media,LSM 12, Stroudsburg.Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-dan Zhu.
2013.
NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets.
In Pro-ceedings of Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Sev-enth International Workshop on Semantic Evalua-tion (SemEval 2013), pages 321?327, Atlanta, Geor-gia.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment Analysis inTwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, USA, June.Alexander Pak and Patrick Paroubek.
2010.
TwitterBased System: Using Twitter for DisambiguatingSentiment Ambiguous Adjectives.
In Proceedingsof the 5th InternationalWorkshop on Semantic Eval-uation, SemEval 10, Los Angeles,USA.Marcel Salathe and Shashank Khandelwal.
2011.
As-sessing Vaccination Sentiments with Online SocialMedia: Implications for Infectious Disease Dynam-ics and Control.
PLoS Computational Biology,7(10):e14118+.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag New York, Inc.,New York, NY, USA.Julio Villena-Roman, Sara Lana-Serrano, EugenioMartnez-Camara, Jose Carlos Gonzalez, and Cristo-bal.
2013.
Tass - Workshop on Sentiment Analy-sis at SEPLN.
In Proceedings of Procesamiento delLenguaje Natural, pages 50:37?44.328
