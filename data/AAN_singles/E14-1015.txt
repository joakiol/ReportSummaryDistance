Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 135?144,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsSpecial Techniques for Constituent Parsing of Morphologically RichLanguagesZsolt Sz?ant?o, Rich?ard FarkasUniversity of SzegedDepartment of Informatics{szanto,rfarkas}@inf.u-szeged.huAbstractWe introduce three techniques for improv-ing constituent parsing for morphologi-cally rich languages.
We propose a novelapproach to automatically find an optimalpreterminal set by clustering morphologi-cal feature values and we conduct exper-iments with enhanced lexical models andfeature engineering for rerankers.
Thesetechniques are specially designed for mor-phologically rich languages (but they arelanguage-agnostic).
We report empiricalresults on the treebanks of five morpho-logically rich languages and show a con-siderable improvement in accuracy and inparsing speed as well.1 IntroductionFrom the viewpoint of syntactic parsing, thelanguages of the world are usually categorizedaccording to their level of morphological rich-ness (which is negatively correlated with config-urationality).
At one end, there is English, astrongly configurational language while there isHungarian at the other end of the spectrum withrich morphology and free word order (Fraser et al.,2013).
A large part of the methodology for syn-tactic parsing has been developed for English butmany other languages of the world are fundamen-tally different from English.
In particular, mor-phologically rich languages ?
the other end of theconfigurational spectrum ?
convey most sentence-level syntactic information by morphology (i.e.
atthe word level), not by configuration.
Because ofthese differences the parsing of morphologicallyrich languages requires techniques that differ fromor extend the methodology developed for English(Tsarfaty et al., 2013).
In this study, we presentthree techniques to improve constituent parsingand these special techniques are dedicated to han-dle the challenges of morphologically rich lan-guages.Constituency parsers have advanced consider-ably in the last two decades (Charniak, 2000;Charniak and Johnson, 2005; Petrov et al., 2006;Huang, 2008) boosted by the availability of thePenn Treebank (Marcus et al., 1993).
Whilethere is a progress on parsing English (especiallythe Penn Treebank), the treebanks of morphologi-cally rich languages have been attracted much lessattention.
For example, a big constituent treebankhas been available for Hungarian for almost 10years (Csendes et al., 2005) and to the best ofour knowledge our work is the first one report-ing results on this treebank.
One reason for themoderate level of interest in constituent parsing ofmorphologically rich languages is the widely heldbelief that dependency structures are better suitedfor representing syntactic analyses for morpho-logically rich languages than constituent represen-tations because they allow non-projective struc-tures (i.e.
discontinuous constituents).
From atheoretical point of view, Tsarfaty et al.
(2010)point out, however, this is not the same as prov-ing that dependency parsers function better thanconstituency parsers for parsing morphologicallyrich languages.
For a detailed discussion, pleasesee Fraser et al.
(2013).From an empirical point of view, the organiz-ers of the recent shared task on ?Statistical Pars-ing of Morphologically Rich Languages?
(Seddahet al., 2013) provided datasets only for languageshaving treebanks in both dependency and con-stituency format and their cross-framework evalu-ation ?
employing the unlabeled TedEval (Tsarfatyet al., 2012) as evaluation procedure ?
revealedthat at 4 out of 9 morphologically rich languages,the results of constituent parsers were higher thanthe scores achieved by the best dependency pars-ing system.
Based on these theoretical issues andempirical results, we support the conclusion of135Fraser et al.
(2013) that ?...
there is no clearevidence for preferring dependency parsing overconstituency parsing in analyzing languages withrich morphology and instead argue that researchin both frameworks is important.
?In this study, we propose answers to the twomain challenges of constituent parsing of mor-phologically rich languages, which are finding theoptimal preterminal set and handling the hugenumber of wordforms.
The size of the pretermi-nal set in the standard context free grammar envi-ronment is crucial.
If we use only the main POStags as preterminals, we lose a lot of informationencoded in the morphological description of thetokens.
On the other hand, using the full mor-phological description as preterminal yields a setof over a thousand preterminals, which results indata sparsity and performance problems as well.The chief contribution of this work is to propose anovel automatic procedure to find the optimal setof preterminals by merging morphological fea-ture values.
The main novelties of our approachover previous work are that it is very fast ?
itoperates inside a probabilistic context free gram-mar (PCFG) instead of using a parser as a blackbox with re-training for every evaluation of a fea-ture combination ?
and it can investigate particularmorphological feature values instead of removinga feature with all of its values.Another challenge is that because of the inflec-tional nature of morphologically rich languagesthe number of wordforms is much higher com-pared with English.
Hence the number ofunknown and very rare tokens ?
i.e.
the tokensthat do not appear in the training dataset ?
ishigher here, which hurts the performance of PCFGparsers.
Following Goldberg and Elhadad (2013),we enhance the lexical model by exploiting anexternal lexicon.
We investigate the applicabilitiesof fully supervised taggers instead of unsupervisedones for gathering external lexicons.Lastly, we introduce novel feature templatesfor an n-best reranker operating on the top of aPCFG parser.
These feature templates are exploit-ing atomic morphological features and achieveimprovements over the standard feature set engi-neered for English.We conducted experiments by the above men-tioned three techniques on Basque, French, Ger-man, Hebrew and Hungarian, five morphologi-cally rich languages.
The BerkeleyParser (Petrovet al., 2006) enriched with these three techniquesachieved state-of-the-art results on each language.2 Related WorkConstituent parsing of English is a well researchedarea.
The field has been dominated by data-driven,i.e.
treebank-based statistical approaches in thelast two decades (Charniak, 2000; Charniak andJohnson, 2005; Petrov et al., 2006).
We extendhere BerkeleyParser (Petrov et al., 2006), whichis a PCFG parser using latent annotations at non-terminals.
Its basic idea is to iteratively split eachnon-terminal into subsymbols thus capturing thedifferent subusage of them instead of manuallydesigned annotations.The constituent parsing of morphologicallyrich languages is a much less investigated field.There exist constituent treebanks for several lan-guages along with a very limited number ofparsing reports on them.
For instance, Petrov(2009) trained BerkeleyParser on Arabic, Bulgar-ian, French, German and Italian and he reportedgood accuracies, but there has been previous workon Hebrew (Goldberg and Elhadad, 2013), Korean(Choi et al., 1994) and Spanish (Le Roux et al.,2012) etc.
The recently organized ?Statistical Pars-ing of Morphologically Rich Languages?
(Seddahet al., 2013) addressed the dependency and con-stituency parsing of nine morphologically rich lan-guages and provides useful benchmark datasetsfor these languages.Our chief contribution in this paper is a pro-cedure to merge preterminal labels.
The relatedwork for this line of research includes the studieson manual refinement of preterminal sets such asMarton et al.
(2010) and Le Roux et al.
(2012).The most closely related approach to our proposalis Dehdari et al.
(2011), who defines metaheuris-tics to incrementally insert or remove morphologi-cal features.
Their approach uses parser ?
trainingand parsing ?
as a black box evaluation of a preter-minal set.
In contrast, our proposal operates as asubmodule of the BerkeleyParser, hence does notrequire the re-training of the parser for every pos-sible preterminal set candidate, thus it is way morefaster.The most successful supervised constituentparsers contain a second feature-rich discrimina-tive parsing step (Charniak and Johnson, 2005;Huang, 2008; Chen and Kit, 2012) as well.
Atthe first stage they apply a PCFG to extract pos-136Basque French German Hebrew Hungarian#sent.
in training 7577 14759 40472 5000 8146#sent.
in dev 948 1235 5000 500 1051#sent.
in test 946 2541 5000 716 1009avg.
token/sent.
12.92 30.13 17.51 25.33 21.76#non-terminal labels 3000 770 994 1196 890#main POS labels 16 33 54 46 16unknown token ratio (dev) 18.35% 3.22% 6.34% 19.94% 19.94%Table 1: Basic statistics of the treebanks used.sible parses.
The n-best list parsers keep justthe 50-100 best parses according to the PCFG(Charniak and Johnson, 2005).
These methodsemploy a large feature set (usually a few mil-lion features) (Collins, 2000; Charniak and John-son, 2005).
These feature sets are engineered forEnglish.
In this study, we introduce feature tem-plates for exploiting morphological informationand investigate their added value over the standardfeature sets.3 Experimental SetupWe conducted experiments on the treebanks ofthe 2013 shared task on ?Statistical Parsing ofMorphologically Rich Languages?
(Seddah et al.,2013).
We used the train/dev/test splits of theshared task?s Basque (Aduriz et al., 2003), French(Abeill?e et al., 2003), Hebrew (Sima?an et al.,2001), German (Brants et al., 2002) and Hun-garian (Csendes et al., 2005) treebanks.
Table 1shows the basic statistics of these treebanks, fora more detailed description about their annotationschemata, domain, preprocessing etc.
please seeSeddah et al.
(2013).As evaluation metrics we employ the PARSE-VAL score (Abney et al., 1991) along with theexact match accuracy (i.e.
the ratio of perfectparse trees).
We use the evalb implementation ofthe shared task1.4 Enhanced Lexical ModelsBefore introducing our proposal and experimentswith preterminal set optimisation, we have to offera solution for the out-of-vocabulary (OOV) prob-lem, which ?
because of the inflectional nature ?is a crucial problem in morphologically rich lan-1Available at http://pauillac.inria.fr/?seddah/evalb_spmrl2013.tar.gz.
An importantchange in this version compared to the original evalb is thepenalization of unparsed sentences.guages.
We follow here Goldberg and Elhadad(2013) and enhance a lexicon model trained on thetraining set of the treebank with frequency infor-mation about the possible morphological analysesof tokens.
We estimate the tagging probabilityP (t|w) of the tag t given the word w byP (t|w) ={Ptb(t|w), if c(w) ?
Kc(w)Ptb(t|w)+Pex(t|w)1+c(w), otherwisewhere c(w) is the count of w in the training set,K is predefined constant, Ptb(t|w) is the proba-bility estimate from the treebank (the relative fre-quency with smoothing) and Pex(t|w) is the prob-ability estimate from an external lexicon.
Wecalculate the emission probabilities P (w|t) fromthe tagging probabilities P (t|w) by applying theBayesian rule.The key question here is how to construct theexternal lexicon.
For a baseline, Goldberg andElhadad (2013) suggest using the uniform dis-tribution over all possible morphological analy-ses coming from a morphological analyser (?uni-form?
).Goldberg and Elhadad (2013) also report con-siderable improvements over the ?uniform?
base-line by relative frequencies counted on a largecorpus which was automatically annotated in theunsupervised POS tagging paradigm (Goldberget al., 2008).
Here we show that even a super-vised morphological tagger without a morpho-logical analyzer can achieve the same level ofimprovement.
We employ MarMot2(Muelleret al., 2013) for predicting full morphologicalanalysis (i.e.
POS tags and morphological fea-tures jointly).
MarMot is a Conditional RandomField tagger which incrementally creates forward-backward lattices of increasing order to prune the2https://code.google.com/p/cistern/137sizable space of possible morphological analy-ses.
We used MarMoT with the default param-eters.
This purely data-driven tagger achieves atagging accuracy of 97.6 evaluated at full mor-phological analyses on the development set of theHungarian treebank, which is competitive with thestate-of-the-art Hungarian taggers which employlanguage-specific rules (e.g.
magyarlanc (Zsibritaet al., 2013)).
The chief advantage of using Mar-Mot instead of an unsupervised tagger is that theformer does not require any morphological lex-icon/analyser (which can lists the possible tagsfor a given word).
This morphological lexi-con/analyser is language-dependent, usually hand-crafted and it has to be compatible with the tree-bank in question.
In contrast, a supervised mor-phological tagger can build a reasonable taggingmodel on the training part of the treebanks ?
espe-cially for morphologically rich languages, wherethe tag ambiguity is generally low ?
thus each ofthese problems is avoided.Table 2 shows the results of various Pex(t|w)estimates on the Hungarian development set.
Thefirst row ?BerkeleyParser?
is our absolute base-line, i.e.
the original implementation of Berke-leyParser3defining signatures for OOVs.
Forthe ?uniform?
results, we used the morphologi-cal analyser module of magyarlanc (Zsibrita et al.,2013).
The last two rows show the results achievedby training MarMot on the treebank?s trainingdataset, having tagged the development set plusa huge unlabeled corpus (10M sentences from theHungarian National Corpus (V?aradi, 2002)) with itthen having counted relative tag frequencies.
Wereport scores on only using the frequencies fromthe development set (?dev?)
and from the concate-nation of the development set and the huge corpus(?huge?
).After a few preliminary experiments, we setK = 7 and use this value thereafter.Table 2 shows that even ?dev?
yields a consid-erable improvement over the baseline parser and?uniform?.
These results are also in line withthe findings of Goldberg and Elhadad (2013), i.e.?uniform?
has some added value and using relativefrequencies gathered from automatically taggedcorpora contributes more.
Although we can seeanother nice improvement by exploiting unlabeledcorpora (?huge?
), we will use the ?dev?
setting in3http://code.google.com/p/berkeleyparser/PARSEVAL EXBerkeleyParser 87.22 12.75uniform 87.31 14.78dev 88.29 15.22huge 89.27 16.97Table 2: The results achieved by using variousexternal lexical models on the Hungarian devel-opment set.the experiments of the next sections as we did nothave access to huge, in-domain unlabeled corporafor each language used in this study.5 Morphological Feature Values asPreterminalsFinding the optimal set of morphological featuresincorporating into the perterminal labels is cru-cial for any PCFG parsers.
Removing morpho-logical features might reduce data sparsity prob-lems while it might lead to loss of information forthe syntactic parser.
In this section, we proposea novel method for automatically finding the opti-mal set of preterminals then we present empiricalresults with this method and compare it to variousbaselines.Merge Procedure for Morphological FeatureValues: There have been studies published onthe automatic reduction of the set of pretermi-nals for constituent parsing.
For instance, Dehdariet al.
(2011) proposed a system which iterativelyremoves morphological features as a unit thenevaluates the preterminal sets by running the train-ing and parsing steps of a black-box constituentparser.
Our motivation here is two-fold.
First,morphological features should not be handled asa unit because different values of a feature mightbehave differently.
Take for instance the degreefeature in Hungarian adjectives.
Here the val-ues positive and superlative behave similarly (canbe merged) while distinguishing comparative andpositive+superlative is useful for syntactic pars-ing because comparative adjectives often have anargument (e.g.
x is more beautiful than y) whilepositive and superlative adjectives are not syntac-tic governors thus have no arguments.
Second,keeping a morphological feature can be useful forparticular POS tags and useless at other particularPOS tags (e.g.
the number of possessed in Hun-garian for nouns and pronouns).138Algorithm 1 The preterminal set merger algorithm.1.
training the standard BerkeleyParser using only main POS tags as preterminals2.
merging each subsymbols at the preterminal level3.
for each POS tag - morphological feature pair(a) split the POS tag for the values of the morphological feature4(b) recalculating the rule probabilities where there are preterminals in the right-hand side by uni-formly distribute the probability mass among subsymbols(c) set the lexical probabilities according to the relative frequencies of morphological valuescounted on gold standard morphological tags of the treebank(d) running 10 iterations of the Expectation-Maximization procedure on the whole treebank ini-tialized with (b)-(c)(e) constructing a fully connected graph whose nodes are the morphological values of the featurein question(f) for every edge of the graph, calculate the loss in likehood for the merging the two subsymbols(the same way as for BerkeleyParser?s merge procedure)4. removing edges from the entire set of graphs (controlled by the parameter th)5. merge the morphological values of the graphs?
connected componentsBased on these observations we propose a pro-cedure which starts from the full morphologicaldescription of a treebank then iteratively mergesparticular morphological feature values and it han-dles the same feature at the different POS tags sep-arately.
The result of this procedure is a clusteringof the possible values of each morphological fea-ture.
The removal of a morphological feature is aspecial case of our approach because if the valuesof the feature in question form one single clusterit does not have any discriminative function any-more.
Hence our proposal can be regarded as ageneralisation of the previous approaches.This general approach requires much more eval-uation of intermediate candidate preterminal sets,which is not feasible within the external black-boxparser evaluation scenario (training and parsingan average sized treebank by the BerkeleyParsertakes more than 1 hour).
Our idea here is that re-training a parser for the evaluation of each preter-minal set candidates is not necessary.
They keyobjective here is to select among preterminal setsbased on their usefulness for the syntactic parser.This is the motivation of the merge procedure ofthe BerkeleyParser.
After randomly splitting non-terminals, BerkeleyParser calculates for each splitthe loss in likelihood incurred when merging thesubsymbols back.
If this loss is small, the newannotation does not carry enough useful informa-tion and can be removed (Petrov et al., 2006).
Ourtask is the same at the preterminal level.
Hence atthe preterminal level, ?
instead of using the auto-matic subsymbol splits of the BerkeleyParser ?
wecall this merging procedure over the morpholog-ical feature values.
Algorithm 1 shows our pro-posal for the preterminal merging procedure.Baseline Preterminal Set Constructions: Thetwo basic approaches for preterminal set con-struction are the use of only the main POS tagset (?mainPOS?)
and the use of the full morpho-logical description as preterminals (?full?).
ForHungarian, we also had access to a linguisticallymotivated, hand-crafted preterminal set (?man-ual?)
which was designed for a morphological tag-ger (Zsibrita et al., 2013).
This manual code setkeeps different morphological features at differ-ent POS tags and merges morphological valuesinstead of fully removing features hence it inspiredour automatic merge procedure introduced in theprevious section.Our last baseline is the repetition of the experi-ments of Dehdari et al.
(2011).
For this, we startedfrom the full morphological feature set and com-pletely removed features (from all POS) one-by-one then re-trained our parser.
We observed thegreatest drop in PARSEVAL score at removing the139Basque French German Hebrew HungarianmainPOS 68.8/3.9 16 78.4/13.9 33 82.3/38.7 54 88.3/12.0 46 82.6/7.3 16full 81.8/18.4 2976 78.9/15.0 676 82.3/40.3 686 88.9/15.2 257 88.3/15.2 680preterminal merger 81.6/16.9 2791 79.7/15.6 480 82.3/39.3 111 89.0/14.6 181 88.5/15.4 642Table 3: PARSEVAL / exact match scores on the development sets.
The third small numbers in cellsshow the size of the preterminal sets.?Num?
feature and the least severe one at remov-ing ?Form?.
?Num?
denotes number for verbs andnominal elements (nouns, adjectives and numer-als), and since subject-verb agreement is deter-mined by the number and person features of thepredicate (the verb) and the subject (the noun),deleting the feature ?Num?
results in a seriousdecline in performance.
On the other hand, ?Form?denotes whether a conjunction is single or com-pound (which is a lexical feature) or whether anumber is spelt with letters, Arabic or Romannumbers (which is an orthographic feature).
It isinteresting to see that their deletion hardly harmsthe PARSEVAL scores, moreover, it can evenimprove the exact match scores, which is probablydue to the fact that the distinction between differ-ent orthographic versions of the same number (e.g.6 and VI) just confused the parser.
On the otherhand, members of a compound conjunction are notattached to each other in any way in the parse tree,and behave similar to single compounds, so thisdistinction might also be problematic for parsing.Results with Various Preterminal Sets: Table4 summarizes the results achieved by our fourbaseline methods along with the scores of twopreterminal sets output by our merger approach attwo different merging threshold th value.#pt PARSEVAL EXmainPOS 16 82.36 5.52manual 72 85.38 9.23full 680 88.29 15.22full - Num 479 87.43 14.49full - Form 635 88.24 15.73merged (th = 0.5) 378 88.36 15.92merged (th = 0.1) 642 88.52 15.44Table 4: The results achieved by using variouspreterminal sets on the Hungarian developmentset.The difference between mainPOS and full issurprisingly high, which indicates that the mor-phological information carried in preterminals isextremely important for the constituent parser andthe BerkeleyParser can handle preterminal sets ofthe size of several hundreds.
For Hungarian, wefound that the full removal of any feature cannotincrease the results.
This finding is contradictorywith Dehdari et al.
(2011) in Arabic, where remov-ing ?Case?
yielded a gain of 1.0 in PARSEVAL.We note that baselines for Arabic and Hungar-ian are also totally different, Dehdari et al.
(2011)reports basically no difference between mainPOSand full in Arabic.We report results of our proposed procedurewith two different merging thresholds.
The th =0.1 case merges only a few morphological featurevalues and it can slightly outperform the ?full?
set-ting (statistically significant5in exact match.).
Onthe other hand, the th = 0.5 setting is competitivewith the ?full?
setting in terms of parsing accuracybut it uses only the third of the preterminals usedby ?full?.
Although it is not statistically better than?full?
in accuracy, it almost halves the running timeof parsing6.Table 3 summarizes the results achieved bythe most important baselines and our approachalong with the size of the particular preterminalsets applied.
The ?full?
results outperform ?main-POS?
at each language with a striking difference atBasque and Hungarian.
These results show that ?contradictory to the general belief ?
the detailedmorphological description is definitely useful inconstituent parsing as well.
The last row of thetable contains the result achieved by our mergerapproach.
Here we run experiments with severalmerging threshold th values and show the highestscores for each language.Our merging proposal could find a better preter-minal set than full on French and Hungarian, itfound a competitive tag set in terms of accuracies5According to two sample t-test with p<0.001.6Parsing the 1051 sentences of the Hungarian develop-ment set takes 15 and 9 minutes with full and th = 0.5respectively (on an Intel Xeon E7 2GHz).140which are much smaller than full on German andHebrew and it could not find any useful merge atBasque.
The output of the merger procedure con-sists of one sixth of preterminals compared withfull.
Manually investigating the clusters, we cansee that it basically merged every morphologicalfeature except case at nouns and adjectives (butmerged case at personal pronouns).
This findingis in line with the experimental results of Fraser etal.
(2013).6 Morphology-based Features in n-bestRerankingn-best rerankers (Collins, 2000; Charniak andJohnson, 2005) are used as second stage after aPCFG parser and they usually achieve consider-able improvement over the first stage parser.
Theyextract a large feature set to describe the n bestoutput of a PCFG parser and they select the bestparse from this set (i.e.
rerank the parses).
Here,we define feature templates exploiting morpho-logical information and investigate their addedvalue for the standard feature sets (engineered forEnglish).
We reimplemented the feature templatesfrom Charniak and Johnson (2005) and Versleyand Rehbein (2009) excluding the features basedon external corpora and use them as our baselinefeature set.We used n = 50 in our experiment and fol-lowed a 5-fold-cross-parsing (a.k.a.
jackknifing)approach for generating unseen parse candidatesfor the training sentences (Charniak and Johnson,2005).
The reranker is trained for the maximumentropy objective function of Charniak and John-son (2005), i.e.
the sum of posterior probabilitiesof the oracles.
We used a slightly modified versionof the Mallet toolkit for reranking (McCallum,2002) and L2 regularizer with its default value forcoefficient.The feature templates of the baseline feature setfrequently incorporate preterminals as atomic fea-ture.
As a first step, we investigated which preter-minal set is the most useful for the baseline fea-ture set.
We took the 50 best output from theparser using the merged preterminal set and usedits preterminals (?merged?)
or only the main POStag (?mainPOS?)
as atomic building blocks for thereranker?s feature extractor.
Table 5 shows thatmainPOS outperformed full.
This is probably dueto data sparsity problems.Based on this observation, we decided to usemainPOS as preterminal in the atomic buildingblock of the baseline features and designed newfeature templates capturing the information in themorphological analysis.
We experimented withthe following templates:For each preterminal of the candidate parse andfor each morphological feature value inside thepreterminal we add the pair of wordform and mor-phological feature value as a new feature.
In a sim-ilar way, we define a reranker feature from everymorphological feature value of the head word ofthe constituent.
For each head-daughter attach-ment in the candidate parse we add each pair of themorphological feature values from the head wordsof the attachment?s participants.
Similarly we takeeach combination of head word?s morphologicalfeatures values from sister constituents.The first two templates enable the reranker toincorporate information into its learnt model fromthe rich morphology of the language at the lexi-cal and constituent levels, while the last two tem-plates might capture (dis)agreement at the mor-phological level.
The motivation for using thesefeatures is that because of the free(er) word orderof morphologically rich languages, morphological(dis)agreement can be a good indicator of attach-ment.Table 5 shows the added value of these fea-ture templates over mainPOS (?extended?
), whichis again statistically significant in exact match.Exploiting the morphological agreement in syn-tactic parsing has been investigated in previousstudies, e.g.
the Bohnet parser (Bohnet, 2010)employs morphological feature value pairs simi-lar to our feature templates and Seeker and Kuhn(2013) introduces an integer linear programmingframework including constraints for morpholog-ical agreement.
However, these works focus ondependency parsing and to the best of our knowl-edge, this is the first study on experimenting withatomic morphological features and their agree-ment in a constituency parsing.PARSEVAL EXreranker (merged morph) 89.05 18.45reranker (mainPOS) 89.33 18.64reranker (extended) 89.47 20.35Table 5: The results achieved by using variousfeature template sets for 50-best reranking on theHungarian development set.141Basque French German Hebrew HungarianBerkeleyParser 79.21 / 19.03 79.53 / 18.46 74.77 / 26.56 87.87 / 14.53 88.22 / 26.96+ Lexical model 82.02 / 25.69 78.91 / 17.87 75.64 / 28.36 88.53 / 13.69 89.09 / 26.76+ Preterminal merger 83.19 / 24.74 79.53 / 18.58 77.12 / 30.02 88.07 / 13.83 89.15 / 28.05+ reranker 83.81 / 25.66 80.31 / 18.91 77.78 / 29.80 88.38 / 15.12 89.57 / 30.23+ reranker + morph feat 84.03 / 26.28 80.41 / 20.07 77.74 / 29.23 88.55 / 15.24 89.91 / 30.55Table 6: PARSEVAL / exact match scores on the test sets.7 Results of the Full SystemAfter our investigations focusing on buildingblocks of our system independently from eachother on the development set, we parsed the testsets of the treebanks adding steps one-by-one.Table 6 summarizes our final results.
We startfrom the BerkeleyParser using the full morpholog-ical descriptions as preterminal set, then we enrichthe lexical model with tagging frequencies gath-ered from the automatic parsing of the test sets(?+ lexical model?).
In the third step we replacethe full preterminal set by the output of our preter-minal merger procedure (?+ preterminal merger?
).We tuned the merging threshold of our methodon the development set for each language.
Thelast two rows contain the results achieved by the50-best reranker with the standard feature set (?+reranker?)
and with the feature set extended bymorphological features (?+ morph features?
).The enhanced lexical model contributes a lotat Basque and considerable improvements arepresent at German and Hungarian as well whileit harmed the results in French.
The advance ofthe preterminal merger approach over the full set-ting is clear at French and Hungarian, similarly tothe development set.
It is interesting that an ratio-nalized preterminal set could compensate the losssuffered by a inadequate lexical model at French.Although the reranking step could furtherimprove the results at each languages we haveto note that the gain (0.5 in average) is muchsmaller here than the gains reported on English(over 1.5).
This might be because of the highnumber of wordforms at morphologically rich lan-guages i.e.
most of feature templates are incor-porate the words itself and the huge dictionarycan indicate data sparsity problems again.
Ourmorphology-based reranking features yielded amoderate improvement at four languages, but webelieve there a lots of space for improvement here.8 ConclusionsIn this study we introduced three techniques forbetter constituent parsing of morphologically richlanguages.
We believe that research in con-stituency parsing is important next to dependencyparsing.
In general, we report state-of-the-artresults with constituent parsers with our entirelylanguage-agnostic techniques.Our chief contribution here is the pretermi-nal merger procedure.
This is a more generalapproach than previous proposals and still muchfaster thank to operating on probabilities from aPCFG instead of employing a full train+parse stepfor evaluating every preterminal set candidate.
Wefound that the inclusion of the rich morphologicaldescription into the preterminal level is crucial forparsing morphologically rich languages.
Our pro-posed preterminal merger approach could outper-form the full setting at 2 out of 5 languages, i.e.
wehave reported gains in parsing accuracies by merg-ing morphological feature values.
At the other lan-guages, the results with the full preterminal set andour approach are competitive in terms of parsingaccuracies while our approach could achieve thesescores with a smaller preterminal set, which leadsto considerable parsing time advantages.We also experimented with exploiting externalcorpora in the lexical model.
Here we showedthat automatic tagging of an off-the-shelf super-vised morphological tagger (MarMot) can con-tribute to the results.
Our last experiment was car-ried out with the feature set of an n-best reranker.We showed that incorporating feature templatesbuilt on morphological information improves theresults.AcknowledgmentsThis work was supported in part by the Euro-pean Union and the European Social Fund throughproject FuturICT.hu (grant no.
: T?AMOP-4.2.2.C-11/1/KONV-2012-0013).142ReferencesAnne Abeill?e, Lionel Cl?ement, and Franc?ois Toussenel.2003.
Building a treebank for french.
In AnneAbeill?e, editor, Treebanks.
Kluwer, Dordrecht.S.
Abney, S. Flickenger, C. Gdaniec, C. Grishman,P.
Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-vans, M. Liberman, M. Marcus, S. Roukos, B. San-torini, and T. Strzalkowski.
1991.
Procedure forquantitatively comparing the syntactic coverage ofenglish grammars.
In E. Black, editor, Proceedingsof the workshop on Speech and Natural Language,pages 306?311.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
D?
?az de Ilarraza, A. Garmendia, and M. Oronoz.2003.
Construction of a Basque dependency tree-bank.
In TLT-03, pages 201?204.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics (Coling 2010), pages 89?97.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Erhard Hinrichs and Kiril Simov, edi-tors, Proceedings of the First Workshop on Tree-banks and Linguistic Theories (TLT 2002), pages24?41.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,ACL ?05, pages 173?180.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st NorthAmerican chapter of the Association for Computa-tional Linguistics conference, pages 132?139.Xiao Chen and Chunyu Kit.
2012.
Higher-order con-stituent parsing and parser combination.
In Pro-ceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 2: ShortPapers), pages 1?5.Key-Sun Choi, Young S Han, Young G Han, and Oh WKwon.
1994.
Kaist tree bank project for korean:Present and future development.
In Proceedingsof the International Workshop on Sharable NaturalLanguage Resources, pages 7?14.
Citeseer.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of theSeventeenth International Conference on MachineLearning, ICML ?00, pages 175?182.D?ora Csendes, J?anos Csirik, Tibor Gyim?othy, andAndr?as Kocsor.
2005.
The Szeged Treebank.
InTSD, pages 123?131.Jon Dehdari, Lamia Tounsi, and Josef van Gen-abith.
2011.
Morphological features for parsingmorphologically-rich languages: A case of arabic.In Proceedings of the Second Workshop on Statis-tical Parsing of Morphologically Rich Languages,pages 12?21, Dublin, Ireland, October.
Associationfor Computational Linguistics.Alexander Fraser, Helmut Schmid, Rich?ard Farkas,Renjing Wang, and Hinrich Sch?utze.
2013.
Knowl-edge sources for constituent parsing of german, amorphologically rich and less-configurational lan-guage.
Computational Linguistics, 39(1):57?85.Yoav Goldberg and Michael Elhadad.
2013.
Wordsegmentation, unknown-word resolution, and mor-phological agreement in a hebrew parsing system.Computational Linguistics, 39(1):121?160.Yoav Goldberg, Meni Adler, and Michael Elhadad.2008.
EM can find pretty good HMM POS-taggers(when given a good start).
In Proceedings of ACL-08: HLT, pages 746?754.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL-08: HLT, pages 586?594.Joseph Le Roux, Benoit Sagot, and Djam?e Seddah.2012.
Statistical parsing of spanish and data drivenlemmatization.
In Proceedings of the ACL 2012Joint Workshop on Statistical Parsing and Seman-tic Processing of Morphologically Rich Languages,pages 55?61.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: the penn treebank.
Compu-tational Linguistics, 19(2):313?330.Yuval Marton, Nizar Habash, and Owen Rambow.2010.
Improving arabic dependency parsing withlexical and inflectional morphological features.
InProceedings of the NAACL HLT 2010 First Work-shop on Statistical Parsing of Morphologically-RichLanguages, pages 13?21.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Thomas Mueller, Helmut Schmid, and HinrichSch?utze.
2013.
Efficient higher-order CRFs formorphological tagging.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 322?332.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 433?440.Slav Petrov.
2009.
Coarse-to-Fine Natural LanguageProcessing.
Ph.D. thesis, University of California atBekeley, Berkeley, CA, USA.143Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, MarieCandito, Jinho D. Choi, Rich?ard Farkas, JenniferFoster, Iakes Goenaga, Koldo Gojenola Gallete-beitia, Yoav Goldberg, Spence Green, Nizar Habash,Marco Kuhlmann, Wolfgang Maier, Yuval Mar-ton, Joakim Nivre, Adam Przepi?orkowski, RyanRoth, Wolfgang Seeker, Yannick Versley, VeronikaVincze, Marcin Woli?nski, and Alina Wr?oblewska.2013.
Overview of the SPMRL 2013 sharedtask: A cross-framework evaluation of parsingmorphologically rich languages.
In Proceedingsof the Fourth Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 146?182.Wolfgang Seeker and Jonas Kuhn.
2013.
Morphologi-cal and syntactic case in statistical dependency pars-ing.
Computational Linguistics, 39(1):23?55.Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,and Noa Nativ.
2001.
Building a Tree-Bank forModern Hebrew Text.
In Traitement Automatiquedes Langues.Reut Tsarfaty, Djam?e Seddah, Yoav Goldberg, SandraKuebler, Yannick Versley, Marie Candito, JenniferFoster, Ines Rehbein, and Lamia Tounsi.
2010.
Sta-tistical parsing of morphologically rich languages(spmrl) what, how and whither.
In Proceedings ofthe NAACL HLT 2010 First Workshop on StatisticalParsing of Morphologically-Rich Languages, pages1?12.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2012.
Cross-framework evaluation for statisticalparsing.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 44?54.Reut Tsarfaty, Djam?e Seddah, Sandra K?ubler, andJoakim Nivre.
2013.
Parsing morphologically richlanguages: Introduction to the special issue.
Com-putational Linguistics, 39(1):15?22.Tam?as V?aradi.
2002.
The hungarian national cor-pus.
In In Proceedings of the Second InternationalConference on Language Resources and Evaluation,pages 385?389.Yannick Versley and Ines Rehbein.
2009.
Scalable dis-criminative parsing for german.
In Proceedings ofthe 11th International Conference on Parsing Tech-nologies (IWPT?09), pages 134?137.J?anos Zsibrita, Veronika Vincze, and Rich?ard Farkas.2013.
magyarlanc: A toolkit for morphological anddependency parsing of hungarian.
In Proceedings ofRANLP.144
