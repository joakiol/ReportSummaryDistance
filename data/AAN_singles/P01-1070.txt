Using Machine Learning Techniques to Interpret WH-questionsIngrid ZukermanSchool of Computer Science and Software EngineeringMonash UniversityClayton, Victoria 3800, AUSTRALIAingrid@csse.monash.edu.auEric HorvitzMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAhorvitz@microsoft.comAbstractWe describe a set of supervised ma-chine learning experiments centeringon the construction of statistical mod-els of WH-questions.
These models,which are built from shallow linguis-tic features of questions, are employedto predict target variables which repre-sent a user?s informational goals.
Wereport on different aspects of the pre-dictive performance of our models, in-cluding the influence of various trainingand testing factors on predictive perfor-mance, and examine the relationshipsamong the target variables.1 IntroductionThe growth in popularity of the Internet highlightsthe importance of developing machinery for gen-erating responses to queries targeted at large un-structured corpora.
At the same time, the accessof World Wide Web resources by large numbersof users provides opportunities for collecting andleveraging vast amounts of data about user activ-ity.
In this paper, we describe research on exploit-ing data collected from logs of users?
queries inorder to build models that can be used to inferusers?
informational goals from queries.We describe experiments which use supervisedmachine learning techniques to build statisticalmodels of questions posed to the Web-based En-carta encyclopedia service.
We focus on mod-els and analyses of complete questions phrasedin English.
These models predict a user?s infor-mational goals from shallow linguistic featuresof questions obtained from a natural languageparser.
We decompose these goals into (1) thetype of information requested by the user (e.g.,definition, value of an attribute, explanation for anevent), (2) the topic, focal point and additional re-strictions posed by the question, and (3) the levelof detail of the answer.
The long-term aim of thisproject is to use predictions of these informationalgoals to enhance the performance of information-retrieval and question-answering systems.
In thispaper, we report on different aspects of the predic-tive performance of our statistical models, includ-ing the influence of various training and testingfactors on predictive performance, and examinethe relationships among the informational goals.In the next section, we review related research.In Section 3, we describe the variables beingmodeled.
In Section 4, we discuss our predic-tive models.
We then evaluate the predictions ob-tained from models built under different trainingand modeling conditions.
Finally, we summarizethe contribution of this work and discuss researchdirections.2 Related ResearchOur research builds on earlier work on the useof probabilistic models to understand free-textqueries in search applications (Heckerman andHorvitz, 1998; Horvitz et al, 1998), and on workconducted in the IR arena of question answering(QA) technologies.Heckerman and Horvitz (1998) and Horvitz etal.
(1998) used hand-crafted models and super-vised learning to construct Bayesian models thatpredict users?
goals and needs for assistance in thecontext of consumer software applications.
Heck-erman and Horvitz?
models considered words,phrases and linguistic structures (e.g., capitaliza-tion and definite/indefinite articles) appearing inqueries to a help system.
Horvitz et al?s modelsconsidered a user?s recent actions in his/her use ofsoftware, together with probabilistic informationmaintained in a dynamically updated user profile.QA research centers on the challenge of en-hancing the response of search engines to a user?squestions by returning precise answers rather thanreturning documents, which is the more commonIR goal.
QA systems typically combine tradi-tional IR statistical methods (Salton and McGill,1983) with ?shallow?
NLP techniques.
One ap-proach to the QA task consists of applying the IRmethods to retrieve documents relevant to a user?squestion, and then using the shallow NLP to ex-tract features from both the user?s question andthe most promising retrieved documents.
Thesefeatures are then used to identify an answer withineach document which best matches the user?squestion.
This approach was adopted in (Kupiec,1993; Abney et al, 2000; Cardie et al, 2000;Moldovan et al, 2000).The NLP components of these systems em-ployed hand-crafted rules to infer the type of an-swer expected.
These rules were built by con-sidering the first word of a question as well aslarger patterns of words identified in the question.For example, the question ?How far is Mars?
?might be characterized as requiring a reply of typeDISTANCE.
Our work differs from traditional QAresearch in its use of statistical models to pre-dict variables that represent a user?s informationalgoals.
The variables under consideration includethe type of the information requested in a query,the level of detail of the answer, and the parts-of-speech which contain the topic the query and itsfocus (which resembles the type of the expectedanswer).
In this paper, we focus on the predictivemodels, rather than on the provision of answers tousers?
questions.
We hope that in the short term,the insights obtained from our work will assistQA researchers to fine-tune the answers generatedby their systems.3 Data CollectionOur models were built from questions identi-fied in a log of Web queries submitted to theEncarta encyclopedia service.
These questionsinclude traditional WH-questions, which beginwith ?what?, ?when?, ?where?, ?which?, ?who?,?why?
and ?how?, as well as imperative state-ments starting with ?name?, ?tell?, ?find?, ?de-fine?
and ?describe?.
We extracted 97,640 ques-tions (removing consecutive duplicates), whichconstitute about 6% of the 1,649,404 queries inthe log files collected during a period of threeweeks in the year 2000.
A total of 6,436 questionswere tagged by hand.
Two types of tags were col-lected for each question: (1) tags describing lin-guistic features, and (2) tags describing high-levelinformational goals of users.
The former were ob-tained automatically, while the latter were taggedmanually.We considered three classes of linguistic fea-tures: word-based, structural and hybrid.Word-based features indicate the presence ofspecific words or phrases in a user?s question,which we believed showed promise for predictingcomponents of his/her informational goals.
Theseare words like ?make?, ?map?
and ?picture?.Structural features include information ob-tained from an XML-encoded parse tree gen-erated for each question by NLPWin (Heidorn,1999) ?
a natural language parser developed bythe Natural Language Processing Group at Mi-crosoft Research.
We extracted a total of 21 struc-tural features, including the number of distinctparts-of-speech (PoS) ?
NOUNs, VERBs, NPs,etc ?
in a question, whether the main noun is plu-ral or singular, which noun (if any) is a propernoun, and the PoS of the head verb post-modifier.Hybrid features are constructed from structuraland word-based information.
Two hybrid fea-tures were extracted: (1) the type of head verb ina question, e.g., ?know?, ?be?
or action verb;and (2) the initial component of a question, whichusually encompasses the first word or two of thequestion, e.g., ?what?, ?when?
or ?how many?,but for ?how?
may be followed by a PoS, e.g.,?how ADVERB?
or ?how ADJECTIVE.
?We considered the following variables rep-resenting high-level informational goals: Infor-mation Need, Coverage Asked, Coverage WouldGive, Topic, Focus, Restriction and LIST.
Infor-mation about the state of these variables was pro-vided manually by three people, with the majorityof the tagging being performed under contract bya professional outside the research team.Information Need is a variable that repre-sents the type of information requested by auser.
We provided fourteen types of informa-tion need, including Attribute, IDentifica-tion, Process, Intersection and Topic It-self (which, as shown in Section 5, are the mostcommon information needs), plus the additionalcategory OTHER.
As examples, the question ?Whatis a hurricane??
is an IDentification query;?What is the color of sand in the Kalahari??
is anAttribute query (the attribute is ?color?
); ?Howdoes lightning form??
is a Process query; ?Whatare the biggest lakes in New Hampshire??
is anIntersection query (a type of IDentification,where the returned item must satisfy a particularRestriction ?
in this case ?biggest?
); and ?Wherecan I find a picture of a bay??
is a Topic Itselfquery (interpreted as a request for accessing anobject directly, rather than obtaining informationabout the object).Coverage Asked and Coverage Would Give arevariables that represent the level of detail in an-swers.
Coverage Asked is the level of detail ofa direct answer to a user?s question.
CoverageWould Give is the level of detail that an infor-mation provider would include in a helpful an-swer.
For instance, although the direct answer tothe question ?When did Lincoln die??
is a sin-gle date, a helpful information provider might addother details about Lincoln, e.g., that he was thesixteenth president of the United States, and thathe was assassinated.
This additional level of de-tail depends on the request itself and on the avail-able information.
However, here we consider theformer factor, viewing it as an initial filter thatwill guide the content planning process of an en-hanced QA system.
The distinction between therequested level of detail and the provided level ofdetail makes it possible to model questions forwhich the preferred level of detail in a responsediffers from the detail requested by the user.
Weconsidered three levels of detail for both coveragevariables: Precise, Additional and Extended,plus the additional category OTHER.
Precise in-dicates that an exact answer has been requested,e.g., a name or date (this is the value of Cover-age Asked in the above example); Additionalrefers to a level of detail characterized by a one-paragraph answer (this is the value of CoverageWould Give in the above example); and Extendedindicates a longer, more detailed answer.Topic, Focus and Restriction contain a PoS inthe parse tree of a user?s question.
These variablesrepresent the topic of discussion, the type of theexpected answer, and information that restrictsthe scope of the answer, respectively.
These vari-ables take 46 possible values, e.g., NOUN , VERBand NP , plus the category OTHER.
For each ques-tion, the tagger selected the most specific PoS thatcontains the portion of the question which bestmatches each of these informational goals.
For in-stance, given the question ?What are the main tra-ditional foods that Brazilians eat?
?, the Topic isNOUN (Brazilians), the Focus is ADJ +NOUN (tra-ditional foods) and the restriction is ADJ (main).As shown in this example, it was sometimes nec-essary to assign more than one PoS to these tar-get variables.
At present, these composite assign-ments are classified as the category OTHER.LIST is a boolean variable which indicateswhether the user is looking for a single answer(False) or multiple answers (True).4 Predictive ModelWe built decision trees to infer high-level in-formational goals from the linguistic featuresof users?
queries.
One decision tree was con-structed for each goal: Information Need, Cov-erage Asked, Coverage Would Give, Topic, Fo-cus, Restriction and LIST.
Our decision trees werebuilt using dprog (Wallace and Patrick, 1993)?
a procedure based on the Minimum MessageLength principle (Wallace and Boulton, 1968).The decision trees described in this section arethose that yield the best predictive performance(obtained from a training set comprised of ?good?queries, as described Section 5).
The trees them-selves are too large to be included in this paper.However, we describe the main attributes iden-tified in each decision tree.
Table 2 shows, foreach target variable, the size of the decision tree(in number of nodes) and its maximum depth, theattribute used for the first split, and the attributesused for the second split.
Table 1 shows examplesand descriptions of the attributes in Table 2.1We note that the decision tree for Focus splitsfirst on the initial component of a question, e.g.,?how ADJ?, ?where?
or ?what?, and that one ofthe second-split attributes is the PoS following theinitial component.
These attributes were also usedto build the hand-crafted rules employed by theQA systems described in Section 2, which con-centrate on determining the type of the expected1The meaning of ?Total PRONOUNS?
is peculiar inour context, because the NLPWin parser tags words suchas ?what?
and ?who?
as PRONOUNs.
Also, the clue at-tributes, e.g., Comparison clues, represent groupings of dif-ferent clues that at design time where considered helpful inidentifying certain target variables.Table 1: Attributes in the decision treesAttribute Example/MeaningAttribute clues e.g., ?name?, ?type of?, ?called?Comparison clues e.g., ?similar?, ?differ?, ?relate?Intersection clues superlative ADJ, ordinal ADJ, relative clauseTopic Itself clues e.g., ?show?, ?picture?, ?map?PoS after Initial component e.g., NOUN in ?which country is the largest?
?verb-post-modifier PoS e.g., NP without PP in ?what is a choreographer?Total PoS number of occurrences of PoS in a question, e.g., Total NOUNsFirst NP plural?
Boolean attributeDefinite article in First NP?
Boolean attributePlural quantifier?
Boolean attributeLength in words number of words in a questionLength in phrases number of NPs + PPs + VPs in a questionTable 2: Summary of decision treesTarget Variable Nodes/Depth First Split Second SplitInformation Need 207/13 Initial component Attribute clues, Comparison clues, Topic Itselfclues, PoS after Initial component, verb-post-modifier PoS, Length in wordsCoverage Asked 123/11 Initial component Topic Itself clues, PoS after Initial component,Head verbCoverage Would Give 69/6 Topic Itself clues Initial component, Attribute cluesTopic 193/9 Total NOUNs Total ADJs, Total AJPs, Total PRONOUNsFocus 226/10 Initial component Topic Itself clues, Total NOUNs, Total VERBs,Total PRONOUNs, Total VPs, Head verb, PoS afterInitial componentRestriction 126/9 Total PPs Intersection clues, PoS after Initial component,Definite article in First NP?, Length in phrasesLIST 45/7 First NP plural?
Plural quantifier?, Initial componentanswer (which is similar to our Focus).
How-ever, our Focus decision tree includes additionalattributes in its second split (these attributes areadded by dprog because they improve predictiveperformance on the training data).5 ResultsOur report on the predictive performance of thedecision trees considers the effect of various train-ing and testing factors on predictive performance,and examines the relationships among the targetvariables.5.1 Training FactorsWe examine how the quality of the training dataand the size of the training set affect predictiveperformance.Quality of the data.
In our context, the qualityof the training data is determined by the wordingof the queries and the output of the parser.
Foreach query, the tagger could indicate whether itwas a BAD QUERY or whether a WRONG PARSEhad been produced.
A BAD QUERY is incoher-ent or articulated in such a way that the parsergenerates a WRONG PARSE, e.g., ?When its hot itexpand??.
Figure 1 shows the predictive perfor-mance of the decision trees built for two train-ing sets: All5145 and Good4617.
The first setcontains 5145 queries, while the second set con-tains a subset of the first set comprised of ?good?queries only (i.e., bad queries and queries withwrong parses were excluded).
In both cases, thesame 1291 queries were used for testing.
As abaseline measure, we also show the predictive ac-Figure 1: Performance comparison: training withall queries versus training with good queries;prior probabilities included as baselineSmall Medium Large X-LargeTrain/All 1878 2676 3765 5145Train/Good 1679 2389 3381 4617Test 376 662 934 1291Table 3: Four training and testing set sizescuracy of using the maximum prior probability topredict each target variable.
These prior probabil-ities were obtained from the training set All5145.The Information Need with the highest prior prob-ability is IDentification, the highest CoverageAsked is Precise, while the highest CoverageWould Give is Additional; NOUN contains themost common Topic; the most common Focus andRestriction are NONE; and LIST is almost alwaysFalse.
As seen in Figure 1, the prior probabilitiesyield a high predictive accuracy for Restrictionand LIST.
However, for the other target variables,the performance obtained using decision trees issubstantially better than that obtained using priorprobabilities.
Further, the predictive performanceobtained for the set Good4617 is only slightly bet-ter than that obtained for the set All5145.
How-ever, since the set of good queries is 10% smaller,it is considered a better option.Size of the training set.
The effect of the sizeof the training set on predictive performance wasassessed by considering four sizes of training/testsets: Small, Medium, Large, and X-Large.
Ta-ble 3 shows the number of training and testqueries for each set size for the ?all queries?
andthe ?good queries?
training conditions.Figure 2: Predictive performance for four trainingsets (1878, 2676, 3765 and 5145) averaged over 5runs ?
All queriesFigure 3: Predictive performance for four trainingsets (1679, 2389, 3381 and 4617) ?
Good queriesThe predictive performance for the all-queriesand good-queries sets is shown in Figures 2 and 3respectively.
Figure 2 depicts the average of theresults obtained over five runs, while Figure 3shows the results of a single run (similar resultswere obtained from other runs performed with thegood-queries sets).
As indicated by these results,for both data sets there is a general improvementin predictive performance as the size of the train-ing set increases.5.2 Testing FactorsWe examine the effect of two factors on the pre-dictive performance of our models: (1) querylength (measured in number of words), and (2) in-formation need (as recorded by the tagger).
Theseeffects were studied with respect to the predic-tions generated by the decision trees obtainedfrom the set Good4617, which had the best per-formance overall.Figure 4: Query length distribution ?
Test setFigure 5: Predictive performance by query length?
Good queriesQuery length.
The queries were divided intofour length categories (measured in number ofwords): length , length , lengthand length.
Figure 4 displays the distribu-tion of queries in the test set according to theselength categories.
According to this distribution,over 90% of the queries have less than 11 words.The predictive performance of our decision treesbroken down by query length is shown in Fig-ure 5.
As shown in this chart, for all target vari-ables there is a downward trend in predictive ac-curacy as query length increases.
Still, for queriesof less than 11 words and all target variables ex-cept Topic, the predictive accuracy remains over74%.
In contrast, the Topic predictions drop from88% (for queries of less than 5 words) to 57%(for queries of 8, 9 or 10 words).
Further, the pre-dictive accuracy for Information Need, Topic, Fo-cus and Restriction drops substantially for queriesthat have 11 words or more.
This drop in predic-tive performance may be explained by two fac-tors.
For one, the majority of the training dataFigure 6: Information need distribution ?
Test setFigure 7: Predictive performance for five mostfrequent information needs ?
Good queriesconsists of shorter questions.
Hence, the applica-bility of the inferred models to longer questionsmay be limited.
Also, longer questions may exac-erbate errors associated with some of the indepen-dence assumptions implicit in our current model.Information need.
Figure 6 displays the dis-tribution of the queries in the test set ac-cording to Information Need.
The fivemost common Information Need categoriesare: IDentification, Attribute, Topic It-self, Intersection and Process, jointly ac-counting for over 94% of the queries.
Figure 7displays the predictive performance of our modelsfor these five categories.
The best performanceis exhibited for the IDentification and TopicItself queries.
In contrast, the lowest predictiveaccuracy was obtained for the Information Need,Topic and Restriction of Intersection queries.This can be explained by the observation that In-tersection queries tend to be the longest queries(as seen above, predictive accuracy drops for longFigure 8: Performance comparison for four pre-diction models: PerfectInformation, BestRe-sults, PredictionOnly and Mixed; prior prob-abilities included as baselinequeries).
The relatively low predictive accuracyobtained for both types of Coverage for Processqueries remains to be explained.5.3 Relations between target variablesTo determine whether the states of our targetvariables affect each other, we built three pre-diction models, each of which includes six tar-get variables for predicting the remaining vari-able.
For instance, Information Need, CoverageAsked, Coverage Would Give, Focus, Restrictionand LIST are incorporated as data (in addition tothe observable variables) when training a modelthat predicts Focus.
Our three models are: Pre-dictionOnly ?
which uses the predicted valuesof the six target variables both for the training setand for the test set; Mixed ?
which uses the actualvalues of the six target variables for the trainingset and their predicted values for the test set; andPerfectInformation ?
which uses actual valuesof the six target variables for both training andtesting.
This model enables us to determine theperformance boundaries of our methodology inlight of the currently observed attributes.Figure 8 shows the predictive accuracy of fivemodels: the above three models, our best modelso far (obtained from the training set Good4617)?
denoted BestResult, and prior probabilities.As expected, the PerfectInformation modelhas the best performance.
However, its predic-tive accuracy is relatively low for Topic and Fo-cus, suggesting some inherent limitations of ourmethodology.
The performance of the Predic-tionOnly model is comparable to that of BestRe-sult, but the performance of the Mixed modelseems slightly worse.
This difference in perfor-mance may be attributed to the fact that the Pre-dictionOnly model is a ?smoothed?
version ofthe Mixed model.
That is, the PredictionOnlymodel uses a consistent version of the target vari-ables (i.e., predicted values) both for training andtesting.
This is not the case for the Mixed model,where actual values are used for training (thus theMixed model is the same as the PerfectInfor-mation model), but predicted values (which arenot always accurate) are used for testing.Finally, Information Need features prominentlyboth in the PerfectInformation/Mixed modeland the PredictionOnly model, being used inthe first or second split of most of the decisiontrees for the other target variables.
Also, as ex-pected, Coverage Asked is used to predict Cov-erage Would Give and vice versa.
These re-sults suggest using modeling techniques whichcan take advantage of dependencies among tar-get variables.
These techniques would enable theconstruction of models which take into accountthe distribution of the predicted values of one ormore target variables when predicting another tar-get variable.6 Discussion and Future WorkWe have introduced a predictive model, builtby applying supervised machine-learning tech-niques, which can be used to infer a user?s key in-formational goals from free-text questions posedto an Internet search service.
The predictivemodel, which is built from shallow linguistic fea-tures of users?
questions, infers a user?s informa-tion need, the level of detail requested by the user,the level of detail deemed appropriate by an infor-mation provider, and the topic, focus and restric-tions of the user?s question.
The performance ofour model is encouraging, in particular for shorterqueries, and for queries with certain informationneeds.
However, further improvements are re-quired in order to make this model practically ap-plicable.We believe there is an opportunity to identifyadditional linguistic distinctions that could im-prove the model?s predictive performance.
Forexample, we intend to represent frequent combi-nations of PoS, such as NOUN +NOUN , which arecurrently classified as OTHER (Section 3).
We alsopropose to investigate predictive models whichreturn more informative predictions than those re-turned by our current model, e.g., a distributionof the probable informational goals, instead of asingle goal.
This would enable an enhanced QAsystem to apply a decision procedure in order todetermine a course of action.
For example, if theAdditional value of the Coverage Would Givevariable has a relatively high probability, the sys-tem could consider more than one InformationNeed, Topic or Focus when generating its reply.In general, the decision-tree generation meth-ods described in this paper do not have the abil-ity to take into account the relationships amongdifferent target variables.
In Section 5.3, we in-vestigated this problem by building decision treeswhich incorporate predicted and actual values oftarget variables.
Our results indicate that it isworth exploring the relationships between severalof the target variables.
We intend to use the in-sights obtained from this experiment to constructmodels which can capture probabilistic depen-dencies among variables.Finally, as indicated in Section 1, this projectis part of a larger effort centered on improv-ing a user?s ability to access information fromlarge information spaces.
The next stage of thisproject involves using the predictions generatedby our model to enhance the performance of QAor IR systems.
One such enhancement pertainsto query reformulation, whereby the inferred in-formational goals can be used to reformulate orexpand queries in a manner that increases thelikelihood of returning appropriate answers.
Asan example of query expansion, if Process wasidentified as the Information Need of a query,words that boost responses to searches for infor-mation relating to processes could be added to thequery prior to submitting it to a search engine.Another envisioned enhancement would attemptto improve the initial recall of the document re-trieval process by submitting queries which con-tain the content words in the Topic and Focus of auser?s question (instead of including all the con-tent words in the question).
In the longer term, weplan to explore the use of Coverage results to en-able an enhanced QA system to compose an ap-propriate answer from information found in theretrieved documents.AcknowledgmentsThis research was largely performed during thefirst author?s visit at Microsoft Research.
The au-thors thank Heidi Lindborg, Mo Corston-Oliverand Debbie Zukerman for their contribution to thetagging effort.ReferencesS.
Abney, M. Collins, and A. Singhal.
2000.
Answerextraction.
In Proceedings of the Sixth Applied Nat-ural Language Processing Conference, pages 296?301, Seattle, Washington.C.
Cardie, V. Ng, D. Pierce, and C. Buckley.2000.
Examining the role of statistical and lin-guistic knowledge sources in a general-knowledgequestion-answering system.
In Proceedings of theSixth Applied Natural Language Processing Con-ference, pages 180?187, Seattle, Washington.D.
Heckerman and E. Horvitz.
1998.
Inferring infor-mational goals from free-text queries: A Bayesianapproach.
In Proceedings of the Fourteenth Confer-ence on Uncertainty in Artificial Intelligence, pages230?237, Madison, Wisconsin.G.
Heidorn.
1999.
Intelligent writing assistance.
InA Handbook of Natural Language Processing Tech-niques.
Marcel Dekker.E.
Horvitz, J. Breese, D. Heckerman, D. Hovel,and K. Rommelse.
1998.
The Lumiere project:Bayesian user modeling for inferring the goals andneeds of software users.
In Proceedings of theFourteenth Conference on Uncertainty in ArtificialIntelligence, pages 256?265, Madison, Wisconsin.J.
Kupiec.
1993.
MURAX: A robust linguistic ap-proach for question answering using an on-line en-cyclopedia.
In Proceedings of the 16th AnnualInternational ACM-SIGIR Conference on Researchand Development in Information Retrieval, pages181?190, Pittsburgh, Pennsylvania.D.
Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea,R.
Girju, R. Goodrum, and V. Rus.
2000.
Thestructure and performance of an open-domain ques-tion answering system.
In ACL2000 ?
Proceedingsof the 38th Annual Meeting of the Association forComputational Linguistics, pages 563?570, HongKong.G.
Salton and M.J. McGill.
1983.
An Introduction toModern Information Retrieval.
McGraw Hill.C.S.
Wallace and D.M.
Boulton.
1968.
An informa-tion measure for classification.
The Computer Jour-nal, 11:185?194.C.S.
Wallace and J.D.
Patrick.
1993.
Coding decisiontrees.
Machine Learning, 11:7?22.
