Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 94?104, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsMixed Membership Markov Modelsfor Unsupervised Conversation ModelingMichael J. PaulCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218, USAmpaul@cs.jhu.eduAbstractRecent work has explored the use of hiddenMarkov models for unsupervised discourseand conversation modeling, where each seg-ment or block of text such as a message in aconversation is associated with a hidden statein a sequence.
We extend this approach to al-low each block of text to be a mixture of mul-tiple classes.
Under our model, the probabilityof a class in a text block is a log-linear func-tion of the classes in the previous block.
Weshow that this model performs well at predic-tive tasks on two conversation data sets, im-proving thread reconstruction accuracy by upto 15 percentage points over a standard HMM.Additionally, we show quantitatively that theinduced word clusters correspond to speechacts more closely than baseline models.1 IntroductionThe proliferation of social media in recent years haslead to an increased use of informal Web data inthe language processing community.
With this ris-ing interest in social domains, it is natural to con-sider models which explicitly incorporate the con-versational patterns of social text.
Compared to thenaive approach of treating conversations as flat doc-uments, models which include conversation struc-ture have been shown to improve tasks such as fo-rum search (Elsas and Carbonell, 2009; Seo et al2009), question answering and expert finding (Xu etal., 2008; Wang et al2011a), and interpersonal re-lationship identification (Diehl et al2007).While conversational features may be important,Web-derived corpora are not always annotated withthis information, and the nature of conversations onthe Web can vary wildly across domains and venues.Addressing these concerns, there has been recentwork with unsupervised models of Web conversa-tions based on hidden Markov models (Ritter et al2010), where each state corresponds to a conversa-tional class or ?act.?
Unlike more traditional uses ofHMMs in which a single token is emitted per timestep, HMM emissions in conversations correspondto entire blocks of text, such that an entire messageis generated at each step.
Because each time step isassociated with a block of variables, we refer to thistype of HMM as a block HMM (Fig.
1a).While block HMMs offer a concise model ofinter-message structure, they have the limitation thateach text block (message) belongs to exactly oneclass.
Many modern generative models of text, incontrast, allow documents to contain many latentclasses.
For example, topic models such as LatentDirichlet Allocation (LDA) (Blei et al2003) as-sume each document has its own distribution overmultiple classes (often called ?topics?).
For manypredictive tasks, topic models outperform single-class generative models such as Naive Bayes.
Theseproperties could similarly be desirable in conversa-tion modeling.
An email might contain a request,a question, and an answer to a previous question ?three distinct dialog acts within a single message.This motivates the desire to allow a message to be amixture of classes.In this paper, we introduce a new type of modelwhich combines the functionality of topic models,which posit latent class assignments to each individ-ual token, with Markovian sequence models, which94?piz1 z2 z3 .
.
.w1 w2 w3 .
.
.N N N(a) Block HMM?
?1 ?2 ?3 .
.
.z1 z2 z3 .
.
.w1 w2 w3 .
.
.N N N(b) LDA?pi1 pi2 pi3 .
.
.z1 z2 z3 .
.
.w1 w2 w3 .
.
.N N N(c) M4Figure 1: The graphical models for the block HMM (left) where each block of tokens depends on exactly one latentclass, LDA (center) where each token individually depends on a latent class, and M4 (right) where the class distri-butions are dependent across blocks.
Some parameters are omitted for simplicity.
This figure depicts the Bayesianvariant of the block HMM (Ritter et al2010) where the transition distributions pi depend on a Dirichlet(?)
prior.govern the transitions between text blocks in a se-quence.
We generalize the block HMM approach sothat there is no longer a one-to-one correspondencebetween states in the Markov chain and latent dis-course classes.
Instead, we allow a state in the HMMto correspond to a mixture of many classes: we re-fer to this family of models as mixed membershipMarkov models (M4).
Instead of defining explicittransition probabilities from one class to another asin a traditional HMM, we define the distribution overclasses as a function of the entire histogram of classassignments of the previous text segment.
We defineour model using the same number of parameters asa standard HMM (?2), and we present a straightfor-ward approximate inference algorithm (?3).While we introduce a general model, we will fo-cus on the task of unsupervised conversation model-ing.
Specifically, we build off the Bayesian blockHMMs used by Ritter et al2010) for modelingTwitter conversations, which will be our primarybaseline.
After discussing related work (?4), wepresent experimental results on a set of Twitter con-versations as well as a set of threads from CNETdiscussion forums (?5).
We show that M4 increasesthread reconstruction accuracy by up to 15% com-pared to the HMM of Ritter et al2010), and wereduce variation of information against speech actannotations by an average of 18% from HMM andLDA baselines.
To the best of our knowledge, thiswork is the first attempt to quantitatively compareunsupervised models against gold standard speechact annotations.2 M4: Mixed Membership Markov ModelsIn this section, we extend the block HMM by intro-ducing mixed membership Markov models (M4).Under the block HMM, as utilized by Ritter et al(2010), messages in a conversation flow according toa Markov process, where the words of messages aregenerated according to language models associatedwith a state in a hidden Markov model.
The intu-ition is that HMM states should correspond to somenotion of a conversation ?act?
such as QUESTION orANSWER.
The intuition is the same under M4, butnow each token in a message is given its own classassignment, according to a class distribution for thatparticular message.
A message?s class distributiondepends on the class assignments of the previousmessage, yielding a model that retains sequential de-pendencies between messages, while allowing forfiner grained class allocation than the block HMM.Modeling messages (or more generally, text blocks)as a mixture of multiple classes rather than a singleclass gives rise to the ?mixed membership?
property.In the subsections below, we formalize and ana-lyze this new model.2.1 Structure AssumptionsWe first define the discourse structure and termi-nology we will be assuming.
The discourse struc-ture is a directed graph, where nodes correspond tosegments of a document (which we will refer to as?blocks?
of text), and the edges define the dependen-cies between them.95Thus, a text block is a set of tokens, while a doc-ument consists of the discourse graph and all blocksassociated with it.
In the context of modeling con-versation threads, which will be the focus of our ex-periments later, we will assume a block correspondsto a single message in a thread.
The parent of a mes-sage m is the message to which it is a response; ifa message is not in response to anything in particu-lar, then it has no parent.
Any replies to the messagem are the children of m. The thread as a whole iscalled a document.The discourse graph should be acyclic.
A directedacyclic graph (DAG) offers a flexible representationof discourse (Rose?
et al1995), but for simplic-ity, we will restrict this and assume that each sub-graph is a tree; i.e.
no message has multiple parents.The graph as a whole may be a forest: for example,someone could write a new message in a conversa-tion that is not directly in reply to any previous mes-sage, so this message would not have any parents,and would form the root of a new tree in the forest.2.2 Generative StoryExtending the block HMM, latent classes in M4 arenow associated with each individual token, ratherthan one class for an entire block.
The key differ-ence between the generative process behind M4 andthe block HMM is that the transition distributionsare defined with a log-linear model, which uses classassignments in a block as features to define the dis-tribution over classes for the children of that block.Put another way, a state in M4 corresponds to a classhistogram, and transitions between states are func-tions of the log-linear parameters.Given a block b, we will use the notation b to de-note the block?s feature vector, which consists of thehistogram of latent class assignments for the tokensof b.1 There are K classes.
Additionally, we assumeeach feature vector has an extra cell containing anindicator denoting whether the block has no parent?
this allows us to learn transitions from a ?start?state.
We also include a bias feature that is always1, to learn a default weight for each class.
There1One could also use other functions of the class histogramsrather than the raw counts themselves.
For example, we experi-mented with binary indicator features (i.e.
?does class k appearanywhere in block b??
), but this performed consistently worsein early experiments, and we do not consider this further.are thus K + 2 features which are used to predictthe probability of each of the K classes.
The fea-tures are weighted by transition parameters, denoted?.
The random variable z denotes a latent class, and?z is a discrete distribution over word types ?
thatis, each class is associated with a unigram languagemodel.
The transition distribution over classes is de-noted pi, which is given in terms of ?
and the featurevector of the parent block.Under this model, a corpus D is generated by:1.
For each (j, k) in the transition matrix ?K?K+2:(a) Draw transition weight ?jk ?
N (0, ?2).2.
For each class j:(a) Draw word distribution ?j ?Dirichlet(?).3.
For each block b of each document d in D:(a) Set class probability pibj =exp(?Tj a)?j?
exp(?Tj?a)for all classes j, where a is the feature vec-tor for block a, the parent of b.
(b) For each token n in block b:i.
Sample class z(b,n) ?
pib.ii.
Sample word w(b,n) ?
?z .For each block of text in a document (e.g.
eachmessage in a conversation), the distribution overclasses pi is computed as a function of the featurevector of the block?s parent and the transition pa-rameters (feature weights) ?.
Each ?jk has an intu-itive interpretation: a positive value means that theoccurrence of class k in a parent block increases theprobability that j will appear in the next block, whilea negative value reduces this probability.The observed words of each block are generatedby repeatedly sampling classes from the block?s dis-tribution pi, and for each sampled class z, a singleword is sampled from the class-specific distributionover words ?z .
In contrast, under the block HMM, aclass z is sampled once from the transition distribu-tion, and words are repeatedly sampled from ?z .We place a symmetric Dirichlet prior on each ?with concentration parameter ?, which smoothes theword distributions, and we place a 0-mean Gaussianprior on each ?
parameter, which acts as a regular-izer.
The graphical diagram is shown in Figure 1along with the block HMM and LDA.
This figure96shows how M4 combines the sequential dependen-cies of the block HMM with the token-specific classassignments of LDA.2.3 DiscussionLike the block HMM, M4 is a type of HMM.
A latentsequence under M4 forms a Markov chain in whicha state corresponds to a histogram of classes.
(Forsimplicity, we are ignoring the extra features of thestart state indicator and bias in this discussion.)
Ifwe assume a priori that the length of a block is un-bounded, then this state space is NK where 0 ?
N.The probability of transitioning from a state b to an-other state b?
?
NK is:P (b?
b?)
?
?NMultinomial(b?|pi(b), N) (1)where N =?k b?k, ?N is the probability that ablock has N tokens,2 and pi(b) is the transition dis-tribution given a vector b.
This follows from thegenerative story defined above, with an additionalstep of generating the number of tokens N from thedistribution ?.We currently define a block b?s distribution pib interms of the discrete feature vector a given by itsparent a.
We could have instead made pib a func-tion of the parent?s distribution pia ?
this would leadto a model that assumes a dynamical system over acontinuous space rather than a Markov chain.
How-ever, as a generative story we believe it makes moresense for a block?s distribution to depend on the ac-tual class values which are emitted by the parent.Similar arguments are made by Blei and Mcauliffe(2007) when designing supervised topic models.Under a block HMM with one class per block,there are K states corresponding to the K classes,requiring K?K parameters to define the transitionmatrix.
Under M4, there is a countably infinite num-ber of states, but the transitions are still defined byK?K parameters (ignoring extra features).
M4 thusutilizes a larger state space without increasing thenumber of free parameters.3 Inference and Parameter EstimationWe must infer the values of the hidden variables zas well as the parameters for the word distributions2The distribution over the number of tokens can be arbitrary,as this is observed and does not affect inference.
In topic mod-els, this is sometimes assumed to be Poisson (Blei et al2003).?
and transition weights ?.
Standard HMM dy-namic programming algorithms cannot straightfor-wardly be used for M4 because of the unboundedlylarge state space.
We instead turn to Markov chainMonte Carlo (MCMC) methods as a tool for approx-imate inference.
We derive a stochastic EM algo-rithm in which we alternate between sampling classassignments for the word tokens and optimizing thetransition parameters, outlined in the following twosubsections.3.1 Latent Class SamplingTo explore the posterior distribution over latentclasses, we use a collapsed Gibbs sampler such thatwe marginalize out each word multinomial ?
andonly need to sample the token assignments z con-ditioned on each other.
Given the current state of thesampler, we sample a token?s class according to:P (z(b,n) = k|z?
(b,n),w, ?, ?)
?
(2)exp(?Tk a)?k?
exp(?Tk?a)nwk + ?nk +W?
?c?C?j(exp(?Tj b)?j?
exp(?Tj?b))njcThe notation nwk indicates the number of tokenswith word type w that have been assigned to topic k.W is the vocabulary size.
a is the parent block of b,and C is the set of b?s children.
b is the feature vectorcorresponding to block b (i.e.
the class histogramplus the bias feature), where the histogram includesthe incremented count of the candidate class k.This sampling distribution is very similar to thatof LDA (Griffiths and Steyvers, 2004), but the distri-bution over ?topics?
is now a function of the previ-ous block, which gives the leftmost term.
The right-most term is a result of the dependency of the childblocks (C) on the class assignments of b.Due to the rightmost term, the complexity of com-puting the sampling distribution is quadratic in thenumber of classes, rather than the linear complexityof a single-class HMM.
Our assumption is that thenumber of sequence-dependent classes (e.g.
speechacts or discourse states) will be reasonably small.
Ifit is desired to have a large number of latent topics asis common in LDA, this model could be combinedwith a standard topic model without sequential de-pendencies, as explored by Ritter et al2010).973.2 Transition Parameter OptimizationDifferentiating the corpus likelihood with respect to?
yields the standard equation for log-linear models:?`?
?zk=?bak(nzb ?
nbexp(?Tz a)?z?
exp(?Tz?a))?
?zk?2(3)where a is the parent of block b, a is the feature vec-tor associated with a, nzb is the number of times classz occurs in block b and nb is the total number of to-kens in block b.Standard optimization methods can be used tolearn these parameters.
In our experiments, we findthat we obtain good results by simply performinga single iteration of gradient ascent after each sam-pling iteration t,3 with the following update:?
(t+1)zk = ?
(t)zk + ?(t)?`?
?zk(4)where ?
is a step size function.4 Related WorkHidden Markov models have a recent history as sim-ple models of document structure.
Stolcke et al(2000) used HMMs as a general model of discoursewith an application to speech acts (or dialog acts)in conversations.
Barzilay and Lee (2004) appliedHMMs as an unsupervised model of discourse.
Thiswork used HMMs to model the progression of sen-tences in articles, and was shown to be useful for or-dering sentences and generating summaries of newsarticles.
More recently, Wang et al2011b) exper-imented with similar tasks using a related HMM-based model called the Structural Topic Model.Unsupervised HMMs were applied to conversa-tional data by Ritter et al2010) who experimentedwith Twitter conversations.
The authors also experi-mented with incorporating a topic model on top ofthe HMM to distinguish speech acts from topicalclusters, with mixed results.
Joty et al2011) ex-tended this work by enriching the emission distribu-tions and using additional features such as speakerand position information.
An approach to unsuper-vised discourse modeling that does not use HMMs is3Incremental updates are justified under the generalized EMalgorithm (Dempster et al1977).
Each gradient step with re-spect to ?
corresponds to a generalized M-step, while each sam-pling iteration corresponds to a stochastic E-step.the latent permutation model of Chen et al2009).This model assumes each segment (e.g.
paragraph)in a document is associated with a latent class ortopic, and the ordering of topics within a documentis modeled as a deviation from some canonical or-dering.Extensions to the block HMM have incorpo-rated mixed membership properties within blocks,notably the Markov Clustering Topic Model(Hospedales et al2009), which allows each HMMstate to be associated with its own distribution overtopics in a topic model.
Like the block HMM, thisstill assumes a relatively small number of HMMstates, but with an extra layer of latent variables be-fore the observations are emitted.
This is more re-strictive than the unbounded state space of M4.Decoupling HMM states from latent classes wasconsidered by Beal et al1997) with the FactorialHMM, which uses factorized state representations.The Factorial HMM is most often used to model in-dependent Markov chains, whereas M4 has a densegraphical model topology: the probability of eachof the latent classes depends on the counts of all ofthe classes in the previous block.
The trick in M4is to define the transition matrix via a function ofa limited number of parameters, allowing tractableinference in a model with arbitrarily many states.In topic models, log-linear formulations of la-tent class distributions4 are utilized in correlatedtopic models (Blei and Lafferty, 2007) as a meansof incorporating covariance structure among topicprobabilities.
Applying log-linear regression to po-tentially many features was combined with LDAby Mimno and McCallum (2008), who model theDirichlet prior over topics as a function of documentfeatures.
In M4, such features would correspond tothe class histograms of previous blocks, introducingadditional dependencies between documents.One topic model that imposes sequential depen-dencies between documents is Sequential LDA (Duet al2010), which models a document as a se-quence of segments (such as paragraphs) governedby a Pitman-Yor process, in which the latent topicdistribution of one segment serves as the base dis-tribution for the next segment.
This is in the spirit4This formulation corresponds to the natural parameteriza-tion of the multinomial distribution.98of our work, where the latent classes in a segmentdepend on the class distribution of the previous seg-ment.
By using the Pitman-Yor process, however,this work assumes topics are positively correlated,i.e.
the occurrence of a topic in one segment makesit likely to appear in the next.
In contrast, we wishto learn arbitrary transitions, both positive and neg-ative, between the latent classes.5 Experiments with Conversation DataWe experiment with two corpora of text-based asyn-chronous conversations on the Web.
One of these isannotated with speech act labels, against which wecompare our unsupervised clusters.
We measure thepredictive capabilities of the model via perplexityexperiments and the task of thread reconstruction.5.1 Data SetsFirst, we use a corpus of discussion threads fromCNET forums (Kim et al2010), which are mostlytechnical discussion and support.
This corpus in-cludes 321 threads and a total of 1309 messages,with an average message length of 78 tokens afterpreprocessing.5 Second, we use the Twitter data setcreated by Ritter et al2010).
We consider 36K con-versation threads for a total of 100K messages withaverage length 13.4 tokens.Both data sets are already annotated with the re-ply structure, so the discourse graph is given.
Wepreprocess the data by treating contiguous blocksof punctuation as tokens, and we remove infrequentwords.
The Twitter corpus has some additional pre-processing, such as converting URLs to a singleword type.5.2 Baseline ModelsOur work is motivated by the Bayesian HMM ap-proach of Ritter et al2010) ?
the model we re-fer to as the block HMM (BHMM) ?
and we con-sider this our primary baseline.
(See also (Goldwa-ter and Griffiths, 2007) for more details on BayesianHMMs with Dirichlet priors.)
We also compareagainst LDA, which makes latent assignments at thetoken-level, but blocks of text are independent of5Three messages in this corpus have multiple parents.
Forthe sake of conciseness, we simply remove these threads ratherthan introducing a method to model multiple parents.each other.
In other words, BHMM models sequen-tial dependencies but allows only single-class mem-bership, whereas LDA uses no sequence informationbut has a mixed membership property.
M4 combinesthese two properties.We use standard Gibbs samplers for both baselinemodels, and we optimize the Dirichlet hyperparam-eters (for the transition and topic distributions) usingMinka?s fixed-point iterations (2003).5.3 Incorporating Background DistributionsIn our experiments, we find that the intrusion ofcommon stop words can make the results difficultto interpret, but we do not want to perform simplestop word removal because common function wordsoften play important roles in the latent classes (i.e.speech acts) of the conversation data we considerhere.
We instead handle this by extending our modelto include a ?background?
distribution over wordswhich is independent of the latent classes in a docu-ment; this was also done by Wang et al2011b).The idea is to introduce a binary switching vari-able x into the model which determines whether aword is generated from the general background dis-tribution or from the distribution specific to a latentclass z.
Loosely, if the marginal probability of aword was given by p(w) =?z p(w|z)p(z), theintroduction of a background distribution gives themarginal probability p(w) = p(x = 0)p(w|B) +p(x = 1)?z p(w|z).
This is common practice andwe will not go into detail; see (Chemudugunta et al2006) for a general example on sampling switchingvariables.
We augment all three models with a back-ground distribution in exactly the same way, so thatthe comparison is fair.
We use a Beta(10.0, 10.0)prior over the switching distribution.5.4 Experimental SetupAll of our results are averaged across four randomlyinitialized chains which are run for 5000 iterations,with five samples collected during the final 500 it-erations.
We take small gradient steps of decreasingsize with ?
(t) = 0.1/(1000 + t).We set ?2 = 10.0 as the variance of the ?weights.
We use optimized asymmetric priors as de-scribed in ?5.2, and we use a symmetric Dirichletfor the word distributions, following Wallach et al(2009).
We sample the scaling hyperparameter ?
via995 10 15 20 25CNETUnigram 63.07 63.07 63.07 63.07 63.07LDA 57.16 54.35 52.88 51.63 50.50BHMM 61.26 61.06 60.92 60.86 60.85M4 60.38 59.58 59.26 59.21 59.25TwitterUnigram 93.00 93.00 93.00 93.00 93.00LDA 83.70 78.40 74.01 70.91 70.16BHMM 90.51 89.94 89.68 89.59 89.38M4 88.44 86.17 85.50 85.55 86.31Table 1: Average perplexity of held-out data for variousnumbers of latent classes.Metropolis-Hastings proposals: we add Gaussian-distributed noise to the log of the current ?, thenexponentiate this to yield the proposed ?(new).
Thislog-space proposal ensures that ?
is always positive.When computing the transition distributions forM4, we normalize the class histograms so that thecounts to sum to 1.
This helps with numeric sta-bility because the input vectors stay within a smallbounded range.65.5 Experimental Results5.5.1 PerplexityWe begin with standard measures of the perplex-ity of held-out data.
For these experiments, wetrain on 75% of the data, and test on the remaining25%.
We run the sampler for 500 iterations using theword distributions and transition parameters learnedduring training; we compute the average perplexityfrom the final ten sampling iterations.Results for different numbers of classes are shownin Table 1.
These results demonstrate the advan-tage of models with the mixed membership property.Although LDA outperforms both sequence models,this is be expected.
Each block?s topic distributionis stochastically generated with LDA, whereas in thetwo sequence models, the distribution over classesis simply a deterministic function of the previousblock.
This allows LDA to infer parameters that fitthe data more tightly.
Comparing only the two se-quence models, we find that M4 does significantlybetter than BHMM in all cases with p < 0.05.6Implementations of both M4 and the block HMM will beavailable at http://cs.jhu.edu/?mpaulBHMM M40.250.300.350.400.450.500.55ThreadReconstruction AccuracyCNETBHMM M40.350.360.370.380.390.400.410.42 TwitterFigure 2: Accuracy at the task of thread reconstruction.The horizontal bar indicates a random baseline.If capturing sequence information is not impor-tant, then LDA may provide a better fit to a corpusthan sequence models.
In the next two subsections,we will consider tasks where the sequential structureis important, thus LDA is not an appropriate choice.5.5.2 Thread ReconstructionA natural predictive task of the sequence modelsis to reconstruct the discourse graph of a documentwhere the structure is unknown.
In the conversa-tion domain, this corresponds to the task of threadreconstruction (Yeh and Harnly, 2006; Wang et al2011c).
Given only a flat structure, can we recoverthe reply structure of messages in the conversation?Previous work with BHMM found the optimalstructure by computing the likelihood of all permu-tations of a thread or sequence (Ritter et al2010;Wang et al2011b).
We take a more practical ap-proach and find the optimal structure as part of ourinference procedure.
We do this by treating the par-ent of each block as a hidden variable to be inferred.The parent of block b is the random variable rb, andwe alternate between sampling values of the latentclasses z and the parents r. The sampling distri-butions are annealed, as a search technique to findthe best configuration of assignments (Finkel et al2005).
At temperature ?
, we sample a block?s parentaccording to:P (rb = a|z, ?)
?
?j(exp(?Tj a)?j?
exp(?Tj?a))njb/?
(5)100For each conversation thread, any message is acandidate for the parent of block b (except b itself)including the dummy ?start?
block.As before, we train on 75% of the data, and runthis experiment on the remaining 25%.
We run thesampler for 500 iterations, cooling ?
by 1% aftereach iteration, where ?
(0) = 1.
We measure accu-racy as the percentage of blocks whose assignmentfor rb matches the true parent.
For each fold, werun this estimation procedure from five random ini-tializations and average the results.
Like Ritter et al(2010), we do not enforce temporal constraints in thethread structure for this experiment.
We are purelyevaluating the predictive abilities of the model ratherthan its performance in a full-fledged reconstructionsetup, which would require richer features beyondthe scope of this paper.Figure 2 shows results comparing M4 againstBHMM.
Because all blocks are independent underLDA, it cannot be used in this experiment; usingLDA would amount to a random baseline.We plot the distribution of results from vari-ous samples and various numbers of classes in{5, .
.
.
, 25}.
Most of the variance is across foldsand samples; we find that there is not a strong trendin accuracy as a function of the number of classes.This suggests that most of the sequence predictionsare carried by a small subset of the classes.On average, M4 outperforms BHMM by morethan 15 points on the CNET corpus.
M4 is also bet-ter on the Twitter corpus, but the difference is not sostark.
This seems to confirm our intuition that theadvantage of M4 over BHMM is greater when theblocks are longer; tweets may be short enough thatthe single-class assumption is not as limiting.5.5.3 Speech Act DiscoveryThus far, we have investigated the predictivepower of the model, but we would also like to deter-mine if the inferred clusters correspond to human-interpretible classes.
In the case of conversationdata, our hope is that some of the latent classesrepresent speech acts or dialog acts (Searle, 1975).While there is a body of work in supervised speechact classification (Cohen et al2004; Bangalore etal., 2006; Surendran and Levow, 2006; Qadir andRiloff, 2011), the variety of conversation domainson the Web motivates the use of unsupervised ap-5 10 15 20 25Number of classes2.02.53.03.54.04.55.0Variationof Information(VI)Speech Act Clustering (CNET)RandomLDABHMMM4Figure 3: The variation of information between thehuman-created speech act annotations of the CNET cor-pus and the latent class assignments by various models.proaches.The CNET corpus is annotated with twelvespeech act classes: QUESTION and ANSWER, whichare both broken down into multiple sub-classes, aswell as RESOLUTION, REPRODUCTION, and OTHER(Kim et al2010).
We would like to quantitativelymeasure how closely the latent states induced by ourmodel match these annotations.7We can measure this with variation of informa-tion (Meila, 2003), which has been used in recentyears for unsupervised evaluation, e.g.
in part-of-speech clustering (Goldwater and Griffiths, 2007).Given two sets of variable assignments z and z?, thevariation of information is defined as H(Z|Z ?)
+H(Z ?|Z).
In other words, given one clustering, howmuch uncertainty do we have about the other?
Re-sults are shown in Figure 3: a lower value corre-sponds to higher similarity.On the CNET corpus, M4 outperforms both base-lines in all cases by a very significant margin.
Qual-itatively, we see clusters and transition parametersthat make sense.
For example, the class with topwords {i,my, have, computer, am, ?, tried, help}is most likely to begin a thread (with ?
= +1.94)and appears to describe questions or requests for7Some messages have multiple labels.
Since messages arenot annotated at finer granularities, we handle this by simplyduplicating such messages, once per label, and measuring clus-tering performance on this expanded set of labeled data whichnow has one label per token.101!
you ?
:)u yourgood !!thanks.
i , it youbut thatim lol itsto in !
.im ?
the atbe going!
* :d lolhaha :p ?..
me !!
:o?
he .
isthe himhis thatwas like.
the of , ?a in is tofor that-url- rtjust #todayanyonepeople?+?++?++++++ ?+++?++?+?
?+++ ?+?
?++?+?+Figure 4: Example output from a model trained on the Twitter corpus with 15 classes (7 shown).
Each node corre-sponds to a class learned by the model, and the most probable words are shown for each class.
The symbols + and?
on the directed edges denote the sign of the ?
associated with transitioning from one class to another, and the sizeof the symbols is scaled by the magnitude of ?.
Non-edge arrows going into a node represent the weight of starting aconversation with that class.
Low-magnitude weights are not shown, and some edges are omitted to avoid clutter.help.
The class is not likely to be followed by itself(?
= ?0.32) but is likely to be followed by the classwith words {you, your, /, com, ., http, windows}(with ?
= +1.38).The Twitter corpus does not have speech act an-notations, so we offer example output in Figure 4.We again see patterns that we might expect to find insocial media conversations, and some classes appearto correspond to speech acts such a declarations, per-sonal questions, and replies.
For example, the classin the center of the figure has words like you and butwhich suggests it is used in reply to other messages,and indeed we see that it has a positive weight offollowing almost every class, but a negative weightfor actually starting a thread.
Conversely, the classcontaining URLs (which corresponds to the act ofsharing news or media) is likely to begin a thread,but is not likely to follow other classes except itself.How well unsupervised models can truly capturespeech acts is an open question.
Much as LDA?topics?
do not always correspond to what humanswould judge to be semantic classes (Chang et al2009), the conversation classes inferred by unsu-pervised sequence models are similarly unlikely tobe a perfect fit to human-assigned classes.
Never-theless, these results suggest M4 is a step forward.Our model provides a framework for defining inter-message transitions as functions of multiple classes,which will be a desirable property for many corpora.6 ConclusionWe have presented mixed membership Markovmodels (M4), which extend the simple HMM ap-proach to discourse modeling by positing class as-signments at the level of individual tokens.
This al-lows blocks of text to belong to potentially multipleclasses, a property that relates M4 to topic models.This type of model can be viewed as an HMM withan expanded state space, but because the transitionprobabilities are a function of a small number of pa-rameters, the output remains human-interpretible.M4 can be taken as a general family of models andcan be readily extended.
In this work, we focusedon introducing a model of inter-message structure,but certainly more sophisticated models of intra-message structure beyond unigram language mod-els could be incorporated into M4.
Standard topicmodel extensions such as n-gram models (Wallach,2006) can straightforwardly be applied here, and in-deed we already applied such an extension by in-corporating background distributions in ?5.3.
Forconversational data, it could make sense to segment102messages (e.g.
into sentences) and constraint eachsegment to belong to one class or speech act; modi-fications along these lines have been applied to topicmodels as well (Gruber et al2007).
While we havefocused on conversation modeling, M4 is a generalprobabilistic model that could be applied to otherdiscourse applications, for example modeling sen-tences or paragraphs in articles rather than messagesin conversations; it could also be applied to data be-yond text.Compared to a Bayesian block HMM, M4 per-forms much better at a variety of tasks.
A draw-back is that the time complexity of inference as pre-sented here is quadratic in the number of classesrather than linear.
Improving this may be the subjectof future research.
Another potential avenue of fu-ture work is to model transitions such that a Dirichletprior for the class distribution of a block, rather thanthe class distribution itself, depends on the previousclass assignments.
This would yield a model thatmore closely resembles LDA, but with topic priorsthat encode sequence information.AcknowledgementsThanks to Matt Gormley, Mark Dredze, Jason Eis-ner, the members of my lab and the anonymous re-viewers for helpful feedback and discussions.
Thismaterial is based upon work supported by a NationalScience Foundation Graduate Research Fellowshipunder Grant No.
DGE-0707427 and a Dean?s Fel-lowship from the Johns Hopkins University WhitingSchool of Engineering.ReferencesSrinivas Bangalore, Giuseppe Di Fabbrizio, and AmandaStent.
2006.
Learning the structure of task-drivenhuman-human dialogs.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand the 44th annual meeting of the Association forComputational Linguistics, ACL-44, pages 201?208.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In HLT-NAACL2004: Main Proceedings, pages 113?120, Boston,Massachusetts, USA, May 2 - May 7.
Association forComputational Linguistics.M.
J. Beal, Z. Ghahramani, and C. E. Rasmussen.
1997.Factorial hidden markov models.
In Machine Learn-ing, volume 29, pages 29?245.D.
Blei and J. Lafferty.
2007.
A correlated topic modelof science.
Annals of Applied Statistics, 1(1):17?35.David M. Blei and Jon D. Mcauliffe.
2007.
Supervisedtopic models.
In Advances in Neural Information Pro-cessing Systems 21.David Blei, Andrew Ng, and Michael Jordan.
2003.
La-tent dirichlet alation.
Journal of Machine LearningResearch, 3.Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,Chong Wang, and David Blei.
2009.
Reading TeaLeaves: How Humans Interpret Topic Models.
In Neu-ral Information Processing Systems (NIPS).Chaitanya Chemudugunta, Padhraic Smyth, and MarkSteyvers.
2006.
Modeling general and specific as-pects of documents with a probabilistic topic model.In NIPS, pages 241?248.Harr Chen, S. R. K. Branavan, Regina Barzilay, andDavid R. Karger.
2009.
Global models of documentstructure using latent permutations.
In Proceedingsof Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, NAACL ?09,pages 371?379.William W. Cohen, Vitor R. Carvalho, and Tom M.Mitchell.
2004.
Learning to classify email into?speech acts?.
In Proceedings of EMNLP 2004,pages 309?316, Barcelona, Spain, July.
Associationfor Computational Linguistics.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the emalgorithm.
Journal of the Royal Statistical Society.
Se-ries B (Methodological), 39(1):1?38.Christopher P. Diehl, Galileo Namata, and Lise Getoor.2007.
Relationship identication for social network dis-covery.
In AAAI?07.Lan Du, Wray Buntine, and Huidong Jin.
2010.
Se-quential latent dirichlet alation: Discover underly-ing topic structures within a document.
2010 IEEEInternational Conference on Data Mining, pages 148?157.Jonathan L. Elsas and Jaime Carbonell.
2009.
It pays tobe picky: An evaluation of thread retrieval in online fo-rums.
In 32nd Annual International ACM SIGIR Con-ference on Research and Development on InformationRetrieval(SIGIR 2009).Jenny Rose Finkel, Trond Grenager, and Christopher D.Manning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In ACL.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of the 45th Annual Meeting of103the Association of Computational Linguistics, pages744?751, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Tom Griffiths and Mark Steyvers.
2004.
Finding scien-tific topics.
In Proceedings of the National Academyof Sciences of the United States of America.Amit Gruber, Michal Rosen-Zvi, and Yair Weiss.
2007.Hidden topic markov models.
In Artificial Intelligenceand Statistics (AISTATS), San Juan, Puerto Rico.Timothy Hospedales, Shaogang Gong, and Tao Xiang.2009.
A markov clustering topic model for miningbehaviour in video.
In International Conference onComputer Vision (ICCV).Shafiq R. Joty, Giuseppe Carenini, and Chin-Yew Lin.2011.
Unsupervised modeling of dialog acts in asyn-chronous conversations.
In IJCAI, pages 1807?1813.Su Nam Kim, Li Wang, and Timothy Baldwin.
2010.Tagging and linking web forum posts.
In Proceedingsof the Fourteenth Conference on Computational Natu-ral Language Learning, CoNLL ?10, pages 192?202.Marina Meila.
2003.
Comparing clusterings by the vari-ation of information.
Learning Theory and Kernel Ma-chines, pages 173?187.D.
Mimno and A. McCallum.
2008.
Topic models condi-tioned on arbitrary features with dirichlet-multinomialregression.
In UAI.Tom Minka.
2003.
Estimating a dirichlet distribution.Ashequl Qadir and Ellen Riloff.
2011.
Classifying sen-tences as speech acts in message board posts.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 748?758,Edinburgh, Scotland, UK., July.
Association for Com-putational Linguistics.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised modeling of twitter conversations.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, HLT ?10, pages 172?180.Carolyn Penstein Rose?, Barbara Di Eugenio, Lori S.Levin, and Carol Van Ess-Dykema.
1995.
Discourseprocessing of dialogues with multiple threads.
In Pro-ceedings of the 33rd annual meeting on Association forComputational Linguistics, ACL ?95, pages 31?38.John Searle, 1975.
A taxonomy of illocutionary acts.University of Minnesota Press, Minneapolis.Jangwon Seo, W. Bruce Croft, and David A. Smith.2009.
Online community search using thread struc-ture.
In ACM Conference on Information and Knowl-edge Management (CIKM 2009), pages 1907?1910.Andreas Stolcke, Noah Coccaro, Rebecca Bates, PaulTaylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-beth Shriberg, Daniel Jurafsky, Rachel Martin, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26(3):339?373,September.Dinoj Surendran and Gina-Anne Levow.
2006.
Dialogact tagging with support vector machines and hiddenmarkov models.
In Interspeech.Hanna M. Wallach, David Mimno, and Andrew McCal-lum.
2009.
Rethinking LDA: Why priors matter.
InNIPS.H.M.
Wallach.
2006.
Topic modeling: beyond bag-of-words.
In ICML ?06: Proceedings of the 23rd inter-national conference on Machine learning, pages 977?984.Hongning Wang, Chi Wang, ChengXiang Zhai, and Ji-awei Han.
2011a.
Learning online discussion struc-tures by conditional random fields.
In 34th AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR?11),pages 435?444.Hongning Wang, Duo Zhang, and ChengXiang Zhai.2011b.
Structural topic model for latent topical struc-ture analysis.
In ACL, pages 1526?1535.
The Associ-ation for Computer Linguistics.Li Wang, Marco Lui, Su Nam Kim, Joakim Nivre, andTimothy Baldwin.
2011c.
Predicting thread discoursestructure over technical web forums.
In Proceedingsof EMNLP 2011, pages 13?25.Gu Xu, Hang Li, and Wei-Ying Ma.
2008.
Fora: Lever-aging the power of internet communities for questionanswering.
In 1st International Workshop on QuestionAnswering on the Web (QAWeb08).Jen-Yuan Yeh and Aaron Harnly.
2006.
Email threadreassembly using similarity matching.
In Proceedingsof the 3rd Conference on Email and Anti-Spam (CEAS2006), pages 64?71.104
