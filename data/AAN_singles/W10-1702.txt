Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 11?16,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsFast Consensus Hypothesis Regeneration for Machine TranslationBoxing Chen, George Foster and Roland KuhnNational Research Council Canada283 Alexandre-Tach?
Boulevard, Gatineau (Qu?bec), Canada J8X 3X7{Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.caAbstractThis paper presents a fast consensus hy-pothesis regeneration approach for ma-chine translation.
It combines the advan-tages of feature-based fast consensus de-coding and hypothesis regeneration.
Ourapproach is more efficient than previouswork on hypothesis regeneration, and itexplores a wider search space than con-sensus decoding, resulting in improvedperformance.
Experimental results showconsistent improvements across languagepairs, and an improvement of up to 0.72BLEU is obtained over a competitivesingle-pass baseline on the Chinese-to-English NIST task.1 IntroductionState-of-the-art statistical machine translation(SMT) systems are often described as a two-passprocess.
In the first pass, decoding algorithms areapplied to generate either a translation N-best listor a translation forest.
Then in the second pass,various re-ranking algorithms are adopted tocompute the final translation.
The re-ranking al-gorithms include rescoring (Och et al, 2004) andMinimum Bayes-Risk (MBR) decoding (Kumarand Byrne, 2004; Zhang and Gildea, 2008;Tromble et al, 2008).
Rescoring uses more so-phisticated additional feature functions to scorethe hypotheses.
MBR decoding directly incorpo-rates the evaluation metrics (i.e., loss function),into the decision criterion, so it is effective intuning the MT performance for a specific lossfunction.
In particular, sentence-level BLEU lossfunction gives gains on BLEU (Kumar andByrne, 2004).The na?ve MBR algorithm computes the lossfunction between every pair of k hypotheses,needing O(k2) comparisons.
Therefore, onlysmall number k is applicable.
Very recently, De-Nero et al (2009) proposed a fast consensus de-coding (FCD) algorithm in which the similarityscores are computed based on the feature expec-tations over the translation N-best list or transla-tion forest.
It is equivalent to MBR decodingwhen using a linear similarity function, such asunigram precision.Re-ranking approaches improve performanceon an N-best list whose contents are fixed.
Acomplementary strategy is to augment the con-tents of an N-best list in order to broaden thesearch space.
Chen et al(2008) have proposed athree-pass SMT process, in which a hypothesisregeneration pass is added between the decodingand rescoring passes.
New hypotheses are gener-ated based on the original N-best hypothesesthrough n-gram expansion, confusion-networkdecoding or re-decoding.
All three hypothesisregeneration methods obtained decent and com-parable improvements in conjunction with thesame rescoring model.
However, since the finaltranslation candidates in this approach are pro-duced from different methods, local feature func-tions (such as translation models and reorderingmodels) of each hypothesis are not directly com-parable and rescoring must exploit rich globalfeature functions to compensate for the loss oflocal feature functions.
Thus this approach is de-pendent on the use of computationally expensivefeatures for rescoring, which makes it inefficient.In this paper, we propose a fast consensus hy-pothesis regeneration method that combines theadvantages of feature-based fast consensus de-coding and hypothesis regeneration.
That is, weintegrate the feature-based similarity/loss func-tion based on evaluation metrics such as BLEUscore into the hypothesis regeneration procedureto score the partial hypotheses in the beam searchand compute the final translations.
Thus, our ap-proach is more efficient than the original three-pass hypothesis regeneration.
Moreover, our ap-proach explores more search space than consen-11sus decoding, giving it an advantage over thelatter.In particular, we extend linear corpus BLEU(Tromble et al, 2008) to n-gram expectation-based linear BLEU, then further extend the n-gram expectation computed on full-length hypo-theses to n-gram expectation computed on fixed-length partial hypotheses.
Finally, we extend thehypothesis regeneration with forward n-gramexpansion to bidirectional n-gram expansion in-cluding both the forward and backward n-gramexpansion.
Experimental results show consistentimprovements over the baseline across languagepairs, and up to 0.72 BLEU points are obtainedfrom a competitive baseline on the Chinese-to-English NIST task.2 Fast Consensus Hypothesis Regenera-tionSince the three hypothesis regeneration methodswith n-gram expansion, confusion network de-coding and re-decoding produce very similar per-formance (Chen et al, 2008), we consider onlyn-gram expansion method in this paper.
N-gramexpansion can (almost) fully exploit the searchspace of target strings which can be generated byan n-gram language model trained on the N-besthypotheses (Chen et al, 2007).2.1 Hypothesis regeneration with bidirec-tional n-gram expansionN-gram expansion (Chen et al, 2007) works asfollows: firstly, train an n-gram language modelbased on the translation N-best list or translationforest; secondly, expand each partial hypothesisby appending a word via overlapped (n-1)-gramsuntil the partial hypothesis reaches the sentenceending symbol.
In each expanding step, the par-tial hypotheses are pruned through a beam-searchalgorithm with scoring functions.Duchateau et al (2001) shows that the back-ward language model contains information com-plementary to the information in the forwardlanguage model.
Hence, on top of the forward n-gram expansion used in (Chen et al, 2008), wefurther introduce backward n-gram expansion tothe hypothesis regeneration procedure.
Backwardn-gram expansion involves letting the partial hy-potheses start from the last words that appearedin the translation N-best list and having the ex-pansion go from right to left.Figure 1 gives an example of backward n-gram expansion.
The second row shows bi-gramswhich are extracted from the original hypothesesin the first row.
The third row shows how a par-tial hypothesis is expanded via backward n-gramexpansion method.
The fourth row lists somenew hypotheses generated by backward n-gramexpansion which do not exist in the original hy-pothesis list.originalhypothesesabout weeks' work .one week's workabout one week'sabout a week workabout one week workbi-grams about weeks', weeks' work, ?,about one, ?,  week work.backwardn-gramexpansionpartial hyp.
week's workn-gram one week'snew partial hyp.
one week's worknewhypothesesabout one week's workabout week's workone weeks' work .one week's work .one week's work .Figure 1: Example of original hypotheses; bi-gramscollected from them; backward expanding a partialhypothesis via an overlapped n-1-gram; and new hy-potheses generated through backward n-gram expan-sion.2.2 Feature-based scoring functionsTo speed up the search, the partial hypothesesare pruned via beam-search in each expandingstep.
Therefore, the scoring functions appliedwith the beam-search algorithm are very impor-tant.
In (Chen et al, 2008), more than 10 addi-tional global features are computed to rank thepartial hypothesis list, and this is not an efficientway.
In this paper, we propose to directly incor-porate the evaluation metrics such as BLEUscore to rank the candidates.
The scoring func-tions of this work are derived from the method oflattice Minimum Bayes-risk (MBR) decoding(Tromble et al, 2008) and fast consensus decod-ing (DeNero et al, 2009), which were originallyinspired from N-best MBR decoding (Kumar andByrne, 2004).From a set of translation candidates E, MBRdecoding chooses the translation that has theleast expected loss with respect to other candi-dates.
Given a hypothesis set E, under the proba-bility model )|( feP , MBR computes the trans-lation e~  as follows:12)|(),(minarg~ fePeeLeEeEe?
?= ????
(1)where f is the source sentence, ),( eeL ?
is the lossfunction of two translations e and e?
.Suppose that we are interested in maximizingthe BLEU score (Papineni et al, 2002) to optim-ize the translation performance.
The loss func-tion is defined as ),(1),( eeBLEUeeL ??=?
,then the MBR objective can be re-written as)|(),(maxarg~ fePeeBLEUeEeEe?
?= ????
(2)E represents the space of the translations.
ForN-best MBR decoding, this space is the N-bestlist produced by a baseline decoder (Kumar andByrne, 2004).
For lattice MBR decoding, thisspace is the set of candidates encoded in the lat-tice (Tromble et al, 2008).
Here, with hypothesisregeneration, this space includes: 1) the transla-tions produced by the baseline decoder either inan N-best list or encoded in a translation lattice,and 2) the translations created by hypothesis re-generation.However, BLEU score is not linear with thelength of the hypothesis, which makes the scor-ing process for each expanding step of hypothe-sis regeneration very slow.
To further speed upthe beam search procedure, we use an extensionof a linear function of a Taylor approximation tothe logarithm of corpus BLEU which was devel-oped by (Tromble et al, 2008).
The originalBLEU score of two hypotheses e and e?
arecomputed as follows.)),(log(41exp(),(),(41?=??
?=?nneePeeeeBLEU ?
(3)where ),( eePn ?
is the precision of n-grams in thehypothesis e given e?
and  ),( ee ??
is a brevitypenalty.
Let |e| denote the length of e. The corpuslog-BLEU gain is defined as follows:)),(log(41)||||1,0min()),(log(41?=?+?
?=?nn eePeeeeBLEU  (4)Therefore, the first-order Taylor approxima-tion to the logarithm of corpus BLEU is shownin Equation (5).?=?
?+=?410 ),(41||),(nnneeceeeG ??
(5)where ),( eecn ?
are the counts of the matched n-grams andn?
( 40 ??
n ) are constant weightsestimated with held-out data.Suppose we have computed the expected n-gram counts from the N-best list or translationforest.
Then we may extend linear corpus BLEUin (5) to n-gram expectation-based linear corpusBLEU to score the partial hypotheses h. That is?
?= ??
?+=410 ),()],'([41||)',(n TtnnnnthtecEhehG ???
(6)where ),( thn?
are n-gram indicator functions thatequal 1 if n-gram t  appears in h  and 0 other-wise; )],'([ tecE n  ( 41 ??
n ) are the real-valuedn-gram expectations.
Different from lattice MBRdecoding, n-gram expectations in this work arecomputed over the original translation N-best listor translation forest;nT  ( 41 ??
n ) are the sets ofn-grams collected from translation N-best list ortranslation forest.
Then we make a further exten-sion: the expectations of the n-gram counts foreach expanding step are computed over the par-tial translations.
The lengths of all partial hypo-theses are the same in each n-gram expandingstep.
For instance, in the 5th n-gram expandingstep, the lengths of all the partial hypotheses are5 words.
Therefore, we use n-gram count expec-tations computed over partial original transla-tions that only contain the first 5 words.
The rea-son is that this solution contains more informa-tion about word orderings, since some n-gramsappear more than others at the beginning of thetranslations while they may appear with the sameor even lower frequencies than others in the fulltranslations.Once the expanding process of hypothesis re-generation is finished, we use a more preciseBLEU metric to score all the translation candi-dates.
We extend BLEU score in (3) to n-gramexpectation-based BLEU.
That is:??????????+?????????==?
??=?
?41 ),()]),'([),,(min(log41|||]'[|1,0minexp)',()(nTtnTtnnnnthctecEthcheEehBLEUhScore(7)where ),( thcn  is the count of  n-gram t in thehypothesis h. The step of choosing the finaltranslation is the same as fast consensus decod-ing (DeNero et al, 2009): first we compute n-13gram feature expectations, and then we choosethe translation that is most similar to the othersvia expected similarity according to feature-based BLEU score as shown in (7).
The differ-ence is the space of translations: the space of fastconsensus decoding is the same as MBR decod-ing, while the space of hypothesis regeneration isenlarged by the new translations produced via n-gram expansion.2.3 Fast consensus hypothesis regenerationWe first generate two new hypothesis lists viaforward and backward n-gram expansion usingthe scoring function in Equation (6).
Then wechoose a final translation using the scoring func-tion in Equation (7) from the union of the origi-nal hypotheses and newly generated hypotheses.The original hypotheses are from the N-best listor extracted from the translation forest.
The newhypotheses are generated by forward or back-ward n-gram expansion or are the union of bothtwo new hypothesis lists (this is called ?bi-directional n-gram expansion?
).3 Experimental ResultsWe carried out experiments based on translationN-best lists generated by a state-of-the-artphrase-based statistical machine translation sys-tem, similar to (Koehn et al, 2007).
In detail, thephrase table is derived from merged counts ofsymmetrized IBM2 and HMM alignments; thesystem has both lexicalized and distance-baseddistortion components (there is a 7-word distor-tion limit) and employs cube pruning (Huang andChiang, 2007).
The baseline is a log-linear fea-ture combination that includes language models,the distortion components, translation model,phrase and word penalties.
Weights on featurefunctions are found by lattice MERT (Machereyet al, 2008).3.1 DataWe evaluated with different language pairs: Chi-nese-to-English, and German-to-English.
Chi-nese-to-English tasks are based on training datafor the NIST 1  2009 evaluation Chinese-to-English track.
All the allowed bilingual corporahave been used for estimating the translationmodel.
We trained two language models: the firstone is a 5-gram LM which is estimated on thetarget side of the parallel data.
The second is a 5-1http://www.nist.gov/speech/tests/mtgram LM trained on the so-called English Giga-word corpus.Chi EngParallelTrainLargeData|S| 10.1M|W| 270.0M 279.1MDev |S| 1,506 1,506?4Test NIST06 |S| 1,664 1,664?4NIST08 |S| 1,357 1,357?4Gigaword |S| - 11.7MTable 1: Statistics of training, dev, and test sets forChinese-to-English task.We carried out experiments for translatingChinese to English.
We first created a develop-ment set which used mainly data from the NIST2005 test set, and also some balanced-genre web-text from the NIST training material.
Evaluationwas performed on the NIST 2006 and 2008 testsets.
Table 1 gives figures for training, develop-ment and test corpora; |S| is the number of thesentences, and |W| is the size of running words.Four references are provided for all dev and testsets.For German-to-English tasks, we used WMT20062 data sets.
The parallel training data con-tains about 1 million sentence pairs and includes21 million target words; both the dev set and testset contain 2000 sentences; one reference is pro-vided for each source input sentence.
Only thetarget-language half of the parallel training dataare used to train the language model in this task.3.2 ResultsOur evaluation metric is IBM BLEU (Papineni etal., 2002), which performs case-insensitivematching of n-grams up to n = 4.Our first experiment was carried out over1000-best lists on Chinese-to-English task.
Forcomparison, we also conducted experiments withrescoring (two-pass) and three-pass hypothesisregeneration with only forward n-gram expan-sion as proposed in (Chen et al, 2008).
In the?rescoring?
and ?three-pass?
systems, we usedthe same rescoring model.
There are 21 rescoringfeatures in total, mainly translation lexiconscores from IBM and HMM models, posteriorprobabilities for words, n-grams, and sentencelength, and language models, etc.
For a completedescription, please refer to (Ueffing et al, 2007).The results in BLEU-4 are reported in Table 2.2http://www.statmt.org/wmt06/14testset NIST?06 NIST?08baseline 35.70 28.60rescoring 36.01 28.97three-pass 35.98 28.99FCD 36.00 29.10Fwd.
36.13 29.19Bwd.
36.11 29.20Bid.
36.20 29.28Table 2: Translation performances in BLEU-4(%)over 1000-best lists for Chinese-to-English task: ?res-coring?
represents the results of rescoring; ?three-pass?, three-pass hypothesis regeneration with for-ward n-gram expansion; ?FCD?, fast consensus de-coding; ?Fwd?, the results of hypothesis regenerationwith forward n-gram expansion; ?Bwd?, backward n-gram expansion; and ?Bid?, bi-directional n-gramexpansion.Firstly, rescoring improved performance overthe baseline by 0.3-0.4 BLEU point.
Three-passhypothesis regeneration with only forward n-gram expansion (?three-pass?
in Table 2) ob-tained almost the same improvements as rescor-ing.
Three-pass hypothesis regeneration exploitsmore hypotheses than rescoring, while rescoringinvolves more scoring feature functions than theformer.
They reached a balance in this experi-ment.
Then, fast consensus decoding (?FCD?
inTable 2) obtains 0.3-0.5 BLEU point improve-ments over the baseline.
Both forward and back-ward n-gram expansion (?Fwd.?
and ?Bwd.?
inTable 2) improved about 0.1 BLEU point overthe results of consensus decoding.
Fast consen-sus hypothesis regeneration (Fwd.
and Bwd.
inTable 2) got better improvements than three-passhypothesis regeneration (?three-pass?
in Table 2)by 0.1-0.2 BLEU point.
Finally, combining hy-pothesis lists from forward and backward n-gramexpansion (?Bid.?
in Table 2), further slightgains were obtained.testset Average timethree-pass 3h 54mFwd.
25mBwd.
28mBid.
40mTable 3: Average processing time of NIST?06 andNIST?08 test sets used in different systems.
Timesinclude n-best list regeneration and re-ranking.Moreover, fast consensus hypothesis regenera-tion is much faster than the three-pass one, be-cause the former only needs to compute one fea-ture, while the latter needs to compute more than20 additional features.
In this experiment, theformer is about 10 times faster than the latter interms of processing time, as shown in Table 3.In our second experiment, we set the size ofN-best list N equal to 10,000 for both Chinese-to-English and German-to-English tasks.
The re-sults are reported in Table 4.
The same trend asin the first experiment can also be observed inthis experiment.
It is worth noticing that enlarg-ing the size of the N-best list from 1000 to10,000 did not change the performance signifi-cantly.
Bi-directional n-gram expansion obtainedimprovements of 0.24 BLEU-score for WMT2006 de-en test set; 0.55 for NIST 2006 test set;and 0.72 for NIST 2008 test set over the base-line.Lang.
ch-en de-entestset NIST?06 NIST?08 Test2006baseline 35.70 28.60 26.92FCD 36.03 29.08 27.03Fwd.
36.16 29.25 27.11Bwd.
36.17 29.22 27.12Bid.
36.25 29.32 27.16Table 4: Translation performances in BLEU-4 (%)over 10K-best lists.We then tested the effect of the extension ac-cording to which the expectations over n-gramcounts are computed on partial hypotheses ratherthan whole candidate translations as described inSection 2.2.
As shown in Table 5, we got tinyimprovements on both test sets by computing theexpectations over n-gram counts on partial hypo-theses.testset NIST?06 NIST?08full 36.11 29.14partial 36.13 29.19Table 5: Translation performances in BLEU-4 (%)over 1000-best lists for Chinese-to-English task:?full?
represents expectations over n-gram counts thatare computed on whole hypotheses; ?partial?represents expectations over n-gram counts that arecomputed on partial hypotheses.3.3 DiscussionTo speed up the search, the partial hypotheses ineach expanding step are pruned.
When pruning isapplied, forward and backward n-gram expan-sion would generate different new hypothesislists.
Let us look back at the example in Figure 1.15Given 5 original hypotheses in Figure 1, if we setthe beam size equal to 5 (the size of the originalhypotheses), the forward and backward n-gramexpansion generated different new hypothesislists, as shown in Figure 2.forward backwardone week's work .about week's workone week's work .about one week's workFigure 2: Different new hypothesis lists generated byforward and backward n-gram expansion.For bi-directional n-gram expansion, the cho-sen translation for a source sentence comes fromthe decoder 94% of the time for WMT 2006 testset, 90% for NIST test sets; it comes from for-ward n-gram expansion 2% of the time for WMT2006 test set, 4% for NIST test sets; it comesfrom backward n-gram expansion 4% of the timefor WMT 2006 test set, 6% for NIST test sets.This proves bidirectional n-gram expansion is agood way of enlarging the search space.4 Conclusions and Future WorkWe have proposed a fast consensus hypothesisregeneration approach for machine translation.
Itcombines the advantages of feature-based con-sensus decoding and hypothesis regeneration.This approach is more efficient than previouswork on hypothesis regeneration, and it exploresa wider search space than consensus decoding,resulting in improved performance.
Experimentsshowed consistent improvements across lan-guage pairs.Instead of N-best lists, translation lattices orforests have been shown to be effective for MBRdecoding (Zhang and Gildea, 2008; Tromble etal., 2008), and DeNero et al (2009) showed howto compute expectations of n-grams from a trans-lation forest.
Therefore, our future work mayinvolve hypothesis regeneration using an n-gramlanguage model trained on the translation forest.ReferencesB.
Chen, M. Federico and M. Cettolo.
2007.
Better N-best Translations through Generative n-gram Lan-guage Models.
In: Proceedings of MT Summit XI.Copenhagen, Denmark.
September.B.
Chen, M. Zhang, A. Aw, and H. Li.
2008.
Regene-rating Hypotheses for Statistical Machine Transla-tion.
In: Proceedings of COLING.
pp105-112.Manchester, UK, August.J.
DeNero, D. Chiang and K. Knight.
2009.
Fast Con-sensus Decoding over Translation Forests.
In: Pro-ceedings of ACL.
Singapore, August.J.
Duchateau, K. Demuynck, and P. Wambacq.
2001.Confidence scoring based on backward languagemodels.
In: Proceedings of ICASSP 2001.
SaltLake City, Utah, USA, May.L.
Huang and D. Chiang.
2007.
Forest Rescoring:Faster Decoding with Integrated Language Models.In: Proceedings of ACL.
pp.
144-151, Prague,Czech Republic, June.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-ran, R. Zens, C. Dyer, O. Bojar, A. Constantin andE.
Herbst.
2007.
Moses: Open Source Toolkit forStatistical Machine Translation.
In: Proceedings ofACL.
pp.
177-180, Prague, Czech Republic.S.
Kumar and W. Byrne.
2004.
Minimum Bayes-riskdecoding for statistical machine translation.
In:Proceedings of NAACL.
Boston, MA, May.W.
Macherey, F. Och, I. Thayer, and J. Uszkoreit.2008.
Lattice-based Minimum Error Rate Trainingfor Statistical Machine Translation.
In: Proceed-ings of EMNLP.
pp.
725-734, Honolulu, USA,October.F.
Och.
2003.
Minimum error rate training in statistic-al machine translation.
In: Proceedings of ACL.Sapporo, Japan.
July.F.
Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K.Eng, V. Jain, Z. Jin, and D. Radev.
2004.
A Smor-gasbord of Features for Statistical Machine Trans-lation.
In: Proceedings of NAACL.
Boston.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: A method for automatic evaluation of ma-chine translation.
In: Proceedings of the ACL 2002.R.
Tromble, S. Kumar, F. J. Och, and W. Macherey.2008.
Lattice minimum Bayes-risk decoding forstatistical machine translation.
In: Proceedings ofEMNLP.
Hawaii, US.
October.N.
Ueffing, M. Simard, S. Larkin, and J. H. Johnson.2007.
NRC?s Portage system for WMT 2007.
In:Proceedings of ACL Workshop on SMT.
Prague,Czech Republic, June.H.
Zhang and D. Gildea.
2008.
Efficient multipassdecoding for synchronous context free grammars.In: Proceedings of ACL.
Columbus, US.
June.16
