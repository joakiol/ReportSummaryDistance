Modeling Consensus: Classifier Combinationfor Word Sense DisambiguationRadu Florian and David YarowskyDepartment of Computer Science andCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218, USA{rflorian,yarowsky}@cs.jhu.eduAbstractThis paper demonstrates the substantial empiricalsuccess of classifier combination for the word sensedisambiguation task.
It investigates more than 10classifier combination methods, including secondorder classifier stacking, over 6 major structurallydifferent base classifiers (enhanced Na?ve Bayes,cosine, Bayes Ratio, decision lists, transformation-based learning and maximum variance boosted mix-ture models).
The paper also includes in-depth per-formance analysis sensitive to properties of the fea-ture space and component classifiers.
When eval-uated on the standard SENSEVAL1 and 2 data setson 4 languages (English, Spanish, Basque, andSwedish), classifier combination performance ex-ceeds the best published results on these data sets.1 IntroductionClassifier combination has been extensively stud-ied in the last decade, and has been shown to besuccessful in improving the performance of diverseNLP applications, including POS tagging (Brill andWu, 1998; van Halteren et al, 2001), base nounphrase chunking (Sang et al, 2000), parsing (Hen-derson and Brill, 1999) and word sense disambigua-tion (Kilgarriff and Rosenzweig, 2000; Stevensonand Wilks, 2001).
There are several reasons whyclassifier combination is useful.
First, by consultingthe output of multiple classifiers, the system will im-prove its robustness.
Second, it is possible that theproblem can be decomposed into orthogonal featurespaces (e.g.
linguistic constraints and word occur-rence statistics) and it is often better to train dif-ferent classifiers in each of the feature spaces andthen combine their output, instead of designing acomplex system that handles the multimodal infor-mation.
Third, it has been shown by Perrone andCooper (1993) that it is possible to reduce the clas-sification error by a factor of ( is the number ofclassifiers) by combination, if the classifiers?
errorsare uncorrelated and unbiased.The target task studied here is word sense disam-biguation in the SENSEVAL evaluation framework(Kilgarriff and Palmer, 2000; Edmonds and Cotton,2001) with comparative tests in English, Spanish,Swedish and Basque lexical-sample sense taggingover a combined sample of 37730 instances of 234polysemous words.This paper offers a detailed comparative evalu-ation and description of the problem of classifiercombination over a structurally and procedurallydiverse set of six both well established and orig-inal classifiers: extended Na?ve Bayes, BayesRa-tio, Cosine, non-hierarchical Decision Lists, Trans-formation Based Learning (TBL), and the MMVCclassifiers, briefly described in Section 4.
Thesesystems have different space-searching strategies,ranging from discriminant functions (BayesRatio)to data likelihood (Bayes, Cosine) to decision rules(TBL, Decision Lists), and therefore are amenableto combination.2 Previous WorkRelated work in classifier combination is discussedthroughout this article.
For the specific task ofword sense disambiguation, the first empirical studywas presented in Kilgarriff and Rosenzweig (2000),where the authors combined the output of the par-ticipating SENSEVAL1 systems via simple (non-weighted) voting, using either Absolute Majority,Relative Majority, or Unanimous voting.
Steven-son and Wilks (2001) presented a classifier com-bination framework where 3 disambiguation meth-ods (simulated annealing, subject codes and selec-tional restrictions) were combined using the TiMBLmemory-based approach (Daelemans et al, 1999).Pedersen (2000) presents experiments with an en-semble of Na?ve Bayes classifiers, which outper-form all previous published results on two ambigu-ous words (line and interest).3 The WSD Feature SpaceThe feature space is a critical factor in classifier de-sign, given the need to fuel the diverse strengths ofthe component classifiers.
Thus its quality is of-ten highly correlated with performance.
For thisAssociation for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
25-32.Proceedings of the Conference on Empirical Methods in NaturalAn ancient stone church stands amid the fields,the sound of bells ...Feat.
Type Word POS LemmaContext ancient JJ ancient/JContext stone NN stone/NContext church NNP church/NContext stands VBZ stand/VContext amid IN amid/IContext fields NN field/NContext ... ... ...Syntactic (predicate-argument) featuresSubjectTo stands_Sbj VBZ stand_Sbj/VModifier stone_mod JJ ancient_mod/JNgram collocational features-1 bigram stone_L JJ ancient_L/J+1 bigram stands_R VBZ stand_R/V1 trigram stone  stands JJVBZ stone/Jstands/V... ... ...
...Figure 1: Example sentence and extracted features fromthe SENSEVAL2 word churchreason, we used a rich feature space based on rawwords, lemmas and part-of-speech (POS) tags in avariety of positional and syntactical relationships tothe target word.
These positions include traditionalunordered bag-of-word context, local bigram andtrigram collocations and several syntactic relation-ships based on predicate-argument structure.
Theiruse is illustrated on a sample English sentence forthe target word church in Figure 1.
While an exten-sive evaluation of feature type to WSD performanceis beyond the scope of this paper, Section 6 sketchesan analysis of the individual feature contribution toeach of the classifier types.3.1 Part-of-Speech Tagging andLemmatizationPart-of-speech tagger availability varied across thelanguages that are studied here.
An electronicallyavailable transformation-based POS tagger (Ngaiand Florian, 2001) was trained on standard labeleddata for English (Penn Treebank), Swedish (SUC-1 corpus), and Basque.
For Spanish, an minimallysupervised tagger (Cucerzan and Yarowsky, 2000)was used.
Lemmatization was performed using anexisting trie-based supervised models for English,and a combination of supervised and unsupervisedmethods (Yarowsky and Wicentowski, 2000) for allthe other languages.3.2 Syntactic FeaturesThe syntactic features extracted for a target worddepend on the word?s part of speech: verbs: the head noun of the verb?s object, par-ticle/preposition and prepositional object; nouns: the headword of any verb-object,subject-verb or noun-noun relationships iden-tified for the target word; adjectives: the head noun modified by the ad-jective.The extraction process was performed using heuris-tic patterns and regular expressions over the parts-of-speech surrounding the target word1.4 Classifier Models for Word SenseDisambiguationThis section briefly introduces the 6 classifier mod-els used in this study.
Among these models, theNa?ve Bayes variants (NB henceforth) (Pedersen,1998; Manning and Sch?tze, 1999) and Cosine dif-fer slightly from off-the-shelf versions, and only thedifferences will be described.4.1 Vector-based Models: Enhanced Na?veBayes and Cosine ModelsMany of the systems used in this research sharea common vector representation, which capturestraditional bag-of-words, extended ngram andpredicate-argument features in a single data struc-ture.
In these models, a vector is created for eachdocument in the collection:     , where is the number of times the featureappears in document ,  is the number of wordsin  and is a weight associated with the feature2.
Confusion between the same word participat-ing in multiple feature roles is avoided by append-ing the feature values with their positional type (e.g.stands_Sbj, ancient_L are distinct from stands andancient in unmarked bag-of-words context).The notable difference between the extendedmodels and others described in the literature, asidefrom the use of more sophisticated features thanthe traditional bag-of-words, is the variable weight-ing of feature types noted above.
These differencesyield a boost in the NB performance (relative to ba-sic Na?ve Bayes) of between 3.5% (Basque) and10% (Spanish), with an average improvement of7.25% over the four languages.4.2 The BayesRatio ModelThe BayesRatio model (BR henceforth) is a vector-based model using the likelihood ratio frameworkdescribed in Gale et al (1992):1The feature extraction on the in English data was per-formed by first identifying text chunks, and then using heuris-tics on the chunks to extract the syntactic information.2The weight depends on the type of the feature : forthe bag-of-word features, this weight is inversely proportionalto the distance between the target word and the feature, whilefor predicate-argument and extended ngram features it is a em-pirically estimated weight (on a per language basis).           where  is the selected sense,  denotes documentsand  denotes features.
By utilizing the binary ra-tio for k-way modeling of feature probabilities, thisapproach performs well on tasks where the data issparse.4.3 The MMVC ModelThe Mixture Maximum Variance Correction classi-fier (MMVC henceforth) (Cucerzan and Yarowsky,2002) is a two step classifier.
First, the sense proba-bility is computed as a linear mixture         where the probability  	 is estimated fromdata and  	 is computed as a weighted normal-ized similarity between the word 	 and the targetword(also taking into account the distance in thedocument between 	 and).
In a second pass, thesense whose variance exceeds a theoretically moti-vated threshold is selected as the final sense label(for details, see Cucerzan and Yarowsky (2002)).4.4 The Discriminative ModelsTwo discriminative models are used in the exper-iments presented in Section 5 - a transformation-based learning system (TBL henceforth) (Brill,1995; Ngai and Florian, 2001) and a non-hierarchical decision lists system (DL henceforth)(Yarowsky, 1996).
For prediction, these systemsutilize local n-grams around the target word (up to3 words/lemma/POS to the left/right), bag-of-wordsand lemma/collocation (20 words around the tar-get word, grouped by different window sizes) andthe syntactic features listed in Section 3.2.The TBL system was modified to include redun-dant rules that do not improve absolute accuracy ontraining data in the traditional greedy training al-gorithm, but are nonetheless positively correlatedwith a particular sense.
The benefit of this approachis that predictive but redundant features in trainingcontext may appear by themselves in new test con-texts, improving coverage and increasing TBL basemodel performance by 1-2%.5 Models for Classifier CombinationOne necessary property for success in combiningclassifiers is that the errors produced by the com-ponent classifiers should not be positively corre-lated.
On one extreme, if the classifier outputs are0.0 0.2 0.4 0.6 0.8 1.0MMVCCosineBayesBayesRatioTBLDecisionListsFigure 2: Empirically-derived classifier similaritystrongly correlated, they will have a very high inter-agreement rate and there is little to be gained fromthe joint output.
On the other extreme, Perrone andCooper (1993) show that, if the errors made by theclassifiers are uncorrelated and unbiased, then byconsidering a classifier that selects the class thatmaximizes the posterior class probability average      (1)the error is reduced by a factor of .
This caseis mostly of theoretical interest, since in practiceall the classifiers will tend to make errors on the?harder?
samples.Figure 3(a) shows the classifier inter-agreementamong the six classifiers presented in Section 4, onthe English data.
Only two of them, BayesRatio andcosine, have an agreement rate of over 80%3, whilethe agreement rate can be as low as 63% (BayesRa-tio and TBL).
The average agreement is 71.7%.
Thefact that the classifiers?
output are not strongly cor-related suggests that the differences in performanceamong them can be systematically exploited to im-prove the overall classification.
All individual clas-sifiers have high stand-alone performance; each isindividually competitive with the best single SEN-SEVAL2 systems and are fortuitously diverse in rel-ative performance, as shown in Table 3(b).
A den-dogram of the similarity between the classifiers isshown in Figure 2, derived using maximum linkagehierarchical agglomerative clustering.5.1 Major Types of Classifier CombinationThere are three major types of classifier combina-tion (Xu et al, 1992).
The most general type is thecase where the classifiers output a posterior classprobability distribution for each sample (which canbe interpolated).
In the second case, systems onlyoutput a set of labels, together with a ordering ofpreference (likelihood).
In the third and most re-strictive case, the classifications consist of just a sin-gle label, without rank or probability.
Combiningclassifiers in each one of these cases has differentproperties; the remainder of this section examinesmodels appropriate to each situation.3The performance is measured using 5-fold cross validationon training data.0.50.550.60.650.70.750.80.85CosineBayesTBLDLBayesRatioClassifier Aggreement (%ofdata)Bayes Cosine BayesRatio DL TBL MMVCMMVC(a) Classifier inter-agreement on SENSEVAL2English dataSystem SENSEVAL1 SENSEVAL2EN EN ES EU SVBaseline 63.2 48.3 45.9 62.7 46.2NB 80.4 65.7 67.9 71.2 66.7BR 79.8 65.3 69.0 69.6 68.0Cosine 74.0 62.2 65.9 66.0 66.4DL 79.9 63.2 65.1 70.7 61.5TBL 80.7 64.4 64.7 69.4 62.7MMVC 81.1 66.7 66.7 69.7 61.9(b) Individual classifier performance; best performers areshown in boldFigure 3: Individual Classifier Properties (cross-validation on SENSEVAL training data)5.2 Combining the Posterior Sense ProbabilityDistributionsOne of the simplest ways to combine the poste-rior probability distributions is via direct averaging(Equation (1)).
Surprisingly, this method obtainsreasonably good results, despite its simplicity andthe fact that is not theoretically motivated under aBayes framework.
Its success is highly dependenton the condition that the classifiers?
errors are un-correlated (Tumer and Gosh, 1995).The averaging method is a particular case ofweighted mixture:4 	   	   	  	   	  (2)where  is the weight assigned to the clas-sifier  in the mixture and  is the poste-rior probability distribution output by classifier ;for  we obtain Equation (1).The mixture interpolation coefficients can becomputed at different levels of granularity.
Forinstance, one can make the assumption that      and then the coefficients willbe computed at word level; if      then the coefficients will be estimated on the entiredata.One way to estimate these parameters is by linearregression (Fuhr, 1989): estimate the coefficientsthat minimize the mean square error (MSE)	  	    	 (3)where    is the target vector of the cor-rect classification of wordin document d:4Note that we are computing a probability conditioned bothon the target word  and the document , because the docu-ments are associated with a particular target word ; this for-malization works mainly for the lexical choice task.     ?
  , being the goldstan-dard sense ofin  and ?
the Kronecker function:?
   if  if As shown in Fuhr (1989), Perrone and Cooper(1993), the solution to the optimization problem (3)can be obtained by solving a linear set of equations.The resulting classifier will have a lower square er-ror than the average classifier (since the averageclassifier is a particular case of weighted mixture).Another common method to compute the  pa-rameters is by using the Expectation-Maximization(EM) algorithm (Dempster et al, 1977).
Onecan estimate the coefficients such as to max-imize the log-likelihood of the data, 	 .
In this particular opti-mization problem, the search space is convex, andtherefore a solution exists and is unique, and it canbe obtained by the usual EM algorithm (see Berger(1996) for a detailed description).An alternative method for estimating the parame-tersis to approximate them with the performanceof the th classifier (a performance-based combiner)(van Halteren et al, 1998; Sang et al, 2000)    _is_correct  (4)therefore giving more weight to classifiers that havea smaller classification error (the method will be re-ferred to as PB).
The probabilities in Equation (4)are estimated directly from data, using the maxi-mum likelihood principle.5.3 Combination based on Order StatisticsIn cases where there are reasons to believe that theposterior probability distribution output by a clas-sifier is poorly estimated5, but that the relative or-dering of senses matches the truth, a combination5For instance, in sparse classification spaces, the Na?veBayes classifier will assign a probability very close to 1 to themost likely sense, and close to 0 for the other ones.strategy based on the relative ranking of sense pos-terior probabilities is more appropriate.
The senseposterior probability can be computed as          (5)where the rank of a sense  is inversely proportionalto the number of senses that are (strictly) more prob-able than sense : 	   This method will tend to prefer senses that appearcloser to the top of the likelihood list for most of theclassifiers, therefore being more robust both in caseswhere one classifier makes a large error and in caseswhere some classifiers consistently overestimate theposterior sense probability of the most likely sense.5.4 The Classifier Republic: VotingSome classification methods frequently used inNLP directly minimize the classification error anddo not usually provide a probability distributionover classes/senses (e.g.
TBL and decision lists).There are also situations where the user does nothave access to the probability distribution, such aswhen the available classifier is a black-box that onlyoutputs the best classification.
A very commontechnique for combination in such a case is by vot-ing (Brill and Wu, 1998; van Halteren et al, 1998;Sang et al, 2000).
In the simplest model, each clas-sifier votes for its classification and the sense thatreceives the most number of votes wins.
The behav-ior is identical to selecting the sense with the highestposterior probability, computed as      ?
     ?
   (6)where ?
is the Kronecker function and   isthe classification of the th classifier.
Theco-efficients can be either equal (in a perfect classifierdemocracy), or they can be estimated with any ofthe techniques presented in Section 5.2.
Section6 presents an empirical evaluation of these tech-niques.Van Halteren et al (1998) introduce a modifiedversion of voting called TagPair.
Under this model,the conditional probability that the word sense is given that classifier  outputs and classifier  out-puts ,         , is com-puted on development data, and the posterior prob-ability is estimated as  	 ?
  ?
 	 (7)where 	    	   	 .Each classifier votes for its classification and everypair of classifiers votes for the sense that is mostlikely given the joint classification.
In the experi-ments presented in van Halteren et al (1998), thismethod was the best performer among the presentedmethods.
Van Halteren et al (2001) extend thismethod to arbitrarily long conditioning sequences,obtaining the best published POS tagging results onfour corpora.6 Empirical EvaluationTo empirically test the combination methods pre-sented in the previous section, we ran experimentson the SENSEVAL1 English data and data from fourSENSEVAL2 lexical sample tasks: English(EN),Spanish(ES), Basque(EU) and Swedish(SV).
Un-less explicitly stated otherwise, all the results in thefollowing section were obtained by performing 5-fold cross-validation6 .
To avoid the potential forover-optimization, a single final evaluation systemwas run once on the otherwise untouched test data,as presented in Section 6.3.The data consists of contexts associated with aspecific word to be sense tagged (target word); thecontext size varies from 1 sentence (Spanish) to5 sentences (English, Swedish).
Table 1 presentssome statistics collected on the training data for thefive data sets.
Some of the tasks are quite challeng-ing (e.g.
SENSEVAL2 English task) ?
as illustratedby the mean participating systems?
accuracies in Ta-ble 5.Outlining the claim that feature selection is im-portant for WSD, Table 2 presents the marginal lossin performance of either only using one of the po-sitional feature classes or excluding one of the po-sitional feature classes relative to the algorithm?sfull performance using all available feature classes.It is interesting to note that the feature-attractivemethods (NB,BR,Cosine) depend heavily on theBagOfWords features, while discriminative methodsare most dependent on LocalContext features.
Foran extensive evaluation of factors influencing theWSD performance (including representational fea-tures), we refer the readers to Yarowsky and Florian(2002).6.1 Combination PerformanceTable 3 shows the fine-grained sense accuracy (per-cent of exact correct senses) results of running the6When parameters needed to be estimated, a 3-1-1 split wasused: the systems were trained on three parts, parameters esti-mated on the fourth (in a round-robin fashion) and performancetested on the fifth; special care was taken such that no ?test?data was used in training classifiers or parameter estimation.SE1 SENSEVAL2EN EN ES EU SV#words 42 73 39 40 40#samples 12479 8611 4480 3444 8716avg #senses/word 11.3 10.7 4.9 4.8 11.1avg #samples/sense 26.21 9.96 23.4 17.9 19.5Table 1: Training set characteristicsPerformance drop relative to full system (%)NB Cosine BR TBL DLBoW Ftrs Only -6.4 -4.8 -4.8 -6.0 -3.2Local Ftrs Only -18.4 -11.5 -6.1 -1.5 -3.3Syntactic Ftrs Only -28.1 -14.9 -5.4 -5.4 -4.8No BoW Ftrs -14.7 -8.1 -5.3 -0.5 -2.0No Local Ftrs -3.5 -0.8 -2.2 -2.9 -4.5No Syntactic Ftrs -1.1 -0.8 -1.3 -1.0 -2.3Table 2: Individual feature type contribution to perfor-mance.
Fields marked with  indicate that the differencein performance was not statistically significant at a level (paired McNemar test).classifier combination methods for 5 classifiers, NB(Na?ve Bayes), BR (BayesRatio), TBL, DL andMMVC, including the average classifier accuracyand the best classification accuracy.
Before examin-ing the results, it is worth mentioning that the meth-ods which estimate parameters are doing so on asmaller training size (3/5, to be precise), and thiscan have an effect on how well the parameters areestimated.
After the parameters are estimated, how-ever, the interpolation is done between probabilitydistributions that are computed on 4/5 of the train-ing data, similarly to the methods that do not esti-mate any parameters.The unweighted averaging model of probabilityinterpolation (Equation (1)) performs well, obtain-ing over 1% mean absolute performance over thebest classifier7, the difference in performance isstatistically significant in all cases except Swedishand Spanish.
Of the classifier combination tech-niques, rank-based combination and performance-based voting perform best.
Their mean 2% absoluteimprovement over the single best classifier is signif-icant in all languages.
Also, their accuracy improve-ment relative to uniform-weight probability interpo-lation is statistically significant in aggregate and forall languages except Basque (where there is gener-ally a small difference among all classifiers).To ensure that we benefit from the performanceimprovement of each of the stronger combinationmethods and also to increase robustness, a final av-eraging method is applied to the output of the bestperforming combiners (creating a stacked classi-fier).
The last line in Table 3 shows the results ob-tained by averaging the rank-based, EM-vote and7The best individual classifier differs with language, asshown in Figure 3(b).SE1 SENSEVAL2Method EN EN ES EU SVIndividual ClassifiersMean Acc 79.5 65.0 66.6 70.4 65.9Best Acc 81.1 66.7 68.8 71.2 68.0Probability InterpolationAveraging 82.7 68.0 69.3 72.2 68.16MSE 82.8 68.1 69.7 71.0 69.2EM 82.7 68.4 69.6 72.1 69.1PB 82.8 68.0 69.4 72.2 68.7Rank-based Combinationrank 83.1 68.6 71.0 72.1 70.3Count-based Combination (Voting)Simple Vote 82.8 68.1 70.9 72.1 70.0TagPair 82.9 68.3 70.9 72.1 70.0EM 83.0 68.4 70.5 71.7 70.0PB 83.1 68.5 70.8 72.0 70.3Stacking (Meta-Combination)Prob.
Interp.
83.2 68.6 71.0 72.3 70.4Table 3: Classifier combination accuracy over 5 baseclassifiers: NB, BR, TBL, DL, MMVC.
Best perform-ing methods are shown in bold.Estimation Level word POS ALL InterpAccuracy 68.1 68.2 68.0 68.4CrossEntropy 1.623 1.635 1.646 1.632Table 4: Accuracy for different EM-weighted probabilityinterpolation models for SENSEVAL2PB-vote methods?
output.
The difference in perfor-mance between the stacked classifier and the bestclassifier is statistically significant for all data setsat a significance level of at least , as measuredby a paired McNemar test.One interesting observation is that for all meth-ods of -parameter estimation (EM, PB and uniformweighting) the count-based and rank-based strate-gies that ignore relative probability magnitudes out-perform their equivalent combination models usingprobability interpolation.
This is especially the casewhen the base classifier scores have substantiallydifferent ranges or variances; using relative rankseffectively normalizes for such differences in modelbehavior.For the three methods that estimate the interpo-lation weights ?
MSE, EM and PB ?
three vari-ants were investigated.
These were distinguished bythe granularity at which the weights are estimated:at word level (  ), at POS level(   ) and over the entire train-ing set (  ).
Table 4 displays the resultsobtained by estimating the parameters using EM atdifferent sample granularities for the SENSEVAL2English data.
The number in the last column is ob-tained by interpolating the first three systems.
Alsodisplayed is cross-entropy, a measure of how well?1.2?1?0.8?0.6?
?0.40 200.20.40.6 English Spanish Swedish BasqueBayes BayesRatio Cosine DL TBL MMVC  Senseval2 datasetDifferenceinAccuracyvs6?wayCombination(a) Performance drop when eliminating one classifier(marginal performance contribution)?3.5?3?2.5?2?1.5?1?0.500.51Bayes BayesRatioCosineDLTBLMMVCPercent of available training data10 20 40 50 80Differenceinclassificationaccuracy(%)(b) Performance drop when eliminating one classifer,versus training data sizeFigure 4: Individual basic classifiers?
contribution to the final classifier combination performance.the combination classifier estimates the sense prob-abilities,       .6.2 Individual Systems Contribution toCombinationAn interesting issue pertaining to classifier combi-nation is what is the marginal contribution to finalcombined performance of the individual classifier.A suitable measure of this contribution is the dif-ference in performance between a combination sys-tem?s behavior with and without the particular clas-sifier.
The more negative the accuracy difference onomission, the more valuable the classifier is to theensemble system.Figure 4(a) displays the drop in performance ob-tained by eliminating in turn each classifier from the6-way combination, across four languages, whileFigure 4(b) shows the contribution of each classifieron the SENSEVAL2 English data for different train-ing sizes (10%-80%)8.
Note that the classifiers withthe greatest marginal contribution to the combinedsystem performance are not always the best singleperforming classifiers (Table 3(b)), but those withthe most effective original exploitation of the com-mon feature space.
On average, the classifier thatcontributes the most to the combined system?s per-formance is the TBL classifier, with an average im-provement of across the 4 languages.
Also,note that TBL and DL offer the greatest marginalcontribution on smaller training sizes (Figure 4(b)).6.3 Performance on Test DataAt all points in this article, experiments have beenbased strictly on the original SENSEVAL1 and SEN-SEVAL2 training sets via cross-validation.
The of-ficial SENSEVAL1 and SENSEVAL2 test sets were8The latter graph is obtained by sampling repeatedly aprespecified ratio of training samples from 3 of the 5 cross-validation splits, and testing on the other 2.unused and unexamined during experimentation toavoid any possibility of indirect optimization on thisdata.
But to provide results more readily compara-ble to the official benchmarks, a single consensussystem was created for each language using linearaverage stacking on the top three classifier combi-nation methods in Table 3 for conservative robust-ness.
The final frozen consensus system for eachlanguage was applied once to the SENSEVAL testsets.
The fine-grained results are shown in Table5.
For each language, the single new stacked com-bination system outperforms the best previously re-ported SENSEVAL results on the identical test data9.As far as we know, they represent the best publishedresults for any of these five SENSEVAL tasks.7 ConclusionIn conclusion, we have presented a comparativeevaluation study of combining six structurally andprocedurally different classifiers utilizing a richcommon feature space.
Various classifier combi-nation methods, including count-based, rank-basedand probability-based combinations are describedand evaluated.
The experiments encompass super-vised lexical sample tasks in four diverse languages:English, Spanish, Swedish, and Basque.9To evaluate systems on the full disambiguation task, it isappropriate to compare them on their accuracy at 100% test-data coverage, which is equivalent to system recall in the offi-cial SENSEVAL scores.
However, it can also be useful to con-sider performance on only the subset of data for which a sys-tem is confident enough to answer, measured by the secondarymeasure precision.
One useful byproduct of the CBV methodis the confidence it assigns to each sample, which we measuredby the number of classifiers that voted for the sample.
If onerestricts system output to only those test instances where allparticipating classifiers agree, consensus system performanceis 83.4% precision at a recall of 43%, for an F-measure of 56.7on the SENSEVAL2 English lexical sample task.
This outper-forms the two supervised SENSEVAL2 systems that only hadpartial coverage, which exhibited 82.9% precision at a recall of28% (F=41.9) and 66.5% precision at 34.4% recall (F=47.9).SENSEVAL1 SENSEVAL2 Sense Classification AccuracyEnglish English Spanish Swedish BasqueMean Official SENSEVAL Systems Accuracy 73.12.9 55.75.3 59.65.0 58.46.6 74.41.8Best Previously Published SENSEVAL Accuracy 77.1% 64.2% 71.2% 70.1% 75.7%Best Individual Classifier Accuracy 77.1% 62.5% 69.6% 68.6% 75.6%New (Stacking) Accuracy 79.7% 66.5% 72.4% 71.9% 76.7%Table 5: Final Performance (Frozen Systems) on SENSEVAL Lexical Sample WSD Test DataThe experiments show substantial variation insingle classifier performance across different lan-guages and data sizes.
They also show that thisvariation can be successfully exploited by 10 differ-ent classifier combination methods (and their meta-voting consensus), each of which outperforms boththe single best classifier system and standard classi-fier combination models on each of the 4 focus lan-guages.
Furthermore, when the stacking consensussystems were frozen and applied once to the other-wise untouched test sets, they substantially outper-formed all previously known SENSEVAL1 and SEN-SEVAL2 results on 4 languages, obtaining the bestpublished results on these data sets.8 AcknowledgementsThe authors would like to thank Noah Smith for hiscomments on an earlier version of this paper, andthe anonymous reviewers for their useful comments.This work was supported by NSF grant IIS-9985033and ONR/MURI contract N00014-01-1-0685.ReferencesA.
Berger.
1996.
Convexity, maximum likelihoodand all that.
http://www.cs.cmu.edu/afs/cs/user/aberger/www/ps/convex.ps.E.
Brill and J. Wu.
1998.
Classifier combination for improvedlexical disambiguation.
In Proceedings of COLING-ACL?98,pages 191?195.E.
Brill.
1995.
Transformation-based error-driven learning andnatural language processing: A case study in part of speechtagging.
Computational Linguistics, 21(4):543?565.S.
Cucerzan and D. Yarowsky.
2000.
Language independentminimally supervised induction of lexical probabilities.
InProceedings of ACL-2000, pages 270?277.S.
Cucerzan and D. Yarowsky.
2002.
Augmented mixture modelsfor lexical disambiguation.
In Proceedings of EMNLP-2002.W.
Daelemans, A. van den Bosch, and J. Zavrel.
1999.
Timbl:Tilburg memory based learner - version 1.0.
Technical Reportilk9803, Tilburg University, The Netherlands.A.P.
Dempster, N.M. Laird, , and D.B.
Rubin.
1977.
Maximumlikelihood from incomplete data via the EM algorithm.
Jour-nal of the Royal statistical Society, 39(1):1?38.P.
Edmonds and S. Cotton.
2001.
SENSEVAL-2: Overview.
InProceedings of SENSEVAL-2, pages 1?6.N.
Fuhr.
1989.
Optimum polynomial retrieval funcions basedon the probability ranking principle.
ACM Transactions onInformation Systems, 7(3):183?204.W.
Gale, K. Church, and D. Yarowsky.
1992.
A method fordisambiguating word senses in a large corpus.
Computers andthe Humanities, 26:415?439.J.
Henderson and E. Brill.
1999.
Exploiting diversity in naturallanguage processing: Combining parsers.
In Proceedings onEMNLP99, pages 187?194.A.
Kilgarriff and M. Palmer.
2000.
Introduction to the specialissue on senseval.
Computer and the Humanities, 34(1):1-13.A.
Kilgarriff and J. Rosenzweig.
2000.
Framework and re-sults for English Senseval.
Computers and the Humanities,34(1):15-48.C.D.
Manning and H. Sch?tze.
1999.
Foundations of StatisticalNatural Language Processing.
MIT Press.G.
Ngai and R. Florian.
2001.
Transformation-based learning inthe fast lane.
In Proceedings of NAACL?01, pages 40?47.T.
Pedersen.
1998.
Na?ve Bayes as a satisficing model.
In Work-ing Notes of the AAAI Symposium on Satisficing Models.T.
Pedersen.
2000.
A simple approach to building ensembles ofnaive bayesian classifiers for word sense disambiguation.
InProceedings of NAACL?00, pages 63?69.M.
P. Perrone and L. N. Cooper.
1993.
When networks disagree:Ensemble methods for hybrid neural networks.
In R. J. Mam-mone, editor, Neural Networks for Speech and Image Process-ing, pages 126?142.
Chapman-Hall.E.
F. Tjong Kim Sang, W. Daelemans, H. Dejean, R. Koeling,Y.
Krymolowsky, V. Punyakanok, and D. Roth.
2000.
Apply-ing system combination to base noun phrase identification.
InProceedings of COLING 2000, pages 857?863.M.
Stevenson and Y. Wilks.
2001.
The interaction of knowl-edge sources in word sense disambiguation.
ComputationalLinguistics, 27(3):321?349.K.
Tumer and J. Gosh.
1995.
Theoretical foundations of linearand order statistics combiners for neural pattern classifiers.Technical Report TR-95-02-98, University of Texas, Austin.H.
van Halteren, J. Zavrel, and W. Daelemans.
1998.
Improv-ing data driven wordclass tagging by system combination.
InProceedings of COLING-ACL?98, pages 491?497.H.
van Halteren, J. Zavrel, and W. Daelemans.
2001.
Im-proving accuracy in word class tagging through the combina-tion fo machine learning systems.
Computational Linguistics,27(2):199?230.L.
Xu, A. Krzyzak, and C. Suen.
1992.
Methods of com-bining multiple classifires and their applications to handwrit-ing recognition.
IEEE Trans.
on Systems, Man.
Cybernet,22(3):418?435.D.
Yarowsky and R. Florian.
2002.
Evaluating sense disambigua-tion performance across diverse parameter spaces.
To appearin Journal of Natural Language Engineering.D.
Yarowsky and R. Wicentowski.
2000.
Minimally supervisedmorphological analysis by multimodal alignment.
In Pro-ceedings of ACL-2000, pages 207?216.D.
Yarowsky.
1996.
Homograph disambiguation in speechsynthesis.
In J. Olive J. van Santen, R. Sproat andJ.
Hirschberg, editors, Progress in Speech Synthesis, pages159?175.
Springer-Verlag.
