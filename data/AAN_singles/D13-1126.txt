Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1270?1280,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsSimulating Early-Termination Search for Verbose Spoken QueriesJerome WhiteIBM ResearchBangalore, KA Indiajerome.white@in.ibm.comDouglas W. OardUniversity of MarylandCollege Park, MD USAoard@umd.eduNitendra RajputIBM ResearchNew Delhi, Indiarnitendra@in.ibm.comMarion ZalkUniversity of MelbourneMelbourne, VIC Australiam.zalk@student.unimelb.edu.auAbstractBuilding search engines that can respond tospoken queries with spoken content requiresthat the system not just be able to find usefulresponses, but also that it know when it hasheard enough about what the user wants to beable to do so.
This paper describes a simula-tion study with queries spoken by non-nativespeakers that suggests that indicates that find-ing relevant content is often possible withina half minute, and that combining featuresbased on automatically recognized words withfeatures designed for automated prediction ofquery difficulty can serve as a useful basis forpredicting when that useful content has beenfound.1 IntroductionMuch of the early work on what has come to becalled ?speech retrieval?
has focused on the use oftext queries to rank segments that are automaticallyextracted from spoken content.
While such an ap-proach can be useful in a desktop environment, halfof the world?s Internet users can access the globalinformation network only using a voice-only mobilephone.
This raises two challenges: 1) in such set-tings, both the query and the content must be spo-ken, and 2) the language being spoken will often beone for which we lack accurate speech recognition.The Web has taught us that the ?ten blue links?paradigm can be a useful response to short queries.That works because typed queries are often fairlyprecise, and tabular responses are easily skimmed.However, spoken queries, and in particular open-domain spoken queries for unrestricted spoken con-tent, pose new challenges that call for new thinkingabout interaction design.
This paper explores the po-tential of a recently proposed alternative, in whichthe spoken queries are long, and only one responsecan be played at a time by the system.
This ap-proach, which has been called Query by Babbling,requires that the user ramble on about what theyare looking for, that the system be able to estimatewhen it has found a good response, and that the userbe able to continue the search interaction by bab-bling on if the first response does not fully meet theirneeds (Oard, 2012).One might question whether users actually will?babble?
for extended periods about their informa-tion need.
There are two reasons to believe thatsome users might.
First, we are particularly inter-ested in ultimately serving users who search for in-formation in languages for which we do not have us-able speech recognition systems.
Speech-to-speechmatching in such cases will be challenging, and wewould not expect short queries to work well.
Sec-ond, we seek to principally serve users who will benew to search, and thus not yet conditioned to issueshort queries.
As with Web searchers, we can ex-pect them to explore initially, then to ultimately set-tle on query strategies that work well enough to meettheir needs.
If longer queries work better for them,it seems reasonable to expect that they would uselonger queries.
Likewise, if systems cannot effec-tively use longer queries to produce useful results,then people will not use them.To get a sense for whether such an interactionmodality is feasible, we performed a simulation1270study for this paper in which we asked people tobabble on some topic for which we already have rel-evance judgments results.
We transcribe those bab-bles using automatic speech recognition (ASR), thennote how many words must be babbled in each casebefore an information retrieval system is first able toplace a relevant document in rank one.
From thisperspective, our results show that people are indeedoften able to babble usefully; and, moreover, thatcurrent information retrieval technology could of-ten place relevant results at rank one within half aminute or so of babbling even with contemporaryspeech recognition technology.The question then arises as to whether a systemcan be built that would recognize when an answeris available at rank one.
Barging in with an answerbefore that point wastes time and disrupts the user;barging in long after that point also wastes time, butalso risks user abandonment.
We therefore want a?Goldilocks?
system that can get it just about right.To this end, we introduce an evaluation measure thatdifferentially penalizes early and late responses.
Ourexperiments using such a measure show that systemscan be built that, on average, do better than could beachieved by any fixed response delay.The remainder of this paper is organized as fol-lows: We begin in Section 2 with a brief review ofrelated work.
Section 3 then describes the designof the ranking component of our experiment; Sec-tion 4 follows with some exploratory analysis of theranking results using our test collection.
Section 6completes the description of our methods with anexplanation of how the stopping classifier is built;Section 7 then presents end-to-end evaluation resultsusing a new measure designed for this task.
Sec-tion 8 concludes the paper with some remarks onfuture work.2 BackgroundThe rapid adoption of remarkably inexpensive mo-bile telephone services among low-literacy usersin developing and emerging markets has generatedconsiderable interest in so-called ?spoken forum?projects (Sherwani et al 2009; Agarwal et al 2010;Medhi et al 2011; Mudliar et al 2012).
It is rel-atively straightforward to collect and store spokencontent regardless of the language in which it is spo-ken; organizing and searching that content is, how-ever, anything but straightforward.
Indeed, the cur-rent lack of effective search services is one of thekey inhibitors that has, to date, limited spoken fo-rums to experimental settings with at most a fewhundred users.
If a ?spoken web?
is to achieve thesame degree of impact on the lives of low-literacyusers in the developing world that the World WideWeb has achieved over the past decade in the devel-oped world, we will need to develop the same keyenabler: an effective search engine.At present, spoken dialog systems of conventionaldesign, such as Siri, rely on complex and expen-sive language-specific engineering, which can eas-ily be justified for the ?languages of wealth?
suchas English, German, and Chinese; but perhaps notfor many of the almost 400 languages that are eachspoken by a million or more people.1 An alterna-tive would be to adopt more of an ?information re-trieval?
perspective by directly matching words spo-ken in the query with words that had been spoken inthe content to be searched.
Some progress has beenmade on this task in the MediaEval benchmark eval-uation, which has included a spoken content match-ing task each year since 2011 (Metze et al 2012).Results for six low-resource Indian and African lan-guages indicate that miss rates of about 0.5 can beachieved on individual terms, with false alarm ratesbelow 0.01, by tuning acoustic components that hadoriginally been developed for languages with rea-sonably similar phonetic inventories.
Our goal inthis paper is to begin to explore how such capabil-ities might be employed in a complete search en-gine for spoken forum content, as will be evaluatedfor the first time at MediaEval 2013.2 The princi-pal impediment to development in this first year ofthat evaluation is the need for relevance judgments,which are not currently available for spoken contentof the type we wish to search.
That considerationhas motivated our design of the simulation study re-ported in this paper.1http://www.ethnologue.com/statistics/size2http://www.multimediaeval.org/mediaeval2013/qa4sw2013/127100.20.40.60.810  20  40  60  80  100ReciprocalRankBabble position (words)Topic 274Babble 1Babble 2Babble 3Figure 1: Reciprocal ranks at for each query making up a given babble.
When retrieving results, a babbler either?latches?
on to a relevant document (Babble 1), moves back-and-forth between relevant documents (Babble 3), or failsto elicit a relevant document at all (Babble 2).3 Setup and MethodThe approach taken in this paper is to simulate, asclosely as possible, babbling about topics for whichwe a) already have relevance judgments available,and b) have the ability to match partial babbles withpotential answers in ways that reflect the errors in-troduced by speech processing.
To this end, wechose to ask non-native English speakers to babble,in English, about an information need that is stimu-lated by an existing English Text Retrieval Confer-ence (TREC) topic for which we already have rel-evance judgments.
An English Automatic SpeechRecognition (ASR) system was then used to gener-ate recognized words for those babbles.
Those rec-ognized words, in turn, have been used to rank orderthe (character-coded written text) news documentsthat were originally used in TREC, the documentsfor which we have relevance judgments.
Our goalthen becomes twofold: to first rank the documentsin such a way as to get a relevant document into rankone; and then to recognize when we have done so.Figure 1 is a visual representation of retrieval re-sults as a person babbles.
For three different bab-bles prompted by TREC Topic 274, it shows the re-ciprocal rank for the query that is posed after eachadditional word is recognized.
We are primarily in-terested in cases where the reciprocal rank is one.33A reciprocal rank of one indicates that a known relevantdocument is in position one; a reciprocal rank of 0.5 indicatesIn these three babbles we see all cases that the re-trieval system must take into account: babbles thatnever yield a relevant first-ranked document (Bab-ble 2); babbles that eventually yield a relevant first-rank document, and that continue to do so as theperson speaks (Babble 1); and babbles that alternatebetween good and bad results as the speaker contin-ues (Babble 3).3.1 Acquiring BabblesTen TREC-5 Ad Hoc topics were selected for thisstudy: 255, 257, 258, 260, 266, 271, 274, 276, 287,and 297 based on our expectation of which of the 50TREC 5 topics would be most suitable for promptedbabbles.
In making this choice, we avoided TRECtopics that we felt would require specialized do-main knowledge, experience with a particular cul-ture, or detailed knowledge of an earlier time period,such as when the topics had been crafted.
For eachtopic, three babbles were created by people speak-ing at length about the same information need thatthe TREC topic reflected.
For convenience, the peo-ple who created the babbles were second-languagespeakers of English selected from information tech-nology companies.
There were a total of ten bab-blers; each recorded, in English, babbles for threetopics, yielding a total of thirty babbles.
We main-tained a balance across topics when assigning topicthat the most highly ranked known relevant document is in po-sition two; 0.33 indicates position three; and so on.1272Transcribed babble Text from ASRSo long time back one of my friend had a ToyotaPryus it uses electric and petrol to increase the toreduce the consumption and increase the mileageI would now want to get information about whycar operators manufacturers or what do they thinkabout electric vehicles in the US well this is whatthe stories say that the car lobby made sure that theelectric vehicles do not get enough support and thetaxes are high by the government but has it changednow are there new technologies that enable to lowercost and also can increase speed for electric vehi-cles I am sure something is being done because ofthe rising prices of fuel these daysSo long time at one of my friends headed towardsthe previous accuses electric in petrol to increasethe to reduce the consumption and increase theminutes and would now want to get informationabout why car operator manufacturers on what tothink about electric vehicles in the us versus whatthe story said that the car lobby make sure that theelectric vehicles to not get enough support to anattack and I try to comment but has changed nowarctic new technologies that enabled to cover costsand also can increase speak for electric vehicles I?msure some clinton gore carls junior chefTable 1: Text from an example babble (274-1).
The left is transcribed through human comprehension; the right is theoutput from an automatic speech recognition engine.numbers to babblers.
All babblers had more thansixteen years of formal education, had a strong com-mand on the English language, and had some in-formation about the topics that they selected.
Theywere all briefed about our motivation for collectingthis data, and about the concept of query by bab-bling.The babbles were created using a phone interface.Each subject was asked to call an interactive voiceresponse (IVR) system.
The system prompted theuser for a three digit topic ID.
After obtaining thetopic ID, the system then prompted the user to startspeaking about what they were looking for.
TRECtopics contain a short title, a description, and a nar-rative.
The title is generally something a user mightpost as an initial Web query; the description is some-thing one person might say to another person whomight then help them search; the narrative is a fewsentences meant to reflect what the user might jotdown as notes to themselves on what they were actu-ally looking for.
For easy reference, the system pro-vided a short description?derived from the descrip-tion and narrative of the TREC topics?that gavethe user the context around which to speak.
Theuser was expected to begin speaking after hearinga system-generated cue, at which time their speechwas recorded.
Two text files were produced from theaudio babbles: one produced via manual transcrip-TREC Topic WERID Title Mean SD255 Environmental protect.
0.434 0.203257 Cigarette consumption 0.623 0.281258 Computer security 0.549 0.289260 Evidence of human life 0.391 0.051266 Prof. scuba diving 0.576 0.117271 Solar power 0.566 0.094274 Electric automobiles 0.438 0.280276 School unif./dress code 0.671 0.094287 Electronic surveillance 0.519 0.246297 Right to die pros/cons 0.498 0.181Average 0.527 0.188Table 2: Average ASR Word Error Rate over 3 babblesper topic (SD=Standard Deviation).tion,4 and one produced by an ASR system; Table 1presents an example.
The ASR transcripts of thebabbles were used by our system as a basis for rank-ing, and as a basis for making the decision on whento barge-in, what we call the ?stopping point.?
Themanual transcriptions were used only for scoring theWord Error Rate (WER) of the ASR transcript foreach babble.4The transcriber is the third author of this paper.1273Judgment at First RankBabble Words Relevant Not Relevant Unknown Scorable First Rel Last Rel WER257-3 74 5 64 5 93% @13 @66 0.414276-3 61 7 46 8 87% @36 @42 0.720258-1 146 2 118 26 82% @28 @29 0.528297-1 117 58 19 40 66% @56 @117 0.594274-3 94 57 0 47 61% @22 @94 0.250274-1 105 49 13 43 59% @57 @105 0.437257-1 191 104 0 87 54% @52 @188 0.764271-1 145 42 26 76 48% @38 @109 0.556287-2 61 26 0 35 43% @33 @61 0.889260-2 93 22 8 63 32% @69 @93 0.500276-2 69 11 2 56 19% @47 @69 0.795260-3 82 6 8 68 17% @17 @62 0.370258-2 94 14 1 79 16% @24 @60 0.389297-3 90 4 2 84 7% @52 @56 0.312266-2 115 6 0 109 5% @47 @52 0.745Table 3: Rank-1 relevance (?Rel?)
judgments and position of first and last scorable guesses.3.2 System SetupThe TREC-5 Associated Press (AP) and Wall StreetJournal (WSJ) news stories were indexed by In-dri (Strohman et al 2004) using the Krovetz stem-mer (Krovetz, 1993), standard English stopword set-tings, and language model matching.
Each babblewas turned into a set of nested queries by sequen-tially concatenating words.
Specifically, the firstquery contained only the first word from the bab-ble, the second query only the first two words, andso on.
Thus, the number of queries presented to In-dri for a given babble was equivalent to the num-ber of words in the babble, with each query differ-ing only by the number of words it contained.
Theresults were scored using trec eval version 9.0.For evaluation, we were interested in the reciprocalrank; in particular, where the reciprocal rank wasone.
This measure tells us when Indri was able toplace a known relevant document at rank one.4 Working with BabblesOur experiment design presents three key chal-lenges.
The first is ranking well despite errors inspeech processing.
Table 2 shows the average WordError Rate (WER) for each topic, over three babbles.Averaging further over all thirty babbles, we see thatabout half the words are correctly recognized.
Whilethis may seem low, it is in line with observationsfrom other spoken content retrieval research: overclassroom lectures (Chelba et al 2007), call centerrecordings (Mamou et al 2006), and conversationaltelephone speech (Chia et al 2010).
Moreover, it isbroadly consistent with the reported term-matchingresults for low density languages in MediaEval.The second challenge lies in the scorability of thesystem guesses.
Table 3 provides an overview ofwhere relevance was found within our collection ofbabbles.
It includes only the subset of babbles forwhich, during the babble, at least one known rele-vant document was found at the top of the rankedlist.
The table presents the number of recognizedwords?a proxy for the number of potential stop-ping points?and at how many of those potentialstopping points the document ranked in position 1is known to be relevant, known not to be relevant, orof unknown relevance.
Because of the way in whichTREC relevance judgments were created, unknownrelevance indicates that no TREC system returnedthe document near the top of their ranked list.
AtTREC, documents with unknown relevance are typ-1274ically scored as if they are not relevant;5 we makethe same assumption.Table 3 also shows how much we would need torely on that assumption: the ?scorable?
fraction forwhich the relevance of the top-ranked document isknown, rather than assumed, ranges from 93 per centdown to 5 per cent.
In the averages that we report be-low, we omit the five babbles with scorable fractionsof 30 per cent or less.
On average, over the 10 top-ics for which more than 30 per cent of the potentialstopping points are scorable, there are 37 stoppingpoints at which our system could have been scoredas successful based on a known relevant documentin position 1.
In three of these cases, the challengefor our stopping classifier is extreme, with only ahandful?between two and seven?of such opportu-nities.A third challenge is knowing when to interruptto present results.
The ultimate goal of our workis to predict when the system should interrupt thebabbler and barge-in to present an answer in whichthey might be interested.
Table 3 next presentsthe word positions at which known relevant docu-ments first and last appear in rank one (?First Rel?
).This are the earliest and latest scorable successfulstopping points.
As can be seen, the first possi-ble stopping point exhibits considerable variation,as does the last.
For some babbles?babble 274-3,for example?almost any choice of stopping pointswould be fine.
In other cases?babble 258-1, forexample?a stopping point prediction would need tobe spot on to get any useful results at all.
Moreover,we can see both cases in different babbles for thesame topic despite the fact that both babblers wereprompted by the same topic; for example, babbles257-1 and 257-3, which are, respectively, fairly easyand fairly hard.Finally, we can look for interaction effects be-tween speech processing errors and scorability.
Therightmost column of Table 3 shows the measuredWER for each scorable babble.
Of the 10 scorablebabbles for which more than 30 per cent of the po-tential stopping points are scorable, three turned outto be extremely challenging for ASR, with word er-ror rates above 0.7.
Overall, however, the WER for5On the assumption that the TREC systems together spanthe range of responses that are likely to be relevant.the 10 babbles on which we focus is 0.56, which isabout the same as the average WER over all 30 bab-bles.In addition to the 15 babbles shown in Table 3,there are another 15 babbles for which no relevantdocument was retrievable.
Of those, only a singlebabble?babble 255-2, at 54 per cent scorable anda WER of 0.402?had more than 30 per cent of thepotential stopping points scorable.5 Learning to StopThere are several ways in which we could pre-dict when to stop the search and barge-in with ananswer?in this paper, we consider a machine learn-ing approach.
The idea is that by building a clas-sifier with enough information about known goodand bad babbles, a learner can make such predic-tions better than other methods.
Our stopping pre-diction models uses four types of features for eachpotential stopping point: the number of words spo-ken so far, the average word length so far, some?surface characteristics?
of those words, and somequery performance prediction metrics.
The surfacecharacteristics that we used were originally devel-oped to quantify writing style?they are particularlyuseful for generating readability grades of a givendocument.
Although many metrics for readabilityhave been proposed, we choose a subset: FleschReading Ease (Flesch, 1948), Flesch-Kincaid GradeLevel (Kincaid et al 1975), Automated Readabil-ity Index (Senter and Smith, 1967), Coleman-Liauindex (Coleman and Liau, 1975), Gunning fog in-dex (Gunning, 1968), LIX (Brown and Eskenazi,2005), and SMOG Grading (McLaughlin, 1969).Our expectation was that a better readability valueshould correspond to use of words that are more suc-cinct and expressive, and that a larger number ofmore expressive words should help the search en-gine to get good responses highly ranked.As post-retrieval query difficulty prediction mea-sures, we choose three that have been prominentin information retrieval research: clarity (Cronen-Townsend et al 2002), weighted informationgain (Zhou and Croft, 2007), and normalized querycommitment (Shtok et al 2012).
Although eachtakes a distinct approach, the methods all comparesome aspect of the documents retrieved by a query127500.20.40.60.810  20  40  60  80  100ReciprocalRankBabble position (words)Topic 274, Babble 1True positive True negative False negative False positiveFigure 2: Predictions for babble 274-1 made by a decision tree classifier trained on 27 babbles for the nine other topics.For each point, the mean reciprocal rank is annotated to indicate the correctness of the guess made by the classifier.Note that in this case, the classifier never made a false positive.
See Figure 1 for an unannotated version of this samebabble.Confusion MatrixClass.
Tn Fp Fn Tp F1 Acy.Bayes 1288 1259 61 291 0.31 55%Reg.
2522 25 253 99 0.42 90%Trees 2499 48 70 282 0.83 96%Table 4: Cross validation accuracy (?Acy.?)
measures forstop-prediction classifiers: naive Bayes, logistic regres-sion, and Decision trees.with the complete collection of documents in thecollection from which that retrieval was performed.They seek to provide some measure of informationabout how likely a query is to have ranked the docu-ments well when relevance judgments are not avail-able.
Clarity measures the difference in the languagemodels induced by the retrieved results and the cor-pus as a whole.
Weighted information gain and nor-malized query commitment look at the scores ofthe retrieved documents, the former comparing themean score of the retrieved set with that of the entirecorpus; the latter measuring the standard deviationof the scores for the retrieved set.Features of all four types were were created foreach query that was run for each babble; that is afterreceiving each new word.
A separate classifier wasthen trained for each topic by creating a binary ob-jective function for all 27 babbles for the nine othertopics, then using every query for every one of thosebabbles as training instances.
The objective func-tion produces 1 if the query actually retrieved a rel-evant document at first rank, and 0 otherwise.
Fig-ure 2 shows an example of how this training datawas created for one babble, and Table 4 shows theresulting hold-one-topic-out cross-validation resultsfor intrinsic measures of classifier accuracy for threeWeka classifiers6.
As can be seen, the decision treeclassifier seems to be a good choice, so in Section 7we compare the stopping prediction model basedon a decision tree classifier trained using hold-one-topic-out cross-validation with three baseline mod-els.6 Evaluation DesignThis section describes our evaluation measure andthe baselines to which we compared.6.1 Evaluation MeasureTo evaluate a stopping prediction model, the funda-mental goal is to stop with a relevant document inrank one, and to do so as close in time as possibleto the first such opportunity.
If the first guess is bad,it would be reasonable to score a second guess, withsome penalty.Specifically, there are several things that we6Naive Bayes, logistic regression, and decision trees (J48)1276would like our evaluation framework to describe.Keeping in mind that ultimately the system will in-terrupt the speaker to notify them of results, we firstwant to avoid the interruption before we have founda good answer.
Our evaluation measure gives nocredit for such a guess.
Second, we want to avoidinterrupting long after finding the first relevant an-swer.
Credit is reduced with increasing delays afterthe first point where we could have barged in.
Third,when we do barge-in, there must indeed be a goodanswer in rank one.
This will be true if we barge-in at the first opportunity, but if we barge-in laterthe good answer we had found might have droppedback out of the first position.
No credit is given ifwe barge-in such a case.
Finally, if a bad positionfor first barge-in is chosen, we would like at least toget it right the second time.
Thus, we limit ourselvesto two tries, awarding half the credit on the secondtry that we could have received had we barged in atthe same point on the first try.The delay penalty is modeled using an exponen-tial distribution that declines with each new wordthat arrives after the first opportunity.
Let q0 be thefirst point within a query where the reciprocal rankis one.
Let pi be the first ?yes?
guess of the predic-tor after point q0.
The score is thus e?
(q0?pi), where?
is the half-life, or the number of words by whichthe exponential decay has dropped to one-half.
Theequation is scaled by 0.5 if i is the second element(guess) of p, and by 0.25 if it is the third.
From Fig-ure 1, some cases the potential stopping points areconsecutive, while in others they are intermittent?we penalize delays from the first good opportunityeven when there is no relevant document in positionone because we feel that best models the user ex-perience.
Unjudged documents in position one aretreated as non-relevant.6.2 Stopping Prediction BaselinesWe chose one deterministic and one random base-line for comparison.
The deterministic baselinemade its first guess at a calculated point in the bab-ble, and continued to guess at each word thereafter.The initial guess was determined by taking the aver-age of the first scorable point of the other 27 out-of-topic babbles.The random baseline drew the first and secondwords at which to guess ?yes?
as samples from a010203040506070800  10  20  30  40  50  60  70  80Classifier guessFirst OpportunitytreesrandomdeterministicFigure 3: First guesses for various classifiers plottedagainst the first instance of rank one documents withina babble.
Points below the diagonal are places where theclassifier guessed too early; points above are guesses toolate.
All 11 babbles for which the decision tree classifiermade a guess are shown.uniform distribution.
Specifically, drawing samplesuniformly, without replacement, across the averagenumber of words in all other out-of-topic babbles.7 ResultsFigure 3 shows the extent to which each classifiersfirst guess is early, on time, or late.
These pointsfalls, respectively, below the main diagonal, on themain diagonal, or above the main diagonal.
Earlyguesses result in large penalties from our scoringfunction, dropping the maximum score from 1.0 to0.5; for late guesses the penalty depends on howlate the guess is.
As can be seen, our decision treeclassifier (?trees?)
guesses early more often than itguesses late.
For an additional four cases (not plot-ted), the decision tree classifier never makes a guess.Figure 4 shows the results for scoring at mostthree guesses.
These results are averaged over alleleven babbles for which the decision tree classi-fier made at least one guess; no guess was made onbabbles 257-3, 266-2, 260-3, or 274-3.
These re-127700.050.10.150.20.250  5  10  15  20  25  30ScoreWindowtrees random deterministicFigure 4: Evaluation using all available babbles in which the tree classifier made a guess.sults are shown for a half-life of five words, whichis a relatively steep penalty function, essentially re-moving all credit after about ten seconds at normalspeaking rates.
The leftmost point in each figure,plotted at a ?window size?
of one, shows the resultsfor the stopping prediction models as we have de-scribed them.
It is possible, and indeed not unusual,for our decision tree classifier to make two or threeguesses in a row, however, in part because it has nofeature telling it how long is has been since its mostrecent guess.
To see whether adding a bit of patiencewould help, we added a deterministic period follow-ing each guess in which no additional guess wouldbe allowed.
We call the point at which this delay ex-pires, and a guess is again allowed, the delay ?win-dow.
?As can be seen, a window size of ten or eleven?allowing the next guess no sooner than the tenth oreleventh subsequent word?is optimal for the deci-sion tree classifier when averaged over these elevenbabbles.
The random classifier has an optimal pointbetween window sizes of 21 and 26, but is gener-ally not as good as the other classifiers.
The deter-ministic classifier displays the most variability, butfor window sizes greater than 14, it is the best solu-tion.
Although it has fewer features available to it?knowing only the mean number of words to the firstopportunity for other topics?it is able to outperformthe decision tree classifier for relatively large win-dow sizes.From this analysis we conclude that our decisiontree classifier shows promise; and that going for-ward, it would likely be beneficial to integrate fea-tures of the deterministic classifier.
We can alsoconclude that these results are, at best, suggestive?a richer test collection will ultimately be required.Moreover, we need some approach to accommodatethe four cases in which the decision tree classifiernever guesses.
Setting a maximum point at whichthe first guess will be tried could be a useful initialheuristic, and one that would be reasonable to applyin practice.8 Conclusions and Future WorkWe have used a simulation study to show that build-ing a system for query by babbling is feasible.
More-over, we have suggested a reasonable evaluationmeasure for this task, and we have shown that sev-eral simple baselines for predicting stopping pointscan be beaten by a decision tree classifier.
Our nextstep is to try these same techniques with spokenquestions and spoken answers in a low-resource lan-guage using the test collection that is being devel-oped for the MediaEval 2013 Question Answeringfor the Spoken Web task.Another potentially productive direction for fu-ture work would be to somehow filter the queriesin ways that improve the rankings.
Many potentialusers of this technology in the actual developing re-gion settings that we wish to ultimately serve willlikely have no experience with Internet search en-gines, and thus they may be even less likely to fo-1278cus their babbles on useful terms to the same extentthat our babblers did in these experiments.
Therehas been some work on techniques for recognizinguseful query terms in long queries, but of course wewill need to do that with spoken queries, and more-over with queries spoken in a language for whichwe have at lest limited speech processing capabili-ties available.
How best to model such a situationin a simulation study is not yet clear, so we havedeferred this question until the MediaEval speech-to-speech test collection becomes available.In the long term, many of the questions we are ex-ploring will also has implications for open-domainWeb search in other hands- or eyes-free applicationssuch as driving a car or operating an aircraft.AcknowledgmentsWe thank Anna Shtok for her assistance with the un-derstanding and implementation of the various queryprediction metrics.
We also thank the anonymousbabblers who provided data that was imperative tothis study.
Finally, we would like to thank the re-viewers, whose comments helped to improve thework overall.References[Agarwal et al010] Sheetal K. Agarwal, Anupam Jain,Arun Kumar, Amit A. Nanavati, and Nitendra Rajput.2010.
The spoken web: A web for the underprivi-leged.
SIGWEB Newsletter, pages 1:1?1:9, June.
[Brown and Eskenazi2005] Jonathan Brown and MaxineEskenazi.
2005.
Student, text and curriculum mod-eling for reader-specific document retrieval.
In Pro-ceedings of the IASTED International Conference onHuman-Computer Interaction.
Phoenix, AZ.
[Chelba et al007] Ciprian Chelba, Jorge Silva, and AlexAcero.
2007.
Soft indexing of speech content forsearch in spoken documents.
Computer Speech andLanguage, 21(3):458?478.
[Chia et al010] Tee Kiah Chia, Khe Chai Sim, HaizhouLi, and Hwee Tou Ng.
2010.
Statistical lattice-basedspoken document retrieval.
ACM Transactions on In-formation Systems, 28(1):2:1?2:30, January.
[Coleman and Liau1975] Meri Coleman and TL Liau.1975.
A computer readability formula designed formachine scoring.
Journal of Applied Psychology,60(2):283.
[Cronen-Townsend et al002] Steve Cronen-Townsend,Yun Zhou, and W. Bruce Croft.
2002.
Predict-ing query performance.
In Proceedings of the 25thannual international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?02, pages 299?306, New York, NY, USA.
ACM.
[Flesch1948] Rudolf Flesch.
1948.
A new readabil-ity yardstick.
The Journal of applied psychology,32(3):221.
[Gunning1968] Robert Gunning.
1968.
The technique ofclear writing.
McGraw-Hill New York.
[Kincaid et al975] J Peter Kincaid, Robert P Fish-burne Jr, Richard L Rogers, and Brad S Chissom.1975.
Derivation of new readability formulas (auto-mated readability index, fog count and flesch readingease formula) for navy enlisted personnel.
Technicalreport, DTIC Document.
[Krovetz1993] Robert Krovetz.
1993.
Viewing morphol-ogy as an inference process.
In Proceedings of the16th annual international ACM SIGIR conference onResearch and development in information retrieval,SIGIR ?93, pages 191?202, New York, NY, USA.ACM.
[Mamou et al006] Jonathan Mamou, David Carmel, andRon Hoory.
2006.
Spoken document retrieval fromcall-center conversations.
In Proceedings of the 29thannual international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?06, pages 51?58, New York, NY, USA.
ACM.
[McLaughlin1969] G Harry McLaughlin.
1969.
Smoggrading: A new readability formula.
Journal of read-ing, 12(8):639?646.
[Medhi et al011] Indrani Medhi, Somani Patnaik,Emma Brunskill, S.N.
Nagasena Gautama, WilliamThies, and Kentaro Toyama.
2011.
Designingmobile interfaces for novice and low-literacy users.ACM Transactions on Computer-Human Interaction,18(1):2:1?2:28.
[Metze et al012] Florian Metze, Etienne Barnard, Mare-lie Davel, Charl Van Heerden, Xavier Anguera, Guil-laume Gravier, Nitendra Rajput, et al2012.
The spo-ken web search task.
In Working Notes Proceedings ofthe MediaEval 2012 Workshop.
[Mudliar et al012] Preeti Mudliar, Jonathan Donner,and William Thies.
2012.
Emergent practices aroundcgnet swara, voice forum for citizen journalism in ru-ral india.
In Proceedings of the Fifth InternationalConference on Information and Communication Tech-nologies and Development, ICTD ?12, pages 159?168,New York, NY, USA.
ACM.
[Oard2012] Douglas W. Oard.
2012.
Query by babbling.In CIKM Workshop on Information and KnowledgeManagement for Developing Regions, October.
[Senter and Smith1967] RJ Senter and EA Smith.
1967.Automated readability index.
Technical report, DTICDocument.1279[Sherwani et al009] Jahanzeb Sherwani, Sooraj Palijo,Sarwat Mirza, Tanveer Ahmed, Nosheen Ali, and RoniRosenfeld.
2009.
Speech vs. touch-tone: Tele-phony interfaces for information access by low liter-ate users.
In International Conference on Informationand Communication Technologies and Development,pages 447?457.
[Shtok et al012] Anna Shtok, Oren Kurland, DavidCarmel, Fiana Raiber, and Gad Markovits.
2012.Predicting query performance by query-drift estima-tion.
ACM Transactions on Information Systems,30(2):11:1?11:35, May.
[Strohman et al004] T. Strohman, D. Metzler, H. Turtle,and W. B. Croft.
2004.
Indri: A language model-based search engine for complex queries.
In Interna-tional Conference on Intelligence Analysis.
[Zhou and Croft2007] Yun Zhou and W. Bruce Croft.2007.
Query performance prediction in web search en-vironments.
In Proceedings of the 30th annual inter-national ACM SIGIR conference on Research and de-velopment in information retrieval, SIGIR ?07, pages543?550, New York, NY, USA.
ACM.1280
