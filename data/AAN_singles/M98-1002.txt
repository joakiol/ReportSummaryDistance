MUC-7 EVALUATIONOF IE TECHNOLOGY:Overview of ResultsElaine Marsh (NRL)Dennis Perzanowski (NRL)MUC-729 April 1998MUC-7 Program CommitteeRalph Grishman (NYU), Co-ChairElaine Marsh (NRL), Co-ChairChinatsu Aone (SRA)Lois Childs (Lockheed-Martin)Nancy Chinchor (SAIC)Jim Cowie (NMSU)Rob Gaizauskas (Sheffield)Megumi Kameyama (SRI)Tom Keenan (DoD)Boyan Onyshkevych (DoD)Martha Palmer (Penn)Beth Sundheim (SPAWARSYSCEN)Marc Vilain (MITRE)Ralph Weischedel (BBN)MUC-7 Evaluation MembersEvaluation Participants:American University in CairoBBNFACILEIsoquestKent Ridge Digital LaboratoriesLockheed-MartinMITRENational Taiwan UniversityNew York UniversityNTTOki Electric Industry Co., Ltd.SRATASC, Inc.University of DurhamUniversity of Edinburgh and ThomsonUniversity of ManitobaUniversity of PennsylvaniaUniversity of SheffieldEvaluation Support:Naval Research LaboratorySAIC (San Diego)MUC-7 Program CommitteeDARPA/ITO and Tipster ProgramLinguistic Data ConsortiumEvaluation Participation by TaskMET:National Taiwan UniversityNational University of SingaporeNew York UniversityNTTOki ElectricNamed Entity:BBNFACILEIsoquestMITRENational Taiwan UniversityNational University of SingaporeNew York UniversityOki ElectricUniversity of DurhamUniversity of Edinburgh and ThomsonUniversity of ManitobaUniversity of SheffieldTemplate Element:American University - CairoBBNFACILELockheed-MartinNew York UniversityOki ElectricSRAUniversity of DurhamUniversity of SheffieldTemplate Relation:American University - CairoBBNOki ElectricSRAUniversity of SheffieldScenario Template:American University - CairoNew York UniversitySRATASCUniversity of SheffieldCoreference:Oki ElectricUniversity of PennsylvaniaUniversity of DurhamUniversity of ManitobaUniversity of SheffieldIE Evaluation Tasks?
Named Entity Task [NE]: Insert SGML tags intothe text to mark each string that represents a person,organization, or location name, or a date or timestamp, or a currency or percentage figure?
Multi-lingual Entity Task [MET]: NE task forChinese and Japanese?
Template Element Task [TE]: Extract basicinformation related to organization, person, andartifact entities, drawing evidence from anywhere inthe textIE Evaluation Tasks?
Template Relation Task [TR]: Extract relationalinformation on employee_of, manufacture_of, andlocation_of relations?
Scenario Template Task [ST]: Extractprespecified event information and relate the eventinformation to particular organization, person, orartifact entities involved in the event.?
Coreference Task [CO]: Capture information oncoreferring expressions: all mentions of a givenentity, including those tagged in NE, TE tasksTraining and Data SetsCorpusNew York Times News Service (supplied by Linguistic DataConsortium)Evaluation Epoch: January 1 - September 11, 1996Approximately 158,000 articles- Training and test sets retrieved from corpus usingManaging Gigabytes text retrieval system using domainrelevant terms.-  2 sets of 100 articles (aircraft accident domain) -preliminary training, including dryrun.-  2 sets of 100 articles selected balanced for relevancy,type and source for formal run (launch event domain).Training and Data Sets (con?t)Training SetTraining keys for NE, TE, TR available from preliminary set of100 articles; CO from preliminary training set of 30 articles.Formal training set of 100 articles and answer keys for ST task.Test Set100 Articles (and answer keys) for NE (Formal Training set)100 articles (and answer keys) for TE, TR, STSubset of 30 articles (and answer keys) for CO task.Test ProcedureSchedule?
2-6 March: Formal Run Test for NE?
9 March: Training set of articles available forelectronic file transfer from SAIC (ST guidelines andkeys).?
31 March: Test set of articles available for electronicfile transfer from SAIC.?
6 April: Deadline for completing TE, TR, ST, and COtests (via electronic file transfer of system outputs toSAIC)Test Procedure (con?t)Notes on testing:?
Tests run by individual participating sites at their ownfacilities, following a written test procedure.?
Sites could conduct official ?optional?
tests in additionto the basic test.?
Adaptive systems were permitted.?
Walkthrough articles for:?
NE?
TR/TR/ST?
COExample Text<DOC><DOCID> nyt960214.0704 </DOCID><STORYID cat=f pri=u> A4479 </STORYID><SLUG fv=taf-z> BC-MURDOCH-SATELLITE-NYT </SLUG><DATE> 02-14 </DATE><NWORDS> 0608 </NWORDS><PREAMBLE>BC-MURDOCH-SATELLITE-NYTMURDOCH SATELLITE FOR LATIN PROGRAMMING EXPLODES ON TAKEOFF(kd)By MARK LANDLERc.1996 N.Y. Times News Service</PREAMBLE><TEXT><p>Example Text (con?t)A Chinese rocket carrying a television satellite exploded seconds after launch Wednesday,dealing a potential blow to Rupert Murdoch's ambitions to offer satellite programming inLatin America.<p>Murdoch's News Corp. is one of four media companies in a partnership that had leasedspace on the Intelsat satellite to offer the Latin American service.
The other partners areTele-Communications Inc., the nation's largest cable operator; Grupo Televisa SA, theMexican broadcaster and publisher, and the giant Brazilian media conglomerate Globo.<p>...<p></TEXT><TRAILER>NYT-02-14-96 2029EST</TRAILER></DOC>Composite Overall Results01020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallNETESTTRCOMUC 7Composite Overall ResultsMUC 601020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallCONESTTENamed Entity Task (NE)?
NE mirrored Multilingual Entity Task?
SGML tagging in text stream from SLUG, DATE,PREAMBLE, TEXT, TRAILER?
Elements: ENAMEX, NUMEX, TIMEX?
Attributes: TYPE, STATUS (keys), MIN (keys)?
Markables?
Names of organizations, persons, locations?
Mentions of dates and times (relative and absolute)?
Direct mentions of currency/percentageNamed Entity (NE) (con?t)?
Non-markables?
Artifacts (Wall Street Journal, MTV)?
Common nouns used in anaphoric reference (the plane, thecompany,)?
Names of groups of people and laws named after people(Republicans, Gramm-Rudman amendment, the Nobelprize)?
Adjectival forms of location names (American, Japanese)?
Miscellaneous uses of numbers which are not specificallycurrency or percentages (1 1/2 points, 1.5 times)?
Caveats: ?newspaper?
style, domain bias toward STtopicNE Overall F-MeasuresMUC 7F-Measure Error Recall Precision93.39 11 92 9591.60 14 90 9390.44 15 89 9288.80 18 85 9386.37 22 85 8785.83 22 83 8985.31 23 85 8684.05 26 77 9283.70 26 79 8982.61 29 74 9381.91 28 78 8777.74 33 76 8076.43 34 75 7869.67 44 66 73Annotators:97.60 4 98 9896.95 5 96 98NE Overall F-MeasuresMUC 6F-measure Error Recall Precision96.42 5 96 9795.66 7 95 9694.92 8 93 9694.00 10 92 9693.65 10 94 9393.33 11 92 9592.88 10 94 9292.74 12 92 9392.61 12 89 9691.20 13 91 9190.84 14 91 9189.06 18 84 9488.19 19 86 9085.82 20 85 8785.73 23 80 9284.95 22 82 89Annotators:96.68 6 95 9893.18 11 92 95NE Scores by Document Section (ERR)sorted by F-MeasureMUC 7F-Measure Slug Date Preamble Text93.39 14 0 7 1391.60 28 0 9 1590.44 24 0 11 1688.80 54 0 16 1986.37 34 0 19 2385.83 28 0 18 2485.31 45 0 25 2484.05 33 0 31 2783.70 39 0 23 2882.61 32 0 27 2781.91 49 0 24 3077.74 100 0 44 3276.43 51 0 34 3669.67 93 0 50 44Annotators:97.60 3 0 2 496.95 2 9 2 6NE Scores by Document Section (ERR)sorted by F-MeasureMUC 6F-Measure Doc Date Dateline Headline Text96.42 0 0 8 595.66 0 0 7 794.92 0 0 8 894 0 0 20 993.65 0 2 16 1093.33 0 4 38 992.88 0 0 18 1092.74 0 0 22 1192.61 100 0 18 991.2 0 0 30 1390.84 3 11 19 1489.06 3 4 28 1888.19 0 0 22 2085.82 0 6 18 2185.73 0 44 53 2184.95 0 0 50 21Annotator:96.68 0 0 7 6NE Subcategory Scores (ERR)sorted by F-measureMUC 7enamex                                        timex        numexF-measure org per loc date time money percent93.39 13 5 10 12 21 891.60 21 7 10 12 19 1190.44 22 8 11 14 21 1988.80 25 12 16 15 22 2386.37 21 22 26 18 18 1585.83 27 19 24 16 20 2085.31 29 16 26 14 23 2184.05 44 22 17 14 19 1083.70 33 22 27 18 19 1582.61 25 10 12 58 100 1781.91 38 19 31 19 17 2177.74 40 24 32 27 27 2676.43 47 32 35 21 22 1769.67 60 47 44 26 22 25Annotators:97.60 3 1 1 5 5 196.95 5 1 3 8 21 8NE Subcategory Scores (ERR)sorted by F-measureMUC 6enamex                                         timex        numexF-measure org per loc date time money percent96.42 10 2 6 3 * 0 095.66 11 3 9 7 * 1 094.92 16 3 7 3 * 0 094.00 16 3 15 9 * 3 093.65 13 4 8 8 * 8 3293.33 16 6 12 9 * 4 692.88 15 4 13 8 * 8 3292.74 16 4 9 16 * 2 092.61 14 4 5 43 * 1 091.20 18 9 19 8 * 6 3690.84 16 10 29 12 * 6 089.06 22 17 18 10 * 3 088.19 29 7 20 17 * 11 3685.82 29 9 16 13 * 6 3285.73 26 14 29 18 * 9 4084.95 45 4 31 10 * 4 32Annotator:96.68 6 1 4 8 * 0 0Distribution of NE tag elementsMUC 7enamex69%timex25%numex6%enamextimexnumexDistribution of NE tag elementsMUC 6timex10%numex8%enamex82%enamextimexnumexDistribution of NEENAMEX tag elementsMUC 7org46%person22%loc32%orgpersonlocDistribution of NE tag elementsENAMEX tag elementsMUC 6location12%person39%organization49%organizationpersonlocationDistribution of NE NUMEX tag elementsMUC 7money69%percent31%moneypercentDistribution of NE NUMEX tag elementsMUC 6percent18%money82%moneypercentDistribution of NE TIMEX tag elementsMUC 7date85%timex15%datetimexNE Results OverallMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallNE Overall ResultsMUC 601020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallNE Results on Walkthrough?
Number of tags in answer key:?
52 Enamex?
1 Numex?
14 Timex?
System scoring:?
Common mistakes on TIMEX: missed early Thursdaymorning, within six months?
Common mistakes on ENAMEX: missed Globo, MURDOCH,Xichang; Long March as TIMEX, ENAMEX?
One site missed only one entity in whole document within sixmonthsNE Results on WalkthroughMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallINFORMATION EXTRACTION:TEMPLATE ELEMENT (TE)?
TEs are independent or neutral wrt scenario:generic objects and slots.?
Separates domain-independent from domain-dependent aspects of extraction.?
Consists of object types defined for a givenscenario, but unconcerned with relevance.?
Answer key contains objects for all organizations,persons, and vehicle artifacts mentioned in thetexts, whether relevant to scenario or not.TE ObjectsMUC 7entity69%location31%locationentityTE ENT_TYPE DistributionMUC 7org57%artifact13%person30%artifactorgpersonTE LOCALE_TYPE DistributionMUC 7airport3%water2%unk5%region13%province10%city31%country36%airportcitycountryprovinceregionunkwaterTE Results OverallMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallTE Overall Results01020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallMUC 6TE Results for TE ObjectsMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100RecalllocationentityTE Results for ENT_TYPEMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallORGPERSONARTIFACTTE Results for LOCALE_TYPEMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallairportcitycountryprovinceregionunkwaterTE ERR Results by SlotMUC 70102030405060708090100BESTAVGTE Results on Walkthrough Article?
Omissions or errors in ENT_DESCRIPTOR (spanof descriptor, descriptor itself)?
Omissions NAME slot: aliases missed (ChinaGreat Wall, News Corp.)?
LOCALE_TYPE (PROVINCE / COUNTRY / CITY)?
ENT_CATEGORY: ORG_OTHER vs. ORG_CO(Space Transportation Association)?
ORG as PERSON (Intelsat); PERSON as ORG(Murdoch)TE Results on WalkthroughMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallTemplate Relations Task (TR)?
New task for MUC-7.?
TRs express domain-independent relationshipsbetween entities, as compared with TEs whichidentify entities themselves.?
TR uses LOCATION_OF, EMPLOYEE_OF, andPRODUCT_OF relations.?
Answer key contains entities for all organizations,persons, and artifacts that enter into these relations,whether relevant to scenario or not.TR OverallMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallTR Results by RelationMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100Recalllocation_ofproduct_ofemployee_ofTR Error ScoresMUC 70102030405060708090100location_of product_of employee_ofBESTAVGTR Results on WalkthroughMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallScenario Template (ST)?
STs express domain and task-specific entities andrelations.
Similar to MUC-6 template.?
ST tests portability to new extraction problem; shorttime frame for system preparation (1 month)?
Scenario concerns vehicle launch events.?
Template consists of one high-level event object(LAUNCH_EVENT) with 7 slots, including 2 relationalobjects (VEHICLE_INFO, PAYLOAD_INFO), 3 set fills(MISSION_TYPE, MISSION_FUNCTION,MISSION_STATUS), and 2 pointers to low-level objects(LAUNCH_SITE, LAUNCH_DATE)Scenario Template (con?t)?
Relational objects have pointers to Template Elements, set-fills.?
Set fills require inferences from the text.?
Test set statistics:  63/100 documents relevant tothe scenario.ST Overall Results?
Systems scored  points lower (F-measure) on STthan on TE.?
Interannotator variability (measured on allarticles) was between 85.15 and 96.64 on the F-measures.?
Document-level relevance judgments (TextFiltering scores), were similar to those for MUC-6,although percentage of relevant articles in textset was greater.ST Slot DistributionMUC 7LOCATION15%ENTITY36%TIME14%VEHICLE_INF8%PAYLOAD_INF14%LAUNCH_EVNT13%ENTITYLOCATIONTIMEVEHICLE_INFPAYLOAD_INFLAUNCH_EVNTST  Template ElementsMUC 7ENTITY72%LOCATION28%ENTITYLOCATIONTE ObjectsMUC 7entity69%location31%locationentityST Template ElementsMUC 7ENTITY57%TIME21%LOCATION22%ENTITYLOCATIONTIMEST ENT_TYPE DistributionMUC 7organization48%artifact49%person3%artifactorganizationpersonTE ENT_TYPE DistributionMUC 7org57%artifact13%person30%artifactorgpersonST LOCALE_TYPE DistributionMUC 7water2% airport10%city21%country44%province9%unk10%region4% airportcitycountryprovinceregionunkwaterTE LOCALE_TYPE DistributionMUC 7airport3%water2%unk5%region13%province10%city31%country36%airportcitycountryprovinceregionunkwaterST Results OverallMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallAnnotatorsSystemsST Overall ResultsMUC 601020304050607080901000 10 20 30 40 50 60 70 80 90RecallST Results for Text FilteringMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallAnnotatorsSystemsST Results on WalkthroughMUC-7?
F-measures for annotators: 98.13, 91.40ERR for annotators: 4%, 14%?
F-Measures for systems(all-1): 35.60-41.18ERR for systems (all-1): 56-75%MUC-6:?
ERR for annotators: 8%?
ERR for systems: 30-89%ST Results on WalkthroughMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallCoreference Task (CO)?
Capture information on coreferringexpressions: all mentions of a given entity,including those tagged in NE, TE tasks.?
Focused on the IDENTITY (IDENT) relation:symmetrical and transitive relation,equivalence classes used for scoring.?
Markables: Nouns, Noun Phrases, PronounsCO Results Overall01020304050607080901000 10 20 30 40 50 60 70 80 90RecallMUC 7CO Overall ResultsMUC 601020304050607080901000 10 20 30 40 50 60 70 80 90 100RecallCO Results for Walkthrough?
Walkthrough article non-relevant for ST?
F-measures range from 23.2-62.3%?
Missing:?
Dates: Thursday, Sept. 10?
Money: $30 Million?
Unusual Conjunctions: GM, GE PROJECTS?
Miscellaneous:Thursday?s meeting, agency?s meeting,FCC?s allocation?, transmissions from satellites to earth stationsUS satellite industry, federal regulatorssatellite downlinks,NEEDED AIRWAVES.CO Results on WalkthroughMUC 701020304050607080901000 10 20 30 40 50 60 70 80 90 100Recall
