Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 613?622,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsLearning from evolving data streams: online triage of bug reportsGrzegorz ChrupalaSpoken Language SystemsSaarland Universitygchrupala@lsv.uni-saarland.deAbstractOpen issue trackers are a type of social me-dia that has received relatively little atten-tion from the text-mining community.
Weinvestigate the problems inherent in learn-ing to triage bug reports from time-varyingdata.
We demonstrate that concept drift isan important consideration.
We show theeffectiveness of online learning algorithmsby evaluating them on several bug reportdatasets collected from open issue trackersassociated with large open-source projects.We make this collection of data publiclyavailable.1 IntroductionThere has been relatively little research to dateon applying machine learning and Natural Lan-guage Processing techniques to automate soft-ware project workflows.
In this paper we addressthe problem of bug report triage.1.1 Issue trackingLarge software projects typically track defect re-ports, feature requests and other issue reports us-ing an issue tracker system.
Open source projectstend to use trackers which are open to both devel-opers and users.
If the product has many users itstracker can receive an overwhelming number ofissue reports: Mozilla was receiving almost 300reports per day in 2006 (Anvik et al2006).
Some-one has to monitor those reports and triage them,that is decide which component they affect andwhich developer or team of developers should beresponsible for analyzing them and fixing the re-ported defects.
An automated agent assisting thestaff responsible for such triage has the potentialto substantially reduce the time and cost of thistask.1.2 Issue trackers as social mediaIn a large software project with a loose, notstrictly hierarchical organization, standards andpractices are not exclusively imposed top-downbut also tend to spontaneously arise in a bottom-up fashion, arrived at through interaction of in-dividual developers, testers and users.
The indi-viduals involved may negotiate practices explic-itly, but may also imitate and influence each othervia implicitly acquired reputation and status.
Thisprocess has a strong emergent component: an in-formal taxonomy may arise and evolve in an is-sue tracker via the use of free-form tags or labels.Developers, testers and users can attach tags totheir issue reports in order to informally classifythem.
The issue tracking software may give usersfeedback by informing them which tags were fre-quently used in the past, or suggest tags basedon the content of the report or other information.Through this collaborative, feedback driven pro-cess involving both human and machine partici-pants, an evolving consensus on the label inven-tory and semantics typically arises, without muchtop-down control (Halpin et al2007).This kind of emergent taxonomy is known asa folksonomy or collaborative tagging and isvery common in the context of social web appli-cations.
Large software projects, especially thosewith open policies and little hierarchical struc-tures, tend to exhibit many of the same emergentsocial properties as the more prototypical socialapplications.
While this is a useful phenomenon,it presents a special challenge from the machine-learning point of view.6131.3 Concept driftMany standard supervised approaches inmachine-learning assume a stationary distributionfrom which training examples are independentlydrawn.
The set of training examples is processedas a batch, and the resulting learned decisionfunction (such as a classifier) is then used on testitems, which are assumed to be drawn from thesame stationary distribution.If we need an automated agent which uses hu-man labels to learn to tag objects the batch learn-ing approach is inadequate.
Examples arrive one-by-one in a stream, not as a batch.
Even moreimportantly, both the output (label) distributionand the input distribution from which the exam-ples come are emphatically not stationary.
As asoftware project progresses and matures, the typeof issues reported is going to change.
As projectmembers and users come and go, the vocabularythey use to describe the issues will vary.
As theconsensus tag folksonomy emerges, the label andtraining example distribution will evolve.
Thisphenomenon is sometimes referred to as conceptdrift (Widmer and Kubat 1996, Tsymbal 2004).Early research on learning to triage tended toeither not notice the problem (C?ubranic?
and Mur-phy 2004), or acknowledge but not address it (An-vik et al2006): the evaluation these authors usedassigned bug reports randomly to training andevaluation sets, discarding the temporal sequenc-ing of the data stream.Bhattacharya and Neamtiu (2010) explicitlyaddress the issue of online training and evalua-tion.
In their setup, the system predicts the out-put for an item based only on items preceding itin time.
However, their approach to incremen-tal learning is simplistic: they use a batch clas-sifier, but retrain it from scratch after receivingeach training example.
A fully retrained batchclassifier will adapt only slowly to changing datastream, as more recent example have no more in-fluence on the decision function that less recentones.Tamrawi et al(2011) propose an incrementalapproach to bug triage: the classes are rankedaccording to a fuzzy set membership function,which is based on incrementally updated fea-ture/class co-occurrence counts.
The model is ef-ficient in online classification, but also adapts onlyslowly.1.4 Online learningThis paucity of research on online learning fromissue tracker streams is rather surprising, giventhat truly incremental learners have been well-known for many years.
In fact one of the firstlearning algorithms proposed was Rosenblatt?sperceptron, a simple mistake-driven discrimina-tive classification algorithm (Rosenblatt 1958).
Inthe current paper we address this situation andshow that by using simple, standard online learn-ing methods we can improve on batch or pseudo-online learning.
We also show that when usinga sophisticated state-of-the-art stochastic gradientdescent technique the performance gains can bequite large.1.5 ContributionsOur main contributions are the following: Firstly,we explicitly show that concept-drift is pervasiveand serious in real bug report streams.
We thenaddress this problem by leveraging state-of-the-art online learning techniques which automati-cally track the evolving data stream and incremen-tally update the model after each data item.
Wealso adopt the continuous evaluation paradigm,where the learner predicts the output for each ex-ample before using it to update the model.
Sec-ondly, we address the important issue of repro-ducibility in research in bug triage automationby making available the data sets which we col-lected and used, in both their raw and prepro-cessed forms.2 Open issue-tracker dataOpen source software repositories and their as-sociated issue trackers are a naturally occurringsource of large amounts of (partially) labeled data.There seems to be growing interest in exploitingthis rich resource as evidenced by existing publi-cations as well as the appearance of a dedicatedworkshop (Working Conference on Mining Soft-ware Repositories).In spite of the fact that the data is publicly avail-able in open repositories, it is not possible to di-rectly compare the results of the research con-ducted on bug triage so far: authors use non-trivial project-specific filtering, re-labeling andpre-processing heuristics; these steps are usuallynot specified in enough detail that they could beeasily reproduced.614Field MeaningIdentifier Issue IDTitle Short description of issueDescription Content of issue report, whichmay include steps to reproduce,error messages, stack traces etc.Author ID of report submitterCCS List of IDs of people CC?d onthe issue reportLabels List of tags associated with is-sueStatus Label describing the current sta-tus of the issue (e.g.
Invalid,Fixed, Won?t Fix)Assigned To ID of person who has been as-signed to deal with the issuePublished Date on which issue report wassubmittedTable 1: Issue report recordTo help remedy this situation we decided to col-lect data from several open issue trackers, use theminimal amount of simple preprocessing and fil-ter heuristics to get useful input data, and publiclyshare both the raw and preprocessed data.We designed a simple record type which actsas a common denominator for several tracker for-mats.
Thus we can use a common representationfor issue reports from various trackers.
The fieldsin our record are shown in Table 1.Below we describe the issue trackers usedand the datasets we build from them.
As dis-cussed above (and in more detail in Section 4.1),we use progressive validation rather than a splitinto training and test set.
However, in orderto avoid developing on the test data, we spliteach data stream into two substreams, by assign-ing odd-numbered examples to the test streamand the even-numbered ones to the developmentstream.
We can use the development stream forexploratory data analysis and feature and param-eter tuning, and then use progressive validation toevaluate on entirely unseen test data.
Below wespecify the size and number of unique labels inthe development sets; the test sets are very similarin size.Chromium Chromium is the open source-project behind Google?s Chrome browser(http://code.google.com/p/chromium/).
We retrieved all the bugsfrom the issue tracker, of which 66,704 have oneof the closed statuses.
We generated two data setsfrom the Chromium issues:?
Chromium SUBCOMPONENT.
Chromiumuses special tags to help triage the bug re-ports.
Tags prefixed with Area- specifywhich subcomponent of the project the bugshould be routed to.
In some cases morethan one Area- tag is present.
Since thisaffects less than 1% of reports, for simplic-ity we treat these as single, compound labels.The development set contains 31,953 items,and 75 unique output labels.?
Chromium ASSIGNED.
In this dataset theoutput is the value of the assignedTofield.
We discarded issues where thefield was left empty, as well as theones which contained the placeholder valueall-bugs-test.chromium.org.
Thedevelopment set contains 16,154 items and591 unique output labels.Android Android is a mobile operating sys-tem project (http://code.google.com/p/android/).
We retrieved all the bugs reports,of which 6,341 had a closed status.
We generatedtwo datasets:?
Android SUBCOMPONENT.
The reportswhich are labeled with tags prefixed withComponent-.
The development set con-tains 888 items and 12 unique output labels.?
Android ASSIGNED.
The output label is thevalue of the assignedTo field.
We dis-carded issues with the field left empty.
Thedevelopment set contains 718 items and 72unique output labels.Firefox Firefox is the well-known web-browserproject (https://bugzilla.mozilla.org).We obtained a total of 81,987 issues with aclosed status.?
Firefox ASSIGNED.
We discarded issueswhere the field was left empty, as well asthe ones which contained a placeholder value(nobody).
The development set contains12,733 items and 503 unique output labels.Launchpad Launchpad is an issue trackerrun by Canonical Ltd for mostly Ubuntu-relatedprojects (https://bugs.launchpad.615net/).
We obtained a total of 99,380 issues witha closed status.?
Launchpad ASSIGNED.
We discarded issueswhere the field was left empty.
The devel-opment set contains 18,634 items and 1,970unique output labels.3 Analysis of concept driftIn the introduction we have hypothesized that inissue tracker streams concept drift would be anespecially acute problem.
In this section we showhow class distributions evolve over time in thedata we collected.A time-varying distribution is difficult to sum-marize with a single number, but it is easy to ap-preciate in a graph.
Figures 1 and 2 show conceptdrift for several of our data streams.
The horizon-tal axis indexes the position in the data stream.The vertical axis shows the class proportions ateach position, averaged over a window containing7% of all the examples in the stream, i.e.
in eachthin vertical bar the proportion of colors used cor-responds to the smoothed class distribution at aparticular position in the stream.Consider the plot for Chromium SUBCOMPO-NENT.
We can see that a bit before the middlepoint in the stream class proportions change quitedramatically: The orange BROWSERUI and vio-let MISC almost disappears, while blue INTER-NALS, pink UI and dark red UNDEFINED takeover.
This likely corresponds to an overhaul in thelabel inventory and/or recommended best practicefor triage in this project.
There are also moregradual and smaller scale changes throughout thedata stream.The Android SUBCOMPONENT stream con-tains much less data so the plot is less smooth, butthere are clear transitions in this image also.
Wesee that light blue GOOGLE all but disappears afterabout two thirds point and the proportion of vio-let TOOLS and light-green DALVIK dramaticallyincreases.In Figure 2 we see the evolution of class pro-portions in the ASSIGNED datasets.
Each plot?sidiosyncratic shape illustrates that there is widevariation in the amount and nature of concept driftin different software project issue trackers.Figure 1: SUBCOMPONENT class distribution changeover time4 Experimental resultsIn an online setting it is important to use an evalu-ation regime which closely mimics the continuoususe of the system in a real-life situation.4.1 Progressive validationWhen learning from data streams the standardevaluation methodology where data is split into aseparate training and test set is not applicable.
Anevaluation regime know as progressive validationhas been used to accurately measure the general-ization performance of online algorithms (Blumet al1999).
Under progressive evaluation, an in-put example from a temporally ordered sequenceis sent to the learner, which returns the prediction.The error incurred on this example is recorded,and the true output is only then sent to the learnerwhich may update its model based on it.
The fi-nal error is the mean of the per-example errors.Thus even though there is no separate test set, theprediction for each input is generated based on amodel trained on examples which do not includeit.In previous work on bug report triage, Bhat-tacharya and Neamtiu (2010) and Tamrawi et al(2011) used an evaluation scheme (close to) pro-616Figure 2: ASSIGNED class distribution change over timegressive validation.
They omit the initial 111thofthe examples from the mean.4.2 Mean reciprocal rankA bug report triaging agent is most likely to beused in a semi-automatic workflow, where a hu-man triager is presented with a ranked list ofpossible outputs (component labels or developerIDs).
As such it is important to evaluate not onlyaccuracy of the top ranking suggesting, but ratherthe quality of the whole ranked list.Previous research (Bhattacharya and Neamtiu2010, Tamrawi et al2011) made an attempt atapproximating this criterion by reporting scoreswhich indicate whether the true output is presentin the top n elements of the ranking, for severalvalues of n. Here we suggest borrowing the meanreciprocal rank (MRR) metric from the informa-tion retrieval domain (Voorhees 2000).
It is de-fined as the mean of the reciprocals of the rank atwhich the true output is found:MRR =1NN?i=1rank(i)?1where rank(i) indicates the rank of the ith trueoutput.
MRR has the advantage of providing asingle number which summarizes the quality ofwhole rankings for all the examples.
MRR is alsoa special case of Mean Average Precision whenthere is only one true output per item.4.3 Input representationSince in this paper we focus on the issues relatedto concept drift and online learning, we kept thefeature set relatively simple.
We preprocess thetext in the issue report title and description fieldsby removing HTML markup, tokenizing, lower-casing and removing most punctuation.
We thenextracted the following feature types:?
Title unigram and bigram counts?
Description unigram and bigram counts?
Author ID (binary indicator feature)?
Year, month and day of submission (binaryindicator features)4.4 ModelsWe tested a simple online baseline, a pseudo-online algorithm which uses a batch model andrepeatedly retrains it, an online model used in pre-vious research on bug triage and two generic on-line learning algorithms.Window Frequency Baseline This baselinedoes not use any input features.
It outputs the617ranked list of labels for the current item basedon the relative frequencies of output labels in thewindow of k previous items.
We tested windowsof size 100 and 1000 and report the better result.SVM Minibatch This model uses the mul-ticlass linear Support Vector Machine model(Crammer and Singer 2002) as implemented inSVM Light (Joachims 1999).
SVM is knownas a state-of-the-art batch model in classificationin general and in text categorization in particu-lar.
The output classes for an input example areranked according to the value of the discriminantvalues returned by the SVM classifier.
In orderto adapt the model to an online setting we retrainit every n examples on the window of k previousexamples.
The parameters n and k can have largeinfluence on the prediction, but it is not clear howto set them when learning from streams.
Here wechose the values (100,1000) based on how feasi-ble the run time was and on the performance dur-ing exploratory experiments on Chromium SUB-COMPONENT.
Interestingly, keeping the windowparameter relatively small helps performance: awindow of 1,000 works better than a window of5,000.Perceptron We implemented a single-pass on-line multiclass Perceptron with a constant learn-ing rate.
It maintains a weight vector for eachoutput seen so far: the prediction function ranksoutputs according to the inner product of the cur-rent example with the corresponding weight vec-tor.
The update function takes the true output andthe predicted output.
If they are not equal, thecurrent input is subtracted from the weight vectorcorresponding to the predicted output and addedto the weight vector corresponding to the true out-put (see Algorithm 1).
We hash each feature to aninteger value and use it as the feature?s index inthe weight vectors in order to bound memory us-age in an online setting (Weinberger et al2009).The Perceptron is a simple but strong baseline foronline learning.Bugzie This is the model described in Tamrawiet al(2011).
The output classes are ranked ac-cording to the fuzzy set membership function de-fined as follows:?
(y,X) = 1?
?x?X(1?n(y, x)n(y) + n(x)?
n(y, x))Algorithm 1 Multiclass online perceptronfunction PREDICT(Y,W,x)return {(y,WTy x) | y ?
Y }procedure UPDATE(W,x, y?, y)if y?
6= y thenWy?
?Wy?
?
xWy ?Wy + xwhere y is the output label, X the set of featuresin the input issue report, n(y, x) the number of ex-amples labeled as y which contain feature x, n(y)number of examples labeled y and n(x) numberof examples containing feature x.
The counts areupdated online.
Tamrawi et al(2011) also usetwo so called caches: the label cache keeps thej% most recent labels and the term cache the kmost significant features for each label.
Sincein Tamrawi et al(2011)?s experiments the labelcache did not affect the results significantly, herewe always set j to 100%.
We select the optimalk parameter from {100, 1000, 5000} based on thedevelopment set.Regression with Stochastic Gradient DescentThis model performs online multiclass learningby means of a reduction to regression.
The re-gressor is a linear model trained using StochasticGradient Descent (Zhang 2004).
SGD updates thecurrent parameter vector w(t) based on the gradi-ent of the loss incurred by the regressor on thecurrent example (x(t), y(t)):w(t+1) = w(t) ?
?
(t)?L(y(t),w(t)Tx(t))The parameter ?
(t) is the learning rate at time t,and L is the loss function.
We use the squaredloss:L(y, y?)
= (y ?
y?
)2We reduce multiclass learning to regression us-ing a one-vs-all-type scheme, by effectively trans-forming an example (x, y) ?
X ?
Y into |Y |(x?, y?)
?
X ?
?
{0, 1} examples, where Y is theset of labels seen so far.
The transform T is de-fined as follows:T (x, y) = {(x?, I(y = y?))
| y?
?
Y, x?h(i,y?)
= xi}where h(i, y?)
composes the index i with the labely?
(by hashing).For a new input x the ranking of the outputsy ?
Y is obtained according to the value of the618prediction of the base regressor on the binary ex-ample corresponding to each class label.As our basic regression learner we use the ef-ficient implementation of regression via SGD,Vowpal Wabbit (VW) (Langford et al2011).
VWimplements setting adaptive individual learningrates for each feature as proposed by Duchi et al(2010), McMahan and Streeter (2010).This is appropriate when there are many sparsefeatures, and is especially useful in learning fromtext from fast evolving data.
The features suchas unigram and bigram counts that we rely on arenotoriously sparse, and this is exacerbated by thechange over time in bug report streams.4.5 ResultsFigures 3 and 4 show the progressive validationresults on all the development data streams.
Thehorizontal lines indicate the mean MRR scores forthe whole stream.
The curves show a moving av-erage of MRR in a window comprised of 7% ofthe total number of items.
In most of the plots it isevident how the prediction performance dependson the concept drift illustrated in the plots in Sec-tion 3: for example on Chromium SUBCOMPO-NENT the performance of all the models drops abit before the midpoint in the stream while thelearners adapt to the change in label distributionthat is happening at this time.
This is especiallypronounced for Bugzie, since it is not able to learnfrom mistakes and adapt rapidly, but simply accu-mulates counts.For five out of the six datasets, Regression SGDgives the best overall performance.
On Launch-pad ASSIGNED, Bugzie scores higher ?
we inves-tigate this anomaly below.Another observation is that the window-basedfrequency baseline can be quite hard to beat:In three out of the six cases, the minibatchSVM model is no better than the baseline.Bugzie sometimes performs quite well, but forChromium SUBCOMPONENT and Firefox AS-SIGNED it scores below the baseline.Regarding the quality of the different datasets,an interesting indicator is the relative error reduc-tion by the best model over the baseline (see Ta-ble 2).
It is especially hard to extract meaning-ful information about the labeling from the inputson the Firefox ASSIGNED dataset.
One possiblecause of this can be that the assignment labelingpractices in this project are not consistent: this im-Dataset RERChromium SUB 0.36Android SUB 0.38Chromium AS 0.21Android AS 0.19Firefox AS 0.16Launchpad AS 0.49Table 2: Best model?s error relative to baseline on thedevelopment setTask Model MRR AccChromium Window 0.5747 0.3467SVM 0.5766 0.4535Perceptron 0.5793 0.4393Bugzie 0.4971 0.2638Regression 0.7271 0.5672Android Window 0.5209 0.3080SVM 0.5459 0.4255Perceptron 0.5892 0.4390Bugzie 0.6281 0.4614Regression 0.7012 0.5610Table 3: SUBCOMPONENT evaluation results on testset.pression seems to be born out by informal inspec-tion.On the other hand as the scores in Table 2indicate, Chromium SUBCOMPONENT, AndroidSUBCOMPOMENT and Launchpad ASSIGNEDcontain enough high-quality signal for the bestmodel to substantially outperform the label fre-quency baseline.On Launchpad ASSIGNED Regression SGDperforms worse than Bugzie.
The concept driftplot for these data suggests one reason: there isvery little change in class distribution over timeas compared to the other datasets.
In fact, eventhough the issue reports in Launchpad range fromyear 2005 to 2011, the more recent ones are heav-ily overrepresented: 84% of the items in the de-velopment data are from 2011.
Thus fast adap-tation is less important in this case and Bugzie isable to perform well.On the other hand, the reason for the less thanstellar score achieved with Regression SGD is dueto another special feature of this dataset: it hasby far the largest number of labels, almost 2,000.This degrades the performance for the one-vs-allscheme we use with SGD Regression.
Prelim-inary investigation indicates that the problem ismostly caused by our application of the ?hash-619Figure 3: SUBCOMPONENT evaluation results on thedevelopment seting trick?
to feature-label pairs (see section 4.4),which leads to excessive collisions with very largelabel sets.
Our current implementation can use atmost 29 bit-sized hashes which is insufficient fordatasets like Launchpad ASSIGNED.
We are cur-rently removing this limitation and we expect itwill lead to substantial gains on massively multi-class problems.In Tables 3 and 4 we present the overall MRRresults on the test data streams.
The picture is sim-ilar to the development data discussed above.5 Discussion and related workOur results show that by choosing the appropri-ate learner for the scenario of learning from datastreams, we can achieve much better results thanby attempting to twist batch algorithm to fit theonline learning setting.
Even a simple and well-know algorithm such as Perceptron can be effec-tive, but by using recent advances in research onSGD algorithms we can obtain substantial im-provements on the best previously used approach.Below we review the research on bug report triagemost relevant to our work.C?ubranic?
and Murphy (2004) seems to be thefirst attempt to automate bug triage.
The authorscast bug triage as a text classification task and useTask Model MRR AccChromium Window 0.0999 0.0472SVM 0.0908 0.0550Perceptron 0.1817 0.1128Bugzie 0.2063 0.0960Regression 0.3074 0.2157Android Window 0.3198 0.1684SVM 0.2541 0.1684Perceptron 0.3225 0.2057Bugzie 0.3690 0.2086Regression 0.4446 0.2951Firefox Window 0.5695 0.4426SVM 0.4604 0.4166Perceptron 0.5191 0.4306Bugzie 0.5402 0.4100Regression 0.6367 0.5245Launchpad Window 0.0725 0.0337SVM 0.1006 0.0704Perceptron 0.3323 0.2607Bugzie 0.5271 0.4339Regression 0.4702 0.3879Table 4: ASSIGNED evaluation results on test setthe data representation (bag of words) and learn-ing algorithm (Naive Bayes) typical for text clas-sification at the time.
They collect over 15,000bug reports from the Eclipse project.
The max-imum accuracy they report is 30% which wasachieved by using 90% of the data for training.In Anvik et al(2006) the authors experimentwith three learning algorithms: Naive Bayes,SVM and Decision Tree: SVM performs best intheir experiments.
They evaluate using precisionand recall rather than accuracy.
They report re-sults on the Eclipse and Firefox projects, with pre-cision 57% and 64% respectively, but very low re-call (7% and 2%).Matter et al(2009) adopt a different approachto bug triage.
In addition to the project?s issuetracker data, they use also the source-code ver-sion control data.
They build an expertise modelfor each developer which is a word count vec-tor of the source code changes committed.
Theyalso build a word count vector for each bug report,and use the cosine between the report and the ex-pertise model to rank developers.
Using this ap-proach (with a heuristic term weighting scheme)they report 33.6% accuracy on Eclipse.Bhattacharya and Neamtiu (2010) acknowl-edge the evolving nature of bug report streamsand attempt to apply incremental learning meth-ods to bug triage.
They use a two-step approach:620Figure 4: ASSIGNED evaluation results on the development setfirst they predict the most likely developer to as-sign to a bug using a classifier.
In a second stepthey rank candidate developers according to howlikely they were to take over a bug from the de-veloper predicted in the first step.
Their approachto incremental learning simply involves fully re-training a batch classifier after each item in thedata stream.
They test their approach on fixedbugs in Mozilla and Eclipse, reporting accuraciesof 27.5% and 38.2% respectively.Tamrawi et al(2011) propose the Bugziemodel where developers are ranked according tothe fuzzy set membership function as definedin section 4.4.
They also use the label (devel-oper) cache and term cache to speed up pro-cessing and make the model adapt better to theevolving data stream.
They evaluate Bugzie andcompare its performance to the models used inBhattacharya and Neamtiu (2010) on seven issuetrackers: Bugzie has superior performance on allof them ranging from 29.9% to 45.7% for top-1output.
They do not use separate validation setsfor system development and parameter tuning.In comparison to Bhattacharya and Neamtiu(2010) and Tamrawi et al(2011), here we focusmuch more on the analysis of concept drift in datastreams and on the evaluation of learning under itsconstraints.
We also show that for evolving issuetracker data, in a large majority of cases SGD Re-gression handily outperforms Bugzie.6 ConclusionWe demonstrate that concept drift is a real, perva-sive issue for learning from issue tracker streams.We show how to adapt to it by leveraging recentresearch in online learning algorithms.
We alsomake our dataset collection publicly available toenable direct comparisons between different bugtriage systems.1We have identified a good learning frameworkfor mining bug reports: in future we would liketo explore smarter ways of extracting useful sig-nals from the data by using more linguisticallyinformed preprocessing and higher-level featuressuch as word classes.AcknowledgmentsThis work was carried out in the context ofthe Software-Cluster project EMERGENT and waspartially funded by BMBF under grant number01IC10S01O.1Available from http://goo.gl/ZquBe621ReferencesAnvik, J., Hiew, L., and Murphy, G. (2006).
Whoshould fix this bug?
In Proceedings of the 28thinternational conference on Software engineer-ing, pages 361?370.
ACM.Bhattacharya, P. and Neamtiu, I.
(2010).
Fine-grained incremental learning and multi-featuretossing graphs to improve bug triaging.
InInternational Conference on Software Mainte-nance (ICSM), pages 1?10.
IEEE.Blum, A., Kalai, A., and Langford, J.
(1999).Beating the hold-out: Bounds for k-fold andprogressive cross-validation.
In Proceedingsof the twelfth annual conference on Computa-tional learning theory, pages 203?208.
ACM.Crammer, K. and Singer, Y.
(2002).
On the al-gorithmic implementation of multiclass kernel-based vector machines.
The Journal of Ma-chine Learning Research, 2:265?292.Duchi, J., Hazan, E., and Singer, Y.
(2010).
Adap-tive subgradient methods for online learningand stochastic optimization.
Journal of Ma-chine Learning Research.Halpin, H., Robu, V., and Shepherd, H. (2007).The complex dynamics of collaborative tag-ging.
In Proceedings of the 16th internationalconference on World Wide Web, pages 211?220.
ACM.Joachims, T. (1999).
Making large-scale svmlearning practical.
In Scho?lkopf, B., Burges,C., and Smola, A., editors, Advances in KernelMethods-Support Vector Learning.
MIT-Press.Langford, J., Hsu, D., Karampatziakis, N.,Chapelle, O., Mineiro, P., Hoffman, M.,Hofman, J., Lamkhede, S., Chopra, S.,Faigon, A., Li, L., Rios, G., and Strehl,A.
(2011).
Vowpal wabbit.
https://github.com/JohnLangford/vowpal_wabbit/wiki.Matter, D., Kuhn, A., and Nierstrasz, O.
(2009).Assigning bug reports using a vocabulary-based expertise model of developers.
In SixthIEEE Working Conference on Mining SoftwareRepositories.McMahan, H. and Streeter, M. (2010).
Adap-tive bound optimization for online convex op-timization.
Arxiv preprint arXiv:1002.4908.Rosenblatt, F. (1958).
The perceptron: A prob-abilistic model for information storage and or-ganization in the brain.
Psychological review,65(6):386.Tamrawi, A., Nguyen, T., Al-Kofahi, J., andNguyen, T. (2011).
Fuzzy set and cache-basedapproach for bug triaging.
In Proceedings ofthe 19th ACM SIGSOFT symposium and the13th European conference on Foundations ofsoftware engineering, pages 365?375.
ACM.Tsymbal, A.
(2004).
The problem of conceptdrift: definitions and related work.
ComputerScience Department, Trinity College Dublin.Voorhees, E. (2000).
The TREC-8 question an-swering track report.
NIST Special Publication,pages 77?82.Weinberger, K., Dasgupta, A., Langford, J.,Smola, A., and Attenberg, J.
(2009).
Featurehashing for large scale multitask learning.
InProceedings of the 26th Annual InternationalConference on Machine Learning, pages 1113?1120.
ACM.Widmer, G. and Kubat, M. (1996).
Learning in thepresence of concept drift and hidden contexts.Machine learning, 23(1):69?101.Zhang, T. (2004).
Solving large scale linearprediction problems using stochastic gradientdescent algorithms.
In Proceedings of thetwenty-first international conference on Ma-chine learning, page 116.
ACM.C?ubranic?, D. and Murphy, G. C. (2004).
Auto-matic bug triage using text categorization.
InIn SEKE 2004: Proceedings of the Sixteenth In-ternational Conference on Software Engineer-ing & Knowledge Engineering, pages 92?97.KSI Press.622
