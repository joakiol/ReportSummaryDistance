Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 688?695,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Unified Tagging Approach to Text NormalizationConghui ZhuHarbin Institute of TechnologyHarbin, Chinachzhu@mtlab.hit.edu.cnJie TangDepartment of Computer ScienceTsinghua University, Chinajietang@tsinghua.edu.cnHang LiMicrosoft Research AsiaBeijing, Chinahangli@microsoft.comHwee Tou NgDepartment of Computer ScienceNational University of Singapore, Singaporenght@comp.nus.edu.sgTiejun ZhaoHarbin Institute of TechnologyHarbin, Chinatjzhao@mtlab.hit.edu.cnAbstractThis paper addresses the issue of text nor-malization, an important yet often over-looked problem in natural language proc-essing.
By text normalization, we meanconverting ?informally inputted?
text intothe canonical form, by eliminating ?noises?in the text and detecting paragraph and sen-tence boundaries in the text.
Previously,text normalization issues were often under-taken in an ad-hoc fashion or studied sepa-rately.
This paper first gives a formaliza-tion of the entire problem.
It then proposesa unified tagging approach to perform thetask using Conditional Random Fields(CRF).
The paper shows that with the in-troduction of a small set of tags, most ofthe text normalization tasks can be per-formed within the approach.
The accuracyof the proposed method is high, becausethe subtasks of normalization are interde-pendent and should be performed together.Experimental results on email data cleaningshow that the proposed method signifi-cantly outperforms the approach of usingcascaded models and that of employing in-dependent models.1 IntroductionMore and more ?informally inputted?
text data be-comes available to natural language processing,such as raw text data in emails, newsgroups, fo-rums, and blogs.
Consequently, how to effectivelyprocess the data and make it suitable for naturallanguage processing becomes a challenging issue.This is because informally inputted text data isusually very noisy and is not properly segmented.For example, it may contain extra line breaks, extraspaces, and extra punctuation marks; and it maycontain words badly cased.
Moreover, the bounda-ries between paragraphs and the boundaries be-tween sentences are not clear.We have examined 5,000 randomly collectedemails and found that 98.4% of the emails containnoises (based on the definition in Section 5.1).In order to perform high quality natural lan-guage processing, it is necessary to perform ?nor-malization?
on informally inputted data first, spe-cifically, to remove extra line breaks, segment thetext into paragraphs, add missing spaces and miss-ing punctuation marks, eliminate extra spaces andextra punctuation marks, delete unnecessary tokens,correct misused punctuation marks, restore badlycased words, correct misspelled words, and iden-tify sentence boundaries.Traditionally, text normalization is viewed as anengineering issue and is conducted in a more orless ad-hoc manner.
For example, it is done by us-ing rules or machine learning models at differentlevels.
In natural language processing, several is-sues of text normalization were studied, but wereonly done separately.This paper aims to conduct a thorough investiga-tion on the issue.
First, it gives a formalization of688the problem; specifically, it defines the subtasks ofthe problem.
Next, it proposes a unified approachto the whole task on the basis of tagging.
Specifi-cally, it takes the problem as that of assigning tagsto the input texts, with a tag representing deletion,preservation, or replacement of a token.
As thetagging model, it employs Conditional RandomFields (CRF).
The unified model can achieve betterperformances in text normalization, because thesubtasks of text normalization are often interde-pendent.
Furthermore, there is no need to definespecialized models and features to conduct differ-ent types of cleaning; all the cleaning processeshave been formalized and conducted as assign-ments of the three types of tags.Experimental results indicate that our methodsignificantly outperforms the methods using cas-caded models or independent models on normali-zation.
Our experiments also indicate that with theuse of the tags defined, we can conduct most of thetext normalization in the unified framework.Our contributions in this paper include: (a) for-malization of the text normalization problem, (b)proposal of a unified tagging approach, and (c)empirical verification of the effectiveness of theproposed approach.The rest of the paper is organized as follows.
InSection 2, we introduce related work.
In Section 3,we formalize the text normalization problem.
InSection 4, we explain our approach to the problemand in Section 5 we give the experimental results.We conclude the paper in Section 6.2 Related WorkText normalization is usually viewed as anengineering issue and is addressed in an ad-hocmanner.
Much of the previous work focuses onprocessing texts in clean form, not texts ininformal form.
Also, prior work mostly focuses onprocessing one type or a small number of types oferrors, whereas this paper deals with manydifferent types of errors.Clark (2003) has investigated the problem ofpreprocessing noisy texts for natural languageprocessing.
He proposes identifying token bounda-ries and sentence boundaries, restoring cases ofwords, and correcting misspelled words by using asource channel model.Minkov et al (2005) have investigated the prob-lem of named entity recognition in informally in-putted texts.
They propose improving the perform-ance of personal name recognition in emails usingtwo machine-learning based methods: ConditionalRandom Fields and Perceptron for learning HMMs.See also (Carvalho and Cohen, 2004).Tang et al (2005) propose a cascaded approachfor email data cleaning by employing Support Vec-tor Machines and rules.
Their method can detectemail headers, signatures, program codes, and ex-tra line breaks in emails.
See also (Wong et al,2007).Palmer and Hearst (1997) propose using a Neu-ral Network model to determine whether a periodin a sentence is the ending mark of the sentence, anabbreviation, or both.
See also (Mikheev, 2000;Mikheev, 2002).Lita et al (2003) propose employing a languagemodeling approach to address the case restorationproblem.
They define four classes for word casing:all letters in lower case, first letter in uppercase, allletters in upper case, and mixed case, and formal-ize the problem as assigning class labels to wordsin natural language texts.
Mikheev (2002) proposesusing not only local information but also globalinformation in a document in case restoration.Spelling error correction can be formalized as aclassification problem.
Golding and Roth (1996)propose using the Winnow algorithm to addressthe issue.
The problem can also be formalized asthat of data conversion using the source channelmodel.
The source model can be built as an n-gramlanguage model and the channel model can be con-structed with confusing words measured by editdistance.
Brill and Moore, Church and Gale, andMayes et al have developed different techniquesfor confusing words calculation (Brill and Moore,2000; Church and Gale, 1991; Mays et al, 1991).Sproat et al (1999) have investigated normaliza-tion of non-standard words in texts, includingnumbers, abbreviations, dates, currency amounts,and acronyms.
They propose a taxonomy of non-standard words and apply n-gram language models,decision trees, and weighted finite-state transduc-ers to the normalization.3 Text NormalizationIn this paper we define text normalization at threelevels: paragraph, sentence, and word level.
Thesubtasks at each level are listed in Table 1.
For ex-ample, at the paragraph level, there are two sub-689tasks: extra line-break deletion and paragraphboundary detection.
Similarly, there are six (three)subtasks at the sentence (word) level, as shown inTable 1.
Unnecessary token deletion refers to dele-tion of tokens like ?-----?
and ?====?, which arenot needed in natural language processing.
Notethat most of the subtasks conduct ?cleaning?
ofnoises, except paragraph boundary detection andsentence boundary detection.Level Task Percentages of NoisesExtra line break deletion 49.53ParagraphParagraph boundary detectionExtra space deletion 15.58Extra punctuation mark deletion 0.71Missing space insertion 1.55Missing punctuation mark insertion 3.85Misused punctuation mark correction 0.64SentenceSentence boundary detectionCase restoration 15.04Unnecessary token deletion 9.69 WordMisspelled word correction 3.41Table 1.
Text Normalization SubtasksAs a result of text normalization, a text is seg-mented into paragraphs; each paragraph is seg-mented into sentences with clear boundaries; andeach word is converted into the canonical form.After normalization, most of the natural languageprocessing tasks can be performed, for example,part-of-speech tagging and parsing.We have manually cleaned up some email data(cf., Section 5) and found that nearly all the noisescan be eliminated by performing the subtasks de-fined above.
Table 1 gives the statistics.1.
i?m thinking about buying a pocket2.
pc    device for my wife this christmas,.3.
the worry that i have is that she won?t4.
be able to sync it to her outlook express5.
contacts?Figure 1.
An example of informal textI?m thinking about buying a Pocket PC device for mywife this Christmas.// The worry that I have is thatshe won?t be able to sync it to her Outlook Expresscontacts.//Figure 2.
Normalized textFigure 1 shows an example of informally input-ted text data.
It includes many typical noises.
Fromline 1 to line 4, there are four extra line breaks atthe end of each line.
In line 2, there is an extracomma after the word ?Christmas?.
The first wordin each sentence and the proper nouns (e.g.,?Pocket PC?
and ?Outlook Express?)
should becapitalized.
The extra spaces between the words?PC?
and ?device?
should be removed.
At the endof line 2, the line break should be removed and aspace is needed after the period.
The text should besegmented into two sentences.Figure 2 shows an ideal output of text normali-zation on the input text in Figure 1.
All the noisesin Figure 1 have been cleaned and paragraph andsentence endings have been identified.We must note that dependencies (sometimeseven strong dependencies) exist between differenttypes of noises.
For example, word case restorationneeds help from sentence boundary detection, andvice versa.
An ideal normalization method shouldconsider processing all the tasks together.4 A Unified Tagging Approach4.1 ProcessIn this paper, we formalize text normalization as atagging problem and employ a unified approach toperform the task (no matter whether the processingis at paragraph level, sentence level, or word level).There are two steps in the method: preprocess-ing and tagging.
In preprocessing, (A) we separatethe text into paragraphs (i.e., sequences of tokens),(B) we determine tokens in the paragraphs, and (C)we assign possible tags to each token.
The tokensform the basic units and the paragraphs form thesequences of units in the tagging problem.
In tag-ging, given a sequence of units, we determine themost likely corresponding sequence of tags by us-ing a trained tagging model.
In this paper, as thetagging model, we make use of CRF.Next we describe the steps (A)-(C) in detail andexplain why our method can accomplish many ofthe normalization subtasks in Table 1.(A).
We separate the text into paragraphs by tak-ing two or more consecutive line breaks as the end-ings of paragraphs.(B).
We identify tokens by using heuristics.There are five types of tokens: ?standard word?,?non-standard word?, punctuation mark, space, andline break.
Standard words are words in naturallanguage.
Non-standard words include severalgeneral ?special words?
(Sproat et al, 1999), emailaddress, IP address, URL, date, number, money,percentage, unnecessary tokens (e.g., ?===?
and690?###?
), etc.
We identify non-standard words byusing regular expressions.
Punctuation marks in-clude period, question mark, and exclamation mark.Words and punctuation marks are separated intodifferent tokens if they are joined together.
Naturalspaces and line breaks are also regarded as tokens.(C).
We assign tags to each token based on thetype of the token.
Table 2 summarizes the types oftags defined.Token Type Tag DescriptionPRV Preserve line breakRPA Replace line break by space Line breakDEL Delete line breakPRV Preserve spaceSpaceDEL Delete spacePSB Preserve punctuation mark and view it as sentence endingPRV Preserve punctuation mark without viewing it as sentence endingPunctuationmarkDEL Delete punctuation markAUC Make all characters in uppercaseALC Make all characters in lowercaseFUC Make the first character in uppercaseWordAMC Make characters in mixed casePRV Preserve the special tokenSpecial tokenDEL Delete the special tokenTable 2.
Types of tagsFigure 3.
An example of taggingFigure 3 shows an example of the tagging proc-ess.
(The symbol ???
indicates a space).
In the fig-ure, a white circle denotes a token and a gray circledenotes a tag.
Each token can be assigned severalpossible tags.Using the tags, we can perform most of the textnormalization processing (conducting seven typesof subtasks defined in Table 1 and cleaning90.55% of the noises).In this paper, we do not conduct three subtasks,although we could do them in principle.
These in-clude missing space insertion, missing punctuationmark insertion, and misspelled word correction.
Inour email data, it corresponds to 8.81% of thenoises.
Adding tags for insertions would increasethe search space dramatically.
We did not do thatdue to computation consideration.
Misspelled wordcorrection can be done in the same framework eas-ily.
We did not do that in this work, because thepercentage of misspelling in the data is small.We do not conduct misused punctuation markcorrection as well (e.g., correcting ?.?
with ???).
Itconsists of 0.64% of the noises in the email data.To handle it, one might need to parse the sentences.4.2 CRF ModelWe employ Conditional Random Fields (CRF) asthe tagging model.
CRF is a conditional probabilitydistribution of a sequence of tags given a sequenceof tokens, represented as P(Y|X) , where X denotesthe token sequence and Y the tag sequence(Lafferty et al, 2001).In tagging, the CRF model is used to find thesequence of tags Y* having the highest likelihoodY* = maxYP(Y|X), with an efficient algorithm (theViterbi algorithm).In training, the CRF model is built with labeleddata and by means of an iterative algorithm basedon Maximum Likelihood Estimation.Transition Featuresyi-1=y?, yi=yyi-1=y?, yi=y, wi=wyi-1=y?, yi=y, ti=tState Featureswi=w, yi=ywi-1=w, yi=ywi-2=w, yi=ywi-3=w, yi=ywi-4=w, yi=ywi+1=w, yi=ywi+2=w, yi=ywi+3=w, yi=ywi+4=w, yi=ywi-1=w?, wi=w, yi=ywi+1=w?, wi=w, yi=yti=t, yi=yti-1=t, yi=yti-2=t, yi=yti-3=t, yi=yti-4=t, yi=yti+1=t, yi=yti+2=t, yi=yti+3=t, yi=yti+4=t, yi=yti-2=t?
?, ti-1=t?, yi=yti-1=t?, ti=t, yi=yti=t, ti+1=t?, yi=yti+1=t?, ti+2=t?
?, yi=yti-2=t?
?, ti-1=t?, ti=t, yi=yti-1=t?
?, ti=t, ti+1=t?, yi=yti=t, ti+1=t?, ti+2=t?
?, yi=yTable 3.
Features used in the unified CRF model6914.3 FeaturesTwo sets of features are defined in the CRF model:transition features and state features.
Table 3shows the features used in the model.Suppose that at position i in token sequence x, wiis the token, ti the type of token (see Table 2), andyi the possible tag.
Binary features are defined asdescribed in Table 3.
For example, the transitionfeature yi-1=y?, yi=y implies that if the current tag isy and the previous tag is y?, then the feature valueis true; otherwise false.
The state feature wi=w,yi=y implies that if the current token is w and thecurrent label is y, then the feature value is true;otherwise false.
In our experiments, an actual fea-ture might be the word at position 5 is ?PC?
and thecurrent tag is AUC.
In total, 4,168,723 featureswere used in our experiments.4.4 Baseline MethodsWe can consider two baseline methods based onprevious work, namely cascaded and independentapproaches.
The independent approach performstext normalization with several passes on the text.All of the processes take the raw text as input andoutput the normalized/cleaned result independently.The cascaded approach also performs normaliza-tion in several passes on the text.
Each process car-ries out cleaning/normalization from the output ofthe previous process.4.5 AdvantagesOur method offers some advantages.
(1) As indicated, the text normalization tasks areinterdependent.
The cascaded approach or the in-dependent approach cannot simultaneously per-form the tasks.
In contrast, our method can effec-tively overcome the drawback by employing a uni-fied framework and achieve more accurate per-formances.
(2) There are many specific types of errors onemust correct in text normalization.
As shown inFigure 1, there exist four types of errors with eachtype having several correction results.
If one de-fines a specialized model or rule to handle each ofthe cases, the number of needed models will beextremely large and thus the text normalizationprocessing will be impractical.
In contrast, ourmethod naturally formalizes all the tasks as as-signments of different types of tags and trains aunified model to tackle all the problems at once.5 Experimental Results5.1 Experiment SettingData SetsWe used email data in our experiments.
We ran-domly chose in total 5,000 posts (i.e., emails) from12 newsgroups.
DC, Ontology, NLP, and ML arefrom newsgroups at Google (http://groups-beta.google.com/groups).
Jena is a newsgroup at Ya-hoo (http://groups.yahoo.com/group/jena-dev).
Wekais a newsgroup at Waikato University (https://list.scms.waikato.ac.nz).
Prot?g?
and OWL are from aproject at Stanford University(http://protege.stanford.edu/).
Mobility, WinServer,Windows, and PSS are email collections from acompany.Five human annotators conducted normalizationon the emails.
A spec was created to guide the an-notation process.
All the errors in the emails werelabeled and corrected.
For disagreements in theannotation, we conducted ?majority voting?.
Forexample, extra line breaks, extra spaces, and extrapunctuation marks in the emails were labeled.
Un-necessary tokens were deleted.
Missing spaces andmissing punctuation marks were added and marked.Mistakenly cased words, misspelled words, andmisused punctuation marks were corrected.
Fur-thermore, paragraph boundaries and sentenceboundaries were also marked.
The noises fell intothe categories defined in Table 1.Table 4 shows the statistics in the data sets.From the table, we can see that a large number ofnoises (41,407) exist in the emails.
We can also seethat the major noise types are extra line breaks,extra spaces, casing errors, and unnecessary tokens.In the experiments, we conducted evaluations interms of precision, recall, F1-measure, and accu-racy (for definitions of the measures, see for ex-ample (van Rijsbergen, 1979; Lita et al, 2003)).Implementation of Baseline MethodsWe used the cascaded approach and the independ-ent approach as baselines.For the baseline methods, we defined severalbasic prediction subtasks: extra line break detec-tion, extra space detection, extra punctuation markdetection, sentence boundary detection, unneces-sary token detection, and case restoration.
Wecompared the performances of our method withthose of the baseline methods on the subtasks.692Data SetNumberofEmailNumberofNoisesExtraLineBreakExtraSpaceExtraPunc.MissingSpaceMissingPunc.CasingErrorSpellingErrorMisusedPunc.Unnece-ssaryTokenNumber ofParagraphBoundaryNumber ofSentenceBoundaryDC 100 702 476 31 8 3 24 53 14 2 91 457 291Ontology 100 2,731 2,132 24 3 10 68 205 79 15 195 677 1,132NLP 60 861 623 12 1 3 23 135 13 2 49 244 296ML 40 980 868 17 0 2 13 12 7 0 61 240 589Jena 700 5,833 3,066 117 42 38 234 888 288 59 1,101 2,999 1,836Weka 200 1,721 886 44 0 30 37 295 77 13 339 699 602Prot?g?
700 3,306 1,770 127 48 151 136 552 116 9 397 1,645 1,035OWL 300 1,232 680 43 24 47 41 152 44 3 198 578 424Mobility 400 2,296 1,292 64 22 35 87 495 92 8 201 891 892WinServer 400 3,487 2,029 59 26 57 142 822 121 21 210 1,232 1,151Windows 1,000 9,293 3,416 3,056 60 116 348 1,309 291 67 630 3,581 2,742PSS 1,000 8,965 3,348 2,880 59 153 296 1,331 276 66 556 3,411 2,590Total 5,000 41,407 20,586 6,474 293 645 1,449 6,249 1,418 265 4,028 16,654 13,580Table 4.
Statistics on data setsFor the case restoration subtask (processing ontoken sequence), we employed the TrueCasingmethod (Lita et al, 2003).
The method estimates atri-gram language model using a large data corpuswith correctly cased words and then makes use ofthe model in case restoration.
We also employedConditional Random Fields to perform caserestoration, for comparison purposes.
The CRFbased casing method estimates a conditionalprobabilistic model using the same data and thesame tags defined in TrueCasing.For unnecessary token deletion, we used rules asfollows.
If a token consists of non-ASCII charac-ters or consecutive duplicate characters, such as?===?, then we identify it as an unnecessary token.For each of the other subtasks, we exploited theclassification approach.
For example, in extra linebreak detection, we made use of a classificationmodel to identify whether or not a line break is aparagraph ending.
We employed Support VectorMachines (SVM) as the classification model (Vap-nik, 1998).
In the classification model we utilizedthe same features as those in our unified model(see Table 3 for details).In the cascaded approach, the prediction tasksare performed in sequence, where the output ofeach task becomes the input of each immediatelyfollowing task.
The order of the prediction tasks is:(1) Extra line break detection: Is a line break aparagraph ending?
It then separates the text intoparagraphs using the remaining line breaks.
(2)Extra space detection: Is a space an extra space?
(3)Extra punctuation mark detection: Is a punctuationmark a noise?
(4) Sentence boundary detection: Isa punctuation mark a sentence boundary?
(5) Un-necessary token deletion: Is a token an unnecessarytoken?
(6) Case restoration.
Each of steps (1) to (4)uses a classification model (SVM), step (5) usesrules, whereas step (6) uses either a languagemodel (TrueCasing) or a CRF model (CRF).In the independent approach, we perform theprediction tasks independently.
When there is aconflict between the outcomes of two classifiers,we adopt the result of the latter classifier, as de-termined by the order of classifiers in the cascadedapproach.To test how dependencies between differenttypes of noises affect the performance of normali-zation, we also conducted experiments using theunified model by removing the transition features.Implementation of Our MethodIn the implementation of our method, we used thetool CRF++, available at http://chasen.org/~taku/software/CRF++/.
We made use of all the defaultsettings of the tool in the experiments.5.2 Text Normalization ExperimentsResultsWe evaluated the performances of our method(Unified) and the baseline methods (Cascaded andIndependent) on the 12 data sets.
Table 5 showsthe five-fold cross-validation results.
Our methodoutperforms the two baseline methods.Table 6 shows the overall performances of textnormalization by our method and the two baselinemethods.
We see that our method outperforms thetwo baseline methods.
It can also be seen that theperformance of the unified method decreases whenremoving the transition features (Unified w/oTransition Features).693We conducted sign tests for each subtask on theresults, which indicate that all the improvements ofUnified over Cascaded and Independent are statis-tically significant (p << 0.01).Detection Task Prec.
Rec.
F1 Acc.Independent 95.16 91.52 93.30 93.81Cascaded 95.16 91.52 93.30 93.81Extra Line BreakUnified 93.87 93.63 93.75 94.53Independent 91.85 94.64 93.22 99.87Cascaded 94.54 94.56 94.55 99.89Extra SpaceUnified 95.17 93.98 94.57 99.90Independent 88.63 82.69 85.56 99.66Cascaded 87.17 85.37 86.26 99.66ExtraPunctuationMark Unified 90.94 84.84 87.78 99.71Independent 98.46 99.62 99.04 98.36Cascaded 98.55 99.20 98.87 98.08Sentence BoundaryUnified 98.76 99.61 99.18 98.61Independent 72.51 100.0 84.06 84.27Cascaded 72.51 100.0 84.06 84.27Unnecessary TokenUnified 98.06 95.47 96.75 96.18Independent 27.32 87.44 41.63 96.22CaseRestoration(TrueCasing) Cascaded 28.04 88.21 42.55 96.35Independent 84.96 62.79 72.21 99.01Cascaded 85.85 63.99 73.33 99.07CaseRestoration(CRF) Unified 86.65 67.09 75.63 99.21Table 5.
Performances of text normalization (%)Text Normalization Prec.
Rec.
F1 Acc.Independent (TrueCasing) 69.54 91.33 78.96 97.90Independent (CRF) 85.05 92.52 88.63 98.91Cascaded (TrueCasing) 70.29 92.07 79.72 97.88Cascaded (CRF) 85.06 92.70 88.72 98.92Unified w/o TransitionFeatures 86.03 93.45 89.59 99.01Unified 86.46 93.92 90.04 99.05Table 6.
Performances of text normalization (%)DiscussionsOur method outperforms the independent methodand the cascaded method in all the subtasks, espe-cially in the subtasks that have strong dependen-cies with each other, for example, sentence bound-ary detection, extra punctuation mark detection,and case restoration.The cascaded method suffered from ignoranceof the dependencies between the subtasks.
For ex-ample, there were 3,314 cases in which sentenceboundary detection needs to use the results of extraline break detection, extra punctuation mark detec-tion, and case restoration.
However, in the cas-caded method, sentence boundary detection is con-ducted after extra punctuation mark detection andbefore case restoration, and thus it cannot leveragethe results of case restoration.
Furthermore, errorsof extra punctuation mark detection can lead toerrors in sentence boundary detection.The independent method also cannot make useof dependencies across different subtasks, becauseit conducts all the subtasks from the raw input data.This is why for detection of extra space, extrapunctuation mark, and casing error, the independ-ent method cannot perform as well as our method.Our method benefits from the ability of model-ing dependencies between subtasks.
We see fromTable 6 that by leveraging the dependencies, ourmethod can outperform the method without usingdependencies (Unified w/o Transition Features) by0.62% in terms of F1-measure.Here we use the example in Figure 1 to show theadvantage of our method compared with the inde-pendent and the cascaded methods.
With normali-zation by the independent method, we obtain:I?m thinking about buying a pocket PC   device for my wifethis Christmas, The worry that I have is that she won?t be ableto sync it to her outlook express contacts.//With normalization by the cascaded method, weobtain:I?m thinking about buying a pocket PC device for my wifethis Christmas, the worry that I have is that she won?t be ableto sync it to her outlook express contacts.//With normalization by our method, we obtain:I?m thinking about buying a Pocket PC device for my wifethis Christmas.// The worry that I have is that she won?t beable to sync it to her Outlook Express contacts.//The independent method can correctly deal withsome of the errors.
For instance, it can capitalizethe first word in the first and the third line, removeextra periods in the fifth line, and remove the fourextra line breaks.
However, it mistakenly removesthe period in the second line and it cannot restorethe cases of some words, for example ?pocket?
and?outlook express?.In the cascaded method, each process carries outcleaning/normalization from the output of the pre-vious process and thus can make use of thecleaned/normalized results from the previous proc-ess.
However, errors in the previous processes willalso propagate to the later processes.
For example,the cascaded method mistakenly removes the pe-riod in the second line.
The error allows case resto-ration to make the error of keeping the word ?the?in lower case.694TrueCasing-based methods for case restorationsuffer from low precision (27.32% by Independentand 28.04% by Cascaded), although their recallsare high (87.44% and 88.21% respectively).
Thereare two reasons: 1) About 10% of the errors inCascaded are due to errors of sentence boundarydetection and extra line break detection in previoussteps; 2) The two baselines tend to restore cases ofwords to the forms having higher probabilities inthe data set and cannot take advantage of the de-pendencies with the other normalization subtasks.For example, ?outlook?
was restored to first lettercapitalized in both ?Outlook Express?
and ?a pleas-ant outlook?.
Our method can take advantage of thedependencies with other subtasks and thus correct85.01% of the errors that the two baseline methodscannot handle.
Cascaded and Independent methodsemploying CRF for case restoration improve theaccuracies somewhat.
However, they are still infe-rior to our method.Although we have conducted error analysis onthe results given by our method, we omit the de-tails here due to space limitation and will reportthem in a future expanded version of this paper.We also compared the speed of our method withthose of the independent and cascaded methods.We tested the three methods on a computer withtwo 2.8G Dual-Core CPUs and three Gigabytememory.
On average, it needs about 5 hours fortraining the normalization models using ourmethod and 25 seconds for tagging in the cross-validation experiments.
The independent and thecascaded methods (with TrueCasing) require lesstime for training (about 2 minutes and 3 minutesrespectively) and for tagging (several seconds).This indicates that the efficiency of our methodstill needs improvement.6 ConclusionIn this paper, we have investigated the problem oftext normalization, an important issue for naturallanguage processing.
We have first defined theproblem as a task consisting of noise eliminationand boundary detection subtasks.
We have thenproposed a unified tagging approach to perform thetask, specifically to treat text normalization as as-signing tags representing deletion, preservation, orreplacement of the tokens in the text.
Experimentsshow that our approach significantly outperformsthe two baseline methods for text normalization.ReferencesE.
Brill and R. C. Moore.
2000.
An Improved ErrorModel for Noisy Channel Spelling Correction, Proc.of ACL 2000.V.
R. Carvalho and W. W. Cohen.
2004.
Learning toExtract Signature and Reply Lines from Email, Proc.of CEAS 2004.K.
Church and W. Gale.
1991.
Probability Scoring forSpelling Correction, Statistics and Computing, Vol.
1.A.
Clark.
2003.
Pre-processing Very Noisy Text, Proc.of Workshop on Shallow Processing of Large Cor-pora.A.
R. Golding and D. Roth.
1996.
Applying Winnow toContext-Sensitive Spelling Correction, Proc.
ofICML?1996.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data, Proc.
of ICML2001.L.
V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.2003.
tRuEcasIng, Proc.
of ACL 2003.E.
Mays, F. J. Damerau, and R. L. Mercer.
1991.
Con-text Based Spelling Correction, Information Process-ing and Management, Vol.
27, 1991.A.
Mikheev.
2000.
Document Centered Approach toText Normalization, Proc.
SIGIR 2000.A.
Mikheev.
2002.
Periods, Capitalized Words, etc.Computational Linguistics, Vol.
28, 2002.E.
Minkov, R. C. Wang, and W. W. Cohen.
2005.
Ex-tracting Personal Names from Email: ApplyingNamed Entity Recognition to Informal Text, Proc.
ofEMNLP/HLT-2005.D.
D. Palmer and M. A. Hearst.
1997.
Adaptive Multi-lingual Sentence Boundary Disambiguation, Compu-tational Linguistics, Vol.
23.C.J.
van Rijsbergen.
1979.
Information Retrieval.
But-terworths, London.R.
Sproat, A.
Black, S. Chen, S. Kumar, M. Ostendorf,and C. Richards.
1999.
Normalization of non-standard words, WS?99 Final Report.http://www.clsp.jhu.edu/ws99/projects/normal/.J.
Tang, H. Li, Y. Cao, and Z. Tang.
2005.
Email datacleaning, Proc.
of SIGKDD?2005.V.
Vapnik.
1998.
Statistical Learning Theory, Springer.W.
Wong, W. Liu, and M. Bennamoun.
2007.
EnhancedIntegrated Scoring for Cleaning Dirty Texts, Proc.
ofIJCAI-2007 Workshop on Analytics for Noisy Un-structured Text Data.695
