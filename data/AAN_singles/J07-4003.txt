Weighted and Probabilistic Context-FreeGrammars Are Equally ExpressiveNoah A. Smith?Carnegie Mellon UniversityMark Johnson?
?Brown UniversityThis article studies the relationship between weighted context-free grammars (WCFGs), whereeach production is associated with a positive real-valued weight, and probabilistic context-freegrammars (PCFGs), where the weights of the productions associated with a nonterminal areconstrained to sum to one.
Because the class of WCFGs properly includes the PCFGs, onemight expect that WCFGs can describe distributions that PCFGs cannot.
However, Z. Chi(1999, Computational Linguistics, 25(1):131?160) and S. P. Abney, D. A. McAllester, andP.
Pereira (1999, In Proceedings of the 37th Annual Meeting of the Association for Computa-tional Linguistics, pages 542?549, College Park, MD) proved that every WCFG distributionis equivalent to some PCFG distribution.
We extend their results to conditional distributions,and show that every WCFG conditional distribution of parses given strings is also the condi-tional distribution defined by some PCFG, even when the WCFG?s partition function diverges.This shows that any parsing or labeling accuracy improvement from conditional estimation ofWCFGs or conditional random fields (CRFs) over joint estimation of PCFGs or hidden Markovmodels (HMMs) is due to the estimation procedure rather than the change in model class,because PCFGs and HMMs are exactly as expressive as WCFGs and chain-structured CRFs,respectively.1.
IntroductionIn recent years the field of computational linguistics has turned to machine learningto aid in the development of accurate tools for language processing.
A widely usedexample, applied to parsing and tagging tasks of various kinds, is a weighted grammar.Adding weights to a formal grammar allows disambiguation (more generally, rankingof analyses) and can lead to more efficient parsing.
Machine learning comes in when wewish to choose those weights empirically.The predominant approach for many years was to select a probabilistic model?such as a hidden Markov model (HMM) or probabilistic context-free grammar(PCFG)?that defined a distribution over the structures allowed by a grammar.
Given a?
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15217, USA.E-mail: nasmith@cs.cmu.edu.??
Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912, USA.E-mail: Mark Johnson@brown.edu.Submission received: 30 November 2005; revised submission received: 11 January 2007; accepted forpublication: 30 March 2007.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 4treebank, maximum likelihood estimation can be applied to learn the probability valuesin the model.More recently, new machine learning methods have been developed or ex-tended to handle models of grammatical structure.
Notably, conditional estimation(Ratnaparkhi, Roukos, and Ward 1994; Johnson et al 1999; Lafferty, McCallum, andPereira 2001), maximum margin estimation (Taskar et al 2004), and unsupervised con-trastive estimation (Smith and Eisner 2005) have been applied to structured models.Weighted grammars learned in this way differ in two important ways from traditional,generative models.
First, the weights can be any positive value; they need not sumto one.
Second, features can ?overlap,?
and it can be difficult to design a generativemodel that uses such features.
The benefits of new features and discriminative trainingmethods are widely documented and recognized.This article focuses specifically on the first of these differences.
It compares theexpressive power of weighted context-free grammars (WCFGs), where each rule isassociated with a positive weight, to that of the corresponding PCFGs, that is, withthe same rules but where the weights of the rules expanding a nonterminal must sumto one.One might expect that because normalization removes one or more degrees of free-dom, unnormalized models should be more expressive than normalized, probabilisticmodels.
Perhaps counterintuitively, previous work has shown that the classes of proba-bility distributions defined by WCFGs and PCFGs are the same (Abney, McAllester, andPereira 1999; Chi 1999).However, this result does not completely settle the question about the expressivepower of WCFGs and PCFGs.
As we show herein, a WCFG can define a conditionaldistribution from strings to trees even if it does not define a probability distributionover trees.
Because these conditional distributions are what are used in classificationtasks and related tasks such as parsing, we need to know the relationship betweenthe classes of conditional distributions defined by WCFGs and PCFGs.
In fact weextend the results of Chi and of Abney et al, and show that WCFGs and PCFGs bothdefine the same class of conditional distribution.
Moreover, we present an algorithmfor converting an arbitrary WCFG that defines a conditional distribution over treesgiven strings but possibly without a finite partition function into a PCFG with thesame rules as the WCFG and that defines the same conditional distribution over treesgiven strings.This means that maximum conditional likelihood WCFGs are non-identifiable, be-cause there are an infinite number of rule weights all of which maximize the conditionallikelihood.2.
Weighted CFGsA CFG G is a tuple ?N, S,?, R?
where N is a finite set of nonterminal symbols, S ?
N isthe start symbol, ?
is a finite set of terminal symbols (disjoint from N), and R is a set ofproduction rules of the form X ?
?
where X ?
N and ?
?
(N ?
?).
A WCFG associatesa positive number called the weight with each rule in R.1 We denote by ?X??
the weightattached to the rule X ?
?, and the vector of rule weights by ?
= {?A??
: A ?
?
?
R}.A weighted grammar is the pair G?
= ?G,?
?.1 Assigning a weight of zero to a rule equates to excluding it from R.478Smith and Johnson Weighted and Probabilistic CFGsUnless otherwise specified, we assume a fixed underlying context-free grammar G.Let ?
(G) be the set of (finite) trees that G generates.
For any ?
?
?
(G), the score s?(?)
of?
is defined as follows:s?(?)
=?(X??)?R(?X??
)f (X??;?)
(1)where f (X ?
?
; ?)
is the number of times X ?
?
is used in the derivation of the tree ?.The partition function Z(?)
is the sum of the scores of the trees in ?(G).Z(?)
=????(G)s?(?
)Because we have imposed no constraints on?, the partition function need not equalone; indeed, as we show subsequently the partition function need not even exist.
If Z(?
)is finite then we say that the WCFG is convergent, and we can define a Gibbs probabilitydistribution over ?
(G) by dividing by Z(?):P?(?)
=s?(?)Z(?
)A probabilistic CFG, or PCFG, is a WCFG in which the sum of the weights of therules expanding each nonterminal is one:?X ?
N,?(X??)?R?X??
= 1 (2)It is easy to show that if G?
is a PCFG then Z(?)
?
1.
A tight PCFG is a PCFG G?for which Z(?)
= 1.
Necessary conditions and sufficient conditions for a PCFG to betight are given in several places, including Booth and Thompson (1973) and Wetherell(1980).We now describe the results of Chi (1999) and Abney, McAllester, and Pereira (1999).Let G = {G?}
denote the set of the WCFGs based on the CFG G (i.e., the WCFGs in Gall have the same underlying grammar G but differ in their rule weight vectors ?
).Let GZ<?
be the subset of G for which the partition function Z(?)
is finite, and letGZ=?
= G \ GZ<?
be the subset of G with an infinite partition function.
Further let GPCFGdenote the set of PCFGs based on G. In general, GPCFG is a proper subset of GZ<?, thatis, every PCFG is also a WCFG, but because there are weight vectors ?
that don?t obeyEquation 2, not all WCFGs are PCFGs.However, this does not mean that WCFGs are more expressive than PCFGs.
Asnoted above, the WCFGs GZ<?
define Gibbs distributions.
Again, for a fixed G, letPZ<?
be the probability distributions over the trees ?
(G) defined by the WCFGs GZ<?and let PPCFG be the probability distributions defined by the PCFGs GPCFG.
Chi (Propo-sition 4) and Abney, McAllester, and Pereira (Lemma 5) showed that PZ<?
= PPCFG,namely, that every WCFG probability distribution is in fact generated by some PCFG.There is no ?PZ=??
because there is no finite normalizing term Z(?)
for such WCFGs.479Computational Linguistics Volume 33, Number 42.1 Chi?s Algorithm for Converting WCFGs to Equivalent PCFGsChi (1999) describes an algorithm for converting a WCFG to an equivalent PCFG.
LetG?
be a WCFG in GZ<?.
If X ?
N is a nonterminal, let ?X(G) be the set of trees rootedin X that can be built using G. Then define:ZX(?)
=???
?X (G)s?(?
)For simplicity, let Zt(?)
= 1 for all t ?
?.
Chi demonstrated that G?
?
GZ<?
impliesthat ZX(?)
is finite for all X ?
N ?
?.For every rule X ?
?
in R define:??X??
=?X?
?|?|?i=1Z?i (?)ZX(?
)where ?i is the ith element of ?
and |?| is the length of ?.
Chi proved that G??
is a PCFGand that P??
(?)
= s?(?)/Z(?)
for all trees ?
?
?
(G).Chi did not describe how to compute the nonterminal-specific partition functionsZX(?).
The ZX(?)
are related by equations of the formZX(?)
=??:X???R?X?
?|?|?i=1Z?i (?
)which constitute a set of nonlinear polynomial equations in ZX(?).
Although a numeri-cal solver might be employed to find the ZX(?
), we have found that in practice iterativepropagation of weights following the method described by Stolcke (1995, Section 4.7.1)converges quickly when Z(?)
is finite.3.
Classifiers and Conditional DistributionsA common application of weighted grammars is parsing.
One way to select a parse treefor a sentence x is to choose the maximum weighted parse that is consistent with theobservation x:??
(x) = argmax???(G):y(?)=xs?(?)
(3)where y(?)
is the yield of ?.
Other decision criteria exist, including minimum-loss de-coding and re-ranked n-best decoding.
All of these classifiers use some kind of dynamicprogramming algorithm to optimize over trees, and they also exploit the conditionaldistribution of trees given sentence observations.
A WCFG defines such a conditionaldistribution as follows:P?(?
| x) =s?(?)?????(G):y(??
)=x s?(??)=s?(?)Zx(?
)(4)480Smith and Johnson Weighted and Probabilistic CFGswhere Zx(?)
is the sum of scores for all parses of x.
Note that Equation (4) will beill-defined when Zx(?)
diverges.
Because Zx(?)
is constant for a given x, solving Equa-tion (3) is equivalent to choosing ?
to maximize P?(?
| x).We turn now to classes of these conditional distribution families.
Let CZ<?
(CPCFG)be the class of conditional distribution families that can be expressed by grammars inGZ<?
(GPCFG, respectively).
It should be clear that, because PZ<?
= PPCFG, CZ<?
= CPCFGsince a conditional family is derived by normalizing a joint distribution by its marginals.We now define another subset of G. Let GZn<?
contain every WCFG G?
= ?G,?
?such that, for all n ?
0,Zn(?)
=????(G):|y(?)|=ns?(?)
< ?
(5)(Note that, to be fully rigorous, we should quantify n in GZn<?, writing ?G?nZn(?)<?.
?We use the abbreviated form to keep the notation crisp.)
For any G?
?
GZn<?, it alsofollows that, for any x ?
L(G), Zx(?)
< ?
; the converse holds as well.It follows that any WCFG in GZn<?
can be used to construct a conditional dis-tribution of trees given the sentence, for any sentence x ?
L(G).
To do so, we onlyneed to normalize s?(?)
by Zx(?)
(Equation (4)).
Let GZn=?
contain the WCFGs wheresome Zn(?)
diverge; this is a subset of GZ=?.2 To see that GZ=?
?
GZn<?
= ?, considerExample 1.Example 1?A?A A = 1, ?A?a = 1This grammar produces binary structures over strings in a+.
Every such tree receivesscore 1.
Because there are infinitely many trees, Z(?)
diverges.
But for any fixed stringan, the number of parse trees is finite.
This grammar defines a uniform conditionaldistribution over all binary trees, given the string.For a grammar G?
to be in GZn<?, it is sufficient that, for every nonterminal X ?
N,the sum of scores of all cyclic derivations X ?+ X be finite.
Conservatively, this can beforced by eliminating epsilon rules and unary rules or cycles altogether, or by requiringthe sum of cyclic derivations for every nonterminal X to sum to strictly less than one.Example 2 gives a grammar in GZn=?
with a unary cyclic derivation that does not?dampen.
?Example 2?A?A A = 1, ?A?A = 1, ?A?a = 1For any given an, there are infinitely many equally weighted parse trees, so even theset of trees for an cannot be normalized into a distribution (Zn(?)
=?).
Generallyspeaking, if there exists a string x ?
L(G) such that the set of trees that derive x is not2 Here, full rigor would require quantification of n, writing ?G?nZn (?)=?.
?481Computational Linguistics Volume 33, Number 4finite (i.e., there is no finite bound on the number of derivations for strings in L(G); thegrammar in Example 2 is a simple example), then GZn<?
and GZ<?
are separable.3For a given CFG G, a conditional distribution over trees given strings is a function??
?
(?
(G) ?
[0, 1]).
Our notation for the set of conditional distributions that can beexpressed by GZn<?
is CZn<?.
Note that there is no ?CZn=??
because an infinite Zn(?
)implies an infinite Z(x) for some sentence x and therefore an ill-formed conditional fam-ily.
Indeed, it is difficult to imagine a scenario in computational linguistics in which non-dampening cyclic derivations (WCFGs in GZn=?)
are desirable, because no linguisticexplanations depend crucially on arbitrary lengthening of cyclic derivations.We now state our main theorem.Theorem 1For a given CFG G, CZn<?
= CZ<?.ProofSuppose we are given weights ?
for G such that G?
?
GZn<?.
We will show that thesequence Z1(?
), Z2(?
), ... is bounded by an exponential function of n, then describea transformation on ?
resulting in a new grammar G??
that is in GZ<?
and definesthe same family of conditional distributions (i.e., ??
?
?
(G),?x ?
L(G), P?(?
| x) =P??
(?
| x)).First we prove that for all n ?
1 there exists some c such that Zn(?)
?
cn.
Given G?,we construct G???
in CNF that preserves the total score for any x ?
L(G).
The existenceof G???
was demonstrated by Goodman (1998, Section 2.6), who gives an algorithm forconstructing the value-preserving weighted grammar G???
from G?.Note that G?
= ?N?, S,?, R?
?, containing possibly more nonterminals and rules than G.The set of (finite) trees ?(G?
)is different from ?
(G); the new trees must be binary andmay include new nonterminals.Next, collapse the nonterminals in N?
into one nonterminal, S. The resulting gram-mar is G???
= ??
{S}, S,?, R?
?, ???.
R?
contains the rule S ?
S S and rules of the form S ?
afor a ?
?.
The weights of these rules are?
?S?S S = ?
= max(1,?
(X?Y Z)?R??
?X?Y Z) (6)?
?S?a = ?
= max(1,?(X?b)?R??
?X?b) (7)The grammar G???
will allow every tree allowed by G???
(modulo labels on nonterminalnodes, which are now all S).
It may allow some additional trees.
The score of a treeunder G???
will be at least as great as the sum of scores of all structurally equivalent treesunder G??
?, because ?
and ?
are defined to be large enough to absorb all such scores.
Itfollows that, for all x ?
L(G):s??
(x) ?
s??
(x) = s?
(x) (8)3 We are grateful to an anonymous reviewer for pointing this out, and an even stronger point: for a givenG, G and GZn<?
have a nonempty set-difference if and only if G has infinite ambiguity (some x ?
L(G)has infinitely many parse trees).482Smith and Johnson Weighted and Probabilistic CFGsSumming over all trees of any given yield length n, we haveZn(??)
?
Zn(??)
= Zn(?)
(9)G?
generates all possible binary trees (with internal nodes undifferentiated) overa given sentence x in L(G).
Every tree generated by G?
with yield length n will havethe same score: ?n?1?n, because every binary tree with n terminals has exactly n ?
1nonterminals.
Each tree corresponds to a way of bracketing n items, so the total numberof parse trees generated by G?
for a string of length n is the number of different waysof bracketing a sequence of n items.
The total number of unlabeled binary bracketingsof an n-length sequence is the nth Catalan number Cn (Graham, Knuth, and Patashnik1994), which in turn is bounded above by 4n (Vardi 1991).
The total number of strings oflength n is |?|n.
ThereforeZn(??)
= Cn|?|n?n?1?n ?
4n|?|n?n?1?n ?
(4|?|??
)n (10)We now transform the original weights ?
as follows.
For every rule (X ?
?)
?
R,let??X??
??X??(8|?|??)t(?)
(11)where t(?)
is the number of ?
symbols appearing in ?.
This transformation results inevery n-length sentence having its score divided by (8|?|??)n.
The relative scores oftrees with the same yield are unaffected, because they are all scaled equally.
ThereforeG??
defines the same conditional distribution over trees given sentences as G?, whichimplies that G?
and G??
have the same highest scoring parses.
Note that any sufficientlylarge value could stand in for 8|?|??
to both (a) preserve the conditional distributionand (b) force Zn(?)
to converge.
We have not found the minimum such value, but 8|?|?
?is sufficiently large.The sequence of Zn(?)
now converges:Zn(??)
?Zn(?)(8|?|??
)n ?
(12)n(12)Hence Z(??)
=?
?n=0 Zn(??)
?
2 and G??
?
GZ<?.
Corollary 1Given a CFG G, CZn<?
= CPCFG.ProofBy Theorem 1, CZn<?
= CZ<?.
We know that PZ<?
= PPCFG, from which it follows thatCZ<?
= CPCFG.
Hence CZn<?
= CPCFG.
To convert a WCFG in CZn<?
into a PCFG, firstapply the transformation in the proof of Theorem 1 to get a convergent WCFG, thenapply Chi?s method (our Section 2.1).
483Computational Linguistics Volume 33, Number 4Figure 1A graphical depiction of the primary result of this article.
Given a fixed set of productions, G isthe set of WCFGs with exactly those productions (i.e., they vary only in the production weights),GZ<?
is the subset of G that defines (joint) probability distributions over trees (i.e., that have afinite partition function Z) and PZ<?
is the set of probability distributions defined by grammarsin GZ<?.
Chi (1999) and Abney, McAllester, and Pereira (1999) proved that PZ<?
isthe same as PPCFG, the set of probability distributions defined by the PCFG GPCFG with the sameproductions as G. Thus even though the set of WCFGs properly includes the set of PCFGs,WCFGs define exactly the same probability distributions over trees as PCFGs.
This articleextends these results to conditional distributions over trees conditioned on their strings.
Eventhough the set GZn<?
of WCFGs that define conditional distributions may be larger than GZ<?and properly includes GPCFG, the set of conditional distributions CZn<?
defined by GZn<?
isequal to the set of conditional distributions CPCFG defined by PCFGs.
Our proof is constructive:we give an algorithm which takes as input a WCFG G ?
GZn<?
and returns a PCFG whichdefines the same conditional distribution over trees given strings as G.Figure 1 presents the main result graphically in the context of earlier results.4.
HMMs and Related ModelsHidden Markov models (HMMs) are a special case of PCFGs.
The structures theyproduce are labeled sequences, which are equivalent to right-branching trees.
We canwrite an HMM as a PCFG with restricted types of rules.
We will refer to the unweighted,finite-state grammars that HMMs stochasticize as ?right-linear grammars.?
Rather thanusing the production rule notation of PCFGs, we will use more traditional HMM nota-tion and refer to states (interchangeable with nonterminals) and paths (interchangeablewith parse trees).In the rest of the article we distinguish between HMMs, which are probabilisticfinite-state automata locally normalized just like a PCFG, and chain-structured Markovrandom fields (MRFs; Section 4.1), in which moves or transitions are associated withpositive weights and which are globally normalized like a WCFG.4 We also distinguishtwo different types of dependency structures in these automata.
Abusing the standardterminology somewhat, in a Mealy automaton arcs are labeled with output or terminalsymbols, whereas in a Moore automaton the states emit terminal symbols.54 We admit that these names are somewhat misleading, because as we will show, chain-structured MRFsalso have the Markov property and define the same joint and conditional distributions as HMMs.5 In formal language theory both Mealy and Moore machines are finite-state transducers (Mealy 1955;Moore 1956); we ignore the input symbols here.484Smith and Johnson Weighted and Probabilistic CFGsA Mealy HMM defines a probability distribution over pairs ?x,?
?, where x is alength-n sequence ?x1, x2, ..., xn?
?
?n and ?
= ?
?0,?1,?2, ...,?n?
?
Nn+1 is a state (ornonterminal) path.
The distribution is given byPHMM(x,?)
=(n?i=1p(xi,?i | ?i?1))p(STOP | ?n) (13)?0 is assumed, for simplicity, to be constant and known; we also assume that everystate transition emits a symbol (noarcs), an assumption made in typical tagging andchunking applications of HMMs.
We can convert a Mealy HMM to a PCFG by including,for every tuple ?x,?,??
(x ?
?
and ?,?
?
N) such that p(x,?
| ?)
> 0, the rule ?
?
x ?,with the same probability as the corresponding HMM transition.
For every ?
such thatp(STOP | ?
), we include the rule ?
?, with probability p(STOP | ?
).A Moore HMM factors the distribution p(x,?
| ?)
into p(x | ?)
?
p(?
| ?).
A MooreHMM can be converted to a PCFG by adding a new nonterminal ??
for every state ?and including the rules ?
?
??
(with probability p(?
| ?))
and ??
?
x ?
(with probabilityp(x | ?)).
Stop probabilities are added as in the Mealy case.
For a fixed number of states,Moore HMMs are less probabilistically expressive than Mealy HMMs, though we canconvert between the two with a change in the number of states.We consider Mealy HMMs primarily from here on.
If we wish to define the distri-bution over paths given words, we conditionalizePHMM(?
| x) =(?ni=1 p(xi,?i | ?i?1))p(STOP | ?n)???
?Nn+1(?ni=1 p(xi,?
?i | ?
?i?1))p(STOP | ?
?n)(14)This is how scores are assigned when selecting the best path given a sequence.For a grammar G that is right-linear, we can therefore talk about the set of HMM(right-linear) grammars GHMM, the set of probability distributions PHMM defined by thosegrammars, and CHMM, the set of conditional distributions over state paths (trees) that theydefine.64.1 Mealy Markov Random FieldsWhen the probabilities in Mealy HMMs are replaced by arbitrary positive weights, theproduction rules can be seen as features in a Gibbs distribution.
The resulting modelis a type of MRF with a chain structure; these have recently become popular in naturallanguage processing (Lafferty, McCallum, and Pereira 2001).
Lafferty et al?s formulationdefined a conditional distribution over paths given sequences by normalizing for eachsequence x:PCMRF(?
| x) =(n?i=1??i?1,xi,?i)??n,STOPZx(?
)(15)6 Of course, the right-linear grammar is a CFG, so we could also use the notation GPCFG, PPCFG, and CPCFG.485Computational Linguistics Volume 33, Number 4Using a single normalizing term Z(?
), we can also define a joint distribution overstates and paths:PCMRF(x,?)
=(n?i=1??i?1,xi,?i)??n,STOPZ(?
)(16)Let G = {G?}
denote the set of weighted grammars based on the unweighted right-linear grammar G. We call these weighted grammars ?Mealy MRFs.?
As in the WCFGcase, we can add the constraint Zn(?)
< ?
(for all n), giving the class GZn<?.Recall that, in the WCFG case, the move from G to GZn<?
had to do with cyclicderivations.
The analogous move in the right-linear grammar case involvesemis-sions (production rules of the form X ?
Y).
If, as in typical applications of finite-statemodels to natural language processing, there are no rules of the form X ?
Y, thenGZn<?
is empty and GZn<?
= G. Our formulae, in fact, assume that there are noemissions.Because Mealy MRFs are a special case of WCFGs, Theorem 1 applies to them.This means that any random field using Mealy HMM features (Mealy MRF) such that?n, Zn(?)
< ?
can be transformed into a Mealy HMM that defines the same conditionaldistribution of tags given words.7Corollary 2For a given right-linear grammar G, CHMM = CZ<?
= CZn<?.Lafferty, McCallum, and Pereira?s conditional random fields are typically trained tooptimize a different objective function than HMMs (conditional likelihood and jointlikelihood, respectively).
Our result shows that optimizing either objective on the setof Mealy HMMs as opposed to Mealy MRFs will achieve the same result, moduloimperfections in the numerical search for parameter values.4.2 Maximum-Entropy Markov ModelsWhile HMMs and chain MRFs represent the same set of conditional distributions, wecan show that the maximum-entropy Markov models (MEMMs) of McCallum, Freitag,and Pereira (2000) represent a strictly smaller class of distributions.An MEMM is a similar model with a different event structure.
It defines the distri-bution over paths given words as:PMEMM(?
| x) =n?i=1p(?i | ?i?1, xi) (17)Unlike an HMM, the MEMM does not define a distribution over output sequences x.The name ?maximum entropy Markov model?
comes from the fact that the conditional7 What if we allow additional features?
It can be shown that, as long as the vocabulary ?
is finite andknown, we can convert any such MRF with potential functions on state transitions and emissions intoan HMM functioning equivalently as a classifier.
If ?
is not fully known, then we cannot sum over allemissions from each state, and we cannot use Chi?s method (Section 2.1) to convert to a PCFG (HMM).486Smith and Johnson Weighted and Probabilistic CFGsdistributions p(?
| ?, x) typically have a log-linear form, rather than a multinomial form,and are trained to maximize entropy.Lemma 1For every MEMM, there is a Mealy MRF that represents the same conditional distribu-tion over paths given symbols.ProofBy definition, the features of the MRF include triples ?
?i?1, xi,?i?.
Assign to theweight ?
?i,xj,?k the value PMEMM(?i | ?k, xj).
Assign to ?
?i,STOP the value 1.
In computingPCMRF(?
| x) (Equation (15)), the normalizing term for each x will be equal to 1.
MEMMs, like HMMs, are defined by locally normalized conditional multinomialdistributions.
This has computational advantages (no potentially infinite Z(?)
terms tocompute).
However, the set of conditional distributions of labels given terminals thatcan be expressed by MEMMs is strictly smaller than those expressible by HMMs (andby extension, Mealy MRFs).Theorem 2For a given right-linear grammar G, CMEMM ?
CHMM.ProofWe give an example of a Mealy HMM whose conditional distribution over paths (trees)given sentences cannot be represented by an MEMM.
We thank Michael Collins forpointing out to us the existence of examples like this one.
Define a Mealy HMM withthree states named 0, 1, and 2, over an alphabet {a, b, c}, as follows.
State 0 is the startstate.Example 3Under this model, PHMM(0, 1, 1 | a, b) = PHMM(0, 2, 2 | a, c) = 1.
These conditional dis-tributions cannot both be met by any MEMM.
To see why, considerp(1 | 0, a) ?
p(1 | 1, b) = p(2 | 0, a) ?
p(2 | 2, c) = 1This implies thatp(1 | 0, a) = p(1 | 1, b) = p(2 | 0, a) = p(2 | 2, c) = 1487Computational Linguistics Volume 33, Number 4But it is impossible for p(1 | 0, a) = p(2 | 0, a) = 1.
This holds regardless of the form ofthe distribution p(?
| ?, x) (e.g., multinomial or log-linear).Because P(0, 1, 1 | a, b) = P(0, 2, 2 | a, c) cannot be met by any MEMM, there aredistributions in the family allowed by HMMs that cannot be expressed as MEMMs,and the latter are less expressive.
It is important to note that this result applies to Mealy HMMs; our result comparesmodels with the same dependencies among random variables.
If the HMM?s distribu-tion p(xi,?i | ?i?1) is factored into p(xi | ?i) ?
p(?i | ?i?1) (i.e., it is a Moore HMM), thenthere may exist an MEMM with the same number of states that can represent somedistributions that the Moore HMM cannot.8One can also imagine MEMMs in which p(?i | ?i?1, xi, ...) is conditioned on moresurrounding context (xi?1 or xi+1, or the entire sequence x, for example).
Conditioningon more context can be done by increasing the order of the Markov model?all ofour models so far have been first-order, with a memory of only the previous state.Our result can be extended to include higher-order MEMMs.
Suppose we allow theMEMM to ?look ahead?
n words, factoring its distribution into p(?i | ?i?1, xi, xi+1, ...,xi+n).Corollary 3A first-order Mealy HMM can represent some classifiers that no MEMM with finitelookahead can represent.ProofConsider again Example 3.
Note that, for all m ?
1, it setsPHMM(0,m 1?s?
??
?1, ..., 1 | amb) = 1PHMM(0, 2, ..., 2?
??
?m 2?s| amc) = 1Suppose we wish to capture this in an MEMM with n symbols of look-ahead.
Lettingm = n + 1,p(1 | 0, an+1) ?
p(1 | 1, anb) ?n?i=1p(1 | 1, an?ib) = 1p(2 | 0, an+1) ?
p(2 | 2, anc) ?n?i=1p(2 | 2, an?ic) = 1The same issue arises as in the proof of Theorem 2: it cannot be that p(1 | 0, an+1) =p(2 | 0, an+1) = 1, and so this MEMM does not exist.
Note that even if we allow the8 The HMM shown in Example 3 can be factored into a Moore HMM without any change to thedistribution.488Smith and Johnson Weighted and Probabilistic CFGsMEMM to ?look back?
and condition on earlier symbols (or states), it cannot representthe distribution in Example 3.
Generally speaking, this limitation of MEMMs has nothing to do with the estima-tion procedure (we have committed to no estimation procedure in particular) but ratherwith the conditional structure of the model.
That some model structures work betterthan others at real NLP tasks was discussed by Johnson (2001) and Klein and Manning(2002).
Our result?that the class of distributions allowed by MEMMs is a strict subsetof those allowed by Mealy HMMs?makes this unsurprising.5.
Practical ImplicationsOur result is that weighted generalizations of classical probabilistic grammars (PCFGsand HMMs) are no more powerful than the probabilistic models.
This means that, inso-far as log-linear models for NLP tasks like tagging and parsing are more successfulthan their probabilistic cousins, it is due to either (a) additional features added tothe model, (b) improved estimation procedures (e.g., maximum conditional likelihoodestimation or contrastive estimation), or both.
(Note that the choice of estimation proce-dure (b) is in principle orthogonal to the choice of model, and conditional estimationshould not be conflated with log-linear modeling.)
For a given estimation criterion,weighted CFGs, and Mealy MRFs, in particular, cannot be expected to behave anydifferently than PCFGs and HMMs, respectively, unless they are augmented with morefeatures.6.
Related WorkAbney, McAllester, and Pereira (1999) addressed the relationship between PCFGs andprobabilistic models based on push-down automaton operations (e.g., the structuredlanguage model of Chelba and Jelinek, 1998).
They proved that, although the conversionmay not be simple (indeed, a blow-up in the automaton?s size may be incurred), givenG, PPCFG and the set of distributions expressible by shift-reduce probabilistic push-downautomata are weakly equivalent.
Importantly, the standard conversion of a CFG into ashift-reduce PDA, when applied in the stochastic case, does not always preserve the prob-ability distribution over trees.
Our Theorem 2 bears a resemblance to that result.
Furtherwork on the relationship between weighted CFGs and weighted PDAs is described inNederhof and Satta (2004).MacKay (1996) proved that linear Boltzmann chains (a class of weighted modelsthat is essentially the same as Moore MRFs) express the same set of distributions asMoore HMMs, under the condition that the Boltzmann chain has a single specific endstate.
MacKay avoided the divergence problem by defining the Boltzmann chain alwaysto condition on the length of the sequence; he tacitly requires all of his models to be inGZn<?.
We have suggested a more applicable notion of model equivalence (equivalenceof the conditional distribution) and our Theorem 1 generalizes to context-free models.7.
ConclusionWe have shown that weighted CFGs that define finite scores for all sentences in theirlanguages have no greater expressivity than PCFGs, when used to define distributions489Computational Linguistics Volume 33, Number 4over trees given sentences.
This implies that the standard Mealy MRF formalism isno more powerful than Mealy HMMs, for instance.
We have also related ?maximumentropy Markov models?
to Mealy Markov random fields, showing that the former is astrictly less expressive weighted formalism.AcknowledgmentsThis work was supported by a Fannie andJohn Hertz Foundation fellowship toN.
Smith at Johns Hopkins University.
Theviews expressed are not necessarily endorsedby the sponsors.
We are grateful to threeanonymous reviewers for feedback thatimproved the article, to Michael Collins forencouraging exploration of this matter andhelpful comments on a draft, and to JasonEisner and Dan Klein for insightfulconversations.
Any errors are the soleresponsibility of the authors.ReferencesAbney, Steven P., David A. McAllester,and Fernando Pereira.
1999.
Relatingprobabilistic grammars and automata.In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics,pages 542?549, College Park, MD.Booth, Taylor L. and Richard A. Thompson.1973.
Applying probability measures toabstract languages.
IEEE Transactions onComputers, 22(5):442?450.Chelba, Ciprian and Frederick Jelinek.1998.
Exploiting syntactic structure forlanguage modeling.
In Proceedingsof the 36th Annual Meeting of theAssociation for Computational Linguisticsand 17th International Conference onComputational Linguistics, pages 325?331,Montreal, Canada.Chi, Zhiyi.
1999.
Statistical properties ofprobabilistic context-free grammars.Computational Linguistics, 25(1):131?160.Goodman, Joshua T. 1998.
Parsing Inside-Out.Ph.D.
thesis, Harvard University,Cambridge, MA.Graham, Ronald L., Donald E. Knuth,and Oren Patashnik.
1994.
ConcreteMathematics.
Addison-Wesley,Reading, MA.Johnson, Mark.
2001.
Joint and conditionalestimation of tagging and parsing models.In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics,pages 314?321, Toulouse, France.Johnson, Mark, Stuart Geman, StephenCanon, Zhiyi Chi, and Stefan Riezler.1999.
Estimators for stochastic?unification-based?
grammars.In Proceedings of the 37th AnnualConference of the Association forComputational Linguistics, pages 535?541,College Park, MD.Klein, Dan and Christopher D. Manning.2002.
Conditional structure versusconditional estimation in NLP models.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing,pages 9?16, Philadelphia, PA.Lafferty, John, Andrew McCallum, andFernando Pereira.
2001.
Conditionalrandom fields: Probabilistic models forsegmenting and labeling sequence data.In Proceedings of the 18th InternationalConference on Machine Learning,pages 282?289, Williamstown, MA.MacKay, David J. C. 1996.
Equivalence oflinear Boltzmann chains and hiddenMarkov models.
Neural Computation,8(1):178?181.McCallum, Andrew, Dayne Freitag, andFernando Pereira.
2000.
Maximumentropy Markov models for informationextraction and segmentation.
InProceedings of the 17th InternationalConference on Machine Learning,pages 591?598, Palo Alto, CA.Mealy, G. H. 1955.
A method forsynthesizing sequential circuits.Bell System Technology Journal,34:1045?1079.Moore, Edward F. 1956.
Gedanken-experiments on sequential machines.In Automata Studies, number 34 inAnnals of Mathematics Studies.Princeton University Press, Princeton,NJ, pages 129?153.Nederhof, Mark-Jan and Giorgio Satta.2004.
Probabilistic parsing strategies.In Proceedings of the 42nd AnnualMeeting of the Association forComputational Linguistics, pages 543?550,Barcelona, Spain.Ratnaparkhi, Adwait, Salim Roukos,and R. Todd Ward.
1994.
A maximumentropy model for parsing.
In Proceedingsof the International Conference on SpokenLanguage Processing, pages 803?806,Yokohama, Japan.Smith, Noah A. and Jason Eisner.
2005.Contrastive estimation: Traininglog-linear models on unlabeled data.490Smith and Johnson Weighted and Probabilistic CFGsIn Proceedings of the 43rd Annual Meetingof the Association for ComputationalLinguistics, pages 354?362, Ann Arbor, MI.Stolcke, Andreas.
1995.
An efficientprobabilistic context-free parsingalgorithm that computes prefixprobabilities.
Computational Linguistics,21(2):165?201.Taskar, Ben, Dan Klein, Michael Collins,Daphne Koller, and Christopher Manning.2004.
Max-margin parsing.
In Proceedingsof the Conference on Empirical Methods inNatural Language Processing, pages 1?8,Barcelona, Spain.Vardi, Ilan.
1991.
Computational Recreations inMathematica.
Addison-Wesley, RedwoodCity, CA.Wetherell, C. S. 1980.
Probabilistic languages:A review and some open questions.Computing Surveys, 12:361?379.491
