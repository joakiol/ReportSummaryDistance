Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 144?151, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsInvestigating the Effects of Selective Sampling on the Annotation TaskBen Hachey, Beatrice Alex and Markus BeckerSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9LW, UK{bhachey,v1balex,s0235256}@inf.ed.ac.ukAbstractWe report on an active learning experi-ment for named entity recognition in theastronomy domain.
Active learning hasbeen shown to reduce the amount of la-belled data required to train a supervisedlearner by selectively sampling more in-formative data points for human annota-tion.
We inspect double annotation datafrom the same domain and quantify poten-tial problems concerning annotators?
per-formance.
For data selectively sampledaccording to different selection metrics,we find lower inter-annotator agreementand higher per token annotation times.However, overall results confirm the util-ity of active learning.1 IntroductionSupervised training of named entity recognition(NER) systems requires large amounts of manuallyannotated data.
However, human annotation is typ-ically costly and time-consuming.
Active learn-ing promises to reduce this cost by requesting onlythose data points for human annotation which arehighly informative.
Example informativity can beestimated by the degree of uncertainty of a singlelearner as to the correct label of a data point (Cohnet al, 1995) or in terms of the disagreement of acommittee of learners (Seung et al, 1992).
Ac-tive learning has been successfully applied to a va-riety of tasks such as document classification (Mc-Callum and Nigam, 1998), part-of-speech tagging(Argamon-Engelson and Dagan, 1999), and parsing(Thompson et al, 1999).We employ a committee-based method where thedegree of deviation of different classifiers with re-spect to their analysis can tell us if an example ispotentially useful.
In a companion paper (Becker etal., 2005), we present active learning experimentsfor NER in radio-astronomical texts following thisapproach.1 These experiments prove the utility ofselective sampling and suggest that parameters for anew domain can be optimised in another domain forwhich annotated data is already available.However there are some provisos for active learn-ing.
An important point to consider is what effectinformative examples have on the annotators.
Arethese examples more difficult?
Will they affect theannotators?
performance in terms of accuracy?
Willthey affect the annotators performance in terms oftime?
In this paper, we explore these questions us-ing doubly annotated data.
We find that selectivesampling does have an adverse effect on annotatoraccuracy and efficiency.In section 2, we present standard active learn-ing results showing that good performance can beachieved using fewer examples than random sam-pling.
Then, in section 3, we address the questionsabove, looking at the relationship between inter-annotator agreement and annotation time and the ex-amples that are selected by active learning.
Finally,section 4 presents conclusions and future work.1Please refer to the companion paper for details of theselective sampling approach with experimental adaptation re-sults as well as more information about the corpus of radio-astronomical abstracts.1442 Bootstrapping NERThe work reported here was carried out in order toassess methods of porting a statistical NER system toa new domain.
We started with a NER system trainedon biomedical literature and built a new system toidentify four novel entities in abstracts from astron-omy articles.
This section introduces the AstronomyBootstrapping Corpus (ABC) which was developedfor the task, describes our active learning approachto bootstrapping, and gives a brief overview of theexperiments.2.1 The Astronomy Bootstrapping CorpusThe ABC corpus consists of abstracts of radio astro-nomical papers from the NASA Astrophysics DataSystem archive2, a digital library for physics, as-trophysics, and instrumentation.
Abstracts were ex-tracted from the years 1997-2003 that matched thequery ?quasar AND line?.
A set of 50 abstractsfrom the year 2002 were annotated as seed mate-rial and 159 abstracts from 2003 were annotated astesting material.
A further 778 abstracts from theyears 1997-2001 were provided as an unannotatedpool for bootstrapping.
On average, these abstractscontain 10 sentences with a length of 30 tokens.
Theannotation marks up four entity types:Instrument-name (IN) Names of telescopes andother measurement instruments, e.g.
Superconduct-ing Tunnel Junction (STJ) camera, Plateau de BureInterferometer, Chandra, XMM-Newton ReflectionGrating Spectrometer (RGS), Hubble Space Tele-scope.Source-name (SN) Names of celestial objects,e.g.
NGC 7603, 3C 273, BRI 1335-0417, SDSSpJ104433.04-012502.2, PC0953+ 4749.Source-type (ST) Types of objects, e.g.
Type II Su-pernovae (SNe II), radio-loud quasar, type 2 QSO,starburst galaxies, low-luminosity AGNs.Spectral-feature (SF) Features that can bepointed to on a spectrum, e.g.
Mg II emission, broademission lines, radio continuum emission at 1.47GHz, CO ladder from (2-1) up to (7-6), non-LTEline.2http://adsabs.harvard.edu/preprint_service.htmlThe seed and test data sets were annotated by twoastrophysics PhD students.
In addition, they anno-tated 1000 randomly sampled sentences from thepool to provide a random baseline for active learn-ing.
These sentences were doubly annotated and ad-judicated and form the basis for our calculations insection 3.2.2 Inter-Annotator AgreementIn order to ensure consistency in annotation projects,corpora are often annotated by more than one an-notator, e.g.
in the annotation of the Penn Treebank(Marcus et al, 1994).
In these cases, inter-annotatoragreement is frequently reported between differentannotated versions of a corpus as an indicator forthe difficulty of the annotation task.
For example,Brants (2000) reports inter-annotator agreement interms of accuracy and f-score for the annotation ofthe German NEGRA treebank.Evaluation metrics for named entity recognitionare standardly reported as accuracy on the tokenlevel, and as f-score on the phrasal level, e.g.Sang (2002), where token level annotation refers tothe B-I-O coding scheme.3 Likewise, we will useaccuracy to report inter-annotator agreement on thetoken level, and f-score for the phrase level.
Wemay arbitrarily assign one annotator?s data as thegold standard, since both accuracy and f-score aresymmetric with respect to the test and gold set.
Tosee why this is the case, note that accuracy can sim-ply be defined as the ratio of the number of tokenson which the annotators agree over the total numberof tokens.
Also the f-score is symmetric, since re-call(A,B) = precision(B,A) and (balanced) f-score isthe harmonic mean of recall and precision (Brants,2000).
The pairwise f-score for the ABC corpus is85.52 (accuracy of 97.15) with class information and86.15 (accuracy of 97.28) without class information.The results in later sections will be reported usingthis pairwise f-score for measuring agreement.For NER, it is also common to compare an anno-tator?s tagged document to the final, reconciled ver-sion of the document, e.g.
Robinson et al (1999)and Strassel et al (2003).
The inter-annotator f-score agreement calculated this way for MUC-7 andHub 4 was measured at 97 and 98 respectively.
The3B-X marks the beginning of a phrase of type X, I-X denotesthe continuation of an X phrase, and O a non-phrasal token.145doubly annotated data for the ABC corpus was re-solved by the original annotators in the presenceof an astronomy adjudicator (senior academic staff)and a computational linguist.
This approach givesan f-score of 91.89 (accuracy of 98.43) with classinformation for the ABC corpus.
Without class in-formation, we get an f-score of 92.22 (accuracy of98.49), indicating that most of our errors are due toboundary problems.
These numbers suggest that ourtask is more difficult than the generic NER tasks fromthe MUC and HUB evaluations.Another common agreement metric is the kappacoefficient which normalises token level accuracyby chance, e.g.
Carletta et al (1997).
This met-ric showed that the human annotators distinguishthe four categories with a reproducibility of K=.925(N=44775, k=2; where K is the kappa coefficient,N is the number of tokens and k is the number ofannotators).2.3 Active LearningWe have already mentioned that there are two mainapproaches in the literature to assessing the informa-tivity of an example: the degree of uncertainty of asingle learner and the disagreement between a com-mittee of learners.
For the current work, we employquery-by-committee (QBC).
We use a conditionalMarkov model (CMM) tagger (Klein et al, 2003;Finkel et al, 2005) to train two different models onthe same data by splitting the feature set.
In this sec-tion we discuss several parameters of this approachfor the current task.Level of annotation For the manual annotation ofnamed entity examples, we needed to decide on thelevel of granularity.
The question arises of what con-stitutes an example that will be submitted to the an-notators.
Possible levels include the document level,the sentence level and the token level.
The most fine-grained annotation would certainly be on the tokenlevel.
However, it seems unnatural for the annota-tor to label individual tokens.
Furthermore, our ma-chine learning tool models sequences at the sentencelevel and does not allow to mix unannotated tokenswith annotated ones.
At the other extreme, one maysubmit an entire document for annotation.
A possi-ble disadvantage is that a document with some inter-esting parts may well contain large portions with re-dundant, already known structures for which know-ing the manual annotation may not be very useful.In the given setting, we decided that the best granu-larity is the sentence.Sample Selection Metric There are a variety ofmetrics that could be used to quantify the degreeof deviation between classifiers in a committee (e.g.KL-divergence, information radius, f-measure).
Thework reported here uses two sentence-level met-rics based on KL-divergence and one based on f-measure.KL-divergence has been used for active learningto quantify the disagreement of classifiers over theprobability distribution of output labels (McCallumand Nigam, 1998; Jones et al, 2003).
It measuresthe divergence between two probability distributionsp and q over the same event space ?
:D(p||q) =?x?
?p(x) logp(x)q(x)(1)KL-divergence is a non-negative metric.
It is zerofor identical distributions; the more different the twodistributions, the higher the KL-divergence.
Intu-itively, a high KL-divergence score indicates an in-formative data point.
However, in the current formu-lation, KL-divergence only relates to individual to-kens.
In order to turn this into a sentence score, weneed to combine the individual KL-divergences forthe tokens within a sentence into one single score.We employed mean and max.The f-complement has been suggested for activelearning in the context of NP chunking as a struc-tural comparison between the different analyses ofa committee (Ngai and Yarowsky, 2000).
It is thepairwise f-measure comparison between the multi-ple analyses for a given sentence:fMcomp =12?M,M ??M(1?
F1(M(t),M?
(t))) (2)where F1 is the balanced f-measure of M(t) andM ?
(t), the preferred analyses of data point t accord-ing to different members M,M ?
of ensemble M.We take the complement so that it is oriented thesame as KL-divergence with high values indicatinghigh disagreement.
This is equivalent to taking theinter-annotator agreement between |M| classifiers.14669707172737475767778798010000  15000  20000  25000  30000  35000  40000  45000F-scoreNumber of Tokens in Training DataAve KL-divergenceRandom samplingFigure 1: Learning curve of the real AL experiment.2.4 ExperimentsTo tune the active learning parameters discussedin section 2.3, we ran detailed simulated experi-ments on the named entity data from the BioNLPshared task of the COLING 2004 InternationalJoint Workshop on Natural Language Processing inBiomedicine and its Applications (Kim et al, 2004).These results are treated in detail in the companionpaper (Becker et al, 2005).We used the CMM tagger to train two differentmodels by splitting the feature set to give multipleviews of the same data.
The feature set was hand-crafted such that it comprises different views whileempirically ensuring that performance is sufficientlysimilar.
On the basis of the findings of the simulationexperiments we set up the real active learning anno-tation experiment using: average KL-divergence asthe selection metric and a feature split that dividesthe full feature set roughly into features of wordsand features derived from external resources.
Assmaller batch sizes require more retraining iterationsand larger batch sizes increase the amount of anno-tation necessary at each round and could lead to un-necessary strain for the annotators, we settled on abatch size of 50 sentences for the real AL experi-ment as a compromise between computational costand work load for the annotator.We developed an active annotation tool and ranreal annotation experiments on the astronomy ab-stracts described in section 2.1.
The tool was givento the same astronomy PhD students for annotationwho were responsible for the seed and test data.
Thelearning curve for selective sampling is plotted infigure 1.4 The randomly sampled data was dou-bly annotated and the learning curve is averaged be-tween the two annotators.Comparing the selective sampling performance tothe baseline, we confirm that active learning pro-vides a significant reduction in the number of exam-ples that need annotating.
In fact, the random curvereaches an f-score of 76 after approximately 39000tokens have been annotated while the selective sam-pling curve reaches this level of performance afteronly ?
24000 tokens.
This represents a substantialreduction in tokens annotated of 38.5%.
In addition,at 39000 tokens, selectively sampling offers an errorreduction of 21.4% with a 3 point improvement inf-score.3 Evaluating Selective SamplingStandardly, the evaluation of active learning meth-ods and the comparison of sample selection metricsdraws on experiments over gold-standard annotatedcorpora, where a set of annotated data is at our dis-posal, e.g.
McCallum and Nigam (1998), Osborneand Baldridge (2004).
This assumes implicitly thatannotators will always produce gold-standard qual-ity annotations, which is typically not the case, as wediscussed in Section 2.2.
What is more, we speculatethat annotators might have an even higher error rateon the supposedly more informative, but possiblyalso more difficult examples.
However, this wouldnot be reflected in the carefully annotated and veri-fied examples of a gold standard corpus.
In the fol-lowing analysis, we leverage information from dou-bly annotated data to explore the effects on annota-tion of selectively sampled examples.To evaluate the practicality and usefulness of ac-tive learning as a generally applicable methodology,it is desirable to be able to observe the behaviourof the annotators.
In this section, we will report onthe evaluation of various subsets of the doubly an-notated portion of the ABC corpus comprising 1000sentences, which we sample according to a sampleselection metric.
That is, examples are added to thesubsets according to the sample selection metric, se-lecting those with higher disagreement first.
Thisallows us to trace changes in inter-annotator agree-4Learning curves reflect the performance on the test set us-ing the full feature set.147ment between the full corpus and selected subsetsthereof.
Also, we will inspect timing information.This novel methodology allows us to experimentwith different sample selection metrics without hav-ing to repeat the actual time and resource intensiveannotation.3.1 Error AnalysisTo investigate the types of classification errors, it iscommon to set up a confusion matrix.
One approachis to do this at the token level.
However, we are deal-ing with phrases and our analysis should reflect that.Thus we devised a method for constructing a confu-sion matrix based on phrasal alignment.
These con-fusion matrices are constructed by giving a doublecount for each phrase that has matching boundariesand a single count for each phrase that does not havematching boundaries.
To illustrate, consider the fol-lowing sentences?annotated with phrases A, B, andC for annotator 1 on top and annotator 2 on bottom?as sentence 1 and sentence 2 respectively:A ABA CABA CASentence 1 will get a count of 2 for A/A and forA/B and a count of 1 for O/C, while sentence 2will get 2 counts of A/O, and 1 count each of O/A,O/B, and O/C.
Table 1 contains confusion matricesfor the first 100 sentences sorted by averaged KL-divergence and for the full set of 1000 random sen-tences from the pool data.
(Note that these confusionmatrices contain percentages instead of raw countsso they can be directly compared.
)We can make some interesting observations look-ing at these phrasal confusion matrices.
The maineffect we observed is the same as was suggested bythe f-score inter-annotator agreement errors in sec-tion 2.1.
Specifically, looking at the full random setof 1000 sentences, almost all errors (Where ?
is anyentity phrase type, ?/O + O/?
errorsall errors = 95.43%) aredue to problems with phrase boundaries.
Compar-ing the full random set to the 100 sentences withthe highest averaged KL-divergence, we can see thatthis is even more the case for the sub-set of 100 sen-tences (97.43%).
Therefore, we can observe that100: A2IN SN ST SF OIN 12.0 0.0 0.0 0.0 0.4SN 0.0 10.4 0.0 0.0 0.4A1 ST 0.0 0.4 30.3 0.0 1.0SF 0.0 0.0 0.0 31.1 3.9O 0.2 0.4 2.9 6.4 ?1000: A2IN SN ST SF OIN 9.4 0.0 0.0 0.0 0.3SN 0.0 10.1 0.2 0.1 0.3A1 ST 0.0 0.1 41.9 0.1 1.6SF 0.0 0.0 0.1 25.1 3.0O 0.3 0.2 2.4 4.8 ?Table 1: Phrasal confusion matrices for documentsub-set of 100 sentences sorted by average KL-divergence and for full random document sub-set of1000 sentences (A1: Annotator 1, A2: Annotator 2).Entity 100 1000Instrument-name 12.4% 9.7%Source-name 10.8% 10.7%Source-type 31.7% 43.7%Spectral-feature 35.0% 28.2%O 9.9% 7.7%Table 2: Normalised distributions of agreed entityannotations.there is a tendency for the averaged KL-divergenceselection metric to choose sentences where phraseboundary identification is difficult.Furthermore, comparing the confusion matricesfor 100 sentences and for the full set of 1000 showsthat sentences containing less common entity typestend to be selected first while sentences containingthe most common entity types are dispreferred.
Ta-ble 2 contains the marginal distribution for annotator1 (A1) from the confusion matrices for the orderedsub-set of 100 and for the full random set of 1000sentences.
So, for example, the sorted sub-set con-tains 12.4% Instrument-name annotations (theleast common entity type) while the full set con-tains 9.7%.
And, 31.7% of agreed entity annota-tions in the first sub-set of 100 are Source-type(the most common entity type), whereas the propor-1480.870.880.890.90.910.920.930.940.950.960.970.980  5000  10000  15000  20000  25000  30000Inter-annotatorAgreement (Acc)Size (Tokens) of KL-sorted Document SubsetKL-divergenceFigure 2: Raw agreement plotted against KL-sorteddocument subsets.tion of agreed Source-type annotations in thefull random set is 43.7%.
Looking at the O row, wealso observe that sentences with difficult phrases arepreferred.
A similar effect can be observed in themarginals for annotator 2.3.2 Annotator PerformanceSo far, the behaviour we have observed is what youwould expect from selective sampling; there is amarked improvement in terms of cost and error ratereduction over random sampling.
However, selec-tive sampling raises questions of cognitive load andthe quality of annotation.
In the following sectionwe investigate the relationship between informativ-ity, inter-annotator agreement, and annotation time.While reusability of selective samples for otherlearning algorithms has been explored (Baldridgeand Osborne, 2004), no effort has been made toquantify the effect of selective sampling on anno-tator performance.
We concentrate first on the ques-tion: Are informative examples more difficult to an-notate?
One way to quantify this effect is to lookat the correlation between human agreement and thetoken-level KL-divergence.
The Pearson correlationcoefficient indicates the degree to which two vari-ables are related.
It ranges between ?1 and 1, where1 means perfectly positive correlation, and ?1 per-fectly negative correlation.
A value of 0 indicates nocorrelation.
The Pearson correlation coefficient onall tokens gives a very weak correlation coefficientof ?0.009.5 However, this includes many trivial to-5In order to make this calculation, we give token-level agree-0.760.770.780.790.80.810.820.830.840.850.86100  200  300  400  500  600  700  800  900  1000Inter-annotatorAgreement (F)Size (Sents) of Selection Metric-sorted SubsetAve KL-divergenceMax KL-divergenceF-complementFigure 3: Human disagreement plotted against se-lection metric-sorted document subsets.kens which are easily identified as being outside anentity phrase.
If we look just at tokens that at leastone of the annotators posits as being part of an en-tity phrase, we observe a larger effect with a Pear-son correlation coefficient of ?0.120, indicating thatagreement tends to be low when KL-divergence ishigh.
Figure 2 illustrates this effect even more dra-matically.
Here we plot accuracy against token sub-sets of size 1000, 2000, .., N where tokens are addedto the subsets according to their KL-divergence, se-lecting those with the highest values first.
Thisdemonstrates clearly that tokens with higher KL-divergence have lower inter-annotator agreement.However, as discussed in sections 2.3 and 2.4,we decided on sentences as the preferred annota-tion level.
Therefore, it is important to explore theserelationships at the sentence level as well.
Again,we start by looking at the Pearson correlation coeffi-cient between f-score inter-annotator agreement (asdescribed in section 2.1) and our active learning se-lection metrics:Ave KL Max KL 1-FAll Tokens ?0.090 ?0.145 ?0.143O Removed ?0.042 ?0.092 ?0.101Here O Removed means that sentences are removedfor which the annotators agree that there are no en-tity phrases (i.e.
all tokens are labelled as beingoutside an entity phrase).
This shows a relation-ment a numeric representation by assigning 1 to tokens onwhich the annotators agree and 0 to tokens on which they dis-agree.1490.650.70.750.80.850.90.95100  200  300  400  500  600  700  800  900  1000Averagetimeper tokenSize (Sents) of Selection Metric-sorted SubsetAve KL-divergenceMax KL-divergenceF-complementFigure 4: Annotation time plotted against selectionmetric-sorted document subsets.ship very similar to what we observed at the tokenlevel: a negative correlation indicating that agree-ment is low when KL-divergence is high.
Again,the effect of selecting informative examples is betterillustrated with a plot.
Figure 3 plots f-score agree-ment against sentence subsets sorted by our sentencelevel selection metrics.
Lower agreement at the leftof these plots indicates that the more informative ex-amples according to our selection metrics are moredifficult to annotate.So, active learning makes the annotation more dif-ficult.
But, this raises a further question: What effectdo more difficult examples have on annotation time?To investigate this, we once again start by lookingat the Pearson correlation coefficient, this time be-tween the annotation time and our selection metrics.However, as our sentence-level selection metrics af-fect the length of sentences selected, we normalisesentence-level annotation times by sentence length:Ave KL Max KL 1-FAll Tokens 0.157 ?0.009 0.082O Removed 0.216 ?0.007 0.106Here we see a small positive correlations for av-eraged KL-divergence and f-complement indicatingthat sentences that score higher according to our se-lection metrics do generally take longer to annotate.Again, we can visualise this effect better by plottingaverage time against KL-sorted subsets (Figure 4).This demonstrates that sentences preferred by ourselection metrics generally take longer to annotate.4 Conclusions and Future WorkWe have presented active learning experiments ina novel NER domain and investigated negative sideeffects.
We investigated the relationship betweeninformativity of an example, as determined by se-lective sampling metrics, and inter-annotator agree-ment.
This effect has been quantified using the Pear-son correlation coefficient and visualised using plotsthat illustrate the difficulty and time-intensiveness ofexamples chosen first by selective sampling.
Thesemeasurements clearly demonstrate that selectivelysampled examples are in fact more difficult to anno-tate.
And, while sentence length and entities per sen-tence are somewhat confounding factors, we havealso shown that selective sampling of informativeexamples appears to increase the time spent on in-dividual examples.High quality annotation is important for buildingaccurate models and for reusability.
While anno-tation quality suffers for selectively sampled exam-ples, selective sampling nevertheless provided a dra-matic cost reduction of 38.5% in a real annotationexperiment, demonstrating the utility of active learn-ing for bootstrapping NER in a new domain.In future work, we will perform further investi-gations of the cost of resolving annotations for se-lectively sampled examples.
And, in related work,we will use timing information to assess token, en-tity and sentence cost metrics for annotation.
Thisshould also lead to a better understanding of the re-lationship between timing information and sentencelength for different selection metrics.AcknowledgementsThe work reported here, including the related de-velopment of the astronomy bootstrapping corpusand the active learning tools, were supported byEdinburgh-Stanford Link Grant (R36759) as part ofthe SEER project.
We are very grateful for the timeand resources invested in corpus preparation by ourcollaborators in the Institute for Astronomy, Univer-sity of Edinburgh: Rachel Dowsett, Olivia Johnsonand Bob Mann.
We are also grateful to Melissa Kro-nenthal and Jean Carletta for help collecting data.150ReferencesShlomo Argamon-Engelson and Ido Dagan.
1999.Committee-based sample selection for probabilisticclassifiers.
Journal of Artificial Intelligence Research,11:335?360.Jason Baldridge and Miles Osborne.
2004.
Ensemble-based active learning for parse selection.
In Pro-ceedings of the 5th Conference of the North AmericanChapter of the Association for Computational Linguis-tics.Markus Becker, Ben Hachey, Beatrice Alex, and ClaireGrover.
2005.
Optimising selective sampling for boot-strapping named entity recognition.
In ICML-2005Workshop on Learning with Multiple Views.Thorsten Brants.
2000.
Inter-annotator agreement for aGerman newspaper corpus.
In Proceedings of the 2ndInternational Conference on Language Resources andEvaluation (LREC-2000).Jean Carletta, Amy Isard, Stephen Isard, Jacqueline C.Kowtko, Gwyneth Doherty-Sneddon, and Anne H.Anderson.
1997.
The reliability of a dialoguestructure coding scheme.
Computational Linguistics,23(1):13?31.David.
A. Cohn, Zoubin.
Ghahramani, and Michael.
I.Jordan.
1995.
Active learning with statistical mod-els.
In G. Tesauro, D. Touretzky, and T. Leen, editors,Advances in Neural Information Processing Systems,volume 7, pages 705?712.
The MIT Press.Jenny Finkel, Shipra Dingare, Christopher Manning,Beatrice Alex Malvina Nissim, and Claire Grover.2005.
Exploring the boundaries: Gene and proteinidentification in biomedical text.
BMC Bioinformat-ics.
In press.Rosie Jones, Rayid Ghani, Tom Mitchell, and EllenRiloff.
2003.
Active learning with multiple view fea-ture sets.
In ECML 2003 Workshop on Adaptive TextExtraction and Mining.Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,Yuka Tateisi, and Nigel Collier.
2004.
Introduc-tion to the bio-entity recognition task at JNLPBA.In Proceedings of the COLING 2004 InternationalJoint Workshop on Natural Language Processing inBiomedicine and its Applications.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D. Manning.
2003.
Named entity recognitionwith character-level models.
In Proceedings the Sev-enth Conference on Natural Language Learning.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated cor-pus of English: The Penn treebank.
ComputationalLinguistics, 19(2):313?330.Andrew McCallum and Kamal Nigam.
1998.
EmployingEM and pool-based active learning for text classifica-tion.
In Proceedings of the 15th International Confer-ence on Machine Learning.Grace Ngai and David Yarowsky.
2000.
Rule writingor annotation: Cost-efficient resource usage for basenoun phrase chunking.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics.Patricia Robinson, Erica Brown, John Burger, NancyChinchor, Aaron Douthat, Lisa Ferro, and LynetteHirschman.
1999.
Overview: Information extractionfrom broadcast news.
In Proceedings DARPA Broad-cast News Workshop.Erik F. Tjong Kim Sang.
2002.
Introduction tothe CoNLL-2002 shared task: Language-independentnamed entity recognition.
In Proceedings of the2002 Conference on Computational Natural LanguageLearning.H.
Sebastian Seung, Manfred Opper, and Haim Som-polinsky.
1992.
Query by committee.
In Computa-tional Learning Theory.Stephanie Strassel, Alexis Mitchell, and Shudong Huang.2003.
Multilingual resources for entity extraction.
InProceedings of the ACL 2003 Workshop on Multilin-gual and Mixed-language Named Entity Recognition.Cynthia A. Thompson, Mary Elaine Califf, and Ray-mond J. Mooney.
1999.
Active learning for naturallanguage parsing and information extraction.
In Pro-ceedings of the 16th International Conference on Ma-chine Learning.151
