Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 745?752,Sydney, July 2006. c?2006 Association for Computational LinguisticsProximity in Context: an empirically grounded computational model ofproximity for processing topological spatial expressions?John D. KelleherDublin Institute of TechnologyDublin, Irelandjohn.kelleher@comp.dit.ieGeert-Jan M. KruijffDFKI GmbHSaarbruc?ken, Germanygj@dfki.deFintan J. CostelloUniversity College DublinDublin, Irelandfintan.costello@ucd.ieAbstractThe paper presents a new model for context-dependent interpretation of linguistic expressionsabout spatial proximity between objects in a nat-ural scene.
The paper discusses novel psycholin-guistic experimental data that tests and verifies themodel.
The model has been implemented, and en-ables a conversational robot to identify objects in ascene through topological spatial relations (e.g.
?Xnear Y?).
The model can help motivate the choicebetween topological and projective prepositions.1 IntroductionOur long-term goal is to develop conversationalrobots with which we can have natural, fluent sit-uated dialog.
An inherent aspect of such situateddialog is reference to aspects of the physical envi-ronment in which the agents are situated.
In thispaper, we present a computational model whichprovides a context-dependent analysis of the envi-ronment in terms of spatial proximity.
We showhow we can use this model to ground spatial lan-guage that uses topological prepositions (?the ballnear the box?)
to identify objects in a scene.Proximity is ubiquitous in situated dialog, butthere are deeper ?cognitive?
reasons for why weneed a context-dependent model of proximity tofacilitate fluent dialog with a conversational robot.This has to do with the cognitive load that process-ing proximity expressions imposes.
Consider theexamples in (1).
Psycholinguistic data indicatesthat a spatial proximity expression (1b) presents aheavier cognitive load than a referring expressionidentifying an object purely on physical features(1a) yet is easier to process than a projective ex-pression (1c) (van der Sluis and Krahmer, 2004).
?The research reported here was supported by the CoSyproject, EU FP6 IST ?Cognitive Systems?
FP6-004250-IP.
(1) a. the blue ballb.
the ball near the boxc.
the ball to the right of the boxOne explanation for this preference is thatfeature-based descriptions are easier to resolveperceptually, with a further distinction among fea-tures as given in Figure 1, cf.
(Dale and Reiter,1995).
On the other hand, the interpretation andrealization of spatial expressions requires effortand attention (Logan, 1994; Logan, 1995).Figure 1: Cognitive loadSimilarly wecan distinguish be-tween the cognitiveloads of processingdifferent forms ofspatial relations.Focusing on staticprepositions, topo-logical prepositionshave a lower cognitive load than projectiveprepositions.
Topological prepositions (e.g.
?at?, ?near?)
describe proximity to an object.Projective prepositions (e.g.
?above?)
describe aregion in a particular direction from the object.Projective prepositions impose a higher cognitiveload because we need to consider different spatialframes of reference (Krahmer and Theune, 1999;Moratz and Tenbrink, 2006).
Now, if we wanta robot to interact with other agents in a waythat obeys the Principle of Minimal CooperativeEffort (Clark and Wilkes-Gibbs, 1986), it shouldadopt the simplest means to (spatially) refer to anobject.
However, research on spatial language inhuman-robot interaction has primarily focused onthe use of projective prepositions.We currently lack a comprehensive model fortopological prepositions.
Without such a model,745a robot cannot interpret spatial proximity expres-sions nor motivate their contextually and pragmat-ically appropriate use.
In this paper, we presenta model that addresses this problem.
The modeluses energy functions, modulated by visual anddiscourse salience, to model how spatial templatesassociated with other landmarks may interfere toestablish what are contextually appropriate waysto locate a target relative to these landmarks.
Themodel enables grounding of spatial expressionsusing spatial proximity to refer to objects in theenvironment.
We focus on expressions using topo-logical prepositions such as ?near?
or ?at?.Terminology.
We use the term target (T) torefer to the object that is being located by a spa-tial expression, and landmark (L) to refer to theobject relative to which the target?s location is de-scribed: ?
[The man]T near [the table]L.?
A dis-tractor is any object in the visual context that isneither landmark nor target.Overview ?2 presents contextual effects we canobserve in grounding spatial expressions, includ-ing the effect of interference on whether two ob-jects may be considered proximal.
?3 discusses amodel that accounts for all these effects, and ?4 de-scribes an experiment to test the model.
?5 showshow we use the model in linguistic interpretation.2 DataBelow we discuss previous psycholinguistic expe-rients, focusing on how contextual factors such asdistance, size, and salience may affect proximity.We also present novel examples, showing that thelocation of other objects in a scene may interferewith the acceptability of a proximal description tolocate a target relative to a landmark.
These exam-ples motivate the model in ?3.1.74 1.90 2.84 3.16 2.34 1.81 2.132.61 3.84 4.66 4.97 4.90 3.56 3.264.06 5.56 7.55 7.97 7.29 4.80 3.913.47 4.81 6.94 7.56 7.31 5.59 3.634.47 5.91 8.52 O 7.90 6.13 4.463.25 4.03 4.50 4.78 4.41 3.47 3.101.84 2.23 2.03 3.06 2.53 2.13 2.00Figure 2: 7-by-7 cell grid with mean goodness ratings forthe relation the X is near O as a function of the position oc-cupied by X.Spatial reasoning is a complex activity that in-volves at least two levels of processing: a geomet-ric level where metric, topological, and projectiveproperties are handled, (Herskovits, 1986); and afunctional level where the normal function of anentity affects the spatial relationships attributed toit in a context, cf.
(Coventry and Garrod, 2004).We focus on geometric factors.Although a lot of experimental work has beendone on spatial reasoning and language (cf.
(Coventry and Garrod, 2004)), only Logan andSadler (1996) examined topological prepositionsin a context where functional factors were ex-cluded.
They introduced the notion of a spatialtemplate.
The template is centred on the land-mark and identifies for each point in its space theacceptability of the spatial relationship betweenthe landmark and the target appearing at that pointbeing described by the preposition.
Logan &Sadler examined various spatial prepositions thisway.
In their experiments, a human subject wasshown sentences of the form ?the X is [relation]the O?, each with a picture of a spatial configura-tion of an O in the center of an invisible 7-by-7cell grid, and an X in one of the 48 surroundingpositions.
The subject then had to rate how wellthe sentence described the picture, on a scale from1(bad) to 9(good).
Figure 2 gives the mean good-ness rating for the relation ?near to?
as a functionof the position occupied by X (Logan and Sadler,1996).
It is clear from Figure 2 that ratings dimin-ish as the distance between X and O increases, butalso that even at the extremes of the grid the rat-ings were still above 1 (min.
rating).Besides distance there are also other factors thatdetermine the applicability of a proximal relation.For example, given prototypical size, the regiondenoted by ?near the building?
is larger than thatof ?near the apple?
(Gapp, 1994).
Moreover, anobject?s salience influences the determination ofthe proximal region associated with it (Regier andCarlson, 2001; Roy, 2002).Finally, the two scenes in Figure 3 show inter-ference as a contextual factor.
For the scene on theleft we can use ?the blue box is near the black box?to describe object (c).
This seems inappropriate inthe scene on the right.
Placing an object (d) beside(b) appears to interfere with the appropriatenessof using a proximal relation to locate (c) relativeto (b), even though the absolute distance between(c) and (b) has not changed.Thus, there is empirical evidence for several746Figure 3: Proximity and distancecontextual factors determining the applicability ofa proximal description.
We argued that the loca-tion of other distractor objects in context may alsointerfere with this applicability.
The model in ?3captures all these factors, and is evaluated in ?4.3 Computational ModelBelow we describe a model of relative proximitythat uses (1) the distance between objects, (2) thesize and salience of the landmark object, and (3)the location of other objects in the scene.
Ourmodel is based on first computing absolute prox-imity between each point and each landmark in ascene, and then combining or overlaying the re-sulting absolute proximity fields to compute therelative proximity of each point to each landmark.3.1 Computing absolute proximity fieldsWe first compute for each landmark an absoluteproximity field giving each point?s proximity tothat landmark, independent of proximity to anyother landmark.
We compute fields on the pro-jection of the scene onto the 2D-plane, a 2D-arrayARRAY of points.
At each point P in ARRAY ,the absolute proximity for landmark L isproxabs = (1 ?
distnormalised(L,P,ARRAY ))?
salience(L).
(1)In this equation the absolute proximity for apoint P and a landmark L is a function of boththe distance between the point and the location ofthe landmark, and the salience of the landmark.To represent distance we use a normaliseddistance function distnormalised (L, P, ARRAY ),which returns a value between 0 and 1.1 Thesmaller the distance between L and P , the higherthe absolute proximity value returned, i.e.
themore acceptable it is to say that P is close to L. Inthis way, this component of the absolute proximityfield captures the gradual gradation in applicabil-ity evident in Logan and Sadler (1996).1We normalise by computing the distance between thetwo points, and then dividing this distance it by the maximumdistance between point L and any point in the scene.We model the influence of visual and dis-course salience on absolute proximity as a func-tion salience(L), returning a value between 0 and1 that represents the relative salience of the land-mark L in the scene (2).
The relative salience ofan object is the average of its visual salience (Svis )and discourse salience (Sdisc),salience(L) = (Svis(L) + Sdisc(L))/2 (2)Visual salience Svis is computed using the algo-rithm of Kelleher and van Genabith (2004).
Com-puting a relative salience for each object in a sceneis based on its perceivable size and its centralityrelative to the viewer?s focus of attention.
The al-gorithm returns scores in the range of 0 to 1.
Asthe algorithm captures object size we can modelthe effect of landmark size on proximity throughthe salience component of absolute proximity.
Thediscourse salience (Sdisc) of an object is computedbased on recency of mention (Hajicova?, 1993) ex-cept we represent the maximum overall salience inthe scene as 1, and use 0 to indicate that the land-mark is not salient in the current context.00.10.20.30.40.50.?0.?0.?0.?1?-3?-3?
?-2?-2?
?-1?-1?
L ?1?1?
?2?2?
?3?3?point locationproximityratin???
?ol?te proximity to L?
?alien?e 1??
?ol?te proximity to L?
?alien?e 0.???
?ol?te proximity to L?
?alien?e 0.5Figure 4: Absolute proximity ratings for landmark L cen-tered in a 2D plane, points ranging from plane?s upper-leftcorner (<-3,-3>) to lower right corner(<3,3>).Figure 4 shows computed absolute proximitywith salience values of 1, 0.6, and 0.5, for pointsfrom the upper-left to the lower-right of a 2Dplane, with the landmark at the center of thatplane.
The graph shows how salience influencesabsolute proximity in our model: for a landmarkwith high salience, points far from the landmarkcan still have high absolute proximity to it.3.2 Computing relative proximity fieldsOnce we have constructed absolute proximityfields for the landmarks in a scene, our next stepis to overlay these fields to produce a measure of747relative proximity to each landmark at each point.For this we first select a landmark, and then iter-ate over each point in the scene comparing the ab-solute proximity of the selected landmark at thatpoint with the absolute proximity of all other land-marks at that point.
The relative proximity of aselected landmark at a point is equal to the abso-lute proximity field for that landmark at that point,minus the highest absolute proximity field for anyother landmark at that point (see Equation 3).proxrel(P,L) = proxabs(P,L)?
MAX?LX #=Lproxabs(P,LX )(3)The idea here is that the other landmark with thehighest absolute proximity is acting in competi-tion with the selected landmark.
If that other land-mark?s absolute proximity is higher than the ab-solute proximity of the selected landmark, the se-lected landmark?s relative proximity for the pointwill be negative.
If the competing landmark?s ab-solute proximity is slightly lower than the abso-lute proximity of the selected landmark, the se-lected landmark?s relative proximity for the pointwill be positive, but low.
Only when the compet-ing landmark?s absolute proximity is significantlylower than the absolute proximity of the selectedlandmark will the selected landmark have a highrelative proximity for the point in question.In (3) the proximity of a given point to a se-lected landmark rises as that point?s distance fromthe landmark decreases (the closer the point is tothe landmark, the higher its proximity score for thelandmark will be), but falls as that point?s distancefrom some other landmark decreases (the closerthe point is to some other landmark, the lower itsproximity score for the selected landmark will be).Figure 5 shows the relative proximity fields of twolandmarks, L1 and L2, computed using (3), in a1-dimensional (linear) space.
The two landmarkshave different degrees of salience: a salience of0.5 for L1 and of 0.6 for L2 (represented by thedifferent sizes of the landmarks).
In this figure,any point where the relative proximity for one par-ticular landmark is above the zero line representsa point which is proximal to that landmark, ratherthan to the other landmark.
The extent to whichthat point is above zero represents its degree ofproximity to that landmark.
The overall proximalarea for a given landmark is the overall area forwhich its relative proximity field is above zero.The left and right borders of the figure representthe boundaries (walls) of the area.Figure 5 illustrates three main points.
First, theoverall size of a landmark?s proximal area is afunction of the landmark?s position relative to theother landmark and to the boundaries.
For exam-ple, landmark L2 has a large open space betweenit and the right boundary: Most of this space fallsinto the proximal area for that landmark.
Land-mark L1 falls into quite a narrow space betweenthe left boundary and L2.
L1 thus has a muchsmaller proximal area in the figure than L2.
Sec-ond, the relative proximity field for some land-mark is a function of that landmark?s salience.This can be seen in Figure 5 by considering thespace between the two landmarks.
In that spacethe width of the proximal area for L2 is greaterthan that of L1, because L2 is more salient.The third point concerns areas of ambiguousproximity in Figure 5: areas in which neither ofthe landmarks have a significantly higher relativeproximity than the other.
There are two such areasin the Figure.
The first is between the two land-marks, in the region where one relative proxim-ity field line crosses the other.
These points areambiguous in terms of relative proximity becausethese points are equidistant from those two land-marks.
The second ambiguous area is at the ex-treme right of the space shown in Figure 5.
Thisarea is ambiguous because this area is distant fromboth landmarks: points in this area would not bejudged proximal to either landmark.
The ques-tion of ambiguity in relative proximity judgmentsis considered in more detail in ?5.??????????????????????????????????????
?
?point lo?ation?relativeproximity????????
?????
?????
?????
??
??????????
?????
???
??
?????
??
?
?Figure 5: Graph of relative proximity fields for two land-marks L1 and L2.
Relative proximity fields were computedwith salience scores of 0.5 for L1 and 0.6 for L2.4 ExperimentBelow we describe an experiment which tests ourapproach (?3) to relative proximity by examining748the changes in people?s judgements of the appro-priateness of the expression near being used to de-scribe the relationship between a target and land-mark object in an image where a second, distractorlandmark is present.
All objects in these imageswere coloured shapes, a circle, triangle or square.4.1 Material and ProcedureAll images used in this experiment contained acentral landmark object and a target object, usu-ally with a third distractor object.
The landmarkwas always placed in the middle of a 7-by-7 grid.Images were divided into 8 groups of 6 imageseach.
Each image in a group contained the targetobject placed in one of 6 different cells on the grid,numbered from 1 to 6.
Figure 6 shows how wenumber these target positions according to theirnearness to the landmark.1 24 5 a6g L cebd f3Figure 6: Relative locations of landmark (L) target posi-tions (1..6) and distractor landmark positions (a..g) in imagesused in the experiment.Groups are organised according to the presenceand position of a distractor object.
In group a thedistractor is directly above the landmark, in groupb the distractor is rotated 45 degrees clockwisefrom the vertical, in group c it is directly to theright of the landmark, in d it is rotated 135 de-grees clockwise from the vertical, and so on.
Thedistractor object is always the same distance fromthe central landmark.
In addition to the distractorgroups a,b,c,d,e,f and g, there is an eighth group,group x, in which no distractor object occurs.In the experiment, each image was displayedwith a sentence of the form The is near the ,with a description of the target and landmark re-spectively.
The sentence was presented under theimage.
12 participants took part in this experi-ment.
Participants were asked to rate the accept-ability of the sentence as a description of the im-age using a 10-point scale, with zero denoting notacceptable at all; four or five denoting moderatelyacceptable; and nine perfectly acceptable.4.2 Results and DiscussionWe assess participants?
responses by comparingtheir average proximity judgments with those pre-dicted by the absolute proximity equation (Equa-tion 1), and by the relative proximity equation(Equation 3).
For both equations we assumethat all objects have a salience score of 1.
Withsalience equal to 1, the absolute proximity equa-tion relates proximity between target and land-mark objects to the distance between those two ob-jects, so that the closer the target is to the landmarkthe higher its proximity will be.
With salienceequal to 1, the relative proximity equation re-lates proximity to both distance between target andlandmark and distance between target and distrac-tor, so that the proximity of a given target objectto a landmark rises as that target?s distance fromthe landmark decreases but falls as the target?s dis-tance from some other distractor object decreases.Figure 7 shows graphs comparing participants?proximity ratings with the proximity scores com-puted by Equation 1 (the absolute proximity equa-tion), and by Equation 3 (the relative proximityequation), for the images in group x and in theother 7 groups.
In the first graph there is no dif-ference between the proximity scores computedby the two equations, since, when there is no dis-tractor object present the relative proximity equa-tion reduces to the absolute proximity equation.The correlation between both computed proximityscores and participants?
average proximity scoresfor this group is quite high (r = 0.95).
For the re-maining 7 groups the proximity value computedfrom Equation 1 gives a fair match to people?sproximity judgements for target objects (the aver-age correlation across these seven groups in Fig-ure 7 is around r = 0.93).
However, relativeproximity score as computed in Equation 3 signifi-cantly improves the correlation in each graph, giv-ing an average correlation across the seven groupsof around r = 0.99 (all correlations in Figure 7are significant p < 0.01).Given that the correlations for both Equation 1and Equation 3 are high we examined whether theresults returned by Equation 3 were reliably closerto human judgements than those from Equation 1.For the 42 images where a distractor object waspresent we recorded which equation gave a resultthat was closer to participants?
normalised aver-749???????????
?
?
?
?
???????????????????????????????????????????????????????????????????????
???
?
??????
????????????????????????????????????
???
?
??????
????????????????????????????????
?
?
?
?
????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????????????
?
?
?
?
????????????????????????????????????????????????????????????????????????
???
????????
???
???????????????????????????????????????????????????????????????????????????
?
?
?
?
????????????????????????????????????????????????????????????????????????
???
???
????
???????????????????????????????????
???
???
????
??????????????????????????????
?
?
?
?
??????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????
????????????????????????????????????
?
?
?
?
????????????????????????????????????????????????????????????????????????
???
???
????
???????????????????????????????????
???
???
????
??????????????????????????????
?
?
?
?
????????????????????????????????????????????????????????????????????????
???
???
????
???????????????????????????????????????????????????????????????????????????????
?
?
?
?
?????????????????????????????????????????????????????????????????????????
???
???
????
??????????????????????????????????????????????????????????????????????
?Figure 7: comparison between normalised proximity scores observed and computed for each group.age for that image.
In 28 cases Equation 3 wascloser, while in 14 Equation 1 was closer (a 2:1advantage for Equation 3, significant in a sign test:n+ = 28, n?
= 14, Z = 2.2, p < 0.05).
We con-clude that proximity judgements for objects in ourexperiment are best represented by relative prox-imity as computed in Equation 3.
These resultssupport our ?relative?
model of proximity.2It is interesting to note that Equation 3 over-estimates proximity in the cases (a, b and g)2Note that, in order to display the relationship betweenproximity values given by participants, computed in Equa-tion 1, and computed in Equation 3, the values displayed inFigure 7 are normalised so that proximity values have a meanof 0 and a standard deviation of 1.
This normalisation simplymeans that all values fall in the same region of the scale, andcan be easily compared visually.where the distractor object is closest to the targetsand slightly underestimates proximity in all othercases.
We will investigate this in future work.5 Expressing spatial proximityWe use the model of ?3 to interpret spatial ref-erences to objects.
A fundamental requirementfor processing situated dialogue is that linguisticmeaning provides enough information to establishthe visual grounding of spatial expressions: Howcan the robot relate the meaning of a spatial ex-pression to a scene it visually perceives, so it canlocate the objects which the expression applies to?Approaches agree here on the need for ontolog-ically rich representations, but differ in how theseare to be visually grounded.
Oates et al (2000)750and Roy (2002) use machine learning to obtaina statistical mapping between visual and linguis-tic features.
Gorniak and Roy (2004) use manu-ally constructed mappings between linguistic con-structions, and probabilistic functions which eval-uate whether an object can act as referent, whereasDeVault and Stone (2004) use symbolic constraintresolution.
Our approach to visual grounding oflanguage is similar to the latter two approaches.We use a Combinatory Categorial Grammar(CCG) (Baldridge and Kruijff, 2003) to describethe relation between the syntactic structure ofan utterance and its meaning.
We model mean-ing as an ontologically richly sorted, relationalstructure, using a description logic-like framework(Baldridge and Kruijff, 2002).
We use OpenCCGfor parsing and realization.3(2) the box near the ball@{b:phys?obj}(box& ?Delimitation?unique& ?Number?singular& ?Quantification?specific singular)& @{b:phys?obj}?Location?
(r : region & near& ?Proximity?proximal& ?Positioning?static)& @{r :region}?FromWhere?
(b1 : phys ?
obj& ball& ?Delimitation?unique& ?Number?singular& ?Quantification?specific singular)Example (2) shows the meaning representationfor ?the box near the ball?.
It consists of sev-eral, related elementary predicates (EPs).
Onetype of EP represents a discourse referent as aproposition with a handle: @{b:phys?obj}(box)means that the referent b is a physical object,namely a box.
Another type of EP states de-pendencies between referents as modal relations,e.g.
@{b:phys?obj}?Location?
(r : region & near)means that discourse referent b (the box) is locatedin a region r that is near to a landmark.
We repre-sent regions explicitly to enable later reference tothe region using deictic reference (e.g.
?there?
).Within each EP we can have semantic features,e.g.
the region r characterizes a static location of band expresses proximity to a landmark.
Example(2) gives a ball in the context as the landmark.We use the sorting information in the utter-ance?s meaning (e.g.
phys-obj, region) for further3http://www.sf.net/openccg/interpretation using ontology-based spatial rea-soning.
This yields several inferences that need tohold for the scene, like DeVault and Stone (2004).Where we differ is in how we check whether theseinferences hold.
Like Gorniak and Roy (2004), wemap these conditions onto the energy landscapecomputed by the proximity field functions.
Thisenables us to take into account inhibition effectsarising in the actual situated context, unlike Gor-niak & Roy or DeVault & Stone.We convert relative proximity fields into prox-imal regions anchored to landmarks to contextu-ally interpret linguistic meaning.
We must decidewhether a landmark?s relative proximity score ata given point indicates that it is ?near?
or ?closeto?
or ?at?
or ?beside?
the landmark.
For this weiterate over each point in the scene, and comparethe relative proximity scores of the different land-marks at each point.
If the primary landmark?s(i.e., the landmark with the highest relative prox-imity at the point) relative proximity exceeds thenext highest relative proximity score by more thana predefined confidence interval the point is in thevague region anchored around the primary land-mark.
Otherwise, we take it as ambiguous and notin the proximal region that is being interpreted.The motivation for the confidence interval is tocapture situations where the difference in relativeproximity scores between the primary landmarkand one or more landmarks at a given point is rel-atively small.
Figure 8 illustrates the parsing of ascene into the regions ?near?
two landmarks.
Therelative proximity fields of the two landmarks areidentical to those in Figure 5, using a confidenceinterval of 0.1.
Ambiguous points are where theproximity ambiguity series is plotted at 0.5.
Theregions ?near?
each landmark are those areas ofthe graph where each landmark?s relative proxim-ity series is the highest plot on the graph.Figure 8 illustrates an important aspect of ourmodel: the comparison of relative proximity fieldsnaturally defines the extent of vague proximal re-gions.
For example, see the region right of L2 inFigure 8.
The extent of L2?s proximal region inthis direction is bounded by the interference ef-fect of L1?s relative proximity field.
Because thelandmarks?
relative proximity scores converge, thearea on the far right of the image is ambiguouswith respect to which landmark it is proximal to.In effect, the model captures the fact that the areais relatively distant from both landmarks.
Follow-751Figure 8: Graph of ambiguous regions overlaid on relativeproximity fields for landmarks L1 and L2, with confidenceinterval=0.1 and different salience scores for L1 (0.5) and L2(0.6).
Locations of landmarks are marked on the X-axis.ing the cognitive load model (?1), objects locatedin this region should be described with a projectiverelation such as ?to the right of L2?
rather than aproximal relation like ?near L2?, see Kelleher andKruijff (2006).6 ConclusionsWe addressed the issue of how we can providea context-dependent interpretation of spatial ex-pressions that identify objects based on proxim-ity in a visual scene.
We discussed availablepsycholinguistic data to substantiate the useful-ness of having such a model for interpreting andgenerating fluent situated dialogue between a hu-man and a robot, and that we need a context-dependent representation of what is (situationally)appropriate to consider proximal to a landmark.Context-dependence thereby involves salience oflandmarks as well as inhibition effects betweenlandmarks.
We presented a model in which wecan address these issues, and we exemplified howlogical forms representing the meaning of spa-tial proximity expressions can be grounded in thismodel.
We tested and verified the model using apsycholinguistic experiment.
Future work will ex-amine whether the model can be used to describethe semantics of nouns (such as corner) that ex-press vague spatial extent, and how the model re-lates to the functional aspects of spatial reasoning.ReferencesJ.
Baldridge and G.J.M.
Kruijff.
2002.
Coupling CCG andhybrid logic dependency semantics.
In Proceedings ofACL 2002, Philadelphia, Pennsylvania.J.
Baldridge and G.J.M.
Kruijff.
2003.
Multi-modal combi-natory categorial grammar.
In Proceedings of EACL 2003,Budapest, Hungary.H.
Clark and D. Wilkes-Gibbs.
1986.
Referring as a collab-orative process.
Cognition, 22:1?39.K.R.
Coventry and S. Garrod.
2004.
Saying, Seeing andActing.
The Psychological Semantics of Spatial Preposi-tions.
Essays in Cognitive Psychology Series.
LawrenceErlbaum Associates.R.
Dale and E. Reiter.
1995.
Computatinal interpretations ofthe gricean maxims in the generation of referring expres-sions.
Cognitive Science, 18:233?263.D.
DeVault and M. Stone.
2004.
Interpreting vague utter-ances in context.
In Proceedings of COLING 2004, vol-ume 2, pages 1247?1253, Geneva, Switzerland.K.P.
Gapp.
1994.
Basic meanings of spatial relations: Com-putation and evaluation in 3d space.
In Proceedings ofAAAI-94, pages 1393?1398.P.
Gorniak and D. Roy.
2004.
Grounded semantic compo-sition for visual scenes.
Journal of Artificial IntelligenceResearch, 21:429?470.E.
Hajicova?.
1993.
Issues of Sentence Structure and Dis-course Patterns, volume 2 of Theoretical and Computa-tional Linguistics.
Charles University Press.A Herskovits.
1986.
Language and spatial cognition: Aninterdisciplinary study of prepositions in English.
Stud-ies in Natural Language Processing.
Cambridge Univer-sity Press.J.D.
Kelleher and G.J.
Kruijff.
2006.
Incremental genera-tion of spatial referring expressions in situated dialog.
InProceedings ACL/COLING ?06, Sydney, Australia.J.
Kelleher and J. van Genabith.
2004.
Visual salience andreference resolution in simulated 3d environments.
AI Re-view, 21(3-4):253?267.E.
Krahmer and M. Theune.
1999.
Efficient generation ofdescriptions in context.
In R. Kibble and K. van Deemter,editors, Workshop on the Generation of Nominals, ESS-LLI?99, Utrecht, The Netherlands.G.D.
Logan and D.D.
Sadler.
1996.
A computational analy-sis of the apprehension of spatial relations.
In M. Bloom,P.and Peterson, L. Nadell, and M. Garrett, editors, Lan-guage and Space, pages 493?529.
MIT Press.G.D.
Logan.
1994.
Spatial attention and the apprehensionof spatial relations.
Journal of Experimental Psychology:Human Perception and Performance, 20:1015?1036.G.D.
Logan.
1995.
Linguistic and conceptual control of vi-sual spatial attention.
Cognitive Psychology, 12:523?533.R.
Moratz and T. Tenbrink.
2006.
Spatial reference inlinguistic human-robot interaction: Iterative, empiricallysupported development of a model of projective relations.Spatial Cognition and Computation.T.
Oates, Z. Eyler-Walker, and P.R.
Cohen.
2000.
Towardnatural language interfaces for robotic agents: Ground-ing linguistic meaning in sensors.
In Proceedings of theFourth International Conference on Autonomous Agents,pages 227?228.T Regier and L. Carlson.
2001.
Grounding spatial languagein perception: An empirical and computational investi-gation.
Journal of Experimental Psychology: General,130(2):273?298.D.K.
Roy.
2002.
Learning words and syntax for a scenedescription task.
Computer Speech and Language, 16(3).I.F.
van der Sluis and E.J.
Krahmer.
2004.
The influence oftarget size and distance on the production of speech andgesture in multimodal referring expressions.
In R. Kibbleand K. van Deemter, editors, ICSLP04.752
