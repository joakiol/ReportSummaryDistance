Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 291?296,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsEmbedding Methods for Fine Grained Entity Type ClassificationDani YogatamaLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213dyogatama@cs.cmu.eduDan Gillick, Nevena LazicGoogle Research1600 Amphitheatre ParkwayMountain View, CA 94043{dgillick,nevena}@google.comAbstractWe propose a new approach to the taskof fine grained entity type classificationsbased on label embeddings that allows forinformation sharing among related labels.Specifically, we learn an embedding foreach label and each feature such that la-bels which frequently co-occur are close inthe embedded space.
We show that it out-performs state-of-the-art methods on twofine grained entity-classification bench-marks and that the model can exploit thefiner-grained labels to improve classifica-tion of standard coarse types.1 IntroductionEntity type classification is the task of assign-ing type labels (e.g., person, location,organization) to mentions of entities in doc-uments.
These types are useful for deeper naturallanguage analysis such as coreference resolution(Recasens et al, 2013), relation extraction (Yao etal., 2010), and downstream applications such asknowledge base construction (Carlson et al, 2010)and question answering (Lin et al, 2012).Standard entity type classification tasks use asmall set of coarse labels, typically fewer than 20(Hirschman and Chinchor, 1997; Sang and Meul-der, 2003; Doddington et al, 2004).
Recent workhas focused on a much larger set of fine grainedlabels (Ling and Weld, 2012; Yosef et al, 2012;Gillick et al, 2014).
Fine grained labels are typ-ically subtypes of the standard coarse labels (e.g.,artist is a subtype of person and author isa subtype of artist), so the label space forms atree-structured is-a hierarchy.
See Figure 1 for thelabel sets used in our experiments.
A mention la-beled with type artist should also be labeledwith all ancestors of artist.
Since we allowmentions to have multiple labels, this is a multi-label classification task.
Multiple labels typicallycorrespond to a single path in the tree (from rootto a leaf or internal node).An important aspect of context-dependent finegrained entity type classification is that mentionsof an entity can have different types dependingon the context.
Consider the following example:Madonna starred as Breathless Mahoney in thefilm Dick Tracy.
In this context, the most appropri-ate label for the mention Madonna is actress,since the sentence talks about her role in a film.
Inthe majority of other cases, Madonna is likely tobe labeled as a musician.The main difficulty in fine grained entity typeclassification is the absence of labeled training ex-amples.
Training data is typically generated au-tomatically (e.g.
by mapping Freebase labels ofresolved entities), without taking context into ac-count, so it is common for mentions to have noisylabels.
In our example, the labels for the mentionMadonna would include musician, actress,author, and potentially others, even though notall of these labels apply here.
Ideally, a finegrained type classification system should be ro-bust to such noisy training data, as well as capableof exploiting relationships between labels duringlearning.
We describe a model that uses a rank-ing loss?which tends to be more robust to la-bel noise?and that learns a joint representation offeatures and labels, which allows for informationsharing among related labels.1A related idea tolearn output representations for multiclass docu-ment classification and part-of-speech tagging wasconsidered in Srikumar and Manning (2014).
Weshow that it outperforms state-of-the-art methodson two fine grained entity-classification bench-marks.
We also evaluate our model on standardcoarse type classification and find that training em-bedding models on all fine grained labels givesbetter results than training it on just the coarse1Turian et al (2010), Collobert et al (2011), and Qi etal.
(2014) consider representation learning for coarse labelnamed entity recognition.291Figure 1: Label sets for Gillick et al (2014)?left, GFT?and Ling and Weld (2012)?right, FIGER.types of interest.2 ModelsIn this section, we describe our approach, which isbased on the WSABIE (Weston et al, 2011) model.Notation We use lower case letters to denotevariables, bold lower case letters to denote vectors,and bold upper case letters to denote matrices.
Letx ?
RDbe the feature vector for a mention, whereD is the number of features and xdis the value ofthe d-th feature.
Let y ?
{0, 1}Tbe the corre-sponding binary label vector, where T is the num-ber of labels.
yt= 1 if and only if the mentionis of type t. We use ytto denote a one-hot binaryvector of size T , where yt= 1 and all other entriesare zero.Model To leverage the relationships among thefine grained labels, we would like a model that canlearn an embedding space for labels.
Our model,based on WSABIE, learns to map both feature vec-tors and labels to a low dimensional space RH(H is the embedding dimension size) such thateach instance is close to its label(s) in this space;see Figure 2 for an illustration.
Relationships be-tween labels are captured by their distances in theembedded space: co-occurring labels tend to becloser, whereas mutually exclusive labels are fur-ther apart.Formally, we are interested in learning the map-ping functions:f(x) : RD?
RH?t ?
{1, 2, .
.
.
, T}, g(yt) : {0, 1}T?
RHIn this work, we parameterize them as linear func-tions f(x,A) = Ax and g(yt,B) = Byt, whereA ?
RH?Dand B ?
RH?Tare parameters.The score of a label t (represented as a one-hotlabel vector yt) and a feature vector x is the dotA>B>ytxAxBytRHFigure 2: An illustration of the standard WSABIE model.x is the feature vector extracted from a mention, and ytisits label.
Here, black cells indicate non-zero and white cellsindicate zero values.
The parameters are matrices A and Bwhich are used to map the feature vector x and the label vec-tor ytinto an embedding space.product between their embeddings:s(x,yt;A,B) = f(x,A) ?
g(yt,B) = Ax ?BytFor brevity, we denote this score by s(x,yt).
Notethat the total number of parameters is (D+T )?H ,which is typically less than the number of pa-rameters in standard classification models that useregular conjunctions of input features with labelclasses (e.g., logistic regression) when H < T .Learning Since we expect the training data tocontain some extraneous labels, we use a rankingloss to encourage the model to place positive la-bels above negative labels without competing witheach other.
Let Y denote the set of positive labelsfor a mention, and let?Y denote its complement.Intuitively, we try to rank labels in Y higher thanlabels in?Y.
Specifically, we use the weighted ap-proximate pairwise (WARP) loss of Weston et al(2011).
For a mention {x,y}, the WARP loss is:?t?Y??t??YR(rank(x,yt))max(1?
s(x,yt) + s(x,y?t), 0)where rank(x,yt) is the margin-infused rank oflabel t: rank(x,yt) =??t?
?YI(1 + s(x,y?t) >s(x,yt)), R(rank(x,yt)) is a function that trans-forms this rank into a weight.
In this work, since292each mention can have multiple positive labels,we choose to optimize precision at k by settingR(k) =?ki=11i.
Favoring precision over recall infine grained entity type classification makes sensebecause if we are not certain about a particular finegrained label for a mention, we should use its an-cestor label in the hierarchy.In order to learn the parameters with this WARPloss, we use stochastic (sub)gradient descent.Inference During inference, we consider thetop-k predicted labels, where k is the maximumdepth of the label hierarchy, and greedily removelabels that are not consistent with other labels (i.e.,not on the same path of the tree).
For example, ifthe (ordered) top-k labels are person, artist,and location, we output only person andartist as the predicted labels.
We use a thresh-old ?
such that y?t= 1 if s(x,yt) > ?
and y?t= 0otherwise.Kernel extension We extend the WSABIEmodel to include a weighting function betweeneach feature and label, similar in spirit to We-ston et al (2014).
Recall that the WSABIEscoring function is: s(x,yt) = Ax ?
Byt=?d(Adxd)>Bt, where Adand Btdenote the col-umn vectors of A and B.
We can weight each(feature, label) pair by a kernel function prior tocomputing the embedding:s(x,yt) =?dKd,t(Adxd)>Bt,where K ?
RD?Tis the kernel matrix.
We usea N -nearest neighbor kernel2and set Kd,t= 1if Adis one of N -nearest neighbors of the labelvector Bt, and Kd,t= 0 otherwise.
In all ourexperiments, we set N = 200.To incorporate the kernel weighting function,we only need to make minor modifications to thelearning procedure.
At every iteration, we firstcompute the similarity between each feature em-bedding and each label embedding.
For each labelt, we then set the kernel values for the N mostsimilar features to 1, and the rest to 0 (update K).We can then follow the learning algorithm for thestandard WSABIE model described above.
At in-ference time, we fix K so this extension is onlyslightly slower than the standard model.2We explored various kernels in preliminary experimentsand found that the nearest neighbor kernel performs the best.The nearest-neighbor kernel introduces nonlin-earities to the embedding model.
It implicitlyplays the role of a label-dependent feature selector,learning which features can interact with which la-bels and turns off potentially noisy features thatare not in the relevant label?s neighborhood.3 ExperimentsSetup and Baselines We evaluate our methodson two publicly available datasets that are man-ually annotated with gold labels for fine grainedentity type classification: GFT (Google FineTypes; Gillick et al, 2014) and FIGER (Ling andWeld, 2012).
On the GFT dataset, we comparewith state-of-the-art baselines from Gillick et al(2014): flat logistic regression (FLAT), an exten-sion of multiclass logistic regression for multilabelclassification problems; and multiple independentbinary logistic regression (BINARY), one per labelt ?
{1, 2, .
.
.
, T}.
On the FIGER dataset, we com-pare with a state-of-the-art baseline from Ling andWeld (2012).We denote the standard embedding method byWSABIE and its extension by K-WSABIE.
We fixour embedding size to H = 50.
We report micro-averaged precision, recall, and F1-score for eachof the competing methods (this is called Loose Mi-cro by Ling and Weld).
When development data isavailable, we use it to tune ?
by optimizing F1-score.Training data Because we have no manuallyannotated data, we create training data using thetechnique described in Gillick et al (2014).
A setof 133,000 news documents are automatically an-notated by a parser, a mention chunker, and anentity resolver that assigns Freebase types to en-tites, which we map to fine grained labels.
Thisapproach results in approximately 3 million train-ing examples which we use to train all the mod-els evaluated below.
The only difference betweenmodels trained for different tasks is the mappingfrom Freebase types.
See Gillick et al (2014) fordetails.Table 1 lists the features we use?the same setas used by Gillick et al (2014), and very similar tothose used by Ling and Weld.
String features arerandomly hashed to a value in 0 to 999,999, whichsimplifies feature extraction and adds some addi-tional regularization (Ganchev and Dredze, 2008).293Feature Description ExampleHead The syntactic head of the mention phrase ?Obama?Non-head Each non-head word in the mention phrase ?Barack?, ?H.
?Cluster Word cluster id for the head word ?59?Characters Each character trigram in the mention head ?
:ob?, ?oba?, ?bam?, ?ama?, ?ma:?Shape The word shape of the words in the mention phrase ?Aa A. Aa?Role Dependency label on the mention head ?subj?Context Words before and after the mention phrase ?B:who?, ?A:first?Parent The head?s lexical parent in the dependency tree ?picked?Topic The most likely topic label for the document ?politics?Table 1: List of features used in our experiments, similar to features in Gillick et al (2014).
Features are extracted from eachmention.
The example mention in context is ... who Barack H. Obama first picked ....GFT Dev GFT Test FIGERTotal mentions 6,380 11,324 778at Level 1 3,934 7,975 568at Level 2 2,215 2,994 210at Level 3 251 335 ?Table 2: Mention counts in our datasets.GFT evaluation There are T = 86 fine grainedlabels in the GFT dataset, as listed in Figure 1.
Thefour top-level labels are: person, location,organization, and other; the remaining la-bels are subtypes of these labels.
The maximumdepth of a label is 3.
We split the dataset into adevelopment set (for tuning hyperparameters) andtest set (see Table 2).The overall experimental results are shown inTable 3.
Embedding methods performed well.Both WSABIE and K-WSABIE outperformed thebaselines by substantial margins in F1-score,though the advantage of the kernel version overthe linear version is only marginally significant.To visualize the learned embeddings, we projectlabel embeddings down to two dimensions usingPCA in Figure 3.
Since there are only 4 top-levellabels here, the fine grained labels are color-codedaccording to their top-level labels for readability.We can see that related labels are clustered to-gether, and the four major clusters correspond toto the top-level labels.
We note that these first twocomponents only capture 14% of the total varianceof the full 50-dimensional space.Method P R F1FLAT 79.22 60.18 68.40BINARY 80.05 62.20 70.01WSABIE 80.58 66.20 72.68K-WSABIE 80.11 67.01 72.98Table 3: Precision (P), Recall (R), and F1-score on the GFTtest dataset for four competing models.
The improvementsfor WSABIE and K-WSABIE over both baselines are statisti-cally significant (p < 0.01).-0.4 -0.2 0.0 0.2 0.4 0.6 0.8-0.6-0.4-0.20.00.20.40.6PC1PC2organizationcompanybroadcastnewseducationgovernmentmilitarymusicpolitical_partysports_leaguesports_teamstock_exchangetransitlocationcelestialcitycountrygeographybody_of_waterislandmountainparkstructureairporthospitaltelrestaurantsports_facilitytheatertransitbridgerailwayroadotherartbroadcastfilmmusicstagewritingawardbody_partcurrencyeventa cidentelectionholidayatural_disastersports_eventviolent_conflictfoodhealthmaladytreatmentherit geinternetlanguageprogramming_languagelegalliving_thinganimalproductcarcomputermobile_phonesoftwarescientificsports_and_leisuresupernaturalpersonartistactorauthordirectormusicathletebusinesscoachdoctoreducationteacherlegalmilitarypolitical_figurereligious_leadertitleorganizationlocationotherpersonFigure 3: Two-dimensional projections of label embed-dings for GFT dataset.
See text for details.FIGER evaluation Our second evaluationdataset is FIGER from Ling and Weld (2012).
Inthis dataset, there are T = 112 labels organizedin a two-level hierarchy; however, only 102appear in our training data (see Figure 1, takenfrom their paper, for the complete set of labels).The training labels include 37 top-level labels(e.g., person, location, product, art,etc.)
and 75 second-level labels (e.g., actor,city, engine, etc.)
The FIGER dataset is muchsmaller than the GFT dataset (see Table 2).Our experimental results are shown in Ta-ble 4.
Again, K-WSABIE performed the best,followed by the standard WSABIE model.
Bothof these methods significantly outperformed Lingand Weld?s best result.Method P R F1Ling and Weld (2012) ?
?
69.30WSABIE 81.85 63.75 71.68K-WSABIE 82.23 64.55 72.35Table 4: Precision (P), Recall (R), and F1-score on theFIGER dataset for three competing models.
We took the F1score from Ling and Weld?s best result (no precision and re-call numbers were reported).
The improvements for WSABIEand K-WSABIE over the baseline are statistically significant(p < 0.01).294Feature learning We investigate whether hav-ing a large fine grained label space is helpful inlearning a good representation for feature vec-tors (recall that WSABIE learns representations forboth feature vectors and labels).
We focus on thetask of coarse type classification, where we wantto classify a mention into one of the four top-levelGFT labels.
We fix the training mentions and learnWSABIE embeddings for feature vectors and la-bels by (1) training only on coarse labels and (2)training on all labels; we evaluate the models onlyon coarse labels.
Training with all labels givesan improvement of about 2 points (F1 score) overtraining with just coarse labels, as shown in Ta-ble 5.
This suggests that including additional sub-type labels can help us learn better feature embed-dings, even if we are not explicitly interested in thedeeper labels.Training labels P R F1Coarse labels only 82.41 77.87 80.07All labels 85.18 79.28 82.12Table 5: Comparison of two WSABIE models on coarsetype classification for GFT.
The first model only used coarsetop-level labels, while the second model was trained on all 86labels.4 DiscussionDesign of fine grained label hierarchy Resultsat different levels of the hierarchies in Table 6show that it is more difficult to discriminate amongdeeper labels.
However, it appears that the depth-2 FIGER types are easier to discriminate than thedepth-2 (and depth-3) GFT labels.
This may sim-ply be an artifact of the very small FIGER dataset,but it suggests it may be worthwhile to flatten theother subtree ini GFT since many of its subtypesdo not obviously share any information.GFT P R F1LEVEL 1 85.22 80.55 82.82LEVEL 2 56.02 37.14 44.67LEVEL 3 65.12 7.89 14.07FIGER P R F1LEVEL 1 82.82 70.42 76.12LEVEL 2 68.28 47.14 55.77Table 6: WSABIE model?s Precision (P), Recall (R), andF1-score at each level of the label hierarchies for GFT (top)and FIGER (bottom).5 ConclusionWe introduced embedding methods for finegrained entity type classifications that outperformsstate-of-the-art methods on benchmark entity-classification datasets.
We showed that thesemethods learned reasonable embeddings for fine-type labels which allowed information sharingacross related labels.AcknowledgementsWe thank Andrew McCallum for helpful discus-sions and anonymous reviewers for feedback onan earlier draft of this paper.ReferencesAndrew Carlson, Justin Betteridge, Richard C. Wang,Estevam R. Hruschka Jr., and Tom M. Mitchell.2010.
Coupled semi-supervised learning for infor-mation extraction.
In Proc.
of WSDM.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.George Doddington, Alexis Mitchell, Mark Przybocki,Lance Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
The automatic content extrac-tion (ACE) program tasks, data, and evaluation.
InProc.
of LREC.Kuzman Ganchev and Mark Dredze.
2008.
Small sta-tistical models by random feature mixing.
In Pro-ceedings of the ACL08 HLT Workshop on MobileLanguage Processing, pages 19?20.Dan Gillick, Nevena Lazic, Kuzman Ganchev, JesseKirchner, and David Huynh.
2014.
Context-dependent fine-grained entity type tagging.
InarXiv.Lynette Hirschman and Nancy Chinchor.
1997.
MUC-7 named entity task definition.
In Proc.
of MUC-7.Thomas Lin, Mausam, and Oren Etzioni.
2012.
Nonoun phrase left behind: Detecting and typing un-linkable entities.
In Proc.
of EMNLP-CoNLL.Xiao Ling and Daniel S. Weld.
2012.
Fine-grainedentity recognition.
In Proc.
of AAAI.Yanjun Qi, Sujatha Das G, Ronan Collobert, and JasonWeston.
2014.
Deep learning for character-basedinformation extraction.
In Proc.
of ECIR.Marta Recasens, Marie-Catherine de Marneffe, andChristopher Potts.
2013.
The life and death of dis-course entities: Identifying singleton mentions.
InProc.
of NAACL.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the conll-2003 shared task:language-independent named entity recognition.
InProc.
of HLT-NAACL.Vivek Srikumar and Christopher D. Manning.
2014.Learning distributed representations for structuredoutput prediction.
In Proc.
of NIPS.295Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proc.
of ACL.Jason Weston, Samy Bengio, and Nicolas Usunier.2011.
Wsabie: Scaling up to large vocabulary im-age annotation.
In Proc.
of IJCAI.Jason Weston, Ron Weiss, and Hector Yee.
2014.Affinity weighted embedding.
In Proc.
of ICML.Limin Yao, Sebastian Riedel, and Andrew McCallum.2010.
Collective cross-document relation extractionwithout labelled data.
In Proc.
of EMNLP.Mohamed Amir Yosef, Sandro Bauer, Johannes Hof-fart, Marc Spaniol, and Gerhard Weikum.
2012.HYENA: Hierarchical type classification for entitynames.
In Proc.
of COLING.296
