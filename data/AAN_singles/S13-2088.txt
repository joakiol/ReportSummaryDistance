Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 530?534, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsKea: Expression-level Sentiment Analysis from Twitter DataAmeeta AgrawalComputer Science and EngineeringYork UniversityToronto, Canadaameeta@cse.yorku.caAijun AnComputer Science and EngineeringYork UniversityToronto, Canadaaan@cse.yorku.caAbstractThis paper describes an expression-level senti-ment detection system that participated in thesubtask A of SemEval-2013 Task 2: Senti-ment Analysis in Twitter.
Our system uses asupervised approach to learn the features fromthe training data to classify expressions in newtweets as positive, negative or neutral.
Theproposed approach helps to understand the rel-evant features that contribute most in this clas-sification task.1 IntroductionIn recent years, Twitter has emerged as an ubiquitousand an opportune platform for social activity.
Ana-lyzing the sentiments of the tweets expressed by aninternational user-base can provide an approximateview of how people feel.
One of the biggest chal-lenges of working with tweets is their short length.Additionally, the language used in tweets is veryinformal, with creative spellings and punctuation,misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, such as, RTfor ?re-tweet?
and #hashtags, which are a type oftagging for tweets.
Although several systems tacklethe task of analyzing sentiments from tweets, thetask of analyzing sentiments at term or phrase-levelwithin a tweet has remained largely unexplored.This paper describes the details of our expression-level sentiment detection system that participated inthe subtask A of SemEval-2013 Task 2: SentimentAnalysis in Twitter (Wilson et al 2013).
The goalis to mark expressions (a term or short phrases) ina tweet with their contextual polarity.
This is chal-lenging given the fact that the entire length of a tweetis restricted to just 140 characters.
We describe thecreation of an SVM classifier that is used to classifythe contextual polarity of expressions within tweets.A feature set derived from various linguistic fea-tures, parts-of-speech tagging and prior sentimentlexicons was used to train the classifier.2 Related WorkSentiment detection from Twitter data has attractedmuch attention from the research community in re-cent times (Go et al 2009; Pang et al 2002; Pangand Lee, 2004; Wilson et al 2005; T. et al 2012).However, most of these approaches classify entiretweets by their overall sentiment (positive, negativeor neutral).The task at hand is to classify expressions withtheir contextual sentiment.
Most of these expres-sions can be found in sentiment lexicons already an-notated with their general polarity, but the focus ofthis task is to detect the polarity of that expressionwithin the context of the tweet it appears in, andtherefore, given the context, the polarity of the ex-pression might differ from that found in any lexicon.One of the primary goals of this task is to facilitatethe creation of a corpus of tweets with sentiment ex-pressions marked with their contextual sentiments.Wilson, Wiebe and Hoffman (Wilson et al 2005)explored the challenges of contextual polarity ofsentiment expressions by first determining whetheran expression is neutral or polar and then disam-biguating the polarity of the polar expressions.
Na-sukawa and Yi (Nasukawa and Yi, 2003) classified530the polarity of target expressions using manually de-veloped patterns.
Both these approaches, however,experimented with general webpages and online re-views but not Twitter data.3 Task SetupThis paper describes the task of recognizing con-textual sentiments of expressions within a tweet.Formally, given a message containing a marked in-stance of a word or a phrase, the task is to determinewhether that instance is positive, negative or neutralin that context.A corpus of roughly 8343 twitter messages wasmade available by the task organizers, where eachtweet included an expression marked as positive,negative or neutral.
Also available was a develop-ment data set containing 1011 tweets with similarlymarked expressions.
The data sets included mes-sages on a broad range of topics such as a mix-ture of entities (e.g., Gadafi, Steve Jobs), products(e.g., kindle, android phone), and events (e.g., Japanearthquake, NHL playoffs).
Keywords and hashtagswere used to identify and collect messages relevantto the selected topic, which were then annotated us-ing Mechanical Turk.
Further details regarding thetask setup may be found in the task description paper(Wilson et al 2013).The evaluation consisted of classifying 4435 ex-pressions in a Twitter data set.
Furthermore, to testthe generalizability of the systems, the task organiz-ers provided a test data set consisting of 2334 SMSmessages, each containing a marked expression, forwhich no prior training data set was made available.4 System DescriptionOur aim by participating in the SemEval-2013 Sen-timent Analysis in Twitter task was to investigatewhat features are most useful in distinguishing thedifferent polarities.
The various steps of building oursystem are described in detail as follows.4.1 TokenizationTweets are known for being notoriously noisy dueto their length restricted to just 140 characters whichforces users to be creative in order to get their mes-sages across.
This poses an inherent challenge whenanalyzing tweets which need to undergo some sig-nificant preprocessing.
The first step includes tok-enizing the words in the tweet.
Punctuation is identi-fied during the tokenization process and marked forinclusion as one of the features in the feature set.This includes Twitter-specific punctuation such as?#?
hashtags, specific emoticons such as ?:)?
andany URL links are replaced by a ?URL?
placeholder.4.2 n-gram featuresEach expression consists of one or more words, withthe average number of words in an expression in thetraining data set found to be 2.
We derive lower-caseunigram and bigram as well as the full string featuresfrom the expressions which are represented by theirfrequency counts in the feature set.
The n-gramswere cleaned (stripped of any punctuation) beforebeing included in the feature set as they were ob-served to provide better results than noisy n-grams.Note that the presence of punctuation did become apart of the feature set as described in 4.3.
We alsoexperimented with word-splitting, especially foundin hashtags (e.g., #iamsohappy); however, contraryto our initial supposition, this step resulted in poorerresults overall due to word-splitting error propaga-tion and was therefore avoided.4.3 POS taggingFor tagging the various parts-of-speech of a tweet,we use the POS tagger (Gimpel et al 2011) that isespecially designed to work with English data fromTwitter.
The tagging scheme encompasses 25 tags(please see (Gimpel et al 2011) for the full listing),including some Twitter-specific tags (which couldmake up as much as 13% of all tags as shown intheir annotated data set) such as ?#?
hashtag (indi-cates topic/category for tweet), ?@?
at-mention (in-dicates another user as a recipient of a tweet), ?RT?re-tweets and URL or email addresses.
The punctu-ation (such as ?
:-)?, ?
:b?, ?
(:?, amongst others) fromthe n-grams is captured using the ?emoticon?
and?punctuation?
tags that are explicitly identified bythis POS tagger trained especially for tweets.Table 1 shows an example using a subset of twoPOS tags for an expression (# Adj.
and # Emoti-con denotes the number of adjectives and emoticonsrespectively).
Other POS tags include nouns (NN),verbs (VB) and so on.
Features incorporating theinformation about the parts-of-speech of the expres-531Esperance will be without star player Youssef Msakni for the first leg of theChampions League final against Al Ahly on Saturday.
#AFRICAPrior Polarity Length POS in Expression POS in Tweet n-gramsPos.
Neg.
Exp.
Tweet #Adj.
#Emoticon #Adj.
#NN ?without?
?star?
?without star?
...0 0 3 23 0 0 1 13 1 1 1 ...Table 1: Sample feature set for an expression (denoted in bold)sion as well as the tweet denoted by their frequenciesproduced better results than using a binary notation.Hence frequency counts were used in the feature set.4.4 Prior sentiment lexiconA prior sentiment lexicon was generated by combin-ing four already existing polarity lexicons includingthe Opinion Lexicon (Hu and Liu, 2004), the Sen-tiWordNet (Esuli and Sebastiani, 2006), the Subjec-tivity Clues database (Wilson et al 2005) and theGeneral Inquirer (Stone and Hunt, 1963).
If anyof the words in the expression are also found in theprior sentiment lexicon, then the frequencies of suchprior positive and negative words are included asfeatures in the feature set.4.5 Other featuresOther features found to be useful in the classificationprocess include the length of the expression as wellas the length of the tweet.
A sample of the featureset is shown in Table 1.4.6 ClassifierDuring development time, we experimented withdifferent classifiers but in the end, the Support Vec-tor Machines (SVM), using the polynomial kernel,trained over tweets from the provided train and de-velopment data outperformed all the other classi-fiers.
The final feature set included four main fea-tures plus the n-grams as well as the features depict-ing the presence or absence of a POS in the expres-sion and the tweet.5 Experiments and DiscussionThe task organizers made available a test data setcomposed of 4435 tweets where each tweet con-tained an instance of an expression whose sentimentwas to be detected.
Another test corpus of 2334SMS messages was also used in the evaluation totest how well a system trained on tweets generalizeson other data types.The metric for evaluating the systems is F-measure.
We participated in the ?constrained?
ver-sion of the task which meant working with only theprovided training data and no additional tweets/SMSmessages or sentences with sentiment annotationswere used.
However, other resources such as sen-timent lexicons can be incorporated into the system.Table 2, which presents the results of our submis-sion in this task, lists the F-score of the positive,negative and neutral classes on the Twitter test data,whereas Table 3 lists the results of the SMS mes-sage data.
As it can be observed from the results,the negative sentiments are classified better than thepositive ones.
We reckon this may be due to thecomparatively fewer ways of expressing a positiveemotion, while the negative sentiment seems to havea much wider vocabulary (our sentiment lexicon has25% less positive words than negative).
Whereasthe positive class has a higher precision, the nega-tive class seems to have a more notable recall.
Themost striking observation, however, is the extremelylow F-score for the neutral class.
This may be due tothe highly skewed proportion (less than 5%) of neu-tral instances in the training data.
In future work, itwill be interesting to see how balancing out the pro-portions of the three classes affects the classificationaccuracy.We also ran some ablation experiments on theprovided Twitter and SMS test data sets after thesubmission.
Table 4 reports the findings of exper-iments where, for example, ?- prior polarities?
in-dicates a feature set excluding the prior polarities.The metric used here is the macro-averaged F-scoreof the positive and the negative class.
The baselinemeasure implements a simple SVM classifier usingonly the words as unigram features in the expres-sion.
Interestingly, contrary to our hypothesis dur-532ing development time, using the POS of the entiretweet was the least helpful feature.
Since this wasan expression level classification task, it seems thatusing the POS features of the entire tweet may mis-guide the classifier.
Unsurprisingly, the prior po-larities turned out to be the most important part ofthe feature set for this classification task as it seemsthat many of the expressions?
contextual polaritiesremained same as their prior polarities.Class Precision Recall F-scorePositive 0.93 0.47 0.62Negative 0.50 0.95 0.65Neutral 0.15 0.12 0.13Macro-average 0.6394Table 2: Submitted results: Twitter test dataClass Precision Recall F-scorePositive 0.85 0.39 0.53Negative 0.59 0.96 0.73Neutral 0.18 0.06 0.09Macro-average 0.6327Table 3: Submitted results: SMS test dataTwitter SMSBaseline 0.821 0.824Full feature set (submitted) 0.639 0.632- Prior polarities 0.487 0.494- Lengths 0.612 0.576- POS expressions 0.646 0.615- POS tweets 0.855 0.856Table 4: Macro-averaged F-score results using differentfeature sets6 ConclusionThis paper presented the details of our system whichparticipated in the subtask A of SemEval-2013 Task2: Sentiment Analysis in Twitter.
An SVM classifierwas trained on a feature set consiting of prior po-larities, various POS and other Twitter-specific fea-tures.
Our experiments indicate that prior polari-ties from sentiment lexicons are significant featuresin this expression level classification task.
Further-more, a classifier trained on just tweets can general-ize considerably well on SMS message data as well.As part of our future work, we would like to explorewhat features are more helpful in not only classify-ing the positive class better, but also distinguishingneutrality from polarity.AcknowledgmentsWe would like to thank the organizers of this taskand the reviewers for their useful feedback.
Thisresearch is funded in part by the Centre for In-formation Visualization and Data Driven Design(CIV/DDD) established by the Ontario ResearchFund.ReferencesAndrea Esuli and Fabrizio Sebastiani.
2006.
Sentiword-net: A publicly available lexical resource for opin-ion mining.
In In Proceedings of the 5th Conferenceon Language Resources and Evaluation (LRECa?06,pages 417?422.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging fortwitter: annotation, features, and experiments.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers - Volume 2, HLT?11, pages 42?47, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.Processing, pages 1?6.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, KDD ?04, pages 168?177, New York, NY, USA.
ACM.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Sentimentanalysis: capturing favorability using natural languageprocessing.
In Proceedings of the 2nd internationalconference on Knowledge capture, K-CAP ?03, pages70?77, New York, NY, USA.
ACM.Bo Pang and Lillian Lee.
2004.
A sentimental edu-cation: sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, ACL ?04, Stroudsburg, PA, USA.Association for Computational Linguistics.533Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In Proceedings of the ACL-02 conference on Empirical methods in natural lan-guage processing - Volume 10, EMNLP ?02, pages 79?86, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Philip J.
Stone and Earl B.
Hunt.
1963.
A computer ap-proach to content analysis: studies using the generalinquirer system.
In Proceedings of the May 21-23,1963, spring joint computer conference, AFIPS ?63(Spring), pages 241?256, New York, NY, USA.
ACM.Amir Asiaee T., Mariano Tepper, Arindam Banerjee, andGuillermo Sapiro.
2012.
If you are happy and youknow it... tweet.
In Proceedings of the 21st ACMinternational conference on Information and knowl-edge management, CIKM ?12, pages 1602?1606, NewYork, NY, USA.
ACM.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the confer-ence on Human Language Technology and EmpiricalMethods in Natural Language Processing, HLT ?05,pages 347?354, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, SaraRosenthal, Veselin Stoyanov, and Alan Ritter.
2013.Semeval-2013 task 2: Sentiment analysis in twitter.
InProceedings of the International Workshop on Seman-tic Evaluation, SemEval ?13, June.534
