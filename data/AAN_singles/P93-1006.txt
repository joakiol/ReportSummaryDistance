USING BRACKETED PARSES TO EVALUATE A GRAMMAR CHECKINGAPPLICATIONRichard H. Wojcik, Philip Harrison, John BremerBoeing Computer Services Research and Technology DivisionP.O.
Box 24346, MS 7L--43Seattle, WA 98124-2964Internet: rwojcik@boeing.com, pharrison@boeing.com, jbremer@boeing.comAbstractWe describe a method for evaluating a grammarchecking application with hand-bracketed parses.A randomly-selected setof sentences was sub-mitted to a grammar checker in both bracketed andunbracketed formats.
A comparison of the result-ing error reports illuminates the relationship be-tween the underlying performance of the parser-grammar system and the error critiques presentedto the user.INTRODUCTIONThe recent development of broad-coveragenatural anguage processing systems has stimu-lated work on the evaluation of the syntactic com-ponent of such systems, for purposes of basic eval-uation and improvement of system performance.Methods utilizing hand-bracketed corpora (suchas the University of Pennsylvania Treebank) as abasis for evaluation metrics have been discussedin Black et al (1991), Harrison et al (1991), andBlack et al (1992).
Three metrics discussed inthose works were the Crossing Parenthesis Score(a count of the number of phrases in the machineproduced parse which cross with one or morephrases in the hand parse), Recall (the percentageof phrases in the hand parse that are also in the ma-chine parse), and Precision (the percentage ofphrases in the machine parse that are in the handparse).We have developed a methodology for usinghand-bracketed parses to examine both the inter-nal and external performance of a grammarchecker.
The internal performance r fers to thebehavior of the underlying system--i.e, the toke-nizer, parser, lexicon, and grammar.
The externalperformance r fers to the error critiques generatedby the system.
1 Our evaluation methodology re-lies on three separate error reports generated froma corpus of randomly selected sentences: 1) areport based on unbracketed sentences, 2)a reportbased on optimally bracketed sentences with ourcurrent system, and 3) a report based on the opti-mal bracketings with the system modified to in-sure the same coverage as the unbracketed corpus.The bracketed report from the unmodified systemtells us something about the coverage of ourunderlying system in its current state.
The brack-eted report from the modified system tells ussomething about he external accuracy of the errorreports presented to the user.Our underlying system uses a bottom-up, fun-ambiguity parser.
Our error detection methodrelies on including grammar ules for parsingerrorful sentences, with error critiques being gen-erated from the occurrence of an error rule in theparse.
Error critiques are based on just one of allthe possible parse trees that he system can find fora given sentence.
Our major concern about theunderlying system is whether the system has a cor-rect parse for the sentence inquestion.
We are alsoconcerned about the accuracy of the selectedparse, but our current methodology does notdirectly address that issue, because correct errorreports do not depend on having precisely the cor-rect parse.
Consequently, our evaluation of theunderlying rammatical coverage is based on asimple metric, namely the parser success rate forsatisfying sentence bracketings (i.e.
correctparses).
Either the parser can produce the optimalparse or it can't.We have a more complex approach to evaluat-ing the performance of the system's ability todetect errors.
Here, we need to look at both the1.
We use the term critique to represent aninstance of an error detected.
Each sentence mayhave zero or more critiques reported for it.38overgeneration a d undergeneration of individualerror critiques.
What is the rate of spurious cri-tiques, or critiques incorrectly reported, and whatis the rate of missed critiques, or critiques notreported.
Therefore we define two additional met-rics, which illuminate the spurious and missed cri-tique rates, respectively:Precision: the percentage of correct critiquesfrom the unbracketed corpus.Recall: the percentage ofcritiques generated froman ideal bracketed corpus that are alsopresent among those in the unbracketedcorpus.Precision tells us what percentage ofreported cri-tiques are reliable, and Recall tells iJs what per-centage of correct critiques have been reported(modulo the coverage).OVERVIEW OF THE APPL ICAT IONThe Boeing Simplified English Checker (a.k.a.the BSEC, cf.
Hoard, Wojcik, and Holzhauser1992) is a type of grammar and style checker, butit is more accurately described as a 'controlled En-glish checker' (cf.
Adriaens 1992).
That is, it re-ports to users on where a text fails to comply withthe aerospace standard for maintenance documen-tation known as Simplified English (AECMA1989).
If the system cannot produce a parse, itprints the message "Can't do SE check."
At pres-ent, the Checker achieves parses for about 90 per-cent of the input strings ubmitted to it.
2 The accu-racy of the error critiques over that 90 percentvaries, but our subjective xperience suggests thatmost sentence reports contain critiques that areuseful in that they flag some bona fide failure tocomply with Simplified English.The NLP methodology underlying the BSECdoes not rely on the type of pattern matching tech-niques used to flag errors in more conventionalcheckers.
It cannot afford simply to ignore sen-tences that are too complex to handle.
As a con-trolled sublanguage, Simplified English requires2.
The 90 percent figure is based on randomsamplings taken from maintenance documents sub-mitted to the BSEC over the past wo years.
Thisfigure has remained relatively consistent for main-tenance documentation, although itvaries withother text domains.that every word conform to specified usage.
Thatis, each word must be marked as 'allowed' in thelexicon, or it will trigger an error critique.
Sincethe standard generally requires that words be usedin only one part of speech, the BSEC produces aparse tree on which to judge vocabulary usage aswell as other types of grammatical violations) Asone would expect, the BSEC often has to choosebetween quite a few alternative parse trees, some-times even hundreds or thousands of them.
Givenits reliance on full-ambiguity parse forests andrelatively little semantic analysis, we have beensomewhat surprised that it works as well as it does.We know of few grammar and style checkersthat rely on the complexity of grammatical naly-sis that the BSEC does, but IBM's Critique is cer-tainly one of the best known.
In discussing the ac-curacy of Critique, Richardson andBraden-Harder (1993:86) define it as "the actual'under the covers' natural language processing in-volved, and the user's perception."
In otherwords, there are really two levels upon which togauge accuracy--that of the internal parser andthat of the reports generated.
They add: "Giventhe state of the art, we may consider it a blessingthat it is possible for the latter to be somewhat bet-ter than the former."
The BSEC, like Critique, ap-pears to be smarter than it really is at guessingwhat the writer had in mind for a sentence struc-ture.
Most error critiques are not affected by incor-rect phrasal attachment, although grossly incor-rect parses lie behind most sentence r ports that gosour.
What we have not fully understood in thepast is the extent to which parsing accuracy affectserror critiques.
What if we could eliminate all thebad parses?
Would that make our system more ac-curate by reducing incorrect critiques, or would itdegrade performance by reducing the overallnumber of correct critiques reported?
We knewthat he system was capable of producing ood er-ror reports from relatively bad parses, but howmany of those error reports even had a reasonablycorrect parse available to them?3.
The Simplified English (SE) standard allowssome exceptions tothe 'single part of speech' rulein its core vocabulary of about a thousand words.The BSEC currently does little to guarantee thatwriters have used a word in the 'Simplified Eng-lish' meaning, only that hey have selected the cor-rect part of speech.39OVERVIEW OF SIMPLIFIEDENGLISHThe SE standard consists of a set of grammar,style, format, and vocabulary restrictions, not allof which lend themselves tocomputational analy-sis.
A computer program cannot yet support thoseaspects of the standard that require deep under-standing, e.g.
the stricture against using a word inany sense other than the approved one, or the re-quirement to begin paragraphs with the topic sen-tence.
What a program can do is count he numberof words in sentences and compound nouns, detectviolations of parts of speech, flag the omission ofrequired words (such as articles) orthe presence ofbanned words (such as auxiliary have and be, etc.
).The overall function of such a program is to pres-ent the writer with an independent check on a fairrange of Simplified English requirements.
Forfurther details on Simplified English and theBSEC, see Hoard et al (1992) and Wojcik et al(1990).Although the BSEC detects a wide variety ofSimplified English and general writing violations,only the error categories in Table 1 are relevant tothis study: Except for illegal comma usage, whichis rather uncommon, the above errors are amongthe most frequent types of errors detected by theBSEC.To date, The Boeing Company is the only aero-space manufacturer to produce aprogram that de-tects such a wide range of Simplified Englishviolations.
In the past, Boeing and other compa-nies have created checkers that report on all wordsthat are potential violations of SE, but such 'wordcheckers' have no way of avoiding critiques forword usage that is correct.
For example, if theword test is used legally as a noun, the word-checking program will still flag the word as a po-tential verb-usage rror.
The BSEC is the onlySimplified English checker in existence that man-ages to avoid this.
aAs Richardson and Braden-Harder (p. 88)pointed out: "We have found...that professionalsseem much more forgiving of wrong critiques, as4.
Oracle's recently released CoAuthor product,which is designed to be used with the Interleafword processor, has the potential to produce gram-matical analyses of sentences, but it only works asa Simplified English word checker at present.long as the time required to disregard them is mini-mal."
In fact, the chief complaint of Boeing tech-nical writers who use the BSEC is when it pro-duces too many nuisance errors.
Soword-checking programs, while inexpensive andeasy to produce, do not address the needs of Sim-plified English writers.POS A known word is used in in-correct part of speech.NON-SE An unapproved word is used.MISSING Articles must be used wherev-ARTICLE er possible in SE.PASSIVE Passives are usually illegal.TWO-COMMANDCommands may not be con-joined when they represent se-quential activities.
Simulta-neous commands may be con-i joined.ING Progressive participles maynot be used in SE.COMMA A violation of comma usage.ERRORi WARNING/CAUTIONWarnings and cautions mustappear in a special format.Usually, an error arises when adeclarative sentence has beenused where an imperative oneis required.Table 1.
Error Types Detected By The BSECTHE PARSER UNDERLY ING THEBSECThe parser underlying the Checker (cf.
Harri-son 1988) is loosely based on GPSG.
The gram-mar contains over 350 rules, and it has been imple-mented in Lucid Common Lisp running on Sunworkstations.
5 Our approach to error critiquingdiffers from that used by Critique (Jensen, Hei-dorn, Miller, and Ravin 1993).
Critique uses atwo-pass approach that assigns an initial canoni-cal parse in so-called 'Chomsky-normal' form.The second pass produces an altered tree that is an-5.
The production version of the BSEC is actual-ly a C program that emulates the lisp developmentversion.
The C version accepts the same rules asthe lisp version, but there are some minor differ-ences between itand the lisp version.
This paperis based solely on the lisp version of the BSEC.40notated for style violations.
No-parses cause thesystem to attempt a 'fitted parse', as a means ofproducing some information on more seriousgrammar violations.
As mentioned earlier, theBSEC generates parse forests that represent allpossible ambiguities vis-a-vis the grammar.There is no 'canonical' parse, nor have we yet im-plemented a 'fitted parse' strategy to reclaim in-formation available in no-parses.
6 Our problemhas been the classic one of selecting the best parsefrom a number of alternatives.
Before the SEChecker was implemented, Boeing's parser hadbeen designed to arrive at a preferred or 'fronted'parse tree by weighting grammatical rules andword entries according to whether we deemedthem more or less desirable.
This strategy is quitesimilar to the one described in Heidorn 1993 andother works that he cites.
In the maintenancemanual domain, we simply observed the behaviorof the BSEC over many sentences and adjusted theweights of rules and words as needed.To get a better idea of how our approach tofronting works, consider the ambiguity in the fol-lowing two sentences:(1) The door was closed.
(2) The damage was repaired.In the Simplified English domain, it is more likelythat (2) will be an example of passive usage, thuscalling for an error report.
To parse (1) as a passivewould likely be incorrect in most cases.
We there-fore assigned the adjective reading of closed a lowweight in order to prefer an adjectival over a verbreading.
Sentence (2) reports a likely event ratherthan a state, and we therefore weight repaired tobe preferred as a passive verb.
Although thismethod for selecting fronted parse trees some-times leads to false error critiques, it works wellfor most cases in our domain.BRACKETED INPUT STRINGSIn order to coerce our system into acceptingonly the desired parse tree, we modified it to ac-cept only parses that satisfied bracketed forms.6.
The BSEC has the capability to report on po-tential word usage violations in no-parses, but theend-users seem to prefer not to use it.
It is oftendifficult o say whether information will be viewedas help or as clutter in error reports.For example, the following sentence produces fiveseparate parses because our grammar attachesprepositional phrases to preceding noun phrasesand verb phrases in several ways.
The structuralambiguity corresponds tofive different interpreta-tions, depending on whether the boy uses a tele-scope, the hill has a telescope on it, the girl on thehill has a telescope, and so on.
(3) The boy saw the girl on the hill with atelescope.We created a lisp operation called spe, for"string, parse, and evaluate," which takes an inputstring and a template.
It returns all possible parsetrees that fit the template.
Here is an example ofan spe form for (3):(SPE 'q'he boy saw the girl on the hill with atelescope.
"(S (NP the boy)(VP (V saw)(NP (NP the girl)(PP on (NP (NP the hill)(PP with a telescope)))))))The above bracketing restricts the parses to justthe parse tree that corresponds to the sense inwhich the boy saw the girl who is identified as be-ing on the hill that has a telescope.
If run throughthe BSEC, this tree will produce an error messagethat is identical to the unbracketed report--viz.that boy, girl, hill, and telescope are NON-SEwords.
In this case, it does not matter which treeis fronted.
As with many sentences checked, theinherent ambiguity in the input string does not af-fect the error critique.Recall that some types of ambiguity do affectthe error reports----e.g, passive vs. adjectival parti-cipial forms.
Here is how the spe operation wasused to disambiguate a sentence from our data:(SPE "Cracks in the impeller blades are not permitted"(S (NP Cracks in the impeller blades)(VP are not (A permitted))))We judged the word permitted to have roughly thesame meaning as stative 'permissible' here, andthat led us to coerce an adjectival reading in thebracketed input.
If the unbracketed input had re-suited in the verb reading, then it would haveflagged the sentence as an illegal passive.
It turnedout that the BSEC selected the adjective reading41in the unbracketed sentence, and there was no dif-ference between the bracketed and unbracketed r-ror critiques in this instance.METHODOLOGYWe followed this procedure in gathering andanalyzing our data: First, we collected aset of datafrom nightly BSEC batch runs extending over athree month period from August through October1991.
The data set consisted of approximately20,000 sentences from 183 documents.
Not all ofthe documents were intended to be in SimplifiedEnglish when they were originally written.
Wewrote a shell program to extract apercentage-stra-tified sample from this data.
After extracting a testset, we ended up culling the data for duplicates,tables, and other spurious data that had made itpast our initial filter.
7 We ended up with 297 sen-tences in our data set.We submitted the 297 sentences to the currentsystem and obtained an error report, which we callthe unbracketed report.
We then created spe formsfor each sentence.
By observing the parse treeswith our graphical interface, we verified that theparse tree we wanted was the one produced by thespe operation.
For 49 sentences, our system couldnot produce the desired tree.
We ran the currentsystem, using the bracketed sentences to producethe unmodified bracketed report.
Next weexamined the 24 sentences which did not haveparses atisfying their bracketings but did, never-theless, have parses in the unbracketed report.
Weadded the lexical information and new grammarrules needed to enable the system to parse thesesentences.
Running the resulting system pro-duced the modified bracketed report.
These newparses produced critiques that we used to evaluatethe critiques previously produced from theunbracketed corpus.
The comparison of theunbracketed report and the modified bracketedreport produced the estimates of Precision andRecall for this sample.'7.
The BSEC falters out tables and certain othertypes of input, but the success rate varies with thetype of text.RESULTSOur 297-sentence corpus had the followingcharacteristics.
The length of the sentences rangedbetween three words and 32 words.
The mediansentence length was 12 words, and the mean was13.8 words, s Table 2 shows the aggregated out-comes for the three reports.Checker Unbrack- Unmodi- ModifiedOutcome eted fled Brack-Brack- etedetedNO 25 49 25PARSENO 123 134 137ERRORONE OR 149 114 135MOREERRORSTotals 297 297 297Table 2: Overview Of The ResultsThe table shows the coverage of the system and theimpact of the spurious parses.
The coverage isreflected in the Unmodified Bracketed column,where 248 parses indicates a coverage of 84 per-cent for the underlying system in this domain.
Thetable also reveals that there were 24 spuriousparses in the unbracketed corpus, correspondingto no valid parse tree in our grammar.
The Modi-fied Bracketed column shows the effect on thereport generator of forcing the system to have thesame coverage as the unbracketed run.Table 3 shows by type the errors detected ininstances where errors were reported.
The Spuri-ous Error column indicates the number of errorsfrom the unbracketed sentences which we judgedto be bad.
The Missed Errors column indicates er-rors which were missed in the unbracketed report,but which showed up in the modified bracketed8.
Since most of the sentences inour corpus wereintended to be in Simplified English, it is not sur-prising that they tended to be under the 20 wordlimit imposed by the standard.42report.
The modified bracketed report containedonly 'actual' Simplified English errors.CategoryPOSNON-SEMISSINGARTICLENOUNCLUS-TERPASSIVETWO-COM-MANDINGCOMMAERRORWARN-ING/CAU-TIONTotalTable 3:Un- Spuri- Miss- Actualbrack- ous ed Errorseted Errors ErrorsErrors120 22 7 10571 6 5 7038 13 1 2630 7 5 2817 7 8 1814 3 3 145 2 0 35 4 0 12 0 0 2302 64 29Types Of Errors Detected267For this data, the estimate of Precision (rate ofcorrect error critiques for unbracketed ata) is(302-64)/302, or 79 percent.
We estimate that thisprecision rate is accurate to within 5 percent with95 percent confidence.
Our estimate of Recall(rate of correct critiques from the set of possiblecritiques) is (267-29)/267, or 89 percent.
We esti-mate that this Recall rate is accurate to within 4percent with 95 percent confidence.It is instructive to look at a report hat containsan incorrectly identified error.
The following re-port resulted from our unbracketed test run:ff strut requires six fluid ounces or more to fill, findleakage source and repair.Two commands - possible error:find leakage source and repairNoun errors:fillAllowed as: VerbVerb errors:requiresUse: be necessaryMissing articles:strutleakage sourceThe bracketed run produced a no-parse for thissentence because of an inadequacy inour grammarthat blocked fill from parsing as a verb.
Since itparsed as a noun in the unbracketed run, the sys-tem complained thatfill was allowed as a verb.
Inour statistics, we counted thefill Noun error as anincorrect POS error and the requires Verb error asa correct one.
This critique contains two POS er-rors, one TWO-COMMAND error, and two MIS-SING ARTICLE error.
Four of the five error cri-tiques are accurate.D ISCUSSIONWe learned several things about our systemthrough this exercise.
First, we learned that the actof comparing unbracketed and unmodifiedbracketed sentences revealed worse performancein the underlying system than we anticipated.
Wehad expected there to be a few more no-parseswith unmodified bracketing, but not so manymore.
Second, the methodology helped us todetect some obscure bugs in the system.
For ex-ample, the TWO-COMMAND and NOUNCLUSTER errors were not being flagged properlyin the unmodified bracketed set because of bugs inthe report generator.
These bugs had not been not-iced because the errors were being flagged proper-ly in some sentences.
When a system gets as largeand complicated as ours, especially when it gener-ates hundreds or thousands of parse trees for somesentences, it becomes very difficult o detect errorsthat only show up sporadically and infrequently in43the data.
Our new methodology provided us witha window on that aspect of system performance.Perhaps a more interesting observation con-cerns the relationship between our system and onelike Critique, which relies on no-parses to triggera fitted parse 'damage repair' phase.
We believethat the fitted-parse strategy is a good one, al-though we have not yet felt a strong need to imple-ment it.
The reason is that our system generatessuch rich parse forests that strings which ought otrigger no-parses quite frequently end up trigger-ing 'weird' parses.
That is, they trigger parses thatare grammatical from a strictly syntactic perspec-five, but inappropriate for the words in their accus-tomed meanings.
A fitted parse strategy wouldnot work with these cases, because the system hasno way of detecting weirdness.
Oddly enough, theexistence of weird parses often has the same effectin error eports as parse fitting in that hey generateerror critiques which are useful.
The more ambi-guity a syntactic system generates, the less likelyit is to need a fitted parse strategy to handle unex-pected input.
The reason for this is that he numberof grammatically correct, but 'senseless' parses islarge enough to get a parse that would otherwisebe ruled out on semantic grounds.Our plans for the use of this methodology are asfollows.
First, we intend to change our currentsystem to improve deficiencies and lack of cover-age revealed by this exercise.
In effect, we plan touse the current est corpus as a training corpus inthe next phase.
Before deploying the changes, wewill collect a new test corpus and repeat ourmethod of evaluation.
We are very interested inseeing how this new cycle of development willaffect the figures of coverage, Precision, andRecall on the next evaluation.REFERENCESAdriaens, G. 1992.
From COGRAM to ALCO-GRAM: Toward a Controlled English Gram-mar Checker.
Proceedings of the fifteenth In-ternational Conference on ComputationalLinguistics.
Ch.
Boitet, ed.
Nantes: COL-ING.
Pp.
595-601.AECMA.
1989.
A Guide for the Preparation ofAircraft Maintenance Documentation i theAerospace Maintenance Language.
AECMASimplified English.
AECMA Document:PSC-85-16598, Change 5.
Paris.Black, E., S. Abney, D. Flickinger, C. Gdaniec, R.Grishman, E Harrison, D. Hindle, R. Ingria,E Jelinek, J. Klavans, M. Liberman, M. Mar-cus, S. Roukos, B. Santorini, and T. Strzal-kowski.
1991.
A Procedure for Quantitative-ly Comparing the Syntactic Coverage ofEnglish Grammars.
Proceedings of theFourth DARPA Speech and Natural Lan-guage Workshop.
Pp.
306-311.Black, E., J. Lafferty, Salim Roukos.
1992.
De-velopment and Evaluation of a Broad-Cover-age Probabilistic Grammar of English-Lan-guage Computer Manuals.
Proceedings of the30th Annual Meeting of the Association forComputational Linguistics.
Pp.
185-192.Gazdar, G., E. Klein, G. Pullum, and I.
Sag.
1985.Generalized Phrase Structure Grammar.Cambridge, Mass.
: Harvard University Press.Harrison, P. 1988.
A New Algorithm for ParsingGeneralized Phrase Structure Grammars.Unpublished Ph.D. dissertation.
Seattle:University of Washington.Harrison, E, S. Abney, E. Black, D. Flickinger, C.Gdaniec, R, Grishman, D. Hindle, R. Ingria,M.
Marcus, B. Santorini, and T. Strzalkowski.1991.
Evaluating Syntax Performance ofParser/Grammars of English.
Proceedings ofNatural Language Processing Systems Evalu-ation Workshop.
Berkeley, California.Heidorn, G. 1993.
Experience with an EasilyComputed Metric for Ranking AlternativeParses.
In Jensen, Heidorn, and Richardson1993.
Pp.
29-45.Hoard, J. E., R. H. Wojcik, and K. Holzhauser.1992.
An Automated Grammar and StyleChecker for Writers of Simplified English.
InEO.
Holt and N. Williams, eds.
1992.
Holt,E O.
1992.
Computers and Writing: State ofthe Art.
Boston: Kluwer.Jensen, K. 1993.
PEG: The PLNLP EnglishGrammar.
In Jensen, Heidorn, and Richard-son 1993.
Pp.
29-45.Jensen, K., G. Heidorn, L. Miller, and Y. Ravin.1993, Parse Fitting and Prose Fixing.
In Jen-sen, Heidorn, and Richardson 1993.
Pp.53-64.44Jensen, K., G. Heidorn, and S. Richardson, eds.1993.
Natural Language Processing: ThePLNLP Approach.
Boston: Kluwer.Ravin, Y.
1993.
Grammar Errors and Style Weak-nesses in a Text-Critiquing System.
In Jen-sen, Heidorn, and Richardson 1993.
Pp.65-76.Richardson, S. and L. Braden-Harder.
1993.
TheExperience of Developing a Large-Scale Nat-ural Language Processing System: Critique.In Jensen, Heidorn, and Richardson 1993.
Pp.78-89.Wojcik, R. H., J. E. Hoard, K. Holzhauser.
1990.The Boeing Simplified English Checker.
Pro-ceedings of the International Conference, Hu-man Machine Interaction and Artificial Intel-ligence in Aeronautics and Space.
Toulouse:Centre d'Etudes et de Recherches de Tou-louse.
Pp.
43-57.45
