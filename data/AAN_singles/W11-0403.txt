Proceedings of the Fifth Law Workshop (LAW V), pages 21?29,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsAnalysis of the Hindi Proposition Bank using Dependency StructureAshwini Vaidya Jinho D. Choi Martha Palmer Bhuvana NarasimhanInstitute of Cognitive ScienceUniversity of Colorado at Boulder{vaidyaa,choijd,mpalmer,narasimb}@colorado.eduAbstractThis paper makes two contributions.
First, wedescribe the Hindi Proposition Bank that con-tains annotations of predicate argument struc-tures of verb predicates.
Unlike PropBanksin most other languages, the Hind PropBankis annotated on top of dependency structure,the Hindi Dependency Treebank.
We explorethe similarities between dependency and pred-icate argument structures, so the PropBank an-notation can be faster and more accurate.
Sec-ond, we present a probabilistic rule-based sys-tem that maps syntactic dependents to seman-tic arguments.
With simple rules, we classifyabout 47% of the entire PropBank argumentswith over 90% confidence.
These preliminaryresults are promising; they show how wellthese two frameworks are correlated.
This canalso be used to speed up our annotations.1 IntroductionProposition Bank (from now on, PropBank) is a cor-pus in which the arguments of each verb predicateare annotated with their semantic roles (Palmer etal., 2005).
PropBank annotation has been carriedout in several languages; most of them are annotatedon top of Penn Treebank style phrase structure (Xueand Palmer, 2003; Palmer et al, 2008).
However, adifferent grammatical analysis has been used for theHindi PropBank annotation, dependency structure,which may be particularly suited for the analysis offlexible word order languages such as Hindi.As a syntactic corpus, we use the Hindi Depen-dency Treebank (Bhatt et al, 2009).
Using de-pendency structure has some advantages.
First, se-mantic arguments1 can be marked explicitly on thesyntactic trees, so annotations of the predicate ar-gument structure can be more consistent with thedependency structure.
Second, the Hindi Depen-dency Treebank provides a rich set of dependencyrelations that capture the syntactic-semantic infor-mation.
This facilitates mappings between syntac-tic dependents and semantic arguments.
A success-ful mapping would reduce the annotation effort, im-prove the inter-annotator agreement, and guide a fullfledged semantic role labeling task.In this paper, we briefly describe our annotationwork on the Hindi PropBank, and suggest mappingsbetween syntactic and semantic arguments based onlinguistic intuitions.
We also present a probabilisticrule-based system that uses three types of rules toarrive at mappings between syntactic and semanticarguments.
Our experiments show some promisingresults; these mappings illustrate how well those twoframeworks are correlated, and can also be used tospeed up the PropBank annotation.2 Description of the Hindi PropBank2.1 BackgroundThe Hindi PropBank is part of a multi-dimensionaland multi-layered resource creation effort for theHindi-Urdu language (Bhatt et al, 2009).
Thismulti-layered corpus includes both dependency an-notation as well as lexical semantic information inthe form of PropBank.
The corpus also producesphrase structure representations in addition to de-1The term ?semantic argument?
is used to indicate all num-bered arguments as well as modifiers in PropBank.21pendency structure.
The Hindi Dependency Tree-bank has created an annotation scheme for Hindiby adapting labels from Panini?s Sanskrit gram-mar (also known as CPG: Computational PaninianGrammar; see Begum et al (2008)).
Previous workhas demonstrated that the English PropBank tagsetis quite similar to English dependency trees anno-tated with the Paninian labels (Vaidya et al, 2009).PropBank has also been mapped to other depen-dency schemes such as Functional Generative De-scription (Cinkova, 2006).2.2 Hindi Dependency TreebankThe Hindi Dependency Treebank (HDT) includesmorphological, part-of-speech and chunking infor-mation as well as dependency relations.
These arerepresented in the Shakti Standard Format (SSF; seeBharati et al (2007)).
The dependency labels de-pict relations between chunks, which are ?minimalphrases consisting of correlated, inseparable enti-ties?
(Bharati et al, 2006), so they are not neces-sarily individual words.
The annotation of chunksalso assumes that intra-chunk dependencies can beextracted automatically (Husain et al, 2010).The dependency tagset consists of about 43 labels,which can be grouped into three categories: depen-dency relation labels, modifier labels, and labels fornon-dependencies (Bharati et al, 2009).
PropBankis mainly concerned with those labels depicting de-pendencies in the domain of locality of verb predi-cates.
The dependency relation labels are based onthe notion of ?karaka?, defined as ?the role played bya participant in an action?.
The karaka labels, k1-5,are centered around the verb?s meaning.
There areother labels such as rt (purpose) or k7t (location)that are independent of the verb?s meaning.2.3 Annotating the Hindi PropBankThe Hindi PropBank (HPB) contains the labeling ofsemantic roles, which are defined on a verb-by-verbbasis.
The description at the verb-specific level isfine-grained; e.g., ?hitter?
and ?hittee?.
These verb-specific roles are then grouped into broader cate-gories using numbered arguments (ARG#).
Eachverb can also have modifiers not specific to the verb(ARGM*).
The annotation process takes place in twostages: the creation of frameset files for individualverb types, and the annotation of predicate argu-ment structures for each verb instance.
As annota-tion tools, we use Cornerstone and Jubilee (Choi etal., 2010a; Choi et al, 2010b).
The annotation isdone on the HDT; following the dependency anno-tation, PropBank annotates each verb?s syntactic de-pendents as their semantic arguments at the chunklevel.
Chunked trees are conveniently displayed forannotators in Jubilee.
PropBank annotations gener-ated in Jubilee can also be easily projected onto theSSF format of the original dependency trees.The HPB currently consists of 24 labels includingboth numbered arguments and modifiers (Table 1).In certain respects, the HPB labels make some dis-tinctions that are not made in some other languagesuch as English.
For instance, ARG2 is subdividedinto labels with function tags, in order to avoidARG2 from being semantically overloaded (Yi,2007).
ARGC and ARGA mark the arguments of mor-phological causatives in Hindi, which is differentfrom the ARG0 notion of ?causer?.
We also intro-duce two labels to represent the complex predicateconstructions: ARGM-VLV and ARGM-PRX.Label DescriptionARG0 agent, causer, experiencerARG1 patient, theme, undergoerARG2 beneficiaryARG3 instrumentARG2-ATR attribute ARG2-GOL goalARG2-LOC location ARG2-SOU sourceARGC causerARGA secondary causerARGM-VLV verb-verb constructionARGM-PRX noun-verb construction2ARGM-ADV adverb ARGM-CAU causeARGM-DIR direction ARGM-DIS discourseARGM-EXT extent ARGM-LOC locationARGM-MNR manner ARGM-MNS meansARGM-MOD modal ARGM-NEG negationARGM-PRP purpose ARGM-TMP temporalTable 1: Hindi PropBank labels.2.4 Empty arguments in the Hindi PropBankThe HDT and HPB layers have different ways ofhandling empty categories (Bhatia et al, 2010).HPB inserts empty arguments such as PRO (emptysubject of a non-finite clause), RELPRO (empty22relative pronoun), pro (pro-drop argument), andgap-pro (gapped argument).
HPB annotates syn-tactic relations between its semantic roles, notablyco-indexation of the empty argument PRO as well asgap-pro.
The example in Figure 1 shows that Mo-han and PRO are co-indexed; thus, Mohan becomesARG0 of read via the empty argument PRO.
There isno dependency link between PRO and read becausePRO is inserted only in the PropBank layer.Mohan wanted to read a book????_?PRO??
???????Mohan_ERGk1vmodARG1ARG0ARG0???
?PRO book read wantk2ARG1Figure 1: Empty argument example.
The upper and loweredges indicate HDT and HPB labels, respectively.3 Comparisons between syntactic andsemantic argumentsIn this section, we describe the mappings betweenHDT and HPB labels based on our linguistic intu-itions.
We show that there are several broad similar-ities between two tagsets.
These mappings form thebasis for our linguistically motivated rules in Sec-tion 4.2.3.
In section 5.5, we analyze whether theintuitions discussed in this section are borne out bythe results of our probabilistic rule-based system.3.1 Numbered argumentsThe numbered arguments correspond to ARG0-3,including function tags associated with ARG2.
InPropBank, ARG0 and ARG1 are conceived asframework-independent labels, closely associatedwith Dowty?s Proto-roles (Palmer et al, 2010).
Forinstance, ARG0 corresponds to the agent, causer, orexperiencer, whether it is realized as the subject ofan active construction or as the object of an adjunct(by phrase) of the corresponding passive.
In this re-spect, ARG0 and ARG1 are very similar to k1 andk2 in HDT, which are annotated based on their se-mantic roles, not their grammatical relation.
On theother hand, HDT treats the following sentences sim-ilarly, whereas PropBank does not:?
The boy broke the window.?
The window broke.The boy and the window are both considered k1 forHDT, whereas PropBank labels the boy as ARG0 andThe window as ARG1.
The window is not consid-ered a primary causer as the verb is unaccusative forPropbank.
For HDT, the notion of unaccusativity isnot taken into consideration.
This is an importantdistinction that needs to be considered while carry-ing out the mapping.
k1 is thus ambiguous betweenARG0 and ARG1.
Also, HDT makes a distinctionbetween Experiencer subjects of certain verbs, label-ing them as k4a.
As PropBank does not make sucha distinction, k4a maps to ARG0.
The Experiencersubject information is included in the correspondingframeset files of the verbs.
The mappings to ARG0and ARG1 would be accurate only if they make useof specific verb information.
The mappings for othernumbered arguments as well as ARGC and ARGA aregiven in Table 2.HDT label HPB labelk1 (karta); k4a (experiencer) Arg0k2 (karma) Arg1k4 (beneficiary) Arg2k1s (attribute) Arg2-ATRk5 (source) Arg2-SOUk2p (goal) Arg2-GOLk3 (instrument) Arg3mk1 (causer) ArgCpk1 (secondary causer) ArgATable 2: Mappings to the HPB numbered arguments.Note that in HDT annotation practice, k3 and k5tend to be interpreted in a broad fashion such thatthey map not only to ARG3 and ARG2-SOU, but alsoto ARGM-MNS and ARGM-LOC (Vaidya and Husain,2011).
Hence, a one-to-one mapping for these la-bels is not possible.
Furthermore, the occurrence ofmorphological causatives (ARGC and ARGA) is fairlylow so that we may not be able to test the accuracyof these mappings with the current data.3.2 ModifiersThe modifiers in PropBank are quite similar in theirdefinitions to certain HDT labels.
We expect a fairlyhigh mapping accuracy, especially as these are notverb-specific.
Table 3 shows mappings between23HDT labels and HPB modifiers.
A problematic map-ping could be ARGM-MNR, which is quite coarse-grained in PropBank, applying not only to adverbsof manner, but also to infinitival adjunct clauses.HDT label HPB labelsent-adv (epistemic adv) ArgM-ADVrh (cause/reason) ArgM-CAUrd (direction) ArgM-DIRrad (discourse) ArgM-DISk7p (location) ArgM-LOCadv (manner adv) ArgM-MNRrt (purpose) ArgM-PRPk7t (time) ArgM-TMPTable 3: Mappings to the HPB modifiers.3.3 Simple and complex predicatesHPB distinguishes annotations between simple andcomplex predicates.
Simple predicates consist ofonly a single verb whereas complex predicates con-sist of a light verb and a pre-verbal element.
Thecomplex predicates are identified with a special labelARGM-PRX (ARGument-PRedicating eXpresstion),which is being used for all light verb annotationsin PropBank (Hwang et al, 2010).
Figure 2 showsan example of the predicating noun mention anno-tated as ARGM-PRX, used with come.
The predicat-ing noun also has its own argument, matter of, in-dicated with the HDT label r6-k1.
The HDT hastwo labels, r6-k1 and r6-k2, for the arguments ofthe predicating noun.
Hence, the argument span forcomplex predicates includes not only direct depen-dents of the verb but also dependents of the noun.??????_?_?????
??????_??
????_??
??
?_?
?hearing_of_during Wed._of matter_of mention_tok7tk7tpofARGM-PRXARG1ARGM-TMPcome??
?r6-k1ARGM-TMPDuring the hearing on Wednesday, the matter was mentionedFigure 2: Complex predicate example.The ARGM-PRX label usually overlaps with theHDT label pof, indicating a ?part of units?
as pre-verbal elements in complex predicates.
However, incertain cases, HPB has its own analysis for noun-verb complex predicates.
Hence, not all the nom-inals labeled pof are labeled as ARGM-PRX.
Inthe example in Figure 3, the noun chunk importantprogress is not considered to be an ARGM-PRX byHPB (in this example, we have pragati hona; (lit)progess be; to progress).
The nominal for PropBankis in fact ARG1 of the verb be, rather than a com-posite on the verb.
Additional evidence for this isthat neither the nominal nor the light verb seem toproject arguments of their own.Important progress has been made in this work??_???_?
?_?k7ppofARG1ARGM-LOC??????
??_??
?
?this_work_LOC important_progress be_PRESFigure 3: HDT vs. HPB on complex predicates.4 Automatic mapping of HDT to HPBMapping between syntactic and semantic structureshas been attempted in other languages.
The PennEnglish and Chinese Treebanks consist of several se-mantic roles (e.g., locative, temporal) annotated ontop of Penn Treebank style phrase structure (Marcuset al, 1994; Xue and Palmer, 2009).
The ChinesePropBank specifies mappings between syntactic andsemantic arguments in frameset files (e.g., SBJ ?ARG0) that can be used for automatic mapping (Xueand Palmer, 2003).
However, these Chinese map-pings are limited to certain types of syntactic argu-ments (mostly subjects and objects).
Moreover, se-mantic annotations on the Treebanks are done inde-pendently from PropBank annotations, which causesdisagreement between the two structures.Dependency structure transparently encodes rela-tions between predicates and their arguments, whichfacilitates mappings between syntactic and seman-tic arguments.
Hajic?ova?
and Kuc?erova?
(2002) triedto project PropBank semantic roles onto the PragueDependency Treebank, and showed that the projec-tion is not trivial.
The same may be true to our case;however, our goal is not to achieve complete map-pings between syntactic and semantic arguments,24but to find a useful set of mappings that can speedup our annotation.
These mappings will be appliedto our future data as a pre-annotation stage, so thatannotators do not need to annotate arguments thathave already been automatically labeled by our sys-tem.
Thus, it is important to find mappings with highprecision and reasonably good recall.In this section, we present a probabilistic rule-based system that identifies and classifies semanticarguments in the HPB using syntactic dependents inthe HDT.
This is still preliminary work; our systemis expected to improve as we annotate more data anddo more error analysis.4.1 Argument identificationIdentifying semantic arguments of each verb pred-icate is relatively easy given the dependency Tree-bank.
For each verb predicate, we consider all syn-tactic dependents of the predicate as its semanticarguments (Figure 4).
For complex predicates, weconsider the syntactic dependents of both the verband the predicating noun (cf.
Section 3.3).??
????
?
??
?
?_ ?
??
???
?
?_ ?
?Kishori Haridwar_from Delhi come_bek1k5k2pARG2-GOLARG2-SOUARG0Kishori came from Haridwar to DelhiFigure 4: Simple predicate example.With our heuristics, we get a precision of 99.11%,a recall of 95.50%, and an F1-score of 97.27% forargument identification.
Such a high precision isexpected as the annotation guidelines for HDT andHPB generally follow the same principles of iden-tifying syntactic and semantic arguments of a verb.About 4.5% of semantic arguments are not identi-fied by our method.
Table 4 shows distributions ofthe most frequent non-identified arguments.Label Dist.
Label Dist.
Label Dist.ARG0 3.21 ARG1 0.90 ARG2?
0.09Table 4: Distributions of non-identified arguments causedby PropBank empty categories (in %).Most of the non-identified argument are antecedentsof PropBank empty arguments.
As shown in Fig-ure 1, the PropBank empty argument has no depen-dency link to the verb predicate.
Identifying sucharguments requires a task of empty category reso-lution, which will be explored as future work.
Fur-thermore, we do not try to identify PropBank emptyarguments for now, which will also be explored later.4.2 Argument classificationGiven the identified semantic arguments, we classifytheir semantic roles.
Argument classification is doneby using three types of rules.
Deterministic rules areheuristics that are straightforward given dependencystructure.
Empirically-derived rules are generatedby measuring statistics of dependency features in as-sociation with semantic roles.
Finally, linguistically-motivated rules are derived from our linguistic intu-itions.
Each type of rule has its own strength; howto combine them is the art we need to explore.4.2.1 Deterministic ruleOnly one deterministic rule is used in our system.When an identified argument has a pof dependencyrelation with its predicate, we classify the argu-ment as ARGM-PRX.
This emphasizes the advan-tage of using our dependency structure: classifyingARGM-PRX cannot be done automatically in mostother languages where there is no information pro-vided for light verb constructions.
This determin-istic rule is applied before any other type of rule.Therefore, we do not generate further rules to clas-sify the ARGM-PRX label.4.2.2 Empirically-derived rulesThree kinds of features are used for the generation ofempirically-derived rules: predicate ID, predicate?svoice type, and argument?s dependency label.
Thepredicate ID is either the lemma or the roleset IDof the predicate.
Predicate lemmas are already pro-vided in HDT.
When we use predicate lemmas, weassume no manual annotation of PropBank.
Thus,rules generated from predicate lemmas can be ap-plied to any future data without modification.
Whenwe use roleset ID?s, we assume that sense annota-tions are already done.
PropBank includes anno-tations of coarse verb senses, called roleset ID?s,that differentiate each verb predicate with different25senses (Palmer et al, 2005).
A verb predicate canform several argument structures with respect to dif-ferent senses.
Using roleset ID?s, we generate morefine-grained rules that are specific to those senses.The predicate?s voice type is either ?active?
or?passive?, also provided in HDT.
There are not manyinstances of passive construction in our current data,which makes it difficult to generate rules generalenough for future data.
However, even with the lackof training instances, we find some advantage of us-ing the voice feature in our experiments.
Finally, theargument?s dependency label is the dependency la-bel of an identified argument with respect to its pred-icate.
This feature is straightforward for the case ofsimple predicates.
For complex predicates, we usethe dependency labels of arguments with respect totheir syntactic heads, which can be pre-verbal ele-ments.
Note that rules generated with complex pred-icates contain slightly different features for predicatelemmas as well; instead of using predicate lemmas,we use joined tags of the predicate lemmas and thelemmas of pre-verbal elements.ID V Drel PBrel #come a k1 ARG0 1come a k5 ARG2-SOU 1come a k2p ARG2-GOL 1come mention a k7t ARGM-TMP 2come mention a r6-k1 ARG1 1Table 5: Rules generated by the examples in Figures 4 and2.
The ID, V, and Drel columns show predicate ID, predicate?svoice type, and argument?s dependency label.
The PBrel col-umn shows the PropBank label of each argument.
The # columnshows the total count of each feature tuple being associated withthe PropBank label.
?a?
stands for active voice.Table 5 shows a set of rules generated by the exam-ples in Figures 4 (come) and 2 (come mention).
Norule is generated for ARGM-PRX because the labelis already covered by our deterministic rule (Sec-tion 4.2.1).
When roleset ID?s are used in place ofthe predicate ID, come and come mention are re-placed with A.03 and A.01, respectively.
Theserules can be formulated as a function rule such that:rule(id, v, drel) = argmax i P (pbreli)where P (pbreli) is a probability of the predictedPropBank label pbreli, given a tuple of features(id, v, drel).
The probability is measured by es-timating a maximum likelihood of each PropBanklabel being associated with the feature tuple.
Forexample, a feature tuple (come, active, k1) can beassociated with two PropBank labels, ARG0 andARG1, with counts of 8 and 2, respectively.
In thiscase, the maximum likelihoods of ARG0 and ARG1being associated with the feature tuple is 0.8 and 0.2;thus rule(come, active, k1) = ARG0.Since we do not want to apply rules with low con-fidence, we set a threshold to P (pbrel), so predic-tions with low probabilities can be filtered out.
Find-ing the right threshold is a task of handling the pre-cision/recall trade-off.
For our experiments, we ran10-fold cross-validation to find the best threshold.4.2.3 Linguistically-motivated rulesLinguistically-motivated rules are applied to argu-ments that the deterministic rule and empirically-derived rules cannot classify.
These rules capturegeneral correlations between syntactic and seman-tic arguments for each predicate, so they are not asfine-grained as empirically-derived rules, but can behelpful for predicates not seen in the training data.The rules are manually generated by our annota-tors and specified in frameset files.
Table 6 showslinguistically-motivated rules for the predicate ?A(come)?, specified in the frameset file, ?A-v.xml?.3Roleset Usage RuleA.01 to comek1 ?
ARG1k2p ?
ARG2-GOLA.03 to arrivek1 ?
ARG1k2p ?
ARG2-GOLk5 ?
ARG2-SOUA.02 light verb No rule providedTable 6: Rules for the predicate ?A (come)?.The predicate ?A?
has three verb senses and eachsense specifies a different set of rules.
For instance,the first rule of A.01 maps a syntactic dependentwith the dependency label k1 to a semantic ar-gument with the semantic label ARG1.
Note thatframeset files include rules only for numbered ar-guments.
Most of these rules should already be in-cluded in the empirically-derived rules as we gain3See Choi et al (2010a) for details about frameset files.26more training data; however, for an early stage ofannotation, these rules provide useful information.5 Experiments5.1 CorpusAll our experiments use a subset of the Hindi Depen-dency Treebank, distributed by the ICON?10 con-test (Husain et al, 2010).
Our corpus contains about32,300 word tokens and 2,005 verb predicates, inwhich 546 of them are complex predicates.
Eachverb predicate is annotated with a verse sense speci-fied in its corresponding frameset file.
There are 160frameset files created for the verb predicates.
Thenumber may seem small compared to the numberof verb predicates.
This is because we do not cre-ate separate frameset files for light verb construc-tions, which comprise about 27% of the predicateinstances (see the example in Table 6).All verb predicates are annotated with argumentstructures using PropBank labels.
A total of 5,375arguments are annotated.
Since there is a relativelysmall set of data, we do not make a separate set forevaluations.
Instead, we run 10-fold cross-validationto evaluate our rule-based system.5.2 Evaluation of deterministic ruleFirst, we evaluate how well our deterministic ruleclassifies the ARGM-PRX label.
Using the determin-istic rule, we get a 94.46% precision and a 100%recall on ARGM-PRX.
The 100% recall is expected;the precision implies that about 5.5% of the time,light verb annotations in the HPB do not agree withthe complex predicate annotations (pof relation) inthe HDT (cf.
Section 3.3).
More analysis needs tobe done to improve the precision of this rule.5.3 Evaluation of empirically-derived rulesNext, we evaluate our empirically-derived rules withrespect to the different thresholds set for P (pbreli).In general, the higher the threshold is, the higherand lower the precision and recall become, respec-tively.
Figure 5 shows comparisons between preci-sion and recall with respect to different thresholds.Notice that a threshold of 1.0, meaning that usingonly rules with 100% confidence, does not give thehighest precision.
This is because the model withthis high of a threshold overfits to the training data.Rules that work well in the training data do not nec-essarily work as well on the test data.0 0.2 0.4 0.6 0.8 1304050607080ThresholdAccuracy (in %)RF1P0.93Figure 5: Accuracies achieved by the empirically derivedrules using (lemma, voice, label) features.
P, R, and F1stand for precisions, recalls, and F1-scores, respectively.We need to find a threshold that gives a high preci-sion (so annotators do not get confused by the au-tomatic output) while maintaining a good recall (soannotations can go faster).
With a threshold of 0.93using features (lemma, voice, dependency label), weget a precision of 90.37%, a recall of 44.52%, andan F1-score of 59.65%.
Table 7 shows accuraciesfor all PropBank labels achieved by a threshold of0.92 using roleset ID?s instead of predicate?s lem-mas.
Although the overall precision stays about thesame, we get a noticeable improvement in the over-all recall using roleset ID?s.
Note that some labelsare missing in Table 7.
This is because either theydo not occur in our current data (ARGC and ARGA)or we have not started annotating them properly yet(ARGM-MOD and ARGM-NEG).5.4 Evaluation of linguistically-motivated rulesFinally, we evaluate the impact of the linguistically-motivated rules.
Table 8 shows accuracies achievedby the linguistically motivated rules applied after theempirically derived rules.
As expected, the linguis-tically motivated rules improve the recall of ARGNsignificantly, but bring a slight decrease in the pre-cision.
This shows that our linguistic intuitions aregenerally on the right track.
We may combine someof the empirically derived rules with linguisticallymotivated rules together in the frameset files so an-notators can take advantage of both kinds of rules inthe future.27Dist.
P R F1ALL 100.00 90.59 47.92 62.69ARG0 17.50 95.83 67.27 79.05ARG1 27.28 94.47 61.62 74.59ARG2 3.42 81.48 37.93 51.76ARG2-ATR 2.54 94.55 40.31 56.52ARG2-GOL 1.61 64.29 21.95 32.73ARG2-LOC 0.87 90.91 22.73 36.36ARG2-SOU 0.83 78.26 42.86 55.38ARG3 0.08 0.00 0.00 0.00ARGM-ADV 3.50 31.82 3.93 7.00ARGM-CAU 1.44 50.00 5.48 9.88ARGM-DIR 0.43 100.00 18.18 30.77ARGM-DIS 1.63 26.67 4.82 8.16ARGM-EXT 1.42 0.00 0.00 0.00ARGM-LOC 10.77 83.80 27.42 41.32ARGM-MNR 6.00 57.14 9.18 15.82ARGM-MNS 0.79 77.78 17.50 28.57ARGM-PRP 2.15 65.52 17.43 27.54ARGM-PRX 10.75 94.46 100.00 97.15ARGM-TMP 7.01 74.63 14.04 23.64Table 7: Labeling accuracies achieved by the empirically de-rived rules using (roleset ID, voice, label) features and a thresh-old of 0.92.
The accuracy for ARGM-PRX is achieved by thedeterministic rule.
The Dist.
column shows a distribution ofeach label.Dist.
P R F1ALL 100.00 89.80 55.28 68.44ARGN 54.12 91.87 72.36 80.96ARGM 45.88 85.31 35.14 49.77ARGN w/o LM 93.63 58.76 72.21Table 8: Labeling accuracies achieved by the linguisticallymotivated rules.
The ARGN and ARGM rows show statistics ofall numbered arguments and modifiers combined, respectively.The ?ARGN w/o LM?
row shows accuracies of ARGN achievedonly by the empirically derived rules.5.5 Error anlaysisThe precision and recall results for ARG0 and ARG1,are better than expected, despite the complexity ofthe mapping (Section 3.1).
This is because they oc-cur most often in the corpus, so enough rules canbe extracted.
The other numbered arguments areclosely related to particular types of verbs (e.g., mo-tion verbs for ARG2-GOL|SOU).
Our linguisticallymotivated rules are more effective for these typesof HPB labels.
We would expect the modifiers tobe mapped independently of the verb, but our ex-periments show that the presence of the verb lemmafeature enhances the performance of modifiers.
Al-though section 3.2 expects one-to-one mappings formodifiers, it is not the case in practice.We observe that the interpretation of labels in an-notation practice is important.
For example, our sys-tem performs poorly for ARGM-ADV because the la-bel is used for various sentential modifiers and canbe mapped to as many as four HDT labels.
On theother hand, HPB makes some fine-grained distinc-tions.
For instance, means and causes are distin-guished using ARGM-CAU and ARGM-MNS labels, adistinction that HDT does not make.
In the examplein Figure 6, we find that aptitude with is assigned toARGM-MNS, but gets the cause label rh in HDT.Rajyapal can call upon any party with his aptitude???????Rajyapal???his??
?
?_ ?aptitude_with??
?
?_ ??any_EMPH???
?_ ??party_DAT???
?_ ???
?_ ?call_can_beFigure 6: Means vs. cause example.6 Conclusion and future workWe provide an analysis of the Hindi PropBank anno-tated on the Hindi Dependency Treebank.
There isan interesting correlation between dependency andpredicate argument structures.
By analyzing thesimilarities between the two structures, we find rulesthat can be used for automatic mapping of syntacticand semantic arguments, and achieve over 90% con-fidence for almost half of the data.
These rules willbe applied to our future data, which will make theannotation faster and possibly more accurate.We plan to use different sets of rules generated bydifferent thresholds to see which rule set leads to themost effective annotation.
We also plan to developa statistical semantic role labeling system in Hindi,once we have enough training data.
In addition, wewill explore the possibility of using existing lexicalresource such as WordNet (Narayan et al, 2002) toimprove our system.AcknowledgementsThis work is supported by NSF grants CNS- 0751089, CNS-0751171, CNS-0751202, and CNS-0751213.
Any opinions,findings, and conclusions or recommendations expressed in thismaterial are those of the authors and do not necessarily reflectthe views of the National Science Foundation.28ReferencesRafiya Begum, Samar Husain, Arun Dhwaj, Dipti MisraSharma, Lakshmi Bai, and Rajeev Sangal.
2008.
De-pendency annotation scheme for indian languages.
InIn Proceedings of the 3rd International Joint Confer-ence on Natural Language Processing, IJCNLP?08.Akshar Bharati, Dipti Misra Sharma, Lakshmi Bai, andRajeev Sangal.
2006.
AnnCorra: Guidelines for POSand Chunk Annotation for Indian Languages.
Techni-cal report, IIIT Hyderabad.Akshar Bharati, Rajeev Sangal, and Dipti Misra Sharma.2007.
Ssf: Shakti standard format guide.
Technicalreport, IIIT Hyderabad.Akshara Bharati, Dipti Misra Sharma, Samar Husain,Lakshmi Bai, Rafiya Begam, and Rajeev Sangal.2009.
Anncorra : Treebanks for indian languages,guidelines for annotating hindi treebank.
Technical re-port, IIIT Hyderabad.Archna Bhatia, Rajesh Bhatt, Bhuvana Narasimhan,Martha Palmer, Owen Rambow, Dipti Misra Sharma,Michael Tepper, Ashwini Vaidya, and Fei Xia.
2010.Empty categories in a hindi treebank.
In Proceedingsof the 7th International Conference on Language Re-sources and Evaluation (LREC?10), pages 1863?1870.Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,Owen Rambow, Dipti Sharma, and Fei Xia.
2009.
AMulti-Representational and Multi-Layered Treebankfor Hindi/Urdu.
In In the Proceedings of the Third Lin-guistic Annotation Workshop held in conjunction withACL-IJCNLP 2009.Jinho D. Choi, Claire Bonial, and Martha Palmer.
2010a.Propbank frameset annotation guidelines using a ded-icated editor, cornerstone.
In Proceedings of the 7thInternational Conference on Language Resources andEvaluation, LREC?10, pages 3650?3653.Jinho D. Choi, Claire Bonial, and Martha Palmer.
2010b.Propbank instance annotation guidelines using a ded-icated editor, jubilee.
In Proceedings of the 7th In-ternational Conference on Language Resources andEvaluation, LREC?10, pages 1871?1875.Silvie Cinkova.
2006.
From PropBank to EngVALLEX:Adapting PropBank-Lexicon to the Valency Theory ofFunctional Generative Description.
In Proceedingsof the fifth International conference on Language Re-sources and Evaluation (LREC 2006), Genova, Italy.Eva Hajic?ova?
and Ivona Kuc?erova?.
2002.
Argu-ment/valency structure in propbank, lcs database andprague dependency treebank: A comparative pi-lot study.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation,LREC?02, pages 846?851.Samar Husain, Prashanth Mannem, Bharat Ram Ambati,and Phani Gadde.
2010.
The ICON-2010 tools conteston Indian language dependency parsing.
In Proceed-ings of ICON-2010 Tools Contest on Indian LanguageDependency Parsing, ICON?10, pages 1?8.Jena D. Hwang, Archna Bhatia, Claire Bonial, AousMansouri, Ashwini Vaidya, Nianwen Xue, and MarthaPalmer.
2010.
PropBank Annotation of MultilingualLight Verb Constructions.
In Proceedings of the Lin-guistic Annotation Workshop at ACL 2010.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert Macintyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The penn tree-bank: Annotating predicate argument structure.
InARPA Human Language Technology Workshop, pages114?119.Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande,and Pushpak Bhattacharyya.
2002.
An experiencein building the indo wordnet - a wordnet for hindi.In Proceedings of the 1st International Conference onGlobal WordNet.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106.Martha Palmer, Olga Babko-Malaya, Ann Bies, MonaDiab, Mohamed Maamouri, Aous Mansouri, and Wa-jdi Zaghouani.
2008.
A pilot arabic propbank.
In Pro-ceedings of the 6th International Language Resourcesand Evaluation, LREC?08, pages 28?30.Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010.Semantic role labeling.
In Graeme Hirst, editor, Syn-thesis Lectures on Human Language Technologies.Morgan and Claypool.Ashwini Vaidya and Samar Husain.
2011.
A classifica-tion of dependencies in the Hindi/Urdu Treebank.
InPresented at the Workshop on South Asian Syntax andSemantics, Amherst, MA.Ashwini Vaidya, Samar Husain, and Prashanth Mannem.2009.
A karaka based dependency scheme for En-glish.
In Proceedings of the CICLing-2009, MexicoCity, Mexico.Nianwen Xue and Martha Palmer.
2003.
Annotating thepropositions in the penn chinese treebank.
In Proceed-ings of the 2nd SIGHAN workshop on Chinese lan-guage processing, SIGHAN?03, pages 47?54.Nianwen Xue and Martha Palmer.
2009.
Adding seman-tic roles to the chinese treebank.
Natural LanguageEngineering, 15(1):143?172.Szu-Ting Yi.
2007.
Automatic Semantic Role Labeling.Ph.D.
thesis, University of Pennsylvania.29
