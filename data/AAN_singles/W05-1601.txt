Statistical Generation: Three Methods Compared and EvaluatedAnja BelzITRI, University of BrightonLewes Road, Brighton BN2 4GJ, UKAnja.Belz@itri.brighton.ac.ukAbstractStatistical NLG has largely meant n-gram mod-elling which has the considerable advantages oflending robustness to NLG systems, and of makingautomatic adaptation to new domains from raw cor-pora possible.
On the downside, n-gram models areexpensive to use as selection mechanisms and havea built-in bias towards shorter realisations.
This pa-per looks at treebank-training of generators, an al-ternative method for building statistical models forNLG from raw corpora, and two different ways ofusing treebank-trained models during generation.Results show that the treebank-trained generatorsachieve improvements similar to a 2-gram gener-ator over a baseline of random selection.
How-ever, the treebank-trained generators achieve thisat a much lower cost than the 2-gram generator,and without its strong preference for shorter real-isations.1 IntroductionTraditionally, NLG systems have been built as deterministicdecision-makers, that is to say, they generate one string ofwords for any given input, in a sequence of decisions that in-creasingly specify the word string.
In practice, this has meantcarefully handcrafting generators to make decisions locally,at each step in the generation process.
Such generators tendto be specialised and domain-specific, are hard to adapt tonew domains, or even subdomains, and have no way of deal-ing with incomplete or incorrect input.
Wide-coverage toolsonly exist for surface realisation, and tend to require highlyspecific and often idiosyncratic inputs.
The rest of NLP hasreached a stage where state-of-the-art tools are expected tobe generic: wide-coverage, reusable and robust.
NLG is lag-ging behind the field on all three counts.The last decade has seen a new generation of NLG method-ologies that are characterised by a separation between the def-inition of the generation space (all possible generation pro-cesses from inputs to outputs) on the one hand, and controlover the decisions that lead to a (set of) output realisation(s),on the other.In making this separation, generate-and-select NLG takesseveral crucial steps towards genericity: reusing systems be-comes easier if the selection mechanism can be adjusted orreplaced separately, without changing the definition of thegeneration space; coverage can be increased more easily ifevery expansion of the generation space does not have to beaccompanied by handcrafted rules controlling the resultingincrease in nondeterminism; and certain types of selectionmethods can provide robustness, for example through prob-abilistic choice.Statistical generation has aroused by far the most interestamong these methods, and it has mostly meant n-gram se-lection: a packed representation of all alternative (partial)realisations is produced, and an n-gram language model isapplied to select the most likely realisation.
N -gram meth-ods have several desirable properties: they offer a fully auto-matic method for building and adapting control mechanismsfor generate-and-select NLG from raw corpora (reusability);they base selections on statistical models (robustness); andthey can potentially be used for deep as well as surface gen-eration.However, n-gram models are expensive to apply: in orderto select the most likely realisation according to an n-grammodel, all alternative realisations have to be generated andthe probability of each realisation according to the model hasto be calculated.
This can get very expensive (even if packedrepresentations of the set of alternatives are used), especiallywhen the system accepts incompletely specified input, be-cause the number of alternatives can be vast.
In Halogen,Langkilde [2000] deals with trillions of alternatives, and thegenerator used in the experiments reported in this paper hasup to 1040 alternative realisations (see Section 4.3 for empir-ical evidence of the relative inefficiency of n-gram genera-tion).Furthermore, n-gram models have a built-in bias towardsshorter strings.
This is because they calculate the likelihoodof a string of words as the joint probability of the words, or,more precisely, as the product of the probabilities of eachword given the n?
1 preceding words.
The likelihood of anystring will therefore generally be lower than that of any ofits substrings (see Section 4.3 for empirical evidence of thisbias).
This is wholly inappropriate for NLG where equallygood realisations can vary greatly in length (see Section 5for discussion of normalisation for length in statistical mod-elling).The research reported in this paper is part of an ongoingresearch project1 the purpose of which is to investigate issuesin generic NLG.
The experiments (Section 4.3) were carriedout to evaluate and compare different methods for exploitingthe frequencies of word sequences and word sequence cooc-currences in raw text corpora to build models for NL gener-ation, and different ways of using such models during gen-eration.
One of the methods uses a standard 2-gram modelfor selection among all realisations (with a new selection al-gorithm, see Section 4.3).
The other two use a treebank-trained model of the generation space (Section 3).
The basicidea behind treebank-training of generators is simple: deter-mine for the strings and substrings in the corpus the differentways in which the generator could have generated them, i.e.the different sequences of decisions that lead to them, thencollect frequency counts for individual decisions, and deter-mine a probability distribution over decisions on this basis.In the experiments, treebank-trained generation models arecombined with two different ways of using them during gen-eration, one locally and one globally optimal (Section 3.1).All three methods are evaluated on a corpus of weather fore-casts (Section 4.1).2 Generate-and-select NLGGenerate-and-select NLG separates the definition of the spaceof all possible generation processes (the generation space)from the mechanism that controls which (set of) realisation(s)is selected as the output.
Generate-and-select methods varyprimarily along three dimensions:(i) Number of (partial) solutions generated at each step:some methods generate all possibilities, then select;some select a subset of partial solutions at each step;some use an automatically adaptable decision moduleto select the (single) next partial solution.
(ii) Type of decision-making module and method ofconstruction/adaptation: statistical models, variousmachine learning techniques or manual construc-tion/adaptation.
(iii) Size of subtask of the generation process that themethod is applied to: from the entire generation processe.g.
in text summarisation, to all of surface realisationfor domain-independent generation.Methods that can in principle be used to stochastically gen-erate text have existed for a long time, but statistical genera-tion from specified inputs started with Japan-Gloss [Knight etal., 1994; 1995] (which replaced PENMAN?s defaults with sta-tistical decisions), while comprehensive statistical generationstarted with Nitrogen [Knight and Langkilde, 1998] (whichrepresented the set of alternative realisations as a word latticeand selected the best with a 2-gram model) and its succes-sor Halogen [Langkilde, 2000] (where the word lattice wasreplaced by a more efficient AND/OR-tree representation).Since then, a steady stream of publications has reportedwork on statistical NLG.
In FERGUS, Bangalore et al usedan XTAG grammar to generate a word lattice representation1Controlled Generation of Text (CoGenT) http://www.itri.brighton.ac.uk/projects/cogent.of a small number of alternative realisations, and a 3-grammodel to select the best [Bangalore and Rambow, 2000b].Humphreys et al [2001] reused a PCFG trained for NL pars-ing to build syntactic generation trees from candidate syntac-tic nodes.Recently, Habash [2004] reported work using structural 2-grams for lexical/syntactic selection tasks (using joint prob-ability of word and parent word in dependency structures,instead of probability of word given preceding word), aswell as conventional n-grams for selection among surfacestrings.
Velldal et al [2004] compared the performance ofa 4-gram model trained on the BNC2 with a Maximum En-tropy model reused from a parsing application and trainedon the small, domain-specific LOGON corpus, finding thatthe domain-specific ME model performs better on the LOGONcorpus, but a combined model performs best.Some statistical NLG research has looked at subproblemsof language generation, such as ordering of NP premodifiers[Shaw and Hatzivassiloglou, 1999; Malouf, 2000], attributeselection in content planning [Oh and Rudnicky, 2000], NPtype determination [Poesio et al, 1999], pronominalisation[Strube and Wolters, 2000], and lexical choice [Bangaloreand Rambow, 2000a].In hybrid symbolic-statistical approaches, White [2004]prunes edges in chart realisation using n-gram models, andVarges uses quantitative methods for determining the weightson instance features in instance-based generation [Varges andMellish, 2001].The likelihood of realisations given concepts or semanticrepresentations has been modeled directly, but is probablylimited to small-scale and specialised applications: summari-sation construed as term selection and ordering [Witbrockand Mittal, 1999], grammar-free stochastic surface realisation[Oh and Rudnicky, 2000], and surface realisation construedas attribute selection and lexical choice [Ratnaparkhi, 2000].Some of the above papers compare the purely statisti-cal methods to other machine learning methods such asmemory-based learning and reinforcement learning.
Someother research has focussed on machine learning methods,e.g.
Walker et al [2001] look at using a boosting algorithm totrain a sentence plan ranker on a corpus of labelled examples,and Marciniak & Strube [2004] construe the entire genera-tion process as a sequence of classification problems, solvedby corpus-trained feature-vector classifiers.Generate-and-select NLG has been applied either to all ofsurface realisation, or to a small subproblem in deep or sur-face realisation (not to the entire generation process); it iseither very expensive or not guaranteed to find the optimalsolution; and the models it has used are either shallow andunstructured, or require manual corpus annotation.3 Treebank-Training of GeneratorsTreebank-training of generators is a method for modellinglikelihoods of realisations in generation.
It is introducedin this section first in terms of the general idea behind it(which could be implemented with various formalisms, train-ing methods and generation algorithms), and then in Sec-2British National Corpus.InputsNL SentencesSemantic RepsDEEP GENERATIONGENERATIONSURFACEFigure 1: Generation space as decision tree.tion 3.1 by a description of the actual technique that was usedin the experiments reported below.The generation space of a generator can be seen as a set ofdecision trees, where the root nodes correspond to the inputs,and the paths down the trees are all possible generation pro-cesses in the generator that lead to a realisation (leaf) node (aview discussed in more detail in [Belz, 2004]).Consider the diagrammatic generation space representationin Figure 1.
It shows examples of three realisations and thesequences of generator decisions that generate them, repre-sented as paths connecting decision nodes and leading to re-alisation nodes.
In this view of the generation space, using ann-gram model is equivalent to following all paths (from thegiven input node) down to the realisations, and then applyingthe model to select the most likely realisation.An alternative is to estimate the likelihood of a realisationin terms of the likelihoods of the generator decisions that giverise to it, looking at the possible sequences of decisions thatgenerate the realisation (its ?derivations?).
One way of doingthis is to say the likelihood of a string is the sum of the like-lihoods of its derivations.
To train such a model on a corpusof raw text, the set of derivations for each sentence is deter-mined, frequencies for individual decisions are added up, anda probability distribution over sets of alternative decisions isestimated.If a sentence has more than one derivation, as in the exam-ple on the right of Figure 1, there are two possibilities: eitherthe frequency counts are evenly divided between them, or dis-ambiguation is carried out to determine the correct derivation.The former uses the surface frequencies of word strings re-gardless of their meaning and structure, as do n-gram models.The latter is complicated by the fact that there is not alwaysa single ?correct?
derivation in generation (e.g.
an expressionmay end up being passivised in more than one way).There are at least three strategies for using a treebank-trained model during generation: (i) select the most likelydecision at each choice point; (ii) select the most likely gener-ation process (joint probability of all decisions); or (iii) selectthe most likely string of words (summed probabilities of allgeneration processes that generate the string).
The first wouldalways make the same decision given the same alternatives,whereas for (ii) and (iii) it would depend also on the otherdecisions in the derivation(s).
On the other hand, the com-plexity of (iii) is much greater than that of (ii) which in turnis greater than that of (i).3.1 Context-free Generator Treebank-Trainingwithout DisambiguationThere are many different ways of representing generatordecisions and annotating sentences with derivations, andtreebank-training is clearly not equally suitable for all typesof generators.
In the current version of the method, gen-eration rules must be context-free with atomic arguments.Derivations for sentences in the corpus are then standardcontext-free derivations, and corpora are annotated in thestandard context-free way with brackets and labels.No disambiguation is performed, the assumption being thatall derivations are equally good for a sentence.
If a sentencehas more than one derivation, frequency counts are dividedequally between them.The three basic steps in context-free generator treebank-training are:1.
For each sentence in the corpus, find all generation pro-cesses that generate it, that is, all the ways in which thegenerator could have generated it.
For each generationprocess, note the sequences of generator decisions in-volved in it (the derivations for the sentence).
If there isno complete derivation, maximal partial derivations areused instead.2.
Annotate the (sub)strings in the sentence with the deriva-tion, resulting in a generation tree for the sentence.
Ifthere is more than one derivation for the sentence, createa set of annotated trees.
The resulting annotated corpusis a generation treebank.3.
Obtain frequency counts for each individual decisionfrom the annotations, adding 1/n to the count for everydecision, where n is the number of alternative deriva-tions; convert counts into probability distributions overalternative decisions, smoothing for unseen decisions.The probability distribution is currently smoothed with thesimple add-1 method3.
This is equivalent to Bayesian estima-tion with a uniform prior probability on all decisions, and isentirely sufficient for present purposes given the very smallvocabulary and the good coverage of the data.
A standardmaximum likelihood estimation is performed: the total num-ber of occurrences of a decision (e.g.
passive) is divided bythe total number of occurrences of all alternatives (e.g.
pas-sive + active).
In the context-free setting, a decision type cor-responds to a nonterminal N , and decisions correspond toexpansion rules N ?
?.
Given a function c(x) which re-turns the frequency count for a decision x, normalising eachoccurrence by the number of derivations for the sentence, theprobability of a decision is obtained in the standard way (R isthe set of all decisions):p(N ?
?)
= c(N ?
?)?i:N?
?i?R c(N ?
?i)(1)3Also known as applying Laplace?s law.Greedy generationOne way of using a treebank-trained generator is to make thesingle most likely decision at each choice point in a genera-tion process.
This is not guaranteed to result in the most likelygeneration process, but the computational cost in applicationis exceedingly low.Viterbi generationThe alternative is to do a Viterbi search of the generationforest for a given input, which maximises the joint likeli-hood of all decisions taken in the generation process.
Thisis guaranteed to select the most likely generation process,but is considerably more expensive.
The efficiency of greedyprobabilistic generation, Viterbi generation and 2-gram post-selection is compared in Section 4.3 below.A possible alternative to greedy search is to use a non-uniform random distribution proportional to the likelihoodsof alternatives.
E.g.
if there are two alternative decisions D1and D2, with the model giving p(D1) = .8 and p(D2) = .2,then the generator would decide D1 with probability .8, andD2 with a probability of .2 for an arbitrary input (instead ofalways deciding D1 as does the greedy generator).
However,such a strategy, while increasing variation, would come at theprice of lowering the overall likelihood of making the rightdecision.
With the strategy of always going for the most fre-quent alternative, the overall likelihood of making the rightdecision when faced with the choice D1 or D2 is simply.8 in the current example (1 for D1, 0 for D2).
With thelikelihood-proportional random strategy, however, the overalllikelihood of making the right decision is only .68 (.64 forD1, .04 for D2).
Variation can alternatively be increased bymaking the model more fine-grained.4 Evaluation on Weather Forecast Generation4.1 Domain and Data: Weather ForecastingThe corpus used in the experiments reported below is theSUMTIME-METEO corpus created by the SUMTIME projectteam in collaboration with WNI Oceanroutes [Sripada et al,2002].
The corpus was collected by WNI Oceanroutes fromthe commercial output of five different (human) forecasters,and each instance in the corpus consists of three numericaldata files (output by three different weather simulators) andthe weather forecast file written by the forecaster on the evi-dence of the data files (and sometimes additional resources).Following the SUMTIME work, the experiments reported be-low focussed on the part of the forecasts that predicts windcharacteristics for the next 15 hours.
Such ?wind statements?look as follows (for 10-08-01):---------------------------------------------2.FORECAST 1500 GMT FRI 10-Aug,TO 0600GMT SAT11-Aug 2001=====WARNINGS: NIL =======WIND(KTS) CONFIDENCE: HIGH10M: WNW-NW 12-15 BACKING W?LY 05-10 BYMIDNIGHT, THEN SW-SSW BY MORNING50M: WNW-NW 15-18 BACKING W?LY 06-12 BYMIDNIGHT, THEN SW-SSW BY MORNING------------------------------------------------------------------------------------------O1, O2 AND O3 OIL FIELDS (EAST OF SHETLAND)10-08-0110/18 WNW 11 13 17 1.7 2.7 NW 1.5 710/21 W 8 10 12 1.5 2.4 NW 1.4 711/00 W 7 8 10 1.4 2.2 NW 1.4 711/03 SW 7 8 10 1.4 2.2 NW 1.3 711/06 SW 7 8 10 1.3 2.1 NW 1.3 711/09 SSW 10 12 15 1.3 2.1 NW 1.2 711/12 S 14 17 21 1.5 2.4 NW 1.2 711/15 S 20 25 31 1.8 2.9 WNW 1.3 711/18 S 22 27 34 1.9 3.0 SW 1.5 811/21 S 24 30 37 2.3 3.7 S 1.7 812/00 S 28 35 43 3.0 4.8 S 1.9 812/03 S 28 35 43 3.0 4.8 S 1.9 812/06 SW 27 33 41 3.0 4.8 S 2.0 812/09 WSW 26 32 40 2.9 4.6 SSW 2.0 812/12 WSW 25 31 39 2.9 4.6 SW 2.0 812/15 WSW 25 31 39 2.9 4.6 WSW 2.0 812/18 WSW 24 30 37 3.1 5.0 WSW 2.1 812/21 SW 23 28 35 2.9 4.6 WSW 2.2 913/00 SW 21 26 32 2.8 4.5 WSW 2.3 913/03 SW 19 23 29 2.4 3.8 WSW 2.1 813/06 SSW 19 23 29 2.2 3.5 SW 2.0 813/09 SSW 20 25 31 2.2 3.5 SSW 1.9 813/12 SSW 21 26 32 2.4 3.8 SSW 1.8 8---------------------------------------------Figure 2: Metereological data file for 10-08-01.To keep things simple, only the data file type that contains(virtually all) the information about wind parameters (the .tabfile type) was used.
Figure 2 is the .tab file correspondingto the above forecast.
The first column is the day/hour timestamp, the second the wind direction predicted for the corre-sponding time period; the third the wind speed at 10m abovethe ground; the fourth the gust speed at 10m; and the fifththe gust speed at 50m.
The remaining columns contain wavedata.The mapping from time series data to forecast is notstraightforward (even when all three data files are taken intoaccount).
An example here is that while the wind directionin the first part of the wind statement is given as WNW-NW,NW does not appear as a wind speed anywhere in the datafile.
Nor is it obvious why the wind speeds 11, 12 and 7 aremapped to the two ranges 12-15 and 5-10.The SUMTIME Project construed the mapping from timeseries data to weather forecasts as two tasks [Sripada et al,2003]: selecting a subset of the time series data to be in-cluded in the forecast, and expressing this subset of numbersas an NL forecast.
The focus of the research reported hereis not the numerical summarisation of time series data, butNLG techniques.
Therefore, the SUMTIME-METEO corpuswas converted into a parallel corpus of wind statements andthe wind data included in each statement.
The wind data is avector of time stamps and wind parameters, and was ?reverse-engineered?, by automatically aligning wind speeds and winddirections in the forecasts with time-stamps in the data file.In order to do this, wind speed and directions in the data filehave to be matched with those in the forecast.
This was notstraightforward either, because more often than not, there isno exact match in the data file for the wind speeds and di-rections in the forecast.
The strategy adopted in the workreported here was to select the time stamp beside the first ex-act match, and to leave time undefined if there was no exactmatch.
The instances in the final training corpus look as fol-lows (same example, 10-08-01, resulting in two instances):---------------------------------------------WNW-NW 12 15 - - - W 05 10 - - 21 SW-SSW - -- - 03WNW-NW 12-15 BACKING W?LY 05-10 BY MIDNIGHTTHEN SW-SSW BY MORNINGSSW 05 10 - - - S - - - - - - 26 30 38 38 -1SSW 05-10 BACKING S?LY AND INCREASING 26-30GUSTS 38 BY END OF PERIOD---------------------------------------------4.2 Automatic Generation of Weather ForecastsThe three generation methods compared below are allgenerate-and-select methods.
The idea was to build a basicgenerator that for any given input generates a set of alterna-tives that reflects all the variation found in the corpus (ratherthan deciding which alternative to select in which context),and then to create statistical decision makers trained on thecorpus to select (a subset of) alternatives.
The rest of this sec-tion describes the basic generator, and the following sectiondescribes the experiments that were carried out.The basic generator was written semi-automatically as aset of generation rules with atomic arguments that convertan input vector of numbers in steps to a set of NL forecasts.The automatic part was analysing the entire corpus with aset of simple chunking rules that split wind statements intowind direction, wind speed, gust speed, gust statements, timeexpressions, transition phrases (such as and increasing), pre-modifiers (such as less than for numbers, and mainly for winddirection), and post-modifiers (e.g.
in or near the low centre).The manual part was to write the chunking rules themselves,and higher-level rules that combine different sequences ofchunks into larger components.The higher-level generation rules were based on an inter-pretation of wind statements as sequences of fairly indepen-dent units of information, each containing as a minimum awind direction or wind speed range, and as a maximum allthe chunks listed above.
The only context encoded in therules was whether a unit of information was the first in a windstatement, and whether a wind statement contained wind di-rection (only), wind speed (only), or both.
The final genera-tor takes as inputs number vectors of length 6 to 36, and hasa large amount of non-determinism.
For the simplest input(one number), it generates 8 alternative realisations.
For themost complex input (36 numbers), it generates 4.96 ?
1040alternatives (as a tightly packed AND/OR-tree).4.3 Experiments and ResultsThe converted corpus (as described in Section 4.1 above) con-sisted of 2,123 instances, corresponding to a total of 22,985words.
This may not sound like much, but considering thatthe entire corpus only has a vocabulary of about 90 words(not counting wind directions), and uses only a handful ofdifferent syntactic structures, the corpus provides extremelygood coverage (an initial impression confirmed by the smalldifferences between training and testing data results below).The corpus was divided at random into training and testingdata at a ratio of 9:1.
The training set was used to treebank-train the weather forecast generation grammar (as describedin Section 3.1) and a back-off 2-gram model (using the SRILMtoolkit, [Stolcke, 2002]).
The treebank-trained generationgrammar was used in conjunction with a greedy and a Viterbigeneration method.
The 2-gram model was used in more orless exactly the way reported in the Halogen papers [Langk-ilde, 2000].
That is to say, the packed AND/OR-tree represen-tation of all alternatives is generated in full, then the 2-grammodel is applied to select the single best one.One small difference to Halogen is that a Viterbi algo-rithm was used to identify the single most likely string.
Thisis achieved as follows.
The AND/OR-tree is interpreted di-rectly as a finite-state automaton where the states correspondto words as well as to the nodes in the AND/OR-tree.
Thetransitions are assigned probabilities according to the 2-grammodel, and then a straightforward single-best Viterbi algo-rithm is applied to find the best path through the automaton.Random selection among all alternatives was used as abaseline.
All results were evaluated against the gold standard(the human written forecasts) of the test set.
Results werevalidated with 5-fold cross-validation.In the following overview of the results4, the similarity be-tween automatically generated forecasts and gold standardwas measured by conventional string-edit (SE) distance withsubstitution at cost 2, and deletion and insertion at cost 1.Baseline results are given as absolute SE scores, results forthe non-random generators in terms of improvement over thebaseline (reduction in string-edit distance, with SE score inbrackets)5.Training set Test setRandom 14.92 14.37TT/Greedy -50.64% (7.36) -50.30% (7.41)TT/Viterbi -52.32% (7.11) -52.11% (7.14)2-gram -58.44% (6.20) -57.91% (6.27)The SE scores show that, as expected, the improvementsfor the training set are slightly larger in all cases.
The greedygenerator achieves a significant improvement over random se-lection, but is outperformed by the Viterbi generator, with 2-gram selection the clear overall winner.The generated strings were also evaluated using the BLEUmetric [Papineni et al, 2001] which is ultimately based on n-gram agreement between generated and gold standard strings.In simple terms, the more 1-grams, 2-grams, ... and n-gramsare the same between two strings, the higher their BLEUscore.
This implies that BLEU with n = 1 is the most closelyrelated to SE scoring (which can also be seen from the simi-larity between the relative scores assigned by BLEU1 and SE4All numbers in this paper are either given in their entirety ortruncated (not rounded).5Percentage reductions were calculated on the non-truncatedstring-edit distances.BLEU1 BLEU2 BLEU3 BLEU4Train Test Train Test Train Test Train TestRandom 0.479 0.485 0.390 0.382 0.310 0.301 0.225 0.215TT/Greedy 0.678 0.671 0.598 0.591 0.521 0.514 0.437 0.429TT/Viterbi 0.674 0.668 0.595 0.589 0.518 0.512 0.434 0.4272-gram 0.708 0.704 0.620 0.614 0.542 0.536 0.460 0.453Table 1: BLEUn scores for training and test sets, 1 ?
n ?
4.to the generators).
Table 1 gives BLEUn scores for the 4 gen-erators, for 1 ?
n ?
4.
These scores give a different im-pression of the results.
BLEU consistently scores the Viterbigenerator lower than the greedy generator, although the dif-ference between them is only 0.003 on average, less than theaverage mean deviation for the two generators across the fiveruns (0.005).
The difference between the random generatorand the other methods increases significantly with increasingn.
For n = 2 and n = 3, the differences between the 2-gramgenerator and the two treebank-trained generators drops no-ticeably.Results were stable over the five runs of the cross-validation.
For the test set, the mean deviation in SE scoreswas between 0.13 and 0.35, and in BLEU scores between0.0034 and 0.0065.
The margins by which the 2-gram gener-ator outperformed the other two were nearly identical acrossthe five runs.
The SE score of the greedy generator was lower(by nearly identical margins) than that of the Viterbi genera-tor in all runs.
All four BLEU scores of the greedy generator,however, were slightly higher than those of the Viterbi gen-erator in four out of five runs, and slightly lower in one run.Small mean deviation figures and consistency of results con-firm the significance of the differences between the scores inthe SE and BLEU evaluations to some extent.There is a debate over the degree to which an evaluationmetric for NLG should be sensitive to word order and wordadjacency.
The appropriate degree of word-order sensitivitymay vary from one evaluation task to the next, but for thetask of evaluating weather forecasts it certainly is important.A clear example is time expressions.
A forecast can containup to five different time expressions, which have to observethe correct chronological order.
It is not appropriate to re-ward the mere presence (regardless of place in the string) of,say, by midnight (which is what some evaluation metrics arespecifically designed to do, e.g.
[Bangalore et al, 2000]).
SEscoring has a tendency to reward proximity to the intendedplace (although not in a very straightforward way), and BLEUis increasingly strict about place with increasing n.The SE score gives an intuitive, initial impression of howmuch the three methods have learned in comparison to thebaseline: it is easy to conceptualise how different one stringis from another if you know there are two insert and deleteoperations between them (not so easy with the BLEU scores).However, by far the more complete picture (and the most ap-propriate to this evaluation task) is given by BLEU.
It showsthat the 2-gram generator does better than the other two, butnot by a very large margin.
The margin is important con-sidering the far greater expense of n-gram generation.
Thefollowing table shows the total amount of time it took to testthe training and test sets with the different methods in one(controlled) run.Total time (minutes) Training set Test setTT/greedy 23m52s 2m06sTT/Viterbi 3h22m33s 27m05s2-gram 24h04m05s 3h35m06sTesting the training set took all methods about 7 times aslong as the test set.
The Viterbi generator took about 13 timeslonger than the greedy generator for the test set, and about 9times longer for the training set.
The 2-gram generator took6.5 times longer than the Viterbi generator on the test set, and7.5 times longer on the training set.To make the comparison fair, the Viterbi and the 2-gramgenerator were implemented identically as far as possible.Both start by generating the packed AND/OR-tree of all alter-natives.
During this process, the former makes a note of therule probabilities (at the AND and OR nodes) along with therules, and then directly identifies the most likely realisationby a Viterbi method.
The 2-gram generator first has to anno-tate the AND/OR-tree with 2-gram probabilities looked up inthe 2-gram model before using the same Viterbi method.
Theoverhead of looking up the 2-gram model to score the packedrepresentation is the only difference between the two methodsand multiplies out into a large overhead in computing timefor the 2-gram generator6, which would make it unsuitablefor use in practical applications.The random generator has no preference for shorter stringsat all, and has an average string length almost twice that of theother generators.
The 2-gram generator has an almost abso-lute preference of shorter over longer strings, and so producesthe shortest strings.
The Viterbi generator does not prefershorter strings, but does prefer shorter derivations, and thereis a correlation between string length and derivation length.The greedy generator does not have a built-in preference forshorter strings or derivations, but it reflects the fact that short(sub)strings were more frequent in the training corpus:gold: 10.8 greedy: 9.3 2-gram: 8.7random: 17.2 Viterbi: 9.0In some application domains, the n-gram model?s prefer-ence for shorter strings is irrelevant: e.g.
in speech recogni-tion (where n-gram models are used widely) the alternativesamong which the model must choose are always of the samelength.
In language generation, where equally good alterna-6Even if the lookup can be implemented more efficiently therewill always be some overhead multiplied by the total number of 2-grams in the packed representation.tives can vary greatly in length, this preference is positivelyharmful (see following section for discussion of methods tocounteract this bias).5 Discussion and Further ResearchThe n-gram model?s bias towards shorter strings is an exam-ple of a general case: whenever the likelihood of a larger unitthat can vary in length (e.g.
sentence) is modelled in terms ofthe joint probability of length-invariant smaller units, largerunits that are composed of fewer smaller units are more likely.The possibility of counteracting this bias has been investi-gated.
In parsing, Magerman & Marcus [1991] and Briscoe& Carroll [1993] used the geometric mean of probabilitiesinstead of the product of probabilities.
In later work, Briscoe& Carroll [1992] use a normalisation approach, the equiva-lent of which for n-gram selection would be to ?pad?
shorteralternatives with extra ?dummy?
words up to the length ofthe longest alternative, and to use the geometric mean ofthe probabilities assigned by the n-gram model to the non-dummy words as the probability of any dummy word.
Ithas been observed that such methods turn a principled proba-bilistic model into an ad hoc scoring function [Manning andSchuetze, 1999, p. 443].
It certainly means getting rid of then-gram model?s particular independence assumptions, with-out replacing them with a principled alternative.The NLG community is traditionally wary of using evalu-ation metrics like the ones used here.
NLU, and in particu-lar the numbers-driven parsing community, has not tended toworry about the fact that the gold standard parse is usuallynot the only correct one.
It is accepted that it is an imperfectway of evaluating results, albeit the only realistic possibility,especially for large amounts of data.
Furthermore, where aparser or generator has been trained directly on a corpus, it isa fair way of estimating how well a method has succeeded inits aim, namely to learn from the corpus.Some typical example outputs from all generators areshown in an appendix to this paper.
The greedy and Viterbigenerators tend to have very similar output.
Both rely on asmall number of short, high-frequency phrases, e.g.
SOON,VEERING, INCREASING, and LATER.
The 2-gram generatorhas a similar tendency, but with a larger number of phrasesand more variation.
If the generation rules allow it, the greedygenerator always prefixes SOON (all three examples), whereasthe Viterbi generator can avoid it (third example).The quality of the 2-gram generator?s output is indepen-dent of the quality of the weather forecast generation rules, aslong as they reflect all the variation in the corpus.
However,the other two generators are entirely dependent on the qualityof the generation rules, in particular on whether enough infor-mation is incorporated in the rules to base decisions on.
Theonly room for improving the 2-gram generator is in using al-ternative statistical estimation techniques, but there is a lot ofpotential for improving the probabilistic-rule-driven genera-tors, by (i) improving the quality of the generation rules (e.g.by incorporating more contextual information in the genera-tion rules, in particular always including a time stamp); (ii)using alternative methods for building treebank models (e.g.extending the local context of rules has proved useful in pars-ing); and (iii) using alternative methods for exploiting tree-bank models during generation, e.g.
it would be good to havea generation strategy that has (some of) the vastly superiorefficiency of the greedy generator without its repetitiveness,while not sacrificing (too much of) the overall likelihood ofmaking the right decision (unlike the non-uniform randomstrategy discussed in Section 3.1).
Future research will lookat all three areas, using for evaluation a larger and more var-ied corpus from a different domain as well as the SUMTIMEcorpus.The three statistical generation methods evaluated in thispaper all work with raw corpora, but vary hugely in efficiency.Generation speed and the ability to adapt generators to newdomains with no annotation bottleneck are crucial for the de-velopment of practical, generic NLG tools.
The work pre-sented in this paper is intended to be a step in this direction.AcknowledgementsThe research reported in this paper is part of the CoGenTproject, an ongoing research project supported under UK EP-SRC Grant GR/S24480/01.
Many thanks to John Carroll,Roger Evans and Richard Power, as well as to the anonymousreviewers, for very helpful comments.References[Bangalore and Rambow, 2000a] S. Bangalore and O. Ram-bow.
Corpus-based lexical choice in natural language gen-eration.
In Proceedings of the 38th Annual Meeting ofthe Association for Computational Linguistics (ACL ?00),pages 464?471, 2000.
[Bangalore and Rambow, 2000b] S. Bangalore and O. Ram-bow.
Exploiting a probabilistic hierarchical model for gen-eration.
In Proceedings of the 18th International Confer-ence on Computational Linguistics (COLING ?00), pages42?48, 2000.
[Bangalore et al, 2000] S. Bangalore, O. Rambow, andS.
Whittaker.
Evaluation metrics for generation.
In Pro-ceedings of the 1st International Conference on NaturalLanguage Generation (INLG ?00), pages 1?8, 2000.
[Belz, 2004] A. Belz.
Underspecification for NLG.
InINLG04 Posters: Extended Abstracts of Posters Presentedat the Third International Conference on Natural Lan-guage Generation.
Technical Report ITRI-04-01, ITRI,University of Brighton, 2004.
[Briscoe and Carroll, 1992] T. Briscoe and J. Carroll.
Prob-abilistic normalisation and unpacking of packed parseforests for unification-based grammars.
In Proceedings ofthe AAAI Fall Symposium on Probabilistic Approaches toNatural Language, pages 33?38, 1992.
[Briscoe and Carroll, 1993] T. Briscoe and J. Carroll.
Gen-eralised probabilistic LR parsing of natural language (cor-pora) with unification-based grammars.
ComputationalLinguistics, 19(1):25?59, 1993.
[Habash, 2004] N. Habash.
The use of a structural n-gramlanguage model in generation-heavy hybrid machine trans-lation.
In A. Belz, R. Evans, and P. Piwek, editors, Pro-ceedings of the Third International Conference on NaturalLanguage Generation (INLG ?04), volume 3123 of LNAI,pages 61?69.
Springer, 2004.
[Humphreys et al, 2001] K. Humphreys, M. Calcagno, andD.
Weise.
Reusing a statistical language model for gen-eration.
In Proceedings of the 8th European Workshop onNatural Language Generation (EWNLG ?01), pages 86?91, 2001.
[Knight and Langkilde, 1998] K. Knight and I. Langkilde.Generation that exploits corpus-based statistical knowl-edge.
In Proceedings of the 36th Annual Meeting of theAssociation for Computational Linguistics and the 17thInternational Conference on Computational Linguistics(COLING-ACL ?98), pages 704?710, 1998.
[Knight et al, 1994] K. Knight, I. Chander, M. Haines,V.
Hatzivassiloglou, E. Hovy, M. Iida, S. Luk, A. Oku-mura, R. Whitney, and K. Yamada.
Integrating knowledgebases and statistics in MT.
In Proceedings of the 1st Con-ference of the Association for Machine Translation in theAmericas (AMTA ?94), pages 134?141, 1994.
[Knight et al, 1995] K. Knight, I. Chander, M. Haines,V.
Hatzivassiloglou, E. Hovy, M. Iida, S. Luk, R. Whit-ney, and K. Yamada.
Filling knowledge gaps in a broad-coverage MT system.
In Proceedings of the Fourteenth In-ternational Joint Conference on Artificial Intelligence (IJ-CAI ?95), pages 1390?1397, 1995.
[Langkilde, 2000] I. Langkilde.
Forest-based statistical sen-tence generation.
In Proceedings of the 6th Applied Natu-ral Language Processing Conference and the 1st Meetingof the North American Chapter of the Association of Com-putational Linguistics (ANLP-NAACL ?00), pages 170?177, 2000.
[Magerman and Marcus, 1991] D. Magerman and M. Mar-cus.
Pearl: A probabilistic chart parser.
In Proceedings ofthe 2nd International Workshop on Parsing Technologies,pages 193?199, 1991.
[Malouf, 2000] R. Malouf.
The order of prenominal adjec-tives in natural language generation.
In Proceedings of the38th Annual Meeting of the Association for ComputationalLinguistics (ACL ?00), pages 85?92, 2000.
[Manning and Schuetze, 1999] C. Manning andH.
Schuetze.
Foundations of Statistical Natural LanguageProcessing.
The MIT Press, 1999.
[Marciniak and Strube, 2004] T. Marciniak and M. Strube.Classification-based generation using TAG.
In A. Belz,R.
Evans, and P. Piwek, editors, Proceedings of the ThirdInternational Conference on Natural Language Genera-tion (INLG ?04), volume 3123 of LNAI, pages 100?109.Springer, 2004.
[Oh and Rudnicky, 2000] A. Oh and A. Rudnicky.
Stochas-tic language generation for spoken dialogue systems.
InProceedings of the ANLP-NAACL 2000 Workshop on Con-versational Systems, pages 27?32, 2000.
[Papineni et al, 2001] K. Papineni, S. Roukos, T. Ward, andW.-J.
Zhu.
BLEU: A method for automatic evaluation ofmachine translation.
IBM research report, IBM ResearchDivision, 2001.
[Poesio et al, 1999] M. Poesio, R. Henschel, J. Hitzeman,and R. Kibble.
Statistical NP generation: A first report.In R. Kibble and K. van Deemter, editors, Proceedings ofESSLLI Workshop on NP Generation, pages 30?42, 1999.
[Ratnaparkhi, 2000] A. Ratnaparkhi.
Trainable methods forsurface natural language generation.
In Proceedings ofthe 6th Applied Natural Language Processing Conferenceand the 1st Meeting of the North American Chapter of theAssociation of Computational Linguistics (ANLP-NAACL?00), pages 194?201, 2000.
[Shaw and Hatzivassiloglou, 1999] J. Shaw and V. Hatzivas-siloglou.
Ordering among premodifiers.
In Proceedings ofthe 37th Annual Meeting of the Association for Computa-tional Linguistics (ACL ?99), pages 135?143, 1999.
[Sripada et al, 2002] S. Sripada, E. Reiter, J.
Hunter, andJ.
Yu.
SUMTIME-METEO: Parallel corpus of naturally oc-curring forecast texts and weather data.
Technical ReportAUCS/TR0201, Computing Science Department, Univer-sity of Aberdeen, 2002.
[Sripada et al, 2003] S. Sripada, E. Reiter, J.
Hunter, andJ.
Yu.
Exploiting a parallel text-data corpus.
In Proceed-ings of Corpus Linguistics 2003, pages 734?743, 2003.
[Stolcke, 2002] A. Stolcke.
SRILM: An extensible languagemodeling toolkit.
In Proceedings of the 7th InternationalConference on Spoken Language Processing (ICSLP ?02),pages 901?904,, 2002.
[Strube and Wolters, 2000] M. Strube and M. Wolters.
Aprobabilistic genre-independent model of pronominaliza-tion.
In Proceedings of the 6th Applied Natural LanguageProcessing Conference and the 1st Meeting of the NorthAmerican Chapter of the Association of ComputationalLinguistics (ANLP-NAACL ?00), pages 18?25, 2000.
[Varges and Mellish, 2001] S. Varges and C. Mellish.Instance-based natural language generation.
In Proceed-ings of the 2nd Meeting of the North American Chapterof the Association for Computational Linguistics (NAACL?01), pages 1?8, 2001.
[Velldal et al, 2004] E. Velldal, S. Oepen, and D. Flickinger.Paraphrasing treebanks for stochastic realization ranking.In Proceedings of the 3rd Workshop on Treebanks and Lin-guistic Theories (TLT ?04), Tuebingen, Germany, 2004.
[Walker, 2001] M. Walker.
An application of reinforcementlearning to dialogue strategy selection in a spoken dialoguesystem for email.
Journal of Artificial Intelligence Re-search, 12:387?416, 2001.
[White, 2004] M. White.
Reining in CCG chart realization.In A. Belz, R. Evans, and P. Piwek, editors, Proceedings ofthe Third International Conference on Natural LanguageGeneration (INLG ?04), volume 3123 of LNAI, pages 182?191.
Springer, 2004.
[Witbrock and Mittal, 1999] M. Witbrock and V. Mittal.Ultra-summarization: A statistical approach to generatinghighly condensed non-extractive summaries.
In Proceed-ings of the 22nd International Conference on Researchand Development in Information Retrieval (SIGGIR ?99),pages 315?316, 1999.A Example output10Jul2001 03/3:Gold ENE SOON 20-24 BACKING NE 08-12 BY LATE AFTERNOONRandom ENE VERY SOON 20-24 AHEAD OF THE FRONT AT FIRST EASING AND BACKING NE?LY 8-12 BY THEAFTERNOONGreedy SOON ENE INCREASING 20-24 VEERING NE 8-12 LATERViterbi SOON ENE INCREASING 20-24 VEERING NE 8-12 LATER2-gram ENE AND 20-24 BACKING NE 8-12 LATER10Nov2000 16/1:Gold N 10 OR LESS VEERING SE AND RISING 20-24 LATE IN THE PERIODRandom N 10 OR LESS THEN LATE IN PERIOD BACKING SE STEADILY INCREASING TO 20-24 IN FRONTALZONE FOR A TIME EARLY EVENINGGreedy SOON N 10 OR LESS VEERING SE INCREASING 20-24 LATERViterbi SOON N 10 OR LESS VEERING SE INCREASING 20-24 LATER2-gram N 10 OR LESS BACKING SE AND 20-24 BY EVENING10Feb2002 04/1:Gold WNW 22-28 GUSTS 38 VEERING NW BY MIDDAY THEN BACKING AND DECREASING W?LY 18-22 BYMID EVENINGRandom WNW 22-28 GUSTS 38 IN ANY SHOWERS THEN SLOWLY BACKING TO NW BY MID MORNING WEST18-22 LATERGreedy SOON WNW 22-28 GUSTS 38-38 VEERING NW LATER VEERING W 18-22 LATERViterbi WNW 22-28 GUSTS 38-38 VEERING NW LATER W 18-22 LATER2-gram WNW 22-28 GUSTS 38 LATER NW LATER W 18-22 LATER
