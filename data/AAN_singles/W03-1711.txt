A Chinese Efficient Analyser Integrating Word Segmentation,Part-Of-Speech Tagging, Partial Parsing and Full  ParsingGuoDong ZHOUInstitute for Infocomm Research21 Heng Mui Keng TerraceSingapore, 119613zhougd@i2r.a-star.edu.sgJian SUInstitute for Infocomm Research21 Heng Mui Keng TerraceSingapore, 119613sujian@ i2r.a-star.edu.sgThe objective of this paper is to develop anefficient analyser for the Chinese language,exploring different intermediate forms, achieving atarget speed in the region of 1,000 wps for fullparsing, 2,000 wps for partial parsing and 10,000wps for word segmentation and part-of-speechtagging, with state-of-art performances.AbstractThis paper introduces an efficient analyser forthe Chinese language, which efficiently andeffectively integrates  word segmentation,part-of-speech tagging, partial parsing and fullparsing.
The Chinese efficient analyser is basedon a Hidden Markov Model (HMM) and anHMM-based tagger.
That is, all thecomponents are based on the sameHMM-based tagging engine.
One advantage ofusing the same single engine is that it largelydecreases the code size and makes themaintenance easy.
Another advantage is that itis easy to optimise the code and thus improvethe speed while speed plays a critical importantrole in many applications.
Finally, theperformances of all the components can benefitfrom the optimisation of existing algorithmsand/or adoption of better algorithms to a singleengine.
Experiments show that all thecomponents can achieve state-of-artperformances with high efficiency for theChinese language.The layout of this paper is as follows.
Section 2describes the Chinese efficient analyser.
Section 3presents the HMM and the HMM-based tagger.Sections 4 and 5 describe the applications of theHMM-based tagger in integrated wordsegmentation and part-of-speech tagging, partialparsing, and full parsing respectively.
Section 6gives the experimental results.
Finally, someconclusions are drawn with possible extensions offuture work in section 7.2 Chinese Efficient AnalyserThe Chinese efficient analyser can be described bythe example as shown in Figure 1.
Here, "."
inFigure 1 means that the current node has not beenchunked till now.
For convenience, it is regarded asa "special chunk" in this paper and others as"normal chunks".
Therefore, every node in Figure 1can be represented as a 3tuple , whereis the i -th chunk in the input chunk sequence and),( iii wpc ic1  IntroductionTraditionally, a text parser outputs a complete parsetree for each input sentence, achieving a speed inthe order of 10 words per second (wps) (Abney1997).
However, for many applications like textmining, a parse tree is not necessary and a speed of10 wps is unacceptable when we have to processmillions of words in thousands of documents in areasonable time (Feldman 1997).
Therefore, there isa compromise between speed and performance inmany applications.?
is the head word of c  and  is the POStag of w  wheniw i ipi .
?ic  ( c  is a normal chunk).In this case, we call node c  a normalchunk node.i)(i p , ii wFull Parsing = N levels of Partial Parsing(N = Parsing Depth and for Figure 1, N=3)S(VB, ??
)3rd-level 3tuple sequence?
is just the word linked with c  and  isthe POS tag of  when  ( c  is a specialchunk).
In this case, we call node c  aspecial chunk or POS node.iw iiiip)iwiw .=ic,( ipFigure 1 shows that, sequentially from bottomto top,1) Given a Chinese sentence (e.g.
?????????????
), it is segmented and tagged into asequence of special chunk, POS and word3tuples (.
(ADJ, ?
? )
.
(NN, ?
? )
.(ADV,?)
.
(VB, ??)
.
(ADJ, , ??)
) via theintegrated word segmeIn this paper, this rescalled "0th-level 3tuple2) The 0th-level 3tuple sequence is then chunkedinto 1st-level 3tuple sequence (NP(NN, ??)
.
(ADV, ?)
.
(VB, ??)
NP(NN, ??))
via1st-level partial parsing, while POS nodes .(ADJ,??)
and .
(NN, ??)
are chunked into a normalchunk node NP(NN, ?
?
), and POSnodes .
(ADJ, ??)
.
(NN, ??)
into NP(NN, ??
).3) The 1st-level 3tuple sequence is further chunkedinto 2nd-level 3 N, ??
)VP(VB, ?? ))
parsing,while mixed al chunknodes .
(ADV, ?
N, ??)
areNP(NN, ??)
VP(VB, ??
)NP(NN, ??)
.(ADV,?)
.(VB,??)
NP(NN, ??)).(ADJ,??)
.(NN,??)
.(ADV,?)
.(VB,??)
.(ADJ,??)
.(NN,??)?
?
?
?
?
?
?
?
?
?
?developed      country             also                 exist              man(English Translation: There also exists many problems in deveFigure 1: An Example Sentence ?????????????
of the Chinese Efficien2nd-level Partial Parsing1st-level  PartialParsingegmentationagging3rd-level Partial Parsing2nd-level 3tuple sequence1st-level 3tuple sequence0th-level 3tuplesequence??)
.
(NNIntegrated Word Sand POS Tntation and POS tagging.ulting 3tuple sequence issequence".chunked i?).
y              problemloped countries) nto atuple sequence (NP(Nvia 2nd-level partial  normal chunt Analyser POS and norm) .
(VB, ??)
NP(Nk node VP(VB, ?4) Finally, 2nd-level 3tuple sequence is chunkedinto 3rd-level 3tuple sequence (S(VB, ??))
via3rd-level partial parsing, while normal chunknodes NP(NN, ?? )
and VP(VB, ?? )
arechunked into a normal chunk node S(VB, ??
).5) In this way, full parsing is completed with afully parsed tree after several levels (3 in theexample of Figure 1) of cascaded partialparsing.3 HMM-based TaggerThe Chinese efficient analyser is based on theHMM-based tagger described in Zhou et al2000a.Given a token sequence G , the goalof tagging is to find a stochastic optimal tagsequence  that maximizesnn ggg L211 =nn tttT L211 =)()(),(log)(log)|(log1111111 nnnnnnnGPTPGTPTPGTP ?+=By assuming mutual informationindependence:?==nininn GtMIGTMI1111 ),(),(  or?= ?=?ninininnnnGPtPGtPGPTPGTP1 111111)()(),(log)()(),(logwe have:?
?==+?=nininiinnnGtPtPTPGTP111111)|(log)(log)(log)|(logBoth the first and second items correspond tothe language model component of the tagger.
Wewill not discuss these two items further in this papersince they are well studied in ngram modeling.
Thispaper will focus on the third item, which is the main differencebetween our tagger and other HMM-based taggers.Ideally, it can be estimated by using theforward-backward algorithm (Rabiner 1989)recursively for the first-order (Rabiner 1989) orsecond-order HMMs (Watson et al1992).
Tosimplify the complexity, several context dependentapproximations on it will be attempted in this paperinstead, as detailed in sections 3 and 4.?=nini GtP11 )|(logAll of this modelling would be for naught wereit not for the existence of an efficient algorithm forfinding the optimal state sequence, thereby"decoding" the original sequence of tags.
Thestochastic optimal tag sequence can be found bymaximizing the previous equation over all thepossible tag sequences.
This is implemented via thewell-known Viterbi algorithm (Viterbi 1967) byusing dynamic programming and an appropriatemerging of multiple theories when they convergeon a particular state.
Since we are interested inrecovering the tag state sequence, we pursue 16theories at every given step of the algorithm.4 Word Segmentation and POS TaggingTraditionally, in Chinese Language Processing,word segmentation and POS tagging areimplemented sequentially.
That is, the inputChinese sentence is segmented into words first andthen the segmented result (in the form of wordlattice or N-best word sequences) is passed to POStagging component.
However, this processingstrategy has following disadvantages:?
The word lexicons used in word segmentationand POS tagging may be different.
Thisdifference is difficult to overcome and largelydrops the system accuracy although differentoptimal algorithms may be applied to wordsegmentation and POS tagging.?
With speed in consideration, the two-stageprocessing strategy is not efficient.Therefore, we apply the strategy of integratingword segmentation and POS tagging in a singlestage.
This can be implemented as follows:1) Given an input sentence, a 3tuple (specialchunk, POS and word) lattice is generated byskimming the sentence from left-to-right, andlooking up the word and POS lexicon todetermine all the possible words and get POStag probability distribution for each possibleword.2) Viterbi algorithm is applied to decode the3tuple lattice to find the most possible POS tagsequence.3) In this way, the given sentence is segmentedinto words with POS tags.The rationale behind the above algorithm is theability of HMM in parallel segmentation andclassification (Rabiner 1989).In order to overcome the coarse n-gram modelsraised by the limited number of orignial POS tagsused in current Chinese POS tag bank (corpus), aword clustering algorithm (Bai et al1998) is appliedto classify words into classes first and then the N(e.g.
N=500) most frequently occurred word classand POS pairs are added to the original POS tag setto achieve more accurate models.
For example,ADJ(<??
>) represents a special POS tag ADJwhich pairs with the word class <??>.
Here, <?
?>is a word class label.
For convenience and clarity,we use the most frequently occurred word in a wordclass as the label to represent the word class.5 Partial Parsing and Full ParsingAs discussed in section 2, obviously partial parsingcan have different levels and full parsing can beachieved by cascading several levels of partialparsing (e.g.
3 levels of cascaded partial parsing canachieve full parsing for the example as shown inFigure 1).In this paper, a certain level (e.g.
l -th level) ofpartial parsing is implemented via a chunkingmodel, built on the HMM-based tagger as describedin section 2, with ( -th level 3tuple sequenceas input.
That is, for the l -th level partial parsing,the chunking model has the ( -th level 3tuplesequence  (Here, 3tuple) as input.
In the meantime, chunktag t  used in the chunking model is structural andconsists of following three parts:)1?lgg 21=)1?lngnG1)iwL,( iii pcg =i?
Boundary Category B: It is a set of four values0, 1, 2, 3, where "0" means that the current3tuple is a whole chunk, "1" means that thecurrent 3tuple is at the beginning of a chunk,"2" means that the current 3tuple is in themiddle of a chunk and "3" means that thecurrent stuple is at the end of a chunk.?
Chunk Category C:  It is used to denote theoutput chunk category of the chunking model,which includes normal chunks and the specialchunk (".").
The reason to include the specialchunk is that some of POS 3tuple in the inputsequence may not be chunked in the currentchunking stage.?
POS Category POS: Because of the limitednumber in boundary category and output chunkcategory, the POS category is added into thestructural tag to represent more accuratemodels.Therefore,  can be represented by, where b  is the boundary type of,  is the output chunk type of t  and  isthe POS type of t .
Obviously, there exist someconstraints between t  and  on the boundarycategories and output chunk categories, as briefedin table 1, where "valid"/"invalid" means the chunktag sequence t  is valid/invalid while "validon"means  is valid on the conditionitiitiii poscb __it icii tt 1?i1?iiciposiciti 1?i =?1 .0 1 2 30 Valid Valid Invalid Invalid1 Invalid Invalid Valid on Validon2 Invalid Invalid Valid Valid3 Valid Valid Invalid InvalidTable 1: Constraints between t  and t  1?i i(Column: b  in t ; Row:  in t ) 1?i 1?i ib iFor the example as shown in Figure 1, we cansee that:?
In the 1st-level partial parsing, the input 3tuplesequence is the 0th-level 3tuple sequence .(ADJ,??)
.
(NN, ??)
.
(ADV, ?)
.
(VB, ??)
.
(ADJ, ?? )
.
(NN, ?? )
and the output tag sequence1_NP_ADJ 3_NP_NN 0_._ADV 0_._VB1_NP_ADJ 3_NP_NN, from where derived isthe 1st-level 3tuple sequence NP(NN, ??)
.
(ADV, ?)
.
(VB, ??)
NP(NN, ??).?
In the 2nd-level partial parsing, the input 3tuplesequence is the 1st-level 3tuple sequenceNP(NN, ??)
.
(ADV, ?)
.
(VB, ??)
NP(NN, ?? )
and the output tag sequence 0_NP_NN1_VP_ADV 2_VP_VB 3_VP_NN, from wherederived is the 2nd-level 3tuple sequence NP(NN,??)
VP(VB, ??).?
In the 3rd-level partial parsing, the input 3tuplesequence is the 2nd-level 3tuple sequenceNP(NN, ??)
VP(VB, ??)
and the output tagsequence 1_S_NN 3_S_VB, from wherederived is the 3rd-level 3tuple sequence S(VB, ??).
In this way, a fully parsed tree is reached.?
In the cascaded chunking procedure, necessaryinformation is stored for back-tracing.Partially/fully parsed trees can be constructedby tracing from the final 3tuple sequence backto 0th-level 3tuple sequence.
Different levels ofpartial parsing can be achieved according to theneed of the application.6 Experimental ResultsThe Chinese efficient analyser is implemented inC++, providing a rapid and easycode-compile-train-test development cycle.
In fact,many NLP systems suffer from a lack of softwareand computer-science engineering effort: runningefficiency is key to performing numerousexperiments, which, in turn, is key to improvingperformance.
A system may have excellentperformance on a given task, but if it takes long tocompile and/or run on test data, the rate ofimprovement of that system will be contrainedcompared to that which can run very efficiently.Moreover, speed plays a critical role in manyapplications such as text mining.All the experiments are implemented on aPentium II/450MHZ PC.
All the performances aremeasured in precisions, recalls and F-measures.Here, the precision (P) measures the number ofcorrect units in the answer file over the total numberof units in the answer file and the recall (R)measures the number of correct units in the answerfile over the total number of units in the key filewhile F-measure is the weighted harmonic mean ofprecision and recall:PRRP++= 22 )1(?
?F  with=1.
2?6.1 Word Segmentation and POS TaggingTable 2 shows the integrated word segmentationand POS tagging results on the Chinese tag bankPFR1.0 of 3.69M Chinese characters (1.12 ChineseWords) developed by Institute of ComputationalLinguistics at Beijing Univ.
Here, 80% of thecorpus is used as formal training data, another 10%as development data and remaining 10% as formaltest data.Function P R F SpeedWord Segment.
97.5 98.2 97.8POS Tagging 93.5 94.1 93.811,000wpsTable 2: Performances of Word Segmentationand POS Tagging (wps: words per second)The word segmentation corresponds tobracketing of the chunking model while POStagging corresponds to bracketing and labelling.Table 2 shows that recall (P) is higher thanprecision (P).
The main reason may be the existenceof unknown words.
In the Chinese efficientanalyser, unknown words are segmented intoindividual Chinese characters.
This makes thenumber of segmented words/POS tagged words inthe system output higher than that in the correctanswer.6.2 Partial Parsing and Full ParsingTable 3 shows the results of 1st-level partial parsingand full parsing, using the PARSEVAL evaluationmethodology (Black et al1991) on the UPENNChinese Tree Bank of 100k words developed byUniv.
of Penn.
Here, 80% of the corpus is used asformal training data, another 10% as developmentdata and remaining 10% as formal test data.Function P R F  SpeedPartial Parsing 85.1 82.5 83.8 4500 wpsFull Parsing 77.1 70.3 73.7 2100 wpsTable 3: Performances of 1st-level Partial Parsingand Full Parsing (wps: words per second)Table 3 shows that the performances of partialparsing and full parsing are quite low, compared tothose of state-of-art partial parsing and full parsingfor the English language (Zhou et al2000a; Collins1997).
The main reason behind is the small size ofthe training corpus used in our experiments.However, the Chinese PENN Tree Bank is thelargest corpus we can find for partial parsing andfull parsing.
Therefore, developing a much largerChinese tree bank (comparable to UPENN EnglishTree Bank) becomes an urgent task for the Chineselanguage processing community.
Actually, the bestindividual system (Zhou et al2000b) inCoNLL?2000 chunking shared task for the Englishlanguage (Tjong et al2000) used the sameHMM-based tagging engine.7  ConclusionThis paper presents an efficient analyser for theChinese language, based on a HMM and a singleengine -- HMM-based tagger.
Experiments showthat the analyser achieves state-of-art performanceat very high speed, which can meet the requirementof speed-critical applications such as text mining.Our future work includes:?
Syntactic analysis of the partial/full parsingresults into a meaningful intermediate form.?
Research and development of Chinese namedentity recognition using the same HMM-basedtagger and its integration to the Chineseefficient analyser.AcknowledgementsThanks go to Institute of Computational Linguisticsat Beijing Univ.
and LDC at Univ.
of Penn.
for freeresearch use of their Chinese Tag Bank and ChineseTree Bank.ReferencesAbney S. 1997.
Part-of-Speech Tagging and PartialParsing.
Corpus-based Methods in NaturalLanguage Processing.
Edited by Steve Youngand Gerrit Bloothooft.
Kluwer AcademicPublishers, Dordrecht.Bai ShuanHu, Li HaiZhou, Lin ZhiWei and YuanBaoSheng.
1998.
Building class-based languagemodels with contextual statistics.
Proceedings ofInternational Conference on Acoustics, Speechand Signal Processing (ICASSP'1998).pages173-176.
Seattle, Washington, USA.Black E. and Abney S. 1991.
A Procedure forQuantitatively Comparing the Syntactic Coverageof English Grammars.
Proceedings of DRAPAworkshop on Speech and Natural Language.pages306-311.
Pacific Grove, CA.
DRAPA.Collins M.J. 1997.
Three Generative, LexicalisedModels for Statistical Parsing.
Proceedings of theThirtieth-Five Annual Meeting of the Associationfor Computational Linguistics (ACL?97).pages184-191.Feldman R. 1997.
Text Mining - Theory andPractice.
Proceedings of the Third InternationalConference on Knowledge Discovery & DataMining (KDD?1997).Rabiner L. 1989.
A Tutorial on Hidden MarkovModels and Selected Applications in SpeechRecognition.
IEEE 77(2), pages257-285.Tjong K.S.
Erik and Buchholz S. 2000.
Introductionto the CoNLL-2000 Shared Task: Chunking.Proceedings of the Conference on ComputationalLanguage Learning (CoNLL'2000).Pages127-132.
Lisbon, Portugal.
11-14 Sept.Viterbi A.J.
1967.
Error Bounds for ConvolutionalCodes and an Asymptotically Optimum DecodingAlgorithm.
IEEE Transactions on InformationTheory, IT 13(2), 260-269.Watson B. and Tsoi A Chunk.
1992.
Second orderHidden Markov Models for speech recognition.Proceeding of the Fourth AustralianInternational Conference on Speech Science andTechnology.
pages146-151.Zhou GuoDong and Su Jian.
2000a.
Error-drivenHMM-based Chunk Tagger withContext-dependent Lexicon.
Proceedings of theJoint Conference on Empirical Methods onNatural Language Processing and Very LargeCorpus (EMNLP/ VLC'2000).
Hong Kong, 7-8Oct.Zhou GuoDong, Su Jian and Tey TongGuan.2000b.
Hybrid Text Chunking.
Proceedings ofthe Conference on Computational LanguageLearning (CoNLL'2000).
Pages163-166.
Lisbon,Portugal, 11-14 Sept.
