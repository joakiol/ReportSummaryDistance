First Joint Conference on Lexical and Computational Semantics (*SEM), pages 624?630,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsFBK: Machine Translation Evaluation and Word Similarity metricsfor Semantic Textual SimilarityJose?
Guilherme C. de SouzaFondazione Bruno KesslerUniversity of TrentoPovo, Trento, Italydesouza@fbk.euMatteo NegriFondazione Bruno KesslerPovo, TrentoItalynegri@fbk.euYashar MehdadFondazione Bruno KesslerPovo, TrentoItalymehdad@fbk.euAbstractThis paper describes the participation of FBKin the Semantic Textual Similarity (STS) taskorganized within Semeval 2012.
Our ap-proach explores lexical, syntactic and se-mantic machine translation evaluation metricscombined with distributional and knowledge-based word similarity metrics.
Our bestmodel achieves 60.77% correlation with hu-man judgements (Mean score) and ranked 20out of 88 submitted runs in the Mean rank-ing, where the average correlation across allthe sub-portions of the test set is considered.1 IntroductionThe Semantic Textual Similarity (STS) task pro-posed at SemEval 2012 consists of examining thedegree of semantic equivalence between two sen-tences and assigning a score to quantify such sim-ilarity ranging from 0 (the two texts are about dif-ferent topics) to 5 (the two texts are semanticallyequivalent).
The complete description of the task,the datasets and the evaluation methodology adoptedcan be found in (Agirre et al, 2012).Typical approaches to measure semantic textualsimilarity exploit information at the lexical level.The proposed solutions range from calculating theoverlap of common words between the two text seg-ments (Salton et al, 1997) to the application ofknowledge-based and corpus-based word similaritymetrics to cope with the low recall achieved by onsimple lexical matching (Mihalcea et al, 2006).Our participation in the STS task is inspired byprevious work on paraphrase recognition, in whichmachine translation (MT) evaluation metrics areused to identify whether a pair of sentences aresemantically equivalent or not (Finch and Hwang,2005; Wan et al, 2006).
Our approach to semantictextual similarity makes use of not only lexical in-formation but also syntactic and semantic informa-tion.
To this aim, our metrics are based on differentnatural language processing tools that provide syn-tactic and semantic annotation.
These include shal-low parsing, constituency parsing, dependency pars-ing, semantic roles labeling, discourse representa-tion analyzer, and named entities recognition.
In ad-dition, we employed distributional and knowledge-based word similarity metrics in an attempt to im-prove the results given by the MT metrics.
The com-puted scores are used as features to train a regressionmodel in a supervised learning framework.Our best run model achieves 60.77% correlationwith human judgements when evaluating the seman-tic similarity of texts from the entire test set andwas ranked in the 20th position (out of 88 submit-ted runs) in the Mean ranking.2 System DescriptionThe system has been designed following a ma-chine learning based approach in which a regres-sion model is induced using different shallow anddeep linguistic features extracted from the datasets.The STS training corpora are first preprocessed us-ing different tools that annotate the texts at differentlevels.
Using the preprocessed data, the features areextracted for each pair and used to train a model thatwill be applied to unseen test pairs.
The trainingset is composed by three datasets (MSRpar, MSRvidand SMTeuroparl) which combined contain a totalof 2234 instances.
The test data is composed by adifferent sample of the same three datasets plus in-stances derived from two additional corpora (OnWN624and SMTnews).
The datasets construction and anno-tation are described in (Agirre et al, 2012).Our system exploits two sets of features which re-spectively build on MT evaluation metrics (2.1) andword similarity metrics (2.2).
The whole feature setis summarized in figure 1.2.1 Machine Translation Evaluation MetricsMT evaluation metrics are designed to assesswhether the output of a MT system is semanticallyequivalent to a set of reference translations.
TheMT evaluation metrics described in this section, im-plemented in the Asiya Open Toolkit for AutomaticMachine Translation (Meta-) Evaluation1 (Gime?nezand Ma`rquez, 2010) are used to extract features atdifferent linguistic levels: lexical, syntactic and se-mantic.
For the syntactic and semantic levels, Asiyacalculates similarity measures based on the linguis-tic elements provided by each kind of annotation.Linguistic elements are defined as ?the linguisticunits, structures, or relationships?
(Gime?nez, 2008)(e.g.
dependency relations, discourse relations,named entities, part-of-speech tags, among others).
(Gime?nez, 2008) defines two simple measures us-ing the linguistic elements of a given linguistic level:overlapping and matching.
Overlapping is ameasure of the proportion of items inside the lin-guistic elements of a certain type shared by bothtexts.
Matching is defined in the same way withthe difference that the order between the items insidea linguistic element is taken into consideration.
Thatis, the items of a linguistic element are concatenatedin a single unit from left to right.2.1.1 Lexical LevelAt the lexical level we explored different n-gramand edit distance based metrics.
The differenceamong them is in the way each algorithm calcu-lates the lexical similarity, which yields to differ-ent results.
We used the following n-gram-basedmetrics: BLEU (Papineni et al, 2002), NIST (Dod-dington, 2002), ROUGE (Lin and Och, 2004), GTM(Melamed et al, 2003), METEOR (Banerjee andLavie, 2005).
Besides those, we also used metricsbased on edit distance.
Such metrics calculate thenumber of edit operations (e.g.
insertions, deletions,and substitutions) necessary to transform one text1http://nlp.lsi.upc.edu/asiya/into the other (the lower the number of edit oper-ations, the higher the similarity score).
The edit-distance-based metrics used were: WER (Nie?
en etal., 2000), PER (Tillmann et al, 1997), TER (Snoveret al, 2006) and TER-Plus (Snover et al, 2009).
Thelexical metrics form a group of metrics that we here-after call lex.2.1.2 Syntactic LevelThe syntactic level was explored by running con-stituency parsing (cp), dependency parsing (dp),and shallow parsing (sp).
Constituency trees wereproduced by the Max-Ent reranking parser (Char-niak, 2005).
The constituency parse trees wereexploited by using three different classes of met-rics that were designed to calculate the similaritiesbetween the trees of two texts: overlapping infunction of a given part-of-speech; matching infunction of a given constituency type; and syntactictree matching (STM) metric proposed by (Liu andGildea, 2005).Dependency trees were obtained using MINI-PAR (Lin, 2003).
Two types of metrics were usedto calculate the similarity between two texts usingdependency trees.
In the first, different similaritymeasures were calculated taking into considerationthree different perspectives: overlap of words thathang in the same level or in a deeper level of thedependency tree; overlap between words that hangdirectly from terminal nodes given a specified part-of-speech; and overlap between words that are ruledby non-terminal nodes given a specified grammat-ical relation (subject, object, relative clause, amongothers).
The second type is an implementation of thehead-word chain matching introduced in (Liu andGildea, 2005).The shallow syntax approach proposed by(Gime?nez, 2008) uses three different tools to ex-plore the parts-of-speech, word lemmas and basephrases chunks, respectively: SVMTool (Gime?nezand Ma`rquez, 2004), Freeling (Carreras et al, 2004)and Phreco (Carreras et al, 2005).
In this type ofmetrics the idea is to measure the similarity betweenthe two texts using parts-of-speech and chunk types.The following metrics were used: overlappingaccording to the part-of-speech; overlapping ac-cording to the chunk type; the accumulated NISTmetric (Doddington, 2002) scores over different625Figure 1: A summary of the class of features explored.sequences (lemmas, parts-of-speech, base phrasechunks and chunk IOB labels).2.1.3 Semantic LevelAt the semantic level we aplored three differenttypes of information, namely: discourse represen-tations, named entities and semantic roles.
Here-after they are respectively referred to as dr, ne, andsr features.
The discourse relations are automat-ically annotated using the C&C Tools (Clark andCurran, 2004).
The following metrics using seman-tic tree representations were proposed by (Gime?nez,2008).
A metric similar to the STM in which se-mantic trees are used instead of constituency trees;the overlapping between discourse representa-tion structures according to their type; and the mor-phosyntactic overlapping of discourse represen-tation structures that share the same type.Named entities metrics are calculated by com-paring the entities that appear in each text.
Thenamed entities were annotated using the BIOS pack-age (Surdeanu et al, 2005).
Two types of metricswere used: the overlapping between the namedentities in each sentence according to their type andthe matching between the named entities in func-tion of their type.Semantic roles were automatically annotated us-ing the SwiRL package (Surdeanu and Turmo,2005).
The arguments and adjuncts annotated ineach sentence are compared according to three dif-ferent metrics: overlapping between the seman-tic roles according to their type; the matching be-tween the semantic roles according to their type; andthe overlapping of the roles without taking intoconsideration their lexical realization.2.2 Word Similarity MetricsBesides the MT evaluation metrics, we experi-mented with lexical semantics by calculating wordsimilarity metrics.
For that, we followed a distri-butional and a knowledge-based word similarity ap-proach.2.2.1 Distributional Word SimilarityAs some previous work on semantic textual tex-tual similarity (Mihalcea et al, 2006) and textualentailment (Kouylekov et al, 2010; Mehdad et al,2010) have shown, distributional word similaritymeasures can improve the performance of both tasksby allowing matches between terms that are lexicallydifferent.
We measure the word similarity comput-ing a set of Latent Semantic Analysis (LSA) metricsover Wikipedia.
The 200,000 most visited articlesof Wikipedia were extracted and cleaned to build the626term-by-document matrix using the jLSI tool2.Using this model we designed three different sim-ilarity metrics that compute the similarity betweenall elements in one text with all elements in the othertext.
For two metrics we calculate the similaritiesbetween different parts-of-speech: (i) similarity overnouns and adjectives, and (ii) similarity over verbs.The third metric computes the similarity betweenall words in the two sentences.
The similarity iscomputed by averaging the pairwise similarity usingthe LSA model between the elements of each text.These metrics are hereafter called lsa.2.2.2 Knowledge-based Word SimilarityIn order to incorporate world knowledge informa-tion about entities (persons, organizations, locations,among others) into our model we experimented withknowledge-based (thesaurus-based) word similaritymetrics.
Usually such approaches have a very lim-ited coverage of concepts due to the reduced size ofthe available thesauri.
In order to increase the cov-erage we extracted concepts from the YAGO2 se-mantic knowledge base (Hoffart et al, 2011) derivedfrom Wikipedia, Wordnet (Miller, 1995) and Geon-ames3.
YAGO2 contains knowledge about 10 mil-lion entities and more than 120 million facts aboutthese entities.In order to link the entities in the text to the enti-ties in YAGO2 we have used ?The Wiki Machine?
(TWM) tool4.
The tool solves the linking problemby disambiguating each entity mention in the text(excluding pronouns) using Wikipedia to provide thesense inventory and the training data (Giuliano etal., 2009).
After preprocessing the datasets withTWM the entities are annotated with their respectiveWikipedia entries represented by their URLs.
Usingthe entity?s URL it is possible to retrieve the Word-net synsets related to the entity?s entry in YAGO2and explore different knowledge-based metrics tocompute word similarity between entities.In our experiments we selected three differ-ent algorithms to calculate word similarity usingYAGO2: Wu-Palmer (Zhibiao and Palmer, 1994),the Leacock-Chodorow (Leacock et al, 1998) and2http://hlt.fbk.eu/en/technology/jlsi3http://www.geonames.org/4http://thewikimachine.fbk.eu/html/index.htmlthe path distance (score based on the shortest paththat connects the senses in the Wordnet hyper-nym/hyponym taxonomy).
Two classes of metricswere designed: (i) the average of the similarity be-tween all the entities in each sentence and (ii) thesimilarity of the pair of elements which have theshortest path in the Wordnet taxonomy among allpossible pairs.
There are six different metrics usingthe three algorithms in total.
An extra metric wasdesigned using only TWM.
The metric is calculatedby taking the number of common entities in the twosentences divided by the total number of entities an-notated in the two sentences.
The metrics describedin this section are part of the yago group.3 Experiments and DiscussionIn this section we present our experiments settings,the configuration of the runs submitted and discussthe results obtained.
All our experiments were madeusing half of the training set for training and halffor testing (development).
Ten different random-izations were run over the training data in orderto obtain ten different pairs of train/developmentsets and reduce overfitting.
We tried several differ-ent regression algorithms and the best performancewas achieved with the implementation of SupportVector Machines (SVM) of the SVMLight package(Joachims, 1998).
We used the radial basis functionkernel with default parameters without any specialtuning for the different datasets.3.1 Submitted Runs and ResultsBased on the results achieved with different featuresets over training data we have selected the bestcombinations for our submission.
The feature setsfor each run are:Run 1: lex, lsa, yago, and a selection offeatures in the cp, dp, sp, dr, ne and srgroups, forming a total of 286 features.Run 2: lex, lsa, and yago, in a total of 50features.Run 3: lex and lsa, forming a total of 43features.The results obtained by our three submitted runsare summarized in table 1.
The table reports the627Runs submittedRun 1 Run 2 Run 3 Base PEDevelopment 0.885 0.863 0.859 - -TestMSp 0.249 0.512 0.516 0.433 0.577MSv 0.611 0.780 0.777 0.299 0.818SMTe 0.149 0.379 0.441 0.454 0.450Wn 0.421 0.622 0.629 0.586 0.629SMTn 0.243 0.547 0.608 0.390 0.608All 0.563 0.643 0.651 0.310 0.789Allnrm 0.712 0.808 0.810 0.673 0.633Mean 0.362 0.588 0.607 0.435 0.829Table 1: Results of each run for each dataset (MSRpar,MSRvid, SMTeuroparl, OnWn, SMTnews) calculatedwith the Pearson correlation between the system?s out-puts and the gold standard annotation.
Official scores ob-tained using the three evaluation scores All, Allnrm andMean.
Development row presents the average results foreach run in the whole training dataset.
Base is the of-ficial baseline system.
Post Evaluation is the experimentran after the evaluation period with models trained for thespecific datasets.Pearson correlation between the system output andthe gold standard annotation provided by the task or-ganizers.
The table also presents the official scoresused to rank the systems and described in (Agirre etal., 2012).
Our best model, Run 3, was ranked 20thaccording to the Mean score, 25th according to theRankNrm score and 32th according to the All scoreamong 88 submitted runs.The ?Development?
row reports the results of ourthree best models in the development phase.
Theresults obtained for the three training datasets arehigher than the results obtained for the testing.
Onehypothesis that might explain this behavior is over-fitting during the training phase due to the way wedivided the training set and carried out the experi-ments.
A different experiment setting to carry outthe development should be tried to evaluate this hy-pothesis.To our surprise, in the test datasets the results ofRun 1 and Run 3 swapped positions: in the train-ing setting Run 1 was the best model and Run 3 thethird best.
The performance of Run 3 was relativelystable across the five datasets ranging from aboutthe 30th to the 48th position the exception beingthe SMTnews dataset.
In this dataset Run 3 was thebest performing run of the evaluation exercise (andRun 2 the second).
One possible explanation for thisbehavior is the fact that Run 3 is based on lexicalfeatures that do not take into consideration the syn-tactic structure of the two texts and therefore is notpenalized by the noise introduced by the texts gen-erated by MT systems.
This hypothesis, however,does not explain why Run 3 score for the SMTeu-roparl dataset was below the baseline score.
Erroranalysis of the effects of different group of featuresin the test datasets is required to better understandsuch behaviors.3.2 Post-evaluation ExperimentsAfter the evaluation period, as a first step towardsthe required error analysis and a better comprehen-sion of the potential of our approach, we performedan experiment to assess the impact of having mod-els trained for specific datasets.
In this experiment,each training dataset (MSRpar, MSRvid and SMTeu-roparl) was used to train a model.
Each dataset?smodel was tested on its respective test dataset.
Themodel for the surprise datasets (OnWn and SMT-news) were trained using the whole training dataset.We used the Run 3 feature set (the best run in theofficial evaluation).
The results of the experimentare reported in the column ?Exp?
of table 1.
Theimpact of having specific models for each datasetis high.
The Mean score goes from .607 to .829and improvements are also observed in the All score(0.789).
These scores would rank our system at the7th position in the Mean rank.
However, it is impor-tant to notice that in a real-world setting, knowledgeabout the source of data is not always available.
Weconsider that having a general model that does notrely on this kind of information represents a more re-alistic way to confront with real-world applications.4 Final RemarksIn this paper we described FBK?s participation inthe STS Semeval 2012 task.
Our approach is basedon a combination of MT evaluation metrics, distri-butional, and knowledge-based word similarity met-rics.
Our best run achieved the 20th position among88 runs in the Mean overall ranking.
An error analy-sis of the problematic test pairs is required to under-stand the potential of our feature sets and improvethe overall performance of our approach.
Along thisdirection, a first experiment with our best featuresand a different strategy already led to significant im-provements in the Mean and All scores (from .651 to628.789 and from .607 to .829, respectively).AcknowledgmentsThis work has been partially supported by the EC-funded project CoSyne (FP7-ICT-4-24853).The authors would like to thank Claudio Giulianofor kindly helping us to preprocess the datasets withthe Wiki Machine.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-lez.
2012.
SemEval-2012 Task 6: A Pilot on SemanticTextual Similarity.
In 6th International Workshop onSemantic Evaluation (SemEval 2012), in conjunctionwith the First Joint Conference on Lexical and Com-putational Semantics (*SEM 2012).Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In ACL 2005Workshop on Intrinsic and Extrinsic Evaluation Mea-sures for MT and/or Summarization.Xavier Carreras, Isaac Chao, Llu?
?s Padro, and MuntsaPadro?.
2004.
Freeling: An open-source suite of lan-guage analyzers.
In 4th International Conference onLanguage Resources and Evaluation (LREC), pages239?242.Xavier Carreras, Llu?
?s Ma`rquez, and Jorge Catro.
2005.Filtering-Ranking Perceptron Learning.
MachineLearning, 60:41?75.Eugene Charniak.
2005.
Coarse-to-fine n-best parsingand MaxEnt discriminative reranking.
In Proceedingsof the 43rd Annual Meeting on, volume 1, pages 173?180.Stephen Clark and James R. Curran.
2004.
Parsing theWSJ using CCG and log-linear models.
In ACL ?04Proceedings of the 42nd Annual Meeting on Associa-tion for Computational Linguistics.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the Second Interna-tional Conference on Human Language TechnologyResearch, pages 138?145.
Morgan Kaufmann Publish-ers Inc.Andrew Finch and YS Hwang.
2005.
Using ma-chine translation evaluation techniques to determinesentence-level semantic equivalence.
In Third Inter-national Workshop on Paraphrasing, pages 17?24.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2004.
SVMTool: Ageneral POS tagger generator based on Support VectorMachines.
In 4th International Conference on Lan-guage Resources and Evaluation (LREC), pages 43?46.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010.
Asiya:An Open Toolkit for Automatic Machine Translation(Meta-) Evaluation.
The Prague Bulletin of Mathe-matical Linguistics, (94):77?86.J.
Gime?nez.
2008.
Empirical Machine Translation andits Evaluation.
Ph.D. thesis.Claudio Giuliano, Alfio Massimiliano Gliozzo, and CarloStrapparava.
2009.
Kernel methods for minimally su-pervised wsd.
Computational Linguistics, 35(4):513?528.Johannes Hoffart, Fabian M. FM Suchanek, KlausBerberich, Edwin Lewis Kelham, Gerard de Melo, andGerhard Weikum.
2011.
YAGO2: Exploring andQuerying World Knowledge in Time, Space, Context,and Many Languages.
In 20th International WorldWide Web Conference (WWW 2011), pages 229?232.Thorsten Joachims.
1998.
Making Large-Scale SVMLearning Practical.
In Bernhard Scholkopf, Christo-pher J. C. Burges, and Alexander J. Smola, editors,Advances in Kernel Methods - Support Vector Learn-ing, pages 41?56.
MIT Press, Cambridge, USA.Milen Kouylekov, Yashar Mehdad, and Matteo Negri.2010.
Mining Wikipedia for Large-Scale Reposito-ries of Context-Sensitive Entailment Rules.
In Seventhinternational conference on Language Resources andEvaluation (LREC 2010), pages 3550?3553, La Val-letta, Malta.Claudia Leacock, George A. Miller, and MartinChodorow.
1998.
Using corpus statistics and Word-Net relations for sense identification.
ComputationalLinguistics, 24(1):147?166.C.Y.
Lin and F.J. Och.
2004.
Automatic evaluationof machine translation quality using longest commonsubsequence and skip-bigram statistics.
In Proceed-ings of the 42nd Annual Meeting on Association forComputational Linguistics, page 605.
Association forComputational Linguistics.Dekang Lin.
2003.
Dependency-Based Evaluation ofMinipar.
Text, Speech and Language Technology,20:317?329.Ding Liu and Daniel Gildea.
2005.
Syntactic featuresfor evaluation of machine translation.
In ACL Work-shop on Intrinsic and Extrinsic Evaluation Measuresfor Machine Translation and/or Summarization, num-ber June, pages 25?32.Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-simo Zanzotto.
2010.
Syntactic/semantic structuresfor textual entailment recognition.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the ACL, number June,pages 1020?1028.I.
Dan Melamed, Ryan Green, and Joseph P. Turian.2003.
Precision and Recall of Machine Translation.
In629Proceedings of the Joint Conference on Human Lan-guage Technology and the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL).Rada Mihalcea, Courtney Corley, and Carlo Strappar-ava.
2006.
Corpus-based and knowledge-based mea-sures of text semantic similarity.
In Proceedings of theAmerican Association for Artificial Intelligence, pages775?780.George A. Miller.
1995.
WordNet: A Lexical Databasefor English.
Communications of the ACM, 38(11):39?41.Sonja Nie?
en, Franz Josef Och, Gregor Leusch, and Her-mann Ney.
2000.
An evaluation tool for machinetranslation: Fast evaluation for MT research.
In Lan-guage Resources and Evaluation, pages 0?6.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics (ACL), number July, pages 311?318.Gerard Salton, Amit Singhal, and Mandar Mitra.
1997.Automatic text structuring and summarization.
Infor-mation Processing &amp;, 33(2):193?207.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Association for Machine Translation in theAmericas.Matthew G. Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
TER-Plus: paraphrase, se-mantic, and alignment enhancements to TranslationEdit Rate.
Machine Translation, 23(2-3):117?127,December.Mihai Surdeanu and Jordi Turmo.
2005.
Semanticrole labeling using complete syntactic analysis.
In9th Conference on Computational Natural LanguageLearning (CoNLL), number June, pages 221?224.Mihai Surdeanu, Jordi Turmo, and Eli Comelles.
2005.Named Entity Recognition from Spontaneous Open-domain Speech.
In 9th International Conference onSpeech Communication and Technology (Interspeech),pages 3433?3436.C Tillmann, S Vogel, H Ney, A. Zubiaga, and H. Sawaf.1997.
Accelerated DP Based Search for StatisticalTranslation.
In Fifth European Conference on SpeechCommunication and Technology, pages 2667?2670.Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.2006.
Using Dependency-Based Features to Take the?Para-farce?
out of Paraphrase.
In 2006 AustralasianLanguage Technology Workshop (ALTW2006), num-ber 2005, pages 131?138.Wu Zhibiao and Martha Palmer.
1994.
Verb Seman-tics and Lexical Selection.
In ACL ?94 Proceedingsof the 32nd annual meeting on Association for Com-putational Linguistics, pages 133?138.630
