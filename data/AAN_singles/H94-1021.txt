Whither Written Language Evaluation?Ralph GrishmanDepartment ofComputer ScienceNew York UniversityNew York, NY 10003Common evaluations have grown to be a major component of allthe ARPA Human Language Technology programs.
In the writtenlanguage community, the largest evaluation program has been theseries of Message Understanding Conferences, which began in 1987\[2,3\].
These evaluations have focussed on the task of analyzingtext and automatically filling templates describing certain classesof events.
These conferences have certainly been a major impetusin the development of systems for performing such "informationextraction" tasks, and thus in demonstrating the potential practicalvalue of some of the written language processing technology.There have been a number of concerns expressed, however, aboutthe trend of these valuations.
First, these valuations - - and par-ticularly the most recent, MUC-5 - -  have consumed large amountsof time, and in particular time spent learning and encoding detailedinformation about he domain, rather than learning about how toprocess language ingeneral.
Second, there has been a focus on tech-nologies which are effective for this task hut may not be effectivefor other, "language understanding" tasks.In response to these concerns, a group of ARPA contractors andGovernment representatives met on December 2-4 in San Diego toplan for the next written language evaluation conference (MUC-6).This paper is a report on the conclusions of this meeting and someof the electronic nterchanges which followed.1.
EVALUATION GOALSAlthough the group met under the banner of MUC ("Message Un-derstanding Conference"), it examined the issues of the evaluationof written language processing systems more generally, and did notlimit itself to the types of evaluations conducted in past MUCs,which had been restricted to "information extraction" (template fill-hag).
The group began by considering the aims of such evaluations,which include?
assessing progress in written language understanding (and inparticular, of ARPA's Tipster Phase 2 technology program)?
guiding research and pushing the technology (by identifyingproblems that need to be addressed)?
maintaining and increasing the interest and participation ofpotential users (by demonsl~ating systems which are "rele-vant" to practical applications)?
drawing more research groups into the evaluation process(and thus fostering the exchange of new ideas)?
lessening substantially the overhead associated with evalua-tionsTo meet hese various goals, the group proposed that MUC-6 consistof a menu of different evaluations.
The evaluations would be runon a single test set, but there would be separate valuation scoresmeasuring different capabilities.
Individual sites would be free toparticipate in any subset of the evaluations.
(Of course, for siteswhich choose- or feel obligated - to participate to the maximum,the richness of the menu which was developed may work against thestated goal of reducing the evaluation overhead.
)The group decided that he corpus hould consist of business-relatedarticles from American ewspapers and wire services.
A large corpusof such texts, part of the corpora for the recent TREC (Text RetrievalEvaluation) Conferences, i  available through the Linguistic DataConsortium.
This includes articles from the Wall Street Journal, theSan Jose Mercury News, and the AP newswire.2.
THE MENUThe menu of evaluations will include rather different types of tasks inorder to meet he range of objectives cited above.
On the one hand,we want to continue valuation on tasks - -  such as "informationextraction"-- which can he seen as prototypes for real applications,and so wiU continue to draw interest from outside the natural lan-guage processing community.
We would like to make these tasksas simple as possible, consistent with a semblance ofreality, so thatevaluationper se  does not become amajor time drain.On the other hand, we are interested inexploring "glass box" eval-uations - -  evaluations of the ability of systems to identify cruciallinguistic relationships which we believe are relevant to a high levelof performance on a wide variety of language understanding tasks.Of course, some people will believe that we have chosen the wrongrelationships, or at least hat natural language systems need not makethese relationships explicit in the process of performing a natural lan-guage analysis task, and so will decline to participate in some or allof the glass box evaluations.
We respect these disagreements, andhave organized the menu of evaluations totake them into account.Any particular choice of internal evaluations ecessarily representssome bet on the path of technical development.
However, we be-lieve that the relationships we have selected are sufficiently basicto understanding that he bet is worth taking, and that by encourag-ing work on these tasks we will push research on natural languageunderstanding i  ways which would not be possible with a limitedapplication task such as information extraction.The menu we came up with includes one task (named entity recog-nition) which, is sufficiently basic to be characterized as both aninternal and an application task; four internal evaluations; and twoapplication-oriented evaluations:120cross-document coreferencemini-MUCPA structure coreference word senseParseval named entity recognitionFigure 1: Potential interrelationships among the evaluations inMUC-6.1.
Named Entity Recognition: Identify company names, or-.
ganization ames, personal names, location names, productnames, dates, times, and money.2.
Parseval: Bracket he syntactic onstituents of the sentence.3.
Predicate-Argument Structure: Identify the relationship be-tween lexical elements in terms of relations uch as logical-subject, logical-object, etc.4.
WordSense Disambiguation: Identify the word sense of eachnoun, verb, adjective, and adverb in the text, using the inven-tory of word senses from WordNet5.
Coreference Resolution: Identify identity of reference, su-perset, and subset relations among text elements, as well assituations where a text element is an implicit argument ofanother (e.g., a subject or object of a nominalization whichappears elsewhere in the text).6.
Mini-MUC: Identify instances of a particular class of event inthe text, and fill a template with the crucial information abouteach instance.7.
Cross-Document Coreference: Identify coreference r lationsbetween objects and events in different articles.Evaluations 3, 4, and 5 are collectively known as Semeval.
Each ofthe seven evaluations can be done independently, but there are poten-tials for using the results of the annotation for one taskin performinganother; these relationships are shown in Figure 1.
Presumably,most participants will generate predicate-argument structure fromparser output, so for them good Parseval performance would be aprerequisite for good performance on the predicate-argument met-ric.
Recognition of named entities is essential for good performanceon both Semeval and Mini-MUC.
Some people will want to usethe Semeval processing/output for the Mini-MUC, and some peo-ple won't; it is an interesting scientific question whether it helps.Cross-Document Coreference r quires the output of MIni-MUC.It will be possible for a site to investigate only one of these links, ifthey wished, rather than starting from the raw text input.
This wouldallow people to build on others' work on named entity recognition, orto assess, assuming perfect or typical results on Semeval, how wellone could do on Mini-MUC.
Moreover, sites may be required to notonly do a run using the (perfectly correct) key for the input to theircomponent, but also using the (imperfect) actual results of somesite participating in the full evaluation, which would be publiclyavailable.
(This might be arranged by staggering the evaluations,with the component evaluations scheduled before the mini-MUCevaluation.)
These experiments would be analogous to the written-language-only part of the SLS evaluations.3.
THE EVALUATIONSIn this section we briefly describe ach of the seven evaluation tasks.For each task we shall need to prepare a sample of text annotatedwith the information we wish the systems under evaluation to ex-tract.
To make the annotations more manageable and inspectable, wehave combined the annotations for named entity recognition, coref-erence, and word sense identification.
They are all encoded usingan SGML tagging of the text, with separate attributes to record eachtype of information.
Merging the annotations does not mean that thecorresponding evaluations will be combined.
We still expect hatthese three evaluations will be scored separately, and that text can beseparately annotated for the three evaluations)To illustrate some of the annotations, members of the MUC-6 com-mittee have annotated one of the "joint venture" news articles fromthe MUC-5 evaluation.
The first two sentences of this article are:Bridgestone Sports Co. said Friday it has set up a jointventure in Talwan with a local concern and a Japanesetrading house to produce golf clubs to be shipped toJapan.
The joint venture, Bridgestone Sports TaiwanCo., capitalized at 20 million New Talwan Dollars,will start production in January 1990 with productionof 20,000 iron and "metal wood" clubs a month.The named-entity / coreference / word sense annotation is shownin Figures 2 and 3; the predicate-argument a notation is shown inFigure 4.
All of these annotations are very preliminary; we expectthey will be revised as annotation progresses.3.1.
Named Entity RecognitionThe experience with MUC-5 indicated that recognition of company,organization, people, and location names is an essential ingredientin understanding business news articles, and is to a considerabledegree separable from the other problems of language interpretation.In addition, such recognition can be of practical value by itself intracking people and organizations in large volume of text.
As aresult, this evaluation may appeal to firms focussed on this limitedtask, who are not involved in more general language understanding.In Figures 2 and 3, the named entity recognition is reflected in allthe SGML elements besides wd: entity for companies and otherorganizations, loc and complex-loc for locations, num for numbers(including percentages), date, and money.
Additional element typeswould be provided for other constructs involving specialized lexicalpatterns, such as times and people's names.
For most of theseelements, one of the attributes gives a normalized form: the decimal1 Although it will be simpler if at least he demarcation ofnamed entitiesis performed first.121<S n=l><entity id=tl type=company ame='Bridgestone Sports CO' ><wd lemma--say sense=\[verb.communieation.0\] ><date value='241189' ><wd id=t2 identical=tl ><wd ><wd id=t3 sense=\[verb.contact.0\] ><wd ><wd id=t4 sense=\[noun.possession.0\] ><wd ><loc id=t5 name='Taiwan' type=country ><wd ><wd ><wd id=t6 sense=\[adj.all.0.territorial.0\] args="\[to S\]"><wd id=t7 sense=\[noun.group.0\] ><wd ><wd ><wd sense=\[adj.pert.0\] ><wd sense=\[nounxelafion.0\] ><wd id=t8 sense=\[noun.group.I\] ><wd ><wd id=t9 sense=\[verb.creation.0\] args="\[1-subj t4\]" ><wd id=tl0 lemma=golf_club sense=\[noun.artifact.0\] ><wd ><wd ><wd lemma=ship sense-\[verb.motion.0\] ><wd ><loc id=tl 1 narne='Japan' type=countzy ><wd ></s>Bridgestone Sports Co. </entity>said </wd>Friday </date>it </wd>has </wd>set up </wd>a </wd>joint venture </wd>in </wd>Taiwan </lot>with </wd>a </wd>local </wd>concern </wd>and </wd>a </wd>Japanese </wd>trading </wd>house </wd>to </wd>produce </wd>golf clubs </wd>to </wd>be </wd>shipped </wd>to </wd>Japan </lot>?
</wd>Figure 2: Named entity / word sense / coreference annotation of first sentence.value of a number, a 6.-digit number for dates, a standardized formfor company names (following MUC-5 rules for company names).3.2.
ParsevalParseval is a measure of the ability of a system to bracket he syn-tactic constituents in a sentence.
This metric has now been in usefor several years, and has been described elsewhere \[1\].
Parsevalmay eventually be supplanted in large part by the "deeper" and moredetailed predicate-argument valuation.
However, for the presentParseval is being retained in order to accomodate participants fo-cussed on surface grammar and participants reluctant o committo predicate-argument valuation until its design is stabilized andproven.3.3.
Predicate-argument structureA very tentative predicate-argument structure for our two sentencesis shown in Figure 4.
As much as possible, we have tried to usethe same structures which have been adopted by the Spoken Lan-guage Coordinating Committee for their predicate-argument valu-ation.
We summarize here, with some simplifications, only the mostessential aspects of this representation.For each event or state in the text, we introduce aDavidsonian eventvariable i, and treat the type and each argument of the event as aseparate predication.
So, for example, Fred fed Francis on Fridaywould be represented as2(ev-type 1 eat)(1-subj 1 Fred)(1-obj 1 Francis)(on 1 Friday)Each elementary predication can be numbered by preceding it witha number and colon.
Roughly speaking, a system would be scoredon the number of such elementary predications it gets correct.
Be-cause this notation is none too readable, however, we also allow theabbreviated form(eat \]event 1\] \[1-subj Fred\] \[1-obj Francis\] \[on Friday\])where \[event 1\] could be omitted if there were no other referencesto the event variable.
An entity, arising from a noun phrase withdeterminer det will be represented by2Assuming that on is a primitive predicate, which is not expanded usingan event variable.
Otherwise we would have the predications (ev-type 2on),O-subj 2 1), and (l-obj 2 Friday).122<s n=2><wd > The </wd><wd id=t21 sense=\[noun.possession.0\] identical=t4 > joint venture </wd><wd > , </wd><entity id=t22 name='Bridgestone Sports Taiwan CO' type=company identical=t21 > Bridgestone Sports Taiwan Co. </entity><wd ><wd id=t23 lemma=capitalize sense=\[verb.cognition.1\] ><wd ><money id=t24 amount='20000000' unit='TWD' ><wd ><wd sense=\[verb.stative.0\] ><wd sense=\[verb.creation.1 \] ><wd id=t25 sense=\[noun.act.2\] identical=t9 args"\[1-subj t21\] \[1-obj tl0\]"><wd ><date id=t26 value='0190' ><wd ><wd id=t27 sense=\[noun.act.2\] sub-of=t25 args="\[l-subj t21\]" ><wd ><hum value='20,000' ><wd sense=\[noun.artifact.1\] ><wd ><wd ><wd sense=\[noun.artifact.0\] ><wd ><wd id=t28 lemma=club sense=\[noun.artifact.1\] sub-of=tlO><wd ><wd sense=\[noun.time.O\] ><wd ><Is>, </wd>capitalized </wd>at </wd>20 million New Taiwan Dollars </money>, </wd>will </wd>start </wd>production </wd>in </wd>January 1990 </date>with </wd>production </wd>of </wd>20,000 </num>iron </wd>and </wd>"</wd>metal wood </wd>"</wd>clubs </wd>a </wd>month </wd>?
</wd>Figure 3: Named entity / word sense / coreference annotation of second sentence.e: (det <restrl restr2 ...>)Each restri is a constraint on the entity, stated as a predicationon index e. Thus "the brown cow which licked Fred" would berepresented by1: (the <(brown \[1-subj 1\]) (cow \[1-subj 1\])(lick \[1-subj 1\] \[1-obj Fred\])>)The notation "?i" means that i is optional; the notation i / j meansthat either i or j  is allowed.The written language group, however, is not taking the same ap-proach to the selection of predicates and role-names as the spokenlanguage group.
The spoken language group aspires to a truly se-mantic representation, i dependent of the particular syntactic formin which it was expressed.
This seems feasible in the highly circum-scribed omain of air traffic information.
It does not seem a feasiblenear-term goal for all of language, or even for all of "business news",which is a very broad domain.
Instead we will be initially using aform of grammatical functional structure, with lexical items as heads(predicate types), and role names uch as logical subject and logicalobject.
The representation will be normalized with respect o onlya limited number of syntactic alternations, uch as passive, dativewith "for", and dative with "to".
I expect hat the representationwill gradually evolve to normalize a larger number of paraphrasticalternations.3.4.
CoreferenceCoreference can be annotated either at the level of the word se-quence or at the level of predicate-argument structure.
By recordingcoreference atthe word level, we lose some distinctions that can becaptured at predicate-argument l vel.
On the other hand, annotatingat the word level allows for evaluation of coreference without gener-ating predicate-argument structure.
So - -  in order to keep the menuitems as independent as possible - -  our current plan is to annotatecoreference at the word level, with the head word of the anaphorpointing to the head word of the antecedent.Coreference is recorded through attributes in the SGML annotation(Figures 2 and 3).
For purposes of reference, lements are annotatedwith an ident attribute.
Identity of reference is indicated by anattribute identical pointing to the antecedent.
A superset/subsetrelation is indicated by asub-ofattribute.
Finally, if a predication hasimplicit arguments which are coreferential with prior text elements,they are annotated as args = "\[role antecedent\]".123(DECL <(say [event i][l-subj 2:Bridgestone-Sports-Co.][l-obj <(set-up [event 3][l-subj 412][l-obj 5:(a <(Joint-venture [l-subj 5])(in [l-subJ 5] [l-obJ Taiwan])6:(with[l-subj 513][l-obj 7:(ANDNP <8:(a <(concern [l-subj 8])(local [l-subJ 8])>)9:(a <(trading-house [l-subj 9])(Japanese [l-subj 9])>)>)])10:(PURPOSE[l-subJ 513][l-obJ <(produce[l-subj ?5][l-obj ii:(NO-DET <(golf-club [l-subj ii])(PLURAL [l-subj Ii])(PURPOSE[l-subj II][l-obj <(ship [event 12][l-subJ ?5][l-obj ii])(to [l-subJ 12][l-obj Japan]>])>)])>])>)]?6 ?I0(PERFECT-TENSE [l-subj 3])>])(PAST-TENSE [l-subJ i])(AT-TIME [l-subj i][l-obJ 13:(DATE <(DAY-OF-WEEK [l-subj 13] [l-obJ Friday])>)])>)(DECL <(start [event i][l-subj 2:(the <(joint-venture [l-subj 2])(IDENTICAL (l-subj 2][l-obj Bridgestone-Sports-Taiwan-Co.])(capitalize [event 3] [l-obj 2])(at [l-subj 3][l-obj 4:(NO-DET <(New-Taiwan-Dollar [l-subj 4])(PLURAL [l-subj 4])(CARDINALITY [l-subj 4][l-obj 20000000])>)])>}][l-obj 5: (NO-DET <(produce [event 5] [l-subj 2])>)])(FUTURE-TENSE [l-subj i])(in [l-subj i][1-obj 6:(DATE <(MONTH [l-subj 6] [l-obj January])(YEAR [l-subj 6] [l-obj 1990])>)])(with [l-subj I][1-obj 7:(NO-DET <(produce [event 7][l-subj 2])[l-obj 8:(NO-DET <(club [l-subj 8])(PLURAL [l-subj 8])(and <(iron [l-subj 8])(metal-wood [l-subj 8])>)(PER [l-subj (CARDINALITY [l-subj 8][l-obj 20000][l-obj month])>)])>)])>)Figure 4: Predicate-argument structure.1243.5.
Word sense identificationThe third element of the Semeval triad is sense identification.
As asense inventory, we hayed used WordNet, which is widely and freelyavailable and is broad in coverage \[4\].
The notation used to refer toparticular WordNet sense was described in \[5\].3.6.
Mini-MUCThis component is the direct descendant of the information extrac-tion tasks in the previous MUCs \[2,3\]?
In response to criticism thatthe evaluation task had gotten too complex, we have endeavored tomake the new information extraction as simple as possible?
The tem-plate will have a hierarchical structure, as in MUC-5, but probablywith only two levels of "objects".
The objects at the lower level willrepresent common business news entities uch as people and compa-nies.
A small inventory of such objects will be defined in advance.The upper level object will then be a simple structure with perhapsfour or five slots, to capture the information about a particular typeof event.The following were suggested astypical of such templates:1?
Location of use of pollution control products?Product:?
Purchaser:Act:Act-Location:2.
Org.
ordering or cancelling order for aircraft.Manufacturer:Model:Buyer:Order status:3.
Companies quote prices on products.Company:Products:Prices:Date:4.
PLANSThe menu of evaluations which has been developed for MUC-6 iscertainly ambitious; perhaps it is too ambitious and will need tobe scaled back.
While the cost of participating in a single one ofthese evaluations should be much less than the effolt required forMUC-5, the effort to prepare all these evaluations will be consid-erable.
Detailed specifications will need to be developed for eachof the evaluations, and substantial nnotated corpora will have to bedeveloped, both as the "case law" for subsequent evaluations andas a training corpus for trainable analyzers.
If this is all success-ful, however, it holds the promise for fostering advances in severalaspects of natural language understanding.A description of the menu of evaluations was disseminated lec-tronically at the end of December 1993.
Further details, includinga sample annotated message, were distributed at the end of Febru-ary 1994.
After a period of public electronic omment, we shallbe recruiting volunteer sites to begin annotating texts, slowly overthe course of the spring, as the specifications are ironed out, morerapidly over the summer, once specifications are more stable.A dry run evaluation, possibly including only a subset of the menuitems, will be conducted in late fall of 1994; MUC-6 is tentativelyscheduled for May of 1995.5.
AcknowledgementsI Wish to thank Jerry Hobbs for sharing with me his write-up of theMUC-6 meeting, and to thank the people who prepared the individualannotations: Jerry Hobbs for predicate-argument and coreference;George Miller and his colleagues at Princeton for word senses; andBeth Sundheim for named entities.
Finally, I wish to thank all the par-ticipants in the MUC-6 meeting - -  Jim Cowie, George Doddington,Donna Harman, Jerry Hobbs, Paul Jacobs, Boyan Onyshkevych,John Prange, Len Schubert, Bill Schultheis, Beth Sundheim, CarlWeir, and Ralph Weischedel - - for their contributions.The author was supported inthe preparation fthis report by the Ad-vanced Research Projects Agency under Grant N00014-90-J-1851from the Office of Naval Research.4?
Stock market min/max during interval.Market:Index:Extreme: H/LEpoch:Because of the simplicity of these templates, a month was felt to besufficient time for developing aparticular extraction system.
In fact,because of concern that a single template introduced too much riskdue to possible faulty template/problem design, itwas suggestedthatworking on three closely related topics within a single month mightbe desirable.3.7.
Cross-document coreferenceOne way in which prior MUC tasks were unrealistic s that hey didnot attempt to link events across documents, even though the corpusfrequently included multiple documents about he same event.
Toremedy this shortcoming, it was suggested that the task of makingsuch event coreference links across documents be included as anadditional item on the evaluation menu.References1.
Black, E., Abney, S., Flickinger, D., Gdaniec, C., Grishman, R.,Hindle, D., Ingria, R., Jelinek, F., Klavans, L, Liberman, M.,Marcus, M., Roukos, S., Santorini, B., and Strzalkowsld, T. Aprocedure for quantitatively comparing the syntactic coverageof English grammars.
Proc.
Fourth DARPA Speech and NaturalLanguage Workshop, Feb. 1991, Pacific Grove, CA, MorganKaufmann.2.
Proceedings ofthe Third Message Understanding Conference(MUC-3).
Morgan Kaufmann, May 1991.3.
Proceedings of the Fourth Message Understanding Conference(MUC-4).
Morgan Kaufmann, June 1992.4.
Miller, G. A.
(ed.
), WordNet: An on-line lexical database.
In-ternational Journal of Lexicography (special issue), 3(4):235-312, 1990.5.
Miller, G. A., Leacock, C., Tengi, R., and Bunker, R. T., "Asemantic oncordance", Proc.
Human Language TechnologyWorkshop, 303-308, Plainsboro, NJ, March, 1993, MorganKaufmann.125
