Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2236?2242,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsBuilding compositional semantics and higher-order inference systemfor a wide-coverage Japanese CCG parserKoji Mineshima1mineshima.koji@ocha.ac.jpRibeka Tanaka1tanaka.ribeka@is.ocha.ac.jpPascual Mart?
?nez-Go?mez2pascual.mg@aist.go.jpYusuke Miyao3yusuke@nii.ac.jpDaisuke Bekki1bekki@is.ocha.ac.jp1Ochanomizu UniversityTokyo, Japan2AISTTokyo, Japan3National Institute of InformaticsThe Graduate University for Advanced StudiesTokyo, JapanAbstractThis paper presents a system that compo-sitionally maps outputs of a wide-coverageJapanese CCG parser onto semantic represen-tations and performs automated inference inhigher-order logic.
The system is evaluatedon a textual entailment dataset.
It is shownthat the system solves inference problems thatfocus on a variety of complex linguistic phe-nomena, including those that are difficult torepresent in the standard first-order logic.1 IntroductionLogic-based semantic representations have playedan important role in the study of semantic parsingand inference.
For English, several methods havebeen proposed to map outputs of parsers based onsyntactic theories like CCG (Steedman, 2000) ontological formulas (Bos, 2015).
Output formulas havebeen used in various tasks, including Question An-swering (Lewis and Steedman, 2013) and Recog-nizing Textual Entailment (RTE) (Bos and Markert,2005; Beltagy et al, 2013; Bjerva et al, 2014).Syntactic and semantic parsing for Japanese, bycontrast, has been dominated by chunk-based de-pendency parsing and semantic role labelling (Kudoand Matsumoto, 2002; Kawahara and Kurohashi,2011; Hayashibe et al, 2011).
Recently, the methodof inducing wide-coverage CCG resources for En-glish (Hockenmaier and Steedman, 2007) has beenapplied to Japanese and a robust CCG parser basedon it has been developed (Uematsu et al, 2015).However, building a method to map CCG trees inJapanese onto logical formulas is not a trivial task,mainly due to the differences in syntactic structuresbetween English and Japanese (Section 3).There are two primary contributions of this pa-per.
First, based on an in-depth analysis of thesyntax-semantics interface in Japanese, we presentthe first system that compositionally derives seman-tic representations for a wide-coverage JapaneseCCG parser.
Output representations are formulasin higher-order logic (HOL) combined with Neo-Davidsonian Event Semantics (Parsons, 1990).
Sec-ond, we demonstrate the capacity of HOL for textualentailment.
We evaluate the system on a Japanesetextual entailment dataset (Kawazoe et al, 2015), adataset constructed in a similar way to the FraCaSdataset for English (Cooper et al, 1994; MacCartneyand Manning, 2007).
Although it is usually thoughtthat HOL is unfeasible for practical applications, theresults show that the entire system is able to performefficient logical inference on complex linguistic phe-nomena such as generalized quantifiers and inten-sional modifiers ?
phenomena that pose challengesto the standard first-order-logic-based approaches.2 Background and system overviewThis section provides a brief overview of the en-tire system as applied to RTE, a task of determin-ing whether a given text (T ) entails, contradicts, oris just consistent with, a given hypothesis (H).
Inlogic-based approaches, the meanings of T and Hare represented by logical formulas; whether the en-tailment relation holds is typically determined bychecking whether T ?H is a theorem in a logicalsystem with the help of a knowledge base.Currently, first-order logic (FOL) is the most pop-2236ular logical system used for RTE (Bos and Mark-ert, 2005; Lewis and Steedman, 2013; Bjerva et al,2014).
One advantage of systems based on FOL isthat practical general-purpose theorem provers andmodel-builders are available.
However, a drawbackis that there are linguistic phenomena that cannot berepresented in the standard FOL; a typical exampleis a generalized quantifier such asmost (Barwise andCooper, 1981).
Accordingly, it has been standard informal semantics of natural language to use HOLas a representation language (Montague, 1974).
Al-though HOL does not have general-purpose theoremprovers, there is room for developing an automatedreasoning system specialized for natural languageinference.
In general, a higher-order representationmakes the logical structure of a sentence more ex-plicit than a first-order encoding does and hence cansimplify the process of proof search (Miller and Na-dathur, 1986).
Recently, based on the evaluation onthe FraCaS dataset (Cooper et al, 1994), Mineshimaet al (2015) showed that a higher-order inferencesystem outperformed the Boxer/Nutcracker?s first-order system (Bos, 2008) in both speed and ac-curacy.
Likewise, Abzianidze (2015) developed ahigher-order prover based on natural logic tableausystem and showed that it achieved high accuracycomparable to state-of-the-art results on the SICKdataset (Marelli et al, 2014).There are three main steps in our pipeline.
Thefocus of this paper is on the last two components.1.
Syntactic parsing Input sentences are mappedonto CCG trees.
We use a Japanese CCG parserJigg (Noji and Miyao, 2016)1, a statistical parserbased on Japanese CCGbank (Uematsu et al, 2015).2.
Semantic parsing CCG derivation trees arecompositionally mapped onto semantic representa-tions in HOL.
The compositional mapping is imple-mented via simply typed ?-calculus in the standardway (Bos, 2008; Mart?
?nez-Go?mez et al, 2016).3.
Logical inference Theorem proving in HOLis performed to check for entailment and contra-diction.
Axioms and proof-search procedures arelargely language-independent, so we use the higher-order inference system of Mineshima et al (2015)2and adapt it for our purpose.1https://github.com/mynlp/jigg2https://github.com/mynlp/ccg2lambdaSyntactic category Semantic representationNP ?NF.
?x(N(Base, x) ?
F (x))S\NPga ?QK.Q(?I.I, ?x.
?v(K(Base, v) ?
(Nom(v) = x)))S\NPga\NPo ?Q2Q1K.Q1(?I.I, ?x1.Q2(?I.I, ?x2.
?v(K(Base, v)?
(Nom(v) = x1) ?
(Acc(v) = x2))))S/S ?SK.S(?Jv.K(?v?.(J(v?)
?
Base(v?
)), v))NP/NP ?QNF.Q(?Gx.N(?y.
(Base(y) ?G(y)), x), F )Table 1: Examples of semantic templates.
Base is the positionin which the base form of a word appears.3 Compositional Semantics and HOL3.1 CCG and semantic lexiconCombinatory Categorial Grammar (CCG) (Steed-man, 2000) is a lexicalized grammar formalismsuitable for implementing a compositional mappingfrom syntax to semantics.
A syntactic category ofCCG is either a basic category such as S and NPor a functional category of the form X/Y or X\Y.The meaning of a sentence is computed from a smallnumber of combinatory rules and the meanings ofconstituent words.
In addition to standard combi-natory rules, the Japanese CCG parser uses a smallnumber of unary type-shifting rules (e.g., the rel-ativization rule that changes the category S\NP toNP/NP), to which suitable meaning compositionrules are given.We follow the standard method of building asemantic lexicon in CCG-based logical seman-tics (Bos, 2008).
There are two kinds of lexical en-tries: (1) semantic templates that are schematic en-tries assigned to syntactic categories, possibly withsyntactic features and (2) lexical entries directly as-signed to a limited number of logical and functionalexpressions.
Lexical entries can be sensitive to aPOS tag, a surface form, and other information con-tained in the parser output.
Table 1 shows semantictemplates for main syntactic categories.
More de-tails will be provided in Section 3.2 and 3.3.We use a language of standard higher-order logic(simple type theory) (Carpenter, 1997) as a represen-tation language.
Expressions in HOL are assignedsemantic types.
We use three basic types: E (En-tity), Ev (Event), and Prop (Proposition).
Thus, thesemantic types of expressions in our system are de-fined by the ruleT ::= E | Ev | Prop | T1 ?
T2where T1 ?
T2 is a function type.First-order language can be taken as a fragmentof this system; apart from logical connectives and2237NP ?
= ((E?Prop)?E?Prop)?(E?Prop)?PropS?
= ((Ev?Prop)?Ev?Prop)?Prop(C1/C2)?
= (C1\C2)?
= C2?
?C1?Figure 1: The mapping from syntactic categories to semantictypes.
?
is right-associative.quantifiers, all primitive expressions in first-orderlogic are confined to constant symbols of type E andpredicates of type E ?
Prop, E ?
E ?
Prop, andso on.
Thus, adopting higher-order language doesnot lead to the loss of the expressive power of first-order language.The Japanese CCG parser simplifies the standardCCG and uses two basic categories, S and NP.
Ac-cordingly, a mapping (?)?
from syntactic categoriesto semantic types can be defined as in Figure 1.Keeping the correspondence between syntactic cat-egories and semantic types in the semantic lexiconguarantees that a well-formed formula is compo-sitionally derived from the meaning assignment toeach leaf of a CCG derivation tree.3.2 Semantic composition for VPsTo model a semantics for VPs in Japanese, we adoptNeo-Davidsonian Event Semantics (Parsons, 1990;Jurafsky and Martin, 2009), which is widely usedin the NLP field.
For instance, the sentence (1) isanalyzed as having the logical form in (2):(1) ???John?NOM????slowly??walk?
?PAST?John walked slowly?
(2) ?v(walk(v)?
(Nom(v)= john) ?
slow(v)?Past(v))In this approach, verbs are analyzed as 1-place pred-icates over events; arguments and adjuncts of VPsare also analyzed as event predicates.
This seman-tic uniformity is suitable to handling Japanese syn-tactic structures in which the arguments of a VPis often implicit and thus the argument-adjunct dis-tinction is less transparent than languages like En-glish (Pietroski, 2005).
As is seen in (2), we adoptthe unique-role requirement for case markers (Carl-son, 1984); for instance, the nominative case markerdoes not denote the relation Nom(v, x), as in theevent semantics in Boxer (Bos, 2008), but the func-tion Nom(v)=x.
This treatment allows us to makeuse of logical properties of equality and hence ismore suited to theorem-proving in our setting.To derive a semantic representation in event se-mantics compositionally, we adopt the composi-tional semantics of VPs in Champollion (2015) andanalyze VPs themselves as introducing existentialquantification over events.
To derive the correctmeaning for VP modifiers, the semantic type of averb is raised so that the verb takes a modifier asargument but not vice versa.
Figures 2 and 3 giveexample derivations.VP modifiers such as slowly license an inferencefrom John walked slowly to John walked, an infer-ence correctly captured by the formula in (2).
In En-glish and Japanese, however, there are intensionalVP modifiers that do not license this inference pat-tern.
Thus, the sentence John almost walked doesnot entail John walked (Dowty, 1979).
While it isnot easy to provide a desirable analysis in first-orderlanguage (Hobbs, 1985), HOL gives a perspicuousrepresentation:(3) ?v(almost(walk, v)?
(Nom(v)= john)?Past(v))Here, almost is a higher-order predicate having thesemantic type (Ev ?
Prop) ?
Ev ?
Prop.
Themeaning assignment to VP modifiers of categoryS/S in Table 1 is for extensional modifiers; anintensional modifier is assigned the representation?SK.S(?Jv.K(Base(J), v)) in the lexical entry,which results in a representation as in (3).3.3 Semantic composition for NPsThe quantificational structure of an NP plays a cru-cial role in capturing basic entailment patterns suchas monotonicity inference.
In the case of English,quantificational structures are specified by the typeof determiners (e.g.
a, the, every, some, no); to-gether with the category distinction between N andNP, which is supported in English CCGbank (Hock-enmaier and Steedman, 2007), one can provide acorrect representation for NPs.By contrast, Japanese is a classifier language,where NPs freely occur without determiners in ar-gument position (Chierchia, 1998).
For example, thesubject in (4) appears in argument position withoutaccompanying any determiner.
(4) ???small?dog?NOM??bark?
?PAST?A small dog barked?2238???
(small)NP/NP?QNF.Q(?Gx.N(?y.
(small(y) ?G(y)), x), F )?
(dog)NP?NF.
?x(N(dog, x) ?
F (x))NP?NF.?x.(N(?y.
(small(y) ?
dog(y)), x) ?
F (x))> ?
(NOM)NPga\NP?Q.QNPga?NF.?x(N(?y.
(small(y) ?
dog(y)), x) ?
F (x))<??
(bark)S\NPga?QK.Q(?I.I, ?x.
?v(K(bark, v)?
(Nom(v) = x)))?
(PAST)S\S?SK.S(?Jv.K(?v?.(J(v?)?
Past(v?
)), v))S\NPga?QK.Q(?I.I, ?x.?v(K(?v?.(bark(v?)?
Past(v?
)), v) ?
(Nom(v) = x)))<BS?K.
?x(small(x) ?
dog(x) ?
?v(K(?v?.(bark(v?)
?
Past(v?
)), v) ?
(Nom(v) = x)))< PERIODS\S?V.V (?I.I)S?x(small(x) ?
dog(x) ?
?v(bark(v) ?
Past(v) ?
(Nom(v) = x)))<Figure 2: A CCG derivation tree for the sentence ?A small dog barked?.?????
(Most)NP/NP?QNF.Most(?x.Q(?G.N(?y.(G(y)?
y = x)), ?x.?
), F )?
(car)NP?NF.
?z(N(car, z)?
F (z))NP?NF.Most(?x.?z(N(?y.(car(y)?
y = x)), z) ?
?
), F )> ?
(NOM)NPga\NP?Q.QNPga?NF.Most(?x.?z(N(?y.(car(y)?
y = x)), z) ?
?
), F )<????
(slowly)S/S?SK.S(?Jv.K(?v?.J(v?)?
slow(v?
), v))??
(move)S\NPga?QK.Q(?I.I, ?x.
?v(K(move, v)?
(Nom(v) = x)))?
(PAST)S\S?SK.S(?Jv.K(?v?.(J(v?)?
Past(v?
)), v))S\NPga?QK.Q(?I.I, ?x.?v(K(?v?.(move(v?)?
Past(v?
)), v) ?
(Nom(v) = x)))<BS\NPga?QK.Q(?I.I, ?x.?v(K(?v?.(move(v?)
?
Past(v?)
?
slow(v?
)), v) ?
(Nom(v) = x)))>BxS?K.Most(?x.
?z(car(z) ?
z = x ?
?
), ?x.?v(K(?v?.(move(v?)
?
Past(v?)
?
slow(v?
)), v) ?
(Nom(v) = x)))< PERIODS\S?V.V (?I.I)SMost(?x.
?z(car(z) ?
z = x ?
?
), ?x.
?v(move(v) ?
Past(v) ?
slow(v) ?
(Nom(v) = x)))<Figure 3: A CCG derivation tree for the sentence ?Most cars moved slowly?.
?
denotes the tautology.Bekki (2010) provides a comprehensive CCG gram-mar for Japanese that adopts the N-NP distinctionand analyzes Japanese bare NPs as accompanyingthe null determiner.
The Japanese CCGbank, bycontrast, simplifies Bekki?s (2010) grammar andavoids the use of the null determiner; it does notuse the category N and takes all NPs in Japanese tohave the syntactic category NP.
This discrepancy inNP-structure between English and Japanese poses achallenge to the standard approach to building com-positional semantics.To provide a compositional semantics adapted forthe Japanese CCG, we take NPs themselves as in-troducing quantification over individuals, along thesame lines as the semantics for VPs.
The semantictype of NPs needs to be raised so that they take NP-modifiers as argument (cf.
the template for NP in Ta-ble 1).
Figure 2 shows a derivation for the sentencein (4), where the adjective smallmodifies the NP dogto form a bare NP small dog.
It should be notedthat the predicate small(x) is correctly inserted in-side the scope of the existential quantification intro-duced by the NP dog.
The so-called privative adjec-tives (e.g.
fake and former) are analyzed in the sameway as intensional VP modifiers.Following the analysis in Mineshima et al (2015),we analyze non-first-order generalized quantifiermost as having the higher-order logical formMost(F,G), where Most has the type of general-ized quantifier (E?
Prop)?
(E?
Prop)?
Prop.Figure 3 shows an example derivation for a sentencecontaining a generalized quantifier most.
Our sys-tem also handles floating quantifiers in Japanese.4 ExperimentsWe evaluate our system3 on Japanese Semantics testsuite (JSeM)4 (Kawazoe et al, 2015), a Japanesedataset for textual entailment designed in a simi-lar way to the FraCaS dataset for English.
Thesedatasets focus on the types of logical inferences thatdo not require world knowledge.
JSeM has Japanesetranslations of FraCaS problems and an extended setof problems focusing on Japanese syntax and se-mantics.
Each problem has one or more premises,followed by a hypothesis.
There are three types ofanswer: yes (entailment), no (contradiction), and un-known (neutral).
Each problem is annotated with thetypes of inference (logical entailment, presupposi-tion, etc.)
and of linguistic phenomena.We evaluate the system on 523 problems in thedataset.
We focus on problems tagged with oneof the five phenomena: generalized quantifier, plu-3The system will be available at https://github.com/mynlp/ccg2lambda.4http://researchmap.jp/community-inf/JSeM/?lang=english2239Section #Problem Gold System SLCQuantifier 337 92.3 78.0 88.4Plural 41 68.3 56.1 51.2Adjective 65 67.7 63.1 44.6Verb 36 77.8 75.0 55.5Attitude 44 88.6 86.4 75.0Total 523 86.0 75.0 76.7Table 2: Accuracy on each section of JSeM.Acc.
Prec.
Rec.
TimeGold parses 86.0 94.9 81.3 3.30sw/o HOL axioms 69.8 93.3 56.5 2.47sSystem parses 75.0 92.7 65.4 3.58sSLC 76.7 77.5 79.3 n/aMost common class (yes) 56.8 56.8 85.6 n/aTable 3: Accuracy, precision, recall, and average proof time.ral, adjective, verb, and attitude.
We use problemswhose inference type is logical entailment, exclud-ing anaphora and presupposition.
We use Kuromoji5for morphological analysis.
To focus on the evalua-tion of semantic parsing and inference, we use goldsyntactic parses, which show an upper bound on theperformance of the semantic component.
Gold syn-tactic parses are manually selected from n-best out-puts of the CCG parser.
For the higher-order infer-ence system, we use the axioms presented in Mi-neshima et al (2015) adapted with the necessarymodification for our event semantics.Given premises P1, ... , Pn and a hypothesis H, thesystem outputs yes (P1??
?
?
?Pn?H is proved), no(P1??
?
??Pn?
?H is proved), or unknown (neitheris proved in a fixed proof-search space).6 We set a30 seconds timeout for each inference run; the sys-tem outputs unknown after it.
The current semanticlexicon has 36 templates and 113 lexical entries.Table 2 and 3 show the results.
The system withgold syntactic parses achieved 86% accuracy on thetotal 523 problems, with high precision and reason-able speed.
There was no timeout.7 The accuracydropped to 70% when ablating HOL axioms (Table3).
SLC refers to the performance of a supervisedlearning classifier8 based on 5-fold cross-validationfor each section.
Although direct comparison is not5http://www.atilika.org/6Note that natural-logic-based systems (MacCartney andManning, 2008) do not handle multi-premised problems.7Our pipeline was run single-threaded on Ubuntu Linux 64bits with a CPU at 2.67GHz.8We used NTCIR RITE baseline tools (http://www.cl.ecei.tohoku.ac.jp/rite2/doku.php).Section #Problem Gold System M15Quantifier 335 92.5 78.2 78.4Plural 38 65.8 52.6 66.7Adjective 21 57.1 47.6 68.2Verb 9 66.7 66.7 62.5Attitude 14 78.6 78.6 76.9Total 417 87.3 74.1 73.3Table 4: The results on a subset of JSeM that is a translation ofFraCaS.
M15 refers to the accuracy of Mineshima et al (2015)on the corresponding sections of FraCaS.possible, our system with gold parses outperforms itfor all sections.Out of the 523 problems, 417 are Japanese trans-lations of the FraCaS problems.
Table 4 shows acomparison between the performance of our systemon this subset of the JSeM problems and the perfor-mance of the RTE system for English in Mineshimaet al (2015) on the corresponding problems in theFraCaS dataset.
Mineshima et al (2015) used sys-tem parses of the English C&C parser (Clark andCurran, 2007).
The total accuracy of our system iscomparable to that of Mineshima et al (2015).Most errors we found are due to syntactic parseerrors caused by the CCG parser, where no cor-rect syntactic parses were found in n-best responses.Comparison between gold parses and system parsesshows that correct syntactic disambiguation im-proves performance.5 ConclusionTo our knowledge, this study provides the first se-mantic parsing system based on CCG that compo-sitionally maps real texts in Japanese onto logicalforms.
We have also demonstrated the capacity ofHOL for textual entailment.
The evaluation on JSeMshowed that our system performs efficient logical in-ference on various semantic phenomena, includingthose that challenge the standard FOL.
The attrac-tiveness of a logic-based system is that it is highlymodular and can be extended with other componentssuch as a robust knowledge base (Lewis and Steed-man, 2013; Beltagy et al, 2013; Bjerva et al, 2014).Such an extension will be a focus of future work.Acknowledgments We are grateful to the threeanonymous reviewers for their helpful commentsand suggestions.
This research has been supportedby the JST CREST program.2240ReferencesLasha Abzianidze.
2015.
A tableau prover for naturallogic and language.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 2492?2502.Jon Barwise and Robin Cooper.
1981.
Generalized quan-tifiers and natural language.
Linguistics and Philoso-phy, 4(2):159?219.Daisuke Bekki.
2010.
A Formal Theory of JapaneseGrammar: The Conjugation System, Syntactic Struc-tures, and Semantic Composition.
Kuroshio.
(InJapanese).Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-rette, Katrin Erk, and Raymond Mooney.
2013.
Mon-tague meets Markov: Deep semantics with probabilis-tic logical form.
In 2nd Joint Conference on Lexi-cal and Computational Semantics: Proceeding of theMain Conference and the Shared Task, pages 11?21.Johannes Bjerva, Johan Bos, Rob van der Goot, andMalvina Nissim.
2014.
The Meaning Factory: Formalsemantics for recognizing textual entailment and deter-mining semantic similarity.
In Proceedings of the 8thInternational Workshop on Semantic Evaluation (Se-mEval 2014), pages 642?646.Johan Bos and Katja Markert.
2005.
Recognising textualentailment with logical inference.
In Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Processing,pages 628?635.Johan Bos.
2008.
Wide-coverage semantic analysis withBoxer.
In Proceedings of the 2008 Conference on Se-mantics in Text Processing, pages 277?286.Johan Bos.
2015.
Open-domain semantic parsing withBoxer.
In Proceedings of the 20th Nordic Conferenceof Computational Linguistics, pages 301?304.Greg Carlson.
1984.
Thematic roles and their role insemantic interpretation.
Linguistics, 22(3):259?280.Bob Carpenter.
1997.
Type-Logical Semantics.
MITpress.Lucas Champollion.
2015.
The interaction of composi-tional semantics and event semantics.
Linguistics andPhilosophy, 38(1):31?66.Gennaro Chierchia.
1998.
Reference to kinds across lan-guage.
Natural Language Semantics, 6(4):339?405.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics,33(4):493?552.Robin Cooper, Richard Crouch, Jan van Eijck, Chris Fox,Josef van Genabith, Jan Jaspers, Hans Kamp, ManfredPinkal, Massimo Poesio, Stephen Pulman, et al 1994.FraCaS ?
a framework for computational semantics.Deliverable, D16.David Dowty.
1979.
Word Meaning and MontagueGrammar.
Springer.Yuta Hayashibe, Mamoru Komachi, and Yuji Matsumoto.2011.
Japanese predicate argument structure analysisexploiting argument position and type.
In Proceedingsof IJCNLP 2011, pages 201?209.Jerry R. Hobbs.
1985.
Ontological promiscuity.
In Pro-ceedings of the 23rd annual meeting on Associationfor Computational Linguistics, pages 60?69.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: a corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Compu-tational Linguistics, 33(3):355?396.Daniel Jurafsky and James H. Martin.
2009.
Speech andLanguage Processing.
Prentice-Hall, Inc.Daisuke Kawahara and Sadao Kurohashi.
2011.
Genera-tive modeling of coordination by factoring parallelismand selectional preferences.
In Proceedings of IJC-NLP 2011, pages 456?464.Ai Kawazoe, Ribeka Tanaka, Koji Mineshima, andDaisuke Bekki.
2015.
An inference problem set forevaluating semantic theories and semantic processingsystems for Japanese.
In Proceedings of LENLS12,pages 67?73.Taku Kudo and Yuji Matsumoto.
2002.
Japanese de-pendency analysis using cascaded chunking.
In Pro-ceedings of the 6th Conference on Natural LanguageLearning, pages 63?69.Mike Lewis and Mark Steedman.
2013.
Combiningdistributional and logical semantics.
Transactions ofthe Association for Computational Linguistics, 1:179?192.Bill MacCartney and Christopher D. Manning.
2007.Natural logic for textual inference.
In Proceedingsof the ACL-PASCAL Workshop on Textual Entailmentand Paraphrasing, pages 193?200.Bill MacCartney and Christopher D. Manning.
2008.Modeling semantic containment and exclusion in nat-ural language inference.
In Proceedings of the 22ndInternational Conference on Computational Linguis-tics, pages 521?528.Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaella Bernardi, and Roberto Zampar-elli.
2014.
A SICK cure for the evaluation of composi-tional distributional semantic models.
In Proceedingsof LREC2014, pages 216?223.Pascual Mart?
?nez-Go?mez, Koji Mineshima, YusukeMiyao, and Daisuke Bekki.
2016. ccg2lambda: acompositional semantics system.
In Proceedings ofACL 2016 System Demonstrations, pages 85?90.Dale Miller and Gopalan Nadathur.
1986.
Some usesof higher-order logic in computational linguistics.
InProceedings of the 24th annual meeting on Associa-tion for Computational Linguistics, pages 247?256.2241Koji Mineshima, Pascual Mart?
?nez-Go?mez, YusukeMiyao, and Daisuke Bekki.
2015.
Higher-order log-ical inference with compositional semantics.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 2055?2061.Richard Montague.
1974.
Formal Philosophy: SelectedPapers.
Yale University Press New Haven.Hiroshi Noji and Yusuke Miyao.
2016.
Jigg: a frame-work for an easy natural language processing pipeline.In Proceedings of ACL 2016 System Demonstrations,pages 103?108.Terence Parsons.
1990.
Events in the Semantics of En-glish.
MIT Press.Paul Pietroski.
2005.
Events and Semantic Architecture.Oxford University Press.Mark Steedman.
2000.
The Syntactic Process.
MITPress.Sumire Uematsu, Takuya Matsuzaki, Hiroki Hanaoka,Yusuke Miyao, and Hideki Mima.
2015.
Integrat-ing multiple dependency corpora for inducing wide-coverage Japanese CCG resources.
ACM Transac-tions on Asian and Low-Resource Language Informa-tion Processing, 14(1):1?24.2242
