Converser?
:Highly Interactive Speech-to-SpeechTranslation for HealthcareMike Dillinger Mark SeligmanSpoken Translation, Inc.
Spoken Translation, Inc.Berkeley, CA, USA 94705 Berkeley, CA, USA 94705mike.dillinger@spokentranslation.commark.seligman@spokentranslation.comAbstractWe describe a highly interactive system forbidirectional, broad-coverage spoken lan-guage communication in the healthcare area.The paper briefly reviews the system's inter-active foundations, and then goes on to dis-cuss in greater depth our TranslationShortcuts facility, which minimizes the needfor interactive verification of sentences afterthey have been vetted.
This facility also con-siderably speeds throughput while maintain-ing accuracy, and allows use by minimallyliterate patients for whom any mode of textentry might be difficult.1 IntroductionSpoken Translation, Inc. (STI) of Berkeley, CAhas developed a commercial system for interactivespeech-to-speech machine translation designed forboth high accuracy and broad linguistic and topicalcoverage.
Planned use is in situations requiringboth of these features, for example in helpingSpanish-speaking patients to communicate withEnglish-speaking doctors, nurses, and other health-care staff.The twin goals of accuracy and broad cov-erage have until now been in opposition: speechtranslation systems have gained tolerable accuracyonly by sharply restricting both the range of topicswhich can be discussed and the sets of vocabularyand structures which can be used to discuss them.The essential problem is that both speech recogni-tion and translation technologies are still quite er-ror-prone.
While the error rates may be tolerablewhen each technology is used separately, the errorscombine and even compound when they are usedtogether.
The resulting translation output is gener-ally below the threshold of usability ?
unless re-striction to a very narrow domain suppliessufficient constraints to significantly lower the er-ror rates of both components.STI?s approach has been to concentrate on inter-active monitoring and correction of both technolo-gies.First, users can monitor and correct thespeaker-dependent speech recognition system toensure that the text, which will be passed to themachine translation component, is completely cor-rect.
Voice commands (e.g.
Scratch That or Cor-rect <incorrect text>) can be used to repairspeech recognition errors.
While these commandsare similar in appearance to those of IBM'sViaVoice or ScanSoft?s Dragon NaturallySpeakingdictation systems, they are unique in that they willremain usable even when speech recognition oper-ates at a server.
Thus, they will provide for the firsttime the capability to interactively confirm or cor-rect wide-ranging text, which is dictated from any-where.Next, during the MT stage, users can monitor,and if necessary correct, one especially importantaspect of the translation ?
lexical disambiguation.STI's approach to lexical disambiguation istwofold: first, we supply a specially controlledback translation, or translation of the translation.Using this paraphrase of the initial input, even amonolingual user can make an initial judgmentconcerning the quality of the preliminary machinetranslation output.
To make this technique effec-tive, we use proprietary facilities to ensure that thelexical senses used during back translation are ap-propriate.In addition, in case uncertainty remains aboutthe correctness of a given word sense, we supply aproprietary set of Meaning Cues?
?
synonyms,definitions, etc.
?
which have been drawn fromvarious resources, collated in a unique database(called SELECT?
), and aligned with the respec-tive lexica of the relevant machine translation sys-tems.
With these cues as guides, the user can selectthe preferred meaning from among those available.Automatic updates of translation and back transla-tion then follow.The result is an utterance, which has beenmonitored and perhaps repaired by the user at twolevels ?
those of speech recognition and transla-tion.
By employing these interactive techniqueswhile integrating state-of-the-art dictation and ma-chine translation programs ?
we work with DragonNaturally Speaking for speech recognition; withWord Magic MT (for the current Spanish system);and with ScanSoft for text-to-speech ?
we havebeen able to build the first commercial-gradespeech-to-speech translation system which canachieve broad coverage without sacrificing accu-racy.2 Translation ShortcutsIn order to accumulate translations that have beenverified by hand and to simplify interaction withthe system, we have developed additional func-tionality called Translation Shortcuts?.Shortcuts are designed to provide two main ad-vantages:First, re-verification of a given utterance is un-necessary.
That is, once the translation of an utter-ance has been verified interactively, it can be savedfor later reuse, simply by activating a Save asShortcut button on the translation verificationscreen.
The button gives access to a dialogue inwhich a convenient Shortcut Category for theShortcut can be selected or created.
At reuse time,no further verification will be required.
(In additionto such dynamically created Personal Shortcuts,any number of prepackaged Shared Shortcuts canbe included in the system.
)Second, access to stored Shortcuts is veryquick, with little or no need for text entry.
Severalfacilities contribute to meeting this design crite-rion.?
A Shortcut Search facility can retrieve aset of relevant Shortcuts given only keywords orthe first few characters or words of a string.
Thedesired Shortcut can then be executed with a singlegesture (mouse click or stylus tap) or voice com-mand.NOTE: If no Shortcut is found, the systemautomatically gives access to the full power ofbroad-coverage, interactive speech translation.
T-hus, a seamless transition is provided betweenShortcuts and full translation.?
A Translation Shortcuts Browser is pro-vided, so that users can find needed Shortcuts bytraversing a tree of Shortcut categories.
Using thisinterface, users can execute Shortcuts even if theirability to input text is quite limited, e.g.
by tappingor clicking alone.The demonstration will show the ShortcutSearch and Shortcuts Browser facilities in use.Points to notice:?
The Translation Shortcuts Panel containsthe Translation Shortcuts Browser, split into twomain areas, Shortcuts Categories (above) andShortcuts List (below).?
The Categories section of the Panel showsthe current selected category, for example Conver-sation, which contains everyday expressions.
Thiscategory has a Staff subcategory, containing ex-pressions most likely to be used by healthcare staffmembers.
There is also a Patients subcategory,used for patient responses.
Such categories as Ad-ministrative topics and Patient?s Current Condi-tion are also available; and new ones can be freelycreated.?
Below the Categories section is the Short-cuts List section, containing a scrollable list of al-phabetized Shortcuts.
(Various other sortingcriteria will be available in the future, e.g.
sortingby frequency of use, recency, etc.)?
Double clicking on any visible Shortcut inthe List will execute it.
Clicking once will selectand highlight a Shortcut.
Typing Enter will exe-cute the currently highlighted Shortcut, if any.?
It is possible to automatically relate op-tions for a patient's response to the previous staffmember?s utterance, e.g.
by automatically going tothe sibling Patient subcategory if the prompt wasgiven from the Staff subcategory.Because the Shortcuts Browser can be usedwithout text entry, simply by pointing and clicking,it enables responses by minimally literate users.
Inthe future, we plan to enable use even by com-pletely illiterate users, through two devices: wewill enable automatic pronunciation of Shortcutsand categories in the Shortcuts Browser via text-to-speech, so that these elements can in effect be readaloud to illiterate users; and we will augmentShared Shortcuts with pictorial symbols, as cluesto their meaning.A final point concerning the Shortcuts Browser:it can be operated entirely by voice commands,although this mode is more likely to be useful tostaff members than to patients.We turn our attention now to the Input Window,which does double duty for Shortcut Search andarbitrary text entry for full translation.
We willconsider the search facility first.?
Shortcuts Search begins automatically assoon as text is entered by any means ?
voice,handwriting, touch screen, or standard keyboard ?into the Input Window.?
The Shortcuts Drop-down Menu appearsjust below the Input Window, as soon as there areresults to be shown.
The user can enter a fewwords at a time, and the drop-down menu will per-form keyword-based searches and present thechanging results dynamically.?
The results are sorted alphabetically.
Vari-ous other sorting possibilities may be useful: byfrequency of use, proportion of matched words,etc.?
The highest priority Shortcut according tothe specified sorting procedure can be highlightedfor instant execution.?
Highlighting in the drop-down menu issynchronized with that of the Shortcuts list in theShortcuts Panel.?
Arrow keys or voice commands can beused to navigate the drop-down menu.?
If the user goes on to enter the exact text ofany Shortcut, e.g.
?Good morning,?
a message willshow that this is in fact a Shortcut, so that verifica-tion will not be necessary.
However, final text notmatching a Shortcut, e.g.
?Good job,?
will bepassed to the routines for full translation with veri-fication.3 Future developmentsWe have already mentioned plans to augment theTranslation Shortcuts facility with text-to-speechand iconic pictures, thus moving closer to a systemsuitable for communication with completely illiter-ate or incapacitated patients.Additional future directions follow.?
Server-based architectures:  We plan tomove toward completely or partially server-basedarrangements, in which only a very thin clientsoftware application ?
for example, a web interface?
will run on the client device.
Such architectureswill permit delivery of our system on smart phonesin the Blackberry or Treo class.
Delivery on hand-helds will considerably diminish the issues ofphysical awkwardness discussed above, and any-time/anywhere/any-device access to the systemwill considerably enlarge its range of uses.?
Pooling Translation Shortcuts:  As ex-plained above, the current system now supportsboth Personal (do-it-yourself) and Shared (pre-packaged) Translation Shortcuts.
As yet, however,there are no facilities to facilitate pooling of Per-sonal Shortcuts among users, e.g.
those in a work-ing group.
In the future, we will add facilities forexporting and importing shortcuts.?
Translation memory: Translation Short-cuts can be seen as a variant of Translation Mem-ory, a facility that remembers past successfultranslations so as to circumvent error-prone re-processing.
However, at present, we save Shortcutsonly when explicitly ordered.
If all other successfultranslations were saved, there would soon be fartoo many to navigate effectively in the TranslationShortcuts Browser.
In the future, however, wecould in fact record these translations in the back-ground, so that there would be no need to re-verifynew input that matched against them.
Messageswould advise the user that verification was beingbypassed in case of a match.?
Additional languages: The full SLT sys-tem described here is presently operational only forbidirectional translation between English andSpanish.
We expect to expand the system to Man-darin Chinese next.
Limited working prototypesnow exist for Japanese and German, though weexpect these languages to be most useful in appli-cation fields other than healthcare.4 ConclusionWe have described a highly interactive system forbidirectional, broad-coverage spoken languagecommunication in the healthcare area.
The paperhas briefly reviewed the system's interactive foun-dations, and then gone on to discuss in greaterdepth issues of practical usability.We have presented our Translation Shortcutsfacility, which minimizes the need for interactiveverification of sentences after they have been vet-ted once, considerably speeds throughput whilemaintaining accuracy, and allows use by minimallyliterate patients for whom any mode of text entrymight be difficult.We have also discussed facilities for multimo-dal input, in which handwriting, touch screen, andkeyboard interfaces are offered as alternatives tospeech input when appropriate.
In order to dealwith issues related to physical awkwardness, wehave briefly mentioned facilities for hands-free oreyes-free operation of the system.Finally, we have pointed toward several direc-tions for future improvement of the system.
