NAACL HLT Demonstration Program, pages 17?18,Rochester, New York, USA, April 2007. c?2007 Association for Computational LinguisticsThe CALO Meeting AssistantL.
Lynn Voss Patrick EhlenEngineering and Systems Division CSLISRI International Stanford UniversityMenlo Park, CA 94025 Stanford, CA 94305loren.voss@sri.com ehlen@stanford.eduandThe DARPA?
CALO Meeting Assistant Project Team*AbstractThe CALO Meeting Assistant is an inte-grated, multimodal meeting assistant tech-nology that captures speech, gestures, andmultimodal data from multiparty interactionsduring meetings, and uses machine learningand robust discourse processing to provide arich, browsable record of a meeting.1 IntroductionTechnologies that assist in making meetings moreproductive have a long history.
The latest chapter inthat history involves projects that integrate recentadvances in speech, natural language understanding,vision, and multimodal interaction technologies inan effort to produce tools that can perceive whathappens at a meeting, extract salient events and in-teractions, and produce a record of the meeting thatpeople can later consult or analyze.Research projects such as the ICSI Meeting Pro-ject (Janin et al 2004) have sought to produce auto-mated and segmented transcripts from natural, multi-party speech as it occurs in meetings.
Others, likethe ISL Smart Meeting Room Task (Waibel et al2003), and the M4 and AMI projects (Nijholt, opden Akker, & Heylen 2005), employ instrumented?
This material is based upon work supported by the DefenseAdvanced Research Projects Agency (DARPA) under ContractNo.
NBCHD030010.
Any opinions, findings, and conclusions orrecommendations expressed in this material are those of theauthors and do not necessarily reflect the views of DARPA orthe Department of Interior-National Business Center.
* The DARPA CALO MA Project is a collaborative effortamong researchers at Adapx, CMU, Georgia Tech, MIT, SRI,Stanford University, UC Berkeley, and UC Santa Cruz.meeting rooms to collect multiple streams of behav-ior data and analyze the interactions of meeting par-ticipants to produce a rich and flexible record oftheir meeting activities, while also providing a sup-portive environment for collaboration.The CALO Meeting Assistant is similar to the lat-ter in that it collects multiple streams of informationabout the behaviors of people in meetings, and as-similates speech, movement, and note-taking behav-ior to create a rich representation of the meeting thatcan be analyzed and reviewed at many levels.
How-ever, a primary aim of the CALO Meeting Assistantis to integrate its observations with those of a largersystem of agents, which can assess the meeting datait collects in the context of the ongoing projects andworkflow in the work lives of each of the meetingparticipants.
Thus, our meeting assistant aims toreach beyond an intelligent room that understandsonly the activities of people in meetings, and at-tempts to understand their overarching concerns andinterpret their behaviors from the perspective ofwhat their meetings mean to them.That overarching system of agents is being devel-oped under the DARPA CALO (Cognitive Assistantthat Learns and Organizes) Program, which seeks toproduce machine learning technology in the form ofpersonalized agents that support high-level knowl-edge workers in carrying out their professional ac-tivities.
The CALO system handles a broad range ofinterrelated decision-making tasks that are tradition-ally resistant to automation; doing so partly by inter-acting with, being advised by, and learning from itsusers.
The CALO system can take initiative on com-pleting routine tasks, and on assisting when the un-expected happens.CALO is designed from the ground up as a cogni-tive system.
Whereas conventional, hand-coded soft-ware excels at a narrow set of capabilities in a17particular domain, cognitive systems maintain ex-plicit, declarative models of their capabilities, ongo-ing activities, and operating environments.
Thesemodels enable CALO to extend and improve its ca-pabilities through learning and adaptation.
Cognitivesystems are better equipped to cope with unexpecteddevelopments, learn to improve over time, and adaptto the contexts and requirements of different situa-tions.
CALO also uses natural interfaces that enablesimple, effective interactions with humans and othercognitive systems.The CALO Meeting Assistance Project is devel-oping capabilities to enable CALO to participate ingroup discussions and meetings.
Unlike instru-mented ?intelligent room?
meeting projects, this sys-tem is designed for users in an office environmentwith access to the Internet, a laptop, and some small,off-the-shelf peripheral devices (such as headsets,webcams, and digital writing devices) to capturespeech, gestures, and handwriting.
It aims to be un-obtrusive by leveraging cross-training, unsupervisedlearning, and lightweight supervision captured fromnormal user interaction (e.g., users reviewing andediting notes, or adding detected action items to a to-do list).These data are transparently processed at a centralserver location and redistributed, so the meeting as-sistant interacts seamlessly with other CALO desk-top functionalities, using a common ontology.2 What it doesThe CALO Meeting Assistant helps its owners bycapturing and interpreting meeting conversationsand activities and, as appropriate, retrieving relevantinformation.
Information gleaned from a meetingcan be incorporated in the respective owner?s CALOknowledge stores to, for example, track commit-ments and remember references to projects, people,places, and dates.
An archive of each meeting pro-vides a searchable record for users, as well as a his-tory of training data for CALO?s learningcomponents.
Learning areas include the following:Speech processing?Automatic transcriptions are pro-duced from conversational speech among multiplespeakers while adapting to speaker and backgroundnoise; recognizing prosodic cues; learning new vo-cabulary; and constructing person, role, and topic-specific language models.Visual recognition?Faces, gaze direction, gestures,and activities are detected, and detection is improvedthrough lightly-supervised learning and unsuper-vised cross-training.Discourse understanding?Dialog moves are recog-nized, topics are segmented and grouped throughsupervised and unsupervised generative models, ac-tion items are detected, and discussions can bethreaded across documents and email.Multimodal reinforcement?Pen, speech, and text in-puts combine to offer natural communications.Meeting activity?Speech and note-taking activitiescombine to provide cross-training for recognizingmeeting phases, and for tracking agendas and docu-ment usage.3 DemoWe demonstrate how the CALO Meeting Assistantcaptures speech, pen, and other meeting data usingan ordinary laptop; produces an automated tran-script; segments by topic; and performs shallow dis-course understanding to produce a list of probableaction items arising from a single, pre-recordedmeeting.
We then demonstrate a Meeting Rapporteurthat provides a meeting summary and allows partici-pants to review and organize the meeting transcript,audio, notes, action items, and topics?all whileproviding actions in a feedback loop that supportsthe meeting assistant?s semi-supervised learningprocess.
Finally, we discuss the potential and currentdevelopment of real-time capabilities that allow us-ers to interact with the meeting assistant during anongoing meeting.ReferencesJanin, A., Ang, J., Bhagat, S., Dhillon, R., Edwards, J.,Marcias-Guarasa, J., Morgan, N., Peskin, B., Shriberg,E., Stolcke, A., Wooters, C., and Wrede, B.
2004.
TheICSI meeting project: Resources and research.
In Pro-ceedings of the 2004 IEEE International Conferenceon Acoustics, Speech, and Signal Processing (ICASSP?04) Meeting Recognition Workshop (NIST RT-04).Nijholt, A., op den Akker, R., and Heylen, D. 2005.
Meet-ings and meeting modeling in smart environments.
AI& Society, 20(2):202-220.Waibel, A., Schultz, T., Bett, M., Denecke, M., Malkin,R., Rogina, I., Stiefelhagen, R., and Yang, J.
2003.SMaRT: The smart meeting room task at ISL.
In Pro-ceedings of the 2003 IEEE International Conferenceon Acoustics, Speech, and Signal Processing (ICASSP'03), pp 752-755.18
