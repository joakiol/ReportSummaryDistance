CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 105?112Manchester, August 2008Acquiring Knowledge from the Web to be used as Selectors for NounSense DisambiguationHansen A. Schwartz and Fernando GomezSchool of Electrical Engineering and Computer ScienceUniversity of Central Florida{hschwartz, gomez}@cs.ucf.eduAbstractThis paper presents a method of acquiringknowledge from the Web for noun sensedisambiguation.
Words, called selectors,are acquired which take the place of aninstance of a target word in its local con-text.
The selectors serve for the system toessentially learn the areas or concepts ofWordNet that the sense of a target wordshould be a part of.
The correct senseis chosen based on a combination of thestrength given from similarity and related-ness measures over WordNet and the prob-ability of a selector occurring within the lo-cal context.
Our method is evaluated usingthe coarse-grained all-words task from Se-mEval 2007.
Experiments reveal that path-based similarity measures perform just aswell as information content similarity mea-sures within our system.
Overall, the re-sults show our system is out-performedonly by systems utilizing training data orsubstantially more annotated data.1 IntroductionRecently, the Web has become the focus for manyword sense disambiguation (WSD) systems.
Dueto the limited amount of sense tagged data avail-able for supervised approaches, systems which aretypically referred to as unsupervised, have turnedto the use of unannotated corpora including theWeb.
The advantage of these systems is that theycan disambiguate all words, and not just a set ofwords for which training data has been provided.In this paper we present an unsupervised systemwhich uses the Web in a novel fashion to performc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.sense disambiguation of any noun, incorporatingboth similarity and relatedness measures.As explained in (Brody et al, 2006), there aregenerally two approaches to unsupervised WSD.The first is referred to as token based, which com-pares the relatedness of a target word to otherwords in its context.
The second approach is typebased, which uses or identifies the most commonsense of a word over a discourse or corpus, and an-notates all instances of a word with the most com-mon sense.
Although the type based approach isclearly bound to fail occasionally, it is commonlyfound to produce the strongest results, rivaling su-pervised systems (McCarthy et al, 2004).
Weidentify a third approach through the use of selec-tors, first introduced by (Lin, 1997), which helpto disambiguate a word by comparing it to otherwords that may replace it within the same localcontext.We approach the problem of word sense dis-ambiguation through a relatively straightforwardmethod that incorporates ideas from the token,type, and selector approaches.
In particular, weexpand the use of selectors in several ways.
First,we revise the method for acquiring selectors to beapplicable to the web, a corpus that is, practicallyspeaking, impossible to parse in whole.
Second,we describe a path-based similarity measure thatis more suited for a portion of our method than therelatedness measures used by token based systems.Finally, we expand the use of selectors to help withdisambiguating nouns other than the one replaced.2 Background2.1 Word Sense DisambiguationA popular approach to using the web or unanno-tated corpora for word sense disambiguation in-volves the use of monosemous relatives.
Monose-mous relatives are words which are similar to a105sense of the target word, but which only have onesense.
By searching text for these words, one canbuild training data for each sense of a target word.This idea was proposed by (Leacock et al, 1998).More recently, the idea has been used to auto-matically create sense tagged corpora (Mihalcea,2002; Agirre and Martinez, 2004) .
These meth-ods queried large corpora with relatives rather thanwith the context.With some resemblances to our approach, (Mar-tinez et al, 2006) present the relatives in contextmethod.
A key similarity of this method with oursis the use of context in the web queries.
They pro-duce queries with relatives in place of the targetword in a context with a window size of up to 6.Similarly, (Yuret, 2007) first chooses substitutesand determines a sense by looking at the proba-bility of a substitute taking the place of the targetword within the Web1T corpus.
The number ofhits each query has on the web is then used to pickthe correct sense.
Our approach differs from thesein that we acquire words(selectors) from the web,and proceed to choose a sense based on similaritymeasures over WordNet (Miller et al, 1993).
Wealso attempt to match the context of the entire sen-tence if possible, and we are more likely to receiveresults from longer queries by including the wild-card instead of pre-chosen relatives.We adopted the term selector from (Lin, 1997)to refer to a word which takes the place of anotherin the same local context.
Lin searched a local con-text database, created from dependency relation-ships over an unannotated corpora in order to findselectors.
In this case, the local context was repre-sented by the dependency relationships.
Given thatthe task of producing a dependency parse databaseof the Web is beyond our abilities, we search forthe surrounding local context as text in order toretrieve selectors for a given word.
Another dif-ference is that we compare the relatedness of se-lectors of other words in the sentence to the targetword, and we also incorporate a path-based simi-larity measure along with a gloss-based relatednessmeasure.2.2 Similarity and Relatedness MeasuresSemantic similarity and relatedness measures havean extensive history.
The measures reported in thiswork were included based on appropriateness withour approach and because of past success accord-ing to various evaluations (Patwardhan et al, 2003;Budanitsky and Hirst, 2006).Many similarity measures have been createdwhich only use paths in the WordNet ontology.One approach is to simply compute the lengthof the shortest path between two concepts overthe hypernym/hyponym relationship (Rada et al,1989).
Other methods attempt to compensate forthe uniformity problem, the idea that some areas ofthe ontology are more dense than others, and thusall edges are not equal.
(Wu and Palmer, 1994)uses the path length from the root to the lowestcommon subsumer(LCS) of two concepts scaledby the distance from the LCS to each concept.
An-other method, by (Leacock et al, 1998), normal-izes path distance based on the depth of hierar-chy.
Our method attempts to produce a normalizeddepth based on the average depth of all conceptswhich are leaf nodes below the lowest commonsubsumer in a tree.We employ several other measures in our sys-tem.
These measures implement various ideassuch as information content (Jiang and Conrath,1997; Lin, 1997) and gloss overlaps (Banerjee andPedersen, 2003).
For our work the path-based andinformation content measures are referred to assimilarity measures, while the gloss-based meth-ods are referred to as relatedness measures.
Re-latedness measures can be used to compare wordsfrom different parts of speech.
In past evaluationsof token based WSD systems, information con-tent and gloss-based measures perform better thanpath-based measures (Patwardhan et al, 2003; Bu-danitsky and Hirst, 2006).3 MethodThe general idea of our method is to find the senseof a target noun which is most similar to all se-lectors which can replace the target and most re-lated to other words in context and their selectors.Our method requires that a test sentence has beenpart-of-speech tagged with noun, verb, and adjec-tive POS, and we use the selectors from all of theseparts of speech as well as noun selectors of pro-nouns and proper nouns.
In this work, we only dis-ambiguate nouns because similarity measures fortarget selectors are based heavily on the depth thatis present in the WordNet noun ontology.
How-ever, we are still able to use verb and adjective se-lectors from the context through relatedness mea-sures working over all parts of speech listed.
Themethod can be broken into two steps:1061.
Acquire probabilities of selectors occurringfor all nouns, verbs, adjectives, pronouns andproper nouns from the Web.2.
Rank the senses of a target noun according tosimilarity with its own selectors and related-ness with other selectors in the context.These steps are described in detail below.
Finally,we also describe a similarity measure we employ.3.1 Acquiring SelectorsWe acquire target selectors and context selectorsfrom the Web.
Target selectors are those wordswhich replace the current target word in the localcontext, while context selectors are words whichmay replace other words in the local context.There are four different types of context selectors:noun context selectors essentially the target se-lectors for other nouns of the sentence.verb context selectors verbs which are found toreplace other verbs in the sentence.adjective context selectors adjectives which re-place other adjectives in the sentence.pro context selectors nouns which replace pro-nouns and proper nouns.A query must be created based on the originalsentence and target word.
This is fairly straightfor-ward as the target word is removed and replacedwith a * to indicate the wildcard.
For example,when searching for selectors of ?batter?
from ?Sheput the batter in the refrigerator.
?, a query of ?Sheput the * in the refrigerator.?
is used.
The queriesare sent through the Yahoo!
Search Web Services1in order to retrieve matching text on the web.The selectors are extracted from the samples re-turned from the web by matching the wildcard ofthe query to the sample.
The wildcard match isthrown out if any of the following conditions aretrue: longer than 4 words, contains any punctua-tion, is composed only of pronouns or the origi-nal word.
Keep in mind we acquire the nouns thatreplace the pronouns of the original sentence, soa selector is never a pronoun.
WordNet is usedto determine if the phrase is a compound and thebase morphological form of the head word.
Re-sults containing head words not found in WordNetare filtered out.
Proper nouns are used if they arefound in WordNet.
Finally, the list of selectors is1http://developer.yahoo.com/search/adjusted so no single word takes up more than 30%of the list.The Web is massive, but unfortunately it is notlarge enough to find results when querying witha whole sentence a majority of the time.
There-fore, we perform truncation of the query to acquiremore selectors.
For this first work with selectorsfrom the web, we chose to create a simple trunca-tion focused just on syntax in order to run quickly.The steps below are followed and the final step isrepeated until a stop condition is met.i Shorten to a size of 10 words.ii Remove end punctuation, if not preceded by *.iii Remove front punctuation, if not proceeded by *.iv Remove determiners (the, a, an, this, that) preceding *.v Remove a single word.When removing a single word, the algorithm at-tempts to keep the * in the center.
Figure 1 demon-strates the loop that occurs until a stop conditionis met: enough selectors are found or the queryhas reached a minimum size.
Since a shorter queryshould return the same results as a longer query, wefilter the selectors from longer query results out ofthe shorter results.
It is important that the criteriato continue searching is based on the number of se-lectors and not on the number of samples, becausemany samples fail to produce a selector.Validationexperiments were performed to verify that eachstep of truncation was helpful in returning more re-sults with valid selectors, although the results arenot reported as the focus is on the method in gen-eral.
Selectors are tied to the queries used to ac-quire them in order to help emphasize results fromlonger queries.The steps to acquire all types of selectors (tar-get or any in context) are the same.
The part ofspeech only plays a part in determining the baseform or compounds when using WordNet.
Notethat all selectors for each noun, verb, adjective, andpronoun/proper can be acquired in one pass, so thatduplicate queries are not sent to the Web.
When theprocess is complete we have a probability value foreach selector word (ws) to occur in a local contextgiven by the acquisition query (q).
The probabilityof wsappearing in q is denoted as:pocc(ws, q)3.2 Ranking SensesThere are essentially two assumptions made in or-der to rank the senses of a noun.107Figure 1: The overall process undertaken to disambiguate a noun.
(Note that selectors only need to beacquired once for each sentence since they can be reused for each target noun.)1.
Similar concepts (or noun senses) appear insimilar syntactic constructions.2.
The meaning of a word is often related toother words in its contextThe first assumption implies the use of a similaritymeasure with target selectors.
The meaning of thetarget selectors should be very similar to that ofthe original word, and thus we compare similaritybetween all target selectors with each sense of theoriginal word.The second assumption reflects the informationprovided by context selectors, for which we use arelatedness measure to compare with the originalword.
Note that because context selectors may beof a different part of speech, we should be sure thismeasure is able to handle multiple parts of speech.Regardless of the similarity or relatedness mea-sure used, the value produced is applied the samefor both target selectors and context selectors.
Weare comparing the senses (or concepts) of the origi-nal target word with all of the selectors.
To find thesimilarity or relatedness of two words, rather thantwo concepts, one can use the maximum value overall concepts of the selector word and all the sensesof the target word, (Resnik, 1999, word similarity):wsr(wt, ws) = maxct,cs[srm(ct, cs)]where srm is a similarity or relatedness measureand ct, csrepresent a sense (concept) of the tar-get word (wt) and selector word (ws) respectively.We would like to get a value for each sense of atarget word if possible, so we derive similarity orrelatedness between one concept and one word as:cwsr(ct, ws) = maxcs[srm(ct, cs)]Intuitively, combining cwsr with poccis the ba-sis for scoring the senses of each noun.
However,we also take several others values into accout, inorder to learn most effectively from Web selectors.The score is scaled by the number of senses of theselector and the length of the query used to ac-quire it.
This gives less ambiguous selectors andthose selectors with a most similar local contexta stronger role.
These values are represented bysenses(ws) and qweight = current lengthoriginal length:score(ct, ws, q)= pocc(ws, q) ?
cwsr(ct, ws) ?qweightsenses(ws)The scores are summed with:sumtype(ct) =?q?wsscore(ct, ws, q)where q ranges over all queries for a type(type) ofselector, and wsranges over all selectors acquiredwith query q.Overall, the algorithm gives a score to eachsense by combining the normalized sums from alltypes of the selectors:Score(ct) =?typesumtype(ct)maxc?wt[sumtype(c)]?
scaletypewhere typ ranges over a type of selector (target,noun context, verb context, adjective context, procontext), c ranges over all senses of the target word(wt), and scaletypeis a constant for each type ofselector.
We experimented with different valuesover 60 instances of the corpus to decide on a scalevalue of 1 for target selectors, a value of 0.5 for108noun and verb context selectors, and a value of0.1 for adjective and pro context selectors.
Thisweights the scores that come from target selectorsequal to that of noun and verb context selectors,while the adjective and pro selectors only play asmall part.Finally, the senses are sorted based on theirScore, and we implement the most frequent senseheuristic as a backoff strategy.
All those senseswithin 5% of the top sense?s Score, are re-sorted,ranking those with lower sense numbers in Word-Net higher.
The highest ranking sense is taken tobe the predicted sense.3.3 Similarity MeasureWe use the notion that similarity is a specific typeof relatedness (Rada et al, 1989; Patwardhan etal., 2003).
For our purposes, a similarity measureis used for nouns which may take the place of atarget word within its local context, while wordswhich commonly appear in other parts of the localcontext are measured by relatedness.
In particular,the similarity measure places emphasis strictly onthe is-a relationship.
As an example, ?bottle?
and?water?
are related but not similar, while ?cup?and ?bottle?
are similar.
Because of this distinc-tion, we would classify our path-based measure asa similarity measure.A well known problem with path-based mea-sures is the assumption that the links between con-cepts are all uniform (Resnik, 1999).
As a re-sponse to this problem, approaches based on in-formation content are used, such as (Resnik, 1999;Jiang and Conrath, 1997; Lin, 1997).
These mea-sures still use the is-a relationship in WordNet, butthey do not rely directly on edges to determine thestrength of a relationship between concepts.
(Pat-wardhan et al, 2003) shows that measures basedon information content or even gloss based mea-sures generally perform best for comparing a wordwith other words in its context for word sense dis-ambiguation.
However, these measures may notbe as suited for relating one word to other wordswhich may replace it (target selectors).
Therefore,our similarity measure examines the use of links inWordNet, and attempts to deal with the uniformityproblem by normalizing depths based on averageleaf node depth.All types of relatedness measures return a valuerepresenting the strength of the relation betweenthe two concepts.
These values usually range be-tween 0 and 1.
Note that concepts are not thesame as words, and the example above assumesone chooses the sense of ?water?
as a liquid andthe sense of ?bottle?
and ?cup?
as a container.
Oursimilarity measure is based on finding the normal-ized depth (nd) of a concept (c) in the WordNetHierarchy:nd(c) =depth(c)ald(c)Where depth is the length from the concept to theroot, and ald returns the average depth of all de-scendants (hyponyms) that do not have hyponymsthemselves (average leaf depth):ald(c) =?L?lnodes(c)depth(l)|lnodes(c)|To be clear, lnodes returns a list of only thosenodes without hyponyms that are themselves hy-ponyms of c. We chose to only use the leaf depthas opposed to all depths of descendants, becauseald produces a value representing maximum depthfor that branch in the tree, which is more appropri-ate for normalization.Like other similarity measures, for any two con-cepts we compute the lowest (or deepest) commonsubsumer, lcs, which is the deepest node in the hi-erarchy which is a hypernym of both concepts.
Thesimilarity between two concepts is then given bythe normalized depth of their lcs:sim(c1, c2) = nd(lcs(c1, c2))Thus, a concept compared to itself will have ascore of 1, while the most dissimilar concepts willhave a score of 0.
Following (Wu and Palmer,1994; Lin, 1997) we scale the measure by eachconcept?s nd as follows:scaled sim(c1, c2) =2 ?
sim(c1, c2)nd(c1) + nd(c2)where our normalized depth replaces the depth orinformation content value used by the past work.4 EvaluationWe evaluated our algorithm using the SemEval2007 coarse-grained all-words task.
In order toachieve a coarse grained sense inventory WordNet2.1 senses were manually mapped to the top-levelof the Oxford Dictionary of English by an expertlexicographer.
This task avoids the issues of a finegranular sense inventory, which provides senses109type insts avgSelstarget 1108 68.5noun context 1108 68.5verb context 591 70.1adj context 362 37.3pro context 372 31.9Table 1: Total word instances for which selectorswere acquired (insts), and average number of se-lectors acquired for use in each instance (avgSels).that are difficult even for humans to distinguish.Additionally, considering how recent the event oc-curred, there is a lot of up-to-date data about theperformance of other disambiguation systems tocompare with.
(Navigli et al, 2007)Out of 2269 noun, verb, adjective, or adverb in-stances we are concerned with disambiguating the1108 noun instances from the 245 sentences in thecorpus .
These noun instances represent 593 differ-ent words.
Since we did not use the coarse-grainedsenses within our algorithm, the predicted senseswere correct if they mapped to the correct coarse-grained sense.
The average instance had 2.5 possi-ble coarse-grained senses.
The average number ofselectors acquired for each word is given in Table1.
The bottom of Table 2 shows the random base-line as well as a baseline using the most frequentsense (MFS) heuristic.
As previously mentioned,many supervised systems only perform marginallybetter than the MFS.
For the SemEval workshop,only 6 of 15 systems performed better than thisbaseline on the nouns (Navigli et al, 2007), all ofwhich used MFS as a back off strategy and an ex-ternal sense tagged data set.
Our results are pre-sented as precision (P), recall (R), and F1 value(F1 = 2 ?
P?RP+R).4.1 Results and DiscussionTable 2 shows the results when using various simi-larity for the target selectors.
We selected gloss-based measures (Banerjee and Pedersen, 2003;Patwardhan et al, 2003) due to the need for han-dling multiple parts of speech for the context se-lectors.
Functionality for our use of many dif-ferent relatedness measurements was provided byWordNet::Similarity (Pedersen et al, 2004).
Ourmethod performs better than the MFS baseline,and clearly better than the random baseline.
Asone can see, the scaled sim (path2) similaritymeasure along with the gloss based relatednessgloss1 gloss2path1 78.8 78.3path2 80.2 78.6path3 78.7 78.6IC1 78.6 79.3IC2 78.5 79.2IC3 78.0 78.1gloss1 78.4 80.0gloss2 78.6 78.9MFS baseline 77.4random baseline 59.1Table 2: Performance of our method, given by F1values (precision = recall), with various similaritymeasures for target selectors: path1= sim (nor-malized depth), path2 = scaled sim, path3 = (Wuand Palmer, 1994), IC1 = (Resnik, 1999), IC2 =(Lin, 1997), IC3 = (Jiang and Conrath, 1997), andrelatedness measures for context selectors: gloss1= (Banerjee and Pedersen, 2003), gloss2 = (Pat-wardhan et al, 2003).
Baselines: MFS = most fre-quent sense, random = random choice of sense.measure of (Banerjee and Pedersen, 2003) gavethe best results.
Note that the path-based and in-formation content measures, in general, performedequally.We experimented with using the gloss-based re-latedness measures in place of similarity measures.The idea was that one measure could be used forboth target selectors and context selectors.
As onecan gather from the bottom of table 2, for the mostpart, the measures performed equally.
The experi-mental runtime of the path-based and informationcontent measures was roughly one-fourth that ofthe gloss-based measures.Table 3 presents results from experiments wherewe only attempted to annotate instances with overa minimum number of target selectors (tMin) andcontext selectors (cMin).
We use steps of four fortarget selectors and steps of ten for context selec-tors, reflecting a ratio of roughly 2 target selectorsfor every 5 context selectors.
It was more commonfor an instance to not have any target selectors thanto not have context selectors, so we present resultswith only a tMin or cMin.
The main goal of theseexperiments was simply to determine if the algo-rithm performed better on instances that we wereable to acquire more selectors.
We were able to seethis was the case as the precision improved at theexpense of recall from avoiding the noun instances110tMin cMin A P R F10 0 1108 80.2 80.2 80.24 0 658 84.4 50.1 62.916 0 561 85.2 43.1 57.20 10 982 81.1 71.9 76.20 40 908 81.3 66.6 73.34 10 603 85.4 46.4 60.18 20 554 85.3 42.6 56.912 30 516 86.4 40.2 54.916 40 497 86.5 38.8 53.5Table 3: Number attempted (A), Precision (P),Recall (R) and F1 values of our method with re-strictions on a minimum number of target selectors(tMin) and context selectors (cMin).sel noMFS 1SPD80.2 79.6 79.8Table 4: Results of a variety of experiments usingpath2 and gloss1 from the previous table.
noMFS= no use of most frequent sense, 1SPD = use of 1sense per discourse.that did not have many selectors.Table 4 shows the results when we modify themethod in a few ways.
All these results usethe path2 (scaled sim) and gloss1 (Banerjee andPedersen, 2003) measures.
The results of Ta-ble 2 include first sense heuristic used as a back-off strategy for close calls, when multiple senseshave a score within 0.05 of each other.
There-fore, we experiment without this heuristic pre-sented as noMFS, and found our method still per-forms strongly.
We also implemented one senseper discourse, reported as 1SPD.
Our experimentalcorpus had five documents, and for each documentwe calculated the most commonly predicted senseand used that for all occurrences of the word withinthe document.
Interestingly, this strategy does notseem to improve the results in our method.4.2 Comparison with other systemsTable 5 shows the results of our method (sel) com-pared with a few systems participating in the Se-mEval coarse-grained all-words task.
These re-sults include the median of all participating sys-tems, the top system not using training data (UPV-WSD) (Buscaldi and Rosso, 2007), and the topsystem using training data (NUS-PT) (Chan etal., 2007).
The best performance reported on thesel med UPV-WSD NUS-PT SSI80.2 71.1 79.33 82.31 84.12Table 5: Comparison of noun F1 values withvarious participants in the SemEval2007 coarse-grained all-words task.nouns for the SemEval coarse-grained task, wasactually from a system by the authors of the task(SSI) (Navigli and Velardi, 2005).
All systemsperforming better than the MFS used the heuris-tic as a backoff strategy when unable to output asense (Navigli et al, 2007).
Also, the systems per-forming better than ours (including SSI) used moresources of sense annotated data.5 ConclusionWe have presented a method for acquiring knowl-edge from the Web for noun sense disambiguation.Rather than searching the web with pre-chosen rel-atives, we search with a string representing the lo-cal context of a target word.
This produces a listof selectors, words which may replace the targetword within its local context.
The selectors arethen compared with the senses of the target wordvia similarity and relatedness measures to choosethe correct sense.
By searching with context in-stead of simply relatives, we are able to insuremore relevant results from the web.
Additionally,this method has an advantage over methods whichuse relatives and context in that it does not restrictthe results to include pre-chosen words.We also show that different types of similarityand relatedness measures are appropriate for dif-ferent roles in our disambiguation algorithm.
Wefound a path-based measure to be best with tar-get selectors while a slower gloss-based methodwas appropriate for context selectors in order tohandle multiple POS.
For many tasks, informationcontent based measures perform better than path-based measures.
However, we found a path-basedmeasure to be just as strong if not stronger in ourapproach.Results of our evaluation using the SemEvalcoarse-grained all-words task showed strength inthe use of selectors from the Web for disambigua-tion.
Our system was out-performed only by sys-tems using training data or substantially more an-notated data.
Future work may improve resultsthrough the use of sense tagged corpora, a gram-matical parse, or other methods commonly used in111WSD.
Additionally, better precision was achievedwhen requiring a minimum number of selectors,giving promise to improved results with morework in acquiring selectors.
This paper has shownan effective and novel method of noun sense dis-ambiguation through the use of selectors acquiredfrom the web.6 AcknowledgementThis research was supported in part by theNASA Engineering and Safety Center underGrant/Cooperative Agreement NNX08AJ98A.ReferencesAgirre, Eneko and David Martinez.
2004.
Unsuper-vised wsd based on automatically retrieved exam-ples: The importance of bias.
In Proceedings ofEMNLP 2004, pages 25?32, Barcelona, Spain, July.Association for Computational Linguistics.Banerjee, S. and T. Pedersen.
2003.
Extended glossoverlaps as a measure of semantic relatedness.
InProceedings of the Eighteenth International JointConference on Artificial Intelligence, pages 805?810, Acapulco.Brody, Samuel, Roberto Navigli, and Mirella Lapata.2006.
Ensemble methods for unsupervised wsd.
InProceedings of the 21st International Conference onComputational Linguistics, pages 97?104, Sydney,Australia.Budanitsky, Alexander and Graeme Hirst.
2006.
Eval-uating wordnet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.Buscaldi, Davide and Paolo Rosso.
2007.
Upv-wsd: Combining different wsd methods by means offuzzy borda voting.
In Proceedings of SemEval-2007, pages 434?437, Prague, Czech Republic, June.Chan, Yee Seng, Hwee Tou Ng, and Zhi Zhong.
2007.Nus-pt: Exploiting parallel texts for word sense dis-ambiguation in the english all-words tasks.
In Pro-ceedings of Proceedings of SemEval-2007, pages253?256, Prague, Czech Republic, June.Jiang, Jay J. and David W. Conrath.
1997.
Semanticsimilarity on corpus statistics and lexical taxonomy.In Proceedings of ROCLING X, Taiwan.Leacock, Claudia, Martin Chodorow, and George A.Miller.
1998.
Using corpus statistics and wordnet re-lations for sense identification.
Computational Lin-guistics, 24(1):147?165.Lin, Dekang.
1997.
Using syntactic dependency as lo-cal context to resolve word sense ambiguity.
In Pro-ceedings of the 35th annual meeting on Associationfor Computational Linguistics, pages 64?71.Martinez, David, Eneko Agirre, and Xinglong Wang.2006.
Word relatives in context for word sensedisambiguation.
In Proceedings of the 2006 Aus-tralasian Language Technology Workshop, pages42?50.McCarthy, Diana, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In Proceedings of the 42nd Meeting ofthe Association for Computational Linguistics, pages279?286, Barcelona, Spain, July.Mihalcea, Rada.
2002.
Bootstrapping large sensetagged corpora.
In Proceedings of the 3rd Inter-national Conference on Languages Resources andEvaluations LREC 2002, Las Palmas, Spain, May.Miller, George, R. Beckwith, Christiane Fellbaum,D.
Gross, and K. Miller.
1993.
Five papers on word-net.
Technical report, Princeton University.Navigli, Roberto and Paola Velardi.
2005.
Structuralsemantic interconnections: A knowledge-based ap-proach to word sense disambiguation.
IEEE Trans.Pattern Anal.
Mach.
Intell., 27(7):1075?1086.Navigli, Roberto, Kenneth C. Litkowski, and Orin Har-graves.
2007.
Semeval-2007 task 07: Coarse-grained english all-words task.
In Proceedings ofSemEval-2007, pages 30?35, Prague, Czech Repub-lic, June.Patwardhan, S., S. Banerjee, and T. Pedersen.
2003.Using Measures of Semantic Relatedness for WordSense Disambiguation.
In Proceedings of the FourthInternational Conference on Intelligent Text Pro-cessing and Computational Linguistics, pages 241?257, Mexico City, Mexico, February.Pedersen, T., S. Patwardhan, and J. Michelizzi.
2004.WordNet::Similarity - Measuring the Relatedness ofConcepts.
In Human Language Technology Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics Demonstrations,pages 38?41, Boston, MA, May.Rada, R., H. Mili, E. Bicknell, and M. Blettner.
1989.Development and application of a metric on semanticnets.
In IEEE Transactions on Systems, Man andCybernetics, volume 19, pages 17?30.Resnik, Philip.
1999.
Semantic similarity in a taxon-omy: An information-based measure and its applica-tion to problems of ambiguity in natural language.Journal of Artificial Intelligence Research, 11:95?130.Wu, Zhibiao and Martha Palmer.
1994.
Verb seman-tics and lexical selection.
In Proceedings of the32nd.
Annual Meeting of the Association for Com-putational Linguistics, pages 133 ?138, New MexicoState University, Las Cruces, New Mexico.Yuret, Deniz.
2007.
Ku: Word sense disambiguationby substitution.
In Proceedings of SemEval-2007,pages 207?214, Prague, Czech Republic, June.112
