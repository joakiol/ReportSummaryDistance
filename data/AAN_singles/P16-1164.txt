Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1736?1745,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsModel Architectures for Quotation DetectionChristian Scheible, Roman Klinger and Sebastian Pad?oInstitut f?ur Maschinelle SprachverarbeitungUniversit?at Stuttgart{scheibcn,klinger,pado}@ims.uni-stuttgart.deAbstractQuotation detection is the task of locatingspans of quoted speech in text.
The stateof the art treats this problem as a sequencelabeling task and employs linear-chain con-ditional random fields.
We question the ef-ficacy of this choice: The Markov assump-tion in the model prohibits it from makingjoint decisions about the begin, end, andinternal context of a quotation.
We per-form an extensive analysis with two newmodel architectures.
We find that (a), sim-ple boundary classification combined witha greedy prediction strategy is competitivewith the state of the art; (b), a semi-Markovmodel significantly outperforms all others,by relaxing the Markov assumption.1 IntroductionQuotations are occurrences of reported speech,thought, and writing in text.
They play an impor-tant role in computational linguistics and digitalhumanities, providing evidence for, e.g., speakerrelationships (Elson et al, 2010), inter-speaker sen-timent (Nalisnick and Baird, 2013) or politeness(Faruqui and Pado, 2012).
Due to a lack of general-purpose automatic systems, such information isoften obtained through manual annotation (e.g.,Agarwal et al (2012)), which is labor-intensive andcostly.
Thus, models for automatic quotation detec-tion form a growing research area (e.g., Pouliquenet al (2007); Pareti et al (2013)).Quotation detection looks deceptively simple,but is challenging, as the following example shows:[The pipeline], the company said, [wouldbe built by a proposed joint venture .
.
.
,and Trunkline .
.
.
will ?build and operate?the system .
.
.
].11Penn Attributions Relation Corpus (PARC), wsj 0260Note that quotations can (i) be signalled by lexi-cal cues (e.g., communication verbs) without quota-tion marks, (ii) contain misleading quotation marks;(iii) be discontinuous, and (iv) be arbitrarily long.Early approaches to quotation detection usehand-crafted rules based on syntactic mark-ers (Pouliquen et al, 2007; Krestel et al, 2008).While yielding high precision, they suffered fromlow recall.
The state of the art (Pareti et al, 2013;Pareti, 2015) treats the task as a sequence classifi-cation problem and uses a linear-chain conditionalrandom field (CRF).
This approach works well forthe prediction of the approximate location of quo-tations, but yields a lower performance detectingtheir exact span.In this paper, we show that linear-chain sequencemodels are a sub-optimal choice for this task.
Themain reason is their length, as remarked above:Most sequence labeling tasks in NLP (such as mostcases of named entity recognition) deal with spansof a few tokens.
In contrast, the median quotationlength on the Penn Attributions Relation Corpus(PARC, Pareti et al (2013)) is 16 tokens and thelongest span has over 100 tokens.
As a result of thestrong Markov assumptions that linear-chain CRFsmake to ensure tractability, they cannot capture?global?
properties of (almost all) quotations andare unable to make joint decisions about the beginpoint, end point, and content of quotations.As our first main contribution in this paper, wepropose two novel model architectures designedto investigate this claim.
The first is simpler thanthe CRF.
It uses token-level classifiers to predictquotation boundaries and combines the boundariesgreedily to predict spans.
The second model ismore expressive.
It is a semi-Markov sequencemodel which relaxes the Markov assumption, en-abling it to consider global features of quotationspans.
In our second main contribution, an analysisof the models?
performances, we find that the sim-1736pler model is competitive with the state-of-the-artCRF.
The semi-Markov model outperforms both ofthem significantly by 3 % F1.
This demonstratesthat the relaxed Markov assumptions help improveperformance.
Our final contribution is to makeimplementations of all models publicly available.22 The Task: Quotation DetectionProblem Definition Following the terminologyestablished by Pareti et al (2013), we deal with thedetection of content spans, the parts of the text thatare being quoted.
To locate such spans, it is helpfulto first detect cues which often mark the begin-ning or end of a quotation.
The following exampleshows an annotated sentence from the PARC cor-pus; each content span (CONT) is associated withexactly one cue span (CUE):Mr. Kaye [denies]CUE[the suit?scharges]CONTand [says]CUE[his onlymistake was taking on Sony in themarketplace]CONT.3Pareti et al (2013) distinguish three types of quo-tations.
Direct quotations are fully enclosed inquotation marks and are a verbatim reproductionof the original utterance.
Indirect quotations para-phrase the original utterance and have no quotationmarks.
Mixed quotations contain both verbatimand paraphrase content and may thus contain quo-tation marks.
Note that the type of a content spanis assigned automatically based on its surface formusing the definitions just given.Quotation Detection as Sequence Modeling Inthis paper, we compare our new model architec-tures to the state-of-the-art approach by Pareti(2015), an extension of Pareti et al (2013).
Theirsystem is a pipeline: Its first component is the cuemodel, a token-level k-NN classifier applied to thesyntactic heads of all verb groups.
After cues aredetected, content spans are localized using the con-tent model, a linear-chain conditional random field(CRF) which makes use of the location of cues inthe document through features.As their system is not publicly available, we re-implement it.
Our cue classifier is an averagedperceptron (Collins, 2002) which we describe inmore detail in the following section.
It uses the2http://www.ims.uni-stuttgart.de/data/qsample3PARC, wsj 2418C1.
Surface form, lemma, and PoS tag for all tokens within awindow of ?5.C2.
Bigrams of surface form, lemma, and PoS tagC3.
Shape of tiC4.
Is any token in a window of ?5 a named entity?C5.
Does a quotation mark open or close at ti(determined bycounting)?
Is tiwithin quotation marks?C6.
Is tiin the list of reporting verbs, noun cue verbs, titles,WordNet persons or organizations, and its VerbNet classC7.
Do a sentence, paragraph, or the document begin or endat ti, ti?1, or ti+1?C8.
Distance to sentence begin and end; sentence lengthC9.
Does the sentence contain tia pronoun/named en-tity/quotation mark?C10.
Does a syntactic constituent starts or ends at ti?C11.
Level of tiin the constituent treeC12.
Label and level of the highest constituent in the treestarting at ti; label of ti?s the parent nodeC13.
Dependency relation with parent or any child of ti(withand without parent surface form)C14.
Any conjunction of C5, C9, C10Table 1: Cue detection features for a token tiatposition i, mostly derived from Pareti (2015)S1.
Is a direct or indirect dependency parent of ticlassifiedas a cue, in the cue list, or the phrase ?according to??S2.
Was any token in a window of ?5 classified as a cue?S3.
Distance to the previous and next cueS4.
Does the sentence containing tihave a cue?S5.
Conjunction of S4 and all features from C14Table 2: Additional features for content span detec-tion, mostly derived from Pareti (2015)features in Table 1.4Our content model is a CRFwith BIOE labels.
It uses all features from Table 1plus features that build on the output of the cueclassifier, shown in Table 2.3 New Model ArchitecturesWhile Pareti (2015) apply sequence modeling forquotation detection, they do not provide an analysiswhat the model learns.
In this paper, we followthe intuition that a linear-chain CRF mostly makeslocal decisions about spans, while ignoring theirglobal structure, such as joint information aboutthe context of the begin and end points.
If this istrue, then (a) a model might work as well as theCRF without learning from label sequences, and (b)a model which makes joint decisions with globalinformation might improve over the CRF.This motivates our two new model architecturesfor the task.
We illustrate the way the differentarchitectures make use of information in Figure 1.Our simpler model (GREEDY) makes strictly lo-cal classification decisions, completely ignoring4For replicability, we give more detailed definitions of thefeatures in the supplementary notes.1737tiCRF:GREEDY:SEMIMARKOV:ti+1 ti+2 ti+3 ti+4 ti+5 ti+6ti ti+1 ti+2 ti+3 ti+4 ti+5 ti+6ti ti+1 ti+2 ti+3 ti+4 ti+5 ti+6Figure 1: Information usage by model architecture.Frames indicate joint decisions on token labels.cueclassifierbeginclassifierendclassifiergreedycombination(GREEDY)semi-Markovmodel(SEMIMARKOV)cuedetectionboundarydetectionspandetectionlinear-chainconditionalrandom field(Pareti et al2013, Pareti2015)Figure 2: Information flow in all three modelsthose around it.
The CRF is able to coordinatedecisions within a window, which is propagatedthrough Viterbi decoding.
The more powerfulmodel (SEMIMARKOV) takes the full span intoaccount and makes a joint decision about the beginand end points.Our intuition about the shortcomings of the CRFis based on an empirical analysis.
However, to sim-plify the presentation, we postpone the presentationof this analysis to Section 6 where we can discussand compare the results of all three models.3.1 Model Decomposition and FormalizationWe first introduce a common formalization for ourmodel descriptions.
Our problem of interest is con-tent span detection, the task of predicting a set S ofcontent spans (tb, te) delimited by their begin andend tokens.
The CRF solves this task by classifyingtokens as begin/end/inside/outside tokens and thussolves a proxy problem.
The problem is difficultbecause corresponding begin and end points needto be matched up over long distances, a challengefor probabilistic finite state automata such as CRFs.In our model, cue detection, the task of detect-ing cue tokens tc(cf.
Section 2), remains the firststep.
However, we then decompose the contentspan problem solved by the CRF by introducingthe intermediary task of boundary detection.
Asillustrated in Figure 2, this means identifying thesets of all begin and end tokens, tband te, ignoringtheir interdependencies.
We then recombine theseAlgorithm 1 GREEDY content span algorithmInput: List of documents D; feature functions fxfor cue,begin, and end (x ?
c, b, e); distance parameter dmax;length parameter `maxOutput: Content span labeling S1: ?c,?b,?e?
TRAINCLASSIFIERS(D,fc,fb,fe)2: for d in D do3: S ?
?4: for token t in d do5: if ?c?fc(t) > 0 then6: tb?
next token right of t .
next beginwhere ?b?fb(t) > 07: te?
next token right of tb.
next endwhere ?e?fe(t) > 08: if |tb?
tc| ?
dmaxand |te?
tb| ?
`maxand OVERLAPPING(tb, te) = ?then9: S ?
S ?
{(tb, te)} .
add spanpredictions with two different strategies, as detailedin Section 3.2 and Section 3.3.
This decompositionhas two advantages: (a), we expect that boundarydetection is easier than content span detection, aswe remove the combinatorial complexity of match-ing begin and end tokens; (b), begin, end, and cuedetection are now three identical classification tasksthat can be solved by the same machinery.We model each of the three tasks (cue/begin/enddetection) with a linear classifier of the formscorex(t) = ?x?fx(t) (1)for a token t, a class x ?
{c, b, e} (for cue, begin,and end), a feature extraction function fx(t), and aweight vector ?x.
We re-use the feature templatesfrom Section 2 to remain comparable to the CRF.We estimate all parameters ?xwith the per-ceptron algorithm, and use parameter averaging(Collins, 2002).
Since class imbalances, whichoccur in the boundary detection tasks, can havestrong effects (Barandela et al, 2003), we train theperceptron with uneven margins (Li et al, 2002).This variant introduces two learning margins: ?
?1for the negative class and ?+1for the positive class.Increasing ?+1at a constant ?
?1increases recall(as failure to predict this class is punished more),potentially at the loss of precision, and vice versa.3.2 Greedy Span DetectionOur first new model, GREEDY (Figure 2, bottomcenter), builds on the assumption that the model-ing of sequence properties in a linear-chain CRFis weak enough that sequence learning can be re-placed by a greedy procedure.
Algorithm 1 showshow we generate a span labeling based on the out-put of the boundary classifiers.
Starting at each cue,1738we add all spans within a given distance dmaxfromthe cue whose length is below a given maximum`max.
If the candidate span is OVERLAPPING withany existing spans, we discard it.
Analogously, wesearch for spans to the left of the cue.
The algo-rithm is motivated by the structure of attributionrelations: each content span has one associated cue.3.3 Semi-Markov Span DetectionOur second model extends the CRF into asemi-Markov architecture which is able to han-dle global features of quotation span candidates(SEMIMARKOV, Figure 2 bottom right).
Follow-ing previous work (Sarawagi and Cohen, 2004), werelax the Markov assumption inside spans.
This al-lows for extracting arbitrary features on each span,such as conjunctions of features on the begin andend tokens or occurrence counts within the span.Unfortunately, the more powerful model archi-tecture comes at the cost of a more difficult predic-tion problem.
Sarawagi and Cohen (2004) proposea variant of the Viterbi algorithm.
This howeverdoes not scale to our application, since the maxi-mum length of a span factors into the predictionruntime, and quotations can be arbitrarily long.
Asan alternative, we propose a sampling-based ap-proach: we draw candidate spans (proposals) froman informed, non-uniform distribution of spans.
Wescore these spans to decide whether they should beadded to the document (accepted) or not (rejected).This way, we efficiently traverse the space of po-tential span assignments while still being able tomake informed decisions (cf.
Wick et al (2011)).To obtain a distribution over spans, we adapt theapproach by Zhang et al (2015).
We introducetwo independent probability distributions: Pbisthe distribution of probabilities of a token being abegin token; Peis the distribution of probabilitiesof a token being an end token.
We sample a singlecontent span proposal (DRAWPROPOSAL) by firstsampling the order in which the boundaries are tobe determined (begin token or end token first) bysampling a binary variable d ?
Bernoulli(0.5).
Ifthe begin token is to be sampled first, we continueby drawing a begin token tb?
Pband finally drawan end token te?
Pewithin a window of up to`maxtokens to the right of tb.
If the end token is tobe sampled first, we proceed conversely.
We alsopropose empty spans, i.e., the removal of existingspans without an replacement.For the distributions Pband Pe, we reuse ourAlgorithm 2 SEMIMARKOV inference algorithmInput: Document d; probability distributions for begin andend (Pb, Pe); feature function for spans g; maximum spanlength `max; number of proposals NOutput: Set of content spans S1: S ?
?2: ?
?
03: for n = 1 to N do4: (tb, te)?
DRAWPROPOSAL(Pb, Pe)5: score?
?
?
g(tb, te)6: O ?
OVERLAPPING(tb, te)7: scoreO??(t?b,t?e)?O?
?
g(t?b, t?e)8: if score > scoreOthen9: S ?
S \O .
remove overlapping10: S ?
S ?
{(tb, te)} .
accept proposal11: if ISTRAINING and ?CORRECT(tb, te) then12: PERCEPTRONUPDATE .
wrongly accepted13: else14: REJECT(tb, te)15: if ISTRAINING and CORRECT(tb, te) then16: PERCEPTRONUPDATE .
wrongly rejectedboundary detection models from Section 3.1.
Foreach class x ?
{b, e} we form a distributionPx(t) ?
exp(scorex(t)/Tx) (2)over the tokens t of a document using the scoresfrom Equation 1.
Txis a temperature hyperparam-eter.
Temperature controls the pronouncedness ofpeaks in the distribution.
Higher temperature flat-tens the distribution and encourages the selectionof tokens with lower scores.
This is useful whenexploration of the sample space is desired.The proposed candidates enter into the decisionalgorithm shown in Algorithm 2.
As shown, thecandidates are scored using a linear model (againas defined in Equation 1).
We use the features ofthe previous models (Table 1 and 2) on the beginand end tokens.
As we now judge complete spanassignments rather than local label assignmentsto tokens, we can add a new span-global featurefunction g(tb, te).
We introduce the features shownin Table 3.
If the candidate?s score is higher thanthe sum of scores of all spans overlapping with it,we accept it and remove all overlapping ones.This model architecture can be seen as a mod-ification of the pipeline of the GREEDY model(cf.
Figure 2).
We again detect cues and boundaries,but then make an informed decision for combiningbegin and end candidates.
In addition, the samplermakes ?soft?
selections of begin and end tokensbased on the model scores rather than simply ac-cepting the classifier decisions.For training, we again use perceptron updates(cf.
Section 3.2).
If the model accepts a wrong1739SettingDirect Indirect Mixed OverallP R F P R F P R F P R FstrictPareti (2015) as reported therein 94 88 91 78 56 65 67 60 63 80 63 71CRF (own re-implementation) 94 93 94 73 58 64 81 68 74 79g67 72GREEDY 92 91 91 69 59 64 72 64 68 75 67 71SEMIMARKOV 93 94 94 73 65 69 81 66 73 79g71cg75cgCombination: CRF+SEMIMARKOV 94 93 94 73 64 69 81 68 74 79g71cg75cgpartialPareti (2015) as reported therein 99 93 96 91 66 77 91 81 86 93 73 82CRF (own re-implementation) 98 96 97 87 70 77 94 83 88 90g77 83GREEDY 97 95 96 83 76 79 93 85 89 88 81c84SEMIMARKOV 97 95 96 83 75 79 92 81 86 88 80 84Combination: CRF+SEMIMARKOV 98 96 97 83 75 79 94 83 88 88 81c84cTable 4: Results on the test set of PARC3.
Best overall strict results in bold.
Models as in Figure 2.g: significantly better than GREEDY; c: significantly better than CRF (both with ?
= 0.05).G1.
Numbers of named entities, lowercased tokens, commas,and pronouns inside the spanG2.
Binned percentage of tokens that depend on a cueG3.
Location of the closest cue (left/right?
), percentage ofdependents on that cueG4.
Number of cues overlapped by the spanG5.
Is there a cue before the first token and/or after the lasttoken of the span (within the same sentence)?
first orafter the last token of the span?, and their conjunctionG6.
Do both the first and the last token depend on a cue?G7.
Binned length of the spanG8.
Does the span match a sentence exactly/off by one token?G9.
Number of sentences covered by the spanG10.
Does the span match one or more constituents exactly?G11.
Is the span direct, indirect, or mixed?G12.
Is the # of quotation marks in the span odd or even?G13.
Is the span is direct and does it contain more than twoquotation marks?Table 3: Global features for content span detectionspan, we perform a negative update (Line 12 inAlgorithm 2).
If a correct span is rejected, wemake a positive update (Line 16).
We iterate overthe documents in random order for a fixed numberE of epochs.
As the sampling procedure takes longto fully label documents, we employ GREEDY tomake initial assignments.
This does not constituteadditional supervision, as the sampler can removeany initial span and thus refute the initialization.This reduces runtime without affecting the result inpractice.4 Experimental SetupData We use the Penn Attribution Relations Cor-pus, version 3 (henceforth PARC3), by Pareti(2015).5It contains AR annotations on the WallStreet Journal part of the Penn Treebank (2,2945Note that the data and thus the results differ from thosepreviously published in (Pareti et al, 2013).news documents).
As in related work, we use sec-tions 1?22 as training set, section 23 as test set,and section 24 as development set.
We performthe same preprocessing as Pareti: We use goldtokenization, lemmatization, part-of-speech tags,constituency parses, gold named entity annotations(Weischedel and Brunstein, 2005), and Stanfordparser dependency analyses (Manning et al, 2014).Evaluation We report precision, recall, andmicro-averaged F1, adopting the two metrics in-troduced by Pareti et al (2013): Strict match con-siders cases as correct where the boundaries of thespans match exactly.
Partial match measures cor-rectness as the ratio of overlap of the predictedand true spans.
In both cases, we report numbersfor each of the three quotation types (direct, indi-rect, mixed) and their micro averages.
Like Pareti(2015), we exclude single-token content spans fromthe evaluation.
To test for statistical significanceof differences, we use the approximate randomiza-tion test (Noreen, 1989) at a significance level of?
= 0.05.Implementation and Hyperparameters Weuse the CRF implementation in MALLET (Mc-Callum, 2002).
We optimize all hyperparametersof the models on the development set.
Our bestmodels use positive margins of ?+= 25 for theboundary and ?+= 15 for the span models, fa-voring recall.
The SEMIMARKOV sampler uses atemperature of Tx= 10 for all classes.
We per-form 15 epochs of training after which the modelshave converged, and draw 1,000 samples for eachdocument.
For the GREEDY model, we obtain thebest results with dmax= 30 and `max= 55.
Forthe SEMIMARKOV sampler, `max= 75 is optimal.1740The high values mirror the presence of very longspans in the data.5 ResultsCue We first evaluate the cue classifier.
We ob-tain an F1of 86 %, with both precision and recall at86 %, which is very close to the 85 % F1of Pareti.CRF Table 4 summarizes the content span re-sults.
First, we compare Pareti?s results to our reim-plementation (the rows denoted with Pareti (2015)and CRF).
There are some differences in how wellthe model performs on certain types of spans: whileour precision is lower for indirect spans, it is higheron mixed spans.
Additionally, our implementa-tion generally has higher recall than Pareti?s.
Hersystem includes several features using proprietarylists (such as a manually curated list of titles) wewere unable to obtain, and complex feature tem-plates that we may interpret differently.
We suspectthat these differences are due to the typical replica-tion problems in NLP (cf.
Fokkens et al (2013)).Overall, however, our model performs quite simi-larly to Pareti?s, with our model scoring an overallF1of 72 % (vs. Pareti?s 71 %) and a partial F1of83 % (vs. 82 %).GREEDY Next, we compare the GREEDY modelto the CRF.
We find its overall performance to becomparable to the CRF, confirming our expecta-tions.
While strict precision is statistically signif-icantly lower for GREEDY (75 % vs. 79 %), strictrecall is not significantly different (bot at 67 %).Considering partial matches, GREEDY has signif-icantly higher recall (81 % vs. 77 %) but signifi-cantly lower precision (88 % vs. 90 %) than theCRF, with an overall comparable F1.
This resultbolsters our hypothesis that the CRF learn only asmall amount of useful sequence information.
Al-though GREEDY ignores label sequences in train-ing completely, it is able to compete with the CRF.Furthermore, the partial match result that GREEDYis a particularly good choice if the main interestis the approximate location of content spans in adocument: The simpler model architecture makesit easier and more efficient to train and apply.
Thecaveat is that GREEDY is particularly bad at locat-ing mixed spans (as indicated by a precision of only72 %): Quotation marks are generally good indica-tors for span boundaries and are often returned asfalse positives by the boundary detection models,so GREEDY tends to incorrectly pick them.SEMIMARKOV Overall, the SEMIMARKOVmodel outperforms the CRF significantly in termsof strict recall (71 % vs. 67 %) and F1(75 %vs.
72 %), while precision remains unaffected (at79 %).
The model performs particularly well onindirect quotations (increasing F1by 5 points to69 %), the most difficult category, where local con-text is insufficient.
Meanwhile, on partial match,the SEMIMARKOV model has a comparable re-call (80 vs. 77 %), but significantly lower precision(88 % vs. 90 %).
The overall partial F1results arenot significantly different.
The improvement onthe strict measures supports our intuition that betterfeatures help in particular in identifying the exactboundaries of quotations, a task that evidently prof-its from global information.Model Combination The complementarystrengths of the CRF and SEMIMARKOV (CRFdetects direct quotations well, SEMIMARKOVindirect quotations) suggest a simple modelcombination algorithm based on the surface formof the spans: First take all direct and mixed spanspredicted by the CRF; then add all indirect spansfrom the SEMIMARKOV model (except for thosewhich would overlap).
This result is our overallbest model under strict evaluation, although it isnot significantly better than the SEMIMARKOVmodel.
Considering partial match, its results areessentially identical to the SEMIMARKOV model.6 AnalysisWe now proceed to a more detailed analysis of theperformance of the three models (CRF, GREEDY,and SEMIMARKOV) and their differences in orderto gain insights into the nature of the quotationdetection task.
In the interest of readability, weorganize this section by major findings instead ofthe actual analyses that we have performed, andadduce for each finding all relevant analysis results.Finding 1: Variation in length does not explainthe differences in model performance.
A pos-sible intuition about our models it that the improve-ment of SEMIMARKOV over CRF is due to a betterhandling of longer quotations.
However, this is notthe case.
Figure 3 shows the recall of the threemodels for quotations binned by lengths.
The mainpatterns hold across all three models: Medium-length spans are the easiest to detect.
Short spansare difficult to detect as they are often part of dis-continuous content spans.
Long spans are also1741[0,10)[10,20)[20,30)[30,40)[40,50)[50,100)Span lengthRecall0.00.20.40.60.8[0,10)[10,20)[20,30)[30,40)[40,50)[50,100)Span lengthRecall0.00.20.40.60.8[0,10)[10,20)[20,30)[30,40)[40,50)[50,100)Span lengthRecall0.00.20.40.60.8Figure 3: Strict recall by span length for CRF (left), GREEDY (center), and SEMIMARKOV model (right)CategoryCountB I Elooking left 27 14 7looking right 1 13 30cue 11 10 7other lexical 31 21 22structural/syntactic 27 44 35punctuation 31 25 36Table 5: Categories of top positive and negativeCRF features for begin (B), inside (I), and end (E)difficult since any wrong intermediary decision canfalsify the prediction.
In fact, the CRF model iseven the best model among the three for very longspans (which are rare).
Those spans exceed the55 and 75 token limits `maxof the GREEDY andSEMIMARKOV models.
Intuitively, for the CRF,most spans are long: even spans which are shortin comparison to other quotations are longer thanthe window within which the CRF operates.
Thisis why span length does not have an influence.Finding 2: Quotations are mostly defined bytheir immediate external context.
A featureanalysis of the CRF model reveals that many impor-tant features refer to material outside the quotationitself.
For each label (B, I, E), we collect the 50features with the highest positive and negative val-ues, respectively.
We first identify the subset ofthose features that looks look left or right.
As theupper part of Table 5 shows, a substantial numberof B (begin) features look to the left, and a numberof E (end) features look to the right.
Thus, thesefeatures do not look at the quotation itself, but atits immediate external context.We next divide the features into four broad cate-gories (cues, other lexical information, structuraland syntactic features, and punctuation includingquotation marks).
The results in the lower part ofTable 5 show that the begin and end classes relyon a range of categories, including lexical, cue andpunctuation outside the quotation.
The situation isdifferent for inside tokens (I), where most featuresexpress structural and syntactic properties of thequotation such as the length of a sentence and itssyntactic relation to a cue.
Together, these observa-tions suggest that one crucial piece of informationabout quotations is their lexical and orthographiccontext: the factors that mark a quotation as a quo-tation.
Another crucial piece are internal structuralproperties of the quotation, while lexical proper-ties of the quotation are not very important: whichmakes sense, since almost anything can be quoted.The feature analysis is bolstered by an error anal-ysis of the false negatives in the high-precisionlow-recall CRF.
The first reason for false nega-tives is indeed the occurrence of infrequent cueswhich the cue model fails to identify (e.g., read oracknowledge).
The second one is that the modeldoes attempt to learn syntactic features, but thatthe structural features that can be learned by theCRF (such as C7, C10 or S4) can model only localwindows of the quality of the quotation, but not itsglobal quality.
This leads us to our third finding.Finding 3: Simple models cannot capture de-pendencies between begin and end boundarieswell.
Given the importance of cues, as evidencedby our Finding 2, we can ask whether the boundaryof the quotation that is adjacent to its associatedcue (?cue-near?)
is easier to identify than the otherboundary (?cue-far?)
whose context is less informa-tive.
To assess this question, we evaluate the recallof individual boundary detection at the token level.For the CRF, ?cue-far?
boundaries of spans indeedtend to be more difficult to detect than ?cue-near?ones.
The results in Table 6 show that both theGREEDY and the CRF model show a marked asym-1742GREEDY CRF SEMIMARKOVcue-near 76 74 76cue-far 72 71 75Table 6: Recall on boundaries by cue positionmetry and perform considerably worse (3 % and4 %, respectively) on the cue-far boundary.
Thisasymmetry is considerably weaker for the SEMI-MARKOV model, where both boundary types arerecognized almost on par.
The reason behind thisfinding is that neither the GREEDY model nor theCRF can condition the choice of the cue-far bound-ary on the cue-near boundary or on global proper-ties of the quotation ?
the GREEDY model, becauseits choices are completely independent, and theCRF model, because its choices are largely inde-pendent due to the Markov assumption.Finding 4: The SEMIMARKOV model benefitsthe most from its ability to handle global fea-tures about content spans.
This leads us to ourfinal finding about why the SEMIMARKOV modeloutperforms the CRF ?
whether it is the model ar-chitecture itself, or the new global features that itallows us to formulate.
We perform an ablationstudy whose results are shown in Figure 4.
We be-gin with only the token-level features on the begin,end, and interior tokens of the span, as introducedin Section 2, i.e., the features that the CRF has atits disposal.
We find that this model performs onpar with the CRF, thus the model architecture onits own does not help.
We then incrementally addthe feature templates containing count statistics ofthe internal tokens (Template G1 in Table 3) andadvanced cue information (G2?G6).
Both give themodel incremental boosts.
Adding syntactic coher-ence features (G7?G13) completes our full featureset and yields the best results.Thus, the difference comes from features thatdescribe global properties of the quotation.
Oneof the most informative (negative) features is theconjunction from G6.
It enforces the constraintthat each content span is associated with a singlecue.
As in the CRF, the actual content of a contentspan does not play a large role.
The only semanticfeatures the model considers concern the presenceof named entities within the span.These observations are completed by analysisof the quotation spans that were correctly detectedby the SEMIMARKOV model, but not the CRF (intoken +internal +cue +structuralFeature setF0.600.650.700.75 **Figure 4: Strict F1for different feature sets in theSEMIMARKOV model.
*: Difference statisticallysignificant.
Dashed line: CRF result.terms of strict recall).
We find a large amount ofspans with highly ambiguous cue-near tokens suchas to (10 % of the cases) that (16 %).
We find thatoften the errors are also related to the frequency orlocation of cues.
As an example, in the sentence[...] he has said [that when he was on thewinning side in the 1960s, he knew thatthe tables might turn in the future]CONT.6the CRF model incorrectly splits the content span atthe second cue candidate knew.
This is, however, anembedded quotation that the model should ignore.In contrast, the SEMIMARKOV model makes useof the fact the tokens of the span depend on thesame cue, and predicts the span correctly.
For thesetokens, the distinction between reported speech andfactual descriptions is difficult.
Arguably, it is theglobal features that help the model make its call.7 Related WorkQuotation detection has been tackled with a numberof different strategies.
Pouliquen et al (2007) use asmall set of rules which has high precision but lowrecall on multilingual text.
Krestel et al (2008) alsopursue a rule-based approach, focusing on the rolesof cue verbs and syntactic markers.
They evaluateon a small set of annotated WSJ documents andagain report high precision but low recall.
Paretiet al (2013) develop the state-of-the-art sequencelabeling approach discussed in this paper.Our sampling approach builds on that of Zhanget al (2015), who pursue a similar strategy for pars-ing, PoS tagging, and sentence segmentation.
Simi-lar semi-Markov model approaches have been usedfor other applications, e.g.
by Yang and Cardie6PARC, wsj 23471743(2012) and Klinger and Cimiano (2013) for sen-timent analysis.
They also predict spans by sam-pling, but they draw proposals based on the tokenor syntactic level.
This is not suitable for quotationdetection as we deal with much longer spans.8 ConclusionWe have considered the task of quotation detection,starting from the hypothesis that linear-chain CRFscannot take advantage of all available sequence in-formation due to its Markov assumptions.
Indeed,our analyses find that the features most importantto recognize a quotation consider its direct con-text of orthographic evidence (such as quotationmarks) and lexical evidence (such as cue words).A simple, greedy algorithm using non-sequentialmodels of quotation boundaries rivals the CRF?sperformance.
For further improvements, we in-troduce a semi-Markov model capable of takinginto account global information about the completespan not available to a linear-chain CRF, such asthe presence of cues on both sides of the quotationcandidate.
This leads to a significant improvementof 3 points F1over the state of the art.On a more general level, we believe that quota-tion detection is interesting as a representative oftasks involving long sequences, where Markov as-sumptions become inappropriate.
Other examplesof such tasks include the identification of chemicalcompound names (Krallinger et al, 2015) and thedetection of annotator rationales (Zaidan and Eis-ner, 2008).
We have shown that a more expressivesemi-Markov model which avoids these assump-tions can improve performance.
More expressivemodels however come with harder inference prob-lems which are compounded when applied to long-sequence tasks.
The informed sampling algorithmwe have described performs such efficient inferencefor our semi-Markov quotation detection model.AcknowledgmentsThis work was funded in part by the DFG throughthe Sonderforschungsbereich 732.
We thank SilviaPareti for kindly providing the PARC dataset aswell as for much information helpful for replicatingher results.
Further thanks go to Anders Bj?orkelundand Kyle Richardson for discussion and comments.ReferencesApoorv Agarwal, Augusto Corvalan, Jacob Jensen, andOwen Rambow.
2012.
Social network analysis ofalice in wonderland.
In Proceedings of the NAACL-HLT 2012 Workshop on Computational Linguisticsfor Literature, pages 88?96, Montr?eal, Canada, June.Association for Computational Linguistics.Ricardo Barandela, Jos?e Salvador S?anchez, VicenteGarc?
?a, and Edgar Rangel.
2003.
Strategies forlearning in class imbalance problems.
PatternRecognition, 36(3):849?851.Michael Collins.
2002.
Discriminative training meth-ods for Hidden Markov Models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 1?8, Philadelphia, PA.David Elson, Nicholas Dames, and Kathleen McKe-own.
2010.
Extracting social networks from liter-ary fiction.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 138?147, Uppsala, Sweden.Manaal Faruqui and Sebastian Pado.
2012.
Towardsa model of formal and informal address in English.In Proceedings of the 13th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 623?633, Avignon, France.Antske Fokkens, Marieke van Erp, Marten Postma,Ted Pedersen, Piek Vossen, and Nuno Freire.
2013.Offspring from reproduction problems: What repli-cation failure teaches us.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, pages 1691?1701, Sofia, Bul-garia.Roman Klinger and Philipp Cimiano.
2013.
Bi-directional inter-dependencies of subjective expres-sions and targets and their value for a joint model.In Proceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics, pages 848?854, Sofia, Bulgaria.Martin Krallinger, Florian Leitner, Obdulia Rabal,Miguel Vazquez, Julen Oyarzabal, and Alfonso Va-lencia.
2015.
CHEMDNER: The drugs and chemi-cal names extraction challenge.
Journal of Chemin-formatics, 7(Suppl 1):S1.Ralf Krestel, Sabine Bergler, and Ren?e Witte.
2008.Minding the source: Automatic tagging of reportedspeech in newspaper articles.
In Proceedings of theInternational Conference on Language Resourcesand Evaluation, pages 2823?2828, Marrakech, Mo-rocco.Yaoyong Li, Hugo Zaragoza, Ralf Herbrich, JohnShawe-Taylor, and Jaz S. Kandola.
2002.
Theperceptron algorithm with uneven margins.
In Pro-ceedings of the Nineteenth International Conferenceon Machine Learning, pages 379?386, Sydney, Aus-tralia.1744Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP NaturalLanguage Processing Toolkit.
In Proceedings ofACL System Demonstrations, pages 55?60, Balti-more, MD.Andrew K. McCallum, 2002.
MALLET: A MachineLearning for Language Toolkit.
User?s manual.Eric T. Nalisnick and Henry S. Baird.
2013.
Character-to-character sentiment analysis in shakespeare?splays.
In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 479?483, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.Eric W. Noreen.
1989.
Computer intensive methodsfor hypothesis testing: An introduction.
Wiley, NewYork.Silvia Pareti, Tim O?Keefe, Ioannis Konstas, James R.Curran, and Irena Koprinska.
2013.
Automaticallydetecting and attributing indirect quotations.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 989?999, Seattle, WA.Silvia Pareti.
2015.
Attribution: A Computational Ap-proach.
Ph.D. thesis, University of Edinburgh.Bruno Pouliquen, Ralf Steinberger, and Clive Best.2007.
Automatic detection of quotations in multi-lingual news.
In Proceedings of Recent Advancesin Natural Language Processing, pages 487?492,Borovets, Bulgaria.Sunita Sarawagi and William W. Cohen.
2004.
Semi-markov conditional random fields for informationextraction.
In Proceedings of Advances in NeuralInformation Processing Systems, pages 1185?1192,Vancouver, BC.Ralph Weischedel and Ada Brunstein.
2005.
BBN pro-noun coreference and entity type corpus.
LinguisticData Consortium, Philadelphia.Michael Wick, Khashayar Rohanimanesh, Kedar Bel-lare, Aron Culotta, and Andrew McCallum.
2011.Samplerank: Training factor graphs with atomic gra-dients.
In Proceedings of the 28th InternationalConference on Machine Learning, pages 777?784,Bellevue, WA.Bishan Yang and Claire Cardie.
2012.
Extracting opin-ion expressions with semi-Markov conditional ran-dom fields.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 1335?1345, Jeju Island, South Ko-rea.Omar Zaidan and Jason Eisner.
2008.
Modeling an-notators: A generative approach to learning from an-notator rationales.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 31?40, Honolulu, HI.Yuan Zhang, Chengtao Li, Regina Barzilay, and Ka-reem Darwish.
2015.
Randomized greedy inferencefor joint segmentation, POS tagging and dependencyparsing.
In Proceedings of the 2015 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 42?52, Denver, CO.1745
