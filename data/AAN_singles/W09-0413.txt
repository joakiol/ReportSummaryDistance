Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 80?84,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsThe Universita?t Karlsruhe Translation System for the EACL-WMT 2009Jan Niehues, Teresa Herrmann, Muntsin Kolss and Alex WaibelUniversita?t Karlsruhe (TH)Karlsruhe, Germany{jniehues,therrman,kolss,waibel}@ira.uka.deAbstractIn this paper we describe the statisticalmachine translation system of the Univer-sita?t Karlsruhe developed for the transla-tion task of the Fourth Workshop on Sta-tistical Machine Translation.
The state-of-the-art phrase-based SMT system is aug-mented with alternative word reorderingand alignment mechanisms as well as op-tional phrase table modifications.
We par-ticipate in the constrained condition ofGerman-English and English-German aswell as in the constrained condition ofFrench-English and English-French.1 IntroductionThis paper describes the statistical MT systemused for our participation in the WMT?09 SharedTranslation Task and the particular language-pair-dependent variations of the system.
We use stan-dard alignment and training tools and a phrase-based SMT decoder for creating state-of-the-artMT systems for our contribution in the transla-tion directions English-German, German-English,English-French and French-English.Depending on the language pair, the baselinesystem is augmented with part-of-speech (POS)-based short-range and long-range word reorderingmodels, discriminative word alignment (DWA)and several modifications of the phrase table.
Ex-periments with different system variants were con-ducted including some of those additional systemcomponents.
Significantly better translation re-sults could be achieved compared to the baselineresults.An overview of the system will follow in Sec-tion 2, which describes the baseline architecture,followed by descriptions of the additional systemcomponents.
Translation results for the differentlanguages and system variants are presented inSection 5.2 Baseline SystemThe core of our system is the STTK decoder (Vo-gel, 2003), a phrase-based SMT decoder with alocal reordering window of 2 words.
The de-coder generates a translation for the input textor word lattice by searching translation modeland language model for the hypothesis that max-imizes phrase translation probabilities and targetlanguage probabilities.
The translation model, i.e.the SMT phrase table is created during the trainingphase by a modified version of the Moses Toolkit(Koehn et al, 2007) applying GIZA++ for wordalignment.
Language models are built using theSRILM Toolkit.
The POS-tags for the reorder-ing models were generated with the TreeTagger(Schmid, 1994) for all languages.2.1 Training, Development and Test DataWe submitted translations for the English-German, German-English, English-French andFrench-English tasks.
All systems were trainedon the Europarl and News Commentary corporausing the Moses Toolkit and apply 4-gram lan-guage models created from the respective mono-lingual News corpora.
All feature weights are au-tomatically determined and optimized with respectto BLEU via MERT (Venugopal et al, 2005).For development and testing we used data pro-vided by the WMT?09, news-dev2009a and news-dev2009b, consisting of 1026 sentences each.3 Word Reordering ModelOne part of our system that differs from the base-line system is the reordering model.
To accountfor the different word orders in the languages, weused the POS-based reordering model presented inRottmann and Vogel (2007).
This model learnsrules from a parallel text to reorder the source side.The aim is to generate a reordered source side thatcan be translated in a more monotone way.80In this framework, first, reordering rules areextracted from an aligned parallel corpus andPOS information is added to the source side.These rules are of the form VVIMP VMFIN PPER?
PPER VMFIN VVIMP and describe how thesource side has to be reordered to match the tar-get side.
Then the rules are scored according totheir relative frequencies.In a preprocessing step to the actual decodingdifferent reorderings of the source sentences areencoded in a word lattice.
Therefore, for all re-ordering rules that can be applied to a sentence theresulting reorderings are added to the lattice if thescore is better than a given threshold.
The decod-ing is then performed on the resulting word lattice.This approach does model the reordering wellif only short-range reorderings occur.
But espe-cially when translating from and to German, thereare also long-range reorderings that require theverb to be shifted nearly across the whole sen-tence.
During this shift of the verb, the rest ofthe sentence remains mainly unchanged.
It doesnot matter which words are in between, since theyare moved as a whole.
Furthermore, rules in-cluding an explicit sequence of POS-tags spanningthe whole sentence would be too specific.
A lotmore rules would be needed to cover long-rangereorderings with each rule being applicable onlyvery sparsely.
Therefore, we model long-range re-ordering by generalizing over the unaffected se-quences and introduce rules with gaps.
(For moredetails see Niehues and Kolss (2009)).
These arelearned in a way similar to the other type of re-ordering rules described above, but contain a gaprepresenting one or several arbitrary words.
It is,for example, possible to have the following ruleVAFIN * VVPP ?
VAFIN VVPP *, which putsboth parts of the German verb next to each other.4 Translation ModelThe translation models of all systems we submit-ted differ in some parts from the baseline system.The main changes done will be described in thissection.4.1 Word AlignmentThe baseline method for creating the word align-ment is to create the GIZA++ alignments in bothdirections and then to combine both alignmentsusing a heuristic, e.g.
grow-diag-final-and heuris-tic, as provided by the Moses Toolkit.
In someof the submitted systems we used a discrimina-tive word alignment model (DWA) to generatethe alignments as described in Niehues and Vogel(2008) instead.
This model is trained on a smallamount of hand-aligned data and uses the lexicalprobability as well as the fertilities generated bythe GIZA++ Toolkit and POS information.
Weused all local features, the GIZA and indicator fer-tility features as well as first order features for 6directions.
The model was trained in three steps,first using the maximum likelihood optimizationand afterwards it was optimized towards the align-ment error rate.
For more details see Niehues andVogel (2008).4.2 Phrase Table SmoothingThe relative frequencies of the phrase pairs are avery important feature of the translation model,but they often overestimate rare phrase pairs.Therefore, the raw relative frequency estimatesfound in the phrase translation tables are smoothedby applying modified Kneser-Ney discounting asdescribed in Foster et al (2006).4.3 Lattice Phrase ExtractionFor the test sentences the POS-based reorderingallows us to change the word order in the sourcesentence, so that the sentence can be translatedmore easily.
But this approach does not reorderthe training sentences.
This may cause problemsfor phrase extraction, especially for long-range re-orderings.
For example, if the English verb isaligned to both parts of the German verb, thisphrase can not be extracted, since it is not contin-uous on the German side.
In the case of Germanas source language, the phrase could be extractedif we also reorder the training corpus.Therefore, we build lattices that encode thedifferent reorderings for every training sentence.Then we can not only extract phrase pairs from themonotone source path, but also from the reorderedpaths.
So it would be possible to extract the ex-ample mentioned before, if both parts of the verbwere put together by a reordering rule.
To limitthe number of extracted phrase pairs, we extracta source phrase only once per sentence even if itmay be found on different paths.
Furthermore, wedo not use the weights in the lattice.If we use the same rules as for the test sets,the lattice would be so big that the number of ex-tracted phrase pairs would be still too high.
Asmentioned before, the word reordering is mainly81a problem at the phrase extraction stage if oneword is aligned to two words which are far awayfrom each other in the sentence.
Therefore, theshort-range reordering rules do not help much inthis case.
So, only the long-range reordering ruleswere used to generate the lattice for the trainingcorpus.
This already leads to an increase of thenumber of source phrases in the filtered phrase ta-ble from 724K to 971K.
The number of phrasepairs grows from 5.1M to 6.7M.4.4 Phrase Table AdaptionFor most of the different tasks there was a hugeamount of parallel out-of-domain training dataavailable, but only a much smaller amount of in-domain training data.
Therefore, we tried to adaptour system to the in-domain data.
We want tomake use of the big out-of-domain data, but donot want to lose the information encoded in the in-domain data.To achieve this, we built an additional phrasetable trained only on the in-domain data.
Sincethe word alignment does not depend heavily on thedomain we used the same word alignment.
Thenwe combined both phrase tables in the followingway.
A phrase pair with features ?
from the firstphrase table is added to the combined one withfeatures < ?, 1 >, where 1 is a vector of ones withlength equal to the number of features in the otherphrase table.
The phrase pairs of the other phrasetable were added with the features < 1, ?
>.5 ResultsWe submitted system translations for the English-German, German-English, English-French andFrench-English task.
Their performance is mea-sured applying the BLEU metric.
All BLEUscores are computed on the lower-cased transla-tions.5.1 English-GermanThe system translating from English to Germanwas trained on the data described in Section 2.1.The first system already uses the POS-based re-ordering model for short-range reorderings.
Theresults of the different systems are shown in Ta-ble 1.We could improve the translation quality on thetest set by using the smoothed relative frequen-cies in the phrase table as described before andby adapting the phrase table.
Then we used thediscriminative word alignment to generate a newword alignment.
For the training of the modelwe used 500 hand-aligned sentences from the Eu-roparl corpus.
By training a translation modelbased on this word alignment we could improvethe translation quality further.
At last we addedthe model for long-range reorderings, which per-forms best on the test set.The improvement achieved by smoothing is sig-nificant at a level of 5%, the remaining changes arenot significant on their own.
In all language pairs,the problem occurs that some features do not leadto an improvement on the development set, but onthe test set.
One reason for this may be that thedevelopment set is quite small.Table 1: Translation results for English-German(BLEU Score)System Dev TestShort-range 13.96 14.99+ Smoothing 14.36 15.38+ Adaptation 13.96 15.44+ Discrim.
WA 14.45 15.61+ Long-range reordering 14.58 15.705.2 German-EnglishThe German-English system was trained on thesame data as the English-German except that weperform compound splitting as an additional pre-processing step.
The compound splitting wasdone with the frequency-based method describedin Koehn et al (2003).
For this language di-rection, the initial system already uses phrase ta-ble smoothing, adaptation and discriminative wordalignment, in addition to the techniques of theEnglish-German baseline system.
The results areshown in Table 2.For this language pair, we could improve thetranslation quality, first, by adding the long-rangereordering model.
Further improvements could beachieved by using lattice phrase extraction as de-scribed before.5.3 English-FrenchFor creating the English-French translations, first,the baseline system as described in Section 2was used.
This baseline was then augmentedwith phrase table smoothing, short-range word re-ordering and phrase table adaptation as describedabove.
In addition, the adapted phrase table was82Table 2: Translation results for German-English(BLEU Score)System Dev TestInitial System 20.52 22.01+ Long-range reordering 21.04 22.36+ Lattice phrase extraction 20.69 22.64postprocessed such that phrase table entries in-clude the same amount of punctuation marks, es-pecially quotation marks, in both source and tar-get phrase.
In contrast to the English?Germanlanguage pairs, the word reordering requiredin English?French translations are restricted torather local word shifts which can be covered bythe short-range reordering feature.
Applying addi-tional long-range reordering is scarcely expectedto yield further improvements for these languagepairs and was not applied specifically in this task.Table 3 shows the results of the system variants.Table 3: Translation results for English-French(BLEU Score)System Dev TestBaseline 20.97 20.87+ Smoothing 21.42 21.32+ Short-range reordering 20.79 22.26+ Adaptation 21.05 21.97+ cleanPT 21.50 21.98Both on development and test set, smoothingthe probabilities in the phrase table resulted in anincrease of nearly 0.5 BLEU points.
Applyingshort-range word reordering did not lead to an im-provement on the development set.
However, theincrease in BLEU on the test set is substantial.
Theopposite is the case when adapting the phrase ta-ble: While phrase table adaptation improves thetranslation quality on the development set, adapta-tion leads to lower scores on the test set.Thus, the system configuration that performedbest on the test set applies phrase table smoothingand short-range word reordering.
For creating thetranslations for our submission, this configurationwas used.5.4 French-EnglishFor the French-English task, similar experimentshave been conducted.
With respect to the base-line system, improvements in translation qualitycould be measured when applying phrase tablesmoothing.
An increase of 0.43 BLEU points wasachieved using short-range word reordering.
Ad-ditional experiments with adapting the phrase ta-ble to the domain of the test set led to further im-provement.
Submissions for the shared task werecreated using the system including all mentionedfeatures.Table 4: Translation results for French-English(BLEU Score)System Dev TestBaseline 21.29 22.41+ Smoothing 21.55 22.59+ Short-range reordering 22.55 23.02+ Adaptation 21.72 23.20+ cleanPT 22.60 23.216 ConclusionsWe have presented our system for the WMT?09Shared Translation Task.
The submissions for thelanguage pairs English-German, German-English,English-French and French-English have beencreated by the STTK decoder applying differentadditional methods for each individual languagepair to enhance translation quality.Word reordering models covering short-range reordering for the English?French andEnglish?German and long-range reordering forEnglish?German respectively proved to result inbetter translations.Smoothing the phrase probabilities in the phrasetable also increased the scores in all cases, whileadapting the phrase table to the test domain onlyshowed a positive influence on translation qualityin some of our experiments.
Further tuning of theadaptation procedure could help to clarify the ben-efit of this method.Using discriminative word alignment as analternative to performing word alignment withGIZA++ did also improve the systems translatingbetween English and German.
Future experimentswill be conducted applying discriminative wordalignment also in the English?French systems.AcknowledgmentsThis work was partly supported by Quaero Pro-gramme, funded by OSEO, French State agencyfor innovation.83ReferencesGeorge Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable Smoothing for Statistical Ma-chine Translation.
In Proc.
of Empirical Methods inNatural Language Processing.
Sydney, Australia.Philipp Koehn, Franz Josef Och and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
InHLT/NAACL 2003.
Edmonton, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proc.
of Second ACL Workshop on Statistical Ma-chine Translation.
Prague, Czech Republic.Jan Niehues and Stephan Vogel.
2008.
DiscriminativeWord Alignment via Alignment Matrix Modeling.In Proc.
of Third ACL Workshop on Statistical Ma-chine Translation.
Columbus, OH, USA.Jan Niehues and Muntsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
InProc.
of Forth ACL Workshop on Statistical MachineTranslation.
Athens, Greece.Kay Rottmann and Stephan Vogel.
2007.
Word Re-ordering in Statistical Machine Translation with aPOS-Based Distortion Model.
In TMI.
Sko?vde,Sweden.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In InternationalConference on New Methods in Language Process-ing.
Manchester, UK.Ashish Venugopal, Andreas Zollman and Alex Waibel.2005.
Training and Evaluation Error Minimiza-tion Rules for Statistical Machine Translation.
InProc.
of ACL 2005, Workshop on Data-drive Ma-chine Translation and Beyond (WPT-05).
Ann Ar-bor, MI.Stephan Vogel.
2003.
SMT Decoder Dissected: WordReordering.
In NLP-KE?03.
Beijing, China.84
