Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 19?27,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPMining of Parsed Data to Derive Deverbal Argument StructureOlga Gurevich Scott A. WatermanMicrosoft / Powerset475 Brannan Street, Ste.
330San Francisco, CA 94107{olya.gurevich,scott.waterman}@microsoft.comAbstractThe availability of large parsed corporaand improved computing resources nowmake it possible to extract vast amountsof lexical data.
We describe the pro-cess of extracting structured data and sev-eral methods of deriving argument struc-ture mappings for deverbal nouns that sig-nificantly improves upon non-lexicalizedrule-based methods.
For a typical model,the F-measure of performance improvesfrom a baseline of about 0.72 to 0.81.1 IntroductionThere is a long-standing division in natural lan-guage processing between symbolic, rule-basedapproaches and data-driven, statistical ones.
Rule-based, human-curated approaches are thought tobe more accurate for linguistic constructions ex-plicitly covered by the rules.
However, suchapproaches often have trouble scaling up to awider range of phenomena or different genres oftext.
There have been repeated moves towards hy-bridized approaches, in which rules created withhuman linguistic intuitions are supplemented byautomatically derived corpus data (cf.
(Klavansand Resnik, 1996)).Unstructured corpus data for English can eas-ily be found on the Internet.
Large corpora oftext annotated with part of speech information arealso available (such as the British National Cor-pus).
However, it is much harder to find widelyavailable, large corpora annotated for syntactic orsemantic structure.
The Penn Treebank (Marcuset al, 1993) has until recently been the only suchcorpus, covering 4.5M words in a single genre offinancial reporting.
At the same time, the accuracyand speed of syntactic parsers has been improvinggreatly, so that in recent years it has become possi-ble to automatically create parsed corpora of rea-sonable quality, using much larger amounts of textwith greater genre variation.
For many NLP tasks,having more training data greatly improves thequality of the resulting models (Banko and Brill,2001), even if the training data are not perfect.We have access to the entire English-languagetext of Wikipedia (about 2M pages) that wasparsed using the XLE parser (Riezler et al, 2002),as well as an architecture for distributed data-mining within this corpus, called Oceanography(Waterman, 2009).
Using the parsed corpus, weextract a large volume of dependency relations andderive lexical models that significantly improvea rule-based system for determining the underly-ing argument structure of deverbal noun construc-tions.2 Deverbal Argument MappingDeverbal nouns, or nominalizations, are nounsthat designate some aspect of the event referredto by the verb from which they are morphologi-cally derived (Quirk et al, 1985).
For example,the noun destruction refers to the action describedby the verb destroy, and destroyer may refer to theagent of that event.
Deverbal nouns are very com-mon in English texts: by one count, about half ofall sentences in written text contain at least onedeverbal noun (Gurevich et al, 2008).
Thus, acomputational system that aims to match multi-ple ways of expressing the same underlying events(such as question answering or search) must beable to deal with deverbal nouns.To interpret deverbal constructions, one must beable to map nominal and prepositional modifiers tothe various roles in the verbal frame.
For intran-sitive verbs, almost any argument of the deverbalnoun is mapped to the verb?s subject, e.g.
abun-dance of food gives rise to subj(abound, food).If the underlying verb is transitive, and the de-verbal noun has two arguments, the mappingsare also fairly straightforward.
For example, thephrase Carthage?s defeat by Rome gives rise to19the arguments subj(defeat, Rome) and obj(defeat,Carthage), based on knowledge that a ?by?
argu-ment usually maps to the subject, and the posses-sive in the presence of a ?by?
argument usuallymaps to the object (Nunes, 1993).However, in many cases a deverbal noun hasonly one argument, even though the underlyingverb may be transitive.
In such cases, our systemhas to decide whether to map the lone argumentof the deverbal onto the subject or object of theverb.
This mapping is in many cases obvious to ahuman: e.g., the king?s abdication corresponds tosubj(abdicate, king), whereas the room?s adorn-ment corresponds to obj(adorn, room).
In somecases, the mapping is truly ambiguous, e.g., Theyenjoyed the support of the Queen vs.
They jumpedto the support of the Queen.
Yet in other cases, thelone argument of the deverbal noun is neither thesubject nor the object of the underlying verb, but itmay correspond to a different (e.g.
prepositional)argument of the verb, as in the travels of 1996 (cor-responding to someone traveled in 1996).
Finally,in some cases the deverbal noun is being used ina truly nominal sense, without an underlying map-ping to a verb, as in Bill Gates?
foundation, andthe possessive is not a verbal argument.The predictive models in this paper focus on thiscase of single arguments of deverbal nouns withtransitive underlying verbs.
To constrain the scopeof the task, we focus on possessive arguments, likethe room?s adornment, and ?of?
arguments, likethe support of the Queen.
Our goal is to improvethe accuracy of verbal roles assigned in such casesby creating lexically-specific preferences for indi-vidual deverbal noun / verb pairs.
Some of ourexperiments also take into account some lexicalproperties of the deverbal noun?s arguments.
Thelexical preferences are derived by comparing ar-gument preferences of verbs with those of relateddeverbal nouns, derived from a large parsed cor-pus using Oceanography.2.1 Current Deverbal Mapping SystemWe have a list of approximately 4000 deverbalnoun / verb pairs, constructed from a combina-tion of WordNet?s derivational links (Fellbaum,1998), NomLex (Macleod et al, 1998), NomL-exPlus (Meyers et al, 2004b) and some indepen-dent curation.
In the current system implementa-tion, we attempt to map deverbal nouns onto cor-responding verbs using a small set of heuristicsdescribed in (Gurevich et al, 2008).
We distin-guish between event nouns like destruction, agen-tive nouns like destroyer, and patient-like nounslike employee.If a deverbal noun maps onto a transitive verband has only one argument, the heuristics are asfollows.
Arguments of agentive nouns become ob-jects while the nouns themselves become subjects,so the ship?s destroyer maps to subj(destroy, de-stroyer); obj(destroy, ship).
Arguments of patient-like nouns become subjects while the nouns them-selves become objects, so the company?s employeebecomes subj(employ, company); obj(employ, em-ployee).The difficult case of event nouns is currentlyhandled through default mappings: possessive ar-guments become subjects (e.g., his confession 7?subj(confess, he)), and ?of?
arguments become ob-jects (e.g., confession of sin 7?
obj(confess, sin)).However, as we have seen from examples above,these defaults are not always correct.
The correctmapping depends on the lexical nature of the de-verbal noun and its corresponding verb, and pos-sibly on properties of the possessive or ?of?
argu-ment as well.2.2 System BackgroundThe deverbal argument mapping occurs in the con-text of a larger semantic search application, wherethe goal is to match alternate forms expressingsimilar concepts.
We are currently processingthe entire text of the English-language Wikipedia,consisting of about 2M unique pages.Parsing in this system is done using the XLEparser (Kaplan and Maxwell, 1995) and a broad-coverage grammar of English (Riezler et al, 2002;Crouch et al, 2009), which produces constituentstructures and functional structures in accordancewith the theory of Lexical-Functional Grammar(Dalrymple, 2001).Parsing is followed by a semantic processingphase, producing a more abstract argument struc-ture.
Semantic representations are created usingthe Transfer system of successive rewrite rules(Crouch and King, 2006).
Numerous construc-tions are normalized and rewritten (e.g., passives,relative clauses, etc.)
to maximize matching be-tween alternate surface forms.
This is the step inwhich deverbal argument mapping occurs.202.3 Evaluation DataTo evaluate the performance of the current andexperimental argument mappings, we extracted arandom set of 1000 sentences from the parsedWikipedia corpus in which a deverbal noun hada single possessive argument.
Each sentence wasmanually annotated with the verb role mappingbetween the deverbal and the possessive argu-ments.
One of six labels were assigned:?
Subject, e.g.
John?s attention?
Object, e.g.
arrangement of flowers?
Other: there is an underlying verb, but therelationship between the verb and the argu-ment is neither subject nor object; these rela-tions often appear as prepositional argumentsin the verbal form, e.g.
Declaration of Delhi?
Noun modifier: the argument modifies thenominal sense of the deverbal noun, ratherthan the underlying verb, although there isstill an underlying event, as in director of 25years?
Not deverbal: the deverbal noun is not usedto designate an event in this context, e.g.
therest of them?
Error: the parser incorrectly identified the ar-gument as modifying the deverbal, or as be-ing the only argument of the deverbalSimilarly, we extracted a sample of 750 sentencesin which a deverbal noun had a single ?of?
argu-ment, and annotated those manually.The distribution of annotations is summarizedin Table 1.
For possessive arguments, the preva-lent role was subject, and for ?of?
arguments it wasobject.The defaults will correctly assign the majorityof arguments roles.Possessive ?Of?total 1000 750unique deverbals 423 338subj 511 (51%) 158 (21%)obj 335 (34%) 411 (55%)other 28 (3%) 50 (7%)noun mod 23 (2%) 18 (2%)not deverbal 21 (2%) 40 (5%)error 82 (8%) 73 (10%)Table 1: Evaluation Role Judgements, with de-faults in bold2.4 Lexicalizing Role MappingsOur basic premise is that knowledge about role-mapping behavior of particular verbs will informthe role-mapping behavior of their correspondingdeverbal nouns.
For example, if a particular argu-ment of a given verb surfaces as the verb?s sub-ject more often than as object, we might also pre-fer the subject role when the same argument oc-curs as a modifier of the corresponding deverbalnoun.
However, as nominal modification con-structions impose their own role-mapping pref-erences (e.g., possessives are more likely to besubjects than objects), we expect different dis-tributions of arguments to appear in the variousdeverbal modification patterns.
Making use ofthis intuition requires collecting sufficient infor-mation about corresponding arguments of verbsand deverbal nouns.
This is available, given alarge parsed corpus, a reasonably accurate and fastparser, and enough computing capacity.
The re-mainder of the paper details our data extraction,model-building methods, and the results of someexperiments.3 Data CollectionOceanography is a pattern extraction and statisticslanguage for analyzing structural relationships incorpora parsed using XLE (Waterman, 2009).
Itsimplifies the task of programming for NL analy-sis over large corpora, and the sorting, counting,and distributional analysis that often characterizesstatistical NLP.
This corpus processing language isaccompanied by a distributed runtime, which usescluster computing to match patterns and collectstatistics simultaneously across many machines.This is implemented in a specialized distributedframework for parsing and text analysis built ontop of Hadoop (D. Cutting et al, ).
Oceanographyprograms compile down to distributed programswhich run in this cluster environment, allowing theNL researcher to state declaratively the data gath-ering and analysis tasks.A typical program consists of two declarativeparts, a pattern matching specification, and a setof statistics declarations.
The pattern matchingsection is written using Transfer, a specialized lan-guage for identifying subgraphs in the dependencystructures used in XLE (Crouch and King, 2006).Transfer rules use a declarative syntax for spec-ifying elements and their relations; in this way,it is much like a very specialized awk or grepfor matching within parse trees and dependencygraphs.Statistics over these matched structures are21also stated declaratively.
The researcher stateswhich sub-elements or tuples are to be counted,and the resulting compiled program will outputcounts.
Conditional distributions and comparisonsbetween distributions are available as well.3.1 Training DataUsing Oceanography, we extracted two sets of re-lations from the parsed Wikipedia corpus, Full-Wiki, with approximately 2 million documents.
Asmaller 10,000-document subset, the 10K set, wasused in initial experiments.
Some comparative re-sults are shown to indicate effects of corpus sizeon results.
Summary corpus statistics are shownin table 2.
The two sets were:1.
All verb-argument pairs, using verb and ar-gument lemmas.
We recorded the verb, theargument, the kind of relation between them(e.g., subject, object, etc.
), and part of speechof the argument, distinguishing also amongpronouns, names, and common nouns.
Foreach combination, we record its frequency ofoccurrence.2.
All deverbal-argument pairs, using deverbalnoun and argument lemmas.
We recorded thedeverbal noun, the argument, the kind of rela-tion (e.g., possessive, ?of?, prenominal modi-fier, etc.)
and part of speech of the argument.We record the frequency of occurrence foreach combination.Some summary statistics about the extracteddata are in Table 2.FullWiki training dataDocuments 2 millionSentences 121,428,873Deverbal nouns with arguments 4,596Unique verbs with deverbals 3,280Verbs with arguments 7,682Deverbal - role - argument sets 21,924,405Deverbal - argument pairs 12,773,621Deverbals with any poss argument 3,802Possessive deverbal - argument pairs 611,192Most frequent: poss(work, he) 75,343Deverbals with any ?of?
argument 4,075?Of?
deverbal- argument pairs 2,108,082Most frequent: of(end, season) 15,282Verb - role - argument sets 72,150,246Verb - argument pairs 40,895,810Overlapping pairs 5,069,479Deverbals with overlapping arguments 3,211Table 2: Training Data4 Assigning RolesThe present method is based on projecting argu-ment type preferences from the verbal usage to thedeverbal.
The intuition is that if an argument X ispreferred as the subject (object) of verb V, then itwill also be preferred in the semantic frame of anoccurrence (N, X) with the corresponding dever-bal noun N.We model these preferences directly using therelative frequency of subject and object occur-rences of each possible argument with each verb.Even with an extremely large corpus, it is unlikelythat one will find direct evidence for all such com-binations, and one will need to generalize the pre-diction.4.1 Deverbal-only ModelThe first model, all-arg, specializes only for thedeverbal, and generalizes over all arguments, re-lying on the overall preference of subject v. ob-ject for the set of arguments that appear with bothverb and deverbal forms.
Take as an exampledeverbal nouns with possessive arguments (e.g.,the city?s destruction).
Given the phrase (X?s N),where N is a deverbal noun related to verb V,Fd(N, V, X) is a function that assigns one of theroles subj, obj, unknown to the pair (V, X).
Inthis deverbal only model, the function depends onN and V only, and not on the argument X. Fd fora any pair (N, V) is calculated as follows:1.
Find all arguments X that occur in the con-struction ?N?s X?
as well as either subj(V, X)or obj(V, X).
X, N, and V have all been lem-matized.
For example, poss(city, destruction)occurs 10 times in the corpus; subj(destroy,city) occurs 3 times, and obj(destroy, city) oc-curs 12 times.
This approach conflates in-stances of the city?s destruction, the cities?destruction, the city?s destructions, etc.2.
For each argument X, calculate the ratio be-tween the number of occurrences of subj(V,X) and obj(V, X).
If the argument occursas subject more than 1.5 times as often asthe object, increment the count of subject-preferring arguments of N by 1.
If the ar-gument occurs as object more than 1.5 timesas often as subject (as would be the case with(destroy, city)), increment the count of object-preferring arguments.
If the ratio in frequen-cies of occurrence is less than the cutoff ratio22of 1.5, neither count is incremented.
In ad-dition to the number of arguments with eachpreference, we keep track of the total num-ber of instances for each argument prefer-ence, summed up over all individual argu-ments with that preference.3.
Compare the number of subject-preferring ar-guments of N with the number of object-preferring arguments.
If one is greater thanthe other by more than 1.5 times, state that thedeverbal noun N has a preference for map-ping its possessive arguments to the appro-priate verbal role.
We ignore cases where thetotal number of occurrences of the winningarguments is too small to be informative (inthe current model, we require it to be greaterthan 1).If there is insufficient evidence for a deverbalN, we fall back to the default preference across alldeverbals.
Subject and object co-occurrences withthe verb forms are always counted, regardless ofother arguments the verb may have in each sen-tence, on the intuition that the semantic role pref-erence of the argument is relatively unaffected andthat this will map to the deverbal construction evenwhen the possessive is the only argument.
Sum-mary preferences for all-args are shown in Ta-ble 3.The same algorithm was applied to detect ar-gument preferences for deverbals with ?of?
argu-ments (such as destruction of the city).
Summarypreferences are shown in Table 4.4.2 Deverbal + Argument Animacy ModelThe second model tries to capture the intuition thatanimate arguments often behave differently thaninanimate ones: in particular, animate argumentsare more often agents, encoded syntactically assubjects.We calculated argument preferences separatelyfor two classes of arguments: (1) animate pro-nouns such as he, she, I; and (2) nouns that werenot identified as names by our name tagger.
Weassumed that arguments in the first group wereanimate, whereas arguments in the second groupwere not.
In these experiments, we did not try toclassify named entities as animate or inanimate,resulting in less training data for both classes ofarguments.
This strategy also incorrectly classifiescommon nouns that refer to people (e.g., occupa-tion names such as teacher).The results of running both models on the 10Kand FullWiki training sets are in Table 3 for pos-sessive arguments and Table 4 for ?of?
arguments.For possessives, animate arguments preferredsubject role mappings much more than the averageacross all arguments.
Inanimate arguments also onthe whole preferred subject mappings, but muchless strongly.For ?of?
arguments, in most cases there weremore object-preferring verbs, except for verbswith animate arguments, which overwhelminglypreferred subjects.
We might therefore expectthere to be a difference in performance betweenthe model that treats all arguments equally and themodel that takes argument animacy into account.Model: all-arg10K FullWikiSubj-preferring 391 (65%) 1786 (67%)Obj-preferring 207 (35%) 884 (33%)Total 598 (100%) 2670 (100%)Model: animacySubj-pref animate 370 (78%) 1941 (79%)Obj-pref animate 106 (22%) 511 (21%)Total animate 476 (100%) 2452 (100%)Subj-pref inanimate 45 (47%) 990 (57%)Obj-pref inanimate 51 (53%) 748 (43%)Total inanimate 96 (100%) 1738 (100%)Table 3: Possessive argument preferencesModel: all-arg10K FullWikiSubj-preferring 143 (30%) 839 (29%)Obj-preferring 328 (70%) 2036 (71%)Total 471 (100%) 2875 (100%)Model: animacySubj-pref animate 70 (83%) 1196 (74%)Obj-pref animate 14 (17%) 423 (26%)Total animate 84 (100 %) 1619 (100%)Subj-pref inanimate 83 (23%) 699 (25%)Obj-pref inanimate 272 (77%) 2068 (75%)Total inanimate 355 (100%) 2767 (100%)Table 4: ?Of?
argument preferences5 ExperimentsThe base system against which we compare thesemodels uses the output of the parser, identifies de-verbal nouns and their arguments, and applies theheuristics described in Section 2.1 to obtain verbroles.
Recall that possessive arguments of transi-tive deverbals map to the subject role, and ?of?
ar-guments map to object.
Also recall that these rulesapply only to eventive deverbals; mapping rulesfor known agentive and patient-like deverbals re-main as before.23In the evaluation, the experimental models takeprecedence: if the model predicts an outcome, itis used.
The default system behavior is used as afallback when the model does not have sufficientevidence to make a prediction.
This stacking ofmodels allows the use of corpus evidence whenavailable, and generalized defaults otherwise.For the animacy model, we used our full sys-tem to detect whether the argument of a deverbalwas animate (more precisely, human).
In addi-tion to the animate pronouns used to generate themodel, we also considered person names, as wellas common nouns that had the hypernym ?person?in WordNet.
If the argument was animate and themodel had a prediction, that was used.
If no pre-diction was available for animate arguments, thenthe inanimate prediction was used.
Failing that,the prediction falls back to the general defaults.5.1 Possessive Arguments of Deverbal NounsModel predictions were compared against thehand-annotated evaluation set described in Sec-tion 2.3.
For each sentence in the evaluation set,we used the models to make a two-way predictionwith respect to the default mapping: is the posses-sive argument of the deverbal noun an underlyingsubject or not.
We ignored test sentences markedas having erroneous parses, leaving 918 (of 1000annotated).
Since we were evaluating the accuracyof the ?subject?
label, all non-subject roles (object,?other?, ?not a deverbal?, and ?nominal modifier?
)were in the same class.
The baseline for compari-son is the default ?subject?
role.The possible outcomes for each sentencewere:?
True Positive: Expected role and role pro-duced by the system are ?subject??
True Negative: Expected role is not subject,and the model did not produce the label sub-ject.
Expected role and produced role maydiffer (e.g.
expected role may be ?other?, andthe model may produce ?object?, but sinceneither one is ?subject?, this counts as correct?
False Positive: Expected role is not subject,but the model produced subject?
False Negative: Expected role is subject, butthe model produced some other roleAs a quick evaluation, we compared baselineand model-predicted results directly in the surfacestring of the sentences, without reparsing the sen-tences or using the semantic rewrite rules.
Theadvantage of this evaluation is that it is very fastto run and is easily reproducible outside of ourspecialized environment.
This evaluation differedfrom the full-pipeline evaluation in two ways: (1)it did not distinguish event deverbals from agen-tive and patient-like deverbals, thus possibly in-troducing errors, and (2) it did not look up all ar-gument lemmas to find out their animacy.
Thisbaseline had precision of 0.56; recall of 1.0, andan F-measure of 0.72.The complete evaluation uses our full NLpipeline, reparsing the sentences and applying allof our deverbal mapping rules as described above.The baseline for this evaluation had a precision of0.65, recall of 0.94, and F-measure of 0.77.
Thedifferences in the two baselines are mostly due tothe full-pipeline evaluation having different map-ping rules for agentive and patient-like deverbals.5.1.1 ResultsResults of applying the models are summarizedin Table 5, for all models, trained with both thesmaller and the larger data sets, and measured withand without using the full pipeline.All models performed better than the baseline.The all-arg model did about the same as the an-imacy model with both training sets.
We suggestsome reasons for this in the next section.It is unambiguously clear that adding lexicalknowledge to the rule system, even when thisknowledge is derived from a relatively small train-ing set, significantly improves performance, andalso that more training data leads to greater im-provements.Model Training Precision Recall F-measureSurface String MeasureBaseline - 0.56 1.00 0.72all-arg 10K 0.64 0.92 0.76animacy 10K 0.62 0.93 0.75all-arg FullWiki 0.68 0.95 0.81animacy FullWiki 0.70 0.92 0.79Full NL pipelineBaseline - 0.65 0.94 0.77all-arg 10K 0.75 0.88 0.81animacy 10K 0.73 0.90 0.80all-arg FullWiki 0.78 0.90 0.84animacy FullWiki 0.81 0.88 0.84Table 5: Performance on deverbal nouns with onepossessive argument5.1.2 Error Analysis and DiscussionWe looked at the errors produced by the best-performing model, all-arg trained on the FullWiki24set.
There were 49 false negatives (i.e.
caseswhere the human judge decided that the underly-ing relationship between the deverbal and its ar-gument is ?subject?, but our system produced adifferent relation or no relation at all), covering39 unique deverbal nouns.
Of these, 20 deverbalnouns were predicted by the model to prefer ob-jects (e.g., Hopkins?
accusation), and 19 did notget assigned either subject or object due to othererrors (including a few mislabeled evaluation sen-tences).Some of the false negatives involved deverbalnouns that refer to reciprocal predicates such ashis marriage, or causative ones such as Berlin?sunity, which could map to subject or objects.
Ourcurrent system does not allow us to express suchambiguity, but it is a possible future improvement.Looking at the false negatives produced by theall-arg model, 3 deverbal nouns received more ac-curate predictions with the animacy model (e.g.,his sight; Peter Kay?s statement).
Intuitively, theanimacy model should in general make more in-formed decisions about the argument mappingsbecause it takes properties of individual argumentsinto account.
However, as we have seen, it doesnot in fact outperform the model that treats all ar-guments the same way.We believe this is due to the fact that the an-imacy model was trained on less data than theall-arg model, because we only considered ani-mate pronouns and common nouns when gener-ating argument-mapping predictions.
Excludingall named entities and non-animate pronouns mostlikely had an effect on the number of deverbals forwhich the model was able to make accurate pre-dictions.
In the next iteration, we would like touse all available arguments, relying on the namedentity type and information available in WordNetfor common nouns to distinguish between animateand inanimate arguments.The all-arg model evaluation resulted in 131false positives (cases where the model predictedthe relation to be ?subject?, but the human judgethought it was something else).
Of these, 105 weremarked by the human judge as having objects, 8 ashaving a verbal relation other than subject or ob-ject, 9 as having nominal modifiers, 9 has havingno deverbal.Altogether, false positives covered 85 uniqueverbs.
Of these, 48 had been explicitly predictedby our model to prefer subjects, and the rest hadno explicit prediction, thus defaulting to having asubject.
3 of these deverbals would have been cor-rectly identified as having objects by the animacymodel (e.g., his composition; her representation).Although it is hard to predict the outcome of astatistical model, we feel that more reliable infor-mation about the animacy of arguments at trainingtime would improve the performance of the ani-macy model, potentially making it better than theall-arg model.5.2 ?Of?
Arguments of Deverbal NounsThe evaluation procedure for ?of?
arguments wasthe same as for possessive arguments, except thatthe default argument mapping was ?object?, andthe evaluated decision was whether a particularrole was object or non-object.
Ignoring sentencewith erroneous parses, we had 677 evaluation ex-amples.5.2.1 ResultsResults for all models are summarized in Table 6.All models outperformed the baseline on all train-ing sets and on both the surface or full-pipelinemeasures.As with possessive arguments, the all-arg andanimacy models performed about the same, withboth the FullWiki and 10K training sets.The 10K-trained animacy model did not doas poorly as might have been expected given itslow prediction rate for deverbals with animate ar-guments in our evaluation set.
The better-than-expected performance may be explained by lowincidence of animate arguments in this set.Model Training Precision Recall F-measureSurface String MeasureBaseline - 0.60 1.00 0.75all-arg 10K 0.68 0.97 0.80animacy 10K 0.66 0.94 0.78all-arg FullWiki 0.71 0.97 0.82animacy FullWiki 0.70 0.91 0.79Full NL pipelineBaseline - 0.61 0.89 0.73all-arg 10K 0.71 0.86 0.78animacy 10K 0.70 0.85 0.77all-arg FullWiki 0.78 0.87 0.82animacy FullWiki 0.80 0.85 0.82Table 6: Performance on deverbal nouns with one?of?
argument5.2.2 Error Analysis and DiscussionWe looked at the errors produced by the best-performing model, all-arg trained on the FullWiki25set.
There were 53 false negatives (cases wherethe human judged marked the relation as ?object?but the system marked it as something else), cov-ering 42 unique deverbal nouns.
Of these 7 were(incorrectly) predicted by the model to prefer sub-jects (e.g., operation of a railway engine), and therest were misidentified due to other errors.There were 101 false positives (cases where thesystem marked the role as object, but the humanjudge disagreed).
Of these, the human judgedmarked 54 as subject, 21 as other verbal role, 13as nominal modifier, and 13 as non-deverbal.Of the 72 unique deverbals in the false-positiveset, our model incorrectly predicted that 38 shouldprefer objects (such as Adoration of the Magi; un-der the direction of Bishop Smith)).
For 30 de-verbals, the model made no prediction, and thedefault mapping to object turned out to be incor-rect.
It is unclear to what extent better informationabout animacy would have helped.6 Related WorkOne of the earliest computational attempts to de-rive argument structures for deverbal nouns is(Hull and Gomez, 1996), with hand-crafted map-ping rules for a small set of individual nouns, ex-emplifying a highly precise but not easily scalablemethod.In recent years, NomBank (Meyers et al,2004a) has provided a set of about 200,000 manu-ally annotated instances of nominalizations witharguments, giving rise to supervised machine-learned approaches such as (Pradhan et al, 2004)and (Liu and Ng, 2007), which perform fairly wellin the overall task of classifying deverbal argu-ments.
However, no evaluation results are pro-vided for specific, problematic classes of nominalarguments such as possessives; it is likely that theamount of annotations in NomBank is insufficientto reliably map such cases onto verbal arguments.(Pado?
et al, 2008) describe an unsupervisedapproach that, like ours, uses verbal argumentpatterns to deduce deverbal patterns, though theresulting labels are semantic roles used in SLRtasks (cf.
(Gildea and Jurafsky, 2000)) ratherthan syntactic roles.
A combination of our muchlarger training set and the sophisticated probabilis-tic methods used by Pado?
et al would most likelyimprove performance for both syntactic and se-mantic roles labelling tasks.7 Conclusions and Future WorkWe have demonstrated that large amounts of lexi-cal data derived from an unsupervised parsed cor-pus improve role assignment for deverbal nouns.The improvements are significant even with a rel-atively small training set, relying on parses thathave not been hand-corrected, using a very sim-ple prediction model.
Larger amounts of extracteddata improve performance even more.There is clearly still headroom for improve-ment in this method.
In a pilot study, we usedargument preferences for individual deverbal-argument pairs, falling back to deverbal-only gen-eralizations when more specific patterns were notavailable.
This model had slightly higher preci-sion and slightly lower recall than the deverbal-only model, suggesting that a more sophisticcatedprobabilistic prediction model may be needed.In addition, performance should improve if weallow non-binary decisions: in addition to map-ping deverbal arguments to subject or object of theunderlying verb, we could allow mappings suchas ?unknown?
or ?ambiguous?.
The same trainingsets can be used to produce a model that makes a3- or 4-way split.
In the possessive and ?of?
sets,the ?unknown / ambiguous?
class would cover be-tween 15% and 20% of all the data.
This thirdpossibility becomes even more important for otherdeverbal arguments.
For example, if the deverbalnoun has a prenominal modifier (as in city destruc-tion), in a third of the cases the underlying relationis neither the subject nor the object (Lapata, 2002).And, of course, the methodology of extractinglexical preferences based on large parsed corporacan be applied to many other NL tasks not relatedto deverbal nouns.AcknowledgmentsWe gratefully acknowledge the helpful advice andcomments of our colleagues Tracy Holloway Kingand Dick Crouch, as well as the three anonymousreviewers.26ReferencesMichele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigua-tion.
In ACL, pages 26?33.Richard S. Crouch and Tracy Holloway King.
2006.Semantics via f-structure rewriting.
In Proceed-ings of the Lexical Functional Grammar Conference2006.Dick Crouch, Mary Dalrymple, Ron Kaplan,Tracy Holloway King, John Maxwell, and PaulaNewman.
2009.
XLE Documentation.
AvailableOn-line.D.
Cutting et al Apache Hadoop Project.http://hadoop.apache.org/.Mary Dalrymple.
2001.
Lexical Functional Grammar.Academic Press.
Syntax and Semantics, volume 34.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press.Daniel Gildea and Daniel Jurafsky.
2000.
Automaticlabeling of semantic roles.
In Proceedings of the38th Annual Meeting of the Association for Com-putational Linguistics, pages 512?520, Hong Kong,October.
Association for Computational Linguistics.Olga Gurevich, Richard Crouch, Tracy Holloway King,and Valeria de Paiva.
2008.
Deverbal nouns inknowledge representation.
Journal of Logic andComputation, 18:385?404.Richard D. Hull and Fernando Gomez.
1996.
Seman-tic interpretation of nominalizations.
In AAAI/IAAI,Vol.
2, pages 1062?1068.Ronald Kaplan and John T. Maxwell.
1995.
A methodfor disjunctive constraint satisfaction.
In Formal Is-sues in Lexical-Functional Grammar.
CSLI Press.Judith Klavans and Philip Resnik, editors.
1996.
TheBalancing Act.
Combining Symbolic and StatisticalApproaches to Language.
The MIT Press.Maria Lapata.
2002.
The disambiguation of nomi-nalizations.
Computational Linguistics, 28(3):357?388.Chang Liu and Hwee Tou Ng.
2007.
Learning pre-dictive structures for semantic role labeling of nom-bank.
In ACL.Catherine Macleod, Ralph Grishman, Adam Meyers,Leslie Barrett, and Ruth Reeves.
1998.
NOMLEX:A lexicon of nominalizations.
In Proceedings ofEURALEX?98.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics, 19(2):313?330.A.
Meyers, R. Reeves, C. Macleod, R. Szekely,V.
Zielinska, B.
Young, and R. Grishman.
2004a.The nombank project: An interim report.
InA.
Meyers, editor, HLT-NAACL 2004 Workshop:Frontiers in Corpus Annotation, pages 24?31,Boston, Massachusetts, USA, May 2 - May 7.
As-sociation for Computational Linguistics.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004b.
The cross-breeding ofdictionaries.
In Proceedings of LREC-2004.Mary Nunes.
1993.
Argument linking in english de-rived nominals.
In Robert Van Valin, editor, Ad-vances in Role and Reference Grammar, pages 375?432.
John Benjamins.Sebastian Pado?, Marco Pennacchiotti, and CarolineSporleder.
2008.
Semantic role assignment forevent nominalisations by leveraging verbal data.
InProceedings of CoLing08.Sameer Pradhan, Honglin Sun, Wayne Ward, James H.Martin, and Daniel Jurafsky.
2004.
Parsing argu-ments of nominalizations in english and chinese.
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Short Papers, pages 141?144, Boston, Massachusetts, USA, May 2 - May 7.Association for Computational Linguistics.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,and Jan Svartvik.
1985.
A Comprehensive Gram-mar of the English Language.
Longman.Stefan Riezler, Tracy Holloway King, Ronald Kaplan,John T. Maxwell II, Richard Crouch, and MarkJohnson.
2002.
Parsing the Wall Street Journalusing a Lexical-Functional Grammar and discrimi-native estimation techniques.
In Proceedings of theACL?02.Scott A. Waterman.
2009.
Distributed parse mining.In Software engineering, testing, and quality assur-ance for natural language processing (SETQA-NLP2009).27
