Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 665?672Manchester, August 2008Semantic role assignment for event nominalisationsby leveraging verbal dataSebastian Pad?oDepartment of LinguisticsStanford University450 Serra MallStanford CA 94305, USApado@stanford.eduMarco Pennacchiotti and Caroline SporlederComputational LinguisticsSaarland UniversityPostfach 15 11 5066041 Saarbr?ucken, Germany{pennacchiotti|csporled}@coli.uni-sb.deAbstractThis paper presents a novel approach tothe task of semantic role labelling for eventnominalisations, which make up a consider-able fraction of predicates in running text,but are underrepresented in terms of train-ing data and difficult to model.
We proposeto address this situation by data expansion.We construct a model for nominal role la-belling solely from verbal training data.
Thebest quality results from salvaging gram-matical features where applicable, and gen-eralising over lexical heads otherwise.1 IntroductionThe last years have seen a large body of work onmodelling the semantic properties of individualwords, both in the form of hand-built resourceslike WordNet and data-driven methods like seman-tic space models.
It is still much less clear how thecombined meaning of phrases can be described.Semantic roles describe an important aspect ofphrasal meaning by characterising the relationshipbetween predicates and their arguments on a seman-tic level (e.g., agent, patient).
They generalise oversurface categories (such as subject, object) and vari-ations (such as diathesis alternations).
Two frame-works for semantic roles have found wide use inthe community, PropBank (Palmer et al, 2005) andFrameNet (Fillmore et al, 2003).
Their corpora areused to train supervised models for semantic rolelabelling (SRL) of new text (Gildea and Jurafsky,2002; Carreras and M`arquez, 2005).
The resultinganalysis can benefit a number of applications, suchc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.as Information Extraction (Moschitti et al, 2003)or Question Answering (Frank et al, 2007).A commonly encountered criticism of seman-tic roles, and arguably a major obstacle to theiradoption in NLP, is their limited coverage.
Sincemanual semantic role tagging is costly, it is hardlyconceivable that gold standard annotation will ulti-mately be available for every predicate of English.In addition, the lexically specific nature of the map-ping between surface syntax and semantic rolesmakes it difficult to generalise from seen predicatesto unseen predicates for which no training data isavailable.
Techniques for extending the coverage ofSRL therefore address an important need.Unfortunately, pioneering work in unsupervisedSRL (Swier and Stevenson, 2004; Grenager andManning, 2006) currently either relies on a smallnumber of semantic roles, or cannot identify equiva-lent roles across predicates.
A promising alternativedirection is automatic data expansion, i.e., lever-aging existing annotations to classify unseen, butsimilar, predicates.
The feasibility of this approachwas demonstrated by Gordon and Swanson (2007)for syntactically similar verbs.
However, their ap-proach requires at least one annotated instance ofeach new predicate, limiting its practicability.In this paper, we present a pilot study on theapplication of automatic data expansion to eventnominalisations of verbs, such as agreement foragree or destruction for destroy.
While event nom-inalisations often afford the same semantic rolesas verbs, and often replace them in written lan-guage (Gurevich et al, 2006), they have played alargely marginal role in annotation.
PropBank hasonly annotated verbs.1 FrameNet annotates nouns,but covers far fewer nouns than verbs.
The same1A follow-up project, NomBank (Meyers et al, 2004), hassince provided annotations for nominal instances, too.665situation holds in other languages (Erk et al, 2003).Our fundamental intuition is that it is possible toincrease the annotation coverage of event nominal-isations by data expansion from verbal instances,since the verbal and nominal predicates share alarge part of the underlying argument structure.
Weassume that annotation is available for verbal in-stances.
Then, for a given instance of a nominal-isation and its arguments, the aim is to assign se-mantic role labels to these arguments.
We solve thistask by constructing mappings between the argu-ments of the noun and the semantic roles realisedby the verb?s arguments.
Crucially, unlike previouswork (Liu and Ng, 2007), we do not employ a clas-sical supervised approach, and thus do not requireany nominal annotations.Structure of the paper.
Sec.
2 provides back-ground on nominalisations and SRL.
Sec.
3 pro-vides concrete details on our expansion-based ap-proach to SRL for nominalisations.
The second partof the paper (Sec.
4?6) provides a first evaluationof different mapping strategies based on syntactic,semantic, and hybrid information.
Sec.
8 concludes.2 NominalisationsNominalisations (or deverbal nouns) are commonlydefined as nouns morphologically derived fromverbs, usually by suffixation (Quirk et al, 1985).They have been classified into at least three cate-gories in the linguistic literature, event, result, andagent/patient nominalisations (Grimshaw, 1990).Event and result nominalisations account for thebulk of deverbal nouns.
The first class refers to anevent/activity/process, with the nominal expressingthis action (e.g.
killing, destruction).
Nouns in thesecond class describe the result or goal of an ac-tion (e.g.
agreement).
Many nominals have bothan event and a result reading (e.g., selection canmean the process of selecting or the selected ob-ject).
Choosing a single reading for an instance isoften difficult; see Nunes (1993); Grimshaw (1990).A smaller class is agent/patient nominalisations.Agent nominals are usually identified by suffixessuch as -er, -or, -ant (e.g.
speaker, applicant), whilepatient nominalisations end with -ee, -ed (e.g.
em-ployee).
While these nominalisations can be anal-ysed as events (the baker?s bread implies that bak-ing has taken place), they more naturally refer toparticipants.
In consequence, agent/patient nomi-nals tend to realise fewer arguments ?
the averagein FrameNet is 1.46 arguments, compared to 1.74PropBankVerbs (Carreras and M`arquez, 2005) 80%Nouns (Liu and Ng, 2007) 73%FrameNetVerbs (Mihalcea and Edmonds, 2005) 72%Nouns (Pradhan et al, 2004) 64%Table 1: F-Scores for supervised SRL (end-to-end)for events/results.
As our goal is nominal SRL, weconcentrate on the event/results class.SRL for nominalisations.
Compared to the wealthof studies on verbal SRL (e.g., Gildea and Juraf-sky (2002); Fleischman and Hovy (2003)), thereis relatively little work that specifically addressesnominal SRL.
Nouns are generally treated likeverbs: the task is split into two classification steps,argument recognition (telling arguments from non-arguments) and argument labelling (labelling recog-nised arguments with a role).
Nominal SRL alsotypically draws on feature sets that are similar tothose for verbs, i.e., comprising mainly syntac-tic and lexical-semantic information (Liu and Ng,2007; Jiang and Ng, 2006).On the other hand, there is converging evidencethat nominal SRL is somewhat more difficult thanverbal SRL.
Table 1 shows some results for bothverbal and nominal SRL from the literature.
Forboth PropBank and for FrameNet, we find a differ-ence of 7?8% F-Score.
Note, however, that thesestudies use different datasets and are thus not di-rectly comparable.In order to confirm the difference between nounsand verbs, we modelled a controlled dataset (de-scribed in detail in Sec.
4) of verbs and corre-sponding event nominalisations.
We used Shal-maneser (Erk and Pad?o, 2006), to our knowledgethe only freely available SRL system that handlesnouns.
SRL models were trained on verbs andnouns separately, using the same settings and fea-tures.
Table 2 shows the results, averaged over 10cross-validation (CV) folds.
Accuracy was aboutequal in the recognition step, and 5% higher forverbs in the labelling step.
We analysed these re-sults by fitting a logit mixed model.
These modelsdetermine which fixed factors are responsible fordifferences in a response variable (here: SRL per-formance) while correcting for imbalances intro-duced by random factors (see Jaeger (2008)).
Wemodelled the training and test set sizes and the pred-icates?
parts of speech as fixed effects, and framesand CV folds as random factors.For both argument recognition and labelling, the666Step Verbs NounsArg recognition (F1, class FE) 0.59 0.60Arg labelling (Accuracy) 0.70 0.65Table 2: FrameNet SRL on verbs and nounsamount of training data turned out to be a signifi-cant factor, i.e., more data leads to higher results.While the part of speech was not systematicallylinked to performance for argument recognition,it was a highly significant predictor of accuracyin the labelling step: Even when training set sizewas taken into account, verbal arguments were stillsignificantly easier to label (z=4.5, p<0.001).In sum, these results lend empirical support toclaims that nominal arguments are less tightly cou-pled to syntactic realisation than verbal ones (Carl-son, 1984); their interpretation is harder to capturewith shallow cues.3 Data Expansion for Nominal SRLThe previous section has established two observa-tions.
First, the argument structures of verbs andtheir event nominalisations correspond largely.
Sec-ond, nominal SRL is a difficult task, even givennominal training data, which is hard to obtain.Our proposal in this paper is to take advan-tage of the first observation to address the sec-ond.
We do so by modelling SRL for event nom-inalisations as a data expansion task ?
i.e., us-ing existing verbal annotations to carry out SRLfor novel nominal instances.
In this manner, wedo away completely with the need for manualannotation of nominal instances that is requiredfor previous supervised approaches (cf.
Sec.
2).Consider the following examples, given in format[constituent]grammatical function/SEMANTIC ROLE:(1) a.
[Peter]Subj/COGNIZERlaughs[about the joke]PP-about/STIMULUSb.
[Peter]Subj/COGNIZERlaughs[at him]PP-at/STIMULUS(2) [Peter?s]Prenom-Gen/?
[hearty]Prenom-Mod/?laughter [about the event]PP-about/?The sentences with the verbal predicate laugh in(1) are labelled with semantic roles, while the NPcontaining the event nominalisation laughter in (2)is not.
The question we face are what informationfrom (1) can be re-used to perform argument recog-nition and labelling on (2), and how.In this respect, there is a fundamental differencebetween lexical-semantic and syntactic information.Lexical-semantic features, such as the head word,are basically independent of the predicate?s part ofspeech.
Thus, the information from (1) that Peter isa COGNIZER can be used directly for the analysis ofthe occurrence of Peter in (2).
Unfortunately, purelexical features tend to be sparse: the head wordof the last role, event, is unseen in (1), and due toits abstract nature, also difficult to classify throughsemantic similarity.
Therefore, it is necessary toconsider syntactic features as well.
However, thesevary substantially between verbs and nouns.
Whenthey are applicable to both parts of speech, somemileage can be apparently gained: the phrase in (2)headed by event can be classified as STIMULUSbecause it is an about-PP like (1a).
In contrast, nodirect inferences can be drawn about prenominalgenitives or modifiers which do not exist for verbs.In the remainder of this paper, we will presentexperiments on different ways of combining syn-tactic and lexical-semantic information to balanceprecision and recall in data expansion.
We addressargument recognition and labelling separately, sincethe two tasks require different kinds of information.We assume that the frame has been determined be-forehand with word sense disambiguation methods.4 DataThe dataset for our study consists of the annotatedFrameNet 1.3 examples.
We obtained pairs of verbsand corresponding event/result nominalisations byintersecting the FrameNet predicate list with a listof nominalisations obtained from Celex (Baayenet al, 1995) and Nomlex (Macleod et al, 1998).We found 306 nominalisations with correspond-ing verbs in the same frame, but excluded somepairs where either the nominalisation was not of theevent/result type, or no annotated FrameNet exam-ples were available for either verb or noun.
The finaldataset, consisting of 265 pairs exemplifying 117frames, served for both the analysis in Section 2 andthe evaluations in subsequent sections.
For the eval-uations, we used the 26,479 verbal role instances(2,066 distinct role types) as training data and the6,502 nominal role instances (993 distinct roletypes) as test data.
The specification of the datasetcan be downloaded from http://www.coli.uni-sb.de/?pado/nom data.html.5 Argument RecognitionArgument recognition is usually modelled as a su-pervised machine learning task.
Unfortunately, ar-667NPPPNPNPNPPeter?s laughter about the jokeFigure 1: Parse tree for example sentencegument recognition ?
at least within predicates ?
re-lies heavily on syntactic features, with the grammat-ical function (or alternatively syntactic path) featureas the single most important predictor (Gildea andJurafsky, 2002).
Since we are bootstrapping fromverbal instances to nominal ones, and since there istypically considerable variation between nominaland verbal subcategorisation patterns, we cannotmodel argument recognition as a supervised task.Instead, we follow up on an idea developed by Xueand Palmer (2004) for verbal SRL, who charac-terise the set of grammatical functions that couldfill a semantic role in the first place.
In our apppli-cation, we simply extract all syntactic arguments ofthe nominalisation, including any premodifiers.
Wemake no attempt to distinguish between adjunctsand compulsory arguments.
Fig.
1 shows an exam-ple: in the NP Peter?s laughter about the joke, thenoun laughter has two syntactic arguments: the PPabout the joke and the premodifying NP Peter?s.Both are extracted as (potential) arguments.This method cannot identify roles that are syntac-tically non-local, i.e., those that are not in the max-imal projection of the frame evoking noun.
Suchroles are more common for nouns than for verbs.Example 3 shows that an ?external?
NP like Bill canbe analysed as filling the HELPER role of the nounhelp.
However, the overall proportion of non-localroles is still fairly small in our data (around 10%).
(3) [Bill]HELPERoffered help in case of need.Table 3 gives the argument recognition results forour rule-based system on all roles in the gold stan-dard and on the local roles alone.
This simple ap-proach is surprisingly effective, achieving an over-all F-Measure of 76.89% on all roles, while on localroles the F-Measure increases to 82.83% due to thehigher recall.
Precision is 82.01%, as not all syn-tactic arguments do fill a role.
For example, modalmodifiers such as hearty in (2) rarely fill a (core)role in FrameNet.
False-negative errors, which af-fect recall, are partly due to parser errors and partlyRoles Precision Recall F-Measureall roles 82.01 72.37 76.89local roles 82.01 83.66 82.83Table 3: Argument recognition (local / all roles).to role fillers that do not correspond to constituentsor that are embedded in syntactic arguments.
For in-stance, in (4) the PP in this country, which fills thePLACE role of cause, is embedded in the PP of suf-fering in this country, which fills the role EFFECT.We extract only the larger PP.
(4) the causes [of suffering[in this country]PP-in]PP-of6 Argument LabellingArgument labelling presents a different picturefrom argument recognition.
Here, both syntacticand lexical-semantic information contribute to suc-cess in the task.
We present three model familiesfor nominal argument labelling that take differentstances with respect to this observation.The first (naive-semantic) and the second (naive-syntactic) model families represent extreme posi-tions that attempt to re-use verbal information asdirectly as possible.
Models from the third fam-ily, distributional models infer the role of a noun?sarguments by computing the semantic similaritybetween nominal arguments and semantic represen-tations of the verb roles given by the role fillers?semantic heads.2 In the lexical-level instantiation,the mapping is established between individual nounarguments and roles.
In the function-level instantia-tion, complete nominal grammatical functions aremapped onto roles.36.1 Naive semantic modelThe naive semantic model (naive sem) implementsthe assumption that lexical-semantic features pro-vide the same predictive evidence for verbal andnominal arguments (cf.
Sec.
3).
It can be thought ofas constructing the trivial identity mapping betweenthe values of nominal and verbal semantic features.To test the usefulness of this model, we train theShalmaneser SRL system (Erk and Pad?o, 2006)on the verbal instances of the dataset described2Usually, the semantic head of a phrase is its syntactic head.Exceptions occur e.g.
for PPs, where the semantic head is thesyntactic head of the embedded NP.3We compute grammatical functions as phrase types plusrelative position; for PPs, we add the preposition.668in Sec.
4, using only the lexical-semantic features(head word, first word, last word).
We then applythe resulting models directly to the correspondingnominal instances.6.2 Naive syntactic modelThe intuition of this model (naive syn) is that gram-matical functions shared between verbs and nounsare likely to express the same semantic roles.
Itmaps all grammatical functions of a verb gvontothe identical functions of the corresponding noungnand then assigns the most frequent role realisedby gvto all arguments with grammatical functiongn.
For example, if PPs headed by about for theverb laugh typically realise the STIMULUS role, allarguments of the noun laughter which are realisedas PP-about are also assigned the STIMULUS role.We predict that this strategy has a ?highprecision?low recall?
profile: It produces reliablemappings for those grammatical functions that arepreserved across verb and noun, in particular prepo-sitional phrases; however, it fails for grammaticalfunctions that only occur for one part of speech.This problem becomes particular pertinent fortwo prominent role types, namely AGENT-styleroles (deep subjects) and PATIENT-style roles (deepobjects).
These roles are usually expressed via dif-ferent and ambiguous noun and verb functions(Gurevich et al, 2006).
For verbs, the AGENTis typically expressed by the Subject, while fornouns it is expressed by a Pre-Modifier.
The PA-TIENT is commonly realised as the Object forverbs, and either as a Pre-Modifier or as a PP-offor nouns.
As the noun?s Pre-Modifier is highlyambiguous, it is also ineffective to apply a non-identity mapping such as (subjectv,Pre-Modifiern)or (objectv,Pre-Modifiern).4A final variation of this model is the generalisednaive syntactic model (naive sem-gen), where weassign the role most frequently realised by a givenfunction across all verbs in the frame.
This methodalleviates data sparseness stemming from functionsnever seen with particular verbs and is fairly safe,since mapping within frames tends to be uniform.6.3 Distributional modelsThe distributional models construct mappings be-tween verbal and nominal semantic heads.
In con-4Lapata (2002) has shown that the mapping can be dis-ambiguated for individual nominalisations.
Her model, usinglexical-semantic, contextual and pragmatic information, is out-side the scope of the present paper.trast to the naive semantic model, they make use ofsome measure of semantic similarity to find map-pings, and optionally use syntactic constraints toguide generalisation.
In this manner, distributionalmodels can deal with unseen feature values moreeffectively.
In sentences (1) and (2), for example,an ideal distributional model would find the headword event in (2) to be more semantically similarto the head joke in (1a) than to head him in (1b).The resulting mapping (joke, event) leads to theassignment of the role STIMULUS to event.Semantic Similarity.
Semantic similarity mea-sures are commonly used to compute similaritybetween two lexemes.
There are two main typesof similarity: Ontology-based, computed throughthe closeness of two lexemes in a lexical database(e.g., WordNet); and distributional, given by somemeasure of the distance between the lexemes?
vec-tor representations in a semantic co-occurrencespace.
We chose the latter approach because it tendsto have a higher coverage and it is knowledge-lean,requiring just an unannotated corpus.We compute distributional-similarity with asemantic space model based on lexical co-occurrences backed by syntactic relations (Pad?oand Lapata, 2007).5 The model is constructed fromthe British National Corpus (BNC), using the 2.000most pairs of words and grammatical functions asdimensions.
As similarity measure, we use cosinedistance on log-likelihood transformed counts.Lexical level model.
The lexical level model(dist-lex) assigns to each nominal argument the verbrole that it is semantically most similar to.
Each roleis represented by the semantic heads of its fillers.For example, suppose that the role STIMULUS ofthe verb laugh has been realised by the heads story,scene, joke, and tale.
Then, in ?Peter?s laughterabout the event?, we analyse event as STIMULUS,since event is similar to these heads.Formally, each argument head l is representedby a co-occurrence vector ~l.
A verb role rv?
Rvismodelled by the centroid ~rvof its instances?
heads:~rv=1|Lrv|?l?Lrv~lRoles are assigned to nominal argument heads ln?Lnby finding the semantically most similar role r5We also experimented with bag-of-words based vectorspaces, which showed worse performance throughout.669while the grammatical function gnis ignored:r(ln, gn) = argmaxrv?Rvsimcos(~ln, ~rv)Function level model.
The syntactic level model(dist-fun) generalises the lexical level model by ex-ploiting the intuition that, within nouns, most se-mantic roles tend to be consistently realised by onespecific grammatical function.
This function canbe identified as the one most semantically similarto the role?s representation.
Following the exam-ple above, suppose that the grammatical functionPP-about of laughter has as semantic heads thelexemes: event, story, news.
Then, it is likely toexpress the role STIMULUS, as its heads are seman-tically similar to those of the verbal fillers of thisrole: story, scene, sentence, tale.
For each nomi-nalisation, this model constructs mappings (rv, gn)between a verbal semantic role rvand a nominalgrammatical function gn.
The representations forroles are computed as described above.
We com-pute the semantic representations for grammaticalfunctions, in parallel to the roles?
definition above,as the centroid of their fillers?
representations Lgn:~gn=1|Lgn|?l?Lgn~lThe assignment of a role to a nominal argumentsis now determined by the argument?s grammaticalfunction gn; its lemma lnonly enters indirectly, viathe similarity computation:r(ln, gn) = argmaxrv?Rvsimcos(~rv, ~gn)This strategy guarantees that each nominal gram-matical function is mapped to exactly one role.
Inthe inverse direction, roles can be left unmapped ormapped to more than one function.66.4 Hybrid modelsOur last class combines the naive and distributionalmodels with a back-off approach.
We first attemptto harness the reliable naive syntactic approachwhenever a mapping for the argument?s grammati-cal function is available.
If this fails, it backs off to adistributional model.
This strategy helps to recoverthe frequent AGENT- and PATIENT-style roles thatcannot be recovered on syntactic grounds.6We also experimented with a global optimisation strategywhere we maximised the overall similarity between roles andfunctions subject to different constraints (e.g., perfect match-ing).
Unfortunately, this strategy did not improve results.System Accuracybaseline random 17.09BLbaseline most common 42.97naive syn 15.29naive syn-gen 21.56Naivenaive sem 24.00dist-lex 44.57Distdist-fun 52.00naive syn + dist-lex 48.22naive syn-gen + dist-lex 50.54naive syn + dist-fun 54.39Hybridnaive syn-gen + dist-fun 56.42Table 4: Results for nominal argument labellingIn (2), a hybrid model would assign therole STIMULUS to the argument headed byevent, using the naive syntactic mapping(PP-aboutv,PP-aboutn) derived from (1a).
For theprenominal modifier, no syntactic mapping is avail-able; thus, it backs off to lexical-semantic evidencefrom (1a-b) to analyse Peter as COGNIZER.We experiment with two hybrid models: naivesyntactic plus lexical level distributional (naive syn+ dist-lex), and naive syntactic plus functional leveldistributional (naive syn + dist-fun).7 Experimental resultsThe results of our experiments are reported in Ta-ble 4.
The models are compared against two base-lines: A random baseline which randomly choosesone of the verb roles for each of the arguments ofthe corresponding noun; a most common baselinewhich assigns to each nominal argument the mostfrequent role of the corresponding verb ?
i.e.
therole which has most fillers.
All models with theexception of naive syn significantly outperform therandom baseline, but only dist-fun and all hybridmodels outperform the most common baseline.In general, the best performing methods are thehybrid ones, with best accuracy achieved by naivesyn-gen + dist-fun.
Non-hybrid approaches alwayshave lower accuracy.
This validates our main hy-pothesis in this paper, namely that the combinationof syntactic information with distributional seman-tics is a promising strategy.Matching our predictions, the low accuracy ofthe naive syntactic model is mainly due to a lackof coverage.
In fact, the model leaves 5,010 of the6,502 gold standard noun fillers unassigned sincethey realise syntactic roles that are unseen for theverbs in question.
A large part of these are Pre-Modifier and PP-of functions, which are centralfor nouns, but mostly ungrammatical for verbs.
On670the 1,492 fillers for which a role was assigned, themodel obtains an accuracy of 67%, indicating a rea-sonably high, but not perfect, accuracy for sharedgrammatical functions.
The remaining errors stemfrom two sources.
First, many grammatical func-tions are ambiguous, causing wrong assignmentsby a ?syntax-only?
model.
For example, PP-in canindicate both TIME and PLACE for many nominal-isations.Second, a certain number of grammaticalfunctions do not preserve their role between verb tonoun (Hull and Gomez, 1996).
For example, PP-torealises the MESSAGE role of the verb require butthe ADDRESSEE role of the noun request.Distributional models show in general better per-formance than the naive syntactic approach (ap-prox.
+25% accuracy).
They do not suffer from thecoverage problem, since they assign a role to eachfiller.
Yet, the accuracy over assigned roles is lowerthan for the syntactic approach (52% for dist-fun).We conclude that in the limited cases where apure syntactic mapping is applicable, it is far morereliable than methods which are mainly based onlexical-semantic information.
The major limitationof the latter is that lexical-semantics tend to failwhen roles are semantically very similar.
For ex-ample, for the noun announcement, the syntactic-level distributional model wrongly builds the map-ping (ADDRESSEE, PP-by) instead of (SPEAKER,PP-by), because the two roles are very similar se-mantically (the computed similarities of the PP-byarguments to ADDRESSEE and SPEAKER in thesemantic space are 0.94 and 0.92, respectively).The syntactic-level distributional model outper-forms the lexical-level, suggesting that generalisingthe mapping at the argument level offers more sta-ble statistical evidence to find the correct role, i.e.
aset of noun arguments better defines the seman-tics of the mapping than a single argument.
Thisis mostly the case when the context vector of theargument is not a good representation because thesemantic head is ambiguous, infrequent or atypical.Consider, for example, the following sentence forthe noun violation:(5) Sterne?s Tristram Shandy consistsof a series of violations [of literaryconventions]PP-OF/NORMThe syntactic-level model builds the correct map-ping (NORM, PP-of ), as the role fillers of the verbviolate (e.g.
principle, right, treaty, law) are verysimilar to the noun?s category fillers (e.g.
conven-tion, rule, agreement, treaty, norm), causing the cen-troids of NORM and PP-of to be close in the space.The lexical-level model, however, builds the incor-rect mapping (PROTAGONIST, convention).
Thishappens because convention is ambiguous, and oneof its senses (?a large formal assembly?)
is compat-ible with the PROTAGONIST role, and happens tohave a large influence on the position of the vectorfor convention.
Unfortunately, this is not the sensein which the word is used in this sentence.8 ConclusionsWe have presented a data expansion approach toSRL for event nominalisations.
Instead of relyingon manually annotated nominal training data, weharness annotated data for verbs to bootstrap a se-mantic role labeller for nouns.
For argument recog-nition, we use a simple rule-based approach.
Forargument labelling, we profit from the fact that theargument structures of event nominalisations andthe corresponding verbs are typically similar.
Thisallows us to learn a mapping between verbal rolesand nominal arguments, using syntactic features,lexical-semantic similarity, or both.We found that our rule-based approach for argu-ment recognition works fairly well.
For argumentlabelling, our approach does not yet attain the per-formance of supervised models, but has the crucialadvantage of not requiring any labelled data fornominal predicates.We achieved the highest accuracy with a hybridsyntactic-semantic model, which indicates that bothtypes of information need to be combined for op-timal results.
A purely syntactic approach resultsin a high precision, but low coverage because fre-quent grammatical functions in particular cannot betrivially mapped.
Backing off to semantic similarityprovides additional coverage.
However, semanticsimilarity has to be considered on the level of com-plete functions rather than individual instances topromote ?uniformity?
in the mappings.In this paper, we have only considered nominalSRL by data expansion, i.e.
we only applied ourapproach to those nominalisations for which wehave annotated data for the corresponding verbs.However, even if no data is available for the corre-sponding verb, it might still be possible to bootstrapfrom other verbs in the same frame (assuming thatthe frame is known for the nominalisation) and weplan to pursue this idea in furture research.
We alsointend to investigate whether a joint optimisation of671the mapping constrained by additional syntactic in-formation such as subcategorisation frames leads tobetter results.
Finally, we will verify that our meth-ods, which we have evaluated on English FrameNetdata, carry over to other corpora and languages.Acknowledgments.
Our work was partly fundedby the German Research Foundation DFG (grantPI 154/9-3).ReferencesBaayen, R., R. Piepenbrock, and L. Gulikers, 1995.
TheCELEX Lexical Database (Release 2).
LDC.Carlson, G. 1984.
Thematic roles and their role in se-mantic interpretation.
Linguistics, 22:259?279.Carreras, X. and L. M`arquez, editors.
2005.
Proceed-ings of the CoNLL shared task: Semantic role la-belling, Ann Arbor, MI.Erk, K. and S. Pad?o.
2006.
Shalmaneser ?
a flexibletoolbox for semantic role assignment.
In Proceed-ings of LREC, Genoa, Italy.Erk, K., A. Kowalski, S. Pad?o, and M. Pinkal.
2003.Towards a resource for lexical semantics: A largeGerman corpus with extensive semantic annotation.In Proceedings of ACL, pages 537?544, Sapporo,Japan.Fillmore, C., C. Johnson, and M. Petruck.
2003.
Back-ground to FrameNet.
International Journal of Lexi-cography, 16:235?250.Fleischman, M. and E. Hovy.
2003.
Maximum entropymodels for FrameNet classification.
In Proceedingsof EMNLP, pages 49?56, Sapporo, Japan.Frank, A., H.-U.
Krieger, F. Xu, H. Uszkoreit, B. Crys-mann, B. J?org, and U. Sch?afer.
2007.
Question an-swering from structured knowledge sources.
Journalof Applied Logic, 5(1):20?48.Gildea, D. and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.Gordon, A. and R. Swanson.
2007.
Generalizingsemantic role annotations across syntactically simi-lar verbs.
In Proceedings of ACL, pages 192?199,Prague, Czechia.Grenager, T. and C. Manning.
2006.
Unsupervised dis-covery of a statistical verb lexicon.
In Proceedingsof EMNLP, pages 1?8, Sydney, Australia.Grimshaw, J.
1990.
Argument Structure.
MIT Press.Gurevich, O., R. Crouch, T. King, and V. de Paiva.2006.
Deverbal nouns in knowledge representation.In Proceedings of FLAIRS, pages 670?675, Mel-bourne Beach, FL.Hull, R. and F. Gomez.
1996.
Semantic interpretationof nominalizations.
In Proceedings of AAAI, pages1062?1068, Portland, OR.Jaeger, T. 2008.
Categorical data analysis: Away fromANOVAs and toward Logit Mixed Models.
Journalof Memory and Language.
To appear.Jiang, Zheng Ping and Hwee Tou Ng.
2006.
Semanticrole labeling of NomBank: A maximum entropy ap-proach.
In Proceedings of EMNLP, pages 138?145,Sydney, Australia.Lapata, M. 2002.
The disambiguation of nominalisa-tions.
Computational Linguistics, 28(3):357?388.Liu, C. and H. Ng.
2007.
Learning predictive structuresfor semantic role labeling of NomBank.
In Proceed-ings of ACL, pages 208?215, Prague, Czechia.Macleod, C., R. Grishman, A. Meyers, L. Barrett, andR.
Reeves.
1998.
Nomlex: A lexicon of nominaliza-tions.
In Proceedings of EURALEX, Li`ege, Belgium.Meyers, A., R. Reeves, C. Macleod, R. Szekely,V.
Zielinska, B.
Young, and R. Grishman.
2004.
An-notating Noun Argument Structure for NomBank.
InProceedings of LREC, Lisbon, Portugal.Mihalcea, Rada and Phil Edmonds, editors.
2005.Proceedings of Senseval-3: The Third InternationalWorkshop on the Evaluation of Systems for the Se-mantic Analysis of Text, Barcelona, Spain.Moschitti, A., P. Morarescu, and S. Harabagiu.
2003.Open-domain information extraction via automaticsemantic labeling.
In Proceedings of FLAIRS, pages397?401, St. Augustine, FL.Nunes, M. 1993.
Argument linking in English de-rived nominals.
In Valin, Robert D. Van, editor, Ad-vances in Role and Reference Grammar, pages 372?432.
John Benjamins.Pad?o, S. and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2):161?199.Palmer, M., D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.Pradhan, S., H. Sun, W. Ward, J. Martin, and D. Ju-rafsky.
2004.
Parsing arguments of nominaliza-tions in English and Chinese.
In Proceedings ofHLT/NAACL, pages 141?144, Boston, MA.Quirk, R., S. Greenbaum, G. Leech, and J. Svartvik.1985.
A Comprehensive Grammar of the EnglishLanguage.
Longman.Swier, R. and S. Stevenson.
2004.
Unsupervised se-mantic role labelling.
In Proceedings of EMNLP,pages 95?102.Xue, N. and M. Palmer.
2004.
Calibrating features forsemantic role labeling.
In Proceedings of EMNLP,pages 88?94, Barcelona, Spain.672
