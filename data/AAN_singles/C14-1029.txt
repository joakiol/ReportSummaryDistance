Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 290?300, Dublin, Ireland, August 23-29 2014.Unsupervised Training Set Generationfor Automatic Acquisition of Technical Terminology in PatentsAlex Judea1Hinrich Schu?tze2So?ren Bru?gmann31Heidelberg Institute for Theoretical Studies, Heidelberg, Germany2Center for Information and Language Processing, University of Munich, Germany3Bru?gmann Software GmbH, Papenburg, GermanyAbstractNLP methods for automatic information access to rich technological knowledge sources likepatents are of great value.
One important resource for accessing this knowledge is the tech-nical terminology of the patent domain.
In this paper, we address the problem of automaticterminology acquisition (ATA), i.e., the problem of automatically identifying all technical termsin a document.
We analyze technical terminology in patents and define the concept of technicalterm based on the analysis.
We present a novel method for labeling large amounts of high-qualitytraining data for ATA in an unsupervised fashion.
We train two ATA methods on this trainingdata, a term candidate classifier and a conditional random field (CRF), and investigate the utilityof different types of features.
Finally, we show that our method of automatically generating train-ing data is effective and the two ATA methods successfully generalize, considerably increasingrecall while preserving high precision relative to a state-of-the-art baseline.1 IntroductionA large part of our technological knowledge is encoded in patents.
Methods for automatically findinginformation in patents and inferring information from patents are thus of great value.
An importantstep in getting access to patent information is identification of technical terminology, i.e., finding thelinguistic expressions that denote the technical concepts of a patent: the methods, processes, substancesand objects that are part of the invention or modified by it.
In the example ?The present inventionrelates to a charging apparatus of a bicycle dynamo?, the bolded compound nouns are the maincontent words and refer to specific technological concepts.
We call such linguistic expressions (technical)terms or terms and their totality the (technical) terminology of a document or domain.We address the task of automatic terminology acquisition (ATA), the task of finding technical termsin texts without reliance on existing resources that list terms of the domain.
In contrast to this standsautomatic terminology recognition (ATR), which we define as finding known terms and their variants(Jacquemin and Bourigault, 2003).
ATA provides input to downstream components like automatic sum-marization, machine translation, ontology building, information extraction and retrieval.
terms ex-tracted by ATA can be semantically classified or mapped to entries in a semantic database (Krauthammerand Nenadic, 2004), but we focus on identifying them without further classification in this paper.Our main contributions are as follows.
(i) We present a method for automatically labeling large amountsof training data for ATA.
(ii) We show that two types of statistical classifiers trained on this trainingdata beat a state-of-the-art baseline, indicating that the automatic labeling is of high quality.
(iii) Westudy different feature types for ATA and investigate how much they contribute to good performance.
Weinvestigate a semi-supervised setting in which features are selected based on a manually labeled evaluationset and a completely unsupervised setting where the feature selection is performed on an automaticallyproduced set.
(iv) Finally, we show that performance strongly depends on correct identification of theboundaries of terms and could be enhanced considerably by improving candidate identification.The paper is organized as follows.
Section 2 gives a definition of technical terminology and provides abrief analysis of terms in patents.
Section 3 presents related work.
Section 4 describes the architecture ofour ATA system: preprocessing, linguistic filtering, automatic labeling of training data, feature selectionand postprocessing.
Section 5 reports evaluation results and analyzes selected features and errors.
Section6 presents our conclusions.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers andproceedings footer are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/2902 Problem DescriptionLet w1...kbe a sequence of words w1, w2, .
.
.
, wkand wka head noun.
w1...kis a term of domain D iff(i) the head noun wkis unmodified (k = 1) or (for k > 1) is modified by sequences of other nouns (?diskcontroller?
), adjectives (?secondary controller?)
or present participles (?writing controller?)
and (ii) itdenotes a concept specific to D.(i) and (ii) describe the syntactic and semantic properties of a term, respectively.
Part (i) restrictsterms to parts of noun phrases.
This is a reasonable restriction that covers most technical terms(Daille et al., 1996) and it has been frequently made in the computational terminology literature.
Weexclude comparatives and superlatives as modifying adjectives because they are rarely used attributivelyin patents and usually modify quantities or qualities of terms(e.g., ?higher shunt currents?
); in otherwords, only ?positive?
(base-form) adjectives are included in our definition.
Note that the number oftokens per term is not restricted by the definition.
Our approach aims to find terms of arbitrary length.Part (ii) of the definition restricts terms to be specific to a domain D. We can set D to a generaldomain like ?electricity?
and be on a par with many prior definitions (Ananiadou, 1994; Georgantopoulosand Piperidis, 2000; Zhang and Fang, 2010), but we can also set D to a narrow domain like ?emergencyprotective circuit arrangements?
(IPC code1H02H).Here, we choose the most general technical domain possible: the domain of all technical subjects.
Thisis a good setting for many downstream tasks, e.g., information retrieval should benefit from a broadcoverage of D. It also makes annotation easier: Non-experts can carry it out with good agreement(Section 5.1) because they simply look for all technical expressions.The syntactic and semantic parts of our definition of term correspond to the concepts of unithood andtermhood , respectively.
Unithood is the degree to which a sequence of tokens is a linguistic unit; andtermhood the degree to which a linguistic unit is a term of a domain (Kageura and Umino, 1996).
Bothaspects have to be covered by ATA systems.Terms in PatentsIn addition to traditional terms like simple nouns (1, ?voltage?
), modified nouns (2, ?secondary arm?
)and nouns modified by prepositional phrases (3, ?trajectory of the lever?
), patents provide also coordina-tions (4, ?constant and variable current?)
and complex constructions (5, ?storage device storing a targettemperature value which a battery is intended to reach?
).For ATA, it seems advisable to exclude infrequent and complex nominal expressions from the definitionof term, both from a terminological and a computational point of view.
Most nominal expressions thatare generally viewed as terms are single nouns, compound nouns, and nouns with an adjectival modifier(Daille et al., 1996); our syntactic definition covers these three types.
Nominal expressions like (5)tend to be long; if we were to count such cases as terms, then it would be unclear where the termends.
When analyzing (5), our first take might be that there is a nucleus (?storage device?)
which ismodified by a verbal phrase (?storing a target temperature value?)
and that the rest of the phrase isnot part of the term.
But it turns out that the whole phrase appears multiple times in its patent; itis a stable way of denoting a part of the invention.
However, the underlying concept is also denoted bysimpler constructions like the nucleus itself, or synonymous terms like ?control circuit?
; these simplerconstructions are covered by our definition.Coordinations like (4) mix multiple concepts (here, ?constant current?
and ?variable current?)
withoutmaking this explicit on the surface.
It is difficult to identify ?constant current?
as a potential termbecause it is non-contiguous and is only indicated by an adjective.
Our treatment of coordinations inthis paper is to only consider sequences satisfying the syntactic definition (i.e., ?variable current?)
to beterms and discard other parts (i.e., ?constant?).
Of course, if both conjuncts are complete terms andsatisfy the syntactic definition, both will be identified as terms.Finally, prepositional phrases like (3) are rather infrequent compared to terms covered by our syntacticdefinition.
They also tend to be highly ambiguous and the underlying concept is often expressed by termscovered by our definition (?lever trajectory?
).3 Related WorkPrevious work on ATA either employs filtering or sequence models.
Filtering combines linguistic andstatistical criteria for (i) extracting a list of candidates (typically word n-grams) based on simple linguisticcriteria, (ii) computing candidate statistics and (iii) using ranking, classification or some other mechanismfor producing a pruned list of terms as output.
Because variation of the surface form of terms is limited,1wwwcms10.wipo.int/classifications/ipc/en/291it makes sense to use word n-grams as the basis for candidate identification ?
even though there are casesthat cannot be found this way, e.g., ?constant current?
or alternations like ?pressure regulating valve?vs.
?valve regulating pressure?.The main difference between ATA methods that rely on filtering is in how they accomplish the rank-ing/pruning of the candidate list.
See Kageura and Umino (1996), Jacquemin (2001) and Pazienza et al.
(2005) for an overview.
In this paper, we accomplish this by training a statistical model to classify termcandidates.
We also run experiments with a sequence model.
Our main innovation is that these modelsare trained on automatically labeled training data.It is difficult to directly compare computational terminology systems because of differences in domain,language, application and task definition.
As an example consider Takeuchi and Collier (2005) who reportan F1of .742.
However, their task definition includes assigning terms to pre-defined categories such asDNA and protein as opposed to simply identifying terms.
In addition, terminologies in the biomedicaland technological domains are different.
In biomedicine, categories like DNA and protein dominate.
Forthese terms, shape features are informative ?
in contrast to terms in patents.
Another difference isthat terms in patents tend to be long whereas DNA and proteins are often single-token abbreviations.3.1 Training Data CollectionOne of our main contributions is unsupervised training data generation (Section 4.3).
Prior work has usedautomatically recognized training data for computational terminology, specifically for ATR (Craven andKumlien, 1999; Hatzivassiloglou et al., 2001; Morgan et al., 2003; Zhang et al., 2010) in the biomedicaldomain.
Given large precompiled term lists they search for occurrences of list elements, e.g., genes, intexts and use the occurrences they find as training examples.
This is similar to distant supervision (Mintzet al., 2009) which also uses pre-existing resources such as gazetteers for, e.g., relation extraction.In contrast, our method is applied to ATA for the technological domain and does not rely on precompiledresources ?
we make use of figure references, which are an inherent part of patents.
Our method canbe characterized as training data identification: we exploit given conditions in patents for our search oftraining data.
In contrast, training data recognition methods need precompiled resources as input andsearch for instances of resource elements in texts.3.2 Learning Algorithms and FeaturesDifferent learning algorithms and feature sets have been used for computational terminology.
Foo andMerkel (2010) use Ripper (Cohen, 1995) with a variety of features to classify uni- and bigram termcandidates.
Hatzivassiloglou et al.
(2001) compare C4.5 (Quinlan, 1993) and Naive Bayes (Duda andHart, 1973).
Zhang et al.
(2010) acquire novel terms using CRFs and syntactic features.
Takeuchi andCollier (2005) find that more training data results in higher F scores.
Large training sets have the samepositive effect in our experiments.
Our approach has the added advantage that the training sets aregenerated completely automatically.4 ApproachAs discussed in the introduction, we address the problem of ATA.
We use the abbreviation ATAS (au-tomatic terminology acquisition system) to refer to our approach in general as well as to the specificimplementation we evaluate in this paper.ATAS consists of three parts: (i) training set generation, (ii) parameter selection and training of theterm candidate classifier (ATAS-TC) and the CRF (ATAS-CRF) and (iii) identification of terminologyin documents.Processing in step (iii) is document by document because some of our features are document-based.ATAS takes a document as input and identifies all terms in the document, using the term candidateclassifier or the CRF learned in (ii).The term candidate classifier (ATAS-TC) decides on entire (multi-token) candidates while the CRFdecides on single tokens.
ATAS-TC heavily relies on candidate computation and its decisions are mutuallyindependent, which is clearly incorrect.
In contrast, ATAS-CRF is less dependent on candidate compu-tation and models dependence of decisions correctly; but it lacks the more ?global?
view of ATAS-TC onentire candidates.
We want to investigate which approach is more suited for ATA.In what follows we describe how we preprocess patents, the linguistic filters used to implement oursyntactic definition of term, automatic labeling of training data (step (i) of ATAS), training of termcandidate classifier and CRF (step (ii) of ATAS), features and feature selection.2924.1 PreprocessingThe preprocessing pipeline consists of the ANNIE tokenizer, OpenNLP sentence splitter, Mate POStagger (Bohnet (2010), retrained for patents) and Mate lemmatizer.
Preprocessing has a big influenceon computational terminology because special domain text poses problems for off-the-shelf components.For example, patents tend to use common language words in rare functions or meanings, e.g., ?said?
asa de facto determiner in contexts such as ?the structure of said component?.
Other problems are theuse of special language words, e.g., substances like ?triphenylphosphine?
and acronyms like ?AC?.
Suchproperties pose serious problems to POS taggers.
Patent citations, acronyms and even product namescan include punctuation, confusing sentence splitters.
Chemical formulas may confuse tokenizers.We adapted our POS tagger and sentence splitter for patent language to deal with unusual punctuationand POS tags ?
especially unusual POS tags of common-language words like ?said?.
This adaptationinvolves training on a manually labeled training set of patent text and some other adjustments; e.g., weonly allow the tag NN for the acronyms ?AC?, ?DC?
and ?A/D?.4.2 FilterWe now describe how we find term candidates that satisfy the syntactic definition; recall that only(possibly modified) nouns can be terms (Section 2).In general, candidate identification strategies using linguistic knowledge perform better.
There are twodifferent strategies of this type: (i) parsing the sentence, extracting nominal chunks from the parse andfurther processing the nominal chunks and (ii) POS tagging the sentence and extracting word sequencesthat satisfy a set of predefined POS patterns.
Because many patent sentences are long and difficult toparse, we adopt the POS pattern approach in this paper.
To this end, we define two simple POS-basedrules for finding term candidates.2PREMODS.
This rule defines a modifier sequence.
It matches a sequence of noun pre-modifiers:(JJ|?/?|VBG|RB|N(N|P))*.3 We include RB because the POS tagger sometimes misclassifies JJ as RB.We include ?/?
because the tokenizer splits abbreviations containing it.CANDIDATE.
This rule defines a term candidate.
It matches either a single noun or PREMODSfollowed by a noun: (PREMODS N(N|P)).
The last noun must be longer than two characters.
Weadd a flag indicating if the candidate comes before a figure reference.
A figure reference consists of anoptional keyword (e.g., ?Figure?, ?Fig.?)
and a sequence of numbers and letters, optionally enclosed inparentheses.We select the longest match in case of overlapping matches and the first longest match in case ofoverlapping matches of the same length.These simple rules will find all terms ?
as well as many non-terms that we will train ATAS to identify ?with two exceptions.
First, due to POS errors some candidates are spurious.
Second, unwanted modifiersmay be part of candidates.
E.g., the rules will only identify ?same battery?
as a candidate and not?battery?.
But only ?battery?
is a valid term.
To address the latter, we manually compiled a stop list of67 modifiers, mostly numerals (?first?)
and adjectives in anaphoric function (?above-mentioned?).
Thesemodifiers are removed from term candidates.4.3 Automatic Labeling of Training DataWe view ATA as either a binary classification task where a term candidate classifier decides if a candidateis a term or not, or as a sequence labeling task where a CRF decides if a token (word) belongs to aterm or not.Large training sets are needed to train such models.
Usually, these sets are produced by expensivehuman labeling.
We present a method for generating high quality training data in an unsupervised waywithout the necessity of precompiled resources.
In principle, our method can be used for any languagefor which machine-readable patents are available.Our starting point is that patents typically contain figure references, i.e., pointers to drawings illus-trating the invention or its parts.
Consider the example: ?.
.
.
so that first clamp-holding secondaryarms (1) .
.
.
?
Here, the figure reference (?(1)?)
points to the illustration ?Figure 1?
and is preceded bythe illustrated term (?clamp-holding secondary arms?).
Illustrated terms may be concrete, as in thisexample, or abstract, e.g., a diagram illustrating properties of a method.We call a term candidate that precedes a figure reference a basic figure reference term candidate(bFRTC).
In a manual inspection of bFRTCs in 12 patents we found that almost 95% of bFRTCs were2JJ, VBG, and RB are POS tags for positive adjectives, gerunds/present participles, and adverbs, respectively.3?*?
is the Kleene star, ???
denotes optionality, and ?|?
denotes alternation.293terms.
Thus, bFRTCs can be used as positive training examples because they usually denote technicalconcepts; they have the advantage of being identifiable with high precision using simple patterns.Once the bFRTCs have been identified, there is a simple way to further increase the size of the trainingdata: we add all extended FRTCs (eFRTCs) to the training set, where we define an eFRTC as a termcandidate whose suffix is a bFRTC.
E.g., if we have identified ?shunt current?
as a bFRTC, then ?ACshunt current?
is an eFRTC.
eFRTCs typically are hyponyms since the modifiers added at the beginningrestrict the bFRTC to a more specific meaning.
This kind of hyponymy is a special case of term derivation,a modification where a base term is further specified by prefixes (Daille et al., 1996).
The strategy ofidentifying eFRTCs can also be applied to free word order languages because figure references tend tohave a local and fixed occurrence pattern similar to English.
We use the term FRTC to refer to bothbFRTCs and eFRTCs.We identify all FRTCs and add them as positive examples to the training set.
We also add the 5%most frequent candidates as positive examples; most of them are FRTCs, so that this step usually addsfew new training examples.We label the following candidates as negative training examples: candidates appearing only once ina patent; patent citations; and measurements.
Citations and measurements (?3 cm?)
are clear non-terms.
We identify them using regular expressions.
Many singletons are non-terms because they denotecommon language (i.e., nontechnical) concepts, e.g., ?time?.
These heuristics for finding negative trainingexamples are not applied to a candidate if it has the same head as a positive training example.We exclude from the training set candidates that do not satisfy any positive or negative criteria.4.4 ClassifiersWe use the L2-regularized logistic regression of LIBLINEAR (Fan et al., 2008) as our term candidateclassifier.
We use LIBLINEAR?s default normalization for continuous-valued attributes (normalizationto range [0, 1]) and the default representation for categorical attributes.
As LIBLINEAR cannot handlemissing values, we replace them with their means and modes.
We set the regularization parameter c = 1.Our sequence model is CRF++4, order 1, with default parameters.
The CRF features are adapted fromthe ATAS-TC features, e.g., term-level features (e.g., TFIDF) are propagated down to the individualtokens of the term.
We also include word trigrams.
We discretize numeric features to three values.4.5 FeaturesWe developed a set of 74 features for ATA.
Some of these features are taken from the literature, someare specific to our approach and make use of the concept of FRTC and some exploit other properties ofpatents (e.g., the importance of the title and the claims in patents).
A final group consists of other novelfeatures that we designed in the course of developing our system.
We now provide an overview.
c refersto a term candidate.Corpus and document statistics.
This feature type captures termhood and unithood of c as well asthe position of c?s first occurrence in the document.
We use a corpus of technical text CTand a generallanguage corpus CG.
For every c ?
CTwe collect the number of patents it appears in, its frequencyand its FRTC frequency, i.e., the number of its occurrences that are FRTCs.
Features that are intendedto indicate termhood include simple frequencies and distributional characteristics (in CTor in a singlepatent).
Finally, we define a measure of frequency deviation (or ?keywordness?)
of h(c), the head of c:bias(h(c)) =fCG(h(c))|CG||CT| ?
fCT(h(c))fCG(resp., fCT) are the frequencies in CG(resp., CT), |X| is the sum of frequencies of all x ?
X.bias(h(c)) measures the deviation between expected frequency of the head of c (estimated on CG) andits actual frequency.
The intuition here is that the frequency of a general language noun like ?time?
willbe similar over text types, resulting in a lower bias.Context.
This feature type captures unigrams and bigrams adjacent to c as well as their POS tags.Part-of-speech.
This feature type captures the POS sequence of c.A patent usually focuses on a narrow technological subdomain.
As a result, many of its terms aresemantically related to each other.
We would like to include features that directly capture semantic simi-larity to other terms because a candidate that is semantically similar to several other already recognizedterms is likely to be a term itself.Our goal in this paper is to address ATA using simple and efficient methods.
For this reason, weapproximate semantic similarity using string similarity because a subset of semantically similar terms are4crfpp.googlecode.com294TutdgTltestTldevTuselpatents 365 5 11 25word tokens 3,422,131 50,007 74,000 152,715word types 292,994 3711 7391 4141bFRTCs 119,316 1264 2558 6503FRTCs 240,240 2371 4942 10,110candidates 353,238 8836 13,099 27,164terms 3814 7220Table 1: Data set statisticsP R F1description1 .704 .797 .748 mean string similarity of c and FRTCs2 .712 .832 .767 frequency of c as an FRTC in CT3 .694 .887 .779 TFIDF of c4 .703 .888 .784 is c uppercase?5 .708 .893 .790 is c followed by a figure reference?6 .710 .896 .792 TFIDF of h(c)7 .711 .895 .793 frequency of h(c) as an FRTC in CT8 .718 .892 .795 bias(h(c))9 .720 .891 .797 # sentences with FRTCs that c occurs in10 .720 .893 .797 C-value of c11 .721 .893 .798 frequency of h(c) in CGTable 2: Features selected on Tldev(setting S).
c: term candidate.
h(c): head of calso similar on the surface.
E.g., the semantic similarity between ?AC power supply source?
and ?ACsupply source?
also manifests itself as string similarity.String similarity.
When designing a similarity measure, we wanted it to satisfy the following criteria:(i) more words in common should result in higher scores and (ii) words in common towards the end of thetwo strings should be weighted higher than words in common at the beginning.
The motivation for (ii)is that candidates differing only in initial modifiers are often cohyponyms and highly related; conversely,candidates with different heads are often not related.To implement this, we represent a candidate c as a vector ~c in |V |-dimensional space where V is thevocabulary.
~ciis set to the position of word wiin c if it occurs and 0 otherwise.
The string similaritybetween c and c?is then defined as the cosine of ~c and~c?.
Example: for ?AC power supply source?
and?AC supply source?, we get the vectors (1, 2, 3, 4) and (1, 0, 2, 3) and the cosine .927; comparing the firststring with ?AC power supply?
with the vector (1, 2, 3, 0) we get the cosine .683.Features in our initial set of 74 that make use of this semantic similarity are: maximum similarity ofc to any FRTC, average similarity of c to all FRTCs in the patent and similarity of c to the rightmostterm candidate in the title.Frantzi and Ananiadou (1997) define C-value(c) as:C-value(c) = log2|c|(f(c)?1|Tc|?b?Tcf(b))where Tcis the set of term candidates containing c and f is frequency in CT. C-value is high for termcandidates that are frequent and occur as parts of many other term candidates ?
this is a good indicatorof termhood.5 Experiments and Evaluation5.1 Data SetsWe hired three students with a bachelor degree in computer science to annotate 16 patents.
The testset Tltestconsists of 5 patents annotated by all three students.
We used majority voting to produce thefinal gold annotations.
The devset Tldevconsists of the remaining 11 patents.
Each Tldevsentence wasannotated by one student.Inter-annotator agreement on Tltestwas .76 (Fleiss?
?).
Most disagreements concern modifiers orcommon nouns (e.g., the term ?battery?
was often not annotated).
More extensive training of theannotators should reduce these problems considerably.As unlabeled data we randomly selected 390 technology patents.
We use 365 as Tutdgfor training datageneration and 25 as Tuselfor unsupervised feature selection.
We made sure the 390 documents are not295in Tldevand Tltest.
We excluded chemical patents because standard preprocessing components often failfor chemical formulas.
Table 1 gives data set statistics.As our technical corpus CTwe use Tutdgand as our general corpus CGall nouns in the 2000 mostfrequent English words from Project Gutenberg5.
This list contains many general nouns which alsoappear in patents (e.g., ?time?)
without containing many technical terms (e.g., ?battery?
); this way, CTand CGgive us a good contrast between technical and non-technical vocabularies (cf.
Section 4.5).One obstacle to comparing systems for ATA in the technical domain is the lack of publicly availableevaluation benchmarks.
We are making our data sets and the annotation guidelines available6.5.2 BaselinesWe define the FRTC baseline as the system that labels all FRTCs and only FRTCs as terms.
Almostall FRTCs are terms, but many terms are not FRTCs; thus, the FRTC baseline has high precision andlow recall.
Our goal is to preserve high precision while considerably increasing recall, or to generalizewell from FRTCs to other terms.Our state of the art baseline is Z-CRF, a reimplementation of the CRF described in (Zhang et al.,2010).
Its feature representation includes POS tags, unigrams, bigrams and syntactic information, e.g.,the number of times a particular token is used in a syntactic function like subject in the training set.Syntactic information is extracted with Mate (Bohnet, 2010).
Z-CRF is trained on Tutdg, just as ATAS.Our last baseline is the well-known C-value (Frantzi and Ananiadou, 1997).
Like our first baseline,it needs no training data.
In contrast to our first baseline, it was specifically designed for terminologyacquisition.
It combines observations about statistical and linguistic properties of terms, i.e., a candidateis preferred as a term if it is long and frequently appears as substring of other candidates.
FollowingFrantzi and Ananiadou (1997) we regard a candidate as term if its C-value it not zero; unlike them, wedo not restrict the length of terms because the computation of long terms did not pose computationalproblems for us.5.3 Evaluation SetupWe evaluate ATAS using precision, recall and F1.
Evaluation is based on candidate tokens (as opposedto candidate types or word tokens); e.g., each instance of a candidate term that is incorrectly classifiedas a term is a false positive.
Evaluation is strict in the sense that a term is counted as a false positiveif there is a single token that is added or missed.We evaluate ATAS in two settings.
In the system (S) setting, the ATAS pipeline described in Section4 (ATAS-TC or ATAS-CRF) is used to identify term candidates.
This is the real-world setting sinceerrors in term candidate identification ?
misplaced boundaries, missing candidates, etc.
?
are a majorsource of error in ATA.We would also like to evaluate candidate classification on gold boundaries (manually verified boundariesof term candidates); this allows us to quantify by how much performance can be improved if candidateidentification is perfect.
However, since gold boundary annotation is expensive, we instead approximatedit: (i) We run automatic term candidate identification.
(ii) We remove all term candidates that overlapwith gold (manually annotated) terms.
(iii) The set of gold term candidates is then the union of allremaining automatically identified candidates and the manually annotated terms.In the gold boundary (G) setting, we provide these gold term candidates to the ATAS pipeline.
Thisallows us to evaluate the performance of term/non-term classification separately from term candidateidentification.5.4 Feature SelectionFor our feature set of 74, we perform forward feature selection for the term candidate classifier byselecting the feature in each step that maximizes system F1.
We perform feature selection (i) on themanually labeled set Tldev(to gauge performance for an optimal or close-to-optimal feature set) and (ii)on the automatically labeled set Tusel(to gauge the performance in a completely unsupervised setting).In the following we explain both settings in more detail.Table 2 gives the features selected in supervised feature selection, i.e., when features are optimizedon Tldev.
Precision remains stable, except for a drop on line 3.
Recall rises steadily from .797 to .893.
F1increases from .748 to .798.The best feature (line 1) is the mean string similarity of a term candidate c to all FRTCs in a document(Section 4.5).
Together with the next best feature (frequency of c as an FRTC in CT) and feature 5 (is5en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/1-100006h-its.org/english/research/nlp/download/terminology.php296ATAS-TC ATAS-CRF BaselinesS-SEL U-SEL S-SEL U-SEL Z-CRF C-value FRTCS G S G S G S G S G S G S GTl dev1 P .721 .838 .690 .796 .732 .844 .727 .854 .867 .891 .384 .749 .839 1.0002 R .893 .892 .825 .818 .815 .699 .755 .679 .563 .607 .292 .355 .344 .3533 F1.798 .864 .752 .807 .771 .765 .741 .756 .683?.722?.314?.471?.488?.522?Tl test4 P .696 .753 .627 .692 .774 .832 .664 .745 .813 .840 .388 .726 .864 1.0005 R .850 .853 .764 .764 .791 .743 .644 .625 .516 .559 .320 .410 .286 .3026 F1.765 .800 .689 .728 .783 .785 .654 .680 .631?.674?.350?.519?.430?.465?Table 3: System (S) and gold boundary (G) results with supervised (S-SEL) and unsupervised (U-SEL)feature selection.?
: significantly lower than corresponding ATAS-TC and ATAS-CRF scores.c followed by a figure reference?)
this supports our intuition for using FRTCs for automatic training setgeneration because they are indeed strong indicators for termness.
Additionally, feature 9 indicates thatcandidates occurring often with FRTCs in sentences are probably terms.
Feature 4 (is c uppercase?)
isselected because uppercase term candidates are often abbreviations and terms.Feature 3 (TFIDF of c) hurts precision, but increases recall, resulting in increased F1.
This featuremodels the hypothesis that a term is frequent in some patents but does not occur in many patents.Patent writers often invent novel terms rather than using standard ones to make finding a patent hard.Thus, a term candidate that occurs often in a few patents could be such an obfuscating term.TFIDF is low for terms with small term frequency.
Features 6 (TFIDF of h(c)) and 10 (C-value of c)can help correctly identify such term candidates as terms.Features 8 and 11 incorporate information from the general purpose corpus CG.
Feature 8 contraststhe frequency of c in CGwith its frequency in CT?
frequencies of terms are higher in CT, frequencies ofnon-terms are similar in both corpora.
Feature 11 is complementary to this.
It makes it more probablethat c is a non-term if its head appears more often in CG.
Additionally, string similarity with the patent?stitle is an effective feature.Unsupervised feature selection, i.e., selection on Tusel, selected seven features that are similarto those selected by supervised selection and that we will discuss now.
The best unsupervised feature(maximum string similarity, 1) and the best supervised feature (mean string similarity) both capturepartial string overlap of c and FRTCs.
For similar reasons, the feature ?string similarity of c and rightmostNP in patent title?
(2) ?
which exploits the importance of the title in analogy to the importance of figurereferences ?
is selected.Other selected features (relative patent frequency of c and its head (3, 4), number of patent sentencesin which c occurs with FRTCs (5), patent frequency of c = 1?
(6)) are also similar to the features selectedin the supervised setting.
They capture frequency distributions of c. However, while many features inthe supervised setting capture distributions of c in CT, in the unsupervised setting, distributions of c inthe patent are more important.
The reason may be that CT-based features (which use all technical textas opposed to the relevant patent in question) are harder to recognize as good predictors if the set usedfor selection is automatically labeled and hence noisier.The last unsupervised feature captures the length of c in tokens (7).
Manual inspection revealed thaton average terms have more tokens than non-terms (1.9 vs. 1.3).5.5 ATAS ResultsTable 3 gives evaluation results for ATA on Tldevand Tltest.
We report results for the ATAS versions(ATAS-TC, ATAS-CRF) and for the baselines (Z-CRF, C-value, FRTC) as well as for using supervised(S-SEL) and unsupervised feature selection (U-SEL) in system setting (S) and gold boundary setting(G).Differences in F1between ATAS and baselines (marked with a ?)
are significant at p < .01.7If notstated otherwise, numbers below are for the system setting (S).We note that F1of the ATAS versions is consistently and considerably better than all baselines inall settings.
E.g., line 6 shows system F1on Tltestof ATAS-TC (.765 for S-SEL, .689 for U-SEL) andATAS-CRF (.783 for S-SEL, .654 for U-SEL) compared to Z-CRF (.631), FRTC (.430), and the C-valuebaseline (.350) .
The better results mainly come from higher recall (except for C-value, which is alsobeaten in precision).
In general, precision of the baselines is higher, but recall much smaller than forATAS.
This shows that (i) statistical classifiers can be successfully trained for ATA using our method7We use approximate randomization (Yeh, 2000) for all significance tests in this paper.297Figure 1: System F1as a function of training set size (in percent) in setting G.for automatically generating training data and (ii) these classifiers beat a state-of-the-art system in bothS-SEL and U-SEL settings.Comparing S-SEL and U-SEL shows that precision and recall for U-SEL are lower than for S-SEL.
Forinstance, F1of ATAS-TC on Tltestis .765 for S-SEL and .689 for U-SEL; F1of ATAS-CRF is .783 forS-SEL and .654 for U-SEL (line 6).
In general, we note a bigger drop in recall than in precision, indicatingthat U-SEL does not generalize as well as S-SEL.
However, the U-SEL numbers are significantly betterthan the Z-CRF FRTC, and C-value baselines.When comparing ATAS-TC with ATAS-CRF we note that ATAS-CRF consistently has higher precisionand lower recall.
In most cases, ATAS-TC has considerably higher recall, leading to higher F1.
This isnot surprising given that feature selection was performed for ATAS-TC.
Nevertheless, ATAS-CRF cancompete with ATAS-TC in terms of F1.
Furthermore, ATAS-CRF produces more stable results becauseit shows less variance in F1across settings.Comparing S and G scores shows that knowing exact boundaries has a great impact on results, especiallyon precision; looking at S-SEL numbers in line 4 in Table 3, precision for ATAS-TC (resp., ATAS-CRF)is .696 in S vs. .753 in G (resp., .774 in S vs. .832 in G).
Similar differences also hold for U-SEL numbers.In general, ATAS-TC profits more from knowing exact boundaries than ATAS-CRF.
This leads us to theconclusion that the linguistic filter would greatly benefit from a (statistical) measure of unithood.
Notethat this also holds for the baselines; deciding about the termness of gold boundary candidates seems tobe easier, especially for C-value.All observations hold for Tldevand Tltest.
However, numbers are higher for Tldevbecause the ratio ofFRTCs to candidates is higher than for Tltest(38% vs. 27%) which improves classification performanceon Tldev?
this holds for ATAS as well as for the baselines.To investigate the quality of the extracted training data, consider Figure 1.
It shows F1in setting Gas a function of training set size in percent of the total training set Tutdg.
For each evaluation point,we randomly add training examples from the full set.
F1starts at .834 for 0.1% of training data (344training examples) and rises to .864 for 100% (353,238 examples), with a small drop at 50%.
Note that1000 examples roughly correspond to one annotated patent.
The main results of this experiment are that(i) a modest amount of automatically labeled training data gives good performance and (ii) the moreautomatically labeled data the better.
The last point is not a trivial finding, given that training data wasgenerated automatically.
The logarithmic graph shows a nearly linear increase in F1for each doubling ofthe training data.To further investigate the quality of the generated training data, we compared automatically andmanually produced training examples.
We compare results for 13238 manual and 13238 automatic labels(setting G, ATAS-TC).
We get precision and recall of .811 and .805 for manual and .762 and .850 forautomatic annotations, resulting in similar F1 scores: .808 vs. .804 for manual and automatic annotations,respectively.
We believe that the differences in recall are an artifact of the randomization we performedbefore removing automatic training samples.
Manual labels are entire patents; in contrast, automaticlabels come from all patents in the training set, leaving us with a more diverse set than the manualversion.5.6 Error AnalysisWe found two major types of false negatives.
First, infrequent terms are problematic.
It is hard to judgetermness when having limited information about a candidate, especially if it appears only once or twicein a document.
Second, POS errors prevent the system from finding some candidates; e.g., the noun?current?
is frequently mistagged as adjective.
Incorrect POS tags also lead to incorrect boundaries.298We found four major types of false positives.
First, incorrect modifiers lead to partially incorrectterms.
27% of false positives are of this type.
Second, incorrectly recognized figure references causeincorrect system decisions; e.g., our patterns incorrectly parse an expression like ?value PBA?
as a figurereference even though it is instead a named output of a component.
Third, very frequent non-terms arecommonly classified as terms.
Almost all frequent candidates are terms, so that the term candidateclassifier has difficulty correctly identifying the exceptions from this pattern.Finally, if a candidate is a term in one context it may be a non-term in another.
A good examplefor this are general single token terms like ?apparatus?.
Before figure references they are terms, e.g.,?one preferred form of apparatus 22?.
In such cases the figure reference serves as a disambiguator.However, in other positions they are non-terms, e.g., ?They include braces, collars, splints and othersimilar apparatus?.6 Conclusion and Future WorkThis paper introduces a method for ATA with two novel aspects: (i) new powerful features for ATAand (ii) a procedure for generating an ATA training set in an unsupervised fashion.
The training setgeneration method produces high quality training data, even when compared to manual annotations.
It islanguage-independent: It can be applied to patents in any language if the definition of term candidatesis modified for the target language.
It is also domain-independent: it can be applied to patents ofany domain.
The training data can be successfully used to train ATA models, both term candidateclassification as well as CRF models.
Even in a completely unsupervised setting the models outperform astate-of-the-art baseline.
We found that using more automatically labeled training data and using betterterm boundaries results in better performance.In future work, we plan to incorporate term variation patterns (Daille et al., 1996; Jacquemin, 2001)in the expansion process to decrease the number of FNs and increase recall.
We would also like toimprove the terminology identification module because we found that incorrect identified boundariesaffect performance greatly.Finally, we are planning to extend our approach to languages other than English.
Our methods arelanguage-independent to the extent that a body of patents exists for many common languages.
Sincewe generate the training set automatically, all we need to do to cover another language is to adapt thelinguistic filters for candidate identification.Acknowledgments.
This work was supported by the European Union (Project Topas, FP7-SME-2011 286639) and by SPP 1335 Scalable Visual Analytics of Deutsche Forschungsgemeinschaft (DFGgrant SCHU 2246/8-2).
We would like to thank the anonymous reviewers for their helpful comments andsuggestions, and Bianca and Luca for their support.ReferencesSophia Ananiadou.
1994.
A methodology for automatic term recognition.
In Proceedings of the 15thconference on Computational linguistics - Volume 2, COLING ?94, pages 1034?1038.Bernd Bohnet.
2010.
Top Accuracy and Fast Dependency Parsing is not a Contradiction.
In Proceedingsof the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89?97, Beijing,China, August.William W. Cohen.
1995.
Fast effective rule induction.
In Twelft International Conference on MachineLearning (ML95), pages 115?123.Mark Craven and Johan Kumlien.
1999.
Constructing biological knowledge bases by extracting infor-mation from text sources.
In Thomas Lengauer, Reinhard Schneider, Peer Bork, Douglas L. Brutlag,Janice I. Glasgow, Hans-Werner Mewes, and Ralf Zimmer, editors, ISMB, pages 77?86.
AAAI.Be?atrice Daille, Beno?
?t Habert, Christian Jacquemin, and Jean Royaute?.
1996.
Empirical Observationof Term Variations and Principles for their Description.
Terminology, 3(2):197?258.Richard O. Duda and Peter E. Hart.
1973.
Pattern Classification and Scene Analysis.
John Wiley &Sons Inc, 1 edition.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal of Machine Learning Research, 9:1871?1874.299Jody Foo and Magnus Merkel.
2010.
Using machine learning to perform automatic term recognition.
InProceedings of the LREC 2010 Workshop on Methods for automatic acquisition of Language Resourcesand their Evaluation Methods, pages 49?54.Katerina T. Frantzi and Sophia Ananiadou.
1997.
Automatic Term Recognition using Contextual Cues.In Proceedings of 3rd DELOS Workshop, Zurich, Switzerland.Byron Georgantopoulos and Stelios Piperidis.
2000.
Term-based Identification of sentences for Text Sum-marisation.
In Proceedings of Second International Conference on Language Resources and Evaluation(LREC2000), pages 1067?1070, Athens, Greece.Vasileios Hatzivassiloglou, Pablo Ariel Dubou, and Andrey Rzhetsky.
2001.
Disambiguating proteins,genes, and rna in text: a machine learning approach.
In ISMB (Supplement of Bioinformatics), pages97?106.Christian Jacquemin and Didier Bourigault.
2003.
Term extraction and automatic indexing.
In RuslanMitkov, editor, The Oxford Handbook of Computational Linguistics, chapter 33.
Oxford UniversityPress.Christian Jacquemin.
2001.
Spotting and Discovering Terms Through Natural Language Processing.
MITPress, April.Kyo Kageura and Bin Umino.
1996.
Methods of automatic term recognition: A review.
Terminology,3(2):259?289.Michael Krauthammer and Goran Nenadic.
2004.
Term identification in the biomedical literature.Journal of Biomedical Informatics, 37(6):512?526, December.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009.
Distant supervision for relation extractionwithout labeled data.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume2-Volume 2, pages 1003?1011.
Association for Computational Linguistics.Alex Morgan, Lynette Hirschman, Alexander Yeh, and Marc Colosimo.
2003.
Gene Name ExtractionUsing FlyBase Resources.
In Proceedings of ACL 2003 Workshop on Natural Language Processing inBiomedicine, pages 1?8, Sapporo, Japan.Maria Teresa Pazienza, Marco Pennacchiotti, Michele Vindigni, and Fabio Massimo Zanzotto.
2005.Ai/nlp technologies applied to spacecraft mission design.
In Proceedings of the 18th internationalconference on Innovations in Applied Artificial Intelligence, IEA/AIE?2005, pages 239?248, London,UK, UK.John Ross Quinlan.
1993.
C4.5: programs for machine learning.
Morgan Kaufmann Publishers Inc., SanFrancisco, CA, USA.Koichi Takeuchi and Nigel Collier.
2005.
Bio-medical entity extraction using support vector machines.Artificial Intelligence in Medicine, 33(2):125?137, February.Alexander Yeh.
2000.
More accurate tests for the statistical significance of result differences.
In Pro-ceedings of the 18th conference on Computational linguistics - Volume 2, COLING ?00, pages 947?953,Stroudsburg, PA, USA.Xing Zhang and Alex Chengyu Fang.
2010.
An ATE system based on probabilistic relations betweenterms and syntactic functions.
In 10th International Conference on Statistical Analysis of TextualData, pages 1135?1143, Sapienza, Italy, June.Xing Zhang, Yan Song, and Alex Chengyu Fang.
2010.
How well conditional random fields can be used innovel term recognition.
In Proceedings of the 24th Pacific Asia Conference on Language, Informationand Computation, pages 583?592, Tohoku University, Sendai, Japan, November.300
