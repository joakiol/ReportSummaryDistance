Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 469?477,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Statistical Model for Unsupervised and Semi-supervised TransliterationMiningHassan Sajjad Alexander Fraser Helmut SchmidInstitute for Natural Language ProcessingUniversity of Stuttgart{sajjad,fraser,schmid}@ims.uni-stuttgart.deAbstractWe propose a novel model to automaticallyextract transliteration pairs from parallel cor-pora.
Our model is efficient, language pairindependent and mines transliteration pairs ina consistent fashion in both unsupervised andsemi-supervised settings.
We model transliter-ation mining as an interpolation of translitera-tion and non-transliteration sub-models.
Weevaluate on NEWS 2010 shared task data andon parallel corpora with competitive results.1 IntroductionTransliteration mining is the extraction of translit-eration pairs from unlabelled data.
Most transliter-ation mining systems are built using labelled train-ing data or using heuristics to extract transliterationpairs.
These systems are language pair dependent orrequire labelled information for training.
Our sys-tem extracts transliteration pairs in an unsupervisedfashion.
It is also able to utilize labelled informationif available, obtaining improved performance.We present a novel model of transliteration min-ing defined as a mixture of a transliteration modeland a non-transliteration model.
The transliterationmodel is a joint source channel model (Li et al,2004).
The non-transliteration model assumes nocorrelation between source and target word charac-ters, and independently generates a source and a tar-get word using two fixed unigram character models.We use Expectation Maximization (EM) to learn pa-rameters maximizing the likelihood of the interpola-tion of both sub-models.
At test time, we label wordpairs as transliterations if they have a higher proba-bility assigned by the transliteration sub-model thanby the non-transliteration sub-model.We extend the unsupervised system to a semi-supervised system by adding a new S-step to theEM algorithm.
The S-step takes the probability es-timates from unlabelled data (computed in the M-step) and uses them as a backoff distribution tosmooth probabilities which were estimated from la-belled data.
The smoothed probabilities are thenused in the next E-step.
In this way, the parame-ters learned by EM are constrained to values whichare close to those estimated from the labelled data.We evaluate our unsupervised and semi-supervised transliteration mining system on thedatasets available from the NEWS 2010 shared taskon transliteration mining (Kumaran et al, 2010b).We call this task NEWS10 later on.
Compared witha baseline unsupervised system our unsupervisedsystem achieves up to 5% better F-measure.
Onthe NEWS10 dataset, our unsupervised systemachieves an F-measure of up to 95.7%, and on threelanguage pairs, it performs better than all systemswhich participated in NEWS10.
We also evaluateour semi-supervised system which additionally usesthe NEWS10 labelled data for training.
It achievesan improvement of up to 3.7% F-measure over ourunsupervised system.
Additional experiments onparallel corpora show that we are able to effectivelymine transliteration pairs from very noisy data.The paper is organized as follows.
Section 2 de-scribes previous work.
Sections 3 and 4 define ourunsupervised and semi-supervised models.
Section5 presents the evaluation.
Section 6 concludes.4692 Previous WorkWe first discuss the literature on semi-supervisedand supervised techniques for transliteration min-ing and then describe a previously defined unsuper-vised system.
Supervised and semi-supervised sys-tems use a manually labelled set of training data tolearn character mappings between source and tar-get strings.
The labelled training data either con-sists of a few hundred transliteration pairs or ofjust a few carefully selected transliteration pairs.The NEWS 2010 shared task on transliteration min-ing (NEWS10) (Kumaran et al, 2010b) is a semi-supervised task conducted on Wikipedia InterLan-guage Links (WIL) data.
The NEWS10 dataset con-tains 1000 labelled examples (called the ?seed data?
)for initial training.
All systems which participatedin the NEWS10 shared task are either supervised orsemi-supervised.
They are described in (Kumaranet al, 2010a).
Our transliteration mining modelcan mine transliterations without using any labelleddata.
However, if there is some labelled data avail-able, our system is able to use it effectively.The transliteration mining systems evaluated onthe NEWS10 dataset generally used heuristic meth-ods, discriminative models or generative models fortransliteration mining (Kumaran et al, 2010a).The heuristic-based system of Jiampojamarn etal.
(2010) is based on the edit distance methodwhich scores the similarity between source and tar-get words.
They presented two discriminative meth-ods ?
an SVM-based classifier and alignment-basedstring similarity for transliteration mining.
Thesemethods model the conditional probability distribu-tion and require supervised/semi-supervised infor-mation for learning.
We propose a flexible genera-tive model for transliteration mining usable for bothunsupervised and semi-supervised learning.Previous work on generative approaches usesHidden Markov Models (Nabende, 2010; Darwish,2010; Jiampojamarn et al, 2010), Finite State Au-tomata (Noeman and Madkour, 2010) and Bayesianlearning (Kahki et al, 2011) to learn transliterationpairs from labelled data.
Our method is differentfrom theirs as our generative story explains the un-labelled data using a combination of a transliterationand a non-transliteration sub-model.
The translit-eration model jointly generates source and targetstrings, whereas the non-transliteration system gen-erates them independently of each other.Sajjad et al (2011) proposed a heuristic-based un-supervised transliteration mining system.
We latercall it Sajjad11.
It is the only unsupervised miningsystem that was evaluated on the NEWS10 datasetup until now, as far as we know.
That system is com-putationally expensive.
We show in Section 5 that itsruntime is much higher than that of our system.In this paper, we propose a novel model-basedapproach to transliteration mining.
Our approachis language pair independent ?
at least for alpha-betic languages ?
and efficient.
Unlike the pre-vious unsupervised system, and unlike the super-vised and semi-supervised systems we mentioned,our model can be used for both unsupervised andsemi-supervised mining in a consistent way.3 Unsupervised Transliteration MiningModelA source word and its corresponding target word canbe character-aligned in many ways.
We refer to apossible alignment sequence which aligns a sourceword e and a target word f as ?a?.
The functionAlign(e, f) returns the set of all valid alignment se-quences a of a word pair (e, f).
The joint transliter-ation probability p1(e, f) of a word pair is the sumof the probabilities of all alignment sequences:p1(e, f) =?a?Align(e,f)p(a) (1)Transliteration systems are trained on a list oftransliteration pairs.
The alignment between thetransliteration pairs is learned with ExpectationMaximization (EM).
We use a simple unigrammodel, so an alignment sequence from functionAlign(e, f) is a combination of 0?1, 1?1, and 1?0 character alignments between a source word e andits transliteration f .
We refer to a character align-ment unit as ?multigram?
later on and represent itby the symbol ?q?.
A sequence of multigrams formsan alignment of a source and target word.
The prob-ability of a sequence of multigrams a is the productof the probabilities of the multigrams it contains.p(a) = p(q1, q2, ..., q|a|) =|a|?j=1p(qj) (2)470While transliteration systems are trained on aclean list of transliteration pairs, our translitera-tion mining system has to learn from data con-taining both transliterations and non-transliterations.The transliteration model p1(e, f) handles only thetransliteration pairs.
We propose a second modelp2(e, f) to deal with non-transliteration pairs (the?non-transliteration model?).
Interpolation with thenon-transliteration model allows the transliterationmodel to concentrate on modelling transliterationsduring EM training.
After EM training, transliter-ation word pairs are assigned a high probability bythe transliteration submodel and a low probability bythe non-transliteration submodel, and vice versa fornon-transliteration pairs.
This property is exploitedto identify transliterations.In a non-transliteration word pair, the charactersof the source and target words are unrelated.
Wemodel them as randomly seeing a source word and atarget word together.
The non-transliteration modeluses random generation of characters from two uni-gram models.
It is defined as follows:p2(e, f) = pE(e) pF (f) (3)pE(e) =?|e|i=1 pE(ei) and pF (f) =?|f |i=1 pF (fi).The transliteration mining model is an interpo-lation of the transliteration model p1(e, f) and thenon-transliteration model p2(e, f):p(e, f) = (1?
?
)p1(e, f) + ?p2(e, f) (4)?
is the prior probability of non-transliteration.3.1 Model EstimationIn this section, we discuss the estimation of the pa-rameters of the transliteration model p1(e, f) and thenon-transliteration model p2(e, f).The non-transliteration model consists of two un-igram character models.
Their parameters are esti-mated from the source and target words of the train-ing data, respectively, and the parameters do notchange during EM training.For the transliteration model, we implement asimplified form of the grapheme-to-phoneme con-verter, g2p (Bisani and Ney, 2008).
In the follow-ing, we use notations from Bisani and Ney (2008).g2p learns m-to-n character alignments between asource and a target word.
We restrict ourselves to0?1,1?1,1?0 character alignments and to a unigrammodel.1 The Expectation Maximization (EM) algo-rithm is used to train the model.
It maximizes thelikelihood of the training data.
In the E-step the EMalgorithm computes expected counts for the multi-grams and in the M-step the multigram probabilitiesare reestimated from these counts.
These two stepsare iterated.
For the first EM iteration, the multigramprobabilities are initialized with a uniform distribu-tion and ?
is set to 0.5.The expected count of a multigram q (E-step) iscomputed by multiplying the posterior probabilityof each alignment a with the frequency of q in a andsumming these weighted frequencies over all align-ments of all word pairs.c(q) =N?i=1?a?Align(ei,fi)(1?
?
)p1(a, ei, fi)p(ei, fi)nq(a)nq(a) is here the number of times the multigram qoccurs in the sequence a and p(ei, fi) is defined inEquation 4.
The new estimate of the probability of amultigram is given by:p(q) =c(q)?q?
c(q?
)(5)Likewise, we calculate the expected count of non-transliterations by summing the posterior probabili-ties of non-transliteration given each word pair:cntr =N?i=1pntr(ei, fi) =N?i=1?p2(ei, fi)p(ei, fi)(6)?
is then reestimated by dividing the expected countof non-transliterations by N .3.2 Implementation DetailsWe use the Forward-Backward algorithm to estimatethe counts of multigrams.
The algorithm has a for-ward variable?
and a backward variable ?
which arecalculated in the standard way (Deligne and Bimbot,1995).
Consider a node r which is connected witha node s via an arc labelled with the multigram q.The expected count of a transition between r and sis calculated using the forward and backward prob-abilities as follows:?
?rs =?
(r) p(q) ?(s)?
(E)(7)1In preliminary experiments, using an n-gram order ofgreater than one or more than one character on the source side orthe target side or both sides of the multigram caused the translit-eration model to incorrectly learn non-transliteration informa-tion from the training data.471where E is the final node of the graph.We multiply the expected count of a transitionby the posterior probability of transliteration (1 ?pntr(e, f)) which indicates how likely the string pairis to be a transliteration.
The counts ?rs are thensummed for all multigram types q over all trainingpairs to obtain the frequencies c(q) which are usedto reestimate the multigram probabilities accordingto Equation 5.4 Semi-supervised Transliteration MiningModelOur unsupervised transliteration mining system canbe applied to language pairs for which no labelleddata is available.
However, the unsupervised sys-tem is focused on high recall and also mines closetransliterations (see Section 5 for details).
In a taskdependent scenario, it is difficult for the unsuper-vised system to mine transliteration pairs accordingto the details of a particular definition of what is con-sidered a transliteration (which may vary somewhatwith the task).
In this section, we propose an exten-sion of our unsupervised model which overcomesthis shortcoming by using labelled data.
The ideais to rely on probabilities from labelled data wherethey can be estimated reliably and to use probabili-ties from unlabelled data where the labelled data issparse.
This is achieved by smoothing the labelleddata probabilities using the unlabelled data probabil-ities as a backoff.4.1 Model EstimationWe calculate the unlabelled data probabilities in theE-step using Equation 4.
For labelled data (contain-ing only transliterations) we set ?
= 0 and get:p(e, f) =?a?Align(e,f)p1(e, f, a) (8)In every EM iteration, we smooth the probabilitydistribution in such a way that the estimates of themultigrams of the unlabelled data that do not occurin the labelled data would be penalized.
We obtainthis effect by smoothing the probability distributionof unlabelled and labelled data using a techniquesimilar to Witten-Bell smoothing (Witten and Bell,1991), as we describe below.Figure 1: Semi-supervised training4.2 Implementation DetailsWe divide the training process of semi-supervisedmining in two steps as shown in Figure 1.
The firststep creates a reasonable alignment of the labelleddata from which multigram counts can be obtained.The labelled data is a small list of transliterationpairs.
Therefore we use the unlabelled data to helpcorrectly align it and train our unsupervised min-ing system on the combined labelled and unlabelledtraining data.
In the expectation step, the prior prob-ability of non-transliteration ?
is set to zero on thelabelled data since it contains only transliterations.The first step passes the resulting multigram proba-bility distribution to the second step.We start the second step with the probability es-timates from the first step and run the E-step sepa-rately on labelled and unlabelled data.
The E-stepon the labelled data is done using Equation 8, whichforces the posterior probability of non-transliterationto zero, while the E-step on the unlabelled data usesEquation 4.
After the two E-steps, we estimatea probability distribution from the counts obtainedfrom the unlabelled data (M-step) and use it as abackoff distribution in computing smoothed proba-bilities from the labelled data counts (S-step).The smoothed probability estimate p?
(q) is:p?
(q) =cs(q) + ?sp(q)Ns + ?s(9)where cs(q) is the labelled data count of the multi-gram q, p(q) is the unlabelled data probability es-timate, and Ns =?q cs(q), and ?s is the numberof different multigram types observed in the Viterbialignment of the labelled data.4725 EvaluationWe evaluate our unsupervised system and semi-supervised system on two tasks, NEWS10 and paral-lel corpora.
NEWS10 is a standard task on translit-eration mining from WIL.
On NEWS10, we com-pare our results with the unsupervised mining sys-tem of Sajjad et al (2011), the best supervisedand semi-supervised systems presented at NEWS10(Kumaran et al, 2010b) and the best supervised andsemi-supervised results reported in the literature forthe NEWS10 task.
For the challenging task of min-ing from parallel corpora, we use the English/Hindiand English/Arabic gold standard provided by Saj-jad et al (2011) to evaluate our results.5.1 Experiments using the NEWS10 DatasetWe conduct experiments on four language pairs: En-glish/Arabic, English/Hindi, English/Tamil and En-glish/Russian using data provided at NEWS10.
Ev-ery dataset contains training data, seed data and ref-erence data.
The NEWS10 data consists of pairs oftitles of the same Wikipedia pages written in dif-ferent languages, which may be transliterations ortranslations.
The seed data is a list of 1000 transliter-ation pairs provided to semi-supervised systems forinitial training.
We use the seed data only in oursemi-supervised system, and not in the unsupervisedsystem.
The reference data is a small subset of thetraining data which is manually annotated with pos-itive and negative examples.5.1.1 TrainingWe word-aligned the parallel phrases of the train-ing data using GIZA++ (Och and Ney, 2003), andsymmetrized the alignments using the grow-diag-final-and heuristic (Koehn et al, 2003).
We extractall word pairs which occur as 1-to-1 alignments (likeSajjad et al (2011)) and later refer to them as theword-aligned list.
We compared the word-alignedlist with the NEWS10 reference data and found thatthe word-aligned list is missing some transliterationpairs because of word-alignment errors.
We built an-other list by adding a word pair for every sourceword that cooccurs with a target word in a paral-lel phrase/sentence and call it the cross-product listlater on.
The cross-product list is noisier but con-tains almost all transliteration pairs in the corpus.Word-aligned Cross-productP R F P R FEA 27.8 97.1 43.3 14.3 98.0 25.0EH 42.5 98.7 59.4 20.5 99.6 34.1ET 32.0 98.1 48.3 17.2 99.6 29.3ER 25.5 95.6 40.3 12.8 99.0 22.7Table 1: Statistics of word-aligned and cross-productlist calculated from the NEWS10 dataset, before min-ing.
EA is English/Arabic, EH is English/Hindi, ET isEnglish/Tamil and ER is English/RussianTable 1 shows the statistics of the word-alignedlist and the cross-product list calculated using theNEWS10 reference data.2 The word-aligned list cal-culated from the NEWS10 dataset is used to com-pare our unsupervised system with the unsupervisedsystem of Sajjad et al (2011) on the same trainingdata.
All the other experiments on NEWS10 usecross-product lists.
We remove numbers from bothlists as they are defined as non-transliterations (Ku-maran et al, 2010b).5.1.2 Unsupervised Transliteration MiningWe run our unsupervised transliteration miningsystem on the word-aligned list and the cross-product list.
The word pairs with a posterior prob-ability of transliteration 1 ?
pntr(e, f) = 1 ?
?p2(ei, fi)/p(ei, fi) greater than 0.5 are selected astransliteration pairs.We compare our unsupervised system with theunsupervised system of Sajjad11.
Our unsupervisedsystem trained on the word-aligned list shows F-measures of 91.7%, 95.5%, 92.9% and 77.7% whichis 4.3%, 3.3%, 2.8% and 1.7% better than the sys-tem of Sajjad11 on English/Arabic, English/Hindi,English/Tamil and English/Russian respectively.Sajjad11 is computationally expensive.
For in-stance, a phrase-based statistical MT system isbuilt once in every iteration of the heuristic proce-dure.
We ran Sajjad11 on the English/Russian word-aligned list using a 2.4 GHz Dual-Core AMD ma-chine, which took almost 10 days.
On the same ma-chine, our transliteration mining system only takes1.5 hours to finish the same experiment.2Due to inconsistent word definition used in the referencedata, we did not achieve 100% recall in our cross-product list.For example, the underscore is defined as a word boundary forEnglish WIL phrases.
This assumption is not followed for cer-tain phrases like ?New York?
and ?New Mexico?.473Unsupervised Semi-supervised/SupervisedSJD OU OS SBest GR DBNEA 87.4 92.4 92.7 91.5 94.1 -EH 92.2 95.7 96.3 94.4 93.2 95.5ET 90.1 93.2 94.6 91.4 95.5 93.9ER 76.0 79.4 83.1 87.5 92.3 82.5Table 2: F-measure results on NEWS10 datasets whereSJD is the unsupervised system of Sajjad11, OU isour unsupervised system built on the cross-product list,OS is our semi-supervised system, SBest is the bestNEWS10 system, GR is the supervised system of Kahkiet al (2011) and DBN is the semi-supervised system ofNabende (2011)Our unsupervised mining system built on thecross-product list consistently outperforms the onebuilt on the word-aligned list.
Later, we consideronly the system built on the cross-product list.
Ta-ble 2 shows the results of our unsupervised sys-tem OU in comparison with the unsupervised sys-tem of Sajjad11 (SJD), the best semi-supervised sys-tems presented at NEWS10 (SBEST ) and the bestsemi-supervised results reported on the NEWS10dataset (GR, DBN ).
On three language pairs, ourunsupervised system performs better than all semi-supervised systems which participated in NEWS10.It has competitive results with the best supervisedresults reported on NEWS10 datasets.
On En-glish/Hindi, our unsupervised system outperformsthe state-of-the-art supervised and semi-supervisedsystems.
Kahki et al (2011) (GR) achievedthe best results on English/Arabic, English/Tamiland English/Russian.
For the English/Arabic task,they normalized the data using language dependentheuristics3 and also used a non-standard evaluationmethod (discussed in Section 5.1.4).On the English/Russian dataset, our unsupervisedsystem faces the problem that it extracts cognatesas transliterations.
The same problem was reportedin Sajjad et al (2011).
Cognates are close translit-erations which differ by only one or two charactersfrom an exact transliteration pair.
The unsupervisedsystem learns to delete the additional one or twocharacters with a high probability and incorrectlymines such word pairs as transliterations.3They applied an Arabic word segmenter which uses lan-guage dependent information.
Arabic long vowels which haveidentical sound but are written differently were merged to oneform.
English characters were normalized by dropping accents.Unsupervised Semi-supervisedP R F P R FEA 89.2 95.7 92.4 92.9 92.4 92.7EH 92.6 99.0 95.7 95.5 97.0 96.3ET 88.3 98.6 93.2 93.4 95.8 94.6ER 67.2 97.1 79.4 74.0 94.9 83.1Table 3: Precision(P), Recall(R) and F-measure(F) of ourunsupervised and semi-supervised transliteration miningsystems on NEWS10 datasets5.1.3 Semi-supervised Transliteration MiningOur semi-supervised system uses similar initial-ization of the parameters as used for unsupervisedsystem.
Table 2 shows on three language pairs, oursemi-supervised system OS only achieves a smallgain in F-measure over our unsupervised systemOU .
This shows that the unlabelled training data isalready providing most of the transliteration infor-mation.
The seed data is used to help the translit-eration mining system to learn the right definitionof transliteration.
On the English/Russian dataset,our semi-supervised system achieves almost 7% in-crease in precision with a 2.2% drop in recall com-pared to our unsupervised system.
This provides a3.7% gain on F-measure.
The increase in precisionshows that the seed data is helping the system in dis-ambiguating transliteration pairs from cognates.5.1.4 DiscussionThe unsupervised system produces lists with highrecall.
The semi-supervised system tends to betterbalance out precision and recall.
Table 3 comparesthe precision, recall and F-measure of our unsuper-vised and semi-supervised mining systems.The errors made by our semi-supervised systemcan be classified into the following categories:Pronunciation differences: English propernames may be pronounced differently in other lan-guages.
Sometimes, English short vowels are con-verted to long vowels in Hindi such as the Englishword ?Lanthanum?
which is pronounced ?Laan-thanum?
in Hindi.
Our transliteration mining systemwrongly extracts such pairs as transliterations.In some cases, different vowels are used in twolanguages.
The English word ?January?
is pro-nounced as ?Janvary?
in Hindi.
Such word pairs arenon-transliterations according to the gold standardbut our system extracts them as transliterations.
Ta-474Table 4: Word pairs with pronunciation differencesTable 5: Examples of word pairs which are wrongly an-notated as transliterations in the gold standardble 4 shows a few examples of such word pairs.Inconsistencies in the gold standard: There areseveral inconsistencies in the gold standard whereour transliteration system correctly identifies a wordpair as a transliteration but it is marked as a non-transliteration or vice versa.
Consider the exampleof the English word ?George?
which is pronouncedas ?Jaarj?
in Hindi.
Our semi-supervised systemlearns this as a non-transliteration but it is wronglyannotated as a transliteration in the gold standard.Arabic nouns have an article ?al?
attached to themwhich is translated in English as ?the?.
There arevarious cases in the training data where an Englishnoun such as ?Quran?
is matched with an Arabicnoun ?alQuran?.
Our mining system classifies suchcases as non-transliterations, but 24 of them are in-correctly annotated as transliterations in the goldstandard.
We did not correct this, and are there-fore penalized.
Kahki et al (2011) preprocessedsuch Arabic words and separated ?al?
from the noun?Quran?
before mining.
They report a match if theversion of the Arabic word with ?al?
appears withthe corresponding English word in the gold stan-dard.
Table 5 shows examples of word pairs whichare wrongly annotated as transliterations.Cognates: Sometimes a word pair differs by onlyone or two ending characters from a true translit-eration.
For example in the English/Russian train-ing data, the Russian nouns are marked with caseswhereas their English counterparts do not mark thecase or translate it as a separate word.
Often theRussian word differs only by the last character froma correct transliteration of the English word.
Dueto the large amount of such word pairs in the En-glish/Russian data, our mining system learns todelete the final case marking characters from theRussian words.
It assigns a high transliteration prob-Table 6: A few examples of English/Russian cognatesability to these word pairs and extracts them astransliterations.
Table 6 shows some examples.There are two English/Russian supervised sys-tems which are better than our semi-supervised sys-tem.
The Kahki et al (2011) system is built on seeddata only.
Jiampojamarn et al (2010)?s best sys-tem on English/Russian is based on the edit distancemethod.
Both of these systems are focused on highprecision.
Our semi-supervised system is focusedon high recall at the cost of lower precision.45.2 Transliteration Mining using ParallelCorporaThe percentage of transliteration pairs in theNEWS10 datasets is high.
We further check the ef-fectiveness of our unsupervised and semi-supervisedmining systems by evaluating them on parallel cor-pora with as few as 2% transliteration pairs.We conduct experiments using two languagepairs, English/Hindi and English/Arabic.
The En-glish/Hindi corpus is from the shared task on wordalignment organized as part of the ACL 2005 Work-shop on Building and Using Parallel Texts (WA05)(Martin et al, 2005).
For English/Arabic, we use200,000 parallel sentences from the United Nations(UN) corpus (Eisele and Chen, 2010).
The En-glish/Hindi and English/Arabic transliteration goldstandards were provided by Sajjad et al (2011).5.2.1 ExperimentsWe follow the procedure for creating the trainingdata described in Section 5.1.1 and build a word-aligned list and a cross-product list from the parallelcorpus.
We first train and test our unsupervised min-ing system on the word-aligned list and compare ourresults with Sajjad et al Table 7 shows the results.Our unsupervised system achieves 0.6% and 1.8%higher F-measure than Sajjad et al respectively.The cross-product list is huge in comparison tothe word-aligned list.
It is noisier than the word-4We implemented a bigram version of our system to learnthe contextual information at the end of the word pairs, but onlyachieved a gain of less than 1% F-measure over our unigramsemi-supervised system.
Details are omitted due to space.475TP FN TN FP P R FEHSJD 170 10 2039 45 79.1 94.4 86.1EHO 176 4 2034 50 77.9 97.8 86.7EASJD 197 91 6580 59 77.0 68.4 72.5EAO 288 0 6440 199 59.1 100 74.3Table 7: Transliteration mining results of our unsuper-vised system and Sajjad11 system trained and testedon the word-aligned list of English/Hindi and En-glish/Arabic parallel corpusTP FN TN FP P R FEHU 393 19 12279 129 75.3 95.4 84.2EHS 365 47 12340 68 84.3 88.6 86.4EAU 277 11 6444 195 58.7 96.2 72.9EAS 272 16 6497 142 65.7 94.4 77.5Table 8: Transliteration mining results of our unsuper-vised and semi-supervised systems trained on the word-aligned list and tested on the cross-product list of En-glish/Hindi and English/Arabic parallel corpusaligned list but has almost 100% recall of transliter-ation pairs.
The English-Hindi cross-product list hasalmost 55% more transliteration pairs (412 types)than the word-aligned list (180 types).
We can notreport these numbers on the English/Arabic cross-product list since the English/Arabic gold standardis built on the word-aligned list.In order to keep the experiment computationallyinexpensive, we train our mining systems on theword-aligned list and test them on the cross-productlist.5 We also perform the first semi-supervised eval-uation on this task.
For our semi-supervised sys-tem, we additionally use the English/Hindi and En-glish/Arabic seed data provided by NEWS10.Table 8 shows the results of our unsupervisedand semi-supervised systems on the English/Hindiand English/Arabic parallel corpora.
Our unsu-pervised system achieves higher recall than oursemi-supervised system but lower precision.
Thesemi-supervised system shows an improvement inF-measure for both language pairs.
We lookedinto the errors made by our systems.
The minedtransliteration pairs of our unsupervised system con-tains 65 and 111 close transliterations for the En-glish/Hindi and English/Arabic task respectively.5There are some multigrams of the cross-product list whichare unknown to the model learned on the word-aligned list.
Wedefine their probability as the inverse of the number of multi-gram tokens in the Viterbi alignment of the labelled and unla-belled data together.The close transliterations only differ by one or twocharacters from correct transliterations.
We thinkthese pairs provide transliteration information tothe systems and help them to avoid problems withdata sparseness.
Our semi-supervised system usesthe seed data to identify close transliterations asnon-transliterations and decreases the number offalse positives.
They are reduced to 35 and 89for English/Hindi and English/Arabic respectively.The seed data and the training data used in thesemi-supervised system are from different domains(Wikipedia and UN).
Seed data extracted from thesame domain is likely to work better, resulting ineven higher scores than we have reported.6 Conclusion and Future WorkWe presented a novel model to automaticallymine transliteration pairs.
Our approach is ef-ficient and language pair independent (for alpha-betic languages).
Both the unsupervised and semi-supervised systems achieve higher accuracy than theonly unsupervised transliteration mining system weare aware of and are competitive with the state-of-the-art supervised and semi-supervised systems.Our semi-supervised system outperformed our un-supervised system, in particular in the presence ofprevalent cognates in the Russian/English data.In future work, we plan to adapt our approachto language pairs where one language is alphabeticand the other language is non-alphabetic such as En-glish/Japanese.
These language pairs require one-to-many character mappings to learn transliterationunits, while our current system only learns unigramcharacter alignments.AcknowledgmentsThe authors wish to thank the anonymous review-ers.
We would like to thank Syed Aoun Raza fordiscussions of implementation efficiency.
HassanSajjad was funded by the Higher Education Com-mission of Pakistan.
Alexander Fraser was fundedby Deutsche Forschungsgemeinschaft grant Modelsof Morphosyntax for Statistical Machine Transla-tion.
Helmut Schmid was supported by DeutscheForschungsgemeinschaft grant SFB 732.
This workwas supported in part by the IST Programme ofthe European Community, under the PASCAL2 Net-work of Excellence, IST-2007-216886.
This publi-cation only reflects the authors?
views.476ReferencesMaximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
Speech Communication, 50(5).Kareem Darwish.
2010.
Transliteration mining withphonetic conflation and iterative training.
In Proceed-ings of the 2010 Named Entities Workshop, Uppsala,Sweden.Sabine Deligne and Fre?de?ric Bimbot.
1995.
Languagemodeling by variable length sequences : Theoreti-cal formulation and evaluation of multigrams.
InProceedings of the IEEE International Conference onAcoustics, Speech, and Signal Processing, volume 1,Los Alamitos, CA, USA.Andreas Eisele and Yu Chen.
2010.
MultiUN: A multi-lingual corpus from United Nation documents.
In Pro-ceedings of the Seventh conference on InternationalLanguage Resources and Evaluation (LREC?10), Val-letta, Malta.Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,Aditya Bhargava, Qing Dou, Mi-Young Kim, andGrzegorz Kondrak.
2010.
Transliteration generationand mining with limited training resources.
In Pro-ceedings of the 2010 Named Entities Workshop, Upp-sala, Sweden.Ali El Kahki, Kareem Darwish, Ahmed Saad El Din,Mohamed Abd El-Wahab, Ahmed Hefny, and WaleedAmmar.
2011.
Improved transliteration mining usinggraph reinforcement.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP), Edinburgh, UK.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofthe Human Language Technology and North Ameri-can Association for Computational Linguistics Con-ference, Edmonton, Canada.A Kumaran, Mitesh M. Khapra, and Haizhou Li.
2010a.Report of NEWS 2010 transliteration mining sharedtask.
In Proceedings of the 2010 Named Entities Work-shop, Uppsala, Sweden.A Kumaran, Mitesh M. Khapra, and Haizhou Li.
2010b.Whitepaper of NEWS 2010 shared task on translitera-tion mining.
In Proceedings of the 2010 Named Enti-ties Workshop, Uppsala, Sweden.Haizhou Li, Zhang Min, and Su Jian.
2004.
A jointsource-channel model for machine transliteration.
InACL ?04: Proceedings of the 42nd Annual Meeting onAssociation for Computational Linguistics, Barcelona,Spain.Joel Martin, Rada Mihalcea, and Ted Pedersen.
2005.Word alignment for languages with scarce resources.In ParaText ?05: Proceedings of the ACL Workshopon Building and Using Parallel Texts, Morristown, NJ,USA.Peter Nabende.
2010.
Mining transliterations fromwikipedia using pair hmms.
In Proceedings of the2010 Named Entities Workshop, Uppsala, Sweden.Peter Nabende.
2011.
Mining transliterations fromWikipedia using dynamic bayesian networks.
In Pro-ceedings of the International Conference Recent Ad-vances in Natural Language Processing 2011, Hissar,Bulgaria.Sara Noeman and Amgad Madkour.
2010.
Languageindependent transliteration mining system using finitestate automata framework.
In Proceedings of the 2010Named Entities Workshop, Uppsala, Sweden.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1).Hassan Sajjad, Alexander Fraser, and Helmut Schmid.2011.
An algorithm for unsupervised transliterationmining with an application to word alignment.
In Pro-ceedings of the 49th Annual Conference of the Associ-ation for Computational Linguistics, Portland, USA.Ian H. Witten and Timothy C. Bell.
1991.
The zero-frequency problem: Estimating the probabilities ofnovel events in adaptive text compression.
In IEEETransactions on Information Theory, volume 37.477
