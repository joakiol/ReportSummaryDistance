Unsupervised Training for Overlapping Ambiguity Resolution inChinese Word SegmentationMu Li, Jianfeng Gao, Changning HuangMicrosoft Research, AsiaBeijing 100080, China{t-muli,jfgao,cnhuang}@microsoft.comJianfeng LiUniversity of Science and Technology of ChinaHefei, 230027, Chinajackwind@mail.ustc.edu.cnAbstractThis paper proposes an unsupervisedtraining approach to resolving overlap-ping ambiguities in Chinese word seg-mentation.
We present an ensemble ofadapted Na?ve Bayesian classifiers thatcan be trained using an unlabelled Chi-nese text corpus.
These classifiers differin that they use context words withinwindows of different sizes as features.The performance of our approach isevaluated on a manually annotated test set.Experimental results show that the pro-posed approach achieves an accuracy of94.3%, rivaling the rule-based and super-vised training methods.1 IntroductionResolving segmentation ambiguities is one of thefundamental tasks for Chinese word segmentation,and has received considerable attention in the re-search community.
Word segmentation ambigui-ties can be roughly classified into two classes:overlapping ambiguity (OA), and combination am-biguity (CA).
In this paper, we focus on the meth-ods of resolving overlapping ambiguities.Consider a Chinese character string ABC, if itcan be segmented into two words either as AB/Cor A/BC depending on different context, ABC iscalled an overlapping ambiguity string (OAS).
Forexample, given a Chinese character string ??
(ge4-guo2-you3), it can be segmented as either? | ?
(each state-owned) in Sentence (1) ofFigure 1, or ? | ?
(every country has) inSentence (2).
(1)  (in) |  (each) |  (state-owned) |  (enterprise)  |   (middle)(in each state-owned enterprise)(2)  (in) | 	 (human rights) |(prob-lem) |  (on) |  (every country) | (have) | (common ground)(Regarding human rights, every country hassome common ground)Figure 1.
Overlapping  ambiguities of Chinesecharacter string ??Our method of resolving overlapping ambigui-ties contains two procedures.
One is to construct anensemble of Na?ve Bayesian classifiers to resolveambiguities.
The other is an unsupervised methodfor training the Na?ve Bayesian classifiers whichcompose the ensemble.
The main issue of the un-supervised training is how to eliminate the nega-tive impact of the OA errors in the training data.Our solution is to identify all OASs in the trainingdata and replace them with a single special token.By doing so, we actually remove the portion oftraining data that are likely to contain OA errors.The classifiers are then trained on the processedtraining data.Our approach is evaluated on a manually anno-tated test set with 5,759 overlapping segmentationambiguities.
Experimental results show that an ac-curacy of 94.3% is achieved.This remainder of this paper is structured as fol-lows: Section 2 reviews previous work.
Section 3defines overlapping ambiguous strings in Chinese.Section 4 describes the evaluation results.
Section5 presents our conclusion.2 Previous WorkPrevious methods of resolving overlapping am-biguities can be grouped into rule-based ap-proaches and statistical approaches.Maximum Matching (MM) based segmentation(Huang, 1997) can be regarded as the simplestrule-based approach, in which one starts from oneend of the input sentence, greedily matches thelongest word towards the other end, and repeats theprocess with the rest unmatched character se-quences until the entire sentence is processed.
Ifthe process starts with the beginning of the sen-tence, it is called Forward Maximum Matching(FMM).
If the process starts with the end of thesentence, it is called Backward Maximum Match-ing (BMM).
Although it is widely used due to itssimplicity, MM based segmentation performspoorly in real text.Zheng and Liu (1997) use a set of manuallygenerated rules, and reported an accuracy of 81%on an open test set.
Swen and Yu (1999) presents alexicon-based method.
The basic idea is that foreach entry in a lexicon, all possible ambiguitytypes are tagged; and for each ambiguity types, asolution strategy is used.
They achieve an accuracyof 95%.
Sun (1998) demonstrates that most of theoverlapping ambiguities can be resolved withouttaking into account the context information.
Hethen proposes a lexicalized rule-based approach.His experiments show that using the 4,600 mostfrequent rules, 51% coverage can be achieved in anopen test set.Statistical methods view the overlappingambiguity resolution as a search or classificationtask.
For example, Liu (1997) uses a word unigramlanguage model, given all possible segmentationsof a Chinese character sequence, to search the bestsegmentation with the highest probability.
Similarapproach can be traced back to Zhang (1991).
Butthe method does not target to overlappingambiguities.
So the disambiguation results are notreported.
Sun (1999) presents a hybrid methodwhich incorporates empirical rules and statisticalprobabilities, and reports an overall accuracy of92%.
Li (2001) defines the word segmentation dis-ambiguation as a binary classification problem.
Lithen uses Support Vector Machine (SVM) withmutual information between each Chinese charac-ter pair as a feature.
The method achieves an accu-racy of 92%.
All the above methods utilize a su-pervised training procedure.
However, a largemanually labeled training set is not always avail-able.
To deal with the problem, unsupervised ap-proaches have been proposed.
For example, Sun(1997) detected word boundaries given an OASusing character-based statistical measures, such asmutual information and difference of t-test.
Hereported an accuracy of approximately 90%.
In hisapproach, only the statistical information within 4adjacent characters is exploited, and lack of word-level statistics may prevent the disambiguationperformance from being further improved.3 Ensemble of Na?ve Bayesian Classifierfor Overlapping Ambiguity Resolution3.1 Problem DefinitionWe first give the formal definition of overlappingambiguous string (OAS) and longest OAS.An OAS is a Chinese character string O thatsatisfies the following two conditions:a) There exist two segmentations Seg1 and Seg2such that 2211 , SegwSegw ???
, where Chinesewords w1 and w2 are different from either literalstrings or positions;b) 2211 , SegwSegw ???
, where w1 and w2overlap.The first condition ensures that there areambiguous word boundaries (if more than oneword segmentors are applied) in an OAS.
In theexample presented in section 1, the string ??
is an OAS but ??
is not becausethe word ??
remains the same in both FMMand BMM segmentations of ? |  | ?
and? |  | ?.
The second condition indicatesthat the ambiguous word boundaries result fromcrossing brackets.
As illustrated in Figure 1, words??
and ??
form a crossing bracket.The longest OAS is an OAS that is not a sub-string of any other OAS in a given sentence.
Forexample, in the case ??
(sheng1-huo2-shui3-ping2, living standard), both ??
and??
are OASs, but only ??
is thelongest OAS because ??
is a  substring of??.
In this paper, we only consider thelongest OAS because both left and right bounda-ries of the longest OAS are determined.Furthermore, we constrain our search spacewithin the FMM segmentation Of and BMM seg-mentation Ob of a given longest OAS.
Accordingto Huang (1997), two important properties of OAShas been identified: (1) if the FMM segmentationis the same as its BMM segmentation (Of = Ob), forexample ?
 ?
(sou1-suo3-yin3-qing2,Search Engine), the probability that the MM seg-mentation is correct is 99%;  Otherwise, (2) if theFMM segmentation differs from its BMM segmen-tation (Of ?
Ob ), for example ??, the prob-ability that at least one of the MM segmentation iscorrect is also 99%.
So such a strategy will notlower the coverage of our approach.Therefore, the overlapping ambiguity resolutioncan be formulized as a binary classification prob-lem as follows:Given a longest OAS O and its context featureset C, let G(Seg, C) be a score function of Seg  for},{ bf OOseg ?
, the overlapping ambiguity reso-lution task is to make the binary decision:??
?<>= ),(),(),(),(COGCOGOCOGCOGOsegbfbbff(1)Note that Of = Ob means that both FMM andBMM arrive at the same result.
The classificationprocess can then be stated as:a) If Of = Ob, then choose either segmentationresult since they are same;b) Otherwise, choose the one with the higherscore G according to Equation (1).For example, in the example of ??, ifOf = Ob  = ? | ?, then ? | ?
is se-lected as the answer.
In another example of ??
in sentence (1) of Figure 1, Of = ? | ?,Ob  = ? | ?.
Assume that C = {, }, i.e.,we used a context window of size 3; then the seg-mentation ?
   |  ?
is selected if>}),{,"|(" G }),{,"|(" G ,otherwise ? | ?
is selected.3.2 Na?ve Bayesian Classifier for OverlappingAmbiguity ResolutionLast section formulates the overlapping ambi-guity resolution of an OAS O as the binary classi-fication between Of and Ob.
This section describesthe use of the adapted Na?ve Bayesian Classifier(NBC) (Duda and Hart, 1973) to address problem.Here, we use the words around O within a windowas features, with w-m?w-1 denoting m words on theleft of the O and w1?wn denoting n words on theright of the O. Na?ve Bayesian Classifier assumesthat all the feature variables are conditionally inde-pendent.
So the joint probability of observing a setof context features C = {w-m?w-1, w1?wn} of asegmentation Seg (Of or Ob) of O is as follows:???=?
?=nmiinmSegwpSegpSegwwwwp,...1,1,...1,1)|()(),,...,((2)Assume that Equation (2) is the score functionin Equation (1) G, we then have two parameters tobe estimated: p(Seg) and p(wi|Seg).
Since we donot have enough labeled training data, we then re-sort to the redundancy property of natural language.Due to the fact that the OAS occupies only in avery small portion of the entire Chinese text, it isfeasible to estimate the word co-occurrence prob-abilities from the portion of corpus that contains nooverlapping ambiguities.
Consider an OAS(xin4-xin1-de, confidently).
The correct segmenta-tion would be ? | ?, if  (cong1-man3,full of) were its context word.
We note thatappears as the left context word of  in bothstrings  and  (,yong3-qi4, courage).
While the former string con-tains an OAS, the latter does not.
We then removeall OAS from the training data, and estimate theparameters using the training data that do not con-tain OAS.
In experiments, we replace all longestOAS that has Of ?
Ob with a special token [GAP].Below, we refer to the processed corpus astokenized corpus.Note that Seg is either the FMM or the BMMsegmentation of O, and all OASs (including Seg)have been removed from the tokenized corpus,thus there are no statistical information available toestimate p(Seg) and p(w-m?w-1,w1?wn|Seg) basedon the Maximum Likelihood Estimation (MLE)principle.
To estimate them, we introduce the fol-lowing two assumptions.1) Since the unigram probability of each word wcan be estimated from the training data, for agiven segmentation Seg=ws1?wsk, we assumethat each word w of Seg is generated inde-pendently.
The probability p(Seg) is approxi-mated by the production of the word unigramprobabilities:?
?=SegwiiwpSegp )()(  (3)2) We also assume that left and right context wordsequences are only conditioned on the leftmostand rightmost words of Seg, respectively.)()()...,(),...()|...()|...()|...,...(111111111sksnsksmsknsmnmwpwpwwwpwwwpwwwpwwwpSegwwwwp?????
?==(4)where the word sequence probabilities P(w-m, ?,w-1, ws1) and P(wsk,w1, ?, wn) are decomposed asproductions of trigram probabilities.
We used astatistical language model toolkit described in (Gaoet al 2002) to build trigram models based on thetokenized corpus.Although the final language model is trainedbased on a tokenized corpus, the approach can beregarded as an unsupervised one from the view ofthe entire training process: the tokenized corpus isautomatically generated by an MM based segmen-tation tool from the raw corpus input with neitherhuman interaction nor manually labeled data re-quired.3.3 Ensemble of Classifiers and MajorityVoteGiven different window sizes, we can obtain dif-ferent classifiers.
We then combine them toachieve better results using the so-called ensemblelearning (Peterson 2000).
Let NBC(l, r)  denote theclassifier with left window size l and right windowsize r. Given the maximal window size of 2, wethen have 9 classifiers, as shown in Table 1.L = 0 l = 1 l = 2r = 0 NBC(0, 0) NBC(1, 0) NBC(2,0)r = 1 NBC(0,1) NBC(1,1) NBC(2,1)r = 2 NBC(0,2) NBC(1,2) NBC(2,2)Table 1.
Bayesian classifiers in the ensembleThe ensemble learning suggests that the ensembleclassification results are based on the majority voteof these classifiers: The segmentation that is se-lected by most classifiers is chosen.4 Experiments and Discussions4.1 SettingsWe evaluate our approach using a manually anno-tated test set, which was selected randomly fromPeople?s Daily news articles of year 1997, contain-ing approximate 460,000 Chinese characters, or247,000 words.
In the test set, 5759 longest OASare identified.
Our lexicon contains 93,700 entries.4.2 OAS DistributionWe first investigate the distribution of differenttypes of OAS in the test set.
In our approach, theperformance upper bound (i.e.
oracle accuracy)cannot achieve 100% because not all the OASs?correct segmentations can be generated by FMMand BMM segmentation.
So it is very useful toknow to what extent our approach can deal withthe problem.The results are shown in Table 2.
We denotethe entire OAS data set as C, and divide it into twosubsets A and B according to the type of OAS.
Itcan be seen from the table that in data set A(Of=Ob), the accuracy of MM segmentationachieves 98.8% accuracy.
Meanwhile, in data set B(Of ?
Ob) the oracle recall of candidates proposedby FMM and BMM is 95.7% (97.2% in the entiredata set C).
The statistics are very close to thosereported in Huang (1997).Of = Ob = COR273147.42%AOAS Of = Ob276347.98%Of = Ob ?
COR320.56%Of  = COR ?
Ob = COR286649.77%BOAS Of ?
Ob299652.02%Of  ?
COR ?
Ob ?
COR1302.26%Table 2.
Distribution of OAS in the test setHere are some examples for the overlappingambiguities that cannot be covered by our ap-proach.
For errors resulting from Of  = Ob ?
COR,a typical example in the literature is !
"#$%(jie2-he2-cheng2-fen1-zi3-shi2,  !
| " | #$ |%).
For errors caused by Of ?
Ob  and Of  ?
COR ?Ob ?
COR, &'()(*+) (chu1-xian4-zai4-shi4-ji4, &' |  | ()) serves as a good exam-ple.
These two types of errors are usually com-posed of several words and need a much morecomplicated search process to determine the finalcorrect output.
Since the number of such errors isvery small, they are not target of our approach inthis paper.4.3 Experimental Results of Ensemble of Na-?ve Bayesian ClassifiersThe classifiers are trained from the People?s Dailynews articles of year 2000, which contain over 24million characters.
The training data is tokenized.That is, all OAS with Of ?
Ob are replaced with thetoken [GAP].
After tokenization, there are16,078,000 tokens in the training data in which203,329 are [GAP], which is 1.26% of the entiretraining data set.
Then a word trigram languagemodel is constructed on the tokenized corpus, andeach Bayesian classifier is built given the languagemodel.l = 0 l = 1 l = 2r = 0 88.73% 88.85% 88.95%r = 1 89.09% 89.39% 89.39%r = 2 88.95% 89.39% 89.35%Table 3.
Accuracy of each individual classifierTable 3 shows the accuracy of each classifieron data set B.
The performance of the ensemblebased on majority vote is 89.79% on data set B,and the overall accuracy on C is 94.13%.
The en-semble consistently outperforms any of its mem-bers.
Classifiers with both left and right contextfeatures perform better than the others becausethey are capable to segment some of the contextsensitive OAS.
For example, contextual informa-tion is necessary to segment the OAS ?,-?
(kan4-tai2-shang4, on the stand) correctly inboth following sentences:.
| , | - | /0 | 12(Look at the performer in the stage)3 |  | 45 | 6 | 7 | ,- |(Stand in the highest stand)Both Peterson (2000) and Brill (1998) foundthat the ultimate success of an ensemble dependson the assumption that classifiers to be combinedmake complementary errors.
We investigate thisassumption in our experiments, and estimate theoracle accuracy of our approach.
Result shows thatonly 6.0% (180 out of 2996) of the OAS in data setB that is classified incorrectly by all the 9 classifi-ers.
In addition, we can see from Table 2, that 130instances of these 180 errors are impossible to becorrect because neither Of nor Ob is the correctsegmentation.
Therefore, the oracle accuracy of theensemble is 94.0%, which is very close to 95.7%,the theoretical upper bound of our approach in dataset B described in Section 4.2.
However, our ma-jority vote based ensemble only achieves accuracyclose to 90%.
This analysis thus suggests that fur-ther improves can be made by using more powerfulensemble strategies.4.4 Lexicalized Rule Based OAS Disambigua-tionWe also conduct a series of experiments to evalu-ate the performance of a widely used lexicalizedrule-based OAS disambiguation approach.
As re-ported by Sun (1998) and Li (2001), over 90% ofthe OAS can be disambiguated in a context-freeway.
Therefore, simply collecting large amount ofcorrectly segmented OAS whose segmentation isindependent of its context would yield pretty goodperformance.We first collected 730,000 OAS with Of ?
Obfrom 20 years?
People?s Daily corpus which con-tains about 650 million characters.
Then approxi-mately 47,000 most frequently occurred OASswere extracted.
For each of the extracted OAS, 20sentences that contain it were randomly selectedfrom the corpus, and the correct segmentation ismanually labeled.
41,000 lexicalized disambigua-tion rules were finally extracted from the labeleddata, whose either MM segmentation (Of or Ob)gains absolute majority, over 95% in our experi-ment.
The rule set covers approximately 80% oc-currences of all the OASs in the training set, whichis very close to that reported in Sun (1998).
Here isa sample rule extracted:  =>  | .
Itmeans that among the 20 sentences that contain thecharacter sequence ??, at least 19 of themare segmented as ? | ?.The performance of the lexicalized rule-basedapproach is shown in Table 4, where for compari-son we also include the performance of using onlyFMM or BMM segmentation algorithm.AccuracyData set B Data set CFMM 49.44% 73.12%BMM 46.31% 71.51%Rule + FMM 83.10% 90.65%Rule + BMM 84.43% 91.33%NBC(0, 0) 88.73% 93.70%Ensemble 89.79% 94.13%Table 4.
Performance comparisonIn Table 4, Rule + FMM means if there is norule applicable to an OAS, FMM segmentation willbe used.
Similarly, Rule + BMM means that BMMsegmentation will be used as backup.
We can seein Table 4 that rule-based systems outperform theirFMM and BMM counterparts significantly, but donot perform as well as our method, even when nocontext feature is used.
This is because that therules can only cover about 76% of the OASs in thetest set with precision 95%, and FMM or BMMperforms poorly on the rest of the OASs.
Althoughthe precision of these lexicalized rules is high, theroom for further improvements is limited.
For ex-ample, to achieve a higher coverage, say 90%,much more manually labeled training data (i.e.81,000 OAS) are needed.5 Conclusion and Future workOur contributions are two-fold.
First, we proposean approach based on an ensemble of adapted na-?ve Bayesian classifiers to resolving overlappingambiguities in Chinese word segmentation.
Second,we present an unsupervised training method ofconstructing these Bayesian classifiers on an unla-beled training corpus.
It thus opens up the possibil-ity for adjusting this approach to a large variety ofapplications.
We perform evaluations using amanually annotated test set.
Results show that ourapproach outperforms a lexicalized rule-based sys-tem.
Future work includes investigation on how toconstruct more powerful classifier for further im-provements.
One promising way is combining ourapproach with Sun?s (1997), with a core set of con-text free OASs manually labeled to improve accu-racy.AcknowledgementsWe would like to thank Wenfeng Yang and Xiao-dan Zhu for helpful discussions on this project andWenfeng?s excellent work on the lexicalized dis-ambiguation rule set construction.ReferencesEric Brill and Wu Jun.
1998.
Classifier combination forimproved lexical disambiguation.
In Proceedings ofthe 36th Annual Meeting of the Association for Com-putational Linguistics, Montreal.
CA.Richard Duda and Peter Hart.
1973.
Pattern Classifica-tion and Scene Analysis.
Wiley, New York, NY.Jianfeng Gao and Joshua Goodman, Li Mingjing, LeeKai-Fu.
2002.
Toward a unified approach to statisti-cal language modeling for Chinese.
ACM Transac-tions on Asian Language Information Processing, 1(1):3-33.Changning Huang.
1997.
Segmentation problem in Chi-nese Processing.
(In Chinese) Applied Linguistics.1:72-78.Rong Li, Shaohui Liu, Shiwei Ye and Zhongzhi Shi.2001.
A Method of Crossing Ambiguities in Chineseword Segmentation Based on SVM and k-NN.
(InChinese) Journal of Chinese Information Processing,15(6): 13-18.Ting Liu, Kaizhu Wang and Xinghai Jiang.
1997.
TheMaximum Probability Segmentation Algorithm ofAmbiguous Character Strings.
(In Chinese) Lan-guage Engineering.
Tsinghua University Press.pp.182-187.Ted Pedersen.
2000.
A Simple Approach to BuildingEnsembles of Naive Bayesian Classifiers for WordSense Disambiguation.
In Proceedings of the FirstAnnual Meeting of the North American Chapter ofthe Association for Computational Linguistics.
Seat-tle, WA.
pp.
63-69Maosong Sun, Changning Huang and Benjamin K. Tsou.1997.
Using Character Bigram for Ambiguity Reso-lution In Chinese Words Segmentation.
(In Chinese)Computer Research and Development, 34(5): 332-339Maosong Sun and Zhengping Zuo.
1998.
Overlappingambiguity in Chinese Text.
(In Chinese) Quantita-tive and Computational Studies on the ChineseLanguage, HK, pp.
323-338Maosong Sun, Zhengping Zuo and Changning Huang.1999.
Algorithm for solving 3-character crossingambiguities in Chinese word segmentation.
(In Chi-nese) Journal of Tsinghua University Science andTechnology, 39(5).Bing Swen and Shiwen Yu.
1999.
A Graded Approachfor the Efficient Resolution of Chinese Word Seg-mentation Ambiguities.
In Proceedings of 5th NaturalLanguage Processing Pacific Rim Symposium.
pp.19-24Junsheng Zhang, Zhida Chen and Shunde Chen.
1991.Constraint Satisfaction and Probabilistic ChineseSegmentation.
(In Chinese) In Proceedings ofROCLING IV (R.O.C.
Computational LinguisticsConference).
pp.
147-165Jiaheng Zheng and Kaiying Liu.
1997.
The Research ofAmbiguity Word ?
Segmentation Technique for theChinese Text.
(In Chinese) Language Engineering,Tsinghua University Press, pp.
201-206
