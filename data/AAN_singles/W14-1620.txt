Proceedings of the Eighteenth Conference on Computational Language Learning, pages 191?200,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsA Rudimentary Lexicon and Semantics Help Bootstrap PhonemeAcquisitionAbdellah Fourtassi ????????????
Emmanuel DupouxLaboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris{abdellah.fourtassi, emmanuel.dupoux}@gmail.comAbstractInfants spontaneously discover the rele-vant phonemes of their language withoutany direct supervision.
This acquisitionis puzzling because it seems to requirethe availability of high levels of linguisticstructures (lexicon, semantics), that logi-cally suppose the infants having a set ofphonemes already.
We show how this cir-cularity can be broken by testing, in real-size language corpora, a scenario wherebyinfants would learn approximate represen-tations at all levels, and then refine them ina mutually constraining way.
We start withcorpora of spontaneous speech that havebeen encoded in a varying number of de-tailed context-dependent allophones.
Wederive, in an unsupervised way, an approx-imate lexicon and a rudimentary seman-tic representation.
Despite the fact thatall these representations are poor approxi-mations of the ground truth, they help re-organize the fine grained categories intophoneme-like categories with a high de-gree of accuracy.One of the most fascinating facts about humaninfants is the speed at which they acquire theirnative language.
During the first year alone, i.e.,before they are able to speak, infants achieve im-pressive landmarks regarding three key languagecomponents.
First, they tune in on the phone-mic categories of their language (Werker and Tees,1984).
Second, they learn to segment the continu-ous speech stream into discrete units (Jusczyk andAslin, 1995).
Third, they start to recognize fre-quent words (Ngon et al., 2013), as well as thesemantics of many of them (Bergelson and Swing-ley, 2012).Even though these landmarks have been doc-umented in detail over the past 40 years of re-search, little is still known about the mechanismsthat are operative in infant?s brain to achieve sucha result.
Current work in early language acquisi-tion has proposed two competing but incompletehypotheses that purports to account for this stun-ning development path.
The bottom-up hypothesisholds that infants converge onto the linguistic unitsof their language through a statistical analysis overof their input.
In contrast, the top-down hypothesisemphasizes the role of higher levels of linguisticstructure in learning the lower level units.1 A chicken-and-egg problem1.1 Bottom-up is not enoughSeveral studies have documented the fact that in-fants become attuned to the native sounds of theirlanguage, starting at 6 months of age (see Ger-vain & Mehler, 2010 for a review).
Some re-searchers have claimed that such an early attune-ment is due to a statistical learning mechanism thatonly takes into account the distributional prop-erties of the sounds present in the native input(Maye et al., 2002).
Unsupervised clustering al-gorithms running on simplified input have, indeed,provided a proof of principle for bottom-up learn-ing of phonemic categories from speech (see forinstance Vallabha et al., 2007).It is clear, however, that distributional learningcannot account for the entire developmental pat-tern.
In fact, phoneme tokens in real speech ex-hibit high acoustic variability and result in phone-mic categories with a high degree of overlap (Hil-lenbrand et al., 1995).
When purely bottom upclustering algorithms are tested on realistic input,they ended up in either a too large number of sub-phonemic units (Varadarajan et al., 2008) or a toosmall number of coarse grained categories (Feld-man et al., 2013a).1911.2 The top-down hypothesisInspection of the developmental data shows thatinfants do not wait to have completed the acqui-sition of their native phonemes to start to learnwords.
In fact, lexical and phonological acquisi-tion largely overlap.
Infant can recognize highlyfrequent word forms like their own names, by asearly as 4 months of age (Mandel et al., 1995).Vice versa, the refinement of phonemic categoriesdoes not stop at 12 months.
The sensitivity to pho-netic contrasts has been reported to continue at 3years of age (Nittrouer, 1996) and beyond (Hazanand Barrett, 2000), on par with the development ofthe lexicon.Some researchers have therefore suggested thatthere might be a learning synergy which allows in-fants to base some of their acquisition not only onbottom up information, but also on statistics overlexical items or even on the basis of word mean-ing (Feldman et al., 2013a; Feldman et al., 2013b;Yeung and Werker, 2009)These experiments and computational models,however, have focused on simplified input or/andused already segmented words.
It remains to beshown whether the said top-down strategies scaleup when real size corpora and more realistic repre-sentations are used.
There are indeed indicationsthat, in the absence of a proper phonological repre-sentation, lexical learning becomes very difficult.For example, word segmentation algorithms thatwork on the basis of phoneme-like units tend todegrade quickly if phonemes are replaced by con-textual allophones (Boruta et al., 2011) or with theoutput of phone recognizers (Jansen et al., 2013;Ludusan et al., 2014).In brief, we are facing a chicken-and-egg prob-lem: lexical and semantic information could helpto learn the phonemes, but phonemes are neededto acquire lexical information.1.3 Breaking the circularity: An incrementaldiscovery procedureHere, we explore the idea that instead of learningadult-like hierarchically organized representationsin a sequential fashion (phonemes, words, seman-tics), infants learn approximate, provisional lin-guistic representations in parallel.
These approxi-mate representations are subsequently used to im-prove each other.More precisely, we make four assumptions.First, we assume that infants start by paying atten-tion to fine grained variation in the acoustic input,thus constructing perceptual phonetic categoriesthat are not phonemes, but segments encoding finegrained phonetic details (Werker and Curtin, 2005;Pierrehumbert, 2003).
Second, we assume thatthese units enable infants to segment proto-wordsfrom continuous speech and store them in this de-tailed format.
Importantly, this proto-lexicon willnot be adult-like: it will contain badly segmentedword forms, and store several alternant forms forthe same word.
Ngon et al.
(2013) have shownthat 11 month old infants recognize frequent soundsequences that do not necessarily map to adultwords.
Third, we assume that infants can use thisimperfect lexicon to acquire some semantic repre-sentation.
As shown in Shukla et al.
(2011), in-fants can simultaneously segment words and asso-ciate them with a visual referent.
Fourth, we as-sume that as their exposure to language develops,infants reorganize these initial categories along therelevant dimensions of their native language basedon cues from all these representations.The aim of this work is to provide a proof ofprinciple for this general scenario, using real sizecorpora in two typologically different languages,and state-of-the-art learning algorithms.The paper is organized as follows.
We beginby describing how we generated the input andhow we modeled different levels of representation.Then, we explain how information from the higherlevels (word forms and semantics) can be used torefine the learning of the lower level (phonetic cat-egories).
Next, we present the results of our sim-ulations and discuss the potential implications forthe language learning process.2 Modeling the representationsHere, we describe how we model different levelsof representation (phonetic categories, lexicon andsemantics) starting from raw speech in Englishand Japanese.2.1 CorpusWe use two speech corpora: the Buckeye Speechcorpus (Pitt et al., 2007), which contains 40 hoursof spontaneous conversations in American En-glish, and the 40 hours core of the Corpus of Spon-taneous Japanese (Maekawa et al., 2000), whichcontains spontaneous conversations and publicspeeches in different fields, ranging from engi-neering to humanities.
Following Boruta (2012),192we use an inventory of 25 phonemes for transcrib-ing Japanese, and for English, we use the set of 45phonemes in the phonemic transcription of Pitt etal.
(2007).2.2 Phonetic categoriesHere, we describe how we model the percep-tual phonetic categories infants learn in a firststep before converging on the functional cate-gories (phonemes).
We make the assumption thatthese initial categories correspond to fine grainedallophones, i.e., different systematic realizationsof phonemes, depending on context.
Allophonicvariation can range from categorical effects due tophonological rules to gradient effects due to coar-ticulation, i.e, the phenomenon whereby adjacentsounds affect the physical realization of a givenphoneme.
An example of a rather categorical allo-phonic rule is given by /r/ devoicing in French:/r/?
{[X] / before a voiceless obstruent[K] elsewhereFigure 1: Allophonic variation of French /r/The phoneme /r/ surfaces as voiced ([K]) be-fore a voiced obstruent like in [kanaK Zon] (?ca-nard jaune?, yellow duck) and as voiceless ([X])before a voiceless obstruent as in [kanaX puXpK](?canard pourpre?, purple duck).
The challengefacing the leaner is, therefore, to distinguish pairsof segments that are in an allophonic relationship([K], [X]) from pairs that are two distinct phonemesand can carry a meaning difference ([K],[l]).Previous work has generated allophonic varia-tion artificially (Martin et al., 2013).
Here, we fol-low Fourtassi et al.
(2014b) in using a linguisti-cally and statistically controlled method, startingfrom audio recordings and using a standard Hid-den Markov Models (HMM) phone recognizer togenerate them, as follows.We convert the raw speech waveform into suc-cessive 10ms frames containing a vector of MelFrequency Cepstrum Coefficients (MFCC).
Weuse 12 MFC coefficients (plus the energy) com-puted over a 25ms window, to which we add thefirst and second order derivatives, yielding 39 di-mensions per frame.The HMM training starts with one three-statemodel per phoneme.
Each state is modeled bya mixture of 17 diagonal Gaussians.
After train-ing, each phoneme model is cloned into context-dependent triphone models, for each context inwhich the phoneme actually occurs (for example,the phoneme /A/ occurs in the context [d?A?g] asin the word /dAg/ (?dog?).
The triphone modelscloned from the phonemes are then retrained, but,this time, only on the relevant subset of the data,corresponding to the given triphone context.
Fi-nally, these detailed models are clustered back intoinventories of various sizes (from 2 to 20 timesthe size of the phonemic inventory) and retrained.Clustering is done state by state using a phoneticfeature-based decision tree, and results in tyingtogether the HMM states of linguistically simi-lar triphones so as to maximize the likelihood ofthe data.
The HMM were built using the HMMToolkit (HTK: Young et al., 2006).2.3 The proto-lexiconFinding word boundaries in the continuous se-quence of phones is part of the problem infantshave to solve without direct supervision.
Wemodel this segmentation using a state-of-the-artunsupervised word segmentation model based onthe Adaptor Grammar framework (Johnson et al.,2007).
The input consists of a phonetic transcrip-tion of the corpus, with boundaries between wordseliminated (we vary this transcription to corre-spond to different inventories with different granu-larity in the allophonic representation as explainedabove).
The model tries to reconstruct the bound-aries based on a Pitman-Yor process (Pitman andYor, 1997), which uses a language-general sta-tistical learning process to find a compact rep-resentation of the input.
The algorithm storeshigh frequency chunks and re-uses them to parsenovel utterances.
We use a grammar which learnsa hierarchy of three levels of chunking and usethe intermediate level to correspond to the lexi-cal level.
This grammar was shown by Fourtassiet al.
(2013) to avoid both over-segmentation andunder-segmentation.2.4 The proto-semanticsIt has been shown that infants can keep track of co-occurrence statistics (see Lany and Saffran (2013)for a review).
This ability can be used to develop asense of semantic similarity as suggested by Har-ris (1954).
The intuition behind the distributionalhypothesis is that words that are similar in mean-ing occur in similar contexts.
In order to modelthe acquisition of this semantic similarity from a193transcribed and segmented corpus, we use one ofthe simplest and most commonly used distribu-tional semantic models, Latent Semantic Analysis(LSA: Landauer & Dumais, 1997).
The LSA al-gorithm takes as input a matrix consisting of rowsrepresenting word types and columns represent-ing contexts in which tokens of the word type oc-cur.
A context is defined as a fixed number ofutterances.
Singular value decomposition (a kindof matrix factorization) is used to extract a morecompact representation.
The cosine of the anglebetween vectors in the resulting space is used tomeasure the semantic similarity between words.Two words have a high semantic similarity if theyhave similar distributions, i.e., if they co-occur inmost contexts.
The model parameters, namely thedimension of the semantic space and the numberof utterances to be taken as defining the contextof a given word form, are set in an unsupervisedway to optimize the latent structure of the seman-tic model (Fourtassi and Dupoux, 2013).
Thus, weuse 20 utterances as a semantic window and set thesemantic space to 100 dimensions.3 MethodHere we explore whether the approximate highlevel representations, built bottom-up and with-out supervision, still contain useful informationone can use to refine the phonetic categories intophoneme-like units.
To this end, we extract po-tential cues from the lexical and the semantic in-formation, and test their performance in discrim-inating allophonic contrasts from non-allophonic(phonemic) contrasts.3.1 Top down cues3.1.1 Lexical cueThe top down information from the lexicon isbased on the insight of Martin et al.
(2013).
It restson the idea that true lexical minimal pairs are notvery frequent in human languages, as compared tominimal pairs due to mere phonological processes(figure 1).
The latter creates alternants of the samelexical item since adjacent sounds condition therealization of the first and final phoneme.
There-fore, finding a minimal pair of words differing inthe first or last segment (as in [kanaX] and [kanaK])is good evidence that these two phones ([K], [X])are allophones of one another.
Conversely, if apair of phones is not forming any minimal pair,it is classified as non-allophonic (phonemic).However, this binary strategy clearly gives riseto false alarms in the (albeit relatively rare) caseof true minimal pairs like [kanaX] (?duck?)
and[kanal] (?canal?
), where ([X], [l]) will be mis-takenly labeled as allophonic.
In order to miti-gate the problem of false alarms, we use Boruta?scontinuous version (Boruta, 2011) and we definethe lexical cue of a pair of phones Lex(x, y) asthe number of lexical minimal pairs that vary onthe first segment (xA, yA) or the last segment(Ax,Ay).
The higher this number, the more thepair of phones is likely to be considered as allo-phonic.The lexical cue is consistent with experimen-tal findings.
For example Feldman et al.
(2013b)showed that 8 month-old infants pay attentionto word level information, and demonstrated thatthey do not discriminate between sound contraststhat occur in minimal pairs (as suggested by ourcue), and, conversely, discriminate contrasts thatoccur in non-minimal pairs.3.1.2 Semantic cueThe semantic cue is based on the intuition thattrue minimal pairs ([kanaX] and [kanal]) are asso-ciated with different events, whereas alternants ofthe same word ([kanaX] and [kanal]) are expectedto co-occur with similar events.We operationalize the semantic cue associatedwith a pair of phones Sem(x, y) as the averagesemantic similarity between all the lexical mini-mal pairs generated by this pair of phones.
Thehigher the average semantic similarity, the morethe learner is prone to classify them as allophonic.We take as a measure of the semantic similar-ity, the cosine of the angle between word vec-tors of the pairs that vary on the final segmentcos(?Ax,Ay) or the first segment cos(?xA, yA).This strategy is similar in principle to the phe-nomenon of acquired distinctiveness, accordingto which, pairing two target stimuli with distinctevents enhances their perceptual differentiation,and acquired equivalence, whereby pairing twotarget stimuli with the same event, impairs theirsubsequent differentiation (Lawrence, 1949).
Inthe same vein, Yeung and Werker (2009) tested 9month-olds english learning infants in a task thatconsists in discriminating two non-native phoneticcategories.
They found that infants succeeded onlywhen the categories co-occurred with two distinctvisual cues.194?
Segmentation Lexicon?
English Japanese English JapaneseAllo./phon.
F P R F P R F P R F P R2 0.61 0.57 0.65 0.45 0.44 0.47 0.29 0.42 0.22 0.23 0.54 0.154 0.52 0.46 0.59 0.38 0.34 0.43 0.22 0.37 0.15 0.16 0.50 0.1010 0.51 0.45 0.59 0.34 0.30 0.38 0.21 0.34 0.16 0.16 0.41 0.1020 0.42 0.38 0.47 0.28 0.26 0.32 0.21 0.29 0.17 0.16 0.32 0.10Table 1 : Scores of the segmentation and the resulting lexicon, as a function of the average number ofallophones per phoneme.
P=Precison, R=Recall and F=F-score.3.1.3 Combined cueFinally, we consider the combination of both cuesin one single cue where the contextual information(semantics) is used as a weighing scheme of thelexical information, as follows:Comb(x, y) =?
(Ax,Ay)?L2cos(?Ax,Ay) +?
(xA,yA)?L2cos(?xA, yA)(1)where {Ax ?
L} is the set of words in the lex-icon L that end in the phone x, and {(Ax,Ay) ?L2} is the set of phonological minimal pairs inL?
L that vary on the final segment.The lexical cue is incremented by one, for ev-ery minimal pair.
The combined cue is, instead,incremented by one, times the cosine of the anglebetween the word vectors of this pair.
When thewords have similar distributions, the angle goes tozero and the cosine goes to 1, and when the wordshave orthogonal distributions, the angle goes to90?and the cosine goes to 0.The semantic information here would basicallyenable us to avoid false alarms generated by poten-tial true minimal pairs like the above-mentionedexample of ( [kanaX] and [kanal]).
Such a pair willprobably score high as far as the lexical cue is con-cerned, but it will score low on the semantic level.Thus, by taking the combination, the model willbe less prone to mistakenly classify ([X], [l]) as al-lophones.3.2 TaskFor each corpus we list all possible pairs of al-lophones.
Some of these pairs are allophones ofthe same phoneme (allophonic pair) and others areallophones of different phonemes (non-allophonicpairs).
The task is a same-different classification,whereby each of these pairs is given a score fromthe cue that is being tested.
A good cue giveshigher scores to allophonic pairs.Only pairs of phones that generate at least onelexical minimal pair are considered.
Phonetic vari-ation that does not cause lexical variation is ?in-visible?
to top down strategies, and is, therefore,more probably clustered through purely bottom upstrategies (Fourtassi et al., 2014b)3.3 EvaluationWe use the same evaluation procedure as Martin etal.
(2013).
This is carried out by computing the as-sociated ROC curve (varying the z-score thresholdand computing the resulting proportions of missesand false alarms).
We then derive the Area Underthe Curve (AUC), which also corresponds to theprobability that given two pairs of phones, one al-lophonic, one not, they are correctly classified onthe basis of the score.
A value of 0.5 representschance and a value of 1 represents perfect perfor-mance.In order to lessen the potential influence of thestructure of the corpus (mainly the order of the ut-terances) on the results, we use a statistical resam-pling scheme.
The corpus is divided into smallblocks of 20 utterances each (the semantic win-dow).
In each run, we draw randomly with re-placement from this set of blocks a sample ofthe same size as the original corpus.
This sam-ple is then used to retrain the acoustic models andgenerate a phonetic inventory that we used to re-transcribe the corpus and re-compute the cues.
Wereport scores averaged over 5 such runs.4 Results and discussion4.1 SegmentationWe first explore how phonetic variation influencesthe quality of the segmentation and the resultinglexicon.
For the evaluation, we use the same mea-sures as Brent (1999) and Goldwater et al.
(2009),namely Segmentation Precision (P), Recall (R)and F-score (F).
Segmentation precision is defined195as the number of correct word tokens found, out ofall tokens posited.
Recall is the number of correctword tokens found, out of all tokens in the idealsegmentation.
The F-score is defined as the har-monic mean of Precision and Recall:F =2 ?
P ?RP + RWe define similar measures for word types (lex-icon).
Table 1 shows the scores as a function ofthe number of allophones per phonemes.
For bothcorpora, the segmentation performance decreasesas we increase the number of allophones.
As forthe lexicon, the recall scores show that only 15to 22% of the ?words?
found by the algorithm inthe English corpus are real words; in Japanese,this number is even lower (between 10 and 15%).This pattern can be attributed in part to the factthat increasing the number of allophones increasesthe number of word forms, which occur thereforewith less frequency, making the statistical learn-ing harder.
Table 2 shows the average number ofword forms per word as a function of the averagenumber of allophones per phoneme, in the case ofideal segmentation.Allo./Phon.
W. forms/Word?
English Japanese2 1.56 1.204 2.03 1.6410 2.69 2.1120 3.47 2.83Table 2 : Average number of word-forms perword as a function of the average number ofallophones per phoneme.Another effect seen in Table 1 is the loweroverall performance of Japanese compared to En-glish.
This difference was shown by Fourtassi etal.
(2013) to be linked to the intrinsic segmenta-tion ambiguity of Japanese, caused by the fact thatJapanese words contain more syllables comparedto English.4.2 Allophonic vs phonemic status of soundcontrastsHere we test the performance of the cues describedabove, in discriminating between allophonic con-trasts from phonemic ones.
We vary the numberof allophones per phoneme, on the one hand (Fig-ure 2a), and the amount of data available to thelearner, on the other hand, in the case of two allo-phones per phonemes (Figure 2b).
In both situa-tions, we compare the case wherein the lexical andsemantic cues are computed on the output of theunsupervised segmentation (right), to the controlcase where these cues are computed on the ideallysegmented speech (left).We see that the overall accuracy of the cues isquite high, even in the case of bad word segmen-tation and very small amount of data.The lexical cue is robust to extreme variationand to the scarcity of data.
Indeed, it does not seemto vary monotonically neither with the number ofallophones, nor with the size of the corpus.
The as-sociated f-score generally remains above the valueof 0.7 (chance level is 0.5).
The semantics, onthe other hand, gets better as the variability de-creases and as the amount of data increases.
Thisis a natural consequence of the fact that the se-mantic structure is more accurate with more dataand with word forms consistent enough to sustaina reasonable co-occurrence statistics.The comparison with the ideal segmentation,shows, interestingly, that the semantics is more ro-bust to segmentation errors than the lexical cue.
Infact, while the lexical strategy performs, overall,better than the semantics under the ideal segmen-tation, the patterns reverses as we move to a a morerealistic (unsupervised) segmentation.These results suggest that both lexical and se-mantic strategies can be crucial to learning thephonemic status of phonetic categories since theyprovide non-redundant information.
This findingis summarized by the combined cue which resiststo both variation and segmentation errors, overall,better than each of the cues taken alone.From a developmental point of view, this showsthat infants can, in principle, benefit from higherlevel linguistic structures to refine their phoneticcategories, even if these structures are rudimen-tary.
Previous studies about top down strategieshave mainly emphasized the role of word forms;the results of this work show that the semanticscan be at least as useful.
Note that the notionof semantics used here is weaker than the clas-sic notion of referential semantics as in a word-concept matching.
The latter might, indeed, notbe fully operative at the early stages of the childdevelopment, since it requires some advanced con-ceptual abilities (like forming symbolic represen-tations and understanding a speaker?s referential196a)Ideal Unsupervised0.50.60.70.80.91.02 5 10 20 2 5 10 20Allophones/PhonemeAUCEnglishIdeal Unsupervised0.50.60.70.80.91.02 5 10 20 2 5 10 20Allophones/PhonemeAUCJapaneseCuesLexicalSemanticCombinedb)Ideal Unsupervised0.50.60.70.80.91.01 2 4 8 20 40 1 2 4 8 20 40Size (in hours)AUCEnglishIdeal Unsupervised0.50.60.70.80.91.01 2 4 8 20 40 1 2 4 8 20 40Size (in hours)AUCJapaneseCuesLexicalSemanticCombinedFigure 2: Same-different scores (AUC) for different cues as a function of the average number of allo-phones per phoneme (a), and as a function of the size of the corpus, in the case of two allophones perphonemes (b).
The scores are shown for both ideal and unsupervised word segmentation in English andJapanese.
The points show the mean scores over 5 runs.
The lines are smoothed interpolations (localregressions) through the means.
The grey band shows a 95% confidence interval.intentions) (Waxman and Gelman, 2009).
Whatwe call the ?semantics?
of a word in this study, isthe general context provided by the co-occurrencewith other words.
Infants have been shown to havea powerful mechanism for tracking co-occurrencerelationships both in the speech and the visual do-main (Lany and Saffran, 2013) .
Our experimentsdemonstrate that a similar mechanism could beenough to develop a sense of semantic similaritythat can successfully be used to refine phoneticcategories.5 General discussion and future workPhonemes are abstract categories that form the ba-sis for words in the lexicon.
There is a traditionalview that they should be defined by their ability tocontrast word meanings (Trubetzkoy, 1939).
Theirfull acquisition, therefore, requires lexical and se-mantic top-down information.
However, since thequality of the semantic representations depends onthe quality of the phonemic representations thatare used to build the lexicon, we face a chicken-and-egg problem.
In this paper, we proposed away to break the circularity by building approxi-mate representation at all the levels.The infants?
initial attunement to language-specific categories was represented in a way thatmirrors the linguistic and statistical properties ofthe speech closely.
We showed that this de-tailed (proto-phonemic) inventory enabled wordsegmentation from continuous transcribed speech,but, as expected, resulted in a low quality lexicon.The poorly segmented corpus was then used to de-rive a semantic similarity matrix between pairs ofwords, based on their co-occurrence statistics.
Theresults showed that information from the derivedlexicon and semantics, albeit very rudimentary,help discriminate between allophonic and phone-mic contrasts, with a high degree of accuracy.Thus, this works strongly support the claim thatthe lexicon and semantics play a role in the re-finement of the phonemic inventory (Feldman et197al., 2013a; Frank et al., 2014), and, interestingly,that this role remains functional under more realis-tic assumptions (unsupervised word segmentation,and bottom-up inferred semantics).
We also foundthat lexical and semantic information were not re-dundant and could be usefully combined, the for-mer being more resistant to the scarcity of dataand variation, and the latter being more resistantto segmentation errors.That being said, this work relies on the assump-tion that infants start with initial perceptual cate-gories (allophones), but we did not show how suchcategories could be constructed from raw speech.More work is needed to explore the robustness ofthe model when these units are learned in an unsu-pervised fashion (Lee and Glass, 2012; Huijbregtset al., 2011; Jansen and Church, 2011; Varadarajanet al., 2008).This work could be seen as a proof of princi-ple for an iterative learning algorithm, wherebyphonemes emerge from the interaction of low levelperceptual categories, word forms, and the seman-tics (see Werker and Curtin (2005) for a similartheoretical proposition).
The algorithm has yet tobe implemented, but it has to address at least twomajor issues: First, the fact that some sound pairsare not captured by top down cues because theydo not surface as minimal word forms.
For in-stance, in English, /h/ and /N/ occur in differentsyllable positions and therefore, cannot appear inany minimal pair.
Second, even if we have enoughinformation about how phonetic categories are or-ganized in the perceptual space, we still need toknow how many categories are relevant in a par-ticular language (i.e., where to stop the categoriza-tion process).For the first problem, Fourtassi et al.
(2014b)showed that the gap could, in principle, be filled bybottom-up information (like acoustic similarity).As for the second problem, a possible directioncould be found in the notion of Self-Consistency.In fact, (Fourtassi et al., 2014a) proposed that anoptimal level of clustering is also a level that glob-ally optimizes the predictive power of the lexicon.Too detailed allophones result in too many syn-onyms.
Too broad classes result in too many ho-mophones.
Somewhere in the middle, the optimalnumber of phonemes optimizes how lexical itemspredict each other.
Future work will address theseissues in more detail in order to propose a com-plete phoneme learning algorithm.AcknowledgmentsThis work was supported in part by the Euro-pean Research Council (ERC-2011-AdG-295810BOOTPHON), the Agence Nationale pour laRecherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02 PSL*), the Fondation de France,the Ecole de Neurosciences de Paris, and theR?egion Ile de France (DIM cerveau et pens?ee).ReferencesElika Bergelson and Daniel Swingley.
2012.
At 6to 9 months, human infants know the meanings ofmany common nouns.
Proceedings of the NationalAcademy of Sciences, 109(9).Luc Boruta, Sharon Peperkamp, Beno?
?t Crabb?e, andEmmanuel Dupoux.
2011.
Testing the robustnessof online word segmentation: Effects of linguisticdiversity and phonetic variation.
In Proceedings ofCMCL, pages 1?9.
Association for ComputationalLinguistics.Luc Boruta.
2011.
Combining Indicators of Al-lophony.
In Proceedings ACL-SRW, pages 88?93.Luc Boruta.
2012.
Indicateurs d?allophonie etde phon?emicit?e.
Doctoral dissertation, Universit?eParis-Diderot - Paris VII.M.
Brent.
1999.
An efficient, probabilistically soundalgorithm for segmentation and word discovery.Machine Learning, 34:71?105.N.
Feldman, T. Griffiths, S. Goldwater, and J. Morgan.2013a.
A role for the developing lexicon in pho-netic category acquisition.
Psychological Review,120(4):751?778.N.
Feldman, B. Myers, K. White, T. Griffiths, andJ.
Morgan.
2013b.
Word-level information influ-ences phonetic learning in adults and infants.
Cog-nition, 127:427?438.Abdellah Fourtassi and Emmanuel Dupoux.
2013.
Acorpus-based evaluation method for distributionalsemantic models.
In 51st Annual Meeting of theAssociation for Computational Linguistics Proceed-ings of the Student Research Workshop, pages 165?171, Sofia, Bulgaria.
Association for ComputationalLinguistics.Abdellah Fourtassi, Benjamin B?orschinger, MarkJohnson, and Emmanuel Dupoux.
2013.
WhyisEn-glishsoeasytosegment?
In Proceedings of CMCL,pages 1?10.
Association for Computational Linguis-tics.Abdellah Fourtassi, Ewan Dunbar, and EmmanuelDupoux.
2014a.
Self-consistency as an inductivebias in early language acquisition.
In Proceedingsof the 36th annual meeting of the Cognitive ScienceSociety.198Abdellah Fourtassi, Thomas Schatz, BalakrishnanVaradarajan, and Emmanuel Dupoux.
2014b.
Ex-ploring the Relative Role of Bottom-up and Top-down Information in Phoneme Learning.
In Pro-ceedings of the 52nd Annual Meeting of the Asso-ciation for Computational Linguistics.Stella Frank, Naomi Feldman, and Sharon Goldwater.2014.
Weak semantic context helps phonetic learn-ing in a model of infant language acquisition.
InProceedings of the 52nd Annual Meeting of the As-sociation of Computational Linguistics.Judit Gervain and Jacques Mehler.
2010.
Speech per-ception and language acquisition in the first year oflife.
Annual Review of Psychology, 61:191?218.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2009.
A Bayesian framework for wordsegmentation: Exploring the effects of context.Cognition, 112(1):21?54.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Valerie Hazan and Sarah Barrett.
2000.
The develop-ment of phonemic categorization in children aged 6to12.
Journal of Phonetics, 28:377?396.James Hillenbrand, Laura A. Getty, Michael J. Clark,and Kimberlee Wheeler.
1995.
Acoustic charac-teristics of american english vowels.
Journal of theAcoustical Society of America, 97:3099?3109.M.
Huijbregts, M. McLaren, and D. van Leeuwen.2011.
Unsupervised acoustic sub-word unit detec-tion for query-by-example spoken term detection.
InProceedings of ICASSP, pages 4436?4439.A.
Jansen and K. Church.
2011.
Towards unsupervisedtraining of speaker independent acoustic models.
InProceedings of INTERSPEECH, pages 1693?1696.Aren Jansen, Emmanuel Dupoux, Sharon Goldwa-ter, Mark Johnson, Sanjeev Khudanpur, KennethChurch, Naomi Feldman, Hynek Hermansky, Flo-rian Metze, Richard Rose, Mike Seltzer, PascalClark, Ian McGraw, Balakrishnan Varadarajan, ErinBennett, Benjamin Borschinger, Justin Chiu, EwanDunbar, Abdallah Fourtassi, David Harwath, Chiaying Lee, Keith Levin, Atta Norouzian, VijayPeddinti, Rachel Richardson, Thomas Schatz, andSamuel Thomas.
2013.
A summary of the 2012 jhuclsp workshop on zero resource speech technologiesand models of early language acquisition.
In Pro-ceedings of ICASSP.Mark Johnson, Thomas L. Griffiths, and Sharon Gold-water.
2007.
Adaptor Grammars: A framework forspecifying compositional nonparametric Bayesianmodels.
In B. Sch?olkopf, J. Platt, and T. Hoffman,editors, Advances in Neural Information ProcessingSystems 19, pages 641?648.
MIT Press, Cambridge,MA.Peter W Jusczyk and Richard N Aslin.
1995.
Infants?detection of the sound patterns of words in fluentspeech.
Cognitive psychology, 29(1):1?23.Thomas K Landauer and Susan T Dumais.
1997.
Asolution to Plato?s problem: The Latent SemanticAnalysis theory of acquisition, induction and rep-resentation of knowledge.
Psychological Review,104(2):211?240.J.
Lany and J. Saffran.
2013.
Statistical learning mech-anisms in infancy.
In J. Rubenstein and P. Rakic, ed-itors, Comprehensive Developmental Neuroscience:Neural Circuit Development and Function in theBrain, volume 3, pages 231?248.
Elsevier, Amster-dam.D.H.
Lawrence.
1949.
Acquired distinctiveness ofcues: I. transfer between discriminations on the ba-sis of familiarity with the stimulus.
Journal of Ex-perimental Psychology, 39(6):770?784.C.
Lee and J.
Glass.
2012.
A nonparametric bayesianapproach to acoustic model discovery.
In Proceed-ings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics: Long Papers-Volume 1, pages 40?49.Bogdan Ludusan, Maarten Versteegh, Aren Jansen,Guillaume Gravier, Xuan-Nga Cao, Mark Johnson,and Emmanuel Dupoux.
2014.
Bridging the gap be-tween speech technology and natural language pro-cessing: an evaluation toolbox for term discoverysystems.
In Proceedings of LREC.Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-toshi Isahara.
2000.
Spontaneous speech corpus ofjapanese.
In LREC, pages 947?952, Athens, Greece.D.R.
Mandel, P.W.
Jusczyk, and D.B.
Pisoni.
1995.
In-fants?
recognition of the sound patterns of their ownnames.
Psychological Science, 6(5):314?317.Andrew Martin, Sharon Peperkamp, and EmmanuelDupoux.
2013.
Learning phonemes with a proto-lexicon.
Cognitive Science, 37(1):103?124.J.
Maye, J. F. Werker, and L. Gerken.
2002.
Infant sen-sitivity to distributional information can affect pho-netic discrimination.
Cognition, 82:B101?B111.C.
Ngon, A. Martin, E. Dupoux, D. Cabrol, M. Duthat,and S. Peperkamp.
2013.
(non)words, (non)words,(non)words: evidence for a protolexicon during thefirst year of life.
Developmental Science, 16(1):24?34.S.
Nittrouer.
1996.
Discriminability and perceptualweighting of some acoustic cues to speech percep-tion by 3-year-olds.
Journal of Speech and HearingResearch, 39:278?297.J.
B. Pierrehumbert.
2003.
Phonetic diversity, statis-tical learning, and acquisition of phonology.
Lan-guage and Speech, 46(2-3):115?154.199J.
Pitman and M. Yor.
1997.
The two-parameterPoisson-Dirichlet distribution derived from a stablesubordinator.
Annals of Probability, 25:855?900.M.
A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-mond, E. Hume, and Fosler-Lussier.
2007.
Buckeyecorpus of conversational speech.M Shukla, K White, and R Aslin.
2011.
Prosodyguides the rapid mapping of auditory word formsonto visual objects in 6-mo-old infants.
Proceedingsof the National Academy of Sciences, 108(15):6038?6043.N.
S. Trubetzkoy.
1939.
Grundz?uge der Phonolo-gie (Principles of phonology).
Vandenhoeck &Ruprecht, G?ottingen, Germany.G.
K. Vallabha, J. L. McClelland, F. Pons, J. F.Werker, and S. Amano.
2007.
Unsupervised learn-ing of vowel categories from infant-directed speech.Proceedings of the National Academy of Sciences,104(33):13273.Balakrishnan Varadarajan, Sanjeev Khudanpur, andEmmanuel Dupoux.
2008.
Unsupervised learningof acoustic sub-word units.
In Proceedings of ACL-08: HLT, Short Papers, pages 165?168.
Associationfor Computational Linguistics.Sandra R. Waxman and Susan A. Gelman.
2009.
Earlyword-learning entails reference, not merely associa-tions.
Trends in Cognitive Sciences, 13(6):258?263.J.
F. Werker and S. Curtin.
2005.
PRIMIR: A develop-mental framework of infant speech processing.
Lan-guage Learning and Development, 1(2):197?234.Janet F. Werker and Richard C. Tees.
1984.
Cross-language speech perception: Evidence for percep-tual reorganization during the first year of life.
In-fant Behavior and Development, 7(1):49 ?
63.H Yeung and J Werker.
2009.
Learning words?
soundsbefore learning how words sound: 9-month-olds usedistinct objects as cues to categorize speech infor-mation.
Cognition, 113:234?243.Steve J.
Young, D. Kershaw, J. Odell, D. Ollason,V.
Valtchev, and P. Woodland.
2006.
The HTK BookVersion 3.4.
Cambridge University Press.200
