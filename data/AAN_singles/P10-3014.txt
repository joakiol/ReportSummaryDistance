Proceedings of the ACL 2010 Student Research Workshop, pages 79?84,Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational LinguisticsEdit Tree Distance alignments for Semantic Role LabellingHector-Hugo Franco-PenyaTrinity College DublinDublin, Ireland.francoph@cs.tcd.ieAbstract?Tree SRL system?
is a Semantic Role Label-ling supervised system based on a tree-distancealgorithm and a simple k-NN implementation.The novelty of the system lies in comparing thesentences as tree structures with multiple rela-tions instead of extracting vectors of featuresfor each relation and classifying them.
The sys-tem was tested with the English CoNLL-2009shared task data set where 79% accuracy wasobtained.1 IntroductionSemantic Role Labelling (SRL) is a natural lan-guage processing task which deals with semanticanalysis at sentence-level.
SRL is the task ofidentifying arguments for a certain predicate andlabelling them.
The predicates are usually verbs.They establish ?what happened?.
The argumentsdetermine events such as ?who?, ?whom?,?where?, etc, with reference to one predicate.The possible semantic roles are pre-defined foreach predicate.
The set of roles depends on thecorpora.SRL is becoming an important tool for infor-mation extraction, text summarization, machinetranslation and question answering (M?rquez, etal, 2008).2 The dataThe data set I used is taken from the CoNLL-2009 shared task (Haji?
et al, 2009) and is partof Propbank.
Propbank (Palmer et al 2005) is ahand-annotated corpus.
It transforms sentencesinto propositions.
It adds a semantic layer to thePenn TreeBank (Marcus et al 1994) and definesa set of semantic roles for each predicate.It is difficult to define universal semantic rolesfor all predicates.
That is why PropBank definesa set of semantic roles for each possible sense ofeach predicate (frame) [See a sample of theframe ?raise?
on the Figure 1 caption].The core arguments are labelled by numbers.Adjuncts, which are common to all predicates,have their own labels, like: AM-LOC, TMP,NEG, etc.
The four most frequent labels in thedata set are: A1:35%, A0:20.86%, A2:7.88% andAM-TMP: 7.72%Propbank was originally built using constitu-ent tree structures, but here only the dependencytree structure version was used.
Note that de-pendency tree structures have labels on the ar-rows.
The tree distance algorithm cannot workwith these labelled arrows and so they are movedto the child node as an extra label.The task performed by the Tree SRL systemconsists of labelling the relations (predicate ar-guments) which are assumed to be already iden-tified.3 Tree DistanceThe tree distance algorithm has already been ap-plied to text entailment (Kouylekov & Magnini,2005) and question answering (Punyakanok et al2004; Emms, 2006) with positive results.The main contribution of this piece of work tothe SRL field is the inclusion of the tree distancealgorithm into an SRL system, working with treestructures in contrast to the classical ?feature ex-traction?
and ?classification?.
Kim et al(2009)developed a similar system for Information Ex-traction.SentencespredicatesargumentsPredicatespersentenceargumentspersub-treeFilesizeinMbTra 39279 179014 393699 4.55 2.20 56.2Dev 1334 6390 13865 4.79 2.17 1.97Evl 2399 10498 23286 4.38 2.22 3.41Table 1: The dataThe data set is divided into three files: training(Tra), development (Dev) and evaluation (Evl).The following table describes the number ofsentences, sub-trees and labels contained inthem, and the ratios of sub-trees per sentencesand relations per sub-tree.79Tai (1979) introduced a criterion for matchingnodes between tree representations (or convert-ing one tree into another one) and (Shasha &Zhang, 1990; Zhang & Shasha, 1989) developedan algorithm that finds an optimal matching treesolution for any given pair of trees.
The advan-tage of this algorithm is that its computationalcost is low.
The optimal matching depends onthe defined atomic cost of matching two nodes.4 Tree SRL system architectureFor the training and testing data set, all possiblesub-trees were extracted.
Figure 3 and Figure 5describe the process.
Then, using the tree dis-tance algorithm, the test sub-trees are labelledusing the training ones.
Finally, the predictedlabels get assembled on the original sentencewhere the test sub-tree came from.
Figure 2 de-scribes the process.A sub-tree extracted from a sentence, containsa predicate node, all its argument nodes and allthe ancestors up to the first common ancestor ofall nodes.
(Figure 1 shows two samples of sub-tree extraction.
Figure 3 describes how sub treesare obtained)Figure 1: Alignment sampleA two sentence sample, in a dependency tree representation.
In each node, the word form and theposition of the word in the sentence are shown.
Straight arrows represent syntactic dependencies.
Thelabel of the dependency is not shown.
The square node represent the predicate that is going to be ana-lyzed, (there can be multiple predicates in a single sentence).
Semi-dotted arrows between a squarenode and an ellipse node represent a semantic relation.
This arrow has a semantic tag (A1, A2, A3and A4).The grey shadow contains all the nodes of the sub tree for the ?rose?
predicate.The dotted double arrows between the nodes of both sentences represent the tree distance alignmentfor both sub-trees.
In this particular case every single node is matched.Both predicate nodes are samples of the frame ?raise?
sense 01 (which means ?go up quantifiably?
)where the core arguments are:A0: Agent, causer of motion A1: Logical subject, patient, thing risingA2: EXT, amount raised A3: Start point A4: End point AM: Medium805 LabellingSuppose that in Figure 1, the bottom sentence isthe query, where the grey shadow contains thesub-tree to be labelled and the top sentence con-tains the sub-tree sample chosen to label thequery.
Then, an alignment between the samplesub-tree and the query sub-tree suggests labellingthe query sub-tree with A1, A2 and A3, wherethe first two labels are right but the last label, A4,is predicted as A3, so it is wrong.It is not necessary to label a whole sub-tree(query) using just a single sub-tree sample.
How-ever, if the whole query is labelled using a singleanswer sample, the prediction is guaranteed to beconsistent (no repeated argument labels).Some possible ways to label the semantic rela-tion using a sorted list of alignments (with eachsub-tree of the training data set) is discussedahead.
Each sub-tree contains one predicate andseveral semantic relations, one for each argumentnode.5.1 Treating relations independentlyIn this sub-section, the neighbouring sub-treesfor one relation of a sub-tree T refers to the near-Input: T: tree structure labelled in post ordertraversalInput: L: list of nodes to be on the sub-tree inpost order traversalOutput: T: Sub-Treeforeach node x in the list domark x as part of the sub-tree;endwhile L contains more than 2 unique values do[minValue , position]=min(L);Value = parent(minValue);Mark value as part of the sub-tree;L[position] = value;endRemove all nodes that are not marked as partof the sub-tree;Figure 5: Sub-tree extractionInput: A sub-tree to be labelledInput: list of alignments sorted by ascendingtree distanceOutput: labelled sub-treeforeach argument(a) in T doforeach alignment (ali) in the sorted list doif there is a semantic relation(ali.function(p),ali.function(a))Then break loop;endendlabel relation p-a with the label of therelation (ali.function(p),ali.function(a));endp is the node predicate.a is a node argument.ali is an alignment between the sub-tree thathas to be labelled and a sub-tree in the train-ing dataset.The method function is explained in Figure 3.Figure 4: Labelling a relation.
(approachA)Figure 3: Sub-tree extraction sample.Assuming that ?p?
(the square node) is a pre-dicate node and the nodes ?a1?
and ?a2?
areits arguments (the arguments are defined bythe semantic relations.
In this case, the semi-doted arrows.
), the sub-tree extracted from theabove sentence will contain the nodes: ?a1?,?a2?, ?p?, all ancestors of ?a1?,?a2?
and ?p?up to the first common one, in this case node?u?, which is also included in the sub-tree.All of the white nodes are not included in thesub-tree.
The straight lines represent syntacticdependency relations.Input: training data set (labelled)Input: testing data set (unlabelled)Output: testing data set (labelled)Load training and testing data;Adapt the trees for the tree distance algorithm;foreach sentence (training & testing data) doobtain each minimal sub-tree for each pre-dicate;endforeach sub-tree T from the testing data docalculate the distance and the alignmentfrom T to each training sub-tree;sort the list of alignments by ascendingtree distance;use the list to label the sub-tree T;Assemble T labels on the original sentenceEndFigure 2: Tree SRL system pseudo code81est sub-trees with which the match with T pro-duces a match between two predicate nodes andtwo argument nodes.
A label from the nearestneighbour(s) can be transferred to T for labellingthe relation.The current implementation (Approach A),described in more detail in Figure 4, labels a re-lation using the first nearest neighbour from a listordered by ascending tree distance.
If there areseveral nearest neighbours, the first one on thelist is used.
This is a naive implementation of thek-NN algorithm where in case of multiple near-est neighbours only one is used and the othersget ignored.A negative aspect of this strategy is that it canselect a different sub-tree based on the input or-der.
This makes the algorithm indeterministic.
Away to make it deterministic can be by extendingthe parameter ?k?
in case of multiple cases at thesame distance or a tie in the voting (ApproachB).5.2 Treating relations dependentlyIn this section, a sample refers to a sub-tree con-taining all arguments and its labels.
The argu-ments for a certain predicate are related.Some strategies can lead to non-consistentstructures (core argument labels cannot appeartwice in the same sub-tree).
Approach B treatsthe relations independently.
It does not have anymechanism to keep the consistency of the wholepredicate structure.Another way is to find a sample that containsenough information to label the whole sub-tree(Approach C).
This approach always generatesconsistent structures.
The limitation of thismodel is that the required sample may not existor the tree distance may be very high, makingthose samples poor predictors.
The implementedmethod (Approach A) indirectly attempts to finda training sample sub-tree which contains labelsfor all the arguments of the predicate.It is expected for tree distances to be smallerthan other sub-trees that do not have informationto label all the desired relations.The system tries to get a consistent structureusing a simple algorithm.
Only in the case whenusing the nearest tree does not lead to labellingthe whole structure, labels are predicted usingmultiple samples, thereby, risking the structureconsistency.Future implementations will rank possiblecandidate labels for each relation (probably usingmultiple samples).A ?joint scoring algorithm?, which is com-monly used (Marquez et al 2008), can be appliedfor consistency checking after finding the rankprobability for all the argument labels for thesame predicate (Approach D).6 Experiments: the matching costThe cost of matching two nodes is crucial to theperformance of the system.
Different atomicmeasures (ways to measure the cost of matchingtwo nodes) that were tested are explained ahead.Results for experiments using these atomicmeasures are given in Table 2.6.1 Binary systemFor Binary system, the atomic cost of matchingtwo nodes is one if label POS or dependency re-lations are different, otherwise the cost is zero.The atomic cost of inserting or deleting a node isalways one.
Note that the measure is totallybased on the syntactic structure (words are notused).6.2 Ternary systemThe next intuitive measure is how the systemwould perform in case of a ternary cost (ternarysystem).
The atomic cost is half if POS or de-pendency relation is different, one if POS anddependency relation are different or zero in allother case.
For this system, Table 2 shows a verysimilar accuracy to the binary one.6.3 Hamming systemThe atomic cost of matching two nodes is thesum of the following sub costs:0.25  if POS is different.0.25  if dependency relation is different.0.25  if Lemma is different.0.25 if one node is a predicate but the other  isnot or if both nodes are predicates but withdifferent lemma.The cost to create or delete nodes is one.Note that the sum of all costs cannot begreater than one.6.4 Predicate match systemThe analysis of results for the previous systemsshows that the accuracy is higher for the sub-trees that are labelled using sub-trees with thesame predicate node.
Consequently, this strategyattempts to force the predicate to be the same.In this system, the atomic cost of matching twonodes is the sum of the following sub costs:820.3  if POS is different.0.3  if dependency relation is different.1 if one is a predicate and the other nodeis not or both nodes are predicates butwith different lemma.The cost to create or delete nodes is one.6.5 Complex systemThis strategy attempts to improve the accuracyby adding an extra label to the argument nodesand using it.The atomic cost of matching two nodes is thesum of the following sub costs:0.1  for each different label (dependency rela-tion or POS or lemma).0.1  for each pair of different labels (depend-ency relation or POS or lemma).0.4  if one node is a predicate and the other isnot.0.4  if both nodes are predicates and lemma isdifferent.2  if one node is marked as an argument andthe other is not or one node is marked as apredicate and the other is not.The atomic cost of deleting or inserting a nodeis: two if the node is an argument or predicatenode and one in any other case.7 ResultsTable 2 shows the accuracy of all the systems.The validation data set is added to the trainingdata set when the system is labelling the evalua-tion data set.
This is a common methodologyfollowed in CoNLL2009 (Li et al 2009).Accuracy is measured as the percentage of se-mantic labels correctly predicted.The implementation of the Tree SRL systemtakes several days to run a single experiment.
Itmakes non viable the idea of using the develop-ment data set for adjusting parameters and that iswhy, for the last three systems (Hamming, Predi-cate Match and Complex), the accuracy over thedevelopment data set is not measured.
The samereason supports adding the development data setto the training data set without over fitting thesystem, because the development data set is notreally used for adjusting parameters.However, the observations of the system on thedevelopment data set shows:1.
If the complexity gets increased (Ternary),the number of cases having the multiplenearest sub-trees gets reduced.2.
The output of the system only contains fiveper cent of inconsistent structures (Binaryand Ternary), which is lower than expected.0.5% of inconsistent sub-trees were de-tected in the training data-set.3.
Higher accuracy for the relations where asub-tree is labelled using a sub-tree samplewhich has the same predicate node.
This hasled to the design of the ?predicate match?and the ?complex?
systems.4.
Some sub-trees are very small (just onenode).
This resulted in low accuracy forthey predicted labels due to multiple nearestneighbours.It is surprising that the hamming measurereaches higher accuracy than the ?predicatematch?, which uses more information, and is alsosurprising that the accuracies for ?Hamming?,?Predicate Match?
and ?Complex?
systems arevery similar.The CoNLL-2009 SRL shared task was evalu-ated on multiple languages: Catalan, Chinese,Czech, English, German, Japanese and Spanish.Some results for those languages using ?TreeSRL System Binary?
are shown in Table 3.Language Accuracy onevaluationTraining dataset size in MbEnglish 64.36% 56Spanish 57.86% 46Catalan 58.49% 43Japanese 50.71% 8German These languages had been ex-cluded from the experiments be-cause some of the sentences didnot follow a dependency tree struc-ture.CzechChineseTable 3: Accuracy for other languages(Binary system)The accuracy results for multiple languagessuggest that the size of the corpora has a stronginfluence on the results of the system perform-ance.The results are not comparable with the rest ofthe CoNLL-2009 systems because the task isdifferent.
This system does not identify argu-ments and does not perform predicate sense dis-ambiguation.System   Evaluation   DevelopmentBinary  64.36% 61.12%Ternary 64.88% 61.28%Hamming 78.01%PredicateMatch76.98%Complex  78.98%Table 2: System accuracy838 ConclusionThe tree distance algorithm has been appliedsuccessfully to build a SRL system.
Future workwill focus on improving the performance of thesystem by: a) trying to extend the sub-treeswhich will contain more contextual information,b) using different approaches to label semanticrelations discussed in Section 5.
Also, the systemwill be expanded to identify arguments using atree distance algorithm.Evaluating the task of identifying the argu-ments and labelling the relations separately willassist in determining which systems to combineto create an hybrid system with better perform-ance.AcknowledgmentsThis research is supported by the Science Foun-dation Ireland (Grant 07/CE/I1142) as part of theCentre for Next Generation Localisation(www.cngl.ie) at Trinity College Dublin.Thanks are due to Dr Martin Emms for his sup-port on the development of this project.ReferencesMartin Emms.
2006.
Variants of Tree Similarity ina Question Answering Task.
In Proceedings ofthe Workshop on Linguistic Distances, held inconjunction with COLING 2006, 100?108, Syd-ney, Australia, Asociation for Computational Lin-guistics.Jan Haji?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Ka-wahara, Maria Antonia Mart?,Luis M?rquez, Adam Meyers, Joakim Nivre, Se-bastian Pad?, Jan ?t?p?nek, Pavel Stravn?k, MihaiSurdeanu, Nianwen Xue and Yi Zhang.
2009.
TheCoNLL-2009 shared task: syntactic and se-mantic dependencies in multiple languages.
InCoNLL '09: Proceedings of the Thirteenth Confe-rence on Computational Natural Language Learn-ing (pp.
1-18).
Morristown, NJ, USA: Associationfor Computational Linguistics.Seokhwan Kim, Minwoo Jeong and Gary GeunbaeLee.
2009.
A Local Tree Alignment-based SoftPattern Matching Approach for InformationExtraction.
Proceedings of NAAACL HLT, 169-172.
Boulder, Colorado, June 2009Milen Kouylekov and Bernardo Magnini.
2005.
Re-cognizing textual entailment with tree editdistance algorithms.
In Recognizing Textual En-tailment (pp.
17-20).
Southampton, U.K.Baoli Li, Martin Emms, Saturnino Luz and Carl Vo-gel.
2009.
Exploring multilingual semantic rolelabeling.
In CoNLL '09: Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning (pp.
73-78).
Morristown, NJ,USA: Association for Computational Linguistics.Mitchell Marcus, Beatrice Santorini and Mary AnnMarcinkiewicz.
1994.
Building a large anno-tated corpus of Eng-lish: The Penn Treebank.Computational linguistics, 19(2), 313?330.Alessandro Moschitti, Daniele Pighin and RobertoBasili.
2008.
Tree kernels for semantic rolelabeling.
Computational Linguistics, 34(2), 193-224.
Cambridge, MA, USA: MIT Press.Lluis M?rquez, Xavier Carreras, Kenneth.
C.Litkowski and Suzanne Stevenson.
2008.
Seman-tic Role Labeling: An Introduction to the Spe-cial Issue.
Computational Linguistics, 34(2), 145-159.Martha Palmer, Paul Kingsbury and Daniel Gildea.2005.
The Proposition Bank: An AnnotatedCorpus of Semantic Roles.
Computational Lin-guistics, 31(1), 71-106.Vasin Punyakanok, Dan Roth and Wen-tau Yih.
2004.Mapping dependencies trees: An applicationto question answering.
In Proceedings ofAI\&Math 2004 (pp.
1-10).
Ford.Dennis Shasha and Kaizhong Zhang.
1990.
Fast al-gorithms for the unit cost editing distance be-tween trees.
J. Algorithms, 11(4), 581-621.
Du-luth, MN, USA: Academic Press, Inc.Kuo-Chung Tai.
1979.
The Tree-to-Tree Correc-tion Problem.
J. ACM, 26(3), 422-433.
NewYork, NY, USA: ACM.Kaizhong Zhang and Dennis Shasha.
1989.
Simplefast algorithms for the editing distance be-tween trees and related problems.
SIAM J.Comput., 18(6), 1245-1262.
Philadelphia, PA,USA: Society for Industrial and Applied Mathe-matics.84
