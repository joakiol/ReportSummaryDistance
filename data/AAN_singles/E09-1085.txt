Proceedings of the 12th Conference of the European Chapter of the ACL, pages 745?753,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsIncremental Dialogue Processing in a Micro-DomainGabriel Skantze1Dept.
of Speech, Music and HearingKTH, Stockholm, Swedengabriel@speech.kth.seDavid SchlangenDepartment of LinguisticsUniversity of Potsdam, Germanydas@ling.uni-potsdam.deAbstractThis paper describes a fully incremental dia-logue system that can engage in dialoguesin a simple domain, number dictation.
Be-cause it uses incremental speech recognitionand prosodic analysis, the system can giverapid feedback as the user is speaking, witha very short latency of around 200ms.
Be-cause it uses incremental speech synthesisand self-monitoring, the system can react tofeedback from the user as the system isspeaking.
A comparative evaluation showsthat na?ve users preferred this system over anon-incremental version, and that it wasperceived as more human-like.
11 IntroductionA traditional simplifying assumption for spokendialogue systems is that the dialogue proceedswith strict turn-taking between user and system.The minimal unit of processing in such systemsis the utterance, which is processed in whole byeach module of the system before it is handed onto the next.
When the system is speaking an ut-terance, it assumes that the user will wait for it toend before responding.
(Some systems acceptbarge-ins, but then treat the interrupted utteranceas basically unsaid.
)Obviously, this is not how natural human-human dialogue proceeds.
Humans understandand produce language incrementally ?
they usemultiple knowledge sources to determine when itis appropriate to speak, they give and receivebackchannels in the middle of utterances, theystart to speak before knowing exactly what tosay, and they incrementally monitor the listener?sreactions to what they say (Clark, 1996).1The work reported in this paper was done while the firstauthor was at the University of Potsdam.This paper presents a dialogue system, calledNUMBERS, in which all components operate in-crementally.
We had two aims: First, to exploretechnical questions such as how the componentsof a modularized dialogue system should be ar-ranged and made to interoperate to support in-cremental processing, and which requirementsincremental processing puts on dialogue systemcomponents (e.g., speech recognition, prosodicanalysis, parsing, discourse modelling, actionselection and speech synthesis).
Second, to in-vestigate whether incremental processing canhelp us to better model certain aspects of humanbehaviour in dialogue systems ?
especially turn-taking and feedback ?
and whether this improvesthe user?s experience of using such a system.2 Incremental dialogue processingAll dialogue systems are ?incremental?, in somesense ?
they proceed in steps through the ex-change of ?utterances?.
However, incrementalprocessing typically means more than this; acommon requirement is that processing startsbefore the input is complete and that the firstoutput increments are produced as soon as possi-ble (e.g., Kilger & Finkler, 1995).
Incrementalmodules hence are those where ?Each processingcomponent will be triggered into activity by aminimal amount of its characteristic input?
(Levelt, 1989).
If we assume that the ?character-istic input?
of a dialogue system is the utterance,this principle demands that ?minimal amounts?
ofan utterance already trigger activity.
It should benoted though, that there is a trade-off betweenresponsiveness and output quality, and that anincremental process therefore should produceoutput only as soon as it is possible to reach adesired output quality criterion.2.1 Motivations & related workThe claim that humans do not understand andproduce speech in utterance-sized chunks, but745rather incrementally, can be supported by animpressive amount of psycholinguistic literatureon the subject (e.g., Tanenhaus & Brown-Schmidt, 2008; Levelt, 1989).
However, when itcomes to spoken dialogue systems, the dominantminimal unit of processing has been the utter-ance.
Moreover, traditional systems follow avery strict sequential processing order of utter-ances ?
interpretation, dialogue management,generation ?
and there is most often no monitor-ing of whether (parts of) the generated messageis successfully delivered.Allen et al (2001) discuss some of the short-comings of these assumptions when modellingmore conversational human-like dialogue.
First,they fail to account for the frequently found mid-utterance reactions and feedback (in the form ofacknowledgements, repetition of fragments orclarification requests).
Second, people oftenseem to start to speak before knowing exactlywhat to say next (possibly to grab the turn), thusproducing the utterance incrementally.
Third,when a speaker is interrupted or receives feed-back in the middle of an utterance, he is able tocontinue the utterance from the point where hewas interrupted.Since a non-incremental system needs to proc-ess the whole user utterance using one module ata time, it cannot utilise any higher level informa-tion for deciding when the user?s turn or utter-ance is finished, and typically has to rely only onsilence detection and a time-out.
Silence, how-ever, is not a good indicator: sometimes there issilence but no turn-change is intended (e.g., hesi-tations), sometimes there isn?t silence, but theturn changes (Sacks et al, 1974).
Speakers ap-pear to use other knowledge sources, such asprosody, syntax and semantics to detect or evenproject the end of the utterance.
Attempts havebeen made to incorporate such knowledgesources for turn-taking decisions in spoken dia-logue systems (e.g., Ferrer et al, 2002; Raux &Eskenazi, 2008).
To do so, incremental dialogueprocessing is clearly needed.Incremental processing can also lead to betteruse of resources, since later modules can start towork on partial results and do not have to waituntil earlier modules have completed processingthe whole utterance.
For example, while thespeech recogniser starts to identify words, theparser can already add these to the chart.
Latermodules can also assist in the processing and forexample resolve ambiguities as they come up.Stoness et al (2004) shows how a reference reso-lution module can help an incremental parserwith NP suitability judgements.
Similarly, Aist etal.
(2006) shows how a VP advisor could help anincremental parser.On the output side, an incremental dialoguesystem could monitor what is actually happeningto the utterance it produces.
As discussed byRaux & Eskenazi (2007), most dialogue manag-ers operate asynchronously from the output com-ponents, which may lead to problems if thedialogue manager produces several actions andthe user responds to one of them.
If the inputcomponents do not have any information aboutthe timing of the system output, they cannot re-late them to the user?s response.
This is evenmore problematic if the user reacts (for examplewith a backchannel) in the middle of systemutterances.
The system must then relate theuser?s response to the parts of its planned outputit has managed to realise, but also be able to stopspeaking and possibly continue the interruptedutterance appropriately.
A solution for handlingmid-utterance responses from the user is pro-posed by Dohsaka & Shimazu (1997).
For in-cremental generation and synthesis, the outputcomponents must also cope with the problem ofrevision (discussed in more detail below), whichmay for example lead to the need for the genera-tion of speech repairs, as discussed by Kilger &Finkler (1995).As the survey above shows, a number of stud-ies have been done on incrementality in differentareas of language processing.
There are, how-ever, to our knowledge no studies on how thevarious components could or should be inte-grated into a complete, fully incremental dia-logue system, and how such a system might beperceived by na?ve users, compared to a non-incremental system.
This we provide here.2.2 A general, abstract modelThe NUMBERS system presented in this paper canbe seen as a specific instance (with some simpli-fying assumptions) of a more general, abstractmodel that we have developed (Schlangen &Skantze, 2009).
We will here only briefly de-scribe the parts of the general model that arerelevant for the exposition of our system.We model the dialogue processing system as acollection of connected processing modules.
Thesmallest unit of information that is communi-cated along the connections is called the incre-mental unit (IU), the unit of the ?minimalamount of characteristic input?.
Depending onwhat the module does, IUs may be audio frames,words, syntactic phrases, communicative acts,746etc.
The processing module itself is modelled asconsisting of a Left Buffer (LB), the Processorproper, and a Right Buffer (RB).
An example oftwo connected modules is shown in Figure 1.
AsIU1 enters the LB of module A, it may be con-sumed by the processor.
The processor may thenproduce new IUs, which are posted on the RB(IU2 in the example).
As the example shows, themodules in the system are connected so that anIU posted on the RB in one module may be con-sumed in the LB of another module.
One RBmay of course be connected to many other LB?s,and vice versa, allowing a range of differentnetwork topologies.Figure 1: Two connected modules.In the NUMBERS system, information is onlyallowed to flow from left to right, which meansthat the LB may be regarded as the input bufferand the RB as the output buffer.
However, in thegeneral model, information may flow in bothdirections.A more concrete example is shown in Figure2, which illustrates a module that does incre-mental speech recognition.
The IUs consumedfrom the LB are audio frames, and the IUs postedin the RB are the words that are recognised.Figure 2: Speech recognition as an example of incre-mental processing.We identify three different generic moduleoperations on IUs: update, purge and commit.First, as an IU is added to the LB, the processorneeds to update its internal state.
In the exampleabove, the speech recogniser has to continuouslyadd incoming audio frames to its internal state,and as soon as the recogniser receives enoughaudio frames to decide that the word ?four?
is agood-enough candidate, the IU holding this wordwill be put on the RB (time-point t1).
If a proces-sor only expects IUs that extend the rightmost IUcurrently produced, we can follow Wir?n (1992)in saying that it is only left-to-right incremental.A fully incremental system (which we aim athere), on the other hand, also allows insertionsand/or revisions.An example of revision is illustrated at time-point t2 in Figure 2.
As more audio frames areconsumed by the recogniser, the word ?four?
isno longer the best candidate for this stretch ofaudio.
Thus, the module must now revoke the IUholding the word ?four?
(marked with a dottedoutline) and add a new IU for the word ?forty?.All other modules consuming these IUs mustnow purge them from their own states and pos-sibly revoke other IUs.
By allowing revision, amodule may produce tentative results and thusmake the system more responsive.As more audio frames are consumed in the ex-ample above, a new word ?five?
is identified andadded to the RB (time-point t3).
At time-point t4,no more words are identified, and the modulemay decide to commit to the IUs that it has pro-duced (marked with a darker shade).
A commit-ted IU is guaranteed to not being revoked later,and can hence potentially be removed from theprocessing window of later modules, freeing upresources.3 Number dictation: a micro-domainBuilding a fully incremental system with a be-haviour more closely resembling that of humandialogue participants raises a series of new chal-lenges.
Therefore, in order to make the task morefeasible, we have chosen a very limited domain ?what might be called a micro-domain (cf.
Edlundet al, 2008): the dictation of number sequences.In this scenario, the user dictates a sequence ofnumbers (such as a telephone number or a creditcard number) to the dialogue system.
This is avery common situation in commercial telephone-based dialogue systems, which however operatein a non-incremental manner: The user is firstasked to read out the whole number sequence,which the system then confirms.
Should the rec-ognition be incorrect, the user has to repeat thewhole sequence again.
In an incremental versionof this scenario, the system might give continu-ous feedback (such as acknowledgements andclarification requests) as the user is reading theforty fiveforty fivefour t1fortyfourfortyforty fiveforty fivet2t3t4time left buffer processor right bufferfourIU2left bufferprocessor right bufferright buffer processorleft bufferIU1 IU1 IU2IU3 IU2IU3module Amodule B747sequence.
When the system repeats the sequence,the user is also given the opportunity to givefeedback and make corrections at any time.
Ob-viously, such a dialogue will result in many morespeaker-shifts, which in turn increases the needfor the rapid turn-taking.
Thus, while the numberdictation task might appear simple at a firstglance (very limited vocabulary, simple syntaxand semantics), there are several issues ?
nottypically considered in most dialogue systems ?which an incremental and more human-like dia-logue system must address.To explore dialogue phenomena in this do-main, we gave 8 pairs of subjects the task ofdictating number sequences to each other.
Thesubjects were sitting in different rooms and usedheadsets to communicate.
To induce more feed-back behaviour, noise was randomly inserted inthe utterances from the information giver (as inthe setting used by Schlangen & Fern?ndez(2007)).
The subjects were all speaking German(their native tongue).
The recorded dialogueswere then transcribed and annotated.
The ob-served dialogue strategies directly influenced thedesign of our system (see below).
Table 1 showsan example of a dialogue between a user and thesystem and illustrates some of the phenomena weintended to model.U.01 two five three -S.02 okayU.03 one forty, five -S.04 sorry, do you mean one and then forty andthen five?U.05 no, one forty fiveS.06 okay, one forty fiveU.07 yeah, and six eight four -S.08 okayU.09 seven twelve nine.S.10 so, from the start, two five three -U.11 yeahS.12 one forty five -S.13 six six four -U.14 no, six eight fourS.15 okay, six eight four -S.16 and then seven twelve nine.U.17 that?s rightTable 1: An example dialogue between a user andthe NUMBERS system.Just like in this example, a common strategyfor the subjects was to first let the informationgiver read the number sequence and then switchroles and let the information follower repeat it.Instead of reading out the whole number se-quence at once, subjects tended to packagepieces of information into what Clark (1996)refers to as installments (in this case smallgroups of numbers).
After each installment, theother speaker may react by giving an acknowl-edgement (as in S.02) a clarification request (asin S.04), a correction (as in U.14), or do nothing(as after S.12).As there are a lot of speaker shifts, there needsto be a mechanism for rapid turn taking.
In theexample above, the system must recognize thatthe last digit in U.01, U.03, U.05 and U.07 endsan installment and calls for a reaction, while thelast digit in U.09 ends the whole sequence.
Oneinformation source that has been observed to beuseful for this is prosody (Koiso et al, 1998).When analysing the recorded dialogues, itseemed like mid-sequence installments mostoften ended with a prolonged duration and arising pitch, while end-sequence installmentsmost often ended with a shorter duration and afalling pitch.
How prosody is used by theNUMBERS system for this classification is de-scribed in section 4.2.4 The NUMBERS system componentsThe NUMBERS system has been implementedusing the HIGGINS spoken dialogue systemframework (Skantze, 2007).
All modules havebeen adapted and extended to allow incrementalprocessing.
It took us roughly 6 months to im-plement the changes described here to a fullyworking baseline system.
Figure 3 shows thearchitecture of the system2.Figure 3: The system architecture.CA = communicative act.This is pretty much a standard dialogue systemlayout, with some exceptions that will be dis-cussed below.
Most notably perhaps is that dia-logue management is divided into a discoursemodelling module and an action manager.
As can2A video showing an example run of the system has beenuploaded tohttp://www.youtube.com/watch?v=_rDkb1K1si8ActionManagerDiscoursemodellerASRSemanticparserTTS AudioCAsAudioCAs +WordsWords +ProsodyCAs +EntitiesCAs +Words748be seen in the figure, the discourse modeller alsoreceives information about what the system itselfsays.
The modules run asynchronously in sepa-rate processes and communicate by sendingXML messages containing the IUs over sockets.We will now characterize each system moduleby what kind of IUs they consume and produce,as well as the criteria for committing to an IU.4.1 Speech recognitionThe automatic speech recognition module (ASR)is based on the Sphinx 4 system (Lamere et al,2003).
The Sphinx system is capable of incre-mental processing, but we have added supportfor producing incremental results that are com-patible with the HIGGINS framework.
We havealso added prosodic analysis to the system, asdescribed in 4.2.
For the NUMBERS domain, weuse a very limited context-free grammar accept-ing number words as well as some expressionsfor feedback and meta-communication.An illustration of the module buffers is shownin Figure 2 above.
The module consumes audioframes (each 100 msec) from the LB and pro-duces words with prosodic features in the RB.The RB is updated every time the sequence oftop word hypotheses in the processing windowschanges.
After 2 seconds of silence has beendetected, the words produced so far are commit-ted and the speech recognition search space iscleared.
Note that this does not mean that othercomponents have to wait for this amount of si-lence to pass before starting to process or that thesystem cannot respond until then ?
incrementalresults are produced as soon as the ASR deter-mines that a word has ended.4.2 Prosodic analysisWe implemented a simple form of prosodicanalysis as a data processor in the Sphinx fron-tend.
Incremental F0-extraction is done by firstfinding pitch candidates (on the semitone scale)for each audio frame using the SMDSF algo-rithm (Liu et al, 2005).
An optimal path betweenthe candidates is searched for, using dynamicprogramming (maximising candidate confidencescores and minimising F0 shifts).
After this, me-dian smoothing is applied, using a window of 5audio frames.In order for this sequence of F0 values to beuseful, it needs to be parameterized.
To find outwhether pitch and duration could be used for thedistinction between mid-sequence installmentsand end-sequence installments, we did a machinelearning experiment on the installment-endingdigits in our collected data.
There were roughlyan equal amount of both types, giving a majorityclass baseline of 50.9%.As features we calculated a delta pitch pa-rameter for each word by computing the sum ofall F0 shifts (negative or positive) in the pitchsequence.
(Shifts larger than a certain threshold(100 cents) were excluded from the summariza-tion, in order to sort out artefacts.)
A durationparameter was derived by calculating the sum ofthe phoneme lengths in the word, divided by thesum of the average lengths of these phonemes inthe whole data set.
Both of these parameterswere tested as predictors separately and in com-bination, using the Weka Data Mining Software(Witten & Frank, 2005).
The best results wereobtained with a J.48 decision tree, and are shownin Table 2.Baseline 50.9%Pitch 81.2%Duration 62.4%Duration + Pitch 80.8%Table 2: The results of the installment classifica-tion (accuracy).As the table shows, the best predictor wassimply to compare the delta pitch parameteragainst an optimal threshold.
While the perform-ance of 80.8% is significantly above baseline, itcould certainly be better.
We do not know yetwhether the sub-optimal performance is due tothe fact that the speakers did not always usethese prosodic cues, or whether there is room forimprovement in the pitch extraction and parame-terization.Every time the RB of the ASR is updated, thedelta pitch parameter is computed for each wordand the derived threshold is used to determine apitch slope class (rising/falling) for the word.
(Note that there is no class for a flat pitch.
Thisclass is not really needed here, since the digitswithin installments are followed by no or onlyvery short pauses.)
The strategy followed by thesystem then is this: when a digit with a risingpitch is detected, the system plans to immedi-ately give a mid-sequence reaction utterance, anddoes so if indeed no more words are received.
Ifa digit with a falling pitch is detected, the systemplans an end-of-sequence utterance, but waits alittle bit longer before producing it, to see if therereally are no more words coming in.
In otherwords, the system bases its turn-taking decisionson a combination of ASR, prosody and silence-thresholds, where the length of the threshold749differs for different prosodic signals, and wherereactions are planned already during the silence.
(This is in contrast to Raux & Eskenazi (2008),where context-dependent thresholds are used aswell, but only simple end-pointing is performed.
)The use of prosodic analysis in combinationwith incremental processing allows theNUMBERS system to give feedback after mid-sequence installments in about 200 ms. Thisshould be compared with most dialogue systemswhich first use a silence threshold of about 750-1500 msec, after which each module must proc-ess the utterance.4.3 Semantic parsingFor semantic parsing, the incremental processingin the HIGGINS module PICKERING (Skantze &Edlund, 2004) has been extended.
PICKERING isbased on a modified chart parser which addsautomatic relaxations to the CFG rules for ro-bustness, and produces semantic interpretationsin the form of concept trees.
It can also use fea-tures that are attached to incoming words, suchas prosody and timestamps.
For example, thenumber groups in U.03 and U.05 in Table 1 ren-der different parses due to the pause lengths be-tween the words.The task of PICKERING in the NUMBERS do-main is very limited.
Essentially, it identifiescommunicative acts (CAs), such as number in-stallments.
The only slightly more complex pars-ing is that of larger numbers such as ?twentyfour?.
There are also cases of ?syntactic ambigu-ity?, as illustrated in U.03 in the dialogue exam-ple above ("forty five" as "45" or "40 5").
In theNUMBERS system, only 1-best hypotheses arecommunicated between the modules, butPICKERING can still assign a lower parsing confi-dence score to an ambiguous interpretation,which triggers a clarification request in S.04.Figure 4 show a very simple example of theincremental processing in PICKERING.
The LBcontains words with prosodic features producedby the ASR (compare with Figure 2 above).
TheRB consists of the CAs that are identified.
Eachtime a word is added to the chart, PICKERINGcontinues to build the chart and then searches foran optimal sequence of CAs in the chart, allow-ing non-matching words in between.
To handlerevision, a copy of the chart is saved after eachword has been added.Figure 4: Incremental parsing.
There is a jump in timebetween t4 and t5.As can be seen at time-point t4, even if allwords that a CA is based on are committed, theparser does not automatically commit the CA.This is because later words may still cause arevision of the complex output IU that has beenbuilt.
As a heuristic, PICKERING instead waitsuntil a CA is followed by three words that are notpart of it until it commits, as shown at time-pointt5.
After a CA has been committed, the wordsinvolved may be cleared from the chart.
Thisway, PICKERING parses a ?moving window?
ofwords.4.4 Discourse modellingFor discourse modelling, the HIGGINS moduleGALATEA (Skantze, 2008) has been extended tooperate incrementally.
The task of GALATEA isto interpret utterances in their context by trans-forming ellipses into full propositions, indentifydiscourse entities, resolve anaphora and keeptrack of the grounding status of concepts (theirconfidence score and when they have beengrounded in the discourse).
As can be seen inFigure 3, GALATEA models both utterances fromthe user as well as the system.
This makes itpossible for the system to monitor its own utter-ances and relate them to the user?s utterances, byusing timestamps produced by the ASR and thespeech synthesiser.In the LB GALATEA consumes CAs from boththe user (partially committed, as seen in Figure4) and the system (always committed, see 4.6).In the RB GALATEA produces an incrementaldiscourse model.
This model contains a list ofresolved communicative acts and list of resolveddiscourse entities.
This model is then consultedby an action manager which decides what thesystem should do next.
The discourse model is40fortyforty fiveforty fivefortyforty fiveforty five404545404545three 62 3 45forty five sixty two three62 3 45t1t2t3t4timefourfour44t5left buffer processor right buffer4four750committed up to the point of the earliest non-committed incoming CA.
In the NUMBERS do-main, the discourse entities are the number in-stallments.4.5 Action managementBased on the discourse model (from the LB), theaction manager (AM) generates system actions(CAs) in semantic form (for GALATEA) with anattached surface form (for the TTS), and putsthem on the RB.
(In future extensions of the sys-tem, we will add an additional generation modulethat generates the surface form from the semanticform.)
In the NUMBERS system, possible systemactions are acknowledgements, clarification re-quests and repetitions of the number sequence.The choice of actions to perform is based on thegrounding status of the concepts (which is repre-sented in the discourse model).
For example, ifthe system has already clarified the first part ofthe number sequence due to an ambiguity, it doesnot need to repeat this part of the sequence again.The AM also attaches a desired timing to theproduced CA, relative to the end time of last userutterance.
For example, if a number group with afinal rising pitch is detected, the AM may tell theTTS to execute the CA immediately after theuser has stopped speaking.
If there is a fallingpitch, it may tell the TTS to wait until 500 msecof silence has been detected from the user beforeexecuting the action.
If the discourse model getsupdated during this time, the AM may revokeprevious CAs and replace them with new ones.4.6 Speech synthesisA diphone MBROLA text-to-speech synthesiser(TTS) is used in the system (Dutoit et al, 1996),and a wrapper for handling incremental process-ing has been implemented.
The TTS consumeswords linked to CAs from the LB, as producedby the AM.
As described above, each CA has atimestamp.
The TTS places them on a queue, andprepares to synthesise and start sending the audioto the speakers.
When the system utterance hasbeen played, the corresponding semantic con-cepts for the CA are sent to GALATEA.
If theTTS is interrupted, the semantic fragments of theCA that corresponds to the words that were spo-ken are sent.
This way, GALATEA can monitorwhat the system actually says and provide theAM with this information.
Since the TTS onlysends (parts of) the CAs that have actually beenspoken, these are always marked as committed.There is a direct link from the ASR to the TTSas well (not shown in Figure 3), informing theTTS of start-of-speech and end-of-speech events.As soon as a start-of-speech event is detected,the TTS stops speaking.
If the TTS does not re-ceive any new CAs from the AM as a conse-quence of what the user said, it automaticallyresumes from the point of interruption.
(Thisimplements a "reactive behaviour" in the sense of(Brooks, 1991), which is outside of the control ofthe AM.
)An example of this is shown in Table 1.
AfterU.09, the AM decides to repeat the whole num-ber sequence and sends a series of CAs to theTTS for doing this.
After S.10, the user givesfeedback in the form of an acknowledgement(U.11).
This causes the TTS to make a pause.When GALATEA receives the user feedback, ituses the time-stamps to find out that the feedbackis related to the number group in S.10 and thegrounding status for this group is boosted.
Whenthe AM receives the updated discourse model, itdecides that this does not call for any revision tothe already planned series of actions.
Since theTTS does not receive any revisions, it resumesthe repetition of the number sequence in S.12.The TTS module is fully incremental in that itcan stop and resume speaking in the middle of anutterance, revise planned output, and can informother components of what (parts of utterances)has been spoken.
However, the actual text-to-speech processing is done before the utterancestarts and not yet incrementally as the utteranceis spoken, which could further improve the effi-ciency of the system.
This is a topic for futureresearch, together with the generation of hiddenand overt repair as discussed by Kilger & Finkler(1995).5 EvaluationIt is difficult to evaluate complete dialogue sys-tems such as the one presented here, since thereare so many different components involved (butsee M?ller et al (2007) for methods used).
In ourcase, we?re interested in the benefits of a specificaspect, though, namely incrementality.
Noevaluation is needed to confirm that an incre-mental system such as this allows more flexibleturn-taking and that it can potentially respondfaster ?
this is so by design.
However, we alsowant this behaviour to result in an improved userexperience.
To test whether we have achievedthis, we implemented for comparison a non-incremental version of the system, very muchlike a standard number dictation dialogue in acommercial application.
In this version, the user751is asked to read out the whole number sequencein one go.
After a certain amount of silence, thesystem confirms the whole sequence and asks ayes/no question whether it was correct.
If not, theuser has to repeat the whole sequence.Eight subjects were given the task of using thetwo versions of the system to dictate numbersequences (in English) to the system.
(The sub-jects were native speakers of German with agood command of English.)
Half of the subjectsused the incremental version first and the otherhalf started with the non-incremental version.They were asked to dictate eight number se-quences to each version, resulting in 128 dia-logues.
For each sequence, they were given atime limit of 1 minute.
After each sequence, theywere asked whether they had succeeded in dictat-ing the sequence or not, as well as to mark theiragreement (on a scale from 0-6) with statementsconcerning how well they had been understoodby the system, how responsive the system was, ifthe system behaved as expected, and how hu-man-like the conversational partner was.
Afterusing both versions of the system, they were alsoasked whether they preferred one of the versionsand to what extent (1 or 2 points, which gives amaximum score of 16 to any version, when total-ling all subjects).There was no significant difference betweenthe two versions with regard to how many of thetasks were completed successfully.
However, theincremental version was clearly preferred in theoverall judgement (9 points versus 1).
Only oneof the more specific questions yielded any sig-nificant difference between the versions: theincremental version was judged to be more hu-man-like for the successful dialogues (5,2 onaverage vs. 4,5; Wilcoxon signed rank test;p<0.05).The results from the evaluation are in line withwhat could be expected.
A non-incremental sys-tem can be very efficient if the system under-stands the number sequence the first time, andthe ASR vocabulary is in this case very limited,which explains why the success-rate was thesame for both systems.
However, the incrementalversion was experienced as more pleasant andhuman-like.
One explanation for the better ratingof the incremental version is that the acknowl-edgements encouraged the subjects to packagethe digits into installments, which helped thesystem to better read back the sequence using thesame installments.6 Conclusions and future workTo sum up, we have presented a dialogue systemthat through the use of novel techniques (incre-mental prosodic analysis, reactive connectionbetween ASR and TTS, fully incremental archi-tecture) achieves an unprecedented level of reac-tiveness (from a minimum latency of 750ms, astypically used in dialogue systems, down to oneof 200ms), and is consequently evaluated asmore natural than more typical setups by humanusers.
While the domain we've used is relativelysimple, there are no principled reasons why thetechniques introduced here should not scale up.In future user studies, we will explore whichfactors contribute to the improved experience ofusing an incremental system.
Such factors mayinclude improved responsiveness, better install-ment packaging, and more elaborate feedback.
Itwould also be interesting to find out when rapidresponses are more important (e.g.
acknowl-edgements), and when they may be less impor-tant (e.g., answers to task-related questions).We are currently investigating the transfer ofthe prosodic analysis to utterances in a largerdomain, where similarly instructions by the usercan be given in installments.
But even within thecurrently used micro-domain, there are interest-ing issues still to be explored.
In future versionsof the system, we will let the modules pass paral-lel hypotheses and also improve the incrementalgeneration and synthesis.
Since the vocabulary isvery limited, it would also be possible to use alimited domain synthesis (Black & Lenzo, 2000),and explore how the nuances of different back-channels might affect the dialogue.
Another chal-lenge that can be researched within this micro-domain is how to use the prosodic analysis forother tasks, such as distinguishing correctionfrom dictation (for example if U.14 in Table 1would not begin with a ?no?).
In general, wethink that this paper shows that narrowing downthe domain while shifting the focus to the model-ling of more low-level, conversational dialoguephenomena is a fruitful path.AcknowledgementsThis work was funded by a DFG grant in theEmmy Noether programme.
We would also liketo thank Timo Baumann and Michaela Attererfor their contributions to the project, as well asAnna Iwanow and Angelika Adam for collectingand transcribing the data used in this paper.752ReferencesAist, G., Allen, J. F., Campana, E., Galescu, L.,G?mez Gallo, C. A., Stoness, S. C., Swift, M., &Tanenhaus, M. (2006).
Software Architectures forIncremental Understanding of Human Speech.
InProceedings of Interspeech (pp.
1922-1925).
Pitts-burgh PA, USA.Allen, J. F., Ferguson, G., & Stent, A.
(2001).
Anarchitecture for more realistic conversational sys-tems.
In Proceedings of the 6th international con-ference on Intelligent user interfaces (pp.
1-8).Black, A., & Lenzo, K. (2000).
Limited domain syn-thesis.
In Proceedings of ICSLP (pp.
410-415).Beijing, China.Brooks, R. A.
(1991).
Intelligence without representa-tion.
Artificial Intelligence, 47, 139-159.Clark, H. H. (1996).
Using language.
Cambridge,UK: Cambridge University Press.Dohsaka, K., & Shimazu, A.
(1997).
System architec-ture for spoken utterance production in collabora-tive dialogue.
In Working Notes of IJCAI 1997Workshop on Collaboration, Cooperation andConflict in Dialogue Systems.Dutoit, T., Pagel, V., Pierret, N., Bataille, F., & Vre-ken, O. v. d. (1996).
The MBROLA project: To-wards a set of high-quality speech synthesizers freeof use for non-commercial purposes.
In Proceed-ings of ICSLIP '96 (pp.
1393-1396).Edlund, J., Gustafson, J., Heldner, M., & Hjalmars-son, A.
(2008).
Towards human-like spoken dialo-gue systems.
Speech Communication, 50(8-9), 630-645.Ferrer, L., Shriberg, E., & Stolcke, A.
(2002).
Is thespeaker done yet?
Faster and more accurate end-ofutterance detection using prosody.
In Proceedingsof ICSLP (pp.
2061-2064).Kilger, A., & Finkler, W. (1995).
Incremental Gener-ation for Real-Time Applications.
Technical ReportRR-95-11, German Research Center for ArtificialIntelligence.Koiso, H., Horiuchi, Y., Tutiya, S., Ichikawa, A., &Den, Y.
(1998).
An analysis of turn-taking andbackchannels based on prosodic and syntactic fea-tures in Japanese Map Task dialogs.
Language andSpeech, 41, 295-321.Lamere, P., Kwok, P., Gouvea, E., Raj, B., Singh, R.,Walker, W., Warmuth, M., & Wolf, P. (2003).
TheCMU SPHINX-4 speech recognition system.. InProceedings of the IEEE Intl.
Conf.
on Acoustics,Speech and Signal Processing.
Hong Kong.Levelt, W. J. M. (1989).
Speaking: From Intention toArticulation.
Cambridge, Mass., USA: MIT Press.Liu, J., Zheng, T. F., Deng, J., & Wu, W. (2005).Real-time pitch tracking based on combinedSMDSF.
In Proceedings of Interspeech (pp.
301-304).
Lisbon, Portugal.M?ller, S., Smeele, P., Boland, H., & Krebber, J.(2007).
Evaluating spoken dialogue systems ac-cording to de-facto standards: A case study.
Com-puter Speech & Language, 21(1), 26-53.Raux, A., & Eskenazi, M. (2007).
A multi-Layerarchitecture for semi-synchronous event-driven di-alogue Management.
In ASRU 2007.
Kyoto, Ja-pan..Raux, A., & Eskenazi, M. (2008).
Optimizing end-pointing thresholds using dialogue features in aspoken dialogue system.
In Proceedings of SIGdial2008.
Columbus, OH, USA.Sacks, H., Schwegloff, E., & Jefferson, G. (1974).
Asimplest systematics for the organization of turn-taking for conversation.
Language, 50, 696-735.Schlangen, D., & Fern?ndez, R. (2007).
Speakingthrough a noisy channel: experiments on inducingclarification behaviour in human-human dialogue.In Proceedings of Interspeech 2007.
Antwerp, Bel-gium.Schlangen, D., & Skantze, G. (2009).
A general, ab-stract model of incremental dialogue processing.
InProceedings of the 12th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL-09).
Athens, Greece.Skantze, G., & Edlund, J.
(2004).
Robust interpreta-tion in the Higgins spoken dialogue system.
InProceedings of ISCA Tutorial and Research Work-shop (ITRW) on Robustness Issues in Conversa-tional Interaction.
Norwich, UK.Skantze, G. (2007).
Error Handling in Spoken Dialo-gue Systems - Managing Uncertainty, Groundingand Miscommunication.
Doctoral dissertation,KTH, Department of Speech, Music and Hearing.Skantze, G. (2008).
Galatea: A discourse modellersupporting concept-level error handling in spokendialogue systems.
In Dybkj?r, L., & Minker, W.(Eds.
), Recent Trends in Discourse and Dialogue.Springer.Stoness, S. C., Tetreault, J., & Allen, J.
(2004).
In-cremental parsing with reference interaction.
InProceedings of the ACL Workshop on IncrementalParsing (pp.
18-25).Tanenhaus, M. K., & Brown-Schmidt, S. (2008).Language processing in the natural world.
InMoore, B. C. M., Tyler, L. K., & Marslen-Wilson,W.
D.
(Eds.
), The perception of speech: fromsound to meaning (pp.
1105-1122).Wir?n, M. (1992).
Studies in Incremental NaturalLanguage Analysis.
Doctoral dissertation,Link?ping University, Link?ping, Sweden.Witten, I. H., & Frank, E. (2005).
Data Mining: Prac-tical machine learning tools and techniques.
SanFrancisco: Morgan Kaufmann.753
