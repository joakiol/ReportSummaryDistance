Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 32?37,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDeep Markov Neural Network for Sequential Data ClassificationMin Yang1Wenting Tu1Wenpeng Yin2Ziyu Lu11Department of Computer Science, The University of Hong Kong, Hong Kong{myang,wttu,zylu}@cs.hku.hk2Center for Information and Language Processing, University of Munich, Germanywenpeng@cis.lmu.deAbstractWe present a general framework for incor-porating sequential data and arbitrary fea-tures into language modeling.
The generalframework consists of two parts: a hiddenMarkov component and a recursive neuralnetwork component.
We demonstrate theeffectiveness of our model by applying itto a specific application: predicting topicsand sentiments in dialogues.
Experimentson real data demonstrate that our methodis substantially more accurate than previ-ous methods.1 IntroductionProcessing sequential data is a significant researchchallenge for natural language processing.
Inthe past decades, numerous studies have beenconducted on modeling sequential data.
HiddenMarkov Models (HMMs) and its variants are rep-resentative statistical models of sequential data forthe purposes of classification, segmentation, andclustering (Rabiner, 1989).
For most aforemen-tioned methods, only the dependencies betweenconsecutive hidden states are modeled.
In naturallanguage processing, however, we find there aredependencies locally and at a distance.
Conser-vatively using the most recent history to performprediction yields overfitting to short-term trendsand missing important long-term effects.
Thus, itis crucial to explore in depth to capture long-termtemporal dynamics in language use.Numerous real world learning problems arebest characterized by interactions between mul-tiple causes or factors.
Taking sentiment analy-sis for dialogues as an example, the topic of thedocument and the author?s identity are both valu-able for mining user?s opinions in the conversa-tion.
Specifically, each participant in the dialogueusually has specific sentiment polarities towardsdifferent topics.
However, most existing sequen-tial data modeling methods are not capable of in-corporating the information from both the topicand the author?s identity.
More generally, thereis no sufficiently flexible sequential model that al-lows incorporating an arbitrary set of features.In this paper, we present a Deep Markov Neu-ral Network (DMNN) for incorporating sequentialdata and arbitrary features into language model-ing.
Our method learns from general sequentialobservations.
It is also capable of taking the or-dering of words into account, and collecting in-formation from arbitrary features associated withthe context.
Comparing to traditional HMM-basedmethod, it explores deeply into the structure ofsentences, and is more flexible in taking exter-nal features into account.
On the other hand, itdoesn?t suffer from the training difficulties of re-current neural networks, such as the vanishing gra-dient problem.The general framework consists of two parts:a hidden Markov component and a neural net-work component.
In the training phase, the hid-den Markov model is trained on the sequential ob-servation, resulting in transition probabilities andhidden states at each time step.
Then, the neuralnetwork is trained, taking words, features and hid-den state at the previous time step as input, to pre-dict the hidden states at the present time step.
Theprocedure is reversed in the testing phase: the neu-ral network predicts the hidden states using wordsand features, then the hidden Markov model pre-dicts the observation using hidden states.A key insight of our method is to use hid-den states as an intermediate representation, asa bridge to connect sentences and observations.By using hidden states, we can deal with arbi-trary observation, without worrying about the is-sue of discretization and normalization.
Hiddenstates are robust with respect to the random noisein the observation.
Unlike recurrent neural net-32work which connects networks between consecu-tive time steps, the recursive neural network in ourframework connects to the previous time step byusing its hidden states.
In the training phase, sincehidden states are inferred by the hidden Markovmodel, the training of recursive neural networksat each time step can be performed separately,preventing the difficulty of learning an extremelydeep neural network.We demonstrate the effectiveness of our modelby applying it to a specific application: predictingtopics and sentiments in dialogues.
In this exam-ple, the sequential observation includes topics andsentiments.
The feature includes the identity ofthe author.
Experiments on real data demonstratethat our method is substantially more accurate thanprevious methods.2 Related workModeling sequential data is an active research field(Lewis and Gale, 1994; Jain et al, 2000; Ra-biner, 1989; Baldi and Brunak, 2001; Kum et al,2005).
The paper proposed by Kum et al (2005)describes most of the existing techniques for se-quential data modeling.
Hidden Markov Mod-els (HMMs) is one of the most successful modelsfor sequential data that is best known for speechrecognition (Rabiner, 1989).
Recently, HMMshave been applied to a variety of applications out-side of speech recognition, such as handwritingrecognition (Nag et al, 1986; Kundu and Bahl,1988) and fault-detection (Smyth, 1994).
Thevariants and extensions of HMMs also includelanguage models (Guyon and Pereira, 1995) andeconometrics (Garcia and Perron, 1996).In order to properly capture more complex lin-guistic phenomena, a variety of neural networkshave been proposed, such as neural probabilisticlanguage model (Bengio et al, 2006), recurrentneural network (Mikolov et al, 2010) and recur-sive neural tensor network (Socher et al, 2013).As opposed to the work that only focuses on thecontext of the sequential data, some studies havebeen proposed to incorporate more general fea-tures associated with the context.
Ghahramani andJordan (1997) proposes a factorial HMMs methodand it has been successfully utilized in natural lan-guage processing (Duh, 2005), computer vision(Wang and Ji, 2005) and speech processing (Gaelet al, 2009).
However, exact inference and param-eter estimation in factorial HMMs is intractable,thus the learning algorithm is difficult to imple-ment and is limited to the study of real-valued datasets.3 The DMNN ModelIn this section, we describe our general frameworkfor incorporating sequential data and an arbitraryset of features into language modeling.3.1 Generative modelGiven a time sequence t = 1, 2, 3, .
.
.
, n, we as-sociate each time slice with an observation (st, ut)and a state label yt.
Here, strepresents the sen-tence at time t, and utrepresents additional fea-tures.
Additional features may include the authorof the sentence, the bag-of-word features and othersemantic features.
The label ytis the item that wewant to predict.
It might be the topic of the sen-tence, or the sentiment of the author.Given tuples (st, ut, yt), it is natural to build asupervised classification model to predict yt.
Re-current neural networks have been shown effectivein modeling temporal NLP data.
However, due tothe depth of the time sequence, training a singleRNN is difficult.
When the time sequence lengthn is large, the RNN model suffers from many prac-tical problems, including the vanishing gradient is-sue which makes the training process inefficient.We propose a Deep Markov Neural Network(DMNN) model.
The DMNN model introducesa hidden state variable Htfor each time slice.
Itserves as an intermediate layer connecting the la-bel ytand the observation (st, ut).
These hiddenvariables disentangle the correlation between neu-ral networks for each sentence, but preserving timeseries dependence.
The time series dependence ismodeled by a Markov chain.
In particular, we as-sume that there is a labeling matrix L such thatP (yt= i|Ht= j) = Lij(1)and a transition matrix T such thatP (Ht+1= i|Ht= j) = Tij(2)These two equations establish the relation be-tween the hidden state and the labels.
On theother hand, we use a neural network model M tomodel the relation between the hidden states andthe observations.
The neural network model takes(Ht?1, st, ut) as input, and predict Htas its out-put.
In particular, we use a logistic model to definethe probability:33P (Ht= i|Ht?1, st, ut) ?
(3)exp((wih, ?
(Ht?1)) + (wiu, ?
(ut)) + (wisN(st) + b))The vectors wh, wu, wsare linear combinationcoefficients to be estimated.
The functions ?, ?and function N turn Ht?1, utand stinto fea-turized vectors.
Among these functions, we rec-ommend choosing ?
(Ht?1) to be a binary vectorwhose Ht?1-th coordinate is one and all other co-ordinates are zeros.
Both function ?
and functionN are modeled by deep neural networks.Since the sentence sthas varied lengths anddistinct structures, choosing an appropriate neuralnetwork to extract the sentence-level feature is achallenge task.
In this paper, we choose N to bethe recursive autoencoder (Socher et al, 2011a),which explicitly takes structure of the sentenceinto account.
The network for defining ?
can bea standard fully connect neural network.3.2 Estimating Model ParametersThere are two sets of parameters to be estimated:the parameters L, T for the Markov chain model,and the parameters wh, wu, ws, ?,N for the deepneural networks.
The training is performed in twophases.
In the first phase, the hidden states {Ht}are estimated based on the labels {yt}.
The emis-sion matrix L and the transition matrix T are es-timated at the same time.
This step can be doneby using the Baum-Welch algorithm (Baum et al,1970; Baum, 1972) for learning hidden Markovmodels.When the hidden states {Ht} are obtained, thesecond phase estimates the remaining parametersfor the neural network model in a supervised pre-diction problem.
First, we use available sentencesto train the structure of the recursive neural net-workN .
This step can be done without using otherinformation besides {st}.
After the structure of Nis given, the remaining task is to train a supervisedprediction model to predict the hidden stateHtforeach time slice.
In this final step, the parameters tobe estimated are wh, wu, wsand the weight coeffi-cients in neural networks N and ?.
By maximiz-ing the log-likelihood of the prediction, all modelparameters can be estimated by stochastic gradientdescent.3.3 PredictionThe prediction procedure is a reverse of the train-ing procedure.
For prediction, we only have thesentence stand the additional feature ut.
By equa-tion (3), we use (s1, u1) to predict H1, then use(H1, s2, u2) to predict H2.
This procedure contin-ues until we have reached Hn.
Note that each Htis a random variable.
Equation (3) yieldsP (Ht= i|s, u) =?jP (Ht= i|st, ut, Ht?1= j)?
P (Ht?1= j|s, u) (4)This recursive formula suggests inferring theprobability distribution P (Ht|s, u) one by one,starting from t = 1 and terminate at t = n. AfterP (Ht|s, u) is available, we can infer the probabil-ity distribution of ytasP (yt= i|s, u) =?jP (yt= i|Ht= j)P (Ht= j|s, u)=?jLi,jP (Ht= j|s, u) (5)which gives the prediction for the label of interest.3.4 Application: Sentiment analysis inconversationSentiment analysis for dialogues is a typical se-quential data modeling problem.The sentimentsand topics expressed in a conversation affectthe interaction between dialogue participants(Suin Kim, 2012).
For example, given a user saythat ?I have had a high fever for 3 days?, the usermay write back positive-sentiment response like ?Ihope you feel better soon?, or it could be negative-sentiment content when the response is ?Sorry, butyou cannot join us today?
(Hasegawa et al, 2013).Incorporating the session?s sequential informationinto sentiment analysis may improve the predic-tion accuracy.
Meanwhile, each participate in thedialogue usually has specific sentiment polaritiestowards different topics.In this paper, the sequential labels available tothe framework include topics and sentiments.
Inthe training dataset, topics are obtained by run-ning an LDA model, while the sentiment labels aremanually labeled.
The feature includes the iden-tity of the author.
In the training phase, the hid-den Markov model is trained on the sequential la-bels, resulting in transition probabilities and hid-den states at each time step.
Then, the recursiveautoencoders (Socher et al, 2011a) is trained, tak-ing words, the identity of the author and hiddenstate at the previous time step as input, to predictthe hidden states at the present time step.
The pro-cedure is reversed in the testing phase: the neu-ral network predicts the hidden states using words34and the identity of the author, then the hiddenMarkov model predicts the observation using hid-den states.4 ExperimentsTo evaluate our model, we conduct experimentsfor sentiment analysis in conversations.4.1 DatasetsWe conduct experiments on both English and Chi-nese datasets.
The detailed properties of thedatasets are described as follow.Twitter conversation (Twitter): The originaldataset is a collection of about 1.3 million conver-sations drawn from Twitter by Ritter et al (2010).Each conversation contains between 2 and 243posts.
In our experiments, we filter the data bykeeping only the conversations of five or moretweets.
This results in 64,068 conversations con-taining 542,866 tweets.Sina Weibo conversation (Sina): since there isno authoritative publicly available Chinese short-text conversation corpus, we write a web crawlerto grab tweets from Sina Weibo, which is themost popular Twitter-like microblogging websitein China1.
Following the strategy used in (Rit-ter et al, 2010), we crawled Sina Weibo for a 3months period from September 2013 to Novem-ber 2013.
Filtering the conversations that containless than five posts, we get a Chinese conversa-tion corpus with 5,921 conversations containing37,282 tweets.For both datasets, we set the ground truth of sen-timent classification of tweets by using human an-notation.
Specifically, we randomly select 1000conversations from each datasets, and then invitethree researchers who work on natural languageprocessing to label sentiment tag of each tweet(i.e., positive, negative or neutral) manually.
From3 responses for each tweet, we measure the agree-ment as the number of people who submitted thesame response.
We measure the performance ofour framework using the tweets that satisfy at least2 out of 3 agreement.For both datasets, data preprocessing is per-formed.
The words about time, numeral words,pronoun and punctuation are removed as they areunrelated to the sentiment analysis task.1http://weibo.comDataset SVM NBSVM RAE Mesnil?s DMNNTwitter 0.572 0.624 0.639 0.650 0.682Sina 0.548 0.612 0.598 0.626 0.652Table 1: Three-way classification accuracy4.2 Baseline methodsTo evaluate the effectiveness of our frameworkon the application of sentiment analysis, we com-pare our approach with several baseline methods,which we describe below:SVM: Support Vector Machine is widely-usedbaseline method to build sentiment classifiers(Pang et al, 2002).
In our experiment, 5000 wordswith greatest information gain are chosen as fea-tures, and we use the LibLinear2to implementSVM.NBSVM: This is a state-of-the-art performer onmany sentiment classification datasets (Wang andManning, 2012).
The model is run using the pub-licly available code3.RAE: Recursive Autoencoder (Socher et al,2011b) has been proven effective in many senti-ment analysis tasks by learning compositionalityautomatically.
The RAE model is run using thepublicly available code4and we follow the samesetting as in (Socher et al, 2011b).Mesnil?s method: This method is proposed in(Mesnil et al, 2014), which achieves the strongestresults on movie reviews recently.
It is a ensem-ble of the generative technique and the discrimi-native technique.
We run this algorithm with pub-licly available code5.4.3 Experiment resultsIn our HMMs component, the number of hiddenstates is 80.
We randomly initialize the matrixof state transition probabilities and the initial statedistribution between 0 and 1.
The emission prob-abilities are determined by Gaussian distributions.In our recursive autoencoders component, we rep-resent each words using 100-dimensional vectors.The hyperparameter used for weighing reconstruc-tion and cross-entropy error is 0.1.For each dataset, we use 800 conversations asthe training data and the remaining are used fortesting.
We summarize the experiment results in2http://www.csie.ntu.edu.tw/~cjlin/liblinear/3http://nlp.stanford.edu/~sidaw4https://github.com/sancha/jrae/zipball/stable5https://github.com/mesnilgr/iclr15.35Table 1.
According to Table 1, the proposed ap-proach significantly and consistently outperformsother methods on both datasets.
This verifies theeffectiveness of the proposed approach.
For exam-ple, the overall accuracy of our algorithm is 3.2%higher than Mesnil?s method and 11.0% higherthan SVM on Twitter conversations dataset.
Forthe Sina Weibo dataset, we observe similar results.The advantage of our model comes from its capa-bility of exploring sequential information and in-corporating an arbitrary number of factors of thecorpus.5 Conclusion and Future WorkIn this paper, we present a general frameworkfor incorporating sequential data into languagemodeling.
We demonstrate the effectiveness ofour method by applying it to a specific appli-cation: predicting topics and sentiments in dia-logues.
Experiments on real data demonstrate thatour method is substantially more accurate thanprevious methods.ReferencesPierre Baldi and S?ren Brunak.
2001.
Bioinformatics:the machine learning approach.
MIT press.Leonard E Baum, Ted Petrie, George Soules, and Nor-man Weiss.
1970.
A maximization technique occur-ring in the statistical analysis of probabilistic func-tions of markov chains.
The annals of mathematicalstatistics, pages 164?171.Leonard E Baum.
1972.
An equality and associ-ated maximization technique in statistical estimationfor probabilistic functions of markov processes.
In-equalities, 3:1?8.Yoshua Bengio, Holger Schwenk, Jean-S?bastienSen?cal, Fr?deric Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
InInnovations in Machine Learning, pages 137?186.Springer.Kevin Duh.
2005.
Jointly labeling multiple sequences:A factorial hmm approach.
In Proceedings of theACL Student Research Workshop, pages 19?24.
As-sociation for Computational Linguistics.Jurgen V Gael, Yee W Teh, and Zoubin Ghahramani.2009.
The infinite factorial hidden markov model.In Advances in Neural Information Processing Sys-tems, pages 1697?1704.Ren?
Garcia and Pierre Perron.
1996.
An analysis ofthe real interest rate under regime shifts.
The Reviewof Economics and Statistics, pages 111?125.Zoubin Ghahramani and Michael I Jordan.
1997.
Fac-torial hidden markov models.
Machine learning,29(2-3):245?273.Isabelle Guyon and Fernando Pereira.
1995.
Designof a linguistic postprocessor using variable memorylength markov models.
In Document Analysis andRecognition, 1995., Proceedings of the Third Inter-national Conference on, volume 1, pages 454?457.IEEE.Takayuki Hasegawa, Nobuhiro Kaji, Naoki Yoshinaga,and Masashi Toyoda.
2013.
Predicting and elicitingaddressee?s emotion in online dialogue.
In ACL (1),pages 964?972.Anil K Jain, Robert P. W. Duin, and Jianchang Mao.2000.
Statistical pattern recognition: A review.Pattern Analysis and Machine Intelligence, IEEETransactions on, 22(1):4?37.Hye-Chung Monica Kum, Susan Paulsen, and WeiWang.
2005.
Comparative study of sequential pat-tern mining models.
In Foundations of Data Miningand knowledge Discovery, pages 43?70.
Springer.Amlan Kundu and Paramrir Bahl.
1988.
Recognitionof handwritten script: a hidden markov model basedapproach.
In Acoustics, Speech, and Signal Process-ing, 1988.
ICASSP-88., 1988 International Confer-ence on, pages 928?931.
IEEE.David D Lewis and William A Gale.
1994.
A se-quential algorithm for training text classifiers.
InProceedings of the 17th annual international ACMSIGIR conference on Research and development ininformation retrieval, pages 3?12.
Springer-VerlagNew York, Inc.Gr?goire Mesnil, Marc?Aurelio Ranzato, TomasMikolov, and Yoshua Bengio.
2014.
Ensembleof generative and discriminative techniques for sen-timent analysis of movie reviews.
arXiv preprintarXiv:1412.5335.Tomas Mikolov, Martin Karafi?t, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.R Nag, K Wong, and Frank Fallside.
1986.
Scriptrecognition using hidden markov models.
In Acous-tics, Speech, and Signal Processing, IEEE Interna-tional Conference on ICASSP?86., volume 11, pages2071?2074.
IEEE.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in naturallanguage processing-Volume 10, pages 79?86.
As-sociation for Computational Linguistics.Lawrence Rabiner.
1989.
A tutorial on hidden markovmodels and selected applications in speech recogni-tion.
Proceedings of the IEEE, 77(2):257?286.36Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Un-supervised modeling of twitter conversations.Padhraic Smyth.
1994.
Hidden markov models forfault detection in dynamic systems.
Pattern recog-nition, 27(1):149?164.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011a.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 151?161.
Association forComputational Linguistics.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011b.Semi-Supervised Recursive Autoencoders for Pre-dicting Sentiment Distributions.
In Proceedings ofthe 2011 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP).Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 1631?1642.
Citeseer.Alice Oh Suin Kim, JinYeong Bak.
2012.
Discover-ing emotion influence patterns in online social net-work conversations.
In SIGWEB ACM Special In-terest Group on Hypertext, Hypermedia, and Web.ACM.Peng Wang and Qiang Ji.
2005.
Multi-view face track-ing with factorial and switching hmm.
In Applica-tion of Computer Vision, 2005.
WACV/MOTIONS?05Volume 1.
Seventh IEEE Workshops on, volume 1,pages 401?406.
IEEE.Sida Wang and Christopher D Manning.
2012.
Base-lines and bigrams: Simple, good sentiment and topicclassification.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics: Short Papers-Volume 2, pages 90?94.
As-sociation for Computational Linguistics.37
