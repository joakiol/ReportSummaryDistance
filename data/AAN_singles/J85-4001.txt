ON THE COMPLEXITY OF ID /LP  PARSING 1G.
Edward Barton, Jr.MIT Artificial Intelligence Laboratory545 Technology SquareCambridge, MA 02139Modern linguistic theory attributes surface complexity to interacting subsystems of constraints.
Forinstance, the ID/LP grammar formalism separates constraints on immediate dominance from those onlinear order.
An ID/LP parsing algorithm by Shieber shows how to use ID and LP constraints directlyin language processing, without expanding them into an intermediate context-free "object grammar".However, Shieber's purported runtime bound underestimates the difficulty of ID/LP parsing.
ID/LPparsing is actually NP-complete, and the worst-case runtime of Shieber's algorithm is actually expo-nential in grammar size.
The growth of parser data structures causes the difficulty.
Some computa-tional and linguistic implications follow; in particular, it is important to note that, desplte its potentialfor combinatorial explosion, Shieber's algorithm remains better than the alternative of parsing anexpanded object grammar.1 INTRODUCTIONIt is common in recent linguistic theories for varioussurface characteristics of a language to be described interms of several different kinds of underlying constraints.ID/LP grammars involve immediate-dominance rules andlinear-order constraints; more broadly, GPSG systemscan also involve feature relationships and metarules(Gazdar et al 1985).
The tree adjunetion grammars ofKroch and Joshi (1985) separate the statement of localconstraints from the projection of those constraints tolarger structures.
The GB-framework of Chomsky(1981:5) identifies the subtheories of bounding, govern-ment, 0-marking, binding, Case, and control.
Whenseveral independent constraints are involved, a systemthat explicitly multiplies out their effects is large, cumber-some, and uninformative.
2 If done properly, the disen-tanglement of different kinds of constraints can result inshorter and more illuminating language descriptions.With any such modular framework, two questionsimmediately arise: how can the various constraints beput back together in parsing, and what are the computa-tional characteristics of the process?
One approach is tocompile a large object grammar that expresses thecombined effects of the constraints in a more familiarformat such as an ordinary context-free grammar (CFG).The context-free object grammar can then be parsed withEarley's (1970) algorithm or any of several other well-known procedures with known computational character-istics.However, in order to apply this method, it is necessaryto expand out the effects of everything that falls outsidethe strict context-free format: rule schemas, metarules,ID rules, LP constraints, feature instantiations, case-marking constraints, etc.
The standard algorithms oper-ate on CFGs, not on extended variants of them.Unfortunately, the object grammar may be huge after theeffects of all nonstandard evices have been expandedout.
Estimates of the object-grammar size for typicalsystems vary from hundreds or thousands 3 up to trillionsof rules (Shieber 1983:4).
With some formalisms, thecontext-free object-grammar pproach is not even possi-ble because the object grammar would be infinite (Shie-ber 1985:145).
Grammar size matters beyond questionsof elegance and clumsiness, for it typically affects proc-essing complexity.
Berwick and Weinberg (1982) arguethat the effects of grammar size can actually dominatecomplexity for a relevant range of input lengths.Given the disadvantages of multiplying out the effectsof separate systems of constraints, Shieber's (1983) workon direct parsing leads in a welcome direction.
Shieberconsiders how one might do parsing with ID/LP gram-mars, which involve two orthogonal kinds of rules.
IDrules constrain immediate dominance irrespective ofconstituent order ("a sentence can be composed of Vwith NP and SBAR complements"), while LP rulesCopyright1985 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is granted provided thatthe copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.
To copyotherwise, or to republish, requires a fee and/or specific permission.0362-613X/85/040205-218503.00Computational Linguistics, Volume 11, Number 4, October-December 1985 205G.
Edward Barton, Jr. On the Complexity of ID/LP Parsingconstrain linear precedence among the daughters of anynode ("if V and SBACR are sisters, then V must precedeSBAR").
Shieber shows how Earley's (1970) algorithmfor parsing context-free grammars (CFGs) can beadapted to use the constraints of ID/LP grammars direct-ly, without the combinatorially explosive step of convert-ing the ID/LP grammar into standard context-free form.Instead of multiplying out all of the possible surfaceinteractions among the ID and LP rules, Shieber's algo-rithm applies them one step at a time as needed.
Surelythis should work better in a parsing application thanapplying Earley's algorithm to an expanded grammarwith trillions of rules, since the worst-case time complexi-ty of Earley's algorithm is proportional to the square ofthe grammar size!Shieber's general approach is on the right track.
Onpain of having a large and cumbersome rule system, theparser designer should first look to linguistics to find thecorrect set of constraints on syntactic structure, thendiscover how to apply some form of those constraints inparsing without multiplying out all possible surfacemanifestations of their effects.Nonetheless, nagging doubts about computationalcomplexity remain.
Although Shieber (1983:15) claimsthat his algorithm is identical to Earley's in time complex-ity, it seems almost too much to hope for that the size ofan ID/LP grammar should enter into the time complexityof ID/LP parsing in exactly the same way that the size ofa CFG enters into the time complexity of CFG parsing.An ID/LP grammar G can enjoy a huge size advantageover a context-free grammar G p for the same language;for example, if G contains only the rule S -~ ~D abcde,the corresponding G r contains 5!
= 120 rules.
In effect,the claim that Shieber's algorithm has the same timecomplexity as Earley's algorithm means that this tremen-dously increased brevity of expression comes free (up toa constant).
The paucity of supporting argument inShieber's article does little to allay these doubts:We will not present a rigorous demonstration of timecomplexity, but it should be clear from the close relationbetween the presented algorithm and Earley's that thecomplexity is that of Earley's algorithm.
In the worstcase, where the LP rules always specify a unique orderingfor the right-hand side of every ID rule, the presentedalgorithm reduces to Earley's algorithm.
Since, given thegrammar, checking the LP rules takes constant ime, thetime complexity of the presented algorithm is identical toEarley's .
.
.
.
That is, it is O( I G 12 n3), where I G I isthe size of the grammar (number of ID rules) and n is thelength of the input.
(: 14f)Many questions remain; for example, why should a situ-ation of maximal constraint represent the worst case, asShieber claims?
4The following sections will investigate the complexityof ID/LP parsing in more detail.
In brief, the outcome isthat Shieber's direct-parsing algorithm usually does havea time advantage over the use of Earley's algorithm onthe expanded CFG, but that it blows up in the worst case.The claim of O( I G 12 n 3) time complexity is mistaken; infact, the worst-case time complexity of ID/LP parsingcannot be bounded by any polynomial in the size of thegrammar and input, unless ~' = ,A'@.
ID/LP parsing isNP-complete.As it turns out, the complexity of ID/LP parsing has itssource in the immediate-domination rules rather than thelinear precedence constraints.
Consequently, the prece-dence constraints will be neglected.
Attention will befocused on unordered context-free grammars (UCFGs),which are exactly like standard context-free grammarsexcept that when a rule is used in a derivation, thesymbols on its right-hand side are considered to be unor-dered and hence may be written in any order.
UCFGsrepresent he special case of ID/LP grammars in whichthere are no LP constraints.
Shieber's ID/LP algorithmcan be used to parse UCFGs simply by ignoring all refer-ences to LP constraints.2 GENERALIZING EARLEY'S ALGORITHMShieber generalizes Earley's algorithm by modifying theprogress datum that tracks progress through a rule.
TheEarley algorithm uses the position of a dot to track.linearadvancement through an ordered sequence of constitu-ents.
The major predicates and operations on suchdotted rules are these:?
A dotted rule is initialized with the dot at the left edge,as in X --, .ABC .?
A dotted rule is advanced across a terminal or nonter-minal that was predicted and has been located in theinput by simply moving the dot to the right.
For exam-ple, X -~ A .BC is advanced across a B by moving thedot to obtain X -~ AB.C  .?
A dotted rule is complete iff the dot is at the right edge.For example, X -~ ABC.
is complete.?
A dotted rule predicts a terminal or nonterminal iff thedot is immediately before the terminal or nonterminal.For example, X -~ A .BC predicts B.UCFG rules differ from CFG rules only in that the right-hand sides represent unordered multisets (that is, setswith repeated elements allowed).
It is thus appropriate touse successive accumulation of set elements in place oflinear advancement through a sequence.
In essence,Shieber's algorithm replaces the standard operations ondotted rules with corresponding operations on what willbe called dotted UCFG rules.
5?
A dotted UCFG rule is initialized with the empty multi-set before the dot and the entire multiset of right-handelements after the dot, as in X --- { } ?
{A, B, C}.?
A dotted UCFG rule is advanced across a terminal ornonterminal that was predicted and has been located inthe input by simply moving one element from themultiset after the dot to the multiset before the dot.For example, X -- {d} ?
{B, C} is advanced across a Bby moving the B to obtain X -* {A, B} ?
{C}.
Similar-206 Computational Linguistics, Volume 11, Number 4, October-December 1985G.
Edward Barton, Jr. On the Complexity of ID/LP Parsingly, X -~ {A } ?
{B, C, C} may be advanced across a C toobtain X-~ {A, C}" {B, C}.?
A dotted UCFG rule is complete iff the multiset afterthe dot is empty.
For example, X -~ {A, B, C} ?
{} iscomplete.?
A dotted UCFG rule predicts a terminal or nonterminaliff the terminal or nonterminal is a member of themultiset after the dot.
For example, X -~ {A} ?
{B, C}predicts B and C.Given these replacements for operations on dotted rules,Shieber's algorithm operates in the same way as Earley'salgorithm.
As usual, each state in the parser's tate setsconsists of a dotted rule tracking progress through aconstituent plus the if~terword position defining theconstituent's left edge (Earley 1970:95, omitting looka-head).
The left-edge position is also referred to as thereturn pointer because of its role in the complete opera-tion of the parser.3 THE ADVANTAGES OF SHIEBER'S ALGORITHMThe first question to ask is whether Shieber's algorithmsaves anything.
Is it faster to use Shieber's algorithm ona UCFG than to use Earley's algorithm on the corre-sponding expanded CFG?
Consider the UCFG G1 thathas only the single rule S -* abcde.
The correspondingCFG Grl has 120 rules spelling out all the permutationsof abcde: S -~ abcde, S -- abced, and so forth.
If thestring abcde is parsed using Shieber's algorithm directlyon G1, the state sets of the parser emain small.
6S O : \[S "-" { } " {a,b,c,d,e}, O\]S 1 : \[S ~ {a} ?
{b,c,d,e}, O\]S z : \[S -*.
{a,b} ?
{c,d,e}, O\]S 3 " \ [S  ~ {a,b,c} ?
{d,e}, O\]3 4 : \ [S  -~ {a,b,c,d} ?
{e}, 0\]S 5 : \[S ~ {a,b,c,d,e} ?
{ }, O\]In contrast, consider what happens if the same string isparsed using Earley's algorithm on the expanded CFGwith its 120 rules.
As Figure 1 illustrates, the state setsof the Earley parser are much larger.
In state set S~, theEarley parser uses 4!
= 24 states to spell out all thepossible orders in which the remaining symbols {b,c,d,e}could appear.
Shieber's modified parser does not spellthem out, but uses the single state \[S ~ {a} ?
{b,c,d,e}, O\]to summarize them all.
Shieber's algorithm should thus befaster, since both parsers work by successively processingall of the states in the state sets.Similar examples show that the Shieber parser canenjoy an arbitrarily large advantage over the use of theEarley parser on the expanded CFG.
Instead of multiply-ing out all surface appearances ahead of time to producean expanded CFG, Shieber's algorithm works out thepossibilities one step at a time, as needed.
This can be anadvantage because not all of the possibilities may arisewith a particular input.
(a)(b)\[S ~ {a} ?
{b,c,d,e}, O\]\[S --~ a.edcb, O\] \[S ~ a.ecbd, O\]\[S -* a.decb, O\] \[S ~ a.cebd, O\]\[S --~ a.ecdb, O\] \[S -~ a.ebcd, O\]\[S ~ a.cedb, O\] \[S --~ a.becd, O\]\[S ~ a.dceb, O\] \[S --~ a.cbed, O\]\[S --~ a.cdeb, O\] \[S --,.
a.bced, O\]\[S --,.
a.edbc, O\] \[S --~ a.dcbe, O\]\[S -~ a.debc, O\] \[S --,.
a.cdbe, O\]\[S --~ a.ebdc, O\] \[S -~ a.dbce, O\]\[S -,.
a.bedc, O\] \[S --~ a.bdce, O\]\[S --~ a.dbec, O\] \[S ~ a.cbde, O\]\[S ~ a.bdec, O\] \[S ~ a.bcde, O\]Figure 1.
The use of the Shieber parser on a UCFG canenjoy a large advantage over the use of the Earley parseron the corresponding expanded CFG.
After having proc-essed the terminal a while parsing the string abcde asdiscussed in the text, the Shieber parser uses the singlestate shown in (a) to keep track of the same informationfor which the Earley parser uses the 24 states in (b).4 COMBINATORIAL EXPLOSION WITHSHIEBER'S ALGORITHMThe answer to the first question is yes, then: it can bemore efficient to use Shieber's parser than to use theEarley parser on an expanded object grammar.
Thesecond question to ask is whether Shieber's parser alwaysenjoys a large advantage.
Does the algorithm blow up indifficult cases?In the presence of lexical ambiguity, Shieber's algo-rithm can suffer from combinatorial explosion.
Considerthe following UCFG, Gz, in which x is five-ways ambig-uous:S -~ ABCDEA -~a lxB -~blxC -~c lxD ~dlxE -~e lxWhat happens if Shieber's algorithm is used to parse thestring xxxxa according to this grammar?
After the firstthree occurrences of x have been processed, the state setof Shieber's parser will reflect the possibility that anythree of the phrases A, B, C, D, and E might have beenencountered in the input and any two of them might5 remain to be parsed.
There will be (q) = 10 statesreflecting progress through the rule expaffding S, in addi-tion to 5 states reflecting phrase completion and 10 statesreflecting phrase prediction (not shown):Computational Linguistics, Volume 11, Number 4, October-December 1985 207G.
Edward Barton, Jr. On the Complexity of ID/LP ParsingS 3" IS- -~\ [ s - .\[s-~\[ s -.\[s-~\[s-~\[s -,\ [ s - .\[ s -~\[s-~{A,B,C} {D,E}, O\]{A,B,D} {C,E}, O\]{A,C,D} {B,E}, O\]{B,C,D} {A,E}, O\]{A,B,E} {C,D}, O\]{A,C,E} {B,D}, O\]{B,C,E} {A,D}, O\]{A,D,E} {B,C}, O\]{B,D,E} {A,C}, O\]{C,D,E} {A,B}, O\]In cases like this, Shieber's algorithm enumerates all ofthe combinations of k elements taken i at a time, where kis the rule length and i is the number of elements alreadyprocessed.
Thus it can be combinatorially explosive.It is important o note that even in this case Shieber'salgorithm wins out over parsing the expanded CFG withEarley's algorithm.
After the same input symbols havebeen processed, the state set of the Earley parser willreflect the same possibilities as the state set of the Shie-ber parser: any three of the required phrases might havebeen located, while any two of them might remain to beparsed.
However, the Earley parser has a less conciserepresentation to work with.
In place of the state involv-ing S -~ {A,B,C} ?
{D,E}, for instance, there will be 3!
?2!
= 12 states involving S -, ABC.DE, S -~ BCA.ED,and so forth.
Instead of a total of 25 states, the Earleystate set will contain 135 = 12 ?
10 + 15 states.
7In the above case, although the parser could not besure of the eategorial identities of the phrases parsed, atleast there was no uncertainty about the number ofphrases and their extent.
We can make matters evenworse for the parser by introducing uncertainty in thoseareas as well.
Let G3 be the result of replacing every x inGz with the empty string e:S -~ ABCDEA "~ a leB --, b leC -c leD --, d ieE - ' -e leThen an A, for instance, can be either an a or nothing.Before any input has been read, the first state set So inShieber's parser must reflect the possibility that thecorrect parse may include any of the 25 = 32 possiblesubsets of A, B, C, D, and E as empty initial constituents.For example, S O must include \[S --- {A,B,C,D,E} ?
{}, 0\]because the input might turn out to be the null string.Similarly, it must include \[S -~ {A,C,E} ?
{B,D}, O\]because the input might turn out to be bd or db.
Countingall possible subsets in addition to other states having todo with predictions, completions, and the parser's startsymbol, there are 44 states in S O .
(There are 338 statesin the corresponding state when the expanded CFG Gt3 isused.
)5 THE SOURCE OF THE DIFFICULTYWhy is Shieber's algorithm potentially exponential ingrammar size despite its "close relation" to Earley's algo-rithm, which has time complexity polynomial in grammarsize?
The answer lies in the size of the state space thateach parser uses.
Relative to grammar size, Shieber'salgorithm~ involves a much larger bound than Earley'salgorithm on the number of states in a state set.
Sincethe main task of the Earley parser is to perform scan,predict, and complete operations on the states in eachstate set (Eariey 1970:97), an explosion in the size of thestate sets will be fatal to any small runtime bound.Given a CFG Go, how many possible dotted rules arethere?
Resulting from each rule X - -  A t .
.
.
Ak, there arek+ 1 possible dotted rules.
Then the number of possibledotted rules is bounded by I G I, if this notation is takento mean the number of symbols that it takes to write Gdown.
An Earley state is a pair \[r,i\], where r is a dottedrule and i is an interword position ranging from 0 to thelength n of the input string.
Because of these limits, nostate set in the Earley parser can contain more thanO( I Q l'n) (distinct) states.The limited size of a state set alows an O( I Go 12 ?
n3)bound to be placed on the runtime of the Earley parser.Informally, the argument (due to Earley) runs as follows.The scan operation on a state can be done in constanttime; the scan operations in a state set thus contribute nomore than O( \[ Ga \[ " n) computational steps.
All of thepredict operations in a state set taken together can add nomore states than the number of rules in the grammar,bounded by I G al ,  since a nonterminal needs to beexpanded only once in a state set regardless of how manytimes it is predicted; hence the predict operations neednot take more than O( I G I " n+ \[ G I ) = o (  I G I ?
n)steps.
Finally, there are the complete operations to beconsidered.
A given completion can do no worse thanadvancing every state in the state set indicated by thereturn pointer.
Therefore, any bound k on state set sizeleads to a bound of k z on the number of steps it takes todo all the completions in a state set.
Here k =O( \[ G I 'n),  so the complete operations in a state set cantake at most O( I G I z .
n 2) steps.
Overall, then, it takesno more than O( I G 12 ?
n 2) steps to process one state setand no more than O(IG a \[ z ?
n 3) steps for the Earleyparser to process them all.In Shieber's parser, though, the state sets can growmuch larger relative to grammar size.
Given a UCFG Gb,how many possible dotted UCFG rules are there?
Result-ing from a rule X -~ A 1 ... A k, there are not k+l  possibledotted rules tracking linear advancement, but 2 k possibledotted UCFG rules tracking accumulation of set elements.In the worst case, the grammar contains only one ruleand k is on the order of I Gel ;  hence the number ofpossible dotted UCFG rules for the whole grammar is notbounded by I Gbl, but by 216b I.
(The bound can bereached; recall that exponentially many dotted rules arecreated in the processing of G 3 from section 4.
)208 Computational Linguistics, Volume 11, Number 4, October-December !
985G.
Edward Barton, Jr. On the Complexity of ID/LP ParsingInformally speaking, the reason why Shieber's parsersometimes uffers from combinatorial explosion is thatthere are exponentially more possible ways to progressthrough an unordered rule expansion than an orderedone.
When disambiguating information is scarce, theparser must keep track of all of them.
In the more gener-al task of parsing ID/LP grammars, the most tractablecase occurs when constraint from the LP relation isstrong enough to force a unique ordering for every ruleexpansion.
Under such conditions, Shieber's parserreduces to Earley's.
However, the case of strongconstraint represents the best case computationally, ratherthan the worst case as Shieber (1983:14) claims.6 ID/LP PARSING IS INHERENTLY DIFFICULTThe worst-case time complexity of Shieber's algorithm isexponential in grammar size rather than quadratic asShieber (1983:15) believed.
Did Shieber simply choose apoor algorithm, or is ID/LP parsing inherently difficult inthe general case?
In fact, the simpler problem of recog-n i z ing  sentences according to a UCFG is NP-complete.
8Consequently, unless ~ = ,./F~, no algorithm for ID/LPparsing can have a runtime bound that is polynomial inthe size of the grammar and input.The proof of NP-completeness involves reducing thevertex cover problem (Garey and Johnson 1979:46) tothe UCFG recognition problem.
Through carefulconstruction of the grammar and input string, it is possi-ble to "trick" the parser into solving a known hard prob-lem.
The vertex cover problem involves finding a smallset of vertices in a graph with the property that everyedge of the graph has at least one endpoint in the set.Figure 2 shows a trivial example.e$be4Figure 2.
This graph illustrates a trivial instance of thevertex cover problem.
The set {c,d} is a vertex cover ofsize 2.To construct a grammar that encodes the question ofwhether the graph in Figure 2 has a vertex cover of size2, first take the vertex names a, b, c, and d as the alpha-bet.
Take START as the start symbol.
Take H 1 throughH 4 as special symbols, one per edge; also take U and Das special dummy symbols.Next, write the rules corresponding to the edges of thegraph.
Edge e a runs from a to c, so include the rules H~-, a and H t -, c. Encode the other edges similarly.Rules expanding the dummy symbols are also needed.Dummy symbol D will be used to soak up excess inputsymbols, so D -, a through D -, d should be rules.Dummy symbol U will also be used to soak up excessinput symbols, but U will be allowed to match only whenthere are four occurrences in a row of the same symbol(one occurrence for each edge).
Take U ~ aaaa,  U - ,bbbb,  U - ,  cccc, and U -~ dddd as the rules expanding U.Now, what does it take for the graph to have a vertexcover of size k = 2?
One way to get a vertex cover is togo through the list of edges and underline one endpointof each edge.
If the vertex cover is to be of size 2, theunderlining must be done in such a way that only twodistinct vertices are ever touched in the process.
Alterna-tively, since there are 4 vertices in all, the vertex coverwill be of size 2 if there are 4-2=2 vertices left untouchedin the underlining process.
This method of finding avertex cover can be translated into a UCFG rule asfollows:START- - , .
H IH2H3H4UUDDDDThat is, each H-symbol is supposed to match the name ofone of the endpoints of the corresponding edge, inaccordance with the rules expanding the H-symbols.Each U-symbol is supposed to correspond to a vertexthat was left untouched by the H-matching, and theD-symbols are just there for bookkeeping.
Figure 3 liststhe complete grammar that encodes the vertex-coverproblem of Figure 2.START - ,  HtHzH3H 4UUDDDDH I -~ a l cH 2 - ,  b lcH 3 - , c ldH 4 - -b idU - ,  aaaa  I bbbb I cccc I ddddD - ,  a I b I c I dFigure 3.
For k = 2, the construction described in thetext transforms the vertex-cover problem of Figure 2 intothis UCFG.
A parse exists for the stringaaaabbbbccccdddd iff the graph in the previous figure hasa vertex cover of size < 2.To make all of this work properly, takeo = aaaabbbbccccddddas the input string to be parsed.
(In general, for everyvertex name x, include in o a contiguous run of occur-rences of x, one occurrence for each edge in the graph.
)The grammar encodes the underlining procedure byrequiring each H-symbol to match one of its endpoints ino.
Since the right-hand side of the START rule is unor-dered, the grammar allows an H-symbol to matchanywhere in the input, hence to match any vertex name(subject to interference from other rules that havealready matched).
Furthermore, since there is one occur-Computational Linguistics, Volume 11, Number 4, October-December 1985 209G.
Edward Barton, Jr. On the Complexity of ID/LP Parsingrence of each vertex name for every edge, all of theedges could conceivably be matched up with the samevertex; that is, it's impossible to run out of vertex-nameoccurrences.
Consequently, the grammar will alloweither endpoint of an edge to be "underlined".
Theparser will have to figure out which endpoints to choose- in other words, which vertex cover to select.
However,the grammar also requires two occurrences of U to matchsomewhere.
U can only match four contiguous identicalinput symbols that have not been matched in any otherway, and thus if the parser chooses a vertex cover that istoo large, the U-symbols will not match and the parsewill fail.
The proper number of D-symbols is given bythe length of the input string, minus the number of edgesin the graph (to account for the /-/~-matches), minus ktimes the number of edges (to account for theU-matches): in this case, 16 - 4 - (2.4) = 4, as illus-trated in the START rule.The net result of this construction is that in order todecide whether o is in the language generated by theUCFG, the parser must in effect search for a vertex coverof size 2 or less.
9 If a parse exists, an appropriate vertexcover can be read off from beneath the H-symbols in theparse tree; conversely, if an appropriate vertex coverexists, it indicates how to construct a parse.
Figure 4shows the parse tree that encodes a solution to thevertex-cover problem of Figure 2.The construction shows that vertex-cover problem isreducible to UCFG recognition.
Furthermore, theconstruction of the grammar and input string can becarried out in polynomial time.
Consequently, UCFGrecognition and the more general task of 1D/LP parsingmust be computationally difficult.
For a more carefuland detailed treatment of the reduction and its correct-ness, see the appendix.7 COMPUTATIONAL IMPLICATIONSThe reduction of Vertex Cover shows that 'the ID /LPparsing problem is NP-complete.
Unless ~ = ,./V~P, thetime complexity of ID/LP parsing cannot be bounded byany polynomial in the size of the grammar and input, l?An immediate conclusion is that complexity analysis mustbe done carefully: despite its similarity to Earley's algo-rithm, Shieber's algorithm does not have complexityO( I G 12 .
n3).
For some choices of grammar and input,its internal structures undergo exponential growth.
Otherconsequences also follow.7.1 PARSING THE OBJECT GRAMMAREven in the face of its combinatorially explosive worst-case behavior, Shieber's algorithm should not be imme-diately cast  aside.
Despite the fact that it sometimesblows up, it still has an advantage over the alternative ofparsing the expanded object grammar.
One interpreta-tion of the NP-completeness result is that the generalcase of ID/LP parsing is inherently difficult; hence itshould not be surprising that Shieber's algorithm for solv-ing that problem can sometimes uffer from combina-torial explosion.
More significant is the fact that parsingwith the expanded CFG blows up in cases that should notbe difficult.
There is nothing inherently difficult aboutparsing the language that consists of all permutations ofthe string abcde, but while parsing that language theEarley parser can use 24 states or more to encode whatthe Shieber parser encodes in only one (section 3).
Toput the point another way, the significant fact is not thatthe Shieber parser can blow up; it is that the use of anexpanded CFG blows up unnecessarily.STARTU U HI H2 Hs  Da aa  a b b bb  e c e c114 D D Dd d d dFigure 4.
The grammar of Figure 3, which encodes the the vertex-cover problem of Figure 2, generates the string o =aaaabbbbccccdddd according to this parse tree.
The vertex cover {c,d} can be read off from the parse tree as the set ofelements dominated by H-symbols.210 Computational Linguistics, Volume 11, Number 4, October-December 1985G.
Edward Barton, J r .
On  the Complexity of ID /LP  Parsing7.2 IS PRECOMPILAT ION POSSIBLE?The present reduction of Vertex Cover to ID/LP Parsinginvolves constructing a grammar and input string thatboth depend on the problem to be solved.
Consequently,the reduction does not rule out tlae possibility thatthrough clever programming one might concentrate mostof the computational difficulty of ID/LP parsing into aseparate precompilation stage, dependent on the grammarbut independent of the input.
According to this optimis-tic scenario, the entire procedure of preprocessing thegrammar and parsing the input string would be as diffi-cult as any NP-complete problem, but after precompila-tion, the time required for parsing a particular inputwould be bounded by a polynomial in grammar size andsentence length.Regarding the case immediately at hand, Shieber'smodified Earley algorithm has no precompilation step.
11The complexity result implied by the reduction thusapplies with full force; any possible precompilation phasehas yet to be proposed.
Moreover, it is by no meansclear that a clever precompilation step is even possible; itdepends on exactly how I GI and n enter into thecomplexity function for ID/LP parsing.
If n enters as afactor multiplying an exponential, precompilation cannothelp enough to ensure that the parsing phase will run inpolynomial time.For example, suppose some parsing problem is knownto require 2161 ?
n 3 steps for solution.
12 If one is willing tospend, say, 10 ?
2161 steps in the precompilation phase, isit possible to reduce parsing-phase complexity to some-thing like I GI 8 ?
n3?
The answer is no.
Since byhypothesis it takes at least 2161 ?
n 3 steps to solve theproblem, there must be at least 2161 ?
n 3 - -  10  ?
2161 stepsleft to perform after the precompilation phase.
Theparameter n is necessarily absent from the precompilationcomplexity, hence the term 2161 ?
n 3 will eventually domi-nate.In a related vein, suppose the precompilation step isconversion from ID/LP to CFG form and the runtimestep is the use of the Earley parser on the expanded CFG.Although the precompilation step does a potentiallyexponential amount of work in producing G p from G,another exponential factor still shows up at runtimebecause I G p I in the complexity bound I G r 1 2 " n3 isexponentially arger than the original I G I ?7.3 POLYNOMINAL-T IME PARSINGOF  A F IXED GRAMMARAs noted above, both grammar and input in the currentvertex-cover reduction depend on the vertex-cover prob-lem to be solved.
The NP-completeness result would bestrengthened if there were a reduction that used the samefixed grammar for all vertex-cover problems, for it wouldthen be possible to prove that a precompilation phasewould be of little avail.
However, unless ~ = ,./t'~, it isimpossible to design such a reduction.
Since grammarsize is not considered to be a parameter of a fixed-gram-mar parsing problem, the use of the Earley parser on theobject grammar constitutes a polynomial-time algorithmfor solving the fixed-grammar ID/LP parsing problem.Although ID/LP parsing for a fixed grammar cantherefore be done in cubic time, that fact represents littlemore than an accounting trick.
The object grammar G pcorresponding to a practical ID/LP grammar would behuge, and if I G' I  2 ?
n 3 complexity is too slow, then itremains too slow when I G r 1 2 is regarded as a constant.The practical irrelevance of polynomial-time parsingfor a fixed grammar sheds some light on another questionthat is sometimes asked.
Can't we have our cake and eatit too by using the ID/LP grammar G directly when wewant to see linguistic generalizations, but parsing theobject grammar G ~ when we want efficient parsing?After all, the Earley algorithm runs in cubic time basedon the length of the input string, and its dependence ongrammar size is only I G r 1 2Essentially, the answer is that using the object gram-mar doesn't help.
The reduction shows that it's notalways easy to process the ID/LP form of the grammar,but it is no easier to use the Earley algorithm on theexpanded form.
As the examples that have beenpresented clearly illustrate, both the Shieber parser andthe Earley parser for a given language can end up withstate sets that contain large numbers of elements.
Theobject grammar does not promote efficient processing;the Shieber parser operating on the ID/LP grammar canoften do better than the Earley parser operating on theobject grammar, be.cause of its more concise represen-tation (section 4).The Earley-algorithm grammar-size factor I G r 1 2looks smaller than the Shieber factor 2161 until onerecalls that G ~ can be exponentially larger than G. Inother words, we can hide the factor 2161 inside I Gr l ,but that doesn't make it any smaller.
Thus parsing islikely to take a great many steps even if we parse theobject grammar, which might mistakenly be thought tobe more efficient than direct parsing.
If an algorithmruns too slowly, it doesn't make it faster if we cover upthe exponential factor and make it a constant K, and it'sunlikely that ID/LP parsing can be done quickly in thegeneral case.7.4 THE POWER OF THE UCFG FORMALISMThe Vertex Cover reduction also helps pin down thecomputational power of the UCFG formalism.
As G 1 andGrl in section 3 illustrated, a UCFG (or an ID/LP gram-mar) can enjoy considerable brevity of expressioncompared to the equivalent CFG.
The NP-completenessresult illuminates this property in two ways.
First, theresult shows that this brevity of expression is sufficient oallow an instance of any problem in ,Tg~ to be stated in aUCFG that is only polynomiaUy larger than the originalproblem instance.
In contrast, if an attempt is made toreplicate the current reduction with a CFG rather thanUCFG, the necessity of spelling out all the orders inwhich the H-, U-, and D-symbols might appear makesComputational Linguistics, Volume 11, Number  4,  October -December  1985 211G.
Edward Barton, Jr. On the Complexity of ID/LP Parsingthe CFG more than polynomially larger than the probleminstance.
Consequently, the reduction fails to establishNP-completeness, which indeed does not hold.
Second,the result shows that the increased expressive power doesnot come free; while the CFG recognition problem can besolved in time O( \[ G lZ'n 3) unless ~ = ,.,F~, the generalUCFG recognition problem cannot be solved in polyno-mial time.The details of the reduction show how powerful asingle UCFG rule can be.
If the UCFG formalism isextended to permit ordinary CFG rules in addition torules with unordered expansions, the grammar thatexpresses a vertex-cover problem needs only one UCFGrule, although that rule may need to be arbitrarily long.
t37.5 THE ROLE OF CONSTRAINTFinally, the discussion of section 5 illustrates the way inwhich the weakening of constraints can often make aproblem computationally more difficult.
It might erro-neously be thought that weak constraints represent hebest case in computational terms, for "weak" constraintssound easy to verify.
However, oftentimes the weaken-ing of constraint multiplies the number of possibilitiesthat must be considered in the course of solving a prob-lem.
In the case at hand, the removal of constraints onthe order in which constituents can appear causes thedependence of parsing complexity on grammar size togrow from \[ G 12 to 21 o l8 LINGUISTIC IMPLICATIONSThe key factors that cause difficulty in ID/LP parsing arefamiliar to linguistic theory.
GB-theory and GPSG bothpermit the existence of constituents hat are empty on thesurface, and thus in principle they both allow the kind ofpathology illustrated by G 3 in section 4, subject o amel-ioration by additional constraints.
Similarly, everycurrent theory acknowledges lexical ambiguity, a keyingredient of the vertex-cover reduction.
Though thereduction illuminates the power of certain mechanismsand formal devices, the direct implications of theNP-completeness result for grammatical theory are few.The reduction does expose the weakness of attemptsto link context-free generative power directly to efficientparsability.
Consider, for instance, Gazdar's (1981 :155)claim that the use of a formalism with only context-freepower can help explain the rapidity of human sentenceprocessing:Suppose .
.
.
that the permitted class of generative gram-mars constituted asubset of those phrase structure gram-mars capable only of generating context-free languages.Such a move would have two important metatheoreticalconsequences, one having to do with learnability, theother with processability .
.
.
.
We would have the begin-nings of an explanation for the obvious, but largelyignored, fact that humans process the utterances theyhear very rapidly.
Sentences of a context-free languageare provably parsable in a time which is proportional tothe cube of the length of the sentence or less.As the arguments and examples in this paper have illus-trated, context-free generative power does not guaranteeefficient parsability.
Every ID/LP grammar technicallygenerates a context-free language, but the potentiallylarge size of the corresponding CFG means that we can'tcount on that fact to give us efficient parsing.
Thus it isimpossible to sustain this particular argument for theadvantages of such formalisms as (early) GPSG overother linguistic theories; instead, GPSG and other moderntheories eem to be (very roughly) in the same boat withrespect o complexity.
In such a situation, the linguisticmerits of various theories are more important thancomplexity results.
(See Berwick (1982), Berwick andWeinberg (1982, 1984), and Ristad (1985) for furtherdiscussion.
)The reduction does not rule out the use of formalismsthat decouple ID and LP constraints; note that Shieber'sdirect parsing algorithm wins out over the use of theobject grammar.
However, if we assume that naturallanguages are efficiently parsable (EP), then computa-tional difficulties in parsing a formalism do indicate thatthe formalism itself does not tell the whole story.
That is,they point out that the range of possible languages hasbeen incorrectly characterized: the additional constraintsthat guarantee efficient parsability remain unstated.Since the general case of parsing ID/LP grammars iscomputationally difficult, if the linguistically relevantID/LP grammars are to be efficiently parsable, theremust be additional factors that guarantee a certainamount of constraint from some source, t4 (Constraintsbeyond the bare ID/LP formalism are required on linguis-tic grounds as well.)
Note that the subset principle oflanguage acquisition (cf.
Berwiek and Weinberg1984:233) would lead the language learner to initiallyhypothesize strong order constraints, to be weakenedonly in response to positive evidence.However, there are other potential ways to guaranteeefficient parsability.
It might turn out that the principlesand parameters of the best grammatical theory permitlanguages that are not efficiently parsable in the worstcase - just as grammatical theory permits sentences thatare deeply center-embedded (Miller and Chomsky1963).
15 In such a situation, difficult languages orsentences would not be expected to turn up in generaluse, precisely because they would be difficult to process.
16The factors that guarantee fficient parsability would notbe part of grammatical theory because they would resultfrom extragrammatical f ctors, i.e.
the resource limita-tions of the language-processing mechanisms.
This "easyway out" is not automatically available, depending as itdoes on a detailed account of processing mechanisms.For example, in the Earley parser, the difficulty of pars-ing a construction can vary widely with the amount oflookahead used (if any).
Like any other theory, anexplanation based on resource limitations must make theright predictions about which constructions will be diffi-cult to parse.212 Computational Linguistics, Volume 11, Number 4, October-December 1985G.
Edward Barton, Jr. On the Complexity of ID /LP  ParsingIn the same way, the language-acquisition procedurecould potentially be the source of some constraints rele-vant to efficient parsability.
Perhaps not all of thelanguages permitted by the principles and parameters ofsyntactic theory are accessible in the sense that they canpotentially be constructed by the language-acquisitioncomponent.
It is to be expected that language-acquisi-tion mechanisms will be subject to various kinds of limi-tations just as all other mental mechanisms are.
Again,however, concrete conclusions must await a detailedproposal.9 APPENDIXThis appendix contains the details of a more carefulreduction of the vertex-cover problem to the UCFGrecognition problem.
This version of the reductionestablishes that the difficulty of UCFG recognition is notdue either to the possibility of empty constituents(e-rules) or to the possibility of repeated symbols in rules(i.e., to the use of multisets rather than sets).
Conse-quently, it is somewhat different from and more complexthan the one sketched in the text.9.1 DEFINING UNORDERED CONTEXT-FREE GRAMMARSDefinition: An unordered CFG (UCFG) is a quadruple (N,E, R, S), where(a) N is a finite set of nonterminals.
(b) Y disjoint from N is a finite, nonempty set of terminalsymbols.
(c) R is a nonempty set of rules (,4, a), where A E N anda E (N U E)*.
The rule (,4, a) may be written as A0g.
(d) S e N is the start symbol.Convention: The grammar G and its components N, E, R,S need not be explicitly mentioned when clear fromcontext.Convention: Unless otherwise noted,(a) A, A', A~, ... denote elements of N;(b) a, a', ai ... denote elements of E;(c) X, Y, X ~, Y, X, Y, .
.
.
.
denote elements of N U E;(d) o, u, u', ui, ... denote elements of E*;(e) a,/3, ~,, 4, ~, denote elements of (NU E)*.Definition G = (N, E, R, S) is e-free iff for every (,4, a)~R, I~1 ~0.Definition: G = (N, E, R, S) is branching iff for some (,4,a>cR, lal >1.Definition: G = (N, E, R, S) is duplicate-free iff forevery (,4, a) E R, a = Y1 --- Y and for all id" ~ \[1,n\], Y= Yj iff i=j.Definition: G = (N, E, R, S) is simple iff it is e-free,duplicate-free, and branching.Note.
The notion of a simple UCFG is introduced inorder to help pin down the source of any computationaldifficulties associated with UCFGs.
For example, sincesimple UCFGs are restricted to be duplicate-free, a diffi-culty that arises with simple UCFGs cannot result fromthe possibility that a symbol may occur more than onceon the right-hand side of a rule.Definition: 4A~-->4a~P (by r) just in case (for some) r =G(A', Y1 "" Y) ~ R and for some permutation p of \[1,n\], A= A'  and a = Yp<l)... Yp<n).
If 4 c E*, also write4A4,~lm 4aq~.GDefinition: L(G) = {o E E*: S ~*  o}Definition: An n-step derivation of q~ from 4 is a sequence(4o .
.
.
.
.
4,) such that 40, = 4, 4, = ~P, and for all i ~ \[0,n- l \] ,  4i ~ 4i+r If it is also true for all i that 4i => lm 4i+1,say that the derivation is leftmost.9.2 DEFINING THE COMPUTATIONAL PROBLEMSDefinition: A possible instance of the problem VERTEXCOVER is a triple (V,E,k), where (V,E) is a finite graphwith at least one edge and at least two vertices, k c N,and k < I V\[ .17 VERTEX COVER itself consists of allpossible instances (V,E,k) such that for some 1f ___ V,I U I < k and for all edges e c E, at least one endpointof e is in l/ .
(Figure 5 gives an example of a VERTEXCOVER instance.
)11)W $ X6ye2V = {v,w,x ,y ,z}E = {e 1, e 2, eye  4, e 5, e 6, eT}with the e i as indicatedk = 3Figure 5.
The triple (V,E,k) is an instance of VERTEXCOVER.
The set I / = {v,x,z,'pf;} is a vertex cover of sizek=3.Fact: VERTEX COVER is NP-complete.
(Garey andJohnson 1979:46)Definition: A possible instance of the problem SIMPLEUCFG RECOGNITION is a pair (G, a), where G is asimple UCFG and o c Z*.
SIMPLE UCFGComputational Linguistics, Volume 11, Number 4, October-December 1985 213G.
Edward Barton, Jr. On the Complexity of ID/LP ParsingRECOGNITION itself consists of all possible instances (G,o) such that o ?
L(G).Notation: Take \[\[.
\[\[ to be any reasonable measure of theencoded input length for a computational problem;continue to use \[.
\[ for set cardinality and strihg length.It is reasonable to require that if S is a set, k ?
H, andI s I > k, then II s \[I > II/~ II, that is, the encoding ofnumbers is better than unary.
It is also reasonable torequire that l\[ ( .... x .
.
.
.  )
II _ II x II.9.3 THE UCFG RECOGNITION PROBLEM IS IN NPLemma 9.1: Let (if0 .
.
.
.
.
q~) be a shortest leftmost deri-vation of ~ from ~0 in a branching e-free UCFG.
If k >IN I+ I  then Iq,~l > I%1.Proof.
There exists some sequence of rules (,4 o, %) ...(Ak_ 1, a~_ 1) such that for all i ?
\[0, k - l \ ] ,  ~ => lm q~i+l by(,4 i, %).
Since G is e-free, I cPi+l \[ > I <Pi\[ always.Case 1.
For some i, I%1 > 1.
Then I~+11 > Iq~il.Hence \[ ~kl < \[ ~0 I.Case 2.
For every i, I%1 = 1.
Then there exist u, 3'such that for every i ?
\[0, k -2 \ ] ,  there is Ari ?
N suchthat ~+1 = u A'~ 3'.
Suppose the A'~ are all distinct.
ThenIN\ [  > k - l ,  hence IN I+ I  > k, hence IN I+ I  >I N I+ I ,  which is impossible.
Hence for some i j  ?
\[0,k -2 \ ] ,  i < j, Ar~ = A~j.
Hence ~+~ = q,j+~, since \[1,1\] hasonly one permutation.
Then ~0 .
.
.
.
.
ff~, q~j+l .
.
.
.
.
q~k) is aleftmost derivation of ~ from ~0 and has length less thank, which is also impossible.Then I~1 > I~01.
\ [ \ ]Corollary 9.2: If G is a branching e-free UCFG and o ?L(G), then o has a leftmost derivation of length at most1o1 "mwherem= \ [N \ [+2.Proof.
Let (q'0 .
.
.
.
, q'k) be a shortest leftmost derivationof o f romS.
Supposek  > \ [a \ [  "m. Consider the sub-derivations(~0, .
.
.
,  ~m)(~ .
.
.
.
.
.
~2m)(~'( I o i -,).m .
.
.
.  '
'~lo I'm)" ( '~lo I "  .
.
.
.
.
'/'~)"Each one except the last has m steps and m > \[N\[  + 1.Then by lemma,I~ lo l .ml  > I~( lo l -1 ) .m\ [> " "> Iq'ml > I,~01 = 1.Then 1ol _> l+\ [o l ,wh ich is imposs ib le .
Hencek  _<I~'1 "m. \[\]Lemma 9.3: H = SIMPLE UCFG RECOGNITION is inthe computational class ,/F~P.Proof.
Let G = (N, X, R, S) be a simple UCFG and o ?Y.*.
Consider the following nondeterministic algorithmwith input (G, o):Step 1.
Write down ~0 = S.Step 2.
Perform the following steps for i f rom 0 to \[ a \[ "m- l ,  where m = \] NI  +2.
(a) Express ~i as ur4i7 ~ by finding the leftmost nontermi-hal, or loop if impossible.
(b) Guess a rule (A~, Yt,l -.- Y,,k i) ?
R and a permutationp~ of \[ 1,k~\], or loop if there is no such rule.
(c) Write down ~i+1 = u~ Y~, pi(1) " ' "  Y/, Pi(/q) 3`i"(d) If t~i+l ---~ O then halt.Step 3.
Loop.It should be apparent hat the algorithm runs in time atworst polynomial in l\[ (G, o)II; note that the length of ~increases by at most a constant amount on each iteration.Assume (G, o) ?
H. Then o has a leftmost derivation oflength at most 1o1 " m by Corol lary 9.2; hence thenondeterministic algorithm will be able to guess it andwill halt.
Conversely, suppose the algorithm halts oninput (G, o).
On the iteration when the algorithm halts,the sequence (~0, .
.
.
,  ff~+l) will constitute a leftmost deri-vation of o from S; hence o ?
L(G) and ( G, o) ?
H.Then there is a nondeterministic algorithm that runs inpolynomial time and accepts exactly H. HenceH ?
JF@.D9.4 THE UCFG RECOGNITION PROBLEM IS NP-COMPLETELemma 9.4: Let (V,E,k) = (V,{ei} , k) be a possibleinstance of VERTEX COVER.
Then it is possible toconstruct, in time polynomial in II VII, UEII, and k, asimple UCFG G(V,E,k) and a string o(V,E,k) such that(G(V,E,k), o(V,E,k)) ~ SIMPLE UCFG RECOGNITIONiff (V,E,k) ?
VERTEX COVER.Proof.
Construct G(V,E,k) as follows.
Let the set N ofnonterminals consist of the following symbols not in V:START, U, D,for i ?
\[1, IE I  1,U~ for i e \[1, I V I-k\],D, for i ?
\[1, \ [E l  " (k - l ) \ ] .II Nil will be at worst polynomial in IIE II, II vii, and k fora reasonable length measure.
Define the terminal vocab-ulary Y to consist of subscripted symbols as follows:Z ={a i 'a ?
V,i ?
\[1, \ [E \ [ \ ]} .Designate START as the start symbol.
Include thefollowing as members of the rule set R:214 Computational Linguistics, Volume 11, Number 4, October-December 1985G.
Edward Barton, Jr. On the Complexity of ID /LP  Parsing(a) Include the ruleSTART -~ 111 ... Hlel Ut ... Ui vl-kDt? "
DIEI "(k-l)"(b) For each ei ?
E, include the rules{/-/~ -~ a~ : a an endpoint of e~}.
(c) For each i ?
\[1, I VI -k \ ] ,  include the rule U~ -~ U.Also include the rules{U~ a a .
.
.a le  I(d) For each i E \[1,D.
Also include:aE  V}.I E I ?
(k- I) \] ,  include the rule D~ -~the rules{D-~a:aE  Y~}.Take G(V,E,k) to be (N, E, R, START).
(Figure 6 showsthe results of applying this construction to the VERTEXCOVER instance of Figure 5.
)Let h : \[1, I V I \] -~ V be some standard enumeration ofthe elements of V. Construct o(V,E,k) as h(1) t ...h(1)lE I ... h( I  VI)1 ... h(J VI) IEI thus o(V,E,k) willhave length I E I  " I V I.It is easy to see that II (G(V,E,k), o(V,E,k))II will 'be atworst polynomial in II Ell, II vii, and k for reasonableII" II.
It will also be possible to construct the grammar andstring in polynomial time.
Finally, note that given thedefinition of a possible instance of VERTEX COVER, thegrammar will be branching, e-free, and duplicate-free,hence simple.Now suppose (V,E,k) ?
VERTEX COVER.
Then thereexist 1/ ~ V and f : E -~ 1 /such  that I I / I  < k and forevery e ?
E, f(e) is an endpoint of e. E is nonempty byhypothesis and I /  must hit every edge, hence \ [ l / \ [cannot be zero.
Construct a parse tree for o(V,E,k)according to G(V,E,k) as follows.Step 1.
Number the elements of V -  1/ as {x~ : i ?
\[1,I V -U  I\]}.
For each x i where i < I V l -k ,  construct anode dominating the substring (xi) t ... (x i) I EI of o(V,E,k)and label it U.
Then construct a node dominating onlythe U-node and label it U r Note that the availablesymbols U/ are numbered from 1 to IV I-k, so it isimpossible to run out of U-symbols.
Also, J U \[ < k andU- - -V ,  hence I v - l / I  = Iv l -  I~ l  > Iv l -k ,  soall of the U-symbols will be used.
Finally, note that U -~a t ... ale I is a rule for any a ?
S and that U~ -- U is  arule for any U rStep 2.
For each ei E E, construct a node dominating the(unique) occurrence of f(e)~ ~ o(V,E,k) and label it H .Step 2 cannot conflict with step 1 because f (e)  ?
V t,hence f(ea) ?
V - V'.
Different parts of step 2 cannotconflict with each other because each one affects asymbol with a different subscript.
Also note that f (e)  isan endpoint of ei and that//~ -- a~ is a rule for any e~ e Eand a an endpoint of e rStep 3.
Number all occurrences of terminals in o(V,E,k)that were not attached in step 1 or step 2.
For the ithsuch occurrence, construct a node dominating the occur-rence and label it D. Then construct another node domi-START -~111I L -~H 7 -~U1 -"U4 -~U -~IDaD 4 --~D 7D10O13 --~DIH1 HE H 3 H4 H 5 1t6 H7 U1 Uz D t D 2 D 3 Dn D 5 D 6 D 7 D 8 D 9 Dto Du D12 D13 DInVl I wl I-I2 -" v~ l y2 1t3 -" w3 I x3wn I z4 115 -" Xs I Y5 H6 -" Y6 I z6X 7 I Z 7u v~ -~ u v~ ~.
vUV 1 V 2 V 3 V 4 V 5"v 6 117 J W 1 W 2 W 3 W 4 W 5 W 6 W 7 I X lx  2X  3 X 4x  5X  6x  7Yl 22 Y3 Y4 -)/5 26 27 I Z 1 Z 2 Z 3 Z 4 Z 5 Z 6 Z 7D D 2 ~ D D 3 ~ DD D 5 --*.
D D 6 ~ DD D 8 ~ D D 9 ~ DD Dl t  ~ D D12 --~ DD D14 ~ Dvl lv21v31v41vslv61vvl wt lw21w31wnlwslw61wTIxl Ix2 ix3 Ix4 Ix5 Ix6 Ix7 I Y, lY2 lY3 lY4 lY5 lY6 lYTII z, I zz\[z3 \[zglz 5 \[z 6\[z 7Figure 6.
The construction of Lemma 9.4 produces this grammar when applied to the VERTEX COVER problem ofFigure 5.
The H-symbols ensure that the solution that is found must hit each of the edges, while the U-symbols ensurethat enough elements of V remain untouched to satisfy the requirement I l / I  _< k. The D-symbols are dummies thatabsorb excess input symbols.
A shorter grammar than this will suffice if the grammar is not required to be duplicate-free.Computational Linguistics, Volume 11, Number 4, October-December 1985 215G.
Edward Barton, Jr. On the Complexity of 1D/LP Parsingnating the D-node and label it D r Note that the stock ofD-symbols runs from 1 to (k - l )  I E I .
Exactly( I VI -k )  ?
I E I  symbols of tr(V,E,k) were accounted forin step 1.
Also, exactly I E I symbols were accounted forin step 2.
The length of a(V ,E ,k )  is I v I  ?
IE I ,  henceexactlyIVI " IE I  - ( IV I -k ) .
IE I  - IE I= IV I "  IE I -  IV I "  IE I  +k .
IE I -  IE I=(k - l ) "  IE Isymbols remain at the beginning of step 3.
D --- a is arule for any a in Z; Di -~D is a rule for any D rStep 4.
Finally, construct a node labeled START thatdominates all of the/at, ~,  and D~ nodes constructed insteps 1, 2, and 3.
The ruleSTART -~ Ha ... Hie I Ua ... U I vl -kDl ... Dlel.<k_t )is in the grammar.
Note also that nodes labeled H t .
.
.
.
.Hlel were constructed in step 2, nodes labeled U a .
.
.
.UI v l-k were constructed in step 1, and nodes labeled D1,?
.. DI e l.~k-1) were constructed in step 3.
Hence the appli-cation of the rule is in accord with the grammar.
Theno(V ,E ,k )  E L (G) .
(Figure 7 illustrates the application ofthis parse-tree construction procedure to the grammarand input string derived-from the VERTEX COVERexample in Figure 5.
)Conversely, suppose o(V ,E ,k )  e L (G) .
Then the deriva-tion of o(V ,E ,k )  from START must begin with the appli-cation of the ruleSTART -~ H a ... H Ie  I U a .. .
U I vl-kD1 ... DIEI'<k-I)and each H must later be expanded as some subscriptedterminal g(H) .
Define f(ei) tO be g(H)  without theSTARTHtH2DID2D3D4D5 Ut DoDTHaDaHsDgD,o U2 DttDt2DxsHiD,4HeH;D D D D D  DD U D D D D D U/A'01 '02 '03 104 '05 '?6 07 Wl~'2'W3W4'WSff)6%07 Xl X2 :Z3 :C4 Z5 Z6 .T7 YlY2Y$~/4YSYeY7 Zl ,Z'2II!iZ 3 Z- 4 Z 5 Z6 Z7Figure 7.
This parse tree shows how the grammar shown in Figure 6 can generate the string o(V,E,k) constructed inLemma 9.4 for the VERTEX COVER problem of Figure 5.
The corresponding VERTEX COVER solution l/.
= {v,x,z}and its intersection with the edges can be read off by noticing which terminals the H-symbols dominate.216 Computational Linguistics, Volume 11, Number 4, October-December 1985G.
Edward Barton, Jr. On the Complexity of ID/LP Parsingsubscript; then by construct ion of the grammar, f (e )  isan endpoint of e i for all e i E E. Define 1/ = {f(e) : e iE}; then it is apparent hat V' __q Vand that V' contains atleast one endpoint of e~ for all e i E E. Also, each U~ for iE \[1, I V I -k \ ]  must be expanded as U, then as somesubstring (a) ,  .
.
.
(ai) le I of o(V,E,k) ,  t8 Since thesubstrings dominated by the /7  and U~ must all bedisjoint, and since there are only I E I subscripted occur-rences of any single symbol from V in o(V,E,k) ,  theremust be I Vl - k distinct elements of V that are notdominated in any of their subscripted versions by any H rThen IV -  V'I -> \[VI - k. Since in addit ion V_c I J ,\] ~ I _< k. Then (V,E,k) in VERTEX COVER.
\ [ \ ]Theorem 1: SIMPLE UCFG RECOGNITION is NP-complete.Proof.
SIMPLE UCFG RECOGNITION is in the class ,.,4"~by Lemma 9.3, hence a polynomial-t ime reduction ofVERTEX COVER to SIMPLE UCFG RECOGNITION issufficient.
Let (V,E,k) be a possible instance of VERTEXCOVER.
Let G be G(V,E,k)  and o be (V,E,k)  asconstructed in Lemma 9.4.
Note that G is simple.The construction of G and o can, by lemma, be carriedout at time at worst polynomial  in IIEII, II VII, and k.Also by lemma (G,o) c SIMPLE UCFG RECOGNITIONiff (V,E,k) c VERTEX COVER.
k is not polynomial  in\[I k II under a reasonable encoding scheme.
However,IE I  > k, hence IIEll > Ilkll; also II(V,E,k}II > IIEII,hence II (V,E,k)II _> k, all by propert ies assumed to holdof I1" II.
Then G and o can in fact be constructed in timeat worst polynomial  in II (V,E,k} II.Hence the VERTEX COVER problem is polynomial-t imereduced to SIMPLE UCFG RECOGNITION.
\ [ \ ]REFERENCESBarton, G. Edward, Jr. 1984 Toward a Principle-Based Parser.
A.I.Memo No.
788, M.I.T.
Artificial Intelligence Laboratory,Cambridge, Massachusetts.Berwick, Robert C. 1982 Computational Complexity and Lexical-Functional Grammar.
American Journal of Computational Linguistics8(3-4): 97-109.Berwick, Robert C. and Weinberg, Amy S. 1982 Parsing Efficiency,Computational Complexity, and the Evaluation of GrammaticalTheories.
Linguistic Inquiry 13(2): 165-191.Berwick, Robert C. and Weinberg, Amy S. 1984 The GrammaticalBasis of Linguistic Performance.
M.I.T.
Press, Cambridge, Massa-chusetts.Chomsky, Noam A.
1980.
Rules and Representations.
ColumbiaUniversity Press, New York, New York.Chomsky, Noam A.
1981 Lectures on Government and Binding.
ForisPublications, Dordrecht, Holland.Earley, Jay 1970 An Efficient Context-Free Parsing Algorithm.Communications of the ACM 13(2): 94-102.Garey, Michael R. and Johnson, David S. 1979 Computers and Intract-ability.
W. H. Freeman and Co., San Francisco, California.Gazdar, Gerald 1981 Unbounded Dependencies and CoordinateStructure.
Linguistic Inquiry 12(2): 155-184.Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan1985 Generalized Phrase Structure Grammar.
Basil Blackwell,Oxford, U.K.Guerssel, Mohamed; Hale, Kenneth; Laughren, Mary; Levin.
Beth; andWhite Eagle, Josie 1985 A Cross-Linguistic Study of TransitivityAlternations.
Paper presented atthe Parasession on Causatives andAgentivity at the Twenty-First Regional Meeting of the ChicagoLinguistic Society, April 1985.Hopcroft, John E. and Ullman, Jeffrey D. 1979 Introduction to Auto-mata Theory, Languages, and Computation.
Addison-Wesley, Read-ing, Massachusetts.Kroch, Anthony S. and Joshi, Aravind K. 1985 The Linguistic Rele-vance of Tree Adjoining Grammars.
Technical Report No.MS-CIS-85-16, Department of Computer and Information Science,Moore School, University of Pennsylvania, Philadelphia, Pennsylva-nia.Levin, Beth; and Rappaport, Malka 1985 The Formation of AdjectivalPassives.
Lexicon Project Working Papers #2, M.I.T.
Center forCognitive Science, Cambridge, Massachusetts.Miller, George A. and Chomsky, Noam A.
1963 Finitary Models ofLanguage Users.
In: Luce, R. D.; Bush, R. R.; and Galanter, E.,Eds., Handbook of Mathematical Psychology, vol.
II.
John Wiley andSons, New York, New York: 419-492.Ristad, Eric S. 1985 GPSG-Recognition is NP-Hard.
A.I.
Memo No.837, M.I.T.
Artificial Intelligence Laboratory, Cambridge, Massa-chusetts.Shieber, Stuart M. 1983 Direct Parsing of ID/LP Grammars.
Techni-cal Report 291R, SRI International, Menlo Park, California.
Alsoappears in Linguistics and Philosophy 7(2).Shieber, Stuart M. 1985 Using Restriction to Extend Parsing Algo-rithms for Complex Feature-Based Formalisms.
ACL-85 confer-ence proceedings, pp.
145-152.NOTES1.
This report describes research done at the Artificial Intel-ligence Laboratory of the Massachusetts Institute ofTechnology.
Support for the Laboratory's artificial intelli-gence research as been provided in part by the AdvancedResearch Projects Agency of the Department of Defenseunder Office of Naval Research contractN00014-80-C-0505.
Partial support for the author'sgraduate studies was provided by the Fannie and JohnHertz Foundation.Useful guidance and commentary during the writing ofthis paper have been provided by Bob Berwick, MichaelSipser, and Joyee Friedman.
The paper was alsoimproved in response to interesting remarks from theanonymous referees for Computational Linguistics.
Theauthor gratefully acknowledges the assistance of BlytheHeepe in preparing the fitures.2.
See Barton (1984) for discussion.3.
This estimate is from an anonymous referee.4.
See section 5; it is in fact the best case.5.
Shieber's representation differs in some ways from therepresentation used here, which was developed independ-ently by the author.
The differences are generally ines-sential, but see note 7.6.
The states related to the auxiliary start symbol andendmarker that are added by some versions of the Earleyparser have been omitted for simplicity.7.
In contrast o the representation illustrated here, Shieber'srepresentation actually suffers to some extent from thesame problem.
Shieber (1983:10) uses an orderedsequence instead of a multiset before the dot; consequent-ly, in place of the state involving S ~ {A,B,C} ?
{D,E},Shieber would have the 3!
= 6 states involving S -*- a ?
{D,E}, where a ranges over the six permutations of ABC.8.
Recognition is simpler than parsing because a recognizeris not required to recover the structure of an input string,but only to decide whether the string is in the languageComputational Linguistics, Volume 1 I, Number 4, October-December 1985 217G.
Edward Barton, Jr. On the Complexity of ID/LP Parsinggenerated by the grammar: that is, whether or not thereexists a parse.9.
If the vertex cover is smaller than expected, theD-symbols will soak up the extra contiguous runs thatcould have been matched by more U-symbols.10.
Even assuming ~ # ,/F~, it does not follow that the timecomplexity must be exponential, though it seems likely tobe.
There are functions such as n ~?g" that fall betweenpolynomials and exponentials.
See Hopcroft and Ullman(1979:341).11.
Shieber (1983:15 n. 6) mentions a possible precompila-tion step, but it is concerned with the LP relation ratherthan the ID rules.12.
It is not known whether the worst-case complexity ofID/LP parsing is exponential, since more generally it is notknown for sure that ,~ # ,.,?'~.13.
The complexity of ID/LP parsing drops as maximum rulelength drops, but so does the succinctness advantage of anID/LP grammar over a standard CFG.14.
In the GB-framework of Chomsky (1981), for instance,the syntactic expression of unordered 0-grids at the .~level is constrained by the principles of Case theory.
In arelated framework, the limited possibilities for projectionfrom "lexical-conceptual structure" to syntactic argumentstructure combine with Case-assignment rules to severelyrestrict the possible configurations (Guerssel et al 1985,Levin 1985).
See also Berwick's (1982) discussion ofconstraints that could be placed on another grammaticalformalism - lexicai-functional grammar - to avoid a simi-lar intractability result.15.
Indeed, one may not conclude a priori that all thesentences of every language permitted by linguistic theoryare algorithmically parsable at all (Chomsky 1980).
Thisis true for a variety of reasons.
Imagine, for instance, thatlinguistic theory allowed the strings ruled out by filters tobe specified by complex enumerators.
Then the strings ofa language would be defined in part by subtracting of f  anr.e.
set, which could lead to nonrecursiveness because thecomplement of an r.e.
set is not always r.e.
But even ifnonreeursive, the set of strings would be perfectly well-de-fined.16.
It is often anecdotally remarked that languages that allowrelatively free word order tend to make heavy use ofinflections.
A rich inflectional system can supply parsingconstraints that make up for the lack of orderingconstraints; thus the situation we do not find is thecomputationally difficult case of weak constraint.17.
This formulation differs trivially from the one cited byGarey and Johnson.18.
The grammar would allow the substring (a}i ... (a~)lE I toappear in any permutation, but in o(V,E,k) it appears onlyin the indicated order.218 Computational Linguistics, Volume l l ,  Number 4, October-December 1985
