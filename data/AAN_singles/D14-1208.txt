Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1937?1941,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsNoisy Or-based model for Relation Extraction using Distant SupervisionAjay Nagesh1,2,31IITB-Monash Research Academyajaynagesh@cse.iitb.ac.inGholamreza Haffari2Faculty of IT, Monash Universitygholamreza.haffari@monash.eduGanesh Ramakrishnan3Dept.
of CSE, IIT Bombayganesh@cse.iitb.ac.inAbstractDistant supervision, a paradigm of rela-tion extraction where training data is cre-ated by aligning facts in a database with alarge unannotated corpus, is an attractiveapproach for training relation extractors.Various models are proposed in recent lit-erature to align the facts in the databaseto their mentions in the corpus.
In thispaper, we discuss and critically analyse apopular alignment strategy called the ?atleast one?
heuristic.
We provide a sim-ple, yet effective relaxation to this strat-egy.
We formulate the inference proce-dures in training as integer linear program-ming (ILP) problems and implement therelaxation to the ?at least one ?
heuris-tic via a soft constraint in this formulation.Empirically, we demonstrate that this sim-ple strategy leads to a better performanceunder certain settings over the existing ap-proaches.1 IntroductionAlthough supervised approaches to relation ex-traction (GuoDong et al., 2005; Surdeanu and Cia-ramita, 2007) achieve very high accuracies, theydo not scale as they are data intensive and the costof creating annotated data is quite high.
To alle-viate this problem, Mintz et al.
(2009) proposedrelation extraction in the paradigm of distant su-pervision.
In this approach, given a database offacts (e.g.
Freebase1) and an unannotated docu-ment collection, the goal is to heuristically alignthe facts in the database to the sentences in thecorpus which contain the entities mentioned in thefact.
This is done to create weakly labeled train-ing data to train a classifier for relation extraction.The underlying assumption is that all mentions of1www.freebase.coman entity pair2 (i.e.
sentences containing the en-tity pair) in the corpus express the same relationas stated in the database.The above assumption is a weak one and isoften violated in natural language text.
For in-stance, the entity pair, (Barack Obama, UnitedStates) participate in more than one relation:citizenOf, presidentOf, bornIn and every men-tion expresses either one of these fixed set of rela-tions or none of them.Consequently, a number of models have beenproposed in literature to provide better heuristicsfor the mapping between the entity pair in thedatabase and its mentions in the sentences of thecorpus.
Riedel et al.
(2010) tightens the assump-tion of distant supervision in the following man-ner: ?Given a pair of entities and their mentions insentences from a corpus, at least one of the men-tions express the relation given in the database?.In other words, it models the problem as that ofmulti-instance (mentions) single-label (relation)learning.
Following this, Hoffmann et al.
(2011)and Surdeanu et al.
(2012) propose models thatconsider the mapping as that of multi-instancemulti-label learning.
The instances are the men-tions of the entity pair in the sentences of the cor-pus and the entity pair can participate in more thanone relation.Although, these models work very well in prac-tice, they have a number of shortcomings.
Oneof them is the possibility that during the align-ment, a fact in the database might not have an in-stantiation in the corpus.
For instance, if our cor-pus only contains documents from the years 2000to 2005, the fact presidentOf(Barack Obama,United States) will not be present in the corpus.In such cases, the distant supervision assumptionfails to provide a mapping for the fact in the cor-pus.In this paper, we address this situation with a2In this paper we restrict ourselves to binary relations1937noisy-or model (Srinivas, 2013) in training the re-lation extractor by relaxing the ?at least one?
as-sumption discussed above.
Our contributions inthis paper are as follows: (i) We formulate the in-ference procedures in the training algorithm as in-teger linear programming (ILP) problems, (ii) Weintroduce a soft-constraint in the ILP objective tomodel noisy-or in training, and (iii) Empirically,our algorithm performs better than Hoffmann etal.
(2011) procedure under certain settings on twobenchmark datasets.Our paper is organized as follows.
In Section 2,we discuss our methodology.
We review the ap-proach of Hoffmann et al.
(2011) and explain ourmodifications to it.
In Section 3, we discuss re-lated work.
In Section 4, we discuss the experi-mental setup and our preliminary results.
We con-clude in Section 4.2 MethodologyOur work extends the work of Hoffmann et al.(2011).
So, we recapitulate Hoffmann?s model inthe following subsection.
Following which our ad-ditions to this model is explained in detail.Hoffmann?s modelHoffmann et al.
(2011) present a multi-instancemulti-label model for relation extraction throughdistant supervision.
In this model, a pair of enti-ties have multiple mentions (sentence containingthe entity pair) in the corpus.
An entity pair canhave one or more relation labels (obtained fromthe database).Objective functionConsider an entity pair (e1, e2) denoted by the in-dex i.
The set of sentences containing the entitypair is denoted xiand the set of relation labels forthe entity pair from the database is denoted by yi.The mention-level labels are denoted by the latentvariable z (there is one variable zjfor each sen-tence j).To learn the parameters ?, the training objectiveto maximize is the likelihood of the facts observedin the database conditioned on the sentences in thetext corpus.?
?= argmax?
?iPr(yi|xi; ?
)= argmax?
?i?zPr(yi, z|xi; ?
)The expression Pr(yi, z|xi) for a given entitypair is defined by two types of factors in the factorgraph.
They are extract factors for each mentionand mention factors between a relation label andall the mentions.The extract factors capture the local signal foreach mention and consists of a bunch of lexicaland syntactic features like POS tags, dependencypath between the entities and so on (Mintz et al.,2009).The mention factors capture the dependency be-tween relation label and its mentions.
Here, the atleast one assumption that was discussed in Section1 is modeled.
It is implemented as a simple deter-ministic OR operator as given below:fmention(yr, z) ={1 if yris true ?
?i : zi= r0 otherwiseTraining algorithmThe learning algorithm is a perceptron-styleparameter update scheme with 2 modifications:i) online learning ii) Viterbi approximation.
Theinference is shown to reduce to the well-knownweighted edge-cover problem which can besolved exactly, although Hoffmann et al.
(2011)provide an approximate solution.Algorithm 1: Hoffmann et al.
(2011) : Train-ingInput : i) ?
: set of sentences, ii) E: set of entitiesmentioned in the sentences, iii) R: set ofrelation labels, iv) ?
: database of factsOutput: Extraction model : ?beginfor t?
1 to T ; /* training iterations */dofor i?
1 to N ; /* No.
of entity pairs */doy?, z?,= argmaxy,zPr(y, z?
?xi; ?
)if y?!
= yithenz?= argmaxzPr(z?
?yi,xi; ?
)?new= ?old+?
(xi, z?)??
(xi, z?
)endOur additions to Hoffmann?s modelIn the training algorithm described above, thereare two MAP inference procedures.
Our con-tributions in this space is two-fold.
Firstly, we1938have formulated these as ILP problems.
As a re-sult of this, the approximate inference therein isreplaced by an exact inference procedure.
Sec-ondly, we replace the deterministic-or by a noisy-or which provides a soft-constraint instead of thehard-constraint of Hoffmann.
(?at least one?
as-sumption)ILP formulationsSome notations: zji: The mention variable zj(or jth sen-tence) taking the relation value i sji: Score for zjtaking the value of i. Scoresare computed from the extract factors yi: relation label being i m : number of mentions (sentences) for thegiven entity pair R: total number of relation labels (excludingthe nil label)Deterministic ORThe following is the ILP formulation for the exactinference argmaxPr(y, z|xi) in the model basedon the deterministic-or:maxZ,Y{m?j=1?i?
{R,nil}[zjisji]}s.t 1.?i?
{R,nil}zji= 1 ?j2.
zji?
yi?j, ?i3.
yi?m?j=1zji?iwhere zji?
{0, 1}, yi?
{0, 1}The first constraint restricts a mention to haveonly one label.
The second and third constraintsimpose the at least one assumption.
This is thesame formulation as Hoffmann but expressed asan ILP problem.
However, posing the inference asan ILP allows us to easily add more constraints toit.Noisy ORAs a case-study, we add the noisy-or soft-constraint in the above objective function.
Theidea is to model the situation where a fact ispresent in the database but it is not instantiated inthe text.
This is a common scenario, as the factspopulated in the database and the text of the corpuscan come from different domains and there mightnot be a very good match.maxZ,Y,?{(m?j=1?i?{R,nil}[zjisji])?
(?i?R?i)}s.t 1.?i?
{R,nil}zji= 1 ?j2.
zji?
yi?j, ?i3.
yi?m?j=1zji+ ?i?iwhere zji?
{0, 1}, yi?
{0, 1}, ?i?
{0, 1}In the above formulation, the objective functionis augmented with a soft penalty.
Also the thirdconstraint is modified with this penalty term.
Wecall this new term ?iand it is a binary variable tomodel noise.
Through this term we encourage atleast one type of configuration but will not disal-low a configuration that does not conform to this.Essentially, the consequence of this is to allow thecase where a fact is present in the database but isnot instantiated in the text.3 Related WorkRelation Extraction in the paradigm of distant su-pervision was introduced by Craven and Kum-lien (1999).
They used a biological database asthe source of distant supervision to discover rela-tions between biological entities.
The progressionof models for information extraction using distantsupervision was presented in Section 1.Surdeanu et al.
(2012) discuss a noisy-ormethod for combining the scores of various sen-tence level models to rank a relation during evalu-ation.
In our approach, we introduce the noisy-ormechanism in the training phase of the algorithm.Our work is inspired from previous workslike Roth and tau Yih (2004).
The use of ILPfor this problem facilitates easy incorporation ofdifferent constraints and to the best of our knowl-edge, has not been investigated by the community.4 ExperimentsThe experimental runs were carried out using thepublicly available Stanford?s distantly supervisedslot-filling system3 (Surdeanu et al., 2011) andHoffmann et al.
(2011) code-base4.3http://nlp.stanford.edu/software/mimlre.shtml4http://www.cs.washington.edu/ai/raphaelh/mr/1939Datasets and EvaluationWe report results on two standard datasets used asbenchmarks by the community namely KBP andRiedel datasets.
A complete description of thesedatasets is provided in Surdeanu et al.
(2012).The evaluation setup and module is the sameas that described in Surdeanu et al.
(2012).
Wealso use the same set of features used by the var-ious systems in the package to ensure that the ap-proaches are comparable.
As in previous work, wereport precision/recall (P/R) graphs to evaluate thevarious techniques.We used the publicly available lp solve pack-age5 to solve our inference problems.Performance of ILPUse of ILP raises concerns about performance asit is NP-hard.
In our problem we solve a separateILP for every entity pair.
The number of variablesis limited by the number of mentions for the givenentity pair.
Empirically, on the KBP dataset (largerof the two datasets), Hoffmann takes around 1hrto run.
Our ILP formulation takes around 8.5hrs.However, MIMLRE algorithm (EM-based) takesaround 23hrs to converge.ResultsWe would primarily like to highlight two settingson which we report the P/R curves and contrastit with Hoffmann et al.
(2011).
Firstly, we re-place the approximate inference in that work withour ILP-based exact inference; we call this set-ting the hoffmann-ilp.
Secondly, we replace thedeterministic-or in the model with a noisy-or, andcall this setting the noisy-or.
We further compareour approach with Surdeanu et al.
(2012).
TheP/R curves for the various techniques on the twodatasets are shown in Figures 1 and 2.We further report the highest F1 point in the P/Rcurve for both the datasets in Tables 1 and 2.Table 1 : Highest F1 point in P/R curve : KBP DatasetPrecision Recall F1Hoffmann 0.306451619 0.197916672 0.2405063349MIMLRE 0.28061223 0.286458343 0.2835051518Noisy-OR 0.297002733 0.189236104 0.2311770916Hoffmann-ilp 0.293010741 0.189236104 0.2299577976DiscussionWe would like to discuss the results in the abovetwo scenarios.5http://lpsolve.sourceforge.net/5.5/0.20.30.40.50.60.70  0.05  0.1  0.15  0.2  0.25  0.3precisionrecallhoffmannnoisyOrhoffmann_ilpmimlreFigure 1: Results : KBP dataset0.20.30.40.50.60.70.80.90  0.05  0.1  0.15  0.2  0.25  0.3precisionrecallhoffmannnoisyOrhoffmann_ilpmimlreFigure 2: Results : Riedel dataset1.
Performance of hoffmann-ilpOn the KBP dataset, we observe thathoffmann-ilp has higher precision in therange of 0.05 to 0.1 at lower recall (0 to 0.04).In other parts of the curve it is very close tothe baseline (although hoffmann?s algorithmis slightly better).
In Table 1, we notice thatrecall of hoffmann-ilp is lower in comparisonwith hoffmann?s algorithm.On the Riedel dataset, we observe thathoffmann-ilp has better precision (0.15 to0.2) than MIMLRE within recall of 0.1.At recall > 0.1, precision drops drastically.This is because, hoffmann-ilp predicts signif-icantly more nil labels.
However, nil labelsare not part of the label-set in the P/R curvesreported in the community.
In Table 2, we seethat hoffmann-ilp has higher precision (0.04)compared to Hoffmann?s algorithm.2.
Performance of noisy-or1940Table 2 : Highest F1 point in P/R curve : Riedel DatasetPrecision Recall F1Hoffmann 0.32054795 0.24049332 0.27480916MIMLRE 0.28061223 0.28645834 0.28350515Noisy-OR 0.317 0.18139774 0.23075178Hoffmann-ilp 0.36701337 0.12692702 0.18862161In Figure 1 we see that there is a big jumpin precision (around 0.4) of noisy-or com-pared to Hoffmann?s model in most parts ofthe curve on the KBP dataset.
However, inFigure 2 (Riedel dataset), we do not see sucha trend.
Although, we do perform better thanMIMLRE (Surdeanu et al., 2012) (precision> 0.15 for recall < 0.15).On both datasets, noisy-or has higher preci-sion than MIMLRE, as seen from Tables 1and 2.
However, the recall reduces.
More in-vestigation in this direction is part of futurework.5 ConclusionIn this paper we described an important addition toHoffmann?s model by the use of the noisy-or softconstraint to further relax the at least one assump-tion.
Since we posed the inference procedures inHoffmann using ILP, we could easily add this con-straint during the training and inference.Empirically, we showed that the resulting P/Rcurves have a significant performance boost overHoffmann?s algorithm as a result of this newlyadded constraint.
Although our system has a lowerrecall when compared to MIMLRE (Surdeanu etal., 2012), it performs competitively w.r.t the pre-cision at low recall.As part of immediate future work, we wouldlike to improve the system recall.
Our ILP for-mulation provides a good framework to add newtype of constraints to the problem.
In the future,we would like to experiment with other constraintslike modeling the selectional preferences of entitytypes.ReferencesMark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informa-tion from text sources.
In Proceedings of the Sev-enth International Conference on Intelligent Systemsfor Molecular Biology, pages 77?86.
AAAI Press.Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.2005.
Exploring various knowledge in relation ex-traction.
In Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics, ACL?05, pages 427?434, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling,Luke Zettlemoyer, and Daniel S. Weld.
2011.Knowledge-based weak supervision for informationextraction of overlapping relations.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies - Volume 1, HLT ?11, pages 541?550,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2 - Volume 2, ACL ?09, pages 1003?1011,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Sebastian Riedel, Limin Yao, and Andrew McCal-lum.
2010.
Modeling relations and their men-tions without labeled text.
In Proceedings of the2010 European conference on Machine learningand knowledge discovery in databases: Part III,ECML PKDD?10, pages 148?163, Berlin, Heidel-berg.
Springer-Verlag.Dan Roth and Wen tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
In In Proceedings of CoNLL-2004,pages 1?8.Sampath Srinivas.
2013.
A generalization of the noisy-or model.
CoRR, abs/1303.1479.Mihai Surdeanu and Massimiliano Ciaramita.
2007.Robust information extraction with perceptrons.
InProceedings of the NIST 2007 Automatic ContentExtraction Workshop (ACE07), March.Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-Closky, Angel X. Chang, Valentin I. Spitkovsky,and Christopher D. Manning.
2011.
Stanford?sdistantly-supervised slot-filling system.
In Proceed-ings of the Fourth Text Analysis Conference (TAC2011), Gaithersburg, Maryland, USA, November.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,and Christopher D. Manning.
2012.
Multi-instancemulti-label learning for relation extraction.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, EMNLP-CoNLL ?12, pages 455?465, Stroudsburg, PA, USA.Association for Computational Linguistics.1941
