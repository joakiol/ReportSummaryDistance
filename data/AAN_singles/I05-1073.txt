R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
838 ?
848, 2005.?
Springer-Verlag Berlin Heidelberg 2005Integrating Punctuation Rules and Na?ve BayesianModel for Chinese Creation Title RecognitionConrad Chen and Hsin-Hsi ChenDepartment of Computer Science and Information Engineering,National Taiwan University, Taipei, Taiwandrchen@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.twhttp://nlg.csie.ntu.edu.tw/Abstract.
Creation titles, i.e.
titles of literary and/or artistic works, compriseover 7% of named entities in Chinese documents.
They are the fourth large sortof named entities in Chinese other than personal names, location names, andorganization names.
However, they are rarely mentioned and studied before.Chinese title recognition is challenging for the following reasons.
There are fewinternal features and nearly no restrictions in the naming style of titles.
Theirlengths and structures are varied.
The worst of all, they are generally composedof common words, so that they look like common fragments of sentences.
Inthis paper, we integrate punctuation rules, lexicon, and na?ve Bayesian modelsto recognize creation titles in Chinese documents.
This pioneer study shows aprecision of 0.510 and a recall of 0.685 being achieved.
The promising resultscan be integrated into Chinese segmentation, used to retrieve relevant informa-tion for specific titles, and so on.1   IntroductionNamed entities are important constituents to identify roles, meanings, and relation-ships in natural language sentences.
However, named entities are productive, so that itis difficult to collect them in a lexicon exhaustively.
They are usually ?unknown?when we process natural language sentences.
Recognizing named entities in docu-ments is indispensable for many natural language applications such as informationretrieval [2], summarization [3], question answering [7], and so on.Identifying named entities is even harder in Chinese than in many Indo-Europeanlanguages like English.
In Chinese, there are no delimiters to mark word boundaries andno special features such as capitalizations to indicate proper nouns, which constitutehuge part of named entities.
In the past, various approaches [1, 4, 10] have been pro-posed to recognize Chinese named entities.
Most of them just focused on MUC-stylenamed entities [8], i.e., personal names, location names, and organization names.
Theextensive studies cover nearly 80% of named entities in real documents [1].
Althoughthe performance of such kinds of named entity recognizers is satisfiable, the rest 20% ofnamed entities are so far rarely mentioned and often ignored in previous studies.These rarely mentioned ones belong to various sorts, such as terminologies, aliasesand nicknames, brands, etc.
These sorts may not occur as frequently as personalnames or location names in a corpus, but the importance of the former in documentsIntegrating Punctuation Rules and Na?ve Bayesian Model 839of specific domains is no less than that of the latter.
For example, knowing names ofdishes would be very important to understand articles about cooking.
Among theserarely addressed named entities, titles of creations, such as book names, song titles,sculpture titles, etc., are one of the most important sorts.
According to Chen & Lee(2004)?s study [1] of Academia Sinica Balanced Corpus (abbreviated ASBC corpushereafter), about 7% of named entities are titles of creations.
In other words, morethan one-third of rarely mentioned named entities are titles of creations.Chinese title recognition is challenging for the following reasons.
There are nolimitations in length and structures of titles.
They might be a common word, e.g.
????
(Mistakes, a Chinese poem), a phrase, e.g.
???????
(Norwegian Wood, asong), a sentence, e.g.
??????????
(Don?t Cry for Me Argentina, a song),or even like nothing, e.g.
?????????
(Rub ?
Undescribable, a Chinese poetrycollection).
Besides, the choice of characters to name titles has no obvious prefer-ences.
Till now, few publications touch on Chinese title recognition.
There are evenno available corpora with titles being tagged.Several QA systems, such as Sekine and Nobata (2004) [9], used fixed patterns anddictionaries to recognize part of titles in English or Japanese.
Lee et al (2004) [6] pro-posed an iterative method that constructs patterns and dictionaries to recognize Englishtitles.
Their method cannot be adapted to Chinese, however, because the most importantfeature employed is capitalization, which does not exist in Chinese.In this paper, we propose a pioneer study of Chinese title recognition.
An approachof integrating punctuation rules, lexicon, and na?ve Bayesian models is employed torecognize creation titles in Chinese documents.
Section 2 discusses some cues for Chi-nese title recognition.
Section 3 gives a system overview.
Punctuation rules and titlegazetteer identify part of titles and filter out part of non-titles.
The rest of undeterminedcandidates are verified by na?ve Bayesian model.
Section 4 addresses which featuresmay be adopted in training na?ve Bayesian model.
Section 5 lists the training and testingmaterials, and shows experimental results.
Section 6 concludes there marks.2   Cues for Chinese Creation Title RecognitionTitles discussed in this paper cover a wide range of creations, including literature,music, painting, sculpture, dance, drama, movies, TV or radio programs, books,newspapers, magazines, research papers, albums, PC games, etc.
All of these titles aretreated as a single sort because they share the same characteristics, i.e., they arenamed by somebody with creativity, and thus there are nearly no regularity or limita-tions on their naming styles.The challenging issue is that, unlike MUC-style named entities (MUC7, 1998), ti-tles are usually composed of common words, and most of them have no internal fea-tures like surnames or entity affixes, e.g.
???
(City) in ?????
(Taipei City).
Inother words, most titles might look just like common strings in sentences.
Thus it iseven more difficult to decide which fragment of sentences might be a title than todetermine if some fragment is a title.For the lack of internal features, external features or context information must befound to decide boundaries of titles.
Table 1 shows some words preceding or follow-ing titles in one-tenth sampling of ASBC corpus with titles tagged manually.
We can840 C. Chen and H.-H. Chenobserve that quotation marks are widely used.
This is because writers usually quotetitles with punctuation marks to make them clear for readers.
The most common usedones are the two pairs of quotation marks ????
and ????.
About 40% of titles arequoted in ????
or ????
in our test corpus.
However, labeling proper nouns isonly one of their functions.
Quotation marks are extensively used in various pur-poses, like dialogues, emphasis, novel words, etc.
In our analysis, only less than 7%of strings quoted in ????
or ????
are creation titles.
It means the disambiguationof the usages of quotation marks is necessary.Table 1.
Preceding and Following Words of Titles in One-tenth Sampling of ASBC CorpusPrecedingWord FrequencyFollowingWord Frequency?
450     ?
442?
216     ?
216?
44     ?
56?
31     ?
32?
25     ?
26?
24     ?
16?
19     ?
13?
16     ?
11?
9     ?
7?
7     ?
6?
6     ?
6?
6     ?
6The most powerful external feature of creation titles is French quotes ???
?,which is defined to represent book names in the set of standard Simplified Chinesepunctuation marks of China [5].
However, they are not standard punctuation marks inTraditional Chinese.
Besides the usage of French quotes to mark book names, writersoften use them to label various types of creation titles.
According to our analysis onWeb searching and the sampling corpus, about 20% of occurrences of titles in Tradi-tional Chinese documents are quoted in ???
?, and nearly no strings other than titleswould be quoted in ????.
This punctuation mark shows a very powerful cue to dealwith title recognition.Nevertheless, there are still 40% of titles without any marks around.
These un-marked titles usually stand for widely known or classic creations.
In other words,these famous works are supposed to be mentioned in many documents many times.Such kinds of titles are extensively known by people like a common vocabulary.
Alexicon of famous creations should cover a large part of these common titles.3   System OverviewBased on the analyses in Section 2, we propose some punctuation rules that exploitthe external features of titles to recognize possible boundaries of titles in ChineseIntegrating Punctuation Rules and Na?ve Bayesian Model 841documents.
Most strings that cannot be titles are filtered by these rules.
Titles withstrong evidences like ??
?
?
are also identified by these rules.
The rest undecidedstrings are denoted as ?possible titles.?
To verify these candidates is somewhat similarto solve word sense disambiguation problem.
Na?ve Bayesian classifier is adopted totell whether a candidate is really a title or not.
The overview of our system is shownin Figure 1.Fig.
1.
System OverviewFig.
2.
Decision Tree of Punctuation Rules and LexiconFigure 2 shows the applications of the punctuation rules and the title lexicon,which are illustrated as a decision tree.
HR1 exploits French quotes ????
to identifytitles like ???????
(Taohua Shan, a traditional Chinese drama by Kong, Shang-Ren) and ??????????
(El General En Su Laberinto, a novel by GarciaMarquez).
HR2a and HR2b then look up the title lexicon to find famous titles like ??????
(Cien Anos de Soledad) and ??????
(Romance of Three Kingdoms).HR3 limits our recognition scope to strings quoted in quotation marks, and HR4 and842 C. Chen and H.-H. ChenHR5 filter out a major sort of non-titles quoted in quotation marks, dialogues, such as?????????????
(I said, ?the audience should be careful!?)
and ????????????????
(Rene Descarte said, ?I think, therefore I am.?
).The title lexicon we use is acquired from the catalogues of the library of our uni-versity.
These titles are sent to Google as query terms.
Only the ones that have everbeen quoted in ??
?
?
in the first 1,000 returned summaries are kept.
The remainedtitles are checked manually and those ones that possibly form a fragment of a com-mon sentence are dropped to avoid false alarms.
After filtering, there are about 7,200entries in this lexicon.
Although the lexicon could cover titles of books only, it is stilluseful because books are the major sort of creations.The punctuation rules and lexicon divide all the strings of a document into threegroups ?
say, titles, non-titles, and possible titles.
All strings that cannot be definitelyidentified by the punctuation rules and lexicon are marked as ?possible?
titles.
Thesepossible titles are then verified by the second mechanism, the na?ve Bayesian model.The na?ve Bayesian model will be specified in the next section.4   Na?ve Bayesian ModelNa?ve Bayesian classifier is widely used in various classification problems in naturallanguage processing.
Since it is simple to implement and easy to train, we adopt it inour system to verify the possible titles suggested by the decision tree.Na?ve Bayesian classification is based on the assumption that each feature beingobserved is independent of one another.
The goal is to find the hypothesis that wouldmaximize the posterior probability P(H|F), where H denotes the classifying hypothe-ses and F denotes the features that determine H. According to Bayesian rule, the pos-terior probability can be rewritten as:P(H | F) = P(H) P(F | H) / P(F) (1)Since P(F) is always the same under different hypotheses, we only need to find whichhypothesis would obtain the maximal value of P(H)P(F|H).
Besides, under the inde-pendence assumption, Equation (1) is rewritten into:P(H | F) = P(H) ?
P( fi | H)     where F = { f1, f2,?, fn } (2)In our system, we have two hypotheses:H1: candidate S is a titleH2: candidate S is not a titleFour features shown below will be considered.
The detail will be discussed in thesubsequent paragraphs.F1: ContextF2: ComponentF3: LengthF4: RecurrenceIntegrating Punctuation Rules and Na?ve Bayesian Model 843Context.
To exploit contextual features, our system adopts a word-based, position-free unigram context model with a window size of 5.
In other words, our contextmodel can be viewed as a combination of ten different contextual features of the na?veBayesian classifier, five of them are left context and the other five are right context.
Itcan be represented as:P(Fcontext|H) = P(L5, L4, L3, L2, L1, R1, R2, R3, R4, R5 | H) (3)Where Li and Ri denote preceding and following words of the possible title we want toverify, and H denotes the hypothesis.If we postulate that the contextual features are independent of each other, thenequation (3) can be transformed to:P(Fcontext|H) = ?
P(Li |H) ?
P(Ri | H) (4)Equation (4) assumes that the distance from a contextual word to a possible title isnot concerned both in training and testing.
The reason is that we do not have a realis-tic, vast, and well-tagged resource for training.
On the other hand, if we want to ex-ploit it in testing, we need a well-tagged corpus to learn the best weights we shouldassign to contextual words of different distances.Component.
Context deals with features surroundings titles.
In contrast, Componentfurther considers the features within titles.
Similar to the above discussion, ourcomponent model is also a word-based, position-free unigram model.
A possible titlewill be segmented into a word sequence by standard maximal matching.
The words inthe segmentation results are viewed as the ?components?
of the possible title, and thecomponent model can be represented as:P(Fcomp|H) = P(C1?Cn | H) = ?
P(Ci | H) (5)Where Ci denotes the component of the possible title we want to verify, and H de-notes the hypothesis.Similar to the context model, the position of a component word is not concernedboth in training and testing.
Besides the availability issue of large training corpus, thelengths of possible titles are varied so that positional information is difficult to beexploited.
Different titles consist of different number of component words.
There areno straightforward or intuitive ways of using positional information.Length.
The definition of Length feature is the number of characters that constitutethe possible title.
It can be represented as:P(Flength|H) = P(the length of S | H) (6)Where S denotes the possible title to be verified and H denotes the hypothesis that Sis a title.Recurrence.
The definition of Recurrence feature is number of occurrences of thepossible title in the input document.
It can be represented as:P(FRec|H) = P(the appearing times of S | H) (7)Where S denotes the possible title to be verified and H denotes the hypothesis that Sis a title.844 C. Chen and H.-H. Chen5   Experiment ResultsThe estimation of P(H) and P(F|H) is the major issue in na?ve Bayesian model.
Thereare no corpora with titles being tagged available.
To overcome this problem, we usedtwo different resources in our training process.
The first one is a collection of about300,000 titles, which is acquired from library catalogues of our university.
This col-lection is used to estimate Component and Length features of titles.
Besides, thesetitles are regarded as queries and submitted to Google.
The returned summaries aresegmented by maximal matching and then used to estimate Context features of titles.Since titles are usually composed of common words, not all query terms in retrievedresults by Google are a title.
Therefore, only the results with query terms quoted inFrench quotes ????
are adopted, which include totally 1,183,451 web page summa-ries.
Recall that French quotes are a powerful cue to recognize creation titles, whichwas discussed in Section 2.The second resource used in training is ASBC corpus.
Since titles in ASBC corpusare not specially tagged and we are short-handed to tag them by ourselves, a compro-mised approach is adopted.
First, the decision tree shown in Figure 2 is used to groupall strings of the training corpus into titles, non-titles, and possible titles.
All titles thusextracted are used to estimate the Recurrence feature of titles, and all possible titlesare treated as non-titles to estimate all features of non-titles.
Since the probability ofpossible titles being titles are much less than being non-titles, the bias of the roughestimation is supposed to be tolerable.We separate one-tenth of ASBC corpus and tag it manually as our testing data.
Therest nine-tenth is used for training.
There are totally 610,760 words in this piece ofdata, and 982 publication or creation titles are found.
During execution of our system,the testing data are segmented by maximal matching to obtain context and componentwords of possible titles.
To estimate P(H), we randomly select 100 possible titlesfrom the training part of ASBC corpus, and classify them into titles and non-titlesmanually.
Then we count the probability of hypotheses from this small sample toapproximate P(H).Table 2 shows the performance of the decision tree proposed in Figure 2 under thetesting data.
If we treat HR2a and HR2b as a single rule that asks ?Is the string anentry in the title lexicon and not in a general dictionary?
?, we could view our rules asan ordered sequence of decisions.
Each rule tells if a part of undecided strings aretitles or non-titles, which is denoted in the column ?Decision Type?
of Table 2.
Thecolumn ?Decided?
shows how many strings can be decided by the correspondingrules, while the columns of ?Undecided Titles?
and ?Undecided Non-Titles?
denotehow many titles and non-titles are remained in the testing data after applying the cor-responding rule.
The correctness of the decision is denoted in the columns of ?Cor-rect?
and ?Wrong?.Table 2 shows that these five rules are very good clues to recognize titles.
HR1,HR2, HR4 and HR5 have precisions of 100%, 94.01%, 99.15%, and 100% respec-tively.
Because the number of non-titles is much larger than that of titles, the actualprecision of HR3 is comparatively meaningless.
These rules could efficiently solve alarge part of the problem.
The rest possible titles are then classified by the na?veBayesian classifier.
The performance is listed in Table 3.
We try different combina-tions of the four features.
F1, F2, F3, and F4 denote Context, Component, Length,Integrating Punctuation Rules and Na?ve Bayesian Model 845and Recurrence, respectively.
The number of True Positives, True Negatives, andFalse Positives are listed.
Precision, recall and F-measure are considered as metrics toevaluate the performance.Table 2.
Performance of Decision Tree in Figure 2DecisionType Decided Correct WrongUndecidedTitlesUndecidedNon-TitlesHR1 Title 216 216 0 766 ~|corpus|2/2HR2 Title 167 126 411 640 ~|corpus|2/2HR3 Non-Title ~|corpus|2/2 ~|corpus|2/2 186 454 5812HR4 Non-Title 1997 1980 17 437 3832HR5 Non-Title 372 372 0 437 3458Note that there are two different numbers in the False Positive, Precision, and F-measure columns in Table 3.
The left number shows the total number of false positiveerrors, and the right one ignores the errors caused by other sorts of named entities.This is because many false positive errors come from other types of named entities.For example, in the sentence ??????????????????????????
(attend 1994 35th International Mathematical Olympiad), ??????????????????????
(?1994 35th International Mathematical Olympiad?)
isa contest name, however, ill-recognized as a title by our system.
Because there arevarious sorts of ill-recognized named entities and most of them have not been thor-oughly studied, there are no efficient ways available to solve these false alarms.
For-tunately, in many applications, there would be little harm incorrectly recognizingthese named entities as titles.The other major source of false positive errors is appearances of monosyllabicwords.
For example, in the sentence ?????????????
(?Render Good forEvil?
is Lao Tzu?s speech), ??????
(?Render Good for Evil?)
are ill-recognizedas titles.
The reason might be that many context and component words of titles arenamed entities or unknown words.
During training, these named entities are neithertagged nor recognized, so that most of these named entities are segmented into se-quences of monosyllabic words.
Therefore, while the na?ve Bayesian classifier en-counters monosyllabic context or component words, it would prefer recognizing thepossible title as a title.From Table 3, we could observe that Context and Component are supportive inboth precision and recall.
Length boosts precision but decreases recall while Recur-rence is on the contrary.
The combination of F1+F2+F3 obtains the best F-measure,but the combination of all features might be more useful in practical applications,1Total 31 of them can be easily corrected by a maximal-matching-driven segmentation.
Forexample, ????
(x?n j?ng, Heart Sutra, a Buddha book) in ??????
(y?ng x?n j?ng y?ng)is an entry in the title lexicon.
However, maximal matching prefers the segmentation of ???
/ ???
(y?ng x?n/j?ng y?ng) than ??
/ ??
/ ??
(y?ng/x?n j?ng/y?ng), so that this falsealarm would be recovered.846 C. Chen and H.-H. Chensince it only sacrifices 1.4% of precision but gains 3% of recall in comparison withthe former.
Table 4 summaries the total performance of our creation title recognitionsystem.
It achieves the F-measure of 0.585.Table 3.
Performance of the Na?ve Bayesian Classifier Using Different FeaturesTruePositiveTrueNegativeFalsePositive Precision Recall F-measureF1 277 160 959 / 772 0.224 / 0.264 0.634 0.331 / 0.373F2 153 284 532 / 332 0.223 / 0.315 0.350 0.273 / 0.332F1 + F3 273 164 859 / 676 0.241 / 0.288 0.625 0.348 / 0.394F2 + F3 148 289 453 / 247 0.246 / 0.375 0.339 0.285 / 0.356F1 + F2 288 149 976 / 722 0.228 / 0.285 0.659 0.339 / 0.398F1 + F4 289 148 1067 / 867 0.213 / 0.250 0.661 0.322 / 0.363F2 + F4 169 268 695 / 467 0.196 / 0.266 0.387 0.260 / 0.315F1 + F2 + F3 286 151 888 / 631 0.244 / 0.312 0.654 0.355 / 0.422F1 + F3 + F4 285 152 946 / 750 0.232 / 0.275 0.652 0.342 / 0.387F2 + F3 + F4 164 273 542 / 320 0.232 / 0.339 0.375 0.287 / 0.356All 299 138 967 / 703 0.236 / 0.298 0.684 0.351 / 0.416Table 4.
Performance of the Title Recognition SystemTruePositiveTrueNegativeFalsePositive Precision Recall F-measureDecision Tree 342 203   41 / 10 0.915 / 0.978 0.685 0.783 / 0.806Na?ve Bayes-ian 299 138  967 / 703 0.236 / 0.298 0.684 0.351 / 0.416Total 641 341 1008 / 713 0.424 / 0.510 0.685 0.524 / 0.5856   ConclusionThis paper presents a pioneer study of Chinese title recognition.
It achieves the preci-sion of 0.510 and the recall of 0.685.
The experiments reveal much valuable informa-tion and experiences for further researches.First, the punctuation rules proposed in this paper are useful to recognize creationtitles with a high precision.
They can relief our burdens in building more resources,make supervised learning feasible, and give us some clues in similar studies like rec-ognition of other sorts of named entities.
These useful rules are also helpful for thoseapplications needing high accuracies.
For example, we can exploit these rules on aninformation retrieval system to filter out noises and show only the information aboutthe requested creation or the publication.Second, na?ve Bayesian classifier could achieve a comparable recall on the verifi-cation of possible titles.
Since we only adopt simple features and use a rough estima-tion in feature model building, the result shows that na?ve Bayesian classifier isIntegrating Punctuation Rules and Na?ve Bayesian Model 847practicable in recognizing creation titles.
In future works, we may find other usefulfeatures and adopt more sophisticated models in na?ve Bayesian classifier to seek ahigher performance, especially in precision.Third, our result shows that recognizing rarely seen sorts of named entities ispracticable.
Because un-recognized named entities might significantly affect subse-quent applications in Chinese, in particular, segmentation, we should not ignore theproblems introduced by Non-MUC style named entities.
Our study suggests that therecognition of these rarely mentioned named entities is promising.
The perform-ances of many applications, such as natural language parsing and understanding,might be boosted through adding the mechanism of recognizing these rarenamed entities.Finally, our research can also be extended to other oriental languages, such asJapanese, in which there are no explicit features like specialized delimiters or capitali-zations to mark creation titles.
Just as Chinese, un-recognized named entities in theselanguages might affect the performances of natural language applications.
Recogniz-ing Non-MUC style named entities is an indispensable task to process theselanguages.AcknowledgementResearch of this paper was partially supported by National Science Council, Taiwan,under the contract NSC94-2752-E001-001-PAE.References1.
Chen, Conrad and Lee, Hsi-Jian.
2004.
A Three-Phase System for Chinese Named EntityRecognition, Proceedings of ROCLING XVI, 2004, 39-48.2.
Chen, Hsin-Hsi, Ding, Yung-Wei and Tsai, Shih-Chung.
1998.
Named Entity Extractionfor Information Retrieval, Computer Processing of Oriental Languages, Special Issue onInformation Retrieval on Oriental Languages, 12(1), 1998, 75-85.3.
Chen, Hsin-Hsi, Kuo, June-Jei, Huang, Sheng-Jie, Lin, Chuan-Jie and Wung, Hung-Chia.2003.
A Summarization System for Chinese News from Multiple Sources, Journal ofAmerican Society for Information Science and Technology, 54(13), November 2003,1224-1236.4.
Chen, Zheng, W. Y. Liu, and F. Zhang.
2002.
A New Statistical Approach to PersonalName Extraction, Proceedings of ICML 2002, 67-74.5.
Gong, Chian-Yian and Liu, Yi-Ling.
1996.
Use of Punctuation Mark.
GB/T15834-1995.http://202.205.177.129/moe-dept/yuxin-/content/gfbz/ managed/020.htm6.
Lee, Joo-Young, Song, Young-In, Kim, Sang-Bum, Chung, Hoojung and Rim, Hae-Chang.
2004.
Title Recognition Using Lexical Pattern and Entity Dictionary, Proceedingsof AIRS04, 342-348.7.
Lin, Chuan-Jie, Chen, Hsin-Hsi, Liu, Che-Chia, Tsai, Ching-Ho and Wung, Hung-Chia.2001.
Open Domain Question Answering on Heterogeneous Data, Proceedings of ACLWorkshop on Human Language Technology and Knowledge Management, July 6-7 2001,Toulouse France, 79-85.848 C. Chen and H.-H. Chen8.
MUC7.
1998.
Proceedings of 7th Message Understanding Conference, Fairfax, VA, 1998,http://www.itl.nist.gov/iaui/894.02/related_projects/muc/index.html.9.
Sakine, Satoshi and Nobata, Chikashi.
2004.
Definition, Dictionaries and Tagger for Ex-tended Named Entity Hierarchy, Proceedings of LREC04.10.
Sun, Jian, J. F. Gao, L. Zhang, M. Zhou, and C. N. Huang.
2002.
Chinese Named EntityIdentification Using Class-based Language Model, Proceedings of the 19th InternationalConference on Computational Linguistics, Taipei, 967-973
