Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 391?394,Dublin, Ireland, August 23-24, 2014.KUNLPLab:Sentiment Analysis on Twitter DataAbstractThis paper presents the system submittedby KUNLPLab for SemEval-2014 Task9- Subtask B: Message Polarity on Twitterdata.
Lexicon features and bag-of-wordsfeatures are mainly used to represent thedatasets.
We trained a logistic regressionclassifier and got an accuracy of 6% in-crease from the baseline feature represen-tation.
The effect of pre-processing on theclassifier?s accuracy is also discussed inthis work.1 IntroductionMicroblogging sites has become a commonway of reflecting peoples?
opinion.
Unlike theregular blogs, the size of a message on a mi-croblogging site is relatively small.
The need toautomatically detect and summarize the sentimentof messages from users on a given topic or prod-uct has gained the interest of researchers.The sentiment of a message can be negative,positive, or neutral.
In the broader sense, automat-ically detecting the polarity of a message wouldhelp business firms easily detect customers?
feed-back on their product or services.
Which in turnhelps them improve their decision making byproviding information of user preferences, prod-uct trend, and user categories.
(Chew and Eysen-bach, 2010; Salethe and Khandelwal,2011).
Sen-timent analysis is also used in other do-mains.
(Mandel et al.,2012).Twitter is one of the mostly widely used mi-croblogging web site with  over 200 million userssend over 400 million tweets daily(September2013).
A peculiar characteristic of a Twitter dataare as follow: emoticons are widely used, themaximum length of a tweet is 140 character, somewords are abbreviated, or some are elongated byrepeating letters of a word multiple times.The organizers of the SemEval-2014 has pro-vided a corpus of tweets and posted a task to au-tomatically detect their respective sentiments.Sub task B of Task 9: Sentiment Analysis onTwitter is describe as followsTask B - Message Polarity Classification?Given a message, classify whether the mes-sage is of positive, negative, or neutral sentiment.For messages conveying both a positive and neg-ative sentiment, whichever is the stronger senti-ment should be chosen.
?This paper describes the system submitted byKUNLBLab for participation in SemEval-2014Task 9 subtask B.
Models were trained using theLIBLINEAR classification library (Fan et al.,2008).
An accuracy of 66.11% is attained by theclassifier by testing on the development set.The remaining of the document is organized asfollows: Section 2 presents a brief literature re-view on sentiment analysis on Twitter data.
Sec-tion 3 discusses the system developed to solve theabove task, characteristics of the dataset, prepress-ing on the dataset, and various feature representa-tion.
Section 4 illustrates the evaluation results.Section 5 presents conclusion and remarks.2  Related WorkSentiment analysis has been studied in NaturalLanguage Processing.
Different approaches havebeen implemented to automatically detect senti-ment on texts (Pang et al., 2002; Pang and Lee,2004; Wiebe and Riloff, 2005; Glance et al., 2005;Wilson et al., 2005).There is also an active research on Sentimentanalysis on Twitter data.
(Go et al., 2009,Bermingham and Smeaton, 2010, and Pak andBeakal Gizachew AssefaKoc Unversitybassefa13@ku.edu.trThis work is licenced under a Creative Commons Attribu-tion 4.0 International License.
Page numbers and proceed-ings footer are added by the organizers.
License details:http://creativecommons.
org/licenses/by/4.0/391Paroubek 2010) consider tweets with good emot-icons as positive examples and tweets with bademoticons as negative examples for the trainingdata, and built a classifier using unigrams and bi-grams as features.Barbosa and Feng (2010) classified the subjec-tivity of tweets based on traditional features withthe inclusion of some witter specific clues such asretweets, hashtags, links, uppercase words, emot-icons, and exclamation and question marks.
(Agarwal et al.
2011 ) introduced a  POS-specific prior polarity features and  used  a treekernel to obviate the need for tedious featureengineering.3  System Description3.1 DatasetThe organizer of SemEval-2014 have providedtraining and development sets.
Table 1 bellow il-lustrates the characteristics of the dataset.Positive Negative NeutralTrain 3045 1,209 4004Dev 575 340 739Table 1.
Dataset characteristics3.2 Pre-processingWe employed two major pre-processing inthe datasets.
Converting terms to their correct rep-resentation, and stemming.Mostly, in Twitter, words are not written intheir correct/full form.
For instance, love,loooove,  looove convey the same meaning as theword love alone regardless of the extent of the em-phasis intended to describe.
Reducing this variousrepresentations of the same term to common wordhelps in better matching them even if they arewritten in different way.
This is more problematicif our features are based on term matching andhence increase the number of unknown terms.The second pre-processing we employed isstemming the terms in the dataset.
In most cases,morphological variants of words have similar se-mantic interpretations and can be considered asequivalent.
The advantage of stemming is two-fold.
Primarily it reduces the number of OOVs(Out Of Vocabulary) terms.
The second one isfeature reduction.3.3 FeaturesThere are two main categories of features usedin the development of this system.
Bag-of-Wordsand sentiment lexicon features.Bag-of-Words features takes a given input textand extracts the raw words as features independ-ent of one another.
One issue in using this featureis how to represent negations.
In the texts ?I likethe movie.
?, and ?I do not like the movie.
?, thesentiment of the words in the two texts is oppositesince the two statements are negations of one an-other.
One way of representing the negated wordis by appending the tag _NOT (Chen (2001) andPang et al.
(2002).
The _NOT tag suffixes allwords between the negation word and the firstpunctuation mark after the negation word.
In theabove example the second text is transformed to ?I do like_NOT  the_NOT movie _NOT?.
In repre-sentation of the negations, we employ the aboveapproach.
Lee Becker et al.
(2013) directly inte-grated the polarized word representation in theirsystem.
One disadvantage of this representationis the number of features doubles in worst case.Sentiment lexicons are words, which have as-sociation with positive or negative sentiments.Unlike the Bag-Of-Words, instead of taking theraw word as a feature, every word has a score,which is a measure of how much positive or neg-ative sentiment the lexicon has.
In this work weuse the NRC Hashtag Sentiment Lexicon, andSentiment140 Lexicon (Mohammad 2013).
Bothlist of lexicons are used in the SemEval 2013 byNRC-Canada team.The NRCHashtag Sentiment Lexicon is basedon the common practice that users use the # sym-bol to emphasis on a topic or a word.
The hashtaglexicon was created from a collection of tweetsthat had a positive or a negative word hashtagsuch as #good, #excellent, #bad, and #terrible(Mohammad 2012).
It  was created from 775,310tweets posted between April and December 2012using a list of 78 positive and negative wordhashtags.
They have provided unigram, bigram,and trigram datasest.
In this work however, weused the unigram features which contains 54,129terms.The Sentiment140  is also a list of words withassociations to positive an negative sentiments.
Ithas the same format as the NRC HashtagSentiment Lexicon.
However, it was created fromthe sentiment140 corpus of 1.6 million tweets, andemoticons were used as positive and negativelabels (instead of hashtagged words).In order to investigate  the effect of the featureslisted above, we have used various combination ofthem.
Table 2 shows 12 kinds of features used forthe system we have developed.The converted versions of the features are theones where the enlongated words are shortened to392their normal form and terms with less than 5occurances in the training set are ignored.Code FeaturesF1 RawBag-Of-WordF2 Bag-Of-WordStemmedF3 ConvertedStemedBag-Of-WordF4 HashtagF5 Sentiment140F6 CombinedLexiconsF7 ConvertedHashtagF8 ConvertedSentiment140F9 ConvertedNegatedHashtagF10 ConvertedNegatedSentiment140F11 ConvertedStemmeLexiconF12 AllCombinedTable 2.
Code of features and their namesThe description of the features is as follow, F1is a raw Bag-Of-Word features in which termswith more than five frequency are taken as fea-tures.
F2 takes the stem of the words whereas F3applies both stemmig and shortening of elongetedwords to the corpus then takes Bag-Of-Word fea-tures of the converted corpus.F4 and F5 are sentiment lexcon featureshashtag.
F6 is a combined Sentiment140, andHashtag features.
F7 and F8 are applications ofthe sentiment lexicons after applying shorteningand steming.
Negative message representation isincluded in features F9 and F10.
F11 is the com-bination of a preprocessed corpus by applicaitonof stemming and short represenation of elnogatedterms, negative message representation,  and ex-tracting a combined sentiword140 and hash tagfeatures.Feature F12 is the combination of all the fea-tures.
If a term after being preprocessed is foundin one of the lexicon features, the lexicon polaritymeasure is taken as feature value.Otherwise; weresort to the Bag-Of-Word feature.3.4 The classifierFor this task, we have used L2 regularized lo-gistic regression and used the LIBLINEAR imple-mentation (Rong-En Fan et al.
).To estimate thehyper parameters, we applied a 10 fold cross val-idation on the training set.
Liblinear implementa-tion of a L2 regularized logistic regression takes asingle cost C parameter.
The value of the cost Cparameter decides the weight between the L1 reg-ularization term and L2 regularization term.
If thevalue of C is less than one, it means the moreweight it given to the L1 regularization term.
Onthe other hand C values more than one gives moreweight to the L2 regularizing term.
The cost pa-rameter C=1 gives the best result on the cross val-idation test.
The same value is used to train ourmodel.4 Evaluation ResultsAs described in Table 2 of section 3.3, the ma-jor features used in this work are bag-of-word andsentiment lexicon features.
In addition to the fea-ture representation, pre-processing has been doneon the datasets.F1 is a baseline feature (raw Bag-Of-Word),with a total accuracy of 60.16.
Simply convertingthe elongated terms to their normal form and ap-plying stemming on the corpus increase the accu-racy from 60.16 to 64.92 (4.76%).Positive Negative Neutral TotalF1 61.71 52.48 60.55 60.16F2` 61.71 51.43 61.18 60.36F3 67.64 62.86 63.64 64.92F4 66.67 52.94 60.10 61.65F5 67.91 54.72 61.00 62.54F6 64.86 55.24 61.47 61.94F7 67.72 60.42 63.07 63.51F8 70.29 58.93 63.02 64.17F9 70.27 56.12 62.28 63.36F10 71.73 59.29 62.86 64.65F11 67.25 62.89 63.14 64.52F12 71.12 61.4 64.13 66.11Table 3.
Results of the evaluation on the devel-opment setF6 (the combined lexicon feature- senti-word140 and hashtag) yields an accuracy of61.94.
Applying conversion, negative representa-tion and stemming raises the accuracy to 64.52(F11)Testset MacroF1LiveJournal2014 63.77SMS2013 55.89Twitter2013 58.12Twitter2014 61.72Twitter2014Sarcasm 44.60Table 4.
Evaluation result on test set393The accuracy of identifying negative sentimentis the least in all features.
This shows that we needa better representation of negated messages.A test dataset was also provided by the organ-izer of semEval-2014.
Table 4 show the accuracyof the KUNPLab classifier.Our model has performed poorly on the Twit-ter2014Sarcasm  test set (44.60%).
The perfor-mance of our classifier on  LiveJournal2014 issimilar to the development set test performance.5 ConclusionThe performance of a classifier depends on fea-ture representation, hyperparameter optimizationand regularization.
In this work, we mainly usedbag-of-word features and sentiment lexicon fea-tures.
We trained a L2 regularized logistic regres-sion model.
Two major features are used to repre-sent the datasets; Bag-of-Word features and Lex-ical features.
It has been shown that stemming theterms increases accuracy of the classifier in eithercase.
The accuracy of the classifier on develop-ment set and training set is reported and hasshown an increase of 6% in accuracy form thebaseline with 95% confidence interval..The eval-uation of our system on SemEval-2014 test data isalso shown with an F measure of 44.60 to 63.77%.6 AcknowledgementI would like to acknowledge Ass.Prof.
Dr.Deniz YURET for his advice, guidance, encour-agement and inspiration to participate inSemEval-2014.
I also like to thank MohammadKhuram SALEEM, and Mohamad IRFAN  forproof reading this document.ReferenceCynthia Chew and Gunther Eysenbach.
2010.
Pandem-ics in the Age of Twitter: Content Analysis ofTweets during the 2009 H1N1 Outbreak.
PLoSONE, 5(11):e14118+, November.Marcel Salath?e and Shashank Khandelwal.
2011.
As-sessing vaccination sentiments with online socialmedia: Implications for infectious disease dynamicsand control.
PLoS Computational Biology, 7(10).Benjamin Mandel, Aron Culotta, John Boulahanis,Danielle Stark, Bonnie Lewis, and Jeremy Ro-drigue.
2012.
A demographic analysis of online sen-timent during hurricane irene.
In Proceedngs of theSecond Workshop on Language in Social Media,LSM?12, pages 27?36, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Sanjiv Das and Mike Chen.
2001.
Yahoo!
for amazon:extracting market sentiment from stock messageboards.
In Proceedings of the 8th Asia Pacific Fi-nance Association Annual Conference.Lee Becker, George Erhart, David Skiba and ValentineMatula.
2013.
AVAYA: Sentiment Analysis onTwitter with Self-Training and Polarity Lexicon Ex-pansion.
Seventh International Workshop on Se-mantic Evaluation (SemEval 2013)Saif Mohammad.
2012.
Emotional Tweets.
In Proceed-ings of the First Joint Conference on Lexical andComputational Semantics (*SEM), pages 246?255,Montr?eal, Canada.
Association for ComputationalLinguistics.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment Classification UsingMachine Learning Techniques.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2002)Mohammad, Saif and Kiritchenko, Svetlana and Zhu,Xiaodan 2013.
NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of TweetsProceedings of the seventh international workshopon Semantic Evaluation Exercises (SemEval-2013).Go, A., Bhayani, R., Huang, L.: Twitter sentiment clas-sification using distant supervision.
CS224N ProjectReport, Stanford (2009)Barbosa, L., Feng, J.: Robust sentiment detection onTwitter from biased and noisy data.
In: Proceedingsof COLING.
pp.
36?44 (2010)Agarwal, A., Xie, B., Vovsha, I., Rambow, O., Pas-sonneau, R.: Sentiment analysis of Twitter data.
In:Proc.
ACL 2011 Workshop on Languages in SocialMedia.
pp.
30?38 (2011)Adam Bermingham and Alan Smeaton.
2010.
Classi-fying sentiment in microblogs: is brevity an ad-vantage is brevity an advantage?
ACM, pages 1833?1836.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa corpus for sentiment analysis and opinion mining.Proceedings of LREC.Glance, N., M. Hurst, K. Nigam, M. Siegler, R. Stockton, and T. Tomokiyo.
2005.
Deriving marketing in-telligence from online discussion.
In Proceedings ofthe eleventh ACM SIGKDD, pages 419?428.
ACM.Wiebe, J. and E. Riloff.
2005.
Creating subjective andobjective sentence classifiers from unannotatedtexts.
Computational Linguistics and IntelligentText Processing, pages 486?497.R.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
LIBLINEAR: A Library for Large LinearClassification, Journal of Machine Learning Re-search 9(2008), 1871-1874394
