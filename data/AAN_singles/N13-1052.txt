Proceedings of NAACL-HLT 2013, pages 487?496,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsApproximate PCFG Parsing Using Tensor DecompositionShay B. CohenDepartment of Computer ScienceColumbia University, USAscohen@cs.columbia.eduGiorgio SattaDepartment of Information EngineeringUniversity of Padua, Italysatta@dei.unipd.itMichael CollinsDepartment of Computer ScienceColumbia University, USAmcollins@cs.columbia.eduAbstractWe provide an approximation algorithm forPCFG parsing, which asymptotically im-proves time complexity with respect to the in-put grammar size, and prove upper bounds onthe approximation quality.
We test our algo-rithm on two treebanks, and get significant im-provements in parsing speed.1 IntroductionThe problem of speeding-up parsing algorithmsbased on probabilistic context-free grammars(PCFGs) has received considerable attention inrecent years.
Several strategies have been proposed,including beam-search, best-first and A?.
In thispaper we focus on the standard approach of approx-imating the source PCFG in such a way that parsingaccuracy is traded for efficiency.Nederhof (2000) gives a thorough presentationof old and novel ideas for approximating non-probabilistic CFGs by means of finite automata,on the basis of specialized preprocessing of self-embedding structures.
In the probabilistic domain,approximation by means of regular grammars is alsoexploited by Eisner and Smith (2005), who filterlong-distance dependencies on-the-fly.Beyond finite automata approximation, Charniaket al(2006) propose a coarse-to-fine approach inwhich an approximated (not necessarily regular)PCFG is used to construct a parse forest for the in-put sentence.
Some statistical parameters are thencomputed on such a structure, and exploited to filterparsing with the non-approximated grammar.
Theapproach can also be iterated at several levels.
Inthe non-probabilistic setting, a similar filtering ap-proach was also proposed by Boullier (2003), called?guided parsing.
?In this paper we rely on an algebraic formulationof the inside-outside algorithm for PCFGs, based ona tensor formulation developed for latent-variablePCFGs in Cohen et al(2012).
We combine themethod with known techniques for tensor decompo-sition to approximate the source PCFG, and developa novel algorithm for approximate PCFG parsing.We obtain improved time upper bounds with respectto the input grammar size for PCFG parsing, andprovide error upper bounds on the PCFG approxi-mation, in contrast with existing heuristic methods.2 PreliminariesThis section introduces the special representation forprobabilistic context-free grammars that we adopt inthis paper, along with the decoding algorithm thatwe investigate.
For an integer i ?
1, we let [i] ={1, 2, .
.
.
, i}.2.1 Probabilistic Context-Free GrammarsWe consider context-free grammars (CFGs) inChomsky normal form, and denote them as(N ,L,R) where:?
N is the finite set of nonterminal symbols, withm = |N |, and L is the finite set of words (lexi-cal tokens), with L?N = ?
and with n = |L|.?
R is a set of rules having the form a?
b c,a, b, c ?
N , or the form a?
x, a ?
N andx ?
L.A probabilistic CFG (PCFG) is a CFG associatedwith a set of parameters defined as follows:?
For each (a?
b c) ?
R, we have a parameterp(a?
b c | a).487?
For each (a?
x) ?
R, we have a parameterp(a?
x | a).?
For each a ?
N , we have a parameter pia,which is the probability of a being the rootsymbol of a derivation.The parameters above satisfy the following nor-malization conditions:?
(a?b c)?Rp(a?
b c | a) +?(a?x)?Rp(a?
x | a) = 1,for each a ?
N , and?a?N pia = 1.The probability of a tree ?
deriving a sentence inthe language, written p(?
), is calculated as the prod-uct of the probabilities of all rule occurrences in ?
,times the parameter pia where a is the symbol at theroot of ?
.2.2 Tensor Form of PCFGsA three-dimensional tensor C ?
R(m?m?m) is aset of m3 parameters Ci,j,k for i, j, k ?
[m].
In whatfollows, we associate with each tensor three func-tions, each mapping a pair of vectors in Rm into avector in Rm.Definition 1 Let C ?
R(m?m?m) be a tensor.Given two vectors y1, y2 ?
Rm, we let C(y1, y2)be them-dimensional row vector with components:[C(y1, y2)]i =?j?[m],k?
[m]Ci,j,ky1j y2k .We also let C(1,2)(y1, y2) be them-dimensional col-umn vector with components:[C(1,2)(y1, y2)]k =?i?[m],j?
[m]Ci,j,ky1i y2j .Finally, we let C(1,3)(y1, y2) be the m-dimensionalcolumn vector with components:[C(1,3)(y1, y2)]j =?i?[m],k?
[m]Ci,j,ky1i y2k .For two vectors x, y ?
Rm we denote by x y ?Rm the Hadamard product of x and y, i.e., [xy]i =xiyi.
Finally, for vectors x, y, z ?
Rm, xy>z> is thetensor D ?
Rm?m?m where Di,j,k = xiyjzk (thisis analogous to the outer product: [xy>]i,j = xiyj).We extend the parameter set of our PCFG suchthat p(a?
b c | a) = 0 for all a?
b c not in R,and p(a?
x | a) = 0 for all a?
x not in R. Wealso represent each a ?
N by a unique index in [m],and we represent each x ?
L by a unique index in[n]: it will always be clear from the context whetherthese indices refer to a nonterminal inN or else to aword in L.In this paper we assume a tensor representationfor the parameters p(a?
b c | a), and we denote byT ?
Rm?m?m a tensor such that:Ta,b,c , p(a?
b c | a).Similarly, we denote by Q ?
Rm?n a matrix suchthat:Qa,x , p(a?
x | a).The root probabilities are denoted using a vector pi ?Rm?1 such that pia is defined as before.2.3 Minimum Bayes-Risk DecodingLet z = x1 ?
?
?xN be some input sentence; we writeT (z) to denote the set of all possible trees for z. Itis often the case that parsing aims to find the high-est scoring tree ??
for z according to the underlyingPCFG, also called the ?Viterbi parse:???
= argmax?
?T (z)p(?
)Goodman (1996) noted that Viterbi parsers do notoptimize the same metric that is usually used forparsing evaluation (Black et al 1991).
He sug-gested an alternative algorithm, which he called the?Labelled Recall Algorithm,?
which aims to fix thisissue.Goodman?s algorithm has two phases.
In the firstphase it computes, for each a ?
N and for each sub-string xi ?
?
?xj of z, the marginal ?
(a, i, j) definedas:?
(a, i, j) =??
?T (z) : (a,i,j)??p(?
).Here we write (a, i, j) ?
?
if nonterminal a spanswords xi ?
?
?xj in the parse tree ?
.488Inputs: Sentence x1 ?
?
?xN , PCFG (N ,L,R), pa-rameters T ?
R(m?m?m), Q ?
R(m?n), pi ?R(m?1).Data structures:?
Each ?
(a, i, j) ?
R for a ?
N , i, j ?
[N ],i ?
j, is a marginal probability.?
Each ?i,j ?
R for i, j ?
[N ], i ?
j, is the high-est score for a tree spanning substring xi ?
?
?xj .Algorithm:(Marginals) ?a ?
N ,?i, j ?
[N ], i ?
j, computethe marginals ?
(a, i, j) using the inside-outsidealgorithm.
(Base case) ?i ?
[N ],?i,i = max(a?xi)?R?
(a, i, i)(Maximize Labelled Recall) ?i, j ?
[N ], i < j,?i,j = maxa?N?
(a, i, j) + maxi?k<j(?i,k + ?k+1,j)Figure 1: The labelled recall algorithm from Goodman(1996).
The algorithm in this figure finds the highestscore for a tree which maximizes labelled recall.
The ac-tual parsing algorithm would use backtrack pointers inthe score computation to return a tree.
These are omittedfor simplicity.The second phase includes a dynamic program-ming algorithm which finds the tree ??
that maxi-mizes the sum over marginals in that tree:??
= argmax?
?T (z)?(a,i,j)???
(a, i, j).Goodman?s algorithm is described in Figure 1.As Goodman notes, the complexity of the secondphase (?Maximize Labelled Recall,?
which is alsoreferred to as ?minimum Bayes risk decoding?)
isO(N3 +mN2).
There are two nested outer loops,each of order N , and inside these, there are two sep-arate loops, one of order m and one of order N ,yielding this computational complexity.
The reasonfor the linear dependence on the number of nonter-minals is the lack of dependence on the actual gram-mar rules, once the marginals are computed.In its original form, Goodman?s algorithm doesnot enforce that the output parse trees are included inthe tree language of the PCFG, that is, certain com-binations of children and parent nonterminals mayviolate the rules in the grammar.
In our experimentswe departed from this, and changed Goodman?s al-gorithm by incorporating the grammar into the dy-namic programming algorithm in Figure 1.
The rea-son this is important for our experiments is that webinarize the grammar prior to parsing, and we needto enforce the links between the split nonterminals(in the binarized grammar) that refer to the samesyntactic category.
See Matsuzaki et al(2005) formore details about the binarization scheme we used.This step changes the dynamic programming equa-tion of Goodman to be linear in the size of the gram-mar (figure 1).
However, empirically, it is the inside-outside algorithm which takes most of the time tocompute with Goodman?s algorithm.
In this paperwe aim to asymptotically reduce the time complex-ity of the calculation of the inside-outside probabili-ties using an approximation algorithm.3 Tensor Formulation of theInside-Outside AlgorithmAt the core of our approach lies the observation thatthere is a (multi)linear algebraic formulation of theinside-outside algorithm.
It can be represented as aseries of tensor, matrix and vector products.
A sim-ilar observation has been made for latent-variablePCFGs (Cohen et al 2012) and hidden Markovmodels, where only matrix multiplication is required(Jaeger, 2000).
Cohen and Collins (2012) use thisobservation together with tensor decomposition toimprove the speed of latent-variable PCFG parsing.The representation of the inside-outside algorithmin tensor form is given in Figure 2.
For example,if we consider the recursive equation for the insideprobabilities (where ?i,j is a vector varying over thenonterminals in the grammar, describing the insideprobability for each nonterminal spanning words ito j):?i,j =j?1?k=iT (?i,k, ?k+1,j)489Inputs: Sentence x1 ?
?
?xN , PCFG (N ,L,R), pa-rameters T ?
R(m?m?m), Q ?
R(m?n), pi ?R(m?1).Data structures:?
Each ?i,j ?
R1?m, i, j ?
[N ], i ?
j, is a rowvector of inside terms ranging over a ?
N .?
Each ?i,j ?
Rm?1, i, j ?
[N ], i ?
j, is acolumn vector of outside terms ranging overa ?
N .?
Each ?
(a, i, j) ?
R for a ?
N , i, j ?
[N ],i ?
j, is a marginal probability.Algorithm:(Inside base case) ?i ?
[N ], ?(a?
xi) ?
R,[?i,i]a = Qa,x(Inside recursion) ?i, j ?
[N ], i < j,?i,j =j?1?k=iT (?i,k, ?k+1,j)(Outside base case) ?a ?
N ,[?1,N ]a = pia(Outside recursion) ?i, j ?
[N ], i ?
j,?i,j =i?1?k=1T(1,2)(?k,j , ?k,i?1)+N?k=j+1T(1,3)(?i,k, ?j+1,k)(Marginals) ?a ?
N ,?i, j ?
[N ], i ?
j,?
(a, i, j) = [?i,j ]a ?
[?i,j ]aFigure 2: The tensor form of the inside-outside algorithm,for calculation of marginal terms ?
(a, i, j).and then apply the tensor product from Definition 1to this equation, we get that coordinate a in ?i,j isdefined recursively as follows:[?i,j ]a =j?1?k=i?b,cTa,b,c ?
?i,kb ?
?k+1,jc=j?1?k=i?b,cp(a?
b c | a)?
?i,kb ?
?k+1,jc ,which is exactly the recursive definition of the insidealgorithm.
The correctness of the outside recursiveequations follows very similarly.The time complexity of the algorithm in this caseis O(m3N3).
To see this, observe that each tensorapplication takes timeO(m3).
Furthermore, the ten-sor T is applied O(N) times in the computation ofeach vector ?i,j and ?i,j .
Finally, we need to com-pute a total ofO(N2) inside and outside vectors, onefor each substring of the input sentence.4 Tensor Decomposition for theInside-Outside AlgorithmIn this section, we detail our approach to approxi-mate parsing using tensor decomposition.4.1 Tensor DecompositionIn the formulation of the inside-outside algorithmbased on tensor T , each vector ?i,j and ?i,j consistsof m elements, where computation of each elementrequires timeO(m2).
Therefore, the algorithm has aO(m3) multiplicative factor in its time complexity,which we aim to reduce by means of an approximatealgorithm.Our approximate method relies on a simple ob-servation.
Given an integer r ?
1, assume thatthe tensor T has the following special form, called?Kruskal form:?T =r?i=1?iuiv>i w>i .
(1)In words, T is the sum of r tensors, where eachtensor is obtained as the product of three vectorsui, vi and wi, together with a scalar ?i.
ExactKruskal decomposition of a tensor is not necessarilyunique.
See Kolda and Bader (2009) for discussionof uniqueness of tensor decomposition.490Consider now two vectors y1, y2 ?
Rm, associ-ated with the inside probabilities for the left (y1) andright child (y2) of a given node in a parse tree.
Letus introduce auxiliary arrays U, V,W ?
Rr?m, withthe i-th row being ui, vi and wi, respectively.
Letalso ?
= (?1, .
.
.
, ?r).
Using the decomposition inEq.
(1) within Definition 1 we can express the arrayT (y1, y2) as:T (y1, y2) =[r?i=1?iuiv>i w>i](y1, y2) =r?i=1?iui(v>i y1)(w>i y2) =(U>(?
V y1 Wy2)).
(2)The total complexity of the computation in Eq.
(2)is nowO(rm).
It is well-known that an exact tensordecomposition for T can be achieved with r = m2(Kruskal, 1989).
In this case, there is no computa-tional gain in using Eq.
(2) for the inside calculation.The minimal r required for an exact tensor decom-position can be smaller than m2.
However, identify-ing that minimal r is NP-hard (H?astad, 1990).In this section we focused on the computa-tion of the inside probabilities through vectorsT (?i,k, ?k+1,j).
Nonetheless, the steps above canbe easily adapted for the computation of the outsideprobabilities through vectors T(1,2)(?k,j , ?k,i?1)and T(1,3)(?i,k, ?j+1,k).4.2 Approximate Tensor DecompositionThe PCFG tensor T will not necessarily have the ex-act decomposed form in Eq.
(1).
We suggest to ap-proximate the tensor T by finding the closest tensoraccording to some norm over Rm?m?m.An example of such an approximate decom-position is the canonical polyadic decomposition(CPD), also known as CANDECOMP/PARAFACdecomposition (Carroll and Chang, 1970; Harsh-man, 1970; Kolda and Bader, 2009).
Given an in-teger r, least squares CPD aims to find the nearesttensor in Kruskal form, minimizing squared error.More formally, for a given tensor D ?
Rm?m?m,let ||D||F =??i,j,kD2i,j,k.
Let the set of tensors inKruskal form Cr be:Cr ={C ?
Rm?m?m | C =r?i=1?iuiv>i w>is.t.
?i ?
R, ui, vi, wi ?
Rm,||ui||2 = ||vi||2 = ||wi||2 = 1}.The least squares CPD of C is a tensor C?
suchthat C?
?
argminC?
?Cr ||C ?
C?||F .
Here, we treatthe argmin as a set because there could be multiplesolutions which achieve the same accuracy.There are various algorithms to perform CPD,such as alternating least squares, direct linear de-composition, alternating trilinear decomposition andpseudo alternating least squares (Faber et al 2003)and even algorithms designed for sparse tensors (Chiand Kolda, 2011).
Most of these algorithms treatthe problem of identifying the approximate tensor asan optimization problem.
Generally speaking, theseoptimization problems are hard to solve, but theywork quite well in practice.4.3 Parsing with Decomposed TensorsEquipped with the notion of tensor decomposition,we can now proceed with approximate tensor pars-ing in two steps.
The first is approximating the ten-sor using a CPD algorithm, and the second is apply-ing the algorithms in Figure 1 and Figure 2 to doparsing, while substituting all tensor product com-putations with the approximate O(rm) operation oftensor product.This is not sufficient to get a significant speed-upin parsing time.
Re-visiting Eq.
(2) shows that thereare additional ways to speed-up the tensor applica-tion T in the context of the inside-outside algorithm.The first thing to note is that the projections V y1and Wy2 in Eq.
(2) can be cached, and do not haveto be re-calculated every time the tensor is applied.Here, y1 and y2 will always refer to an outside oran inside probability vector over the nonterminals inthe grammar.
Caching these projections means thatafter each computation of an inside or outside proba-bility, we can immediately project it to the necessaryr-dimensional space, and then re-use this computa-tion in subsequent application of the tensor.The second thing to note is that the U projectionin T can be delayed, because of rule of distributiv-ity.
For example, the step in Figure 2 that computes491the inside probability ?i,j can be re-formulated asfollows (assuming an exact decomposition of T ):?i,j =j?1?k=iT (?i,k, ?k+1,j)=j?1?k=1U>(?
V ?i,k W?k+1,j)= U>(j?1?k=1(?
V ?i,k W?k+1,j)).
(3)This means that projection through U can be doneoutside of the loop over splitting points in the sen-tence.
Similar reliance on distributivity can be usedto speed-up the outside calculations as well.The caching speed-up and the delayed projectionspeed-up make the approximate inside-outside com-putation asymptotically faster.
While na?
?ve applica-tion of the tensor yields an inside algorithm whichruns in time O(rmN3), the improved algorithmruns in time O(rN3 + rmN2).5 Quality of Approximate Tensor ParsingIn this section, we give the main approximation re-sult, that shows that the probability distribution in-duced by the approximate tensor is close to the orig-inal probability distribution, if the distance betweenthe approximate tensor and the rule probabilities isnot too large.Denote by T (N) the set of trees in the tree lan-guage of the PCFG with N words (any nontermi-nal can be the root of the tree).
Let T (N) be theset of pairs of trees ?
= (?1, ?2) such that the to-tal number of binary rules combined in ?1 and ?2 isN ?
2 (this means that the total number of wordscombined is N ).
Let T?
be the approximate ten-sor for T .
Denote the probability distribution in-duced by T?
by p?.1 Define the vector ?(?)
such that[?(?
)]a = Ta,b,c ?
p(?1 | b) ?
p(?2 | c) where the root?1 is nonterminal b and the root of ?2 is c. Similarly,define [??(?
)]a = T?a,b,c ?
p?
(?1 | b) ?
p?
(?2 | c).Define Z(a,N) =??
?T (N)[??(?)]a.
In addition,define D(a,N) =??
?T (N) |[??(?
)]a ?
[?(?
)]a|1Here, p?
does not have to be a distribution, because T?
couldhave negative values, in principle, and its slices do not have tonormalize to 1.
However, we just treat p?
as a function that mapstrees to products of values according to T?
.and define F (a,N) = D(a,N)/Z(a,N).
De-fine ?
= ||T?
?
T ||F .
Last, define ?
=min(a?b c)?R p(a?
b c | a).
Then, the followinglemma holds:Lemma 1 For any a and any N , it holds:D(a,N) ?
Z(a,N)((1 + ?/?
)N ?
1)Proof sketch: The proof is by induction on N .Assuming that 1 + F (b, k) ?
(1 + ?/?
)k and1 + F (c,N ?
k ?
1) ?
(1 + ?/?
)N?k?1 for Fdefined as above (this is the induction hypothesis), itcan be shown that the lemma holds.
Lemma 2 The following holds for any N :??
?T (N)|p?(?)?
p(?
)| ?
m((1 + ?/?
)N ?
1)Proof sketch: Using Ho?lder?s inequality andLemma 1 and the fact that Z(a,N) ?
1, it followsthat:??
?T (N)|p?(?)?
p(?
)| ???
?T (N),a|[?(?
)]a ?
[??(?)]a|?
(?aZ(a,N))((1 + ?/?
)N ?
1)?
m((1 + ?/?
)N ?
1)Then, the following is a result that explains howaccuracy changes as a function of the quality of thetensor approximation:Theorem 1 For anyN , and  < 1/4, it holds that if?
??2Nm, then:??
?T (N)|p?(?)?
p(?
)| ?
Proof sketch: This is the result of applying Lemma 2together with the inequality (1 + y/t)t?
1 ?
2y forany t > 0 and y ?
1/2.
492We note that Theorem 1 also implicitly boundsthe difference between a marginal ?
(a, i, j) and itsapproximate version.
A marginal corresponds to asum over a subset of summands in Eq.
(1).A question that remains at this point is to decidewhether for a given grammar, the optimal ?
that canbe achieved is large or small.
We define:?
?r = minT?
?Cr||T ?
T?
||F (4)The following theorem gives an upper bound onthe value of ?
?r based on intrinsic property of thegrammar, or more specifically T .
It relies on thefact that for three-dimensional tensors, where eachdimension is of length m, there exists an exact de-composition of T using m2 components.Theorem 2 Let:T =m2?i=1?
?iu?i (v?i )>(w?i )>be an exact Kruskal decomposition of T such that||u?i ||2 = ||v?i ||2 = ||w?i || = 1 and ?
?i ?
?
?i+1 fori ?
[m2 ?
1].
Then, for a given r, it holds:?
?r ?m2?i=r+1|?
?i |Proof: Let T?
be a tensor that achieves the minimumin Eq.
(4).
Define:T ?r =r?i=1?
?iu?i (v?i )>(w?i )>Then, noting that ?
?r is a minimizer of the normdifference between T and T?
and then applying thetriangle inequality and then Cauchy-Schwartz in-equality leads to the following chain of inequalities:?
?r = ||T ?
T?
||F ?
||T ?
T?r||F= ||m2?i=r+1?
?iu?i (v?i )>(w?i )>||F?m2?i=r+1|?
?i | ?
||u?i (v?i )>(w?i )>||F =m2?i=r+1|?
?i |as required.
6 ExperimentsIn this section, we describe experiments that demon-strate the trade-off between the accuracy of the ten-sor approximation (and as a consequence, the accu-racy of the approximate parsing algorithm) and pars-ing time.Experimental Setting We compare the tensor ap-proximation parsing algorithm versus the vanillaGoodman algorithm.
Both algorithms were imple-mented in Java, and the code for both is almost iden-tical, except for the set of instructions which com-putes the dynamic programming equation for prop-agating the beliefs up in the tree.
This makes theclocktime comparison reliable for drawing conclu-sions about the speed of the algorithms.
Our im-plementation of the vanilla parsing algorithm is lin-ear in the size of the grammar (and not cubic in thenumber of nonterminals, which would give a worserunning time).In our experiments, we use the method describedin Chi and Kolda (2011) for tensor decomposition.2This method is fast, even for large tensors, as longas they are sparse.
Such is the case with the tensorsfor our grammars.We use two treebanks for our comparison: thePenn treebank (Marcus et al 1993) and the Arabictreebank (Maamouri et al 2004).
With the Penntreebank, we use sections 2?21 for training a max-imum likelihood model and section 22 for parsing,while for the Arabic treebank we divide the data intotwo sets, of size 80% and 20%, one is used for train-ing a maximum likelihood model and the other isused for parsing.The number of binary rules in the treebank gram-mar is 7,240.
The number of nonterminals is 112and the number of preterminals is 2593Unary rulesare removed by collapsing non-terminal chains.
Thisincreased the number of preterminals.
The numberof binary rules in the Arabic treebank is significantlysmaller and consists of 232 rules.
We run all parsingexperiments on sentences of length ?
40.
The num-ber of nonterminals is 48 and the number of preter-2We use the implementation given in Sandia?s Mat-lab Tensor Toolbox, which can be downloaded at http://www.sandia.gov/?tgkolda/TensorToolbox/index-2.5.html.3.493rank (r) baseline 20 60 100 140 180 220 260 300 340Arabic speed 0.57 0.04 0.06 0.1 0.12 0.16 0.19 0.22 0.26 0.28F1 63.78 51.80 58.39 63.63 63.77 63.88 63.82 63.84 63.80 63.88English speed 3.89 0.15 0.21 0.30 0.37 0.44 0.52 0.60 0.70 0.79F1 71.07 57.83 61.67 68.28 69.63 70.30 70.82 71.42 71.28 71.13Table 1: Results for the Arabic and English treebank of parsing using a vanilla PCFG with and without tensor decom-position.
Speed is given in seconds per sentence.minals is 81.Results Table 1 describes the results of compar-ing the tensor decomposition algorithm to the vanillaPCFG parsing algorithm.The first thing to note is that the running time ofthe parsing algorithm is linear in r. This indeedvalidates the asymptotic complexity of the inside-outside component in Goodman?s algorithm with theapproximate tensors.
It also shows that most of thetime during parsing is spent on the inside-outside al-gorithm, and not on the dynamic programming algo-rithm which follows it.In addition, compared to the baseline which usesa vanilla CKY algorithm (linear in the number ofrules), we get a speed up of a factor of 4.75 forArabic (r = 140) and 6.5 for English (r = 260)while retaining similar performance.
Perhaps moresurprising is that using the tensor approximation ac-tually improves performance in several cases.
Wehypothesize that the cause of this is that the tensordecomposition requires less parameters to expressthe rule probabilities in the grammar, and thereforeleads to better generalization than a vanilla maxi-mum likelihood estimate.We include results for a more complex model forArabic, which uses horizontal Markovization of or-der 1 and vertical Markovization of order 2 (Kleinand Manning, 2003).
This grammar includes 2,188binary rules.
Parsing exhaustively using this gram-mar takes 1.30 seconds per sentence (on average)with an F1 measure of 64.43.
Parsing with tensordecomposition for r = 280 takes 0.62 seconds persentence (on average) with an F1 measure of 64.05.7 DiscussionIn this section, we briefly touch on several other top-ics related to tensor approximation.7.1 Approximating the Probability of a StringThe probability of a sentence z under a PCFG is de-fined as p(z) =??
?T (z) p(?
), and can be approx-imated using the algorithm in Section 4.3, runningin time O(rN3 + rmN2).
Of theoretical interest,we discuss here a time O(rN3 + r2N2) algorithm,which is more convenient when r < m.Observe that in Eq.
(3) vector ?i,j always appearswithin one of the two terms V ?i,j and W?i,j inRr?1, whose dimensions are independent of m.We can therefore use Eq.
(3) to compute V ?i,j asV ?i,j = V U>(?j?1k=1(?
V ?i,k W?k+1,j)),where V U> is a Rr?r matrix that can becomputed off-line, i.e., independently ofz.
A symmetrical relation can be usedto compute W?i,j .
Finally, we can writep(z) = pi>U(?N?1k=1 (?
V ?1,k W?k+1,N )),where pi>U is a R1?r vector that can again becomputed off-line.
This algorithm then runs in timeO(rN3 + r2N2).7.2 Applications to Dynamic ProgrammingThe approximation method presented in this paper isnot limited to PCFG parsing.
A similar approxima-tion method has been used for latent-variable PCFGs(Cohen and Collins, 2012), and in general, ten-sor approximation can be used to speed-up inside-outside algorithms for general dynamic program-ming algorithms or weighted logic programs (Eisneret al 2004; Cohen et al 2011).
In the general case,the dimension of the tensors will not be necessarilyjust three (corresponding to binary rules), but can beof a higher dimension, and therefore the speed gaincan be even greater.
In addition, tensor approxima-tion can be used for computing marginals of latentvariables in graphical models.For example, the complexity of the forward-494backward algorithm for HMMs can be reduced tobe linear in the number of states (as opposed toquadratic) and linear in the rank used in an approxi-mate singular-value decomposition (instead of ten-sor decomposition) of the transition and emissionmatrices.7.3 Tighter (but Slower) Approximation UsingSingular Value DecompositionThe accuracy of the algorithm depends on the abilityof the tensor decomposition algorithm to decomposethe tensor with a small reconstruction error.
The de-composition algorithm is performed on the tensor Twhich includes all rules in the grammar.Instead, one can approach the approximation bydoing a decomposition for each slice of T separatelyusing singular value decomposition.
This will leadto a more accurate approximation, but will also leadto an extra factor of m during parsing.
This factoris added because now there is not a single U , V andW , but instead there are such matrices for each non-terminal in the grammar.8 ConclusionWe described an approximation algorithm for prob-abilistic context-free parsing.
The approximation al-gorithm is based on tensor decomposition performedon the underlying rule table of the CFG grammar.The approximation algorithm leads to significantspeed-up in PCFG parsing, with minimal loss in per-formance.ReferencesE.
Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A procedurefor quantitatively comparing the syntactic coverage ofEnglish grammars.
In Proceedings of DARPA Work-shop on Speech and Natural Language.P.
Boullier.
2003.
Guided earley parsing.
In 8th In-ternational Workshop on Parsing Technologies, pages43?54.J.
D. Carroll and J. J. Chang.
1970.
Analysis of indi-vidual differences in multidimensional scaling via anN-way generalization of Eckart-Young decomposition.Psychometrika, 35:283?319.E.
Charniak, M. Johnson, M. Elsner, J. Austerweil,D.
Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore,M.
Pozar, and T. Vu.
2006.
Multilevel coarse-to-finepcfg parsing.
In Proceedings of HLT-NAACL.E.
C. Chi and T. G. Kolda.
2011.
On tensors, spar-sity, and nonnegative factorizations.
arXiv:1112.2414[math.NA], December.S.
B. Cohen and M. Collins.
2012.
Tensor decomposi-tion for fast latent-variable PCFG parsing.
In Proceed-ings of NIPS.S.
B. Cohen, R. J. Simmons, and N. A. Smith.
2011.Products of weighted logic programs.
Theory andPractice of Logic Programming, 11(2?3):263?296.S.
B. Cohen, K. Stratos, M. Collins, D. F. Foster, andL.
Ungar.
2012.
Spectral learning of latent-variablePCFGs.
In Proceedings of ACL.J.
Eisner and N. A. Smith.
2005.
Parsing with soft andhard constraints on dependency length.
In Proceed-ings of IWPT, Parsing ?05.J.
Eisner, E. Goldlust, and N. A. Smith.
2004.
Dyna: Adeclarative language for implementing dynamic pro-grams.
In Proc.
of ACL (companion volume).N.
M. Faber, R. Bro, and P. Hopke.
2003.
Recent devel-opments in CANDECOMP/PARAFAC algorithms: acritical review.
Chemometrics and Intelligent Labora-tory Systems, 65(1):119?137.J.
Goodman.
1996.
Parsing algorithms and metrics.
InProceedings of ACL.R.
A. Harshman.
1970.
Foundations of the PARAFACprocedure: Models and conditions for an ?explana-tory?
multi-modal factor analysis.
UCLA working pa-pers in phoentics, 16:1?84.J.
H?astad.
1990.
Tensor rank is NP-complete.
Algo-rithms, 11:644?654.H.
Jaeger.
2000.
Observable operator models for dis-crete stochastic time series.
Neural Computation,12(6):1371?1398.D.
Klein and C. D. Manning.
2003.
Accurate unlexical-ized parsing.
In Proceedings of ACL.T.
G. Kolda and B. W. Bader.
2009.
Tensor decomposi-tions and applications.
SIAM Rev., 51:455?500.J.
B. Kruskal.
1989.
Rank, decomposition, and unique-ness for 3-way and N-way arrays.
In R. Coppi andS.
Bolasco, editors, Multiway Data Analysis, pages 7?18.M.
Maamouri, A. Bies, T. Buckwalter, and W. Mekki.2004.
The Penn Arabic Treebank: Building a large-scale annotated Arabic corpus.
In Proceedings NEM-LAR.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn treebank.
Computational Linguistics,19(2):313?330.495T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Proba-bilistic CFG with latent annotations.
In Proceedingsof ACL.M.-J.
Nederhof.
2000.
Practical experiments with regu-lar approximation of context-free languages.
Compu-tational Linguistics, 26(1):17?44.496
