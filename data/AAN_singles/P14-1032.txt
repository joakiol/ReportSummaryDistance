Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 336?346,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsProduct Feature Mining: Semantic Clues versus Syntactic ConstituentsLiheng Xu, Kang Liu, Siwei Lai and Jun ZhaoNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences, Beijing, 100190, China{lhxu, kliu, swlai, jzhao}@nlpr.ia.ac.cnAbstractProduct feature mining is a key subtaskin fine-grained opinion mining.
Previ-ous works often use syntax constituents inthis task.
However, syntax-based methodscan only use discrete contextual informa-tion, which may suffer from data sparsity.This paper proposes a novel product fea-ture mining method which leverages lexi-cal and contextual semantic clues.
Lexicalsemantic clue verifies whether a candidateterm is related to the target product, andcontextual semantic clue serves as a softpattern miner to find candidates, which ex-ploits semantics of each word in contextso as to alleviate the data sparsity prob-lem.
We build a semantic similarity graphto encode lexical semantic clue, and em-ploy a convolutional neural model to cap-ture contextual semantic clue.
Then LabelPropagation is applied to combine both se-mantic clues.
Experimental results showthat our semantics-based method signif-icantly outperforms conventional syntax-based approaches, which not only minesproduct features more accurately, but alsoextracts more infrequent product features.1 IntroductionIn recent years, opinion mining has helped cus-tomers a lot to make informed purchase decisions.However, with the rapid growth of e-commerce,customers are no longer satisfied with the over-all opinion ratings provided by traditional senti-ment analysis systems.
The detailed functions orattributes of products, which are called productfeatures, receive more attention.
Nevertheless, aproduct may have thousands of features, whichmakes it impractical for a customer to investigatethem all.
Therefore, mining product features au-tomatically from online reviews is shown to be akey step for opinion summarization (Hu and Liu,2004; Qiu et al, 2009) and fine-grained sentimentanalysis (Jiang et al, 2011; Li et al, 2012).Previous works often mine product features viasyntactic constituent matching (Popescu and Et-zioni, 2005; Qiu et al, 2009; Zhang et al, 2010).The basic idea is that reviewers tend to commenton product features in similar syntactic structures.Therefore, it is natural to mine product features byusing syntactic patterns.
For example, in Figure 1,the upper box shows a dependency tree producedby Stanford Parser (de Marneffe et al, 2006), andthe lower box shows a common syntactic patternfrom (Zhang et al, 2010), where <feature/NN>is a wildcard to be fit in reviews and NN denotesthe required POS tag of the wildcard.
Usually, theproduct name mp3 is specified, and when screenmatches the wildcard, it is likely to be a productfeature of mp3.Figure 1: An example of syntax-based prod-uct feature mining procedure.
The word screenmatches the wildcard <feature/NN>.
Therefore,screen is likely to be a product feature of mp3.Generally, such syntactic patterns extract prod-uct features well but they still have some limita-tions.
For example, the product-have-feature pat-tern may fail to find the fm tuner in a very similarcase in Example 1(a), where the product is men-tioned by using player instead of mp3.
Similarly,it may also fail on Example 1(b), just with have re-placed by support.
In essence, syntactic pattern is336a kind of one-hot representation for encoding thecontexts, which can only use partial and discretefeatures, such as some key words (e.g., have) orshallow information (e.g., POS tags).
Therefore,such a representation often suffers from the datasparsity problem (Turian et al, 2010).One possible solution for this problem is us-ing a more general pattern such as NP-VB-feature,where NP represents a noun or noun phrase andVB stands for any verb.
However, this pattern be-comes too general that it may find many irrelevantcases such as the one in Example 1(c), which is nottalking about the product.
Consequently, it is verydifficult for a pattern designer to balance betweenprecision and generalization.Example 1:(a) This player has an::fm:::::tuner.
(b) This mp3 supports::::wma:::file.
(c) This review has helped:::::people a lot.
(d) This mp3 has some:::::flaws.To solve the problems stated above, it is ar-gued that deeper semantics of contexts shall be ex-ploited.
For example, we can try to automaticallydiscover that the verb have indicates a part-wholerelation (Zhang et al, 2010) and support indicatesa product-function relation, so that both sth.
haveand sth.
support suggest that terms following themare product features, where sth.
can be replacedby any terms that refer to the target product (e.g.,mp3, player, etc.).
This is called contextual se-mantic clue.
Nevertheless, only using contexts isnot sufficient enough.
As in Example 1(d), we cansee that the word flaws follows mp3 have, but itis not a product feature.
Thus, a noise term maybe extracted even with high contextual support.Therefore, we shall also verify whether a candi-date is really related to the target product.
We callit lexical semantic clue.This paper proposes a novel bootstrapping ap-proach for product feature mining, which lever-ages both semantic clues discussed above.
Firstly,some reliable product feature seeds are automat-ically extracted.
Then, based on the assumptionthat terms that are more semantically similar tothe seeds are more likely to be product features,a graph which measures semantic similarities be-tween terms is built to capture lexical semanticclue.
At the same time, a semi-supervised con-volutional neural model (Collobert et al, 2011) isemployed to encode contextual semantic clue.
Fi-nally, the two kinds of semantic clues are com-bined by a Label Propagation algorithm.In the proposed method, words are representedby continuous vectors, which capture latent se-mantic factors of the words (Turian et al, 2010).The vectors can be unsupervisedly trained on largescale corpora, and words with similar semanticswill have similar vectors.
This enables our methodto be less sensitive to lexicon change, so that thedata sparsity problem can be alleviated .
The con-tributions of this paper include:?
It uses semantics of words to encode contextualclues, which exploits deeper level informationthan syntactic constituents.
As a result, it minesproduct features more accurately than syntax-based methods.?
It exploits semantic similarity between wordsto capture lexical clues, which is shown to bemore effective than co-occurrence relation be-tween words and syntactic patterns.
In addition,experiments show that the semantic similarityhas the advantage of mining infrequent productfeatures, which is crucial for this task.
For ex-ample, one may say ?This hotel has low waterpressure?, where low water pressure is seldommentioned, but fatal to someone?s taste.?
We compare the proposed semantics-based ap-proach with three state-of-the-art syntax-basedmethods.
Experiments show that our methodachieves significantly better results.The rest of this paper is organized as follows.
Sec-tion 2 introduces related work.
Section 3 describesthe proposed method in details.
Section 4 gives theexperimental results.
Lastly, we conclude this pa-per in Section 5.2 Related WorkIn product feature mining task, Hu and Liu (2004)proposed a pioneer research.
However, the asso-ciation rules they used may potentially introducemany noise terms.
Based on the observation thatproduct features are often commented on by simi-lar syntactic structures, it is natural to use patternsto capture common syntactic constituents aroundproduct features.Popescu and Etzioni (2005) designed some syn-tactic patterns to search for product feature candi-dates and then used Pointwise Mutual Information(PMI) to remove noise terms.
Qiu et al (2009)proposed eight heuristic syntactic rules to jointlyextract product features and sentiment lexicons,where a bootstrapping algorithm named Double337Propagation was applied to expand a given seedset.
Zhang et al (2010) improved Qiu?s workby adding more feasible syntactic patterns, and theHITS algorithm (Kleinberg, 1999) was employedto rank candidates.
Moghaddam and Ester (2010)extracted product features by automatical opinionpattern mining.
Zhuang et al (2006) used varioussyntactic templates from an annotated movie cor-pus and applied them to supervised movie featureextraction.
Wu et al (2009) proposed a phraselevel dependency parsing for mining aspects andfeatures of products.As discussed in the first section, syntactic pat-terns often suffer from data sparsity.
Further-more, most pattern-based methods rely on termfrequency, which have the limitation of findinginfrequent but important product features.
A re-cent research (Xu et al, 2013) extracted infrequentproduct features by a semi-supervised classifier,which used word-syntactic pattern co-occurrencestatistics as features for the classifier.
However,this kind of feature is still sparse for infrequentcandidates.
Our method adopts a semantic wordrepresentation model, which can train dense fea-tures unsupervisedly on a very large corpus.
Thus,the data sparsity problem can be alleviated.3 The Proposed MethodWe propose a semantics-based bootstrappingmethod for product feature mining.
Firstly, someproduct feature seeds are automatically extracted.Then, a semantic similarity graph is created tocapture lexical semantic clue, and a ConvolutionalNeural Network (CNN) (Collobert et al, 2011) istrained in each bootstrapping iteration to encodecontextual semantic clue.
Finally we use LabelPropagation to find some reliable new seeds forthe training of the next bootstrapping iteration.3.1 Automatic Seed GenerationThe seed set consists of positive labeled examples(i.e.
product features) and negative labeled exam-ples (i.e.
noise terms).
Intuitively, popular productfeatures are frequently mentioned in reviews, sothey can be extracted by simply mining frequentlyoccurring nouns (Hu and Liu, 2004).
However,this strategy will also find many noise terms (e.g.,commonly used nouns like thing, one, etc.).
Toproduce high quality seeds, we employ a DomainRelevance Measure (DRM) (Jiang and Tan, 2010),which combines term frequency with a domain-specific measuring metric called Likelihood RatioTest (LRT) (Dunning, 1993).
Let ?
(t) denotes theLRT score of a product feature candidate t,?
(t) =pk1(1?
p)n1?k1pk2(1?
p)n2?k2pk11(1?
p1)n1?k1pk22(1?
p2)n2?k2(1)where k1and k2are the frequencies of t in thereview corpus R and a background corpus1B, n1and n2are the total number of terms in R and B,p = (k1+ k2)/(n1+ n2), p1= k1/n1and p2=k2/n2.
Then a modified DRM2is proposed,DRM(t) =tf(t)max[tf(?
)]?1log df(t)?| log ?
(t)| ?min| log ?(?
)|max| log ?(?
)| ?min| log ?(?
)|(2)where tf(t) is the frequency of t inR and df(t) isthe frequency of t in B.All nouns in R are ranked by DRM(t) in de-scent order, where top N nouns are taken as thepositive example set V+s.
On the other hand, Xuet al (2013) show that a set of general nouns sel-dom appear to be product features.
Therefore, weemploy their General Noun Corpus to create thenegative example set V?s, where N most frequentterms are selected.
Besides, it is guaranteed thatV+s?
V?s= ?, i.e., conflicting terms are taken asnegative examples.3.2 Capturing Lexical Semantic Clue in aSemantic Similarity GraphTo capture lexical semantic clue, each word is firstconverted into word embedding, which is a con-tinuous vector with each dimension?s value corre-sponds to a semantic or grammatical interpretation(Turian et al, 2010).
Learning large-scale wordembeddings is very time-consuming (Collobert etal., 2011), we thus employ a faster method namedSkip-gram model (Mikolov et al, 2013).3.2.1 Learning Word Embedding forSemantic RepresentationGiven a sequence of training words W ={w1, w2, ..., wm}, the goal of the Skip-grammodel is to learn a continuous vector space EB ={e1, e2, ..., em}, where eiis the word embeddingof wi.
The training objective is to maximize the1Google-n-Gram (http://books.google.com/ngrams) isused as the background corpus.2The df(t) part of the original DRM is slightly modifiedbecause we want a tf ?
idf -like scheme (Liu et al, 2012).338average log probability of using word wtto pre-dict a surrounding word wt+j,?EB = argmaxet?EB1mm?t=1?
?c?j?c,j 6=0log p(wt+j|wt; et)(3)where c is the size of the training window.
Basi-cally, p(wt+j|wt; et) is defined as,p(wt+j|wt; et) =exp(e?Tt+jet)?mw=1exp(e?Twet)(4)where e?iis an additional training vector associ-ated with ei.
This basic formulation is impracti-cal because it is proportional to m. A hierarchicalsoftmax approximation can be applied to reducethe computational cost to log2(m), see (Morin andBengio, 2005) for details.To alleviate the data sparsity problem, EB isfirst trained on a very large corpus3(denoted byC), and then fine-tuned on the target review cor-pusR.
Particularly, for phrasal product features, astatistic-based method in (Zhu et al, 2009) is usedto detect noun phrases in R. Then, an Unfold-ing Recursive Autoencoder (Socher et al, 2011) istrained on C to obtain embedding vectors for nounphrases.
In this way, semantics of infrequent termsin R can be well captured.
Finally, the phrase-based Skip-gram model in (Mikolov et al, 2013)is applied onR.3.2.2 Building the Semantic Similarity GraphLexical semantic clue is captured by measuring se-mantic similarity between terms.
The underlyingmotivation is that if we have known some productfeature seeds, then terms that are more semanti-cally similar to these seeds are more likely to beproduct features.
For example, if screen is knownto be a product feature of mp3, and lcd is of highsemantic similarity with screen, we can infer thatlcd is also a product feature.
Analogously, termsthat are semantically similar to negative labeledseeds are not product features.Word embedding naturally meets the demandabove: words that are more semantically similarto each other are located closer in the embeddingspace (Collobert et al, 2011).
Therefore, we canuse cosine distance between two embedding vec-tors as the semantic distance measuring metric.Thus, our method does not rely on term frequency3Wikipedia(http://www.wikipedia.org) is used in practice.to rank candidates.
This could potentially improvethe ability of mining infrequent product features.Formally, we create a semantic similarity graphG = (V,E,W ), where V = {Vs?
Vc} is thevertex set, which contains the labeled seed set Vsand the unlabeled candidate set Vc; E is the edgeset which connects every vertex pair (u, v), whereu, v ?
V ; W = {wuv: cos(EBu, EBv)} is afunction which associates a weight to each edge.3.3 Encoding Contextual Semantic ClueUsing Convolutional Neural NetworkThe CNN is trained on each occurrence of seedsthat is found in review texts.
Then for a candidateterm t, the CNN classifies all of its occurrences.Since seed terms tend to have high frequency inreview texts, only a few seeds will be enough toprovide plenty of occurrences for the training.3.3.1 The architecture of the ConvolutionalNeural NetworkThe architecture of the Convolutional Neural Net-work is shown in Figure 2.
For a product featurecandidate t in sentence s, every consecutive sub-sequence qiof s that containing t with a windowof length l is fed to the CNN.
For example, asin Figure 2, if t = {screen}, and l = 3, thereare three inputs: q1= [the, ipod, screen], q2=[ipod, screen, is], q3= [screen, is, impressive].Partially, t is replaced by a token ?*PF*?
to re-move its lexicon influence4.Figure 2: The architecture of the ConvolutionalNeural Network.To get the output score, qiis first converted intoa concatenated vector xi= [e1; e2; ...; el], whereejis the word embedding of the j-th word.
Inthis way, the CNN serves as a soft pattern miner:4Otherwise, the CNN will quickly get overfitting on t, be-cause very few seed lexicons are used for the training.339since words that have similar semantics have sim-ilar low-dimension embedding vectors, the CNNis less sensitive to lexicon change.
The network iscomputed by,y(1)i= tanh(W(1)xi+ b(1)) (5)y(2)= max(y(1)i) (6)y(3)= W(3)y(2)+ b(3)(7)where y(i)is the output score of the i-th layer, andb(i)is the bias of the i-th layer; W(1)?
Rh?
(nl)and W(3)?
R2?hare parameter matrixes, wheren is the dimension of word embedding, and h isthe size of nodes in the hidden layer.In conventional neural models, the candidateterm t is placed in the center of the window.
How-ever, from Example 2, when l = 5, we can see thatthe best windows should be the bracketed texts(Because, intuitively, the windows should containmp3, which is a strong evidence for finding theproduct feature), where t = {screen} is at theboundary.
Therefore, we use Equ.
6 to formulatea max-convolutional layer, which is aimed to en-able the CNN to find more evidences in contextsthan conventional neural models.Example 2:(a) The [screen of this mp3 is] great.
(b) This [mp3 has a great screen].3.3.2 TrainingLet ?
= {EB,W(?
), b(?)}
denotes all the trainableparameters.
The softmax function is used to con-vert the output score of the CNN to a probability,p(t|X; ?)
=exp(y(3))?|C|j=1exp(y(3)j)(8)whereX is the input set for term t, andC = {0, 1}is the label set representing product feature andnon-product feature, respectively.To train the CNN, we first use Vsto collect eachoccurrence of the seeds in R to form a trainingset Ts.
Then, the training criterion is to minimizecross-entropy over Ts,??
= argmin?|Ts|?i=1?
log ?ip(ti|Xi; ?)
(9)where ?iis the binomial target label distributionfor one entry.
Backpropagation algorithm withmini-batch stochastic gradient descent is used tosolve this optimization problem.
In addition, someuseful tricks can be applied during the training.The weight matrixes W(?
)are initialized by nor-malized initialization (Glorot and Bengio, 2010).W(1)is pre-trained by an autoencoder (Hinton,1989) to capture semantic compositionality.
Tospeed up the learning, a momentum method is ap-plied (Sutskever et al, 2013).3.4 Combining Lexical and ContextualSemantic Clues by Label PropagationWe propose a Label Propagation algorithm tocombine both semantic clues in a unified process.Each term t ?
V is assumed to have a label dis-tribution Lt= (p+t, p?t), where p+tdenotes theprobability of the candidate being a product fea-ture, and on the contrary, p?t= 1?
p+t.
The clas-sified results of the CNN which encode contextualsemantic clue serve as the prior knowledge,It=???
(1, 0), if t ?
V+s(0, 1), if t ?
V?s(r+t, r?t), if t ?
Vc(10)where (r+t, r?t) is estimated by,r+t=count+(t)count+(t) + count?
(t)(11)where count+(t) is the number of occurrences ofterm t that are classified as positive by the CNN,and count?
(t) represents the negative count.Label Propagation is applied to propagate theprior knowledge distribution I to the product fea-ture distribution L via semantic similarity graphG, so that a product feature candidate is deter-mined by exploring its semantic relations to all ofthe seeds and other candidates globally.
We pro-pose an adapted version on the random walkingview of the Adsorption algorithm (Baluja et al,2008) by updating the following formula until Lconverges,Li+1= (1?
?
)MTLi+ ?DI (12)where M is the semantic transition matrix builtfrom G; D = Diag[log tf(t)] is a diagonal ma-trix of log frequencies, which is designed to as-sign higher ?confidence?
scores to more frequentseeds; and ?
is a balancing parameter.
Particu-larly, when ?
= 0, we can set the prior knowledgeI without Vcto L0so that only lexical semanticclue is used; otherwise if ?
= 1, only contextualsemantic clue is used.3403.5 The Bootstrapping FrameworkWe summarize the bootstrapping framework of theproposed method in Algorithm 1.
During boot-strapping, the CNN is enhanced by Label Propaga-tion which finds more labeled examples for train-ing, and then the performance of Label Propaga-tion is also improved because the CNN outputs amore accurate prior distribution.
After running forseveral iterations, the algorithm gets enough seeds,and a final Label Propagation is conducted to pro-duce the results.Algorithm 1: Bootstrapping using semantic cluesInput: The review corpusR, a large corpus COutput: The mined product feature list PInitialization: Train word embedding set EB first onC, and then onRStep 1: Generate product feature seeds Vs(Section 3.1)Step 2: Build semantic similarity graph G (Section 3.2)while iter < MAX ITER doStep 3: Use Vsto collect occurrence set TsfromRfor trainingStep 4: Train a CNNN on Ts(Section 3.3)Apply mini-batch SGD on Equ.
9;Step 5: Run Label Propagation (Section 3.4)Classify candidates usingN to setup I;L0?
I;repeatLi+1?
(1?
?
)MTLi+ ?DI;until ||Li+1?
Li||2< ?
;Step 6: Expand product feature seedsMove top T terms from Vcto Vs;iter++endStep 7: Run Label Propagation for a final result LfRank terms by L+fto get P , where L+f> L?f;4 Experiments4.1 Datasets and Evaluation MetricsDatasets: We select two real world datasets toevaluate the proposed method.
The first oneis a benchmark dataset in Wang et al (2011),which contains English review sets on two do-mains (MP3 and Hotel)5.
The second dataset isproposed by Chinese Opinion Analysis Evalua-tion 2008 (COAE 2008)6, where two review sets(Camera and Car) are selected.
Xu et al (2013)had manually annotated product features on thesefour domains, so we directly employ their annota-tion as the gold standard.
The detailed informationcan be found in their original paper.5http://timan.cs.uiuc.edu/downloads.html6http://ir-china.org.cn/coae2008.htmlEvaluation Metrics: We evaluate the proposedmethod in terms of precision(P), recall(R) and F-measure(F).
The English results are evaluated byexact string match.
And for Chinese results, weuse an overlap matching metric, because deter-mining the exact boundaries is hard even for hu-man (Wiebe et al, 2005).4.2 Experimental SettingsFor English corpora, the pre-processing are thesame as that in (Qiu et al, 2009), and for Chinesecorpora, the Stanford Word Segmenter (Changet al, 2008) is used to perform word segmenta-tion.
We select three state-of-the-art syntax-basedmethods to be compared with our method:DP uses a bootstrapping algorithm named asDouble Propagation (Qiu et al, 2009), which isa conventional syntax-based method.DP-HITS is an enhanced version of DP pro-posed by Zhang et al (2010), which ranks productfeature candidates bys(t) = log tf(t) ?
importance(t) (13)where importance(t) is estimated by the HITS al-gorithm (Kleinberg, 1999).SGW is the Sentiment Graph Walking algo-rithm proposed in (Xu et al, 2013), which firstextracts syntactic patterns and then uses randomwalking to rank candidates.
Afterwards, word-syntactic pattern co-occurrence statistic is usedas feature for a semi-supervised classifier TSVM(Joachims, 1999) to further refine the results.
Thistwo-stage method is denoted as SGW-TSVM.LEX only uses lexical semantic clue.
LabelPropagation is applied alone in a self-trainingmanner.
The dimension of word embedding n =100, the convergence threshold ?
= 10?7, and thenumber of expanded seeds T = 40.
The size ofthe seed set N is 40.
To output product features,it ranks candidates in descent order by using thepositive score L+f(t).CONT only uses contextual semantic clue,which only contains the CNN.
The window sizel is 5.
The CNN is trained with a mini-batch sizeof 50.
The hidden layer size h = 250.
Finally,importance(t) in Equ.
13 is replaced with r+tinEqu.
11 to rank candidates.LEX&CONT leverages both semantic clues.341MethodMP3 Hotel Camera Car Avg.P R F P R F P R F P R F FDP 0.66 0.57 0.61 0.66 0.60 0.63 0.71 0.70 0.70 0.72 0.65 0.68 0.66DP-HITS 0.65 0.62 0.63 0.64 0.66 0.65 0.71 0.78 0.74 0.69 0.68 0.68 0.68SGW 0.62 0.68 0.65 0.63 0.71 0.67 0.69 0.80 0.74 0.66 0.71 0.68 0.69LEX 0.64 0.74 0.69 0.65 0.75 0.70 0.69 0.84 0.76 0.68 0.78 0.73 0.72CONT 0.68 0.65 0.66 0.69 0.68 0.68 0.74 0.77 0.75 0.74 0.70 0.72 0.71SGW-TSVM 0.73 0.71 0.72 0.75 0.73 0.74 0.78 0.81 0.79 0.76 0.73 0.74 0.75LEX&CONT 0.74 0.75 0.74 0.75 0.77 0.76 0.80 0.84 0.82 0.79 0.79 0.79 0.78Table 1: Experimental results of product feature mining.
The precision or recall of CONT is the averageperformance over five runs with different random initialization of parameters of the CNN.
Avg.
standsfor the average score.4.3 The Semantics-based Methods vs.State-of-the-art Syntax-based MethodsThe experimental results are shown in Table 1,from which we have the following observations:(i) Our method achieves the best performanceamong all of the compared methods.
Wealso equally split the dataset into five sub-sets, and perform one-tailed t-test (p ?
0.05),which shows that the proposed semantics-based method (LEX&CONT) significantly out-performs the three syntax-based strong com-petitors (DP, DP-HITS and SGW-TSVM).
(ii) LEX&CONT which leverages both lexical andcontextual semantic clues outperforms ap-proaches that only use one kind of semanticclue (LEX and CONT), showing that the com-bination of the semantic clues is helpful.
(iii) Our methods which use only one kind ofsemantic clue (LEX and CONT) outperformsyntax-based methods (DP, DP-HITS andSGW).
Comparing DP-HITS with LEX andCONT, the difference between them is thatDP-HITS uses a syntax-pattern-based algo-rithm to estimate importance(t) in Equ.
13,while our methods use lexical or contextual se-mantic clue instead.
We believe the reason thatLEX or CONT is better is that syntactic pat-terns only use discrete and local information.In contrast, CONT exploits latent semantics ofeach word in context, and LEX takes advantageof word embedding, which is induced fromglobal word co-occurrence statistic.
Further-more, comparing SGW and LEX, both methodsare base on random surfer model, but LEX getsbetter results than SGW.
Therefore, the word-word semantic similarity relation used in LEXis more reliable than the word-syntactic patternrelation used in SGW.
(iv) LEX&CONT achieves the highest recallamong all of the evaluated methods.
SinceDP and DP-HITS rely on frequency for rank-ing product features, infrequent candidates areranked low in their extracted list.
As for SGW-TSVM, the features they used for the TSVMsuffer from the data sparsity problem for in-frequent terms.
In contrast, LEX&CONT isfrequency-independent to the review corpus.Further discussions on this observation aregiven in the next section.4.4 The Results on Extracting InfrequentProduct FeaturesWe conservatively regard 30% product featureswith the highest frequencies in R as frequent fea-tures, so the remaining terms in the gold standardare infrequent features.
In product feature miningtask, frequent features are relatively easy to find.Table 2 shows the recall of all the four approachesfor mining frequent product features.
We can seethat the performance are very close among differ-ent methods.
Therefore, the recall mainly dependson mining the infrequent features.Method MP3 Hotel Camera CarDP 0.89 0.92 0.86 0.84DP-HITS 0.89 0.91 0.86 0.85SGW-TSVM 0.87 0.92 0.88 0.87LEX&CONT 0.89 0.91 0.89 0.87Table 2: The recall of frequent product features.Figure 3 gives the recall of infrequent prod-uct features, where LEX&CONT achieves the bestperformance.
So our method is less influencedby term frequency.
Furthermore, LEX gets betterrecall than CONT and all syntax-based methods,which indicates that lexical semantic clue does aidto mine more infrequent features as expected.3421 2 3 4 5 6 7 8 9.5.6.7.8.91.0LEX&CONTCONTLEX(a) MP31 2 3 4 5 6 7 8 9.5.6.7.8.91.0LEX&CONTCONTLEX(b) Hotel1 2 3 4 5 6 7 8 9.5.6.7.8.91.0LEX&CONTCONTLEX(c) Camera1 2 3 4 5 6 7 8 9.5.6.7.8.91.0LEX&CONTCONTLEX(d) CarFigure 4: Accuracy (y-axis) of product feature seed expansion at each bootstrapping iteration (x-axis).The error bar shows the standard deviation over five runs.MethodMP3 Hotel Camera CarP R F P R F P R F P R FFW-5 0.62 0.63 0.62 0.64 0.64 0.64 0.68 0.73 0.70 0.67 0.66 0.66FW-9 0.64 0.65 0.64 0.66 0.68 0.67 0.70 0.76 0.73 0.71 0.70 0.70CONT 0.68 0.65 0.66 0.69 0.68 0.68 0.74 0.77 0.75 0.74 0.70 0.72Table 3: The results of convolutional method vs. the results of non-convolutional methods.MP3 Hotel Camera CarRecall.4.5.6.7.8.9 DPDP-HITSSGW-TSVMCONTLEXLEX&CONTFigure 3: The recall of infrequent features.
Theerror bar shows the standard deviation over fivedifferent runs.4.5 Lexical Semantic Clue vs. ContextualSemantic ClueThis section studies the effects of lexical seman-tic clue and contextual semantic clue during seedexpansion (Step 6 in Algorithm 1), which is con-trolled by ?.
When ?
= 1, we get the CONT; andif ?
is set 0, we get the LEX.
To take into accountthe correctly expanded terms for both positive andnegative seeds, we use Accuracy as the evaluationmetric,Accuracy =#TP + #TN# Extracted Seedswhere TP denotes the true positive seeds, and TNdenotes the true negative seeds.Figure 4 shows the performance of seed ex-pansion during bootstrapping, in which the accu-racy is computed on 40 seeds (20 being positiveand 20 being negative) expanded in each itera-tion.
We can see that the accuracies of CONT andLEX&CONT retain at a high level, which showsthat they can find reliable new product featureseeds.
However, the performance of LEX oscil-lates sharply and it is very low for some points,which indicates that using lexical semantic cluealone is infeasible.
On another hand, comparingCONT with LEX in Table 1, we can see that LEXperforms generally better than CONT.
AlthoughLEX is not so accurate as CONT during seed ex-pansion, its final performance surpasses CONT.Consequently, we can draw conclusion that CONTis more suitable for the seed expansion, and LEXis more robust for the final result production.To combine advantages of the two kinds of se-mantic clues, we set ?
= 0.7 in Step 5 of Algo-rithm 1, so that contextual semantic clue plays akey role to find new seeds accurately.
For Step 7,we set ?
= 0.3.
Thus, lexical semantic clue isemphasized for producing the final results.4.6 The Effect of Convolutional LayerTwo non-convolutional variations of the proposedmethod are used to be compared with the convo-lutional method in CONT.
FW-5 uses a traditionalneural network with a fixed window size of 5 toreplace the CNN in CONT, and the candidate termto be classified is placed in the center of the win-dow.
Similarly, FW-9 uses a fixed window sizeof 9.
Note that CONT uses a 5-term dynamicwindow containing the candidate term, so the ex-ploited number of words in the context is equiva-lent to FW-9.343Table 3 shows the experimental results.
We cansee that the performance of FW-5 is much worsethan CONT.
The reason is that FW-5 only exploitshalf of the context as that of CONT, which is notsufficient enough.
Meanwhile, although FW-9 ex-ploits equivalent range of context as that of CONT,it gets lower precisions.
It is because FW-9 hasapproximately two times parameters in the param-eter matrix W(1)than that in Equ.
5 of CONT,which makes it more difficult to be trained withthe same amount of data.
Also, lengths of manysentences in the review corpora are shorter than 9.Therefore, the convolutional approach in CONT isthe most effective way among these settings.4.7 Parameter StudyWe investigate two key parameters of the proposedmethod: the initial number of seeds N , and thesize of the window l used by the CNN.Figure 5 shows the performance under differ-ent N , where the F-Measure saturates when Nequates to 40 and beyond.
Hence, very few seedsare needed for starting our algorithm.N10 20 30 40 50 60F-Measure.65.70.75.80.85MP3Hote lCame raCarFigure 5: F-Measure vs. N for the final results.Figure 6 shows F-Measure under different win-dow size l. We can see that the performance isimproved little when l is larger than 5.
Therefore,l = 5 is a proper window size for these datasets.l2 3 4 5 6 7F-Measure.5.6.7.8.9MP3Hote lCame raCarFigure 6: F-Measure vs. l for the final results.5 Conclusion and Future WorkThis paper proposes a product feature miningmethod by leveraging contextual and lexical se-mantic clues.
A semantic similarity graph is builtto capture lexical semantic clue, and a convo-lutional neural network is used to encode con-textual semantic clue.
Then, a Label Propaga-tion algorithm is applied to combine both seman-tic clues.
Experimental results prove the effec-tiveness of the proposed method, which not onlymines product features more accurately than con-ventional syntax-based method, but also extractsmore infrequent product features.In future work, we plan to extend the proposedmethod to jointly mine product features along withcustomers?
opinions on them.
The learnt seman-tic representations of words may also be utilizedto predict fine-grained sentiment distributions overproduct features.AcknowledgementThis work was sponsored by the NationalBasic Research Program of China (No.2012CB316300), the National Natural Sci-ence Foundation of China (No.
61272332 andNo.
61202329), the National High Technol-ogy Development 863 Program of China (No.2012AA011102), and CCF-Tencent Open Re-search Fund.
This work was also supported inpart by Noahs Ark Lab of Huawei Tech.
Ltm.ReferencesShumeet Baluja, Rohan Seth, D. Sivakumar, YushiJing, Jay Yagnik, Shankar Kumar, DeepakRavichandran, and Mohamed Aly.
2008.
Videosuggestion and discovery for youtube: Taking ran-dom walks through the view graph.
In Proceedingsof the 17th International Conference on World WideWeb, WWW ?08, pages 895?904, New York, NY,USA.
ACM.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing chinese word segmen-tation for machine translation performance.
In Pro-ceedings of the Third Workshop on Statistical Ma-chine Translation, StatMT ?08, pages 224?232.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
J. Mach.
Learn.
Res., 12:2493?2537,November.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed344dependency parses from phrase structure parses.
InProceedings of the IEEE / ACL?06 Workshop onSpoken Language Technology.Ted Dunning.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
Comput.
Linguist.,19(1):61?74, March.Xavier Glorot and Yoshua Bengio.
2010.
Understand-ing the difficulty of training deep feedforward neuralnetworks.
In Proceedings of the International Con-ference on Artificial Intelligence and Statistics.Geoffrey E. Hinton.
1989.
Connectionist learning pro-cedures.
Artificial Intelligence, 40(1C3):185 ?
234.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177, New York, NY, USA.
ACM.Xing Jiang and Ah-Hwee Tan.
2010.
Crctol: Asemantic-based domain ontology learning system.Journal of the American Society for Information Sci-ence and Technology, 61(1):150?168.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, andTiejun Zhao.
2011.
Target-dependent twitter sen-timent classification.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Vol-ume 1, HLT ?11, pages 151?160, Stroudsburg, PA,USA.
Association for Computational Linguistics.Thorsten Joachims.
1999.
Transductive inference fortext classification using support vector machines.
InProceedings of the 16th International Conference onMachine Learning, pages 200?209.Jon M. Kleinberg.
1999.
Authoritative sources in ahyperlinked environment.
J. ACM, 46(5):604?632,September.Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, andXiaoyan Zhu.
2012.
Cross-domain co-extraction ofsentiment and topic lexicons.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers - Volume 1, ACL?12, pages 410?419, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Kang Liu, Liheng Xu, and Jun Zhao.
2012.
Opin-ion target extraction using word-based translationmodel.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 1346?1356, Jeju Island, Korea,July.
Association for Computational Linguistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Samaneh Moghaddam and Martin Ester.
2010.
Opin-ion digger: An unsupervised opinion miner fromunstructured product reviews.
In Proceedings ofthe 19th ACM International Conference on Informa-tion and Knowledge Management, CIKM ?10, pages1825?1828, New York, NY, USA.
ACM.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the international workshop on arti-ficial intelligence and statistics, AISTATS05, pages246?252.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InProceedings of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, HLT ?05, pages 339?346.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2009.
Expanding domain sentiment lexicon throughdouble propagation.
In Proceedings of the 21st in-ternational jont conference on Artifical intelligence,IJCAI?09, pages 1199?1204.Richard Socher, Eric H Huang, Jeffrey Pennington,Andrew Y Ng, and Christopher D Manning.
2011.Dynamic pooling and unfolding recursive autoen-coders for paraphrase detection.
In NIPS?2011, vol-ume 24, pages 801?809.Ilya Sutskever, James Martens, George Dahl, and Ge-offrey Hinton.
2013.
Distributed representations ofwords and phrases and their compositionality.
InProceedings of the 30 th International Conferenceon Machine Learning.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, ACL ?10, pages 384?394,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Hongning Wang, Yue Lu, and ChengXiang Zhai.
2011.Latent aspect rating analysis without aspect key-word supervision.
In Proceedings of the 17th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, KDD ?11, pages 618?626, New York, NY, USA.
ACM.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39(2-3):165?210.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.2009.
Phrase dependency parsing for opinion min-ing.
In Proceedings of the 2009 Conference on Em-pirical Methods in Natural Language Processing:Volume 3 - Volume 3, EMNLP ?09, pages 1533?1541, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.345Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and JunZhao.
2013.
Mining opinion words and opinion tar-gets in a two-stage framework.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1764?1773, Sofia, Bulgaria, August.
Association forComputational Linguistics.Lei Zhang, Bing Liu, Suk Hwan Lim, and EamonnO?Brien-Strain.
2010.
Extracting and ranking prod-uct features in opinion documents.
In Proceedingsof the 23rd International Conference on Compu-tational Linguistics: Posters, COLING ?10, pages1462?1470, Stroudsburg, PA, USA.
Association forComputational Linguistics.Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, andMuhua Zhu.
2009.
Multi-aspect opinion pollingfrom textual reviews.
In Proceedings of the 18thACM Conference on Information and KnowledgeManagement, CIKM ?09, pages 1799?1802, NewYork, NY, USA.
ACM.Li Zhuang, Feng Jing, and Xiao-Yan Zhu.
2006.Movie review mining and summarization.
In Pro-ceedings of the 15th ACM International Conferenceon Information and Knowledge Management, CIKM?06, pages 43?50, New York, NY, USA.
ACM.346
