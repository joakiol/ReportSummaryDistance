Proceedings of the 7th Workshop on Statistical Machine Translation, pages 232?242,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUsing Syntactic Head Information in Hierarchical Phrase-Based TranslationJunhui Li Zhaopeng Tu?
Guodong Zhou?
Josef van GenabithCentre for Next Generation LocalisationSchool of Computing, Dublin City University?
Key Lab.
of Intelligent Info.
ProcessingInstitute of Computing Technology, Chinese Academy of Sciences?School of Computer Science and TechnologySoochow University, China{jli,josef}@computing.dcu.ietuzhaopeng@ict.ac.cn gdzhou@suda.edu.cnAbstractChiang?s hierarchical phrase-based (HPB)translation model advances the state-of-the-artin statistical machine translation by expandingconventional phrases to hierarchical phrases?
phrases that contain sub-phrases.
How-ever, the original HPB model is prone to over-generation due to lack of linguistic knowl-edge: the grammar may suggest more deriva-tions than appropriate, many of which maylead to ungrammatical translations.
On theother hand, limitations of glue grammar rulesin the original HPB model may actually pre-vent systems from considering some reason-able derivations.
This paper presents a sim-ple but effective translation model, called theHead-Driven HPB (HD-HPB) model, whichincorporates head information in translationrules to better capture syntax-driven informa-tion in a derivation.
In addition, unlike theoriginal glue rules, the HD-HPB model allowsimproved reordering between any two neigh-boring non-terminals to explore a larger re-ordering search space.
An extensive set of ex-periments on Chinese-English translation onfour NIST MT test sets, using both a smalland a large training set, show that our HD-HPB model consistently and statistically sig-nificantly outperforms Chiang?s model as wellas a source side SAMT-style model.1 IntroductionChiang?s hierarchical phrase-based (HPB) transla-tion model utilizes synchronous context free gram-mar (SCFG) for translation derivation (Chiang,2005; Chiang, 2007) and has been widely adoptedin statistical machine translation (SMT).
Typically,such models define two types of translation rules:hierarchical (translation) rules which consist of bothterminals and non-terminals, and glue (grammar)rules which combine translated phrases in a mono-tone fashion.
However, due to lack of linguisticknowledge, Chiang?s HPB model contains only onetype of non-terminal symbol X , often making itdifficult to select the most appropriate translationrules.1One important research question is therefore howto refine the non-terminal category X using linguis-tically motivated information: Zollmann and Venu-gopal (2006) (SAMT) e.g.
use (partial) syntacticcategories derived from CFG trees while Zollmannand Vogel (2011) use word tags, generated by ei-ther POS analysis or unsupervised word class in-duction.
Almaghout et al (2011) employ CCG-based supertags.
Mylonakis and Sima?an (2011) uselinguistic information of various granularities suchas Phrase-Pair, Constituent, Concatenation of Con-stituents, and Partial Constituents, where applica-ble.By contrast, and inspired by previous work inparsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the in-tuition that linguistic heads provide important in-formation about a constituent or distributionally de-fined fragment, as in HPB.
We identify heads usinglinguistically motivated dependency parsing, anduse head information to refine X.Furthermore, Chiang?s HPB model suffers fromlimited phrase reordering by combining translated1Another non-terminal symbol S is used in glue rules.232(a) (b)zuotian chuxi huiyiattended a meeting yesterdayX2 X1X1 X2S2S1S2S1zuotian chuxi huiyiattended a meeting yesterdayX4 X3X3 X4X2X1X2S2X1S1S1X1Figure 1: Example of derivations disallowed in Chiang?sHPB model.
The rules with dotted lines are not coveredin Chiang?s model.phrases in a monotonic way with glue rules.
Inaddition, once a glue rule is adopted, it requiresall rules above it to be glue rules.
For exam-ple, given a Chinese-English sentence pair (?
?/zuotian1 ?
?/chuxi2 ?
?/huiyi3, Attended2 a3meeting3 yesterday1), a correct translation is impos-sible via HPB derivations in Figure 1.
For the deriva-tion in Figure 1(a), swap reordering in the glue rule(i.e., S1 ?
?S2X2, X2S2?)
is disallowed and, evenif such a swap reordering is available, it lacks usefulinformation for rule selection.
For the derivation inFigure 1(b), the combination of two non-terminals(i.e., X2 ?
?X3X4, X3X4?)
is disallowed to forma new non-terminal which in turn is a sub-phrase ofa hierarchical rule.
These limitations prevent tra-ditional HPB systems from even considering somereasonable derivations.To tackle the problem of glue rules, He (2010) ex-tended the HPB model by using bracketing transduc-tion grammar (Wu, 1996) instead of the monotoneglue rules, and trained an extra classifier for gluerules to predict reorderings of neighboring phrases.By contrast, our HD-HPB model refines the non-terminal symbol X with syntactic head informa-tion and provides flexible reordering rules, includingswap, which can mix freely with hierarchical trans-lation rules for better interleaving of translation andreordering in translation derivations.Different from the soft constraint modelingadopted in (Chan et al, 2007; Marton and Resnik,2008; Shen et al, 2009; He et al, 2010; Huang etal., 2010; Gao et al, 2011), our approach encodessyntactic information in translation rules.
However,the two approaches are not mutually exclusive, aswe could also include a set of syntax-driven featuresinto our translation model.
Our approach maintainsthe advantages of Chiang?s HPB model while at thesame time incorporating head information and flex-ible reordering in a derivation in a natural way.
Ex-periments on Chinese-English translation using fourNIST MT test sets show that our HD-HPB modelsignificantly outperforms Chiang?s HPB as well as aSAMT-style refined version of HPB.The paper is structured as follows: Section 2describes the synchronous context-free grammar(SCFG) in our HD-HPB translation model.
Sec-tion 3 presents our model and features, followed bythe decoding algorithm in Section 4.
We report ex-perimental results in Section 5.
Finally we concludein Section 6.2 Head-Driven HPB Translation ModelLike Chiang (2005) and Chiang (2007), our HD-HPB translation model adopts a synchronous con-text free grammar, a rewriting system which gen-erates source and target side string pairs simultane-ously using a context-free grammar.
In particular,each synchronous rule rewrites a non-terminal intoa pair of strings, s and t, where s (or t) contains ter-minals and non-terminals from the source (or target)language and there is a one-to-one correspondencebetween the non-terminal symbols on both sides.A good and informative inventory of non-terminalsymbols is always important, especially for a suc-cessful SCFG-based translation model.
Instead ofcollapsing all non-terminals in the source languageinto a single symbol X as in Chiang (2007), ideallynon-terminals should capture important informationof the word sequences they cover to be able to prop-erly discriminate between similar and different wordsequences during translation.
This motivates ourapproach to provide syntax-enriched non-terminalsymbols.
Given a word sequence f ij from position ito position j, we refine the non-terminal symbol Xto reflect some of the internal syntactic structure of233??/NROuzhou??/NNbaguo??/ADlianming??/VVzhichi?
?/NRmeiguo?/P dui?
?/NNcelie?/NRyirootEightEuropeancountriesjointlysupportAmerica?sstandagainstIraqFigure 2: An example word alignment for a Chinese-English sentence pair with the dependency parse tree for theChinese sentence.
Here, each Chinese word is attached with its POS tag and Pinyin.the word sequence covered by X .
A correct transla-tion rule selection therefore not only maps terminalsinto terminals, but is both constrained and guidedby syntactic information in the non-terminals.
Atthe same time, it is not clear whether an ?ideal?
ap-proach that captures a full syntactic analysis of thestring fragment covered by a non-terminal is feasi-ble: the diversity of syntactic structures could maketraining impossible and lead to serious data sparse-ness issues.
As a compromise, given a word se-quence f ij , we first find heads and then concatenatethe POS tags of these heads as f ij?s non-terminalsymbol.2 Our approach is guided by the intuitionthat linguistic heads provide important informationabout a constituent or distributionally defined frag-ment, as in HPB.
Specifically, we adopt dependencystructure to derive heads, which are defined as:Definition 1.
For word sequence f ij , wordfk (i ?
k ?
j) is regarded as a head if it is domi-nated by a word outside of this sequence.Note that this definition (i) allows for a word se-quence to have one or more heads (largely due tothe fact that a word sequence is not necessarily lin-guistically constrained) and (ii) ensures that headsare always the highest heads in the sequence from adependency structure perspective.
For example, theword sequence ouzhou baguo lianming in Figure 2has two heads (i.e., baguo and lianming, ouzhou isnot a head of this sequence since its headword baguofalls within this sequence) and the non-terminal cor-responding to the sequence is thus labeled as NN-AD.
It is worth noting that in this paper we onlyrefine non-terminal X on the source side to head-informed ones, while still usingX on the target side.2Note that instead of POS tags, it is also possible to use othertypes of syntactic information associated with heads to refinenon-terminal symbols (Section 5.5.2).In our HD-HPB model, the SCFG is defined asa tuple ?
?, N,?,?,<?, where ?
is a set of sourcelanguage terminals,N is a set of non-terminals cate-gorizing terminals in ?, ?
is a set of target languageterminals, ?
is a set of non-terminals categorizingterminals in ?, and < is a set of translation rules.A rule ?
in < is in the form of ?Ps ?
s, Pt ?
t, ??,where:?
Ps ?
N and Pt ?
?;?
s ?
(?
?N)+ and t ?
(?
?
?)+?
?
is a bijection between non-terminals in s and t.According to the occurrence of terminals in s andt, we group the rules in the HD-HPB model into twocategories: head-driven hierarchical rules (HD-HRs)and non-terminal reordering rules (NRRs), wherethe former have at least one terminal on both sourceand target sides and the later have no terminals.
Forrule extraction, we first identify initial phrase pairson word-aligned sentence pairs by using the samecriterion as most phrase-based translation models(Och and Ney, 2004) and Chiang?s HPB model (Chi-ang, 2005; Chiang, 2007).
We extract HD-HRs andNRRs based on initial phrase pairs, respectively.2.1 HD-HRs: Head-Driven Hierarchical RulesAs mentioned, a HD-HR has at least one terminalon both source and target sides.
This is the sameas the hierarchical rules defined in Chiang?s HPBmodel (Chiang, 2007), except that we use head POS-informed non-terminal symbols in the source lan-guage.
We look for initial phrase pairs that con-tain other phrases and then replace sub-phrases withtheir corresponding non-terminal symbols.
Giventhe word alignment as shown in Figure 2, Table 1demonstrates the difference between hierarchicalrules in Chiang (2007) and HD-HRs defined here.234phrase pairs hierarchical rule head-driven hierarchical rulecelie, stand X?celie, standNN?celie,X?standdui yi celie1, stand1 against Iraq X?dui yi X1, X1 against IraqNN?dui yi NN1,X?X1 against Iraqzhichi meiguo, support America?s X?zhichi meiguo, support America?sVV-NR?zhichi meiguo,X?support America?szhichi meiguo1 dui yi celie2,support America?s1 stand2 against IraqX?X1 dui yi X2,X1 X2 against IraqVV?VV-NR1 dui yi NN2,X?X1 X2 against IraqTable 1: Comparison of hierarchical rules in Chiang (2007) and HD-HRs.
Indexed underlines indicate sub-phrasesand corresponding non-terminal symbols.
The non-terminals in HD-HRs (e.g., NN, VV, VV-NR) capture the head(s)POS tags of the corresponding word sequence in the source language.Similar to Chiang?s HPB model, our HD-HPBmodel will result in a large number of rules causingproblems in decoding.
To alleviate these problems,we filter our HD-HRs according to the same con-straints as described in Chiang (2007).
Moreover,we discard rules that have non-terminals with morethan four heads.2.2 NRRs: Non-terminal Reordering RulesNRRs are translation rules without terminals.
Givenan initial phrase pair?f ij , ei?j?
?, we check all otherinitial phrase pairs?fkl , ek?l?
?which satisfy k = j+1(i.e., phrase fkl is located immediately to the rightof f ij in the source language).
For their targetside translations, there are four possible positionalrelationships: monotone, discontinuous monotone,swap, and discontinuous swap.
In order to differen-tiate non-terminals from those in the target language(i.e., X), we use Y as a variable for non-terminals inthe source language, and obtain four types of NRRs:?
Monotone ?Y ?
Y1Y2, X ?
X1X2?;?
Discontinuous monotone?Y ?
Y1Y2, X ?
X1 .
.
.
X2?;?
Swap ?Y ?
Y1Y2, X ?
X2X1?;?
Discontinuous swap?Y ?
Y1Y2, X ?
X2 .
.
.
X1?.For example in Figure 2, the NRR for initialphrase pairs ?zhichi meiguo, support America?s?and ?dui yi celie, stand against Iraq?
would be?V V ?
V V -NR1NN2, X ?
X1X2?.Merging two neighboring non-terminals into asingle non-terminal, NRRs enable the translationmodel to explore a wider search space.
During train-ing, we extract four types of NRRs and calculateprobabilities for each type.
To speed up decoding,we currently (i) only use monotone and swap NRRsand (ii) limit the number of non-terminals in a NRRto 2.3 Log-linear Model and FeaturesFollowing Och and Ney (2002), we depart from thetraditional noisy-channel approach and use a generallog-linear model.
Let d be a derivation from sen-tence f in the source language to sentence e in thetarget language.
The probability of d is defined as:P (d) ?
?i?i (d)?i (1)where ?i are features defined on derivations and?i are feature weights.
In particular, we use a fea-ture set analogous to the default feature set of Chi-ang (2007), which includes:?
Phd-hr (t|s) and Phd-hr (s|t), translation probabili-ties for HD-HRs;?
Plex (t|s) and Plex (s|t), lexical translation proba-bilities for HD-HRs;?
Ptyhd-hr = exp (?1), rule penalty for HD-HRs;?
Pnrr (t|s), translation probability for NRRs;?
Ptynrr = exp (?1), rule penalty for NRRs;?
Plm (e), language model;?
Ptyword (e) = exp (?|e|), word penalty.235Algorithm 1: Decoding AlgorithmInput: Sentence f1n in the source languageDependency structure of f1nHD-HR rule set HDHRNRR rule set NRRInitial phrase length KOutput: Best derivation d?1.
set chart[i, j]=NIL (1 ?
i ?
j ?
n);2. for l from 1 to n do3.
for all i, j such that j ?
i = l do4.
if l ?
K do5.
for all derivations d derived fromHDHR spanning from i to j do6.
add d into chart[i, j]7. for all derivations d derived fromNRR spanning from i to j do8.
add d into chart[i, j]9. set d?
as the top derivation of chart[1, n]10.return d?It is worth pointing out that we define translationprobabilities for NRRs only for the direction fromsource language to target language, although trans-lation probabilities for HD-HRs are defined for bothdirections.
This is mostly due to the fact that a NRRexcludes terminals and has only two options on thetarget side (i.e., either X ?
X1X2 or X ?
X2X1).4 DecodingOur decoder is based on CKY-style chart parsingwith beam search.
Given an input sentence f , it findsa sentence e in the target language derived from thebest derivation d?
among all possible derivations D:d?
= arg maxd?DP (D) (2)Algorithm 1 presents the decoding process.
Givena source sentence, it searches for the best deriva-tion bottom-up.
For a source span [i, j], it appliesboth types of HD-HRs and NRRs.
However, HD-HRs are only applied to generate derivations span-ning no more than K words ?
the initial phraselength limit used in training to extract HD-HRs ?while NRRs are applied to derivations spanning anylength.
Unlike in Chiang (2007), it is possible fora non-terminal generated by a NRR to be includedafterwards by a HD-HR or another NRR.
Similar toChiang (2007) in generating k-best derivations fromi to j, we make use of cube pruning (Huang and Chi-ang, 2005) with an integrated language model foreach derivation.5 ExperimentsWe evaluate the performance of our HD-HPB modeland compare it with our implementation of Chiang?sHPB model (Chiang, 2007), a source-side SAMT-style refined version of HPB (SAMT-HPB), and theMoses implementation of HPB.
For fair compari-son, we adopt the same parameter settings for HD-HPB, HPB and SAMT-HPB systems, including ini-tial phrase length (as 10) in training, the maximumnumber of non-terminals (as 2) in translation rules,maximum number of non-terminals plus terminals(as 5) on the source, prohibition of non-terminalsto be adjacent on the source, beam threshold ?
(as10?5) (to discard derivations with a score worse than?
times the best score in the same chart cell), beamsize b (as 200) (i.e.
each chart cell contains at mostb derivations).
For Moses HPB, we use ?grow-diag-final-and?
to obtain symmetric word alignments, 10for the maximum phrase length, and the recom-mended default values for all other parameters.5.1 Experimental SettingsTo examine the efficacy of our approach on trainingdatasets of different scales, we first train translationmodels on a small-sized corpus, and then scale to alarger one.
We use the 2002 NIST MT evaluationtest data (878 sentence pairs) as the developmentdata, and the 2003, 2004, 2005, 2006-news NISTMT evaluation test data (919, 1788, 1082, and 616sentence pairs, respectively) as the test data.
To findheads, we parse the source sentences with the Berke-ley Parser3 (Petrov and Klein, 2007) trained on Chi-nese TreeBank 6.0 and use the Penn2Malt toolkit4to obtain dependency structures.We obtain the word alignments by runningGIZA++ (Och and Ney, 2000) on the corpus inboth directions, applying ?grow-diag-final-and?
re-finement (Koehn et al, 2003).
We use the SRI lan-guage modeling toolkit to train a 5-gram languagemodel on the Xinhua portion of the Gigaword corpusand standard MERT (Och, 2003) to tune the feature3http://code.google.com/p/berkeleyparser/4http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html/236weights on the development data.For evaluation, the NIST BLEU script (version12) with the default settings is used to calculate theNIST and the BLEU scores, which measures case-insensitive matching of n-grams with n up to 4.
Totest whether a performance difference is statisticallysignificant, we conduct significance tests followingthe paired bootstrap approach (Koehn, 2004).
In thispaper, ?**?
and ?*?
denote p-values less than0.01 and in-between [0.01, 0.05), respectively.5.2 Results on Small DataTo test the HD-HPB models, we firstly carried outexperiments using the FBIS corpus as training data,which contains ?240K sentence pairs.
Table 2 liststhe rule table sizes.
The full rule table size (includ-ing HD-HRs and NRRs) of our HD-HPB model isabout 1.5 times that of Chiang?s, largely due to re-fining the non-terminal symbolX in Chiang?s modelinto head-informed ones in our model.
It is alsounsurprising, that the test set-filtered rule table sizeof our model is only about 0.8 times that of Chi-ang?s: this is due to the fact that some of the re-fined translation rule patterns required by the testset are unattested in the training data.
Furthermore,the rule table size of NRRs is much smaller thanthat of HD-HRs since a NRR contains only twonon-terminals.
Table 3 lists the translation perfor-mance with NIST and BLEU scores.
Note that ourre-implementation of Chiang?s original HPB modelperforms on a par with Moses HPB.
Table 3 showsthat our HD-HPB model significantly outperformsChiang?s HPB model with an average improvementof 1.32 in BLEU and 0.16 in NIST (and similar im-provements over Moses HPB).Although HD-HPB has small size of phrase ta-bles compared to HPB, it still consumes more timein decoding (e.g., 15.1 vs. 11.0), mostly due to theflexible reordering of NRRs.5.3 Results on Large DataWe also conduct experiments on larger trainingdata with ?1.5M sentence pairs from the LDCdataset.5 Table 4 lists the rule table sizes and Ta-ble 5 presents translation performance with NIST5This dataset includes LDC2002E18, LDC2003E07,LDC2003E14, Hansards portion of LDC2004T07,LDC2004T08 and LDC2005T06and BLEU scores.
It shows that our HD-HPB modelconsistently outperforms Chiang?s HPB model withan average improvement of 1.91 in BLEU and 0.35in NIST (similar for Moses HPB).
Compared to theimprovement achieved on the small data, it is en-couraging to see that our HD-HPB model benefitsmore from larger training data with little adverse ef-fect on decoding time which increases only slightlyfrom 15.1 to 16.6 seconds per sentence.5.4 Comparison with SAMT-HPBComparing the performance of SAMT-HPB withregular HPB in Table 3 and Table 5, it is interest-ing to see that in general the SAMT-style approachleads to a deterioration of translation performancefor the small training set (e.g., 30.09 for SAMT-HPBvs.
30.64 for HPB) while it comes into its own forthe large training set (e.g., 33.54 for SAMT-HPB vs.32.95 for HPB), indicating that the SAMT-style ap-proach is more prone to data sparseness than HPB(or, indeed, HD-HPB).Comparing the performance of SAMT-HPB withHD-HPB, shows that our head-driven non-terminalrefining approach consistently outperforms theSAMT-style approach on an extensive set of ex-periments (for each test set p < 0.01), indicatingthat head information is more effective than (par-tial) CFG categories.
To make the comparison fair,it is important to note that our implementation ofsource-side SAMT-HPB includes the same sophis-ticated non-terminal re-ordering NRR rules as HD-HPB (Section 2.2 ).
Thus the performance differ-ences reported here are not due to different reorder-ing capabilities, but to the discriminative impact ofthe head information in HD-HPB over SAMT-styleannotation.
Taking lianming zhichi in Figure 2 as anexample, HD-HPB labels the span VV, as lianmingis dominated by zhichi, effecively ignoring lianmingin the translation rule, while the SAMT label isADVP:AD+VV6 which is more susceptible to datasparsity (Table 2 and Table 4).
In addition, SAMTresorts to X if a text span fails to satisify pre-definedcategories.
Examining initial phrases extracted fromthe SAMT training data shows that 28% of them arelabeled as X.
Finally, for Chinese syntactic analy-6The constituency structure for lianming zhichi is (VP(ADVP (AD lianming)) (VP (VV zhichi) ...)).237System Total Rules MT 03 MT 04 MT 05 MT 06 Avg.HPB 39.6M 2.8M 4.7M 3.3M 3.0M 3.4MHD-HPB 59.5/0.6M 1.9/0.1M 3.4/0.2M 2.3/0.2M 2.0/0.1M 2.4/0.2MSAMT-HPB 70.1/0.4M 2.2/0.2M 4.0/0.2M 2.7/0.2M 2.3/0.2M 2.8/0.2MTable 2: Rule table sizes of different models trained on small data.
Note: 1) SAMT-HPB indicates our HD-HPB modelwith the non-terminal scheme of Zollmann and Venugopal (2006); 2) For HD-HPB and SAMT-HPB, the rule sizesseparated by / indicate HD-HRs and NRRs, respectively; 2) Except for ?Total Rules?, the figures correspond to rulesfiltered on the corresponding test set.SystemMT 03 MT 04 MT 05 MT 06 Avg.TimeNIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEUMoses HPB 7.377 29.67 8.209 33.60 7.571 29.49 6.773 28.90 7.483 30.42 NAHPB 8.137 29.75 9.050 34.06 8.264 30.09 7.788 28.64 8.310 30.64 11.0HD-HPB 8.308 31.01** 9.211 35.11** 8.426 31.57** 7.930 30.15** 8.469 31.96 15.1SAMT-HPB 7.886 29.14* 8.703 33.32** 7.961 29.49* 7.307 28.41 7.964 30.09 17.3HD-HR+Glue 7.966 29.51 8.826 33.68 8.116 29.84 7.474 28.51 8.095 30.39 5.4Table 3: NIST and BLEU (%) scores of different models trained on small data.
Note: 1) HD-HR+Glue indicates ourHD-HPB model replacing NRRs with glue rules; 2) Significance tests for Moses HPB, HD-HPB, SAMT-HPB andHD-HR+Glue are done against HPB.System Total Rules MT 03 MT 04 MT 05 MT 06 Avg.HPB 206.8M 11.3M 17.6M 12.9M 10.4M 13.0MHD-HPB 318.6/2.3M 7.3/0.3M 12.2/0.4M 8.5/0.3M 6.7/0.2M 8.7/0.3MSAMT-HPB 371.0/1.1M 8.6/0.3M 14.3/0.4M 10.1/0.3M 7.9/0.3M 10.2/0.3MTable 4: Rule table sizes of different models trained on large data.SystemMT 03 MT 04 MT 05 MT 06 Avg.TimeNIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEUMoses HPB 7.914 32.94* 8.429 35.16 7.962 32.18 6.483 29.88* 7.697 32.54 NAHPB 8.583 33.59 9.114 35.39 8.465 32.20 7.532 30.60 8.423 32.95 13.7HD-HPB 8.885 35.50** 9.494 37.61** 8.871 34.56** 7.839 31.78** 8.772 34.86 16.6SAMT-HPB 8.644 34.07 9.245 36.52** 8.618 32.90* 7.543 30.66 8.493 33.54 19.1HD-HR+Glue 8.831 34.58** 9.435 36.55** 8.821 33.84** 7.863 31.06 8.737 34.01 6.7Table 5: NIST and BLEU (%) scores of different models trained on large data.
Note: System labels and significancetesting as in Table 3.238sis, dependency structure is more reliable than con-stituency structure.
Moreover, SAMT-HPB takesmore time in decoding than HD-HPB due to largerphrase tables.5.5 Discussion5.5.1 Individual Contribution of HD-HRs andNRRsExamining translation output shows that on aver-age each sentence employs 16.6/5.2 HD-HRs/NRRsin our HD-HPB model, compared to 15.9/3.6 hier-archical rules/glue rules in Chiang?s model, provid-ing further indication of the importance of NRRs intranslation.
In order to separate out the individualcontributions of the novel HD-HRs and NRRs, wecarry out an additional experiment (HD-HR+Glue)using HD-HRs with monotonic glue rules only (ad-justed to refined rule labels, but effectively switchingoff the extra reordering power of full NRRs) bothon the small and the large datasets, with interest-ing results: Table 3 (HD-HR+Glue) shows that forthe small training set most of the improvement ofour full HD-HPB model comes from the NRRs, asRR+Glue performs on the same level as Chiang?soriginal and Moses HPB (the differences are notstatistically significant), perhaps indicating sparse-ness for the refined HD-HRs given the small train-ing set.
Table 5 shows that for the large trainingset, HD-HRs come into their own: on average morethan half of the improvement over HPB (Chiang andMoses) comes from the refined HD-HRs, the restfrom NRRs.It is not surprising that compared to the othersHD-HR+Glue takes much less time in decoding.This is due to the fact that 1) compared to HPB, therefined translation rule patterns on the source sidehave fewer entries in phrase table; 2) compared toHD-HPB, HD-HR+Glue switches off the extra re-ordering of NRRs.
The decoding time for HD-HPBand HD-HR+Glue suggests that NRRs are more thandoubling the time required to decode.5.5.2 Different Head Label SetsExamining initial phrases extracted from the largesize training data shows that there are 63K typesof refined non-terminals with respect to 33 types ofPOS tags.
Considering the sparseness in translationrules caused by this comparatively detained POS tagset, we carry out an experiment with a reduced setof non-terminal types by using a less granular POStag set (C-HPB).
Moreover, due to the fact that con-catenation of POS tags of heads mostly captures in-ternal structure of a text span, it is interesting to ex-amine the effect of other syntactic labels, in partic-ular dependency labels, to try to better capture theimpact of the external context on the text span.
Tothis end, we replace the POS tag of head with itsincoming dependency label (DL-HPB), or the com-bination of (the original fine-grained) POS tag andits dependency label (POS-DL-HPB).
For C-HPBwe use the coarse POS tag set obtained by group-ing the 33 types of Chinese POS tags into 11 typesfollowing Xia (2000).
For example, we generalizeall verbal tags (e.g., VA, VC, VE, and VV ) and allnominal tags (e.g., NR, NT, and NN) into Verb andNoun, respectively.
We use the dependency labelsin Penn2Malt which defines 9 types of dependencylabels for Chinese, including AMOD, DEP, NMOD,P, PMOD, ROOT, SBAR, VC, and VMOD.7Table 6 shows the results trained on large data.Although the number of non-terminal types de-creased sharply from 63K to 3K, using the coarsePOS tag set in C-HPB surprisingly lowers the per-formance with 1.1 BLEU scores on average (e.g.,33.75 vs. 34.86), indicating that grouping POStags using simple linguistic rules is inappropriate forHD-HPB.
We still believe that this initial negativefinding should be supplemented by future work ongroupping POS tags using machine learning tech-niques considering contextual information.Table 6 also shows that replacing POS tagsof heads with their dependency labels (DL-HPB)substantially lowers the average performance from34.86 on BLEU score to 32.54, probably due tothe very coarse granularity of the dependency la-bels used.
In addition, replacing non-terminal labelwith more refined tags (e.g., combination of originalPOS tag and dependency label) also lowers trans-lation performance (POS-DL-HPB).
Further experi-ments with more fine-grained dependency labels arerequired.7Some other types of dependency labels (e.g., SUB, OBJ)are generated from function tags which are not available in ourautomatic parse trees.239VV-NR1 duiyiNN2VV?,X?X1X2againstIraq(b)zhichimeiguo1dui yi celie2,supportAmerica?s 1stand 2againstIraqVV-NR?zhichimeiguo,X?supportAmerica?s(a)zhichimeiguo,supportAmerica?sFigure 3: Examples of pharse pairs and their head-driventranslation rules with dependency relation, regarding Fig-ure 2System MT 03 MT 04 MT 05 MT 06 Avg.HPB 33.59 35.39 32.20 30.60 32.95HD-HPB 35.50 37.61 34.56 31.78 34.86C-HPB 34.10 36.43 33.46 31.00 33.75DL-HPB 32.81 35.19 32.27 29.89 32.54POS-DL-HPB 34.08 36.78 33.14 30.43 33.61HD-DEP-HPB 35.48 38.17 34.81 32.38 35.21Table 6: BLEU (%) scores of models trained on largedata.5.5.3 Encoding Full Dependency Relations inTranslation RuleXie et al (2011) present a dependency-to-stringtranslation model with a complete dependency struc-ture on the source side and a moderate average im-provement of 0.46 BLEU over the HPB baseline.
Bycontrast, in our HD-HPB approach, dependency in-formation is used to identify heads in the strings cov-ered by non-terminals in HD-HR rules, and to refinenon-terminal labels accordingly, with an average im-provement of 1.91 in BLEU over the HPB baseline(when trained on the large data).
This raises thequestion whether and to what extent complete (un-labeled) dependency information between the stringand the heads in head-labeled non-terminal parts ofthe source side of SCFGs in HD-HPB can furtherimprove results.Given the source side of a translation rule (ei-ther HD-HR or NRR), say Ps ?
s1 .
.
.
sm (whereeach si is either a terminal or a head POS in a re-fined non-terminal), in a further set of experimentswe keep the full unlabeled dependency relations be-tween s1 .
.
.
sm so as to capture contextual syntacticinformation in translation rules.
For example, on thesource side of Figure 3 (b) where VV-NR maps intowords zhichi and meiguo while NN maps into wordcelie, we keep the full unlabeled dependency rela-tions among words {zhichi, meiguo, dui, yi, celie}.HD-DEP-HPB (Table 6) augments translation rulesin HD-HPB with full dependency relations on thesource side.
This further boosts the performanceby 0.35 BLEU scores on average over HD-HPB andoutperforms the HPB baseline by 2.26 BLEU scoreson average.5.5.4 Error AnalysisWe carried out a manual error analysis compar-ing the outputs of our HD-HPB system with thoseof Chiang?s (both trained on the large data).
We ob-serve that improved BLEU score often correspond tobetter topological ordering of phrases in the hierar-chical structure of the source side, with a direct im-pact on which words in a source sentence should betranslated first, and which later.
As ungrammaticaltranslations are often due to inappropriate topologi-cal orderings of phrases in the hierarchical structure,guiding the translation through appropriate topolog-ical ordering should improve translation quality.
Togive an example, consider the following input sen-tence from the 04 NIST MT test data and its twotranslation results:?
Input: ?
?0 ?
?1 ?2 ?3 ?
?4 ???
?5 ?
?6 ?7 ?
?8 ??9?
HPB: chinese delegation to us dollar purchase ofmore high technology equipment?
HD-HPB: chinese delegation went to the unitedstates to buy more us high - tech equipmentFigure 4 demonstrates the topological orderingsin the two hierarchical structures.
In addition to dis-fluency and some grammar errors (e.g., a main verbis missing), the basic HPB system also makes mis-takes in reordering (e.g., ?
?4 ???
?5 ?
?6translated as dollar purchase of more).
The poortranslation quality, unsurprisingly, is caused by in-appropriate topological ordering (Figure 4(a)).
Bycomparison, the topological ordering reflected in thehierarchical structure of our HD-HPB model bet-ter respects syntactic structure (Figure 4(b)).
Let240???0????1???
2???
3????4??????5????6???
7????8???
?9?X [4?4]?X [6?6]?X [4?6]?X [3?7]?X [3?8]?X [2?9]?X [1?9]?X [0?9]?S [0?9]?(a).?Topological?orderings?of?phrases?in?Chiang?s?HPB.?(b).?Improved?topological?orderings?of?phrases?in?HD?HPB.1.
?S [0?9]??
?X [0?9],????????????????????X[0?9]?2.
?X [0?9]?????
[0?0]?X [1?9],?????????????????????chinese?X[1?9]?3.
?X [1?9]?????
[1?1]?X [2?9],????????????????????delegation?X[2?9]?4.
?X [2?9]????
[2?2]?X[3?8]???[9?9],???????????????????
?to?X [3?8]?equipment?5.
?X [3?8]??
?X [3?7]???,?
?X [3?7]?technology?6.
?X [3?7]????
[3?3]?X[4?6]??
[7?7],?
?us?X [4?6]?high?7.
?X [4?6]??
?X [4?4]?????
[5?5]?X [6?6],?X [6?6]?X [4?4]?of?more?8.
?X [4?4]?????[4?4],??purchase?9.
?X [6?6]?????[6?6],??dollar1.
?VV[0?9]??
?NN [0?1]?VV [2?9],???????????????X??
?X [0?1]?X[2?9]?2.?NN[0?1]?????
[0?0]?NN [1?1],????????????????X??
?chinese?X [1?1]??3.?NN[1?1]?????[1?1],????????????????X???delegation?4.?VV[2?9]????
[2?2]??[3?3]?VV[4?9],????????X??
?went?to?the?united?states?to?X [4?9]?5.?VV[4?9]???VV?M[4?6]??[7?7]???
[8?8]?NN [9?9],?????????????X??
?X [4?6]?high??tech?X[9?9]??6.?VV?M[4?6]?????[4?4]?M[5?6],???????????????????X???buy?X[5?6]??7.?M[5?6]???CD[5?5]?M[6?6],??????????????X??
?X [5?5]?X[6?6]?8.?CD[5?5]???????[5?5],???????????????X???more?9.?M[6?6]?????[6?6],??????????????X???us?10.
?NN [9?9]?????[9?9],???????????????X???equipment?root??
NR 0??
NN 1?
VV 2?
NR 3?
?VV 4????
CD 5??
M 6?
JJ 7??
NN 8??
NN 9CD[5-5]M[6-6]M[5-6]VV-M[4-6]VV[4-9]VV[2-9]NNVV[0-9][0-1]NNNN[1-1][9-9]Figure 4: An example Chinese sentence and its two hierarchical structures.
Note: subscript [i-j] represents spanningfrom word i to word j on the source side.us refer to the HD-HPB hierarchical structure onthe source side as translation parse tree and to thetreebank-based parser derived tree as syntactic parsetree from which we obtain unlabeled dependencystructure.
Examining the translation parse trees ofour HD-HPB model shows that phrases with 1/2/3/4heads account for 64.9%/23.1%/8.8%/3.2%, respec-tively.
Compared to 37.9% of the phrases in thetranslation parse trees of the HPB model, 43.2% ofthe phrases of our HD-HPB model correspond to alinguistically motivated constituent in the syntacticparse tree with exactly the same text span.
In sum,therefore, instead of simply enforcing hard linguisticconstraints imposed by a full syntactic parse struc-ture, our model opts for a successful mix of linguis-tically motivated and combinatorial (matching sub-phrases in HPB) constraints.6 ConclusionIn this paper, we present a head-driven hierarchi-cal phrase-based translation model, which adoptshead information (derived through unlabeled depen-dency analysis) in the definition of non-terminalsto better differentiate among translation rules.
Inaddition, improved and better integrated reorder-ing rules allow better reordering between consecu-tive non-terminals through exploration of a largersearch space in the derivation.
Our model main-tains the strengths of Chiang?s HPB model while atthe same time it addresses the over-generation prob-lem caused by using a uniform non-terminal symbol.Experimental results on Chinese-English translationacross a wide range of training and test sets demon-strate significant and consistent improvements of ourHD-HPB model over Chiang?s HPB model as wellas over a source side version of the SAMT-stylemodel.Currently, we only consider head information in aword sequence.
In the future work, we will exploitmore syntactic and semantic information to system-atically and automatically define the inventory ofnon-terminals (in source and target).
For example,for a non-terminal symbol VV, we believe it willbenefit translation if we use fine-grained dependencylabels (subject, object etc.)
used to link it to its gov-erning head elsewhere in the translation rule.AcknowledgmentsThis work was supported by Science Foundation Ire-land (Grant No.
07/CE/I1142) as part of the Cen-tre for Next Generation Localisation (www.cngl.ie)at Dublin City University.
It was also partiallysupported by Project 90920004 under the NationalNatural Science Foundation of China and Project2012AA011102 under the ?863?
National High-Tech Research and Development of China.
Wethank the reviewers for their insightful comments.ReferencesHala Almaghout, Jie Jiang, and Andy Way.
2011.
CCGcontextual labels in hierarchical phrase-based SMT.
InProceedings of EAMT 2011, pages 281?288.241Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Proceedings of ACL 2007, pages33?40.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL 2000, pages 132?139.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL 2005, pages 263?270.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics, 29(4):589?637.Yang Gao, Philipp Koehn, and Alexandra Birch.
2011.Soft dependency constraints for reordering in hierar-chical phrase-based translation.
In Proceedings ofEMNLP 2011, pages 857?868.Zhongjun He, Yao Meng, and Hao Yu.
2010.
Maxi-mum entropy based phrase reordering for hierarchicalphrase-based translation.
In Proceedings of EMNLP2010, pages 555?563.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of IWPT 2005, pages 53?64.Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.2010.
Soft syntactic constraints for hierarchicalphrase-based translation using latent syntactic distri-butions.
In Proceedings of EMNLP 2010, pages 138?147.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of NAACL 2003, pages 48?54.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP 2004, pages 388?395.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrased-based translation.In Proceedings of ACL-HLT 2008, pages 1003?1011.Markos Mylonakis and Khalil Sima?an.
2011.
Learninghierarchical translation structure with linguistic anno-tations.
In Proceedings of ACL-HLT 2011, pages 642?652.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of ACL2000, pages 440?447.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statisti-cal machine translation.
In Proceedings of ACL 2002,pages 295?302.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL2003, pages 160?167.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACL2007, pages 404?411.Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2009.
Effective use of linguis-tic and contextual information for statistical machinetranslation.
In Proceedings of EMNLP 2009, pages72?80.Dekai Wu.
1996.
A polynomial-time algorithm for sta-tistical machine translation.
In Proceedings of ACL1996, pages 152?158.Fei Xia.
2000.
The part-of-speech tagging guidelines forthe Penn Chinese Treebank (3.0).
Technical ReportIRCS-00-07, University of Pennsylvania Institute forResearch in Cognitive Science Technical.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A noveldependency-to-string model for statistical machinetranslation.
In Proceedings of EMNLP 2011, pages216?226.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of NAACL 2006 - Workshop on StatisticalMachine Translation, pages 138?141.Andreas Zollmann and Stephan Vogel.
2011.
A word-class approach to labeling PSCFG rules for machinetranslation.
In Proceedings of ACL-HLT 2011, pages1?11.242
