Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 388?398,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsExploiting Conversation Structure in Unsupervised Topic Segmentation forEmailsShafiq Joty and Giuseppe Carenini and Gabriel Murray and Raymond T. Ng{rjoty, carenini, gabrielm, rng}@cs.ubc.caDepartment of Computer ScienceUniversity of British ColumbiaVancouver, BC, V6T 1Z4, CanadaAbstractThis work concerns automatic topic segmen-tation of email conversations.
We present acorpus of email threads manually annotatedwith topics, and evaluate annotator reliabil-ity.
To our knowledge, this is the first suchemail corpus.
We show how the existing topicsegmentation models (i.e., Lexical Chain Seg-menter (LCSeg) and Latent Dirichlet Alloca-tion (LDA)) which are solely based on lex-ical information, can be applied to emails.By pointing out where these methods fail andwhat any desired model should consider, wepropose two novel extensions of the modelsthat not only use lexical information but alsoexploit finer level conversation structure in aprincipled way.
Empirical evaluation showsthat LCSeg is a better model than LDA forsegmenting an email thread into topical clus-ters and incorporating conversation structureinto these models improves the performancesignificantly.1 IntroductionWith the ever increasing popularity of emails andweb technologies, it is very common for people todiscuss issues, events, agendas or tasks by email.Effective processing of the email contents can beof great strategic value.
In this paper, we studythe problem of topic segmentation for emails, i.e.,grouping the sentences of an email thread into aset of coherent topical clusters.
Adapting the stan-dard definition of topic (Galley et al, 2003) to con-versations/emails, we consider a topic is somethingabout which the participant(s) discuss or argue orexpress their opinions.
For example, in the emailthread shown in Figure 1, according to the major-ity of our annotators, participants discuss three top-ics (e.g., ?telecon cancellation?, ?TAG document?,and ?responding to I18N?).
Multiple topics seem tooccur naturally in social interactions, whether syn-chronous (e.g., chats, meetings) or asynchronous(e.g., emails, blogs) conversations.
In multi-partychat (Elsner and Charniak, 2008) report an averageof 2.75 discussions active at a time.
In our email cor-pus, we found an average of 2.5 topics per thread.Topic segmentation is often considered a pre-requisite for other higher-level conversation analy-sis and applications of the extracted structure arebroad, encompassing: summarization (Harabagiuand Lacatusu, 2005), information extraction and or-dering (Allan, 2002), information retrieval (Dias etal., 2007), and intelligent user interfaces (Dredze etal., 2008).
While extensive research has been con-ducted in topic segmentation for monologues (e.g.,(Malioutov and Barzilay, 2006), (Choi et al, 2001))and synchronous dialogs (e.g., (Galley et al, 2003),(Hsueh et al, 2006)), none has studied the problemof segmenting asynchronous multi-party conversa-tions (e.g., email).
Therefore, there is no reliable an-notation scheme, no standard corpus, and no agreed-upon metrics available.
Also, it is our key hypothe-sis that, because of its asynchronous nature, and theuse of quotation (Crystal, 2001), topics in an emailthread often do not change in a sequential way.
As aresult, we do not expect models which have provedsuccessful in monologue or dialog to be as effectivewhen they are applied to email conversations.Our contributions in this paper aim to remedy388these problems.
First, we present an email corpusannotated with topics and evaluate annotator agree-ment.
Second, we adopt a set of metrics to mea-sure the local and global structural similarity be-tween two annotations from the work on multi-partychat disentanglement (Elsner and Charniak, 2008).Third, we show how the two state-of-the-art topicsegmentation methods (i.e., LCSeg and LDA) whichare solely based on lexical information and makestrong assumptions on the resulting topic models,can be effectively applied to emails, by having themto consider, in a principled way, a finer level struc-ture of the underlying conversations.
Experimen-tal results show that both LCSeg and LDA benefitwhen they are extended to consider the conversa-tional structure.
When comparing the two methods,we found that LCSeg is better than LDA and thisadvantage is preserved when they are extended toincorporate conversational structure.2 Related WorkThree research areas are directly related to our study:a) text segmentation models, b) probabilistic topicmodels, and c) extracting and representing the con-versation structure of emails.Topic segmentation has been extensively studiedboth for monologues and dialogs.
(Malioutov andBarzilay, 2006) uses the minimum cut model to seg-ment spoken lectures (i.e., monologue).
They form aweighted undirected graph where the vertices repre-sent sentences and the weighted links represent thesimilarity between sentences.
Then the segmenta-tion problem can be solved as a graph partitioningproblem, where the assumption is that the sentencesin a segment should be similar, while sentences indifferent segments should be dissimilar.
They op-timize the ?normalized cut?
criterion to extract thesegments.
In general, the minimization of the nor-malized cut criterion is NP-complete.
However, thelinearity constraint on text segmentation for mono-logue allows them to find an exact solution in poly-nomial time.
In our extension of LCSeg, we usea similar method to consolidate different segments;however, in our case the linearity constraint is ab-sent.
Therefore, we approximate the optimal solu-tion by spectral clustering (Shi and Malik, 2000).Moving to the task of segmenting dialogs, (Galleyet al, 2003) first proposed the lexical chain basedunsupervised segmenter (LCSeg) and a supervisedsegmenter for segmenting meeting transcripts.
Theirsupervised approach uses C4.5 and C4.5 rules binaryclassifiers with lexical and conversational features(e.g., cue phrase, overlap, speaker, silence, and lex-ical cohesion function).
Their supervised approachperforms significantly better than LCSeg.
(Hsuehet al, 2006) follow the same approaches as (Galleyet al, 2003) on both manual transcripts and ASRoutput of meetings.
They perform segmentation atboth coarse (topic) and fine (subtopic) levels.
Forthe topic level, they achieve similar results as (Gal-ley et al, 2003), with the supervised approach out-performing LCSeg.
However, for the subtopic level,LCSeg performs significantly better than the super-vised one.
In our work, we show how LCSeg per-forms when applied to the temporal ordering of theemails in a thread.
We also propose its extension toleverage the finer conversation structure of emails.The probabilistic generative topic models, suchas LDA and its variants (e.g., (Blei et al, 2003),(Steyvers and Griffiths, 2007)), have proven to besuccessful for topic segmentation in both mono-logue (e.g., (Chen et al, 2009)) and dialog (e.g.,(Georgescul et al, 2008)).
(Purver et al, 2006) usesa variant of LDA for the tasks of segmenting meet-ing transcripts and extracting the associated topiclabels.
However, their approach for segmentationdoes not perform better than LCSeg.
In our work,we show how the general LDA performs when ap-plied to email conversations and describe how it canbe extended to exploit the conversation structure ofemails.Several approaches have been proposed to cap-ture an email conversation .
Email programs (e.g.,Gmail, Yahoomail) group emails into threads usingheaders.
However, our annotations show that top-ics change at a finer level of granularity than emails.
(Carenini et al, 2007) present a method to capture anemail conversation at the finer level by analyzing theembedded quotations in emails.
A fragment quota-tion graph (FQG) is generated, which is shown to bebeneficial for email summarization.
In this paper, weshow that topic segmentation models can also bene-fit significantly from this fine conversation structureof email threads.3893 Corpus and Evaluation MetricsThere are no publicly available email corpora anno-tated with topics.
Therefore, the first step was todevelop our own corpus.
We have annotated theBC3 email corpus (Ulrich et al, 2008) with top-ics1.
The BC3 corpus, previously annotated withsentence level speech acts, meta sentence, subjectiv-ity, extractive and abstractive summaries, is one of agrowing number of corpora being used for email re-search.
The corpus contains 40 email threads fromthe W3C corpus2.
It has 3222 sentences and an av-erage of 5 emails per thread.3.1 Topic AnnotationTopic segmentation in general is a nontrivial andsubjective task (Hsueh et al, 2006).
The conver-sation phenomenon called ?Schism?
makes it evenmore challenging for conversations.
In schism anew conversation takes birth from an existing one,not necessarily because of a topic shift but becausesome participants refocus their attention onto eachother, and away from whoever held the floor in theparent conversation and the annotators can disagreeon the birth of a new topic (Aoki et al, 2006).
In theexample email thread shown in Figure 1, a schismtakes place when people discuss about ?respondingto I18N?.
All the annotators do not agree on the factthat the topic about ?responding to I18N?
swervesfrom the one about ?TAG document?.
The annota-tors can disagree on the number of topics (i.e., someare specific and some are general), and on the topicassignment of the sentences3.
To properly design aneffective annotation manual and procedure we per-formed a two-phase pilot study before carrying outthe actual annotation.
For the pilot study we pickedfive email threads randomly from the corpus.
In thefirst phase of the pilot study we selected five uni-versity graduate students to do the annotation.
Wethen revised our instruction manual based on theirfeedback and the source of disagreement found.
In1The BC3 corpus had already been annotated for email sum-marization, speech act recognition and subjectivity detection.This new annotation with topics will be also made publiclyavailable at http://www.cs.ubc.ca/labs/lci/bc3.html2http://research.microsoft.com/en-us/um/people/nickcr/w3c-summary.html3The annotators also disagree on the topic labels, howeverin this work we are not interested in finding the topic labels.the second phase we tested with a university postdocdoing the annotation.For the actual annotation we selected three com-puter science graduates who are also native speakersof English.
They annotated 39 threads of the BC3corpus4.
On an average they took seven hours to an-notate the whole dataset.BC3 contains three human written abstract sum-maries for each email thread.
With each email threadthe annotators were also given an associated humanwritten summary to give a brief overview of the cor-responding conversation.
The task of finding topicswas carried out in two phases.
In the first phase, theannotators read the conversation and the associatedsummary and list the topics discussed.
They spec-ify the topics by a short description (e.g., ?meetingagenda?, ?location and schedule?)
which provides ahigh-level overview of the topic.
The target numberof topics and the topic labels were not given in ad-vance and they were instructed to find as many top-ics as needed to convey the overall content structureof the conversation.In the second phase the annotators identify themost appropriate topic for each sentence.
However,if a sentence covers more than one topic, they wereasked to label it with all the relevant topics accordingto their order of relevance.
If they find any sentencethat does not fit into any topic, they are told to labelthose as the predefined topic ?OFF-TOPIC?.
Wher-ever appropriate they were also asked to make use oftwo other predefined topics: ?INTRO?
and ?END?.INTRO (e.g., ?hi?, ?hello?)
signifies the section (usu-ally at the beginning) of an email that people use tobegin their email.
Likewise, END (e.g., ?Cheers?,?Best?)
signifies the section (usually at the end) thatpeople use to end their email.
The annotators car-ried out the task on paper.
We created the hierar-chical thread view (?reply to?
relation) using ?TAB?s(indentation) and each participant?s name is printedin a different color as in Gmail.Table 1 shows some basic statistics computed onthe three annotations of the 39 email threads5.
On4The annotators in the pilot and in the actual study were dif-ferent so we could reuse the threads used in pilot study.
How-ever, one thread on which the pilot annotators agree fully, wasused as an example in the instruction manual.
This gives 39threads left for the actual study.5We got 100% agreement on the two predefined topics ?IN-390average we have 26.3 sentences and 2.5 topics perthread.
A topic contains an average of 12.6 sen-tences.
The average number of topics active at atime is 1.4.
The average entropy is 0.94 and cor-responds (as described in detail in the next section)to the granularity of the annotation.
These statistics(number of topics and topic density) indicate that thedataset is suitable for topic segmentation.Mean Max MinNumber of sentences 26.3 55 13Number of topics 2.5 7 1Avg.
topic length 12.6 35 3Avg.
topic density 1.4 3.1 1Entropy 0.94 2.7 0Table 1: Corpus statistics of human annotationsMetrics Mean Max Min1-to-1 0.804 1 0.31lock 0.831 1 0.43m-to-1 0.949 1 0.61Table 2: Annotator agreement in the scale of 0 to 13.2 Evaluation MetricsIn this section we describe the metrics used to com-pare different human annotations and system?s out-put.
As different annotations (or system?s output)can group sentences in different number of clusters,metrics widely used in classification, such as the ?statistic, are not applicable.
Again, our problem oftopic segmentation for emails is not sequential in na-ture.
Therefore, the standard metrics widely used insequential topic segmentation for monologues anddialogs, such as Pk and WindowDiff(WD), arealso not applicable.
We adopt the more appropri-ate metrics 1-to-1, lock and m-to-1, introduced re-cently by (Elsner and Charniak, 2008).
The 1-to-1metric measures the global similarity between twoannotations.
It pairs up the clusters from the twoannotations in a way that maximizes (globally) thetotal overlap and then reports the percentage of over-lap.
lock measures the local agreement within a con-TRO?
and ?END?.
In all our computation (i.e., statistics, agree-ment, system?s input) we excluded the sentences marked as ei-ther ?INTRO?
or ?END?text of k sentences.
To compute the loc3 metric forthe m-th sentence in the two annotations, we con-sider the previous 3 sentences: m-1, m-2 and m-3,and mark them as either ?same?
or ?different?
de-pending on their topic assignment.
The loc3 scorebetween two annotations is the mean agreement onthese ?same?
or ?different?
judgments, averaged overall sentences.
We report the agreement found in 1-to-1 and lock in Table 2.
In both of the metrics weget high agreement, though the local agreement (av-erage of 83%) is little higher than the global agree-ment (average of 80%).If we consider the topic of a randomly picked sen-tence as a random variable then its entropy measuresthe level of detail in an annotation.
If the topics areevenly distributed then the uncertainty (i.e., entropy)is higher.
It also increases with the increase of thenumber of topics.
Therefore, it is a measure of howspecific an annotator is and in our dataset it variesfrom 0 6 to 2.7.
To measure how much the annota-tors agree on the general structure we use the m-to-1metric.
It maps each of the source clusters to thesingle target cluster with which it gets the highestoverlap, then computes the total percentage of over-lap.
This metric is asymmetrical and not a measureto be optimized7, but it gives us some intuition aboutspecificity (Elsner and Charniak, 2008).
If one an-notator divides a cluster into two clusters then, them-to-1 metric from fine to coarse is 1.
In our corpusby mapping from fine to coarse we get an m-to-1average of 0.949.4 Topic Segmentation ModelsDeveloping automatic tools for segmenting an emailthread is challenging.
The example email thread inFigure 1 demonstrates why.
We use different col-ors and fonts to represent sentences of different top-ics8.
One can notice that email conversations aredifferent from written monologues (e.g., newspaper)and dialogs (e.g., meeting, chat) in various ways.As a communication media Email is distributed (un-like face to face meeting) and asynchronous (unlike60 uncertainty happens when there is only one topic found7hence we do not use it to compare our models.82 of the 3 annotators agree on this segmentation.
Green rep-resents topic 1 (?telecon cancellation?
), orange indicates topic 2(?TAG document?)
and magenta represents topic 3 (?respondingto I18N?
)391chat), meaning that different people from differentlocations can collaborate at different times.
There-fore, topics in an email thread may not change insequential way.
In the example, we see that topic 1(i.e., ?telecon cancellation?)
is revisited after somegaps.The headers (i.e., subjects) do not convey muchinformation and are often misleading.
In the exam-ple thread, participants use the same subject (i.e.,20030220 telecon) but they talk about ?respondingto I18N?
and ?TAG document?
instead of ?teleconcancellation?.
Writing style varies among partici-pants, and many people tend to use informal, shortand ungrammatical sentences.
These properties ofemail limit the application of techniques that havebeen successful in monologues and dialogues.LDA and LCSeg are the two state-of-the-art mod-els for topic segmentation of multi-party conversa-tion (e.g., (Galley et al, 2003), (Hsueh et al, 2006),(Georgescul et al, 2008)).
In this section, at first wedescribe how the existing models of topic segmen-tation can be applied to emails.
We then point outwhere these methods fail and propose extensions ofthese basic models for email conversations.4.1 Latent Dirichlet Allocation (LDA)Our first model is the probabilistic LDA model(Steyvers and Griffiths, 2007).
This model relies onthe fundamental idea that documents are mixtures oftopics, and a topic is a multinomial distribution overwords.
The generative topic model specifies the fol-lowing distribution over words within a document:P (wi) =T?j=1P (wi|zi = j)P (zi = j)Where T is the number of topics.
P (wi|zi = j) isthe probability of word wi under topic j and P (zi =j) is the probability that jth topic was sampled forthe ith word token.
We refer the multinomial dis-tributions ?
(j) = P (w|zi = j) and ?
(d) = P (z)as topic-word distribution and document-topic dis-tribution respectively.
(Blei et al, 2003) refined thisbasic model by placing a Dirichlet (?)
prior on ?.
(Griffiths and Steyvers, 2003) further refined it byplacing a Dirichlet (?)
prior on ?.
The inferenceproblem is to find ?
and ?
given a document set.Variational EM has been applied to estimate thesetwo parameters directly.
Instead of estimating ?
and?, one can also directly estimate the posterior distri-bution over z = P (zi = j|wi) (topic assignmentsfor words).
One efficient estimation technique usesGibbs sampling to estimate this distribution.This framework can be directly applied to anemail thread by considering each email as a doc-ument.
Using LDA we get z = P (zi = j|wi)(i.e., topic assignments for words).
By assuming thewords in a sentence occur independently we can esti-mate the topic assignments for sentences as follows:P (zi = j|sk) =?wi?skP (zi = j|wi)where, sk is the kth sentence for which we canassign the topic by: j?
= argmaxjP (zi = j|sk).4.2 Lexical Chain Segmenter (LCSeg)Our second model is the lexical chain based seg-menter LCSeg, (Galley et al, 2003).
LCSeg as-sumes that topic shifts are likely to occur wherestrong term repetitions start and end9.
LCSeg at firstcomputes ?lexical chains?
for each non-stop wordbased on word repetitions.
It then ranks the chainsaccording to two measures: ?number of words in thechain?
and ?compactness of the chain?.
The morecompact (in terms of number of sentences) and themore populated chains get higher scores.The algorithm then works with two adjacent anal-ysis windows, each of a fixed size k which is em-pirically determined.
For each sentence boundary,LCSeg computes the cosine similarity (or lexical co-hesion function) at the transition between the twowindows.
Low similarity indicates low lexical cohe-sion, and a sharp change signals a high probabilityof an actual topic boundary.
This method is similarto TextTiling (Hearst, 1997) except that the similar-ity is computed based on the scores of the ?lexicalchains?
instead of ?term counts?.
In order to applyLCSeg on email threads we arrange the emails basedon their temporal relation (i.e., arrival time) and ap-ply the LCSeg algorithm to get the topic boundaries.9One can also consider other lexical semantic relations (e.g.,synonym, hypernym, hyponym) in lexical chaining.
However,Galley et al, (Galley et al, 2003) uses only repetition relationas previous research results (e.g., (Choi, 2000)) account onlyfor repetition.392From: Brian To: rdf core Subject: 20030220 telecon Date: Tue Feb 17 13:52:15	                                                  [a]   !"  #!From: Jeremy To: Brian Subject: Re: 20030220 telecon Date: Wed Feb 18 05:18:10	$% #!I think that means we will not formally respond to I18N on the charmod comments, shall I tell them          [d]that we do not intend to, but that the e-mail discussion has not shown any disagreement.e.g.
I have informed the RDF Core WG of your decisions, and no one has indicated unhappiness                [e]- however we have not formally discussed these issues;  and are not likely to.From: Brian To: Jeremy Subject: Re: 20030220 telecon Date: Wed Feb 18 13:16:21> I think that means we will not formally respond to I18N on the charmod comments, shall> I tell them that we do not intend to, but that the e-mail discussion has not shown any disagreement.Ah.
Is this a problem.
Have I understood correctly they are going through last call again anyway.
[f]> e.g.
I have informed the RDF Core WG of your decisions, and no one has indicated unhappiness> - however we have not formally discussed these issues; and are not likely to.When is the deadline?
I'm prepared to decide by email so we can formally respond by email.
[g]From: Pat To: Brian Subject: Re: 20030220 telecon Date: Wed Feb 18 16:56:26 	                            [h]$      &   '#From: Jeremy To: Brian Subject: Re: 20030220 telecon Date: Thu Feb 19 05:42:21> Ah.
Is this a problem.> Have I understood correctly they are going through last call again anyway.Yes - I could change my draft informal response to indicate that if we have any other formal                        [j]response it will be included in our LC review comments on their new documents.> When is the deadline?> I'm prepared to decide by email so we can formally respond by email.Two weeks from when I received the message ....i.e.
during Cannes                                                               [k]-I suspect that is also the real deadline, in that I imagine they want to make their final decisions atCannes.I am happy to draft a formal response that is pretty vacuous, for e-mail vote.
[l]From: Brian To: Pat Subject: Re: 20030220 telecon Date: Thu Feb 19 06:10:53 	      >Likewise, whether or not anyone else in the WG agrees with any of my own personal comments, ?
[m]!  "#  	 	"#$% & 		From: Brian To: JeremySubject: Re: 20030220 telecon Date: Thu Feb 19 10:06:57> I am happy to draft a formal response that is pretty vacuous, for e-mail vote.Please do.
[o]Figure 1: Sample thread from the BC3 corpus.
Each dif-ferent color/font indicates a different topic.
Right mostcolumn specifies the fragments (sec 4.4).Figure 2: Fragment Quotation Graph for emails4.3 Limitation of Existing ApproachesThe main limitation of the two models discussedabove is that they take the bag-of-words (BOW)assumption without considering the fact that anemail thread is a multi-party, asynchronous conver-sation10.
The only information relevant to LDA isterm frequency.
LCSeg considers both term fre-quency and how closely the terms occur in a docu-ment.
These models do not consider the word order,syntax and semantics.
However, several improve-ments of LDA over the BOW approach have beenproposed.
(Wallach, 2006) extends the model be-yond BOW by considering n-gram sequences.
(Grif-fiths et al, 2005) presents an extension of the topicmodel that is sensitive to word-order and automat-ically learns the syntactic as well as semantic fac-tors that guide word choice.
(Boyd-Graber and Blei,2010) describes another extension to consider syn-tax of the text.
As described earlier, one can alsoincorporate lexical semantics (i.e., synonym, hyper-nym, hyponym) into the LCSeg model.
However,we argue that these models are still inadequate forfinding topics in emails especially when topics areclosely related (e.g., ?extending the meeting?
and?scheduling the meeting?)
and distributional varia-tions are subtle.
To better identify the topics in anemail thread we need to consider the email specificconversation features (e.g., reply-to relation, usageof quotations).
As can be seen in the example (Fig-ure 1), people often use quotations to talk about thesame topic.
In fact in our corpus we found an av-erage quotation usage of 6.44 per thread.
Therefore,10though in LCSeg we provide minimal conversation struc-ture in the form of temporal relation between emails.393we need to leverage this useful information in a prin-cipled way to get the best out of our models.
Specif-ically, we need to capture the conversation structureat the fragment (quotation) level and to incorporatethis structure into our models.In the next section, we describe how one can cap-ture the conversation structure at the fragment levelin the form of Fragment Quotation Graph (hence-forth, FQG).
In Section 4.5 and 4.6 respectively, weshow how the LDA and LCSeg models can be ex-tended so that they take this conversation structureinto account for topic segmentation.4.4 Extracting Conversation StructureWe demonstrate how to build a FQG through the ex-ample email thread involving 7 emails shown in Fig-ure 1.
For convenience we do not show the real con-tent but abbreviate them as a sequence of fragments.In the first pass by processing the whole threadwe identify the new (i.e., quotation depth 0) andquoted (i.e., quotation depth > 0) fragments basedon the usage of quotation (>) marks.
For instance,email E3 contains two new fragments (f, g), andtwo quoted fragments (d, e) of depth 1.
E2 containsabc and de.
Then in the second step, we compare thefragments with each other and based on the overlapwe find the distinct fragments.
If necessary we splitthe fragments in this step.
For example, de in E2 isdivided into d and e distinct fragments when com-pared with the fragments of E3.
This process gives15 distinct fragments which constitute the verticesof the FQG.
In the third step, we compute the edges,which represent referential relations between frag-ments.
For simplicity we assume that any new frag-ment is a potential reply to its neighboring quotedfragments.
For example, for the fragments of E4 wecreate two edges from h ((h,a),(h,b)) and one edgefrom i ((i,b)).
We then remove the redundant edges.In E6 we found the edges (n,h), (n,a) and (n,m).
As(h,a) is already there we exclude (n,a).
The FQGwith all the redundant edges removed is shown at theright in Figure 2.
If an email does not contain quotesthen the fragments of that email are connected to thefragments of the source email to which it replies.The advantage of the FQG is that it captures theconversation at finer granularity level in contrast tothe structure found by the ?reply-to?
relation at theemail level, which would be merely a sequence fromE1 to E7 in this example.
Another advantage ofthis structure is that it allows us to find the ?hiddenfragments?.
Hidden fragments are quoted fragments(shaded fragment m in fig 2 which corresponds tothe fragment made bold in fig 1), whose originalemail is missing in the user?s inbox.
(Carenini etal., 2007) study this phenomenon and its impact onemail summarization in detail.4.5 Regularizing LDA with FQGThe main advantage of the probabilistic (Bayesian)models is that they allow us to incorporate multipleknowledge sources in a coherent way in the form ofpriors (or regularizer).
We want to regularize LDAin a way that will force two sentences in the same oradjacent fragments to fall in the same topical cluster.The first step forwards this aim is to regularize thetopic-word distribution with a word network suchthat two connected words get similar topic distribu-tions.
Then we can easily extend it to fragments.
Inthis section, at first we describe how one can regu-larize the LDA model with a word network, then weextend this by regularizing LDA with FQG.Assume we are given a word network as an undi-rected graph with nodes (V ) representing the wordsand the edges (E) representing the links betweenwords.
We want to regularize the LDA model suchthat two connected words u, v have similar topic-word distributions (i.e., ?
(u)j ?
?
(v)j for j = 1 .
.
.
T ).Note that the standard conjugate Dirichlet prior on?
is limited in that all words share a common vari-ance parameter, and are mutually independent ex-cept normalization constraint (Minka, 1999).
There-fore it does not allow us to encode this knowledge.Very recently, (Andrzejewski et al, 2009) showshow to encode ?must-link?
and ?cannot-link?
(be-tween words) into the LDA model by using a Dirich-let Forest prior.
We reimplemented this model; how-ever, we only use its capability of encoding ?must-links?.
Therefore, we just illustrate how to encode?must-links?
here.
Interested readers can see (An-drzejewski et al, 2009) for the method of encoding?cannot-links?.Must links such as (a, b), (b, c), or (x, y) in Fig-ure 3(A) can be encoded into the LDA model by us-ing a Dirichlet Tree (henceforth, DT) prior.
Like thetraditional Dirichlet, DT is also a conjugate to themultinomial but under a different parameterization.394Instead of representing a multinomial sample as theoutcome of a K-sided die, in this representation werepresent a sample as the outcome of a finite stochas-tic process.
The probability of a leaf is the product ofbranch probabilities leading to that leaf.
The wordsconstitute the leaves of the tree.DT distribution is the distribution over leaf prob-abilities.
Let ?n be the DT edge weight leading intonode n, C(n) be the children of node n, L be theleaves of the tree, I the internal nodes, and L(n)be the leaves in the subtree under n. We gener-ate a sample ?k from Dirichlet Tree(?)
by draw-ing a multinomial at each internal node i ?
I fromDirichlet(?C(i)) (i.e., the edge weights from i toits children).
The probability density function ofDT(?k|?)
is given by:DT (?k|?)
?
(?l?L ?k?l?1l)(?i?I(?j?L(i) ?kj)?
(i))Here ?
(i) = ?i ?
?j?C(i) ?j (i.e., the differ-ence between the in-degree and out-degree of inter-nal node i.
Note that if ?
(i) = 0 for all i ?
I , thenthe DT reduces to the typical Dirichlet distribution.Suppose we have the following (Figure 3(A))word network.
The network can be decomposedinto a collection of chains (e.g., (a,b,c), (p), and(x,y)).
For each chain having number of elementsmore than one (e.g., (a,b,c), (x,y)), we have a subtree(see Figure 3(B)) in the DT with one internal node(blank in figure) and the words as leaves.
We assign??
as the weights of these edges where ?
is the reg-ularization strength and ?
is the hyperparameter ofthe symmetric Dirichlet prior on ?.
The root node ofthe Dirichlet tree then connects to the internal node iwith weight |L(i)|?.
The other nodes (words) whichform single element chains (e.g, (p)) are connectedto the root directly with weight ?.
Notice that when?
= 1 (i.e., no regularization), ?
(i) = 0 and ourmodel reduces to the original LDA.
By tuning ?
wecontrol the strength of regularization.Figure 3: Incorporating word network into DTTo regularize LDA with FQG, we form the wordnetwork where a word is connected to the words inthe same or adjacent fragments.
Specifically, if wordwi ?
fragx and word wj ?
fragy (wi 6= wj), wecreate a link (wi, wj) if x = y or (x, y) ?
E, whereE is the set of edges of the FQG.
Implicitly by doingthis we want two sentences in the same or adjacentfragments to have similar topic distributions, and fallin the same topical cluster.4.6 LCSeg with FQGIf we examine the FQG carefully, different paths(considering the fragments of the first email as rootnodes) can be interpreted as subconversations.
Aswe walk down a path topic shifts may occur alongthe pathway.
We incorporate FQG into the LCSegmodel in three steps.
First, we extract the paths ofa FQG.
We then apply LCSeg algorithm on each ofthe extracted paths separately.
This process gives thesegmentation decisions along the paths of the FQG.Note that a fragment can be in multiple paths (e.g.,f , g, in Figure 2) which will cause its sentences tobe in multiple segments found by LCSeg.
There-fore, as a final step we need a consolidation method.Our intuition is that sentences in a consolidated seg-ment should fall in same segments more often whenwe apply LCSeg in step 2.
To consolidate the seg-ments found, we form a weighted undirected graphwhere the vertices V represent the sentences and theedge weights w(u, v) represent the number of timessentence u and v fall in the same segment.
The con-solidation problem can be formulated as a N-mincutgraph partitioning problem where we try to optimizethe Normalized Cut criterion:Ncut(A,B) =cut(A,B)assoc(A, V )+cut(B,A)assoc(B, V )where cut(A,B) = ?u?A,v?Bw(u, v) andassoc(A, V ) = ?u?A,t?V w(u, t) is the total con-nection from nodes in partition A to all nodes in thegraph and assoc(B, V ) is similarly defined.
How-ever, solving this problem turns out to be NP-hard.Hence, we approximate the solution following (Shiand Malik, 2000) which has been successfully ap-plied to image segmentation in computer vision.This approach makes a difference only if FGQcontains more than one path.
In fact in our corpuswe found an average paths of 7.12 per thread.395Avg.
Topic LDA LDA +FQG LCSeg LCSeg +FQG Speaker Block 5Number 2.10 1.90 2.2 2.41 4.87 5.69Length 13.3 15.50 13.12 12.41 5.79 4.60Density 1.83 1.60 1.01 1.39 1.37 1.00Entropy 0.98 0.75 0.81 0.93 1.88 2.39Table 3: Corpus statistics of different system?s annotation5 ExperimentsWe ran our four systems LDA, LDA+FQG, LCSeg,and LCSeg+FQG on the dataset11.
The statisticsof these four annotations and two best performingbaselines (i.e., ?Speaker?
and ?Block 5?
as describedbelow) are shown in Table 3.
For brevity we justmention the average measures.
Comparing with Ta-ble 1, we see that these fall within the bounds of thehuman annotations.We compare our results in Table 4, where we alsoprovide the results of some simple baseline systems.We evaluated the following baselines and report thebest two in Table 4.All different: Each sentence is a separate topic.All same: The whole thread is a single topic.Speaker: The sentences from each participantconstitute a separate topic.Blocks of k(= 5, 10, 15): Each consecutivegroup of k sentences is a topic.Most of these baselines perform rather poorly.All different is the worst baseline with mean 1-to-1 score of 0.10 (max: 0.33, min: 0.03) and meanloc3 score of 0.245 (max: 0.67, min: 0).
Block10 has mean 1-to-1 score of 0.35 (max: 0.71, min:0.13) and mean loc3 score of 0.584 (max: 0.76,min: 0.31).
Block 15 has mean 1-to-1 score of0.32 (max: 0.77, min: 0.16) and mean loc3 scoreof 0.56 (max: 0.82, min: 0.38).
All same is optimalfor threads containing only one topic, but its perfor-mance rapidly degrades as the number of topics ina thread increases.
It has mean 1-to-1 score of 0.28(max: 112, min: 0.11) and mean loc3 score of 0.5411For a fair comparison of the systems we set the same topicnumber per thread for all of them.
If at least two of the anno-tators agree on the topic number we set that number, otherwisewe set the floor value of the average topic number.
?
is set to 20in LDA+FQG.12The maximum value of 1 is due to the fact that for somethreads some annotators found only one topic(max: 1, min: 0.34).As shown in Table 4, Speaker and Blocks of 5 aretwo strong baselines especially for the loc3.
In gen-eral, our systems perform better than the baselines,but worse than the gold standard.
Of all the systems,the basic LDA model performs very disappointingly.In the local agreement it even fails to beat the base-lines.
A likely explanation is that the independenceassumption made by LDA when computing the dis-tribution over topics for a sentence from the distribu-tion over topics for the words causes sentences in alocal context to be excessively distributed over top-ics.
Another possible explanation for LDA?s disap-pointing performance is the limited amount of dataavailable for training.
In our corpus, the averagenumber of sentences per thread is 26.3 (see table 1)which might not be sufficient for the LDA models.If we compare the performance of the regularizedLDA (in the table LDA+FQG) with the basic LDAwe get a significant (p=0.0002 (1-to-1), p=9.8e-07(loc3)) improvement in both of the measures 13.
Thissupports our claim that sentences connected by ref-erential relations in the FQG usually refer to thesame topic.
The regularization also prevents the lo-cal context from being overly distributed over topics.A comparison of the basic LCSeg with the basicLDA reveals that LCSeg is a better model for emailtopic segmentation (p=0.00017 (1-to-1), p<2.2e-16(loc3)).
One possible reason is that LCSeg extractsthe topics keeping the local context intact.
An-other reason could be the term weighting schemeemployed by LCSeg.
Unlike LDA, which considersonly ?repetition?, LCSeg also considers how tightlythe ?repetition?
happens.
When we incorporate theconversation structure (i.e., FQG) into LCSeg (in thetable LCSeg+FQG), we get a significant improve-ment in the 1-to-1 measure over the basic LCSeg(p=0.0014).
Though the local context (i.e., loc3) suf-13Tests of significance were done by paired t-test with df=116396Baselines Systems HumanScores Speaker Block 5 LDA LDA+FQG LCSeg LCSeg+FQGMean 1-to-1 0.52 0.38 0.57 0.62 0.62 0.68 0.80Max 1-to-1 0.94 0.77 1.00 1.00 1.00 1.00 1.00Min 1-to-1 0.23 0.14 0.24 0.24 0.33 0.33 0.31Mean loc3 0.64 0.57 0.54 0.61 0.72 0.71 0.83Max loc3 0.97 0.73 1.00 1.00 1.00 1.00 1.00Min loc3 0.27 0.42 0.38 0.38 0.40 0.40 0.43Table 4: Comparison of Human, System and best Baseline annotationsfers a bit, the decrease in performance is minimaland it is not significant.
The fact that LCSeg is abetter model than LDA is also preserved when weincorporate FQG into them (p=2.140e-05 (1-to-1),p=1.3e-09 (loc3)).
Overall, LCSeg+FQG is the bestmodel for this data.6 Future WorkThere are some other important features that ourmodels do not consider.
The ?Speaker?
feature isa key source of information.
A participant usu-ally contributes to the same topic.
The best base-line ?Speaker?
in Table 4 also favours this claim.Another possibly critical feature is the ?mention ofnames?.
In multi-party discussion people usuallymention each other?s name for the purpose of dis-entanglement (Elsner and Charniak, 2008).
In ourcorpus we found 175 instances where a participantmentions other participant?s name.
In addition tothese, ?Subject of the email?, ?topic-shift cue words?can also be beneficial for a model.
As a next stepfor this research, we will investigate how to exploitthese features in our methods.We are also interested in the near future to transferour approach to other similar domains by hierarchi-cal Bayesian multi-task learning and other domainadaptation methods.
We plan to work on both syn-chronous (e.g., chats, meetings) and asynchronous(e.g., blogs) domains.7 ConclusionIn this paper we presented an email corpus annotatedfor topic segmentation.
We extended LDA and LC-Seg models by incorporating the fragment quotationgraph, a fine-grain model of the conversation, whichis based on the analysis of quotations.
Empiricalevaluation shows that the fragment quotation graphhelps both these models to perform significantly bet-ter than their basic versions, with LCSeg+FQG be-ing the best performer.AcknowledgmentsWe are grateful to the 6 pilot annotators, 3 test an-notators and to the 3 anonymous reviewers for theirhelpful comments.
This work was supported inpart by NSERC PGS award, NSERC BIN project,NSERC discovery grant and Institute for Comput-ing, Information and Cognitive Systems (ICICS) atUBC.ReferencesJames Allan, 2002.
Topic detection and tracking: event-based information organization, pages 1?16.
KluwerAcademic Publishers, Norwell, MA, USA.David Andrzejewski, Xiaojin Zhu, and Mark Craven.2009.
Incorporating domain knowledge into topicmodeling via dirichlet forest priors.
In Proceedingsof the 26th Annual International Conference on Ma-chine Learning (ICML?09), pages 25?32, New York,NY, USA.
ACM.Paul M. Aoki, Margaret H. Szymanski, Luke D.Plurkowski, James D. Thornton, Allison Woodruff,and Weilie Yi.
2006.
Where?s the ?party?
in ?multi-party??
: analyzing the structure of small-group socia-ble talk.
In Proceedings of the 2006 20th anniversaryconference on Computer supported cooperative work(CSCW ?06), pages 393?402, New York, NY, USA.ACM.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
JMLR, 3:993?1022.Jordan L. Boyd-Graber and David M. Blei.
2010.
Syn-tactic topic models.
CoRR, abs/1002.4665.G.
Carenini, R. T. Ng, and X. Zhou.
2007.
Summarizingemail conversations with clue words.
In Proceedings397of the 16th international conference on World WideWeb, pages 91?100.
ACM New York, NY, USA.Harr Chen, S. R. K. Branavan, Regina Barzilay, andDavid R. Karger.
2009.
Global models of documentstructure using latent permutations.
In NAACL?09,pages 371?379, Morristown, NJ, USA.
ACL.Freddy Y. Y. Choi, Peter Wiemer-Hastings, and JohannaMoore.
2001.
Latent semantic analysis for text seg-mentation.
In In Proceedings of EMNLP, pages 109?117, Pittsburgh, PA USA.Freddy Y. Y. Choi.
2000.
Advances in domain inde-pendent linear text segmentation.
In Proceedings ofthe 1st North American chapter of the Association forComputational Linguistics conference, pages 26?33,San Francisco, CA, USA.
Morgan Kaufmann Publish-ers Inc.David Crystal, 2001.
Language and the Internet.
Cam-bridge University Press.Gae?l Dias, Elsa Alves, and Jose?
Gabriel Pereira Lopes.2007.
Topic segmentation algorithms for text summa-rization and passage retrieval: an exhaustive evalua-tion.
In AAAI?07: Proceedings of the 22nd nationalconference on Artificial intelligence, pages 1334?1339.
AAAI Press.Mark Dredze, Hanna M. Wallach, Danny Puller, and Fer-nando Pereira.
2008.
Generating summary keywordsfor emails using topics.
In IUI ?08, pages 199?206,New York, NY, USA.
ACM.Micha Elsner and Eugene Charniak.
2008.
You talkingto me?
a corpus and algorithm for conversation dis-entanglement.
In Proceedings of ACL-08: HLT, pages834?842, Ohio, June.
ACL.Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,and Hongyan Jing.
2003.
Discourse segmentation ofmulti-party conversation.
In ACL ?03: Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics, pages 562?569, Morristown, NJ,USA.
Association for Computational Linguistics.M.
Georgescul, A. Clark, and S. Armstrong.
2008.
Acomparative study of mixture models for automatictopic segmentation of multiparty dialogues.
In ACL-08:HLT, pages 925?930, Ohio, June.
ACL.Thomas L. Griffiths and Mark Steyvers.
2003.
Predic-tion and semantic association.
In Advances in NeuralInformation Processing Systems.
MIT Press.Thomas L. Griffiths, Mark Steyvers, David M. Blei, andJoshua B. Tenenbaum.
2005.
Integrating topics andsyntax.
In In Advances in Neural Information Pro-cessing Systems, pages 537?544.
MIT Press.Sanda Harabagiu and Finley Lacatusu.
2005.
Topicthemes for multi-document summarization.
In SIGIR?05:, pages 202?209, New York, NY, USA.
ACM.Marti A. Hearst.
1997.
Texttiling: segmenting textinto multi-paragraph subtopic passages.
Comput.
Lin-guist., 23(1):33?64, March.Pei Hsueh, Johanna D. Moore, and Steve Renals.
2006.Automatic segmentation of multiparty dialogue.
InProceedings of the European Chapter of the Associ-ation for Computational Linguistics (EACL), Trento,Italy.
ACL.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In Pro-ceedings of the ACL?06, pages 25?32, Sydney, Aus-tralia, July.
ACL.T.
Minka.
1999.
The dirichlet-tree distribution.
Techni-cal report, Justsystem Pittsburgh Research Center.Matthew Purver, Konrad P. Ko?rding, Thomas L. Griffiths,and Joshua B. Tenenbaum.
2006.
Unsupervised topicmodelling for multi-party spoken discourse.
In Pro-ceedings of the ACL?06, pages 17?24, Sydney, Aus-tralia.
ACL.Jianbo Shi and Jitendra Malik.
2000.
Normalized cutsand image segmentation.
IEEE Trans.
Pattern Anal.Mach.
Intell., 22(8):888?905.M.
Steyvers and T. Griffiths, 2007.
Latent SemanticAnalysis: A Road to Meaning, chapter Probabilistictopic models.
Laurence Erlbaum.J.
Ulrich, G. Murray, and G. Carenini.
2008.
A publiclyavailable annotated corpus for supervised email sum-marization.
In EMAIL-2008 Workshop, pages 428?435.
AAAI.Hanna M. Wallach.
2006.
Topic modeling: beyond bag-of-words.
In ICML ?06, pages 977?984, NY, USA.398
