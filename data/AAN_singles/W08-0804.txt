Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 19?20,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSmall Statistical Models by Random Feature MixingKuzman Ganchev and Mark DredzeDepartment of Computer and Information ScienceUniversity of Pennsylvania, Philadelphia, PA{kuzman,mdredze}@cis.upenn.eduAbstractThe application of statistical NLP systems toresource constrained devices is limited by theneed to maintain parameters for a large num-ber of features and an alphabet mapping fea-tures to parameters.
We introduce randomfeature mixing to eliminate alphabet storageand reduce the number of parameters withoutseverely impacting model performance.1 IntroductionStatistical NLP learning systems are used for manyapplications but have large memory requirements, aserious problem for mobile platforms.
Since NLPapplications use high dimensional models, a largealphabet is required to map between features andmodel parameters.
Practically, this means storingevery observed feature string in memory, a pro-hibitive cost for systems with constrained resources.Offline feature selection is a possible solution, butstill requires an alphabet and eliminates the poten-tial for learning new features after deployment, animportant property for adaptive e-mail or SMS pre-diction and personalization tasks.We propose a simple and effective approach toeliminate the alphabet and reduce the problem of di-mensionality through random feature mixing.
Weexplore this method on a variety of popular datasetsand classification algorithms.
In addition to alpha-bet elimination, this reduces model size by a factorof 5?10 without a significant loss in performance.2 MethodLinear models learn a weight vector over featuresconstructed from the input.
Features are constructedas strings (e.g.
?w=apple?
interpreted as ?containsthe word apple?)
and converted to feature indicesmaintained by an alphabet, a map from strings tointegers.
Instances are efficiently represented as asparse vector and the model as a dense weight vec-tor.
Since the alphabet stores a string for each fea-ture, potentially each unigram or bigram it encoun-ters, it is much larger than the weight vector.Our idea is to replace the alphabet with a randomfunction from strings to integers between 0 and anintended size.
This size controls the number of pa-rameters in our model.
While features are now eas-ily mapped to model parameters, multiple featurescan collide and confuse learning.
The collision rateis controlled by the intended size.
Excessive colli-sions can make the learning problem more difficult,but we show significant reductions are still possiblewithout harming learning.
We emphasize that evenwhen using an extremely large feature space to avoidcollisions, alphabet storage is eliminated.
For theexperiments in this paper we use Java?s hashCodefunction modulo the intended size rather than a ran-dom function.3 ExperimentsWe evaluated the effect of random feature mix-ing on four popular learning methods: Perceptron,MIRA (Crammer et al, 2006), SVM and Maximumentropy; with 4 NLP datasets: 20 Newsgroups1,Reuters (Lewis et al, 2004), Sentiment (Blitzeret al, 2007) and Spam (Bickel, 2006).
For eachdataset we extracted binary unigram features andsentiment was prepared according to Blitzer et al(2007).
From 20 Newsgroups we created 3 binarydecision tasks to differentiate between two similar1http://people.csail.mit.edu/jrennie/20Newsgroups/1970 75 80 85 90  0 10 20 30 40 5060 70 80 90thousands offeaturesfeature mixingno feature mixing70 75 80 85 90  0 10 20 30 40 5060 70 80 90thousands offeaturesfeature mixingno feature mixingFigure 1: Kitchen appliance reviews.
Left: Maximum en-tropy.
Right: Perceptron.
Shaded area and vertical linesextend one standard deviation from the mean.labels from computers, science and talk.
We cre-ated 3 similar problems from Reuters from insur-ance, business services and retail distribution.
Senti-ment used 4 Amazon domains (book, dvd, electron-ics, kitchen).
Spam used the three users from taskA data.
Each problem had 2000 instances except for20 Newsgroups, which used between 1850 and 1971instances.
This created 13 binary classification prob-lems across four tasks.
Each model was evaluatedon all problems using 10-fold cross validation andparameter optimization.
Experiments varied modelsize to observe the effect of feature collisions on per-formance.Results for sentiment classification of kitchen ap-pliance reviews (figure 1) are typical.
The originalmodel has roughly 93.6k features and its alphabetrequires 1.3MB of storage.
Assuming 4-byte float-ing point numbers the weight vector needs under0.37MB.
Consequently our method reduces storageby over 78% when we keep the number of param-eters constant.
A further reduction by a factor of 2decreases accuracy by only 2%.Figure 2 shows the results of all experimentsfor SVM and MIRA.
Each curve shows normalizeddataset performance relative to the full model as thepercentage of original features decrease.
The shadedrectangle extends one standard deviation above and1.02  1  0.98  0.96  0.94  00.5 11.5 2Relative # features1.02  1  0.98  0.96  0.94  00.5 11.5 2Relative # featuresFigure 2: Relative performance on all datasets for SVM(left) and MIRA (right).76 78 80 82 84 86 88  0 24 6 8 10 12 14 16thousands offeaturesfeature mixingno feature mixing76 78 80 82 84 86 88  0 24 6 8 10 12 14 16thousands offeaturesfeature mixingno feature mixingFigure 3: The anomalous Reuters dataset from figure 2for Perceptron (left) and MIRA (right).below full model performance.
Almost all datasetsperform within one standard deviation of the fullmodel when using feature mixing set to the totalnumber of features for the problem, indicating thatalphabet elimination is possible without hurting per-formance.
One dataset (Reuters retail distribution) isa notable exception and is illustrated in detail in fig-ure 3.
We believe the small total number of featuresused for this problem is the source of this behavior.On the vast majority of datasets, our method can re-duce the size of the weight vector and eliminate thealphabet without any feature selection or changes tothe learning algorithm.
When reducing weight vec-tor size by a factor of 10, we still obtain between96.7% and 97.4% of the performance of the originalmodel, depending on the learning algorithm.
If weeliminate the alphabet but keep the same size weightvector, model the performance is between 99.3%of the original for MIRA and a slight improvementfor Perceptron.
The batch learning methods are be-tween those two extremes at 99.4 and 99.5 for max-imum entropy and SVM respectively.
Feature mix-ing yields substantial reductions in memory require-ments with a minimal performance loss, a promisingresult for resource constrained devices.ReferencesS.
Bickel.
2006.
Ecml-pkdd discovery challengeoverview.
In The Discovery Challenge Workshop.J.
Blitzer, M. Dredze, and F. Pereira.
2007.
Biographies,bollywood, boom-boxes and blenders: Domain adap-tation for sentiment classification.
In ACL.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Ressearch, 7.D.
D. Lewis, Y. Yand, T. Rose, and F. Li.
2004.
Rcv1:A new benchmark collection for text categorization re-search.
JMLR, 5:361?397.20
