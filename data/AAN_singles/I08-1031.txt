Hypothesis Selection in Machine Transliteration: A Web Mining ApproachJong-Hoon Oh and Hitoshi IsaharaComputational Linguistics GroupNational Institute of Information and Communications Technology (NICT)3-5 Hikaridai,Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan{rovellia,isahara}@nict.go.jpAbstractWe propose a new method of selecting hy-potheses for machine transliteration.
Wegenerate a set of Chinese, Japanese, and Ko-rean transliteration hypotheses for a givenEnglish word.
We then use the set of translit-eration hypotheses as a guide to finding rel-evant Web pages and mining contextual in-formation for the transliteration hypothesesfrom the Web page.
Finally, we use themined information for machine-learning al-gorithms including support vector machinesand maximum entropy model designed toselect the correct transliteration hypothesis.In our experiments, our proposed methodbased on Web mining consistently outper-formed systems based on simple Web countsused in previous work, regardless of the lan-guage.1 IntroductionMachine transliteration has been a great challengefor cross-lingual information retrieval and machinetranslation systems.
Many researchers have devel-oped machine transliteration systems that accept asource language term as input and then output itstransliteration in a target language (Al-Onaizan andKnight, 2002; Goto et al, 2003; Grefenstette et al,2004; Kang and Kim, 2000; Li et al, 2004; Meng etal., 2001; Oh and Choi, 2002; Oh et al, 2006; Quand Grefenstette, 2004).
Some of these have usedthe Web to select machine-generated transliterationhypotheses and have obtained promising results (Al-Onaizan and Knight, 2002; Grefenstette et al, 2004;Oh et al, 2006; Qu and Grefenstette, 2004).
Moreprecisely, they used simple Web counts, estimated asthe number of hits (Web pages) retrieved by a Websearch engine.However, there are several limitations imposed onthe ability of Web counts to select a correct translit-eration hypothesis.
First, the assumption that hitcounts approximate the Web frequency of a givenquery usually introduces noise (Lapata and Keller,2005).
Moreover, some Web search engines disre-gard punctuation and capitalization when matchingsearch terms (Lapata and Keller, 2005).
This cancause errors if such Web counts are relied on to se-lect transliteration hypotheses.
Second, it is not easyto consider the contexts of transliteration hypothe-ses with Web counts because Web counts are esti-mated based on the number of retrieved Web pages.However, as our preliminary work showed (Oh etal., 2006), transliteration or translation pairs oftenappear as parenthetical expressions or tend to be inclose proximity in texts; thus context can play an im-portant role in selecting transliteration hypotheses.For example, there are several Chinese, Japanese,and Korean (CJK) transliterations and their counter-parts in a parenthetical expression, as follows.1) 12(Adrienne1Clarkson2)2) ?1	?2(glucose1oxidase2)3) 1	2(diphenol1oxidase2)Note that the subscripted numbers in all examplesrepresent the correspondence between the Englishword and its CJK counterpart.
These parentheti-cal expressions are very useful in selecting translit-233eration hypotheses because it is apparent that theyare translation pairs or transliteration pairs.
How-ever, we cannot fully use such information with Webcounts.To address these problems, we propose a newmethod of selecting transliteration hypotheses.
Wewere interested in how to mine information relevantto the selection of hypotheses and how to select cor-rect transliteration hypotheses using the mined in-formation.
To do this, we generated a set of CJKtransliteration hypotheses for a given English word.We then used the set of transliteration hypothesesas a guide to finding relevant Web page and min-ing contextual information for the transliteration hy-potheses from the Web page.
Finally, we usedthe mined information for machine-learning algo-rithms including support vector machines (SVMs)and maximum entropy model designed to select thecorrect transliteration hypothesis.This paper is organized as follows.
Section 2 de-scribes previous work based on simple Web counts.Section 3 describes a way of generating transliter-ation hypotheses.
Sections 4 and 5 introduce ourmethods of Web mining and selecting transliterationhypotheses.
Sections 6 and 7 deal with our exper-iments and the discussion.
Conclusions are drawnand future work is discussed in Section 8.2 Related workWeb counts have been used for selecting translit-eration hypotheses in several previous work (Al-Onaizan and Knight, 2002; Grefenstette et al, 2004;Oh et al, 2006; Qu and Grefenstette, 2004).
Be-cause the Web counts are estimated as the number ofhits by a Web search engine, they greatly depend onqueries sent to a search engine.
Previous work hasused three types of queries?monolingual queries(MQs) (Al-Onaizan and Knight, 2002; Grefen-stette et al, 2004; Oh et al, 2006), bilingualsimple queries (BSQs) (Oh et al, 2006; Qu andGrefenstette, 2004), and bilingual bigram queries(BBQs) (Oh et al, 2006).
If we let S be a sourcelanguage term and H = {h1, ?
?
?
, hr} be a set ofmachine-generated transliteration hypotheses of S,the three types of queries can be defined asMQ: hi (e.g., ,?, and	).BSQ: s and hi without quotations (e.g., Clinton  , Clinton ?, and Clinton 	).BBQ: Quoted bigrams composed of S and hi (e.g.,?Clinton ?, ?Clinton ??, and?Clinton	?
).MQ is not able to determine whether hi is a counter-part of S, but whether hi is a frequently used targetterm in target-language texts.
BSQ retrieves Webpages if S and hi are present in the same documentbut it does not take the distance between S and hiinto consideration.
BBQ retrieves Web pages where?S hi?
or ?hi S?
are present as a bigram.
The rel-ative order of Web counts over H makes it possibleto select transliteration hypotheses in the previouswork.3 Generating Transliteration HypothesesLet S be an English word, P be a pronuncia-tion of S, and T be a target language translitera-tion corresponding to S. We implement English-to-CJK transliteration systems based on three dif-ferent transliteration models ?
a grapheme-basedmodel (S ?
T ), a phoneme-based model (S ?
Pand P ?
T ), and a correspondence-based model(S ?
P and (S, P ) ?
T ) ?
as described in ourpreliminary work (Oh et al, 2006).
P and T are seg-mented into a series of sub-strings, each of whichcorresponds to a source grapheme.
We can thuswrite S = s1, ?
?
?
, sn = sn1, P = p1, ?
?
?
, pn = pn1,and T = t1, ?
?
?
, tn = tn1, where si, pi, and ti rep-resent the ith English grapheme, English phonemescorresponding to si, and target language graphemescorresponding to si, respectively.
Given S, ourtransliteration systems generate a sequence of ti cor-responding to either si (in Eq.
(1)) or pi (in Eq.
(2))or both of them (in Eq.
(3)).PrG(T |S) = Pr(tn1|sn1) (1)PrP (T |S) = Pr(pn1|sn1)?
Pr(tn1|pn1) (2)PrC(T |S) = Pr(pn1|sn1)?
Pr(tn1|sn1, pn1) (3)The maximum entropy model was used to estimateprobabilities in Eqs.
(1)?
(3) (Oh et al, 2006).
Weproduced the n-best transliteration hypotheses usinga stack decoder (Schwartz and Chow, 1990).
We234then created a set of transliteration hypotheses com-prising the n-best transliteration hypotheses.4 Web MiningLet S be an English word and H = {h1, ?
?
?
, hr} beits machine-generated set of transliteration hypothe-ses.
We use S and H to generate queries sent to asearch engine1 to retrieve the top-100 snippets.
Acorrect transliteration and its counterpart tend to bein close proximity on CJK Web pages.
Our goal inWeb mining was to find such Web pages and mineinformation that would help to select transliterationhypotheses from these pages.To find these Web pages, we used three kinds ofqueries, Q1=(S and hi), Q2=S, and Q3=hi, whereQ1is the same as BSQ?s query and Q3is the sameas MQ?s.
The three queries usually result in differentsets of Web pages.
We categorize the retrieved Webpages by Q1, Q2, and Q3into W1, W2, and W3.
Weextract three kinds of features from Wl as follows,where l = 1, 2, 3.?
Freq(hi,Wl): the number of occurrences of hiin Wl?
DFreqk(hi,Wl): Co-occurrence of S and hiwith distance dk ?
D in the same snippet ofWl.?
PFreqk(hi,Wl): Co-occurrence of S and hias parenthetical expressions with distance dk ?D in the same snippet of Wl.
Parenthetical ex-pressions are detected when either S or hi is inparentheses.We define D = {d1, d2, d3} with three ranges ofdistances between S and hi, where d1(d < 5),d2(5 ?
d < 10), and d3(10 ?
d ?
15).
We counteddistance d with the total number of characters (orwords)2 between S and hi.
Here, we can take thecontexts of transliteration hypotheses into accountusing DFreq and PFreq; while Freq is countedregardless of the contexts of the transliteration hy-potheses.Figure 1 shows examples of how to calculateFreq, DFreqk, and PFreqk, where S = Clinton,1We used Google (http://www.google.com)2Depending on whether the languages had spacing units,words (for English and Korean) or characters (for Chinese andJapanese) were chosen to calculate d.???????
?1(Bill Clinton1)??????????????????????????????????????????
(My Life)????2???????????????????????????
?3(Hillary Rodham Clinton2)??1997?????
...1(Bill Clinton1)(My Life)23(Hillary Rodham Clinton2) 1997...W1: Q1=(Clinton ???)::???4?Clinton3????????1??Kerry?::?2?
?John Kerry?????????????????????????????5?Clinton4?????????????????????????????????Bush??"???"????
???6?Clinton5???3??Kerry?
...::4Clinton3 1Kerry ::2John Kerry5Clinton4Bush ""6Clinton5 3Kerry ...Snippet1Snippet2Figure 1: Web corpora collected by Clinton and Snippet1123Clinton11 41 68Clinton272 29 2Snippet2456Clinton30 36 81Clinton440 0 37Clinton585 41 0Snippet2123Clinton36 9 85Clinton432 29 42Clinton577 74 1Table 1: Distance between Clinton and Chinesetransliteration hypotheses in Fig.
1hi= in W1 collected by Q1=(Clinton ).
The subscripted numbers of Clinton and  were used to indicate how many times they oc-curred in W1.
In Fig.
1,  occurs six timesthus Freq(hi,W1) = 6.
Table 1 lists the dis-tance between Clinton andwithin each snip-pet of W1.
We can obtain DFreq1(hi,W1) =5.
PFreq1(hi,Wl) is calculated by detectingparenthetical expressions between S and hi whenDFreq1(hi,Wl) is counted.
Because all S inW1(Clinton1to Clinton5) are in parentheses,PFreq1(hi,W1) is the same as DFreq1(hi,W1).We ignore Freq, DFreqk, and PFreqk when hiis a substring of other transliteration hypotheses be-cause hi usually has a higher Freq, DFreqk, andPFreqk than hj if hi is a substring of hj .
Let a235set of transliteration hypotheses for S = Clintonbe H= {h1= , h2= }.
Here, h2is asubstring of h1.
In Fig.
1, h2appears six times asa substring of h1and three times independently inSnippet2.
Moreover, independently used h2(1,2, and 3) and S (Clinton3and Clinton5) aresufficiently close to count DFreqk and PFreqk.Therefore, the Freq, DFreqk, and PFreqk of h1will be lower than those of h2if we do not takethe substring relation between h1and h2into ac-count.
Considering the substring relation, we ob-tain Freq(h2,W1) = 3, DFreq1(h2,W1) = 1,DFreq2(h2,W1) = 2, PFreq1(h2,W1) = 1, andPFreq2(h2,W1) = 2.5 Hypothesis SelectionWe select transliteration hypotheses by rankingthem.
A set of transliteration hypotheses, H ={h1, h2, ?
?
?
, hr}, is ranked to enable a correct hy-pothesis to be identified.
We devise a rank function,g(hi) in Eq.
(4), that ranks a correct transliterationhypothesis higher and the others lower.g(hi) : H ?
{R : R is ordering of hi ?
H} (4)Let xi ?
X be a feature vector of hi ?
H, yi ={+1,?1} be the training label for xi, and T D ={td1=< x1, y1>, ?
?
?
, tdz =< xz, yz >} be thetraining data for g(hi).
We prepare the training datafor g(hi) as follows.1.
Given each English word S in the training-set,generate transliteration hypotheses H.2.
Given hi ?
H, assign yi by looking for S andhi in the training-set ?
yi = +1 if hi is a cor-rect transliteration hypothesis corresponding toS, otherwise yi = ?1.3.
For each pair (S, hi), generate its feature vectorxi.4.
Construct a training data set, T D:?
T D = T D+?T D??
T D+ tdi where yi = +1?
T D? tdj where yj = ?1We used two machine-learning algorithms, sup-port vector machines (SVMs)3 and maximum en-tropy model4 for our implementation of g(hi).
TheSVMs assign a value to each transliteration hypoth-esis (hi) usinggSVM (hi) = w ?
xi + b (5)where w denotes a weight vector.
Here, we use thepredicted value of gSVM (hi) rather than the pre-dicted class of hi given by SVMs because our rank-ing function, as represented by Eq.
(4), determinesthe relative ordering between hi and hj in H. Aranking function based on the maximum entropymodel assigns a probability to hi usinggMEM (hi) = Pr(yi = +1|xi) (6)We can finally obtain a ranked list for the given H?the higher the g(hi) value, the better the hi.5.1 FeaturesWe represent the feature vector, xi, with two typesof features.
The first is the confidence scores of higiven by Eqs.
(1)?
(3) and the second is Web-basedfeatures ?
Freq, DFreqk, and PFreqk.
To nor-malize Freq, DFreqk, and PFreqk, we use theirrelative frequency over H as in Eqs.
(7)?
(9), wherek = 1, 2, 3 and l = 1, 2, 3.RF (hi,Wl) =Freq(hi,Wl)?hj?HFreq(hj,Wl)(7)RDFk(hi,Wl) =DFreqk(hi,Wl)?hj?HDFreqk(hj,Wl)(8)RPFk(hi,Wl) =PFreqk(hi,Wl)?hj?HPFreqk(hj,Wl)(9)Figure 2 shows how to construct feature vectorxi from a given English word, Rachel, and its Chi-nese hypotheses, H, generated from our translitera-tion systems.
We can obtain r Chinese translitera-tion hypotheses and classify them into positive andnegative samples according to yi.
Note that yi = +1if and only if hi is registered as a counterpart of Sin the training data.
The bottom of Fig.
2 shows ourfeature set representing xi.
There are three confi-dence scores in P (hi|S) according to transliterationmodels and the three Web-based features Web(W1),Web(W2), and Web(W3).3SVM light (Joachims, 2002)4?Maximum Entropy Modeling Toolkit?
(Zhang, 2004)236?????????????????
?hr?h5h4h3h2h1H-1-1-1-1-1+1yr?y5y4y3y2y1YRachelRF(hi,W1)RDF1(hi,W1)RDF2(hi,W1)RDF3(hi,W1)RPF1(hi,W1)RPF2(hi,W1)RPF3(hi,W1)Web (W1)RF(W3)RDF1(hi,W3)RDF2(hi,W3)RDF3(hi,W3)RPF1(hi,W3)RPF2(hi,W3)RPF3(hi,W3)RF(hi,W2)RDF1(hi,W2)RDF2(hi,W2)RDF3(hi,W2)RPF1(hi,W2)RPF2(hi,W2)RPF3(hi,W2)PrG(hi|S)PrP(hi|S)PrC(hi|S)Web (W3)Web (W2)Pr(hi|S)xitd1 ?
TD+ td2, td3,  td4, td5,?,tdr?
TD-xr?x5x4x3x2x1XFigure 2: Feature vectors6 ExperimentsWe evaluated the effectiveness of our system in se-lecting CJK transliteration hypotheses.
We used thesame test set used in Li et al (2004) (ECSet) for Chi-nese transliterations (Xinhua News Agency, 1992)and those used in Oh et al (2006) for Japaneseand Korean transliterations ?
EJSET and EK-SET (Breen, 2003; Nam, 1997).
We divided the testECSet EJSet EKSetTraining Set 31,299 8,335 5,124Development Set 3,478 1,041 1,024Blind Test Set 2,896 1,041 1,024Total 37,694 10,417 7,172Table 2: Test data setsdata into training, development, and blind test setsas in Table 2.
The training set was used to train ourthree transliteration models to generate the n-besttransliteration hypotheses5.
The development setwas used to train hypothesis selection based on sup-port vector machines and maximum entropy model.We used the blind test set for evaluation.
The eval-uation was done in terms of word accuracy (WA).WA is the proportion of correct transliterations inthe best hypothesis by a system to correct transliter-ations in the blind test set.System ECSet EJSet EKSetKANG00 N/A N/A 54.1GOTO03 N/A 54.3 N/ALI04 70.1 N/A N/AGM 69.0 61.6 59.0PM 56.6 54.4 56.7CM 69.9 65.0 65.1Table 3: WA of individual transliteration systems(%)6.1 Results: Web counts vs.
Web miningWe compared our transliteration system with threeprevious ones, all of which were based on agrapheme-based model (Goto et al, 2003; Kang andKim, 2000; Li et al, 2004).
LI046 is an English-to-Chinese transliteration system, which simultane-ously takes English and Chinese contexts into con-sideration (Li et al, 2004).
KANG00 is an English-to-Korean transliteration system and GOTO03 is anEnglish-to-Japanese one ?
they segment a chunk ofEnglish graphemes and identify the most relevantsequence of target graphemes corresponding to thechunk (Goto et al, 2003; Kang and Kim, 2000) 7.GM, PM, and CM, which are respectively basedon Eqs.
(1)?
(3), are the transliteration systems weused for generating transliteration hypotheses.
Ourtransliteration systems showed comparable or betterperformance than the previous ones regardless of thelanguage.We compared simple Web counts with our Webmining for hypothesis selection.
We used the sameset of transliteration hypotheses H then comparedtheir performance in hypothesis selection with twomeasures, relative frequency and g(hi).
Tables 4 and5 list the results.
Here, ?Upper bound?
is a systemthat always selects the correct transliteration hypoth-esis if there is a correct one inH.
?Upper bound?
can5We set n = 10 for the n-best.
Thus, n ?
r ?
3?
n whereH = {h1, h2, ?
?
?
, hr}6The WA of LI04 was taken from the literature, where thetraining data were the same as the union of our training set andthe development set while the test data were the same as in ourtest set.
In other words, LI04 used more training data than oursdid.
With the same setting as LI04, our GM, PM, and CM pro-duced respective WAs of 70.0, 57.7, and 71.7.7We implemented KANG00 (Kang and Kim, 2000) andGOTO03 (Goto et al, 2003), and tested them with the samedata as ours.237System ECSet EJSet EKSetWCMQ 16.1 40.4 34.7BSQ 45.8 74.0 72.4BBQ 34.9 78.1 79.3WMRF (W1) 62.9 78.4 77.1RDF (W1) 70.8 80.4 80.2RPF (W1) 73.5 79.7 79.4RF (W2) 63.5 76.2 74.8RDF (W2) 67.1 79.2 78.9RPF (W2) 69.6 79.1 78.4RF (W3) 37.9 53.9 55.8RDF (W3) 76.4 69.0 70.2RPF (W3) 76.8 68.3 68.7Upper bound 94.6 93.5 93.2Table 4: Web counts (WC) vs.
Web mining (WM):hypothesis selection by relative frequency (%)System ECSet EJSet EKSetWC MEMWC 74.7 86.1 85.6SVMWC 74.8 86.9 86.5WM MEMWM 82.0 88.2 85.8SVMWM 83.9 88.5 86.7Upper bound 94.6 93.5 93.2Table 5: Web counts (WC) vs.
Web mining (WM):hypothesis selection by g(hi) (%)also be regarded as the ?Coverage?
of H generatedby our transliteration systems.
MQ, BSQ, and BBQin the upper section of Table 4, represent hypothesisselection systems based on the relative frequency ofWeb counts over H, the same measure used in Oh etal.
(2006):WebCountsx(hi)?hj?HWebCountsx(hj)(10)where WebCountsx(hi) is a function returningWeb counts retrieved by x ?
{MQ,BSQ,BBQ}RF (Wl), RDF (Wl), and RPF (Wl) in Table 4 rep-resent hypothesis selection systems with their rela-tive frequency, where RDF (Wl) and RPF (Wl) use?3k=1 RDFk(hj ,Wl) and?3k=1 RPFk(hj ,Wl),respectively.
The comparison in Table 4 showswhich is best for selecting transliteration hy-potheses when each relative frequency is usedalone.
Table 5 compares Web counts with fea-tures mined from the Web when they are usedas features in g(hi) ?
{Pr(hi|S), Web(Wl)} inMEMWM and SVMWM (our proposed method),while {Pr(hi|S), WebCountsx(hi)} in MEMWCand SVMWC .
Here, Web(Wl) is a set of minedfeatures from Wl as described in Fig .2.????????
(a Man To Call My Own) ????????????
?- ????????
(a Man To CallMy Own), ????ranchhouse??????????????????????????????
?????????????????????????...
(a Man To Call My Own)- (a Man To CallMy Own), ranchhouse...??????(4/03)???????????,?????????,???????,?????????????????????,????????,?????
...
???????(Academy)??????????,???????????????...
(4/03), , ,, ,... (Academy) ,...Snippet1 retrieved by BSQ: Aman ???
?Snippet2 retrieved by MQ: ????
(meaning Agard)????
?|Cliff De Young| ?
?| ?
?| ?
?| EO?????????
| The Secret Life of Zoey (TV) ????
?2002 ????????????????
, ?????
, ?????
, ??????
, Avery Raskin.
?????
?Larry Carter.
??
?4.92?|Cliff De Young| | | | EO| The Secret Life of Zoey (TV) 2002, , ,, Avery Raskin.
Larry Carter.
4.92UNESCO.
General Conference; 32nd; Election of member????????????????.
?.
1987--1991.
???????????????????.
????.
(1976).
1987--1991.
??????????????.
2001--2005.
????.
1993--1997....UNESCO.
General Conference; 32nd; Election of e ber?
?
?
.
.
1987--1991.
??
?
.
.
(1976).
1987--1991.
??
.
2001--2005. .
1993--1997....Snippet3 retrieved by MQ: ??????
(meaning Rawcliffe)Snippet4 retrieved by MQ: ??????
(meaning Aldersey)Figure 3: Snippets causing errors in Web countsThe results in the tables show that our systemsconsistently outperformed systems based on Webcounts, especially for Chinese.
This was due to thedifference between languages.
Japanese and Chi-nese do not use spaces between words.
However,Japanese is written using three different alphabetsystems, called Hiragana, Katakana, and Kanji, thatassist word segmentation.
Moreover, words writtenin Katakana are usually Japanese transliterations offoreign words.
This makes it possible for a Websearch engine to effectively retrieve Web pages con-taining given Japanese transliterations.
Like En-glish, Korean has spaces between words (or wordphrases).
As the spaces in the languages reduce am-biguity in segmenting words, a Web search enginecan correctly identify Web pages containing givenKorean transliterations.
In contrast, there is a se-vere word-segmentation problem with Chinese thatcauses Chinese Web search engines to incorrectlyretrieve Web pages, as shown in Fig.
3.
For example,Snippet1is not related to ?Aman?
but to ?a man?.238Snippet2contains a super-string of a given Chinesequery, which corresponds to ?Academy?
rather thanto ?Agard?, which is the English counterpart of theChinese transliteration.
Moreover, Web searchengines ignore punctuation marks in Chinese.
InSnippet3and Snippet4, ?,?
and ??
in the under-lined terms are disregarded, so the Web counts basedon such Web documents are noisy.
Thus, noise inthe Chinese Web counts causes systems based onWeb counts to produce more errors than our sys-tems do.
Our proposed method can filter out suchnoise because our systems take punctuation marksand the contexts of transliterations in Web mininginto consideration.
Thus, our systems based on fea-tures mined from the Web were able to achieve thebest performance.
The results revealed that our sys-tems based on the Web-mining technique can effec-tively be used to select transliteration hypotheses re-gardless of the language.6.2 Contribution of Web corporaECSet EJSet EKSetSVM MEM SVM MEM SVM MEMBase 73.3 73.8 67.0 66.1 66.0 66.4W181.7 79.7 87.6 87.3 86.1 85.1W280.8 79.5 86.9 86.0 83.8 82.1W377.2 76.7 83.0 82.8 79.8 77.3W1+283.8 82.3 88.5 87.9 86.3 85.9W1+381.9 80.1 87.6 87.8 86.1 84.7W2+381.4 79.8 88.0 87.7 85.1 84.3WAll83.9 82.0 88.5 88.2 86.7 85.8Table 6: Contribution of Web corporaIn Web mining, we used W1, W2, and W3, col-lected by respective queries Q1=(S and hi), Q2=S,and Q3=hi.
To investigate their contribution, wetested our proposed method with different combina-tions of Web corpora.
?Base?
is a baseline systemthat only uses Pr(hi|S) as features but does not usefeatures mined from the Web.
We added featuresmined from different combinations of Web corporato ?Base?
from W1to WAll.In Table 6, we can see that W1, a set of Web pagesretrieved by Q1, tends to give more relevant infor-mation than W2and W3, because Q1can searchmore Web pages containing both S and hi in the top-100 snippets if S and hi are a correct transliterationpair.
Therefore, its performance tends to be superiorin Table 6 if W1is used, especially for ECSet.
How-ever, as W1occasionally retrieves few snippets, it isnot able to provide sufficient information.
Using W2or W3, we can address the problem.
Thus, combina-tions of W1and others (W1+2, W1+3, WAll) pro-vided better WA than W1.7 DiscussionSeveral Web mining techniques for translitera-tion lexicons have been developed in the last fewyears (Jiang et al, 2007; Oh and Isahara, 2006).The main difference between ours and those previ-ous ones is in the way a set of transliteration hy-potheses (or candidates) is created.Jiang et al (2007) generated Chinese transliter-ations for given English words and searched theWeb using the transliterations.
They generated onlythe best transliteration hypothesis and focused onWeb mining to select transliteration lexicons ratherthan selecting transliteration hypotheses.
The besttransliteration hypothesis was used to guide Websearches.
Then, transliteration candidates weremined from the retrieved Web pages.
Therefore,their performance greatly depended on their abil-ity to mine transliteration candidates from the Web.However, this system might create errors if it can-not find a correct transliteration candidate from theretrieved Web pages.
Because of this, their sys-tem?s coverage and WA were relatively poor thanours 8.
However, our transliteration process was ableto generate a set of transliteration hypotheses withexcellent coverage and could thus achieve superiorWA.Oh and Isahara (2006) searched the Web usinggiven source words and mined the retrieved Webpages to find target-language transliteration candi-dates.
They extracted all possible sequences oftarget-language characters from the retrieved Websnippets as transliteration candidates for which thebeginnings and endings of the given source word8Since both Jiang et al?s (2007) and ours used Chinesetransliterations of personal names as a test set, we can indirectlycompare our coverage and WA with theirs (Jiang et al, 2007).Jiang et al (2007) achieved a 74.5% coverage of transliterationcandidates and 47.5% WA, while ours achieved a 94.6% cov-erage of transliteration hypotheses and 82.0?83.9% WA239and the extracted transliteration candidate were pho-netically similar.
However, while this can exponen-tially increase the number of transliteration candi-dates, ours used the n-best transliteration hypothe-ses but still achieved excellent coverage.8 ConclusionWe have described a novel approach to selectingtransliteration hypotheses based on Web mining.
Wefirst generated CJK transliteration hypotheses for agiven English word and retrieved Web pages us-ing the transliteration hypotheses and the given En-glish word as queries for a Web search engine.
Wethen mined features from the retrieved Web pagesand trained machine-learning algorithms using themined features.
Finally, we selected transliterationhypotheses by ranking them.
Our experiments re-vealed that our proposed method worked well re-gardless of the language, while simple Web countswere not effective, especially for Chinese.Because our method was very effective in select-ing transliteration pairs, we expect that it will alsobe useful for selecting translation pairs.
We plan toextend our method in future work to selecting trans-lation pairs.ReferencesY.
Al-Onaizan and Kevin Knight.
2002.
Translatingnamed entities using monolingual and bilingual re-sources.
In Proc.
of ACL ?02, pages 400?408.J.
Breen.
2003.
EDICT Japanese/English dictionary .le.The Electronic Dictionary Research and DevelopmentGroup, Monash University.
http://www.csse.monash.edu.au/?jwb/edict.html.I.
Goto, N. Kato, N. Uratani, and T. Ehara.
2003.Transliteration considering context information basedon the maximum entropy method.
In Proc.
of MT-Summit IX, pages 125?132.Gregory Grefenstette, Yan Qu, and David A. Evans.2004.
Mining the Web to create a language modelfor mapping between English names and phrases andJapanese.
In Proc.
of Web Intelligence, pages 110?116.Long Jiang, Ming Zhou, Lee-Feng Chien, and ChengNiu.
2007.
Named entity translation with Web min-ing and transliteration.
In Proc.
of IJCAI, pages 1629?1634.Thorsten Joachims.
2002.
Learning to Classify Text Us-ing Support Vector Machines: Methods, Theory andAlgorithms.
Kluwer Academic Publishers.I.
H. Kang and G. C. Kim.
2000.
English-to-Koreantransliteration using multiple unbounded overlappingphoneme chunks.
In Proc.
of COLING ?00, pages418?424.Mirella Lapata and Frank Keller.
2005.
Web-basedmodels for natural language processing.
ACM Trans.Speech Lang.
Process., 2(1):3.H.
Li, M. Zhang, and J. Su.
2004.
A joint source-channelmodel for machine transliteration.
In Proc.
of ACL?04, pages 160?167.H.M.
Meng, Wai-Kit Lo, Berlin Chen, and K. Tang.2001.
Generating phonetic cognates to handle namedentities in English-Chinese cross-language spokendocument retrieval.
In Proc.
of Automatic SpeechRecognition and Understanding, 2001.
ASRU ?01,pages 311?314.Y.
S. Nam.
1997.
Foreign dictionary.
Sung An Dang.Jong-Hoon Oh and Key-Sun Choi.
2002.
An English-Korean transliteration model using pronunciation andcontextual rules.
In Proc.
of COLING2002, pages758?764.Jong-Hoon Oh and Hitoshi Isahara.
2006.
Mining theWeb for transliteration lexicons: Joint-validation ap-proach.
In Web Intelligence, pages 254?261.Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.2006.
A comparison of different machine transliter-ation models.
Journal of Artificial Intelligence Re-search (JAIR), 27:119?151.Yan Qu and Gregory Grefenstette.
2004.
Finding ideo-graphic representations of Japanese names written inLatin script via language identification and corpus val-idation.
In Proc.
of ACL ?04, pages 183?190.Richard Schwartz and Yen-Lu Chow.
1990.
The N-bestalgorithm: An efficient and exact procedure for findingthe N most likely sentence hypothesis.
In Procs.
ofICASSP ?90, pages 81?84.Xinhua News Agency.
1992.
Chinese transliteration offoreign personal names.
The Commercial Press.L.
Zhang.
2004.
Maximum entropy model-ing toolkit for python and C++.
http://homepages.inf.ed.ac.uk/s0450736/software/maxent/manual.pdf.240
