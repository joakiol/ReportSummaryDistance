Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP 2009, pages 15?18,Suntec, Singapore, 6 August 2009.c?2009 ACL and AFNLPSome Challenges in the Design ofComparative and Evaluative Question Answering SystemsNathalie Rose LimDe La Salle University-Manila2401 Taft Avenue, PhilippinesUniversit?e Paul Sabatier118 route de Narbonne, Francenats.lim@delasalle.phPatrick Saint-DizierIRIT-Universit?e Paul Sabatier118 route de Narbonne,Toulouse, Francestdizier@irit.frRachel RoxasDe La Salle University-Manila2401 Taft Avenue,Manila, Philippinesrachel.roxas@dlsu.edu.phAbstractComparative and evaluative question an-swering (QA) requires a detailed semanticanalysis of comparative expressions andcomplex processing.
Semantics of predi-cates from questions have to be translatedto quantifiable criteria before extractionof information can be done.
This paperpresents some challenges faced in answer-ing comparative and evaluative questions.An application on the domain of businessintelligence is discussed.1 IntroductionIn the recently updated paper by Burger, et al(2009), it is indicated that new types of questionslike evaluative and comparative questions mustbe targeted in question answering (QA) systems.Evaluative refers to the consideration of at leastone property or criteria over one or more enti-ties and the computation of the associated values.Comparative refers to the evaluation of objects de-pending on one or more criteria and classifyingthose objects depending on the returned values.Included in comparative is the identification of theextreme, i.e., the superlatives, the topmost objects.In such cases, the focus of the questions is on theproperties at stake in the evaluation, leading tothe comparison.
Thus, comparative and evaluativeQA involves answering questions that require vari-ous forms of inference related to evaluation beforean answer can be given.
Since evaluation is nec-essary, the answer is not lifted from source text, asin the case of answering factoid, definition, or listquestions.
Instead, natural language answers willhave to be constructed from the results of numericand non-numeric evaluations of the criteria.Currently, to our knowledge, there are no sys-tems that answer comparative and evaluative ques-tions.
The closest applications to comparing orevaluating information are implemented throughnatural language database interfaces (Olawsky,1989) and database queries (e.g., via SQL state-ments).
In the former, the user is prompted tochoose among a set of candidate interpretations ofcomparative expressions to indicate his intent.
Thecomparisons are based on quantifiable predicates(i.e., those measurable by count, mass, or value).Using database queries restrict the possible ques-tions that can be raised and is far less natural anduser-friendly than using human language.
It alsodoes not allow producing cooperative responses.Recent researches in linguistics on the seman-tics of comparatives and superlatives (Kennedy,2006) can be used as a basis in answering com-parative and evaluative questions.
The next sec-tion discusses some challenges we have identifiedas crucial for the development of comparative andevaluative QA systems.
We briefly propose someresearch directions we have explored or evaluated.We end this short document by a few illustrationsfrom two applications we have worked on duringthe past year.2 ChallengesThe processes involved in classic components of aQA system are not only more complex but differ-ent for comparative and evaluative QA.2.1 Question Analysis and Semantics ofComparativesA question analyzer must identify the comparativeexpressions in the question and decompose itinto meaningful constituents, among which arethose properties that will be evaluated and theparameters of the comparison.
Issues include:- Identifying the type of comparisonComparisons may be in relation to propertieswithin the same object, degree of comparisons ofthe same property between different objects, or15different properties of different objects (Kennedy,2006).
In some simple situations, comparativerelations in sentences can be extracted automati-cally via machine learning (Jindal and Liu, 2006).Their approach determines whether the expressionis non-equal gradable, equative, or superlative.From this, the type of comparison may be deter-mined from the semantics of the predicate and theproperties of the objects through the pairabilityconstraints.
In our approach, we want to explorein more depth semantic and conceptual issues andtheir dependence to context, users, and domains.- Determining semantic meaning and con-verting to quantifiable measuresThe properties at stake in the comparison areembedded in the semantics of the words in thequestion, and possibly in the context that comeswith the question.
To date, there is obviously nowidely available lexical resource containing anexhaustive list of comparative predicates, appliedto precise terms, together with the propertiesinvolved.
These can possibly be derived, to a lim-ited extent, from existing resources like FrameNetor from an ontology where relationships betweenconcepts and terms can be mapped.
However,this is tractable for very simple situations, andin most cases, identifying those properties isa major challenge.
We plan to explore, overrestricted domains, ways to accurately identifythose properties through different resources (likeGenerative Lexicon) and elaborate on inferentialmodels to associate properties for evaluation.- Determining limits, ranges, and values thatare relative depending on the objectThe standard of comparison (i.e., the value) asso-ciated to the predicate may be different based onthe context, i.e., depending on the object that it isassociated to and on the type of predicate.
Prop-erties of predicates may be underspecified and/orpolysemic and would gain context only when as-sociated with the object.
One such predicate is in-novative.
The following are some properties thatcan be used to evaluate innovative.?
innovative product: type of product, numberof entities interested in acquiring the product?
innovative company: strategy employed, typeof product it produces?
innovative research: number of papers pub-lished on the same research, number of cita-tions from other authorsTo automatically determine the properties, includ-ing default values, to be used in the evaluation,other available sources indicating some range ofvalues may be tapped, as is done in answer fusion(Girju, 2001).
But rather than retrieving the partialanswer, properties needed for evaluation mustbe retrieved or inferred.
In terms of values, wehave either numerical values (where comparisonsare quite easy to handle) or textual values (thatare often discrete).
It is then necessary to definecomparative scales along basic properties so thatthose values get ordered.
This is a major challengefor our project.- Processing superlatives and other forms ofquantification related to comparisonsSuperlatives and other forms of quantifications inconnection with comparative expressions can alsobe used on top of the basic evaluative expressions.As the semantics of the predicate may encompassmultiple properties, strict evaluation of these maytrim the list prematurely.
Consider the question:?
Which companies take the most risk?Take most risk entails different dimensions frombeing conservative.
In the context of businessintelligence, evaluation could be in terms of theamount of investments, types of products investedin, the partners being taken, or all of these criteria.If a strict evaluation of all these criteria is done,the result may not be complete or accurate.
Weare exploring on relaxing the evaluation of mul-tiple properties before determining the top resultsand on evaluating the superlative of each of theproperties so as to identify which of the propertiesthe object has not met.2.2 Answer DeterminationOnly when the predicate/s is/are decomposed intoproperties can proper evaluation take place.
Wehave two situations: either the QA system is con-nected to a database (which may have been con-structed from natural language data as in the caseof economic news) or it searches for the responseon the Web.
In the first case, the main challenge isto convert the concepts of the query into those ofthe conceptual schema of the database.In the second case, relevant data must besearched on the Web.
A straightforward procedure16consists of extracting keywords from the ques-tion, then getting results from search engines fromwhich, via local grammars associated to proper-ties, relevant values may be extracted.
We alreadysuccessfully conducted such an experiment for nu-merical data fusion (Moriceau, 2006).2.3 Response GenerationThe answer cannot be lifted from the source text,thus a response generator component should bepart of a comparative and evaluative QA system.As response is to be generated from the resultsof numeric and textual comparisons of the crite-ria, it is necessary to go through complex sentencegeneration, involving comparative expressions.
Incase the response is not direct, it is also neces-sary to elaborate adapted forms of cooperativity,by providing the user with adequate forms of ex-planations, elaborations, examples (of properties),and other relevant information.
This is clearly amajor challenge, since the quality of the responsewill reflect the overall credibility of the system.3 ApplicationsWe first carried out a relatively simple experimenton the business intelligence domain, where the cri-teria for evaluation are almost an exact science.The difficulty is to get the expertise in economicsand to formulate it in terms of properties ?visible?in the related economic news.
An example ques-tion is given in (1).1.
Which private biotechcompanies in Asia have thehighest number of transactionsfrom 2005 to 2008?News articles are used as the source of informationto answer these types of questions.
They are fac-tual, structured, and concise.
They do not containconflicting information, though there is the possi-bility of updates but the date is normally includedin the information to provide temporal perspective.Rhetorical relations between sentences are beingexplored to give hints as to the relevance of infor-mation in the sentences.
Semantic dependenciesvia thematic roles of arguments within each sen-tence are being considered to extract data.
Fromthe semantic dependency representation, a con-ceptual representation of these information is cre-ated using type-feature structure with the follow-ing information:Location and Date are complex types contain-ing info like country and month, respectively.TransCategory and TransType are transaction cat-egories and its transaction subtype.
There can beat most ten companies, where each contains infor-mation like the name and location of the company.The ContractedItem is also a complex type con-taining information like worth of the product.To build this knowledge base, other web sourcesare used and a set of inferencing rules is developedto retrieve and store the required information.Similarly, questions are represented semanti-cally using thematic roles.
Then, a conceptual rep-resentation is built to map the question focus withthe answer from the type-feature representation ofthe news.
To illustrate, the question type-featurestructure contains the following:For criteria or properties that are already inthe conceptual representation, these are used inthe evaluation and/or comparison.
For question(1), occurrences of each company that fit the con-straints (e.g., location Asia), are counted and theresulting values are compared to determine the topcompanies.However, in (2), the sample question involves anon-directly translatable predicate.2.
Does Company X take more risksthan Company Y?Non-directly translatable predicates can be quan-tifiable by one criterion (e.g., active company:company with above-mean number of transac-tions), quantifiable by multiple criteria (e.g., com-pany that take-risk: active company that has trans-actions every year, and has alliances every yearbut always with new partners or has unstable part-ners), polysemous (e.g., stable can mean ability toresist motion, steady in purpose, or established),17and/or underspecified (e.g., stable company vs.stable partner, though partner is also a company,the criteria is not the same.
Stable company is anactive company that may not have alliances everyyear or have alliances every year but always withold partners, whereas a stable partner is a companywith alliances every year).
There is also the issueof metonymy.
In the context of company, the setof quantifiable properties associated to companycould be number of employees, number of transac-tions, type of partners, and so on.
Choosing whichof these properties to associate to evaluate a pred-icate (like stable) is a challenge.In this application, the categories, classifica-tions, boundaries (what the term entails), and eval-uation criteria of the terms are defined by an ex-pert, so the result is consistent and objective.
Thechallenge is to analyze the given information andconvert it to machine tractable instructions.
Atpresent, set theory is used to define constraints andto generate the answer.
It should be noted that itis one expert?s interpretation of the terminologiesused in the constraints.
Others may have differentcriteria to associate with the predicates.Other domains, like tourism, may be more chal-lenging.
Aside from information sources being notpurely textual (i.e., some may be in tables or di-agrams), the evaluation criteria for questions (3)and (4) may be subjective and may produce con-flicting results.
For example, value for moneyis subjective since certain amenities may not beimportant to the user.
This can be resolved byprompting the user for additional criteria, by hav-ing a user profile, or by comparing with other enti-ties (in this case, other hotels) to determine what isconsidered the norm (as a gauge to what is excep-tional).
It is also possible to generate different re-sults based on the various criteria and present theseto the user with explanations on the basis used.3.
Which hotels in Singaporeoffer the most value for moneyfor stay from August 28, 2009?4.
Which Asian cities are mostkid-friendly?5.
Which hotels in Asia are mostkid-friendly?As mentioned, the properties at stake in the eval-uation could be different if the question focus waschanged, as in the case of ?kid-friendly?
in ques-tion (5).
In question (4), the criteria for a kid-friendly city could be one with avenues for funand entertainment (like theme parks, zoos, parks)and a city with low crime rate (or specifically, lowchild abuse rate).
On the other hand, a kid-friendlyhotel would be one with amenities for supervisedor planned activities, proximity to entertainmentvenues, larger rooms, or special menu for kids.The criteria or properties cannot be easily and re-liably accessed from an ontology.
Our challengehere is to elaborate means to get those properties.A direction we are investigating includes learn-ing these properties from the web, but we may befaced with the recurrent problem of data sparse-ness, besides the fact that the web contains manyerroneous statements.AcknowledgmentsThe authors would like to thank Prof. Brigitte Gayof Ecole Superieure de Commerce - Toulouse forher invaluable inputs regarding comparative andevaluative questions in business intelligence.ReferencesJohn Burger, et al 2009.
Issues, Tasks andProgram Structures to Roadmap Research inQuestion & Answering.
Available in: www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper v2.doc.Roxana Girju.
2001.
Answer Fusion with On-line Ontology Development.
In Students ResearchWorkshop of the Second Meeting of the NorthAmerican Chapter of the ACL.
Available in:lyle.smu.edu/?
roxana/papers/NAACL01.ps.Nitin Jindal and Bing Liu.
2006.
Mining ComparativeSentences and Relations.
In Proceedings of the 21stAAAI Conference on Artificial Intelligence.
AAAIPress, California,USA.Christopher Kennedy.
2006.
Comparatives, SemanticsOf.
In K. Allen (section editor) Lexical and LogicalSemantics; Encyclopedia of Language and Linguis-tics, 2nd Edition.
Elsevier, Oxford.V?eronique Moriceau.
2006.
Numeric Data Integrationfor Cooperative Question-Answering.
In Proceed-ings of the Knowledge and Reasoning for LanguageProcessing Workshop (KRAQ 2006).
ACL, Italy.Duane Olawsky.
1989.
The Lexical Semantics of Com-parative Expressions in a Multi-level Semantic Pro-cessor.
In Proceedings of the 27th Annual Meetingon ACL.
ACL, USA.18
