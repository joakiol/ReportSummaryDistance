Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 83?87,Baltimore, Maryland USA, 27 June 2014.c?2014 Association for Computational LinguisticsAs Long as You Name My Name Right:Social Circles and Social Sentiment in the Hollywood HearingsOren Tsur?
?Dan Calacci?David Lazer?borentsur@seas.harvard.edu dcalacci@ccs.neu.edu d.lazer@neu.edu?Lazer Laboratory, Northeastern University?School of Engeneering and Applied Sciences, Harvard UniversitybHarvard Kennedy School, Harvard UniversityAbstractThe Hollywood Blacklist was based ona series of interviews conducted by theHouse Committee on Un-American Activ-ities (HUAC), trying to identify membersof the communist party.
We use variousNLP algorithms in order to automaticallyanalyze a large corpus of interview tran-scripts and construct a network of the in-dustry members and their ?naming?
rela-tions.
We further use algorithms for Senti-ment Analysis in order to add a psycholog-ical dimension to the edges in the network.In particular, we test how different typesof connections are manifested by differentsentiment types and attitude of the inter-viewees.
Analysis of the language used inthe hearings can shed new light on the mo-tivation and role of network members.1 IntroductionA growing body of computational research isfocused on how language is used and how itshapes/is shaped by a community of speakers.Computational works in the nexus of languageand the social arena deal with various topics suchas language accommodation (Danescu-Niculescu-Mizil and Lee, 2011; Danescu-Niculescu-Mizilet al., 2011), demographic language variation(Eisenstein et al., 2010; O?Connor et al., 2010),the factors that facilitate the spread of informationin Q&A forums and social networks (Adamic etal., 2008; Bian et al., 2009; Romero et al., 2011) orthe correlation between words and social actions(Adali et al., 2012).All of these works analyze the language and thesocial dynamics in online communities, mainlydue to the increasing popularity of online socialnetworks and greater availability of such data.However, large scale socio-linguistic analysisshould not be restricted to online communities andcan be applied in many social and political settingsbeyond the online world.
Two examples are thestudy of power structures in arguments before theU.S.
Supreme Court (Danescu-Niculescu-Mizil etal., 2012) and the evolution of specific words andphrases over time as reflected in Google Books(Goldberg and Orwant, 2013).In this paper we propose using network scienceand linguistic analysis in order to understand thesocial dynamics in the entertainment industry dur-ing one of its most controversial periods ?
the ?redscare?
and the witch hunt for Communists in Hol-lywood during 1950?s.Historical background The Hollywood hear-ings (often confused with Senator McCarthy?shearings and allegations) were a series of inter-views conducted by the House Committee on Un-American Activities (HUAC) in the years 1947?1956.
The purpose of the committee was toconduct ?hearings regarding the communist in-filtration of the motion picture industry?
(fromthe HUAC Annual Report).
The committee sub-poenaed witnesses such as Ayn Rand (writer),Arthur Miller (writer), Walt Disney (producer), fu-ture U.S. president Ronald Reagan (Screen ActorsGuild), Elia Kazan (writer, actor, director) and Al-bert Maltz (Screen Writers Guild).
Some of thewitnesses were ?friendly?
while some others wereuncooperative1, refusing to ?name names?
or selfincriminate2.
Those who were named and/or wereuncooperative were often jailed or effectively losttheir job.Arguably, many friendly witnesses felt theywere complying with their patriotic duty.
Many1A note about terminology: by using the terms friendlyand uncooperative there is no implied moral judgment ?
theseare the terms used in the literature.2It should be noted that being a member of the Communistparty was not illegal, however, some individuals avoided self?incrimination?
either in an effort to protect their job or asan ideological declaration in favor of privacy protection as acivil right protected by the constitution.83others were threatened or simply manipulated toname names, and some later admitted to coop-erating for other reasons such as protecting theirwork or out of personal vendettas and professionaljealousies.
It is also suspected that some nam-ing occurred due to increasing professional ten-sion between some producers and the Screen Writ-ers Guild or (Navasky, 2003).Motivation In this work we analyze a collectionof HUAC hearings.
We wish to answer the follow-ing questions:1.
Do sentiment and other linguistic categoriescorrelate with naming relations?2.
Can we gain any insight on the social dynam-ics between the people in the network?3.
Does linguistic and network analysis supportany of the social theories about dynamics atHollywood during that time?In order to answer the questions above we builda social graph of members of the entertainment in-dustry based on the hearings and add sentiment la-bels on the graph edges.
Layering linguistic fea-tures on a the social graph may provide us withnew insights related to the questions at hand.
Inthis short paper we describe the research frame-work, the various challenges posed by the data andpresent some initial promising results.2 DataIn this work we used two types of datasets: Hear-ing Transcripts and Annual Reports.
Snippetsfrom hearings can be found in Figures 1(a) and1(b), Figure 1(c) shows a snippet from an annualreport.
The transcripts data is based on 47 inter-views conducted by the HUAC in the years 1951?2.
Each interview is either a long statement (1(a) )or a sequence of questions by the committee mem-bers and answers by a witness (1(b)).
In total, ourhearings corpus consists of 2831 dialogue acts andhalf a million words.3 Named Entity Recognition andAnaphora ResolutionThe snippets in Figure 1 illustrates some of thechallenges in processing HUAC data.
The firstchallenge is introduced by the low quality of theavailable documents.
Due to the low quality of(a) A snippet from the testimony of Elia Kazan, (actor, writer and director, 3times Academy Awards winner), 4.10.1952.
(b) A snippet from the testimony of Harold Ashe?s (journalist) testimony 9.17-19.1951.
(c) A snippet from 1951 annual report.Figure 1: Snippets from HUAC hearings and anannual report.the documents the OCR output is noisy, contain-ing misidentified characters, wrong alignment ofsentences and missing words.
These problems in-troduce complications in tasks like named entityrecognition and properly parsing sentences.Beyond the low graphic quality of the docu-ments, the hearings present the researcher with thetypical array of NLP challenges.
For example, thehearing excerpt in 1(b) contains four dialogue actsthat need to be separated and processed.
The com-mittee member (Mr. Tavenner) mentions the nameStanley Lawrence, later referred to by the witness(Mr. Ashe) as Mr. Lawrence and he thus corefer-ence resolution is required before the graph con-struction and the sentiment analysis phases.As a preprocessing stage we performed namedentity recognition (NER), disambiguation and uni-fication.
For the NER task we used the StanfordNER (Finkel et al., 2005) and for disambiguationand unification we used a number of heuristicsbased on edit distance and name distribution.84We used the Stanford Deterministic Corefer-ence Resolution System (Lee et al., 2011) to re-solve anaphoric references.4 Naming Graph vs. Mentions GraphIn building the network graph of the members ofthe entertainment industry we distinguish betweenmentioning and naming in our data.
While manynames may be mentioned in a testimony (either bya committee member or by the witness, see ex-ample in Figures 1(a) and 1(b)), not all names arepractically ?named?
(=identified) as Communists.We thus use the hearings dataset in order to builda social graph of mentions (MG) and the annual re-ports are used to build a naming graph (NG).
TheNG is used as a ?gold standard?
in the analysisof the sentiment labels in the MG. Graph statisticsare presented in Table 1.While the hearings are commonly perceived asan ?orgy of informing?
(Navasky, 2003), the dif-ference in network structure of the graphs portraysa more complex picture.
The striking difference inthe average out degree suggests that while manynames were mentioned in the testimonies (eitherin a direct question or in an answer) ?
majority ofthe witnesses avoided mass-explicit naming3.
Thevariance in outdegree suggests that most witnessesdid not cooperate at all or gave only a name ortwo, while only a small number of witnesses gavea long list of names.
These results are visuallycaptured in the intersection graph (Figure 2) andwere also manually verified.The difference between the MG and the NGgraph in the number of nodes with out-going edges(214 vs. 66) suggests that the HUAC used otherinformers that were not subpoenaed to testify in ahearing4.In the remainder of this paper we analyze the thedistribution of the usage of various psychologicalcategories based on the role the witnesses play.5 Sentiment Analysis and PsychologicalCategories5.1 Sentiment AnalysisWe performed the sentiment analysis in two dif-ferent settings: lexical and statistical.
In the lexi-3Ayn Rand and Ronald Reagan, two of the most ?friendly?witnesses (appeared in front of the HUAC in 1947), did notname anyone.4There might be some hearings and testimonies that areclassified or still not publicly accessible.MG NG IntersectionNum of nodes 1353 631 122Num of edges 2434 842 113Nodes / Edges 0.55 0.467 1Avg.
out degree 36.87 3.93 8.7Avg.
in degree 1.82 1.83 1.04Var(outdegree) 3902.62 120.75 415.59Var(indegree) 4.0 2.51 1.04Nodes with out going edges 66 214 13Nodes with incoming edges 1341 459 109Reciprocity 0.016 0.012 0Table 1: Network features of the Mentions graph,the Naming graph and the intersection of thegraphs.Figure 2: Naming graph based on the intersec-tion of the mentions and the naming data.
Largernode size indicates a bigger out degree; Color in-dicates the in degree (darker nodes were namedmore times).cal setting we combine (Ding et al., 2008) and theLIWC lexicon (Tausczik and Pennebaker, 2010).In the statistical setting we use NaSent (Socher etal., 2013).The motivation to use both methods is twofold:first ?
while statistical models are generally morerobust, accurate and sensitive to context, they re-quire parsing of the processed sentences.
Parsingour data is often problematic due to the noise in-troduced by the OCR algorithm due to the poorquality of the documents (see Figure 1).
We ex-pected the lexicon-based method to be more toler-ant to noisy or ill-structured sentences.
We optedfor the LIWC since it offers an array of sentimentand psychological categories that might be rele-vant in the analysis of such data.85Stanford LIWCPos 75 292Neg 254 37Table 2: Confusion matrix for Stanford and LIWCsentiment algorithms.Aggregated Sentiment A name may be men-tioned a number of times in a single hearing, eachtime with a different sentiment type or polarity.The aggregated sentiment weight of a witness i to-ward a mentioned name j is computed as follows:sentiment(i, j) = maxc?CAT?k?Uijscore(ukij, c)|Uij|(1)Where CAT is the set of categories used byLIWC or Stanford Sentiment and Uijis the setof all utterances (dialogue acts) in which witnessi mentions the name j.
The score() function isdefined slightly different for each setting.
In theLIWC setting we define score as:score(ukij, c) =|{w ?
ukij|w ?
c}||ukij|(2)In the statistical setting, Stanford Sentiment re-turns a sentiment category and a weight, we there-fore use:score(ukij, c) ={wc, if sentiment found0, if c was not returned(3)Unfortunately, both approaches to sentimentanalysis were not as useful as expected.
Mostgraph edges did not have any sentiment label, ei-ther due to the limited sentiment lexicon of theLIWC or due to the noise induced in the OCRprocess, preventing the Stanford Sentiment enginefrom parsing many of the sentences.
Interestingly,the two approaches did not agree on most sen-tences (or dialogue acts).
The sentiment confu-sion matrix is presented in Table 2, illustrating thechallenge posed by the data.5.2 Psychological CategoriesThe LIWC lexicon contains more than just posi-tive/negative categories.
Table 3 presents a sampleof LIWC categories and associated tokens.
Fig-ure 3 presents the frequencysave in which eachpsychological category is used by friendly and un-cooperative witnesses.
While the Pronoun cate-gory is equally used by both parties, the uncooper-ative witnesses tend to use the I, Self and You cate-gories while the friendly witnesses tend to use theOther and Social.
A somewhat surprising resultis that the Tentat category is used more by friendlywitnesses ?
presumably reflecting their discomfortwith their position as informers.Figure 3: Frequencies of selected LIWC cate-gories in friendly vs. uncooperative testimonies.Category Typical WordsCogmech abandon, accept, avoid, admit, know, questionExcl although, besides, but, exceptI I, I?d, I?ll, I?m, I?ve, me, mine, my, myselfInsight accept, acknowledge, conclude, know, rationaljob work, position, benefit, dutyNegate no, nope, nothing, neither, never, isn?t , can?tOther he, him, herself, themPreps about, against, along, from, outside, sincePronouns I, anybody, anyone, something, they, youSelf I, mine, ours, myself, usSocial acquaintance, admit, party, comrade, confess, friend, humanTentat ambiguous, tentative, undecided, depend, hesitant, guessYou thou, thoust, thy, y?all, ya, ye, you, you?dTable 3: LIWC categories and examples of typicalwords6 Conclusion and Future WorkIn this short paper we take a computational ap-proach in analyzing a collection of HUAC hear-ings.
We combine Natural Language Process-ing and Network Science techniques in order togain a better understanding of the social dynam-ics within the entertainment industry in its dark-est time.
While sentiment analysis did not proveas useful as expected, analysis of network struc-tures and the language usage in an array of psycho-logical dimensions reveals differences betweenfriendly and uncooperative witnesses.Future work should include a better preprocess-ing of the data, which is also expected to improvethe sentiment analysis.
In future work we will an-alyze the language use in a finer granularity of wit-ness categories, such as the ideological informer,the naive informer and the vindictive informer.
Wealso hope to expand the hearings corpora to in-clude testimonies from more years.ReferencesSibel Adali, Fred Sisenda, and Malik Magdon-Ismail.2012.
Actions speak as loud as words: Predicting86relationships from social behavior data.
In Proceed-ings of the 21st international conference on WorldWide Web, pages 689?698.
ACM.Lada A Adamic, Jun Zhang, Eytan Bakshy, and Mark SAckerman.
2008.
Knowledge sharing and yahooanswers: everyone knows something.
In Proceed-ings of the 17th international conference on WorldWide Web, pages 665?674.
ACM.Jiang Bian, Yandong Liu, Ding Zhou, EugeneAgichtein, and Hongyuan Zha.
2009.
Learning torecognize reliable users and content in social mediawith coupled mutual reinforcement.
In Proceedingsof the 18th international conference on World WideWeb, pages 51?60.
ACM.Cristian Danescu-Niculescu-Mizil and Lillian Lee.2011.
Chameleons in imagined conversations: Anew approach to understanding coordination of lin-guistic style in dialogs.
In Proceedings of the Work-shop on Cognitive Modeling and ComputationalLinguistics, ACL 2011.Cristian Danescu-Niculescu-Mizil, Michael Gamon,and Susan Dumais.
2011.
Mark my words!
Lin-guistic style accommodation in social media.
InProceedings of WWW, pages 745?754.Cristian Danescu-Niculescu-Mizil, Lillian Lee,Bo Pang, and Jon Kleinberg.
2012.
Echoes ofpower: Language effects and power differences insocial interaction.
In Proceedings of WWW, pages699?708.Xiaowen Ding, Bing Liu, and Philip S. Yu.
2008.
Aholistic lexicon-based approach to opinion mining.In Proceedings of the 2008 International Conferenceon Web Search and Data Mining, WSDM ?08, pages231?240, New York, NY, USA.
ACM.Jacob Eisenstein, Brendan O?Connor, Noah A Smith,and Eric P Xing.
2010.
A latent variable modelfor geographic lexical variation.
In Proceedings ofthe 2010 Conference on Empirical Methods in Nat-ural Language Processing, pages 1277?1287.
Asso-ciation for Computational Linguistics.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.
Association for Computational Lin-guistics.Yoav Goldberg and Jon Orwant.
2013.
Syntactic-ngrams over time from a very large corpus of englishbooks.
In Second Joint Conference on Lexical andComputational Semantics.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve coref-erence resolution system at the conll-2011 sharedtask.
In Proceedings of the Fifteenth Conference onComputational Natural Language Learning: SharedTask, pages 28?34.
Association for ComputationalLinguistics.Victor S Navasky.
2003.
Naming Names: With a NewAfterword by the Author.
Macmillan.Brendan O?Connor, Jacob Eisenstein, Eric P Xing, andNoah A Smith.
2010.
A mixture model of demo-graphic lexical variation.
In Proceedings of NIPSworkshop on machine learning in computational so-cial science, pages 1?7.Daniel M Romero, Brendan Meeder, and Jon Klein-berg.
2011.
Differences in the mechanics of in-formation diffusion across topics: idioms, politi-cal hashtags, and complex contagion on twitter.
InProceedings of the 20th international conference onWorld wide web, pages 695?704.
ACM.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, pages 1631?1642, Stroudsburg, PA, October.Association for Computational Linguistics.Yla R. Tausczik and James W. Pennebaker.
2010.
ThePsychological Meaning of Words: LIWC and Com-puterized Text Analysis Methods.
Journal of Lan-guage and Social Psychology, 29(1):24?54, March.87
