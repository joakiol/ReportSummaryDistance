Corpus-Based Linguistic Indicators for Aspectual ClassificationEric V. SiegelDepartment  of Computer  ScienceColumbia UniversityNew York, NY 10027Abst rac tFourteen indicators that measure the frequencyof lexico-syntactic phenomena linguistically re-lated to aspectual class are applied to aspec-tual classification.
This group of indicators isshown to improve classification performance fortwo aspectual distinctions, stativity and com-pletedness (i.e., telicity), over unrestricted setsof verbs from two corpora.
Several of these in-dicators have not previously been discovered tocorrelate with aspect.1 In t roduct ionAspectual classification maps clauses to a smallset of primitive categories in order to reasonabout time.
For example, events such as,"You called your father," are distinguished fromstates such as, "You resemble your father.
"These two high-level categories correspond toprimitive distinctions in many domains, e.g., thedistinction between procedure and diagnosis inthe medical domain.Aspectual classification further distinguishesevents according to completedness (i.e., telicity),which determines whether an event reaches aculmination point in time at which a new stateis introduced.
For example, "I made a fire"is culminated, since a new state is introduced- something is made, whereas, "I gazed at thesunset" is non-culminated.Aspectual classification is necessary for inter-preting temporal modifiers and assessing tem-poral entailments (Vendler, 1967; Dowty, 1979;Moens and Steedman, 1988; Dorr, 1992), andis therefore a necessary component for applica-tions that perform certain natural anguage in-terpretation, atural anguage generation, sum-marization, information retrieval, and machinetranslation tasks.Aspect introduces a large-scale, domain-dependent lexical classification problem.
Al-though an aspectual lexicon of verbs would suf-fice to classify many clauses by their main verbonly, a verb's primary class is often domain-dependent (Siegel, 1998b).
Therefore, it is nec-essary to produce a specialized lexicon for eachdomain.Most approaches to automatically catego-rizing words measure co-occurrences betweenopen-class lexical items (Schfitze, 1992; Hatzi-vassiloglou and McKeown, 1993; Pereira etal., 1993).
This approach is limited since co-occurrences between open-class lexical items issparse, and is not specialized for particular se-mantic distinctions uch as aspect.In this paper, we describe an expandableframework to classify verbs with linguistically-specialized numerical indicators.
Each linguis-tic indicator measures the frequency of a lexico-syntactic marker, e.g.
the perfect ense.
Thesemarkers are linguistically related to aspect, sothe indicators are specialized for aspectual clas-sification in particular.
We perform an evalua-tion of fourteen linguistic indicators over unre-stricted sets of verbs from two corpora.
Whenused in combination, this group of indicators isshown to improve classification performance fortwo aspectual distinctions: stativity and com-pletedness.
Moreover, our analysis reveals apredictive value for several indicators that havenot previously been discovered to correlate withaspect in the linguistics literature.The following section further describes as-pect, and introduces linguistic insights that areexploited by linguistic indicators.
The next sec-tion describes the set of linguistic indicatorsevaluated in this paper.
Then, our experimen-tal method and results are given, followed by adiscussion and conclusions.112Table 1: Aspectual classes.
This table comes fromMoens and Steedman (Moens and Steedman, 1988).CulmEVENTS STATESpunctual extendedCULM CULMPROCESSrecognize build a houseNon- POINT PROCESSCulm hiccup run, swimunderstand2 Aspect  in Natura l  LanguageTable 1 summarizes the three aspectual distinc-tions, which compose five aspectual categories.In addition to the two distinctions describedin the previous ection, atomicity distinguishesevents according to whether they have a timeduration (punctual versus extended).
Therefore,four classes of events are derived: culmination,culminated process, process, and point.
Theseaspectual distinctions are defined formally byDowty (1979).Several researchers have developed modelsthat incorporate aspectual class to assess tem-poral constraints between clauses (Passonneau,1988; Dorr, 1992).
For example, stativity mustbe identified to detect emporal constraints be-tween clauses connected with when, e.g., in in-terpreting (1),(1) She had good strength when objectivelytested.the following temporal relationship holds:haveI I I test IHowever, in interpreting (2),(2) Phototherapy was discontinued when thebilirubin came down to 13.the temporal relationship is different:COmeI IdiscontinueI IThese aspectual distinctions are motivated bya series of entailment constraints.
In particu-lar, certain lexico-syntactic features of a clause,such as temporal adjuncts and tense, are con-strained by and contribute to the aspectual classof the clause (Vendler, 1967; Dowty, 1979).Tables 2 illustrates an array of linguistic con-Table 2: Several aspectual markers and associatedconstraints on aspectual class, primarily from Kla-vans' summary (1994).I f  a c lause can occur:  then  it is:with a temporal adverb Event(e.g., then)in progressive ExtendedEventwith a duration in-PP Cu lm Event(e.g., in an hour)in the perfect ense Cu lm Eventor S ta testraints.
Each entry in this table describes anaspectual marker and the constraints on the as-pectual category of any clause that appears withthat marker.
For example, a clause must bean extended event to appear in the progressivetense, e.g.,(3) He was prospering in India.
(extended),which contrasts with,(4) *You were noticing it.
(punctual).and,(5) *She was seeming sad.
(state).As a second example, an event must be cul-minated to appear in the perfect ense, for ex-ample,(6) She had made an attempt.
(culm.
),which contrasts with,(7) *He has cowered own.
(non-culm.
)3 L ingu is t i c  Ind icatorsThe best way to exploit aspectual markers is notobvious, since, while the presence of a marker ina particular clause indicates a constraint on theaspectual class of the clause, the absence thereofdoes not place any constraint.
Therefore, aswith most statistical methods for natural lan-guage, the linguistic onstraints associated withmarkers are best exploited by a system thatmeasures co-occurrence frequencies.
For exam-ple, a verb that appears more frequently in theprogressive is more likely to describe an event.Klavans and Chodorow (1992) pioneered the ap-plication of statistical corpus analysis to aspec-tuai classification by ranking verbs according tothe frequencies with which they occur with cer-tain aspectual markers.Table 3 lists the linguistic indicators evalu-ated for aspectual classification.
Each indica-113Ling Indicator Example Clausef requency~tnot" or "never"temporal adverbno subjectpast/pres particduration in-PPperfectpresent enseprogressivemanner adverbevaluation adverbpast tenseduration for-PPcontinuous adverb(not app l i cab le )She can not  explain why.I saw to it then.He was admitted.... blood pressure going up.She built it in an hour .They have landed.I am happy.I am behaving myself.She studied dil igently.They performed horr ib ly .I was happy.I sang for ten minutes.She will live indefinitely.Table 3: Fourteen linguistic indicators evaluated foraspectual classification.tor has a unique value for each verb.
The firstindicator, f requency,  is simply the frequencywith which each verb occurs over the entirecorpus.
The remaining 13 indicators measurehow frequently each verb occurs in a clausewith the named linguistic marker.
For exam-ple, the next three indicators listed measure thefrequency with which verbs 1) are modified bynot or never, 2) are modified by a temporal ad-verb such as then or frequently, and 3) have nodeep subject (e.g., passive phrases uch as, "Shewas admitted to the hospital").
Further detailsregarding these indicators and their linguisticmotivation is given by Siegel (1998b).There are several reasons to expect superiorclassification performance when employing mul-tiple linguistic indicators in combination ratherthan using them individually.
While individ-ual indicators have predictive value, they arepredictively incomplete.
This incompletenesshas been illustrated empirically by showing thatsome indicators help for only a subset of verbs(Siegel, 1998b).
Such incompleteness i  due in?
part to sparsity and noise of data when com-puting indicator values over a corpus with lim-ited size and some parsing errors.
However, thisincompleteness i  also a consequence of the lin-guistic characteristics of various indicators.
Forexample:?
Aspectual coercion such as iteration com-promises indicator measurements (Moensand Steedman, 1988).
For example, apunctual event appears with the progres-sive in, "She was sneezing for a week.
"(point --, process --.
culminated process)In this example, for a week can only modifyan extended event, requiring the first coer-cion.
In addition, this for-PP also makesan event culminated, causing the secondtransformation.?
Some aspectual markers such as thepseudo-cleft and manner adverbs test forintentional events, and therefore are notcompatible with all events, e.g., "*I dieddi l igent ly ."?
The progressive indicator's predictivenessfor stativity is compromised by the factthat many locat ion  verbs can appear withthe progressive, ven in their stative sense,e.g.
"The book was lying on the shelf.
"(Dowty, 1979)?
Several indicators measure phenomenathat are not linguistically constrained byany aspectuM category, e.g., the presenttense, f requency and not/never indicators.4 Method  and  Resu l tsIn this section, we evaluate the set offourteen linguistic indicators for two aspec-tual distinctions: stativity and completed-ness.
Evaluation is over corpora of med-ical reports and novels, respectively.
Thisdata is summarized in Table 4 (available atwww.
CS.
columbia, edu/~evs/YerbData).First, linguistic indicators are each evalu-ated individually.
A training set is used to se-lect indicator value thresholds for classification.Then, we report the classification performanceachieved by combining multiple indicators.
Inthis case, the training set is used to optimize amodel for combining indicators.
In both cases,evaluation is performed over a separate test setof clauses.The combination of indicators is performedby four standard supervised learning algo-rithms: decision tree induction (Quinlan, 1986),CART (Friedman, 1977), log-linear regression(Santner and Duffy, 1989) and genetic program-ming (GP) (Cramer, 1985; Koza, 1992).A pilot study showed no further improve-ment in accuracy or recall tradeoff by additionallearning algorithms: Naive Bayes (Duda and114stativity completednesscorpus: 3,224 med reports 10 novelss i ze :  1,159,891 846,913parsedclauses: 97,973training: 739 (634 events)testing: 739 (619 events)verbs intest set: 222 204clausesexcluded: be and have stative75,289307 (196 culm)308 (195 culm)Table 4: Two classification problems on differentdata sets.Hart, 1973), Ripper (Cohen, 1995), ID3 (Quin-lan, 1986), C4.5 (Quinlan, 1993), and met-alearning to combine learning methods (Chanand Stolfo, 1993).4.1 Stat iv i tyOur experiments are performed across a cor-pus of 3,224 medical discharge summaries.
Amedical discharge summary describes the symp-toms, history, diagnosis, treatment and outcomeof a patient's visit to the hospital.
These re-ports were parsed with the English Slot Gram-mar (ESG) (McCord, 1990), resulting in 97,973clauses that were parsed fully with no self-diagnostic errors (ESG produced error messageson 12,877 of this corpus' 51,079 complex sen-tences).Be and have, the two most popular verbs, cov-ering 31.9% of the clauses in this corpus, arehandled separately from all other verbs.
Clauseswith be as their main verb, comprising 23.9% ofthe corpus, always denote a state.
Clauses withhave as their main verb, composing 8.0% of thecorpus, are highly ambiguous, and have beenaddressed separately by considering the directobject of such clauses (Siegel, 1998a).4.1.1 Manua l  Mark ing1,851 clauses from the parsed corpus were man-ually marked according to stativity.
As a lin-guistic test for marking, each clause was testedfor readability with "What happened was... ,1A comparison between human markers for thistest performed over a different corpus is re-ported below in Section 4.2.1.
Of these, 3731 Manual labeling followed astrict set of linguistically-motivated guidelines, e.g., negations were ignored(Siegel, 199Sb).Linguistic Stative Event T-testIndicator Mean Mean P-valuefrequency 932.89 667.57 0.0000"not" or "never" 4.44% 1.56% 0.0000temporal adverb 1.00% 2.70% 0.0000no subject 36.05% 57.56% 0.0000past/pres pattie 20.98% 15.37% 0.0005duration in-PP 0.16% 0.60% 0.0018perfect 2.27% 3.44% 0.0054present tense 11.19% 8.94% 0.0901progressive 1.79% 2.69% 0.0903manner adverb 0.00% 0.03% 0.1681evaluation adverb 0.69% 1.19% 0.1766past tense 62.85% 65.69% 0.2314duration for-PP 0.59% 0.61% 0.8402continuous adverb 0.04% 0.03% 0.8438Table 5: Indicators discriminate between states andevents.clauses were rejected because of parsing prob-lems.
This left 1,478 clauses, divided equallyinto training and testing sets.83.8% of clauses with main verbs other thanbe and have are events, which thus provides abaseline method of 83.8% for comparison.
Sinceour approach examines only the main verb of aclause, classification accuracy over the test caseshas a maximum of 97.4% due to the presence ofverbs with multiple classes.4.1.2 Ind iv idua l  Ind icatorsThe values of the indicators listed in Table 5were computed, for each verb, across the 97,973parsed clauses from our corpus of medical dis-charge summaries.The second and third columns of Table 5 showthe average value for each indicator over stativeand event clauses, as measured over the trainingexamples.
For example, 4.44% of stative clausesare modified by either not or never, but only1.56% of event clauses were so modified.The fourth column shows the results of T-tests that compare indicator values over stativetraining cases to those over event cases for eachindicator.
As shown, the differences in stativeand event means are statistically significant (p< .01) for the first seven indicators.Each indicator was tested individually forclassification accuracy by establishing a classifi-cation threshold over the training data, and val-idating performance over the testing data usingthe same threshold.
Only the f requency indi-cator succeeded in significantly improving clas-115States Eventsacc recall prec recall precdt 93.9% 74.2% 86.4% 97.7% 95.1%GP 91.2% 47.4% 97.3% 99.7% 90.7%llr 86.7% 34.2% 68.3% 96.9% 88.4%bl 83.8% 0.0% 100.0% 100.0% 83.8%b12 94.5% 69.2% 95.4% 99.4% 94.3%Table 6: Comparison of three learning methodsand two performance baselines, distinguishing statesfrom events.sification accuracy by itself, achieving an accu-racy of 88.0%.
This improvement in accuracywas achieved simply by discriminating the pop-ular verb show as a state~ but classifying allother verbs as events.
Although many domainsmay primarily use show as an event, its appear-ances in medical discharge summaries, uch as,"His lumbar puncture showed evidence of whitecells," primarily utilize show to denote a state.4.1.3 Indicators in  Combinat ionThree machine learning methods successfullycombined indicator values, improving classifi-cation accuracy over the baseline measure.
Asshown in Table 6, the decision tree attained thehighest accuracy, 93.9%.
Binomial tests showedthis to be a significant improvement over the88.0% accuracy achieved by the f requency indi-cator alone, as well as over the other two learn-ing methods.
No further improvement in classi-fication performance was achieved by CART.The increase in the number of stative clausescorrectly classified, i.e.
stative recall, illustratesan even greater improvement over the base-line.
As shown in Table 6, the three learn-ing methods achieved stative recalls of 74.2%,47.4% and 34.2%, as compared to the 0.0% sta-tive recall achieved by the baseline, while onlya small loss in recall over event clauses was suf-fered.
The baseline does not classify any stativeclauses correctly because it classifies all clausesas events.Classification performance is equally compet-itive without he f requency indicator, althoughthis indicator appears to dominate over oth-ers.
When decision tree induction was employedto combine only the 13 indicators other thanfrequency, the resulting decision tree achieved92.4% accuracy and 77.5% stative recall.4.2 CompletednessIn medical discharge summaries, non-culminated event clauses are rare.
Therefore,our experiments for classification according tocompletedness are performed across a corpusof ten novels comprising 846,913 words.
Thesenovels were parsed with ESG, resulting in75,289 fully-parsed clauses (22,505 of 59,816sentences produced errors).4.2.1 Manual  Marking884 clauses from the parsed corpus were man-ually marked according to completedness.
Ofthese, 109 were rejected because of parsingproblems, and 160 rejected because they de-scribed states.
The remaining 615 clauses weredivided into training and test sets such that thedistribution of classes was equal.
The baselinemethod in this case achieves 63.3% accuracy.The linguistic test was selected for this taskby Passonneau (1988): If a clause in the pastprogressive necessarily entails the past tensereading, the clause describes a non-culminatedevent.
For example, We were talking just likemen (non-culm.)
entails that We talked justlike men, but The woman was building a house(culm.)
does not necessarily entail that Thewoman built a house.
Cross-checking betweenlinguists shows high agreement.
In particular,in a pilot study manually annotating 89 clausesfrom this corpus according to stativity, two lin-guists agreed 81 times.
Of 57 clauses agreedto be events, 46 had agreement with respect ocompletedness.The verb say (point), which occurs nine timesin the test set, was initially marked incorrectlyas culminated, since points are non-extendedand therefore cannot be placed in the progres-sive.
After some initial experimentation, wecor-rected the class of each occurrence of say in thedata.4.2.2 Individual IndicatorsTable 7 is analogous to Table 5 for complete-ness.
The differences in culminated and non-culminated means are statistically significant (p< .05) for the first six indicators.
However, forcompletedness, no indicator was shown to sig-nificantly improve classification accuracy overthe baseline.116Linguistic Culm Non-Culm T-testIndicator Mean Mean P-valueperfect 7.87% 2.88% 0.0000temporal adverb 5.60% 3.41% 0.0000manner adverb 0.19% 0.61% 0.0008progressive 3.02% 5.03% 0.0031past/pres partic 14 .03% 17.98% 0.0080no subject 30.77% 26.55% 0.0241duration in-PP 0.27% 0.06% 0.0626present tense 17.18% 14.29% 0.0757duration for-PP 0.34% 0.49% 0.1756continuous adverb 0.10% 0.49% 0.2563frequency 345.86 286.55 0.5652"not" or "never" 3.41% 3.15% 0.6164evaluation adverb 0.46% 0.39% 0.7063past tense 53.62% 54.36% 0.7132Table 7: Indicators discriminate between culmi-nated and non-culminated vents.accCulminated Non-Culmrecall prec recall precCART 74.0% 86.2% 76.0% 53.1% 69.0%llr 70.5% 83.1% 73.6% 48.7% 62.5%lit2 67.2% 81.5% 71.0% 42.5% 57.1%GP 68.6% 77.3% 74.2% 53.6% 57.8%dt 68.5% 86.2% 70.6% 38.1% 61.4%bl 63.3% 100.0% 63.3% 0.0% 100.0%b12 70.8% 94.9% 69.8% 29.2% 76.7%Table 8: Comparison of four learning methodsand two performance baselines, distinguishing cul-minated from non-culminated vents.4.2.3 Indicators in CombinationAs shown in Table 8, the highest accuracy,74.0%, was attained by CART.
A binomial testshows this is a significant improvement over the63.3% baseline.The increase in non-culminated recall illus-trates a greater improvement over the baseline.As shown in Table 8, non-culminated recalls ofup to 53.6% were achieved by the learning meth-ods, compared to 0.0%, achieved by the base-line.Additionally, a non-culminated F-measure of61.9 was achieved by GP, when optimizing forF-Measure, improving over 53.7 attained by theoptimal uninformed baseline.
F-measure com-putes a tradeoff between recall and precision(Van Rijsbergen, 1979).
In this work, we weighrecall and precision equally, in which case,recall*precisionF - measure  = (recall+precision)f2Automatic methods highly prioritized theperfect indicator.
The induced ecision tree usesthe perfect indicator as its first discriminator,log-linear egression ranked the perfect indica-tor as fourth out of fourteen, function trees cre-ated by GP include the perfect indicator as oneof five indicators used together to increase clas-sification performance, and the perfect indicatortied as most highly correlated with completed-ness (cf.
Table 7).5 D iscuss ionSince certain verbs are aspectually ambiguous,and, in this work, clauses are classified by theirmain verb only, a second baseline approachwould be to simply memorize the majority as-pect of each verb in the training set, and classifyverbs in the test set accordingly.
In this case,test verbs that did not appear in the training setwould be classified according to majority class.However, classifying verbs and clauses accord-ing to numerical indicators has several impor-tant advantages over this baseline:?
Handles  rare or un labeled verbs.
Theresults we have shown serve to estimateclassification performance over "unseen"verbs that were not included in the super-vised training sample.
Once the systemhas been trained to distinguish by indi-cator values, it can automatically classifyany verb that appears in unlabeled cor-pora, since measuring linguistic indicatorsfor a verb is fully automatic.
This also ap-plies to verbs that are underrepresented inthe training set.
For example, one nodeof the resulting decision tree trained todistinguish according to stativity identifies19 stative test cases without misclassifyingany of 27 event test cases with verbs thatoccur only one time each in the trainingset.?
Success when training doesn't includetest verbs.
To test this, all test verbswere eliminated from the training set, andlog-linear egression was trained over thissmaller set to distinguish according to com-pletedness.
The result is shown in Table 8("llr2").
Accuracy remained higher thanthe baseline "br' (bl2 not applicable), andthe recall tradeoff is felicitous..
Improved per formance.
Memorizingmajority aspect does not achieve as highan accuracy as the linguistic indicators for117completedness, nor does it achieve as widea recall tradeff or both stativity and com-pletedness.
These results are indicated asthe second baselines ("bl2") in tables 6 and8, respectively.?
Scalar values ass igned to each verb al-low the tradeoff between recall and preci-sion to be selected for particular applica-tions by selecting the classification thresh-old.
For example, in a separate study, op-timizing for F-measure resulted in a moredramatic tradeoff in recall values as com-pared to those attained when optimizingfor accuracy (Siegel, 1998b).
Moreover,such scalar values can provide input to sys-tems that perform reasoning on fuzzy oruncertainty knowledge.?
Th is  f ramework  is expandab le  sinceadditional indicators can be introducedby measuring the frequencies of additionalaspectual markers.
Furthermore, indica-tors measured over multiple clausal con-stituents, e.g., main verb-object pairs, al-leviate verb ambiguity and sparsity andimprove classification performance (Siegel,1998b).6 Conc lus ionsWe have developed a full-scale system for aspec-tual classification with multiple linguistic indi-cators.
Once trained, this system can automati-cally classify all verbs appearing in a corpus, in-cluding "unseen" verbs that were not includedin the supervised training sample.
This frame-work is expandable, since additional lexico-syntactic markers may also correlate with as-pectual class.
Future work will extend this ap-proach to other semantic distinctions in naturallanguage.Linguistic indicators uccessfully exploit lin-guistic insights to provide a much-neededmethod for aspectual classification.
When com-bined with a decision tree to classify accordingto stativity, the indicators achieve an accuracyof 93.9% and stative recall of 74.2%.
When com-bined with CART to classify according to com-pletedness, indicators achieved 74.0% accuracyand 53.1% non-culminated recall.A favorable tradeoff in recall presents an ad-vantage for applications that weigh the identi-fication of non-dominant classes more heavily(Cardie and Howe, 1997).
For example, cor-rectly identifying occurrences of for that denoteevent durations relies on positively identifyingnon-culminated vents.
A system that summa-rizes the duration of events which incorrectlyclassifies "She ran (for a minute)" as culmi-nated will not detect that "for a minute" de-scribes the duration of the run event.
This is be-cause durative for-PPs that modify culminatedevents denote the duration of the ensuing state,e.g., I leJt the room for a minute.
(Vendler,1967)Our analysis has revealed several insights re-garding individual indicators.
For example,both duration in-PP and manner adverb areparticularly valuable for multiple aspectual dis-tinctions - they were ranked in the top two po-sitions by log-linear modeling for both stativityand completedness.We have discovered several new linguistic in-dicators that are not traditionally linked to as-pectual class.
In particular, verb frequency withno deep subject was positively correlated withboth stativity and completedness.
Moreover,four other indicators are newly linked to stativ-ity: (1) Verb frequency,  (2) occurrences modi-fied by "not" or "never", (3) occurrences in thepast or present participle, and (4) occurrencesin the perfect ense.
Additionally, another threewere newly linked to completedness: (1) occur-rences modified by a manner adverb, (2) occur-rences in the past or present participle, and (3)occurrences in the progressive.These new correlations can be understood inpragmatic terms.
For example, since points(non-culminated, punctual events, e.g., hiccup)are rare, punctual events are likely to be cul-minated.
Therefore, an indicator that discrim-inates events according to extendedness, e.g.,the progressive, past/present participle, and du-ration for-PP, is likely to also discriminate be-tween culminated and non-culminated vents.As a second example, the not/never indica-tor correlates with stativity in medical reportsbecause diagnoses (i.e., states) are often ruledout in medical discharge summaries, e.g., "Thepatient was not  hypertensive," but procedures(i.e., events) that were not done are not usu-ally mentioned, e.g., '~.An examination was notperformed.
"118AcknowledgementsKathleen R. McKeown was extremely helpful regard-ing the formulation of this work and Judith L. Kla-vans regarding linguistic techniques, and they, alongwith Min-Yen Kan and Dragomir R. Radev provideduseful feedback on an earlier draft of this paper.This research was supported in part by theColumbia University Center for Advanced Technol-ogy in High Performance Computing and Commu-nications in Healthcare (funded by the New YorkState Science and Technology Foundation), the Of-fice of Naval Research under contract N00014-95-1-0745 and by the National Science Foundation undercontract GER-90-24069.Re ferencesC.
Cardie and N. Howe.
1997.
Improving mi-nority class prediction using case-specific featureweights.
In D. Fisher, editor, Proceedings of theFourteenth International Conference on MachineLearning.
Morgan Kaufmann.P.K.
Chan and S.J.
Stolfo.
1993.
Toward multistrat-egy parallel and distributed learning in sequenceanalysis.
In Proceedings of the First InternationalConference on Intelligent Systems for MolecularBiology.W.
Cohen.
1995.
Fast effective rule induction.
InProc.
12th Intl.
Conf.
Machine Learning, pages115-123.N.
Cramer.
1985.
A representation for the adap-tive generation of simple sequential programs.
InJ.
Grefenstette, ditor, Proceedings of the \[First\]International Conference on Genetic Algorithms.Lawrence Erlbaum.B.& Dorr.
1992.
A two-level knowledge represen-tation for machine translation: lexical seman-tics and tense/aspect.
In James Pustejovskyand Sabine Bergler, editors, Lexieal Semanticsand Knowledge Representation.
Springer Verlag,Berlin.D.R.
Dowty.
1979.
Word Meaning and MontagueGrammar.
D. Reidel, Dordrecht, W. Germany.R.
O. Duda and P.E.
Hart.
1973.
Pattern Classifi-cation and Scene Analysis.
Wiley, New York.J.H.
Friedman.
1977.
A recursive partitioning deci-sion rule for non-parametric classification.
IEEETransactions on Computers.V.
Hatzivassiloglou and K. McKeown.
1993.
To-wards the automatic identification of adjectivalscales: clustering adjectives according to mean-ing.
In Proceedings of the 31st Annual Meeting ofthe ACL, Columbus, Ohio, June.
Association forComputational Linguistics.J.L.
Klavans and M. Chodorow.
1992.
Degrees ofstativity: the lexical representation of verb as-pect.
In Proceedings of the 14th InternationalConference on Computation Linguistics.J.L.
Klavans.
1994.
Linguistic tests over large cor-pora: aspectual classes in the lexicon.
Technicalreport, Columbia University Dept.
of ComputerScience.
unpublished manuscript.J.R.
Koza.
1992.
Genetic Programming: On theprogramming of computers by means of naturalselection.
MIT Press, Cambridge, MA.M.C.
McCord.
1990.
SLOT GRAMMAR.
InR.
Studer, editor, International Symposium onNatural Language and Logic.
Springer Verlag.M.
Moens and M. Steedman.
1988.
Temporal ontol-ogy and temporal reference.
Computational Lin-guistics, 14(2).R.J.
Passonneau.
1988.
A computational model ofthe semantics of tense and aspect.
ComputationalLinguistics, 14(2).F.
Pereira, N. Tishby, and L. Lee.
1993.
Distribu-tional clustering of english words.
In Proceedingsof the 31st Conference of the ACL, Columbus,Ohio.
Association for Computational Linguistics.J.R.
Quinlan.
1986.
Induction of decision trees.
Ma-chine Learning, 1(1):81-106.J.R.
Quinlan.
1993.
C~.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, CA.T.J.
Santner and D.E.
Duffy.
1989.
The StatisticalAnalysis of Discrete Data.
Springer-Verlag, NewYork.H.
Schfitze.
1992.
Dimensions of meaning.
In Pro-ceedings of Supereomputing.E.V.
Siegel and K.R.
McKeown.
1996.
Gatheringstatistics to aspectually classify sentences with agenetic algorithm.
In K. Oflazer and H. Somers,editors, Proceedings of the Second InternationalConference on New Methods in Language Process-ing, Ankara, Turkey, Sept. Bilkent University.E.V.
Siegel.
1997.
Learning methods for combininglinguistic indicators to classify verbs.
In Proceed-ings of the Second Conference on Empirical Meth-ods in Natural Language Processing, Providence,RI, August.E.V.
Siegel.
1998a.
Disambiguating verbs with thewordnet category of the direct object.
In Proced-ings of the Usage of WordNet in Natural LanguageProcessing Systems Workshop, Montreal, Canada.E.V.
Siegel.
1998b.
Linguistic Indicators for Lan-guage Understanding: Using machine learningmethods to combine corpus-based indicators foraspectual classification of clauses.
Ph.D. thesis,Columbia University.C.J.
Van Rijsbergen.
1979.
Information Retrieval.Butterwoths, London.Z.
Vendler.
1967.
Verbs and times.
In Linguistics inPhilosophy.
Cornell University Press, Ithaca, NY.119
