A Formalism for Universal Segmentat ion of TextJ u l ien  Qu in tGETA-CLIPS- IMAG,  BP 53, F-38041 Grenoble Cedex 9, FranceXerox Research Centre Europe, 6, chemin de Maupertuis,  F-38240 Meylan, Francee-mail: ju l ien ,  quin'c@iraag.frAbstractSumo is a formalism for universal segmentationof text.
Its purpose is to provide a franleworkfor the creation of segmentation applications.
Itis called "universal" as tile formalism itself isindependent of the language of the documentsto process and independent of the levels of seg-mentation (e.g.
words, sentences, paragraphs,nlorphemes...) considered by the target applica-tion.
This framework relies on a layered struc-ture representing the possible segmentations ofthe document.
This structure and the tools tomanipulate it are described, followed by detailedexamples highlighting some features of Sumo.Int roduct ionTokenization, or word segmentation, is a fun-damental task of ahnost all NLP systems.
Inlanguages that use word separators in their writ-ing, tokenization seenls easy: every sequence ofcharacters between two whitespaces or punctu-ation marks is a word.
This works reasonablywell, but exceptions are handled in a cumber-some way.
On the other hand, there are lan-guages that do not use word separators.
A muchnlore complicated processing is needed, closerto morphological nalysis or part-of-speech tag-ging.
Tokenizers designed for those languagesare generally very tied to a given system andlanguage.Itowever, the gap becomes maller when welook at sentence segmentation: a simplistic ap-proach would not be sufficient because of theambiguity of punctuation signs.
And if weconsider the segmentation of a document intohigher-level units such as paragraphs, ections,and so on, we can notice that language becomesless relevant.These observations lead to the definition ofour formalism for segmentation ( ot just tok-enization) that considers tile process indepen-dently fl:om the language.
By describing a seg-mentation systenl formally, a clean distinctioncan be made between tile processing itself andtile linguistic data it uses.
This entails the abil-ity to develop a truly multilingual system by us-ing a common segmentation e gine ~br the vari-ous languages of the system; conversely, one canimagine evaluating several segmentation , eth-ods by using the same set of data with differentstrategies.Sumo is the name of the proposed formal-isnl, evolving from initial work by (Quint, 1999;Quint, 2000).
Some theoretical works from theliterature also support this approach: (Guo,1997) shows that sonle segmentation techniquescan be generalized to any language, regardless oftheir writing systenl.
The sentence segmenter of(Pahner and Hearst, 1997) and the issues raisedby (Habert et al, 1.998) prove that even in l~n-glish or French, segmentation is not so trivial.Lastly, (A~t-Mokhtar, 1997) handles all kinds ofpresyntactic processing in one step, arguing thatthere are strong interactions between segnlenta-tion and morphology.1 The  F ramework  for  Segmentat ion1.1 Overv iewThe framework revolves around the documentrepresentation chosen for Sulno, which is alayered structure, each layer being a view ofthe document at a given level of seglnentation.These layers are introduced by the author of thesegmentation application as needed and are notimposed by Sulno.
The example in section 3.1uses a two-layer structure (figure 4) correspond-ing to two levels of segmentation, characters andwords.
To extend this to a sentence seglnenter,a third level for sentences i added.These levels of segmentation can have a lin-656g,fistic or structural evel, but "artificiar' levelscan be introduced a.s well when needed.
It is alsointeresting to note that several ayers can belongto the same level.
In the example of section 3.3,the result structure can have an indefinite num-ber of levels, and all levels are of the same kind.We (:all item the segmentation unit o\['a doc-untent at a given segmentation level (e.g.
itemsof the word level are words).
The document isthen represented at every segmentation level in1;erms of its items; I)ecause segmentation is usu-ally ambiguous, item .qraph.~ are used to  \['actorizeall the possible segmc'l,ta.tions.
Ambiguity issuesare furthel' addressed in section 2.3.The main processing i)aradigms of Sumo areident{/icatio'n and h'ansJbrmation,.
With ideutifi-cal;ion, new item graphs are built by identif'yingitems fi'om a source graph using a segmentationresource, q'hese graphs are 1;hen modified l)ytranslbrula.tion processes.
Section 2 gives thedetails al)out both identificatio~l and t\]'a.nsfofmation.1.
:2 I tem Graphs  .
'l'lle iten:l gral)hs are directed acyclic gral)hs;they are similar to the word graphs of (Amtru 1)et al, 11996) or the string graphs of (C'olmer-auer, 1970).
They are actually rel)resente(I 1)ymeans of finite-sta.te automata (see section 2.\]).IH order to facilitate their manilmlation , two a(1-ditio~tal prol)erties are on forced: these m Jtom ataahvays lm.ve a single start-state and finite-slate,and no dangling arcs (this is verified by pruningthe automata fter modifications).
The exam-pies of section 3 show va.rio~ls iteln graphs.An item is an arc in the automato~l.
An arcis a complex structure containing a label (gen-erally the surface /brm of the item), named at-tributes and relations.
Attributes are llsed tohold information on the item, like part of speechtags (see section 3.2).
These attributes can alsobe viewed as annotations in the same sense asthe annotation graphs of (Bird el; 3l., 2000).1.3 Rela t ionsRelations are links between levels.
Items froma given graph are linked to items of the graphfrom which they were identified.
We call thefirst graph the Iowcr graph and the gral)h thatwas the source \[br the identification the uppergraph.
Relations exist between a path in theupper graph and either a path or a subgraph inthe lower graph.Figure i illustrates the first kind of relation,called path relation.
This example in French is arelation between the two characters of the word"du" which is really a contraction of"de le".Figure 1: A path relationFigure 2 illustrates the other ldnd of relationcalled subgraph relation.
In this example thesentence ABCI)EI, G. (we can imagine that Athrough G are Chinese characters) is related toseveral possible segmentations.AB CD ~ E"-( ) " 9 - - .A BC DE ~ FG .
( ) ,<) -,<) ~() ,-() ,-(), ,  > ~ /7~.
BCDEF G ~ '  1"l?
.
:~mCDZFG %-OFigElre 2: A graph relationThe interested reader may refer to (Pla.nas,1998) for a conq)arable 8trllctul;e (multiple lay-ers of a document and relations) used in tra.ns-lation memory.2 Process ing  a Document2.1 Descr ip t ion  of  a DocmnentThe core of the document representation is theitem graph, which is represented by a finite-state automaton.
Since regular expressions de-fine finite-state automata, they can be used todescribe an item graph.
Itowever, our expres-sions are extended because the items are morecomplex than simple symbols; new operators areintroduced:?
attributes are introduced by an @ sign;?
path relations are delimited by { and };?
tile inlbrmation concerning a given item areparenthesized using \[ and \].657As an exemple, the relation of figure 1 is de-scribed by the following expression:\[ de le { d u } \]2.2 Ident i f icat ionIdentification is the process of identifying newitems froln a source graph.
Using the sourcegraph and a segmentation resource, new itemsare built to form a new graph.
A segmentationresource, or simply resource, describes the vo-cabulary of the language, by defining a mappingbetween the source and the target level of seg-mentation.
A resource is represented by a finite-state transducer in Sumo; identification is per-formed by applying the transducer to the sourceautomaton to produce the target automaton,like in regular finite-state calculus.Resources can be compiled by regular expres-sions or indentification rules.
In the former case,one can use the usual operations of finite-statecalculus to compile the resource: union, inter-section, composition, etc) A benefit of the useof Sumo structures to represent resources i thatnew resources can be built easily from the doc-ument that is being processed.
(Quint, 1999)shows how to extract proper nouns from a textin order to extend the lexicon used by the seg-reenter to provide more acurate results.In the latter case, rules are specified a.s shownin section 3.3.
The left hand side of a rule de-scribes a suhpath in the source graph, while theright hand side describes the associated subpathin the target graph.
A path relation is createdbetween the two sequences of items.
In an iden-tific~tion rule, one can introduce variables (for:callback), and even calls to transformation func-tions (see next section).
Naturally, these possi-bilities cannot be expressed by a strict finite-state structure, even with our extended formal-ism; hence, calculus with the resulting struc-tures is limited.A special kind of identification is the auto-matic segmentation that takes place at the entrypoint of the process.
A character graph can becreated automatically by segmenting an inputtext document, knowing its encoding.
This textdocument can be in raw form or XML format.Another possibility for input is to use a graph1The semanl, ics of these operations is broadened toaccomodate the more complex nature of the items.of items that was created previously, either bySumo, or converted to the tbrmat recognized by~1_11\]10.2.3 Trans format ionAmbiguity is a central issue when talking aboutsegmentation.
Tile absence or ambiguity ofword separators can lead to multiple segmen-tations, and more than one of them can have ameaning.
As (Sproat et al, 1996) testify, severalnative Chinese speakers do not always agree onone unique tokenization for a given sentence.Th~nks to the use of item graphs, Sumo canhandle ambiguity efficiently.
Why try to fullydisambiguate a tokenization when there is noagreement on a single best solution?
Moreover:,segmentation is usually just a basic step of pro-cessing in an NLP system, and some decisionsmay need more information than what a set-reenter is able to provide.
An uninformed choiceat this stage can affect the next stages in a neg-ative way.
Transformations are a way to mod-ify the item graphs so that the "good" paths(segmentations) can be kept and the "bad" onesdiscarded.
We can also of course provide flllldisambiguation (see section 3.1 for instance) bymeans of transformations.In Sumo transformations are handled bytransformation 5mctions that manipulate theobjects of the tbrmalism: graphs, nodes, items,paths (a special kind of graph), etc.
These func-tions are written using an imperative languageillustrated in section 3.1.
A transformation caneither be apl)lied directly to a graph or attachedto a graph relation.
In the latter case, the orig-inal graph is not modified, and its transformedcounterpart is only accessible through the rela-tion.Transformation functions allow to control theflow of the process, using looping and condition-sis.
An important implication is that a sameresource can be applied iteratively; as shown by(Roche, 1994:) this feature allows to implementsegmentation models much more powerful thansimple regular languages (see section 3.3 for anexample).
Another consequence is that a Sumoapplication consists of one big transformationfunction returning the completed Sumo struc-ture as a result.6583 Examples  o f  Use3.1 Max imum token izat ionSome cla.ssic heuristics for tokenization a.reclassified 1) 3, (G i% 1997) under the collectivemonil<er of mare\]mum tokenization.
This s{Betiondescribes how to iml)lement a.
"maxilnnm tok-enizer" tha.t tokenizes raw text doculnerits in al A\]- given language and cha.racter encoding (e.g.
ea<(!l\] glish in .
.
.
.
.
, French in Iso-Latin-l, Chinese illBig5 or GB).8.1.1 Comlnon  set -upOur tokenizer is built with two levels: the in-put level is the character level, automaticallysegmented using the encoding intbrmation.
Thetoken level is built from these cha, racters, first by~li exllaustive identification of the tol<ens, thenby re(hieing the UHlnber o\]" 1)>~tlis to tile one coil-sidere(1 tlle best 1)y the Ma.xil\]\]Ul\]\] ]bkenizationheuristic.The system works ill three stel)S , with com-plete code shown ill figure 3.
First, the charac-ter level is created 1) 3, automatic segnleutation(lines ;1-5, input  leve i  being the special gi'aphthat is automatically created from a. ra,w filethrongh stdiu).
The second step is to create theword grapli 1)y identif'ying words D'oln chata.ctoPllsiiig a dictiona.ry.
A resour(:e called ABgdic iscreated from a transducer file (lines 6-8), thenthe gra,ph words is created by identifying it, enisfrom the SOllrCe level characters  llSing the re-soIIrCO ABCdic  (lines 9-12).
The third step is thedisalnl)igua,tion of' the woM level t)y al)l)lying a,Ma,xiniiin~ Toke\]iization lmuristic (line 13).i characters: input level {2 encoding: <ASCII, UTF-8, Big5...>3 type: raw;4 from: stdin;5}6 ABCdic: resource {7 file: ' CABCdic.
sumo' ' ;8}9 words: graph <- identify {i0 source: characters;il resource: ABCdic ;12 }13 words <- ft(words.start-node);lPigure 3: Maximuln 'lk}kenizer in Sumolqgure 4 illustrates the situatiori for the ill-put string "ABCI)I~FG" where A through G axecharacters and A, AB, B, BC, 13Cl)\]';le, C, CI),13, 1)E, E, F, I"C and (3 are words folmd in theresource ABCdic.
The situation shown is afterline 12 and before line 13.A B C ~-< D E F G (3 (3 ~ .-<3 - ~- - ) (2 )  .
-<3 ~ -  .<~Z/,<=-,>, T A'?M \ - /  /" BCDEF /Figllre 4: lPxhaustive tokenization of the stringAB A)LI GWe will see in the next three subsections l;hedifferent heuristics and their implementations inS l l l \ ] \ ]O .3.1.2 Forward  Maxhnum Token izat ionl%rward maxilnlltn 'lbkenization consists ofscanning tile string from left to right and select-ing the token of maxinulm lerigth any time anambiglfity occurs.
On the exalnple of figure d,tile resl,lt tokeliization of the inI)~lt string would1)e A I~/CD/I'\]/IeG.lqgnre 5 shows a t'lmction called f t  that 1)uildsa path recursively by traversing tile token graph,al)l)ending the longest item to the pa.th at eachnode.
f t  ta.kes a, node as input and retlirils a.path (line 1).
If tile node is final, the enll)tyl)atll is retm'ned (lines 2-3), otherwise the arrayof items of tlle nodes In.
items) is sea.rched andthe longest item store(\] in longest  (lines 4-10).The returned pa,th consists of this longest itemprepended to the longest path starting from thedestination ode of this item (line 11).a.
:t.a Backward  Max inmm Token lzat ionl~a.ckward Maximum Tokenization is tile sameas librward Maximum 'lbkenization except thatthe string is scanned fi'om right to left, insteadof left to right.
On the example of figure 4,the tokenization of the input string would yieldA/I~C/I)E/1,'C under Backward Maximum To-kenization.A function bt can be written.
It is very sim-ila.r to f t ,  except that it works backward bylooking at incoming arcs of' the considered node.bt is cMled on the final state of tile graph and659i function ft (n: node) -> path {23456789I0Ii1213 }if final(n) {return ();} else {longest: item <- n.items\[l\];foreach it in n.items\[2..\] {if it.length > longest.length {longest <- it;}}return (longest # ft(longest.dest));Figure 5: The ft functionstops when at the initial node.
Another imple-mentation of this function is to apply f t  on thereversed graph and then reversing the path ob-tained.3.1.4 Shortest  TokenizationShortest Tokenization is concerned with mini-mizing (;he overall number of tokens in the text.On the example of figure 4, the tokenization ofthe input string would yield A/BCI)I~,I:/G un-der shortest tokenization.Figure 6 shows a fnnction called st that findsthe shortest path in the graph.
This functionis adapted from an algorithm for: single-sourceshortest paths discovery in a DAG given by(Cormen et al, 1990).
It calls another func-tion, t_sort ,  returning a list of the nodes of thegraph in topological order.
The initializationsare done in lines 2-6, the core of the algorithm isin the loop of lines 7-14 that computes the short-est path to every node, storing for each node its"predecessor".
Lines 1.5-20 then build the path,which is returned in line 21.3.1.5 Combinat ion of Max imumTokenizat ion techniquesOne of the features of Sumo is to allow the com-parison of different segmentation strategies us-ing the same set of data.
As we have .just seen,the three strategies described above can indeedbe compared efficiently by modifying only partof the third step of the processing.
Letting thesystem run three times on the same set of inputdocuments can then give three different sets ofresults to be compared by the author of the sys-tem (against each other" and against a referencetokenization, for instance).i function st (g:graph) -> path {2 d: list <- (); // distances3 p: list <- (); // predecessors4 foreach n in (g.nodes) {5 d\[n\] = integer.max; // ~Cinf inite~6 }?
foreach n: node in t_sort(g.nodes) {8 foreach it in n.items {9 if (d\[it.dest\] > din\] + i) then {i0 d\[it.dest\] = din\] + i;i i  p\[it.dest\] = n;12 }13 }14 }15 n <- g.end; // end state16 sp: path <- (n); // path17 while (n != g.start) {18 n = pin\];19 sp = (n # sp);2O }21 return sp;22 }Figrlre 6: the st functionAnd yet a different set-up for our "maximumtokenizer" would be to select not .just the op-timal pa.th according to one of the heuristics,but the paths selected by the three of them, asshown in figure 7.
Combining the three pathsinto a graph is perfbrmed by changing line 13 infigure 3 to:words <- ft(words.start-node) Ibt(words.end-node) \]st(words.start-node);AB CD EFigure 7: Three maximum tokenizations3.2 Statist ical Tokenlzat ion and Part  ofSpeech TaggingThis example shows a more complicated tok-enization system, using the same sort of set-upas the one from section 3.1, with a disalnbigua-tion process using statistics (namely, a bigrammodel).
Our reference for this model is theChasen Japanese tokenizer and part of speech660tagger documented in (Ma.tsumoto el; el., 1999).
'.l'his example is a high-level description of howto implemen~ a simila.r system with Sumo.The set-up for this example adds a new levelto the pre.vious example: the "bigra.m level.
"The word level is still built by identification us-ing dictionaries, then the bigraln level is builtby computing a. connectivity cost between eachpair of tokens.
This is the level that will beused for disambigu~tion r selection of the bestsolutions.3.2.1 Exhaust ive  Segmentat ionAll possible segmentartiOns ~re derived from thecharacter level to create the word level.
Timre,~onrce used \['or this is a dictionary of the la.n-gua,ge that maps the surface form of the words(in terms of their characters) to their base form,part of speech, and a. cost (Chasell also a.ddsl)ronunciation, co1\jugation type, and semanticinformation).
Al l  this inlbrmation is stored inthe item as attril)utes, the base form heing usedas the label for the item.
I,'igure 8 sllows theidentificaJ;ion of lille word "ca.ts" which is identi-fied as "cat", with category "noun" (i.e.
@CAT=N)and with some cost k (@COST=k).c a t s ( ) ,,< )~, .
/~  ,*( ) ~.
).... , \ /  .....cat  @CAT=N @COST=kFigure 8: Identification of the wor<l "cats"3.2 .2  S ta t i s t i ca l  D isambiguat ionThe disambiguation method relies on a bigranlmodel: each pair of successive items has a "con-nectivity cost".
In the bigram level, tim "cost"attribute of an item W will be the connectiv-ity cost of W and a following item X.
Note thatif a same W can be followed by severaJ itemsX, Y, etc.
with different connectivity costs fore~ch p~tir, then W will be replicated with a. dif-ferent "cost" attribute, l:igure 9 shows a wordW followed by either X or Y, with two differentconnectivity costs h and U.The implementation f this technique in Su meis straightibrward.
Assume there is a fllllCtionf that, given two items, computes their connec-tivity cost (depending on both of their category,i)ldividual cost, etc.)
mid returns the first item/ f0~!
coopt=w() Y'"~OFigure 9: Connectivity costs for Wwith its modified cost.
We write the followingrule a.nd a,pply it to the word graph to creat;ethe bigram graph:_ \[$wl = .
e .
\ ]  _ \[$~2 = @.\]->  eva l ( f ($wl ,  $2) )Tiffs r,lle can be read as: for any word $wlwith any attribute (" ."
matches any label, "O .
"a.ny set of attributes) followed by any word $w2with any attribute ("_" being a context separa-tor), create the item returned by the fimctionf ($ul,  $u2).I)isambiguaJ;ion is then be perforlned by se-lecting the pa.th with optimal cost in this graph;but we ca,n also select a.ll paths with a cost co lresl)onclillg to a certain threshold or the n bestt)a.ths, etc.
Note also that this model is easily ex-tensible to any kind of n-grams.
A new fllnctionf($wl .
.
.
.
.
Swn) must be provided to corn-pule the connectivity costs of this sequence ofitems, and the above rule m,lst be modified totake a larger context into accom~t.3.3 A For lnal  ExmnpleThis last examl~h'~ is more formal and serw~sas an ilhlstra.tion of some powerful features of'Sumo.
(Cohnerauer, 1970) has a similar exam-pie implemented using Q systems.
In both casesthe goaJ is to  transform an input string of tiltlbrm a~'%"~c ' , n > 0 into a single item ,S' (as-suming theft the input a,lphal)et does not contain,S'), meaning tha.t the input string is a word ofthis laaguage.The set-up here is once again to start with alower level automatically created fl'om the input,then to build intermediate l vels until ~ finallevel containing oMy the item S is produced (atwhich point the input is recognized), or until theprocess Call no longer carry on (at which pointthe input is rejected)..I hc building of intermediary levels is handledby the identifica.tioll rule below:# S?
a \[$A=a*\] b \[$B=b*\] c \ [$c=c*\]  #-> S SA SB $C661What this rule does is identify a string of theform S?aa*bb*cc*, storing all a's but the firstone in the varia.ble SA, all b's but the first one in$B and M1 c's but the first one in $C.
The firsttriplet abc (with a possible S in front) is thenabsorbed by ,5', and the remaining a's, b's andc's are rewritten after ,5'.Figure 1.0 illustrates the first application ofthis rule to the input sequence aabbcc, creatingthe first intermediate l vel; subsequent applica-tions of this rule will yield the only item ,5'., .
.
~ _ ) a  a b b c c8Figure 10: First application of the ruleConclusionWe have described the main features of Sumo, adedicated formalism \[br segmentation f text.
Adocument is represented by item graphs at different levels of segmentation, which a.llows mul-tiple segmentations of the same document a.tthe same time.
Three detailed ex~mples illus-trated the features of Sumo discussed here.
Forthe sake of simplicity some aspects could notbe evoked in this paper, they include: manage-ment of the segmentation resources, ef\[iciencyof the systems written in Sumo, larger a.pplica-tions, evaluation of segmentation systems.Sumo is currently being prototyped by the au-thor.ReferencesSala.h APr-MOKHTAR, "Du texte ASCII autexte lemmatis5 : la prfsyntaxe n une seule6tape", in Proceedings of TALN-97, pages 60-69, Grenoble, France, June, 1997.Jan W. AMTRUP, Henrik HE,N~ and UweJEST, l/Vhat's in a Word Graph.
Evaht-ation and Enhancement of Word Lattices,Verbmobil report 186, Universit'~tt ltamburg,ht tp  ://www.
d fk i .
de/,  l)ecember, 1997.Steven BraD, David DAY, John GAROFOI,O,John ItENDERSON, Christophe LAPRUN andMark \[,mERMAN, "ATI,AS: A Flexible andExtensible Architecture for Linguistic Anne-tation", in Proceedings of L RI~C 2000, Athens,Greece, May, 2000.Ala.in (JOLMEI~AUER, Lc,~ ,~yst~1)~,c~" (2 ott "ttltformalis'm.e pour analyser et synth.dtiser desphrases s~tr ordinateur', Publication internenumfro 43, Universit6 de Montr6a.1, 1970.Thomas H. COaM~.TN, Charles E. I,I;ISERSONand Ronald L. Rwl~,s'r, Introduction to Al-gorithms, MIT Press, Cambridge, Massachus-sets, 1990.Jin Guo, "Critical Tokenization and its Prop-erties", in Computational Linguistics, 23(4),pages 569-596, December, 1997.B.
HABERT, G. ADI)A, M.
ADDA-I)I~CI<EI~., P.BOULA l)E MARI';;UIL, S. I?I~,I{I/AI/I, O.
\];'Ell-RET, G. \]LI,ouz a,nd P. PAI{OUBI,;K, "TowardsTokenization Evaluation", in Proceedings ofLREC-98, pages 4:27-431, 1998.Yuji MATSUMOTO, Akira \[(\]TAUClII, TatsuoYAMASItlTA el; Yoshitaka HIRANO, JapaneseMorphological Analysis System Cha,5'cn ver-sion 2.0 Man'aal, Technical Report NAIST-IS-TR99009, Nara Institute of Science and Tech-nology, Nara., April, 11999.David 1).
PALMER a.nd Ma.rti A. ItEAI{S'G"Adaptative Multilingual Sentence 13oundaryDisambigua.tion", in Computational Ling'uis-lies, 23(2), pages 241-267, June, 11997.h;mmanuel P1,ANAS, 7'\['2LA.
,5~tr'uct'urcs et algorithmcs pour la 7}'aduction Fond& sur laMdmoirc, Th6se d'lnformatique, Universit6.1 oseph l,'ourier, Grenoble, 1998..Julien QUIN% "Towm'ds a fbrmalism forlanguage-independent text segnmntatioJl", inProcccdin9 s of NLPRS'99, pages 404-408, Bei-jing, November, 1999.Julien QUrN% "Universal Segmentation of Textwith the Sumo l,brnmlism", im Proceedings ofNLP 2000, pages 1.6-26, Pa.tras, Greece, June,2000.Em manuel ROCIIE, "Two Parsing Algorithms byMeans of Finite-State Tl:ansducers", in Pro-ceedings of COLING-9/~, pages 431-435, 1994.Richard SPI~,oArp, Chilin SIIIII, William GaLl.
;and Nancy CIIANG, "A Stochastic Finite-State Word-Segmentation Algorithm for Chi-nese", in Computational Linguistics 22(3),pages 377-404, 1996.662
