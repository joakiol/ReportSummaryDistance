Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 87?95,ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSyntax-driven Learning of Sub-sentential Translation Equivalents andTranslation Rules from Parsed Parallel CorporaAlon Laviealavie@cs.cmu.eduLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAAlok Parlikaraup@cs.cmu.eduLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAVamshi Ambativambati@cs.cmu.eduLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAAbstractWe describe a multi-step process for automati-cally learning reliable sub-sentential syntacticphrases that are translation equivalents of eachother and syntactic translation rules betweentwo languages.
The input to the process is acorpus of parallel sentences, word-aligned andannotated with phrase-structure parse trees.We first apply a newly developed algorithmfor aligning parse-tree nodes between the twoparallel trees.
Next, we extract all alignedsub-sentential syntactic constituents from theparallel sentences, and create a syntax-basedphrase-table.
Finally, we treat the node align-ments as tree decomposition points and extractfrom the corpus all possible synchronous par-allel tree fragments.
These are then convertedinto synchronous context-free rules.
We de-scribe the approach and analyze its applicationto Chinese-English parallel data.1 IntroductionPhrase-based Statistical MT (PB-SMT) (Koehn etal., 2003) has become the predominant approach toMachine Translation in recent years.
PB-SMT re-quires broad-coverage databases of phrase-to-phrasetranslation equivalents.
These are commonly ac-quired from large volumes of automatically word-aligned sentence-parallel text corpora.
Accurateidentification of sub-sentential translation equiva-lents, however, is a critical process in all data-drivenMT approaches, including a variety of data-drivensyntax-based approaches that have been developedin recent years.
(Chiang, 2005) (Imamura et al,2004) (Galley et al, 2004).In this paper, we describe a multi-step process forautomatically learning reliable sub-sentential syn-tactic phrases that are translation equivalents of eachother and syntactic translation rules between twolanguages.
The input to the process is a corpus ofparallel sentences, word-aligned and annotated withphrase-structure parse trees for both languages.
Ourmethod consists of three steps.
In the first step,we apply a newly developed algorithm for aligningparse-tree nodes between the two parallel trees.
Inthe second step, we extract all aligned sub-sententialsyntactic constituents from the parallel sentences,and create a syntax-based phrase-table.
Our syn-tactic phrases come with constituent ?labels?
whichcan guide their syntactic function during decoding.In the final step, we treat the node alignments astree decomposition points and extract from the cor-pus all possible synchronous parallel tree fragments.These are then converted into synchronous context-free rules.
Our methods do not depend on any spe-cific properties of the underlying phrase-structurerepresentations or the parsers used, and were de-signed to be applicable even when these represen-tations are quite different for the two languages.The approach described is used to acquire the re-sources for a statistical syntax-based MT approachthat we have developed (Stat-XFER), briefly de-scribed below.
The resulting resources can, how-ever, be used in any syntax-based data-driven MTapproach other than our own.
The focus of this pa-per is on our syntax-driven process for extractingphrases and rules from data.
We describe the ap-proach and analyze its effectiveness when applied tolarge-volumes of Chinese-English parallel data.871.1 The Stat-XFER MT FrameworkStat-XFER is a search-based syntax-driven frame-work for building MT systems.
The underlying for-malism is based on synchronous context-free gram-mars.
The synchronous rules can optionally be aug-mented by unification-style feature constraints.
Thesynchronous grammars can be acquired automati-cally from data, but also manually developed by ex-perts.
A simple example transfer-rule (for Chinese-to-English) can be seen below:{NP,1062753}NP::NP [DNP NP] -> [NP PP]((*score* 0.946640316205534)(X2::Y1)(X1::Y2))Each rule has a unique identifier followed by asynchronous rule for both source and target sides.The alignment of source-to-target constituents is ex-plicitly represented using ?X?
indices for the sourceside, and ?Y?
indices for the target side.
Rules canalso have lexical items on either side, in which caseno alignment information is required for these ele-ments.
Feature constraints can optionally be speci-fied for both source and target elements of the rule.We do not address the learning of feature constraintsin the work described here, and concentrate onlyon the acquisition of the synchronous CFG rules.The rules can be modeled statistically and assignedscores, which can then be used as decoding features.The Stat-XFER framework also includes a fully-implemented transfer engine that applies the trans-fer grammar to a source-language input sentence atruntime, and produces collections of scored wordand phrase-level translations according to the gram-mar.
These are collected into a lattice data-structure.Scores are based on a log-linear combination of sev-eral features, and a beam-search controls the un-derlying parsing and transfer process.
A second-stage monotonic decoder is responsible for combin-ing translation fragments into complete translationhypotheses (Lavie, 2008)2 PFA Algorithm for Node Aligment2.1 Objectives of the AlgorithmOur objective of the first stage of our approach is todetect sub-sentential constituent correspondences inparallel sentences, based on phrase-structure parsesfor the two corresponding sentences.
Given a pairof parallel sentences and their corresponding parsetrees, our goal is to find pairings of nodes in thesource and target trees whose yields are translationequivalents of each other.
Our current approach onlyconsiders complete constituents and their contigiousyields, and will therefore not align discontiguousphrases or partial constituents.
Similar to phrase ex-traction methods in PB-SMT, we rely on word-levelalignments (derived manually or automatically) asindicators for translation equivalence.
The assump-tion applied is that if two words are aligned witheach other, they carry the same meaning and can betreated as translation equivalents.
Constituents aretreated as compositional units of meaning and trans-lation equivalence.2.2 Related WorkAligning nodes in parallel trees has been in-vestigated by a number of previous researchers.
(Samuelsson and Volk, 2007) describe a process formanual alignment of nodes in parallel trees.
Thisapproach is well suited for generating reliable par-allel treebanks, but is impractical for accumulatingresources from large parallel data.
(Tinsley et al,2007) use statistical lexicons derived from automaticstatistical word alignment for aligning nodes in par-allel trees.
In our approach, we use the word align-ment information directly, which we believe may bemore reliable than the statistical lexicon.
(Groves etal., 2004) propose a method of aligning nodes be-tween parallel trees automatically, based on wordalignments.
In addition to the word alignment in-formation, their approach uses the constituent labelsof nodes in the trees, and the general structure of thetree.
Our approach is more general in the sense thatwe only consider the word alignments, thereby mak-ing the approach applicable to any parser or phrase-structure representation, even ones that are quite dif-ferent for the two languages involved.882.3 Unaligned Words and ContiguityWord-level alignment of phrase-level translationequivalents often leaves some words unaligned.
Forexample, some languages have articles, while oth-ers do not.
It is thus reasonable to expect that con-stituent pairs in parallel trees that are good transla-tion equivalents of each other may contain some un-aligned words.
Our PFA node-alignment algorithmallows for such constituents to be matched.Different languages have different word orders.
InEnglish, an adjective always comes before a noun,while in French, in most cases, the adjective fol-lows its noun.
Our node alignment algorithm allowsaligning of constituents regardless of the word orderexpressed by the linear precedence relation of theirsub-constituents.
As long as one piece of contiguoustext dominated by a node covers the same word-levelalignments as the yield of a node in the parallel tree,the two nodes can be aligned.2.4 Wellformedness constraintsGiven a pair of word-aligned sentences and theircorresponding parse trees S and T , represented assets of constituent nodes, our PFA node alignmentalgorithm produces a collection of aligned node-pairs (Si, Tj).
The underlying assumptions of com-positionality in meaning and word-level alignmentsbeing indicative of translation equivalence lead di-rectly to the following node alignment wellformed-ness criteria:1.
If a node Si is linked to a node Tj , then anynode within the subtree of node Si can only belinked to nodes within the subtree of node Tj .2.
If a node Si is linked to a node Tj , then anynode that dominates the node Si can only belinked to nodes that dominate the node Tj .3.
If a node Si is linked to a node Tj , then theword alignments of the yields of the two con-stituents must satisfy the following:(a) Every word in the yield of the node Simust be aligned to one or more words inthe yield of the node Tj , or it should beunaligned.
(b) Every word in the yield of the node Tjmust be aligned to one or more words inthe yield of the node Si, or it should beunaligned.
(c) There should be at least one alignment be-tween the yields of nodes Si and Tj .
Thus,the words in the yields can not all be un-aligned.2.5 Arithmetic RepresentationOur PFA algorithm uses a arithmetic mapping thatelegently carries over the constraints characterizedby the wellformedness constraints elaborated above.This mapping is designed to ensure that each alignedword, which carries a distinct ?piece of meaning?can be uniquely identified, and also inherently re-flects the compositional properties of constituenttranslation equivalence.
This is accomplished byassigning numerical values to the nodes of the twoparse trees being aligned, in a bottom-up fashion,starting from the leaf nodes of the trees.
Leaf nodesthat correspond to words that are aligned are eachassigned a unique prime number.
Unaligned leafnodes are assigned a value of ?1?.
Constituent nodesin the parse trees are then assigned a value that isthe product of all its sub-constituent nodes.
Becauseof the arithmetic property that any composite num-ber can be uniquely factored into primes, it shouldbe evident that the value of every constituent nodeuniquely identifies the aligned words that are cov-ered by its yield.
Consequently, by assigning thesame prime values to the aligned words of both trees,retrieving aligned constituent nodes is as simple asfinding the set of nodes in the two trees that carry thesame numerical value.
Note that by assigning valuesof ?1?
to unaligned words, these unaligned wordsdo not influence the numerical values assigned toconstituent nodes, thus reflecting their treatment as?don?t cares?
with respect to the translation equiva-lence of constituent nodes.2.6 Description of the PFA AlgorithmThe PFA algorithm uses the concept of ?compositemeaning as prime factorization?, and hence the name(Prime Factorization and Alignments).
The algo-rithm assigns values to the leaf nodes, propogatesthe values up the tree, and then compares the nodevalues across the trees to align the nodes.
As de-scribed above, leaf nodes which have word align-ments are assigned unique prime numbers, and the89Figure 1: Node-Aligned Parallel Sentencessame prime is assigned to the corresponding alignedwords in the parallel sentences.
Leaf nodes corre-sponding to unaligned words are assigned the value?1?.
The treatment of ?one-to-many?
word align-ments is a special case.
Such alignments are con-sidered to carry the same meaning, and should thusbe assigned the same value.
To accomplish this, if asingle word is aligned to multiple words in the otherlanguage, we assign the same prime number to allwords on the ?multiple?
side, and assign the productof these to the single word equivalent.Another special case is when the parse trees con-tain unary productions.
In this case, the values ofboth nodes involved in this production are the same.Our node alignment algorithm breaks this ?tie?
byselecting the node that is ?lower?
in the tree (thedaughter node of the unary production).
A simi-lar situation with two nodes being assigned identicalvalues can arise when one or more unaligned wordsare attached directly to the parent node.
Here too,our algorithm aligns the ?lower?
node and leavesthe ?higher?
node unaligned.
These decisions reflectour desire to be conservative with respect to suchambiguous cases, and their implications on the no-tion of translational equivalence.
This also providessome robustness against noisy alignments.It is straightfoward to verify that the PFA algo-rithm satisfies the wellformedness constraints de-scribed above.
Also, since multiplication is com-mutative, the algorithm is not effected by differingword orders within parallel constituent structures.The PFA algorithm run on a sample Chinese-English parallel sentence is shown in Figure 1.
Thevalue of each node as shown as a part of its label.The aligned nodes are marked by shapes.
A trianglealigns to a triangle, and squares to squares.3 Syntax-based Sub-sentential PhraseExtractionThe alignment of nodes as described in the previoussection allows us to build a comprehensive syntax-based phrase-to-phrase translation lexicon from aparallel corpus.
To build a syntax-based ?phrasetable?, we simply extract all aligned constituentnodes along with their yields and enter them intoa database, while accumulating frequency counts.In addition to the source-to-target phrase corre-spondences, we record the constituent labels of thealigned constituent nodes on both the source and tar-get sides (which may be different).
These labels?connect?
the phrases with synatactic transfer rulesduring decoding.
The set of phrases extracted fromthe example sentence in Figure 1 is shown in Fig-ure 2.90Figure 2: Phrases extracted from Aligned NodesThe process of building syntax-based ?phrase ta-bles?
from large corpora of sentence-parallel data isquite similar to the corresponding process in phrase-based SMT systems.
Our phrase correspondences,however, only reflect contiguous and complete con-stituent correspondences.
We also note that the ex-tracted phrase tables in both approaches can be mod-eled statistically in similar ways.
Similar to commonpractice in PB-SMT, we currently use the frequencycounts of the phrases to calculate relative likelihoodestimates and use these as features in our Stat-XFERdecoder.4 Evaluation of the PFA algorithmThe accuracy of our node alignment algorithm de-pends on both the quality of the word alignmentsas well as the accuracy of the parse trees.
We per-formed several experiments to assess the effects ofthese underlying resources on the accuracy of ourapproach.
The most accurate condition is when theparallel sentences are manually word-aligned, andwhen verified correct parse trees are available forboth source and target sentences.
Performance isexpected to degrade when word alignments are pro-duced using automatic methods, and when correctparse trees are replaced with automatic parser out-put.
In these experiments, we used a manually word-aligned parallel Chinese-English TreeBank consist-ing of 3342 parallel sentences.4.1 Manual Constituent Node AlignmentsWe first investigated the accuracy of our approachunder the most accurate condition.
We sampled 30sentences from the Chinese-English treebank cor-pus.
A bilingual expert from our group then man-ually aligned the nodes in these trees.
These nodePrecision Recall F-1 F-0.50.8129 0.7325 0.7705 0.7841Table 1: Accuracy of PFA Node Alignments againstManual Node Alignmentsalignments were then used as a ?gold standard?.
Wethen used the accurate parse trees and the manuallycreated word alignments for these sentence pairs,and ran the PFA node algorithm, and compared theresulting node alignments with the gold standardalignments.
The Precision, Recall, F-1 and F-0.5 re-sults are reported in Table 1.We manually inspected cases where there was amismatch between the manual and automatic nodealignments, and found several trends.
Many ofthe alignment differences were the result of one-to-many or many-to-many word alignemnts.
For ex-ample, in some cases a verb in Chinese was word-aligned to an auxiliary and a head verb on the en-glish side (e.g.
have and put).
The PFA algorithmin this case node-aligns the VP that governs the Chi-nese verb to the VP that contains both auxiliary andhead verbs on the English side.
The gold standardhuman alignments, however, in some cases, alignedthe VP of the Chinese verb to the English VP thatgoverns just the main verb.
Other mismatches wereattributed to errors or inconsistencies in the manualword alignment and to the treatment of traces andfillers in the parse trees.4.2 Effect of Using Automatic WordAlignmentsWe next tested how sensitive the PFA algorithm isto errors in automatic word alignment.
We use theentire 3342 sentences in the parallel treebank forthis experiment.
We first ran the algorithm withthe correct parse trees and manual word-alignmentsas input.
We use the resulting node alignmentsas the gold standard in this case.
We then usedGIZA++ to get bidirectional word alignments, andcombined them using various strategies.
In this sce-nario, the trees are high-quality (from the treebank),but the alignments are noisy.
The results obtainedare shown in Table 2.
Unsurprisingly, the ?Union?combination method has the best precision but worstrecall, while the ?Intersection?
combination methodhas the best recall but worst precision.
The four91Comb Method Prec Rec F-1 F-0.5Intersection 0.6382 0.5395 0.5846 0.6014Union 0.8114 0.2915 0.4288 0.5087Sym1 0.7142 0.4534 0.5546 0.5992Sym2 0.7135 0.4631 0.5616 0.6045Grow-Diag-Final 0.7777 0.3462 0.4790 0.5493Grw-Diag-Fin-And 0.6988 0.4700 0.5619 0.6011Table 2: Manual Trees, Automatic Node Alignmentsother methods for combining word alignments fallin between.
Three of the four (all except ?grow-diag-final?)
behave quite similarly.
We generally be-lieve that precision is somewhat more important thanrecall for this task, and have thus used the ?sym2?method (Ortiz-Mart?
?nez et al, 2005) (which has thebest F-0.5 score) for our translation experiments.4.3 Effect of Using Automatic ParsesWe evaluated the effect of parsing errors (as re-flected in automatically derived parse trees) on thequality of the node alignments.
We parsed the tree-bank corpus on both English and Chinese using theStanford parser, and extracted phrases using manualword alignments.
Compared to the phrases extractedfrom the manual trees, we obtained a precision of0.8749, and a recall of 0.7227, that is, an F-0.5 mea-sure of 0.8174.
We then evaluated the most ?noisy?condition that involves both automatic word align-ments and automatic parse trees.
We evaluated thephrase extraction with different Viterbi combinationstrategies.
The ?sym2?
combination gave the bestresults, with a precision of 0.6251, recall of 0.3566,thus an F-0.5 measure of 0.4996.5 Synchronous Tree Fragment and CFGRule Extraction5.1 Related WorkSyntax-based reordering rules can be used as a pre-processing step for PB-SMT (and other approaches),to decrease the word-order and syntactic distor-tion between the source and target languages (Xiaand McCord, 2004).
A variety of hierarchical andsyntax-based models, which are applied during de-coding, have also been developed.
Many of theseapproaches involve automatic learning and extrac-tion of the underlying syntax-based rules from data.The underlying formalisms used has been quitebroad and include simple formalisms such as ITGs(Wu, 1997), hierarchical synchronous rules (Chiang,2005), string to tree models by (Galley et al, 2004)and (Galley et al, 2006), synchronous CFG modelssuch (Xia and McCord, 2004) (Yamada and Knight,2001), synchronous Lexical Functional Grammarinspired approaches (Probst et al, 2002) and others.Most of the previous approaches for acquiringsyntactic transfer or reordering rules from paral-lel corpora use syntactic information from only oneside of the parallel corpus, typically the target side.
(Hearne and Way, 2003) describes an approach thatuses syntactic information from the source side toderive reordering subtrees, which can then be usedwithin a ?data-oriented translation?
(DOT) MT sys-tem, similar in framework to (Poutsma, 2000).
Ourwork is different from the above in that we use syn-tactic trees for both source and target sides to inferconstituent node alignments, from which we thenlearn synchronous trees and rules.
Our process ofextraction of rules as synchronous trees and thenconverting them to synchronous CFG rules is mostsimilar to that of (Galley et al, 2004).5.2 Synchronous Tree Fragment PairExtractionThe main concept underlying our syntactic rule ex-traction process is that we treat the node alignmentsdiscovered by the PFA algorithm (described in pre-vious sections) as synchronous tree decompositionpoints.
This reflects the fact that these nodes denotepoints in the synchronous parse trees where transla-tion correspondences can be put together composi-tionally.
Using the aligned nodes as decompositionpoints, we break apart the synchronous trees intocollections of minimal synchronous tree fragments.Finally, the synchronous fragments are also con-verted into synchronous context-free rules.
Theseare then collected into a database of synchronousrules.The input to our rule extraction process consists ofthe parallel parse trees along with their node align-ment information.
The constituent nodes in the par-allel trees that were aligned by the PFA node align-ment algorithm are treated as tree decompositionpoints.
At each such decomposition point, splitingthe two parallel trees results in two partial trees ortree fragments.
One synchronous pair consists of92the subtrees that are headed by the aligned nodeswhere the decomposition took place.
Since the sub-trees are rooted at aligned nodes, their yields aretranslation equivalents of each other.
The other syn-chronous tree fragment pair consists of the remain-ing portions of the trees.
The translation equivalenceof the complete tree (or subtree) prior to decomposi-tion implies that these tree fragments (which excludethe detached subtrees) also correspond to translationequivalents.
The tree fragments that are obtained bydecomposing the synchronous trees in this fashionare similar to the Synchronous Tree Insertion Gram-mar of (Shieber and Schabes, 1990).We developed a tree traversal algorithm that de-composes parallel trees into all minimal tree frag-ments.
Given two synchronous trees and their nodealignment decomposition information, our tree frag-ment extraction algorithm operates by an ?in-order?traversal of the trees top down, starting from the rootnodes.
The traversal can be guided by either thesource or target parse tree.
Each node in the treethat is marked as an aligned node triggers a decom-position.
The subtree that is rooted at this node isremoved from the currently traversed tree.
A copyof the removed subtree is then recursively processedfor top-down decomposition.
If the current tree nodebeing explored is not an aligned node (and thus is nota decomposition point), the traversal continues downthe tree, possibly all the way to the leaves of the tree.Decomposition is performed on the correspondingparallel tree at the same time.
We apply this pro-cess on all the aligned constituent nodes (decompo-sition points) to obtain all possible decomposed syn-chronous tree fragment pairs from the original par-allel parse trees.
This results in a collection of allminimal synchronous subtree fragments.
These syn-chronous subtree fragments are minimal in the sensethat they do not contain any internal aligned nodes.Another property of the synchronous subtree frag-ments is that their frontier nodes are either alignednodes from the original tree or leaf nodes (corre-sponding to lexical items).
Figure 3 shows somesample tree fragment pairs that were obtained fromthe example discussed earlier in Figure 1.5.3 Synchronous Transfer Rule CreationIn the last step, we convert the synchronous treefragment pairs obtained as described above into syn-Figure 3: Tree Fragment Pairs Extracted from AlignedNodeschronous context-free rules.
This creates rules in aformat that is compatible with the Stat-XFER for-malism that was described in Section 1.
Our systemcurrently does not use the internal tree structure in-formation that is contained in the synchronous treefragments.
Therefore, only the syntactic category la-bels of the roots of the tree fragments, and the nodeson the fragment frontier are relevant to decoding.This in essense corresponds to a ?flattening?
of thesynchronous tree fragment into a synchronous con-text free style rule.The flattening of the tree fragments is accom-plished by an ?in-order?
traversal on each of the treefragments to produce a string representation.
Fron-tier nodes in the fragment are either labeled con-stituent nodes or leaf nodes of the original parse tree.These form the right-hand sides of the flattened rule.The positions of the constituent nodes in the outputstring are numbered to keep track of alignment of thenodes, which is often non-monotonic due to reorder-ing between the source and target languages.
Finallythe root constituent label of the source tree fragmentbecomes the source-side parent category of the rule,while the root label of the target tree fragment be-comes the target side parent category.Accurate automatic transfer rule learning re-quires accurate word alignments and parse struc-tures.
Thus, to favor high precision (at the expenseof some loss of recall), in our work to date on Chi-nese and other languages, while we extract syntacticphrases from all available parallel data, we extract93rules only from manually word-aligned parsed par-allel data.
To compensate for the limited amount ofdata, we generalize the rules as much as possible.Elements in the rules that originate from leaf nodesin the parse trees are generalized to their part-of-speech categories, if the corresponding words wereone-to-one aligned in the parallel sentences.
Un-aligned words and words that are part of one-to-many alignments are not generalized to the POSlevel and remain lexicalized in the final rule.The phrase table extracted from the corpus and therules are scored together to ensure that they are con-sistent when used in our translation system.
For allStat-XFER experiments to date, we have used justthe source side conditionig with a constant smooth-ing factor for robustness to noise.6 Extraction Applied to Chinese-EnglishParallel DataWe used the pipeline of PFA node alignment fol-lowed by rule extraction to build resources for aStat-XFER Chinese-to-English MT system.
Thesyntax-based phrase table was constructed fromtwo large parallel corpora released by LDC for theDARPA/GALE program.
The parallel sentences forboth English and Chinese were parsed using theStanford parser.
The first corpus consists of about1.2 million sentence pairs.
Our extraction processapplied to this corpus resulted in a syntax-basedphrase table of about 9.2 million entries.
The otherdata source used was a parallel corpus of about 2.6million sentences, but many of its entries were froma Chinese-English lexicon.
From this corpus, we ex-tracted 8.75 million phrases.Rule learning was performed on a 10K-sentenceparallel corpus that was manually word-aligned, re-leased by LDC for the DARPA/GALE program.This manually word-aligned corpus includes the par-allel Chinese-English treebank of 3,343 sentencepairs.
The treebank sentences come with verifiedcorrect parse trees for English and Chinese.
The restof the 10K corpus was parsed by the Stanford parser.The complete 10K parallel corpus was node alignedand rules were extracted as described in Section 5.Figure 3 shows two synchronous tree fragments thatwere extracted from the example node-aligned sen-tence pair in Figure 1.
After generalization and flat-Figure 4: Rules Extracted from Tree PairsTable 3: Statistics for Chinese-English Rulestening, we obtain rules such as those shown in Fig-ure 4.
The above process resulted in a collectionof almost 100K rules.
Some statistics on this ruleset are shown in Table 3.
Analysis of this rule setindicates that only about 4% of these rules were ob-served more than once in the data.
These includethe most general and useful rules for mapping Chi-nese syntactic structures to their corresponding En-glish structures.
Most of the ?singleton?
rules arehighly lexicalized.
A large portion of the singletonrules are noisy rules, but many of them are good anduseful rules.
Experiments indicate that removing allsingleton rules hurts translation performance.7 ConclusionsThe process described in this paper provides a fullyautomated solution for extracting large collectionof reliable syntax-based phrase tables and syntac-tic synchronous transfer rules from large volumesof parsed parallel corpora.
In conjunction with theStat-XFER syntax-based framework, this provides afully automated solution for building syntax-basedMT systems.
The current performance of this ap-proach still lags behind state-of-the-art phrase-basedsystems when trained on the same parallel data but isshowing encouraging improvements.
Furthermore,the resources extracted by our process can be usedby various other syntax-based MT approaches.94ReferencesDavid Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In ACL ?05: Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 263?270, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Su-san Dumais; Daniel Marcu and Salim Roukos, editors,HLT-NAACL 2004: Main Proceedings, pages 273?280, Boston, Massachusetts, USA, May 2 - May 7.Association for Computational Linguistics.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In ACL ?06:Proceedings of the 21st International Conference onComputational Linguistics and the 44th annual meet-ing of the ACL, pages 961?968, Morristown, NJ, USA.Association for Computational Linguistics.Declan Groves, Mary Hearne, and Andy Way.
2004.
Ro-bust sub-sentential alignment of phrase-structure trees.In COLING ?04: Proceedings of the 20th internationalconference on Computational Linguistics, page 1072,Morristown, NJ, USA.
Association for ComputationalLinguistics.M.
Hearne and A.
Way.
2003.
Seeing the wood for thetrees: Data-oriented translation.Kenji Imamura, Hideo Okuma, Taro Watanabe, and Ei-ichiro Sumita.
2004.
Example-based machine transla-tion based on syntactic transfer with statistical mod-els.
In COLING ?04: Proceedings of the 20th in-ternational conference on Computational Linguistics,page 99, Morristown, NJ, USA.
Association for Com-putational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In NAACL?03: Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,pages 48?54, Morristown, NJ, USA.
Association forComputational Linguistics.Alon Lavie.
2008.
A general search-based syntax-drivenframework for machine translation.
In Invited paper inProceedings of CICLing-2008, pages 362?375.
Com-putational Linguistics and Intelligent Text Processing,LNCS 4919,Springer.D.
Ortiz-Mart?
?nez, I.
Garc?
?a-Varea, and F. Casacuberta.2005.
Thot: a toolkit to train phrase-based statisti-cal translation models.
In Tenth Machine TranslationSummit.
AAMT, Phuket, Thailand, September.Arjen Poutsma.
2000.
Data-oriented translation.
In Pro-ceedings of the 18th conference on Computational lin-guistics, pages 635?641, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Katharina Probst, Lori Levin, Erik Peterson, Alon Lavie,and Jaime Carbonell.
2002.
Mt for minority lan-guages usingelicitation-based learning of syntactic-transfer rules.
Machine Translation, 17(4):245?270.Yvonne Samuelsson and Martin Volk.
2007.
Alignmenttools for Parallel Treebanks.
In Proceedings of theGLDV Fruhjahrstagung.Stuart M. Shieber and Yves Schabes.
1990.
Synchronoustree-adjoining grammars.
In Proceedings of the 13thConference on Computational Linguistics, pages 253?258, Morristown, NJ, USA.
Association for Computa-tional Linguistics.John Tinsley, Mary Hearne, and Andy Way.
2007.
Ex-ploiting Parallel Treebanks to Improve Phrase-BasedStatistical Machine Translation.
In Proceedings ofthe Sixth International Workshop on Treebanks andLinguistic Theories (TLT-07), pages 175?187, Bergen,Norway.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical machine translation system with automaticallylearned rewrite patterns.
In COLING ?04: Proceed-ings of the 20th International Conference on Compu-tational Linguistics, page 508, Morristown, NJ, USA.Association for Computational Linguistics.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In ACL ?01: Proceedingsof the 39th Annual Meeting on Association for Compu-tational Linguistics, pages 523?530, Morristown, NJ,USA.
Association for Computational Linguistics.95
