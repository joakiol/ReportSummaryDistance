Qualitative and Quantitative Modelsof Speech TranslationHiyan AlshawiAT,~T Bell Laboratories600 Mountain AvenueMurray Hill, NJ 07974, USAhiyan@research, at t.comAbstractThis paper compares a qualitative reasoning model oftranslation with a quantitative statistical model.
Weconsider these models within the context of two hy-pothetical speech translation systems, starting with alogic-based esign and pointing out which of its char-acteristics are best preserved or eliminated in movingto the second, quantitative design.
The quantitativelanguage and translation models are based on relationsbetween lexical heads of phrases.
Statistical parame-ters for structural dependency, lexical transfer, and lin-ear order are used to select a set of implicit relationsbetween words in a source utterance, a correspondingset of relations between target language words, and themost likely translation of the original utterance.1.
IntroductionIn recent years there has been a resurgence of interest instatistical approaches to natural language processing.Such approaches are not new, witness the statisticalapproach to machine translation suggested by Weaver(1955), but the current level of interest is largely dueto the success of applying hidden Markov models andN-gram language models in speech recognition.
Thissuccess was directly measurable in terms of word recog-nition error rates, prompting language processing re-searchers to seek corresponding improvements in per-formance and robustness.
A speech translation system,which by necessity combines peech and language tech-nology, is a natural place to consider combining the sta-tistical and conventional pproaches and much of thispaper describes probabilistic models of structural lan-guage analysis and translation.
Our aim will be to pro-vide an overall model for translation with the best ofboth worlds.
Various factors will lead us to concludethat a lexicalist statistical model with dependency re-lations is well suited to this goal.As well as this quantitative approach, we will considera constraint/logic based approach and try to distinguishcharacteristics that we wish to preserve from those thatare best replaced by statistical models.
Although per-haps implicit in many conventional pproaches to trans-lation, a characterization in logical terms of what is be-ing done is rarely given, so we will attempt o makethat explicit here, more or less from first principh's.Before proceeding, I will first examine some fashiou-able distinctions in section 2 in order to clarify the is-sues involved in comparing these approaches.
I willattempt o argue that the important distinction is notso much a rational-empirical or symbolic-statistical dis-tinction but rather a qualitative-quantitative on .
Thisis followed by discussion of the logic-based model insection 3, the overall quantitative model in section 4,monolingual models in section 5, translation modelsin section 6, and some conclusions in section 7.
Weconcentrate hroughout on what information about lan-guage and translation is coded and how it is express('das logical constraints or statistical parameters.
Al-though important, we will say little about search al-gorithms, rule acquisition, or parameter estimation.2.
Qualitative and Quantitative ModelsOne contrast often taken for granted is the identifica-tion of a 'statistical-symbolic' distinction in languageprocessing as an instance of the empirical vs. rationaldebate.
I believe this contrast has been exaggeratedthough historically it has had some validity ill termsof accepted practice.
Rule based approaches have be-come more empirical in a number of ways: First, a moreempirical approach is being adopted to grammar devel-opment whereby the rule set is modified according toits performance against corpora of natural text (e.g.Taylor, Grovel and Briscoe 1989).
Second, there is aclass of techniques for learning rules from text, a recentexample being Brill 1993.
Conversely, it is possible toimagine building a language model in which all prob-abilities are estimated according to intuition withoutreference to any real data, giving a probabilistic mod~,lthat is not empirical.Most language processing labeled as statistical in-volves associating real-number valued parameters toconfigurations of symbols.
This is not surprising iventhat natural language, at least in written form, is explic-itly symbolic.
Presumably, classifying a system as sym-bolic must refer to a different set of (internal) symbols,but even this does not rule out many statistical sys-trrrls modeling events involving nonterminal categoriesand word senses.
Given that the notion of a symbol,let.
alone an 'internal symbol', is itself a slippery one, itmay he unwise to build our theories of language, or eventl,.
way we classify different heories, on this notion.Instead, it would seem that the real contrast drivingthe shift towards tatistics in language processing is acontrast between qualitative systems dealing exclusivelywith combinatoric constraints, and quantitative systemsthat involve computing numerical functions.
This bearsdir~.ctly on the problems of brittleness and complexitythat discrete approaches to language processing sharewll,ll, for example, reasoning systems based on tradi-tional logical inference.
It relates to the inadequacy ofthe dominant theories in linguistics to capture 'shades'of meaning or degrees of acceptability which are oftenrecognized by people outside the field as important in-herent properties of natural anguage.
The qualitative-quantitative distinction can also be seen as underlyingthe difference between classification systems based onI'cature specifications, as used in unification formalisms(Shicber 1986), and clustering based on a variable de-gr~,e of granularity (e.g.
Pereira, Tishby and Lee 1993).It seems unlikely that these continuously variable as-pcct:s of fluent natural anguage can be captured by apurely combinatoric model.
This naturally leads to theqtwstion of how best to introduce quantitative model-i,g into language processing.
It is not, of course, nec-,,ssary for the quantities of a quantitative model to beprobabilities.
For example, we may wish to define real-valued functions on parse trees that reflect the extentto which the trees conform to, .say, minimal attachmentand parallelism between conjuncts.
Such functions havebeen used in tandem with statistical functions in ex-periments on disambiguation (for instance Alshawi and(',a.rter 1994).
Another example is connection strengthsi, m~ural network approaches to language processing,th,mgh it.
has been shown that certain networks are~,tfectively computing probabilities (Richard and Lipp-mann 1991).Nevertheless, probability theory does offer a coher-ent and relatively well understood framework for select-ing between uncertain alternatives, making it a naturalchoice for quantitative language processing.
The casef.r probability theory is strengthened by a well devel-,,p-d empirical methodology in the form of statisticalI,:~ramet.ccr estimation.
There is also the strong connec-l i,,n between probability theory and the formal theory.1" i.formation and communication, a connection thathas been exploited in speech recognition, for exampleI~qing tim concept of entropy to provide a motivated way,.f measuring the complexity of a recognition problem(.h'lim'k et ai.
1992).I",v,'n if probability t|wory remains, as it currentlyis, th,, m~.l.llod of clloicc in making language processingqu.ntitative, this still h~aw:s the fieht wide open in terms.,f carving up languag~ processing into an appropriateset ,,f ,,wmts tbr probability theory to work with.
Fortranslation, a very direct apprgach using parametersbased on surface positions of words in source and targetsentences was adopted in the Candide system (Brownet at.
1990).
However, this does not capture importantstructural properties of natural anguage.
Nor does ittake into account generalizations about ranslation thatare independent of the exact word order in source andtarget sentences.
Such generalizations are, of course,central to qualitative structural approaches to transla-tion (e.g.
Isabelle and Macklovitch 1986, Alshawi et at.1992).The aim of the quantitative language and translationmodels presented insections 5and 6 is to employ proba~bilistic parameters that reflect linguistic structure with-out discarding rich lexical information or making themodels too complex to train automatically.
In terms ofa traditional classification, this would be seen as a 'hy-brid symbolic-statistical' system because it deals withlinguistic structure.
From our perspective, it can beseen as a quantitative v rsion of the logic-based modelbecause both models attempt to capture similar infor-mation (about the organization of words into phrasesand relations holding between these phrases or their ref-erents), though the tools of modeling are substantiallydifferent.3.
Dissecting a Logic-Based SystemWe now consider a hypothetical speech translation sys-tem in which the language processing components fol-low a conventional qualitative transfer design.
Al-though hypothetical, this design and its components aresimilar to those used in existing database query (Raynerand Alshawi 1992) and translation systems (Alshawi etal 1992).
More recent versions of these systems havebeen gradually taking on a more quantitative flavor,particularly with respect o choosing between alterna-tive analyses, but our hypothetical system will be morepurist in its qualitative approach.The overall design is as follows.
We assume thata speech recognition subsystem delivers a list of textstrings corresponding to transcriptions of an input ut-terance.
These recognition hypotheses are passed to aparser which applies a logic-based grammar and lexiconto produce a set of logical forms, specifically formulasin first order logic corresponding topossible interpreta-tions of the utterance.
The logical forms are filtered bycontextual nd word-sense constraints, and one of themis passed to the translation component.
The translationrelation is expressed by a set of first order axioms whichare used by a theorem prover to derive a target languagelogical form that is equivalent (in some context) to thesource logical form.
A grammar for tile target languageis then applied to the target form, generating a syntaxtree whose fringe is passed to a speech synthesizer.
"Faking the various components in turn, we make anote of undesirable properties that might be improvedby quantitative modeling.2Ana lys i s  and  Generat ionA grammar, expressed as a set of syntactic rules (ax-ioms) Gsv, and a set of semantic rules (axioms)Gsem isused to support a relation form holding between stringss and logical forms ?
expressed in first order logic:a.y.
u a,.m f o m( s, ?)
.
"The relation form is many-to-many, associating astring with linguistically possible logical form interpre-tations.
In the analysis direction, we are given s andsearch for logical forms ?, while in generation we searchfor strings s given ?.For analysis and generation, we are treating stringss and logical forms ?
as object level entities.
In inter-pretation and translation, we will move down from thismeta-level reasoning to reasoning with the logical formsas propositions.The list of text strings handed by the recognize/tothe parser can be assumed to be ordered in accordancewith some acoustic scoring scheme internal to the rec-ognizer.
The magnitude of the scores is ignored by ourqualitative language processor; it simply processes thehypotheses one at a time until it finds one for whichit can produce a complete logical form interpretationthat passes grammatical nd interpretation constraints,at which point it discards the remaining hypotheses.Clearly, discarding the acoustic score and taking thefirst hypothesis that satisfies the constraints may leadto an interpretation that is less plausible than one deriv-able from a hypothesis further down in the recognitionlist.
But there is no point in processing these laterhypotheses since we will be forced, to select one inter-pretation essentially at random,Syntax  The syntactic rules in Gsv.
relate 'category'predicates co, ct, c2 holding of a string and two spanningsubstrings (we limit the rules here to two daughters forsimplicity):c0(s0) A daughters(so, sl, s2)el(st) A cz(s2) A (so = concat(st, s2))(Here, and subsequently, variables like so and st areimplicitly universally quantified.)
G~v,~ also includeslexical axioms for particular strings w consisting of sin-gle words:el(w), .
.
.For a feature-based grammar, these rules can in-clude conjuncts constraining the values, a l ,a~,.
.
.
,  ofdiscrete-valued functions f on the strings:f(w) = al, f(so) = f(St).The main problem here is that such grammars haveno notion of a degree of grammatical cceptability - asentence is either grammatical or ungrammatical.
Forsmall grammars this means that perfectly acceptablestrings are often rejected; for large grammars we got avast number of alternative trees so the chance of seh'ct-ing the correct tree for simple Nell{.CllCes C;tl l  gel.
worso~Lg the gralnmar cow'rago increas,,s.
'\['hcre is also tl,.problem of requiring increasingly comph,x feature setsto describe idiosyncrasies in the lexicon.Semant ics  Semantic grammar axioms belonging toGsem specify a 'composition' function g for deriving alogical form for a phrase from those for its subphrasos:form(so, g(?t, ?2))daughters(so, st, s2)Acj (st)Ac2(s2)Acl~(s0)A form(sl, el) A form(s2, ?2)The interpretation rules for strings l)ottom out ill a setof lexical semantic rules associating words with pred-icates (pl,P2,...) corresponding to 'word senses'.
Fora particular word and syntactic ategory, there will boa (small, possibly empty) finite set of such word sensepredicates:el(w) ~ form(w,p~)cdiw) ~ form(w,pim).First order logic was assunmd as the semantic repre-sentation language because it comes with well under-stood, if not very practieM, inferential machinery forconstraint solving.
However, applying this machinoryrequires making logical forms fine grained to a degroeoften not warranted by the information the speaker ofan utterance intended to convey.
An example of this isexplicit scoping which leads (again) to large numlmrs ofalternatives which the qualitative model has diflicultychoosing between.
Also, many natural language sen-tences cannot be expressed in first order logic withoutresort to elaborate formulas requiring complex seman-tic composition rules.
These rules can be simplilied byusing a higher order logic but at the expense of cw.nless practical inferential machinery.In applying the grammar in generation we arefaced with the problem of balancing over and under-generation by tweaking rammatical constraints, therebeing no way to prefer fully grammatical target sen-tences over more marginal ones.
Qualitative approachesto grammar tend to emphasize the ability to capl, urogeneralizations a the main measure of success in lin-guistic modeling.
This might explain why producingappropriate l xical collocations i rarely addressed seri-ously in these models, even though lexical collocationsare important for fluent generation.
'/'he study of col-locations for generation fits in more naturally with sl.a-tistical techniques, as illustrated by Smajda and McK-eown (1990).In terpreta t ionIn the logic-based model, interpretation is the processof identifying from the possible interpretations ~ of s for3which fo rm(s ,  qt) hold, ones that are consistent with the,',,m~,xt of interpretation.
We can state this as follows:/f U.~'U A ~ O.Ih.r,., we haw~ separated the context into a contingents,,I ,ff contextual propositions S and a set R of (mono-l i ngual) 'meaning postulates', or selectional restrictions,that constrain the word sense predicates in all contexts..1 is a set of assumptions sufficient o support the in-I,'rl)n'lation ?
given S and R. In other words, this ish,~crl)rctal, ion as abduction' (Itobbs et al 1988), since~!
)(i,('lion, not deduction, is needed to arrive at the:~>.
'd H II I~tiOIIS ,4.
'l'h(" ,host common types of meaning postulates in Rart, t h,,s~" for restriction, hyponymy, and disjointness,, \l,l'<:.~sed a.
'~ fo l lows :HI ( .
l ' l ,  X2) ~ p2(x!  )
res t r i c t ion ;t,:?
(x) --* p3(x )  hyponymy;-~(pa(x) A p4(x)) disjointness.Although there are compilation techniques (e.g.
Mel-lish 19~) which allow sclectional constraints tated inthis fashion to be implemented efficiently, the schemei~ I,rol)lematic iu other respects.
To start with, the as-s~t~ttl~l ion of a small set of senses for a word is at best;~wkward because it is difficult to arrive at an optimalgra,ularity for sense distinctions.
Disambiguation withs,qcctionai restrictions expressed as meaning postulatesis also prol)lematic because it is virtually impossible to,levis, a set of postulates that will always filter all but,,t,, alt.crnative.
We are thus forced to under-filter andmake an arbitrary choice between remaining alterna-tives.Log ic  based  t rans la t ionIn hoth the quantitative and qualitative models we takea t ransfi~r approach to translation.
We do not depend.
!~ im.
('rlingual symbols, but instead map a representa-I i,:)n with constants associated with the source languageinlx) a corresponding expression with constants from thel ar~ct language.
For the qualitative model, the opera-hh, notion of correspondence is based on logical equiva-hql('e and the constants are source word sense predicatesI'1, t"-' .
.
.
.
and target sense predicates ql,  q2, .
.
.
.More specifically, we will say the translation relationhH we~,n a source logical form Cs and a target logicali;,r~t 6t holds if we have/~ u .
'~' u A' ~ (q~., ~ ~,)wh,.n, I~ is a s~.t of monolingual and bilingual mean-I J;:.
i,t).~l.ulal.es, and  ,S' is ;t se t  o f  fo rmulas  characterizingI.h*' ~'lli'l','llt COllt~xt.
.
'l I is a s,,t of assumptions thatin,h=,h's I.h,' assunlptions A which SUl)ported ~bs.
ilereI,ili,,~ual me;ruing i~osl.ulal.~.s a.re first order axioms re-hll.ing source and target sense predicates.
A typicalI,ilin~ual posl.ulate Ibr translal.ing between Pl an(I qlii~it~;lil h,, of th,.
for,n:p5(~1) ~ (p1(~1, z2) ~ ql(z l ,  z2)).The need for the assumptions A' arises when a sourcelanguage word is vaguer that its possible translationsin the target language, so different choices of targetwords will correspond to translations under differentassumptions.
For example, the condition ps(x l )  abovemight be proved from the input logical form, or it mightneed to be assumed.In the general case, finding solutions (i.e.
A', ~bt pairs)for the abductive schema is an undecidable theoremproving problem.
This can be alleviated by placing re-strictions on the form of meaning postulates and inputformulas and using heuristic search methods.
Althoughsuch an approach was applied with some success ina limited-domain system translating logical forms intodatabase queries (Rayner and Alshawi 1992), it is likelyto be impractical for language translation with tens ofthousands of sense predicates and related axioms.Setting aside the intractability issue, this approachdoes not offer a principled way of choosing between al-ternative solutions proposed by the prover.
One wouldlike to prefer solutions with 'minimal' sets of assump-tions, but it is difficult to find motivated efinitions forthis minimization in a purely qualitative framework.4.
Quant i ta t ive  Mode l  ComponentsMov ing  to  a Quant i ta t ive  Mode lIn moving to a quantitative architecture, we propose toretain many of the basic characteristics of the qualita-tive model:?
A transfer organization with analysis, transfer, andgeneration components.?
Monolingual models that can be used for both anal-ysis and generation.?
Translation models that exclusively code contrastive(cross-linguistic) information.?
Hierarchical phrases capturing recursive linguisticstructure.Instead of feature based syntax trees and first-orderlogical forms we will adopt a simpler, monostratal rep-resentation that is more closely related to those foundin dependency grammars (e.g.
Hudson 1984).
Depen-dency representations have been used in large scalequalitative machine translation systems, notably byMcCord (1988).
The notion of a lexical 'head' of aphrase is central to these representations because theyconcentrate on relations between such lexical heads.
Inour case, the dependency representation is monostratalin that the relations may include ones normally classi-fied as belonging to syntax, semantics or l)ragmatics.One salient property of our language model is that itis strongly lexical: it consists of statistical parametersassociated with relations between lexical items and thenumber and ordering of dependents of lexical heads.This lexical anchoring facilitates tatistical training and4sensitivity to lexical variation and collocations.
In orderto gain the benefits of probabilistic modeling, we replacethe task of developing large rule sets with the task ofestimating large numbers of statistical parameters forthe monolingual nd translation models.
This gives riseto a new cost trade-off in human annotation/judgementversus barely tractable fully automatic training.
It alsonecessitates further research on lexical similarity andclustering (e.g.
Pereira, Tishby and Lee 1993, Dagan,Marcus and Markovitch 1993) to improve parameterestimation from sparse data.T rans la t ion  v ia  Lex ica l  Re la t ion  GraphsThe model associates phrases with relation graphs.
Arelation graph is a directed labeled graph consisting ofa set of relation edges.
Each edge has the form of anatomic proposition~(wi, w~)where r is a relation symbol, wi is the lexical head ofa phrase and wj is the lexical head of another phrase(typically asubphrase of the phrase headed by w~).
Thenodes wi and wj are word occurrences representable bya word and an index, the indices uniquely identifyingparticular occurrences of the words in a discourse orcorpus.
The set of relation symbols is open ended, butthe first argument of the relation is always interpretedas the head and the second as the dependent with re-spect to this relation.
The relations in the models forthe sour~:e and target languages need not be the same,or even overlap.
To keep the language models simple,we will mainly restrict ourselves here to dependencygraphs that are trees with unordered siblings.
In partic-ular, phrases will always be contiguous strings of wordsand dependents will always be heads of subphrases.Ignoring algorithmic issues relating to compactly rep-resenting and efficiently searching the space of alterna-tive hypotheses, the overall design of the quantitativesystem is as follows.
The speech recognizer producesa set of word-position hypotheses (perhaps in the formof a word lattice) corresponding to a set of string hy-potheses for the input.
The source language model isused to compute a set of possible relation graphs, withassociated probabilities, for each string hypothesis.
Aprobabilistic graph translation model then provides, foreach source relation graph, the probabilities of derivingcorresponding graphs with word occurrences from thetarget language.
These target graphs include all thewords of possible translations of the utterance hypothe-ses but do not specify the surface order of these words.Probabilities for different possible word orderings arecomputed according to ordering parameters which formpart of the target language model.In the following section we explain how the probabil-ities for these various processing stages are combined toselect the most likely target word sequence.
This wordsequence can then be handed to the speech synthesizer.For tighter integration between getmraliovt aml sy,,tl,~',sis, information about the derivation of I.Iw l,arg,'l uII,erance can also I)c passed to the syuthesizcr.In tegrated  Stat is t ica l  Mode lThe probabilities associated with phrases in the abov,,description are computed according to the statisticalmodels for analysis, translation, and generation.
In thissection we show the relationship between these mod-els to arrive at an overall statistical model of sp,,,.," htranslation.
We are not considering training ismws inthis paper, though a number of now familiar techniquesranging from methods for maximum likelihood estima-tion to direct estimation using fully annotated ata areapplicable.The objects involved in the overall model are as Jbl-lows (we omit target speech synthesis under the, as-sumption that it proceeds deterministically from a tar-get language word string):?
A0: (acoustic evidence for) source language spe~'ch?
Wo: source language word string?
Wz: target language word string?
C0: source language relation graph?
Ct: target language relation graphGiven a spoken input in the source language, wewish to find a target language string that is the mostlikely translation of the input.
We are thus interestc.din the conditional probability of We given A,.
Thisconditional probability can be expressed as follows (of.Chang and Su 1993):P(WdA,) =~W,,C,,Ct P(WolAo) P(C, IW,, A,)P(CdCo, W,, A?)
PCWd(:,, C,, W.,, 4, ).We now apply some simplifying independence .s-sumptions concerning relation graphs.
Specifically.
thattheir derivation from word strings is independent ofacoustic information; that their translation is indepen-dent of the original words and acoustics involved; andthat target word string generation from target relationedges is independent of the source language represent, a-tions.
The extent o which these (Markovian) assump-tions hold depend on the extent o which relation edgesrepresent all the relevant information for translation.In particular it means they should express aspects ofsurface relevant o meaning, such as topicalization, aswell as predicate argument structure.
In any case, thesimplifying assumptions give the following:P(W~IA, ) _~~w.,c.,c, P( W, IA, ) P(C01W,) P( Ct lCo ) P( Wt I?
:, ).This can be rewritten with two applications of Bay,,~5I ' l lh':v" L.,W.. ,C~,('t P( A, IW,) ( I / P(A.,)) P(WolC,)P(C,) P(C~IC, ) P(W, ICt).Since A, is given, l IP(A,) is a constant which can beignored in finding the maximum of P(Wt\]As).
Deter-mining Wt that maximizes P(WdA, ) therefore involvesthe following factors:* I'(A, I W, ): source language acoustics?
/'(\[.V, IC,): source language generation.
I'(C.,): source content relations?
/'(('tiCs): source to target ransfer?
I'(IVtlC't ): target language generationWc a.,~ume that the speech recognizer provides acous-tic scores proportional to P(A, IW, ) (or logs thereof).Sud~ scores are normally computed by speech recogni-l i,,n systems, although they are usually also multipliedby w,,rd-based language model probabilities P(W,)which we do not require in this application context.
()ur approach to language modeling, which covers thecorn.cat analysis and language generation factors, is pre-:~,,uted in section 5 and the transfer probabilities fallumh,r the translation model of section 6.Finally note thai.
by another application of Bayes,-,d,, w,, can replace the two factors P(C,)P(CdC,) byI'(Ct)l'(C, lCt} without changing other parts of themodel.
Tiffs latter fornmlation allows us to apply con-straints imposed by the target language model to ill-t,'r inappropriate possibilities suggested by analysis andtra.sfi~r.
In some respects this is similar to Dagan andItai's (I 994) approach to word sense disambiguation us-ing statistical associations in a second language.5.
Language Mode lsLanguage Product ion  Mode l~).r bmguage model can be viewed in terms of a proba-bihstic generative process based on the choice of lexical"heads" of phrases and the recursive generation of sub-;,bra~es and their ordering.
For this purpose, we can de-(ira, tho head word of a phrase to be the word that moststrongly influences the way the phrase may be com-biucd with other phrases.
This notion has been centralto a number of approaches to grammar for some time,including theories like dependency grammar (HudsonI!~7(;, 1990) and HPSG (Pollard and Sag 1987).
More;,'~,.t,l.ly, the statistical properties of associations be-Iw,.,'n words, and more particularly heads of phrases,JL:t.~ J~,~'~,l|lql, al  a.el.iw; area of research (e.g.
Chang, l,uo,aml Su 1992; Ilindlc and R.ooth 1993).
'l'h,' language model factors the statistical derivation,,f a .~'ul.ence with word string W as follows:I ' ( l l )  = ~, :  P(C) P(WIC)where C ranges over relation graphs.
The contentmodel, P(C), and generation model, P(WIC), are com-ponents of the overall statistical model for spoken lan-guage translation given earlier.
This decomposition fP(W) can be viewed as first deciding on the content ofa sentence, formulated as a set of relation edges accord-ing to a statistical model for P(C), and then decidingon word order according to P(WIC ).Of course, this decomposition simplifies the realitiesof language production in that real language is alwaysgenerated in the context of some situation S (real orimaginary), so a more comprehensive model would beconcerned with P(CIS), i.e.
language production incontext.
This is less important, however, in the trans-lation setting since we produce Ct in the context of asource relation graph C, and we assume the availabilityof a model for P(CtlC,).Content Derivation ModelThe model for deriving the relation graph of a phraseis taken to consist of choosing a lexical head h0 for thephrase (what the phrase is 'about') followed by a seriesof 'node expansion' steps.
An expansion step takes anode and chooses apossibly empty set of edges (relationlabels and ending nodes) starting from that node.
Herewe consider only the case of relation graphs that aretrees with unordered siblings.To start with, let us take the simplified case where ahead word h has no optional or duplicated ependents(i.e.
exactly one for each relation).
There will be a setof edgesE(h) = {rl(h, wl), r~(h, w2) ... r~(h, wk)}corresponding to the local tree rooted at h with depen-dent nodes Wl...wk.
The set of relation edges for theentire derivation is the union of these local edge sets.To determine the probability of deriving a relationgraph C for a phrase headed by h0 we make use ofparameters ('dependency parameters')P(r(h,w)lh, r)for the probability, given a node h and a relation r,that w is an r-dependent of h. Under the assumptionthat the dependents ofa head are chosen independentlyfrom each other, the probability of deriving C is:P(C) = P(Top(ho)) I~Ir(h.~)?c P(r(h, w)lh, r)where P(Top(ho)) is the probability of choosing h0 tostart the derivation.If we now remove the assumption made earlier thatthere is exactly one r-dependent of a head, we need toelaborate the derivation model to include choosing thenumber of such dependents.
We model this by param-etersP(N(r,n)lh)6that is, the I)rol)aifility that head h h+~ n r-dep(m(lents.We will r,ffer t,o t,|lis I)robability ;m a '(let, all parameter'.Our previous assmnption amounted to stating that thiswas always 1 for n = 1 or for n = 0.
Detail parametersallow us to model, for example, the number of adjectivalmodifiers of a noun or the 'degree' to which a particularargument of a verb is optional.
The probability of anexpansion of h giving rise to local edges E(h) is now:P(E(h) lh)  =Fir P(N(r, nr)lh) k(nr) I\]l<i<r~ P(r(h, w\[)lh , r).where r ranges over the set of relation labels and h hasnr r-dependents w~... w nP .
k(nr) is a combinatorie con-stant for taking account of the fact that we are not dis-tinguishing permutations of the dependents (e.g.
thereare n,.!
permutations of the r-dependents of h if thesedependents are all distinct).So if h0 is the root of a tree C, we haveP(C) = P(Top(ho)) rIheh~aa,(c) P(Ec(h)lh)where heads(C) is thc set of nodes in C and Ec(h) isthe set of edges headed by h in C.The above formulation is only an approximation forrelation graphs that are not trees because the indepen-dence assumptions which allow the dependency param-eters to be simply multiplied together no longer holdfor the general case.
Dependency graphs with cycles doarise as the most natural analyses of certain linguisticconstructions, but calculating their probabilities on anode by node basis as above may still provide proba-bility estimates that are accurate nough for practicalpurposes.Generat ion  Mode lWe now return to the generation model P(WIC).
Asmentioned earlier, since C includes the words in W anda set of relations between them, the generation modelis concerned only with surface order.
One possibility isto use 'bi-relation' parameters for the probability thatan ri-dependent immediately follows an u-dependent.This approach is problematic for oui: overall statisti-cal model because such parameters are not independentfrom the 'detail' parameters specifying the number ofr-dependents of a head.We therefore adopt the use of 'sequencing' parame-ters, these being probabilities of particular orderings ofdependents given that the multiset of dependency rela-tions is known.
We let the identity relation e stand forthe head itself.
Specifically, we have parametersP(slM(s))where s is a sequence of relation labels including an oc-currence of e and M(s) is the multiset for this sequence.For a head h in a relation graph C, let swch be the se-quence of dependent relations induced by a particularword string W generated from C. We now haves>(WlC) = I-Ih~w(Il.
~-~--~ ) l ' ( .
sw < "h I M ( ~'w < "h ))where It ranges over all the heads in (;, aud m. is I.h<'number of occurrences of r in sW(:h, assuming that allorderings of nr-dependents are equally likely.
We canthus use these sequencing parameters directly in ouroverall model.To summarize, our monolingual models are specifi,'dby:* topmost head parameters P(Top(h))* dependency parameters P(r(h, w)lh, r)+ detail parameters P(N(r, n)lh )* sequencing parameters P(s\[M(s))The overall model splits the contributions of ('ollt~mtP(C) and ordering P(WIC ).
However, we may alsowant a model for P(W),  for example for pruning spec(:hrecognition hypotheses.
Combining our content ;rod or-dering models we get:P(W) = Z P(C) P(WIC)c= ~C P(Top(hc)) H P(swc'hlh)hEWH P( r (h ,  w) lh ,  ,')r(h,w)eE?
(h)The parameters P(slh ) can be derived by combiningsequencing parameters with the detail parameters forh.6.
T rans la t ion  Mode lMapp ing  Re la t ion  GraphsAs already mentioned, the translation model delinesmappings between relation graphs C., for the sourcelanguage and Ct for the target language.
A direct(though incomplete)justification of translation via n.-lation graphs may be based on a simple referential viewof natural language semantics.
Thus nominals andtheir modifiers pick out entities in a (real or imagi-nary) world, verbs and their modifiers refer to actionsor events in which the entities participate in roles in-dicated by the edge relations.
Under this view, thepurpose of the translation mapping is to determhm atarget language relation graph that provides the bestapproximation to the referential function induced bythe source relation graph.
We call this approximatingreferential equivalence.This referential view of semantics i not adequate fortaking account of much of the complexity of naturallanguage including many aspects of quantification, dis-tributivity and modality.
This means it cannot capturesome of the subtleties that a theory based on logicalequivalence might be expected to.
On the other hand,when we proposed a logic based approach as our quali-tative model, we had to restrict it to a simple first order7logic anyway for computational reasons, and even thenit did not appear to be practical.
Thus using the moreimpow~rished lexical relations representation may nottw costing us much in practice.One aspect of the representation that is particularlyuseful in the translation application is its conveniencefor partial and/or incremental representation f contentwe can refine the representation by the addition of fur-thor edges.
A fully specified enotation of the meaningof a s,mtence is rarely required for translation, and asw,~ pointed out when discussing logic representations, ac~mq~lete specification may not have been intended byth,, slwaker.
Although we have not provided a denota-tio.al semantics for sets of relation edges, we anticipatethat this will be possible along the lines developed inm(motonic semantics (Alshawi and Crouch 1992).T rans la t ion  Parameters'1'o bc practical, a model for P(CtIC,) needs to decom-pose the source and target graphs C~ and Ct into sub-graphs mall enough that subgraph translation parame-ters can be estimated.
We do this with the help of 'nodea.lignment relations' between the nodes of these graphs.
'l'lmse alignment relations are similar in some respectsto the alignments used by Brown et al (1990) in theirsurface translation model.
The translation probabilityis then the sum of probabilities over different alignments.t:I'(C, ICo) = ~s  P(C. flC,).There are different ways to model P(Ct,.tIC,) corre-sp(mding to different kinds of alignment relations anddifferent independence assumptions about the transla-tion mapping.l"or our quantitative design, we adopt a simple modelin which lexical and relation (structural) probabilitiesare assumed to be independent.
In this model the align-nlent relations are functions from the word occurrence~lodes of Ct to the word occurrences of C~.
The ideais that .t(,j) = wi means that the source word occur-r('ncc wi 'gave rise' to the target word occurrence vj.
'l'lw inverse relation .t-1 need not be a function, allow-ing different numbers of words in the source and targetsentences.We decompose P(C~,.tIC,) into 'lexical' and 'struc-tural' probabilities as follows:I'(Ct, f ie,)  = P(N,, IIN,)P(EtINt, .t, C,)where Nt and N, are the node sets for Ct and C0 respec-tiw.ly, and Et is the set of edges for the target graph.The lirst factor P(Nt, .fiN,) is the lexical componentit~ ~.hat i does not take into account any of the relationsin I.he source graph C.,.
This lexical component is thepro,luct of alignment probabilities for each node of N,:PCN,, fiN, ) =H wiEN.
"?
}lwd.That is, the probability that .I' maps exactly the (possi-bly empty) subset {vi*... v~} of Nt to wi.
These sets areassumed to be disjoint for different source graph nodes,so we can replace the factors in the above product withparameters:P(MIw)where w is a source language word and M is a multisetof target language words.We will derive a target set of edges Et of Ct by kderivation steps which partition the set of source edgesE, into subgraphs St ...  Sk.
These subgraphs give riseto disjoint sets of relation edges T1 ... Tk which togetherform Et.
The structural component of our translationmodel will be the sum of derivation probabilities forsuch an edge set Et.For simplicity, we assume here that the source graphC, is a tree.
This is consistent with our earlier assump-tions about the source language model.
We take ourpartitions of the source graph to be the edge sets forlocal trees.
This ensures that the the partitioning isdeterministic so the probability of a derivation is theproduct of the probabilities of derivation steps.
Morecomplex models with larger partitions rooted at a nodeare possible but these require additional parameters forpartitioning.
For the simple model it remains to specifyderivation step probabilities.The probability of a derivation step is given by pa-rameters of the form:P(T, qS,', .tdwhere S~ and T\[ are unlabeled graphs and ffi is a nodealignment function from T\[ to S~.
Unlabeled graphsare just like our relation edge graphs except that thenodes are not labeled with words (the edges still haverelation labels).
To apply a derivation step we need anotion of graph matching that respects edge labels: g isan isomorphism (modulo node labels) from a graph Gto a graph H if g is a one-one and onto function fromthe nodes of G to the nodes of H such thatr(a, b) e V iff r(g(a), g(b)) ?
H.The derivation step with parameter P(T\[IS~,f~ ) isapplicable to the source edges St, under the alignmentf, giving rise to the target edges Ti if (i) there is an iso-morphism hi from S~ to Si (ii) there is an isomorphismgi from ~ to T~' (iii) for any node v of Ti it must be thecase thathi(fi(gi(v))) -- f(v).This last condition ensures that the target graph parti-tions join up in a way that is compatible with the nodealignrn,:nt f,Tile factoring of the translation model into theselexical and structural components means that it willovergenerate because these aspects are not indepen-dent in translation between real natural anguages.
It8is therefore appropriate to filter translation hypothesesby re.scoring according to the version of the overall sta-tistical model that included the factors P(Ct)P(ColCt)so that the target language model constrains the out-put of the translation model.
Of course, in this case weneed to model the translation relation in the 'reverse'direction.
This can be done in a parallel fashion to theforward direction described above.7.
ConclusionsOur qualitative and quantitative models have a similaroverall structure and there are clear parallels betweenthe factoring of logical constraints and statistical pa-rameters, for example monolingual postulates and de-pendency parameters, bilingual postulates and trans-lation parameters.
The parallelism would have beencloser if we had adopted ID/LP style rules (Gazdar etal.
1985) in the qualitative model.
However, we arguedin section 3 that our qualitative model suffered fromlack of robustness, from having only the crudest meansfor choosing between competing hypotheses, and frombeing computationally intractable for large vocabular-ies.The quantitative model is in a much better positionto cope with these problems.
It is less brittle becausestatistical associations have replaced constraints (feat-ural, selectional, etc.)
that must be satisfied exactly.The probabilistic models give us a systematic and wellmotivated way of ranking alternative hypotheses.
Com-putationally, the quantitative model lets us escape fromthe undecidability of logic-based reasoning.
Becausethis model is highly lexical, we can hope that the in-put words will allow effective pruning by limiting thenumber of search paths having significantly high prob-abilities.We retained some of the basic assumptions about thestructure of language when moving to the quantitativemodel.
In particular, we preserved the notion of hierar-chical phrase structure.
Relations motivated by depen-dency grammar made it possible to do this without giv-ing up sensitivity to lexical collocations which underpinsimple statistical models like N-grams.
The quantita-tive model also reduced overall complexity in terms ofthe sets of symbols used.
In addition to words, it onlyrequired symbols for dependency relations, whereas thequalitative model required symbol sets for linguisticcategories and features, and a set of word sense sym-bols.
Despite their apparent importance to translation,the quantitative system can avoid the use of word sensesymbols (and the problems of granularity they give riseto) by exploiting statistical associations between wordsin the target language to filter implicit sense choices.Finally, here is a summary of our reasons for com-bining statistical methods with dependency representa-tions in our language and translation models:?
inherent lexical sensitivity of dependency representa-tions, facilitating parameter estimation;* quantitative preference based on probabilistic deriva-tion and translation;?
incremental nd/or partial speeilication of tlw ~',~tl-tent of utterances, particularly useful in I, ranslatiou;?
decomposition f complex utterances through rccur-sive linguistic structure.These factors suggest hat dependency grammar willplay an increasingly important role as language pro-cessing systems eek to combine both structural andcolloeational information.AcknowledgementsI am grateful to Fernando Pereira, Mike Riley, and hloDagan for valuable discussions on the issues addressedin this paper.
Fernando Pereira and !do Dagan alsoprovided helpful comments on a draft of the paper.Re ferencesAlshawi, H., D. Carter, B. Gamback and M. Rayner.1992.
"Swedish-English QLF Translation".
In H. AI-shawl (ed.)
The Core Language Engine, Cambridge,Mass.
: MIT Press.Alshawi, H. and R. Crouch.
1992.
"Monotonic Seman-tic Interpretation".
Proceedings of the 30th AnnualMeeting of the Association for Computational Lin-guistics, Newark, Delaware.Alshawi, H. and D. Carter.
1994.
"Training and Seal-ing Preference Functions for Disambiguation".
Toappear in Computational Linguistics.Brill, E. 1993.
"Automatic Grammar Induction andParsing Free Text: A Transformation-Based Ap-proach".Proceedings of the 31st Annual Meeting ofthe Association for Computational Linguistics, 259265.Brown, P., J. Cocke, S. Della Pietra, V. Della Pietra, F.Jelinek, J. Lafferty, R. Mercer and P. Rossin.
1990.
"A Statistical Approach to Machine TranslatioJl".Computational Linguistics 16:79-85.Chang, J., Y. Lua, and K. Su.
1992.
"GPSM: A Gen-eralized Probabilistic Semantic Model for AmbiguityResolution".
Proceedings of the 30th Annual Meet-ing of the Association for Computational Linguistics,177-192.Chang, J., K. Su.
1993.
"A Corpus-Based Statistics-Oriented Transfer and Generation Model for MachineTranslation".
Proceedings of the 5th InternationalConference on Theoretical and Methodological Issuesin Machine Translation.Dagan I. and A. Itai.
1994.
"Word Sense Disambigua-tion Using a Second Language Monolingual Corpus".To appear in Computational Linguistics.9l)agan, 1., S. Marcus and S. Markoviteh.
1993.
"Con-textual Word Similarity and Estimation from SparseData".
Proceedings ofthe 31st meeting of the Associ-atio~ for Computational Linouistics, ACL, 164-171.
(;azdar, G., E. Klein, G.K. Pullum, and I.A.Sag.
1985.Generalised Phrase Structure Grammar.
Oxford:Blackwell.Ilindle, D. and M. Rooth.
1993.
"Structural Ambiguityand Lexical Relations".
Computational Linguistics19:103-120.Ilobbs, J.R., M. Stickel, P. Martin and D. Edwards.1988.
"Interpretation asAbduction", Proceedings ofthe 26th Annual Meeting of the Association for Com-putational Linguistics, Buffalo, New York, 95-103.Itudson, R.A. 1984.
Word Grammar.
Oxford: Black-woll.Isah~,llo, P. and E. Macklovitch.
1986.
"Transfer andMT Modularity", Eleventh International Conferenceon Computational Linguistics, Bonn, 115-117..Iclinek, F., R.L.
Mercer and S. Roukos.
1992.
"Princi-ples of Lexieal Language Modeling for Speech Recog-nition".
In S. Furui and M.M.
Sondhi (eds.
), Ad-vances in Speech Signal Processing, New York: Mar-col Dekker Inc.M,:llish, C.S.
1988.
"Implementing Systemic Classifi-cation by Unification".
Computational Linguistics14:40-51.McCord, M. 1988.
"A Multi-Target Machine Trans-lation System".
Proceedings of the InternationalConference on Fifth Generation Computer Systems,'\[bkyo, Japan, 1141-1149.I'ereira, F., N. Tishby and L. Lee.
1993.
"Distri-butional Clustering of English Words".
Proceedingsof the 31st meeting of the Association for Computa-tzonal Linguistics, ACL, 183-190.I'oll~trd, C.J.
and I.A.
Sag.
1987.
Information Based,';yntax and Semantics: Volume I ~ Fundamentals.CSI,I Lecture Notes, Number 13.
Center for theStudy of Language and Information, Stanford, Cali-fornia.llayner, M. and H. Alshawi.
1992.
"Deriving DatabaseQueries from Logical Forms by Abductive DefinitionExpansion".
Proceedings of the Third Conference onApplied Natural Language Processing, Trento, Italy.i{ichard, M.D.
and R.P.
Lippmann.
1991.
"Neural Net-work Classiliers Estimate Bayesian a posteriori Prob-;d,ilili~.s".
Neural Computation 3:461-483.Shi,.l~,,r, S.M.
1986.
An Introduction to Unification-Ilascd Approaches to Grammar.
CSLI Lecture Notes,Number 4.
Center for the Study of Language andI i~ form ation, Stanford, California.Smajda, F. and K. McKeown.
1990.
"AutomaticallyExtracting and Representing Collocations for Lan-guage Generation".
In Proceedings of the $Sth An-nual Meeting of the Association for ComputationalLinguistics, Pittsburgh.Taylor, L., C. Grover, sad E.J.
Briscoe.
1989.
"TheSyntactic Regularity of English Noun Phrases".
Pro-ceeedings of the 4th European ACL Conference, 256-263.Weaver, W. 1955.
"Translation".
In W. Locke andA.
Booth (eds.
), Machine Translation of Languages,Cambridge, Mass.
: MIT Press.10
