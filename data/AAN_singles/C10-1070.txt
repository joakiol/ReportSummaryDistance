Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 617?625,Beijing, August 2010Revisiting Context-based Projection Methods forTerm-Translation Spotting in Comparable CorporaAudrey LarocheOLST ?
De?p.
de linguistique et de traductionUniversite?
de Montre?alaudrey.laroche@umontreal.caPhilippe LanglaisRALI ?
DIROUniversite?
de Montre?alfelipe@iro.umontreal.caAbstractContext-based projection methods foridentifying the translation of terms incomparable corpora has attracted a lot ofattention in the community, e.g.
(Fung,1998; Rapp, 1999).
Surprisingly, none ofthose works have systematically investi-gated the impact of the many parameterscontrolling their approach.
The presentstudy aims at doing just this.
As a test-case, we address the task of translatingterms of the medical domain by exploit-ing pages mined from Wikipedia.
One in-teresting outcome of this study is that sig-nificant gains can be obtained by using anassociation measure that is rarely used inpractice.1 IntroductionIdentifying translations of terms in comparablecorpora is a challenge that has attracted many re-searchers.
A popular idea that emerged for solv-ing this problem is based on the assumption thatthe context of a term and its translation share sim-ilarities that can be used to rank translation candi-dates (Fung, 1998; Rapp, 1999).
Many variants ofthis idea have been implemented.While a few studies have investigated patternmatching approaches to compare source and tar-get contexts (Fung, 1995; Diab and Finch, 2000;Yu and Tsujii, 2009), most variants make use ofa bilingual lexicon in order to translate the wordsof the context of a term (often called seed words).De?jean et al (2005) instead use a bilingual the-saurus for translating these.Another distinction between approaches lies inthe way the context is defined.
The most com-mon practice, the so-called window-based ap-proach, defines the context words as those cooc-curing significantly with the source term withinwindows centered around the term.1 Some studieshave reported gains by considering syntacticallymotivated co-occurrences.
Yu and Tsujii (2009)propose a resource-intensive strategy which re-quires both source and target dependency parsers,while Otero (2007) investigates a lighter approachwhere a few hand coded regular expressions basedon POS tags simulate source parsing.
The latterapproach only requires a POS tagger of the sourceand the target languages as well as a small par-allel corpus in order to project the source regularexpressions.Naturally, studies differ in the way each co-occurrence (either window or syntax-based) isweighted, and a plethora of association scoreshave been investigated and compared, the like-lihood score (Dunning, 1993) being among themost popular.
Also, different similarity measureshave been proposed for ranking target context vec-tors, among which the popular cosine measure.The goal of the different authors who inves-tigate context-projection approaches also varies.Some studies are tackling the problem of iden-tifying the translation of general words (Rapp,1999; Otero, 2007; Yu and Tsujii, 2009) whileothers are addressing the translation of domainspecific terms.
Among the latter, many are trans-lating single-word terms (Chiao and Zweigen-baum, 2002; De?jean et al, 2005; Prochasson et1A stoplist is typically used in order to prevent functionwords from populating the context vectors.617al., 2009), while others are tackling the translationof multi-word terms (Daille and Morin, 2005).The type of discourse might as well be of con-cern in some of the studies dedicated to bilingualterminology mining.
For instance, Morin et al(2007) distinguish popular science versus scien-tific terms, while Saralegi et al (2008) target pop-ular science terms only.The present discussion only focuses on a fewnumber of representative studies.
Still, it is al-ready striking that a direct comparison of themis difficult, if not impossible.
Differences in re-sources being used (in quantities, in domains,etc.
), in technical choices made (similarity mea-sures, context vector computation, etc.)
and in ob-jectives (general versus terminological dictionaryextraction) prevent one from establishing a clearlandscape of the various approaches.Indeed, many studies provide some figures thathelp to appreciate the influence of some param-eters in a given experimental setting.
Notably,Otero (2008) studies no less than 7 similarity mea-sures for ranking context vectors while comparingwindow and syntax-based methods.
Morin et al(2007) consider both the log-likelihood and themutual information association scores as well asthe Jaccard and the cosine similarity measures.Ideally, a benchmark on which researcherscould run their translation finder would ease thecomparison of the different approaches.
However,designing such a benchmark that would satisfy theevaluation purposes of all the researchers is far tooambitious a goal for this contribution.
Instead, weinvestigate the impact of some major factors influ-encing projection-based approaches on a task oftranslating 5,000 terms of the medical domain (themost studied domain), making use of French andEnglish Wikipedia pages extracted monolinguallythanks to an information retrieval engine.
Whilethe present work does not investigate all the pa-rameters that could potentially impact results, webelieve it constitutes the most complete and sys-tematic comparison made so far with variants ofthe context-based projection approach.In the remainder of this paper, we describe theprojection-based approach to translation spottingin Section 2 and detail the parameters that directlyinfluence its performance.
The experimental pro-tocol we followed is described in Section 3 andwe analyze our results in Section 4.
We discussthe main results in the light of previous work andpropose some future avenues in Section 5.2 Projection-based variantsThe approach we investigate for identifying termtranslations in comparable corpora is similar to(Rapp, 1999) and many others.
We describe in thefollowing the different steps it encompasses andthe parameters we are considering in the light oftypical choices made in the literature.2.1 ApproachStep 1 A comparable corpus is constructed foreach term to translate.
In this study, the source andtarget corpora are sets of Wikipedia pages relatedto the source term (S) and its reference transla-tion (T ) respectively (see Section 3.1).
The degreeof corpus preprocessing varies greatly from onestudy to another.
Complex linguistic tools suchas terminological extractors (Daille and Morin,2005), parsers (Yu and Tsujii, 2009) or lemma-tizers (Rapp, 1999) are sometimes used.In our case, the only preprocessing that takesplace is the deletion of the Wikipedia symbols per-taining to its particular syntax (e.g.
[[ ]]).2 It isto be noted that, for the sake of simplicity and gen-erality, our implementation does not exploit inter-language links nor structural elements specific toWikipedia documents, as opposed to (Yu and Tsu-jii, 2009).Step 2 A context vector vs for the source termS is built (see Figure 1 for a made-up example).This vector contains the words that are in the con-text of the occurrences of S and are strongly cor-related to S. The definition of ?context?
is one ofthe parameters whose best value we want to find.Context length can be based on a number of units,for instance 3 sentences (Daille and Morin, 2005),windows of 3 (Rapp, 1999) or 25 words (Prochas-son et al, 2009), etc.
It is an important parame-ter of the projection-based approach.
Should thecontext length be too small, we would miss wordsthat would be relevant in finding the translation.On the other hand, if the context is too large, it2We used a set of about 40 regular expressions to do this.618might contain too much noise.
At this step, a sto-plist made of function words is applied in orderto filter out context words and reduce noise in thecontext vector.Additionally, an association measure is used toscore the strength of correlation between S andthe words in its contexts; it serves to normalizecorpus frequencies.
Words that have a high as-sociation score with S are more prominent in thecontext vector.
The association measure is the sec-ond important parameter we want to study.
As al-ready noted, most authors use the log-likelihoodratio to measure the association between collo-cates; some, like (Rapp, 1999), informally com-pare the performance of a small number of associ-ation measures, or combine the results obtainedwith different association measures (Daille andMorin, 2005).Figure 1: Step 2Step 3 Words in vs are projected into the targetlanguage with the help of the bilingual seed lexi-con (Figure 2).
Each word in vs which is presentin the bilingual lexicon is translated, and thosetranslations define the projected context vector vp.Words that are not found in the bilingual lexiconare simply ignored.
The size of the seed lexi-con and its content are therefore two importantparameters of the approach.
In previous studies,seed lexicons vary between 16,000 (Rapp, 1999)and 65,000 (De?jean et al, 2005) entries, a typicalsize being around 20,000 (Fung, 1998; Chiao andZweigenbaum, 2002; Daille and Morin, 2005).Figure 2: Step 3Step 4 Context vectors vt are computed for eachcandidate term in the target language corpus (Fig-ure 3).
The dimension of the target-vector spaceis defined to be the one induced by the projec-tion mechanism described in Step 3.
The con-text vector vt of each candidate term is computedas in Step 2.
Therefore, in Step 4, the parame-ters of context definition and association measureare important and take the same values as thosein Step 2.
Note that in this study, on top of allsingle terms, we also consider target bigrams aspotential candidates (99.5 % of our reference tar-get terms are composed of at most two words).As such, our method can handle complex terms(of up to two words), as opposed to most previ-ous studies, without having to resort to a separateterminological extraction as in (Daille and Morin,2005).Figure 3: Step 4Step 5 Context vectors vt are ranked in decreas-ing order of their similarity with vp (Figure 4).The similarity measure between context vectorsvaries among studies: city-block measure (Rapp,1999), cosine (Fung, 1998; Chiao and Zweigen-baum, 2002; Daille and Morin, 2005; Prochassonet al, 2009), Dice or Jaccard indexes (Chiao andZweigenbaum, 2002; Daille and Morin, 2005),etc.
It is among the parameters whose effect weexperimentally evaluate.Figure 4: Step 52.2 Parameters studiedThe five steps we described involve many param-eters, the values of which can influence at varyingdegrees the performance of a translation spotter.In the current study, we considered the followingparameter values.Context We considered contexts defined as thecurrent sentence or the current paragraph involv-619ing S. We also considered windows of 5 and 25words on both sides of S.Association measure Following the aforemen-tioned studies, we implemented these popularmeasures: pointwise mutual information (PMI),log-likelihood ratio (LL) and chi-square (?2).
Wealso implemented the discounted log-odds (LO)described by (Evert, 2005, p. 86) in his work oncollocation mining.
To our knowledge, this asso-ciation measure has not been used yet in transla-tion spotting.
It is computed as:odds-ratiodisc = log (O11 +12)(O22 + 12)(O12 + 12)(O21 + 12)where Oij are the cells of the 2?2 contingencymatrix of a word token s cooccurring with theterm S within a given window size.3Similarity measure We implemented four mea-sures: city-block, cosine, as well as Dice and Jac-card indexes (Jurafsky and Martin, 2008, p. 666).Our implementations of Dice and Jaccard areidentical to the DiceMin and JaccardMin similar-ity measures reported in (Otero, 2008) and whichoutperformed the other five metrics he tested.Seed lexicon We investigated the impact of boththe size of the lexicon and its content.
We startedour study with a mixed lexicon of around 5,000word entries: roughly 2,000 of them belong tothe medical domain, while the other entries be-long to the general language.
We also consideredmixed lexicons of 7,000, 9,000 and 11,000 entries(where 2,000 entries are related to the medical do-main), as well as a 5,000-entry general languageonly lexicon.2.3 Cognate heuristicMany authors are embedding heuristics in orderto improve their approach.
For instance, Chiaoand Zweigenbaum (2002) propose to integrate areverse translation spotting strategy in order to im-prove precision.
Prochasson et al (2009) boostthe strength of context words that happen to betransliterated in the other language.
A somehow3For instance, O21 stands for the number of windowscontaining S but not s.generalized version of this heuristic has been de-scribed in (Shao and Ng, 2004).In this work, we examine the performanceof the best configuration of parameters wefound, combined with a simple heuristic basedon graphic similarity between source and tar-get terms, similar to the orthographic features in(Haghighi et al, 2008)?s generative model.
Thisis very specific to our task where medical termsoften (but not always) share Latin or Greek roots,such as microvillosite?s in French and microvilli inEnglish.In this heuristic, translation candidates whichare cognates of the source term are ranked firstamong the list of translation candidates.
In ourimplementation, two words are cognates if theirfirst four characters are identical (Simard et al,1992).
One interesting note concerns the word-order mismatch typically observed in French andEnglish complex terms, such as in ADN mitochon-drial (French) and mitochondrial DNA (English).We do treat this case adequately.3 Experimental protocolIn order to pinpoint the best configuration of val-ues for the parameters identified in Section 2.2,four series of experiments were carried out.
Inall of them, the task consists of spotting transla-tion candidates for each source language term us-ing the resources4 described below.
The quality ofthe results is evaluated with the help of the metricsdescribed in Section 3.2.3.1 ResourcesCorpora The comparable corpora are made ofthe (at most) 50 French and English Wikipediadocuments that are the most relevant to the sourceterm and to its reference translation respectively.These documents are retrieved with the NLGbAseInformation Retrieval tool.5 The average tokencount of all the 50-document corpora as well asthe average frequency of the source and targetterms in these corpora for our four series of ex-periments are listed in Table 1.4Our resources are available at http://olst.ling.umontreal.ca/?audrey/coling2010/.
They wereacquired as described in (Rubino, 2009).5http://nlgbase.org/620Experiment1 2 3 4Tokenss 89,431 73,809 42,762 90,328Tokenst 52,002 27,517 12,891 38,929|S| 296 184 66 306|T | 542 255 104 404Table 1: 50-document corpora averagesThe corpora are somewhat small (most corporain previous studies are made of at least a millionwords).
We believe this is more representative ofa task where we try to translate domain specificterms.
Some of the Wikipedia documents maycontain a handful of parallel sentences (Smith etal., 2010), but this information is not used in ourapproach.
The construction of the corpus involvesa bias in that the reference translations are usedto obtain the most relevant target language docu-ments.
However, since our objective is to com-pare the relative performance of different sets ofparameters, this does not affect our results.
Infact, as per (De?jean et al, 2005) (whose compa-rable corpora are English and German abstracts),the use of such an ?ideal?
corpus is common (as in(Chiao and Zweigenbaum, 2002), where the cor-pus is built from a specific query).Seed lexicon The mixed seed lexicon we use istaken from the Heymans Institute of Pharmacol-ogy?s Multilingual glossary of technical and pop-ular medical terms.6 Random general languageentries from the FreeLang7 project are also in-corporated into the lexicon for some of our exper-iments.Reference translations The test set is com-posed of 5,000 nominal single and multi-wordpairs of French and English terms from the MeSH(Medical Subject Heading) thesaurus.83.2 Evaluation metricsThe performance of each set of parameters in theexperiments is evaluated with Top N precision(PN ), recall (RN ) and F-measure (FN ), as wellas Mean Average Precision (MAP).
Precision is6http://users.ugent.be/?rvdstich/eugloss/welcome.html7http://www.freelang.net/8http://www.nlm.nih.gov/mesh/the number of correct translations (at most 1 persource term) divided by the number of terms forwhich our system gave at least one answer; recallis equal to the ratio of correct translations to thetotal number of terms.
F-measure is the harmonicmean of precision and recall:F-measure = 2?
(precision?
recall)(precision+ recall)The MAP represents in a single figure the qual-ity of a system according to various recall levels(Manning et al, 2008, p. 147?148):MAP(Q) = 1|Q|j=1?|Q|1mjk=1?mjPrecision(Rjk)where |Q| is the number of terms to be trans-lated, mj is the number of reference translationsfor the jth term (always 1 in our case), andPrecision(Rjk) is 0 if the reference translationis not found for the jth term or 1/r if it is (r is therank of the reference translation in the translationcandidates).4 ExperimentsIn Experiment 1, 500 single and multi-word termsmust be translated from French to English usingeach of the 64 possible configurations of these pa-rameters: context definition, association measureand similarity measure.
In Experiment 2, we sub-mit to the 8 best variants 1,500 new terms to de-termine with greater confidence the best 2, whichare again tested on the last 3,000 of the test terms(Experiment 3).
In Experiment 4, using 1,350 fre-quent terms, we examine the effects of seed lex-icon size and specificity and we apply a heuristicbased on cognates.4.1 Experiment 1The results of the first series of experiments on500 terms can be analysed from the point of viewof each of the parameters whose values variedamong 64 configurations (Section 2.2).
The max-imal MAP reached for each parametric value isgiven in Table 2.The most notable result is that, of the four as-sociation measures studied, the log-odds ratio is621Param.
Value Best MAP In config.association LO 0.536 sentence cosineLL 0.413 sentence DicePMI 0.299 sentence city-block?2 0.179 sentence Dicesimilarity cosine 0.536 sentence LODice 0.520 sentence LOJaccard 0.520 sentence LOcity-block 0.415 sentence LOcontext sentence 0.536 cosine LOparagraph 0.460 cosine LO25 words 0.454 cosine LO5 words 0.361 Dice LOTable 2: Best MAP in Experiment 1significantly superior to the others in every vari-ant.
There is as much as 34 % difference be-tween LO and other measures for Top 1 recall.This is interesting since most previous works usethe log-likelihood, and none use LO.
Our best re-sults for LO (with cosine sentence) and LL (withDice sentence) are in Table 3.
Note that the oraclerecall is 93 % (7 % of the source and target termswere not in the corpus).Assoc.
R1 R20 P1 P20 F1 F20 MAPLO 39.4 84.8 42.3 91.0 40.8 87.8 0.536LL 29.0 75.2 31.3 81.0 30.1 78.0 0.413Table 3: Best LO and LL configurations scoresAnother relevant observation is that the param-eters interact with each other.
When the similar-ity measure is cosine, PMI results in higher Top 1F-scores than LL, but the Top 20 F-scores are bet-ter with LL.
PMI is better than LL when usingcity-block as a similarity measure, but LL is betterthan PMI when using Dice and Jaccard indexes.
?2 gives off the worst MAP in all but 4 of the 64parametric configurations.As for similarity measures, the Dice and Jac-card indexes have identical performances, in ac-cordance with the fact that they are equivalent(Otero, 2008).9 Influences among parameters arealso observable in the performance of similaritymeasures.
When the association measure is LO,the cosine measure gives slightly better Top 1 F-9For this reason, whenever ?Dice?
is mentioned from thispoint on, it also applies to the Jaccard index.scores, while the Dice index performs slightly bet-ter with regards to Top 20 F-scores.
Dice is betterwhen the association measure is LL, with a Top 1F-score gain of about 15 % compared to the co-sine.Again, in the case of context definitions, rel-ative performances depend on the other param-eters and on the number of top translation can-didates considered.
With LO, sentence contextshave the highest Top 1 F-measures, while Top 20F-measures are highest with paragraphs, and 5-word contexts are the worst.4.2 Experiment 2The best parametric values found in Experiment 1were put to the test on 1,500 different test termsfor scale-up verification.
Along with LO, whichwas the best association measure in the previousexperiment, we used LL to double-check its rel-ative inefficiency.
For all of the 8 configurationsevaluated, LL?s recall, precision and MAP remainworse than LO?s.
In particular, LO?s MAP scoreswith the cosine measure are more than twice ashigh as LL?s (respectively 0.33 and 0.124 for sen-tence contexts).
As in Experiment 1, the Diceindex is significantly better for LL compared tothe cosine, but not for LO.
In the case of LO,sentence contexts have better Top 1 performancesthan paragraphs, and vice versa for Top 20 per-formances (see Table 4; oracle recall is 93.5 %).Hence, paragraph contexts would be more usefulin tasks consisting of proposing candidate transla-tions to lexicographers, while sentences would bemore appropriate for automatic bilingual lexiconconstruction.Ctx R1 R20 P1 P20 F1 F20 MAPSent.
23.1 63.9 27.8 76.6 25.23 69.68 0.336Parag.
20.1 70.0 22.9 79.7 21.41 74.54 0.325Table 4: LO Dice configuration scoresThe cosine and Dice similarity measures havesimilar performances when LO is used.
Moreover,we observe the effect of source and target termfrequencies in corpus.
As seen in Table 1, thesefrequencies are on average about half smaller inExperiment 2 as they are in Experiment 1, whichresults in significantly lower performances for all6228 variants.
As Figure 5 shows for the variantLO cosine sentence, terms that are more frequenthave a greater chance of being correctly translatedat better ranks.Figure 5: Average rank of correct translationaccording to average source term frequencyHowever, the relative performance of the differ-ent parametric configurations still holds.4.3 Experiment 3In Experiment 3, we evaluate the two best config-urations from Experiment 2 with 3,000 new termsin order to verify the relative performance of thecosine and Dice similarity measures.
As Table 5shows, cosine has slightly better Top 1 figures,while Dice is a little better when considering theTop 20 translation candidates.
Therefore, as pre-viously mentioned, the choice of similarity mea-sure (cosine or Dice) should depend on the goalof translation spotting.
Note that the scores in Ex-periment 3 are much lower than those of Experi-ments 1 and 2 because of low term frequencies inthe corpus (see Table 1 and Figure 5).
Also, oraclerecall is only 71.1 %.Sim.
R1 R20 P1 P20 F1 F20 MAPCosine 9.8 28.1 20.7 59.4 13.3 38.15 0.232Dice 9.4 28.9 19.8 61.2 12.75 39.26 0.286Table 5: LO sentence configuration scores4.4 Experiment 4In the last series of experiments, we examine theinfluence of the bilingual seed lexicon specificityand size, using the 1,350 terms which have sourceand target frequencies ?
30 from the 1,500 and3,000 sets used in Experiments 2 and 3 (oracle re-call: 100 %).
We tested the different lexicons (seeSection 2.2) on the 4 parametric configurationsmade of sentence contexts, LO or LL associationmeasures, and cosine or Dice similarity measures.Yet again, LO is better than LL.
MAP scores forLO in all variants are comprised in [0.466?0.489];LL MAPs vary between 0.135 and 0.146 when thecosine is used and between 0.348 and 0.380 whenthe Dice index is used.According to our results, translation spottingis more accurate when the seed lexicon contains(5,000) entries from both the medical domainand general language instead of general languagewords only, but only by a very small margin.Table 6 shows the results for the configurationLO cosine sentence.
The fact that the differenceLex.
R1 R20 P1 P20 F1 F20 MAPGen.
+ med.
39.3 87.0 39.6 87.6 39.4 87.3 0.473Gen.
only 38.8 88.1 39.0 88.5 38.9 88.3 0.471Table 6: LO cosine sentence configuration scoresis so small could be explained by our resources?properties.
The reference translations from MeSHcontain terms that are also used in other domainsor in the general language, e.g.
terms from thecategory ?people?
(Ne?ve?ol and Ozdowska, 2006).Wikipedia documents retrieved by using those ref-erences may in turn not belong to the medical do-main, in which case medical terms from the seedlexicon are not appropriate.
Still, the relativelygood performance of the general language-onlylexicon supports (De?jean et al, 2005, p. 119)?sclaim that general language words are useful whenspotting translations of domain specific terms,since the latter can appear in generic contexts.Lexicon sizes tested are 5,000 (the mixed lex-icon used in previous experiments), 7,000, 9,000and 11,000 entries.
The performance (based onMAP) is better when 7,000- and 9,000-entry lexi-cons are used, because more source language con-text words can be taken into account.
However,when the lexicon reaches 11,000, Top 1 MAPscores and F-measures are slightly lower thanthose obtained with the 7,000-entry one.
This mayhappen because the lexicon is increased with gen-eral language words; 9,000 of the 11,000 entries623are not from the medical domain, making it harderfor the context words to be specific.
It would beinteresting to study the specificity of context vec-tors built from the source corpus.
Still, the dif-ferences in scores are small, as Table 7 shows(see Table 6 for the results obtained with 5,000entries).
This is because, in our implementation,context vector size is limited to 20, as in (Dailleand Morin, 2005), in order to reduce processingtime.
The influence of context vector sizes shouldbe studied.Lex.
size R1 R20 P1 P20 F1 F20 MAP7,000 41.5 88.8 41.6 89.1 41.5 88.9 0.4889,000 40.9 89.3 41.1 89.7 41.0 89.5 0.48911,000 40.1 89.8 40.2 90.1 40.1 89.9 0.484Table 7: LO cosine sentence configuration scoresThe parameters related to the seed lexicon donot have as great an impact on the performanceas the choice of association measure does: thebiggest difference in F-measures for Experiment 4is 2.9 %.
At this point, linguistic-based heuris-tics such as graphic similarity should be usedto significantly increase performance.
We ap-plied the cognate heuristic (Section 2.3) on theTop 20 translation candidates given by the vari-ant LO sentence 9,000-entry lexicon using cosineand Dice similarity measures.
Without the heuris-tic, Top 1 performances are better with cosine,while Dice is better for Top 20.
Applying the cog-nate heuristic makes the Top 1 precision go from41.1 % to 55.2 % in the case of cosine, and from39.6 % to 53.9 % in the case of Dice.5 DiscussionOur results show that using the log-odds ratio asthe association measure allows for significantlybetter translation spotting than the log-likelihood.A closer look at the translation candidates ob-tained when using LL, the most popular asso-ciation measure in projection-based approaches,shows that they are often collocates of the refer-ence translation.
Therefore, LL may fare better inan indirect approach, like the one in (Daille andMorin, 2005).Moreover, we have seen that the cosine simi-larity measure and sentence contexts give morecorrect top translation candidates, at least whenLO is used.
Indeed, the values of the differentparameters influence one another in most cases.Parameters related to the seed lexicon (size, do-main specificity) are not of great influence on theperformance, but this may in part be due to ourresources and the way they were built.The highest Top 1 precision, 55.2 %, wasreached with the following parameters: sentencecontexts, LO, cosine and a 9,000-entry mixed lex-icon, with the use of a cognate heuristic.In future works, other parameters which in-fluence the performance will be studied, amongwhich the use of a terminological extractor to treatcomplex terms (Daille and Morin, 2005), morecontextual window configurations, and the use ofsyntactic information in combination with lexicalinformation (Yu and Tsujii, 2009).
It would alsobe interesting to compare the projection-basedapproaches to (Haghighi et al, 2008)?s genera-tive model for bilingual lexicon acquisition frommonolingual corpora.One latent outcome of this work is thatWikipedia is surprisingly suitable for mining med-ical terms.
We plan to check its adequacy forother domains and verify that LO remains a bet-ter association measure for different corpora anddomains.AcknowledgmentsWe are deeply grateful to Raphae?l Rubino whoprovided us with the data material we have beenusing in this study.
We thank the anonymous re-viewers for their suggestions.ReferencesChiao, Yun-Chuang and Pierre Zweigenbaum.
2002.Looking for candidate translational equivalents inspecialized, comparable corpora.
In 19th Inter-national Conference on Computational Linguistics,pages 1208?1212.Daille, Be?atrice and Emmanuel Morin.
2005.
French-English terminology extraction from comparablecorpora.
In 2nd International Joint Conference onNatural Language Processing, pages 707?718.De?jean, Herve?, E?ric Gaussier, Jean-Michel Renders,and Fatiha Sadat.
2005.
Automatic processing of624multilingual medical terminology: Applications tothesaurus enrichment and cross-language informa-tion retrieval.
Artificial Intelligence in Medicine,33(2):111?124.
Elsevier Science, New York.Diab, Mona and Steve Finch.
2000.
A statistical word-level translation model for comparable corpora.
InProceedings of the Conference on Content-BasedMultimedia Information Access.Dunning, Ted.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1):61?74.Evert, Stefan.
2005.
The Statistics of Word Cooccur-rences.
Word Pairs and Collocations.
Ph.D. thesis,Universita?t Stuttgart.Fung, Pascale.
1995.
A pattern matching methodfor finding noun and proper noun translations fromnoisy parallel corpora.
In 33rd Annual Meetingof the Association for Computational Linguistics,pages 236?243.Fung, Pascale.
1998.
A statistical view on bilinguallexicon extraction: From parallel corpora to non-parallel corpora.
In 3rd Conference of the Associa-tion for Machine Translation in the Americas, pages1?17.Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Human LanguageTechnology and Association for Computational Lin-guistics, pages 771?779.Jurafsky, Daniel and James H. Martin.
2008.
Speechand Language Processing.
Prentice-Hall.Manning, Christopher D., Prabhakar Raghavan, andHinrich Schu?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press.Morin, Emmanuel, Be?atrice Daille, Koichi Takeuchi,and Kyo Kageura.
2007.
Bilingual terminologymining ?
using brain, not brawn comparable cor-pora.
In 45th Annual Meeting of the Association forComputational Linguistics, pages 664?671.Ne?ve?ol, Aure?lie and Sylwia Ozdowska.
2006.
Termi-nologie me?dicale bilingue anglais/franc?ais: usagescliniques et bilingues.
Glottopol, 8.Otero, Pablo Gamallo.
2007.
Learning bilingual lexi-cons from comparable English and Spanish corpora.In Machine Translation Summit 2007, pages 191?198.Otero, Pablo Gamallo.
2008.
Evaluating two differentmethods for the task of extracting bilingual lexiconsfrom comparable corpora.
In 1st Workshop Buildingand Using Comparable Corpora.Prochasson, Emmanuel, Emmanuel Morin, and KyoKageura.
2009.
Anchor points for bilingual lexi-con extraction from small comparable corpora.
InMachine Translation Summit XII, pages 284?291.Rapp, Reinhard.
1999.
Automatic identification ofword translations from unrelated English and Ger-man corpora.
In 37th Annual Meeting of the Associ-ation for Computational Linguistics, pages 66?70.Rubino, Raphae?l.
2009.
Exploring context variationand lexicon coverage in projection-based approachfor term translation.
In Proceedings of the Stu-dent Research Workshop associated with RANLP?09, pages 66?70.Saralegi, X., I. San Vicente, and A. Gurrutxaga.
2008.Automatic extraction of bilingual terms from com-parable corpora in a popular science domain.
In1st Workshop Building and Using Comparable Cor-pora.Shao, Li and Hwee Tou Ng.
2004.
Mining new wordtranslations from comparable corpora.
In 20th Inter-national Conference on Computational Linguistics,pages 618?624.Simard, Michel, George Foster, and Pierre Isabelle.1992.
Using cognates to align sentences in bilin-gual corpora.
In 4th Conference on Theoreticaland Methodological Issues in Machine Translation,pages 67?81.Smith, Jason R., Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences from compa-rable corpora using document level alignment.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of theACL, pages 403?411.Yu, Kun and Junichi Tsujii.
2009.
Bilingual dictionaryextraction from Wikipedia.
In Machine TranslationSummit XII.625
