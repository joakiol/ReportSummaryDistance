Proceedings of the SIGDIAL 2014 Conference, pages 51?59,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsProbabilistic Human-Computer Trust HandlingFlorian Nothdurft?, Felix Richter?and Wolfgang Minker?
?Institute of Communications Engineering?Institute of Artificial IntelligenceUlm UniversityUlm, Germanyflorian.nothdurft, felix.richter, wolfgang.minker@uni-ulm.deAbstractHuman-computer trust has shown to be acritical factor in influencing the complex-ity and frequency of interaction in techni-cal systems.
Particularly incomprehensi-ble situations in human-computer interac-tion may lead to a reduced users trust in thesystem and by that influence the style ofinteraction.
Analogous to human-humaninteraction, explaining these situations canhelp to remedy negative effects.
In this pa-per we present our approach of augment-ing task-oriented dialogs with selected ex-planation dialogs to foster the human-computer trust relationship in those kindsof situations.
We have conducted a web-based study testing the effects of differentgoals of explanations on the componentsof human-computer trust.
Subsequently,we show how these results can be used inour probabilistic trust handling architec-ture to augment pre-defined task-orienteddialogs.1 IntroductionHuman-computer interaction (HCI) has evolved inthe past decades from classic stationary interactionparadigms featuring only human and computer to-wards intelligent agent-based paradigms featuringmultiple devices and sensors in intelligent envi-ronments.
For example, ubiquitous computing nolonger seems to be a vision of future HCI, but hasbecome reality, at least in research labs and pro-totypical environments.
Additionally, the tasks atechnical system has to solve cooperatively withthe user have become increasingly complex.
How-ever, this change from simple task solver to intel-ligent assistant requires the acceptance of and thetrust in the technical system as dialogue partnerand not only as ordinary service device.Especially trust has shown to be a crucial part inthe interaction between human and technical sys-tem.
If the user does not trust the system and itsactions, advices or instructions the way of interac-tion may change up to complete abortion of futureinteraction (Parasuraman and Riley, 1997).
Espe-cially those situations in which the user does notunderstand the system or does not expect the wayhow the system acts are critical to have a negativeimpact on the human-computer trust (HCT) rela-tionship (Muir, 1992).
Those situations do occurusually due to incongruent models of the system:During interaction the user builds a mental modelof the system and its underlying processes deter-mining system actions and output.
However, ifthis perceived mental model and the actual systemmodel do not match the HCT relationship may beinfluenced negatively (Muir, 1992).
This may, forexample, be due to a mismatch in the expected andthe actual system action and output.For example, if a technical system would assistthe user in having his day scheduled in a time ef-fective manner, the user would be in a vulnerablesituation of relying on the reasoning capabilities ofthe system.
However, when the user-expected timeschedule does not match the system-generated, thequestion arises if the user will trust the system, de-spite lacking the knowledge if the schedule is cor-rect.
If the user trusts the automated day schedul-ing capability of the system, he will probably at-tend the appointments exactly as scheduled.
How-ever, if he does not trust this automated outcomehe won?t rely on it and will question the plan.Therefore, the goal should be to detect thosecritical situations in HCI and to react appropri-ately.
If we take a look at how humans detectand handle critical situations, we can conclude thatthey use contextual information combined with in-terpreted multimodal body analysis (e.g., facialexpression, body posture, speech prosody) for de-tection and usually some sort of explanation to51Goals DetailsTransparency How was the systems answer reached?Justification Explain the motives of the answer?Relevance Why is the answer a relevant answer?Conceptualization Clarify the meaning of conceptsLearning Learn something about the domainTable 1: Goals of explanation after (S?rmo andCassens, 2004).
These goals subsume differentkinds of explanation as e.g., why, why-not, what-if, how-to explanationsclarify the process of reasoning (i.e.
increasingtransparency and understandability).
As even hu-mans are sometimes insecure about judging the di-alog partner and to decide whether and which typeof reaction would be appropriate, it seems validthat a technical system will not overcome this is-sue of uncertainty.
Therefore, we assume that thetransfer of this problem to a technical system canonly be handled effectively by incorporating un-certainty and thus using a probabilistic model.
Inthe remainder of this paper, we will first elaboratehow to react to not understandable situations andsecondly present how to incorporate these findingsinto a multimodal dialogue system using a proba-bilistic model.2 Coping with IncomprehensibleSituationsAnalogous to human-human interaction provid-ing explanations in not understandable situationsin HCI can reduce the loss of trust (Glass et al.,2008).
However, HCT is not a one-dimensionalsimple concept.
It may be devided into severalcomponents, which all have to be well-functioningto have the user trust a technical system.
Exis-tent studies concentrated on showing that explana-tions or different kinds of explanations can influ-ence HCT in general (Lim et al., 2009).
So, whatis lacking currently is which explanations do influ-ence which bases of human-computer trust.2.1 ExplanationsIn general, explanations are given to clarify,change or impart knowledge.
Usually the implicitidea consists of aligning the mental models of theparticipating parties.
The mental model is the per-ceived representation of the real world, or in ourcase of the technical system and its underlyingprocesses.
In this context explanations try to es-tablish a common ground between the parties inthe sense that the technical system tries to clar-ify its actual model to the user.
This is the at-tempt of aligning the user?s mental model to theactual system.
However, explanations do not al-ways have the goal of aligning mental models, butcan be used for other purposes as well.
Analogousto human-human interaction, in human-computerinteraction the sender of the explanation pursues acertain goal, with respect to the addressee, whichshould be achieved.
The question remains, howthese different goals of explanation (see table 1)map to HCT, meaning, how they influence HCTor components of it.2.2 Human-Computer TrustMayer et al.
(1995) define trust in human-humaninteraction to be ?the extent to which one party iswilling to depend on somebody or something, ina given situation with a feeling of relative secu-rity, even though negative consequences are pos-sible?.
For HCI trust can be defined as ?the atti-tude that an agent will help achieve an individual?sgoals in a situation characterized by uncertaintyand vulnerability?
(Lee and See, 2004).
Techni-cal Systems which serve as intelligent assistantswith the purpose of helping the user in complex aswell as in critical situations seem to be very de-pendent on an intact HCT relationship.
However,trust is multi-dimensional and consists of severalbases.
For human relationships, Mayer et al.
de-fined three levels that build the bases of trust: abil-ity, integrity and benevolence.
The same holds forHCI, where HCT is a composite of several bases.For human-computer trust Madsen and Gregor(2000) constructed a hierarchical model (see fig-ure 1) resulting in five basic constructs or so-calledbases of trust, which can be divided in two generalcomponents, namely cognitive-based and affect-based bases.
In short-term human-computer in-teraction, cognitive-based HCT components seemto be more important, because it will be easier toinfluence those.
Perceived understandability canbe seen in the sense that the human supervisoror observer can form a mental model and predictfuture system behavior.
The perceived reliabil-ity of the system, in the usual sense of repeated,consistent functioning.
And technical competencemeans that the system is perceived to perform thetasks accurately and correctly based on the inputinformation.
In this context it is important to men-tion, that as Mayer already stated, the bases oftrust are separable, yet related to one another.
Allbases must be perceived highly for the trustee to be52Figure 1: Human-computer trust model: Personalattachment and faith build the bases for affect-based trust.
Rerceived understandability, techni-cal competence and reliability for cognition-basedtrust.deemed trustworthy.
If any of the bases does notfulfill this requirement, the overall trustworthinesscan suffer (Madsen and Gregor, 2000).3 Related WorkPrevious work on handling trust in technical sys-tems was done for example by Glass et al.
(2008).They investigated factors that may change thelevel of trust users are willing to place in adaptiveagents.
Among these verified findings were state-ments like ?provide the user with the informationprovenance for sources used by the system?, ?in-telligently modulating the granularity of feedbackbased on context- and user-modeling?
or ?supplythe user with access to information about the in-ternal workings of the system?.
However, what ismissing in Glass et al.
?s work is the idea of rat-ing the different methods to uphold HCT in gen-eral and the use of a complex HCT model.
Otherrelated work was for example done by Lim et al.
(2009) on how different kinds of explanations canimprove the intelligibility of context-aware intel-ligent systems.
They concentrate on the effect ofWhy, Why-not, How-to and What-if explanationson trust and understanding system?s actions or re-actions.
The results showed that Why and Why-not explanations were the best kind of explanationto increase the user?s understanding of the sys-tem, though trust was only increase by providingWhy explanations.
Drawbacks of this study werethat they did only concentrate on understandingthe system and trusting the system in general anddid not consider that HCT is on the one hand notonly influenced by the user?s understanding of thesystem and on the other hand that if one base oftrust is flawed, the HCT in general will be dam-aged (Mayer et al., 1995).Regarding the issue of trusting a technical sys-tem or its actions and reactions related work ex-ists for example on ?credibility?
(Fogg and Tseng,1999).
However, this term developed in the webcommunity focusing on the believability of exter-nal sources.
The term trust is used in the webresearch community as well as in work on ?trustin automation?.
However, as Fogg stated himselflater (Tseng and Fogg, 1999) credibility should becalled believability and trust-in-automation shouldbe called dependability to reduce the missunder-standings.
In this work we use the term human-computer trust and its model by Madsen and Gre-gor (2000) subsuming both terms.4 Experiment on ExplanationEffectivenessThe insight that human-computer trust is not asimple but complex construct and the lack of di-rected methods to influence components of HCTmotivated us to conduct an experiment which triedto overcome some of these issues.
The use of ex-planations to influence HCT bases in a directedand not arbitrary way, depends on whether aneffective mapping of explanation goals to HCTbases can be found.
This means, that we haveto identify which goal of explanation influenceswhich base of trust in the most effective way.Therefore, the goal was to change undirectedstrategies to handle HCT issues into directed andwell-founded ones, substantiating the choice andgoal of explanation.For that we conducted a web-based study in-ducing events to create not understandable or notexpected situations and then compared the effectsof the different goals of explanations on the HCT-bases.
For our experiment we concentrated on jus-tification and transparency explanations.
Justifica-tions are the most obvious goal an explanation canpursue.
The main idea of this goal is to providesupport for and increase confidence in given sys-tem advices or actions.
The goal of transparencyis to increase the users understanding in how thesystem works and reasons.
This can help the userto change his perception of the system from ablack-box to a system the user can comprehend.Thereby, the user can build a mental model of thesystem and its underlying reasoning processes.The participants in the experiment where ac-53quired by using flyers in the university as well asthrough facebook.
The age of the participants wasin a range from 14 to 61, with the mean being 24,1.Gender wise, the distribution was 59% (male) to41% (female), with most of the participants be-ing students.
For the participation the students didreceive a five euro voucher for a famous onlinestore.
However, this was only granted when fin-ishing the complete experiment.
Therefore, partic-ipants dropping out of the experiment would waivethe right on the voucher.4.1 Set-UpThe main objective of the participants to organizefour parties for friends or relatives in a web-basedenvironment.
This means that they had to use thebrowser at home or the university to organize forexample, the music, select the type and amount offood or order drinks.
Each party was describedby an initial screen depicting the key data for theparty.
This included which tasks had to be accom-plished and how many people were expected tojoin (see figure 2).
Each task was implementedas a single web-page, with the goal to organizeone part of the party (i.e., dinner, drinks, or cham-pagne reception).
The user had to choose fromseveral drop-down menus which item should beordered for the party and in what number.
For ex-ample, the user had to order the components ofthe dinner (see figure 3).
When an entry insidea drop-down menu was chosen, the system gavean advice on how much of this would be neededto satisfy the needs of one guest.
Additionally, be-fore the participant could move on to the next task,the orders were checked by the system.
The sys-tem would output whether the user had selectedtoo much, too little or the right amount and only ifeverything was alright could proceed to the nexttask.
The experiment consisted in total of fourrounds.
The first two rounds were meant to gosmoothly and were supposed to get the subjectused to the system and by that building a men-tal model of it.
After the first two rounds a HCTquestionnaire was presented to the user.
As ex-pected the user has built a relationship with thesystem by gaining an understanding of the systemsprocesses.
The next two rounds were meant toinfluence the HCT-relationship negative with un-expected external events.
These unexpected, andincongruent to the user?s mental model, systemevents were influencing pro-actively the decisionsFigure 2: General information on the party.
Howmany people plan to attend the event and what typeof tasks have to be accomplished.and solutions the user made to solve the task.
Thismeans, without warning, the user was overruledby the system and either simply informed by thischange, or was presented an additional justifica-tion or transparency explanation as seen in figure3.
In this figure we can see that the user?s order(?Bestellungsliste?)
was changed pro-actively be-cause of an external event.
Here the attendance ofsome participants was cancelled in the reservationsystem, thus the system did intervene.
This pro-active change was explained at the bottom of theweb-page by, in this case, providing a justification(?The order was changed by the system, becausethe number of attending persons decreased?).
Thematching transparency explanation would not onlyprovide a reason, but explain how the system an-swer was reached (?Due to recent events the or-der was changed by the system.
The order vol-ume has been reduced, because several personscanceled their attendance in the registration sys-tem.?).
Events like this occurred several times inthe rounds 3 and 4 of the party planning.4.2 Results139 starting participants were distributed amongthe three test groups (no explanation, transparency,justifications).
98 accomplished round 2, reach-ing the point until the external events were in-duced and 59 participants completed the experi-ment.
The first main result was that 47% fromthe group receiving no explanations quit during54Figure 3: This screenshot shows one of the tasks the user has to accomplish.
In this case dinner (?Haupt-gerichte?)
including entree (?Vorspeisen?)
and desserts has to be ordered.the critical rounds 3 and 4.
However, if expla-nations were presented only 33% (justifications)and 35% (transparency) did quit.
This means thateventhough the participants would encounter neg-ative consequences of losing the reward money,they did drop out of the experiment.
Therefore,we can state that the use of explanations in incom-prehensible and not expected situations can helpto keep the human-computer interaction running.The main results from the HCT-questionnaires canbe seen in figure 4.
The data states that providingno explanations in rounds three and four resultedin a decrease in several bases of trust.
Therefore,we can conclude that the external events did in-deed result in our planned negative change in trust.Perceived understandability diminished on aver-age over the people questioned by 1.2 on a Lik-ert scale with a range from 1 to 5 when providingno explanation at all compared to only 0.4 whenproviding transparency explanations (no explana-tion vs. transparency t(34)=-3.557 p<0.001), andon average by 0.5 with justifications (no expla-nation vs. justifications t(36)=-2.023 p<0.045).Omitting explanations resulted in an average de-crease of 0.9 for the perceived reliability, withtransparency explanations in a decrease of 0.4 andfor justifications in a decrease of 0.6 (no explana-tion vs. transparency t(34)=-2.55 p<0.015).These results support our hypotheses that trans-parency explanations can help to reduce the neg-ative effects of trust loss regarding the user?s per-ceived understandability and reliability of the sys-tem in incomprehensible and unexpected situa-tions.
Especially for the base of understandability,meaning the prediction of future outcomes, trans-parency explanations fulfill their purpose in a goodway.
Additionally, they seem to help with the per-ception of a reliable, consistent system.
The re-sults show that it is worthwhile to augment ongo-ing dialogs with explanations to maintain HCT.While analyzing the data we did not find anystatistically significant differences between pro-viding transparency and justification explanations.However, this could be due to limited differencesin the goals of explanation.
Usually, the trans-parency explanations in the experiment were in-cluding more information on what happened in-side the system, and how the system did recognizethe external event (e.g., the reduction of attend-ing persons).
In future experiments we will try todistinguish those two goals of explanations morefrom each other.
For example, the justification forreduce attendance to an event can be changed tosomething like ?The order was changed by the sys-tem, because otherwise you would have too muchfood?
instead of ?The order was changed by thesystem, because the number of attending personsdecreased?
and by that making it more differentfrom the transparency explanation (?Due to recentevents the order was changed by the system.
Theorder volume has been reduced, because severalpersons canceled their attendance in the registra-tion system.?).
In the following, we will describehow this is used in our developed explanation aug-55Figure 4: This figure shows the changes of HCT bases from round 2 to round 4.
The scale was a 5 pointlikert scale with e. g., 1 the system being not understandable at all and 5 the opposite.mentation architecture (see figure 5).5 ImplementationThe augmentation of the dialog is done using twodifferent kinds of dialog models.
On the one handwe are using a classic dialog model based on afinite-state machine approach for the task-orientedpart of the dialog.
On the other hand a planner(M?uller et al., 2012) is used to generate from aPOMDP a decision tree.
This POMDP is usedonly for the augmentation of the task-oriented partof the dialog with explanations.
The communi-cation between each module of the architectureis controlled by a XML-based message-orientedmiddleware (Schr?oder, 2010), using a publish-subscribe system to distribute the XML-messages.In order to decide when to induce additional ex-planations, on one hand critical situations in HCIhave to be recognized and on the other hand, ifnecessary the appropriate type of explanation hasto be given.
Obviously, recognizing those situa-tions cannot be done solely by using informationcoming from interaction and its history.
Multi-modal input as speech recognition accuracy, fa-cial expressions or any other sensor informationcan help to improve the accuracy of recognizingcritical moments in HCI.
However, mapping sen-sor input to semantic information is usually doneby classifiers and those classifiers convey a certainamount of probabilistic inaccuracy which has tobe handled.
Therefore, a decision model has to beable to handle probabilistic information in a suit-able manner.5.1 Probabilistic Decision ModelFor the problem representation when and how toreact, a so-called partially observable Markov de-cision process (POMDP) was chosen and formal-ized in the Relational Dynamic Influence Dia-gram Language (RDDL) (Sanner, 2010).
RDDLis a uniform language which allows an efficientdescription of POMDPs by representing its con-stituents (actions, observations, belief state) withvariables.
Formally, a POMDP consists of a setS of world states, a set A of system actions, anda set O of possible observations the system canmake.
Further, transition probabilities P (s?|s, a)describe the dynamics of the environment, i.e., theprobability of the successor world state being s?when action a is executed in state s. The obser-vation probabilities P (o|s?, a) represent the sen-sors of the system in terms of the probability ofmaking observation o when executing a resultedin successor world state s?.
Each time the systemexecutes an action a, it receives a reward R(s, a)which depends on the world state s the action wasexecuted in.
The overall goal of the system is tomaximize the accumulated reward it receives overa fixed number of time steps.
(For more informa-tion on POMDPs, see Kaelbling et al.
(1998).
)A POMDP is then used by a planner (Silver andVeness, 2010; M?uller et al., 2012) to search for apolicy that determines the system?s behavior.
Thispolicy is, e.g., represented as a decision tree thatrecommends the most suitable action based on thesystem?s previous actions and observations.
Forexample, a policy for a POMDP that models HCIwith respect to HCT, can thus represent a decisiontree which represents a guideline for a dialog flowwhich ensures an intact HCT-relationship.The RDDL model is a probabilistic representa-tion of the domain, which determines when andhow to augment the dialog with explanations atrun-time.
Each observation o consists of the du-56Figure 5: The architecture consists of two dialog models, a fission and fusion engine, sensors as well asthe multimodal interface representation to interact with the user.
The dialog models can be seperated ina task-oriented FSM-dialog model and into a POMDP-based decision tree for explanation augmentation.This decision tree is generated from a POMDP-model by a planner.ration of interaction for each dialog step as well asthe semantic information of the input (i.e., whichaction in the interface was triggered by speech,touch or point-and-click interaction).
Those typesof interaction can bring along uncertainty (e.g.,speech recognition rate).
The state s in terms ofHCT is modeled by its respective bases, namelyunderstandability, technical-competence, reliabil-ity, faith and personal attachment.
The systemactions A are the dialogs presented to the user.These are the different goals of explanations (jus-tification, transparency, conceptualization, rele-vance and learning) as well as the task-orientedpart of the dialog represented by a so-called com-municative function(c) with c from set C (e.g.,question, inform, answer, offer, request, instruct).This means, that in the POMDP only the com-municative function of the task-oriented dialogs isrepresented without the specific content.The transition probabilities are defined as con-ditional probability functions (CPFs) and modeluser behavior dependent on the system?s actionsand the user?s current HCT values.
Basically, con-ditional functions are defined using if else for allwanted cases.
For example, we defined that theuser?s understanding in s?will probably be highif a transparency explanation was the last systemaction.
When the user?s understanding is indeedhigh in s?, the observation will probably be thatthe user clicked okay, and the time he took for theinteraction was around his usual amount taken forexplanations.
From this observation, a planner caninfer that the transparency explanation indeed in-creased the user?s understanding.Now, the quest is to define the reward func-tion R(s, a) in a way that it leads to an optimalflow of actions.
I.e., the system should receive apenalty when the bases of trust do not remain in-tact, and actions should incur a cost so that the sys-tem only executes them when trust is endangered.However, because POMDPs tend to be becomevery quick very complex, we chose to seperatethe task-oriented dialog from the additional dialogaugmentation with explanations when needed.5.2 Dialog Augmentation ProcessThe task-oriented dialog is modeled as a classicfinite-state machine (FSM).
Each dialog action hasseveral interaction possibilities, each leading toanother specified dialog action.
Each of those di-alog action is represented as POMDP action a aspart of C (communicative function(c)).
As alreadymentioned, only the communicative function ismodeled to reduce the complexity in the POMDP.The HCI is started using the FSM-based di-alog model approach and uses the POMDP tocheck whether the user?s trust or components ofthe user?s trust are endangered.
At run-time thenext action in the FSM is compared to the onedetermined by the POMDP (see figure 6).
Thismeans, that if the next action in the FSM is not thesame as the one planned by the POMDP, the dia-57Figure 6: This figure shows the comparison ofFSM to Decision Tree.
The next action m3in theFSM does not correspond to the one endorsed bythe POMDP Decision Tree.
Therefore, the dialogwill be augmented by explanation action mE.log flow is interrupted, and the ongoing dialog isaugmented by the proposed explanation.
For ex-ample, if the user is presented currently a commu-nicative function of type inform and the decisiontree recommends to provide a transparency expla-nation, because the understanding and reliabilityare probably false, the originally next step in theFSM is postponed and first the explanation is pre-sented.
The other way around, if the next action inthe FSM is subsumed by the one scheduled by thePOMDP, the system does not need to intervene.For example, if the next FSM-action is to instructthe user about how to connect amplifier and re-ceiver and the POMDP would recommend an ac-tion of type communicative function instruct, nodialog augmentation is needed.6 Dialog InterfaceEach dialog action in the FSM as well as the ex-planation dialogs are represented by a so-called di-alog goal, which is allocated on the one hand atype of communicative function c. On the otherhand the dialog content is composed of multipleinformation objects referencing so-called informa-Figure 7: A typical output presentation of the fis-sion component of a dialog goal.
Here the usergets instruction on how to connect the BluRay-Player with an HDMI cable.tion IDs in the information model.
Each informa-tion object can consist of different types (e.g., text,audio, and pictures).
For interface presentationthe dialog goal is passed to the fission which se-lects and combines the information objects at run-time by a fission sub-component to compose theuser interface in a user- and situation-adaptive way(Honold et al., 2012).
In figure 7 we can see a typi-cal interface for a transmitted dialog goal in whichthe user can interact via speech, touch or GUI.7 Conclusion and Future WorkIn this paper we showed the necessity to deal withcritical situations in HCI in a probabilistic ap-proach.
The advantage of our approach is thatthe designer still can define a FSM-based task-oriented dialog.
Usually most commercial sys-tems are still based on such approaches.
However,expanding the dialog by a probabilistic decisionmodel seems to be a valuable choice.
Our experi-ment on the influence of explanations on HCT hasclearly shown, that it is worthwhile to augment theongoing dialog by transparency or justification ex-planations for an intact HCT relationship.
In thefuture we will run experiments on how effectivethe hybrid FSM-POMDP approach is compared toclassic as well as POMDP dialog systems.AcknowledgmentThis work was supported by the TransregionalCollaborative Research Centre SFB/TRR 62?Companion-Technology for Cognitive TechnicalSystems?
which is funded by the German Re-search Foundation (DFG).58ReferencesB.
J. Fogg and Hsiang Tseng.
1999.
The elements ofcomputer credibility.
In Proceedings of the SIGCHIconference on Human Factors in Computing Sys-tems, CHI ?99, pages 80?87, New York, NY, USA.ACM.Alyssa Glass, Deborah L. McGuinness, and MichaelWolverton.
2008.
Toward establishing trust in adap-tive agents.
In IUI ?08: Proceedings of the 13th in-ternational conference on Intelligent user interfaces,pages 227?236, NY, USA.
ACM.Frank Honold, Felix Sch?ussel, and Michael Weber.2012.
Adaptive probabilistic fission for multimodalsystems.
In Proceedings of the 24th AustralianComputer-Human Interaction Conference, OzCHI?12, pages 222?231.L.
P. Kaelbling, M. L. Littman, and A. R. Cassandra.1998.
Planning and acting in partially observablestochastic domains.
Artificial Intelligence, pages99?134.John D. Lee and Katrina A.
See.
2004.
Trust in au-tomation: Designing for appropriate reliance.
Hu-man Factors, 46(1):50?80.Brian Y. Lim, Anind K. Dey, and Daniel Avrahami.2009.
Why and why not explanations improve theintelligibility of context-aware intelligent systems.In Proceedings of the SIGCHI Conference on Hu-man Factors in Computing Systems, CHI ?09, pages2119?2128, NY, USA.
ACM.Maria Madsen and Shirley Gregor.
2000.
Measur-ing human-computer trust.
In Proceedings of the 11th Australasian Conference on Information Systems,pages 6?8.Roger C. Mayer, James H. Davis, and F. David Schoor-man.
1995.
An Integrative Model of Organiza-tional Trust.
The Academy of Management Review,20(3):709?734.B M Muir.
1992.
Trust in automation: Part i. theoret-ical issues in the study of trust and human interven-tion in automated systems.
In Ergonomics, pages1905?1922.Felix M?uller, Christian Sp?ath, Thomas Geier, and Su-sanne Biundo.
2012.
Exploiting expert knowl-edge in factored POMDPs.
In Proceedings of the20th European Conference on Artificial Intelligence(ECAI 2012), pages 606?611.Raja Parasuraman and Victor Riley.
1997.
Humansand automation: Use, misuse, disuse, abuse.
HumanFactors: The Journal of the Human Factors and Er-gonomics Society, 39(2):230?253, June.Scott Sanner.
2010.
Relational dynamic in-fluence diagram language (rddl): Languagedescription.
http://users.cecs.anu.edu.au/ ssan-ner/IPPC2011/RDDL.pdf.Marc Schr?oder.
2010.
The semaine api: Towardsa standards-based framework for building emotion-oriented systems.
Advances in Human-Machine In-teraction, (319406):21.D.
Silver and J. Veness.
2010.
Monte-carlo planningin large POMDPs.
In NIPS, pages 2164?2172.F.
S?rmo and J. Cassens.
2004.
Explanation goalsin case-based reasoning.
In Proceedings of the7th European Conference on Case-Based Reason-ing, pages 165?174.Shawn Tseng and B. J. Fogg.
1999.
Credibility andcomputing technology.
Commun.
ACM, 42(5):39?44, May.59
