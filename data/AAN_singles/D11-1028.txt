Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 304?312,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Model of Discourse Predictions in Human Sentence ProcessingAmit Dubey and Frank Keller and Patrick SturtHuman Communication Research Centre, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, UK{amit.dubey,frank.keller,patrick.sturt}@ed.ac.ukAbstractThis paper introduces a psycholinguisticmodel of sentence processing which combinesa Hidden Markov Model noun phrase chun-ker with a co-reference classifier.
Both mod-els are fully incremental and generative, giv-ing probabilities of lexical elements condi-tional upon linguistic structure.
This allowsus to compute the information theoretic mea-sure of surprisal, which is known to correlatewith human processing effort.
We evaluateour surprisal predictions on the Dundee corpusof eye-movement data show that our modelachieve a better fit with human reading timesthan a syntax-only model which does not haveaccess to co-reference information.1 IntroductionRecent research in psycholinguistics has seen agrowing interest in the role of prediction in sentenceprocessing.
Prediction refers to the fact that the hu-man sentence processor is able to anticipate upcom-ing material, and that processing is facilitated whenpredictions turn out to be correct (evidenced, e.g.,by shorter reading times on the predicted word orphrase).
Prediction is presumably one of the factorsthat contribute to the efficiency of human languageunderstanding.
Sentence processing is incremental(i.e., it proceeds on a word-by-word basis); there-fore, it is beneficial if unseen input can be antici-pated and relevant syntactic and semantic structureconstructed in advance.
This allows the processor tosave time and makes it easier to cope with the con-stant stream of new input.Evidence for prediction has been found in a rangeof psycholinguistic processing domains.
Semanticprediction has been demonstrated by studies thatshow anticipation based on selectional restrictions:listeners are able to launch eye-movements to thepredicted argument of a verb before having encoun-tered it, e.g., they will fixate an edible object as soonas they hear the word eat (Altmann and Kamide,1999).
Semantic prediction has also been shown inthe context of semantic priming: a word that is pre-ceded by a semantically related prime or by a seman-tically congruous sentence fragment is processedfaster (Stanovich and West, 1981; Clifton et al,2007).
An example for syntactic prediction can befound in coordinate structures: readers predict thatthe second conjunct in a coordination will have thesame syntactic structure as the first conjunct (Fra-zier et al, 2000).
In a similar vein, having encoun-tered the word either, readers predict that or and aconjunct will follow it (Staub and Clifton, 2006).Again, priming studies corroborate this: Compre-henders are faster at naming words that are syntacti-cally compatible with prior context, even when theybear no semantic relationship to it (Wright and Gar-rett, 1984).Predictive processing is not confined to the sen-tence level.
Recent experimental results also provideevidence for discourse prediction.
An example is thestudy by van Berkum et al (2005), who used a con-text that made a target noun highly predictable, andfound a mismatch effect in the ERP (event-relatedbrain potential) when an adjective appeared that wasinconsistent with the target noun.
An example is (wegive translations of their Dutch materials):(1) The burglar had no trouble locating the secretfamily safe.a.
Of course, it was situated behind a304bigneu but unobtrusive paintingneu.b.
Of course, it was situated behind abigcom but unobtrusive bookcasecom.Here, the adjective big, which can have neutral orcommon gender in Dutch, is consistent with the pre-dicted noun painting in (1-a), but inconsistent with itin (1-b), leading to a mismatch ERP on big in (1-b)but not in (1-a).Previous results on discourse effects in sentenceprocessing can also be interpreted in terms of pre-diction.
In a classical paper, Altmann and Steed-man (1988) demonstrated that PP-attachment pref-erences can change through discourse context: if thecontext contains two potential referents for the tar-get NP, then NP-attachment of a subsequent PP ispreferred (to disambiguate between the two refer-ents), while if the context only contains one targetNP, VP-attachment is preferred (as there is no needto disambiguate).
This result (and a large body ofrelated findings) is compatible with an interpretationin which the processor predicts upcoming syntacticattachment based on the presence of referents in thepreceding discourse.Most attempts to model prediction in human lan-guage processing have focused on syntactic pre-diction.
Examples include Hale?s (2001) surprisalmodel, which relates processing effort to the con-ditional probability of the current word given theprevious words in the sentence.
This approach hasbeen elaborated by Demberg and Keller (2009) in amodel that explicitly constructs predicted structure,and includes a verification process that incurs ad-ditional processing cost if predictions are not met.Recent work has attempted to integrate semanticand discourse prediction with models of syntacticprocessing.
This includes Mitchell et al?s (2010)approach, which combines an incremental parserwith a vector-space model of semantics.
However,this approach only provides a loose integration ofthe two components (through simple addition oftheir probabilities), and the notion of semantics usedis restricted to lexical meaning approximated byword co-occurrences.
At the discourse level, Dubey(2010) has proposed a model that combines an incre-mental parser with a probabilistic logic-based modelof co-reference resolution.
However, this modeldoes not explicitly model discourse effects in termsof prediction, and again only proposes a loose in-tegration of co-reference and syntax.
Furthermore,Dubey?s (2010) model has only been tested on twoexperimental data sets (pertaining to the interactionof ambiguity resolution with context), no broad cov-erage evaluation is available.The aim of the present paper is to overcome theselimitations.
We propose a computational model thatcaptures discourse effects on syntax in terms of pre-diction.
The model comprises a co-reference com-ponent which explicitly stores discourse mentionsof NPs, and a syntactic component which adjustthe probabilities of NPs in the syntactic structurebased on the mentions tracked by the discourse com-ponent.
Our model is HMM-based, which makesit possible to efficiently process large amounts ofdata, allowing an evaluation on eye-tracking cor-pora, which has recently become the gold-standardin computational psycholinguistics (e.g., Dembergand Keller 2008; Frank 2009; Boston et al 2008;Mitchell et al 2010).The paper is structured as follows: In Section 2,we describe the co-reference and the syntactic mod-els and evaluate their performance on standard datasets.
Section 3 presents an evaluation of the overallmodel on the Dundee eye-tracking corpus.
The pa-per closes with a comparison with related work anda general discussion in Sections 4 and 5.2 ModelThis model utilises an NP chunker based upon a hid-den Markov model (HMM) as an approximation tosyntax.
Using a simple model such as an HMM fa-cilitates the integration of a co-reference component,and the fact that the model is generative is a prereq-uisite to using surprisal as our metric of interest (assurprisal require the computation of prefix probabil-ities).
The key insight in our model is that humansentence processing is, on average, facilitated whena previously-mentioned discourse entity is repeated.This facilitation depends upon keeping track of a listof previously-mentioned entities, which requires (atthe least) shallow syntactic information, yet the fa-cilitation itself is modeled primarily as a lexical phe-nomenon.
This allows a straightforward separationof concerns: shallow syntax is captured using theHMM?s hidden states, whereas the co-reference fa-305cilitation is modeled using the HMM?s emissions.The vocabulary of hidden states is described in Sec-tion 2.1 and the emission distribution in Section 2.22.1 Syntactic ModelA key feature of the co-reference component of ourmodel (described below) is that syntactic analysisand co-reference resolution happen simultaneously.This could potentially slow down the syntactic anal-ysis, which tends to already be quite slow for ex-haustive surprisal-based incremental parsers.
There-fore, rather than using full parsing, we use an HMM-based NP chunker which allows for a fast analysis.NP chunking is sufficient to extract NP discoursementions and, as we show below, surprisal valuescomputed using HMM chunks provide a useful fiton the Dundee eye-movement data.To allow the HMM to handle possessive construc-tions as well as NP with simple modifiers and com-plements, the HMM decodes NP subtrees with depthof 2, by encoding the start, middle and end of asyntactic category X as ?
(X?, ?X?
and ?X)?, respec-tively.
To reduce an explosion in the number ofstates, the category begin state ?(X?
only appears atthe rightmost lexical token of the constituent?s left-most daughter.
Likewise, ?X)?
only appears at theleftmost lexical token of the constituent?s rightmostdaughter.
An example use of this state vocabularycan be seen in Figure 1.
Here, a small degree of re-cursion allows for the NP ((new york city?s) generalobligation fund) to be encoded, with the outer NP?sleft bracket being ?announced?
at the token ?s, whichis the rightmost lexical token of the inner NP.
Hid-den states also include part-of-speech (POS) tags,allowing simultaneous POS tagging.
In the exam-ple given in Figure 1, the full state can be read bylisting the labels written above a word, from top tobottom.
For example, the full state associated with?s is (NP-NP)-POS.
As ?s can also be a contractionof is, another possible state for ?s is VBZ (withoutrecursive categories as we are only interested in NPchunks).The model uses unsmoothed bi-gram transitionprobabilities, along with a maximum entropy dis-tribution to guess unknown word features.
The re-sulting distribution has the form P(tag|word) and istherefore unsuitable for computing surprisal values.However, using Bayes?
theorem we can compute:P(word|tag) = P(tag|word)P(word)P(tag) (1)which is what we need for surprisal.
The pri-mary information from this probability comes fromP(tag|word), however, reasonable estimates ofP(tag) and P(word) are required to ensure the prob-ability distribution is proper.
P(tag) may be esti-mated on a parsed treebank.
P(word), the probabil-ity of a particular unseen word, is difficult to esti-mate directly.
Given that our training data containsapproximately 106 words, we assume that this prob-ability must be bounded above by 10?6.
As an ap-proximation, we use this upper bound as the proba-bility of P(word).Training The chunker is trained on sections 2?22 of the Wall Street Journal section of the PennTreebank.
CoNLL 2000 included chunking as ashared task, and the results are summarized by TjongKim Sang and Buchholz (2000).
Our chunker is notcomparable to the systems in the shared task for sev-eral reasons: we use more training data, we tag si-multaneously (the CoNLL systems used gold stan-dard tags) and our notion of a chunk is somewhatmore complex than that used in CoNLL.
The bestperforming chunker from CoNLL 2000 achieved anF-score of 93.5%, and the worst performing systeman F-score of 85.8%.
Our chunker achieves a com-parable F-score of 85.5%, despite the fact that it si-multaneously tags and chunks, and only uses a bi-gram model.2.2 Co-Reference ModelIn a standard HMM, the emission probabilities arecomputed as P(wi|si) where wi is the ith word and siis the ith state.
In our model, we replace this with achoice between two alternatives:P(wi|si) ={ ?Pseen before(wi|si)(1??
)Pdiscourse new(wi|si) (2)The ?discourse new?
probability distribution is thestandard HMM emission distribution.
The ?seen be-fore?
distribution is more complicated.
It is in partbased upon caching language models.
However, thecontents of the cache are not individual words but306(NP NP NP NP)(NP NP) (NP NP NP NP) NP (NP NP NP)JJ NN IN NNP NNP NNP POS JJ NN NNS VBN RP DT NN NNstrong demand for new york city ?s general obligation bonds propped up the municipal marketFigure 1: The chunk notation of a tree from the training data.Variable Typel, l?
List of trie nodesw,wi Wordst Tagn,n?
Trie nodesl?
List(root of mention trie)for w?
w0 to wn dol??
ll?
/0Clear tag freq array f tClear word freq array f wtfor t ?
tag set dofor n ?
l?
dof t(t)?
f t(t)+FreqO f (n, t)n??
Getchild(w, t)if n?
6= /0 thenf wt(t)?
f wt(t)+FreqO f (n?,w, t)l?
n?
:: lend ifend forend forPseen before(w|t) = f t(t)/ f wt(t)end forFigure 2: Looking up entries from the NP Cacherather a collection of all NPs mentioned so far in thedocument.Using a collection of NPs rather than individualwords complicates the decoding process.
If m is thesize of a document, and n is the size of the currentsentence, decoding occurs in O(mn) time as opposedto O(n), as the collection of NPs needs to be ac-cessed at each word.
However, we do not store theNPs in a list, but rather a trie.
This allows decodingto occur in O(n logm) time, which we have foundto be quite fast in practise.
The algorithm used tokeep track of currently active NPs is presented inFigure 2.
This shows how the distribution Pseen beforeis updated on a word-by-word basis.
At the end ofeach sentence, the NPs of the Viterbi parse are addedto the mention trie after having their leading arti-cles stripped.
A weakness of the algorithm is thatmentions are only added on a sentence-by-sentencebasis (disallowing within-sentence references).
Al-though the algorithm is intended to find whole-stringmatches, in practise, it will count any NP whose pre-fix matches as being co-referent.A consequence of Equation 2 is that co-referenceresolution is handled at the same time as HMM de-coding.
Whenever the ?seen before?
distribution isapplied, an NP is co-referent with one occurring ear-lier.
Likewise, whenever the ?discourse new?
dis-tribution is applied, the NP is not co-referent withany NP appearing previously.
As one choice or theother is made during decoding, the decoder there-fore also selects a chain of co-referent entities.
Gen-erally, for words which have been used in this dis-course, the magnitude of probabilities in the ?seenbefore?
distribution are much higher than in the ?dis-course new?
distribution.
Thus, there is a strongbias to classify NPs which match word-for-word asbeing co-referent.
There remains a possibility thatthe model primarily captures lexical priming, ratherthan co-reference.
However, we note that stringmatch is a strong indicator of two NPs being corefer-307ent (cf.
Soon et al 2001), and, moreover, the match-ing is done on an NP-by-NP basis, which is moresuitable for finding entity coreference, rather than aword-by-word basis, which would be more suitablefor lexical priming.An appealing side-effect of using a simple co-reference decision rule which is applied incremen-tally is that it is relatively simple to incremen-tally compute the transitive closure of co-referencechains, resulting in the entity sets which are thenused in evaluation.The co-reference model only has one free param-eter, ?, which is estimated from the ACE-2 corpus.The estimate is computed by counting how often arepeated NP actually is discourse new.
In the currentimplementation of the model, ?
is constant through-out the test runs.
However, ?
could possibly bea function of the previous discourse, allowing formore complicated classification probabilities.3 Evaluation3.1 DataOur evaluation experiments were conducted uponthe Dundee corpus (Kennedy et al, 2003), whichcontains the eye-movement record of 10 participantseach reading 2,368 sentences of newspaper text.This data set has previously been used by Dembergand Keller (2008) and Frank (2009) among others.3.2 EvaluationEye tracking data is noisy for a number of rea-sons, including the fact that experimental partici-pants can look at any word which is currently dis-played.
While English is normally read in a left-to-right manner, readers often skip words or makeregressions (i.e., look at a word to the left of theone they are currently fixating).
Deviations froma strict left-to-right progression of fixations moti-vate the need for several different measures of eyemovement.
The model presented here predicts theTotal Time that participants spent looking at a re-gion, which includes any re-fixations after lookingaway.
In addition to total time, other possible mea-sures include (a) First Pass, which measures the ini-tial fixation and any re-fixations before looking atany other word (this occurs, for instance, if the eyeinitially lands at the start of a long word ?
the eyewill tend to re-fixate on a more central viewing lo-cation), (b) Right Bounded reading time, which in-cludes all fixations on a word before moving to theright of the word (i.e., re-fixations after moving leftare included), and (c) Second Pass, which includesany re-fixation on a word after looking at any otherword (be it to the left or the right of the word of inter-est).
We found that the model performed similarlyacross all these reading time metrics, we thereforeonly report results for Total Time.As mentioned above, reading measures are hy-pothesised to correlate with Surprisal, which is de-fined as:S(wt) =?
log(P(wt |w1...wt1) (3)We compute the surprisal scores for the syntax-onlyHMM, which does not have access to co-referenceinformation (henceforth referred to as ?HMM?
)and the full model, which combines the syntax-only HMM with the co-reference model (henceforth?HMM+Ref?).
To determine if our Dundee corpussimulations provide a reasonable model of humansentence processing, we perform a regression anal-ysis with the Dundee corpus reading time measureas the dependent variable and the surprisal scores asthe independent variable.To account for noise in the corpus, we also usea number of additional explanatory variables whichare known to strongly influence reading times.These include the logarithm of the frequency of aword (measured in occurrences per million) and thelength of a word in letters.
Two additional explana-tory variables were available in the Dundee corpus,which we also included in the regression model.These were the position of a word on a line, andwhich line in a document a word appeared in.
Asparticipants could only view one line at a time (i.e.,one line per screen), these covariates are known asline position and screen position, respectively.All the covariates, including the surprisal es-timates, were centered before including them inthe regression model.
Because the HMM andHMM+Ref surprisal values are highly collinear, theHMM+Ref surprisal values were added as residualsof the HMM surprisal values.In a normal regression analysis, one must eitherassume that participants or the particular choice of308items add some randomness to the experiment, andeither each participant?s responses for all items mustbe averaged (treating participants as a random fac-tor), or all participant?s responses for each item isaveraged (treating items as a random factor).
How-ever, in the present analysis we utilise a mixed ef-fects model, which allows both items and partici-pants to be treated as random factors.1The are a number of criteria which can be usedto test the efficacy of one regression model over an-other.
These include the Aikake Information Cri-terion (AIC), the Bayesian Information Criterion(BIC), which trade off model fit and number ofmodel parameters (lower scores are better).
It is alsocommon to compare the log-likelihood of the mod-els (higher log-likelihood is better), in which case a?2 can be used to evaluate if a model offers a sig-nificantly better fit, given the number of parametersis uses.
We test three models: (i) a baseline, withonly low-level factors as independent variables; (ii)the HMM model, with the baseline factors plus sur-prisal computed by the syntax-only HMM; and (iii)the HMM+Ref model which includes the raw sur-prisal values of the syntax-only HMM and the sur-prisal of the HMM+Ref models as computed as aresidual of the HMM surprisal score.
We comparethe HMM and HMM+Ref to the baseline, and theHMM+Ref model against the HMM model.Some of the data needed to be trimmed.
If, due todata sparsity, the surprisal of a word goes to infinityfor one of the models, we entirely remove that wordfrom the analysis.
This occurred seven times formthe HMM+Ref model, but did not occur at all withthe HMM model.
Some of the eye-movement datawas trimmed, as well.
Fixations on the first and lastwords of a line were excluded, as were tracklosses.However, we did not trim any items due to abnor-1We assume that each participant and item bias the readingtime of the experiment.
Such an analysis is known as havingrandom intercepts of participant and item.
It is also possibleto assume a more involved analysis, known as random slopes,where the participants and items bias the slope of the predictor.The model did not converge when using random intercept andslopes on both participant and item.
If random slopes on itemswere left out, the HMM regression model did converge, but notthe HMM+Ref model.
As the HMM+Ref is the model of inter-est random slopes were left out entirely to allow a like-with-likecomparison between the HMM and HMM+Ref regression mod-els.mally short or abnormally long fixation durations.3.3 ResultsThe result of the model comparison on Total Timereading data is summarised in Table 1.
To allow thiswork to be compared with other models, the lowerpart of the table gives the abosolute AIC, BIC andlog likelihood of the baseline model, while the upperpart gives delta AIC, BIC and log likelihood scoresof pairs of models.We found that both the HMM and HMM+Refprovide a significantly better fit with the readingtime data than the Baseline model; all three crite-ria agree: AIC and BIC lower than for the base-line, and log-likelihood is higher.
Moreover, theHMM+Ref model provides a significantly better fitthan the HMM model, which demonstrates the bene-fit of co-reference information for modeling readingtimes.
Again, all three measures provide the sameresult.Table 2 corroborates this result.
It list themixed-model coefficients for the HMM+Ref modeland shows that all factors are significant predic-tors, including both HMM surprisal and residualizedHMM+Ref surprisal.4 Related WorkThere have been few computational models of hu-man sentence processing that have incorporateda referential or discourse-level component.
Niv(1994) proposed a parsing model based on Com-binatory Categorial Grammar (Steedman, 2001), inwhich referential information was used to resolvesyntactic ambiguities.
The model was able to cap-ture effects of referential information on syntacticgarden paths (Altmann and Steedman, 1988).
Thismodel differs from that proposed in the present pa-per, as it is intended to capture psycholinguistic pref-erences in a qualitative manner, whereas the aimof the present model is to provide a quantitativefit to measures of processing difficulty.
Moreover,the model was not based on a large-scale grammar,and was not tested on unrestricted text.
Spivey andTanenhaus (1998) proposed a sentence processingmodel that examined the effects of referential infor-mation, as well as other constraints, on the resolu-tion of ambiguous sentences.
Unlike Niv (1994),309From To ?
AIC ?
BIC ?
logLik ?2 SignificanceBaseline HMM -80 -69 41 82.112 p < .001Baseline HMM+Ref -99 -89 51 101.54 p < .001HMM HMM+Ref -19 -8 11 21.424 p < .001Model AIC BIC logLikBaseline 10567789 10567880 -5283886Table 1: Model comparison (upper part) and absolute scores for the Baseline model (lower part)Coefficient Estimate Std Error t-value(Intercept) 991.4346 23.7968 41.66log(Word Frequency) -55.3045 1.4830 -37.29Word Length 128.6216 1.4677 87.63Screen Position -1.7769 0.1326 -13.40Line Position 10.1592 0.7387 13.75HMM 12.1287 1.3366 9.07HMM+Ref 19.2772 4.1627 4.63Table 2: Coefficients of the HMM+Ref model on Total Reading Times.
Note that t > 2 indicates that the factor inquestion is a significant predictor.Spivey and Tanenhaus?s (1998) model was specifi-cally designed to provide a quantitative fit to readingtimes.
However, the model lacked generality, beingdesigned to deal with only one type of sentence.
Incontrast to both of these earlier models, the modelproposed here aims to be general enough to provideestimated reading times for unrestricted text.
In fact,as far as we are aware, the present paper representsthe first wide-coverage model of human parsing thathas incorporated discourse-level information.5 DiscussionThe primary finding of this work is that incorporat-ing discourse information such as co-reference intoan incremental probabilistic model of sentence pro-cessing has a beneficial effect on the ability of themodel to predict broad-coverage human parsing be-haviour.Although not thoroughly explored in this paper,our finding is related to an ongoing debate about thestructure of the human sentence processor.
In par-ticular, the model of Dubey (2010), which also sim-ulates the effect of discourse on syntax, is aimed atexamining interactivity in the human sentence pro-cessor.
Interactivity describes the degree to whichhuman parsing is influenced by non-syntactic fac-tors.
Under the weakly interactive hypothesis, dis-course factors may prune or re-weight parses, butonly when assuming the strongly interactive hypoth-esis would we argue that the sentence processor pre-dicts upcoming material due to discourse factors.Dubey found that a weakly interactive model sim-ulated a pattern of results in an experiment (Grodneret al, 2005) which was previously believed to pro-vide evidence for the strongly interactive hypothesis.However, as Dubey does not provide broad-coverageparsing results, this leaves open the possibility thatthe model cannot generalise beyond the experimentsexpressly modeled in Dubey (2010).The model presented here, on the other hand,is not only broad-coverage but could also be de-scribed as a strongly interactive model.
The stronginteractivity arises because co-reference resolutionis strongly tied to lexical generation probabilities,which are part of the syntactic portion of our model.This cannot be achieve in a weakly interactivemodel, which is limited to pruning or re-weightingof parses based on discourse information.
As ouranalysis on the Dundee corpus showed, the lexicalprobabilities (in the form of HMM+Ref surprisal)are key to improving the fit on eye-tracking data.We therefore argue that our results provide evidence310against a weakly interactive approach, which may besufficient to model individual phenomena (as shownby Dubey 2010), but is unlikely to be able to matchthe broad-coverage result we have presented here.We also note that psycholinguistic evidence for dis-course prediction (such as the context based lexi-cal prediction shown by van Berkum et al 2005,see Section 1) is also evidence for strong interac-tivity; prediction goes beyond mere pruning or re-weighting and requires strong interactivity.ReferencesGerry Altmann and Mark Steedman.
Interactionwith context during human sentence processing.Cognition, 30:191?238, 1988.Gerry T. M. Altmann and Yuki Kamide.
Incremen-tal interpretation at verbs: Restricting the domainof subsequent reference.
Cognition, 73:247?264,1999.Marisa Ferrara Boston, John T. Hale, ReinholdKliegl, and Shravan Vasisht.
Surprising parseractions and reading difficulty.
In Proceedings ofACL-08:HLT, Short Papers, pages 5?8, 2008.Charles Clifton, Adrian Staub, and Keith Rayner.Eye movement in reading words and sentences.In R V Gompel, M Fisher, W Murray, and R LHill, editors, Eye Movements: A Window in Mindand Brain, pages 341?372.
Elsevier, 2007.Vera Demberg and Frank Keller.
Data from eye-tracking corpora as evidence for theories of syn-tactic processing complexity.
Cognition, 109:192?210, 2008.Vera Demberg and Frank Keller.
A computationalmodel of prediction in human parsing: Unifyinglocality and surprisal effects.
In Proceedings ofthe 29th meeting of the Cognitive Science Society(CogSci-09), 2009.Amit Dubey.
The influence of discourse on syntax:A psycholinguistic model of sentence processing.In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics (ACL2010), Uppsala, Sweden, 2010.Stefan Frank.
Surprisal-based comparison betweena symbolic and a connectionist model of sentenceprocessing.
In 31st Annual Conference of theCognitive Science Society (COGSCI 2009), Ams-terdam, The Netherlands, 2009.Lyn Frazier, Alan Munn, and Charles Clifton.
Pro-cessing coordinate structure.
Journal of Psy-cholinguistic Research, 29:343?368, 2000.Daniel J. Grodner, Edward A. F. Gibson, and Du-ane Watson.
The influence of contextual constraston syntactic processing: Evidence for strong-interaction in sentence comprehension.
Cogni-tion, 95(3):275?296, 2005.John T. Hale.
A probabilistic earley parser as a psy-cholinguistic model.
In In Proceedings of the Sec-ond Meeting of the North American Chapter ofthe Asssociation for Computational Linguistics,2001.A.
Kennedy, R. Hill, and J. Pynte.
The dundee cor-pus.
In Proceedings of the 12th European confer-ence on eye movement, 2003.Jeff Mitchell, Mirella Lapata, Vera Demberg, andFrank Keller.
Syntactic and semantic factors inprocessing difficulty: An integrated measure.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, Uppsala,Sweden, 2010.M.
Niv.
A psycholinguistically motivated parser forCCG.
In Proceedings of the 32nd Annual Meet-ing of the Association for Computational Linguis-tics (ACL-94), pages 125?132, Las Cruces, NM,1994.W.
M. Soon, H. T. Ng, and D. C. Y. Lim.
A ma-chine learning approach to coreference resolutionof noun phrases.
Computational Linguistics, 27(4):521?544, 2001.M.
J. Spivey and M. K. Tanenhaus.
Syntactic am-biguity resolution in discourse: Modeling the ef-fects of referential context and lexical frequency.Journal of Experimental Psychology: Learning,Memory and Cognition, 24(6):1521?1543, 1998.Kieth E. Stanovich and Richard F. West.
The effectof sentence context on ongoing word recognition:Tests of a two-pricess theory.
Journal of Exper-imental Psychology: Human Perception and Per-formance, 7:658?672, 1981.Adrian Staub and Charles Clifton.
Syntactic predic-tion in language comprehension: Evidence from311either .
.
.or.
Journal of Experimental Psychology:Learning, Memory, and Cognition, 32:425?436,2006.Mark Steedman.
The Syntactic Process.
BradfordBooks, 2001.Erik F. Tjong Kim Sang and Sabine Buchholz.
In-troduction to the conll-2000 shared task: Chunk-ing.
In Proceedings of CoNLL-2000 and LLL-2000, pages 127?132.
Lisbon, Portugal, 2000.Jos J.
A. van Berkum, Colin M. Brown, Pienie Zwit-serlood, Valesca Kooijman, and Peter Hagoort.Anticipating upcoming words in discourse: Evi-dence from erps and reading times.
Journal of Ex-perimental Psychology: Learning, Memory andCognition, 31(3):443?467, 2005.Barton Wright and Merrill F. Garrett.
Lexical deci-sion in sentences: Effects of syntactic structure.Memory and Cognition, 12:31?45, 1984.312
