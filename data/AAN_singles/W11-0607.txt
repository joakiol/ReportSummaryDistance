Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 58?66,Portland, Oregon, June 2011. c?2011 Association for Computational LinguisticsComposing and Updating Verb Argument Expectations:A Distributional Semantic ModelAlessandro LenciUniversity of Pisa, Department of Linguisticsvia S. Maria, 3656126 Pisa (Italy)alessandro.lenci@ling.unipi.itAbstractThe aim of this paper is to present a com-putational model of the dynamic compositionand update of verb argument expectations us-ing Distributional Memory, a state-of-the-artframework for distributional semantics.
Theexperimental results conducted on psycholin-guistic data sets show that the model is ableto successfully predict the changes on the pa-tient argument thematic fit produced by differ-ent types of verb agents.1 IntroductionA number of studies using different experimentalparadigms (priming, self-paced reading, etc.)
haveshown that verbs (eat) activate expectations aboutnouns occurring as their arguments (cheese)(McRaeet al, 1998), and vice versa (McRae et al, 2005).Nouns also activate expectations about other nounsoccurring as co-arguments in the same event (key ?door)(Hare et al, 2009).
These behavioral effectssupport the hypothesis that in the mental lexiconverbs and their arguments are arranged into a webof mutual expectations.
Verb argument expectationsencoded in lexical representations are exploited bysubjects to determine the plausibility of a noun as anargument of a particular verb (thematic fit, or selec-tional preferences in the linguistic literature), whichhas been proved to have important effects on humansentence processing (McRae et al, 1998).In a recent work, Bicknell et al (2010) bring evi-dence suggesting a more complex view of the orga-nization and on-line use of verb argument expecta-tions.
In fact, the expectations about the likely fillersof a given verb argument (e.g., the patient role) de-pend on the way another verb argument (e.g., theagent role) is filled.
For instance, if the agent noun isjournalist, the most likely patient for the verb checkis spelling, while if the agent noun is mechanic, themost likely patient for the same verb is brakes.
Asa consequence, thematic fit judgments are also sen-sitive to the way other roles of the same verb arefilled.
Bicknell et al (2010) show that this fact hasclear consequences for sentence processing, and ar-gue that subjects dynamically compute and updateverb argument expectations and thematic fit duringon-line sentence comprehension, by integrating var-ious types of knowledge about events and their ar-guments.The aim of this paper is to present a computa-tional model of the dynamic composition and up-date of verb argument expectations using Distri-butional Memory (DM)(Baroni and Lenci, 2010),a state-of-the-art Distributional Semantic Model(DSM).
DSMs (aka vector space models) repre-sent word meaning with vectors encoding corpus-based co-occurence statistics, under the assumptionof the so-called Distributional Hypothesis (Millerand Charles, 1991; Sahlgren, 2006): Words occur-ring in similar contexts are also semantically sim-ilar (Landauer and Dumais, 1997; Pado?
and Lap-ata, 2007; Turney and Pantel, 2010).
Thematic fitjudgments have already been successfully modeledwith DSMs (Erk et al, 2010), but to the best of ourknowledge the problem of how thematic fit is dy-namically updated depending on the way other ar-guments are filled has not been addressed yet.
Thecore of our proposal is that Distributional Memory58can be used to represent the subject?s expectationsabout the most likely words co-occurring in givensyntactic role.
We will add to the original Distri-butional Memory framework a model for verb argu-ment expectation composition called ECU, Expec-tation Composition and Update.
Specifically, wewill show how the expectations of an agent-verb pairabout their patient noun argument can be composi-tionally derived from the DM representation of theverb and the DM representation of its agent.
ECUis evaluated on the data set used in Bicknell et al(2010), and the experimental results show that it isable to successfully predict the changes on the pa-tient thematic fit with a verb, depending on differentagent fillers.
More generally, we want to argue thatthe ECU model proposed here can represent a gen-eral and viable approach to address compositionalityin distributional semantics.After reviewing some related work in section 2,we present Distributional Memory (section 3) andits use to model verb-argument composition (section4).
Experiments and evaluation are reported in sec-tion 5.2 Background and related workElman (2009) argues that the information on pre-ferred fillers of one verb argument depends on whatthe filler is of one of the other arguments.
For in-stance, the most likely patient of cut is wood, whenthe agent is lumberjack, but it is meat, when theagent is butcher.
This claim finds an empirical con-firmation in the experiments reported by Bicknell etal.
(2010), in which subjects are presented with sen-tence pairs like the following ones:(1) The journalistAG checked the spellingPA ofhis latest report.
(congruent condition)(2) The mechanicAG checked the spellingPA ofhis latest report.
(incongruent condition)Each pair contains the same verb and patient argu-ment, while differing for the agent argument.
In thecongruent condition, the patient is a preferred argu-ment of the verb, given the agent, e.g.
spelling issomething which is typically checked by a journal-ist.
In the incongruent condition, the patient is not apreferred argument of the verb, given its agent, e.g.spelling is not something that is typically checked bya mechanic, who rather checks brakes, engines, etc.Thematic fit judgments used to determine congru-ent and incongruent agent-verb-patient tuples werecollected in an off-line norming study.
Bicknell etal.
(2010) report that self-paced reading times wereshorter at the word directly following the patient forthe congruent than the incongruent items.
Similarresults were obtained in an event-related brain po-tential (ERP) experiment, in which an N400 effectwas observed immediately at the patient noun in theincongruent condition.
In eye-tracking experiments,Kamide et al (2003) also demonstrated that the the-matic fit of an object depended on the other verb ar-gument fillers.The conclusion drawn by Bicknell et al (2010) isthat verb argument expectations and thematic fit arenot simply stored in the lexicon, but are rather dy-namically updated during sentence comprehension,by integrating various types of knowledge.
In fact,if the verb expectations about an argument role de-pend on the nouns filling its other arguments, thehypothesis that they are compositionally updatedis highly plausible, since, ?it is difficult to envi-sion how the potentially unbounded number of con-texts that might be relevant could be anticipated andstored in the lexicon?
(Elman 2009: 21).Thematic fit judgments have been successfullymodeled in distributional semantics.
Erk et al(2010) propose the Exemplar-Based Model of Se-lectional Preferences, in turn based on Erk (2007).The thematic fit of a noun n as an argument of averb v is measured with the similarity in a vectorspace between n and a set of noun exemplars oc-curring in the same argument role of v. A relatedapproach is adopted by Baroni and Lenci (2010),the main difference being that the thematic fit of nis measured by comparing its vector with a ?proto-type?
vector obtaining by averaging over the vectorsof the most typical arguments of v. In both cases,the distributional measure of thematic fit is shown tobe highly correlated with human plausibility judge-ments.
Their success notwithstanding, these mod-els fall short of accounting for the dynamical andcontext-sensitive nature of thematic fit.
In the nextsection, we will extend the Baroni and Lenci (2010)approach with a model for verb-argument composi-tion, which is able to account for the argument inter-dependency phenomena shown by the experiments59in Bicknell et al(2010).If verb argument expectations are likely to be dy-namically computed integrating knowledge of theverb with information about its fillers, modeling the-matic fit with DSM requires us to address composi-tional representations.
DSMs have mostly addressedsemantic issues related to the representation of thecontent of single words.
However, growing effortshave recently been devoted to the problem of howto build distributional semantic representations ofcomplex expressions (e.g., phrases, sentences, etc.
)by composing the distributional representations oftheir component lexical items (Kintsch, 2001; Clarkand Pulman, 2007; Widdows, 2008; Mitchell andLapata, 2010).
Different proposals to address com-positionality in DSM exist, but the most commonapproach is to model semantic composition as vectorcomposition.
Mitchell and Lapata (2010) systemat-ically explore various vector composition functions(e.g., vector addition, vector product, and other moresophisticated variants thereof), which are used tobuild distributional vector representations for verb-noun and adjective-noun phrases.
The various mod-els for vector composition are then evaluated in aphrase similarity task.Erk and Pado?
(2008) address a partially differ-ent and yet crucial aspect of compositionality, i.e.,the fact that when words are composed, they tendto affect each other?s meanings.
The meaning ofrun in The horse runs is in fact different from itsmeaning in The water runs (Kintsch, 2001).
Erkand Pado?
(2008) claim that words are associatedwith various types of expectations (typical events fornouns, and typical arguments for verbs)(McRae etal., 1998; McRae et al, 2005) that influence eachother when words compose, thereby altering theirmeaning.
They model this context-sensitive com-positionality by distinguishing the lemma vector ofa word w1 (i.e.
its out-of-context representation),from its vector in the context of another word w2.The vector-in-context for w1 is obtained by combin-ing the lemma vector of w1 with the lemma vectorsof the expectations activated byw2.
For instance, thevector-in-context assigned to run in The horse runsis obtained by combining the lemma vector of runwith the lemma vectors of the most typical verbs inwhich horse appears as a subject (e.g.
gallop, trot,etc.).
Like in Mitchell and Lapata (2010), variousfunctions to build vectors in contexts are tested.
Erkand Pado?
(2008) evaluate their model for context-sensitive vector representation to predict verb simi-larity in context (e.g.
slump in the context of shoul-der is more similar to slouch than to decline) and torank paraphrases.Our model draws close inspiration from Erk andPado?
(2008), with which it shares the importance ofverb argument expectations.
However, differentlyfrom them, we want to model how the combinationof a verb with an argument affects its expectationsabout the likely fillers of its other arguments.
WhileErk and Pado?
(2008) test their model on a standardword similarity task (i.e.
they measure the similar-ity between the vector-in-context of a verb with thevector of another ?landmark?
verb), we evaluate ourmodel for compositionality in distributional seman-tics in a thematic fit task.
Indeed, to the best of ourknowledge this is the first time in which the issuesof thematic fit and compositionality in DSMs are ad-dressed together.3 Distributional MemoryDistributional Memory (DM) (Baroni and Lenci,2010) is a framework for distributional semanticsaiming at generalizing over different existing ty-pologies of semantic spaces.
Distributional Memoryrepresents corpus-extracted distributional facts as aweighted tuple structure T , a set of weighted word-link-word tuples ?
?w1, l, w2?,?
?, such that w1 andw2 belong to W , a set of content words (e.g.
nouns,verbs, etc.
), and l belongs to L, a set of syntag-matic co-occurrence links between words in a text(e.g.
syntactic dependencies, lexicalized patterns,etc.).
For instance, the tuple ?
?book,obj, read?,?
?encodes the piece of distributional information thatbook co-occurs with read in the corpus, and objspecifies the type of syntagmatic link between thesewords, i.e.
direct object.
The score ?
is some func-tion of the co-occurrence frequency of the tuple ina corpus and is used to characterize its statisticalsalience.Distributional Memory belongs to the family ofso-called structured DSMs, which take into accountthe crucial role played by syntactic structures inshaping the distributional properties of words.
Toqualify as context of a target item, a word must be60linked to it by some (interesting) lexico-syntactic re-lation, which is also typically used to distinguish thetype of this co-occurrence (Lin, 1998; Pado?
and Lap-ata, 2007).
Differently from other structured DSMs,the tuple structure T is formally represented as a3-way geometrical object, namely a third order la-beled tensor.
A tensor is a multi-way array (Turney,2007; Kolda and Bader, 2009), i.e.
a generalizationof vectors (first order tensors) and matrices (secondorder tensors).
Different semantic spaces are thengenerated ?on demand?
through tensor matriciza-tion, projecting the tuple tensor onto 2-way matrices,whose rows and columns represent semantic spacesto deal with different semantic tasks.For instance, the space W1?LW2 is formed byvectors for words and the dimensions represent theattributes of these words in terms of lexico-syntacticrelations with lexical collocates, such as ?obj, read?,or ?use, pen?.
Consistently, this space is most suit-able to address tasks involving the measurement ofthe ?attributional similarity?
between words (Tur-ney, 2006), such as synonym detection or modelingselectional preferences.
Instead, the space W1W2?Lcontains vectors associated with word pairs, whosedimensions are links between these pairs.
This spaceis thus suitable to address tasks involving the mea-surement of so-called ?relational similarity?
(Tur-ney, 2006), such as analogy detection or relationclassification (cf.
Baroni and Lenci 2010 for moredetails about the Distributional Memory spaces andtasks).
Crucially, these spaces are now alternative?views?
of the same underlying distributional mem-ory formalized in the tensor.
Many semantic tasks(such as analogical similarity, selectional prefer-ences, property extraction, synonym detection, etc.
),which are tackled in the literature with different, of-ten unrelated semantic spaces, are addressed in DMwith the same distributional tensor, harvested onceand for all from the corpus.
This is the reason whyDistributional Memory is claimed to be a generalpurpose resource for semantic modeling.Depending on the selection of the sets W and Land of the scoring function ?, different DM mod-els can be generated.
The Distributional Memoryinstantiation chosen for the experiments reported inthis paper is TypeDM, whose links include lexical-ized dependency paths and lexico-syntactic shallowpatterns, with a scoring function based on patterntype frequency.1 We have chosen TypeDM, be-cause it is the best performing DM model acrossthe various semantic tasks addressed in Baroni andLenci (2010).
The TypeDM tensor contains about130M non-zero tuples automatically extracted froma corpus of about 2.83 billion tokens, obtained byconcatenating the the Web-derived ukWaC corpus(about 1,915 billion tokens),2 a mid-2009 dump ofthe English Wikipedia (about 820 million tokens),3and the British National Corpus (about 95 milliontokens).4 The resulting concatenated corpus wastokenized, POS-tagged and lemmatized with theTreeTagger5 and dependency-parsed with the Malt-Parser.6The TypeDM word set (WTypeDM ) contains30,693 lemmas (20,410 nouns, 5,026 verbs and5,257 adjectives).
These are the top 20,000 most fre-quent nouns and top 5,000 most frequent verbs andadjectives in the corpus, augmented with lemmas invarious standard test sets in distributional semantics,such as the TOEFL and SAT lists.
The TypeDMlink set (LTypeDM ) contains 25,336 direct and in-verse links formed by (partially lexicalized) syntac-tic dependencies and patterns.
This is a sample ofthe links in LTypeDM :?
obj: The journalist is checking his article ?
?article, obj, check??
verb: The journalist is checking his article ?
?journalist, verb, article??
sbj tr: The journalist is checking his article?
?journalist, sbj tr, check??
preposition: I saw a journalist with a pen?
?pen, with, journalist??
such as: ?NOUN such as NOUN?
and ?suchNOUN as NOUN?
: animals such as cats ?
?animal, such as+ns+ns, cat?1The TypeDM tensor is publicly available at http://clic.cimec.unitn.it/dm2http://wacky.sslmit.unibo.it/3http://en.wikipedia.org/wiki/Wikipedia:Database_download4http://www.natcorp.ox.ac.uk5http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/6http://w3.msi.vxu.se/?nivre/research/MaltParser.html61The first two links above are the most relevant onesfor the purposes of the present paper: obj links atransitive verb and its direct object, and verb is alexically underspecified link between a subject nounand a complement noun of the same verb.The scoring function ?
is the Local MutualInformation (LMI) (Evert, 2005) computed on linktype frequency (negative LMI values are raised to 0):LMI = Oijk logOijkEijk(1)Oijk and Eijk are respectively the observed and ex-pected frequency of a triple ?wi, lj , wk?.4 Composing verb argument expectationsIn this section, we address the fact that the infor-mation on preferred fillers of one verb argument de-pends on the filler of its other arguments by propos-ing a model for Expectation Composition and Up-date (ECU), which will then be computationally for-malized with Distributional Memory.ECU relies on the hypothesis that nouns and verbsare linked in a web of mutual expectations.
Verbsare associated with expectations about their likelyarguments, and nouns have expectations about theevents they are involved with and also about othernouns co-occuring in the same events (cf.
section1).
We argue that, when words compose (e.g.
averb and a noun), their expectations are integratedand updated.
Specifically, we focus here on how thecomposition of a verb and its agent argument deter-mines an update of the verb expectations for its pa-tient argument.
Let EXPA(v) be the expectationsof a verb v about its patient arguments, i.e.
a setof nouns likely to occur as verb patients.
For in-stance, EXPA(check) = mistakes, engines, books,etc.
Let EX(nAG) be the expectations about typi-cal events performed and typical entities acted uponby the agent noun.
For instance, EX(mechanic) =mechanics fix cars, mechanics check oil, etc.
ECUis formally defined as follows:EXPA(?nAG, v?)
= f(EX(nAG), EXPA(v)) (2)f is some function for expectation composition andupdate (cf.
below).
ECU assumes that the result ofsemantically composing the verb and its agent ar-gument is an update of the verb expectations aboutits patient argument.
EXPA(?nAG, v?)
are the up-dated expectations of v about its patient arguments,resulting from the composition of its original ex-pectations with the agent?s expectations.
For in-stance, the result of composing check with the agentargument mechanic is a new set of expectationsEXPA(?mechanic, check?)
formed by objects thatare likely checked by mechanics, such as cars, en-gines, wheels, etc.
These updated expectations area function of the typical patients of checking events,and of the typical patients of events performed bymechanics.4.1 Modeling ECU with DistributionalMemoryThe tuple structure of the DM tensor is well suitedto represent the web of mutual expectations in whichlexical items are arranged.
In fact, given a word w,the expectations of w, EX(w), can be defined asthe subset of the DM tensor formed by the tuples?
?w1, l, w2?,?
?, such that w = w1 or w = w2.
Thetuple score ?
determines the statistical salience andtypicality of a particular expectation.To model ECU with TypeDM, we approximatethe patient semantic role with the syntactic depen-dency DM link of obj (cf.
section 3).
The expec-tations about the typical patient arguments of a verbv (EXPA(v)) thus correspond to the set of TypeDMtuples ?ni, obj, v?
: e.g, EXPA(check) = ?mistake,obj, check?, ?engine, obj, check?, etc.
We modelEX(nAG) with the set of DM tuples ?nAG, verb,nj?, which characterize the typical patients (i.e., di-rect objects) of events performed by the agent nounnAG: e.g, EX(mechanic) = ?mechanic, verb,car?, ?mechanic, verb, oil?, ?mechanic, verb,engine?, etc.The expectation composition function f ofequation 2 is modeled as a tensor updating function:f modifies the TypeDM tensor by updating thescores of the relevant subset of tuples.
Followingcurrent compositionality models in distributionalsemantics (cf.
Mitchell and Lapata 2010), we focushere on two alternative versions of f :62check ?journalist, check?
?mechanic, check?site article carpage book tyrewebsite information workbox question pricedetail fact vehiclelink report joblist site systemfile source bikerecord content valueinformation account problemTable 1: Original TypeDM expectations for check andtheir compositional updates obtained with f = PRODUCTSUMFor each tuple ?
?ni, obj, v?, ?i?
?
EXPA(v),?
?ni, obj, v?, ?u?
?
EXPA(?nAG, v?
), and?u =???
?i + ?j if ?
?nAG, verb, nj?, ?j??
EX(nAG) and ni = nj?i otherwise(3)PRODUCTFor each tuple ?
?ni, obj, v?, ?i?
?
EXPA(v),?
?ni, obj, v?, ?u?
?
EXPA(?nAG, v?
), and?u =???
?i ?
?j if ?
?nAG, verb, nj?, ?j??
EX(nAG) and ni = nj0 otherwise(4)The idea underlying both types of tensor updat-ing functions is that the verb expectations about itslikely patients are modified with the score of the tu-ples representing objects that are typical patients ofevents performed by the agent noun.
With SUM,expectation composition is a linear function of thescore of the tuples in EXPA(v) and in EX(nAG):EXPA(?nAG, v?)
contains all the tuples belongingto EXPA(v), but their score is added to the scoreof the tuples in EX(nAG) sharing the same objectnoun.
With the PRODUCT function, the updated ex-pectations only include the tuples inEXPA(v) shar-ing the same objects with a tuple in EX(nAG).
Thescore of these tuples is the product of the originaltuple score, while the score of the other tuples inEXPA(v) is set to 0.Table 1 reports a sample output of the ap-plication of ECU to the TypeDM tensor.
Theleft column contains the objects of the top-scoring tuples of EXPA(check) in the originalTypeDM tensor (ordered by decreasing values of?).
The central column contains the top-scoringnouns in EXPA(?journalist, check?
), composi-tionally derived by updating EXPA(check) withEX(journalist): the nouns are ordered by de-creasing value of ?
modified according to the PROD-UCT composition function.
We can notice that theupdated verb argument expectations include nounsconsistent with what journalists typically check.
Theright column instead contains the top-scoring nounsin EXPA(?mechanic, check?
), derived from up-dating EXPA(check) with EX(mechanic): thecomposition function is still the PRODUCT.
The dif-ference with the central column is striking: the top-scoring nouns in the updated verb argument expec-tations are now related to what mechanics typicallycheck.5 Experiments and evaluationThe ECU model for the compositional update ofverb-argument expectations has been evaluated bymeasuring the thematic fit between an agent-verbpair (?nAG, v?)
and a patient noun argument (nPA)of the same verb.
Thematic fit is computed withthe verb expectations in EXPA(?nAG, v?
), which inturned have been obtained by composing EX(nAG)and EXPA(v) with either of the two functions de-scribed in section 4.
In the following subsections,we illustrate the data sets used for the experiments,the procedure to compute the compositional the-matic fit in TypeDM, and the results of the experi-ments.5.1 Data setsTwo data sets of agent-verb-patient triples fromBicknell et al (2010) have been used to test ECU:?
bicknell.64 - 64 test triples used in the self-paced reading and ERP experiments in Bicknellet al (2010);?
bicknell.100 - 100 test triples, a superset ofbicknell.64.Triples are organized in pairs, each sharing the sameverb, but differing for the agent and patient nouns:?
journalistAG - check - spellingPA?
mechanicAG - check - brakePA63Patients in each triple were produced by 47 sub-jects as the prototypical (congruent) arguments ofthe verbs, given a certain agent.
The patient nounin one triple is incongruent for the other triple withthe same verb: e.g., brake is the incongruent patientfor the mechanicAG - check pair.
The bicknell.100dataset contains all the triples produced in the orig-inal norming study.
The bicknell.64 data set is asubset of the normed triples selected by Bicknell etal.
(2010) after removing test items that were poten-tially problematic for the behavioral experiments.5.2 ProcedureThe thematic fit of a noun nPA as the patient of?nAG, v?
is measured with the cosine between thevector of nPA in the TypeDM W1?LW2 space andthe ?prototype?
vector in the same space built withthe vectors of the top-k expected objects belong-ing to EX(nAG, v).
This is an extension of theapproach to selectional preferences modeling pre-sented in Baroni and Lenci (2010) (in turn inspiredto Erk 2007).
These are the steps used to com-pute the compositional thematic fit in the TypeDMW1?LW2 space:1. we select a set of k of prototypical patientnouns nPA for ?nAG, v?
(in the reported exper-iments we set k = 20).
The selected nounsare the ni in the k tuples ?
?ni, obj, v?, ?u??
EXPA(?nAG, v?)
with the highest score ?.The patient nouns in the datasets are excluded;2. the vectors in the W1?LW2 TypeDM spaceof the selected nouns are normalized andsummed.
The result is a centroid vector rep-resenting an abstract ?patient prototype vector?for ?nAG, v?;3.
for each nAG - v - nPA test triple (e.g., journal-istAG - check - spellingPA), we measure i.)
thecosine between nPA and the ?patient prototypevector?
for the congruent ?nAG, v?
pair, (e.g.,journalistAG - check) and ii.)
the cosine be-tween nPA and the ?patient prototype vector?for the incongruent ?nAG, v?
pair, belonging tothe other triple with the same verb v (e.g., me-chanicAG - check).For each test triple, we score a ?hit?
if nPA has ahigher thematic fit (i.e., cosine) with the congruent?nAG, v?
pair, than with the incongruent one.
For in-stance, if cosine(?journalist, check?, spelling) >cosine(?mechanic, check?, spelling), we score a?hit?, otherwise we score a ?fail?.5.3 ResultsExperiments to model the verb-argument composi-tional thematic fit have been carried out with the twoECU functions, SUM and PRODUCT, each tested onboth datasets.
Model performance has been evalu-ated with ?hit?
accuracy, i.e.
the percentage of ?hits?scored on each data set.
As a baseline, we have sim-ply adopted the random accuracy.
The results of theECU models are reported in table 2.We can notice that when the verb-argument ex-pectations are compositionally updated with thePRODUCT function, the model is able to signifi-cantly outperform the baseline accuracy with bothdata sets.
Conversely, SUM is never able to go be-yond the baseline.
This is remindful of the resultsreported by Erk and Pado?
(2008) and Mitchell andLapata (2010), in which multiplicative vector com-position achieves better performance in the (verb incontext or phrase) similarity tasks than (at least sim-ple) additive functions.
In fact, the advantage of themultiplicative function is that it allows the composi-tion process to highlight the dimensions shared bythe vectors of the component words, thereby em-phasizing context effects.
Something similar canbe argued to explain the results of the current ex-periments.
With PRODUCT the expectations ofEXPA(?nAG, v?)
are a non-linear function of theexpectations about patient nouns shared by v andnAG.
Therefore, the objects that are likely to bechecked by a mechanic depend on the things that areboth typical patients of checking events and typicalpatients of actions performed by a mechanic.
Thisresults in a stronger thematic fit in the congruentcondition than in the incongruent one.We also carried out experiments to investigatewhether the choice of the parameter k (the numberof nouns selected to build the ?prototype patient vec-tor?)
affects the model performance.
However, weobtained no significant difference with respect to thevalues reported in table 2.64data set ECU function accuracy p-valuebicknell.64 SUM 40.62%bicknell.64 PRODUCT 84.37% 3.798e-08 ***bicknell.100 SUM 37.5%bicknell.100 PRODUCT 73% 4.225e-06 ***baseline 50%Table 2: Results of the thematic fit experiments (p-valuescomputed with a ?2 test).6 Conclusions and further directions ofresearchPsycholinguistic evidence has proved that verb ar-gument thematic fit is highly context-sensitive.
Infact, subjects?
sensitivity to the likelihood of a nounas a verb argument strongly depends on the nounsfilling other arguments of the same verb.
Thesedata hint at a dynamic process underlying verb ar-gument expectation and thematic fit computation,resulting from the compositional integration of theverb expectations with those activated by its argu-ments.
In this paper, we have presented ECU, a dis-tributional semantic model for the compositional up-date of verb argument expectations.
ECU has beenapplied to Distributional Memory, a state-of-the-artDistributional Semantic Model, whose core tensorof corpus-derived tuples is particularly suited to rep-resent word expectations.
ECU has been tested suc-cesfully in an experiment to measure the thematicfit between an agent-verb pair (?nAG, v?)
and a pa-tient noun argument (nPA) of the same verb, withthe data set used in the psycholinguistic experimentsreported in Bicknell et al (2010).
The good resultswe have obtained prove that DSMs can provide in-teresting computational models of the compositionalupdate of thematic fit.
Of course, other factors be-sides verb-argument knowledge may also contributeto the context-sensitive nature of thematic fit.
How-ever, it is worth noticing that one of the hypothesesadvanced by Bicknell et al (2010) to explain theirexperimental results is indeed that subjects use theirknowledge of statistical linguistic regularities.
Thisis exactly the type of knowledge that is representedin the Distributional Memory tensor structure and isexploited by ECU.Starting from the experimental results in Bick-nell et al (2010) on sentence on-line processing,in this paper we have addressed the issue of howthe agent of a verb modulates the subjects?
expec-tations about its patients.
On the other hand, there isbroad evidence that the meaning of a verb is predom-inantly modulated by its object.
This suggests thatECU should also be applied to model how the pref-erences about the agent argument are determined bythe choice of the verb object.
We leave this issue forfuture research.Besides being a computational model for thematicfit, we also claim that the ECU approach has amore general relevance for the issue of how to ad-dress compositionality in DSMs.
In fact, let us as-sume that part of the semantic content of a wordconsists of expectations about likely co-occurringwords, which in turn can be modeled with subsetsof a distributional tuple tensor.
We can thereforeclaim that (at least part of) the effect of the semanticcomposition of words is to update their expectationsabout other co-occurring words, like ECU does.
Wehave seen here that this hypothesis finds a nice con-firmation with verb-argument composition.
We be-lieve that an interesting empirical question is to in-vestigate to what extent this hypothesis can be gen-eralized to other cases of compositionality.In the future, we also plan to experiment withother types of expectation composition functions.Moreover we will extend the ECU model to tacklecontext-sensitive effects in the thematic fit with re-spect to other types of verb argument relations, be-sides agent and patient ones.
In fact, Matsuki etal.
(submitted) have reported that patient and instru-ment verb arguments show interdependency effectssimilar to the ones between agents and patients thatwe have addressed in this paper.AcknowledgmentsI am very grateful to Ken McRae for providing thedata set used in this paper.
I also thank Marco Ba-roni - the co-author of Distributional Memory - andthe two anonymous reviewers for their helpful com-ments.
The usual disclaimers apply.65ReferencesMarco Baroni and Alessandro Lenci.
2010.
Distribu-tional Memory : A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.Klinton Bicknell, Jeffrey L. Elman, Mary Hare,Ken McRae, and Marta Kutas.
2010.
Effects of eventknowledge in processing verbal arguments.
Journal ofMemory and Language, 63(4): 489?505.Stephen Clark and Stephen Pulman.
2007.
Combiningsymbolic and distributional models of meaning.
InProceedings of the AAAI Spring Symposium on Quan-tum Interaction: 52?55.Jeffrey L. Elman.
2009.
On the meaning of words anddinosaur bones: Lexical knowledge without a lexicon.Cognitive Science, 33(4): 547?582.Katrin Erk.
2007.
A simple, similarity-based model forselectional preferences.
In Proceedings of ACL: 216?223.Katrin Erk and Sebastian Pado?.
2008.
A structured vec-tor space model for word meaning in context.
In Pro-ceedings of EMNLP 08: 897?906.Katrin Erk, Sebastian Pado?, and Ulrike Pado?.
2010.
Aflexible, corpus-driven model of regular and inverseselectional preferences.
Computational Linguistics,36(4): 723?763.Stefan Evert.
2005.
The Statistics of Word Cooccur-rences.
Ph.D. dissertation, Stuttgart University.Mary Hare, Michael Jones, Caroline Thomson,Sarah Kelly,and Ken McRae.
2009.
Activatingevent knowledge.
Cognition, 111(2): 151?167.Yuki Kamide, Gerry T.M.
Altmann, and Sarah L. Hay-wood.
2003.
The time-course of prediction in incre-mental sentence processing: Evidence from anticipa-tory eye movements.
Journal of Memory and Lan-guage, 49: 133?156.Walter Kintsch.
2001.
Predication.
Cognitive Science,25(2): 173?202.Tamara Kolda and Brett Bader.
2009.
Tensor decomposi-tions and applications.
SIAM Review, 51(3): 455?500.Thomas Landauer and Susan Dumais.
1997.
A solu-tion to Plato?s problem: The latent semantic analysis.theory of acquisition, induction, and representation ofknowledge.
Psychological Review, 104(2): 1?31.Dekang Lin 1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING-ACL: 768?774.Kazunaga Matsuki, Tracy Chow, Mary Hare, Jeffrey L.Elman, Christoph Scheepers, and Ken McRae.
sub-mitted for publication.
Event-based plausibility im-mediately influences on-line language comprehensionKen McRae, Michael J. Spivey-Knowlton, andMichael K.Tanenhaus.
1998.
Modeling the in-fluence of thematic fit (and other constraints) inon-line sentence comprehension Journal of Memoryand Language, 38: 283?312.Ken McRae, Mary Hare, Jeffrey L. Elman, and Todd Fer-retti.
2005.
A basis for generating expectancies forverbs from nouns Memory & Cognition, 33(7): 1174?1184.GeorgeMiller andWalter Charles.
1991.
Contextual cor-relates of semantic similarity Language and CognitiveProcesses, 6: 1?28.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8): 1388?1429.Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2): 161?199.Magnus Sahlgren.
2006.
The Word-Space Model.
Ph.D.dissertation, Stockholm University.Peter D. Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3): 379?416.Peter D. Turney.
2007.
Empirical evaluation of four ten-sor decomposition algorithms.
Technical Report ERB-1152, NRC.Peter D. Turney, Patrick Pantel.
2010.
From frequency tomeaning : Vector space models of semantics.
Journalof Artificial Intelligence Research, 37: 141?188.Dominic Widdows.
2008.
Semantic vector products:Some initial investigations.
In Proceedings of the Sec-ond AAAI Symposium on Quantum Interaction: 1?8.66
