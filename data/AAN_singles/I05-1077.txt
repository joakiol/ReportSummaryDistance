R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
885 ?
895, 2005.?
Springer-Verlag Berlin Heidelberg 2005Tense Tagging for Verbs in Cross-Lingual Context:A Case StudyYang Ye1 and Zhu Zhang21Department of Linguistics, University of Michigan, USA2School of Information and Department of Electrical Engineering and Computer Science,University of Michigan, USAyye@umich.edu, zhuzhang@umich.eduAbstract.
The current work applies Conditional Random Fields to the problemof temporal reference mapping from Chinese text to English text.
The learningalgorithm utilizes a moderate number of linguistic features that are easy and in-expensive to obtain.
We train a tense classifier upon a small amount of manu-ally labeled data.
The evaluation results are promising according to standardmeasures as well as in comparison with a pilot tense annotation experiment in-volving human judges.
Our study exhibits potential value for full-scale machinetranslation systems and other natural language processing tasks in a cross-lingual scenario.1   IntroductionTemporal resolution is a crucial dimension in natural language processing.
The fact thattense does not necessarily exist as a grammatical category in many languages poses achallenge on cross-lingual applications, e.g.
machine translation.
The fact that Englishtenses and Chinese aspect markers align at word level on one hand and sub-word levelon the other hand poses a challenge for temporal reference distinction translation in astatistical machine translation (MT) system.
A word-based alignment algorithm will notbe able to capture the temporal reference distinction when mapping between Chineseand English.
Being able to successfully map the temporal reference distinction in Chi-nese text through disparate features onto the most appropriate tenses for the parallelEnglish text is an important criterion for good translation quality.
Languages have vari-ous levels of time reference distinction representation: some have finer grained tensesthan others, as typological studies have shown.
When facing the unbalanced levels oftemporal reference distinction between a pair of languages, we have to optimize themapping between the two temporal systems through intelligent learning.
Most machinetranslation systems do not have a separate temporal reference resolution module, but ifwe can integrate a special module into them, the temporal reference resolution of thesystem could be corrected accordingly and yield a better translation.
Other than machinetranslation, in cross-lingual question answering (CLQA) with English as the target lan-guage, the ability to successfully formulate queries and maintain the temporal referenceinformation in the original questions is desirable.886 Y. Ye and Z. Zhang2   Related Work2.1   Temporal Reference Modeling in Cross-Lingual ScenarioThe nature of being past, present or future is highly relative and hence the informationcontained in tenses is often referred to as temporal reference distinction.
While thereis a large body of research on temporal reference in formal semantics and logic aswell as in other disciplines of Linguistics, works in cross-lingual temporal referencemapping remain inadequate.Campbell et.
al.
[1] proposed a language-neutral framework for representing se-mantic tense.
This framework is called the Language Neutral Syntax (LNS).
Based onthe observation that grammatical or morphological tenses in different languages donot necessarily mean the same thing, they interpret semantic tense to be largely a rep-resentation of event sequence; their work did not attempt direct and explicit represen-tations of tenses.
The tense node in the LNS tree contains either global tense feature(also known as ?absolute tense?)
or anchorable tense feature (also known as ?relativetense?).
This work treated compound tenses as being represented by primary and sec-ondary tense features.
The tense in an embedded clause is anchored to the tense in thematrix clause.
Campbell?s work attempted neither a strict nor a deep semantic repre-sentation of tenses, but rather a syntactic representation that is language-neutral.
Inaddition, similar to most of its peer works in tense modeling, it only attacked theproblem in a scope of individual sentences.Pustejovsky et.
al.
[2] reported an annotation scheme, the TimeML metadata formarkup of events and their anchoring in documents.
The challenge of human labelingof links among eventualities were discussed to the full fledge in their paper showingthat inter-annotator consistency for links is a hard-to-reach ideal.
The automatic?time-stamping?
was attempted earlier on a small sample of text in an earlier work ofMani [3].
The result was not particularly promising showing need for bigger size oftraining data as well as more predictive features, especially on the discourse level.
Atthe word level, semantic representation of tenses could be approached in various waysdepending on different applications.
None of the previous works were designed par-ticularly for cross-lingual temporal reference distinction mapping and the challengesof this mapping for some language pairs have not received full attention.2.2   Temporal Reference Mapping Between Chinese and EnglishSince temporal reference distinction mapping is of particular interest of cross-lingualnatural language processing tasks, the pilot works for tense classification in Chinesewere naturally motivated by machine translation scenario.
Olsen et.
al.
[4] attackedtense reconstructing for Chinese text in the scenario of Chinese to English MT.
Ontop of the more overt features, their work made use of the telicity information en-coded in the lexicons through the use of Lexical Conceptual Structures (LCS).
Basedon the dichotomy of grammatical aspect and lexical aspect, they proposed that pasttense corresponds to the telic LCS which is either inherently telic or derived telic.While grammatical aspect markings supersede the LCS, in the absence of grammati-cal aspect marking, verbs that have telic LCS are translated into past tense and presenttense otherwise.
This work, while pushing tense reconstruction one step further to-wards the semantics embedded in the events, is subject to the risk of adopting one-to-Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 887one mapping between grammatical aspect markings and tenses hence oversimplifiesthe temporal reference situation in Chinese text.
Additionally, their binary tense tax-onomy is oversimplifying the rich temporal reference system that exists in Chinese.Li et.
al.
[5] proposed a computational model based on machine learning and het-erogeneous collaborative bootstrapping for analyzing temporal relations in a Chinesemultiple-clause sentence.
The core model is a set of rules that map the combinationaleffects of a set of linguistic features to one class of temporal relations for one eventpair.
Their work showed promising results for combining machine learning algorithmsand linguistic features to achieve temporal relation resolution, but did not directly ad-dress cross-lingual temporal reference information mapping.
The nature of the taskthey were attacking is B Series temporal resolution in Mctaggart?s terminology.3   Problem Definition3.1   The Taxonomy of TensesIn the current literature, the taxonomy of tenses typically includes the three basictenses (present, past and future) plus their combination with the progressive and per-fect grammatical aspects, because in English tense and aspect are morphologicallymerged.
This yields a taxonomy of 13 tenses.
We collapse these 13 tenses into a tax-onomy of three classes: present, past and future.
The reason for this collapse is two-fold: linguistically, this three-class taxonomy conforms more strictly with the welldefined tripartite temporal reference distinction [6]; and in practice, only nine tensesoccurred  in  our  data  set:  simple past, simple future, simple present, present perfect,Fig.
1.
Tense Taxonomy1.
PresentE,R,S2.
PastE,R S3.
FutureE,R SPerfect4.E R,S 5.ER,S6.E SR 7.ER S8.ES R9.ES RProgressiveE10.R,SE11.R S12.S RE13.R S E888 Y. Ye and Z. Zhangpresent progressive, past perfect, past progressive, past future and present perfect pro-gressive.
Some tenses are very sparse in the data set yielding little value from thelearning perspective.
Figure 11 shows the tense taxonomy.
In the graph, for each ofthe thirteen tenses, we provide the timeline representation for the configuration of thethree time points under Reichenbachian system.
E stands for the event time, R standsfor the reference time and S stands for the speech time.
It is observed that in terms ofthe relationship between the speech time and the event time, the thirteen tenses couldbe grouped into three categories: tense 1 and tense 5 have the event time overlappingwith the speech time; tense 2, 4, 6, 7, 10, 11 and 13 have the event time being prior tothe speech time; tense 3, 8, 9 and 12 have the event time being later than the speechtime.
These three categories form our collapsed tense taxonomy.3.2   Problem FormulationIn general, the tense tagging problem for verbs can be formalized as a standard classi-fication or labeling problem, in which we try to learn a classifierC: V?Twhere V is the set of verbs (each described by a feature vector), and T is the set ofpossible tense tags (defined by the taxonomy above).This is, however, a somewhat simplistic view of the picture.
Just as temporalevents are usually sequentially correlated, verbs in adjacent linguistic utterances arenot independent.
Therefore the problem should be further formalized as a sequentiallearning problem, where we try tag a sequence of verbs (V1, ?, Vn) with a sequenceof tense tags (t1, ?, tn).
This formalization shares similarities with many other prob-lems inside and outside the computational linguistics community, such as informationextraction from web pages, part-of-speech tagging, protein and DNA sequence analy-sis, and computer intrusion detection.4   Data4.1   Data SummaryWe use 52 pairs of parallel Chinese-English articles from LDC release.
The 52 Chi-nese articles from Xinhua News Service consist of 20626 Chinese characters in totalwith each article containing between about 340 and 400 Chinese characters The Chi-nese documents are in Chinese Treebank format with catalog number LDC2001T11.The parallel English articles are from Multiple-Translation Chinese (MTC) Corpusfrom LDC with catalog number LDC2002T01.
We use the best human translationsout of 10 translation teams2 as our gold-standard parallel English data.1For tense 13, it is controversial whether the event time precedes or succeeds the speech time.(e.g.
for ?I was going to ask him at that time?, it is not clear whether the asking event hashappened by the speech time.)
This graph only represents the authors?
hunch about the tensetaxonomy for this particular project.2Two LDC personnel, one a Chinese-dominant bilingual and the other an English-dominant bi-lingual, performed this ranking.
There was overall agreement on the ranking between the twoand minor discrepancies were resolved through discussion and comparison of additional files.Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 8894.2   Obtaining Tense Tags from the DataThe decision of the granularity level of the data points in the current project is a non-trivial issue.
Recently it has been argued that tense should be regarded as a categoryof the whole sentence, or in logical terms of the whole proposition, since it relates tothe truth value of the proposition as a whole, rather than just some property of theverb.
While we agree with this assertion, in the interest of focusing on our immediategoal of assigning an appropriate tense tag to the parallel verb in the target language,we adopt the more traditional analysis of tense as a category of the verb on the basisof its morphological attachment to the verb.There are a total of 1542 verbs in the 52 Chinese source articles.
We manuallyaligned these verbs in the Chinese source article with their corresponding verbs inEnglish; this yields a subset of 712 verbs out of the 1542 verbs being translated intoEnglish as verbs.
We see a dramatic nominalization (i.e.
verbal expressions in Chi-nese are translated into nominal phrases in English) process in Chinese-to-Englishtranslation through the dramatic contrast between these two numbers.
We excludedthe verbs that are not translated as verbs into the parallel English text.
This exclusionis based on the rationale that another choice of syntactic structure might retain theverbal status in the target English sentence, but the tense of those potential Englishverbs would be left to the joint decision of a set of disparate features.
Those tenses areunknown in our training data.5   Tense Tagging by Learning5.1   Temporal Reference Distinction in Chinese TextAssigning accurate tense tags to the English verbs in Chinese-to-English MachineTranslation is equivalent to understanding temporal reference distinction in the sourceChinese text.
Since there are no morphologically realized tenses in Chinese, the tempo-ral reference distinction in Chinese is encoded in disparate linguistic features.
Figure 2shows how various features in simple Chinese sentences jointly represent the temporalreference distinction information.
For complex sentences with an embedding structure,these  features  will  behave  in  a more complicated way in that the anaphoric relationsSentence-final modalparticleverb + Post-verbal marker +Temporaladverbial+Lexical properties (Vendler, 1967):activity, state, achievement andaccomplishmentFig.
2.
Temporal Structure for a Simple Chinese Sentence890 Y. Ye and Z. Zhangbetween the reference time and speech time hold differently for main verbs and verbsin embedded structure.
While world knowledge is beyond the scope of our computa-tional capacity at this stage, we expect that the various linguistic features will be ableto approximately reconstruct the temporal reference distinction for Chinese verbs.5.2   The Feature SpaceThere are a big variety of heterogeneous features that contribute to the temporal refer-ence semantics of Chinese verbs.
Tenses in English, while manifesting temporal ref-erence distinction, do not always reflect the distinction at the semantic level, as isshown in the sentence ?I will leave when he comes.?
Hornstein [7] accounted for thistype of phenomenon by proposing the Constraints on Derived Tense Structures.Hence the feature space we propose to use consists of the features that contribute tothe semantic level temporal reference construction as well as those contributing to thetense generation from that semantic level.The feature space includes the following 11 features:feature1: whether the current sentence contains a temporal noun phrase, atemporal location phrase or a temporal prepositional phrase;feature2: whether or not the current verb is in quoted speech;feature3: whether the current verb appears in relative clause or sententialcomplement;feature4: whether or not the current verb is in news headlines;feature5: previous word's POS;feature6: current verb's POS, there are three types of verbs in the corpora: theregular verbs (VV); the copula ?shi4?3 (VC) and the verb ?you3?
(VE);feature7: next word's POS;feature8: whether or not the verb is followed by the aspect marker ?le?
;feature9: whether or not the verb is followed by the aspect marker ?zhe?
;feature10: whether or not the verb is followed by the aspect marker ?guo?
;feature11: whether or not the verb is a main verb;The above 11 features include lexical features as well as syntactic features.
Noneof the above features is expensive to obtain.
We aim to show that the temporal refer-ence distinction classe, as a semantic feature of the verb, could be predicted by learn-ing from inexpensive linguistic features that are easily available.
Feature 11 is moti-vated by the observation that tense in English is used to inform the reader (listener) ofwhen the event associating with the main verb occurs with respect to the time of ut-terance while the tense of an embedded verb does not necessarily indicate this rela-tionship directly.
In the current paper, we have a different definition for main verb:any verb that is not in embedded structure is treated as a main verb including thoseverbs appearing in adjunct clauses.5.3   Learning Algorithm: Conditional Random FieldConditional Random Fields (CRF) is a formalism well-suited for learning and predic-tion on sequential data.
It is a probabilistic framework proposed by Lafferty [8] for3The digit at the end of the syllable here indicates the tone.
?Shi4?
means ?be?
and ?you3?means ?have?.Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 891labeling and segmenting structured data, such as sequences, trees and lattices.
Theconditional nature of CRFs relaxes the independence assumptions required by tradi-tional Hidden Markov Models (HMMs); CRFs also avoid the label bias problem ex-hibited by maximum entropy Markov models (MEMMs) and other conditionalMarkov models based on directed graphical models.
CRFs have been shown to per-form well on a number of real-world problems, in particular, NLP problems such asshallow parsing [9], table extraction [10], and named entity recognition [11].For our experiments, we use the off-the-shelf implementation of CRFs provided byMALLET [12].6   Experiments and Evaluation6.1   Preliminary Experiment with Tense Annotation by Human JudgesIn order to evaluate the empirical challenge of tense generation in a Chinese-to-English Machine Translation system, a pilot experiment of tense annotation for Chi-nese text by native judges was carried.
The annotation experiment was carried out on20 news articles from LDC Xinhua News release with category numberLDC2001T11.
The articles were divided into 4 groups with 5 articles in each group.For each group, three native Chinese speakers annotated the tense of the verbs in thearticles.
Prior to annotating the data, the judges underwent brief training during whichthey were asked to read an example of a Chinese sentence for each tense and makesure they understand the examples.
During the annotation, the judges were asked toread whole articles first and then select a tense tag based on the context of each verb.In cases where the judges were unable to decide the tense of a verb, they were in-structed to tag it as ?unknown?.Kappa scores were calculated for the three human judges?
annotation results.Kappa score is the de facto standard for evaluating inter-judge agreement on taggingtasks.
It is defined by the following formula (1), where P(A) is the observed agree-ment among the judges and P(E) is the expected agreement:)(1)()(EPEPAPk?
?=  (1)The annotation was originally carried out on the taxonomy of 13 tenses.
We col-lapsed these 13 tenses into three tenses as discussed in section 3.1.
Table 1 summa-rizes the kappa statistics for the human annotation results after we collapse the tenses:Table 1.
Kappa Scores for Human Tense Annotation for Xinhua News on Collapsed TenseClassesXinhuanews 1Xinhuanews 2Xinhuanews 3Xinhuanews 4Kappa scorefor 3 judges 0.409 0.440 0.317 0.325892 Y. Ye and Z. ZhangThere are different interpretations as to what is a good level of agreement and whatkappa scores are considered low.
But generally, a kappa score of lower than 0.40 fallsinto the lower range of agreement4.
Even if we consider the meta-linguistic nature ofthe task, the kappa scores we observe belong to the poor-fair range of agreement, il-lustrating the challenge of temporal reference mapping across Chinese and English.The difficulty of tense classification demonstrated by these experiments with humanjudges provides an upper bound on the performance of automatic machine classifica-tion.
As challenging a task as it is, tense generation for English verbs in a Chinese-to-English Machine Translation system must address this cross-lingual mapping problemin order to obtain an accurate translation result.6.2   Experimental Setup and Evaluation MetricsIt is conceivable that the granularity of sequences may matter in learning from datawith sequential relationship, and in the context of verb tense tagging, it naturally mapsto the granularity of discourse.
Based on this conjecture, we experiment with two dif-ferent sequential granularities:?
Sentence-level sequence: each sentence is treated as a sequence;?
Paragraph-level sequence: each sentence is treated as a sequence, and there is noboundary between sentences within the paragraph.All results are obtained by 5-fold cross validation.
The classifier?s performance isevaluated against the tenses from the best-ranked human translation parallel Englishtext.To evaluate the performance of classifiers, we measure the standard classificationaccuracy where accuracy is defined as in equation (2):spredictionofnumbertotalspredictioncorrectofnumberaccuracy =  (2)To measure how well the classifier does on each class respectively, we computeprecision, recall, and F-measure, which are defined respectively in equation (3), (4)and (5):hitsofnumbertotalhitscorrectofnumberecision =Pr  (3)hitlistperfectofsizehitscorrectofnumbercall =Re  (4)callecisioncallecisionmeasureFRePrRePr2+??=?
(5)4http://www.childrens-mercy.org/stats/definitions/kappa.htmTense Tagging for Verbs in Cross-Lingual Context: A Case Study 8936.3   Experimental ResultsThe evaluation is carried on the collapsed tense taxonomy that consists of three tenseclasses: present, past and future.
This collapse is motivated by two reasons: linguisti-cally, this collapse reflects the accommodation of the ?gray area?
that exists in the 13-way tense taxonomy; practically, the collapse helps to alleviate the sparse data prob-lem.
Ideally, with a large enough data set that could cover the less-common tenses,the full-fledged tense taxonomy is desirable given that the ?gray area?
could be ana-lyzed and included into the evaluation.
The CRF-based tense classifier yielded theperformance in Table 2 and Table 3:Table 2.
Sentence-level sequence: overall accuracy 58.21%Precision Recall F-measurePresent 42.50% 27.48% 32.07%Past 67.57% 79.55% 72.10%Future 29.66% 25.56% 21.56%Table 3.
Paragraph-level sequence: overall accuracy 58.05%Precision Recall F-measurePresent 38.79% 32.44% 33.96%Past 69.12% 75.72% 71.59%Future 33.16% 30.25% 26.59%An accuracy of around 60% seems not satisfactory if viewed in isolation, but whencontrasted with the kappa score of human tense annotating discussed above, the cur-rent evaluation indicates promising results for our algorithm.
Even though the humanjudges underwent only minimal training, their poor-to-fair kappa scores indicate thatthis is a very hard problem.
Therefore while there is certainly room for improvement,the tagging performance of our algorithm is quite promising.It is noticed that the granularity of sequences does not seem to yield significantlydifferent performance based on the current data.
However, whether this is true in gen-eral remains an open question.7   DiscussionsThere are four important dimensions for any natural language processing tasks:?
The data: ideally, the data used should be as representative as possible of awide range of genres unless the target application is focused on a certain nar-row domain;?
The feature space: ideally, the features should be easily available and havewide coverage over the predicting space of the target problem; the more so-894 Y. Ye and Z. Zhangphisticated and the more expensive the features are, the less we could claim togain from the learning algorithms.?
The learning algorithm: nowadays, various machine-learning algorithms havebeen proposed and applied in different natural language task domains.
A learn-ing algorithm should be chosen to appropriately explore the feature space.?
The evaluation: ideally, evaluation from multiple perspectives is desired to re-solve disagreements.Reflecting upon these dimensions for the current paper, from the data perspective,we focused on news report genre where the temporal thread progression is relativelysimpler than many other genres.
When facing temporal reference classification formore complicated genres, larger amounts of training data would be necessary forlearning a more sophisticated classifier.
Fortunately, the amount of accessible paralleldata is growing and it is always possible to obtain the tense tags for the Chinese verbsautomatically using an off-the-shelf aligning tool although this might introduce a cer-tain amount of noise.As for the choice of the predicting features, the current project does not utilize anylexical semantic features owing to the limited lexical semantic knowledge resourcesfor Chinese.
We expect such knowledge resources, if available, would enhance thefeature vector and boost the classification performance.
Additionally, it is observedthat for a Chinese-to-English MT system, tense generation in English is significantlysubject to the syntactic constraints.
Hence when integrating into a MT system, thecurrent learning algorithm might have opportunity to employ additional features fromother parts of the system, for example, syntactic features for English could be addedto the current feature space.Regarding the choice of learning algorithm, we chose CRFs, a learning algorithmfor sequential data, based on the fact that tenses for verbs in a certain discourse unitare not independent of each other.From the evaluation point of view, the current work evaluates the classifier againstthe tenses from a certain human translation team.
The frequent disagreements amongthe human annotators illustrate the difficulty of constructing a gold standard againstwhich to evaluate the performance of our classifier.
Lastly, measuring BLEU scorechange brought about by integrating the current classifier into a statistical MT systemwould be desirable, such that we can better understand the practical implications ofthis study for MT systems.8   Conclusions and Future WorkThe current work has shown how a moderate set of shallow and inexpensive linguisticfeatures can be combined with a standard machine learning algorithm for learning atense classifier trained on a moderate number of data points, with promising results.
Atense resolution module built upon the current framework could enhance a MT systemwith its temporal reference distinction resolution.Several issues to be explored in future work are the following: First, our currenttraining corpus of Xinhua News articles is rather homogeneous, hence the classifiertrained exclusively on this data set may not be robust when carried over to data fromdifferent source.
This will become particularly important if we want to integrate theTense Tagging for Verbs in Cross-Lingual Context: A Case Study 895current work into a general-domain MT system.
Secondly, related to the homogeneityof our training data, we only explored a limited number of features, while the featurespace could be expanded to include a richer and wider scope.
For example, discoursestructure features have not been explored.
Finally, we are very interested in evaluatingour work against existing MT systems with regard to temporal mapping.References1.
Campbell, R., Aikawa, T., Jiang, Z., Lozano, C., Melero, M and Wu, A.: A Language-Neutral Representation of Temporal Information.
In Proceedings of the Workshop on An-notation Standards for Tempora Information in Natural Language, LREC 2002, Las Pal-mas de Gran Canaria, Spain (2002) 13-21.2.
Pustejovsky, J., Ingria, B., Sauri, R., Castano, J., Littman, J., Gaizauskas, R., Setzer, A.,Katz, G. and Mani, I.: The Specification Language TimeML.
In Mani, I., Pustejovsky, J.,and Gaizauskas, R (eds.).
(2004) The Language of Time: A Reader.
Oxford UniversityPress, to appear3.
Mani, I.: "Recent Developments in Temporal Information Extraction (Draft)", In Nicolov,N., and Mitkov, R. Proceedings of RANLP'03, John Benjamins, to appear.4.
Olson, M., Traum, D., Van-ess Dykema, C. and Weinberg, A.: Implicit Cues for ExplicitGeneration: Using Telicity as a Cue for Tense Structure in a Chinese to English MT System,in proceedings Machine Translation Summit VIII, Santiago de Compostela (Spain) (2001)5.
Li, W., Wong, K. F., Hong, C. and Yuan, C.: Applying Machine Learning to ChineseTemporal Relation Resolution, Proceedings of the 42nd Annual Meeting of the Associa-tion for Computational Linguistics (2004) 582-5886.
Reichenbach, H.: Elements of Symbolic Logic, The Macmillan Company (1947)7.
Dorr, B. J. and Gaasterland, T.: "Constraints on the Generation of Tense, Aspect, and Con-necting Words from Temporal Expressions," Technical Report CS-TR-4391, UMIACS-TR-2002-71, LAMP-TR-091, University of Maryland, College Park, MD (2002)8.
Lafferty, J., McCallum, A. and Pereira, F.: Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proceedings of ICML-01, (2001) 282-2899.
Sha, F. and Pereira, F.: Shallow Parsing with Conditional Random Fields, In Proceedingsof the 2003 Human Language Technology Conference and North American Chapter of theAssociation for Computational Linguistics (HLT/NAACL-03) (2003)10.
Pinto, D., McCallum, A., Lee, X. and Croft, W. B.: Table Extraction Using ConditionalRandom Fields.
In Proceedings of the 26th Annual International ACM SIGIR Conferenceon Research and Development in Information Retrieval (SIGIR 2003) (2003)11.
McCallum, A. and Li, W.: Early Results for Named Entity Recognition with ConditionalRandom Fields, Feature Induction and Web-Enhanced Lexicons.
In Proceedings of theSeventh Conference on Natural Language Learning (CoNLL) (2003)12.
McCallum, A. K.: MALLET: A Machine Learning for Language Toolkithttp://mallet.cs.umass.edu.
(2002)
