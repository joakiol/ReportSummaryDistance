Proceedings of the SIGDIAL 2013 Conference, pages 132?136,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsExploring Features For Localized Detection of Speech Recognition ErrorsEli Pincus and Svetlana Stoyanchev and Julia HirschbergDepartment of Computer Science, Columbia University, USAelipincus@gmail.com & sstoyanchev@cs.columbia.edu& julia@cs.columbia.eduAbstractWe address the problem of localized errordetection in Automatic Speech Recognition(ASR) output to support the generation of tar-geted clarifications in spoken dialogue sys-tems.
Localized error detection finds specificmis-recognized words in a user utterance.
Tar-geted clarifications, in contrast with generic?please repeat/rephrase?
clarifications, targeta specific mis-recognized word in an utter-ance (Stoyanchev et al 2012a) and requireaccurate detection of such words.
We extendand modify work presented in (Stoyanchev etal., 2012b) by experimenting with a new setof features for predicting the likelihood of alocal error in an ASR hypothesis on an un-sifted version of the original dataset.
We im-prove over baseline results, where only ASR-generated features are used, by constructingoptimal feature sets for utterance and wordmis-recognition prediction.
The f-measure foridentifying incorrect utterances improves by2.2% and by 3.9% for identifiying incorrectwords.1 IntroductionSpoken Dialogue Systems typically indicate their lackof understanding of user input by simple requests forrepetition or rephrasing ?
?I?m sorry, I didn?t under-stand you.
?, or ?Can you please repeat??.
Howeverhuman conversational partners generally provide moretargeted clarification requests.
Corpus analysis of hu-man conversations have shown that people are morelikely to indicate what they have understood and whatthey have not understood by producing reprise clar-ification questions (Purver, 2004; Stoyanchev et al2012a), as illustrated in the following exchange whereXXX indicates a word misunderstood by speaker B:A: Do you have any XXX in your bag?B: Do I have any what in my bag?A reprise clarification question targets a specific mis-recognized word and incorporates recognized contextinto a clarification question.We investigate replacing generic please repeat clari-fications with more natural targeted clarifications in au-tomatic spoken systems.
Targeted clarifications allowusers to provide a concise response to a clarificationquestion which is beneficial for spoken systems accept-ing broad vocabulary and flexible syntax.
Examples ofsuch systems include tutoring systems, intelligent as-sistants, and spoken translation systems (Litman andSilliman, 2004; Dzikovska et al 2009; Akbacak et al2009).To enable Spoken Dialogue Systems (SDS) to gen-erate targeted clarification questions, we must first beable to identify mis-recognized words with high accu-racy.
We term such mis-recognition detection localizederror detection.
Accurate distinction between correctlyand incorrectly recognized words is essential to the cre-ation of appropriate targeted clarification questions.In previous research on recognition error detection indialogue systems, researchers have addressed error de-tection at the utterance level (Hirschberg et al 2004;Komatani and Okuno, 2010).
In this paper we presentresults of classification experiments designed to de-tect localized errors within the utterance.
Our base-line results are obtained from a classifier trained onlyon word posterior probabilities generated by an Auto-matic Speech Recognition (ASR) engine.
ASR confi-dence score computation is an active research area, re-lying upon acoustic and lexical collocation informationto compute confidence scores.
We determine whetherimprovement over baseline can be achieved by traininga classifier for utterance and word mis-recognition pre-diction on an expanded feature set that includes lexical,positional, prosodic, semantic, syntactic as well as ad-ditional ASR score features.
All of the features we ex-periment with can be computed from an ASR hypothe-sis without affecting the performance of a SDS materi-ally.
After determining optimal feature sets we experi-ment with one- and two-stage approaches for localizederror detection.
The first simply identifies whether aword is correctly recognized or not.
The second firstclassifies an utterance as incorrect or correct and thenclassifies errors only on utterances labeled incorrect.132This work extends earlier work in which we eval-uated a smaller set of syntactic and prosodic fea-tures (Stoyanchev et al 2012b).
In addition to im-provements implemented in the ASR engine that weuse to produce ASR hypotheses, our current work re-ports results on a larger dataset which includes com-mands to the system and utterances containing disflu-encies.
Here, we propose a framework for localizederror detection that does not rely upon pre-filtering ofthe dataset.In Section 2 we describe our corpus.
In Section 3we discuss our classification experiments.
In Section4 we discuss our results.
In Section 5 we present ourconclusions and discuss future research.2 DataWe conduct our machine learning experiments on theDARPA TRANSTAC corpus (Weiss et al 2008).
TheTRANSTAC corpus is comprised of staged conversa-tions between American military personnel and Ara-bic interviewees utilizing IraqComm speech-to-speechtranslation system (Akbacak et al 2009).
This datawas collected by NIST between 2005 and 2008 in eval-uation exercises.
The dataset contains audio record-ings and manual transcript of English and Arabic utter-ances.
We used SRI?s DynaSpeak (Franco et al 2002)speech recognition system to recognize the English ut-terances and use posterior probabilities from DynaS-peak as our baseline feature.
We create a corpus fromthis dataset that contains over 99% of the English ut-terances.
38 utterances were removed from the dataseteither for lack of actual speech data or errors in refer-ence transcription.
26.2% of our cleaned corpus con-sist of mis-recognized instances and 6.4% of the totalwords in it are incorrectly recognized by DynaSpeak(see Table 1).
We are using an unsifted version of thecorpus used in our previous work (Stoyanchev et al2012b) whose hypotheses were produced with a newversion of the DynaSpeak ASR system.
In our previouswork utterances containing disfluencies and commandsto the system were excluded.
We seek to avoid the cas-cading errors that would follow from implementing a2-step framework for localized error detection wherethe first step is command and disfluency detection andthe second step is localized error detection.
The 1-stepframework also has the advantage of working for all ut-terances including ones that contain commands or dis-fluencies.
Due to these differences, our current resultsare not directly comparable with our previous results.Table 1: Corpus statisticsOverall Correct ASR Incorr ASRAll utts.
3,952 2,914 (73.7%) 1,038 (26.2%)All wrds.
25,333 23,705 (93.6%) 1,628 (6.4%)wrds in err utts 7,888 6,260 (79.4%) 1,628 (20.6%)3 MethodWe analyze how the performance of predicting mis-recognized utterances and words is affected by theuse of lexical, positional, prosodic, semantic, and syn-tactic features in addition to ASR confidence scores.We perform machine learning experiments using theWeka Machine Learning Library to construct a J48decision tree classifier boosted with MultiBoostABmethod (Witten and Eibe, 2005).Baseline confidence features We use ASR posteriorscores extracted from the log files output by Dynaspeakas a baseline feature set in our experiments.
In the utter-ance mis-recognition prediction experiment, we calcu-late the average of the logarithm of the ASR posteriorscores over all words in the hypothesis.
In the wordmis-recognition prediction experiment we use the log-arithm of the posterior score of a given word.Feature selection We run a heuristic feature ex-ploration experiment to identify optimal feature setsfor predicting mis-recognized utterances and mis-recognized words.
We first use a greedy approachadding one feature at a time to the baseline ASR featureset and only keep a feature in the set if it improves F-measure predicting mis-recognition.
We then use an al-ternate greedy approach in which we begin with a fea-ture set composed of all extracted features and proceedto remove one feature at a time and only leave it outof the set if incorrect F-measure improved or remainedthe same with its absence.
The second approach yieldsthe optimal feature sets for both utterance and wordmis-recognition prediction.
Table 2 lists the featuresthat make up these optimal sets.
For incorrect utter-ance prediction, we run a 10-fold cross validation onall utterances.
For incorrect word prediction, we run a10-fold cross validation on all words in mis-recognizedutterances.1 We next describe the features we found tobe useful in prediction and those that did not improveperformance.3.1 Useful FeaturesASR context features We use the logarithm of the pos-terior score of a given word and the average of the log-arithm of the posterior scores for both a given word andits surrounding context.
We use one word context be-fore and after the given word.
We also use the averageof the logarithm of the posterior scores for all words inthe utterance.Lexical features We hypothesize that properties ofwords such as length and frequency are predictive ofwhether a word is correctly recognized.
In particular,noting that words of greater length are often better rec-ognized by an ASR engine, we examine the length, fre-quency, and posterior score of the maximum and min-1Because of the size limitations of our dataset feature se-lection and evaluation are performed on the same dataset.133imum words in an utterance.
For mis-recognized ut-terance prediction, we find that the average length of aword in the utterance are useful features for predictingboth mis-recognized utterances and words.
For mis-recognized word prediction, we find the word length ofthe surrounding words, the current word, and the fre-quency of the longest word in an utterance are useful.We also find that utterance length calculated in words isa useful feature for predicting both utterance and wordmis-recognition.Positional features Motivated by the use of dialoguehistory features in Lopes et al2011), we find that thelocation of the hypothesis relative to the speaker?s firstutterance in the dialogue (utterance location) is a use-ful feature.
Similarly, we obtain improvement from theword index feature, the distance of the word from thefirst word in the utterance.Syntactic POS tags were shown to be helpful in ourprevious work and we find that these tags improve thecurrent results as well.
We obtain these from the Stan-ford POS tagger (et al 2003).
In mis-recognized ut-terance prediction, we use unigram and bigram countsof POS tags as a feature.
For mis-recognized word pre-diction, we use the word?s POS tag as well as the POStag for the surrounding one or two words.We obtain a binary Func/Content feature using afunction word list to distinguish function from contentwords.
The list includes certain adverbs, conjunctions,determiners, modal verbs, primary verbs such as be,prepositions, pronouns, and WP-pronouns.
These tagsalso boost our ability to identify mis-recognized words.The feature Func/Tot ratio is the fraction of functionwords to total words in an ASR hypothesis.
We hy-pothesize that an extreme value of the Func/Tot ratiomay indicate a potential mis-recognition, and it doesimprove both utterance and word mis-recognition pre-diction.3.2 Less Useful FeaturesFeatures we do not find helpful include information as-sociated with the minimum length word in the utter-ance, the fraction of words in an utterance that pos-sess greater length than the average length word in thecorpus, as well as syntactic features such as a depen-dency tag assigned to the word.
Additional unhelp-ful features include prosodic features, such as shimmerand jitter identified by PRAAT (Boersma and Weenink,2013) and pitch and phrase information extracted fromAuToBI(Rosenberg, 2010) software.
Performing a se-mantic role label of our hypotheses with the softwareSENNA (Collobert et al 2011) also did not providehelpful semantic features.System Performance To evaluate performance ofour mis-recognized word classifier, we use the selectedfeatures in 1-stage and 2-stage approaches.
First, wetrain models for utterance and word classification sep-Table 2: FeaturesCat Specific In Optimal UttFeature SetIn OptimalWrd FeatureSetASR Log Post Score Yes (avg of allwrds in utt)Yes (curr wrd)ASR-CTXLog Post Score No Yes (avg of currwrd, curr wrdcontext, avg ofall wrds in utt)Lex Wrd length Yes (avg wrdlength in utt)Yes(curr,prev,next)Max Wrd freq No YesUtt length Yes YesPOS Utt location Yes YesWord Index No Yes (curr)Syn POS Tag Yes (unigramand bigramcount)Yes(curr,prev,next)Func/Cont tag No Yes (curr, prev,next)Func/Tot ratio Yes Yesarately on 80% of the dataset with up-sampling (35%)2of the incorrect instances as well as with the actual dis-tribution of incorrect instances in the corpus (20.6% ut-terances, 6.4% words).
We then test these models onthe remaining 20% of the dataset using the 1-stage and2-stage approach.
In the 1-stage approach we test on20% of the total words in the corpus.
In the 2-stage ap-proach we first test on 20% of the total utterances in thecorpus and then only test on the words in the utteranceslabeled as mis-recognized.4 ResultsNew Feature Experiments Using our newly con-structed utterance feature set we are able to boost incor-rect utterance classification F-measure by 2.2% from.597 to .610 (see Table 3).
The increase in F-measurefor incorrect utterance mis-recognition is due to an in-crease in incorrect utterance recall from .531 to .555.There is a slight decrease in incorrect utterance pre-cision from .682 to .678.
Overall classification accu-racy improves by 2.1% points (absolute) from 81.2%to 83.3%.
Using our newly constructed word featureset we are able to improve incorrect word classificationF-measure by 3.9% from .620 to .644 (see Table 4).
Forincorrect word classification there is an increase in bothmis-recognized word precision and recall; the formerincreasing from .678 to .719 and the latter increasingfrom .571 to .584.
The results for incorrect word clas-sification represent a statistically significant improve-2This percentage was derived empirically.134Table 3: Utterance new feature experiment resultsFeature Correct Incorrect % F-Measure Incorr Imp AccuracyP ?
R ?
F P ?
R ?
F over ASR OnlyASR .845 ?
.912 ?
.877 .682 ?
.531 ?
.597 - 81.2%ASR+LEX+POS+SYN .851 ?
.906 ?
.878 .678 ?
.555 ?
.610 2.2% 83.3%Table 4: Word new feature experiment resultsFeature Correct Incorrect % F-Measure Incorr Imp AccuracyP ?
R ?
F P ?
R ?
F over ASR onlyASR .893 ?
.930 ?
.911 .678 ?
.571 ?
.620 - 85.5%ASR+LEX+POS+SYN .897 ?
.941 ?
.918 .719 ?
.584 ?
.644 3.9% 86.7%Table 5: 1-stage and 2-stage approach resultsExperiment Correct Incorrect AccuracyP ?
R ?
F P ?
R ?
FMaj.
Baseline .94 ?
1.00 ?
.97 - ?
0 ?
- 94%1-stage original .97 ?
.94 ?
.96 .39 ?
.57 ?
.46 92%1-stage (35% upsample) .98 ?
.90 ?
.94 .31 ?
.72 ?
.44 89%2-stage original .96 ?
.98 ?
.97 .51 ?
.34 ?
.41 94%2-stage (35% upsample) .96 ?
.96 ?
.96 .41 ?
.46 ?
.43 93%ment3.
Overall classification accuracy improves by1.2% points (absolute) from 85.5% to 86.7%.1-stage and 2-stage experiments To estimate howwell a dialogue system could perform incorrect wordclassification we run our 1-stage and 2-stage ap-proaches.
The 1-stage approaches (with and withoutup-sampling) are able to achieve higher recall; whilethe 2-stage approaches (with and without up-sampling)are able to achieve higher precision.
The 2-stage re-sult?s higher precision is not surprising given that thisapproach has two chances to filter out correct words?
first with utterance classification and then withword classification.
In our 1-stage approach with up-sampling we are able to identify almost 3/4 (72%) ofthe incorrect words in the corpus (see Table 5).
Inour 2-stage approach without up-sampling we are ableto accurately label just over 1/2 (51%) of the total in-stances we identify as incorrect.
In future work we willexperiment with additional features in order to boostprecision for incorrect word classification to a levelsuitable for use in the construction of reprise clarifi-cation questions.5 ConclusionsWe have presented results of machine learning exper-iments that utilize new features to improve localizeddetection of ASR errors to assist spoken dialogue sys-tem?s production of reprise clarification questions.
Weconducted feature selection experiments to find optimalfeature sets to train classifiers for utterance and wordmis-recognition prediction.
We find that certain lexi-cal, positional, and syntactic features improve classi-fication results over a baseline feature set containingonly ASR posterior score features.
We improve incor-rect F-measure for utterance mis-recognition predictionby 2.2% by adding utterance length, location, fraction3?2test(p < .01)of function words to total words, average word length,and unigram and bigram count to the baseline featureset.
By removing average word length as well as uni-gram and bigram count from this optimal set for utter-ances and adding the current word?s ASR-context fea-tures, length, distance from first word, POS tag, Con-tent/Function tag as well as the length of the current?swords surrounding 1 or 2 word contexts, we improveincorrect F-measure for word mis-recognition predic-tion by 3.9% .
We then employ these feature sets in1-stage and 2-stage approached to obtain our final re-sults.
The 2-stage (no up-sampling) approach yields thehighest precision for detection of word mis-recognitionat 51% while the 1-stage (with 35% up-sampling) ap-proach yields the highest recall for detection of wordmis-recognition at 72%.In order to implement this approach in a workingdialog system we would need to increase our wordmis-recognition precision.
The presence of false pos-itives in mis-recognition prediction (correctly recog-nized words classified as mis-recognized) could leadto unnecessary clarification requests ?
potentially de-railing the dialogue.In future work we will experiment with additionalcorpora as well as with an even more fine-grained ap-proach to local error detection, looking for deletions,insertions, and substitutions.
Potentially, optimal clas-sifiers could be found for each of these types of mis-recognition.
If we are able to identify the type of ASRerror as well as its location, we should be able to im-prove our construction of clarifications questions.We will also continue our investigation of how to usereprise clarification questions in SDS.
Once we havedetected localized ASR errors we must still refine ourstrategies for constructing clarification questions usingthis information.
We are also studying how appropri-ate and inappropriate reprise clarification questions arehandled by SDS users.135ReferencesM.
Akbacak, H. Franco, M. Frandsen, S. Hasan,H.
Jameel, A. Kathol, S. Khadivi, X. Lei, A. Man-dal, S. Mansour, K. Precoda, C. Richey, D. Ver-gyri, W. Wang, M. Yang, and J. Zheng.
2009.
Re-cent advances in sri?s iraqcomm; iraqi arabic-englishspeech-to-speech translation system.
In Acoustics,Speech and Signal Processing, 2009.
ICASSP 2009.IEEE International Conference on, pages 4809?4812.P.
Boersma and D. Weenink.
2013.
Praat: do-ing phonetics by computer [computer program].http://www.fon.hum.uva.nl/praat/.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.M.
O. Dzikovska, C. B. Callaway, E. Farrow, J. D.Moore, N. Steinhauser, and G. Campbell.
2009.Dealing with interpretation errors in tutorial dia-logue.
In Proceedings of the SIGDIAL 2009 Con-ference: The 10th Annual Meeting of the Special In-terest Group on Discourse and Dialogue, SIGDIAL?09, pages 38?45, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.K.
Toutanova et al2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1.H.
Franco, J. Zheng, J. Butzberger, F. Cesari, M. Fr,J.
Arnold, V. Ramana, A. Stolcke R. Gadde, andV.
Abrash.
2002.
Dynaspeak: Sri?s scalable speechrecognizer for embedded and mobile systems.
InProceedings of the second international conferenceon Human Language Technology Research, HLT?02, pages 25?30, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.J.
Hirschberg, D. J. Litman, and M. Swerts.
2004.Prosodic and other cues to speech recognition fail-ures.
Speech Communication, 43(1-2):155?175.K.
Komatani and H. G. Okuno.
2010.
Online errordetection of barge-in utterances by using individualusers utterance histories in spoken dialogue system.D.
J. Litman and S. Silliman.
2004.
Itspoke: anintelligent tutoring spoken dialogue system.
InDemonstration Papers at HLT-NAACL 2004, HLT-NAACL?Demonstrations ?04, pages 5?8, Strouds-burg, PA, USA.J.
Lopes, M. Eskenazi, and I. Trancoso.
2011.
To-wards choosing better primes for spoken dialog sys-tems.
In Proceedings of the IEEE Workshop onAutomatic Speech Recognition and Understanding(ASRU), Keystone, CO.M.
Purver.
2004.
The Theory and Use of ClarificationRequests in Dialogue.
Ph.D. thesis, King?s College,University of London.A.
Rosenberg.
2010.
Autobi - a tool for auto-matic tobi annotation.
In Takao Kobayashi, Kei-kichi Hirose, and Satoshi Nakamura, editors, IN-TERSPEECH, pages 146?149.
ISCA.S.
Stoyanchev, A. Liu, and J. Hirschberg.
2012a.
Clar-ification questions with feedback 2012.
In Interdis-ciplinary Workshop on Feedback Behaviors in Dia-log.S.
Stoyanchev, P. Salletmayr, J. Yang, andJ.
Hirschberg.
2012b.
Localized detection ofspeech recognition errors.
In Spoken LanguageTechnology Workshop (SLT), 2012 IEEE, pages25?30.B.
Weiss, C. Schlenoff, G. Sanders, M. Steves, S. Con-don, J. Phillips, and D. Parvaz.
2008.
Perfor-mance evaluation of speech translation systems.
InNicoletta Calzolari (Conference Chair) et al editor,Proceedings of the Sixth International Conferenceon Language Resources and Evaluation (LREC?08),Marrakech, Morocco, may.
European LanguageResources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.I.
Witten and F. Eibe.
2005.
Data Mining: Practi-cal machine learning tools and techniques.
MorganKaufmann, San Francisco, 2nd edition.136
