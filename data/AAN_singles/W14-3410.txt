Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 68?76,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsStructuring Operative Notes using Active LearningKirk Roberts?National Library of MedicineNational Institutes of HealthBethesda, MD 20894kirk.roberts@nih.govSanda M. HarabagiuHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75080sanda@hlt.utdallas.eduMichael A. SkinnerUniversity of Texas Southwestern Medical CenterChildren?s Medical Center of DallasDallas, TX 75235michael.skinner@childrens.comAbstractWe present an active learning method forplacing the event mentions in an operativenote into a pre-specified event structure.Event mentions are first classified into ac-tion, peripheral action, observation, andreport events.
The actions are further clas-sified into their appropriate location withinthe event structure.
We examine how uti-lizing active learning significantly reducesthe time needed to completely annotate acorpus of 2,820 appendectomy notes.1 IntroductionOperative reports are written or dictated after ev-ery surgical procedure.
They describe the courseof the operation as well as any abnormal find-ings in the surgical process.
Template-based andstructured methods exist for recording the opera-tive note (DeOrio, 2002), and in many cases havebeen shown to increase the completeness of sur-gical information (Park et al., 2010; Gur et al.,2011; Donahoe et al., 2012).
The use of naturallanguage, however, is still preferred for its expres-sive power.
This unstructured information is typi-cally the only vehicle for conveying important de-tails of the procedure, including the surgical in-struments, incision techniques, and laparoscopicmethods employed.The ability to represent and extract the infor-mation found within operative notes would enable?Most of this work was performed while KR was at theUniversity of Texas at Dallas.powerful post-hoc reasoning methods about surgi-cal procedures.
First, the completeness problemmay be alleviated by indicating gaps in the sur-gical narrative.
Second, deep semantic similaritymethods could be used to discover comparable op-erations across surgeons and institutions.
Third,given information on the typical course and find-ings of a procedure, abnormal aspects of an oper-ation could be identified and investigated.
Finally,other secondary use applications would be enabledto study the most effective instruments and tech-niques across large amounts of surgical data.In this paper, we present an initial method foraligning the event mentions within an operativenote to the overall event structure for a procedure.A surgeon with experience in a particular proce-dure first describes the overall event structure.
Asupervised method enhanced by active learning isthen employed to rapidly build an information ex-traction model to classify event mentions into theevent structure.
This active learning paradigm al-lows for rapid prototyping while also taking ad-vantage of the sub-language characteristics of op-erative notes and the common structure of opera-tive notes reporting the same type of procedure.
Afurther goal of this method is to aid in the eval-uation of unsupervised techniques that can auto-matically discover the event structure solely fromthe narratives.
This would enable all the objectivesoutlined above for leveraging the unstructured in-formation within operative notes.This paper presents a first attempt at this ac-tive learning paradigm for structuring appendec-tomy reports.
We intentionally chose a well-understood and relatively simple procedure to en-68sure a straight-forward, largely linear event struc-ture where a large amount of data would be eas-ily available.
Section 3 describes a generic frame-work for surgical event structures and the particu-lar structure chosen for appendectomies.
Section 4details the data used in this study.
Section 5 de-scribes the active learning experiment for filling inthis event structure for operative notes.
Section 6reports the results of this experiment.
Section 7analyzes the method and proposes avenues for fur-ther research.
First, however, we outline the smallamount of previous work in natural language pro-cessing on operative notes.2 Previous WorkAn early tool for processing operative notes wasproposed by Lamiell et al.
(1993).
They developan auditing tool to help enforce completeness inoperative notes.
A syntactic parser converts sen-tences in an operative note into a graph structurethat can be queried to ensure the necessary surgicalelements are present in the narrative.
For appen-dectomies, they could determine whether answerswere specified for questions such as ?What wasthe appendix abnormality??
and ?Was cautery ordrains used??.
Unlike what we propose, they didnot attempt to understand the narrative structure ofthe operative note, only ensure that a small num-ber of important elements were present.
Unfortu-nately, they only tested their rule-based system onfour notes, so it is difficult to evaluate the robust-ness and generalizability of their method.More recently, Wang et al.
(2014) proposed amachine learning (ML) method to extract patient-specific values from operative notes written inChinese.
They specifically extract tumor-relatedinformation from patients with hepatic carcinoma,such as the size/location of the tumor, and whetherthe tumor boundary is clear.
In many ways this issimilar in purpose to Lamiell et al.
(1993) in thesense that there are operation-specific attributes toextract.
However, while the auditing function pri-marily requires knowing whether particular itemswere stated, their method extracts the particularvalues for these items.
Furthermore, they em-ploy an ML-based conditional random field (CRF)trained and tested on 114 operative notes.
The pri-mary difference between the purpose of these twomethods and the purpose of our method lies in theattempt to model all the events that characterize asurgery.
Both the work of Lamiell et al.
(1993)and Wang et al.
(2014) can be used for complete-ness testing, and Wang et al.
(2014) can be usedto find similar patients.
The lack of understand-ing of the event structure, however, prevents thesemethods from identifying similar surgical methodsor unexpected surgical techniques, or from accom-plishing many other secondary use objectives.In a more similar vein to our own approach,Wang et al.
(2012) studies actions (a subset ofevent mentions) within an operative note.
Theynote that various lexico-syntactic constructionscan be used to specify an action (e.g., incised, theincision was carried, made an incision).
Like ourapproach, they observed sentences can be catego-rized into actions, perceptions/reports, and other(though we make this distinction at the event men-tion level).
They adapted the Stanford Parser(Klein and Manning, 2003) with the SpecialistLexicon (Browne et al., 1993) similar to Huanget al.
(2005).
They do not, however, propose anyautomatic system for recognizing and categoriz-ing actions.
Instead, they concentrate on evalu-ating existing resources.
They find that many re-sources, such as UMLS (Lindberg et al., 1993) andFrameNet (Baker et al., 1998) have poor coverageof surgical actions, while Specialist and WordNet(Fellbaum, 1998) have good coverage.A notable limitation of their work is that theyonly studied actions at the sentence level, look-ing at the main verb of the independent clause.We have found in our study that multiple actionscan occur within a sentence, and we thus study ac-tions at the event mention level.
Wang et al.
(2012)noted this shortcoming and provide the followingillustrative examples:?
The patient was taken to the operating roomwhere general anesthesia was administered.?
After the successful induction of spinal anes-thesia, she was placed supine on the operat-ing table.The second event mention in the first sentence(administered) and the first event mention in thesecond sentence (induction) are ignored in Wanget al.
(2012)?s study.
Despite the fact that theyare stated in dependent clauses, these mentionsmay be more semantically important to the narra-tive than the mentions in the independent clauses.This is because a grammatical relation does notnecessarily imply event prominence.
In a furtherstudy, Wang et al.
(2013) work toward the creationof an automatic extraction system by annotating69PropBank (Palmer et al., 2005) style predicate-argument structures on thirty common surgical ac-tions.3 Event Structures in Operative NotesSince operations are considered to be one of theriskier forms of clinical treatment, surgeons fol-low strict procedures that are highly structured andrequire significant training and oversight.
Thus,a surgeon?s description of a particular operationshould be highly similar with a different descrip-tion of the same type of operation, even if writ-ten by a different surgeon at a different hospital.For instance, the two examples below were writ-ten by two different surgeons to describe the eventof controlling the blood supply to the appendix:?
The 35 mm vascular Endo stapler device wasfired across the mesoappendix...?
The meso appendix was divided with electro-cautery...In these two examples, the surgeons use differentlexical forms (fired vs. divided), syntactic forms(mesoappendix to the right or left of the EVENT),different semantic predicate-argument structures(INSTRUMENT-EVENT-ANATOMICALOBJECTvs.
ANATOMICALOBJECT-EVENT-METHOD),and even different surgical techniques (stapling orcautery).
Still, these examples describe the samestep in the operation and thus can be mapped tothe same location in the event structure.In order to recognize the event structure in op-erative notes, we start by specifying an eventstructure to a particular operation (e.g., mastec-tomy, appendectomy, heart transplant) and createa ground-truth structure based on expert knowl-edge.
Our goal is then to normalize the event men-tions within a operative note to the specific surgi-cal actions in the event structure.
While the lex-ical, syntactic, and predicate-argument structuresvary greatly across the surgeons in our data, manyevent descriptions are highly consistent withinnotes written by the same surgeon.
This is es-pecially true of events with little linguistic vari-ability, typically largely procedural but necessaryevents that are not the focus of the surgeon?s de-scription of the operation.
An example of low-variability is the event of placing the patient onthe operating table, as opposed to the event of ma-nipulating the appendix to prepare it for removal.Additionally, while there is considerable lexicalvariation in how an event is mentioned, the ter-minology for event mentions is fairly limited, re-sulting in reasonable similarity between surgeons(e.g., the verbal description used for the dividingof the mesoappendix is typically one of the fol-lowing mentions: fire, staple, divide, separate, re-move).3.1 Event Structure RepresentationOperative notes contain event mentions of manydifferent event classes.
Some classes correspondto actions performed by the surgeon, while oth-ers describe findings, provide reasonings, or dis-cuss interactions with patients or assistants.
Thesedistinctions are necessary to recognizing the eventstructure of an operation, in which we are primar-ily concerned with surgical actions.
We considerthe following event types:?
ACTION: the primary types of events in anoperation.
These typically involve physi-cal actions taken by the surgeon (e.g., cre-ating/closing an incision, dividing tissue), orprocedural events (e.g., anesthesia, transferto recovery).
With limited exceptions, AC-TIONs occur in a strict order and the ithAC-TION can be interpreted as enabling the (i +1)thACTION.?
P ACTION: the peripheral actions that areoptional, do not occur within a specific placein the chain of ACTIONs, and are not consid-ered integral to the event structure.
Examplesinclude stopping unexpected bleeding and re-moving benign cysts un-connected with theoperation.?
OBSERVATION: an event that denotes theact of observing a given state.
OBSERVA-TIONs may lead to ACTION (e.g., the ap-pendix is perforated and therefore needs tobe removed) or P ACTIONs (e.g., a cyst isfound).
They may also be elaborations to pro-vide more details about the surgical methodbeing used.?
REPORT: an event that denotes a verbal in-teraction between the surgeon and a patient,guardian, or assistant (such as obtaining con-sent for an operation).The primary class of events that we are interestedin here are ACTIONs.
Abstractly, one can view atype of operation as a directed graph with specifiedstart and end states.
The nodes denote the events,while the edges denote enablements.
An instanceof an operation then can be represented as some70Figure 1: Graphical representation of a surgicalprocedure with ACTIONs A, B, C , D, E, and F ,OBSERVATION O, and P ACTION G. (a) strictsurgical graph (only actions), (b) surgical graphwith an observation invoking an action, (c) surgi-cal graph with an observation invoking a periph-eral action.path between the start and end nodes.In its simplest form, a surgical graph is com-posed entirely of ACTION nodes (see Figure 1(a)).It is possible to add expected OBSERVATIONsthat might trigger a different ACTION path (Fig-ure 1(b)).
Finally, P ACTIONs can be representedas optional nodes in the surgical graph, which mayor may not be triggered by OBSERVATIONs (Fig-ure 1(c)).
This graphical model is simply a con-ceptual aid to help design the action types.
Themodel currently plays no role in the automaticclassification.
For the remainder of this sectionwe focus on a relatively limited surgical proce-dure that can be interpreted as a linear chain ofACTIONs.3.2 Appendectomy RepresentationAcute appendicitis is a common condition requir-ing surgical management, and is typically treatedby removing the appendix, either laparoscopicallyor by using an open technique.
Appendectomiesare the most commonly performed urgent surgi-cal procedure in the United States.
The procedureis relatively straight-forward, and the steps of theprocedure exhibit little variation between differ-ent surgeons.
The third author (MS), a surgeonwith more than 20 years of experience in pedi-atric surgery, provided the following primary AC-TIONs:?
APP01: transfer patient to operating room?
APP02: place patient on table?
APP03: anesthesia?
APP04: prep?
APP05: drape?
APP06: umbilical incision?
APP07: insert camera/telescope?
APP08: insert other working ports?
APP09: identify appendix?
APP10: dissect appendix away from otherstructures?
APP11: divide blood supply?
APP12: divide appendix from cecum?
APP13: place appendix in a bag?
APP14: remove bag from body?
APP15: close incisions?
APP16: wake up patient?
APP17: transfer patient to post-anesthesiacare unitIn the laparoscopic setting, each of these actions isa necessary part of the operation, and most shouldbe recorded in the operative note.
Additionally,any number of P ACTION, OBSERVATION, andREPORT events may be interspersed.4 DataIn accordance with generally accepted medicalpractice and to comply with requirements of TheJoint Commission, a detailed report of any surgicalprocedure is placed in the medical record within24 hours of the procedure.
These notes include thepreoperative diagnosis, the post-operative diagno-sis, the procedure name, names of surgeon(s) andassistants, anesthetic method, operative findings,complications (if any), estimated blood loss, and adetailed report of the conduct of the procedure.
Toensure accuracy and completeness, such notes aretypically dictated and transcribed shortly after theprocedure by the operating surgeon or one of theassistants.To obtain the procedure notes for this study,The Children?s Medical Center (CMC) of Dal-las electronic medical record (EMR) was queriedfor operative notes whose procedure contained theword ?appendectomy?
(CPT codes 44970, 44950,44960) for a preoperative diagnosis of ?acute ap-pendicitis?
(ICD9 codes 541, 540.0, 540.1).
Atthe time of record acquisition, the CMC EMR hadbeen in operation for about 3 years, and 2,820notes were obtained, having been completed by 12pediatric surgeons.
In this set, there were 2,75771Surgeon Notes Events Wordssurgeon18 291 2,305surgeon2311 16,379 134,748surgeon3143 6,897 57,797surgeon4400 8,940 62,644surgeon5391 15,246 114,684surgeon6307 9,880 77,982surgeon7397 10,908 74,458surgeon834 2,401 20,391surgeon92 100 973surgeon10355 9,987 89,085surgeon11380 14,211 135,215surgeon1292 2,417 19,364Total 2,820 97,657 789,646Table 1: Overview of corpus by surgeon.laparoscopic appendectomies and 63 open proce-dures.
The records were then processed automat-ically to remove any identifying information suchas names, hospital record numbers, and dates.
Forthe purposes of this investigation, only the sur-geon?s name and the detailed procedure note werecollected for further study.
Owing to the completeanonymity of the records, the study received an ex-emption from the University of Texas Southwest-ern Medical Center and CMC Institutional ReviewBoards.
Table 1 contains statistics about the distri-bution of notes by surgeon in our dataset.5 Active Learning FrameworkActive learning is becoming a more and morepopular framework for natural language annota-tion in the biomedical domain (Hahn et al., 2012;Figueroa et al., 2012; Chen et al., 2013a; Chen etal., 2013b).
In an active learning setting, instead ofperforming manual annotation separate from auto-matic system development, an existing ML classi-fier is employed to help choose which examplesto annotate.
Thus, human annotators can focus onexamples that would prove difficult for a classifier,which can dramatically reduce overall annotationtime.
However, active learning is not without pit-falls, notably sampling bias (Dasgupta and Hsu,2008), re-usability (Tomanek et al., 2007), andclass imbalance (Tomanek and Hahn, 2009).
Inour work, the purpose of utilizing an active learn-ing framework is to produce a fully-annotated cor-pus of labeled event mentions in as small a periodof time as possible.
To some extent, the goal offull-annotation alleviates some of the active learn-ing issues discussed above (re-usability and classimbalance), but sampling bias could still lead tosignificantly longer annotation time.Our goal is to (1) distinguish event mentions inone of the four classes introduced in Section 3.1(event type annotation), and (2) further classify ac-tions into their appropriate location in the eventstructure (on this data, appendectomy type anno-tation).
While most active learning methods areused with the intention of only manually labelinga sub-set of the data, our goal is to annotate everyevent mention so that we may ultimately evaluateunsupervised techniques on this data.
Our activelearning experiment thus proceeds in two paral-lel tracks: (i) a traditional active learning processwhere the highest-utility unlabeled event mentionsare classified by a human annotator, and (ii) abatch annotation process where extremely simi-lar, ?easy?
examples are annotated in large groups.Due to small intra-surgeon language variation, andrelatively small inter-surgeon variation due to thelimited terminology, this second process allows usto annotate large numbers of unlabeled examplesat a time.
The batch labeling largely annotates un-labeled examples that would not be selected by theprimary active learning module because they aretoo similar to the already-labeled examples.
Aftera sufficient amount of time being spent in tradi-tional active learning, the batch labeling is usedto annotate until the batches produced are insuf-ficiently similar and/or wrong classifications aremade.
After a sufficent number of annotations aremade with the active learning method, the choiceof when to use the active learning or batch anno-tation method is left to the discretion of the anno-tator.
This back-and-forth is then repeated itera-tively until all the examples are annotated.For both the active learning and batch labelingprocesses, we use a multi-class support vector ma-chine (SVM) using a simple set of features:F1.
Event mention?s lexical form (e.g., identified)F2.
Event mention?s lemma (identify)F3.
Previous words (3-the, 2-appendix, 1-was)F4.
Next words (1-and, 2-found, 3-to, 4-be,5-ruptured)F5.
Whether the event is a gerund (false)Features F3 and F4 were constrained to only returnwords within the sentence.To sample event mentions for the active learner,we combine several sampling techniques to ensurea diversity of samples to label.
This meta-samplerchooses from 4 different samplers with differingprobability p:1.
UNIFORM: Choose (uniformly) an unlabeledinstance (p = 0.1).
Formally, let L be the72set of manually labeled instances.
Then, theprobability of selecting an event eiis:PU(ei) ?
?(ei/?
L)Where ?
(x) is the delta function that returns1 if the condition x is true, and 0 otherwise.Thus, an unlabeled event has an equal prob-ability of being selected as every other unla-beled event.2.
JACCARD: Choose an unlabeled instance bi-ased toward those whose word context is leastsimilar to the labeled instances using Jac-card similarity (p = 0.2).
This sampler pro-motes diversity to help prevent sampling bias.Let Wibe the words in ei?s sentence.
Thenthe probability of selecting an event with theJACCARD sampler is:PJ(ei) ?
?(ei/?
L) minej?L[(1?Wi?WjWi?Wj)?
]Here, ?
is a parameter to give more weight todissimilar sentences (we set ?
= 2).3.
CLASSIFIER: Choose an unlabeled instancebiased toward those the SVM assigned lowconfidence values (p = 0.65).
Formally, letfc(ei) be the confidence assigned by the clas-sifier to event ei.
Then, the probability of se-lecting an event with the CLASSIFIER sam-pler is:PC(ei) ?
?(ei/?
L)(1?
fc(ei))The SVM we use provides confidence valueslargely in the range (-1, 1), but for some veryconfident examples this value can be larger.We therefore constrain the raw confidencevalue fr(ei) and place it within the range [0,1] to achieve the modified confidence fc(ei)above:fc(ei) =max(min(fr(ei), 1),?1) + 12In this way, fc(ei) can be guaranteed to bewithin [0, 1] and can thus be interpreted as aprobability.4.
MISCLASSIFIED: Choose (uniformly) a la-beled instance that the SVM mis-classifiesduring cross-validation (p = 0.05).
Let f(ei)be the classifier?s guess and L(ei) be themanual label for event ei.
Then the proba-bility of selecting an event is:PM(ei) ?
?(ei?
L)?
(f(ei) 6= L(ei))Event Type Precision Recall F1ACTION 0.79 0.90 0.84NOT EVENT 0.75 0.82 0.79OBSERVATION 0.71 0.57 0.63P ACTION 0.66 0.40 0.50REPORT 1.00 0.58 0.73Active Learning Accuracy: 76.4%Batch Annotation Accuracy: 99.5%Table 2: Classification results for event types.
Ex-cept when specified, results are for data annotatedusing the active learning method, while the batchannotation results include all data.The first annotation was made using the UNIFORMsampler.
For every new annotation, the meta-sampler chooses one of the above sampling meth-ods according to the above p values, and that sam-pler selects an example to annotate.
For each se-lected sample, it is first assigned an event type.
If itis assigned as an ACTION, the annotator further as-signs its appropriate action type.
The CLASSIFIERand MISCLASSIFIED samplers alternate betweenthe event type and action type classifiers.
Thesefour samplers were chosen to balance the tra-ditional active learning approach (CLASSIFIER),while trying to prevent classifier bias (UNIFORMand JACCARD), while also allowing mis-labeleddata to be corrected (MISCLASSIFIED).
An eval-uation of the utility of the individual samplers isbeyond the scope of this work.6 ResultsFor event type annotation, two annotators single-annotated 1,014 events with one of five event types(ACTION, P ACTION, OBSERVATION, REPORT,and NOT EVENT).
The classifier?s accuracy onthis data was 75.9% (see Table 2 for a breakdownby event type).
However, the examples were cho-sen because they were very different from the cur-rent labeled set, and thus we would expect them tobe more difficult than a random sampling.
Whenone includes the examples annotated using batchlabeling, the overall accuracy is 99.5%.For action type annotation, the same two anno-tators labeled 626 ACTIONs with one of the 17 ac-tion types (APP01?APP17).
The classifier?s accu-racy on this data was again a relatively low 72.2%(see Table 3 for a breakdown by action type).However, again, these examples were expected tobe difficult for the classifier.
When one includesthe examples annotated using batch labeling, theoverall accuracy is 99.4%.73Action Type Precision Recall F1APP01 0.91 0.77 0.83APP02 1.00 0.67 0.80APP03 1.00 0.67 0.80APP04 0.95 0.95 0.95APP05 1.00 1.00 1.00APP06 0.79 0.72 0.76APP07 0.58 0.58 0.58APP08 0.65 0.75 0.70APP09 0.82 0.93 0.87APP10 0.63 0.73 0.68APP11 0.50 0.50 0.50APP12 0.61 0.56 0.58APP13 0.94 0.94 0.94APP14 0.71 0.73 0.72APP15 0.84 0.79 0.82APP16 0.93 0.81 0.87APP17 0.84 0.89 0.86Active Learning Accuracy: 71.4%Batch Annotation Accuracy: 99.4%Table 3: Classification results for action types.7 DiscussionThe total time allotted for annotation was approxi-mately 12 hours, split between two annotators (thefirst author and a computer science graduate stu-dent).
Prior to annotation, both annotators weregiven a detailed description of an appendectomy,including a video of a procedure to help asso-ciate the actual surgical actions with the narrativedescription.
After annotation, 1,042 event typeswere annotated using the active learning method,90,335 event types were annotated using the batchmethod, and 6,279 remained un-annotated.
Sim-ilarly, 658 action types were annotated using theactive learning method, 35,799 action types wereannotated using the batch method, and 21,151 re-mained un-annotated.
A greater proportion of ac-tions remained un-annotated due to the lower clas-sifier confidence associated with the task.
Eventand action types were annotated in unison, but weestimate during the active learning process it tookabout 25 seconds to annotate each event (both theevent type and the action type if classified as anACTION).
The batch process enabled the annota-tion of an average of 3 event mentions per second.This rapid annotation was made possible bythe repetitive nature of operative notes, especiallywithin an individual surgeon?s notes.
For exam-ple, the following statements were repeated over100 times in our corpus:?
General anesthesia was induced.?
A Foley catheter was placed under sterileconditions.?
The appendix was identified and seemed tobe acutely inflamed.The first example was used by an individual sur-geon in 95% of his/her notes, and only used threetimes by a different surgeon.
In the second exam-ple, the sentence is used in 77% of the surgeon?snotes while only used once by another surgeon.The phrase ?Foley catheter was placed?, however,was used 133 times by other surgeons.
In the con-text of an appendectomy, this action is unambigu-ous, and so only a few annotations are needed torecognize the hundreds of actual occurrences inthe data.
Similarly, with the third example, thephrase ?the appendix was identified?
was used inover 600 operative notes by 10 of the 12 surgeons.After a few manual annotations to achieve suffi-cient classification confidence, the batch processcan identify duplicate or near-duplicate events thatcan be annotated at once, greatly reducing the timeneeded to achieve full annotation.Unfortunately, the most predictable parts of asurgeon?s language are typically the least inter-esting from the perspective of understanding thecritical points in the narrative.
As shown in theexamples above, the highest levels of redundancyare found in the most routine aspects of the op-eration.
The batch annotation, therefore, is quitebiased and the 99% accuracies it achieves cannotbe expected to hold up once the data is fully an-notated.
Conversely, the active learning processspecifically chooses examples that are differentfrom the current labeled set and thus are more dif-ficult to classify.
Active learning is more likely tosample from the ?long tail?
than the most frequentevents and actions, so the performance on the cho-sen sample is certainly a lower bound on the per-formance of a completely annotated data set.
Ifone assumes the remaining un-annotated data willbe of similar difficulty to the data sampled by theactive learner, one could project an overall eventtype accuracy of 97% and an overall action typeaccuracy of 89%.
This furthermore assumes noimprovements are made to the machine learningmethod based on this completed data.One way to estimate the potential bias in batchannotation is by observing the differences in thedistributions of the two data sets.
Figure 2 showsthe total numbers of action types for both theactive learning and batch annotation portions ofthe data.
For the most part, the distributionsare similar.
APP08 (insert other working ports),APP10 (dissect appendix away from other struc-tures), APP11 (divide blood supply), APP12 (di-74Figure 2: Frequencies of action types in the active learning (AL) portion of the data set (left vertical axis)and the batch annotation (BA) portion of the data set (right vertical axis).vide appendix from cecum), and APP14 (removebag from body) are the most under-represented inthe batch annotation data.
This confirms our hy-pothesis that some of the most interesting eventshave the greatest diversity in expression.In Section 2 we noted that a limitation of the an-notation method of Wang et al.
(2012) was that asentence could only have one action.
We largelyovercame this problem by associating a single sur-gical action with an event mention.
This has onenotable limitation, however, as occasionally a sin-gle event mention corresponds to more than oneaction.
In our data, APP11 and APP12 are com-monly expressed together:?
Next, the mesoappendix and appendix isstapledAPP11/APP12and then the appendix isplacedAPP13in an endobag.Here, a coordination (?mesoappendix and ap-pendix?)
is used to associate two events (the sta-pling of the mesoappendix and the stapling ofthe appendix) with the same event mention.
Inthe event extraction literature, this is a well-understood occurrence, as for instance TimeML(Pustejovsky et al., 2003) can represent more thanone event with a single event mention.
In practice,however, few automatic TimeML systems handlesuch phenomena.
Despite this, for our purpose theannotation structure should likely be amended sothat we can account for all the important actionsin the operative note.
This way, gaps in our eventstructure will correspond to actual gaps in the nar-rative (e.g., dividing the blood supply is a criticalstep in an appendectomy and therefore needs to fitwithin the event structure).Finally, the data in our experiment comes froma relatively simple procedure (an appendectomy).It is unclear how well this method would general-ize to more complex operations.
Most likely, thedifficulty will lie in actions that are highly ambigu-ous, such as if more than one incision is made.In this case, richer semantic information will benecessary, such as the spatial argument that indi-cates where a particular event occurs (Roberts etal., 2012).8 ConclusionWith the increasing availability of electronic oper-ative notes, there is a corresponding need for deepanalysis methods to understand the note?s narra-tive structure to enable applications for improvingpatient care.
In this paper, we have presented amethod for recognizing how event mentions in anoperative note fit into the event structure of the ac-tual operation.
We have proposed a generic frame-work for event structures in surgical notes with aspecific event structure for appendectomy opera-tions.
We have described a corpus of 2,820 opera-tive notes of appendectomies performed by 12 sur-geons at a single institution.
With the ultimate goalof fully annotating this data set, which contains al-most 100,000 event mentions, we have shown howan active learning method combined with a batchannotation process can quickly annotate the ma-jority of the corpus.
The method is not withoutits weaknesses, however, and further annotation islikely necessary.Beyond finishing the annotation process, our ul-timate goal is to develop unsupervised methodsfor structuring operative notes.
This would en-able expanding to new surgical procedures withouthuman intervention while also leveraging the in-creasing availability of this information.
We haveshown in this work how operative notes have lin-guistic characteristics that result in parallel struc-tures.
It is our goal to leverage these characteris-tics in developing unsupervised methods.75AcknowledgmentsThe authors would like to thank Sanya Peshwanifor her help in annotating the data.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of ACL/COLING.Allen C. Browne, Alexa T. McCray, and Suresh Srini-vasan.
1993.
The SPECIALIST Lexicon.
Tech-nical Report NLM-LHC-93-01, National Library ofMedicine.Yukun Chen, Hongxin Cao, Qiaozhu Mei, Kai Zheng,and Hua Xu.
2013a.
Applying active learning to su-pervised word sense disambiguation in MEDLINE.J Am Med Inform Assoc, 20:1001?1006.Yukun Chen, Robert Carroll, Eugenia R. McPeek Hinz,Anushi Shah, Anne E. Eyler, Joshua C. Denny, ,and Hua Xu.
2013b.
Applying active learningto high-throughput phenotyping algorithms for elec-tronic health records data.
J Am Med Inform Assoc,20:e253?e259.Sanjoy Dasgupta and Daniel Hsu.
2008.
HierarchicalSampling for Active Learning.
In Proceedings of theInternational Conference on Maching Learning.J.K.
DeOrio.
2002.
Surgical templates for orthopedicoperative reports.
Orthopedics, 25(6):639?642.Laura Donahoe, Sean Bennett, Walley Temple, AndreaHilchie-Pye, Kelly Dabbs, EthelMacIntosh, and Ge-off Porter.
2012.
Completeness of dictated oper-ative reports in breast cancer?the case for synopticreporting.
J Surg Oncol, 106(1):79?83.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Rosa L. Figueroa, Qing Zeng-Treitler, Long H. Ngo,Sergey Goryachev, and Eduardo P. Wiechmann.2012.
Active learning for clinical text classification:is it better than random sampling?
J Am Med InformAssoc, 19:809?816.I.
Gur, D. Gur, and J.A.
Recabaren.
2011.
The com-puterized synoptic operative report: A novel tool insurgical residency education.
Arch Surg, pages 71?74.Udo Hahn, Elena Beisswanger, Ekaterina Buyko, andErik Faessler.
2012.
Active Learning-Based CorpusAnnotation ?
The PATHOJEN Experience.
In Pro-ceedings of the AMIA Symposium, pages 301?310.Yang Huang, Henry J Lowe, Dan Klein, and Rus-sell J Cucina.
2005.
Improved Identification ofNoun Phrases in Clinical Radiology Reports Usinga High-Performance Statistical Natural LanguageParser Augmented with the UMLS Specialist Lex-icon.
J Am Med Inform Assoc, 12:275?285.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of ACL,pages 423?430.James M Lamiell, Zbigniew M Wojcik, and JohnIsaacks.
1993.
Computer Auditing of Surgical Op-erative Reports Written in English.
In Proc AnnuSymp Comput Appl Med Care, pages 269?273.Donald A.B.
Lindberg, Betsy L. Humphreys, andAlexa T. McCray.
1993.
The Unified Medical Lan-guage System.
Methods of Information in Medicine,32(4):281?291.Martha Palmer, Paul Kingsbury, and Daniel Gildea.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.Jason Park, Venu G. Pillarisetty, Murray F. Brennan,and et al.
2010.
Electronic Synoptic Operative Re-porting: Assessing the Reliability and Completenessof Synoptic Reports for Pancreatic Resection.
J AmColl Surgeons, 211(3):308?315.James Pustejovsky, Jose?
Castano, Robert Ingria, RoserSaur?
?, Robert Gaizauskas, Andrea Setzer, GrahamKatz, and Dragomir Radev.
2003.
TimeML: Ro-bust Specification of Event and Temporal Expres-sions in Text.
In Proceedings of the Fifth Interna-tional Workshop on Computational Semantics.Kirk Roberts, Bryan Rink, Sanda M. Harabagiu,Richard H. Scheuermann, Seth Toomay, TravisBrowning, Teresa Bosler, and Ronald Peshock.2012.
A Machine Learning Approach for Identi-fying Anatomical Locations of Actionable Findingsin Radiology Reports.
In Proceedings of the AMIASymposium.Katrin Tomanek and Udo Hahn.
2009.
Reducing ClassImbalance during Active Learning for Named EntityAnnotation.
In Proceedings of KCAP.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007.
An Approach to Text Corpus Construc-tion which Cuts Annotation Costs and MaintainsReusability of Annotated Data.
In Proceedings ofEMNLP/CoNLL, pages 486?495.Yan Wang, Serguei Pakhomov, Nora E. Burkart,James O. Ryan, and Genevieve B. Melton.
2012.A Study of Actions in Operative Notes.
In Proceed-ings of the AMIA Symposium, pages 1431?1440.Yan Wang, Serguei Pakhomov, and Genevieve BMelton.
2013.
Predicate Argument StructureFrames for Modeling Information in OperativeNotes.
In Studies in Health Technology and Infor-matics (MEDINFO), pages 783?787.Hui Wang, Weide Zhang, Qiang Zeng, Zuofeng Li,Kaiyan Feng, and Lei Liu.
2014.
Extracting impor-tant information from Chinese Operation Notes withnatural language processing methods.
J Biomed In-form.76
