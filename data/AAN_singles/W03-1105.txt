Poisson Naive Bayes for Text Classification with Feature WeightingSang-Bum Kim, Hee-Cheol Seo and Hae-Chang RimDept.
of CSE., Korea University5-ka Anamdong, SungPuk-ku, SEOUL 136-701, KOREAsbkim,hcseo,rim@nlp.korea.ac.krAbstractIn this paper, we investigate the use ofmultivariate Poisson model and featureweighting to learn naive Bayes text clas-sifier.
Our new naive Bayes text classifi-cation model assumes that a document isgenerated by a multivariate Poisson modelwhile the previous works consider a doc-ument as a vector of binary term featuresbased on the presence or absence of eachterm.
We also explore the use of featureweighting for the naive Bayes text classifi-cation rather than feature selection, whichis a quite costly process when a smallnumber of the new training documents arecontinuously provided.Experimental results on the two test col-lections indicate that our new model withthe proposed parameter estimation and thefeature weighting technique leads to sub-stantial improvements compared to theunigram language model classifiers thatare known to outperform the original purenaive Bayes text classifiers.1 IntroductionThe naive Bayes classifier has been one of the coreframeworks in the information retrieval research formany years.
Recently, naive Bayes is emerged as aresearch topic itself because it sometimes achievesgood performances on various tasks, compared tomore complex learning algorithms, in spite of thewrong independence assumptions on naive Bayes.Similarly, naive Bayes is also an attractive ap-proach in the text classification task because it issimple enough to be practically implemented evenwith a great number of features.
This simplicity en-ables us to integrate the text classification and filter-ing modules with the existing information retrievalsystems easily.
It is because that the frequency re-lated information stored in the general text retrievalsystems is all the required information in naiveBayes learning.
No further complex generaliza-tion processes are required unlike the other machinelearning methods such as SVM or boosting.
More-over, incremental adaptation using a small numberof new training documents can be performed by justadding or updating frequencies.Several earlier works have extensively studied thenaive Bayes text classification (Lewis, 1992; Lewis,1998; McCallum and Nigam, 1998).
However,their pure naive Bayes classifiers considered a doc-ument as a binary feature vector, and so they can?tutilize the term frequencies in a document, result-ing the poor performances.
For that reason, theunigram language model classifier (or multinomialnaive Bayes text classifier) has been referred as analternative and promising naive Bayes by a num-ber of researchers(McCallum and Nigam, 1998; Du-mais et al, 1998; Yang and Liu, 1999; Nigam etal., 2000).
Although the unigram language modelclassifiers usually outperform the pure naive Bayes,they also have given the disappointing results com-pared to many other statistical learning methodssuch as nearest neighbor classifiers(Yang and Chute,1994), support vector machines(Joachims, 1998),and boosting(Schapire and Singer, 2000), etc.In the real world, an operational text classifica-tion system is usually placed in the environmentwhere the amount of human-annotated training doc-uments is small in spite of the hundreds of thousandsclasses.
Moreover, re-training of the text classifiersis required since a small number of new trainingdocuments are continuously provided.
In this envi-ronment, naive Bayes is probably the most appropri-ate model for the practical systems rather than othercomplex learning models.
Therefore, more inten-sive studies about the naive Bayes text classificationmodel are required.In this paper, we revisit the naive Bayes frame-work, and propose a Poisson naive Bayes model fortext classification with a statistical feature weight-ing method.
Feature weighting has many advan-tages compared to the previous feature selection ap-proaches, especially when the new training exam-ples are continuously provided.
Our new model as-sumes that a document is generated by a multivari-ate Poisson model, and their parameters are esti-mated by weighted averaging of the normalized andsmoothed term frequencies over all the training doc-uments.
Under the assumption, we have tested thefeature weighting approach with three measures: in-formation gain, -statistic, and newly introducedprobability ratio.
With the proposed model and fea-ture weighting techniques, we can get much betterperformance without losing the simplicity and effi-ciency of the naive Bayes model.The remainder of this paper is organized as fol-lows.
The next section presents a naive Bayes frame-work for the text classification briefly.
Section 3describes our new naive Bayes model and the pro-posed technique, and the experimental results arepresented in Section 4.
Finally, we conclude the pa-per by suggesting possible directions for future workin Section 5.2 Naive Bayes Text ClassificationA naive Bayes classifier is a well-known and highlypractical probabilistic classifier, and has been em-ployed in many applications.
It assumes that allattributes of the examples are independent of eachother given the context of the class, that is, an in-dependent assumption.
Several studies show thatnaive Bayes performs surprisingly well in many do-mains(Domingos and Pazzani, 1997) in spite of itswrong independent assumption.In the context of text classification, the probabil-ity of class  given a document is calculated byBayes?
theorem as follows:       (1)Now, if we define a new function , (2)then, Equation (1) can be rewritten as     (3)Using Equation (3), we can get the posterior prob-ability  by obtaining , which is a form oflog ratio similar to the BIM retrieval model(Jones etal., 2000).
It means that the linked independence as-sumption(Cooper et al, 1992), which explains thatthe strong independent assumption can be relaxedin the BIM model, is sufficient for the use of naiveBayes text classification model.With this framework, two representative naiveBayes text classification approaches are well intro-duced in (McCallum and Nigam, 1998).
They desig-nated the pure naive Bayes as multivariate Bernoullimodel, and the unigram language model classifier asmultinomial model.
Instead, we introduce multivari-ate Poisson model to improve the pure naive Bayestext classification in the next section.3 Poisson Naive Bayes Text Classification3.1 OverviewThe Poisson distribution is most commonly used tomodel the number of random occurrences of somephenomenon in a specified unit of space or time,for example, the number of phone calls received bya telephone operator in a 10-minute period.
If wethink that the occurrence of each term is a randomoccurrence in a fixed unit of space (i.e.
a lengthof document) the Poisson distribution is intuitivelysuitable to model the term frequencies in a givendocument.
For that reason, the use of Poisson modelis widely investigated in the IR literature, but it israrely used for the text classification task(Lewis,1998).
It motivates us to adopt the Poisson modelfor learning the naive Bayes text classification.Our model assumes that is generated by multi-variate Poisson model.
In other words, a documentis a random vector which consists of the Poissonrandom variables , and has the value of within-term-frequency for the 	-th term.
Thus, if weassume the independence among the terms in , aprobability of is represented by,    (4)where,   is a vocabulary size, and each   is given by,  (5)where,is the Poisson mean.As a result, the function of Equation (2) isrewritten using Equations (4) and (5) as follows:    (6)where,and is the Poisson mean forin class and class , respectively.The most important issues of this work are as fol-lows: How to decide the frequency representingthe characteristic of document ? How to estimate the model parameterand representing the characteristic of each class?We propose the possible answers in the next subsec-tion.3.2 Parameter EstimationSince is a frequency of a term 	 in a documentwith a fixed length according to the definition ofPoisson distribution, we should normalize the actualterm frequencies in the documents with the differentlength.
In addition, many earlier works in NLP andIR fields recommend that smoothing term frequen-cies is necessary in order to build a more accuratemodel.Thus, we estimate as the normalized andsmoothed frequency of actual term frequency ,represented by,       (7)where  is a laplace smoothing parameter,  is anyhuge value which makes all thein our model aninteger value1, and is the length of .Conceptually,can be regarded as the value es-timated by the following steps : 1) Add  of all  terms to the document , 2) Scale up to whosetotal length is  without changing the proportion offrequency for each term, 3) Countin .Then, Poisson mean, which represents an aver-age number of occurrence ofin the documents be-longing to class , is estimated using the normalizedand smoothedvalues over the training documentsas follows: (8)where is the set of training documents belongingto class , and 2 is the interpolation of theuniform probability and the probability proportionalto the length of the document, and the interpolationis calculated as follows:     (9)Simple averaging of, the case of =1, seems tobe correct to estimate.
However, the statistics1Since is a value of random variable representingthe frequency in our Poisson distribution, we multiply the nor-malized frequency with some unnatural constant  to make integer value.
However,  is dropped in the final induced func-tion.2We use the notation  for the distribution definedonly in the training documents, to distinguish it from the no-tation  used in the Section 2.from the long documents can be more reliable thanthose in the short documents, hence we try to inter-polate between the two different probabilities withthe parameter  ranging from 0 to 1.
Consequently,is a weighted average over all training documentsbelonging to the class , and for the class  can beestimated in the same manner.3.3 Feature WeightingFeature selection is often performed as a preprocess-ing step for the purpose of both reducing the fea-ture space and improving the classification perfor-mance.
Text classifiers are then trained with variousmachine learning algorithms in the resulting featurespace.
(Yang and Pedersen, 1997) investigated somemeasures to select useful term features includingmutual information(MI), information gain(IG), and-statistics(CHI), etc.
On the contrary, (Joachims,1998) claimed that there is no useless term features,and it is preferable to use all term features.
It isclear that learning and classification become veryefficient when the feature space is considerably re-duced.
However, there is no definite conclusionabout the contribution of feature selection to im-prove overall performances of the text classificationsystems.
It may considerably depend on the em-ployed learning algorithm.
We believe that properexternal feature selection or weighting is required toimprove the performances of naive Bayes since thenaive Bayes has no framework of the discriminativeoptimizing process in itself.
Of the two possible ap-proaches, feature selection is very inefficient in casethat the additional training documents are providedcontinuously.
It is because the feature set shouldbe redefined according to the modified term statis-tics in the new training document set, and classifiersshould be trained again with this new feature set.
Forthat reason, we prefer to use feature weighting toimprove naive Bayes rather than feature selection.With the feature weighting method, our is rede-fined as follows: W  (10)where, is the weight of feature for the class ,and Wis the normalization factor, that is,.In our work, three measures are used to weightTable 1: Two-way contingency tablepresence ofabsence oflabeled as  a bnot labeled as  c deach term feature: information gain, -statisticsand probability ratio.
Information gain (or aver-age mutual information) is an information-theoreticmeasure defined by the amount of reduced uncer-tainty given a piece of information.
We use the in-formation gain value as the weight of each term forthe class , and the value is calculated using a docu-ment event model as follows:    (11)   where, for example,  is the number of docu-ments belonging to the class  divided by the totalnumber of documents, and   is the number ofdocuments without the term  divided by the totalnumber of documents, etc.Second measure we used is - statistics devel-oped for the statistical test of the hypothesis.
In thetext classification, given a two-way contingency ta-ble for each termand class  as represented in Ta-ble 1, is calculated as follows:          (12)where, ,, and  indicate the number of documentsfor each cell in the above contingency table.
(Yang and Pedersen, 1997) compared the variousfeature selection methods, and concluded that thesetwo measures are most effective for their kNN andLLSF classification models.Finally, we introduce a new measure - probabilityratio.
Probability ratio is defined by,(13)This measure calculates the sum of the ratio of twoclass-conditional probabilities from each class andits reciprocal.
The former term and the latter termare representing the degree of predicting positiveand negative class respectively.
The weight usingthis measure always has a positive value higher than2.We have conducted the experiments with thesethree measures for the feature weighting test, andthe results are given in Section 4.3.4 Implementation IssuesBy a couple of simple arithmetic operations, our fi-nal function can be rewritten as follows:W   (14)where,       In this equation,and are just weighted av-erage of  -dropped, that is,  .
,andare the class-specific constants, and  is a con-stant over all the classes and documents.
If the class is fixed, ,and  can be dropped, and theranking function is defined as follows:  (15)When we use this ranking function , the calcu-lation of the exact posterior probability  pre-sented in Section 2 becomes impossible.
However,it is trivial since most of IR systems do not have in-terest on exact posterior probability.
In addition, allthe parameters in our model is guaranteed to be cal-culated by the incremental way.0.70.710.720.730.740.750.760.770.780.790 0.2 0.4 0.6 0.8 1MicroF1alphaReuters21578PNBUnigram ModelFigure 1: MicroF1 Performances for Reuters21578according to interpolation parameter  for estimat-ingand (without feature weighting)4 Experimental Result4.1 Data and Evaluation MeasureOur experiments were performed on the twodatasets: Reuters21578 and KoreanNews2002 col-lection.
Reuters21578 collection is the most widelyused benchmark dataset for the text categorizationresearch.
We have used ?ModApte?
split version,which consists of 9603 training documents and 3299test documents.
There are 90 categories, and eachdocument has one or more of the categories.We have built another benchmark collection - Ko-reanNews2002 collection.
KoreanNews2002 collec-tion is composed of 15,000 news articles publishedduring the year of 2002.
The articles are collectedfrom a number of Korean news portal websites, andeach article is labeled with exactly one of the 46classes.
All the documents have date stamps at-tached and have been ordered according to their datestamps.
With this date order, we divided them intothe former 10,000 documents for training and thelatter 5,000 documents for testing.The performances are evaluated using popular F1measure, and the F1 values for each class are micro-averaged(MicroF1) and macro-averaged(MacroF1)to examine the general classification performances.4.2 Proposed Model : PNB (vs. UM)Figure 1 shows the performances of our new modelnamed Poisson naive Bayes(PNB) classifiers ac-Table 2: Performances of UM and PNB on theReuters21578 collectionUM PNB(min) PNB(max)MicroF1 0.7212 0.7644 0.7706MacroF1 0.3214 0.4227 0.4358Table 3: Performances of UM and PNB on the Ko-reanNews2002 collectionUM PNB(min) PNB(max)MicroF1 0.6502 0.7031 0.7094MacroF1 0.5208 0.5859 0.5949cording to the interpolation parameter  for estimat-ing Poisson meanand .
The baseline methodis a unigram model classifier (UM) which is alsoreferred to multinomial naive Bayes classifier de-scribed in (McCallum and Nigam, 1998).
Our pro-posed PNB clearly outperforms the UM.Although there is no significant difference of Mi-croF1 values among the various  values, the F1value of each class is considerably affected by the values.
Figure 2 presents the fluctuations of theF1 values for 4 classes in Reuters21578 collection.From this result, we can assume that there is noglobal optimal value of , but each class has its ownoptimal .
In our experiments, many of the classeshave the highest F1 value when  is about 0.8 or0.9 except some classes such as corn class whichshows the highest F1 value at  .
Similarresults are obtained in the KoreanNews2002 collec-tions.Table 2 and 3 shows the MicroF1 and MacroF1values of the unigram model classifiers and ourPNB on the two collections, where PNB(min) andPNB(max) are the highest and lowest values at dif-ferent .
In any cases, PNB is superior to UM.4.3 Feature Weighting : PNB-IG,CHI,PrRWe have fixed the interpolation parameter  at 0.8,and evaluated the following feature weighting meth-ods: PNB-IG with information gain, PNB-CHI with-statistic, and PNB-PrR with probability ratio.
Inthese experiments, some important behaviors of fea-ture weighted PNB classifiers are observed from theresults.
In order to explain the phenomenon, wehave grouped the classes into the bins according to0.840.860.880.90.920.940.960 0.2 0.4 0.6 0.8 1F1alphaReuters21578 - acqPNBUnigram Model0.660.680.70.720.740.760.780 0.2 0.4 0.6 0.8 1F1alphaReuters21578 - grainPNBUnigram Model0.60.620.640.660.680.70.720 0.2 0.4 0.6 0.8 1F1alphaReuters21578 - interestPNBUnigram Model0.440.460.480.50.520.540.560 0.2 0.4 0.6 0.8 1F1alphaReuters21578 - cornPNBUnigram ModelFigure 2: Performances for 4 categories inReuters21578 according to interpolation parameter for estimatingand (without feature weighting)0.20.30.40.50.60.70.80.9110 100 1000MacroF1for eachbeanavg # of train doc for each beanReuters21578PNBPNB-IGPNB-CHIPNB-PrR0.40.450.50.550.60.650.70.750.80.850.90 100 200 300 400 500 600 700MacroF1for eachbeanAvg # of train docs for each beanKoreanNewsPNBPNB-IGPNB-CHIPNB-PrRFigure 3: MacroF1 performances of the bins on Re-tures21578 and KoreanNews2002the number of training documents for each class.
5bins are generated in both Reuters21578 and Kore-anNews2002 collection.The different average F1 performance of each binis shown in Figure 3.
The clear observation fromthis result is that feature weighting is highly effec-tive in the bins of the classes with a small num-ber of training documents, but hardly contributesthe performances for the bins of the classes withsufficiently many training documents.
In the binswith enough training documents, simple PNB clas-sifiers show the similar performances to the PNBwith feature weighting methods.
This tendency ismore clearly captured in the Reuters21578 collec-tion, where a third of the classes have fewer than10 training documents.
In contrast, two thirds ofthe classes in the KoreanNews2002 collection havemore than a hundred of training documents.Among the feature weighting methods, PNB-PrR performs stably than PNB-IG and PNB-CHI.PNB-IG or PNB-CHI somewhat degrades the per-formance in the classes with the large number oftraining documents, while PNB-PrR maintains thegood performances in those classes on both of thecollections.
On the other hand, PNB-IG and PNB-CHI considerably improve the performances in therare categories though the improvement is some-what different from the two collections.
For ex-ample, PNB-CHI significantly improves the simplePNB on the Reuters21578 collection while PNB-IG is very effective on the KoreanNews2002 collec-tion.
Thus, we can realize that the proper featureweighting method depends on the characteristics ofthe collection, and different feature weighting strate-gies should be adopted to improve the naive Bayestext classification.From these observations, we tested another clas-sifier PNB which employ different feature weight-ing method for each bin to obtain the near opti-mal performances.
Table 4 and 5 show the sum-mary of the performances including PNB on theboth collections.
Our proposed model with featureweighting methods are very effective compared tothe baseline UM method.
Moreover, the perfor-mance of bin-optimized PNB in Reuters21578 col-lection shows that Poisson naive Bayes with featureweighting methods can achieve the state-of-the-artperformances achieved by SVM or kNN which arereported in (Yang and Liu, 1999; Joachims, 1998).5 Conclusion and Future WorkIn this paper, we propose a Poisson naive Bayes textclassification model with feature weighting.
Ournew model uses the normalized and smoothed termfrequencies for each document, and Poisson param-eters are calculated by weighted averaging the fre-quencies over all training documents.
Experimentalresults show that the proposed model is quite use-ful to build probabilistic text classification systemswithout requiring any extra cost compared to thetraditional simple naive Bayes or unigram languagemodel classifiers.Further improvement is achieved by a featureweighting technique.
In our experiments, threemeasures including chi-square statistics, informa-tion gain, and newly introduced probability ratio areTable 4: Summary of the performances on the Reuters21578 collectionUM PNB PNB-IG PNB-CHI PNB-PrR PNBMicroF1 0.7212 0.7690 0.7971 0.8167 0.8190(+13.56%) 0.8341MacroF1 0.3414 0.4307 0.5800 0.6601(+93.35%) 0.5899 0.6645Table 5: Summary of the performances on the KoreanNews2002 collectionUM PNB PNB-IG PNB-CHI PNB-PrR PNBMicroF1 0.6502 0.7056 0.7114 0.7122 0.7409(+13.95%) 0.7438MacroF1 0.5208 0.5906 0.6305(+21.06%) 0.5748 0.6119 0.6662adopted to weigh each term feature.
The resultsshow that feature weighting considerably improvesthe performances for the classes with a small num-ber of training documents, but not for the classeswith the sufficient training documents.
Probabilityratio also performs well, especially in the classeswith the great number of training documents whereother feature weighting methods show the unsatis-factory performances.For the future work, we will try to developsome automatic methods of selecting proper featureweighting measures and determining the interpola-tion parameters for the different classes.
Further-more, we will explore applications of our approachin other tasks such as adaptive filtering and relevancefeedback.ReferencesWilliam S. Cooper, Fredric C. Gey, and Daniel P. Dabney.1992.
Probabilistic retrieval based on staged logsiticregression.
Proceedings of SIGIR-92, 15th ACM In-ternational Conference on Research and Developmentin Information Retrieval, pages 198?210.Pedro Domingos and Michael J. Pazzani.
1997.
On theoptimality of the simple bayesian classifier under zero-one loss.
Machine Learning, 29(2/3):103?130.Susan Dumais, John Plat, David Heckerman, andMehranSahami.
1998.
Inductive learning algorithms andrepresentation for text categorization.
Proceedings ofCIKM-98, 7th ACM International Conference on In-formation and Knowledge Management, pages 148?155.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: learning with many relevant fea-tures.
Proceedings of ECML-98, 10th European Con-ference on Machine Learning, pages 137?142.Karen Sparck Jones, Steve Walker, and Stephen E.Robertson.
2000.
A probabilistic model of informa-tion retrieval: development and comparative experi-ments - part 1.
Information Processing and Manage-ment, 36(6):779?808.David D. Lewis.
1992.
Representation and learningin information retrieval.
Ph.D. thesis, Departmentof Computer Science, University of Massachusetts,Amherst, US.David D. Lewis.
1998.
Naive (Bayes) at forty: The in-dependence assumption in information retrieval.
Pro-ceedings of ECML-98, 10th European Conference onMachine Learning, pages 4?15.Andrew K. McCallum and Kamal Nigam.
1998.
Em-ploying EM in pool-based active learning for text clas-sification.
Proceedings of ICML-98, 15th Interna-tional Conference on Machine Learning, pages 350?358.Kamal Nigam, Andrew K. McCallum, Sebastian Thrun,and Tom M. Mitchell.
2000.
Text classification fromlabeled and unlabeled documents using EM.
MachineLearning, 39(2/3):103?134.Robert E. Schapire and Yoram Singer.
2000.
BOOSTEX-TER: a boosting-based system for text categorization.Machine Learning, 39(2/3):135?168.Yiming Yang and Christopher G. Chute.
1994.
Anexample-based mapping method for text categoriza-tion and retrieval.
ACM Transactions on InformationSystems, 12(3):252?277.Yiming Yang and Xin Liu.
1999.
A re-examination oftext categorization methods.
Proceedings of SIGIR-99, 22nd ACM International Conference on Researchand Development in Information Retrieval, pages 42?49.Yiming Yang and Jan O. Pedersen.
1997.
A comparativestudy on feature selection in text categorization.
Pro-ceedings of ICML-97, 14th International Conferenceon Machine Learning, pages 412?420.
