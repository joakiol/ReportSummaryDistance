Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 54?62,Dublin, Ireland, August 23-24, 2014.SemEval-2014 Task 7: Analysis of Clinical TextSameer Pradhan1, No?emie Elhadad2, Wendy Chapman3,Suresh Manandhar4and Guergana Savova11Harvard University, Boston, MA,2Columbia University, New York, NY3University of Utah, Salt Lake City, UT,4University of York, York, UK{sameer.pradhan,guergana.savova}@childrens.harvard.edu, noemie.elhadad@columbia.edu,wendy.chapman@utah.edu, suresh@cs.york.ac.ukAbstractThis paper describes the SemEval-2014,Task 7 on the Analysis of Clinical Textand presents the evaluation results.
It fo-cused on two subtasks: (i) identification(Task A) and (ii) normalization (Task B)of diseases and disorders in clinical reportsas annotated in the Shared Annotated Re-sources (ShARe)1corpus.
This task wasa follow-up to the ShARe/CLEF eHealth2013 shared task, subtasks 1a and 1b,2butusing a larger test set.
A total of 21 teamscompeted in Task A, and 18 of those alsoparticipated in Task B.
For Task A, thebest system had a strict F1-score of 81.3,with a precision of 84.3 and recall of 78.6.For Task B, the same group had the beststrict accuracy of 74.1.
The organizershave made the text corpora, annotations,and evaluation tools available for future re-search and development at the shared taskwebsite.31 IntroductionA large amount of very useful information?bothfor medical researchers and patients?is presentin the form of unstructured text within the clin-ical notes and discharge summaries that form apatient?s medical history.
Adapting and extend-ing natural language processing (NLP) techniquesto mine this information can open doors to bet-ter, novel, clinical studies on one hand, and helppatients understand the contents of their clini-cal records on the other.
Organization of this1http://share.healthnlp.org2https://sites.google.com/site/shareclefehealth/evaluation3http://alt.qcri.org/semeval2014/task7/This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/shared task helps establish state-of-the-art bench-marks and paves the way for further explorations.It tackles two important sub-problems in NLP?named entity recognition and word sense disam-biguation.
Neither of these problems are new toNLP.
Research in general-domain NLP goes backto about two decades.
For an overview of thedevelopment in the field through roughly 2009,we refer the refer to Nadeau and Sekine (2007).NLP has also penetrated the field of bimedicalinformatics and has been particularly focused onbiomedical literature for over the past decade.
Ad-vances in that sub-field has also been documentedin surveys such as one by Leaman and Gonza-lez (2008).
Word sense disambiguation also hasa long history in the general NLP domain (Nav-igli, 2009).
In spite of word sense annotations inthe biomedical literature, recent work by Savovaet al.
(2008) highlights the importance of annotat-ing them in clinical notes.
This is true for manyother clinical and linguistic phenomena as the var-ious characteristics of the clinical narrative presenta unique challenge to NLP.
Recently various ini-tiatives have led to annotated corpora for clini-cal NLP research.
Probably the first comprehen-sive annotation performed on a clinical corporawas by Roberts et al.
(2009), but unfortunatelythat corpus is not publicly available owing to pri-vacy regulations.
The i2b2 initiative4challengeshave focused on such topics as concept recog-nition (Uzuner et al., 2011), coreference resolu-tion (Uzuner et al., 2012), temporal relations (Sunet al., 2013) and their datasets are available to thecommunity.
More recently, the Shared AnnotatedResources (ShARe)1project has created a corpusannotated with disease/disorder mentions in clini-cal notes as well as normalized them to a conceptunique identifier (CUI) within the SNOMED-CTsubset of the Unified Medical Language System54http://www.i2b2.org5https://uts.nlm.nih.gov/home.html54Train Development TestNotes 199 99 133Words 94K 88K 153KDisorder mentions 5,816 5,351 7,998CUI-less mentions 1,639 (28%) 1,750 (32%) 1,930 (24%)CUI-ied mentions 4,117 (72%) 3,601 (67%) 6,068 (76%)Contiguous mentions 5,165 (89%) 4,912 (92%) 7,374 (92%)Discontiguous mentions 651 (11%) 439 (8%) 6,24 (8%)Table 1: Distribution of data in terms of notes and disorder mentions across the training, developmentand test sets.
The disorders are further split according to two criteria ?
whether they map to a CUI orwhether they are contiguous.
(UMLS) (Campbell et al., 1998).
The task of nor-malization is a combination of word/phrase sensedisambiguation and semantic similarity where aphrase is mapped to a unique concept in an on-tology (based on the description of that concept inthe ontology) after disambiguating potential am-biguous surface words, or phrases.
This is espe-cially true with abbreviations and acronyms whichare much more common in clinical text (Moon etal., 2012).
The SemEval-2014 task 7 was one ofnine shared tasks organized at the SemEval-2014.It was designed as a follow up to the shared tasksorganized during the ShARe/CLEF eHealth 2013evaluation (Suominen et al., 2013; Pradhan et al.,2013; Pradhan et al., 2014).
Like the previousshared task, we relied on the ShARe corpus, butwith more data for training and a new test set.
Fur-thermore, in this task, we provided the options toparticipants to utilize a large corpus of unlabeledclinical notes.
The rest of the paper is organized asfollows.
Section 2 describes the characteristics ofthe data used in the task.
Section 3 describes thetasks in more detail.
Section 4 explains the evalu-ation criteria for the two tasks.
Section 5 lists theparticipants of the task.
Section 6 discusses the re-sults on this task and also compares them with theShARe/CLEF eHealth 2013 results, and Section 7concludes.2 DataThe ShARe corpus comprises annotations overde-identified clinical reports from a US intensivecare department (version 2.5 of the MIMIC IIdatabase6) (Saeed et al., 2002).
It consists ofdischarge summaries, electrocardiogram, echocar-diogram, and radiology reports.
Access to datawas carried out following MIMIC user agreementrequirements for access to de-identified medical6http://mimic.physionet.org ?
Multiparameter IntelligentMonitoring in Intensive Caredata.
Hence, all participants were required to reg-ister for the evaluation, obtain a US human sub-jects training certificate7, create an account to thepassword-protected MIMIC site, specify the pur-pose of data usage, accept the data use agree-ment, and get their account approved.
The anno-tation focus was on disorder mentions, their var-ious attributes and normalizations to an UMLSCUI.
As such, there were two parts to the annota-tion: identifying a span of text as a disorder men-tion and normalizing (or mapping) the span to aUMLS CUI.
The UMLS represents over 130 lex-icons/thesauri with terms from a variety of lan-guages and integrates resources used world-widein clinical care, public health, and epidemiology.A disorder mention was defined as any span of textwhich can be mapped to a concept in SNOMED-CT and which belongs to the Disorder semanticgroup8.
It also provided a semantic network inwhich every concept is represented by its CUIand is semantically typed (Bodenreider and Mc-Cray, 2003).
A concept was in the Disorder se-mantic group if it belonged to one of the follow-ing UMLS semantic types: Congenital Abnormal-ity; Acquired Abnormality; Injury or Poisoning;Pathologic Function; Disease or Syndrome; Men-tal or Behavioral Dysfunction; Cell or Molecu-lar Dysfunction; Experimental Model of Disease;Anatomical Abnormality; Neoplastic Process; andSigns and Symptoms.
The Finding semantic typewas left out as it is very noisy and our pilot studyshowed lower annotation agreement on it.
Follow-ing are the salient aspects of the guidelines used to7The course was available free of charge on the Internet, for example,via the CITI Collaborative Institutional Training Initiative athttps://www.citiprogram.org/Default.aspor, the US National Institutes of Health (NIH) athttp://phrp.nihtraining.com/users.8Note that this definition of Disorder semantic group did not include theFindings semantic type, and as such differed from the one of UMLS Seman-tic Groups, available at http://semanticnetwork.nlm.nih.gov/SemGroups55annotate the data.?
Annotations represent the most specific dis-order span.
For example, small bowel ob-struction is preferred over bowel obstruction.?
A disorder mention is a concept in theSNOMED-CT portion of the Disorder se-mantic group.?
Negation and temporal modifiers are not con-sidered part of the disorder mention span.?
All disorder mentions are annotated?eventhe ones related to a person other than the pa-tient and including acronyms and abbrevia-tions.?
Mentions of disorders that are coreferen-tial/anaphoric are also annotated.Following are a few examples of disorder men-tions from the data.Patient found to have lower extremity DVT.
(E1)In example (E1), lower extremity DVT is markedas the disorder.
It corresponds to CUI C0340708(preferred term: Deep vein thrombosis of lowerlimb).
The span DVT can be mapped to CUIC0149871 (preferred term: Deep Vein Thrombo-sis), but this mapping would be incorrect becauseit is part of a more specific disorder in the sen-tence, namely lower extremity DVT.A tumor was found in the left ovary.
(E2)In example (E2), tumor ... ovary is annotated as adiscontiguous disorder mention.
This is the bestmethod of capturing the exact disorder mentionin clinical notes and its novelty is in the fact thateither such phenomena have not been seen fre-quently enough in the general domain to gatherparticular attention, or the lack of a manuallycurated general domain ontology parallel to theUMLS.Patient admitted with low blood pressure.
(E3)There are some disorders that do not have a rep-resentation to a CUI as part of the SNOMED CTwithin the UMLS.
However, if they were deemedimportant by the annotators then they were anno-tated as CUI-less mentions.
In example (E3), lowblood pressure is a finding and is normalized asa CUI-less disorder.
We constructed the annota-tion guidelines to require that the disorder be areasonable synonym of the lexical description of aSNOMED-CT disorder.
There are a few instanceswhere the disorders are abbreviated or shortenedin the clinical note.
One example is w/r/r, whichis an abbreviation for concepts wheezing (CUIC0043144), rales (CUI C0034642), and ronchi(CUI C0035508).
This abbreviation is also some-times written as r/w/r and r/r/w.
Another is gsw forgunshot wound and tachy for tachycardia.
Moredetails on the annotation scheme is detailed in theguidelines9and in a forthcoming manuscript.
Theannotations covered about 336K words.
Table 1shows the quantity of the data and the split acrossthe training, development and test sets as well asin terms of the number of notes and the number ofwords.2.1 Annotation QualityEach note in the training and development set wasannotated by two professional coders trained forthis task, followed by an open adjudication step.By the time we reached annotating the test data,the annotators were quite familiar with the anno-tation and so, in order to save time, we decidedto perform a single annotation pass using a seniorannotator.
This was followed by a correction passby the same annotator using a checklist of frequentannotation issues faced earlier.
Table 2 shows theinter-annotator agreement (IAA) statistics for theadjudicated data.
For the disorders we measure theagreement in terms of the F1-score as traditionalagreement measures such as Cohen?s kappa andKrippendorf?s alpha are not applicable for measur-ing agreement for entity mention annotation.
Wecomputed agreements between the two annotatorsas well as between each annotator and the final ad-judicated gold standard.
The latter is to give asense of the fraction of corrections made in theprocess of adjudication.
The strict criterion con-siders two mentions correct if they agree in termsof the class and the exact string, whereas the re-laxed criteria considers overlapping strings of the9http://goo.gl/vU8KdWDisorder CUIRelaxed Strict Relaxed StrictF1F1Acc.
Acc.A1-A2 90.9 76.9 77.6 84.6A1-GS 96.8 93.2 95.4 97.3A2-GS 93.7 82.6 80.6 86.3Table 2: Inter-annotator (A1 and A2) and goldstandard (GS) agreement as F1-score for the Dis-order mentions and their normalization to theUMLS CUI.56Institution User ID Team IDUniversity of Pisa, Italy attardi UniPIUniversity of Lisbon, Portugal francisco ULisboaUniversity of Wisconsin, Milwaukee, USA ghiasvand UWMUniversity of Colorado, Boulder, USA gung CLEARUniversity of Guadalajara, Mexico herrera UGTaipei Medical University, Taiwan hjdai TMUUniversity of Turku, Finland kaewphan UTUUniversity of Szeged, Hungary katona SZTE-NLPQueensland University of Queensland, Australia kholghi QUT AEHRCKU Leuven, Belgium kolomiyets KULUniversidade de Aveiro, Portugal nunes BioinformaticsUAUniversity of the Basque Country, Spain oronoz IxaMedIBM, India parikh ThinkMinerseasy data intelligence, India pathak ezDIRelAgent Tech Pvt.
Ltd., India ramanan RelAgentUniversidad Nacional de Colombia, Colombia riveros MindLab-UNALIIT Patna, India sikdar IITPUniversity of North Texas, USA solomon UNTUniversity of Illinois at Urbana Champaign, USA upadhya CogCompThe University of Texas Health Science Center at Houston, USA wu UTH CCBEast China Normal University, China yi ECNUTable 3: Participant organization and the respective User IDs and Team IDs.same class as correct.
The reason for checkingthe class is as follows.
Although we only use thedisorder mention in this task, the corpus has beenannotated with some other UMLS types as welland therefore there are instances where a differ-ent UMLS type is assigned to the same characterspan in the text by the second annotator.
If exactboundaries are not taken into account then the IAAagreement score is in the mid-90s.
For the task ofnormalization to CUIs, we used accuracy to assessagreement.
For the relaxed criterion, all overlap-ping disorder spans with the same CUI were con-sidered correct.
For the strict criterion, only disor-der spans with identical spans and the same CUIwere considered correct.3 Task DescriptionThe participants were evaluated on the followingtwo tasks:?
Task A ?
Identification of the character spansof disorder mentions.?
Task B ?
Normalizing disorder mentions toSNOMED-CT subset of UMLS CUIs.For Task A, participants were instructed to developa system that predicts the spans for disorder men-tions.
For Tasks B, participants were instructedto develop a system that predicts the UMLS CUIwithin the SNOMED-CT vocabulary.
The input toTask B were the disorder mention predictions fromTask A.
Task B was optional.
System outputs ad-hered to the annotation format.
Each participantwas allowed to submit up to three runs.
The en-tire set of unlabeled MIMIC clinical notes (exclud-ing the test notes) were made available to the par-ticipants for potential unsupervised approaches toenhance the performance of their systems.
Theywere allowed to use additional annotations in theirsystems, but this counted towards the total allow-able runs; systems that used annotations outsideof those provided were evaluated separately.
Theevaluation for all tasks was conducted using theblind, withheld test data.
The participants wereprovided a training set containing clinical text aswell as pre-annotated spans and named entities fordisorders (Tasks A and B).4 Evaluation CriteriaThe following evaluation criteria were used:?
Task A ?
The system performance was eval-uated against the gold standard using theF1-score of the Precision and Recall values.There were two variations: (i) Strict; and (ii)Relaxed.
The formulae for computing thesemetrics are mentioned below.Precision = P =DtpDtp+ Dfp(1)Recall = R =DtpDtp+ Dfn(2)Where, Dtp= Number of true positives dis-order mentions; Dfp= Number of false pos-itives disorder mentions; Dfn= Number offalse negative disorder mentions.
In the strictcase, a span was counted as correct if it wasidentical to the gold standard span, whereas57Task AStrict RelaxedTeam ID User ID Run P R F1P R F1Data(%) (%) (%) (%) (%) (%)UTH CCB wu 0 84.3 78.6 81.3 93.6 86.6 90.0 T+DUTH CCB wu 1 80.8 80.5 80.6 91.6 90.7 91.1 T+DUTU kaewphan 1 76.5 76.7 76.6 88.6 89.9 89.3 T+DUWM ghiasvand 0 78.7 72.6 75.5 91.1 85.6 88.3 T+DUTH CCB wu 2 68.0 84.9 75.5 83.8 93.5 88.4 T+DUTU kaewphan 0 77.3 72.4 74.8 90.1 85.6 87.8 TIxaMed oronoz 1 68.1 78.6 73.0 87.2 89.0 88.1 T+DUWM ghiasvand 0 77.5 67.9 72.4 90.9 81.2 85.8 TRelAgent ramanan 0 74.1 70.1 72.0 89.5 84.0 86.7 T+DIxaMed oronoz 0 72.9 70.1 71.5 88.5 80.8 84.5 T+DezDI pathak 1 75.0 68.2 71.4 91.5 82.7 86.9 TCLEAR gung 0 80.7 63.6 71.2 92.0 72.3 81.0 TezDI pathak 0 75.0 67.7 71.2 91.4 81.9 86.4 TULisboa francisco 0 75.3 66.3 70.5 91.4 81.5 86.2 TULisboa francisco 1 75.2 66.0 70.3 90.9 80.6 85.5 TULisboa francisco 2 75.2 66.0 70.3 90.9 80.6 85.5 TBioinformaticsUA nunes 0 81.3 60.5 69.4 92.9 69.3 79.4 T+DThinkMiners parikh 0 73.4 65.0 68.9 89.2 80.2 84.4 TThinkMiners parikh 1 74.9 61.7 67.7 90.7 75.8 82.6 TECNU yi 0 75.4 61.1 67.5 89.8 72.2 80.0 T+DUniPI attardi 2 71.2 60.1 65.2 89.7 76.6 82.6 T+DUNT solomon 0 64.7 62.8 63.8 81.5 79.9 80.7 T+DUniPI attardi 1 65.9 61.2 63.5 90.2 77.5 83.4 T+DBioinformaticsUA nunes 2 75.3 53.8 62.8 86.5 62.1 72.3 T+DBioinformaticsUA nunes 1 60.0 62.1 61.0 69.8 72.3 71.0 T+DUniPI attardi 0 53.9 68.4 60.2 77.8 88.5 82.8 T+DCogComp upadhya 1 63.9 52.9 57.9 82.3 68.3 74.6 T+DCogComp upadhya 2 64.1 52.0 57.4 82.9 67.5 74.4 T+DCogComp upadhya 0 63.6 51.5 56.9 81.9 66.5 73.4 T+DTMU hjdai 0 52.4 57.6 54.9 91.4 76.5 83.3 T+DMindLab-UNAL riveros 2 56.1 53.4 54.7 76.9 67.7 72.0 TMindLab-UNAL riveros 1 57.8 51.5 54.5 77.7 65.4 71.0 TTMU hjdai 1 62.2 42.9 50.8 89.9 65.2 75.6 T+DIITP sikdar 0 50.0 47.9 48.9 81.5 79.7 80.6 T+DIITP sikdar 1 47.3 45.8 46.5 78.9 77.6 78.2 T+DIITP sikdar 2 45.0 48.1 46.5 76.9 82.6 79.6 T+DMindLab-UNAL riveros 0 32.1 56.5 40.9 43.9 72.5 54.7 TSZTE-NLP katona 1 54.7 25.2 34.5 88.4 40.1 55.1 TSZTE-NLP katona 2 54.7 25.2 34.5 88.4 40.1 55.1 TQUT AEHRC kholghi 0 38.7 29.8 33.7 90.6 70.9 79.5 T+DSZTE-NLP katona 0 57.1 20.5 30.2 91.8 32.5 48.0 TKUL kolomiyets 0 65.5 17.8 28.0 72.1 19.6 30.8 PUG herrera 0 11.4 23.4 15.3 25.9 49.0 33.9 PTable 4: Performance on test data for participating systems on Task A ?
Identification of disorder men-tions.Task AStrict RelaxedTeam ID User ID Run P R F1P R F1Data(%) (%) (%) (%) (%) (%)hjdai TMU 1 0.687 0.922 0.787 0.952 1.000 0.975 Twu UTH CCB 0 0.877 0.710 0.785 0.962 0.789 0.867 Twu UTH CCB 1 0.828 0.747 0.785 0.941 0.853 0.895 TBest ShARe/CLEF-2013 performance 0.800 0.706 0.750 0.925 0.827 0.873 Tghiasvand UWM 0 0.827 0.675 0.743 0.958 0.799 0.871 Tpathak ezDI 0 0.813 0.670 0.734 0.954 0.800 0.870 Tpathak ezDI 1 0.809 0.667 0.732 0.954 0.801 0.871 Twu UTH CCB 2 0.657 0.790 0.717 0.806 0.893 0.847 Tfrancisco ULisboa 1 0.803 0.646 0.716 0.954 0.781 0.858 Tfrancisco ULisboa 2 0.803 0.646 0.716 0.954 0.781 0.858 Tfrancisco ULisboa 0 0.796 0.642 0.711 0.959 0.793 0.868 Toronoz IxaMed 0 0.766 0.650 0.703 0.936 0.752 0.834 Toronoz IxaMed 1 0.660 0.721 0.689 0.899 0.842 0.870 Thjdai TMU 0 0.667 0.414 0.511 0.912 0.591 0.717 Tsikdar IITP 0 0.525 0.430 0.473 0.862 0.726 0.788 Tsikdar IITP 2 0.467 0.440 0.453 0.812 0.775 0.793 Tsikdar IITP 1 0.493 0.410 0.448 0.828 0.706 0.762 TTable 5: Performance on development data for participating systems on Task A ?
Identification of disor-der mentions.58in the relaxed case, a span overlapping withthe gold standard span was also consideredcorrect.?
Task B ?
Accuracy was used as the perfor-mance measure for Task 1b.
It was defined asfollows:Accuracystrict=Dtp?NcorrectTg(3)Accuracyrelaxed=Dtp?NcorrectDtp(4)Where, Dtp= Number of true positive disor-der mentions with identical spans as in thegold standard; Ncorrect= Number of cor-rectly normalized disorder mentions; and Tg= Total number of disorder mentions in thegold standard.
For Task B, the systems wereonly evaluated on annotations they identifiedin Task A.
Relaxed accuracy only measuredthe ability to normalize correct spans.
There-fore, it was possible to obtain very high val-ues for this measure by simply dropping anymention with a low confidence span.5 ParticipantsA total of 21 participants from across the worldparticipated in Task A and out of them 18 also par-ticipated in Task B.
Unfortunately, although inter-ested, the ThinkMiners team (Parikh et al., 2014)could not participate in Task B owing to someUMLS licensing issues.
The participating organi-zations along with the contact user?s User ID andtheir chosen Team ID are mentioned in Table 3.Eight teams submitted three runs, six submittedtwo runs and seven submitted just one run.
Outof these, only 13 submitted system description pa-pers.
We based our analysis on those system de-scriptions.6 System ResultsTables 4 and 6 show the performance of the sys-tems on Tasks A and B.
None of the systems usedany additional annotated data so we did not haveto compare them separately.
Both tables mentionperformance of all the different runs that the sys-tems submitted.
Given the many variables, we de-liberately left the decision on how many and howto define these runs to the individual participant.They used various different ways to differentiatetheir runs.
Some, for example, UTU (Kaewphan etal., 2014), did it based on the composition of train-ing data, i.e., whether they used just the trainingdata or both the training and the development datafor training the final system, which highlightedthe fact that adding development data to trainingbumped the F1-score on Task A by about 2 percentpoints.
Some participants, however, did not makeuse of the development data in training their sys-tems.
This was partially due to the fact that we hadnot explicitly mentioned in the task descriptionthat participants were allowed to use the develop-ment data for training their final models.
In orderto be fair, we allowed some users an opportunityto submit runs post evaluation where they used theexact same system that they used for evaluationbut used the development data as well.
We addeda column to the results tables showing whether theparticipant used only the training data (T) or bothtraining and development data (T+D) for trainingtheir system.
It can be seen that even though theaddition of development data helps, there are stillsystems that perform in the lower percentile whohave used both training and development data fortraining, indicating that both the features and themachine learning classifier contribute to the mod-els.
A novel aspect of the SemEval-2014 sharedtask that differentiates it from the ShARE/CLEFtask?other than the fact that it used more data anda new test set?is the fact that SemEval-2014 al-lowed the use of a much larger set of unlabeledMIMIC notes to inform the models.
Surprisingly,only two of the systems (ULisboa (Leal et al.,2014) and UniPi (Attardi et al., 2014)) used theunlabeled MIMIC corpus to generalize the lexicalfeatures.
Another team?UTH CCB(Zhang et al.,2014)?used off-the-shelf Brown clusters10as op-posed to training them on the unlabeled MIMICII data.
For Task B, the accuracy of a systemusing the strict metric was positively correlatedwith its recall on the disorder mentions that wereinput to it (i.e., recall for Task A), and did notget penalized for lower precision.
Therefore onecould essentially gain higher accuracy in Task Bby tuning a system to provide the highest men-tion recall in Task A potentially at the cost of pre-cision and the overall F1-score and using thosementions as input for Task B.
This can be seenfrom the fact that the run 2 for UTH CCB (Zhanget al., 2014) system with the lowest F1-score has10Personal conversation with the participants as it was notvery clear in the system description paper.59Task BStrict RelaxedTeam ID User ID Run Acc.
Acc.
Data(%) (%)UTH CCB wu 2 74.1 87.3 T+DUTH CCB wu 1 70.8 88.0 T+DUTH CCB wu 0 69.4 88.3 T+DUWM ghiasvand 0 66.0 90.9 T+DRelAgent ramanan 0 63.9 91.2 T+DUWM ghiasvand 0 61.7 90.8 TIxaMed oronoz 0 60.4 86.2 T+DUTU kaewphan 1 60.1 78.3 T+DezDI pathak 1 59.9 87.8 TezDI pathak 0 59.2 87.4 TUTU kaewphan 0 57.7 79.7 TBioinformaticsUA nunes 1 53.1 85.5 T+DBioinformaticsUA nunes 0 52.7 87.0 T+DCLEAR gung 0 52.5 82.5 TTMU hjdai 0 48.9 84.9 T+DUNT solomon 0 47.0 74.8 T+DUniPI attardi 0 46.7 68.3 T+DBioinformaticsUA nunes 2 46.3 86.1 T+DMindLab-UNAL riveros 2 46.1 86.3 TIxaMed oronoz 1 43.9 55.8 T+DMindLab-UNAL riveros 0 43.5 77.1 TUniPI attardi 1 42.8 69.9 T+DUniPI attardi 2 41.7 69.3 T+DMindLab-UNAL riveros 1 41.1 79.7 TULisboa francisco 2 40.5 61.5 TULisboa francisco 1 40.4 61.2 TULisboa francisco 0 40.2 60.6 TECNU yi 0 36.4 59.5 T+DTMU hjdai 1 35.8 83.4 T+DIITP sikdar 0 33.3 69.6 T+DIITP sikdar 2 33.2 69.1 T+DIITP sikdar 1 31.9 69.6 T+DCogComp upadhya 1 25.3 47.9 T+DCogComp upadhya 2 24.8 47.7 T+DCogComp upadhya 0 24.4 47.3 T+DKUL kolomiyets 0 16.5 92.8 PUG herrera 0 12.5 53.4 PTable 6: Performance on test data for participat-ing systems on Task B ?
Normalization of disordermentions to UMLS (SNOMED-CT subset) CUIs.Task BStrict RelaxedTeam ID User ID Run Acc.
Acc.
Data(%) (%)TMU hjdai 0 0.716 0.777 TTMU hjdai 1 0.716 0.777 TUTH CCB wu 2 0.713 0.903 TUTH CCB wu 1 0.680 0.910 TUTH CCB wu 0 0.647 0.910 TUWM ghiasvand 0 0.623 0.923 TezDI pathak 0 0.603 0.900 TezDI pathak 1 0.600 0.899 TBest ShARe/CLEF-2013 performance 0.589 0.895 TIxaMed oronoz 0 0.556 0.855 TIxaMed oronoz 1 0.421 0.584 TULisboa francisco 2 0.388 0.601 TULisboa francisco 1 0.385 0.596 TULisboa francisco 0 0.377 0.588 TIITP sikdar 2 0.318 0.724 TIITP sikdar 0 0.312 0.725 TIITP sikdar 1 0.299 0.730 TTable 7: Performance on development datafor some participating systems on Task B ?Normalization of disorder mentions to UMLS(SNOMED-CT subset) CUIs.the best accuracy for Task B and vice-versa forrun 0 with run 1 in between the two.
In order tofairly compare the performance between two sys-tems one would have to provide perfect mentionsas input to Task B.
One of the systems?UWMGhiasvand and Kate (2014)?did run some abla-tion experiments using gold standard mentions asinput to Task B and obtained a best performanceof 89.5F1-score (Table 5 of Ghiasvand and Kate(2014)) as opposed to 62.3 F1-score (Table 7) inthe more realistic setting which is a huge differ-ence.
In the upcoming SemEval-2014 where thissame evaluation is going to carried out under Task14, we plan to perform supplementary evaluationwhere gold disorder mentions would be input tothe system while attempting Task B.
An inter-esting outcome of planning a follow-on evalua-tion to the ShARe/CLEF eHealth 2013 task wasthat we could, and did, use the test data from theShARe/CLEF eHealth 2013 task as the develop-ment set for this evaluation.
After the main eval-uation we asked participants to provide the sys-tem performance on the development set using thesame number and run convention that they submit-ted for the main evaluation.
These results are pre-sented in Tables 5 and 7.
We have inserted the bestperforming system score from the ShARe/CLEFeHealth 2013 task in these tables.
For Task A, re-ferring to Tables 4 and 5, there is a boost of 3.7absolute percent points for the F1-score over thesame task (Task 1a) in the ShARe/CLEF eHealth2013.
For Task B, referring to Tables 6 and 7, thereis a boost of 13.7 percent points for the F1-scoreover the same task (Task 1b) in the ShARe/CLEFeHealth 2013 evaluation.
The participants usedvarious approaches for tackling the tasks, rang-ing from purely rule-based/unsupervised (RelA-gent (Ramanan and Nathan, 2014), (Matos etal., 2014), KUL11) to a hybrid of rules and ma-chine learning classifiers.
The top performing sys-tems typically used the latter.
Various versionsof the IOB formulation were used for tagging thedisorder mentions.
None of the standard varia-tions on the IOB formulation were explicitly de-signed or used to handle discontiguous mentions.Some systems used novel variations on this ap-proach.
Probably the simplest variation was ap-plied by the UWM team (Ghiasvand and Kate,2014).
In this formulation the following labeledsequence ?the/O left/B atrium/I is/O moderately/O11Personal communication with participant.60dilated/I?
can be used to represent the discontigu-ous mention left atrium...dilated, and can be con-structed as such from the output of the classifica-tion.
The most complex variation was the one usedby the UTH CCB team (Zhang et al., 2014) wherethey used the following set of tags?B, I, O, DB,DI, HB, HI.
This variation encodes discontiguousmentions by adding four more tags to the I, O andB tags.
These are variations of the B and I tagswith either a D or a H prefix.
The prefix H indi-cates that the word or word sequence is the sharedhead, and the prefix D indicates otherwise.
An-other intermediate approach used by the ULisboateam (Leal et al., 2014) with the tagset?S, B, I,O, E and N. Here, S represents the single tokenentity to be recognized, E represents the end of anentity (which is part of one of the prior IOB vari-ations) and an N tag to identify non-contiguousmentions.
They don?t provide an explicit exam-ple usage of this tag set in their paper.
Yet anothervariation was used by the SZTE-NLP team (Ka-tona and Farkas, 2014).
This used tags B, I, L, Oand U.
Here, L is used for the last token similar toE earlier, and U is used for a unit-token mention,similar to S earlier.
We believe that the only ap-proach that can distinguish between discontiguousdisorders that share the same head word/phrase isthe one used by the UTH CCB team (Zhang etal., 2014).
The participants used various machinelearning classifiers such as MaxEnt, SVM, CRF incombination with rich syntactic and semantic fea-tures to capture the disorder mentions.
As men-tioned earlier, a few participants used the avail-able unlabeled data and also off-the-shelf clustersto better generalize features.
The use of vectorspace models such as cosine similarities as wellas continuous distributed word vector representa-tions was useful in the normalization task.
Theyalso availed of tools such as MetaMap and cTakesto generate features as well as candidate CUIs dur-ing normalizations.7 ConclusionWe have created a reference standard with highinter-annotator agreement and evaluated systemson the task of identification and normalizationof diseases and disorders appearing in clinicalreports.
The results have demonstrated that anNLP system can complete this task with reason-ably high accuracy.
We plan to annotate anotherevaluation using the same data as part of the inthe SemEval-2015, Task 1412adding another taskof template filling where the systems will iden-tify and normalize ten attributes the identified dis-ease/disorder mentions.AcknowledgmentsWe greatly appreciate the hard work and feed-back of our program committee members and an-notators David Harris, Jennifer Green and GlennZaramba.
Danielle Mowery, Sumithra Velupillaiand Brett South for helping prepare the manuscriptby summarizing the approaches used by varioussystems.
This shared task was partially sup-ported by Shared Annotated Resources (ShARe)project NIH 5R01GM090187 and Temporal His-tories of Your Medical Events (THYME) project(NIH R01LM010090 and U54LM008748).ReferencesGiuseppe Attardi, Vitoria Cozza, and Daniele Sartiano.2014.
UniPi: Recognition of mentions of disordersin clinical text.
In Proceedings of the InternationalWorkshop on Semantic Evaluations, Dublin, Ireland,August.Olivier Bodenreider and Alexa McCray.
2003.
Ex-ploring semantic groups through visual approaches.Journal of Biomedical Informatics, 36:414?432.Keith E. Campbell, Diane E. Oliver, and Edward H.Shortliffe.
1998.
The Unified Medical LanguageSystem: Towards a collaborative approach for solv-ing terminologic problems.
J Am Med Inform Assoc,5(1):12?16.Omid Ghiasvand and Rohit J. Kate.
2014.
UWM: Dis-order mention extraction from clinical text using crfsand normalization using learned edit distance pat-terns.
In Proceedings of the International Workshopon Semantic Evaluations, Dublin, Ireland, August.Suwisa Kaewphan, Kai Hakaka1, and Filip Ginter.2014.
UTU: Disease mention recognition and nor-malization with crfs and vector space representa-tions.
In Proceedings of the International Workshopon Semantic Evaluations, Dublin, Ireland, August.Melinda Katona and Rich?ard Farkas.
2014.
SZTE-NLP: Clinical text analysis with named entity recog-nition.
In Proceedings of the International Work-shop on Semantic Evaluations, Dublin, Ireland, Au-gust.Andr?e Leal, Diogo Gonc?alves, Bruno Martins, andFrancisco M. Couto.
2014.
ULisboa: Identifica-tion and classification of medical concepts.
In Pro-ceedings of the International Workshop on SemanticEvaluations, Dublin, Ireland, August.12http://alt.qcri.org/semeval2015/task1461Robert Leaman and Graciela Gonzalez.
2008.
Ban-ner: an executable survey of advances in biomedicalnamed entity recognition.
In Pacific Symposium onBiocomputing, volume 13, pages 652?663.S?ergio Matos, Tiago Nunes, and Jos?e Lu?
?s Oliveira.2014.
BioinformaticsUA: Concept recognition inclinical narratives using a modular and highly ef-ficient text processing framework.
In Proceedingsof the International Workshop on Semantic Evalua-tions, Dublin, Ireland, August.Sungrim Moon, Serguei Pakhomov, and Genevieve BMelton.
2012.
Automated disambiguation ofacronyms and abbreviations in clinical texts: Win-dow and training size considerations.
In AMIA AnnuSymp Proc, pages 1310?1319.David Nadeau and Satoshi Sekine.
2007.
A sur-vey of named entity recognition and classification.Lingvisticae Investigationes, 30(1):3?26.Roberto Navigli.
2009.
Word sense disambiguation.ACM Computing Surveys, 41(2):1?69, February.Ankur Parikh, Avinesh PVS, Joy Mustafi, Lalit Agar-walla, and Ashish Mungi.
2014.
ThinkMiners:SemEval-2014 task 7: Analysis of clinical text.
InProceedings of the International Workshop on Se-mantic Evaluations, Dublin, Ireland, August.Sameer Pradhan, No?emie Elhadad, Brett South, DavidMartinez, Lee Christensen, Amy Vogel, HannaSuominen, Wendy W. Chapman, and GuerganaSavova.
2013.
Task 1: ShARe/CLEF eHealthEvaluation Lab 2013.
In Working Notes of CLEFeHealth Evaluation Labs.Sameer Pradhan, No?emie Elhadad, Brett South, DavidMartinez, Lee Christensen, Amy Vogel, HannaSuominen, Wendy W. Chapman, and GuerganaSavova.
2014.
Evaluating the state of the art indisorder recognition and normalization of the clin-ical narrative.
In Journal of the American MedicalInformatics Association (to appear).S.
V. Ramanan and P. Senthil Nathan.
2014.
RelA-gent: Entity detection and normalization for diseasesin clinical records: a linguistically driven approach.In Proceedings of the International Workshop on Se-mantic Evaluations, Dublin, Ireland, August.Angus Roberts, Robert Gaizauskas, Mark Hepple,George Demetriou, Yikun Guo, Ian Roberts, andAndrea Setzer.
2009.
Building a semantically an-notated corpus of clinical texts.
J Biomed Inform,42(5):950?66.Mohammed Saeed, C. Lieu, G. Raber, and R.G.
Mark.2002.
MIMIC II: a massive temporal ICU patientdatabase to support research in intelligent patientmonitoring.
Comput Cardiol, 29.Guergana K. Savova, A. R. Coden, I. L. Sominsky,R.
Johnson, P. V. Ogren, P. C. de Groen, and C. G.Chute.
2008.
Word sense disambiguation acrosstwo domains: Biomedical literature and clinicalnotes.
J Biomed Inform, 41(6):1088?1100, Decem-ber.Weiyi Sun, Anna Rumshisky, and?Ozlem Uzuner.2013.
Evaluating temporal relations in clinical text:2012 i2b2 Challenge.
Journal of the American Med-ical Informatics Association, 20(5):806?13.Hanna Suominen, Sanna Salanter?a, Sumithra Velupil-lai, Wendy W. Chapman, Guergana Savova,Noemie Elhadad, Sameer Pradhan, Brett R. South,Danielle L. Mowery, Gareth J. F. Jones, JohannesLeveling, Liadh Kelly, Lorraine Goeuriot, DavidMartinez, and Guido Zuccon.
2013.
Overview ofthe ShARe/CLEF eHealth evaluation lab 2013.
InWorking Notes of CLEF eHealth Evaluation Labs.
?Ozlem Uzuner, Brett R South, Shuying Shen, andScott L DuVall.
2011.
2010 i2b2/VA challenge onconcepts, assertions, and relations in clinical text.Journal of the American Medical Informatics Asso-ciation, 18(5):552?556.
?Ozlem Uzuner, Andreea Bodnari, Shuying Shen, TylerForbush, John Pestian, and Brett R South.
2012.Evaluating the state of the art in coreference res-olution for electronic medical records.
Jour-nal of American Medical Informatics Association,19(5):786?791, September.Yaoyun Zhang, Jingqi Wang, Buzhou Tang, YonghuiWu, Min Jiang, Yukun Chen, and Hua Xu.
2014.UTH CCB: A report for SemEval 2014 task 7 anal-ysis of clinical text.
In Proceedings of the Interna-tional Workshop on Semantic Evaluations, Dublin,Ireland, August.62
