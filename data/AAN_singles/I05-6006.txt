The Syntactically Annotated ICE Corpusand the Automatic Induction of a Formal GrammarAlex Chengyu FangDepartment of Chinese, Translation and LinguisticsCity University of Hong Kongacfang@cityu.edu.hkAbstractThe International Corpus of English isa corpus of national and regionalvarieties of English.
The mega-wordBritish component has been con-structed, grammatically tagged, andsyntactically parsed.
This article is adescription of work that aims at theautomatic induction of a wide-coveragegrammar from this corpus as well as anempirical evaluation of the grammar.
Itfirst of all describes the corpus and itsannotation schemes and then presentsempirical statistics for the grammar.
Iwill then evaluate the coverage and theaccuracy of such a grammar whenapplied automatically in a parsingsystem.
Results show that the grammarenabled the parser to achieve 86.1%recall rate and 83.5% precision rate.1 IntroductionThe International Corpus of English (ICE) is aproject that aims at the construction of acollection of corpora of English in countries andregions where English is used either as a first oras an official language (Greenbaum 1992).
Eachcomponent corpus comprises one million wordsof both written and transcribed spoken samplesthat are then annotated at grammatical andsyntactic levels.
The British component of theICE corpus was used to automatically induce alarge formal grammar, which was subsequentlyused in a robust parsing system.
In what follows,this article will first of all describe theannotation schemes for the corpus and theevaluation of a formal grammar automaticallyinduced from the corpus in terms of its potentialcoverage when tested with empirical data.Finally, this article will present an evaluation ofthe grammar through its application in a robustparsing system in terms of labelling andbracketing accuracies.1.1 The ICE wordclass annotation schemeThere are altogether 22 head tags and 71 featuresin the ICE wordclass tagging scheme, resulting inabout 270 grammatically possible combinations.Compared with 134 tags for LOB, 61 for BNC,and 36 for Penn Treebank, the ICE tagset isperhaps the most detailed in automatic appli-cations.
They cover all the major English wordclasses and provide morphological, grammatical,collocational, and sometimes syntactic inform-ation.
A typical ICE tag has two components: thehead tag and its features that bring out thegrammatical features of the associated word.
Forinstance, N(com,sing) indicates that the lexicalitem associated with this tag is a common (com)singular (sing) noun (N).Tags that indicate phrasal collocationsinclude PREP(phras) and ADV(phras), pre-positions (as in [1]) and adverbs (as in [2]) that arefrequently used in collocation with certain verbsand adjectives:[1] Thus the dogs?
behaviour had been changedbecause they associated the bell with the food.
[2] I had been filming The Paras at the time, andBrian had had to come down to Wales with therecords.Some tags, such as PROFM(so,cl) (pronominalso representing a clause as in [3]) andPRTCL(with) (particle with as in [4]), indicatethe presence of a clause; so in [3] signals an49abbreviated clause while with in [4] a non-finiteclause:[3] If so, I?ll come and meet you at the station.
[4] The number by the arrows represents theorder of the pathway causing emotion, with thecortex lastly having the emotion.Examples [5]-[7] illustrate tags that note specialsentence structures.
There in [5] is tagged asEXTHERE, existential there that indicates amarked sentence order.
[6] is an example of thecleft sentence (which explicitly marks the focus),where it is tagged as CLEFTIT.
[7] exemplifiesanticipatory it, which is tagged as ANTIT:[5] There were two reasons for the secrecy.
[6] It is from this point onwards that RomanBritain ceases to exist and the history of sub-Roman Britain begins.
[7] Before trying to answer the question it isworthwhile highlighting briefly some of thedifferences between current historians.The verb class is divided into auxiliaries andlexical verbs.
The auxiliary class notes modals,perfect auxiliaries, passive auxiliaries, semi-auxiliaries, and semip-auxiliaries (those followedby -ing verbs).
The lexical verbs are furtherannotated according to their complementationtypes.
There are altogether seven types: complextransitive, complex ditransitive, copular, dimono-transitive, ditransitive, intransitive, mono-transitive, and TRANS.
Figure 1 shows the sub-categorisations of the verb class.TransitiveIntransitiveLexical VerbMono-transitiveCopulaDi-transitiveDi-mono-transtiveTransComplex-transitiveFigure 1: The ICE subcategorisation for verbsThe notation TRANS of the transitive verb class isused in the ICE project to tag those transitiveverbs followed by a noun phrase that may be thesubject of the following non-finite clause.
Thistype of verb can be analysed differently accordingto various tests into, for instance, monotransitives,ditransitives and complex transitives.
To avoidarbitrary decisions, the complementing non-finiteclause is assigned a catch-all term ?transitivecomplement?
in parsing, and its preceding verb isaccordingly tagged as TRANS in order to avoidmaking a decision on its transitivity type.
Thisverb type is best demonstrated by [8]-[11]:[8] Just before Christmas, the producer of GoingPlaces, Irene Mallis, had asked me to make adocumentary on ?warm-up men?.
[9] They make others feel guilty and isolatethem.
[10] I can buy batteries for the tape - but I can seemyself spending a fortune!
[11]The person who booked me in had hiseyebrows shaved and replaced by straight blackpainted lines and he had earrings, not only in hisears but through his nose and lip!In examples [8]-[11], asked, make, see, and hadare all complemented by non-finite clauses withovert subjects, the main verbs of these non-finiteclauses being infinitive, present participle and pastparticiple.As illustrated by examples [1]-[11], the ICEtagging scheme has indeed gone beyond thewordclass to provide some syntactic informationand has thus proved itself to be an expressiveand powerful means of pre-processing forsubsequent parsing.1.2 The ICE parsing schemeThe ICE parsing scheme recognises five basicsyntactic phrases.
They are adjective phrase(AJP), adverb phrase (AVP), noun phrase (NP),prepositional phrase (PP), and verb phrase (VP).Each tree in the ICE parsing scheme is re-presented as a functionally labelled hierarchy,with features describing the characteristics of eachconstituent, which is represented as a pair offunction-category labels.
In the case of a terminalnode, the function-category descriptive labels areappended by the lexical item itself in curlybrackets.
Figure 2 is such a structure for [12].
[12]We will be introducing new exam systems forboth schools and universities.50According to Figure 2, we know that [12] is aparsing unit (PU) realised by a clause (CL), whichgoverns three daughter nodes: SU NP (NP assubject), VB VP (VP as verbal), and OD NP (NPas direct object).
Each of the three daughter nodesare sub-branched until the leaves nodes with theinput tokens in curly brackets.
The direct objectnode, for example, has three immediateconstituents: NPPR AJP (AJP as NP pre-modifier), NPHD N(com,plu) (plural commonnoun as the NP head), and NPPO PP (PP as NPpost-modifier).Figure 2: A parse tree for [12]Note that in the same example, the head of thecomplementing NP of the prepositional phrase isinitially analysed as a coordinated construct(COORD), with two plural nouns as the conjoins(CJ) and a coordinating conjunction as co-ordinator (COOR).In all, there are 58 non-terminal parsingsymbols in the ICE parsing scheme, comparedwith 20 defined in the Penn Treebank project.
TheSuzanne Treebank has 43 function/categorysymbols, discounting those that are represented asfeatures in the ICE system.2 The generation of a formal grammarThe British component of the ICE corpus,annotated in fashions described above, has beenused to automatically generate a formalgrammar that has been subsequently applied inan automatic parsing system to annotate the restof the corpus (Fang 1995, 1996, 1999).
Thegrammar consists of two sets of rules.
The firstset describes the five canonical phrases (AJP,AVP, NP, PP, VP) as sequences of grammaticaltags terminating at the head of the phrase.
Forexample, the sequence AUX(modal,pres)AUX(prog,infin) V(montr,ingp) is a VPrule describing instantiations such as will beintroducing in [12].
The second set describes theclause as sequences of phrase types.
The stringin [12], for instance, is described by a sequenceNP VP NP PP in the set of clausal rules.To empirically characterise the grammar,the syntactically parsed ICE corpus was dividedinto ten equal parts according to the number ofcomponent texts.
One part was set aside fortesting, which was further divided into five testsets.
The remaining nine parts were used astraining data in a leave-one-out fashion.
In thisway, the training data was used to generate 9consecutive training sets, each increased by onepart over the previous set, with Set 1 formed ofone training set, Set 2 two training sets, and Set3 three training sets, etc.
The evaluation thus notonly aims to establish the potential coverage ofthe grammar but also to indicate the functionbetween the coverage of the grammar and thetraining data size.Figure 3 shows the growth of the number ofphrase structure rules as a function of the growthof training data size.
The Y-axis indicates thenumber of rules generated from the training dataand the X-axis the gradual increase of thetraining data size.02500500075001000012500150001 2 3 4 5 6 7 8 9AVPAJPNPPPVPFigure 3: The number of phrase structure rules as afunction of growing training data sizeIt can be observed that AJP and AVP show onlya marginal increase in the number of differentrules with the increase of training data size,therefore demonstrating a relatively small coreset.
In comparison, VPs are more varied but stillexhibit a visible plateau of growth.
The othertwo phrases, NP and PP, show a much morevaried set of rules not only through their largenumbers (9,184 for NPs and 13,736 for PPs) butalso the sharp learning curve.
There are manyreasons for the potentially large set of rules forPPs since they structurally subsume the clauseas well as all the phrase types.
Their largenumber is therefore more or less expected.
The51large set of NP rules is however a bit surprisingsince they are often characterised, perhaps toosimplistically, as comprising a determiner group,a premodifier group, and the noun head but thegrammar has 9,184 different rules for this phrasetype.
While this phenomenon calls for furtherinvestigations, we are concerned with only thecoverage issue for the moment in the currentarticle.3 The coverage of the formal grammarThe coverage of the formal grammar isevaluated through individual rule sets for thefive canonical phrase types separately.
Thecoverage by the clausal rules will also be report-ed towards the end of this section.3.1 The coverage of AJP rulesAs Figure 4 suggests, the coverage of thegrammar, when tested with the five samples, isconsistently high ?
all above 99% even when thegrammar was trained from only one ninth of thetraining set.
The increase of the size of the train-ing set does not show significant enhancementof the coverage.9999.299.499.699.81001 2 3 4 5 6 7 8 9Set 1Set 2Set 3Set 4Set 5Figure 4: The coverage of AJP rules3.2 The coverage of AVP rulesLike AJP rules, high coverage can be achievedwith a small training set since when trained withonly one ninth of the training data, the AVPrules already showed a high coverage of above99.4% and quickly approaching 100%.
SeeFigure 5.99.499.599.699.799.899.91001 2 3 4 5 6 7 8 9Set 1Set 2Set 3Set 4Set 5Figure 5: The coverage of AVP rules3.3 The coverage of NP rulesAlthough lower than AVP and AJP discussedabove, the NP rules show a satisfactorily highcoverage when tested by the five samples.
Ascan be seen from Figure 6, the initial coveragewhen trained with one ninth of the training datais generally above 97%, rising proportionally asthe training data size increases, to about 99%.This seems to suggest a mildly complex natureof NP structures.9797.59898.59999.51001 2 3 4 5 6 7 8 9Set 1Set 2Set 3Set 4Set 5Figure 6: The coverage of NP rules3.4 The coverage of VP rulesVPs do not seem to pose significant challenge tothe parser.
As Figure 7 indicates, the initialcoverage is all satisfactorily above 97.5%.
Set 1even achieved a coverage of over 98.5% whenthe grammar was trained with only one ninth ofthe training data.
As the graph seems to suggest,the learning curve arrives at a plateau whentrained with about half of the total training data,suggesting a centralised use of the rules.97.59898.59999.51001 2 3 4 5 6 7 8 9Set 1Set 2Set 3Set 4Set 5Figure 7: The coverage of VP rules3.5 The coverage of PP rulesAs is obvious from Figure 8, PPs are perhaps themost complex of the five phrases with an initialcoverage of just over 70%.
The learning curve issharp, culminating between 85% and 90% withthe full training data set.
As far as parserconstruction is concerned, this phrase alonedeserves special attention since it explains muchof the structural complexity of the clause.
Based52on this observation, a separate study was carriedout to automatically identify the syntacticfunctions of PPs.7075808590951 2 3 4 5 6 7 8 9Set 1Set 2Set 3Set 4Set 5Figure 8: The coverage of PP rules3.6 The coverage of clausal rulesClausal rules present the most challengingproblem since, as Figure 9 clearly indicates,their coverage is all under 67% even whentrained with all of the training data.
Thisobservation seems to reaffirm the usefulness ofrules at phrase level but the inadequacy ofclause structural rules.
Indeed, it is intuitivelyclear that the complexity of the sentence ismainly the result of the combination of clausesof various kinds.555759616365671 2 3 4 5 6 7 8 9Set 1Set 2Set 3Set 4Set 5Figure 9: The coverage of CL rules3.7 DiscussionThis section presented an evaluation of thegrammar in terms of its coverage as a functionof growing training data size.
As is shown, theparsed corpus resulted in excellent grammar setsfor the canonical phrases, AJP, AVP, NP, PP,and VP: except for PPs, all the phrase structurerules achieved a wide coverage of about 99%.The more varied set for PPs demonstrated acoverage of nearly 90%, not as high as what isachieved for the other phrases but still highlysatisfactory.The coverage of clause structure rules, onthe other hand, showed a considerably poorerperformance compared with the phrases.
Whenall of the training data was used, these rulescovered just over 65% of the testing data.In view of these empirical observations, itcan be reliably concluded that the corpus-basedgrammar construction holds a promisingapproach in that the phrase structure rulesgenerally have a high coverage when tested withunseen data.
The same approach has also raisedtwo questions at this stage: Does the high-coverage grammar also demonstrate a highprecision of analysis?
Is it possible to enhancethe coverage of the clause structure rules withinthe current framework?4 Evaluating the accuracy of analysisThe ICE project used two major annotationtools: AUTASYS and the Survey Parser.AUTASYS is an automatic wordclass taggingsystem that applies the ICE tags to words in theinput text with an accuracy rate of about 94%(Fang 1996a).
The tagged text is then fed intothe Survey Parser for automated syntacticanalysis.
The parsing model is one that tries toidentify an analogy between the input string anda sentence is that already syntactically analysedand stored in a database (Fang 1996b and 2000).This parser is driven by the previously describedformal grammar for both phrasal and clausalanalysis.
In this section, the formal grammar ischaracterised through an empirical evaluation ofthe accuracy of analysis by the Survey Parser.4.1 The NIST evaluation schemeThe National Institute of Science and Tech-nology (NIST) proposed an evaluation schemethat looks at the following properties whencomparing recognition results with the correctanswer:CPTTintsconstituenofnumberintsconstituencorrectofnumberRateMatchCorrect =CPTTintsconstituenofnumberintsconstituendsubstituteofnumberRateonSubstituti =CPTTintsconstituenofnumberintsconstituendeletedofnumberRateDeletion =intsconstitueninsertedofnumberInsertion PT=CPTTintsconstituenofnumberinsertionsofnumberintsconstituencorrectofnumberRateCombined !=Notably, the correct match rate is identical to thelabelled or bracketed recall rate.
The commonlyused precision score is calculated as the totalnumber of correct nodes over the sum of correct,substituted, and inserted nodes.
The insertion53score, arguably, subsumes crossing bracketserrors since crossing brackets errors are causedby the insertion of constituents even though notevery insertion causes an instance of crossingbrackets violation by definition.
In this respect,the crossing brackets score only implicitly hintsat the insertion problem while the insertion rateof the NIST scheme explicitly addresses thisissue.Because of the considerations above, theevaluations to be reported in the next sectionwere conducted using the NIST scheme.
Toobjectively present the two sides of the samecoin, the NIST scheme was used to evaluate theSurvey Parser in terms of constituent labellingand constituent bracketing before the two arefinally combined to yield performance scores.In order to conduct a precise evaluation ofthe performance of the parser, the experimentslook at the two aspects of the parse tree:labelling accuracy and bracketing accuracy.Labelling accuracy expresses how manycorrectly labelled constituents there are perhundred constituents and is intended to measurehow well the parser labels the constituents whencompared to the correct tree.
Bracketing ac-curacy attempts to measure the similarity of theparser tree to that of the correct one byexpressing how many correctly bracketedconstituents there are per hundred constituents.In this section, the NIST metric scheme will beapplied to the two properties separately beforean attempt is made to combine the two to assessthe overall performance of the Survey Parser.The same set of test data described in theprevious section was used to create four test setsof 1000 trees each to evaluate the performanceof the grammar induced from the training setsdescribed earlier.4.2 Labelling AccuracyTo evaluate labelling accuracy with the NISTscheme, the method is to view the labelledconstituents as a linear string with attachmentbracketing removed.
For [12], as an example,Figure 10 is a correct tree and Figure 11 is aparser-produced tree.
[12] It was probably used in the SouthernStates as well.Figure 10: A correct tree for [12]After removing the bracketed structure, we thenhave two flattened sequences of constituentlabels and compare them using the NISTscheme, which will yield the following statistics:Total # sentences evaluated : 1Total # constituent labels : 42Total # correct matches : 37 (88.1%)Total # labels substituted : 5 (11.9%)Total # labels deleted : 0 ( 0.0%)Total # labels inserted : 6Overall labelling accuracy : 73.8%Figure 11: A parser-produced tree for [12]Accordingly, we may concretely claim that thereare 42 constituent labels according to the correcttree, of which 37 (88.1%) are correctly labelledby the parser, with 5 substitutions (11.9%), 0deletion, and 6 insertions.
The overall labellingaccuracy is then calculated as 73.8%.A total of 4,000 trees, divided into four setsof 1,000 each, were selected from the test data toevaluate the labelling accuracy of the parser.Empirical results show that the parser achievedan overall labelling precision of over 80%.54Test Set 1 Test Set 2 Test Set 3 Test Set 4# % # % # % # %Tree 1000 1000 1000 1000Node 31676 34095 31563 30140Correct 27329 86.3 29263 85.8 27224 86.3 26048 86.4Subs 3214 10.1 3630 10.6 3253 10.3 3084 10.2Del 1133 3.6 1202 3.5 1086 3.4 1008 3.3Ins 2021 2316 1923 1839Prec.
83.9 83.1 84.1 84.1Overall 79.9 79.0 80.2 80.3Table 1: Labelling accuracyTable 1 shows that the Survey Parser scored86% or better in terms of correct match (labelledrecall) and nearly 84% in terms of labelledprecision rate for the four sets.
About 10% ofthe constituent labels are wrong (Subs) with adeletion rate (Del) of about 3.5%.
Countinginsertions (Ins), the overall labelling accuracy bythe parser is around 80%.4.3 Bracketing AccuracyA second aspect of the evaluation of thegrammar through the use of the Survey Parserinvolves the measuring of its attachmentprecision, an attempt to characterise thesimilarity of the parser-produced hierarchicalstructure to that of the correct parse tree.
Toestimate the precision of constituent attachmentof a tree, a linear representation of thehierarchical structure of the parse tree is design-ed which ensures that wrongly attached non-terminal nodes are penalised only once if theirsister and daughter nodes are correctly aligned.Table 2 shows that the parser achievednearly 86% for the bracketed correct match and82.8% for bracketing precision.
Consideringinsertions and deletions, the overall accuracyaccording to the NIST scheme is about 77%.This indicates that for every 100 bracket pairs77 are correct, with 23 substituted, deleted, orinserted.
In other words, for a tree of 100constituents, 23 edits are needed to conform tothe correct tree structure.Test Set 1 Test Set 2 Test Set 3 Test Set 4# % # % # % # %Tree 1000 1000 1000 1000Node 12451 13390 12411 11858Correct 10679 85.8 11402 85.2 10620 85.6 10271 86.6Subs 1088 8.7 1249 9.3 1115 9.0 968 8.2Del 648 5.5 739 5.5 676 5.4 619 5.2Ins 1127 1297 1092 1029Prec 82.8 81.7 82.8 83.7Overall 76.7 75.5 76.8 77.9Table 2: Bracketing accuracy4.4 Combined accuracyThe combined score for both labelling andbracketing accuracy is achieved through re-presenting both constituent labelling and un-labelled bracketing in a linear string described inthe previous sections.Table 3 gives the total number of trees inthe four test sets and the total number ofconstituents.
The number of correct matches,substitutions, insertions and deletions are in-dicated and combined scores computed ac-cordingly.
The table shows that the parserscored 86% and 83.5% respectively for labelledrecall and precision.
It is also shown that theparser achieved an overall performance of about79%.
Considering that the scoring programtends to underestimate the success rate, it isreasonable to assume a real overall combinedperformance of 80%.Test Set 1 Test Set 2 Test Set 3 Test Set 4# % # % # % # %Tree 1000 1000 1000 1000Node 44127 47485 43974 41998Correct 38008 86.1 40665 85.6 37844 86.1 36319 86.5Subs 4302 9.7 4879 10.3 4368 9.9 4052 9.6Del 1781 4.0 1941 4.1 1762 4.0 1627 3.9Ins 3148 3613 3015 2868Prec 83.6 82.7 83.7 83.9Overall 79.0 78.0 79.2 79.6Table 3: Combined accuracy4.5 DiscussionAlthough the scores for the grammar and theparser look both encouraging and promising, itis difficult to draw straightforward comparisonswith other systems.
Charniak (2000) reports amaximum entropy inspired parser that scored90.1% average precision/recall when trained andtested with sentences from the Wall StreetJournal corpus (WSJ).
While the difference inprecision/recall between the two parsers mayindicate the difference in terms of performancebetween the two parsing approaches, therenevertheless remain two issues to be investigat-ed.
Firstly, there is the issue of how text typesmay influence the performance of the grammarand indeed the parsing system as a whole.Charniak (2000) uses WSJ as both training andtesting data and it is reasonable to expect a fairlygood overlap in terms of lexical co-occurrencesand linguistic structures and hence goodperformance scores.
Indeed, Gildea (2001)suggests that the standard WSJ task seems to besimplified by its homogenous style.
It is thus yet55to be verified how well the same system willperform when trained and tested on a more?balanced?
corpus such as ICE.
Secondly, it isnot clear what the performance will be forCharniak?s parsing model when dealing with amuch more complex grammar such as ICE,which has almost three times as many non-terminal parsing symbols.
The performance ofthe Survey Parser is very close to that of anunlexicalised PCFG parser reported in Klain andManning (2003) but again WSJ was used fortraining and testing and it is not clear how welltheir system will scale up to a typologicallymore varied corpus.5 ConclusionThis article described a corpus of contemporaryEnglish that is linguistically annotated at bothgrammatical and syntactic levels.
It thendescribed a formal grammar that is auto-matically generated from the corpus andpresented statistics outlining the learning curveof the grammar as a function of training datasize.
Coverage by the grammar was presentedthrough empirical tests.
It then reported the useof the NIST evaluation metric for the evaluationof the grammar when applied by the SurveyParser on test sets totalling 4,000 trees.Through the size of the grammar in terms ofthe five canonical phrases as a function ofgrowth in training data size, it was observed thatthe learning curves for AJP, AVP, and VPculminated fairly rapidly with growing trainingdata size.
In contrast, NPs and PPs demonstratea sharp learning curve, which may havesuggested that there would be a lack ofsufficient coverage by the grammar for thesetwo phrase types.
Experiments show that such agrammar still had a satisfactory coverage forthese two with a near total coverage for the otherthree phrase types.The NIST scheme was used to evaluate theperformance of the grammar when applied in theSurvey Parser.
An especially advantageousfeature of the metric is the calculation of anoverall parser performance rate that takes intoaccount the total number of insertions in theparse tree, an important structural distortionfactor when calculating the similarity betweentwo trees.
A total of 4,000 trees were used toevaluate the labelling and bracketing accuraciesof the parse trees automatically produced by theparser.
It is shown that the LR rate is over 86%and LP is about 84%.
The bracketed recall is85.8% with a bracketed precision of 82.8%.Finally, an attempt was made to estimate thecombined performance score for both labellingand bracketing accuracies.
The combined recallis 86.1% and the combined precision is 83.5.These results show both encouraging andpromising performance by the grammar in termsof coverage and accuracy and therefore arguestrongly for the case of inducing formalgrammars from linguistically annotated corpora.A future research topic is the enhancement ofthe recall rate for clausal rules, which nowstands at just over 65%.
It is of great benefit tothe parsing community to verify the impact thesize of the grammar has on the performance ofthe parsing system and also to use a typo-logically more balanced corpus than WSJ as aworkbench for grammar/parser development.ReferencesCharniak, E. 2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1stAnnual Meeting of the North AmericanChapter of the ACL, Seattle, Washington.Fang, A.C. 1996a.
Grammatical tagging andcross-tagset mapping.
In S. Greenbaum(ed).Fang, A.C. 1996b.
The Survey Parser: Designand development.
In S. Greenbaum (ed).Fang, A.C. 2000.
From Cases to Rules and ViceVersa: Robust Practical Parsing withAnalogy.
In Proceedings of the Sixth Inter-national Workshop on Parsing Tech-nologies, 23-25 February 2000, Trento,Italy.
pp 77-88.Gildea, D. 2001.
Corpus variation and parserperformance.
In Proceedings of the Con-ference on Empirical Methods in NaturalLanguage Processing, 2001.Greenbaum, S. 1992.
A new corpus of English:ICE.
In Directions in Corpus Linguistics:Proceedings of Nobel Symposium 82,Stockholm 4-8 August 1991, ed.
by JanSvartvik.
Berlin: Mouton de Gruyter.
pp171-179.Greenbaum, S. 1996.
The International Corpusof English.
Oxford: Oxford UniversityPress.Klein, D. and C. Manning, 2003.
Accurate un-lexicalized parsing.
In Proceeding of the41st Annual Meeting of the Association forComputational Linguistics, July 2003. pp423-430.56
