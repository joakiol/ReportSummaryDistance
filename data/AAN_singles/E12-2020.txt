Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 97?101,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsHadoopPerceptron: a Toolkit for Distributed Perceptron Training andPrediction with MapReduceAndrea GesmundoComputer Science DepartmentUniversity of GenevaGeneva, Switzerlandandrea.gesmundo@unige.chNadi TomehLIMSI-CNRS andUniversite?
Paris-SudOrsay, Francenadi.tomeh@limsi.frAbstractWe propose a set of open-source softwaremodules to perform structured PerceptronTraining, Prediction and Evaluation withinthe Hadoop framework.
Apache Hadoopis a freely available environment for run-ning distributed applications on a com-puter cluster.
The software is designedwithin the Map-Reduce paradigm.
Thanksto distributed computing, the proposed soft-ware reduces substantially execution timeswhile handling huge data-sets.
The dis-tributed Perceptron training algorithm pre-serves convergence properties, thus guar-anties same accuracy performances as theserial Perceptron.
The presented modulescan be executed as stand-alone software oreasily extended or integrated in complexsystems.
The execution of the modules ap-plied to specific NLP tasks can be demon-strated and tested via an interactive web in-terface that allows the user to inspect thestatus and structure of the cluster and inter-act with the MapReduce jobs.1 IntroductionThe Perceptron training algorithm (Rosenblatt,1958; Freund and Schapire, 1999; Collins, 2002)is widely applied in the Natural Language Pro-cessing community for learning complex struc-tured models.
The non-probabilistic nature of theperceptron parameters makes it possible to incor-porate arbitrary features without the need to cal-culate a partition function, which is required forits discriminative probabilistic counterparts suchas CRFs (Lafferty et al 2001).
Additionally, thePerceptron is robust to approximate inference inlarge search spaces.Nevertheless, Perceptron training is propor-tional to inference which is frequently non-linearin the input sequence size.
Therefore, training canbe time-consuming for complex model structures.Furthermore, for an increasing number of tasks isfundamental to leverage on huge sources of dataas the World Wide Web.
Such difficulties renderthe scalability of the Perceptron a challenge.In order to improve scalability, Mcdonald etal.
(2010) propose a distributed training strat-egy called iterative parameter mixing, and showthat it has similar convergence properties to thestandard perceptron algorithm; it finds a separat-ing hyperplane if the training set is separable; itproduces models with comparable accuracies tothose trained serially on all the data; and reducestraining times significantly by exploiting comput-ing clusters.With this paper we present the HadoopPer-ceptron package.
It provides a freely availableopen-source implementation of the iterative pa-rameter mixing algorithm for training the struc-tured perceptron on a generic sequence labelingtasks.
Furthermore, the package provides two ad-ditional modules for prediction and evaluation.The three software modules are designed withinthe MapReduce programming model (Dean andGhemawat, 2004) and implemented using theApache Hadoop distributed programming Frame-work (White, 2009; Lin and Dyer, 2010).
Thepresented HadoopPerceptron package reduces ex-ecution time significantly compared to its serialcounterpart while maintaining comparable perfor-mance.97PerceptronIterParamMix(T = {(xt,yt)}|T |t=1)1.
Split T into S pieces T = {T1, .
.
.
,TS}2. w = 03. for n : 1..N4.
w(i,n) = OneEpochPerceptron(Ti ,w)5. w =?i ?i,nw(i,n)6. return wOneEpochPerceptron(Ti ,w?)1.
w(0) = w?
; k = 02. for n : 1..T3.
Let y?
= argmaxy?
w(k).f(xt,y?t)4. if y?
6= yt5.
x(k+1) = x(k) + f(xt,yt)?
f(xt,y?t)6. k = k + 17. return w(k)Figure 1: Distributed perceptron with iterative param-eter mixing strategy.
Each w(i,n) is computed in par-allel.
?n = {?1,n, .
.
.
, ?S,n}, ?
?i,n ?
?n : ?i,n ?0 and ?n : ?i ?i,n = 1.2 Distributed Structured PerceptronThe structured perceptron (Collins, 2002) is anonline learning algorithm that processes train-ing instances one at a time during each trainingepoch.
In sequence labeling tasks, the algorithmpredicts a sequence of labels (an element fromthe structured output space) for each input se-quence.
Prediction is determined by linear opera-tions on high-dimensional feature representationsof candidate input-output pairs and an associatedweight vector.
During training, the parameters areupdated whenever the prediction that employedthem is incorrect.Unlike many batch learning algorithms that caneasily be distributed through the gradient calcula-tion, the perceptron online training is more subtleto parallelize.
However, Mcdonald et al(2010)present a simple distributed training through a pa-rameter mixing scheme.The Iterative Parameter Mixing is given in Fig-ure 2 (Mcdonald et al 2010).
First the trainingdata is divided into disjoint splits of example pairs(xt,yt) where xt is the observation sequence andyt is the associated labels.
The algorithm pro-ceeds to train a single epoch of the perceptronalgorithm for each split in parallel, and mix thelocal models weights w(i,n) to produce the globalweight vector w. The mixed model is then passedto each split to reset the perceptron local weights,and a new iteration is started.
Mcdonald et al(2010) provide bound analysis for the algorithmand show that it is guaranteed to converge and finda seperation hyperplane if one exists.3 MapReduce and HadoopMany algorithms need to iterate over numberof records and 1) perform some calculation oneach of them and then 2) aggregate the results.The MapReduce programming model implementsa functional abstraction of these two operationscalled respectively Map and Reduce.
The Mapfunction takes a value-key pairs and produces alist of key-value pairs: map(k, v) ?
(k?, v?)?
;while the input the Reduce function is a key withall the associated values produced by all the map-pers: reduce(k?, (v?)?)
?
(k?
?, v??)?.
The modelrequires that all values with the same key are re-duced together.Apache Hadoop is an open-source implementa-tion of the MapReduce model on cluster of com-puters.
A cluster is composed by a set of comput-ers (nodes) connected into a network.
One nodeis designated as the Master while other nodesare referred to as Worker Nodes.
Hadoop is de-signed to scale out to large clusters built fromcommodity hardware and achieves seamless scal-ability.
To allow rapid development, Hadoophides system-level details from the applicationdeveloper.The MapReduce runtime automaticallyschedule worker assignment to mappers and re-ducers;handles synchronization required by theprogramming model including gathering, sort-ing and shuffling of intermediate data across thenetwork; and provides robustness by detectingworker failures and managing restarts.
The frame-work is built on top of he Hadoop DistributedFile System (HDFS), which allows to distributethe data across the cluster nodes.
Network trafficis minimized by moving the process to the nodestoring the data.
In Hadoop terminology an entireMapReduce program is called a job while individ-ual mappers and reducers are called tasks.4 HadoopPerceptron ImplementationIn this section we give details on how the train-ing, prediction and evaluation modules are im-plemented for the Hadoop framework using the98Figure 2: HadoopPerceptron in MapReduce.MapReduce programming model1.Our implementation of the iterative parame-ter mixing algorithm is sketched in Figure 2.At the beginning of each iteration, the train-ing data is split and distributed to the workernodes.
The set of training examples in adata split is streamed to map workers as pairs(sentence-id, (xt,yt)).
Each map worker per-forms a standard perceptron training epoch andoutputs a pair (feature-id, wi,f ) for each feature.The set of such pairs emitted by a map worker rep-resents its local weight vector.
After map workershave finished, the MapReduce framework guaran-tees that all local weights associated with a givenfeature are aggregated together as input to a dis-tinct reduce worker.
Each reduce worker producesas output the average of the associated featureweight.
At the end of each iteration, the reduceworkers outputs are aggregated into the global av-eraged weight vector.
The algorithm iterates Ntimes or until convergence is achieved.
At thebeginning of each iteration the weight vector ofeach distinct model is initialized with the globalaveraged weight vector resultant from the previ-ous iteration.
Thus, for all the iterations exceptfor the first, the global averaged weight vector re-sultant from the previous iteration needs to be pro-vided the map workers.
In Hadoop it is possibleto pass this information via the Distributed CacheSystem.In addition to the training module, the Hadoop-Perceptron package provides separate modulesfor prediction and evaluation both of them aredesigned as MapReduce programs.
The evalu-1The Hadoop Perceptron toolkit is available fromhttps://github.com/agesmundo/HadoopPerceptron .ation module output the accuracy measure com-puted against provided gold standards.
Predictionand evaluation modules are independent from thetraining modules, the weight vector given as inputcould have been computed with any other systemusing any other training algorithm as long as theyemploy the same features.The implementation is in Java, and we inter-face with the Hadoop cluster via the native JavaAPI.
It can be easily adapted to a wide range ofNLP tasks.
Incorporating new features by mod-ifying the extensible feature extractor is straight-forward.
The package includes the implementa-tion of the basic feature set described in (Suzukiand Isozaki, 2008).5 The Web User InterfaceHadoop is bundled with several web interfacesthat provide concise tracking information for jobs,tasks, data nodes, etc.
as shown in Figure 3.
Theseweb interfaces can be used to demonstrate theHadoopPerceptron running phases and monitorthe distributed execution of the training, predic-tion and evaluation modules for several sequencelabeling tasks including part-of-speech taggingand named entity recognition.6 ExperimentsWe investigate HadoopPerceptron training timeand prediction accuracy on a part-of-speech(POS) task using the PennTreeBank corpus (Mar-cus et al 1994).
We use sections 0-18 of the WallStreet Journal for training, and sections 22-24 fortesting.We compare the regular percepton trained se-rially on all the training data with the distributedperceptron trained with iterative parameter mix-ing with variable number of splits S ?
{10, 20}.For each system, we report the prediction accu-racy measure on the final test set to determineif any loss is observed as a consequence of dis-tributed training.For each system, Figure 4 plots accuracy re-sults computed at the end of every training epochagainst consumed wall-clock time.
We observethat iterative mixing parameter achieves compa-rable performance to its serial counterpart whileconverging orders of magnitude faster.Furthermore, we note that the distributed al-gorithm achieves a slightly higher final accuracy99Figure 3: Hadoop interfaces for HadoopPerceptron.Figure 4: Accuracy vs. training time.
Each point cor-responds to a training epoch.than serial training.
Mcdonald et al(2010) sug-gest that this is due to the bagging effect thatthe distributed training has, and due to parametermixing that is similar to the averaged perceptron.We note also that increasing the number ofsplits increases the number of epoch required toattain convergence, while reducing the time re-quired per epoch.
This implies a trade-off be-tween slower convergence and quicker epochswhen selecting a larger number of splits.7 ConclusionThe HadoopPerceptron package provides the firstfreely-available open-source implementation ofiterative parameter mixing Perceptron Training,Prediction and Evaluation for a distributed Map-Reduce framework.
It is a versatile stand alonesoftware or building block, that can be easilyextended, modified, adapted, and integrated inbroader systems.HadoopPerceptron is a useful tool for the in-creasing number of applications that need to per-form large-scale structured learning.
This is thefirst freely available implementation of an ap-proach that has already been applied with successin private sectors (e.g.
Google Inc.).
Making itpossible for everybody to fully leverage on hugedata sources as the World Wide Web, and developstructured learning solutions that can scale keep-ing feasible execution times and cluster-networkusage to a minimum.AcknowledgmentsThis work was funded by Google and The Scot-tish Informatics and Computer Science Alliance(SICSA).
We thank Keith Hall, Chris Dyer andMiles Osborne for help and advice.100ReferencesMichael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In EMNLP ?02:Proceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing, Philadel-phia, PA, USA.Jeffrey Dean and Sanjay Ghemawat.
2004.
Mapre-duce: simplified data processing on large clusters.In Proceedings of the 6th Symposium on Opeart-ing Systems Design and Implementation, San Fran-cisco, CA, USA.Yoav Freund and Robert E. Schapire.
1999.
Largemargin classification using the perceptron algo-rithm.
Machine Learning, 37(3):277?296.John Lafferty, Andrew Mccallum, and FernandoPereira.
2001.
John lafferty and andrew mc-callum and fernando pereira.
In Proceedings ofthe International Conference on Machine Learning,Williamstown, MA, USA.Jimmy Lin and Chris Dyer.
2010.
Data-Intensive TextProcessing with MapReduce.
Morgan & ClaypoolPublishers.Mitchell P. Marcus, Beatrice Santorini, and Mary A.Marcinkiewicz.
1994.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics, 19(2):313?330.Ryan Mcdonald, Keith Hall, and Gideon Mann.
2010.Distributed training strategies for the structured per-ceptron.
In NAACL ?10: Proceedings of the 11thConference of the North American Chapter of theAssociation for Computational Linguistics, Los An-geles, CA, USA.Frank Rosenblatt.
1958.
The Perceptron: A proba-bilistic model for information storage and organiza-tion in the brain.
Psychological Review, 65(6):386?408.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervised sequential labeling and segmentation us-ing giga-word scale unlabeled data.
In ACL ?08:Proceedings of the 46th Conference of the Associa-tion for Computational Linguistics, Columbus, OH,USA.Tom White.
2009.
Hadoop: The Definitive Guide.O?Reilly Media Inc.101
