Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1391?1396,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsSampling Techniques for Streaming Cross Document CoreferenceResolutionLuke Shrimpton?Victor Lavrenko?Miles Osborne?
?School of Informatics, University of Edinburgh: luke.shrimpton@ed.ac.uk?School of Informatics, University of Edinburgh: vlavrenk@inf.ed.ac.uk?Bloomberg, London: mosborne29@bloomberg.netAbstractWe present the first truly streaming cross doc-ument coreference resolution (CDC) system.Processing infinite streams of mentions forcesus to use a constant amount of memory and sowe maintain a representative, fixed sized sam-ple at all times.
For the sample to be repre-sentative it should represent a large number ofentities whilst taking into account both tempo-ral recency and distant references.
We intro-duce new sampling techniques that take intoaccount a notion of streaming discourse (cur-rent mentions depend on previous mentions).Using the proposed sampling techniques weare able to get a CEAFe score within 5% ofa non-streaming system while using only 30%of the memory.1 IntroductionCross document coreference resolution (CDC) -identifying mentions that refer to the same entityacross documents - is a prerequisite when combin-ing entity specific information from multiple docu-ments.
Typically large scale CDC involves apply-ing a scalable clustering algorithm to all the men-tions.
We consider streaming CDC, hence our sys-tem must conform to the streaming computationalresource model (Muthukrishnan, 2005).
Each men-tion is processed in bounded time and only a con-stant amount of memory is used.
Honoring theseconstraints ensures our system can be applied to in-finite streams such as newswire or social media.Storing all the mentions in memory is clearly in-feasible, hence we need to either compress mentionsor store a sample.
Compression is more computa-tionally expensive as it involves merging/forgettingmention components (for example: components ofa vector) whereas sampling decides to store or for-get whole mentions.
We investigate sampling tech-niques due to their computational efficiency.
We ex-plore which mentions should be stored while per-forming streaming CDC.
A sample should repre-sent a diverse set of entities while taking into ac-count both temporal recency and distant mentions.We show that using a notion of streaming discourse,where what is currently being mentioned dependson what was previously mentioned significantly im-proves performance on a new CDC annotated Twit-ter corpus.2 Related WorkThere are many existing approaches to CDC (Baggaand Baldwin, 1998; Lee et al, 2012; Andrews etal., 2014).
Few of them scale to large datasets.Singh et al (2011) proposed a distributed hierar-chical factor graph approach.
While it can processlarge datasets, the scalability comes from distribut-ing the problem.
Wick et al (2012) proposed a sim-ilar approach based on compressing mentions, whilescalable it does not conform to the streaming re-source model.
The only prior work that addressedonline/streaming CDC (Rao et al, 2010) was alsonot constrained to the streaming model.
None ofthese approaches operate over an unbounded streamprocessing mentions in constant time/memory.13913 Entities in StreamsStreams like Twitter are well known as being real-time and highly bursty.
Some entities are continu-ally mentioned throughout the stream (eg: PresidentObama) whereas others burst, suddenly peak in pop-ularity then decay (eg: Phillip Hughes, a cricketerwho died following a bowling injury).Capturing the information required to performstreaming CDC in constant space requires us to sam-ple from the stream.
For example we may consideronly the most recent information (eg: the previous24 hours worth of mentions).
This may not be idealas it would result in a sample biased towards burst-ing entities, neglecting the continually mentionedentities.
We propose three properties that are im-portant when sampling mentions from a stream oftweets:?
Recency: There should be some bias towardsrecent mentions to take into account the real-time nature of the stream.
The set of entitiesmentioned one day is likely to be similar to theset of entities mentioned on the following day.?
Distant Reference: The temporal gap betweenmentions of the same entity can vary drasti-cally, recency captures the small gaps though tocapture the larger gaps older mentions shouldbe stored.
A mention should be correctly re-solved if the last mention of the entity was ei-ther a day or a week ago.?
Entity Diversity: The sample should containmentions of many entities instead of storinglots of mentions about a few entities.
If thesample only contains mentions of the mosttweeted about entity (the one that is bursting)it is impossible to resolve references to otherentities.These properties suggest we should take into ac-count a notion of streaming discourse when sam-pling: mentions sampled should depend on the pre-vious mentions (informed sampling).4 ApproachWe implemented a representative pairwise stream-ing CDC system using single link clustering.
Men-tion similarity is a linear combination of mentiontext and contextual similarity (weighted 0.8 and 0.2respectively) similar to Rao et al (2010).
Mentiontext similarity is measured using cosine similarity ofcharacter skip bigram indicator vectors and contex-tual similarity is measured using tf-idf weighted co-sine similarity of tweet terms.
The stream is pro-cessed sequentially: we resolve each mention byfinding its nearest neighbor in the sample, linkingthe two mentions if the similarity is above the link-ing threshold.5 Sampling TechniquesThe sampling techniques we investigate are summa-rized below.
Each technique has an insertion andremoval policy that are followed each time a men-tion is processed.
New sampling techniques are indi-cated by a star (*).
The new sampling techniques re-quire the nearest neighbor to be identified.
As this isalready computed during resolution hence the over-head of these new techniques is very low.
Parame-ters particular to each sampling technique are notedin square brackets and are set using a standard gridsearch on a training dataset.?
Exact: To provide an upper bound on perfor-mance we forgo the constraints of the stream-ing resource model and store all previouslyseen mentions:Insertion: Add current mention to sample.
Re-moval: Do nothing.?
Window: We sample a moving window of themost recent mentions (first in, first out).
Forexample this technique assumes that if we areprocessing mentions on Monday with a win-dow of approximately 24 hours all relevant en-tities were mentioned since Sunday.Insertion: Add current mention to sample.
Re-moval: Remove oldest mention.?
Uniform Reservoir Sampling (Uniform-R):A uniform sample of previously seen mentionswill capture a diverse set of entities from theentire stream - taking into account diversity anddistant references.
This can be achieved usinga reservoir sample (Vitter, 1985).
We assumeeach previously seen mention is equally likelyto help resolve the current mention.1392Insertion: Add current mention with probabil-ity pi.
Removal: If a mention was insertedchoose a mention uniformly at random to re-move.Setting pi= k/N where k is the sample sizeand N is the number of items seen ensures thesample is uniform (Vitter, 1985).?
Biased Reservoir Sampling (Biased-R): Toresolve both distant and recent references weshould store recent mentions and some oldermentions.
An uninformed approach from ran-domized algorithms is to use an exponentiallybiased reservoir sample (Aggarwal, 2006; Os-borne et al, 2014).
It will store mostly re-cent mentions but will probabilistically allowolder mentions to stay in the sample.
For ex-ample this technique will sample lots of men-tions from yesterday and less from the day be-fore yesterday.Insertion: Add current mention with proba-bility piRemoval: If a mention was insertedchoose a mention uniformly at random to re-move.Unlike uniform reservoir sampling piis con-stant.
A higher value puts more emphasis onthe recent past.
[pi]?
Cache*: We should keep past mentions criti-cal to resolving current references (an informedimplementation of recency) and allow men-tions to stay in the sample for an arbitrary pe-riod of time to help resolve distance references.For example if the same mention is used to re-solve a reference on Saturday and Sunday itshould be in the sample on Monday.Insertion: Add current mention to sample.
Re-moval: Choose a mention that was not recentlyused to resolve a reference uniformly at randomand remove it.When a mention is resolved we find its mostsimilar mention in the sample, recording its usein a first in, first out cache of size n. The men-tion to be removed is chosen from the set ofmentions not in the cache.
We set n equal toa proportion of the sample size.
[Proportion ofmentions to keep]?
Diversity*: If we store fewer mentions abouteach distinct entity we can represent more enti-ties in the sample.
For example if news breaksthat a famous person died yesterday the sampleshould not be full of mentions about that en-tity at the expense of other entities mentionedtoday.Insertion: Add current mention to sample.
Re-moval: If there is a sufficiently similar mentionin the sample remove it else choose uniformlyat random to be removed.We remove the past mention most similar to thecurrent mention, but only if the similarity ex-ceeds a threshold.
[Replacement Threshold]?
Diversity-Cache (D-C)*: We combine Diver-sity and Cache sampling.Insertion: Add current mention to sample.
Re-moval: If there is a sufficiently similar mentionin the sample remove it else remove a mentionthat has not recently been used to resolve a ref-erence chosen uniformly at random.For this technique we first choose the replace-ment threshold then the proportion of mentionsto keep.
[Replacement threshold and propor-tion of mentions to keep]6 DatasetWe collected 52 million English tweets from the 1%sample of all tweets sent over a 77 day period.
Weperformed named entity recognition using Ritter etal.
(2011).
It is clearly infeasible for us to annotateall the mentions in the dataset.
Hence we annotateda sample of the entities.
As with most prior work wefocused on person named entity mentions (of whichthere is approximately 6 million in the dataset).To select the entities we first sampled two namesbased on how frequently they occur in the dataset:?Roger?
was chosen randomly from the low fre-quency names (between 1,000 and 10,000 occur-rences) and ?Jessica?
was chosen similarly frommedium frequency names (10,000 to 100,0000 oc-currences).
We first annotated all mentions ofthe names ?Roger?
and ?Jessica?
discarding entitiesmentioned once.
For the remaining entities we an-notated all their mentions (not restricting to men-tions that contained the words ?Roger?
or ?Jessica?
).1393Figure 1: Mention Frequency Distribution.This covers a diverse selection of people including:?Roger Federer?
(tennis player) and ?Jessie J?
(thesinger whose real name is ?Jessica Cornish?)
as wellas less popular entities such as porn stars and jour-nalists1.Some statistics of the dataset are summarized intable 1.
We also plot the mention frequency (how of-ten each entity was mentioned) distribution in figure1 which shows a clear power law distribution similarto what Rao et al (2010) reported on the New YorkTimes annotated corpus.
We show that recency anddistant reference are important aspects by plottingthe time since previous mention of the same entity(gap) for each mention in figure 2.
The gap is of-ten less than 24 hours demonstrating the importanceof recency.
There are also plenty of mentions withmuch larger gaps, demonstrating the need to be ableto resolve distant references.Source Mentions Entities Wiki Page ExistsRoger 5,794 137 69%Jessica 10,543 129 46%All 16,337 266 58%Table 1: Mention/entity counts and percentage of entitiesthat have a Wikipedia page.7 ExperimentsAs we are processing a stream we use a rolling eval-uation protocol.
Our corpus is split up into 11 con-stant sized temporally adjacent blocks each lasting1The annotations, including links to Wikipedia pages whenavailable, can be downloaded from https://sites.google.com/site/lukeshr/.Figure 2: Distribution of time since previous mention ofthe same entity (gap).
Each bar represents 24 hours.approximately one week.
Parameters are set using astandard grid search on one block then we progressto the next block to evaluate.
The first block is re-served for setting the linking threshold prior to ourrolling evaluation.
We report the average over theremaining blocks.
For all sampling techniques thathave a randomized component we report an averageover 10 runs.As the sample size will have a large effect on per-formance we evaluate using various sample sizes.We base our sample size on the average amount ofmentions per day (78,450) and evaluate our systemwith sample sizes of 0.25,0.5,1,2 times the averageamount of mentions per day.We evaluate using CEAFe (Luo, 2005), it is theonly coreference evaluation metric that can be triv-ially adapted to datasets with a sample of annotatedentities.
With no adaption it measures how well asmall amount of entities align with a system out-put over a large amount.
To make the evaluationmore representative we only use response clustersthat contain an annotated mention.
This scales pre-cision and maintains interpretability.
We determineif observed differences are significant by using aWilcoxon signed-rank test with a p value of 5% overthe 9 testing points.
Results are shown in table 2.?
Window: This shows the performance that canbe achieved by only considering recency.?
Uniform Reservoir Sampling (Uniform-R):This shows the performance achieved by usingan uninformed technique to store a diverse setof older mentions.1394Sample Sampling CEAFeSize Technique P R F0.25 Days Window 24.2 67.2 35.6Uniform-R 23.0 67.2 34.3Biased-R 24.8 67.8 36.319,613 Cache ?
25.5 67.2 37.0Mentions Diversity ?
27.6 69.2 39.4D-C ?
?
30.1 69.7 42.00.5 Days Window 31.3 69.4 43.1Uniform-R 29.9 69.1 41.7Biased-R 31.6 69.9 43.539,225 Cache ?
32.9 69.7 44.7Mentions Diversity ?
37.5 71.6 49.2D-C ?
?
40.1 72.3 51.61.0 Days Window 40.7 72.0 52.0Uniform-R 39.3 71.4 50.6Biased-R 40.9 72.3 52.378,450 Cache ?
42.0 72.0 53.1Mentions Diversity ?
48.5 74.3 58.7D-C ?
?
49.8 74.5 59.72.0 Days Window 50.2 74.1 59.8Uniform-R 49.2 73.7 58.9Biased-R 50.3 74.1 59.9156,900 Cache ?
50.9 74.1 60.4Mentions Diversity ?
55.2 75.2 63.7D-C ?
55.5 75.3 63.9?600,000 Exact 59.7 75.4 66.6MentionsTable 2: CEAFe performance for various sample sizesand sampling techniques.
?
indicates significant improve-ment over Window sampling.
?
indicates significant im-provement over Diversity sampling?
Biased Reservoir Sampling (Biased-R): Un-informed sampling of older mentions is not suf-ficient to significantly improve performance.?
Cache: By using an informed model of recencywe keep mentions critical to resolving refer-ences currently being tweeted resulting in a sig-nificant performance improvement.?
Diversity: By using an informed technique toincrease the amount of distinct entities repre-sented in the sample we significantly improveperformance.?
Diversity-Cache (D-C): By combining thenew sampling techniques we significantly im-prove performance.
Once we have increasedthe amount of entities represented in the sam-ple we are still able to benefit from an informedmodel of recency.Using uninformed sampling techniques (reservoirsampling) does not result in a significant perfor-mance improvement over Window sampling, onlyinformed sampling techniques show a significantimprovement.
As the sample size increases the per-formance difference decreases.
With larger samplesthere is space to represent more entities and it is lesslikely to remove a useful mention at random.8 ConclusionWe presented the first truly streaming CDC sys-tem, showing that significantly better performance isachieved by using an informed sampling techniquethat takes into account a notion of streaming dis-course.
We are able to get to within 5% of an ex-act system?s performance while using only 30% ofthe memory required.
Instead of improving perfor-mance by using an uninformed sampling techniqueand doubling the memory available, similar perfor-mance can be achieved by using the same amount ofmemory and a informed sampling technique.
Fur-ther work could look at improving the similaritymetric used, applying these sampling techniques toother streaming problems or adding a mention com-pression component.ReferencesCharu C Aggarwal.
2006.
On biased reservoir samplingin the presence of stream evolution.
In Proceedings ofthe 32nd international conference on Very large databases, pages 607?618.
VLDB Endowment.Nicholas Andrews, Jason Eisner, and Mark Dredze.2014.
Robust entity clustering via phylogenetic in-ference.
In Association for Computational Linguistics(ACL).Amit Bagga and Breck Baldwin.
1998.
Entity-basedcross-document coreferencing using the vector spacemodel.
In Proceedings of the 17th international con-ference on Computational linguistics-Volume 1, pages79?85.
Association for Computational Linguistics.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entity and1395event coreference resolution across documents.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages489?500.
Association for Computational Linguistics.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of Human LanguageTechnology Conference and Conference on EmpiricalMethods in Natural Language Processing.S Muthukrishnan.
2005.
Data streams: Algorithms andapplications.
Now Publishers Inc.Miles Osborne, Ashwin Lall, and Benjamin Van Durme.2014.
Exponential reservoir sampling for streaminglanguage models.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics (Volume 2: Short Papers), pages 687?692.
Asso-ciation for Computational Linguistics.Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with applica-tion to twitter.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 181?189.
Association for Computational Lin-guistics.Delip Rao, Paul McNamee, and Mark Dredze.
2010.Streaming cross document entity coreference resolu-tion.
In Coling 2010: Posters, pages 1050?1058.
Col-ing 2010 Organizing Committee.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An experi-mental study.
In EMNLP.Sameer Singh, Amarnag Subramanya, Fernando Pereira,and Andrew McCallum.
2011.
Large-scale cross-document coreference using distributed inference andhierarchical models.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages793?803.
Association for Computational Linguistics.Jeffrey S Vitter.
1985.
Random sampling with a reser-voir.
ACM Transactions on Mathematical Software(TOMS), 11(1):37?57.Michael Wick, Sameer Singh, and Andrew McCallum.2012.
A discriminative hierarchical model for fastcoreference at large scale.
In Proceedings of the 50thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 379?388.Association for Computational Linguistics.1396
