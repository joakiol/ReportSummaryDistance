Named Entity Recognition in Biomedical Texts using an HMM ModelShaojun ZhaoDepartment of Computing ScienceUniversity of AlbertaEdmonton, Canada, T6G 2H8shaojun@cs.ualberta.caAbstractAlthough there exists a huge number ofbiomedical texts online, there is a lack of toolsgood enough to help people get information orknowledge from them.
Named entityRecognition (NER) becomes very importantfor further processing like informationretrieval, information extraction andknowledge discovery.
We introduce a HiddenMarkov Model (HMM) for NER, with a wordsimilarity-based smoothing.
Our experimentshows that the word similarity-basedsmoothing can improve the performance byusing huge unlabeled data.
While manysystems have laboriously hand-coded rules forall kinds of word features, we show that wordsimilarity is a potential method toautomatically get word formation, prefix,suffix and abbreviation informationautomatically from biomedical texts, as wellas useful word distribution information.1 IntroductionIn the Message Understanding Conference(MUC), Named entity Recognition aims to classifyproper nouns, dates, time, measures and locations,etc.
Many researchers adapt their systems fromMUC to the biomedical domain, such as (Fukudaet al1998), (Proux et al1998), (Nobata et al2000),(Collier et al2000), (Gaizauskas et al2000),(Kazama et al2002), (Takeuchi et al2002), (Leeet al2003) and (Zhou et al2004).
As opposed torule-based systems, machine learning-basedsystems could train their models on labeled data.But due to the irregular forms of biomedical texts,people still need to carefully choose word featuresfor their systems.
This work requires domainspecific knowledge.
How to get the domainknowledge automatically is a question that has notbeen fully investigated.
Our system is built on anHMM model with the words themselves as thefeatures.
Huge unlabeled corpus is gathered fromMEDLINE.
Word similarity information iscomputed from the corpus and we use a wordsimilarity-based smoothing to overcome the datasparseness.2 Data Preparation2.1 Labeled DataOur labeled data is from GENIA 3.02 (Ohta et al2002), which contains 2,000 abstracts (360Kwords).
It has been annotated with semanticinformation such as DNA, protein annotations.These are useful for training models.
It containsPart of Speech (POS) information as well.Although POS is not considered very useful forNER in newspaper articles, it can dramaticallyimprove the performance of NER in biomedicaltexts (Zhou et al2004).
Our model is trained fromthis labeled data.2.2 Unlabeled DataWe downloaded 17G XML abstract data fromMEDLINE, which contains 1,381,132 abstracts.Compared to the labeled data, we have far moreunlabeled data, and the amount of availableunlabeled data increases every day.
We used thisunlabeled data for computing word similarity.
Weextracted 66,303,526 proximity relationships fromthe unlabeled data.3 Distributional Word Similarity?Words that tend to appear in the same contextstend to have similar meanings.?
(Harris 1968).
Forexample, the words corruption and abuse aresimilar because both of them can be subjects ofverbs like arouse, become, betray, cause, continue,cost, exist, force, go on, grow, have, increase, leadto, and persist, etc, and both of them can modifynouns like accusation, act, allegation, appearance,and case, etc.Many methods have been proposed to computedistributional similarity between words, e.g.,(Hindle, 1990), (Pereira et al 1993), (Grefenstette1994) and (Lin 1998).
Almost all of the methodsrepresent a word by a feature vector where eachfeature corresponds to a type of context in whichthe word appeared.843.1 Proximity-based SimilarityIt is natural to use dependency relationship(Mel'?uk, 1987) as features, but a parser has to beavailable.
Since biomedical text is highly irregular,and is very different from text like newspaper, aparser developed for the newspaper domain maynot perform very well on biomedical text.
Sincemost dependency relationships involve words thatare situated close to one another, the dependencyrelationships can often be approximated by co-occurrence relationships within a small window(Turney 2001); (Terra and Clarke 2003).
Wedefine the features of the word w to be the firstnon-stop word on either side of w and theintervening stop words (which can be defined asthe top-k most frequent words in the corpus).
Forexample, for a sentence ?He got a job from thiscompany.?
(Considering a, from and this to be stopwords.
), the features of job provided by thissentence are shown in Table 1.Features Frequency(left, got) 0.50(left, a) 0.50(right ,from) 0.33(right, this) 0.33(right, company) 0.33?
?Table 1: Features for word ?job?3.2 Computing Word SimilarityOnce the contexts of a word are represented as afeature vector, the similarity between two wordscan be computed using their context vectors.
Weuse (u1, u2 ?
un) and (v1, v2 ?
vn) to denote thefeature vectors for the words u and v respectively,where n is the number of feature types extractedfrom a corpus.
We use fi to denote the ith feature.The point-wise mutual information (PMI)between a feature fi and a word u measures thestrength association between them.
It is defined as:( ) ( )( ) ( )????????
?= uPfPufPufpmiiii,log,where P(fi,u) is the probability of fi co-occurringwith u; P(fi) is the probability of fi co-occurringwith any word; and P(u) is the probability of anyfeature co-occurring with u.The similarity between word u and v is definedas the Cosine of PMI:( ) ( ) ( )( ) ( )???===?
?=ni ini ini iiwordvfpmiufpmivfpmiufpmivusim12121,,,,,Different similarity measures of distributionalsimilarity can affect the quality of the result to sstatistically significant degree.
(Zhao and Lin 2004)shows that the Cosine of PMI is a significantlybetter similarity measure than several othercommonly used similarity measures.Similar words are computed for each word in theunlabeled data.
Only a subset of the similarityinformation is useful, because the similarity ofwords outside of the training data and test datavocabulary is not used.
We only take into accountthe similar words that occur in the training datamore than 10 times and only those word pairswhich have point-wise mutual information greaterthan a threshold (0.04).
Table 2 shows thecomputing result for ?IL-0?1:Similar Words Similarityinterleukin-0 0.510891IL-00    0.486665IFN-gamma 0.44945TNF-alpha    0.44702GM-CSF   0.438226TNF 0.37703IL-0beta 0.365072interferon-gamma 0.350704IL0 0.336974?
?Table 2: Similar words for ?IL-0?Table 2 also shows that the similar words cancapture word formation (IL-00, IL-0beta, and IL0etc) and abbreviation (interleukin-0) information.A complete list of these word pairs and theirsimilarity is available online 2 .
The rule-basedsystem may not able to capture words like IL-0ra,IL-0Ralpha, which are in the similar word list ofIL-0, and it is very likely that they belong to thesame semantic category.
Many different kinds ofexpressions for numbers (like 0, 00-00, 00.00, -00,0/0, five, six, 0-, iii, IV etc) are grouped togetherautomatically.4 HMM Model and Smoothing SchemaWe follow the HMM model introduced in (Zhouet al2004).
The structure of an HMM modelcontains States and observations.
In our model,each state is represented by a semantic tag, or aPOS tag if the semantic tag is not available; eachobservation contains a word sequence.
The maincomputing difficulty in (Zhou et al2004) is theprobability of a tag given a word sequence:formula (1).
We use formula (2) to estimateformula (1).
If the bigram is unseen in the trainingdata, we use formula (3).
If the unigram is alsounseen, we use the unknown information which is1 We changed any single digit to 0.2 http://www.cs.ualberta.ca/~shaojun/biolist.txt85gathered from the low frequency words in thetraining data.
( )( )( ) (3)                                    |(2)                       ,|(1)                       |1ttttttwordtagPwordwordtagPcewordsequentagP+We find that about 26% of the bigrams (wordt,wordt+1) in the testing data is unseen, so thesmoothing is critical.In order to compute formula (1), we can use theback-off (Katz 1987); (Bikel et al1999) approach.Baseline1 and Baseline2 in our system usedifferent back-off schema.The following formula is introduced in (Lee1999) for word similarity-based smoothing:)4(),()|(),()|()()(1??????+??
?=ttttwSwttwSwtttttt wwsimwtagPwwsimwtagPwhere S(w) is a set of candidate similar words andsim(w,w?)
is the similarity between word w and w?.Word similarity-based smoothing approach is usedin our system to make advantage of the hugeunlabeled corpus.
In order to plug the wordsimilarity-based smoothing into our HMM model,we made several extensions to formula (4).For each word w, we define p as the distributionof w?s tags, which are annotated in the trainingdata.
We use the KL-Divergence to compute thedistance between two distributions:( ) ( ) ( )( )( )?=xxpxpxpppKLD21log|| 121We define the similarity between the tagdistributions of word w and w?
as:( ) ( ) ( )( )wtagPwtagPKLDwwsimtag ?+=?
||||11,The harmonic average of word similarity and tagdistribution similarity is defined as the similarity ofword w and w?
used in our system.
( ) ( ) ( )( ) ( )wwsimwwsimwwsimwwsimwwstagwordtagword?+???
?=?,,,,2,So, we get formula (5) and (6).
Formula (5) isfor bigram smoothing and formula (6) is forunigram smoothing.
( )( ) ( )( ) ( )( )( )( ) ( )( ) ( )( )(6),|,|(5),,|,,|11111111111111????++++++++??????++??++++???=??
?=ttttttttwSwttwSwttttttwSwttwSwttttttttwwswtagPwwswtagPwwswwtagPwwswwtagPBecause it is natural to back-off from bigram tounigram, in our system, a back-off smoothingapproach is combined with the word similarity-based smoothing.
We follow these procedures tocompute formula (1).1.
Check the frequency of the bigram (wt, wt+1).If the frequency is high (>10), use formula(2).
Stop.2.
Check the frequency of the unigram (wt).
Ifthe frequency of the unigram is high (>30),use formula (3).
Stop.3.
Try formula (5) for bigram smoothing, andcheck the frequency summary of the similarwords involved in the smoothing.
If thesummary is high (>10), use formula (5).Stop.4.
Try formula (6) for unigram smoothing, andcheck the frequency summary for this case.If the summary is high (>30), use formula(6).
Stop.5.
If the bigram is not unseen, use formula (2).Stop.6.
If the unigram is not unseen, use formula (3).Stop.7.
Use low frequency (<5) word information inthe training data and formula (3).Our Baseline1 uses step 5, 6 and 7; Baseline2uses step 1, 2, 5, 6 and 7.5 Experiment ResultThe experiment results are shown in Table 3:Methods R P F-scoreBaseline1 64.77% 59.87% 62.22%Baseline2 66.99% 61.25% 63.99%Our system 69.41% 62.98% 66.04%Table 3: Performance comparisonThe Baseline2 outperforms Baseline1 because itprevents from using low frequency unigrams, andour system outperforms Baseline1 and Baseline2because it prevents from using low frequencybigrams and unigrams.
Our system benefits fromhuge unlabeled corpus.6 ConclusionWe trained an HMM model on labelled data torecognize named entities in biomedical texts.
Wordsimilarity information was computed from hugeunlabeled data.
A word similarity-based smoothingmethod was integrated into the system, andimproved the overall performance.
We would liketo see if it could also be plugged into other existingsystems, and hopefully also improve theirperformance.We also argue that the automatically acquiredsimilar words are rich with word features, such asword formation, prefix, suffix, abbreviation,expression variation and clustering information.We will further investigate the usefulness of themin the future.867 AcknowledgementsThanks to Dekang Lin and other members in theNatural Language Processing Group at theUniversity of Alberta for helpful discussion, theanonymous reviewers for their insightfulcomments.
This material is based upon worksupported by the Alberta Ingenuity Centre forMachine Learning (AICML).ReferencesBikel, D., Schwartz, R., Weischedel, R. 1999.
AnAlgorithm that Learns What's in a Name.
In Proc.of Machine Learning (Special Issue on NLP).Collier, N., Nobata, C., Tsujii, J.
2000.Extracting the names of genes and gene productswith a hidden Markov model.
In Proc.
ofCOLING 2000, pages 201-207.Fukuda, K., Tsunoda, T., Tamura, A., Takagi, T.1998.
?Toward Information extraction:Identifying protein names from biologicalpapers?, in Proc.
of the Pacific Symposium onBiocomputing 98 (PSB 98), HawaiiGaizauskas, R., Demetriou, G., Humphreys, K.2000.
Term Recognition and Classification inBiological Science Journal Articles.
In Proc.
ofthe Computational Terminology for Medical andBiological Applications Workshop of the 2ndInternational Conference on NLP, pages 37-44.Grefenstette, G. 1994.
Explorations in AutomaticThesaurus Discovery, Kluwer AcademicPublishers, BostonHarris, Z.S.
1968.
Mathematical Structures ofLanguage.
New York: Wiley.Hindle, D. 1990.
Noun Classification fromPredicate-Argument Structures.
In Proceedingsof ACL-90.
pp.
268-275.
Pittsburgh,PennsylvaniaKatz, S.M.
1987.
Estimation of Component of aSpeech Recognizer.
IEEE Transactions onAcoustics.
Speech and Signal Processing.
35.400-401.Kazama, J., Makino, T., Ohta, Y., Tsujii, J.
2002.Tuning Support Vector Machines for BiomedicalNamed Entity Recognition.
In Proc.
of theWorkshop on Natural Language Processing inthe Biomedical Domain (at ACL?2002), pages 1-8Lee, K.J., Hwang, Y.S., Rim H.C. 2003.
Twophase biomedical NE Recognition based onSVMs.
In Proceedings of the ACL-03 Workshopon Natural Language Processing in Biomedicine.pp.33-40.
Sapporo, JapanLee, L. 1999.
Measures of distributional similarity.In Proc.
of the 37th Annual Meeting of theAssociation for Computational Linguistics, 1999,pp.
25-32.Lin, D. 1998.
Automatic Retrieval and Clusteringof Similar Words.
In Proceedings of COLING-ACL98.
Montreal, Canada.Mel'?uk, I.
A., 1987.
Dependency Syntax: theoryand practice.
State University of New YorkPress.
Albany, NY.Nobata, C., Collier, N., Tsujii, J.
2000.Comparison between Tagged Corpora for theNamed Entity Task.
In the Proceedings of ACL2000 Workshop on Comparing Corpora.
HongKong, China.
pp.
20-27Ohta, T., Tateisi, Y., Kim, J., Mima, H., Tsujii, J.2002.
The GENIA corpus: An annotatedresearch abstract corpus in molecular biologydomain.
In Proc.
of HLT 2002.Pereira, F., Tishby, N., Lee, L. 1993.Distributional Clustering of English Words.
InProceedings of ACL-93.
pp.
183-190.
Columbus,Ohio.Proux, D., Rechenmann, F., Julliard, L., Pillet, V.,Jacq, B.
1998.
Detecting Gene Symbols andNames in Biological Texts: A First Step towardPertinent Information Extraction.
In Proc.
ofGenome Inform Ser Workshop Genome Inform,pages 72-80.Takeuchi, K., Collier, N. 2002.
Use of SupportVector Machines in Extended Named EntityRecognition.
In Proc.
of the Sixth Conference onNatural Language Learning (CONLL 2002),pages 119-125.Terra, E. L., Clarke, C. 2003.
Frequency Estimatesfor Statistical Word Similarity Measures.
In theProceedings of the 2003 Human LanguageTechnology Conference, pp.244-251.
Edmonton,Canada, MayTurney, P.D.
2001.
Mining the Web for synonyms:PMI-IR versus LSA on TOEFL, Proceedings ofthe Twelfth European Conference on MachineLearning (ECML-2001), Freiburg, Germany, pp.491-502.Zhou, G., Zhang, J., Su, J., Shen, D., Tan, C. 2004.Recognizing names in biomedical texts: amachine learning approach.
BioinformaticsAdvance Access.Zhao, S., Lin, D. 2004.
A Nearest-NeighborMethod for Resolving PP-Attachment Ambiguity.In Proceedings of the First International JointConference on Natural Language Processing,2004.
Sanya, China.87
