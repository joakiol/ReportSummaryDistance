Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 18?27,Beijing, August 2010Textual Entailment Recognition using Word Overlap,Mutual Information and Subpath SetYuki MuramatsuNagaoka University ofTechnologymuramatsu@jnlp.orgKunihiro UdakaNagaoka University ofTechnologyudaka@jnlp.orgKazuhide YamamotoNagaoka University ofTechnologyyamamoto@jnlp.orgAbstractWhen two texts have an inclusionrelation, the relationship between them iscalled entailment.
The task ofmechanically distinguishing such arelation is called recognising textualentailment (RTE), which is basically akind of semantic analysis.
A variety ofmethods have been proposed for RTE.However, when the previous methodswere combined, the performances werenot clear.
So, we utilized each method asa feature of machine learning, in order tocombine methods.
We have dealt withthe binary classification problem of twotexts exhibiting inclusion, and proposeda method that uses machine learning tojudge whether the two texts present thesame content.
We have built a programcapable to perform entailment judgmenton the basis of word overlap, i.e.
thematching rate of the words in the twotexts, mutual information, and similarityof the respective syntax trees (SubpathSet).
Word overlap was calclated byutilizing BiLingual EvaluationUnderstudy (BLEU).
Mutual informationis based on co-occurrence frequency, andthe Subpath Set was determined by usingthe Japanise WordNet.
A Confidence-Weighted Score of 68.6% was obtainedin the mutual information experiment onRTE.
Mutual information and the use ofthree methods of SVM were shown to beeffective.1 IntroductionThis paper can help solve textual entailmentproblems.
Researchers of natural languageprocessing have recently become interested inthe automatic recognition of textual entailment(RTE), which is the task of mechanicallydistinguishing an inclusion relation.
Textimplication recognition is the task of taking atext (T) and a hypothesis (H), and judgingwhether one (the text) can be inferred from theother (hypothesis).
Here below is an exampletask.
In case of entailment, we call the relation tobe ?true?
).Example 1: Textual entailment recognition.T: Google files for its long-awaited IPO.H: Google goes public.Entailment Judgment: True.For such a task, large applications such asquestion answering, information extraction,summarization and machine translation areinvolved.
A large-scale evaluation workshop hasbeen conducted to stimulate research onrecognition of entailment (Dagan et al, 2005).These authors divided the RTE methods into sixmethods.
We focused on 3 methods of them.P?rez and Alfonseca?s method (P?rez andAlfonseca, 2005) used Word Overlap.
Thismethod is assumed to have taken place whenwords or sentences of the text and the hypothesisare similar, hence the relation should be true.P?rez and Alfonseca used  the BLEU algorithmto calculate the entailment relationship.Glickman et als.
method was considered asusing statistical lexical relations.
These authorsassumed that the possibility of entailment werehigh when the co-occurrence frequency of theword in the source and the target were high.18While this may be correct, we believenevertheless that it problematic not to considerthe co-occurrence of the hypothesis words.
Thisbeing so, we proposed to use mutual information.Finally, Herrera et als.
method is based onSyntactic matching.
They calculated the degreeof similarity of the syntax tree.We combined these three methods usingmachine learning techniques.2 Related WorksDagan et al (Dagan et al 2005) conductedresearch in 2005 on how to evaluate data ofRTE; the authors insisted on the need ofsemantic analysis.
As a first step, theyconsidered the problem of textual entailment,proposing how to build evaluation data.
Theualso organised and workshop on this topic.
Theirevaluation data are problems of binaryclassification of the texts to be compared.
Theyused a sentence extracted from a newspapercorpus, and built a hypothesis from this textusing one of seven methods: question answering,sentence comprehension, information extraction,machine translation, paraphrasing, informationretrieval and comparable documents.
Theyproposed a method of evaluation using RTE, andthey introduced several RTE methods.Odani et al (Odani et al 2005) did research onthe construction of evaluation data in Japan,mentionning that there was a problem in theevaluation data of Dagan et al For example,they stated that ?The evaluation data that heconstructed are acting some factors.
So it isdifficult to discuss the problem?.
Next, they didan RTE evaluation data using Japanese.
Theinference factors for judging entailmentjudgment were divided into five categories:inclusion, lexicon (words that can?t be declined),lexicon (declinable words), syntax and inference.The subclassification was set for eachclassification, and Japanese RTE evaluation datawas constructed.
In addition, a dictionary andWeb text were used for the entailment judgment.The authors were able to solve entailmentjudgment with words or phrases containingsynonyms and/or a super-sub type relation.However, this classification lacks precision.For example, they defined the term ?lexicon(words that cannot be declined)?
as ?Themeaning and the character of the noun that existsin text are data from which information on thetruth of hypothesis is given?.
Given this lack ofclarity, we considered this method to be difficultto reproduce.However, the evaluation data they built isgeneral and available for public use.
Regardingthe research using the evaluation data of suchRTE, there have been many reports in theworkshop.For example, P?rez and Alfonseca (P?rez andAlfonseca, 2005) assumed that the possibility ofentailment was high when the text matched thehypothesis.
The concordance rate of the text andthe hypothesis was then calculated for judgingthe text and the hypothesis of the inclusionrelation.
In their research, they used BiLingualEvaluation Understudy (BLEU) to evaluatemachine translation.
An entailment judgment of?true?
was given when the BLEU score wasabove than a given threshold decided in advance.The evaluation data of Dagan et al was used inthe experiment, and its accuracy was about 50%.The evaluation data of comparable documenttypes were the results with the highest accuracy.Hence the authors concluded that this methodcan be considered as a baseline of RTE.
Wedealt with it as word overlap.Glickman et al (Glickman et al 2005)conducted research using co-occurring words.They assumed that the entailment judgment was?true?
when the probability of co-occurrencebetween the text and the hypothesis was high.
Inaddition, the content word of the text with thehighest co-occurrence probability was calculatedfrom the content word of all of the hypotheses,and it was proposed as a method for entailmentjudgment.
A Web search engine was used tocalculate in the co-occurrence probability.
Thisexperiment yielded an accuracy ofapproximately 58%, while the evaluation data ofcomparable document types was about 83%.This being so, the authors concluded that theyhave been able to improve the results with thehelp of other deep analytical tools.
We improvedthis method, and used it as mutual information.Herrera et al (Herrera et al, 2005) focused onsyntactic similarity.
They assumed that theentailment judgment was ?true?
when thesyntactic similarity of the text and the hypothesiswas high.
In addition, they used WordNet forconsidering identifiable expressions.
The results19of the experiment yielded an accuracy ofapproximately 57%.
We improved this method,and used it then as subpath set.Prodromos Malakasiotis and IonAndroutsopoulos (Prodromos Malakasiotis andIon Androutsopoulos, 2007) used SupportVector Machines.
They assumed that theentailment judgment was ?true?
when thesimilarity of words, POS tags and chunk tagswere high.
The results of the experiment yieldedan accuracy of approximately 62%.
However,they forgot to combine past RTE methods asfeature of SVM.The authors of this paper present a new RTEmethod.
We propose to combine word overlap,mutual information and subpath sets.
We dealtwith SVM by using 3 methods equally asfeatures, and we estimated higher precision thanwhen using individual; independent methods.3 Textual Entailment Evaluation DataWe used the textual entailment evaluation dataof Odani et al for the problem of RTE.
Thisevaluation data is generally available to thepublic at the Kyoto University1.The evaluation data comprises the inferencefactor, subclassification, entailment judgment,text and hypothesis.
Table 1 gives an example.The inference factor is divided into fivecategories according to the definition providedby Odani et al: inclusion, lexicon (indeclinableword), lexicon (declinable word), syntax andinference.
They define the classificationviewpoint of each inference factor as follows:Example 2: Classification criteria of inferencefactors?
Inclusion: The text almost includes thehypothesis.Table 1: RTE Evaluation data of Odani et al1 http://www.nlp.kuee.kyoto-u.ac.jp/nl-resource?
Lexicon (Indeclinable Word): Information ofthe hypothesis is given by the meaning or thebehaviour of the noun in the text.?
Lexicon (Declinable Word): Information ofthe hypothesis is given by the meaning or thebehaviour of the declinable word in the text.?
Syntax: The text and the hypothesis have arelation of syntactic change.
?Inference: Logical form.They divided the data into 166 subclasses,according to each inference factor.
Theentailment judgment is a reliable answer in thetext and the hypothesis.
It is a difficult problemto entailment judgment for the criteria answer.Therefore, when they reported on the RTEworkshop, they assumed the followingclassification criteria:Example 3: Classification criteria of entailmentdetermination.??
(Talw): When the text is true, the hypothesisis always true.??
(Talm): When the text is true, the hypothesisis almost true.??
(Fmay): When the text is true, the hypothesismay be true.??
(Falw): When the text is true, the hypothesisis false.In terms of the text and the hypothesis, whenwe observed the evaluation data, the evaluationdata accounted for almost every sentence in boththe texts and the hypotheses, and also thehypotheses were shorter than the texts.There is a bias in the number of problemsevaluated by the inference factor and by thesubclassification.
The number of evaluation dataopen to the public now stands at 2471.Inference Factor Sub-ClassificationEntailmentJudgmentText HypothesisLexicon(Indeclinable Word)Behavior ?
Toyota openeda luxury car  shop.Lexus isa luxury car.204 Proposal MethodUp now, a number of methods have beenproposed for RTE.
However, when the previousmethods were combined, the performances werehard to judge.
Hence, we used each method as afeature of machine learning, and combined themthen.The input text and the hypothesis wereconsidered as a problem of binary classification(?true?
or ?false?).
Therefore, we employedsupport vector machines (Vapnik, 1998), whichare often used to address binary classificationproblems (in fact, we implemented our systemwith Tiny SVM).
With this method we achievedhigher precision than with individualindependent methods.Figure 1 shows our proposed method.Figure 1: Our Proposed MethodIn the following sections, we will describe thethree features used in machine learning.4.1 Word OverlapIt is assumed that when words or sentences ofthe text and the hypothesis are similar, therelation should be true.
P?rez and Alfonsecaused a BLEU algorithm to calculate theentailment between the text and the hypothesis.BLEU is often used to evaluate the quality ofmachine translation.
Panieni et al provided thefollowing definition of BLEU.
In particular, theBLEU score between length r of the sentence Band length c of the sentence A is given by theformulas (1) and (2):( )1( , ) exp( log( ) / ) (1)1 {1, / } (2)niiBleu A B BP p nBP exp max r c=== ?
?where pi represents the matching rate of n-gram.The n-gram of this method was calculated asword n-gram.
We assumed n = 1 and used thepublic domain program NTCIR7 2 .
Here is anexample of the calculation.Example 4: Calculation by BLEU.T:???????????
(The moon isEarth's satellite.)H:???????????
(The moon isaround the Earth.
)BLEU:0.75We estimated n = 1 for the following reasons:1.
The reliability of word overlap is not highwhen n is large.2.
The calculated result of BLEU oftenbecomes 0 when n is large.First, we will explain the reason 1 mentionedabove.
The report of Kasahara et al (Kasahara etal., 2010) is a reproduction of the one providedby P?rez et al(P?rez et al, 2005).
They preparedan original RTE evaluation set of readingcomprehension type, and proposed a new RTEsystem using a BLEU algorithm.
When theyexperimented by increasing the maximumnumber of elements n of word n-gram from 1 to4, the optimum maximum number of elements nis 3.
They proposed the following analysis: if thehypothesis is shorter than the text, with n = 4,then the frequency is low in word 4-gram.However, the accidental coincidence of the word4-gram significantly affected BLEU.
When n islarge, the reliability of the word overlapdecreases.Next, as an explanation of reason 2, when thelength of the targeted sentence is short, thenumerical result of BLEU sometimes becomes 0.For example, the number of agreements of 4-gram becomes 0 when calculating with n = 4,and the BLEU value sometimes becomes 0.2 http://www.nlp.mibel.cs.tsukuba.ac.jp/bleu_kit/WordOverlapSubpathSetSVMTrueFalseT:xxxxxH:yyyyyMutualInformationEvaluation DataScore CalculationResourceProcessing21Such calculations accounted for approximately69% of the Odani et al evaluation set.4.2 Mutual InformationGlickman et al (Glickman et al 2005) assumedthat the possibility of entailment is high whenthe co-occurrence frequency of the word in thetext and the hypothesis is high.
Therefore, theyproposed a method of total multiplication, bysearching for the word with the highest co-occurrence frequency from all the words of thehypothesis, as shown in formulas (3) and (4):,( 1| ) max ( , ) (3)( , ) (4)u h V tu vvP Trh t lep u vnlep u vn?
?= = ???
?P(Trh=1|t) expresses the probability ofentailment between the text and the hypothesis.In these formulas, u is the content word of thehypothesis (noun, verb, adjective or unknownword); v the content word of the text; nrepresents the number of Web search hits; nu, v isthe number of hits when the words u and v aresearched on the Web.
But, when the contentword of the text is low frequency, the numericalresult of the lep(u, v) increases for P(Trh=1|t).We believe that it was a problem not to take intoaccount the co-occurrence of the hypothesiswords.
In addition, their method to handle longsentences and reaching the conclusion ?false?
isproblematic.
This is why, we considered Rodneyet als.
method (Rodney et al 2006) andproposed the use of mutual information, which iscalculates on the basis of the formulas (5) and(6):,1( 1| ) max ( , ) (5)( )( , ) log (6)( ) ( )u h V tu vu vP Trh t lep u vp nlep u vp n p n?
?= = ??
?
?uu is the number of the content words of thehypothesis.
Hence, 1/u averages product of maxlep(u,v).
This being so we considered that thismodel can do entailment judgmentsindependantly of the length of the hypothesis.It searches for the word of the text consideringthat the mutual information reaches themaximum value from each of the hypothesiswords.
When P(Trh=1|t) is higher than anarbitrary threshold value, it is judged to be?
true?
, and ?false?
in the opposite case.Glickman assumed the co-occurrence frequencyto be the number of Web-search hits.
However,we estimated that the reliability of the co-occurrence frequency was low, because the co-occurrence of the Web search engine was a widewindow.
This is why, we used the Japanese WebN-gram3.
In particular, we used 7-gram data, andcalculated the co-occurrence frequency nu, v,frequency nu and nv of the word.
p(ni) wascalculated by (?)
the frequency ni divided thenumber of all words.
Japanese Web N-gram wasmade from 20,036,793,177 sentences, including255,198,240,937 words.
The unique number of7-gram is 570,204,252.To perform morphological analysis, we usedMecab4, for example:Example 5: Calculation by mutual information.T:????????????????
(Theair conditioner works in this room.)H:????
(It is cool.
)Mutual Information:10.0( )( ) ( ),1( 1| ) max ( , )1( , )u V tP Trh t lep u vp nlep u vp n p n?
?= = ?= ?????????
(cool,the air conditioner)???
(cool) ????
(the air conditioner)?
(7)-log 10.0 (8)This method actually standardises the result bydividing by the maximum value of lep(u, v).
Asa result, p reaches the value 1 from 0.
We usedthe discounting for nu, nv,and nu, v,, because azero-frequency problem had occurred whencalculating the frequency.
There are somemethods for discounting.
We used the additivemethod reported by Church and Gale (Churchand Gale, 1991).
They compared somediscounting methods by using the newspapercorpus.
The addition method is shown as follows.
( ) 1( ) (9)C wP w" V+=+3 http://www.gsk.or.jp/catalog/GSK-2007-C/4 http://mecab.sourceforge.net/22The additive method assumed N to be thenumber of all words in a corpus.
C(w) is thefrequency of word w in the corpus.
V is aconstant to adjust the total of the appearanceprobability to 1.
It is equal to the unique numberof words w. The additive method is very simple,it adds a constant value to occurrence countC(w).
The method of adding 1 to the occurrencecount is called Laplace method also.4.3 Subpath SetHerrera et al (Herrera et al, 2005) parsed thehypothesis and the text, and they calculated thedegree of similarity of the syntax tree from both.Our method also deals with the degree ofsimilarity of the syntax tree.
The tree kernelmethod of Collins and Duffy (M. Collins andN.
Duffy, 2002) shows the degree of similarityof the syntax tree; however, it requires muchtime to calculate the degree of similarity.Therefore, we employed the subpath set ofIchikawa et al This latter calculates partialroutes from the root to the leaf of the syntax tree.Our method assumes the node to be a contentword (noun, verb, adjective or unknown word)in the syntax tree, while the branch is adependency relation.
For parsing we relied onCabocha5 .The frequency vector was assumed to comprisea number of partial routes, similar to theapproach of Ichikawa et al (Ichikawa et al,2005).
The number of partial routes is unique.However, even if the same expression is shownfor the word with a different surface form, it isnot possible to recognise it as the same node.Therefore, we used the Japanese version ofWordNet (Bond et al, 2009), in which a wordwith a different surface can be treated as thesame expression, because Japanese WordNetcontains synonyms.
The same expressions of ourmethod were hypernym words, hyponym wordsand synonym words in Japanese Word Net,because RTE sometimes considered thehierarchical dictionary of the hypernym and thehyponym word to be the same expression.However, our hypernym and hyponym wordswere assumed to be a parent and a child node ofthe object word, as shown in Figure 3.5 http://chasen.org/~taku/software/cabocha/Example 6: Calculation by subpath set.T:????????????????
2 ????
(T:The point adheres by the twice because it iscampaigning.)H:????????????????????
2????
(H:The point adheres usually by the twicebecause it is campaigning.
)Subpath:0.86Figure 2: Partial route chart of subpath set.The number of partial routes is 7, and 6 partialroutes overlap in T and H. So, the subpath is0.86 (6/7).5 EvaluationThe textual entailment evaluation data of Odaniet al, described in Section 3, was used in theexperiment.
The entailment judgment of fourvalues is manually given to the textualentailment evaluation data.
In our experiment weconsidered ?Talw?
and ?Talm?
to be ?true?
and?
Fmay?
and ?
Falw?
as ?
false?
.
Theevaluation method used was a Confidence-Weight Score (CWS, also known as AveragePrecision), proposed by Dagan et al.
As for theclosed test, the threshold value with themaximum CWS was used.11/1( )1( )ki Allii kAccuracy Correct AllCWS r precision kkprecision k rk?
??
?=??
(10)= ?
(11)= (12)All = Number of all evaluation data.
Correct =Number of correct answer data.
If k is a correctanswer, rk = 1.
If k is an incorrect answer, rk = 0.23When the Entailment judgment annotated inevaluation data matches with the Entailmentjudgment of our method, the answer is true.The threshold of the Closed test was setbeforehand (0?th?1).
When it was above thethreshold, it was judged ?true?.
When it washigher than the threshold, it was judged ?false?.SVM was used to calculate the value of threemethods (word overlap, mutual information andsubpath set) as the features for learning data, wasexperimented.Open test was experimented 10-fold cross-validations.
9 of the data divided into 10 wereutilized as the learning data.
Remaining 1 wasused as an evaluation data.
It looked for thethreshold that CWS becomes the maximum fromamong the learning data.
It experimented on thethreshold for which it searched by the learningdata to the evaluation data.
It repeats until alldata that divides this becomes an evaluation data,averaged out.
(Or we experimented Leave-one-out cross validation.
)Using the SVM, experiments were conductedon the numerical results of Sections 4.1 to 4.3 asthe features.The textual entailment evaluation datanumbered 2472: ?Talw?
: 924, ?Talm?
: 662, ?Fmay?
:262 and ?Falw?
: 624, and there were 4356 words.The total number of words was 43421.
Tables 2and 3 show the results of the experiment, whichfocused respectively on the closed and opentests,.
When the ?true?
textual entailmentevaluation data ?Talw?
only and ?Talw and Talm?was used, mutual information achieved the bestperformance.
When the true data ?Talm?
only wasused, SVM achieved the best performance.Table 2: Results of the RTE experiments6 DiscussionIn this section, we discuss the relation betweeneach 3 method value assumed to be the criterionof judgment and CWS in the closed test.
Whenthe ?true?
evaluation data was assumed to be?Talm?
only in the open test, the result of SVMexceeded the results of the closed test.
We thenconsider the relation between SVM and CWS.6.1 Close Test of Ward OverlapWe believe that the results of the experiments ofword overlap were more effective than othermethods, because they achieved the bestperformance excluding ?Talm?
and ?Talw and Talm?in 3 methods.
Figure 3 shows the relation toCWS when BLEU value changes.01020304050607080901000 0.2 0.4 0.6 0.8 1BLEUCWS[%] TalwTmayTalw and TmayFigure 3: Results of the closed test of theRTE experiments by word overlap.The tendency shown in Figure 3 did not changemuch when the relation between the thresholdvalue and CWS was observed, even though the?true?
evaluation data was changed.CWSClosed Test Open TestTalw Talm Talw and Talm Talw Talm Talw and TalmWord Overlap 53.0% 57.9% 62.1% 39.0% 60.2% 59.3%Mutual Informaition 55.9% 52.9% 68.6% 53.4% 55.6% 67.4%Subpath Set 54.5% 57.0% 61.8% 45.0% 59.7% 61.1%SVM 51.4% 61.2% 63.5% 49.9% 61.9% 64.1%24However, the entailment judgment of the wordoverlap method becomes nearly ?false?
when theBLEU value is 1 (or ?true?
when BLEU score is0.)
Table 3 shows the entailment judgment whenthe BLEU value is 0 or 1.We assumed that BLEU value that CWSbecomes the maximum depends on the ratio ofnumber of T and F in the evaluation set.However, when true condition is ?Talw?
only, Tis more than F (T:924,F:886).
And when truecondition is ?Talm?
only, F is more than T(T:662,F:886).
For this reason, The possibility ofour assumption is low because both trueconditions are BLEU value that CWS becomesthe maximum is 1.6.2 Close Test of Mutual InformationWe believe that the results of the experiments ofmutual information were more effective thanother methods, because they achieved the bestperformance excluding ?Talm?
in 3 methods.Figure 4 shows the relation to CWS whenmutual information value changes.The tendency shown in Figure 4 did not changemuch when the relation between mutualinformation value and CWS was observed, eventhough the ?true?
evaluation data was changed.When mutual information values are from 0.2(or 0.3) to 1, CWS increased.
However, theentailment judgment of the mutual informationmethod becomes almost ?true?
when mutualinformation score is near 1 (or ?false?
whenmutual information score value is near 0.
)Table 3: Entailment judgment in closed testof word overlap (T=True, F=False).Table 4: Entailment judgment in closed testof mutual information (T=True, F=False,MI=mutual information).01020304050607080901000 0.2 0.4 0.6 0.8 1Mutual InformationCWS[%] TalwTalmTalw and TalmFigure 4: Results of the closed test of the RTEexperiments by mutual information.Table 4 shows the entailment judgment when themutual information value is near 0 or 1.
Ourresults showed most entailment judgment resultsto be almost ?true?
(or almost ?false?)
for theoptimal threshold value in the evaluation data.Therefore, we considered that the method ofRTE using mutual information should bereviewed.6.3  Close Test of Subpath SetWe believe that the results of the experiments ofsubpath set were not better than other methods.Figure 5 shows the relation to CWS whensubpath set (SS) value changes.The tendency shown in Figure 5 changed muchwhen the relation between the threshold valueand CWS was observed, even though the ?true?evaluation data was changed.
When the trueconditions are ?Talw?
and ?Talm?, the tendencieswere very near.Answer/System T/T T/F F/T F/F CWSTalw               (Bleu=1) 5 919 12 874 53.0Talm               (Bleu=1) 0 662 12 874 57.9 True ConditionTalw and Talm (Bleu=0) 1586 0 886 0 62.1Answer/System T/T T/F F/T F/F CWSTalw               (MI=0.72) 924 0 884 2 55.9Talm               (MI=0) 0 2 662 884 52.9TrueConditionTalw and Talm (MI=0.68) 1586 0 884 2 68.625However, when the true conditions were ?Talw?and ?Talw and Talm?, the tendencies were different.The tendency of ?Talw?
was rising.
The tendencyof ?Talw and Talm?
was dropping until the subpathset value was 0.2.
The entailment judgment ofthe mutual information method becomes almost?true?
when subpath set value was near 1)01020304050607080901000 0.2 0.4 0.6 0.8 1Subpath SetCWS[%] TalwTalmTalw and TalmFigure 5: Results of the closed test of the RTEexperiments by subpath set.Table 5 shows the entailment judgment when thethreshold value is near 0 or 1.
Our resultsshowed most entailment judgment results to bealmost ?true?
(or almost ?false?)
for the optimalsubpath set value in the evaluation data.6.4 Open Test of SVMThe open tests were conducted in 10-fold cross-validation , and the experimental result is theiraverage.
Figure 6 shows the related chart 10-foldcross-validation.When the true data were assumed to be ?Talm?only, the maximum value of CWS was 70.3%.As a result, the result of 10?fold cross validationexceeded the closed test.Table 5: Entailment judgment in closed test ofsubpath set (T=True, F=False, SS=subpath set).01020304050607080901001 3 5 7 9Fold NCWS[%] TalwTmayTalw and TmayFigure 6: Results of the open test of the RTEexperiments by SVM.When the true data was assumed to be ?Talw?only, the minimum value of CWS was 42.7%.We focused on the difference between themaximum and minimum value in 10-fold cross-validation.
When the true answer was assumedto be ?Talm?, the difference between themaximum and minimum value is the greatest(15.3 points) in the open tests, and ?Talw andTalm?
was the lowest with 11.6 points.We believe that when the result ?Talm?
was?true?, it was consequently more unstable than?Talw and Talm?, because there was a largeramount of evaluation data ?Talw and Talm?.7 ConclusionWe built a Japanese textual entailmentrecognition system based on the past methods ofRTE.
We considered the problem of RTE as aproblem of binary classification, and built a newmodel of RTE for machine learning.
Weproposed machine learning to consider thematching rate of the words of the text and thehypothesis, using mutual information andsimilarity of the syntax tree.
The method ofusing mutual information and the use of threemethods of SVM tunrned out to be effective.Answer/System T/T T/F F/T F/F CWSTalw               (SS=1) 9 915 14 872 54.5Talm               (SS=1) 1 661 14 872 57.0 True ConditionTalw and Talm (SS=0) 1586 0 886 0 61.826In the future, we will consider changing thedomain of the evaluation data and theexperiment.
Moreover, we will propose a newmethod for the feature of machine learning.We will also consider to expand WordNet.Shnarch et al (Shnarch et al, 2009) researchedthe extraction from Wikipedia of lexicalreference rules, identifying references to termmeaning triggered by other terms.
Theyevaluated their lexical reference relation for RTE.They improved previous RTE methods.
We willuse their method for ours in order to expandJapanese WordNet.
We believe that this can helpus improve our method/results.ReferencesMichitaka Odani, Tomohide Shibata, SadaoKurohashi, Takayuki Nakata, Building data ofjapanese Text Entailment and recognition ofinferencing relation based on automatic achievedsimilar expression.
In Proceeding of 14th AnnualMeeting of the Association for "atural LanguageProcessing, pp.
1140-1143, 2008 (in Japanese)Diana P?rez and Enrique Alfonseca.
Application ofthe Bleu algorithm for recognising textual entailment.In Proceedings of the first PASCAL RecognizingTextual Entailment Challenge, pp.
9-12, 2005Oren Glickman, Ido Dagan and Moshe Koppel.Web Based Probabilistic Textual Entailment.
InProceedings of the PASCAL Recognizing TextualEntailment Challenge, pp.
33-36, 2005Francis Bond, Hitoshi Isahara, Sanae Fujita,Kiyotaka Uchimoto, Takayuki Kuribayashi andKyoko Kanzaki.
Enhancing the Japanese WordNet.In the 7th Workshop on Asian Language Resources,in conjunction with ACL-IJCNLP, pp.
1-8, 2009Hiroshi Ichikawa, Taiichi Hashimoto, TakenobuTokunaka and Hodumi Tanaka.
New methods toretrieve sentences based on syntactic similarity.Information Processing Society of Japan SIG"L"ote, pp39-46, 2005(in Japanese)Kishore Panieni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
BLEU: a Method for  AutomaticEvaluation of Machine Translation.
In Proceedingsof the Annual Meeting of the Association forComputational Linguistics, pp.
311-318, 2002Ido Dagan, Oren Glickman and Bernardo Magnini.The PASCAL Recognizing Textual EntailmentChallenge.
In Proceedings of the first PASCALRecognizing Textual Entailment Challenge, pp.
1-8,2005Jes?s Herrera, Anselmo Pe?as and Felisa Verdejo,Textual Entailment Recognition Based onDependency Analysis and WordNet.
In Proceedingsof the first PASCAL Recognizing Textual EntailmentChallenge, pp.
21-24, 2005Kaname Kasahara, Hirotoshi Taira and MasaakiNagata, Consider of the possibility TextualEntailment applied to Reading Comprehension Taskconsisted of multi documents.
In Proceeding of 14thAnnual Meeting of the Association for "aturalLanguage Processing, pp.
780-783, 2010 (inJapanese)M. Collins and N. Duffy.
Convolution kernel fornatural language.
In Advances in "eural InformationProccessing Systems ("IPS), volume 16, pages 625?632, 2002.Prodromos Malakasiotis and Ion Androutsopoulos.Learning Textual Entailment using SVMs and StringSimilarity Measures.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment andParaphrasing, pp.
42-47, 2007Vladimir N. Vapnik, The Statisitcal LearningTheory.
Springer, 1998.Church, K. W. & Gale, W. A.. A comparison of theenhanced Good-Turing and deleted estimationmethods for estimating probabilities of Englishbigrams.
Computer Speech and Language, volume 5,19-54.Rodney D. Nielsen, Wayne Ward and James H.Martin.
Toward Dependency Path based Entailment.In Proceedings of the second PASCAL RecognizingTextual Entailment Challenge, pp.
44-49, 2006Eyal Shnarch, Libby barak, Ido Dagan.
ExtractingLexical Reference Rules from Wikipedia.
InProceedings of the Annual Meeting of theAssociation for Computational Linguistics, pp.
450-458, 200927
