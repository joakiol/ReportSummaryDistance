Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2017?2021,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsBrainBench:A Brain-Image Test Suite for Distributional Semantic ModelsHaoyan XuUniversity of VictoriaVictoria, BC, Canadaexu@uvic.caBrian MurphyQueen?s University BelfastBelfast, Northern Ireland, UKbrian.murphy@qub.ac.ukAlona FysheUniversity of VictoriaVictoria, BC, Canadaafyshe@uvic.ca?AbstractThe brain is the locus of our language abil-ity, and so brain images can be used to groundlinguistic theories.
Here we introduce Brain-Bench, a lightweight system for testing dis-tributional models of word semantics.
Wecompare the performance of several models,and show that the performance on brain-imagetasks differs from the performance on behav-ioral tasks.
We release our benchmark test aspart of a web service.1 IntroductionThere is active debate over how we should test se-mantic models.
In fact, in 2016 there was an en-tire workshop dedicated to the testing of semanticrepresentations (RepEval, 2016).
Several before ushave argued for the usage of brain data to test se-mantic models (Anderson et al, 2013; Murphy et al,2012; Anderson et al, 2015), as a brain image repre-sents a snapshot of one person?s own semantic rep-resentation.
Still, testing semantic models againstbrain imaging data is rarely done by those not in-timately involved in psycholinguistics or neurolin-guistics.
This may be due to a lack of familiaritywith neuroimaging methods and publicly availabledatasets.We present the first iteration of BrainBench,a new system that makes it easy to test seman-tic models using brain imaging data (Availableat http://www.langlearnlab.cs.uvic.ca/brainbench/).
Our system has methodologythat is similar to popular tests based on behavioral?Corresponding Authordata (see Section 2.2), and has the additional benefitof being fast enough to offer as a web service.2 The TasksHere we outline the set of tasks we used to evaluateseveral popular Distributional Semantic (DS) mod-els.2.1 Brain Image DataFor BrainBench we use two brain image datasetscollected while participants viewed 60 concretenouns with line drawings (Mitchell et al, 2008; Su-dre et al, 2012).
One dataset was collected us-ing fMRI (Functional Magnetic Resonance Imag-ing) and one with MEG (Magnetoencephalography).Each dataset has 9 participants, but the participantssets are disjoint, thus there are 18 unique partici-pants in all.
Though the stimuli is shared across thetwo experiments, as we will see, MEG and fMRI arevery different recording modalities and thus the dataare not redundant.fMRI measures the change in blood oxygen levelsin the brain, which varies according to the amountof work being done by a particular brain area.
AnfMRI image is a 3D volume of the brain where eachpoint in the volume (called a voxel) represents brainactivity at a particular place in the brain.
In the fMRIdataset used here, each voxel represents a 3mm x3mm x 5mm area of the brain.
Each of the 60 wordswas presented 6 times in random order, for a total of360 brain images.
The number of voxels depends onthe size and shape of a person?s brain, but there arearound 20,000 voxels per participant in this dataset.MEG measures the magnetic field caused by2017many neurons firing in the same direction at thesame time.
This signal is very weak, and so mustbe measured in a magnetically shielded room.
TheMEG machine is essentially a large helmet with 306sensors that measure aspects of the magnetic fieldsat different locations in the brain.
A MEG brain im-age is the time signals recorded from each of thesesensors.
Here, the sampling rate is 200 Hz.
For eachword, the MEG recording is 800ms long resulting in306 ?
160 data points.
Each of the words was pre-sented 20 times (in random order) for a total of 1200brain images.
For simplicity we will use the term?brain image feature?
to refer to both fMRI voxelsand MEG sensor/time points.A non-trivial portion of our participants?
brain ac-tivity may be driven by the low-level visual proper-ties of the word/line-drawing stimulus, rather thanby semantics.
As there is a possibility of confound-ing visual properties with semantic properties, wehave attempted to remove the activity attributable tovisual properties from the brain images.
In total wehave 11 visual features which include things like thelength of the word, the number of white pixels, andfeatures of the line drawing (Sudre et al, 2012).
Toremove the visual stimulus?
contribution to the sig-nal, we train a regression model that predicts the sig-nal in each brain image feature as a function of the11 visual features.
We then subtract the predictedvalue from the observed value of the brain imagefeature.
This process is known as ?partialling out?an effect.
Thus, the signal that remains in the brainimage will not be correlated with the visual stimuli,and should only be related to the semantics of theword itself (or noise).Brain images are quite noisy, so we used themethodology from Mitchell et al (2008) to selectthe most stable brain image features for each of the18 participants.
The stability metric assigns a highscore to features that show strong self-correlationover presentations of the same word.
We noticedthat tuning the number of features to keep made lit-tle or no difference in the absolute ordering of thedifferent DS models.
Thus, we use the optimal num-ber of features averaged over all 6 DS models de-scribed in Section 3: the top 13% of MEG sen-sor/time points, and 3% of fMRI voxels.
Finally,we average all brain images corresponding to repe-titions of the same word.2.2 Behavioral TasksWe include, for comparison, four popular word vec-tor evaluation benchmarks.MEN This dataset contains 3,000 word pairs, suchthat each word appears frequently in two separatecorpora.
Human participants were presented withtwo word pairs and asked to choose the word pairthat was more related, resulting in a ranking of re-latedness amongst word pairs (Bruni and Baroni,2013).SimLex-999 A word pairing task meant to specifi-cally target similarity rather than the more broad ?re-latedness?
(Hill et al, 2015).WS-353-[SIM|REL] A set of 353 word pairs withrelatedness ratings (Finkelstein et al, 2002).
Thisdataset was subsequently split into sets where thepairs denote similarity and relatedness, named WS-353-SIM and WS-353-REL, respectively (Agirre etal., 2009).3 Distributional ModelsWe test six semantic models against both the fMRIand behavioral datasets.
The six models are:Skip-gram: A neural network trained to predictthe words before and after the current word, giventhe current word.
We selected a model with 300dimensions trained on the Google news corpus(Mikolov et al, 2013).Glove: A regression-based model that combinesglobal context information (term-document cooc-currence) with local information (small windows ofword-word cooccurrence) (Pennington et al, 2014).This 300-dimensional model was trained on theWikipedia and Gigaword 5 corpora combined.RNN: A recurrent neural network with 640-dimensional hidden vectors.
These models aretrained to predict the next word in a sequenceand have the ability to encode (theoretically) in-finitely distant contextual information (Mikolov etal., 2011).
The model was trained on broadcast newstranscriptions.Global: A neural network model that incorporatesglobal and local information, like that of the Glove2018model (Huang et al, 2012).
This model is oursmallest, with dimension 50, and was trained onWikipedia.Cross-lingual: A tool that projects distributionalrepresentations from multiple language into a sharedrepresentational space (Faruqui and Dyer, 2014).Here we use the German-English model (512 dimen-sions), trained on the WMT-2011 corpus.Non-distributional: This model is based solelyon hand-crafted linguistic resources (Faruqui andDyer, 2015).
Several resources like WordNet (Fell-baum, 1998) and FrameNet (Baker et al, 1998) arecombined to make very sparse word vector repre-sentations.
Due to their sparsity, these vectors are ofvery high dimension (171, 839).
This is a particu-larly interesting model because it is not built from acorpus (unlike every other model in this list).Note that we are not aiming to compare the good-ness of any of these distributional models, as theyare trained on different corpora with different algo-rithms.
Instead, we wish to compare the patterns ofperformance on behavioral benchmarks to that of abrain-image based task.4 MethodologyEach of the behavioral tasks included here assigns asimilarity score to word pairs.
For each DS modelwe calculate the correlation between the vectors forevery pair of words in the behavioral datasets.
Wethen calculate the correlation between the DS vectorcorrelations and the behavioral scores.We follow a very similar methodology for thebrain image datasets.
Let us represent each DSmodel with a matrix X ?
Rw?p where w is the num-ber of words for which we have brain images (herew = 60), and p is the number of dimensions in aparticular DS model.
From X we calculate the cor-relation between each pair of word vectors, resultingin a matrix CDS ?
Rw?w.Let us represent each participant?s brain imageswith a matrix Y ?
Rw?v where v is the number ofselected brain image features.
From this matrix wecalculate the correlation between each pair of brainimages, resulting in a matrix CBI ?
Rw?w (BI forbrain image).
This final representation is similar tothe behavioral tasks above, but now we have a simi-larity measure for every pair of words in our dataset.Here is where the evaluation for brain imagingtasks differs from the behavioral tasks.
Instead ofmeasuring the correlation between CBI and CDS ,as is done in Representational Similarity Analysis(RSA) (Kriegeskorte et al, 2008), we use the test-ing methodology from Mitchell et al (2008), whichwe will refer to as the 2 vs. 2 test.
The 2 vs. 2 testwas developed to help detect statistically significantpredictions on brain imaging data, and, compared toRSA, can better differentiate the performance of amodel from chance.
We perform a 2 vs. 2 test for allpairs of CDS and CBI (that is, for every pair of DSmodel and fMRI/MEG participant).For each 2 vs. 2 test we select the same two words(rows) w1, w2 from CDS and CBI .
We omit thecolumns which correspond to the correlation to w1and w2, as they contain a perfect signal for the 2vs.
2 test.
We now have four vectors, CDS(w1),CDS(w2), CBI(w1) and CBI(w2), all of lengthw ?
2.
We compute the correlation (corr) betweenvectors derived from CDS and CBI to see if:corr(CDS(w1), CBI(w1)) + corr(CDS(w2), CBI(w2))(the correlation of correctly matched rows: w1 to w1and w2 to w2) is greater than:corr(CDS(w1), CBI(w2)) + corr(CDS(w2), CBI(w1))(the correlation of incorrectly matched rows).
If thecorrectly matched rows are more similar than incor-rectly matched rows, then the 2 vs. 2 test is consid-ered correct.
We perform the 2 vs. 2 test for all pos-sible pairs of words, for 1770 tests in total.
The 2 vs.2 accuracy is the percentage of 2 vs. 2 tests correct.Chance is 50%.Our process of computing 2 vs. 2 accuracy overrows of a correlation matrix is different than theoriginal methodology for these datasets (Mitchell etal., 2008; Sudre et al, 2012).
Previous work trainedregression models that took brain images as inputand predicted the dimensions of a DS model as out-put.
Training these regression models for all 1770pairs of words takes hours to complete, whereas thetest we suggest here is much faster, and the correla-tion matrices CBI can be computed ahead of time.This makes the tests fast enough to offer as a webservice.
We hope our web offering will remove bar-riers to the wider adoption of brain-based tests fromwithin the computational linguistics community.2019Figure 1: Performance of Distributional Semanticmodels on the brain-image datasets.Figure 2: Performance of Distributional Semanticmodels on several benchmark behavioral tasks.5 ResultsFigure 1 shows the results for each of the DS mod-els against the fMRI and MEG datasets.
On aver-age, the Skip-gram, Glove and Cross-lingual mod-els perform quite well, whereas the multi-layer NNs(RNN, Global) perform less well.
The one DSmodel to be built from hand-crafted resources (Non-distributional) performs poorly on both brain imagetests.As previously mentioned, we are not claiming toshow that any one of the DS models is better thanany other.
Indeed, that would be comparing applesto oranges, as each DS model is trained with a differ-ent algorithm on a different corpus.
Instead, noticethat the pattern of performance for the fMRI task isremarkably similar to the pattern on the MEN behav-ioral task.
This is interesting given that our datasetcontains only 60 words and the MEN dataset con-tains > 700.
On the MEG data, the Cross-lingualmodel performs best, and its performance pattern isunlike any of the behavioral tasks in Figure 2.
Theaveraged BrainBench results are most similar to theresults for WS-353-REL.
However, averaging the re-sults together may be misleading, as the fMRI andMEG result patterns are different.6 DiscussionThere are some caveats about the analyses herein.Firstly, the brain-based tests include only 60 con-crete nouns, so they will necessarily favor distri-butional models with good noun representations,regardless of the representations of other parts ofspeech.
We are currently working with various re-search groups to expand the number of brain-imagedatasets included in this benchmark to have a morediverse test base.
The behavioral benchmarks werenot reduced to include only the 60 words for whichwe have brain data, because this would have ren-dered the benchmarks essentially useless, as veryrarely are a pair of the 60 words from the brain im-age data scored as a pair in the behavioral bench-marks.7 ConclusionWe have presented our new system, BrainBench,which is a fast and lightweight alternative to pre-vious methods for comparing DS models to brainimages.
Our proposed methodology is more similarto well-known behavioral tasks, as BrainBench alsouses the similarity of words as a proxy for mean-ing.
We hope that this contribution will bring brainimaging tests ?to the masses?
and encourage discus-sion around the testing of DS models against brainimaging data.References[Agirre et al2009] Eneko Agirre, Enrique Alfonseca,Keith Hall, Jana Kravalova, Marius Pas, and AitorSoroa.
2009.
A Study on Similarity and RelatednessUsing Distributional and WordNet-based Approaches.In Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the ACL,pages 19?27.
[Anderson et al2013] Andrew J Anderson, Elia Bruni,Ulisse Bordignon, Massimo Poesio, and Marco Ba-roni.
2013.
Of words , eyes and brains : Correlat-ing image-based distributional semantic models withneural representations of concepts.
In Proceedings of2020the Conference on Empirical Methods on Natural Lan-guage Processing.
[Anderson et al2015] Andrew James Anderson, EliaBruni, Alessandro Lopopolo, Massimo Poesio, andMarco Baroni.
2015.
Reading visually embodiedmeaning from the brain: Visually grounded compu-tational models decode visual-object mental imageryinduced by written text.
NeuroImage, 120:309?322.
[Baker et al1998] Collin F. Cf Baker, Charles J. Fillmore,and John B. Lowe.
1998.
The Berkeley FrameNetProject.
In Proceedings of the 36th annual meetingon Association for Computational Linguistics -, vol-ume 1, page 86.
Association for Computational Lin-guistics.
[Bruni and Baroni2013] Elia Bruni and Marco Baroni.2013.
Multimodal Distributional Semantics.
Journalof Artificial Intelligence Research, 48.
[Faruqui and Dyer2014] Manaal Faruqui and Chris Dyer.2014.
Improving vector space word representationsusing multilingual correlation.
Proceedings of theEuropean Association for Computational Linguistics,pages 462?471.
[Faruqui and Dyer2015] Manaal Faruqui and Chris Dyer.2015.
Non-distributional Word Vector Representa-tions.
Acl-2015, pages 464?469.
[Fellbaum1998] Christiane Fellbaum.
1998.
WordNet:An Electronic Lexical Database.
MIT Press, Cam-bridge, MA.
[Finkelstein et al2002] Lev Finkelstein, EvgeniyGabrilovich, Yossi Matias, Ehud Rivlin, ZachSolan, Gadi Wolfman, and Eytan Ruppin.
2002.Placing search in context: the concept revisited.
ACMTransactions on Information Systems, 20(1):116?131.
[Hill et al2015] Felix Hill, Roi Reichart, and Anna Ko-rhonen.
2015.
SimLex-999: Evaluating SemanticModels with (Genuine) Similarity Estimation.
Com-putational Linguistics, 41(4):665?695.
[Huang et al2012] Eric H Huang, Richard Socher,Christopher D Manning, and Andrew Ng.
2012.Improving word representations via global contextand multiple word prototypes.
Proceedings of the 50thAnnual Meeting of the Association for ComputationalLinguistics: Long Papers-Volume 1, pages 873?882.
[Kriegeskorte et al2008] Nikolaus Kriegeskorte, MariekeMur, and Peter Bandettini.
2008.
Representationalsimilarity analysis - connecting the branches of sys-tems neuroscience.
Frontiers in systems neuroscience,2(November):4, jan.[Mikolov et al2011] Toma?s?
Mikolov, Stefan Kombrink,Anoop Deoras, Luka?s?
Burget, and Jan C?ernocky?.2011.
RNNLM ?
Recurrent Neural Network Lan-guage Modeling Toolkit.
In Proceedings of Auto-matic Speech Recognition and Understanding (ASRU),pages 1?4.
[Mikolov et al2013] Tomas Mikolov, Greg Corrado, KaiChen, and Jeffrey Dean.
2013.
Efficient Estimation ofWord Representations in Vector Space.
Proceedings ofthe International Conference on Learning Representa-tions (ICLR 2013), pages 1?12.
[Mitchell et al2008] Tom M Mitchell, Svetlana VShinkareva, Andrew Carlson, Kai-Min Chang, Vi-cente L Malave, Robert A Mason, and Marcel AdamJust.
2008.
Predicting human brain activity associatedwith the meanings of nouns.
Science (New York, N.Y.),320(5880):1191?5, may.
[Murphy et al2012] Brian Murphy, Partha Talukdar, andTom Mitchell.
2012.
Selecting Corpus-SemanticModels for Neurolinguistic Decoding.
In First JointConference on Lexical and Computational Semantics(*SEM), pages 114?123, Montreal, Quebec, Canada.
[Pennington et al2014] Jeffrey Pennington, RichardSocher, and Christopher D Manning.
2014.
GloVe: Global Vectors for Word Representation.
In Con-ference on Empirical Methods in Natural LanguageProcessing, Doha, Qatar.
[RepEval2016] RepEval.
2016.
RepEval workshop,ACL.
https://sites.google.com/site/repevalacl16/.
[Sudre et al2012] Gustavo Sudre, Dean Pomerleau, MarkPalatucci, Leila Wehbe, Alona Fyshe, Riitta Salmelin,and Tom Mitchell.
2012.
Tracking Neural Coding ofPerceptual and Semantic Features of Concrete Nouns.NeuroImage, 62(1):463?451, may.2021
