First Joint Conference on Lexical and Computational Semantics (*SEM), pages 20?29,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsMeasuring Semantic Relatedness using Multilingual RepresentationsSamer HassanUniversity of North TexasDenton, TXsamer@unt.eduCarmen BaneaUniversity of North TexasDenton, TXcarmenbanea@my.unt.eduRada MihalceaUniversity of North TexasDenton, TXrada@cs.unt.eduAbstractThis paper explores the hypothesis that se-mantic relatedness may be more reliably in-ferred by using a multilingual space, as com-pared to the typical monolingual representa-tion.
Through evaluations using several state-of-the-art semantic relatedness systems, ap-plied on standard datasets, we show that amultilingual approach is better suited for thistask, and leads to improvements of up to 47%with respect to the monolingual baseline.1 IntroductionSemantic relatedness is the task of quantifying thestrength of the semantic connection between tex-tual units, be they words, sentences, or documents.For instance, one may want to determine how se-mantically related are two words such as car andautomobile, or two pieces of text such as I love an-imals and I own a pet.
It is one of the main tasksexplored in the field of natural language processing,as it lies at the core of a large number of applica-tions such as information retrieval (Ponte and Croft,1998), query reformulation (Metzler et al, 2007;Yih and Meek, 2007; Sahami and Heilman, 2006;Broder et al, 2008), image retrieval (Leong and Mi-halcea, 2009; Goodrum, 2000), plagiarism detection(Hoad and Zobel, 2003; Shivakumar and Garcia-Molina, 1995; Broder et al, 1997; Heintze, 1996;Brin et al, 1995; Manber, 1994), information flow(Metzler et al, 2005), sponsored search (Broder etal., 2008), short answer grading (Mohler and Mihal-cea, 2009a; Pulman and Sukkarieh, 2005; Mitchellet al, 2002), and textual entailment (Dagan et al,2005).The typical approach to semantic relatedness is toeither measure the distance between the constituentwords by using a knowledge base such as Word-Net or Roget (e.g., (Leacock and Chodorow, 1998;Lesk, 1986; Jarmasz and Szpakowicz, 2003; Peder-sen et al, 2004)), or to calculate the similarity be-tween the word distributions in very large corpora(e.g., (Landauer et al, 1991; Lin, 1998; Gabrilovichand Markovitch, 2007)).
With almost no exception,these methods have been applied on one language ata time ?
English, most of the time, although mea-sures of relatedness have also been explored on lan-guages such as German (Zesch et al, 2007), Chinese(Li et al, 2005), Japanese (Kazama et al, 2010), andothers.In this paper, we take a step further and ex-plore a joint multilingual semantic relatedness met-ric, which aggregates semantic relatedness scoresmeasured on several different languages.
Specifi-cally, in our method, in order to measure the re-latedness of two textual units, we first determinetheir relatedness in multiple languages, and conse-quently infer a final relatedness score by averagingthe scores calculated in the individual languages.Our hypothesis is that a multilingual representa-tion can enrich the relatedness space and addressrelevant issues such as polysemy (i.e., find that twooccurrences of the same word in language L1 rep-resent two different meanings because of differenttranslations in language L2) and synonymy (i.e., findthat two words in language L1 are related becausethey have the same translation in language L2).
Weshow that by measuring relatedness in a multilingualspace, we are able to improve over a traditional re-latedness measure that relies exclusively on a mono-lingual representation.Through experiments using several state-of-the-art measures of relatedness, applied on a multilin-gual space including English, Arabic, Spanish, andRomanian, we aim to answer the following research20questions: (1) Does the task of semantic relatednessbenefit from a multilingual representation, as com-pared to a monolingual one?
(2) Does the translationquality affect the results?
and (3) Do the findingshold for different relatedness datasets?The paper is organized as follows.
First, weoverview related work on word and text related-ness, and on multilingual natural language process-ing.
We then briefly describe three corpus-basedmeasures of relatedness, and present several wordand text datasets that have been used in the past toevaluate relatedness.
We then present evaluationsand experiments addressing each of the three re-search questions, and discuss our findings.2 Related WorkSemantic relatedness.
The approaches for seman-tic relatedness that have been considered to datecan be grouped into knowledge-based and corpus-based.
Knowledge-based methods derive a measureof relatedness by utilizing lexical resources and on-tologies such as WordNet (Miller, 1995) to mea-sure definitional overlap (Lesk, 1986), term dis-tance within a graphical taxonomy (Leacock andChodorow, 1998), term depth in the taxonomy as ameasure of specificity (Wu and Palmer, 1994), andothers.
The application of such measures to a lan-guage other than English requires the availability ofthe lexical resource in that language; furthermore,even though taxonomies such as WordNet (Miller,1995) are available in a number of languages1, theircoverage is still limited, and often times they are notpublicly available.
For these reasons, in multilingualsettings, these measures often become untractable.On the other side, corpus-based measuressuch as Latent Semantic Analysis (LSA) (Lan-dauer et al, 1991), Explicit Semantic Analy-sis (ESA) (Gabrilovich and Markovitch, 2007),Salient Semantic Analysis (SSA) (Hassan and Mi-halcea, 2011), Pointwise Mutual Information (PMI)(Church and Hanks, 1990), PMI-IR (Turney, 2001),Second Order PMI (Islam and Inkpen, 2006), Hy-perspace Analogues to Language (HAL) (Burgesset al, 1998) and distributional similarity (Lin, 1998)employ probabilistic approaches to decode the se-mantics of words.
They consist of unsupervisedmethods that utilize the contextual information andpatterns observed in raw text to build semantic pro-files of words, and thus they can be easily transferred1http://www.illc.uva.nl/EuroWordNet/to a new language provided that a large corpus in thatlanguage is available.Multilingual natural language processing.
Alsorelevant is the work done on multilingual text pro-cessing, which attempts to improve the performanceof different natural language processing tasks byintegrating information drawn from multiple lan-guages.
For instance, (Cohn and Lapata, 2007) ex-plore the use of triangulation for machine transla-tion, where multiple translation models are learnedusing multilingual parallel corpora.
The model wasfound especially beneficial for languages where thetraining dataset was small, thus suggesting that thismethod may be particularly useful for languageswith scarce resources.
(Davidov and Rappoport,2009) experiment with the use of multiple languagesto enhance an existing lexicon.
In their experiments,using three source languages and 45 intermediatelanguages, they find that the multilingual resourcescan lead to significant improvements in concept ex-pansion.
(Banea et al, 2010) explore the use ofparallel multilingual corpora to improve subjectivityclassification in a target language, finding that theuse of multilingual representations for subjectivityanalysis improves over the monolingual classifiers.Similarly, (Banea and Mihalcea, 2011) investigatethe use of multilingual contexts for word sense dis-ambiguation.
By leveraging on the translations ofthe annotated contexts in multiple languages, a mul-tilingual thematic space emerges that better disam-biguates target words.Finally, there are two lines of work that exploresemantic distances in a multilingual space.
First,(Besanc?on and Rajman, 2002) examine the notionthat the distances between document vectors withina language correlate with the distances between theircorresponding vectors in a parallel corpus.
Thesefindings provide clues about the possibility of reli-able semantic knowledge transfer across languageboundaries.
Second, (Hassan and Mihalcea, 2009)propose a framework to compute semantic relat-edness between two words in different languages,by considering Wikipedia articles in multiple lan-guages.
The method differs from the one proposedhere, as we aggregate relatedness over monolingualspaces rather than measuring cross-lingual related-ness, and we do not specifically use the inter-wikilinks between Wikipedia pages.213 Measures of Text RelatednessIn this work, we focus on corpus-based metricsbecause of their unsupervised nature, their flexi-bility, scalability, and portability to different lan-guages.
Specifically, we utilize three popular mod-els, LSA (Landauer et al, 1991), ESA (Gabrilovichand Markovitch, 2007), and SSA (Hassan and Mi-halcea, 2011).
In these models, the semantic profileof a word is expressed in terms of the explicit (ESA),implicit (LSA), or salient (SSA) concepts.
All threemodels are trained on the Wikipedia 2010 corporacorresponding to the four languages of interest (En-glish, Arabic, Spanish, Romanian).Explicit Semantic Analysis.
ESA (Gabrilovichand Markovitch, 2007) uses encyclopedic knowl-edge in an information retrieval framework to gen-erate a semantic interpretation of words.
Since en-cyclopedic knowledge is typically organized intoconcepts (or topics), each concept is described us-ing definitions and examples.
ESA relies on thedistribution of words inside the encyclopedic de-scriptions.
It builds semantic representations fora given word using a word-document association,where each document represents a Wikipedia article.In this vector representation, the semantic interpre-tation of a text can be modeled as an aggregation ofthe semantic vectors of its individual words.Latent Semantic Analysis.
In LSA (Landauer etal., 1991), term-context associations are captured bymeans of a dimensionality reduction operated by asingular value decomposition (SVD) on the term-by-context matrix T, where the matrix is induced froma large corpus.
This reduction entails the abstractionof meaning by collapsing similar contexts and dis-counting noisy and irrelevant ones, hence transform-ing the real world term-context space into a word-latent-concept space which achieves a much deeperand concrete semantic representation of words.Salient Semantic Analysis.
SSA (Hassan andMihalcea, 2011) incorporates a similar semanticabstraction and interpretation of words, by usingsalient concepts gathered from encyclopedic knowl-edge, where a concept is defined as an unambigu-ous word or phrase with a concrete meaning, whichcan afford an encyclopedic definition.
The linksavailable between Wikipedia articles, obtained ei-ther through manual annotation by the Wikipediausers or using an automatic annotation process, areregarded as clues or salient features within the textthat help define and disambiguate its context.
Thismethod seeks to determine the semantic relatednessof words by measuring the distance between theirconcept-based profiles, where a profile consists ofco-occurring salient concepts found within a givenwindow size in a very large corpus.4 DatasetsTo evaluate the representation strength of a multilin-gual semantic relatedness model we employ severalstandard word-to-word and text-to-text datasets.
Foreach of these datasets, we make use of their repre-sentation in the four languages of interest.4.1 Word RelatednessWe construct our multilingual word-to-worddatasets building upon three word relatednessdatasets that have been widely used in the past.Rubenstein and Goodenough (Rubenstein andGoodenough, 1965) (RG65) consists of 65 wordpairs ranging from synonymy pairs (e.g., car -automobile) to completely unrelated words (e.g.,noon - string).
The participating terms in all thepairs are non-technical nouns annotated by 51 hu-man judges on a scale from 0 (unrelated) to 4 (syn-onyms).Miller-Charles (Miller and Charles, 1991) (MC30)is a subset of RG65, consisting of 30 word pairs an-notated for relatedness by 38 human subjects, usingthe same 0 to 4 scale.WordSimilarity-353 (Finkelstein et al, 2001)(WS353), also known as Finkelstein-353, consistsof 353 word pairs annotated by 13 human experts,on a scale from 0 (unrelated) to 10 (synonyms).While containing the MC30 set, it poses an addi-tional degree of difficulty by also including phrases(e.g., ?Wednesday news?
), proper names and tech-nical terms.To enable a multilingual representation, we usethe multilingual datasets introduced by (Hassan andMihalcea, 2009), which are based upon MC30 andWS353.
These multilingual datasets are built us-ing manual translations, following the same guide-lines adopted for the generation and the annotationof their original English counterparts.
These manu-ally translated collections, available in Arabic, Span-ish, and Romanian, allow us to infer an upper boundfor the multilingual semantic relatedness model.Moreover, in order to provide a more realisticscenario, where manual translations are not avail-able, we also create multilingual datasets by auto-matically translating the three English datasets into22Arabic, Spanish and Romanian.2 Similar to how themanually translated datasets were created by provid-ing the bilingual speakers with one word pair at atime, for the automatic translation each word pair isprocessed as a single query to the translation engine.Thus, the co-occurrence metrics derived from largecorpora are able to play a role in providing a dis-ambiguated translation instead of defaulting to themost frequently used sense if the words were to beprocessed individually.
This allows for the embed-ded word pair relatedness to be transferred to otherlanguages as well.4.2 Text RelatednessWe use three standard text-to-text datasets.Lee50 (Lee and Welsh, 2005) is a compilation of50 documents collected from the Australian Broad-casting Corporation?s news mail service.
Each doc-ument is scored by ten annotators on a scale from 1(unrelated) to 5 (alike) based on its semantic related-ness to all the other documents.
The users?
annota-tion is then averaged per document pair, resulting in2,500 document pairs annotated with their similarityscores.
Since it was found that there was no signif-icant difference between annotations given a differ-ent order of the documents in a pair (Lee and Welsh,2005), the evaluations are carried out on only 1225document pairs after ignoring duplicates.Li30 (Li et al, 2006) is a sentence pair similar-ity dataset obtained by replacing each of the RG65word-pairs with their respective definitions extractedfrom the Collins Cobuild dictionary (Sinclair, 2001).Each sentence pair was scored between 0 (unrelated)to 4 (alike) by 32 native English speakers, and theirannotations were averaged.
Due to the skew in thescores toward low similarity sentence-pairs, they se-lected a subset of 30 sentences from the 65 sentencepairs to maintain an even relatedness distribution.AG400 (Mohler and Mihalcea, 2009b) is a domainspecific dataset from the field of computer science,used to evaluate the application of semantic relat-edness measures to real world applications such asshort answer grading.
We employ the version pro-posed by (Hassan and Mihalcea, 2011) which con-sists of 400 student answers along with the corre-sponding questions and correct instructor answers.Each student answer was graded by two judges ona scale from 0 (completely wrong) to 5 (perfect an-swer).
The correlation between human judges was2For all the automatic translations we used the GoogleTranslate service.measured at 0.64.First, we construct a multilingual, manually trans-lated text-to-text relatedness dataset based on thestandard Li30 corpus.3 Native speakers of Spanish,Romanian and Arabic, who were also highly profi-cient in English, were asked to translate the entriesdrawn from the English collection.
They were pre-sented with one sentence at a time, and asked to pro-vide the appropriate translation into their native lan-guage.
Since we had five Spanish, two Arabic, andtwo Romanian translators, an arbitrator (native to thelanguage) was charged with merging the candidatetranslations by proposing one sentence per language.Furthermore, to test the abstraction of semanticsfrom the choice of underlying language, we askedthree different Spanish human experts to re-score theSpanish text-pair translations on the same scale usedin the construction of the English collection.
Thecorrelation between the relatedness scores assignedduring this experiment and the scores assigned to theoriginal English experiment was 0.77 ?
0.86, indi-cating that the translations provided by the bilingualjudges were correct and preserved the semantics ofthe original English text-pairs.
As was the casefor the manually constructed word-to-word datasetspreviously described, the metrics obtained on themanually translated Li30 dataset will also act as anupper bound for the text-to-text evaluations.Finally, for a more sensible scenario where thetext fragments do not require manual translationsin order to compute their semantic relatedness, wecreate a multilingual version of the three Englishdatasets by employing statistical machine translationto translate the texts into the other three languages.Each text pair was processed through two separatequeries to the translation engine, since the two textfragments contain sufficient information to promptan in-context translation on their own.5 FrameworkWe generate SSA, LSA and ESA vectorial modelsfor English, Romanian, Arabic, and Spanish, usingthe same Wikipedia 2010 versions for all the sys-tems (e.g., the SSA, LSA and ESA relatednessmeasures for Spanish are all trained on the sameSpanish Wikipedia version).We construct a multilingual model by consideringa word- or text-pair from a source language along3Dataset is available for download at lit.csci.unt.edu/index.php?P=research/downloads23with its translations in the other languages.
To eval-uate this multilingual model in a way that reducesthe bias that may arise from choosing one languageover the other, we do the following: we start from asource language and generate all the possible combi-nations of this language with the available languageset {ar, en, es, ro}.
Within each combination, weaverage the monolingual model scores for the lan-guages in this combination with respect to the targetword- or text-pair into a final relatedness score.For example, let us consider Spanish as the sourcelanguage, then the possible combinations of the lan-guages that include the source language will be{{es}, {es, ar}, {es, ro}, {es, en}, {es, ar, en},{es, ar, ro}, {es, en, ro}, and {es, ar, en, ro}}.For each possible combination, we aggregate thescores of the languages in that combination.
In thissetting, a combination of size (cardinality) one willalways be the source language and will serve as thebaseline.
For every combination (e.g.
{es, ar}),we average the individual monolingual relatednessscores for a given word- or text-pair in this set.Finally, to calculate the overall correlation ofthese generated multilingual models (one system percombination size) with the human scores, we av-erage the correlation scores achieved over all thedatasets in a given combination (e.g., {es, ar}) withall correlation scores achieved under other combina-tions of the same size (e.g., {es, ro}, {es, en}).
Thisin effect allows us to observe the cumulative perfor-mance irrespective of language choice, as we extendthe multilingual model to include more languages.Formally, let N be the number of languages, Cnbe the set of all language combinations of size n, andci be one of the possible combinations of size n,Cn = {ci | |ci| = n, 0 < i <(Nn)} (1)then the relatedness of a word- or text-pair p fromthe dataset P under this combination can be repre-sented as:Simci(p) =1|ci|?l?ciSiml(p) (2)where Siml(p) is the relatedness score of the word-or text-pair p in the monolingual model of languagel.
To evaluate the performance of the multilingualmodel, let Di be the generated relatedness distribu-tion for the dataset P using the combination ci:Di = {?p, Simci(p)?
| p ?
P}.
(3)Then, the correlation between the gold standarddistribution G and the generated scores can be cal-culated as follows:CorrelCn(D,G) =1|Cn|?ci?CnCorrelci(Di, G),(4)where Correl can stand for Pearson (r), Spearman(?
), or their harmonic mean (?
), as also reported in(Hassan and Mihalcea, 2011).6 EvaluationsIn this section we revisit the questions formulated inthe introduction, and based on different experimentsetups following the framework introduced in Sec-tion 5, we provide an answer to each one of them.Does the task of semantic relatedness benefitfrom a multilingual representation?
We evalu-ate the three semantic relatedness models, namelyLSA, ESA and SSA on our manually constructedmultilingual word relatedness (MC30, WS353)and text relatedness datasets (LI30), as described inSection 4.Figure 1 plots the correlation scores achievedacross all the languages against the gold stan-dard and then averaged across all the multilingualdatasets.
The figure shows a clear and steady im-provement (25% - 28% with respect to the mono-lingual baseline) achieved when more languages areincorporated into the relatedness model.
It is worthnoting that both the Pearson and Spearman correla-tions exhibit the same improvement pattern, whichconfirms our hypothesis that adding more languageshas a positive impact on the relatedness scores.
Thefact that this trend is visible across all the systemssupports the idea that a multilingual representationconstitutes a better model for determining semanticrelatedness.
Furthermore, we notice that SSA is thebest performing system under these settings, with acorrelation improvement of approximately 15%.To further analyze the role of the multilingualmodel and to explore whether some languages ben-efit from using this abstraction more than others,we plot the correlation scores achieved by the indi-vidual languages averaged over all the systems andthe datasets in Figure 2.
We notice a sharp rise inperformance associated with the addition of morelanguages to the Arabic (42%) and the Romanian(47%) models, and a slower rise for Spanish (23%).The performance of English is also affected, but ona smaller scale (4%) when compared to the other240.30.40.50.60.70.80.91  2  3  4Number of Languages?ESALSASSA0.40.60.81  2  3  4r0.40.60.81  2  3  4?Figure 1: Manual translation - average correlation (?,r, ?)
obtained from incorporating scores from models inother languageslanguages.
Not surprisingly, this correlates with thesize of each corpus, where Arabic and Romanian arethe smallest, while English is the largest.The results support the notion that resource poorlanguages can benefit from languages with richerand larger resources, such as English or Spanish.Furthermore, incorporating additional languages toEnglish also leads to small improvements, which in-dicates that the benefit, while disproportionate, ismutual.Does the quality of translations affect the results?As a natural next step, we investigate the role playedby the manual translations in the performance of themultilingual model.
Since the previous evaluationsrequire the availability of the word- or text-pairsin multiple languages, we attempt to see if we caneliminate this restriction by automating the trans-lation process using statistical machine translation(MT).
Therefore, for a multilingual model employ-ing automated settings, the manual models proposedpreviously constitute an upper bound.We use the Google MT engine4 to translate ourmultilingual datasets into the target languages (en,es, ar, and ro).
We then repeat all the evaluationsusing the newly constructed datasets.Figure 3 shows the correlation scores achievedacross all the languages and averaged across all themultilingual datasets constructed using automatictranslation.
We again see a clear and steady im-4This API is now offered as a paid service; Microsoft orBabelfish automatic translation services are publicly available.0.30.40.50.60.70.80.91  2  3  4Number of Languages?arenesro0.40.60.81  2  3  4r0.40.60.81  2  3  4?Figure 2: Manual translation - average correlation (?,r, ?)
obtained by supplementing a source language withscores from other languagesprovement (12% - 35% with respect to the mono-lingual baseline) similar to the observed pattern inthe corresponding manual evaluations (Figure 1).While the overall achieved performance for SSAhas dropped (from ?
= 0.793 to ?
= 0.71) whencompared to the manual settings, we are still ableto improve over the baseline (?
= 0.635).
LSAseems to experience the highest relative improve-ment (35%), which might be due to its ability tohandle noise in these automatic settings.
Over-all Pearson and Spearman correlations exhibit thesame improvement pattern, which supports the no-tion that even with the possibility of introducingnoise through miss-translations, the models overallbenefit from the additional clues provided by themultilingual representation.To explore the effect of automatic translation onthe individual languages, we plot the correlationscores achieved vis-a`-vis a reference language, andaverage over all the systems and the automaticallytranslated datasets in Figure 4, in a similar fashionto Figure 2.We notice the similar rise in performance asso-ciated with the addition of more languages to theArabic (20%) and the Romanian (37%) models, anda slower rise for Spanish (16%) and English (8%).The effect of the automatic translation quality is ev-ident for the Arabic language where the automatictranslation seems to slow down the improvementwhen compared to the manual translations (Figure2).
A similar behavior is also observed in Spanishand Romanian but on a lower scale.250.30.40.50.60.70.80.91  2  3  4Number of Languages?ESALSASSA0.40.60.81  2  3  4r0.40.60.81  2  3  4?Figure 3: Automatic translation - average correlation (?,r, ?)
obtained from incorporating scores from models inother languagesA very interesting consideration is that Englishexperiences a stronger improvement when using au-tomatic translations (8%) compared to manual trans-lations (4%).
This can be attributed to the trans-lation engine quality in transferring English text toother languages and to the fact that the statisticaltranslation (when accurate) can lead to a transla-tion that makes use of more frequently used words,which contribute to more robust relatedness mea-sures.
When presented with a word pair, humanjudges may provide a translation influenced by theform/root of the word in the source language, whichmay not be as commonly used as the output of aMT system.
For example, when presented with thepair ?coast - shore,?
a Romanian translator may betempted to provide ?coasta??
as a translation candi-date for the first word in the pair, as it resembles theEnglish word in form.
However, the Romanian wordis highly ambiguous, and in an authoritative Roma-nian dictionary5 its primary sense is that of rib, fol-lowed by side, slope, and ultimately coast.
Thus, aMT system using a statistical inference may providea stronger translation such as ?t?a?rm?
that is far lessambiguous, and whose primary meaning is the oneintended by the original pair.Overall, the trend is positive and follows thepattern previously observed on the manually con-structed datasets.
This suggests that an automatictranslation, even if more noisy, is beneficial and pro-vides a way to reinforce semantic relatedness in a5http://dexonline.ro/definitie/coasta0.30.40.50.60.70.80.91  2  3  4Number of Languages?arenesro0.40.60.81  2  3  4r0.40.60.81  2  3  4?Figure 4: Automatic translation - average correlation (?,r, ?)
obtained by supplementing a source language withscores from other languagesgiven language with information coming from mul-tiple languages with no manual effort.Do our findings hold for different relatednessdatasets?
At last, encouraged by the small perfor-mance difference between the use of manual ver-sus automatic translations, we seek to explore howthis multilingual model behaves under the differentparadigms dictated by word relatedness versus textrelatedness scenarios.
Since our previous experi-ments were constrained to collections for which wealso had a manual translation, we perform a largerscale evaluation by including automatically trans-lated word relatedness (RG65) and text relatedness(LEE50 and AG400) datasets into all the languagesin our language set, and repeat all the word-to-wordand text-to-text evaluations.Table 1 shows the correlation scores achieved us-ing automatic translations on the word relatednessdatasets.
Most models on most datasets benefit fromthe multilingual representation (as shown by the fig-ures in bold).
Specifically, the SSA model has animprovement in ?
of 26% for WS353 and 15% forMC30.
This improvement is most evident in thecase of the largest dataset WS353, where all themultilingual models exhibit a consistent and strongperformance.Table 2 reports the results obtained for the textrelatedness datasets using automatic translation.While the ESA performance suffers in the multi-lingual model, it is overshadowed by the improve-ment experienced by LSA and SSA.
The multilin-26r ?
?Models MC30 RG65 WS353 MC30 RG65 WS353 MC30 RG65 WS353ESAen 0.645 0.644 0.487 0.742 0.768 0.525 0.690 0.701 0.506ESAml 0.723 0.741 0.515 0.766 0.759 0.519 0.744 0.75 0.517LSAen 0.509 0.450 0.435 0.525 0.499 0.436 0.517 0.473 0.436LSAml 0.538 0.566 0.487 0.484 0.569 0.517 0.510 0.567 0.502SSAen 0.771 0.824 0.543 0.688 0.772 0.553 0.727 0.797 0.548SSAml 0.873 0.807 0.674 0.803 0.795 0.713 0.836 0.801 0.693Table 1: Automatic translation - r, ?, ?
correlations on the word relatedness datasets using multilingual models.r ?
?Models LI30 LEE50 AG400 LI30 LEE50 AG400 LI30 LEE50 AG400ESAen 0.792 0.756 0.434 0.797 0.48 0.392 0.795 0.587 0.412ESAml 0.776 0.648 0.382 0.742 0.339 0.358 0.759 0.445 0.369LSAen 0.829 0.776 0.400 0.824 0.523 0.359 0.826 0.625 0.379LSAml 0.856 0.765 0.46 0.855 0.502 0.404 0.856 0.606 0.43SSAen 0.840 0.744 0.520 0.843 0.371 0.501 0.841 0.495 0.510SSAml 0.829 0.743 0.539 0.87 0.41 0.521 0.849 0.528 0.53Table 2: Automatic translation - r, ?, ?
correlations on the text relatedness datasets using multilingual models.gual model reports some of the best scores in theliterature, such as a correlations of r = 0.856 and?
= 0.87 for LI30 achieved by LSA and SSA, re-spectively.
Not surprisingly, SSA is still a top con-tender, achieving the highest scores for AG400 andLI30.
In AG400, SSA reports a ?
of 0.53 whichrepresents a 4% improvement over the English SSAmodel (?
= 0.51) and a 16% improvement over thebest knowledge-based system J&C (?
= 0.457).It is important to note that the evaluation in Ta-bles 1 and 2 are restricted to data translated from En-glish into a target language.
English, as a resource-rich language, has an extensive and robust monolin-gual model, yet it can still be enhanced with addi-tional clues originating from other languages.
Ac-cordingly, we only expected small improvements inthese two experiments, unlike the cases where westart from resource-poor languages such as Roma-nian or Arabic (see Figures 2 and 4).7 ConclusionIn this paper, we showed how a semantic relatednessmeasure computed in a multilingual space is ableto acquire and leverage additional information fromthe multilingual representation, and thus be strength-ened as more languages are taken into considera-tion.
Our experiments seem to suggest that combi-nations of multiple languages supply additional in-formation to derive a semantic relatedness betweentexts in an automatic framework.
Since establishingsemantic relatedness requires us to employ cogni-tive processes that are in large part independent ofthe language that we speak, it comes at no surprisethat using relatedness clues originating from morethan one language allows for a better identificationof relationships between texts.
While efficiency maybe a concern, it is worth noting that the method ishighly parallelizable, as the individual relatednessmeasures obtained before the aggregation step canbe calculated in parallel.Notably, all the relatedness measures that we ex-perimented with exhibited the same improvementtrend.
While this framework allows languages withscarce electronic resources, such as Romanian andArabic, to obtain very large improvements in seman-tic relatedness as compared to the monolingual mea-sures, improvements are also noticed for languageswith richer resources such as English.AcknowledgmentsThis material is based in part upon work sup-ported by the National Science Foundation CA-REER award #0747340 and IIS award #1018613.Any opinions, findings, and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof the National Science Foundation.27ReferencesC.
Banea and R. Mihalcea.
2011.
Word sense disam-biguation with multilingual features.
In InternationalConference on Semantic Computing, Oxford, UK.C.
Banea, R. Mihalcea, and J. Wiebe.
2010.
Multilingualsubjectivity: Are more languages better?
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics (Coling 2010), pages 28?36, Bei-jing, China, August.R.
Besanc?on and M. Rajman.
2002.
Evaluation of a vec-tor space similarity measure in a multilingual frame-work.
In Proceedings of the Third International Con-ference on Language Resource and Evaluation (LREC2002), Las Palmas, Spain.S.
Brin, J. Davis, and H. Garcia-Molina.
1995.
Copy de-tection mechanisms for digital documents.
In ACM In-ternational Conference on Management of Data (SIG-MOD 1995).A.
Z. Broder, S. C. Glassman, M. S. Manasse, andG.
Zweig.
1997.
Syntactic clustering of the web.Comput.
Netw.
ISDN Syst., 29(8-13):1157?1166.A Z. Broder, P. Ciccolo, M. Fontoura, E. Gabrilovich,V.
Josifovski, and L. Riedel.
2008.
Search advertisingusing web relevance feedback.
In CIKM ?08: Pro-ceeding of the 17th ACM conference on Informationand knowledge management, pages 1013?1022, NewYork, NY, USA.
ACM.C.
Burgess, K. Livesay, and K. Lund.
1998.
Explorationsin context space: words, sentences, discourse.
Dis-course Processes, 25(2):211?257.K.
Church and P. Hanks.
1990.
Word association norms,mutual information, and lexicography.
ComputationalLinguistics, 16(1):22?29.T.
Cohn and M. Lapata.
2007.
Machine translation bytriangulation: making effective use of multi-parallelcorpora.
In Proceedings of the 45th Annual Meetingof the Association of Computational Linguistics, pages728?735, Prague, Czech Republic.I.
Dagan, O. Glickman, and B. Magnini.
2005.
The PAS-CAL recognising textual entailment challenge.
In Pro-ceedings of the PASCAL Workshop.D.
Davidov and A. Rappoport.
2009.
Enhancementof lexical concepts using cross-lingual web mining.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages 852?861, Singapore.L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, and E. Ruppin.
2001.
Plac-ing search in context: the concept revisited.
In ACMPress, editor, The Tenth International World Wide WebConference, pages 406?414, Hong Kong.E.
Gabrilovich and S. Markovitch.
2007.
Computingsemantic relatedness using Wikipedia-based explicitsemantic analysis.
In Proceedings of the 20th Inter-national Joint Conference on Artificial Intelligence,pages 1606?1611, Hyderabad, India.A.
Goodrum.
2000.
Image information retrieval: Anoverview of current research.
Informing Science,3(2):63?66.S.
Hassan and R. Mihalcea.
2009.
Cross-lingual seman-tic relatedness using encyclopedic knowledge.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing, pages 1192?1201, Singapore.
Association for Computational Lin-guistics.S.
Hassan and R. Mihalcea.
2011.
Measuring semanticrelatedness using salient encyclopedic concepts.
Arti-ficial Intelligence, Special Issue, xx(xx).N.
Heintze.
1996.
Scalable document fingerprinting.
InIn Proc.
USENIX Workshop on Electronic Commerce.T.
C. Hoad and J. Zobel.
2003.
Methods for identifyingversioned and plagiarized documents.
J.
Am.
Soc.
Inf.Sci.
Technol., 54(3):203?215.A.
Islam and D. Inkpen.
2006.
Second order co-occurrence PMI for determining the semantic similar-ity of words.
In Proceedings of the Fifth Conferenceon Language Resources and Evaluation, volume 2,Genoa, Italy, July.M.
Jarmasz and S. Szpakowicz.
2003.
Roget?s thesaurusand semantic similarity.
In Proceedings of the confer-ence on Recent Advances in Natural Language Pro-cessing RANLP-2003, Borovetz, Bulgaria, September.J.
Kazama, S. De Saeger, K. Kuroda, M. Murata, andK.
Torisawa.
2010.
A bayesian method for robustestimation of distributional similarities.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, Uppsala, Sweden.T.
K. Landauer, D. Laham, B. Rehder, and M. E.Schreiner.
1991.
How well can passage meaning bederived without using word order?
A comparison oflatent semantic analysis and humans.
In Proceedingsof the 19th annual meeting of the Cognitive ScienceSociety, pages 412?417, Mawhwah, N. Erlbaum.C.
Leacock and M. Chodorow, 1998.
Combining localcontext and WordNet similarity for word sense identi-fication, pages 305?332.M.
D. Lee and M. Welsh.
2005.
An empirical evaluationof models of text document similarity.
In Proceedingsof the 27th annual meeting of the Cognitive ScienceSociety, pages 1254?1259, Stresa, Italy.C.
W. Leong and R. Mihalcea.
2009.
Explorations inautomatic image annotation using textual features.
InProceedings of the Third Linguistic Annotation Work-shop, pages 56?59, Suntec, Singapore, August.
Asso-ciation for Computational Linguistics.28M.
Lesk.
1986.
Automatic sense disambiguation usingmachine readable dictionaries.
In Proceedings of the5th annual international conference on Systems docu-mentation - SIGDOC ?86, pages 24?26, Toronto, On-tario.
ACM Press.W.
Li, Q. Lu, and R. Xu.
2005.
Similarity based chinesesynonym collocation extraction.
International Journalof Computational Linguistics and Chinese LanguageProcessing, 10(1).Y.
Li, D. McLean, Z.
A. Bandar, J. D. O?Shea, andK.
Crockett.
2006.
Sentence similarity based on se-mantic nets and corpus statistics.
IEEE Transactionson Knowledge and Data Engineering, 18(8):1138?1150, August.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the Fifteenth Interna-tional Conference on Machine Learning, pages 296?304, Madison, Wisconsin.U.
Manber.
1994.
Finding similar files in a large file sys-tem.
In USENIX WINTER 1994 TECHNICAL CON-FERENCE, pages 1?10.D.
Metzler, Y. Bernstein, W. Bruce Croft, A. Moffat,and J. Zobel.
2005.
Similarity measures for track-ing information flow.
In CIKM ?05: Proceedingsof the 14th ACM international conference on Infor-mation and knowledge management, pages 517?524,New York, NY, USA.
ACM.D.
Metzler, S. T. Dumais, and C. Meek.
2007.
Similaritymeasures for short segments of text.
In GiambattistaAmati, Claudio Carpineto, and Giovanni Romano, edi-tors, ECIR, volume 4425 of Lecture Notes in ComputerScience, pages 16?27.
Springer.G.
A. Miller and W. G. Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.G.
A. Miller.
1995.
WordNet: a Lexical database forenglish.
Communications of the Association for Com-puting Machinery, 38(11):39?41.T.
Mitchell, T. Russell, P. Broomhead, and N. Aldridge.2002.
Towards robust computerised marking of free-text responses.
In roceedings of the 6th Interna-tional Computer Assisted Assessment (CAA) Confer-ence, Loughborough, UK.
Loughborough University.M.
Mohler and R. Mihalcea.
2009a.
Text-to-text seman-tic similarity for automatic short answer grading.
InEACL, pages 567?575.
The Association for ComputerLinguistics.M.
Mohler and R. Mihalcea.
2009b.
Text-to-text seman-tic similarity for automatic short answer grading.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 567?575, Stroudsburg, PA, USA.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.WordNet::Similarity - measuring the relatedness ofconcepts.
In Proceedings of the Nineteenth Na-tional Conference on Artificial Intelligence (AAAI-04),demonstrations, San Jose, CA.J.
Ponte and W. Croft.
1998.
A language modeling ap-proach to information retrieval.
In Proceedings of theAnnual International SIGIR Conference on Researchand Development in Information Retrieval, pages 275?281, Melbourne, Australia.S.
G. Pulman and J.
Z. Sukkarieh.
2005.
Automaticshort answer marking.
In EdAppsNLP 05: Proceed-ings of the second workshop on Building EducationalApplications Using NLP, pages 9?16, Morristown, NJ,USA.
Association for Computational Linguistics.H.
Rubenstein and J.
B. Goodenough.
1965.
Contextualcorrelates of synonymy.
Communications of the ACM,8(10):627?633, October.M.
Sahami and T. D. Heilman.
2006.
A web-based ker-nel function for measuring the similarity of short textsnippets.
In WWW ?06: Proceedings of the 15th inter-national conference on World Wide Web, pages 377?386, New York, NY, USA.
ACM.N.
Shivakumar and H. Garcia-Molina.
1995.
Scam: Acopy detection mechanism for digital documents.
In2nd International Conference in Theory and Practiceof Digital Libraries (DL 1995).J.
Sinclair.
2001.
Collins cobuild English dictionary foradvanced learners.
Harper Collins, 3rd edition.P.
D. Turney.
2001.
Mining the Web for Synonyms:PMI-IR versus LSA on TOEFL.
In Proceedings ofthe 12th European Conference on Machine Learning,pages 491?502, Freiburg, Germany.Z.
Wu and M. Palmer.
1994.
Verbs semantics and lexicalselection.
In Proceedings of the 32nd annual meetingon Association for Computational Linguistics, pages133?-138, Las Cruces, New Mexico.W.
T. Yih and C. Meek.
2007.
Improving similarity mea-sures for short segments of text.
In AAAI?07: Pro-ceedings of the 22nd national conference on Artificialintelligence, pages 1489?1494.
AAAI Press.T.
Zesch, I. Gurevych, and M. Mu?hlha?user.
2007.
Com-paring Wikipedia and German Wordnet by EvaluatingSemantic Relatedness on Multiple Datasets.
In Pro-ceedings of Human Language Technologies: The An-nual Conference of the North American Chapter of theAssociation for Computational Linguistics.29
