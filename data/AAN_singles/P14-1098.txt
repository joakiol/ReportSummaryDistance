Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1041?1051,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsStructured Learning for Taxonomy Induction with Belief PropagationMohit BansalTTI Chicagombansal@ttic.eduDavid BurkettTwitter Inc.dburkett@twitter.comGerard de MeloTsinghua Universitygdm@demelo.orgDan KleinUC Berkeleyklein@cs.berkeley.eduAbstractWe present a structured learning approachto inducing hypernym taxonomies using aprobabilistic graphical model formulation.Our model incorporates heterogeneous re-lational evidence about both hypernymyand siblinghood, captured by semanticfeatures based on patterns and statisticsfrom Web n-grams and Wikipedia ab-stracts.
For efficient inference over tax-onomy structures, we use loopy beliefpropagation along with a directed span-ning tree algorithm for the core hyper-nymy factor.
To train the system, we ex-tract sub-structures of WordNet and dis-criminatively learn to reproduce them, us-ing adaptive subgradient stochastic opti-mization.
On the task of reproducingsub-hierarchies of WordNet, our approachachieves a 51% error reduction over achance baseline, including a 15% error re-duction due to the non-hypernym-factoredsibling features.
On a comparison setup,we find up to 29% relative error reductionover previous work on ancestor F1.1 IntroductionMany tasks in natural language understanding,such as question answering, information extrac-tion, and textual entailment, benefit from lexicalsemantic information in the form of types and hy-pernyms.
A recent example is IBM?s Jeopardy!system Watson (Ferrucci et al, 2010), which usedtype information to restrict the set of answer can-didates.
Information of this sort is present in termtaxonomies (e.g., Figure 1), ontologies, and the-sauri.
However, currently available taxonomiessuch as WordNet are incomplete in coverage (Pen-nacchiotti and Pantel, 2006; Hovy et al, 2009),unavailable in many domains and languages, andvertebratemammalplacentalcow rodentsquirrel ratmetatherianmarsupialkangarooreptilediapsidsnake crocodiliananapsidchelonianturtle1Figure 1: An excerpt of WordNet?s vertebrates taxonomy.time-intensive to create or extend manually.
Therehas thus been considerable interest in building lex-ical taxonomies automatically.In this work, we focus on the task of taking col-lections of terms as input and predicting a com-plete taxonomy structure over them as output.
Ourmodel takes a loglinear form and is representedusing a factor graph that includes both 1st-orderscoring factors on directed hypernymy edges (aparent and child in the taxonomy) and 2nd-orderscoring factors on sibling edge pairs (pairs of hy-pernym edges with a shared parent), as well as in-corporating a global (directed spanning tree) struc-tural constraint.
Inference for both learning anddecoding uses structured loopy belief propagation(BP), incorporating standard spanning tree algo-rithms (Chu and Liu, 1965; Edmonds, 1967; Tutte,1984).
The belief propagation approach allows usto efficiently and effectively incorporate hetero-geneous relational evidence via hypernymy andsiblinghood (e.g., coordination) cues, which wecapture by semantic features based on simple sur-face patterns and statistics from Web n-grams andWikipedia abstracts.
We train our model to max-imize the likelihood of existing example ontolo-gies using stochastic optimization, automaticallylearning the most useful relational patterns for fulltaxonomy induction.As an example of the relational patterns that our1041system learns, suppose we are interested in build-ing a taxonomy for types of mammals (see Fig-ure 1).
Frequent attestation of hypernymy patternslike rat is a rodent in large corpora is a strong sig-nal of the link rodent ?
rat.
Moreover, siblingor coordination cues like either rats or squirrelssuggest that rat is a sibling of squirrel and addsevidence for the links rodent ?
rat and rodent?
squirrel.
Our supervised model captures ex-actly these types of intuitions by automatically dis-covering such heterogeneous relational patterns asfeatures (and learning their weights) on edges andon sibling edge pairs, respectively.There have been several previous studies ontaxonomy induction.
e.g., the incremental tax-onomy induction system of Snow et al (2006),the longest path approach of Kozareva and Hovy(2010), and the maximum spanning tree (MST)approach of Navigli et al (2011) (see Section 4 fora more detailed overview).
The main contributionof this work is that we present the first discrimina-tively trained, structured probabilistic model overthe full space of taxonomy trees, using a struc-tured inference procedure through both the learn-ing and decoding phases.
Our model is also thefirst to directly learn relational patterns as part ofthe process of training an end-to-end taxonomicinduction system, rather than using patterns thatwere hand-selected or learned via pairwise clas-sifiers on manually annotated co-occurrence pat-terns.
Finally, it is the first end-to-end (i.e., non-incremental) system to include sibling (e.g., coor-dination) patterns at all.We test our approach in two ways.
First, onthe task of recreating fragments of WordNet, weachieve a 51% error reduction on ancestor-basedF1 over a chance baseline, including a 15% errorreduction due to the non-hypernym-factored sib-ling features.
Second, we also compare to the re-sults of Kozareva and Hovy (2010) by predictingthe large animal subtree of WordNet.
Here, weget up to 29% relative error reduction on ancestor-based F1.
We note that our approach falls at adifferent point in the space of performance trade-offs from past work ?
by producing complete,highly articulated trees, we naturally see a moreeven balance between precision and recall, whilepast work generally focused on precision.1To1While different applications will value precision andrecall differently, and past work was often intentionallyprecision-focused, it is certainly the case that an ideal solu-tion would maximize both.avoid presumption of a single optimal tradeoff, wealso present results for precision-based decoding,where we trade off recall for precision.2 Structured Taxonomy InductionGiven an input term set x = {x1, x2, .
.
.
, xn},we wish to compute the conditional distributionover taxonomy trees y.
This distribution P (y|x)is represented using the graphical model formu-lation shown in Figure 2.
A taxonomy tree y iscomposed of a set of indicator random variablesyij(circles in Figure 2), where yij= ON meansthat xiis the parent of xjin the taxonomy tree(i.e.
there exists a directed edge from xito xj).One such variable exists for each pair (i, j) with0 ?
i ?
n, 1 ?
j ?
n, and i 6= j.2In a factor graph formulation, a set of factors(squares and rectangles in Figure 2) determines theprobability of each possible variable assignment.Each factor F has an associated scoring function?F, with the probability of a total assignment de-termined by the product of all these scores:P (y|x) ?
?F?F(y) (1)2.1 Factor TypesIn the models we present here, there are threetypes of factors: EDGE factors that score individ-ual edges in the taxonomy tree, SIBLING factorsthat score pairs of edges with a shared parent, anda global TREE factor that imposes the structuralconstraint that y form a legal taxonomy tree.EDGE Factors.
For each edge variable yijinthe model, there is a corresponding factor Eij(small blue squares in Figure 2) that depends onlyon yij.
We score each edge by extracting a setof features f(xi, xj) and weighting them by the(learned) weight vector w. So, the factor scoringfunction is:?Eij(yij) ={exp(w ?
f(xi, xj)) yij= ONexp(0) = 1 yij= OFFSIBLING Factors.
Our second model also in-cludes factors that permit 2nd-order features look-ing at terms that are siblings in the taxonomy tree.For each triple (i, j, k) with i 6= j, i 6= k, andj < k,3we have a factor Sijk(green rectangles in2We assume a special dummy root symbol x0.3The ordering of the siblings xjand xkdoesn?t mat-ter here, so having separate factors for (i, j, k) and (i, k, j)would be redundant.1042y01 y02 y0ny1ny12y21 y2nyn1 yn2E02E01 E0nE1nE12E21 E2nEn1 En2T(a) Edge Features Onlyy01 y02 y0ny1ny12y21 y2nyn1 yn2E02E01 E0nE1nE12E21 E2nEn1 En2S12nS21nSn12T(b) Full ModelFigure 2: Factor graph representation of our model, both without (a) and with (b) SIBLING factors.Figure 2b) that depends on yijand yik, and thuscan be used to encode features that should be ac-tive whenever xjand xkshare the same parent, xi.The scoring function is similar to the one above:?Sijk(yij, yik) ={exp(w ?
f(xi, xj, xk)) yij= yik= ON1 otherwiseTREE Factor.
Of course, not all variable as-signments y form legal taxonomy trees (i.e., di-rected spanning trees).
For example, the assign-ment ?i, j, yij= ON might get a high score, butwould not be a valid output of the model.
Thus,we need to impose a structural constraint to ensurethat such illegal variable assignments are assigned0 probability by the model.
We encode this in ourfactor graph setting using a single global factor T(shown as a large red square in Figure 2) with thefollowing scoring function:?T(y) ={1 y forms a legal taxonomy tree0 otherwiseModel.
For a given global assignment y, letf(y) =?i,jyij=ONf(xi, xj) +?i,j,kyij=yik=ONf(xi, xj, xk)Note that by substituting our model?s factor scor-ing functions into Equation 1, we get:P (y|x) ?
{exp(w ?
f(y)) y is a tree0 otherwiseThus, our model has the form of a standard loglin-ear model with feature function f .2.2 Inference via Belief PropagationWith the model defined, there are two main in-ference tasks we wish to accomplish: computingexpected feature counts and selecting a particulartaxonomy tree for a given set of input terms (de-coding).
As an initial step to each of these pro-cedures, we wish to compute the marginal prob-abilities of particular edges (and pairs of edges)being on.
In a factor graph, the natural infer-ence procedure for computing marginals is beliefpropagation.
Note that finding taxonomy trees isa structurally identical problem to directed span-ning trees (and thereby non-projective dependencyparsing), for which belief propagation has previ-ously been worked out in depth (Smith and Eisner,2008).
Therefore, we will only briefly sketch theprocedure here.Belief propagation is a general-purpose infer-ence method that computes marginals via directedmessages passed from variables to adjacent fac-tors (and vice versa) in the factor graph.
Thesemessages take the form of (possibly unnormal-ized) distributions over values of the variable.
Thetwo types of messages (variable to factor or fac-tor to variable) have mutually recursive defini-tions.
The message from a factor F to an adjacentvariable V involves a sum over all possible val-ues of every other variable that F touches.
Whilethe EDGE and SIBLING factors are simple enoughto compute this sum by brute force, performingthe sum na?
?vely for computing messages from theTREE factor would take exponential time.
How-1043ever, due to the structure of that particular factor,all of its outgoing messages can be computed si-multaneously in O(n3) time via an efficient adap-tation of Kirchhoff?s Matrix Tree Theorem (MTT)(Tutte, 1984) which computes partition functionsand marginals for directed spanning trees.Once message passing is completed, marginalbeliefs are computed by merely multiplying to-gether all the messages received by a particularvariable or factor.2.2.1 Loopy Belief PropagationLooking closely at Figure 2a, one can observethat the factor graph for the first version of ourmodel, containing only EDGE and TREE factors,is acyclic.
In this special case, belief propagationis exact: after one round of message passing, thebeliefs computed (as discussed in Section 2.2) willbe the true marginal probabilities under the cur-rent model.
However, in the full model, shownin Figure 2b, the SIBLING factors introduce cy-cles into the factor graph, and now the messagesbeing passed around often depend on each otherand so they will change as they are recomputed.The process of iteratively recomputing messagesbased on earlier messages is known as loopy beliefpropagation.
This procedure only finds approx-imate marginal beliefs, and is not actually guar-anteed to converge, but in practice can be quiteeffective for finding workable marginals in mod-els for which exact inference is intractable, as isthe case here.
All else equal, the more roundsof message passing that are performed, the closerthe computed marginal beliefs will be to the truemarginals, though in practice, there are usually di-minishing returns after the first few iterations.
Inour experiments, we used a fairly conservative up-per bound of 20 iterations, but in most cases, themessages converged much earlier than that.2.3 TrainingWe used gradient-based maximum likelihoodtraining to learn the model parameters w. Sinceour model has a loglinear form, the derivativeof w with respect to the likelihood objective iscomputed by just taking the gold feature vec-tor and subtracting the vector of expected featurecounts.
For computing expected counts, we runbelief propagation until completion and then, foreach factor in the model, we simply read off themarginal probability of that factor being active (ascomputed in Section 2.2), and accumulate a par-tial count for each feature that is fired by that fac-tor.
This method of computing the gradient can beincorporated into any gradient-based optimizer inorder to learn the weights w. In our experimentswe used AdaGrad (Duchi et al, 2011), an adaptivesubgradient variant of standard stochastic gradientascent for online learning.2.4 DecodingFinally, once the model parameters have beenlearned, we want to use the model to find taxon-omy trees for particular sets of input terms.
Notethat if we limit our scores to be edge-factored,then finding the highest scoring taxonomy treebecomes an instance of the MST problem (alsoknown as the maximum arborescence problemfor the directed case), which can be solved effi-ciently in O(n2) quadratic time (Tarjan, 1977) us-ing the greedy, recursive Chu-Liu-Edmonds algo-rithm (Chu and Liu, 1965; Edmonds, 1967).4Since the MST problem can be solved effi-ciently, the main challenge becomes finding a wayto ensure that our scores are edge-factored.
In thefirst version of our model, we could simply set thescore of each edge to be w?f(xi, xj), and the MSTrecovered in this way would indeed be the high-est scoring tree: arg maxyP (y|x).
However, thisstraightforward approach doesn?t apply to the fullmodel which also uses sibling features.
Hence, atdecoding time, we instead start out by once moreusing belief propagation to find marginal beliefs,and then set the score of each edge to be its beliefodds ratio:bYij(ON)bYij(OFF).53 FeaturesWhile spanning trees are familiar from non-projective dependency parsing, features based onthe linear order of the words or on lexical identi-4See Georgiadis (2003) for a detailed algorithmic proof,and McDonald et al (2005) for an illustrative example.
Also,we constrain the Chu-Liu-Edmonds MST algorithm to out-put only single-root MSTs, where the (dummy) root has ex-actly one child (Koo et al, 2007), because multi-root span-ning ?forests?
are not applicable to our task.Also, note that we currently assume one node per term.
Weare following the task description from previous work wherethe goal is to create a taxonomy for a specific domain (e.g.,animals).
Within a specific domain, terms typically just havea single sense.
However, our algorithms could certainly beadapted to the case of multiple term senses (by treating thedifferent senses as unique nodes in the tree) in future work.5The MST that is found using these edge scores is actuallythe minimum Bayes risk tree (Goodman, 1996) for an edgeaccuracy loss function (Smith and Eisner, 2008).1044ties or syntactic word classes, which are primarydrivers for dependency parsing, are mostly unin-formative for taxonomy induction.
Instead, induc-ing taxonomies requires world knowledge to cap-ture the semantic relations between various unseenterms.
For this, we use semantic cues to hyper-nymy and siblinghood via features on simple sur-face patterns and statistics in large text corpora.We fire features on both the edge and the siblingfactors.
We first describe all the edge featuresin detail (Section 3.1 and Section 3.2), and thenbriefly describe the sibling features (Section 3.3),which are quite similar to the edge ones.For each edge factor Eij, which represents thepotential parent-child term pair (xi, xj), we addthe surface and semantic features discussed below.Note that since edges are directed, we have sepa-rate features for the factors Eijversus Eji.3.1 Surface FeaturesCapitalization: Checks which of xiand xjarecapitalized, with one feature for each value of thetuple (isCap(xi), isCap(xj)).
The intuition is thatleaves of a taxonomy are often proper names andhence capitalized, e.g., (bison, American bison).Therefore, the feature for (true, false) (i.e., parentcapitalized but not the child) gets a substantiallynegative weight.Ends with: Checks if xjends with xi, or not.
Thiscaptures pairs such as (fish, bony fish) in our data.Contains: Checks if xjcontains xi, or not.
Thiscaptures pairs such as (bird, bird of prey).Suffix match: Checks whether the k-length suf-fixes of xiand xjmatch, or not, for k =1, 2, .
.
.
, 7.LCS: We compute the longest common substringof xiand xj, and create indicator features forrounded-off and binned values of |LCS|/((|xi|+|xj|)/2).Length difference: We compute the signed lengthdifference between xjand xi, and create indica-tor features for rounded-off and binned values of(|xj| ?
|xi|)/((|xi| + |xj|)/2).
Yang and Callan(2009) use a similar feature.3.2 Semantic Features3.2.1 Web n-gram FeaturesPatterns and counts: Hypernymy for a term pair(P=xi, C=xj) is often signaled by the presenceof surface patterns like C is a P, P such as Cin large text corpora, an observation going backto Hearst (1992).
For each potential parent-childedge (P=xi, C=xj), we mine the top k strings(based on count) in which both xiand xjoccur(we use k=200).
We collect patterns in both direc-tions, which allows us to judge the correct direc-tion of an edge (e.g., C is a P is a positive signalfor hypernymy whereas P is a C is a negative sig-nal).6Next, for each pattern in this top-k list, wecompute its normalized pattern count c, and firean indicator feature on the tuple (pattern, t), forall thresholds t (in a fixed set) s.t.
c ?
t. Oursupervised model then automatically learns whichpatterns are good indicators of hypernymy.Pattern order: We add features on the order (di-rection) in which the pair (xi, xj) found a pattern(in its top-k list) ?
indicator features for booleanvalues of the four cases: P .
.
.
C, C .
.
.
P , neitherdirection, and both directions.
Ritter et al (2009)used the ?both?
case of this feature.Individual counts: We also compute the indi-vidual Web-scale term counts cxiand cxj, andadd a comparison feature (cxi>cxj), plus featureson values of the signed count difference (|cxi| ?|cxj|)/((|cxi| + |cxj|)/2), after rounding off, andbinning at multiple granularities.
The intuition isthat this feature could learn whether the relativepopularity of the terms signals their hypernymy di-rection.3.2.2 Wikipedia Abstract FeaturesThe Web n-grams corpus has broad coverage butis limited to up to 5-grams, so it may not containpattern-based evidence for various longer multi-word terms and pairs.
Therefore, we supplementit with a full-sentence resource, namely Wikipediaabstracts, which are concise descriptions (henceuseful to signal hypernymy) of a large variety ofworld entities.Presence and distance: For each potential edge(xi, xj), we mine patterns from all abstracts inwhich the two terms co-occur in either order, al-lowing a maximum term distance of 20 (becausebeyond that, co-occurrence may not imply a rela-tion).
We add a presence feature based on whetherthe process above found at least one pattern forthat term pair, or not.
We also fire features onthe value of the minimum distance dminat which6We also allow patterns with surrounding words, e.g., theC is a P and C , P of.1045the two terms were found in some abstract (plusthresholded versions).Patterns: For each term pair, we take the top-k?patterns (based on count) of length up to l fromits full list of patterns, and add an indicator featureon each pattern string (without the counts).
We usek?=5, l=10.
Similar to the Web n-grams case, wealso fire Wikipedia-based pattern order features.3.3 Sibling FeaturesWe also incorporate similar features on siblingfactors.
For each sibling factor Sijkwhich rep-resents the potential parent-children term triple(xi, xj, xk), we consider the potential sibling termpair (xj, xk).
Siblinghood for this pair would beindicated by the presence of surface patterns suchas either C1or C2, C1is similar to C2in large cor-pora.
Hence, we fire Web n-gram pattern featuresand Wikipedia presence, distance, and pattern fea-tures, similar to those described above, on eachpotential sibling term pair.7The main differencehere from the edge factors is that the sibling fac-tors are symmetric (in the sense that Sijkis redun-dant to Sikj) and hence the patterns are undirected.Therefore, for each term pair, we first symmetrizethe collected Web n-grams and Wikipedia patternsby accumulating the counts of symmetric patternslike rats or squirrels and squirrels or rats.84 Related WorkIn our work, we assume a known term set anddo not address the problem of extracting relatedterms from text.
However, a great deal of pastwork has considered automating this process, typ-ically taking one of two major approaches.
Theclustering-based approach (Lin, 1998; Lin andPantel, 2002; Davidov and Rappoport, 2006; Ya-mada et al, 2009) discovers relations based on theassumption that similar concepts appear in sim-7One can also add features on the full triple (xi, xj, xk)but most such features will be sparse.8All the patterns and counts for our Web and Wikipediaedge and sibling features described above are extracted afterstemming the words in the terms, the n-grams, and the ab-stracts (using the Porter stemmer).
Also, we threshold thefeatures (to prune away the sparse ones) by considering onlythose that fire for at least t trees in the training data (t = 4 inour experiments).Note that one could also add various complementary types ofuseful features presented by previous work, e.g., bootstrap-ping using syntactic heuristics (Phillips and Riloff, 2002),dependency patterns (Snow et al, 2006), doubly anchoredpatterns (Kozareva et al, 2008; Hovy et al, 2009), and Webdefinition classifiers (Navigli et al, 2011).ilar contexts (Harris, 1954).
The pattern-basedapproach uses special lexico-syntactic patterns toextract pairwise relation lists (Phillips and Riloff,2002; Girju et al, 2003; Pantel and Pennacchiotti,2006; Suchanek et al, 2007; Ritter et al, 2009;Hovy et al, 2009; Baroni et al, 2010; Ponzettoand Strube, 2011) and semantic classes or class-instance pairs (Riloff and Shepherd, 1997; Katzand Lin, 2003; Pas?ca, 2004; Etzioni et al, 2005;Talukdar et al, 2008).We focus on the second step of taxonomy induc-tion, namely the structured organization of termsinto a complete and coherent tree-like hierarchy.9Early work on this task assumes a starting par-tial taxonomy and inserts missing terms into it.Widdows (2003) place unknown words into a re-gion with the most semantically-similar neigh-bors.
Snow et al (2006) add novel terms by greed-ily maximizing the conditional probability of a setof relational evidence given a taxonomy.
Yang andCallan (2009) incrementally cluster terms basedon a pairwise semantic distance.
Lao et al (2012)extend a knowledge base using a random walkmodel to learn binary relational inference rules.However, the task of inducing full taxonomieswithout assuming a substantial initial partial tax-onomy is relatively less well studied.
There issome prior work on the related task of hierarchicalclustering, or grouping together of semanticallyrelated words (Cimiano et al, 2005; Cimiano andStaab, 2005; Poon and Domingos, 2010; Fountainand Lapata, 2012).
The task we focus on, though,is the discovery of direct taxonomic relationships(e.g., hypernymy) between words.We know of two closely-related previous sys-tems, Kozareva and Hovy (2010) and Navigli etal.
(2011), that build full taxonomies from scratch.Both of these systems use a process that startsby finding basic level terms (leaves of the fi-nal taxonomy tree, typically) and then using re-lational patterns (hand-selected ones in the case ofKozareva and Hovy (2010), and ones learned sep-arately by a pairwise classifier on manually anno-tated co-occurrence patterns for Navigli and Ve-lardi (2010), Navigli et al (2011)) to find interme-diate terms and all the attested hypernymy linksbetween them.10To prune down the resulting tax-9Determining the set of input terms is orthogonal to ourwork, and our method can be used in conjunction with vari-ous term extraction approaches described above.10Unlike our system, which assumes a complete set ofterms and only attempts to induce the taxonomic structure,1046onomy graph, Kozareva and Hovy (2010) use aprocedure that iteratively retains the longest pathsbetween root and leaf terms, removing conflictinggraph edges as they go.
The end result is acyclic,though not necessarily a tree; Navigli et al (2011)instead use the longest path intuition to weightedges in the graph and then find the highest weighttaxonomic tree using a standard MST algorithm.Our work differs from the two systems abovein that ours is the first discriminatively trained,structured probabilistic model over the full spaceof taxonomy trees that uses structured inferencevia spanning tree algorithms (MST and MTT)through both the learning and decoding phases.Our model also automatically learns relational pat-terns as a part of the taxonomic training phase, in-stead of relying on hand-picked rules or pairwiseclassifiers on manually annotated co-occurrencepatterns, and it is the first end-to-end (i.e., non-incremental) system to include heterogeneous re-lational information via sibling (e.g., coordina-tion) patterns.5 Experiments5.1 Data and Experimental RegimeWe considered two distinct experimental setups,one that illustrates the general performance ofour model by reproducing various medium-sizedWordNet domains, and another that facilitatescomparison to previous work by reproducing themuch larger animal subtree provided by Kozarevaand Hovy (2010).General setup: In order to test the accuracyof structured prediction on medium-sized full-domain taxonomies, we extracted from WordNet3.0 all bottomed-out full subtrees which had atree-height of 3 (i.e., 4 nodes from root to leaf),and contained (10, 50] terms.11This gives us761 non-overlapping trees, which we partition intoboth these systems include term discovery in the taxonomybuilding process.11Subtrees that had a smaller or larger tree height were dis-carded in order to avoid overlap between the training and testdivisions.
This makes it a much stricter setting than othertasks such as parsing, which usually has repeated sentences,clauses and phrases between training and test sets.To project WordNet synsets to terms, we used the first (mostfrequent) term in each synset.
A few WordNet synsets havemultiple parents so we only keep the first of each such pair ofoverlapping trees.
We also discard a few trees with duplicateterms because this is mostly due to the projection of differentsynsets to the same term, and theoretically makes the tree agraph.70/15/15% (533/114/114 trees) train/dev/test sets.Comparison setup: We also compare our method(as closely as possible) with related previous workby testing on the much larger animal subtree madeavailable by Kozareva and Hovy (2010), who cre-ated this dataset by selecting a set of ?harvested?terms and retrieving all the WordNet hypernymsbetween each input term and the root (i.e., an-imal), resulting in ?700 terms and ?4,300 is-aancestor-child links.12Our training set for this an-imal test case was generated from WordNet us-ing the following process: First, we strictly re-move the full animal subtree from WordNet in or-der to avoid any possible overlap with the test data.Next, we create random 25-sized trees by pickingrandom nodes as singleton trees, and repeatedlyadding child edges from WordNet to the tree.
Thisprocess gives us a total of ?1600 training trees.13Feature sources: The n-gram semantic featuresare extracted from the Google n-grams corpus(Brants and Franz, 2006), a large collection ofEnglish n-grams (for n = 1 to 5) and their fre-quencies computed from almost 1 trillion tokens(95 billion sentences) of Web text.
The Wikipediaabstracts are obtained via the publicly availabledump, which contains almost ?4.1 million ar-ticles.14Preprocessing includes standard XMLparsing and tokenization.
Efficient collection offeature statistics is important because these mustbe extracted for millions of query pairs (for eachpotential edge and sibling pair in each term set).For this, we use a hash-trie on term pairs (sim-ilar to that of Bansal and Klein (2011)), and scanonce through the n-gram (or abstract) set, skippingmany n-grams (or abstracts) based on fast checksof missing unigrams, exceeding length, suffix mis-matches, etc.5.2 Evaluation MetricAncestor F1: Measures the precision, recall, andF1= 2PR/(P +R) of correctly predicted ances-12This is somewhat different from our general setup wherewe work with any given set of terms; they start with a largeset of leaves which have substantial Web-based relationalinformation based on their selected, hand-picked patterns.Their data is available at http://www.isi.edu/?kozareva/downloads.html.13We tried this training regimen as different from that ofthe general setup (which contains only bottomed-out sub-trees), so as to match the animal test tree, which is of depth12 and has intermediate nodes from higher up in WordNet.14We used the 20130102 dump.1047System P R F1Edges-Only ModelBaseline 5.9 8.3 6.9Surface Features 17.5 41.3 24.6Semantic Features 37.0 49.1 42.2Surface+Semantic 41.1 54.4 46.8Edges + Siblings ModelSurface+Semantic 53.1 56.6 54.8Surface+Semantic (Test) 48.0 55.2 51.4Table 1: Main results on our general setup.
On the devel-opment set, we present incremental results on the edges-onlymodel where we start with the chance baseline, then use sur-face features only, semantic features only, and both.
Finally,we add sibling factors and features to get results for the full,edges+siblings model with all features, and also report thefinal test result for this setting.tors, i.e., pairwise is-a relations:P =|isagold?
isapredicted||isapredicted|, R =|isagold?
isapredicted||isagold|5.3 ResultsTable 1 shows our main results for ancestor-basedevaluation on the general setup.
We present a de-velopment set ablation study where we start withthe edges-only model (Figure 2a) and its randomtree baseline (which chooses any arbitrary span-ning tree for the term set).
Next, we show resultson the edges-only model with surface features(Section 3.1), semantic features (Section 3.2), andboth.
We see that both surface and semantic fea-tures make substantial contributions, and they alsostack.
Finally, we add the sibling factors and fea-tures (Figure 2b, Section 3.3), which further im-proves the results significantly (8% absolute and15% relative error reduction over the edges-onlyresults on the ancestor F1 metric).
The last rowshows the final test set results for the full modelwith all features.Table 2 shows our results for comparison tothe larger animal dataset of Kozareva and Hovy(2010).15In the table, ?Kozareva2010?
refersto Kozareva and Hovy (2010) and ?Navigli2011?refers to Navigli et al (2011).16For appropri-15These results are for the 1st order model due to the scaleof the animal taxonomy (?700 terms).
For scaling the 2ndorder sibling model, one can use approximations, e.g., prun-ing the set of sibling factors based on 1st order link marginals,or a hierarchical coarse-to-fine approach based on taxonomyinduction on subtrees, or a greedy approach of adding a fewsibling factors at a time.
This is future work.16The Kozareva and Hovy (2010) ancestor results are ob-tained by using the output files provided on their webpage.System P R F1Previous WorkKozareva2010 98.6 36.2 52.9Navigli2011??97.0??43.7??60.3?
?This PaperFixed Prediction 84.2 55.1 66.6Free Prediction 79.3 49.0 60.6Table 2: Comparison results on the animal dataset ofKozareva and Hovy (2010).
Here, ?Kozareva2010?
refers toKozareva and Hovy (2010) and ?Navigli2011?
refers to Nav-igli et al (2011).
For appropriate comparison to each previ-ous work, we show our results both for the ?Fixed Prediction?setup, which assumes the true root and leaves, and for the?Free Prediction?
setup, which doesn?t assume any prior in-formation.
The ??
results of Navigli et al (2011) represent adifferent ground-truth data condition, making them incompa-rable to our results; see Section 5.3 for details.ate comparison to each previous work, we showresults for two different setups.
The first setup?Fixed Prediction?
assumes that the model knowsthe true root and leaves of the taxonomy to providefor a somewhat fairer comparison to Kozareva andHovy (2010).
We get substantial improvementson ancestor-based recall and F1 (a 29% relativeerror reduction).
The second setup ?Free Predic-tion?
assumes no prior knowledge and predicts thefull tree (similar to the general setup case).
Onthis setup, we do compare as closely as possibleto Navigli et al (2011) and see a small gain in F1,but regardless, we should note that their results areincomparable (denoted by ??
in Table 2) becausethey have a different ground-truth data condition:their definition and hypernym extraction phase in-volves using the Google define keyword, whichoften returns WordNet glosses itself.We note that previous work achieves higher an-cestor precision, while our approach achieves amore even balance between precision and recall.Of course, precision and recall should both ide-ally be high, even if some applications weigh oneover the other.
This is why our tuning optimizedfor F1, which represents a neutral combinationfor comparison, but other F?metrics could alsobe optimized.
In this direction, we also tried anexperiment on precision-based decoding (for the?Free Prediction?
scenario), where we discard anyedges with score (i.e., the belief odds ratio de-scribed in Section 2.4) less than a certain thresh-old.
This allowed us to achieve high values of pre-cision (e.g., 90.8%) at still high enough F1 values(e.g., 61.7%).1048Hypernymy featuresC and other P > P > CC , P of C is a PC , a P P , including CC or other P P ( CC : a P C , american PC - like P C , the PSiblinghood featuresC1and C2C1, C2(C1or C2of C1and / or C2, C1, C2and either C1or C2the C1/ C2<s> C1and C2</s>Table 3: Examples of high-weighted hypernymy and sibling-hood features learned during development.butterflycopperAmerican copperhairstreakStrymon melinusadmiralwhite admiral1Figure 3: Excerpt from the predicted butterfly tree.
The termsattached erroneously according to WordNet are marked in redand italicized.6 AnalysisTable 3 shows some of the hypernymy and sibling-hood features given highest weight by our model(in general-setup development experiments).
Thetraining process not only rediscovers most of thestandard Hearst-style hypernymy patterns (e.g., Cand other P, C is a P), but also finds variousnovel, intuitive patterns.
For example, the patternC, american P is prominent because it capturespairs like Lemmon, american actor and Bryon,american politician, etc.
Another pattern > P >C captures webpage navigation breadcrumb trails(representing category hierarchies).
Similarly, thealgorithm also discovers useful siblinghood fea-tures, e.g., either C1or C2, C1and / or C2, etc.Finally, we look at some specific output errorsto give as concrete a sense as possible of some sys-tem confusions, though of course any hand-chosenexamples must be taken as illustrative.
In Figure3, we attach white admiral to admiral, whereasthe gold standard makes these two terms siblings.In reality, however, white admirals are indeed aspecies of admirals, so WordNet?s ground truthturns out to be incomplete.
Another such exampleis that we place logistic assessment in the evalu-bottleflaskvacuum flask thermos Erlenmeyer flaskwine bottle jeroboam1Figure 4: Excerpt from the predicted bottle tree.
The termsattached erroneously according to WordNet are marked in redand italicized.ation subtree of judgment, but WordNet makes ita direct child of judgment.
However, other dictio-naries do consider logistic assessments to be eval-uations.
Hence, this illustrates that there may bemore than one right answer, and that the low re-sults on this task should only be interpreted assuch.
In Figure 4, our algorithm did not recog-nize that thermos is a hyponym of vacuum flask,and that jeroboam is a kind of wine bottle.
Here,our Web n-grams dataset (which only contains fre-quent n-grams) and Wikipedia abstracts do notsuffice and we would need to add richer Web datafor such world knowledge to be reflected in thefeatures.7 ConclusionOur approach to taxonomy induction allows het-erogeneous information sources to be combinedand balanced in an error-driven way.
Direct indi-cators of hypernymy, such as Hearst-style contextpatterns, are the core feature for the model and arediscovered automatically via discriminative train-ing.
However, other indicators, such as coordina-tion cues, can indicate that two words might besiblings, independently of what their shared par-ent might be.
Adding second-order factors to ourmodel allows these two kinds of evidence to beweighed and balanced in a discriminative, struc-tured probabilistic framework.
Empirically, wesee substantial gains (in ancestor F1) from siblingfeatures, and also over comparable previous work.We also present results on the precision and recalltrade-offs inherent in this task.AcknowledgmentsWe would like to thank the anonymous review-ers for their insightful comments.
This workwas supported by BBN under DARPA contractHR0011-12-C-0014, 973 Program China Grants2011CBA00300, 2011CBA00301, and NSFCGrants 61033001, 61361136003.1049ReferencesMohit Bansal and Dan Klein.
2011.
Web-scale fea-tures for full-scale parsing.
In Proceedings of ACL.Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-simo Poesio.
2010.
Strudel: A corpus-based seman-tic model based on properties and types.
CognitiveScience, 34(2):222?254.Thorsten Brants and Alex Franz.
2006.
The GoogleWeb 1T 5-gram corpus version 1.1.
LDC2006T13.Yoeng-Jin Chu and Tseng-Hong Liu.
1965.
On theshortest arborescence of a directed graph.
ScienceSinica, 14(1396-1400):270.Philipp Cimiano and Steffen Staab.
2005.
Learningconcept hierarchies from text with a guided agglom-erative clustering algorithm.
In Proceedings of theICML 2005 Workshop on Learning and ExtendingLexical Ontologies with Machine Learning Meth-ods.Philipp Cimiano, Andreas Hotho, and Steffen Staab.2005.
Learning concept hierarchies from text cor-pora using formal concept analysis.
Journal of Arti-ficial Intelligence Research, 24(1):305?339.Dmitry Davidov and Ari Rappoport.
2006.
Effi-cient unsupervised discovery of word categories us-ing symmetric patterns and high frequency words.In Proceedings of COLING-ACL.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Jack Edmonds.
1967.
Optimum branchings.
Journalof Research of the National Bureau of Standards B,71:233?240.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Un-supervised named-entity extraction from the Web:An experimental study.
Artificial Intelligence,165(1):91?134.David Ferrucci, Eric Brown, Jennifer Chu-Carroll,James Fan, David Gondek, Aditya A Kalyanpur,Adam Lally, J William Murdock, Eric Nyberg, JohnPrager, Nico Schlaefer, and Chris Welty.
2010.Building watson: An overview of the DeepQAproject.
AI magazine, 31(3):59?79.Trevor Fountain and Mirella Lapata.
2012.
Taxonomyinduction using hierarchical random graphs.
In Pro-ceedings of NAACL.Leonidas Georgiadis.
2003.
Arborescence optimiza-tion problems solvable by edmonds algorithm.
The-oretical Computer Science, 301(1):427?437.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2003.
Learning semantic constraints for the auto-matic discovery of part-whole relations.
In Proceed-ings of NAACL.Joshua Goodman.
1996.
Parsing algorithms and met-rics.
In Proceedings of ACL.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Marti Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofCOLING.Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff.2009.
Toward completeness in concept extractionand classification.
In Proceedings of EMNLP.Boris Katz and Jimmy Lin.
2003.
Selectively using re-lations to improve precision in question answering.In Proceedings of the Workshop on NLP for Ques-tion Answering (EACL 2003).Terry Koo, Amir Globerson, Xavier Carreras, andMichael Collins.
2007.
Structured prediction mod-els via the matrix-tree theorem.
In Proceedings ofEMNLP-CoNLL.Zornitsa Kozareva and Eduard Hovy.
2010.
Asemi-supervised method to learn and construct tax-onomies using the Web.
In Proceedings of EMNLP.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.2008.
Semantic class learning from the web withhyponym pattern linkage graphs.
In Proceedings ofACL.Ni Lao, Amarnag Subramanya, Fernando Pereira, andWilliam W. Cohen.
2012.
Reading the web withlearned syntactic-semantic inference rules.
In Pro-ceedings of EMNLP.Dekang Lin and Patrick Pantel.
2002.
Concept discov-ery from text.
In Proceedings of COLING.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Haji?c.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof HLT-EMNLP.Roberto Navigli and Paola Velardi.
2010.
Learningword-class lattices for definition and hypernym ex-traction.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics.Roberto Navigli, Paola Velardi, and Stefano Faralli.2011.
A graph-based algorithm for inducing lexicaltaxonomies from scratch.
In Proceedings of IJCAI.Patrick Pantel and Marco Pennacchiotti.
2006.Espresso: Leveraging generic patterns for automati-cally harvesting semantic relations.
In Proceedingsof COLING-ACL.1050Marius Pas?ca.
2004.
Acquisition of categorized namedentities for web search.
In Proceedings of CIKM.Marco Pennacchiotti and Patrick Pantel.
2006.
On-tologizing semantic relations.
In Proceedings ofCOLING-ACL.William Phillips and Ellen Riloff.
2002.
Exploitingstrong syntactic heuristics and co-training to learnsemantic lexicons.
In Proceedings of EMNLP.Simone Paolo Ponzetto and Michael Strube.
2011.Taxonomy induction based on a collaborativelybuilt knowledge repository.
Artificial Intelligence,175(9):1737?1756.Hoifung Poon and Pedro Domingos.
2010.
Unsuper-vised ontology induction from text.
In Proceedingsof ACL.Ellen Riloff and Jessica Shepherd.
1997.
A corpus-based approach for building semantic lexicons.
InProceedings of EMNLP.Alan Ritter, Stephen Soderland, and Oren Etzioni.2009.
What is this, anyway: Automatic hypernymdiscovery.
In Proceedings of AAAI Spring Sympo-sium on Learning by Reading and Learning to Read.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In Proceedings ofEMNLP.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenousevidence.
In Proceedings of COLING-ACL.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowl-edge.
In Proceedings of WWW.Partha Pratim Talukdar, Joseph Reisinger, MariusPas?ca, Deepak Ravichandran, Rahul Bhagat, andFernando Pereira.
2008.
Weakly-supervised acqui-sition of labeled class instances using graph randomwalks.
In Proceedings of EMNLP.Robert E. Tarjan.
1977.
Finding optimum branchings.Networks, 7:25?35.William T. Tutte.
1984.
Graph theory.
Addison-Wesley.Dominic Widdows.
2003.
Unsupervised methodsfor developing taxonomies by combining syntacticand statistical information.
In Proceedings of HLT-NAACL.Ichiro Yamada, Kentaro Torisawa, Jun?ichi Kazama,Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-cis Bond, and Asuka Sumida.
2009.
Hypernym dis-covery based on distributional similarity and hierar-chical structures.
In Proceedings of EMNLP.Hui Yang and Jamie Callan.
2009.
A metric-basedframework for automatic taxonomy induction.
InProceedings of ACL-IJCNLP.1051
