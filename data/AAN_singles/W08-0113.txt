Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 84?87,Columbus, June 2008. c?2008 Association for Computational LinguisticsA Simple Method for Resolution of Definite Referencein a Shared Visual ContextAlexander SiebertBerlin-BrandenburgischeAkademie der Wissenschaftensiebert@bbaw.deDavid SchlangenDepartment of LinguisticsUniversity of Potsdam, Germanydas@ling.uni-potsdam.deAbstractWe present a method for resolving definite ex-ophoric reference to visually shared objectsthat is based on a) an automatically learned,simple mapping of words to visual features(?visual word semantics?
), b) an automat-ically learned, semantically-motivated utter-ance segmentation (?visual grammar?
), and c)a procedure that, given an utterance, uses b)to combine a) to yield a resolution.
We evalu-ated the method both on a pre-recorded corpusand in an online setting, where it performedwith 81% (chance: 14%) and 66% accuracy,respectively.
This is comparable to results re-ported in related work on simpler settings.1 The TaskThe method described in this paper is a module ofa dialogue system that acts as a collaborator of ahuman player in the task of manipulating visuallypresent puzzle objects.
An example scene is shownin Figure 1 (the indices a and b are added here forillustrative purposes).
Given utterances like those in(1), the task of the module is to identify the likelyreferents (here, a and b, respectively).1(1) a.Take the piece in the middle on the left side.b.Take the piece in the middle.More formally, the task can be characterised as fol-lows: possibly starting with an a priori assump-tion about likely referents (e.g., from knowledge of1Our system is implemented for German input; for ease ofdescription we use examples from our corpus translated intoEnglish here.Figure 1: Example Scenediscourse salience), the module uses the evidencepresent in the utterance (words, syntax) and in thevisual scene (visual features) to derive at a new as-sumption about likely referents.
If we call such anassumption a confidence function c that assigns toeach object in the domain O, a number between 0and 1; i.e., c : O ?
R, then reference resolution is afunction r that takes a triple of an initial confidencefunction c, an utterance u, and a visual scene repre-sentation v to yield an updated confidence functionc?.
Formally: r : C ?
U ?
V ?
C.In the following, we describe the resourcesneeded to set up the module, its subcomponents, andthe evaluation we performed.
We close by relatingthe proposed method to prior work and discussingfuture extensions.2 Resources2.1 CorpusAs our method is based on automatically learnedmodels, a corpus is required.
Our intended use caseis similar to the setting described in (Schlangen andFerna?ndez, 2007), but with the addition of a sharedvisual context.
We collected 300 scene descriptions84(of scenes containing between 1 and 12 distinct,monochrome shapes, randomly placed and rotatedon a rectangular area) using the two-part methodol-ogy of (Siebert et al, 2007) that yields recordingsand quality assessments (here: attempts to followother subjects?
instructions).
We also later recordedan additional 300 scene descriptions by a singlespeaker, to further increase our data base.After transcription of the recordings (239 min-utes of audio material), we discarded roughly 6%of the instructions because they could not be fol-lowed by the evaluators, and a further 4% becausethe complexity of the descriptions was outside thescope of what we wanted to model.
The remaininginstructions were then automatically cleaned fromdysfluencies, morphologically lemmatised and POStagged, and annotated as described below.2.2 Computer VisionThe other required resource is a visual perceptionalgorithm.
We use it to compute a feature repre-sentation of every visual scene as presented in thedata collection:2 First, each object is represented bya number of object features such as size / length /height of the bounding box, center of gravity, num-ber of edges.
Second, topological features note foreach object the distance to certain points on theboard (edges, center, etc.)
and to other objects.
(For details on the computation of such features seefor example (Regier and Carlson, 2001).)
Lastly,we also compute groupings of objects by clusteringalong columns and rows or both (see Figure 2 for anillustration).
For each group, we compute two setsof topological features, one for the objects withinthe group (e.g., distance to the center of the group),and one for the configuration of groups (distance ofgroup to other objects).
This set of features was se-lected to be representative of typical basic visual fea-tures.3 Components3.1 Visual GrammarThe ?visual grammar?
segments utterances accord-ing to functional aspects on two levels.
The first2At the moment, the input to the algorithm is a symbolicrepresentation of the scene (which object is where); the featuresare designed to also be derivable from digital images instead,using standard computer vision techniques (Shapiro and Stock-man, 2001); this is future work, however.Figure 2: Scene with Horizontal Group Detectiondescribes the macro-structure of a spatial expres-sion, i.e., the division into target (the denoted ob-ject; T) and optional landmarks (other objects; LM)and their relation to the target (R; see example in Ta-ble 2).
The second level annotates the spatial-lexicalfunction of each word, e.g., whether the word de-notes a piece or a configuration of pieces (Table 1).A fully ?parsed?
example is shown in Table 2.Name Description Examplesl lexical reference T,piece,crossd r topological direction top left Cornerd s topological distance outer leftd n numeric second columnp g group (perceptually active) from the left columng s synthetic group the three pieces on the leftf landmark field N in the Middler prepositional relation in the middlegrad grading function exactly rightTable 1: Visual Lexical Functions of Wordsthe cross from the second column from left at the topl r d n p g r d r d r(a) - Annotation of spatial lexical functionsT R LM LM LM LM T(b) - Segmentation of visual spatial partsTable 2: Example Annotation / ?Parse?Given the requirement for robustness, we decidedagainst a hand-written grammar for deriving suchannotations; the moderate size of our corpus onthe other hand made for example Markov model-based approaches difficult to apply.
We hence chosetransformation-based learning to create this (shal-low) segmentation grammar, converting the seg-mentation task into a tagging task (as is done in85(Ramshaw and Marcus, 1995), inter alia).
In our ap-proach, each token that is to be tagged is itself repre-sented in three different forms or layers: lemmatisedword, as POS-tag, and by its spatial-functional tag(as in Table 1; added by simple look-up).
All theselayers can be accessed in the learned rules.
Apartfrom this, the module is a straightforward imple-mentation of (Ramshaw and Marcus, 1995), whichin turn adapts (Brill, 1993) for syntactic chunking.3.2 Visual Word SemanticsTo learn the visual semantics of words we imple-mented a simple technique for grounding words inperceptions.
Roughly, the idea is to extract fromall instances in which a word was used in the train-ing corpus and all associated scenes a prototypicalvisual meaning representation by identifying thosefeatures whose values best predict the appropriate-ness of the word given a scene.
(This is similar inspirit to the approach used in (Roy, 2002).
)As material for learning, we only used the sim-ple expressions (target only, no landmark) in thecorpus, to ensure that all words used were in someway ?about?
the target.
The algorithm iterates overall pairs of utterance and scene and saves for eachlemma all visual information.
This creates for eachlemma a matrix of feature values with as many rowsas there were occurrences of the lemma.
The valuesin each column (that is, for each feature) are thennormalised to the interval [-1, 1] and the standarddeviation is recorded.The next tasks then are a) to compute one sin-gle representative value for each feature, but onlyb) for those features that carry semantic weight forthe given word (i.e., to compute a dimensionality re-duction).
E.g., for the lemma ?left?, we want the fea-ture x distance to center to be part of the semanticmodel, but not y distance to center.One option for a) is to simply take the averagevalue as representative for a feature (for a givenword).
While this works for some words, it causesproblems for others which imply a maximisationand not a prototypisation.
E.g., the lemma left isbest represented by maximal values of the featurex distance to center, not by the average of all val-ues for all occurrences of left (this will yield some-thing like leftish).
Perhaps surprisingly, representa-tion through the majority value, i.e., choosing themost frequent value as representative for a feature(for a given word), performed better, and is hencethe method we chose.For b), dimensionality reduction, we again chosea very simple approach (much simpler than for ex-ample (Roy, 2002)): features are filtered out as ir-relevant for a given lemma features if their varianceis above a certain threshold.
To give an example,for the lemma left the distribution of values of thefeature x distance to center varies with a ?
of 0.05,that of y distance to center with a ?
of 0.41.
Weempirically determined the setting of the thresholdsuch that it excluded the latter.33.3 CombinationFigure 3: Steps of the Algorithm for Example UtteranceThe combination algorithm works through thesegmented utterance and combines visual word se-mantics to yield a reference hypothesis.
Figure 3illustrates this process for the example from Table 2.On detecting a landmark segment (Step 1), the res-olution algorithm ?activates?
the appropriate group;which one this is is determined by the p g item inthe landmark segment.
(Here: column).
The groupis then treated as a single object, and (Step 2) thesemantics of topological terms (d r or d s) in thelandmark segment is applied to it (more on this ina second).
For our example, this yields a rankingof all columns with respect to their ?left-ness?.
Theordinal ?second?
finally simply picks out the secondelement on this list?the second group w.r.t.
the prop-erty of leftness (Step 3).
The expressions in the tar-get segment are now only applied to the membersof the group that was selected in this way; i.e., thesemantic models of ?top?
and ?cross?
are now onlyapplied to the objects in that column (Steps 4 to 6).3With more data and hence the possibility to set aside a de-velopment set, one could and should of course set such a thresh-old automatically.86Semantic word models are applied through a sim-ple calculation of distance between values (of se-mantic model and actual scene): the closer, the bet-ter the match of word to scene.
(Modulo selectivityof a feature; for a feature that occurred for all lem-mata with a high specificity (small ?
), good matchesare expected to be closer to the prototype value thanfor features with a high variability.
)This method encodes parts of the utterance se-mantics procedurally, namely the way how certainphrases (here grouped under the label landmark) se-mantically modify other phrases (here grouped un-der the label target).
This encoding makes the al-gorithm perhaps harder to understand than seman-tic composition rules tied to syntactic rules, but italso affords a level of abstraction over specific syn-tactic rules: our very general concepts of landmarkand target cover various ways of modification (e.g.through PPs or relative clauses), adding to the ro-bustness of the approach.4 EvaluationWith an f-score of 0.985 (10-fold cross validation),the transformation-based learning of the segmen-tation performs quite well, roughly at the levelof state-of-the-art POS-taggers (albeit with a muchsmaller tag inventory).
Also evaluated via cross-validation on the corpus, the resolution componentas a whole performs with an accuracy of 80.67%(using frequency-based word-semantic features; itdrops to 66.95% for average-based).
There were onaverage 7 objects in each scene in the corpus; i.e.the baseline of getting the reference right by chanceis 14%.
Our system significantly improves over thisbaseline.We also evaluated the system in a more realis-tic application situation.
We asked subjects to referto certain pieces in presented scenes (via typed ut-terances); here, the system reached a success-rateof 66% (7 subjects, 100 scene / utterance pairs).While this is considerably lower than the corpus-based evaluation, it is still on a par with relatedsystems using more complicated resolution methods(Roy, 2002; Gorniak and Roy, 2004).
We also thinkthese results represent the lower end of the perfor-mance range that can be expected in practical use,as in an interactive dialogue system users have timeto adapt to the capabilities of the system.5 ConclusionsWe have presented a method for resolving defi-nite, exophoric reference to objects that are visu-ally co-present to user and system.
The methodcombines automatically acquired models (a ?visualword semantics?, a simple, but effective mapping be-tween visual features and words; and a ?visual gram-mar?, a semantically motivated segmentation of ut-terances) and hard-coded knowledge (combinationprocedure).
To us, this combines the strengths oftwo approaches: statistical, where robustness andwide coverage is required, hard-coding, where few,but complex patterns are concerned.We are currently integrating the module into aworking dialogue system; in future work we will in-vestigate the use of digital images as input format.AcknowledgementsThis work was supported by DFG through an EmmyNoether Programm Grant to the second author.ReferencesEric Brill.
1993.
A Corpus-Based Approach to LanguageLearning.
Ph.D. thesis, University of Pennsylvania.Peter Gorniak and Deb Roy.
2004.
Grounded semanticcomposition for visual scenes.
In Journal of ArtificalIntelligence Research.Lance A. Ramshaw and Mitchell P. Marcus.
1995.
Textchunking using transformation-based learning.
In Pro-ceedings of the Third Workshop on Very Large Cor-pora, pages 82?94.Terry Regier and Laura A. Carlson.
2001.
Groundingspatial language in perception: An empirical and com-putational investigation.
In Journal of ExperimentalPsychology, volume 130, pages 273?298.Deb Roy.
2002.
Learning words and syntax for a visualdescription task.
Computer Speech and Language,16(3).David Schlangen and Raquel Ferna?ndez.
2007.
Beyondrepair: Testing the limits of the conversational repairsystem.
In Proceedings of SIGdial 2007, pages 51?54, Antwerp, Belgium, September.Linda G. Shapiro and George C. Stockman.
2001.
Com-puter Vision.
Prentice Hall, New Jersey, USA.Alexander Siebert, David Schlangen, and RaquelFerna?ndez.
2007.
An implemented method for dis-tributed collection and assessment of speech data.
InProceedings of SIGdial 2007, Antwerp, Belgium.87
