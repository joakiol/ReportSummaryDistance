Learning a syntagmatic and paradigmatic structure from languagedata with a bi-multigram modelSab ine  De l igne  and Yosh inor i  Sag isakaATR- ITL ,  dept l ,  2-2 HikaridaiSeika cho, Soraku gun, Kyoto  fu 619-0224, Japan.Abst rac tIn this paper, we present a stochastic language mod-eling tool which aims at retrieving variable-lengthphrases (multigrams), assuming bigram dependen-cies between them.
The phrase retrieval can be in-termixed with a phrase clustering procedure, so thatthe language data are iteratively structured at botha paradigmatic and a syntagmatic level in a fully in-tegrated way.
Perplexity results on ATR travel ar-rangement data with a bi-multigram odel (assum-ing bigram correlations between the phrases) comevery close to the trigram scores with a reduced num-ber of entries in the language model.
Also the abilityof the class version of the model to merge semanti-cally related phrases into a common class is illus-trated.1 In t roduct ionThere is currently an increasing interest in statisti-cal language models, which in one way or anotheraim at exploiting word-dependencies spanning overa variable number of words.
Though all these mod-els commonly relax the assumption of fixed-lengthdependency of the conventional ngram model, theycover a wide variety of modeling assumptions and ofparameter estimation frameworks.
In this paper, wefocus on a phrase-based approach, as opposed to agram-based approach: sentences are structured intophrases and probabilities are assigned to phrases in-stead of words.
Regardless of whether they are gramor phrase-based, models can be either determinis-tic or stochastic.
In the phrase-based framework,non determinism is introduced via an ambiguity onthe parse of the sentence into phrases.
In practice,it means that even if phrase abe is registered as aphrase, the possibility of parsing the string as, forinstance, lab\] \[c\] still remains.
By contrast, in a de-terministic approach, all co-occurences of a, b and cwould be systematically interpreted as an occurenceof phrase \[abel.Various criteria have been proposed to derivephrases in a purely statistical way 1; data likeli-I i.e.
without us ing grammar  ules like in Stochast ic  Con-text Free Grammars .hood, leaving-one-out likelihood (PLies et al, 1996),mutual information (Suhm and Waibel, 1994), andentropy (Masataki and Sagisaka, 1996).
The use ofthe likelihood criterion in a stochastic framework al-lows EM principled optimization procedures, but itis prone to overlearning.
The other criteria tendto reduce the risk of overlearning, but their opti-mization relies on heuristic procedures (e.g.
wordgrouping via a greedy algorithm (Matsunaga ndSagayama, 1997)) for which convergence and opti-mality are not theoretically guaranteed.
The workreported in this paper is based on the multigrammodel, which is a stochastic phrase-based model,the parameters of which are estimated according toa likelihood criterion using an EM procedure.
Themultigram approach was introduced in (Bimbot etal., 1995), and in (Deligne and Bimbot, 1995) it wasused to derive variable-length phrases under the as-sumption of independence of the phrases.
Variousways of theoretically releasing this assumption weregiven in (Deligne et al, 1996).
More recently, ex-periments with 2-word multigrams embedded in adeterministic variable ngram scheme were reportedin (Siu, 1998).In section 2 of this paper, we further formulatea model with bigram (more generally ~-gram) de-pendencies between the phrases, by including aparadigmatic aspect which enables the clusteringof variable-length phrases.
It results in a stochas-tic class-phrase model, which can be interpolatedwith the stochastic phrase model, in a similar wayto deterministic approaches.
In section 3 and 4,the phrase and class-phrase models are evaluated interms of perplexity values and model size.2 Theoretical formulation of themul t ig rams2.1 Variable-length phrase distributionIn the multigram framework, the assumption ismade that sentences result from the concatenationof variable-length phrases, called multigrams.
Thelikelihood of a sentence is computed by summingthe likelihood values of all possible segmentations ofthe sentence into phrases.
The likelihood computa-300tion for any particular segmentation i to phrases de-pends on the model assumed to describe the depen-dencies between the phrases.
We call b i -mult igrammodel the model where bigram dependencies areassumed between the phrases.
For instance, by lim-iting to 3 words the maximal ength of a phrase, thebi-multigram likelihood of the string "a b c d" is:p(\[,,\] I #) p(\[b\] I \[hi) p(\[c\] \] \[b\]) p(\[d\] \] \[c\])p(\[,,\] I #)p(\[b\] I \[a\])p(\[cd\]l\[b\])p(\[,,\] I #) p(\[b?\] I \[,,\]) p(\[d\] I \[bc\])p(\[a\] I #)p(\[bcd\]l\[a\])p(\[ab\] l #) p(\[c\] l \[ab\]) p(\[d\] l \[c\])p(\[ab\] I #) p(\[cd\] I \[,,b\])p(\[:bc\] I #) p(\[d\] I \[:bc\])To )resent the general formalism of the model inthis section, we assume ~-gram correlations betweenthe phrases, and we note n the maximal ength of aphrase (in the above example, ~=2 and n=3).
LetW denote a string of words, and {S} the set of pos-sible segmentations on W. The likelihood of W is:z (w)= ~ z(w,s)se{s}(1)and the likelihood of a segmentation S of W is:c (w,s) = I-I P(S(,) I s( ,_~-+~).. .s(,_~)) (2)with s(~) denoting the phrase of rank (r) in the seg-mentation S. The model is thus fully defined bythe set of ~-gram probabilities on the set {8i} i of allthe phrases which can be formed by combining 1, 2,...up to n words of the vocabulary.
Maximum like-lihood (ML) estimates of these probabilities can beobtained by formulating the estimation problem asa ML estimation from incomplete data (Dempster etal., 1977), where the unknown data is the underly-ing segmentation S. Let Q(k, k+ 1) be the followingauxiliary function computed with the likelihoods ofiterations k and k + 1 :Q(k ,k+ 1) = ~ ?.(k)(SIW)log?
(k+')(W, S)SE{S}(3)It has been shown in (Dempster et al, 1977)that if Q(k,k + 1) > Q(k,k),  then ?
(k+l)(W) >?(k)(W).
Therefore the reestimation equation ofp(sir I si, .
.
.sir_,),  at iteration (k + 1), can bederived by maximizing Q(k, k + 1) over the set ofparameters of iteration (k + 1), under the set of con-straints ~"~'.a" p(sir \ [s i , .
.
.s i r_ , )  = 1, hence:P(k+l)(siv I Si, .. "Sir_,) =ESE{S} C(8ia .. .81-~_, 8i-~, S) x f_.
(k)(S I W) (4) ~sels} c(si, sir_,, S) x ?
(k)(S I W)where c(si, .
.
.
si-~, S) is the number ofoccurences ofthe combination of phrases l, .
.
.
siw in the segmen-tation S. Reestimation equation (4) can be imple-mented by means of a forward-backward algorithm,such as the one described for bi-multigrams (~ = 2)in the appendix of this paper.
In a decision-orientedscheme, the reestimation equation reduces to:c(si, .
.
.
si.~_, s~, S "(k))p(k+l)(si-~ I si, .
.
.s i r_ , )  = c(si, .
.
.s i r_ , ,  S "(k))(5)where S *(k), the segmentation maximizing?
:(k)(S \] W), is retrieved with a Viterbi algo-rithm.Since each iteration improves the model in the senseof increasing the likelihood /:(k)(W), it eventuallyconverges to a critical point (possibly a localmaximum).2.2 Variable-length phrase clusteringRecently, class-phrase based models have gainedsome attention (Ries et al, 1996), but usuallyit assumes a previous clustering of the words.Typically, each word is first assigned a word-classlabel "< Ck >", then variable-length phrases\[Ck,Ck2...Ck,\] of word-class labels are retrieved,each of which leads to define a phrase-class labelwhich can be denoted as "< \[Ck,Ck2...Ch\] >".
Butin this approach only phrases of the same lengthcan be assigned the same phrase-class label.
Forinstance, the phrases "thank you for" and "thankyou very much for" cannot be assigned the sameclass label.
We propose to address this limitationby directly clustering phrases instead of words.For this purpose, we assume bigram correlationsbetween the phrases (~ = 2), and we modify thelearning procedure of section 2.1, so that eachiteration consists of 2 steps:?
Step !
Phrase clustering:{ p(k)(si I s~) }, {p(~)(cq(.
)IC~(.,)), p(k)(s~ I c,(.,)) }?
Step 2 Bi-multigram reestimation:{ p(~)(cq(.,) I cq(.,)), p(~)(s~ I cq(.j)) }, {p(~+')(s~ I si) }Step 1 takes a phrase distribution as an input,assigns each phrase sj to a class Cq(,.
), and out-puts the corresponding class dmtnbutmn.
In ourexperiments, the class assignment is performed bymaximizing the mutual information between adja-cent phrases, following the line described in (Brown301et al, 1992), with only the modification that can-didates to clustering are phrases instead of words.The clustering process is initialized by assigning eachphrase to its own class.
The loss in average mutualinformation when merging 2 classes is computed forevery pair of classes, and the 2 classes for which theloss is minimal are merged.
After each merge, theloss values are updated and the process is repeatedtill the required number of classes is obtained.Step _2 consists in reestimating a phrase distributionusing the bi-multigram reestimation equation (4)or (5), with the only difference that the likelihoodof a parse, instead of being computed as in Eq.
(2),is now computed with the class estimates, i.e.
as:?
(W,S) = 1"I p(Cq(,.))
l Cq(s._.))
p(s(.)
l Cq(,.
)))T(6)This is equivalent to reestimating p(k+l)(sj \[ Si)from p(k)(Cq(, D \[ Cq(,,)) x p(k)(sj \[ Cq(,D), insteadofp(k)(sj \[ si) as was the case in section 2.1.Overall, step 1 ensures that the class assignmentbased on the mutual information criterion is optimalwith respect o the current estimates of the phrasedistribution and step _2 ensures that the phrase dis-tribution optimizes the likelihood computed accord-ing to (6) with the current estimates of the ciassdistribution.
The training data are thus iterativelystructured in a fully integrated way, at both aparadigmatic level (step 1) and a syntagmatic level(step 2_).2.3 Interpolat ion of stochastic lass-phraseand phrase modelsWith a class model, the probabilities of 2 phrasesbelonging to the same class are distinguished onlyaccording to their unigram probability.
As it is un-likely that this loss of precision be compensated bythe improved robustness of the estimates of the classdistribution, class based models can be expected todeteriorate the likelihood of not only train but alsotest data, with respect o non-class based models.However, the performance of non-class models canbe enhanced by interpolating their estimates withthe class estimates.
We first recall the way linearinterpolation is performed with conventional wordngram models, and then we extend it to the case ofour stochastic phrase-based approach.
Usually, lin-ear interpolation weights are computed so as to max-imize the likelihood of cross evaluation data (Jelinekand Mercer, 1980).
Denoting by A and (1 - A) theinterpolation weights, and by p+ the interpolated es-timate, it comes for a word bigram model:I i) =a p(w i I w,) + ( l -a) p(Cq(wj) I cq(w,)) Iwith A having been iteratively estimated on a crossevaluation corpus l,V?~o,, as:1 A (k) p(wj \[ wi)A(k+l) - T?
',.o,, Z c(wiwj) p(~)(wj I wi) (8) i jwhere Tcro,, is the number of words in Weros,, andc(wiwj) the number of co-occurences of the wordswi and wj in Wero,~.In the case of a stochastic phrase based model -where the segmentation into phrases is not known apriori - the above computation of the interpolationweights till applies, however, it has to be embeddedin dynamic programming to solve the ambiguity onthe segmentation:A(k+l) _ 1 S-" e(sis~\] S *(k)) A(k) p(sj I si)c(S'(~)) ~ p(~)(si I si) s,2(9)where S "(k) the most likely segmentation f Wero,sgiven the current estimates p(~)(sj I si) can be re-trieved with a Viterbi algorithm, and where c(S*(k))is the number of sequences in the segmentationS "(k).
A more accurate, but computationally moreinvolved solution would be to compute A(~+1) as the~(k) p(sj I s~) expectation of 1over the set of segmentations {S} on Wcross, us-ing for this purpose a forward-backward algorithm.However in the experiments reported in section 4,we use Eq (9) only.3 Exper iments  with phrase basedmodels3.1 Protocol  and databaseEvaluation protocol A motivation to learn bi-gram dependencies between variable length phrasesis to improve the predictive capability of conven-tional word bigram models, while keeping the num-ber of parameters in the model lower than in theword trigram case.
The predictive capability is usu-ally evaluated with the perplexity measure:PP  = e-rXtogC(w)where T is the number of words in W. The lowerPP  is, the more accurate the prediction of the modelis.
In the case of a stochastic model, there are ac-tually 2 perplexity values PP  and PP* computedrespectively from ~"\]~s ?
(W,S) and ?
(W,S*) .
Thedifference PP* - PP  is always positive or zero, andmeasures the average degree of ambiguity on a parseS of W, or equivalently the loss in terms of predic-tion accuracy, when the sentence likelihood is ap-proximated with the likelihood of the best parse, asis done in a speech recognizer.302In section 3.2, we first evaluate the loss (PP"  - PP )using the forward-backward estimation procedure,and then we study the influence of the estimationprocedure itself, i.e.
Eq.
(4) or (5), in terms of per-plexity and model size (number of distinct 2-upletsof phrases in the model).
Finally, we compare theseresults with the ones obtained with conventional n-gram models (the model size is thus the number ofdistinct n-uplets of words observed), using for thispurpose the CMU-Cambridge toolkit (Clarkson andRosenfeld, 1997).Training protocol Experiments are reported forphrases having at most n = 1, 2, 3 or 4 words (forn =1, bi-multigrams correspond to conventional bi-grams).
The bi-multigram probabilities are initial-ized using the relative frequencies ofall the 2-upletsof phrases observed in the training corpus, and theyare reestimated with 6 iterations.
The dictionaries ofphrases are pruned by discarding all phrases occur-ing less than 20 times at initialization, and less than10 times after each iteration s, except for the 1-wordphrases which are kept with a number of occurrencesset to 1.
Besides, bi-multigram and n-gram prob-abilities are smoothed with the backoff smoothingtechnique (Katz, 1987) using Witten-Bell discount-ing (Witten and Bell, 1991) 3.Database Experiments are run on ATR travel ar-rangement data (see Tab.
1).
This database con-sists of semi-spontaneous dialogues between a hotelclerk and a customer asking for travel/accomodationinformations.
All hesitation words and false startswere mapped to a single marker "*uh*".Train testNb sentences 13 650 2 430Nb tokens 167 000 29 000 (1% OOV)Vocabulary 3 525 + 280 OOVTable 1: ATR Travel Arrangement Data3.2 ResultsAmbiguity on a parse (Table 2) The difference(PP"  - PP )  usually remains within about 1 point ofperplexity, meaning that the average ambiguity on aparse is low, so that relying on the single best parseshould not decrease the accuracy of the predictionvery much.Influence of the est imation procedure (Ta-ble 3) As far as perplexity values are concerned,2Using different pruning thresholds values did not dra-matically affect the results on our data, provided that thethreshold at initialization is in the range 20-40, and that thethreshold of the iterations is less than 10.3The Witten-Bell discounting was chosen, because ityielded the best perplexity scores with conventional n-gramson our test data.mmmmmmmmmTable 2: Ambiguity on a parse.the estimation scheme seems to have very little in-fluence, with only a slight advantage in using theforward-backward training.
On the other hand, thesize of the model at the end of the training is about30% less with the forward-backward training: ap-proximately 40 000 versus 60 000, for a same testperplexity value.
The bi-multigram results tend toindicate that the pruning heuristic used to discardphrases does not allow us to fully avoid overtrain-ing, since perplexities with n =3, 4 (i.e.
dependen-cies possibly spanning over 6 or 8 words) are higherthan with n =2 (dependencies limited to 4 words).Test perplexity values PP"n 1 2 3 4F.-B.
56.0 45.1 45.4 46.3Viterbi 56.0 45.7 45.9 46.2Model sizen 1 2 3 4F.-B.
32505 42347 43672 43186Viterbi 32505 65141 67258 67295Table 3: Influence of the estimation procedure:forward-backward (F.-B.)
or Viterbi.Comparison with n-grams (Table 4) The low-est bi-multigram perplexity (43.9) is still higher thanthe trigram score, but it is much closer to the tri-gram value (40.4) than to the bigram one (56.0) 4The number of entries in the bi-multigram odel ismuch less than in the trigram model (45000 versus75000), which illustrates the ability of the model toselect most relevant phrases.I \[';~--\] I .~  ,\] ~ l, l , l i .~.~ I,  \ [~- I  w.i~in (and n) 1 2 3 4n-gram 314.2 56.0 40.4 39.8bimultigrams 56.0 43.9 44.2 45.0Model sizen (and n) 1 2 3 4n-gram 3526 32505 75511 112148bimultigrams 32505 42347 43672 43186Table 4: Comparison with n-grams: Test perplexityvalues and model size.4Besides, the trig-ram score depends on the discountedscheme: with a linear discounting, the trlg'ram perplexity onour test data was 48.1.3034 Experiments with class-phrasebased models4.1 Protocol  and databaseEvaluation protocol  In section 4.2, we compareclass versions and interpolated versions of the bi-gram, trigram and bi-multigram models, in termsof perplexity values and of model size.
For bigrams(resp.
trigrams) of classes, the size of the model isthe number of distinct 2-uplets (resp.
3-uplets) ofword-classes observed, plus the size of the vocab-ulary.
For the class version of the bi-multigrams,the size of the model is the number of distinct 2-uplets of phrase-classes, plus the number of distinctphrases maintained.
In section 4.3, we show samplesfrom classes of up to 5-word phrases, to illustratethe potential benefit of clustering relatively long andvariable-length p rases for issues related to languageunderstanding.Training protocol  All non-class models are thesame as in section 3.
The class-phrase models aretrained with 5 iterations of the algorithm describedin section 2.2: each iteration consists in clusteringthe phrases into 300 phrase-classes (step 1), and inreestimating the phrase distribution (step 2) withEq.
(4).
The bigrams and trigrams of classes are es-timated based on 300 word-classes derived with thesame clustering algorithm as the one used to clusterthe phrases.
The estimates of all the class ditribu-tions are smoothed with the backoff technique likein section 3.
Linear interpolation weights betweenthe class and non-class models are estimated basedon Eq.
(8) in the case of the bigram or trigram mod-els, and on Eq.
(9) in the case of the bi-multigrammodel.Database The training and test data used to trainand evaluate the models are the same as the onesdescribed in Table 1.
We use an additional set of7350 sentences and 55000 word tokens to estimatethe interpolation weights of the interpolated models.4.2 ResultsThe perplexity scores obtained with the non-class,class and interpolated versions of a bi-multigrammodel (limiting to 2 words the size of a phrase),and of the bigram and trigram models are in Ta-ble 5.
Linear interpolation with the class based mod-els allows us to improve each model's performanceby about 2 points of perplexity: the Viterbi perplex-ity score of the interpolated bi-multigrams (43.5) re-mains intermediate between the bigram (54.7) andtrigram (38.6) scores.
However in the trigram case,the enhancement of the performance is obtained atthe expense of a great increase of the number ofentries in the interpolated model (139256 entries).In the bi-multigram case, the augmentation of themodel size is much less (63972 entries).
As a re-sult, the interpolated bi-multigram model still hasfewer entries than the word based trigram model(75511 entries), while its Viterbi perplexity scorecomes even closer to the word trigram score (43.5versus 40.4).
Further experiments studying the in-fluence of the threshold values and of the numberof classes till need to be performed to optimize theperformances for all models.Test perplexity values PP"non-classbigrams 56.04bimultigrams 45.1trigrams 40.4class66.357.449.3Model sizenon-classbigrams 32505bimultigrams 4234775511 trigramsclass204712162563745interpolated54.743.538.6interpolated5297663972139256Table 5: Comparison of class-phrase bi-multigramsand of class-word bigrams and trigrams: Test per-plexity values and model size.4.3 ExamplesClustering variable-length phrases may provide anatural way of dealing with some of the language dis-fluencies which characterize spontaneous tterances,like the insertion of hesitation words for instance.
Toillustrate this point, examples of phrases which weremerged into a common cluster during the trainingof a model allowing phrases of up to n = 5 wordsare listed in Table 6 (the phrases containing the hes-itation marker "*uh*" are in the upper part of thetable).
It is often the case that phrases differingmainly because of a speaker hesitation are mergedtogether.Table 6 also illustrates another motivation for phraseretrieval and clustering, apart from word prediction,which is to address issues related to topic identifica-tion, dialogue modeling and language understand-ing (Kawahara et al, 1997).
Indeed, though theclustered phrases in our experiments were derivedfully blindly, i.e.
with no semantic/pragmatic in-formation, intra-class phrases often display a strongsemantic orrelation.
To make this approach effec-tively usable for speech understanding, constraintsderived from semantic or pragmatic knowledge (likespeech act tag of the utterance for instance) couldbe placed on the phrase clustering process.5 ConclusionAn algorithm to derive variable-length phrases as-suming bigram dependencies between the phraseshas been proposed for a language modeling task.
Ithas been shown how a paradigmatic element could304{ yes_that_will ; *uh*_that_would }{ yes_that_will_be ; *uh*_yes_that's }{ *uh*_by_the ; and_by_the }{ yes_*uh*i ; i_see_i ){ okay_i_understand ; *uh*_yes_please ){ could_you_recommend ; *uh*_is_there }{ *uh*_could_you_tell ; and_could_you.tell }{ so_that_will ; yes_that_will ; yes_that_would ;uh*.that_would ){ if_possible_i'd_like ; we_would_like ; *uh*_i_want }{ that_sounds_good ; *uh*-i_understand ){ *uh*_i_really ; *uh*_i_don't }{ *uh*_i'm.staying ; andA'm.staying }{ all_right_we ; *uh*_yes.i ){ good_morning ; good_afternoon ; hello }{ sorry_to_keep_you_waiting ; hello_front_desk ;thank_you_very_much ; t ank_you_for_calling ;you're_very.welcome ; yes_that's_correct ;yes_that's_right }{ non.smoking ; western_style ; first_class ;japanese_style }{ familiar_with ;in_charge_of }{ could_you_tell_me ; do_you_know }{ how/ong ; how_much ; what_time ;uh*_what_time ; *uh*_how_much ;and_how_much ; and_what_time }{ explain ; tell_us ; tell_me ; tell_me_about ;tell_me_what ; tell_me_how ; tell_me_how_much ;tell_me_the ; give_me ; give_me_the ;give_me_your ; please_tell_me }{ are_there ; are_there_any ; if_there_are ;iLthereis ;if_you_have ; if_there's ;do_you_have ; do_you_have_a ; do_you_have_any ;we_have_two ; is_there ; is_there_any ;is_there_a ; is_there_anything ; *uh*_is_there ;uh*_do_you_have }{ tomorrow_morning ; nine_o'clock ; eight_o'clock ;seven_o'clock ; three_p.m.
; august_tenth ;in_the_morning ; six_p.m.
; six_o'clock }{ we'd_like ; i'dAike ; i_would_like }{ that'll_be_fine ; that's_fine ; i_understand }{ kazuko_suzuki ; mary ; mary_phillips ;thomas_nelson ; suzuki ; amy_harris ;john ;john_phillips }{ fine ; no_problem ; anything_else }{ return_the.car ; pick_it_up }{ todaiji ; kofukuji ; brooklyn ; enryakuji ;hiroshima ; las_vegas ; saltAake_city ; chicago ;kinkakuji ; manhattan ; miami ; kyoto_station ;this_hotel ; our_hotel ; your_hotel ;the_airport ; the_hotel }Table 6: Example of phrases assigned to a commoncluster, with a model allowing up to 5-word phrases(clusters are delimited with curly brackets)be integrated within this framework, allowing to as-sign common labels to phrases having a differentlength.
Experiments on a task oriented corpus haveshown that structuring sentences into phrases resultsin large reductions in the bigram perplexity value,while still keeping the number of entries in the lan-guage model nmch lower than in a trigram model,especially when these models are interpolated withclass based models.
These results might be furtherimproved by finding a more efficient pruning strat-egy, allowing the learning of even longer dependen-cies without over-training, and by further experi-menting with the class version of the phrase-basedmodel.Additionally, the semantic relevance of the clustersof phrases motivates the use of this approach inthe areas of dialogue modeling and language under-standing.
In that case, semantic/pragmatic infor-mations could be used to constrain the clustering ofthe phrases.Append ix :  Forward-backwarda lgor i thm for  the  es t imat ion  o f  theb i -mul t ig ram parametersEquation (4) can be implemented at a complexity ofO(n~T), with n the maximal ength of a sequenceand T the number of words in the corpus, using aforward-backward algorithm.
Basically, it consistsin re-arranging the order of the summations of thenumerator and denominator of Eq.
(4): the likeli-hood values of all the segmentations where sequencesj occurs after sequence si, with sequence si end-ing at the word at rank (t), are summed up first;and then the summation is completed by summingover t. The cumulated likelihood of all the segmen-tations where sj follows si, and si ends at (t), can bedirectly computed as a product of a forward and of abackward variable.
The forward variable representsthe likelihood of the first t words, where the last liwords are constrained to form a sequence:=The backward variable represents the conditionallikelihood of the last ( T - t )  words, knowing thatthey are preceded by the sequence \[w(t_zi+l)...w(0\]:=Assuming that the likelihood of a parse is computedaccording to Eq.
(2), then the reestimation equation(4) can be rewritten as shown in Tab.
7.The variables a and/3 can be calculated accordingto the following recursion equations (assuming astart and an end symbol at rank t = 0 and t = T+I) :305p(k+l)(s j \[Si) .- ~T=I  O~(t, It) p(k)(Sj ISi) ~(t "1- lj, lj) 6i(t -- li -}- 1) 6j(t + 1)E,  ~(t, li) l~(t, It) 6 i ( t - - l i+ l )li and lj refer respectively to the lengths of the sequences si and sj, and where the Kronecker function 5k(t)equals 1 if the word sequence starting at rank t is sk, and equals 0 if not.Table 7: Forward-backward reestimationfor 1 < t < T+ 1, and 1 < ii <_ n:na(t, It) E a(t - li, l) (') "-" p(\[Wit_l , , l ) \ ]  \[ \[W(~tTili~+l\])I=la(0, 1) = 1, a(0, 2) = ... = a(0, n) = 0.fo r0<t  <T,  and l< l j  < n:I Z(t + l, l)I=l~(T+ 1, 1) = 1, f l (T+ 1,2) = ... = /~(T+ 1,n) = 0.In the case where the likelihood of a parse iscomputed with the class assumption, i.e.
ac-cording to (6), the term p(k)(sj \[st) in thereestimation equation shown in Table 7 shouldbe replaced by its class equivalent, i.e.
byp(k)(Cq(, D ICq(,,)) p(k)(sj \[ Cq(,D).
In the recursionequation of ~, the term p(\[W~)_t,+l)\]l\[Wft_Tt'__~+l\])is replaced by the corresponding class bigram prob-ability multiplied by the class conditional prob-ability of the sequence \[W~_)t,+l)\].
A similarchange affects the recursion equation of ~, withP(tW~::~l\]ltW~:)b+,)\]) being replaced by the cor-responding class bigram probability multiplied bythe class conditional probability of the sequenceReferencesF.
Bimbot, R. Pieraccini, E. Levin, and B. Atal.1995.
Variable-length sequence modeling: Multi-grams.
IEEE Signal Processing Letters, 2(6),June.P.F.
Brown, V.J.
Della Pietra, P.V.
de Souza, J.C.Lai, and R.L.
Mercer.
1992.
Class-based n-grammodels of natural anguage.
Computational Lin-guistics, 18(4):467-479.P.
Clarkson and R. Rosenfeld.
1997.
Statistical lan-guage modeling using the cmu-cambridge toolkit.Proceedings of EUROSPEECH 9ZS.
Deligne and F. Bimbot.
1995.
Language modelingby variable length sequences: theoretical formula-tion and evaluation of multigrams.
Proceedings ofICASSP 95.S.
Deligne, F. Yvon, and F. Bimbot.
1996.
In-troducing statistical dependencies and structuralconstraints in variable-length sequence models.
InGrammatical Inference : Learning Syntax fromSentences, Lecture Notes in Artificial Intelligence1147, pages 156-167.
Springer.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum-likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistics So-ciety, 39(1):1-38.F.
Jelinek and R.L.
Mercer.
1980.
Interpolated esti-mation of markov source parameters from sparsedata.
Proceedings of the workshop on PatternRecognition in Practice, pages 381-397.S.
M. Katz.
1987.
Estimation of probabilities fromsparse data for the language model componentof a speech recognizer.
IEEE Trans.
on Acous-tic, Speech, and Signal Processing, 35(3):400-401,March.T.
Kawahara, S. Doshita, and C. H. Lee.1997.
Phrase language models for detection andverification-based speech understanding.
Proceed-ings of the 1997 IEEE workshop on AutomaticSpeech Recognition and Understanding, pages 49-56, December.H.
Masataki and Y. Sagisaka.
1996.
Variable-order n-gram generation by word-class plittingand consecutive word grouping.
Proceedings ofICASSP 96.S.
Matsunaga nd S. Sagayama.
1997.
Variable-length language modeling integrating lobal con-straints.
Proceedings of EUROSPEECH 97.K.
Ries, F. D. Buo, and A. Waibel.
1996.
Classphrase models for language modeling.
Proceedingsof ICSLP 96.M.
Siu.
1998.
Learning local lezicai structure inspontaneous speech language modeling.
Ph.D. the-sis, Boston University.B.
Suhm and A. Waibel.
1994.
Towards better lan-guage models for spontaneous speech.
Proceedingsof ICSLP 94.I.H.
Witten and T.C.
Bell.
1991.
The zero-frequencyproblem: estimating the probabilities of novelevents in adaptative text compression.
IEEETrans.
on Information Theory, 37(4):1085-1094,July.306
