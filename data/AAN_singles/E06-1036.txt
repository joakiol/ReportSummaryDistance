Recognizing Textual Parallelisms with edit distance and similarity degreeMarie Gue?gan and Nicolas HernandezLIMSI-CNRSUniversite?
de Paris-Sud, Franceguegan@aist.enst.fr | hernandez@limsi.frAbstractDetection of discourse structure is crucialin many text-based applications.
This pa-per presents an original framework for de-scribing textual parallelism which allowsus to generalize various discourse phe-nomena and to propose a unique methodto recognize them.
With this prospect, wediscuss several methods in order to iden-tify the most appropriate one for the prob-lem, and evaluate them based on a manu-ally annotated corpus.1 IntroductionDetection of discourse structure is crucial in manytext-based applications such as Information Re-trieval, Question-Answering, Text Browsing, etc.Thanks to a discourse structure one can preciselypoint out an information, provide it a local context,situate it globally, link it to others.The context of our research is to improve au-tomatic discourse analysis.
A key feature of themost popular discourse theories (RST (Mann andThompson, 1987), SDRT (Asher, 1993), etc.)
isthe distinction between two sorts of discourse re-lations or rhetorical functions: the subordinatingand the coordinating relations (some parts of atext play a subordinate role relative to other parts,while some others have equal importance).In this paper, we focus our attention on a dis-course feature we assume supporting coordinationrelations, namely the Textual Parallelism.
Basedon psycholinguistics studies (Dubey et al, 2005),our intuition is that similarities concerning the sur-face, the content and the structure of textual unitscan be a way for authors to explicit their intentionto consider these units with the same rhetorical im-portance.Parallelism can be encountered in many specificdiscourse structures such as continuity in infor-mation structure (Kruijff-Korbayova?
and Kruijff,1996), frame structures (Charolles, 1997), VP el-lipses (Hobbs and Kehler, 1997), headings (Sum-mers, 1998), enumerations (Luc et al, 1999), etc.These phenomena are usually treated mostly inde-pendently within individual systems with ad-hocresource developments.In this work, we argue that, depending on de-scription granularity we can proceed, computingsyntagmatic (succession axis of linguistic units)and paradigmatic (substitution axis) similaritiesbetween units can allow us to generically handlesuch discourse structural phenomena.
Section 2introduces the discourse parallelism phenomenon.Section 3 develops three methods we implementedto detect it: a similarity degree measure, a stringediting distance (Wagner and Fischer, 1974) and atree editing distance1 (Zhang and Shasha, 1989).Section 4 discusses and evaluates these methodsand their relevance.
The final section reviews re-lated work.2 Textual parallelismOur notion of parallelism is based on similaritiesbetween syntagmatic and paradigmatic represen-tations of (constituents of) textual units.
Thesesimilarities concern various dimensions from shal-low to deeper description: layout, typography,morphology, lexicon, syntax, and semantics.
Thisaccount is not limited to the semantic dimensionas defined by (Hobbs and Kehler, 1997) who con-sider text fragments as parallel if the same predi-cate can be inferred from them with coreferentialor similar pairs of arguments.1For all measures, elementary units considered are syn-tactic tags and word tokens.281We observe parallelism at various structural lev-els of text: among heading structures, VP ellipsesand others, enumerations of noun phrases in asentence, enumerations with or without markerssuch as frame introducers (e.g.
?In France, .
.
.
InItaly, .
.
.
?)
or typographical and layout markers.The underlying assumption is that parallelism be-tween some textual units accounts for a rhetoricalcoordination relation.
It means that these units canbe regarded as equally important.By describing textual units in a two-tier frame-work composed of a paradigmatic level and syn-tagmatic level, we argue that, depending on thedescription granularity we consider (potentially atthe character level for item numbering), we candetect a wide variety of parallelism phenomena.Among parallelism properties, we note that theparallelism of a given number of textual units isbased on the parallelism of their constituents.
Wealso note that certain semantic classes of con-stituents, such as item numbering, are more effec-tive in marking parallelism than others.2.1 An example of parallelismThe following example is extracted from our cor-pus (see section 4.1).
In this case, we have an enu-meration without explicit markers.For the purposes of chaining, each type of linkbetween WordNet synsets is assigned a directionof up, down, or horizontal.Upward links correspond to generalization: forexample, an upward link from apple to fruit indi-cates that fruit is more general than apple.Downward links correspond to specialization:for example, a link from fruit to apple would havea downward direction.Horizontal links are very specific specializations.The parallelism pattern of the first two items is de-scribed as follows:[JJ + suff =ward] links correspond to [NN + suff= alization] : for example , X link from Y to Z .This pattern indicates that several item con-stituents can be concerned by parallelism and thatsimilarities can be observed at the typographic,lexical and syntactic description levels.
Tokens(words or punctuation marks) having identicalshallow descriptions are written in italics.
TheX, Y and Z variables stand for matching any non-parallel text areas between contiguous parallel tex-tual units.
Some words are parallel based ontheir syntactic category (?JJ?
/ adjectives, ?NN?
/nouns) or suffix specifications (?suff?
attribute).The third item is similar to the first two items butwith a simpler pattern:JJ links U [NN + suff =alization] W .Parallelism is distinguished by these types of sim-ilarities between sentences.3 MethodsThree methods were used in this study.
Given apair of sentences, they all produce a score of sim-ilarity between these sentences.
We first presentthe preprocessing to be performed on the texts.3.1 Prior processing applied on the textsThe texts were automatically cut into sentences.The first two steps hereinafter have been appliedfor all the methods.
The last third was not appliedfor the tree editing distance (see 3.3).
Punctua-tion marks and syntactic labels were henceforwardconsidered as words.1.
Text homogenization: lemmatization togetherwith a semantic standardization.
Lexical chainsare built using WordNet relations, then words arereplaced by their most representative synonym:Horizontal links are specific specializations.horizontal connection be specific specialization .2.
Syntactic analysis by (Charniak, 1997)?s parser:( S1 ( S ( NP ( JJ Horizontal ) (NNS links ) ( VP( AUX are) ( NP ( ADJP ( JJ specific ) ( NNSspecializations ) ( SENT .)))))))3.
Syntactic structure flattening:S1 S NP JJ Horizontal NNS links VP AUX areNP ADJP JJ specific NNS specializations SENT.3.2 Wagner & Fischer?s string edit distanceThis method is based on Wagner & Fischer?sstring edit distance algorithm (Wagner and Fis-cher, 1974), applied to sentences viewed as stringsof words.
It computes a sentence edit distance, us-ing edit operations on these elementary entities.The idea is to use edit operations to transformsentence S1 into S2.
Similarly to (Wagner and Fis-cher, 1974), we considered three edit operations:1. replacing word x ?
S1 by y ?
S2: (x ?
y)2. deleting word x ?
S1: (x ?
?)3.
inserting word y ?
S2 into S1: (?
?
y)By definition, the cost of a sequence of edit op-erations is the sum of the costs2 of the elementary2We used unitary costs in this study282operations, and the distance between S1 and S2 isthe cost of the least cost transformation of S1 intoS2.
Wagner & Fischer?s method provides a simpleand effective way (O(|S1||S2|)) to compute it.
Toreduce size effects, we normalized by |S1|+|S2|2 .3.3 Zhang & Shasha?s algorithmZhang & Shasha?s method (Zhang and Shasha,1989; Dulucq and Tichit, 2003) generalizes Wag-ner & Fischer?s edit distance to trees: given twotrees T1 and T2, it computes the least-cost se-quence of edit operations that transforms T1 intoT2.
Elementary operations have unitary costs andapply to nodes (labels and words in the syntactictrees).
These operations are depicted below: sub-stitution of node c by node g (top figure), inser-tion of node d (middle fig.
), and deletion of noded (bottom fig.
), each read from left to right.Tree edit distance d(T1, T2) is determined aftera series of intermediate calculations involving spe-cial subtrees of T1 and T2, rooted in keyroots.3.3.1 Keyroots, special subtrees and forestsGiven a certain node x, L(x) denotes its left-most leaf descendant.
L is an equivalence rela-tion over nodes and keyroots (KR) are by definitionthe equivalence relation representatives of high-est postfix index.
Special subtrees (SST) are thesubtrees rooted in these keyroots.
Consider a treeT postfix indexed (left figure hereinafter) and itsthree SSTs (right figure).SST(k1) rooted in k1 is denoted:T [L(k1), L(k1) + 1, .
.
.
, k1].
E.g: SST(3) =T [1, 2, 3] is the subtree containing nodes a, b, d.A forest of SST(k1) is defined as:T [L(k1), L(k1) + 1, .
.
.
, x], where x is anode of SST(k1).
E.g: SST(3) has 3 forests :T [1] (node a), T [1, 2] (nodes a and b) and itself.Forests are ordered sequences of subtrees.3.3.2 An idea of how it worksThe algorithm computes the distance between allpairs of SSTs taken in T1 and T2, rooted inincreasingly-indexed keyroots.
In the end, the lastSSTs being the full trees, we have d(T1, T2).In the main routine, an N1 ?
N2 array calledTREEDIST is progressively filled with valuesTREEDIST(i, j) equal to the distance between thesubtree rooted in T1?s ith node and the subtreerooted in T2?s jth node.
The bottom right-handcell of TREEDIST is therefore equal to d(T1, T2).Each step of the algorithm determines the editdistance between two SSTs rooted in keyroots(k1, k2) ?
(T1 ?
T2).
An array FDIST is ini-tialized for this step and contains as many linesand columns as the two given SSTs have nodes.The array is progressively filled with the distancesbetween increasing forests of these SSTs, simi-larly to Wagner & Fischer?s method.
The bot-tom right-hand value of FDIST contains the dis-tance between the SSTs, which is then stored inTREEDIST in the appropriate cell.
Calculationsin FDIST and TREEDIST rely on the double re-currence formula depicted below:The first formula is used to compute the dis-tance between two forests (a white one and a blackone), each of which is composed of several trees.The small circles stand for the nodes of highestpostfix index.
Distance between two forests is de-fined as the minimum cost operation between threepossibilities: replacing the rightmost white tree bythe rightmost black tree, deleting the white node,or inserting the black node.The second formula is analogous to the first one,in the special case where the forests are reduced toa single tree.
The distance is defined as the mini-mum cost operation between: replacing the whitenode with the black node, deleting the white node,or inserting the black node.283It is important to notice that the first formulatakes the left context of the considered subtreesinto account3 : ancestor and left sibling orders arepreserved.
It is not possible to replace the whitenode with the black node directly, the whole sub-tree rooted in the white node has to be replaced.The good thing is, the cost of this operation hasalready been computed and stored in TREEDIST.Let?s see why all the computations required at agiven step of the recurrence formula have alreadybeen calculated.
Let two SSTs of T1 and T2 berooted in pos1 and pos2.
Considering the symme-try of the problem, let?s only consider what hap-pens with T1.
When filling FDIST(pos1, pos2),all nodes belonging to SST(pos1) are run through,according to increasing postfix indexes.
Considerx ?
T [L(pos1), .
.
.
, pos1]:If L(x) = L(pos1), then x belongs to the left-most branch of T [L(pos1), .
.
.
, pos1] and forestT [L(pos1), .
.
.
, x] is reduced to a single tree.
Byconstruction, all FDIST(T [L(pos1), .
.
.
, y],?)
fory ?
x ?
1 have already been computed.
If thingsare the same for the current node in SST(pos2),then TREEDIST(T [L(pos1), .
.
.
, x],?)
can becalculated directly, using the appropriate FDISTvalues previously computed.If L(x) 6= L(pos1), then x does not belongto the leftmost branch of T [L(pos1), .
.
.
, pos1]and therefore x has a non-empty left contextT [L(pos1), .
.
.
, L(x)?1].
Let?s see why comput-ing FDIST(T [L(pos1), .
.
.
, x],?)
requires valueswhich have been previously obtained.?
If x is a keyroot, since the algorithmruns through keyroots by increasing order,TREEDIST(T [L(x), .
.
.
, x],?)
has alreadybeen computed.?
If x is not a keyroot, then there exists a nodez such that : x < z < pos1, z is a keyrootand L(z) = L(x).
Therefore x belongs tothe leftmost branch of T [L(z), .
.
.
, z], whichmeans TREEDIST(T [L(z), .
.
.
, x],?)
hasalready been computed.Complexity for this algorithm is :O(|T1| ?
|T2| ?
min(p(T1), f(T1)) ?
min(p(T2), f(T2)))where d(Ti) is the depth Ti and f(Ti) is the num-ber of terminal nodes of Ti.3The 2nd formula does too, since left context is empty.3.4 Our proposal: a degree of similarityThis final method computes a degree of similar-ity between two sentences, considered as lists ofsyntactic (labels) and lexical (words) constituents.Because some constituents are more likely to in-dicate parallelism than others (e.g: the list itemmarker is more pertinent than the determiner ?a?
),a crescent weight function p(x) ?
[0, 1] w.r.t.pertinence is assigned to all lexical and syntac-tic constituents x.
A set of special subsentencesis then generated: the greatest common divisor ofS1 and S2, gcd(S1, S2), is defined as the longestlist of words common to S1 and S2.
Then foreach sentence Si, the set of special subsentencesis computed using the words of gcd(S1, S2) ac-cording to their order of appearance in Si.
Forexample, if S1 = cabcad and S2 = acbae,gcd(S1, S2) = {c, a, b, a}.
The set of subsen-tences for S1 is {caba, abca} and the set for S2 isreduced to {acba}.
Note that any generated sub-sentence is exactly the size of gcd(S1, S2).For any two subsentences s1 and s2, we definea degree of similarity D(s1, s2), inspired fromstring edit distances:D(s1, s2) =nXi=1?dmax ?
d(xi)dmax?
p(xi)?8>>>><>>>>>:n size of all subsentencesxi ith constituent of s1dmax max possible dist.
between any xi ?
s1 and itsparallel constituent in s2, i.e.
dmax = n ?
1d(xi) distance between current constituent xiin s1 and its parallel constituent in s2p(xi) parallelism weight of xiThe further a constituent from s1 is from itssymmetric occurrence in s2, the more similarthe compared subsentences are.
Eventually, thedegree of similarity between sentences S1 and S2is defined as:D(S1, S2) =2|S1| + |S2|?
maxs1,s2D(s1, s2)ExampleConsider S1 = cabcad and S2 = acbae, alongwith their subsentences s1 = caba and s?1 = abcafor S1, and s2 = acba for S2.
The degrees ofparallelism between s1 and s2, and between s?1and s2 are computed.
The mapping between theparallel constituents is shown below.284For example:D(s1, s2) =4Xi=1?3 ?
d(xi)3 ?
p(xi)?= 2/3p(c) + 2/3p(a) + p(b) + p(a)Assume p(b) = p(c) = 12 and p(a) = 1.
ThenD(s1, s2) = 2.5 and, similarly D(s?1, s2) ' 2.67.Therefore the normalized degree of parallelism isD(S1, S2) = 25+6 ?
2.67, which is about 0.48.4 EvaluationThis section describes the methodology employedto evaluate performances.
Then, after a prelimi-nary study of our corpus, results are presented suc-cessively for each method.
Finally, the behavior ofthe methods is analyzed at sentence level.4.1 MethodologyOur parallelism detection is an unsupervised clus-tering application: given a set of pairs of sen-tences, it automatically classifies them into theclass of the parallelisms and the remaindersclass.
Pairs were extracted from 5 scientific ar-ticles written in English, each containing about200 sentences: Green (ACL?98), Kan (Kan etal.
WVLC?98), Mitkov (Coling-ACL?98), Oakes(IRSG?99) and Sand (Sanderson et al SIGIR?99).The idea was to compute for each pair a paral-lelism score indicating the similarity between thesentences.
Then the choice of a threshold deter-mined which pairs showed a score high enough tobe classified as parallel.Evaluation was based on a manual annotationwe proceeded over the texts.
In order to reducecomputational complexity, we only considered theparallelism occurring between consecutive sen-tences.
For each sentence, we indicated the indexof its parallel sentence.
We assumed transitivity ofparallelism : if S1//S2 and S2//S3, then S1//S3.It was thus considered sufficient to indicate the in-dex of S1 for S2 and the index of S2 for S3 toaccount for a parallelism between S1, S2 and S3.We annotated pairs of sentences where textualparallelism led us to rhetorically coordinate them.The decision was sometimes hard to make.
Yetwe annotated it each time to get more data and tostudy the behavior of the methods on these exam-ples, possibly penalizing our applications.
In theend, 103 pairs were annotated.We used the notions of precision (correctness)and recall (completeness).
Because efforts in im-proving one often result in degrading the other,the F-measure (harmonic mean) combines theminto a unique parameter, which simplifies compar-isons of results.
Let P be the set of the annotatedparallelisms and Q the set of the pairs automati-cally classified in the parallelisms after the use ofa threshold.
Then the associated precision p, recallr and F-measure f are defined as:p = |P ?Q||Q| r =|P ?Q||P | f =21/p + 1/qAs we said, the unique task of the implementedmethods was to assign parallelism scores to pairsof sentences, which are collected in a list.
Wemanually applied various thresholds to the listand computed their corresponding F-measure.
Wekept as a performance indicator the best F-measurefound.
This was performed for each method andon each text, as well as on the texts all gatheredtogether.4.2 Preliminary corpus studyThis paragraph underlines some of the character-istics of the corpus, in particular the distribution ofthe annotated parallelisms in the texts for adjacentsentences.
The following table gives the percent-age of parallelisms for each text:Parallelisms Nb of pairsGreen 39 (14.4 %) 270Kan 12 (6 %) 200Mitkov 13 (8.4 %) 168Oakes 22 (13.7 %) 161Sand 17 (7.7 %) 239All gathered 103 (9.9 %) 1038Green and Oakes show significantly more paral-lelisms than the other texts.
Therefore, if we con-sider a lazy method that would put all pairs in theclass of parallelisms, Green and Oakes will yielda priori better results.
Precision is indeed directlyrelated to the percentage of parallelisms in the text.In this case, it is exactly this percentage, and itgives us a minimum value of the F-measure ourmethods should at least reach:Precision Recall F-measureGreen 14.4 100 25.1Kan 6 100 11.3Mitkov 8.4 100 15.5Oakes 13.7 100 24.1Sand 7.7 100 14.3All 9.9 100 18.04.3 A baseline: counting words in commonWe first present the results of a very simple andthus very fast method.
This baseline counts the285words sentences S1 and S2 have in common, andnormalizes the result by |S1|+|S2|2 in order to re-duce size effects.
No syntactic analysis nor lexicalhomogenization was performed on the texts.Results for this method are summarized in the fol-lowing table.
The last column shows the loss (%)in F-measure after applying a generic threshold(the optimal threshold found when all texts aregathered together) on each text.F-meas.
Prec.
Recall Thres.
LossGreen 45 34 67 0.4 2Kan 24 40 17 0.9 10Mitkov 22 13 77 0.0 8Oakes 45 78 32 0.8 7Sand 23 17 35 0.5 1All 30 23 42 0.5 -We first note that results are twice as good aswith the lazy approach, with Green and Oakesfar above the rest.
Yet this is not sufficient for areal application.
Furthermore, the optimal thresh-old is very different from one text to another,which makes the learning of a generic thresholdable to detect parallelisms for any text impossible.The only advantage here is the simplicity of themethod: no prior treatment was performed on thetexts before the search, and the counting itself wasvery fast.4.4 String edit distanceWe present the results for the 1st method below:F-meas.
Prec.
Recall Thres.
LossGreen 52 79 38 0.69 0Kan 44 67 33 0.64 2Mitkov 38 50 31 0.69 0Oakes 82 94 73 0.68 0Sand 47 54 42 0.72 9All 54 73 43 0.69 -Green and Oakes still yield the best results, butthe other texts have almost doubled theirs.
Resultsfor Oakes are especially good: an F-measure of82% guaranties high precision and recall.In addition, the use of a generic threshold oneach text had little influence on the value of theF-measure.
The greatest loss is for Sand and onlycorresponds to the adjunction of four pairs of sen-tences in the class of parallelisms.
The selection ofa unique generic threshold to predict parallelismsshould therefore be possible.4.5 Tree edit distanceThe algorithm was applied using unitary editcosts.
Since it did not seem natural to establishmappings between different levels of the sentence,edit operations between two constituents of dif-ferent nature (e.g: substitution of a lexical by asyntactic element) were forbidden by a prohibitivecost (1000).
However, this banning only improvedthe results shyly, unfortunately.F-meas.
Prec.
Recall Thres.
LossGreen 46 92 31 0.72 3Kan 44 67 33 0.75 0Mitkov 43 40 46 0.87 11Oakes 81 100 68 0.73 0Sand 52 100 35 0.73 2All 51 73 39 0.75 -As illustrated in the table above, results arecomparable to those previously found.
We note anespecially good F-measure for Sand: 52%, against47% for the string edit distance.
Optimal thresh-olds were quite similar from one text to another.4.6 Degree of similarityBecause of the high complexity of this method, aheuristic was applied.
The generation of the sub-sentences is indeed in?Ckini , ki being the numberof occurrences of the constituent xi in gcd, andni the number of xi in the sentence.
We choseto limit the generation to a fixed amount of sub-sentences.
The constituents that have a great Ckinibring too much complexity: we chose to eliminatetheir (ni ?
ki) last occurrences and to keep theirki first occurrences only to generate subsequences.An experiment was conducted in order todetermine the maximum amount of subsentencesthat could be generated in a reasonable amount oftime without significant performance loss and 30was a sufficient number.
In another experiment,different parallelism weights were assigned tolexical constituents and syntactic labels.
The aimwas to understand their relative importance forparallelisms detection.
Results show that lexicalconstituents have a significant role, but conclu-sions are more difficult to draw for syntacticlabels.
It was decided that, from now on, the lex-ical weight should be given the maximum value, 1.Finally, we assigned different weights to thesyntactic labels.
Weights were chosen after count-ing the occurrences of the labels in the corpus.
Infact, we counted for each label the percentage ofoccurrences that appeared in the gcd of the paral-lelisms with respect to those appearing in the gcdof the other pairs.
Percentages were then rescaledfrom 0 to 1, in order to emphasize differences286between labels.
The obtained parallelism valuesmeasured the role of the labels in the detection ofparallelism.
Results for this experiment appear inthe table below.F-meas.
Prec.
Recall Thres.
LossGreen 55 59 51 0.329 2Kan 47 80 33 0.354 5Mitkov 35 40 31 0.355 0Oakes 76 80 73 0.324 4Sand 29 20 59 0.271 0All 50 59 43 0.335 -The optimal F-measures were comparable tothose obtained in 4.4 and the correspondingthresholds were similar from one text to another.This section showed how the three proposedmethods outperformed the baseline.
Each of themyielded comparable results.The next section presents the results at sentencelevel, together with a comparison of these threemethods.4.7 Analysis at sentence levelThe different methods often agreed but sometimesreacted quite differently.Well retrieved parallelismsSome parallelisms were found by each methodwith no difficulty: they were given a high degreeof parallelism by each method.
Typically, suchsentences presented a strong lexical and syntacticsimilarity, as in the example in section 2.Parallelisms hard to findOther parallelisms received very low scoresfrom each method.
This happened when the an-notated parallelism was lexically and syntacticallypoor and needed either contextual information orexternal semantic knowledge to find keywords(e.g: ?first?, ?second?, .
.
.
), paraphrases or pat-terns (e.g: ?X:Y?
in the following example (Kan)):Rear: a paragraph in which a link just stoppedoccurring the paragraph before.No link: any remaining paragraphs.Different methods, different resultsEventually, we present some parallelisms thatobtained very different scores, depending on themethod.First, it seems that a different ordering of theparallel constituents in the sentences alter the per-formances of the edit distance algorithms (3.2;3.3).
The following example (Green) received alow score with both methods:When we consider AnsV as our dependent vari-able, the model for the High Web group is stillnot significant, and there is still a high probabil-ity that the coefficient of LI is 0.For our Low Web group, who followed signif-icantly more intra-article links than the HighWeb group, the model that results is significantand has the following equation: <EQN/>.This is due to the fact that both algorithms do notallow the inversion of two constituents and thusare unable to find all the links from the first sen-tence to the other.
The parallelism measure is ro-bust to inversion.Sometimes, the syntactic parser gave differentanalyses for the same expression, which mademapping between the sentences containing this ex-pression more difficult, especially for the tree editdistance.
The syntactic structure has less impor-tance for the other methods, which are thus moreinsensitive to an incorrect analysis.Finally, the parallelism measure seems moreadapted to a diffuse distribution of the parallelconstituents in the sentences, whereas edit dis-tances seem more appropriate when parallel con-stituents are concentrated in a certain part of thesentences, in similar syntactic structures.
The fol-lowing example (Green) obtained very high scoreswith the edit distances only:Strong relations are also said to exist betweenwords that have synsets connected by a singlehorizontal link or words that have synsets con-nected by a single IS-A or INCLUDES relation.A regular relation is said to exist between twowords when there is at least one allowable pathbetween a synset containing the first word and asynset containing the second word in the Word-Net database.5 Related workExperimental work in psycholinguistics hasshown the importance of the parallelism effect inhuman language processing.
Due to some kindof priming (syntactic, phonetic, lexical, etc.
), thecomprehension and the production of a parallel ut-terance is made faster (Dubey et al, 2005).So far, most of the works were led in order toacquire resources and to build systems to retrievespecific parallelism phenomena.
In the field of in-formation structure theories, (Kruijff-Korbayova?and Kruijff, 1996) implemented an ad-hoc system287to identify thematic continuity (lexical relation be-tween the subject parts of consecutive sentences).
(Luc et al, 1999) described and classified markers(lexical clues, layout and typography) occurring inenumeration structures.
(Summers, 1998) also de-scribed the markers required for retrieving head-ing structures.
(Charolles, 1997) was involved inthe description of frame introducers.Integration of specialized resources dedicatedto parallelism detection could be an improvementto our approach.
Let us not forget that our fi-nal aim remains the detection of discourse struc-tures.
Parallelism should be considered as an ad-ditional feature which among other discourse fea-tures (e.g.
connectors).Regarding the use of parallelism, (Hernandezand Grau, 2005) proposed an algorithm to parsethe discourse structure and to select pairs of sen-tences to compare.Confronted to the problem of determining tex-tual entailment4 (the fact that the meaning ofone expression can be inferred from another)(Kouylekov and Magnini, 2005) applied the(Zhang and Shasha, 1989)?s algorithm on the de-pendency trees of pairs of sentences (they did notconsider syntactic tags as nodes but only words).They encountered problems similar to ours due topre-treatment limits.
Indeed, the syntactic parsersometimes represents in a different way occur-rences of similar expressions, making it harder toapply edit transformations.
A drawback concern-ing the tree-edit distance approach is that it is notable to observe the whole tree, but only the subtreeof the processed node.6 ConclusionTextual parallelism plays an important role amongdiscourse features when detecting discourse struc-tures.
So far, only occurrences of this phenomenonhave been treated individually and often in an ad-hoc manner.
Our contribution is a unifying frame-work which can be used for automatic processingwith much less specific knowledge than dedicatedtechniques.In addition, we discussed and evaluated severalmethods to retrieve them generically.
We showedthat simple methods such as (Wagner and Fis-cher, 1974) can compete with more complex ap-proaches, such as our degree of similarity and the4Compared to entailment, the parallelism relation is bi-directional and not restricted to semantic similarities.
(Zhang and Shasha, 1989)?s algorithm.Among future works, it seems that variationssuch as the editing cost of transformation for editdistance methods and the weight of parallel units(depending their semantic and syntactic charac-teristics) can be implemented to enhance perfor-mances.
Combining methods also seems an inter-esting track to follow.ReferencesNicholas Asher.
1993.
Reference to abstract objects indiscourse.
Kluwer, Dordrecht.E.
Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In AAAI.M.
Charolles.
1997.
L?encadrement du discours -univers, champs, domaines et espaces.
Cahier derecherche linguistique, 6.Amit Dubey, Patrick Sturt, and Frank Keller.
2005.Parallelism in coordination as an instance of syntac-tic priming: Evidence from corpus-based modeling.In HLTC and CEMNLP, Vancouver.S.
Dulucq and L. Tichit.
2003.
RNA SecondaryStructure Comparison: Exact Analysis of the Zhang-Shasha Tree Edit Algorithm.
Theoretical ComputerScience, 306(1-3):471?484.N.
Hernandez and B. Grau.
2005.
De?tection au-tomatique de structures fines du discours.
In TALN,France.J.
R. Hobbs and A. Kehler.
1997.
A theory of paral-lelism and the case of vp ellipsis.
In ACL.M.
Kouylekov and B. Magnini.
2005.
RecognizingTextual Entailment with Tree Edit Distance Algo-rithms.
PASCAL Challenges on RTE.I.
Kruijff-Korbayova?
and G.-J.
M. Kruijff.
1996.
Iden-tification of topic-focus chains.
In DAARC, vol-ume 8, pages 165?179.
University of Lancaster, UK.C.
Luc, M. Mojahid, J. Virbel, Cl.
Garcia-Debanc, andM.-P. Pe?ry-Woodley.
1999.
A linguistic approachto some parameters of layout: A study of enumera-tions.
In AAAI, North Falmouth, Massachusets.W.
C. Mann and S. A. Thompson.
1987.
Rhetori-cal structure theory: A theory of text organisation.Technical report isi/rs-87-190.K.
M. Summers.
1998.
Automatic Discovery of Logi-cal Document Structure.
Ph.D. thesis, U. of Cornell.R.A.
Wagner and M.J. Fischer.
1974.
The String-to-String Correction Problem.
Journal of the ACM,21(1):168?173.K.
Zhang and D. Shasha.
1989.
Simple fast algo-rithms for the editing distance between trees andrelated problems.
SIAM Journal on Computing,18(6):1245?1262.288
