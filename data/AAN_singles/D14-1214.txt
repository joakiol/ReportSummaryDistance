Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1997?2007,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsMajor Life Event Extraction from Twitter based onCongratulations/Condolences Speech ActsJiwei Li1, Alan Ritter2, Claire Cardie3and Eduard Hovy41Computer Science Department, Stanford University, Stanford, CA 94305, USA2Department of Computer Science and Engineering, the Ohio State University, OH 43210, USA3Computer Science Department, Cornell University, Ithaca, NY 14853, USA4Language Technology Institute, Carnegie Mellon University, PA 15213, USAjiweil@stanford.edu ritter.1492@osu.educardie@cs.cornell.edu ehovy@andrew.cmu.eduAbstractSocial media websites provide a platformfor anyone to describe significant eventstaking place in their lives in realtime.Currently, the majority of personal newsand life events are published in a tex-tual format, motivating information ex-traction systems that can provide a struc-tured representations of major life events(weddings, graduation, etc.
.
.
).
This pa-per demonstrates the feasibility of accu-rately extracting major life events.
Oursystem extracts a fine-grained descriptionof users?
life events based on their pub-lished tweets.
We are optimistic that oursystem can help Twitter users more easilygrasp information from users they take in-terest in following and also facilitate manydownstream applications, for example re-altime friend recommendation.1 IntroductionSocial networking websites such as Facebook andTwitter have recently challenged mainstream me-dia as the freshest source of information on im-portant news events.
In addition to an importantsource for breaking news, social media presents aunique source of information on private events, forexample a friend?s engagement or college gradua-tion (examples are presented in Figure 1).
Whilea significant amount of previous work has inves-tigated event extraction from Twitter (e.g., (Rit-ter et al., 2012; Diao et al., 2012)), existing ap-proaches mostly focus on public bursty event ex-traction, and little progress has been made towardsthe problem of automatically extracting the majorlife events of ordinary users.A system which can automatically extract ma-jor life events and generate fine-grained descrip-tions as in Figure 1 will not only help Twitterusers with the problem of information overload bysummarizing important events taking place in theirfriends lives, but could also facilitate downstreamapplications such as friend recommendation (e.g.,friend recommendation in realtime to people whowere just admitted into the same university, getthe same jobs or internships), targeted online ad-vertising (e.g., recommend baby care products tonewly expecting mothers, or wedding services tonew couples), information extraction, etc.Before getting started, we first identify a num-ber of key challenges in extracting significant lifeevents from user-generated text, which account thereason for the lack of previous work in this area:Challenge 1: Ambiguous Definition for Ma-jor Life Events Major life event identificationis an open-domain problem.
While many types ofevents (e.g., marriage, engagement, finding a newjob, giving birth) are universally agreed to be im-portant, it is difficult to robustly predefine a list ofcharacteristics for important life events on whichalgorithms can rely for extraction or classification.Challenge 2: Noisiness of Twitter Data: Theuser-generated text found in social media websitessuch as Twitter is extremely noisy.
The languageused to describe life events is highly varied andambiguous and social media users frequently dis-cuss public news and mundane events from theirdaily lives, for instance what they ate for lunch.Even for a predefined life event category, suchas marriage, it is still difficult to accurately iden-tify mentions.
For instance, a search for thekeyphrase ?get married?
using Twitter Search1re-sults in a large number of returned results that donot correspond to a personal event:?
I want to get married once.
No divorce & nocheating, just us two till the end.
(error: wishes)1https://twitter.com/search?q=get?married1997Figure 1: Examples of users mentioning personal life events on Twitter.?
Can Adam Sandler and Drew Barrymore justdrop the pretense and get married already?
(error: somebody else)?
I got married and had kids on purpose(error: past)Challenge 3: the Lack of Training Data Col-lecting sufficient training data in this task for ma-chine learning models is difficult for a number ofreasons: (1) A traditional, supervised learning ap-proach, requires explicit annotation guidelines forlabeling, though it is difficult to know which cat-egories are most representative in the data apriori.
(2) Unlike public events which are easily identi-fied based on message volume, significant privateevents are only mentioned by one or several usersdirectly involved in the event.
Many important cat-egories are relatively infrequent, so even a largeannotated dataset may contain just a few or no ex-amples of these categories, making classificationdifficult.In this paper, we present a pipelined system thataddresses these challenges and extracts a struc-tured representation of individual life events basedon users?
Twitter feeds.
We exploit the insight toautomatically gather large volumes of major lifeevents which can be used as training examples formachine learning models.
Although personal lifeevents are difficult to identify using traditionalapproaches due to their highly diverse nature, wenoticed that users?
followers often directly replyto such messages with CONGRATULATIONS orCONDOLENCES speech acts, for example:User1: I got accepted into Harvard !User2: Congratulations !These speech acts are easy to identify with highprecision because the possible ways to expressthem are relatively constrained.
Instead of directlyinspecting tweets to determine whether they corre-spond to major life events, we start by identifyingreplies corresponding to CONGRATULATIONS orCONDOLENCES, and then retrieve the messagethey are in response to, which we assume refer toimportant life events.The proposed system automatically identifiesmajor life events and then extracts correspondentevent properties.
Through the proposed system,we demonstrate that it is feasible to automaticallyreconstruct a detailed list of individual life eventsbased on users?
Twitter streams.
We hope thatwork presented in this paper will facilitate down-stream applications and encourage follow-up workon this task.2 System OverviewAn overview of the components of the system ispresented in Figure 2.
Pipeline1 first identifiesthe major life event category the input tweet talksabout and filters out the irrelevant tweets and willbe described in Section 4.
Next, Pipeline2, as,demonstrated in Section 5, identifies whether thespeaker is directly involved in the life event.
Fi-nally, Pipeline3 extracts the property of event andwill be illustrated in Section 6.Section 3 serves as the preparing step for thepipelined system, describing how we collect train-ing data in large-scale.
The experimental evalua-tion regarding each pipeline of the system is pre-sented in the corresponding section (i.e., Section4,5,6) and the end-to-end evaluation will be pre-1998Figure 2: System Overview.
Blue: original input tweets.
Red: filtered out tweets.
Magenta: life eventcategory.
Green: life event property.
Pipeline 1 identifies the life category the input tweet talks about(e.g., marriage, graduation) and filter out irrelevant tweets (e.g., I had beef stick for lunch).
Pipeline 2identifies whether the speaker is directly involved in the event.
It will preserve self-reported information(i.e.
?I got married?)
and filtered out unrelated tweets (e.g., ?my friend Chris got married?).
Pipeline3 extracts the property of event (e.g.
to whom the speaker married or the speaker admitted by whichuniversity).sented in Section 7.3 Personal Life Event ClusteringIn this section, we describe how we identify com-mon categories of major life events by leverag-ing large quantities of unlabeled data and obtaina collection of tweets corresponding to each typeof identified event.3.1 Response based Life Event DetectionWhile not all major life events will elicit CON-GRATULATIONS or CONDOLENCES from a user?sfollowers, this technique allows us to collect largevolumes of high-precision personal life eventswhich can be used to train models to recognize thediverse categories of major life events discussedby social media users.3.2 Life Event ClusteringBased on the above intuition, we develop an ap-proach to obtain a list of individual life event clus-ters.
We first define a small set of seed responseswhich capture common CONGRATULATIONS andCONDOLENCES, including the phrases: ?Congrat-ulations?, ?Congrats?, ?Sorry to hear that?, ?Awe-some?, and gather tweets that were observed withseed responses.
Next, an LDA (Blei et al., 2003)2based topic model is used to cluster the gathered2Topic Number is set to 120.tweets to automatically identify important cate-gories of major life events in an unsupervised way.In our approach, we model the whole conversationdialogue as a document3with the response seeds(e.g., congratulation) masked out.
We furthermoreassociate each sentence with a single topic, fol-lowing strategies adopted by (Ritter et al., 2010;Gruber et al., 2007).
We limit the words in ourdocument collection to verbs and nouns whichwe found to lead to clearer topic representations,and used collapsed Gibbs Sampling for inference(Griffiths and Steyvers, 2004).Next one of the authors manually inspected theresulting major life event types inferred by themodel, and manually assigned them labels suchas ?getting a job?, ?graduation?
or ?marriage?and discarded incoherent topics4.
Our methodol-ogy is inspired by (Ritter et al., 2012) that usesa LDA-CLUSTERING+HUMAN-IDENTIFICATIONstrategy to identify public events from Twitter.Similar strategies have been widely used in un-supervised information extraction (Bejan et al.,2009; Yao et al., 2011) and selectional preference3Each whole conversation usually contains multipletweets and users.4While we applied manual labeling and coherence eval-uation in this work, an interesting direction for future workis automatically labeling major life event categories follow-ing previous work on labeling topics in traditional document-based topic models (Mimno et al., 2011; Newman et al.,2010).1999Figure 3: Illustration of bootstrapping process.Input: Reply seed list E = {e}, Tweet conversation col-lection T = {t}, Retrieved Tweets Collection D = ?.Identified topic list L=?BeginWhile not stopping:1.
For unprocessed conversation t ?
Tif t contains reply e ?
E,?
add t to D: D = D + t.?
remove t from T : T = T ?
t2.
Run streaming LDA (Yao et al., 2009) on newly addedtweets in D.3.
Manually Identify meaningful/trash topics, giving labelto meaningful topics.4.
Add newly detected meaningful topic l to L.5.
For conversation t belonging to trash topics?
remove t from D: D = D ?
t6.
Harvest more tweets based on topic distribution.7.
Manually identify top 20 responses to tweets harvestedfrom Step 6.8.
Add meaningful responses to E.EndOutput: Identified topic list L. Tweet collection D.Figure 4: Bootstrapping Algorithm for Response-based Life event identification.modeling (Kozareva and Hovy, 2010a; Robertsand Harabagiu, 2011).Conversation data was extracted from the CMUTwitter Warehouse of 2011 which contains a totalnumber of 10% of all published tweets in that year.3.3 Expanding dataset using BootstrappingWhile our seed patterns for identifying mes-sages expressing CONGRATULATIONS and CON-DOLENCES are very high precision, they don?tcover all the possible ways these speech actscan be expressed.
We therefore adopt a semi-supervised bootstrapping approach to expand ourreply seeds and event-related tweets.
Our boot-strapping approach is related to previous workon semi-supervised information harvesting (e.g.,(Kozareva and Hovy, 2010b; Davidov et al.,2007)).
To preserve the labeled topics from thefirst iteration, we apply a streaming approach toinference (Yao et al., 2009) over unlabeled tweets(those which did not match one of the responseFigure 5: Illustration of data retrieved in each stepof bootstrapping.congratulations (cong, congrats); (that?s) fantastic; (so) cool;(I?m) (very) sorry to hear that; (that?s) great (good) new;awesome; what a pity; have fun; great; that sucks; toobad; (that?s) unfortunate; how sad; fabulous; (that?s)terrific; (that?s) (so) wonderful; my deepest condolences;Table 1: Responses retrieved from Bootstrapping.seeds).
We collect responses to the newly addedtweets, then select the top 20 frequent replies5.Next we manually inspect and filter the top rankedreplies, and use them to harvest more tweets.
Thisprocess is then repeated with another round ofinference in LDA including manual labeling ofnewly inferred topics, etc... An illustration of ourapproach is presented in Figure 3 and the detailsare presented in Figure 4.
The algorithm outputsa collection of personal life topics L, and a collec-tion of retrieved tweets D. Each tweet d ?
D isassociated with a life event topic l, l ?
L.We repeat the bootstrapping process for 4 iter-ations and end up with 30 different CONGRATU-LATIONS and CONDOLENCES patterns (shown inTable 1) and 42 coherent event types which refer tosignificant life events (statistics for harvested datafrom each step is shown in Figure 5).
We showexamples of the mined topics with correspondenthuman labels in Table 3, grouped according to aspecific kind of resemblance.3.4 Summary and DiscussionThe objective of this section is (1) identifying acategory of life events (2) identifying tweets asso-ciated with each event type which can be used ascandidates for latter self reported personal infor-mation and life event category identification.We understand that the event list retrieved fromour approach based on replies in the conversationis far from covering all types of personal events(especially the less frequent life events).
But our5We only treat the first sentence that responds to the be-ginning of the conversation as replies.2000Life Event ProportionBirthday 9.78Job 8.39WeddingEngagement7.24Award 6.20Sports 6.08Anniversary 5.44Give Birth 4.28Graduate 3.86Death 3.80Admission 3.54InterviewInternship3.44Moving 3.26Travel 3.24Illness 2.45Life Event ProportionVacation 2.24Relationship 2.16Exams 2.02Election 1.85New Car 1.65Running 1.42Surgery 1.20Lawsuit 0.64Acting 0.50Research 0.48Essay 0.35Lost Weight 0.35Publishing 0.28Song 0.22OTHER 15.31Table 2: List of automatically discovered life eventtypes with percentage (%) of data covered.list is still able to cover a large proportion of IM-PORTANT and COMMON life events.
Our latterwork is focused on given a random tweet, identi-fying whether it corresponds to one of the 42 typesof life events in our list.Another thing worth noting here is that, whilecurrent section is not focused on self-reported in-formation identification, we have already obtaineda relatively clean set of data with a large pro-portion of non self-reported information relatedtweets being screened: people do not usually re-spond to non self-reported information with com-monly used replies, or in other words, with repliesthat will pass our next step human test6.
These nonself-reported tweets would therefore be excludedfrom training data.4 Life Event IdentificationIn this section, we focused on deciding whether agiven tweet corresponds to one of the 42 prede-fined life events.Our training dataset consists of approximately72,000 tweets from 42 different categories of lifeevents inferred by our topic model as describedin Section 3.
We used the top 25% of tweets forwhich our model assigned highest probability toeach topic.
For sparsely populated topics we usedthe top 50% of tweets to ensure sufficient cover-age.We further collected a random sample of about10 million tweets from Twitter API7as non-life6For example, people don?t normally respond to ?I wantto get married once?
(example in Challenge 2, Section 1)with ?Congratulations?.7https://dev.twitter.com/Human Label Top wordsWedding&engagementwedding, love, ring, engagement,engaged, bride, video, marryingRelationshipBeginboyfriend, girlfriend, date, check,relationship, see, lookAnniversary anniversary, years, year, married,celebrating, wife, celebrate, loveRelation End/Devoicerelationship, ended, hurt, hate, de-voice, blessings, singleGraduation graduation, school, college, gradu-ate, graduating, year, gradAdmission admitted, university, admission, ac-cepted, college, offer, schoolExam passed, exam, test, school,semester, finished, exams,midtermsResearch research, presentation, journalism,paper, conference, go, writingEssay & Thesis essay, thesis, reading, statement,dissertation, complete, projectJob job, accepted, announce, join, join-ing, offer, starting, announced,workInterview& In-ternshipinterview, position, accepted, in-ternship, offered, start, workMoving house, moving, move, city, home,car, place, apartment, town, leavingTravel leave, leaving, flight, home, miss,house, airport, packing, morningVacation vocation, family, trip, country, go,flying, visited, holiday, HawaiiWinning Award won, award, support, awards, win-ning, honor, scholarship, prizeElection/Promotion/Nominationpresident, elected, run, nominated,named, promotion, cel, selected,business, votePublishing book, sold, writing, finished, read,copy, review, release, books, coverContract signed, contract, deal, agreements,agreed, produce, dollar, meetingsong/ video/ al-bum releasevideo, song, album, check, show,see, making, radio, loveActing play, role, acting, drama, played,series, movie, actor, theaterDeath dies, passed, cancer, family, hospi-tal, dad, grandma, mom, grandpaGive Birth baby, born, boy, pregnant, girl, lbs,name, son, world, daughter, birthIllness ill, hospital, feeling, sick, cold, flu,getting, fever, doctors, coughSurgery surgery, got, test, emergency, blood,tumor, stomachs, hospital, pain,brainSports win, game, team, season, fans,played, winning, football, luckRunning run, race, finished, race, marathon,ran, miles, running, finish, goalNew Car car, buy, bought, cars, get, drive,pick, seat, color, dollar, meetLost Weight weight, lost, week, pounds, loss,weeks, gym, exercise, runningBirthday birthday, come, celebrate, party,friends, dinner, tonight, friendLawsuit sue, sued, file, lawsuit, lawyer, dol-lars, illegal, court, jury.Table 3: Example event types with top words dis-covered by our model.2001event examples and trained a 43-class maximumentropy classifier based on the following features:?
Word: The sequence of words in the tweet.?
NER: Named entity Tag.?
Dictionary: Word matching a dictionaries ofthe top 40 words for each life event category(automatically inferred by the topic model).The feature value is the term?s probabilitygenerated by correspondent event.?
Window: If a dictionary term exists, left andright context words within a window of 3words and their part-of-speech tags.Name entity tag is assigned from Ritter et al?sTwitter NER system (Ritter et al., 2011).
Part-of-Speech tags are assigned based on Twitter POSpackage (Owoputi et al., 2013) developed byCMU ARK Lab.
Dictionary and Window areconstructed based on the topic-term distributionobtained from the previous section.The average precision and recall are shown inTable 4.
And as we can observe, the dictionary(with probability) contributes a lot to the perfor-mance and by taking into account a more compre-hensive set of information around the key word,classifier on All feature setting generate signifi-cantly better performance, with 0.382 previsionand 0.48 recall, which is acceptable considering(1) This is is a 43-way classification with muchmore negative data than positive (2) Some types ofevents are very close to each other (e.g., Leavingand Vocation).
Note that recall is valued more thanprecision here as false-positive examples will befurther screened in self-reported information iden-tification process in the following section.Feature Setting Precision RecallWord+NER 0.204 0.326Word+NER+Dictionary 0.362 0.433All 0.382 0.487Table 4: Average Performance of Multi-ClassClassifier on Different Feature Settings.
Negativeexamples (non important event type) are not con-sidered.5 Self-Reported InformationIdentificationAlthough a message might refer to a topic cor-responding to a life event such as marriage, theevent still might be one in which the speaker isnot directly involved.
In this section we describethe self reported event identification portion of ourpipeline, which takes output from Section 4 andfurther identifies whether each tweet refers to anevent directly involving the user who publishes it.Direct labeling of randomly sampled Twittermessages is infeasible for the following reasons:(1) Class imbalance: self-reported events are rela-tively rare in randomly sampled Twitter messages.
(2) A large proportion of self-reported informationrefers to mundane, everyday topics (e.g., ?I justfinished dinner!?).
Fortunately, many of the tweetsretrieved from Section 3 consist of self-reportedinformation and describe major life events.
Thecandidates for annotation are therefore largely nar-rowed down.We manually annotated 800 positive examplesof self-reported events distributed across the eventcategories identified in Section 3.
We ensuredgood coverage by first randomly sampling 10 ex-amples from each category, the remainder weresampled from the class distribution in the data.Negative examples of self-reported informationconsisted of a combination of examples from theoriginal dataset8and randomly sampled messagesgathered by searching for the top terms in each ofthe pre-identified topics using the Twitter Searchinterface9.
Due to great varieties of negative sce-narios, the negative dataset constitutes about 2500tweets.5.1 FeaturesIdentifying self-reported tweet requires sophisti-cated feature engineering.
Let u denote the termwithin the tweet that gets the highest possibilitygenerated by the correspondent topic.
We experi-mented with combinations of the following typesof features (results are presented in Table ??):?
Bigram: Bigrams within each tweet (punctu-ation included).?
Window: A window of k ?
{0, 1, 2} wordsadjacent to u and their part-of-speech tags.?
Tense: A binary feature indicating past tenseidentified in by the presence of past tenseverb (VBD).?
Factuality: Factuality denotes whether oneexpression is presented as corresponding toreal situations in the world (Saur??
and Puste-jovsky, 2007).
We use Stanford PragBank10,8Most tweets in the bootstrapping output are positive.9The majority of results returned by Twitter Search arenegative examples.10http://compprag.christopherpotts.net/factbank.html2002an extension of FactBank (Saur??
and Puste-jovsky, 2009) which contains a list of modalwords such as ?might?, ?will?, ?want to?etc11.?
I: Whether the subject of the tweet is first per-son singular.?
Dependency: If the subject is first personsingular and the u is a verb, the dependencypath between the subject and u (or non-dependency).Tweet dependency paths were obtained from(Kong et al., 2014).
As the tweet parser we useonly supports one-to-one dependency path iden-tification but no dependency properties, Depen-dency is a binary feature.
The subject of eachtweet is determined by the dependency link to theroot of the tweet from the parser.Among the features we explore, Word encodesthe general information within the tweet.
Win-dow addresses the information around topic keyword.
The rest of the features specifically addresseach of the negative situations described in Chal-lenge 2, Section 1: Tense captures past event de-scription, Factuality filters out wishes or imagi-nation, I and Dependency correspond to whetherthe described event involves the speaker.
We builta linear SVM classifier using SVMlightpackage(Joachims, 1999).5.2 EvaluationFeature Setting Acc Pre RecBigram+Window 0.76 0.47 0.44Bigram+Window+Tense+Factuality0.77 0.47 0.46all 0.82 0.51 0.48Table 5: Performance for self-report informationidentification regarding different feature settings.We report performance on the task of identi-fying self-reported information in this subsection.We employ 5-fold cross validation and report Ac-curacy (Accu), Prevision (Prec) and Recall (Rec)regarding different feature settings.
The Tense,Factuality, I and Dependency features positivelycontribute to performance respectively and thebest performance is obtained when all types of fea-tures are included.11Due to the colloquial property of tweets, we also intro-duced terms such as ?gonna?, ?wanna?, ?bona?.precision recall F10.82 0.86 0.84Table 7: Performance for identifying properties.6 Event Property ExtractionThus far we have described how to automaticallyidentify tweets referring to major life events.
Inaddition, it is desirable to extract important prop-erties of the event, for example the name of theuniversity the speaker was admitted to (See Figure1).
In this section we take a supervised approach toevent property extraction, based on manually an-notated data for a handfull of the major life eventcategories automatically identified by our system.While this approach is unlikely to scale to the di-versity of important personal events Twitter usersare discussing, our experiments demonstrate thatevent property extraction is indeed feasible.We cast the problem of event property extrac-tion as a sequence labeling task, using ConditionalRandom Fields (Lafferty et al., 2001) for learningand inference.
To make best use of the labeleddata, we trained a unified CRF model for closelyrelated event categories which often share proper-ties; the full list is presented in Table 6 and welabeled 300 tweets in total.
Features we used in-clude:?
word token, capitalization, POS?
left and right context words within a windowof 3 and the correspondent part-of-speechtags?
word shape, NER?
a gazetteer of universities and employers bor-rowed from NELL12.We use 5-fold cross-validation and report resultsin Table 7.7 End-to-End ExperimentThe evaluation for each part of our system hasbeen demonstrated in the corresponding section.We now present a real-world evaluation: to whatdegree can our trained system automatically iden-tify life events in real world.7.1 DatasetWe constructed a gold-standard life event datasetusing annotators from Amazon?s Mechanical Turk(Snow et al., 2008) using 2 approaches:12http://rtw.ml.cmu.edu/rtw/kbbrowser/2003Life Event Property(a) Acceptance, Graduation Name of University/College(b) Wedding, Engagement, Falling love Name of Spouse/ partner/ bf/ gf(c) Getting a job, interview, internship Name of Enterprise(d) Moving to New Places, Trip, Vocation, Leaving Place, Origin, Destination(e) Winning Award Name of Award, PrizeTable 6: Labeling Event Property.?
Ask Twitter users to label their own tweets(Participants include friends, colleagues ofthe authors and Turkers from Amazon Me-chanical Turk13).?
Ask Turkers to label other people?s tweets.For option 1, we asked participants to directly la-bel their own published tweets.
For option 2, foreach tweet, we employed 2 Turkers.
Due to theambiguity in defining life events, the value co-hen?s kappa14as a measure of inter-rater agree-ment is 0.54; this does not show significant inter-annotator agreement.
The authors examined dis-agreements and also verified all positively labeledtweets.
The resulting dataset contains around 900positive tweets and about 60,000 negative tweets.To demonstrate the advantage of leveraginglarge quantities of unlabeled data, the first base-line we investigate is a Supervised model which istrained on the manually annotated labeled dataset,and evaluated using 5 fold cross validation.
OurSupervised baseline consists of a linear SVMclassifier using bag of words, NER and POS fea-tures.
We also tested a second baseline thatcombines Supervised algorithm with an our self-reported information classifier, denoted as Super-vised+Self.Results are reported in Table 8; as we can ob-serve, the fully supervised approach is not suitablefor this task with only one digit F1 score.
Theexplanations are as follows: (1) the labeled datacan only cover a small proportion of life events(2) supervised learning does not separate impor-tant event categories and will therefore classifyany tweet with highly weighted features (e.g., themention of ?I?
or ?marriage?)
as positive.
By us-ing an additional self-reported information classi-fier in Supervised+Self, we get a significant boostin precision with a minor recall loss.13https://www.mturk.com/mturk/welcome14http://en.wikipedia.org/wiki/Cohen?s_kappaApproach Precision RecallOur approach 0.62 0.48Supervised 0.13 0.20Supervised+Self 0.25 0.18Table 8: Performance for different approaches foridentifying life events in real world.Approach Precision RecallStep 1 0.65 0.36Step 2 0.64 0.43Step 3 0.62 0.48Table 9: Performance for different steps of boot-strapping for identifying life events in real world.Another interesting question is to what degreethe bootstrapping contributes to the final results.We keep the self-reported information classifierfixed (though it?s based the ultimate identifieddata source), and train the personal event classifierbased on topic distributions identified from eachof the three steps of bootstrapping15.
Precisionand recall at various stages of bootstrapping arepresented in Table 9.
As bootstrapping continues,the precision remains roughly constant, but recallincreases as more life events and CONGRATULA-TIONS and CONDOLENCES are discovered.8 Related WorkOur work is related to three lines of NLP re-searches.
(1) user-level information extraction onsocial media (2) public event extraction on socialmedia.
(3) Data harvesting in Information Extrac-tion, each of which contains large amount of re-lated work, to which we can not do fully justice.User Information Extraction from TwitterSome early approaches towards understandinguser level information on social media is focusedon user profile/attribute prediction (e.g.,(Ciot etal., 2013)) user-specific content extraction (Diao15which are 24, 38, 42-class classifiers, where 24, 38, 42denoted the number of topics discovered in each step of boot-strapping (see Figure 5).2004et al., 2012; Diao and Jiang, 2013; Li et al., 2014)or user personalization (Low et al., 2011) identifi-cation.The problem of user life event extraction wasfirst studied by Li and Cardie?s (2014).
They at-tempted to construct a chronological timeline forTwitter users from their published tweets based ontwo criterion: a personal event should be personaland time-specific.
Their system does not explic-itly identify a global category of life events (andtweets discussing correspondent event) but identi-fies the topics/events that are personal and time-specific to a given user using an unsupervised ap-proach, which helps them avoids the nuisance ofexplicit definition for life event characteristics andacquisition of labeled data.
However, their sys-tem has the short-coming that each personal topicneeds to be adequately discussed by the user andtheir followers in order to be detected16.Public Event Extraction from Twitter Twitterserves as a good source for event detection owingto its real time nature and large number of users.These approaches include identifying bursty pub-lic topics (e.g.,(Diao et al., 2012)), topic evolution(Becker et al., 2011) or disaster outbreak (Sakakiet al., 2010; Li and Cardie, 2013) by spotting theincrease/decrease of word frequency.
Some otherapproaches are focused on generating a structuredrepresentation of events (Ritter et al., 2012; Ben-son et al., 2011).Data Acquisition in Information ExtractionOur work is also related with semi-supervised dataharvesting approaches, the key idea of which isthat some patterns are learned based on seeds.They are then used to find additional terms, whichare subsequently used as new seeds in the patternsto search for additional new patterns (Kozarevaand Hovy, 2010b; Davidov et al., 2007; Riloffet al., 1999; Igo and Riloff, 2009; Kozareva etal., 2008).
Also related approaches are distant orweakly supervision (Mintz et al., 2009; Craven etal., 1999; Hoffmann et al., 2011) that rely on avail-able structured data sources as a weak source ofsupervision for pattern extraction from related textcorpora.16The reason is that topic models use word frequency fortopic modeling.9 Conclusion and DiscussionIn this paper, we propose a pipelined system formajor life event extraction from Twitter.
Experi-mental results show that our model is able to ex-tract a wide variety of major life events.The key strategy adopted in this work is to ob-tain a relatively clean training dataset from largequantity of Twitter data by relying on minimumefforts of human supervision, and sometimes is atthe sacrifice of recall.
To achieve this goal, we relyon a couple of restrictions and manual screenings,such as relying on replies, LDA topic identifica-tion and seed screening.
Each part of system de-pends on the early steps.
For example, topic clus-tering in Section 3 not only offers training data forevent identification in Section 4, but prepares thetraining data for self-information identification inSection 5.
.We acknowledge that our approach is notperfect due to the following ways: (1) The systemis only capable of discovering a few categoriesof life events with many others left unidentified.
(2) Each step of the system will induce errors andnegatively affected the following parts.
(3) Someparts of evaluations are not comprehensive dueto the lack of gold-standard data.
(4) Among allpipelines, event property identification in Section6 still requires full supervision in CRF model,making it hard to scale to every event type17.How to address these aspects and generate a moreaccurate, comprehensive and fine-grained lifeevent list for Twitter users constitute our furtherwork.AcknowledgementsA special thanks is owned to Myle Ott for sug-gestions on bootstrapping procedure in data har-vesting.
The authors want to thank Noah Smith,Chris Dyer and Alok Kothari for useful com-ments, discussions and suggestions regarding dif-ferent steps of the system and evaluations.
Wethank Lingpeng Kong and members of Noah?sARK group at CMU for providing the tweet de-pendency parser.
All data used in this work is ex-tracted from CMU Twitter Warehouse maintainedby Brendan O?Connor, to whom we want to ex-press our gratitude.17We view weakly supervised life event property extrac-tion as an interesting direction for future work.2005ReferencesHila Becker, Mor Naaman, and Luis Gravano.
2011.Beyond trending topics: Real-world event identifi-cation on twitter.
ICWSM, 11:438?441.Cosmin Adrian Bejan, Matthew Titsworth, AndrewHickl, and Sanda M Harabagiu.
2009.
Nonparamet-ric bayesian models for unsupervised event corefer-ence resolution.
In NIPS, pages 73?81.Edward Benson, Aria Haghighi, and Regina Barzilay.2011.
Event discovery in social media feeds.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 389?398.
As-sociation for Computational Linguistics.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet allocation.
the Journal of ma-chine Learning research, 3:993?1022.Morgane Ciot, Morgan Sonderegger, and Derek Ruths.2013.
Gender inference of twitter users in non-english contexts.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, Seattle, Wash, pages 18?21.Mark Craven, Johan Kumlien, et al.
1999.
Construct-ing biological knowledge bases by extracting infor-mation from text sources.
In ISMB, volume 1999,pages 77?86.Dmitry Davidov, Ari Rappoport, and Moshe Koppel.2007.
Fully unsupervised discovery of concept-specific relationships by web mining.
In AnnualMeeting-Association For Computational Linguis-tics, volume 45, page 232.Qiming Diao and Jing Jiang.
2013.
A unified modelfor topics, events and users on twitter.
In Proceed-ings of the 2013 Conference on Empirical Methodsin Natural Language Processing, pages 1869?1879.Qiming Diao, Jing Jiang, Feida Zhu, and Ee-PengLim.
2012.
Finding bursty topics from microblogs.In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics: LongPapers-Volume 1, pages 536?544.
Association forComputational Linguistics.Thomas L Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the Nationalacademy of Sciences of the United States of Amer-ica, 101(Suppl 1):5228?5235.Amit Gruber, Yair Weiss, and Michal Rosen-Zvi.2007.
Hidden topic markov models.
In Inter-national Conference on Artificial Intelligence andStatistics, pages 163?170.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S Weld.
2011.
Knowledge-based weak supervision for information extractionof overlapping relations.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies-Volume 1, pages 541?550.
Association for Compu-tational Linguistics.Sean P Igo and Ellen Riloff.
2009.
Corpus-based se-mantic lexicon induction with web-based corrobora-tion.
In Proceedings of the Workshop on Unsuper-vised and Minimally Supervised Learning of LexicalSemantics, pages 18?26.
Association for Computa-tional Linguistics.Thorsten Joachims.
1999.
Making large scale svmlearning practical.Lingpeng Kong, Nathan Schneider, SwabhaSwayamdipta, Archna Bhatia, Chris Dyer, andNoah Smith.
2014.
A dependency parser for tweets.In EMNLP.Zornitsa Kozareva and Eduard Hovy.
2010a.
Learn-ing arguments and supertypes of semantic relationsusing recursive patterns.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 1482?1491.
Associationfor Computational Linguistics.Zornitsa Kozareva and Eduard Hovy.
2010b.
Notall seeds are equal: Measuring the quality of textmining seeds.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 618?626.
Association for Computa-tional Linguistics.Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy.2008.
Semantic class learning from the web withhyponym pattern linkage graphs.
In ACL, volume 8,pages 1048?1056.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.Jiwei Li and Claire Cardie.
2013.
Early stageinfluenza detection from twitter.
arXiv preprintarXiv:1309.7340.Jiwei Li and Claire Cardie.
2014.
Timeline generation:Tracking individuals on twitter.
WWW, 2014.Jiwei Li, Alan Ritter, and Eduard Hovy.
2014.Weakly supervised user profile extraction from twit-ter.
ACL.Yucheng Low, Deepak Agarwal, and Alexander JSmola.
2011.
Multiple domain user personaliza-tion.
In Proceedings of the 17th ACM SIGKDD in-ternational conference on Knowledge discovery anddata mining, pages 123?131.
ACM.David Mimno, Hanna M Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.Optimizing semantic coherence in topic models.
In2006Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 262?272.
Association for Computational Linguistics.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 1003?1011.
Association forComputational Linguistics.David Newman, Jey Han Lau, Karl Grieser, and Tim-othy Baldwin.
2010.
Automatic evaluation oftopic coherence.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 100?108.
Association for Computa-tional Linguistics.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL-HLT, pages 380?390.Ellen Riloff, Rosie Jones, et al.
1999.
Learning dic-tionaries for information extraction by multi-levelbootstrapping.
In AAAI/IAAI, pages 474?479.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Un-supervised modeling of twitter conversations.Alan Ritter, Sam Clark, Oren Etzioni, et al.
2011.Named entity recognition in tweets: an experimentalstudy.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1524?1534.
Association for Computational Linguis-tics.Alan Ritter, Oren Etzioni, Sam Clark, et al.
2012.Open domain event extraction from twitter.
In Pro-ceedings of the 18th ACM SIGKDD internationalconference on Knowledge discovery and data min-ing, pages 1104?1112.
ACM.Kirk Roberts and Sanda M Harabagiu.
2011.
Unsuper-vised learning of selectional restrictions and detec-tion of argument coercions.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 980?990.
Association forComputational Linguistics.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquake shakes twitter users: real-timeevent detection by social sensors.
In Proceedingsof the 19th international conference on World wideweb, pages 851?860.
ACM.Roser Saur??
and James Pustejovsky.
2007.
Deter-mining modality and factuality for text entailment.In Semantic Computing, 2007.
ICSC 2007.
Interna-tional Conference on, pages 509?516.
IEEE.Roser Saur??
and James Pustejovsky.
2009.
Factbank:A corpus annotated with event factuality.
Languageresources and evaluation, 43(3):227?268.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y Ng.
2008.
Cheap and fast?but is itgood?
: evaluating non-expert annotations for natu-ral language tasks.
In Proceedings of the conferenceon empirical methods in natural language process-ing, pages 254?263.
Association for ComputationalLinguistics.Limin Yao, David Mimno, and Andrew McCallum.2009.
Efficient methods for topic model inferenceon streaming document collections.Limin Yao, Aria Haghighi, Sebastian Riedel, and An-drew McCallum.
2011.
Structured relation discov-ery using generative models.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 1456?1466.
Associationfor Computational Linguistics.2007
