Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 402?411, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLocally Training the Log-Linear Model for SMTLemao Liu1, Hailong Cao1, Taro Watanabe2, Tiejun Zhao1, Mo Yu1, CongHui Zhu11School of Computer Science and TechnologyHarbin Institute of Technology, Harbin, China2National Institute of Information and Communication Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan{lmliu,hailong,tjzhao,yumo,chzhu}@mtlab.hit.edu.cntaro.watanabe@nict.go.jpAbstractIn statistical machine translation, minimumerror rate training (MERT) is a standardmethod for tuning a single weight with regardto a given development data.
However, due tothe diversity and uneven distribution of sourcesentences, there are two problems suffered bythis method.
First, its performance is highlydependent on the choice of a development set,which may lead to an unstable performancefor testing.
Second, translations become in-consistent at the sentence level since tuning isperformed globally on a document level.
Inthis paper, we propose a novel local trainingmethod to address these two problems.
Un-like a global training method, such as MERT,in which a single weight is learned and usedfor all the input sentences, we perform trainingand testing in one step by learning a sentence-wise weight for each input sentence.
We pro-pose efficient incremental training methods toput the local training into practice.
In NISTChinese-to-English translation tasks, our lo-cal training method significantly outperformsMERT with the maximal improvements up to2.0 BLEU points, meanwhile its efficiency iscomparable to that of the global method.1 IntroductionOch and Ney (2002) introduced the log-linear modelfor statistical machine translation (SMT), in whichtranslation is considered as the following optimiza-tion problem:e?
(f ;W ) = arg maxeP(e|f ;W )= arg maxeexp{W ?
h(f, e)}?e?
exp{W ?
h(f, e?
)}= arg maxe{W ?
h(f, e)}, (1)where f and e (e?)
are source and target sentences,respectively.
h is a feature vector which is scaledby a weight W .
Parameter estimation is one ofthe most important components in SMT, and var-ious training methods have been proposed to tuneW .
Some methods are based on likelihood (Och andNey, 2002; Blunsom et al2008), error rate (Och,2003; Zhao and Chen, 2009; Pauls et al2009; Gal-ley and Quirk, 2011), margin (Watanabe et al2007;Chiang et al2008) and ranking (Hopkins and May,2011), and among which minimum error rate train-ing (MERT) (Och, 2003) is the most popular one.All these training methods follow the samepipeline: they train only a single weight on a givendevelopment set, and then use it to translate all thesentences in a test set.
We call them a global train-ing method.
One of its advantages is that it allows usto train a single weight offline and thereby it is effi-cient.
However, due to the diversity and uneven dis-tribution of source sentences(Li et al2010), thereare some shortcomings in this pipeline.Firstly, on the document level, the performance ofthese methods is dependent on the choice of a devel-opment set, which may potentially lead to an unsta-ble translation performance for testing.
As referredin our experiment, the BLEU points on NIST08 are402Source  Candidate Translationiif  jije  h  score1 ?
?
??
?
1 I am students .
<2, 1> 0.52 I was students .
<1,1> 0.22 ??
??
?
?
1 week several today ?
<1,2> 0.32 today several weeks .
<3,2> 0.1(a) (b)2 21 2 222,0 ( , ) ( , )h f e h f e?
?
??
?2 22 2 212,0 ( , ) ( , )h f e h f e?
??
?1 11 1 11, 0 ( , ) ( , )h f e h f e?
??
?1 12 1 111,0 ( , ) ( , )h f e h f e?
?
??
?2 22 2 21( , ) ( , )h f e h f e?1 11 1 12( , ) ( , )h f e h f e?<-2,0><-1,0><1,0><2,0>0h1h.
.
* *2 21 2 22( , ) ( , )h f e h f e?1 12 1 11( , ) ( , )h f e h f e?Figure 1: (a).
An Example candidate space of dimensionality two.
score is a evaluation metric of e. (b).
The non-linearly separable classification problem transformed from (a) via tuning as ranking (Hopkins and May, 2011).
Sincescore of e11 is greater than that of e12, ?1, 0?
corresponds to a possitive example denoted as ??
?, and ?
?1, 0?
corre-sponds to a negative example denoted as ?*?.
Since the transformed classification problem is not linearly separable,there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.
However, one canobtain e11 and e21 with weights: ?1, 1?
and ?
?1, 1?, respectively.19.04 when the Moses system is tuned on NIST02by MERT.
However, its performance is improved to21.28 points when tuned on NIST06.
The automaticselection of a development set may partially addressthe problem.
However it is inefficient since tuningrequires iteratively decoding an entire developmentset, which is impractical for an online service.Secondly, translation becomes inconsistent on thesentence level (Ma et al2011).
Global trainingmethod such as MERT tries to optimize the weighttowards the best performance for the whole set, andit can not necessarily always obtain good translationfor every sentence in the development set.
The rea-son is that different sentences may need differentoptimal weights, and MERT can not find a singleweight to satisfy all of the sentences.
Figure 1(a)shows such an example, in which a development setcontains two sentences f1 and f2 with translations eand feature vectors h. When we tune examples inFigure 1(a) by MERT, it can be regarded as a non-linearly separable classification problem illustratedin Figure 1(b).
Therefore, there exists no singleweightW which simultaneously obtains e11 and e21as translation for f1 and f2 via Equation (1).
How-ever, we can achieve this with two weights: ?1, 1?for f1 and ?
?1, 1?
for f2.In this paper, inspired by KNN-SVM (Zhang etal., 2006), we propose a local training method,which trains sentence-wise weights instead of a sin-gle weight, to address the above two problems.Compared with global training methods, such asMERT, in which training and testing are separated,our method works in an online fashion, in whichtraining is performed during testing.
This onlinefashion has an advantage in that it can adapt theweights for each of the test sentences, by dynam-ically tuning the weights on translation exampleswhich are similar to these test sentences.
Similarto the method of development set automatical selec-tion, the local training method may also suffer theproblem of efficiency.
To put it into practice, wepropose incremental training methods which avoidretraining and iterative decoding on a developmentset.Our local training method has two advantages:firstly, it significantly outperforms MERT, especiallywhen test set is different from the development set;secondly, it improves the translation consistency.Experiments on NIST Chinese-to-English transla-tion tasks show that our local training method sig-nificantly gains over MERT, with the maximum im-provements up to 2.0 BLEU, and its efficiency iscomparable to that of the global training method.2 Local Training and TestingThe local training method (Bottou and Vapnik,1992) is widely employed in computer vision(Zhang et al2006; Cheng et al2010).
Comparedwith the global training method which tries to fita single weight on the training data, the local onelearns weights based on the local neighborhood in-formation for each test example.
It is superior to403the global one when the data sets are not evenlydistributed (Bottou and Vapnik, 1992; Zhang et al2006).Algorithm 1 Naive Local Training MethodInput: T = {ti}Ni=1(test set), K (retrieval size),Dev(development set), D(retrieval data)Output: Translation results of T1: for all sentence ti such that 1 ?
i ?
N do2: Retrieve the training examples Di with sizeK for ti from D according to a similarity;3: Train a local weight W i based on Dev andDi;4: Decode ti with W i;5: end forSuppose T be a test set, Dev a development set,and D a retrieval data.
The local training in SMTis described in the Algorithm 1.
For each sentenceti in test set, training examples Di is retrieved fromD using a similarity measure (line 2), a weight W iis optimized on Dev and Di (line 3)1, and, finally,ti is decoded with W i for testing (line 4).
At theend of this algorithm, it returns the translation re-sults for T .
Note that weights are adapted for eachtest sentence ti in line 3 by utilizing the translationexamples Di which are similar to ti.
Thus, our localtraining method can be considered as an adaptationof translation weights.Algorithm 1 suffers a problem of training effi-ciency in line 3.
It is impractical to train a weightW i on Dev and Di from scratch for every sen-tence, since iteratively decodingDev andDi is timeconsuming when we apply MERT.
To address thisproblem, we propose a novel incremental approachwhich is based on a two-phase training.On the first phase, we use a global trainingmethod, like MERT, to tune a baseline weight onthe development set Dev in an offline manner.
Onthe second phase, we utilize the retrieved examplesto incrementally tune sentence-wise local weightsbased on the baseline weight.
This method cannot only consider the common characteristics learntfrom the Dev, but also take into account the knowl-1Usually, the quality of development set Dev is high, sinceit is manually produced with multiple references.
This is themain reason why Dev is used as a part of new development setto train W i.edge for each individual sentence learnt from sim-ilar examples during testing.
On the phase of in-cremental training, we perform decoding only oncefor retrieved examples Di, though several rounds ofdecoding are possible and potentially better if onedoes not seriously care about training speed.
Fur-thermore, instead of on-the-fly decoding, we decodethe retrieval data D offline using the parameter fromour baseline weight and its nbest translation candi-dates are saved with training examples to increasethe training efficiency.Algorithm 2 Local Training Method Based on In-cremental TrainingInput: T = {ti}Ni=1 (test set), K (retrieval size),Dev (development set),D = {?fs, rs?
}s=Ss=1 (retrieval data),Output: Translation results of T1: Run global Training (such as MERT) on Dev toget a baseline weight Wb; // Phase 12: Decode each sentence in D to getD = {?fs, cs, rs?
}s=Ss=1 ;3: for all sentence ti such that 1 ?
i ?
N do4: Retrieve K training examples Di ={?f ij , cij , rij?
}j=Kj=1 for ti from D according toa similarity;5: Incrementally train a local weight W i basedon Wb and Di; // Phase 26: Decode ti with W i;7: end forThe two-phase local training algorithm is de-scribed in Algorithm 2, where cs and rs denote thetranslation candidate set and reference set for eachsentence fs in retrieval data, respectively, and K isthe retrieval size.
It globally trains a baseline weightWb (line 1), and decodes each sentence in retrievaldata D with the weight Wb (line 2).
For each sen-tence ti in test set T , it first retrieves training exam-ples Di from D (line 4), and then it runs local train-ing to tune a local weight W i (line 5) and performstesting with W i for ti (line 6).
Please note that thetwo-phase training contains global training in line 1and local training in line 5.From Algorithm 2, one can see that our method iseffective even if the test set is unknow, for example,in the scenario of online translation services, sincethe global training on development set and decoding404on retrieval data can be performed offline.In the next two sections, we will discuss the de-tails about the similarity metric in line 4 and the in-cremental training in line 5 of Algorithm 2.3 Acquiring Training ExamplesIn line 4 of Algorithm 2, to retrieve training exam-ples for the sentence ti , we first need a metric toretrieve similar translation examples.
We assumethat the metric satisfy the property: more similar thetest sentence and translation examples are, the bettertranslation result one obtains when decoding the testsentence with the weight trained on the translationexamples.The metric we consider here is derived froman example-based machine translation.
To retrievetranslation examples for a test sentence, (Watanabeand Sumita, 2003) defined a metric based on thecombination of edit distance and TF-IDF (Manningand Schu?tze, 1999) as follows:dist(f1, f2) = ?
?
edit-dist(f1, f2)+(1?
?)?
tf-idf(f1, f2), (2)where ?
(0 ?
?
?
1) is an interpolation weight,fi(i = 1, 2) is a word sequence and can be alsoconsidered as a document.
In this paper, we extractsimilar examples from training data.
Like example-based translation in which similar source sentenceshave similar translations, we assume that the optimaltranslation weights of the similar source sentencesare closer.4 Incremental Training Based onUltraconservative UpdateCompared with retraining mode, incremental train-ing can improve the training efficiency.
In the fieldof machine learning research, incremental traininghas been employed in the work (Cauwenberghs andPoggio, 2001; Shilton et al2005), but there is lit-tle work for tuning parameters of statistical machinetranslation.
The biggest difficulty lies in that the fea-ture vector of a given training example, i.e.
transla-tion example, is unavailable until actually decodingthe example, since the derivation is a latent variable.In this section, we will investigate the incrementaltraining methods in SMT scenario.Following the notations in Algorithm 2, Wb isthe baseline weight, Di = {?f ij , cij , rij?
}Kj=1 denotestraining examples for ti.
For the sake of brevity, wewill drop the index i, Di = {?fj , cj , rj?
}Kj=1, in therest of this paper.
Our goal is to find an optimalweight, denoted by W i, which is a local weight andused for decoding the sentence ti.
Unlike the globalmethod which performs tuning on the whole devel-opment set Dev +Di as in Algorithm 1, W i can beincrementally learned by optimizing onDi based onWb.
We employ the idea of ultraconservative update(Crammer and Singer, 2003; Crammer et al2006)to propose two incremental methods for local train-ing in Algorithm 2 as follows.Ultraconservative update is an efficient way toconsider the trade-off between the progress made ondevelopment set Dev and the progress made on Di.It desires that the optimal weight W i is not onlyclose to the baseline weight Wb, but also achievesthe low loss over the retrieved examples Di.
Theidea of ultraconservative update can be formalizedas follows:minW{d(W,Wb) + ?
?
Loss(Di,W )}, (3)where d(W,Wb) is a distance metric over a pairof weights W and Wb.
It penalizes the weightsfar away from Wb and it is L2 norm in this paper.Loss(Di,W ) is a loss function of W defined on Diand it evaluates the performance of W over Di.
?is a positive hyperparameter.
If Di is more similarto the test sentence ti, the better performance will beachieved for the larger ?.
In particular, ifDi consistsof only a single sentence ti, the best performancewill be obtained when ?
goes to infinity.4.1 Margin Based Ultraconservative UpdateMIRA(Crammer and Singer, 2003; Crammer et al2006) is a form of ultraconservative update in (3)whoseLoss is defined as hinge loss based on marginover the pairwise translation candiates in Di.
It triesto minimize the following quadratic program:12||W ?Wb||2+?KK?j=1max1?n?|cj |(`jn?W ?
?h(fj , ejn))with?h(fj , ejn) = h(fj , ej?)?
h(fj , ejn), (4)405where h(fj , e) is the feature vector of candidate e,ejn is a translation member of fj in cj , ej?
is theoracle one in cj , `jn is a loss between ej?
and ejnand it is the same as referred in (Chiang et al2008),and |cj | denotes the number of members in cj .Different from (Watanabe et al2007; Chianget al2008) employing the MIRA to globally trainSMT, in this paper, we apply MIRA as one of localtraining method for SMT and we call it as marginbased ultraconservative update (MBUU for shortly)to highlight its advantage of incremental training inline 5 of Algorithm 2.Further, there is another difference betweenMBUU and MIRA in (Watanabe et al2007; Chi-ang et al2008).
MBUU is a batch update modewhich updates the weight with all training examples,but MIRA is an online one which updates with eachexample (Watanabe et al2007) or part of examples(Chiang et al2008).
Therefore, MBUU is more ul-traconservative.4.2 Error Rate Based UltraconservativeUpdateInstead of taking into account the margin-basedhinge loss between a pair of translations as the Lossin (3), we directly optimize the error rate of trans-lation candidates with respect to their references inDi.
Formally, the objective function of error ratebased ultraconservative update (EBUU) is as fol-lows:12?W ?Wb?2 +?KK?j=1Error(rj ; e?
(fj ;W )), (5)where e?
(fj ;W ) is defined in Equation (1), andError(rj , e) is the sentence-wise minus BLEU (Pa-pineni et al2002) of a candidate e with respect torj .Due to the existence of L2 norm in objectivefunction (5), the optimization algorithm MERT cannot be applied for this question since the exact linesearch routine does not hold here.
Motivated by(Och, 2003; Smith and Eisner, 2006), we approxi-mate the Error in (5) by the expected loss, and thenderive the following function:12?W?Wb?2+?KK?j=1?eError(rj ; e)P?
(e|fj ;W ),(6)Systems NIST02 NIST05 NIST06 NIST08Moses 30.39 26.31 25.34 19.07Moses hier 33.68 26.94 26.28 18.65In-Hiero 31.24 27.07 26.32 19.03Table 1: The performance comparison of the baseline In-Hiero VS Moses and Moses hier.withP?
(e|fj ;W ) =exp[?W ?
h(fj , e)]?e?
?cj exp[?W ?
h(fj , e?
)], (7)where ?
> 0 is a real number valued smoother.
Onecan see that, in the extreme case, for ?
?
?, (6)converges to (5).We apply the gradient decent method to minimizethe function (6), as it is smooth with respect to ?.Since the function (6) is non-convex, the solutionobtained by gradient descent method may depend onthe initial point.
In this paper, we set the initial pointas Wb in order to achieve a desirable solution.5 Experiments and Results5.1 SettingWe conduct our experiments on the Chinese-to-English translation task.
The training data is FBIScorpus consisting of about 240k sentence pairs.
Thedevelopment set is NIST02 evaluation data, and thetest datasets are NIST05, NIST06,and NIST08.We run GIZA++ (Och and Ney, 2000) on thetraining corpus in both directions (Koehn et al2003) to obtain the word alignment for each sen-tence pair.
We train a 4-gram language model onthe Xinhua portion of the English Gigaword cor-pus using the SRILM Toolkits (Stolcke, 2002) withmodified Kneser-Ney smoothing (Chen and Good-man, 1998).
In our experiments the translation per-formances are measured by case-insensitive BLEU4metric (Papineni et al2002) and we use mteval-v13a.pl as the evaluation tool.
The significance test-ing is performed by paired bootstrap re-sampling(Koehn, 2004).We use an in-house developed hierarchicalphrase-based translation (Chiang, 2005) as our base-line system, and we denote it as In-Hiero.
To ob-tain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-406Methods Steps SecondsGlobal method Decoding 2.0Local method Retrieval +0.6Local training +0.3Table 2: The efficiency of the local training and testingmeasured by sentence averaged runtime.Methods NIST05 NIST06 NIST08Global MERT 27.07 26.32 19.03Local MBUU 27.75+ 27.88+ 20.84+EBUU 27.85+ 27.99+ 21.08+Table 3: The performance comparison of local train-ing methods (MBUU and EBUU) and a global method(MERT).
NIST05 is the set used to tune ?
for MBUU andEBUU, and NIST06 and NIST08 are test sets.
+ meansthe local method is significantly better than MERT withp < 0.05.lect the best-performing one as our baseline for thefollowing experiments.
As Table 1 indicates, ourbaseline In-Hiero is comparable to the phrase-basedMT (Moses) and the hierarchical phrase-based MT(Moses hier) implemented in Moses, an open sourceMT toolkit2 (Koehn et al2007).
Both of these sys-tems are with default setting.
All three systems aretrained by MERT with 100 best candidates.To compare the local training method in Algo-rithm 2, we use a standard global training method,MERT, as the baseline training method.
We do notcompare with Algorithm 1, in which retraining isperformed for each input sentence, since retrainingfor the whole test set is impractical given that eachsentence-wise retraining may take some hours oreven days.
Therefore, we just compare Algorithm2 with MERT.5.2 Runtime ResultsTo run the Algorithm 2, we tune the baseline weightWb on NIST02 by MERT3.
The retrieval data is setas the training data, i.e.
FBIS corpus, and the re-trieval size is 100.
We translate retrieval data withWb to obtain their 100 best translation candidates.We use the simple linear interpolated TF-IDF met-ric with ?
= 0.1 in Section 3 as the retrieval metric.2See web: http://www.statmt.org3Wb is exactly the weight of In-Hiero in Table 1.NIST05 NIST06 NIST08NIST02 0.665 0.571 0.506Table 4: The similarity of development and three testdatasets.For an efficient tuning, the retrieval process is par-allelized as follows: the examples are assigned to 4CPUs so that each CPU accepts a query and returnsits top-100 results, then all these top-100 results aremerged into the final top-100 retrieved examples to-gether with their translation candidates.
In our ex-periments, we employ the two incremental trainingmethods, i.e.
MBUU and EBUU.
Both of the hyper-parameters ?
are tuned on NIST05 and set as 0.018and 0.06 for MBUU and EBUU, respectively.
Inthe incremental training step, only one CPU is em-ployed.Table 2 depicts that testing each sentence with lo-cal training method takes 2.9 seconds, which is com-parable to the testing time 2.0 seconds with globaltraining method4.
This shows that the local methodis efficient.
Further, compared to the retrieval, thelocal training is not the bottleneck.
Actually, if weuse LSH technique (Andoni and Indyk, 2008) in re-trieval process, the local method can be easily scaledto a larger training data.5.3 Results and AnalysisTable 3 shows the main results of our local train-ing methods.
The EBUU training method signifi-cantly outperforms the MERT baseline, and the im-provement even achieves up to 2.0 BLEU points onNIST08.
We can also see that EBUU and MBUU arecomparable on these three test sets.
Both of thesetwo local training methods achieve significant im-provements over the MERT baseline, which provesthe effectiveness of our local training method overglobal training method.Although both local methods MBUU and EBUUachieved improvements on all the datasets, theirgains on NIST06 and NIST08 are significantlyhigher than those achieved on NIST05 test dataset.We conjecture that, the more different a test set anda development set are, the more potential improvem-4The runtime excludes the time of tuning and decoding on Din Algorithm 2, since both of them can be performanced offline.4070 .
0 0 0 .
0 2 0 .
0 4 0 .
0 6 0 .
0 8 0 .
1 01 82 02 22 42 62 8N I S T 0 5 N I S T 0 6 N I S T 0 8BLEU lFigure 2: The peformance of EBUU for different ?
overall the test datasets.
The horizontal axis denotes the val-ues of ?
in function (6), and the vertical one denotes theBLEU points.Metthods Dev NIST08NIST02 19.03MERT NIST05 20.06NIST06 21.28EBUU NIST02 21.08Table 5: The comparison of MERT with different de-velopment datasets and local training method based onEBUU.nts local training has for the sentences in this test set.To test our hypothesis, we measured the similaritybetween the development set and a test set by theaverage value5 of accumulated TF-IDF scores of de-velopment dataset and each sentence in test datasets.Table 4 shows that NIST06 and NIST08 are moredifferent from NIS02 than NIST05, thus, this is po-tentially the reason why local training is more effec-tive on NIST06 and NIST08.As mentioned in Section 1, the global trainingmethods such as MERT are highly dependent on de-velopment sets, which can be seen in Table 5.
There-fore, the translation performance will be degraded ifone chooses a development data which is not close5Instead of using the similarity between two documents de-velopment and test datasets, we define the similarity as the av-erage similarity of the development set and the sentences in testset.
The reason is that it reduces its dependency on the numberof sentences in test dataset, which may cause a bias.Methods Number PercentsMERT 1735 42.3%EBUU 1606 39.1%Table 6: The statistics of sentences with 0.0 sentence-level BLEU points over three test datasets.to the test data.
We can see that, with the help of thelocal training, we still gain much even if we selectedan unsatisfactory development data.As also mentioned in Section 1, the global meth-ods do not care about the sentence level perfor-mance.
Table 6 depicts that there are 1735 sentenceswith zero BLEU points in all the three test datasetsfor MERT.
Besides obtaining improvements on doc-ument level as referred in Table 3, the local trainingmethods can also achieve consistent improvementson sentence level and thus can improve the users?experiences.The hyperparameters ?
in both MBUU (4) andEBUU (6) has an important influence on transla-tion performance.
Figure 2 shows such influencefor EBUU on the test datasets.
We can see that, theperformances on all these datasets improve as ?
be-comes closer to 0.06 from 0, and the performancecontinues improving when ?
passes over 0.06 onNIST08 test set, where the performance constantlyimproves up to 2.6 BLEU points over baseline.
Asmentioned in Section 4, if the retrieved examples arevery similar to the test sentence, the better perfor-mance will be achieved with the larger ?.
There-fore, it is reasonable that the performances improvedwhen ?
increased from 0 to 0.06.
Further, the turn-ing point appearing at 0.06 proves that the ultra-conservative update is necessary.
We can also seethat the performance on NIST08 consistently im-proves and achieves the maximum gain when ?
ar-rives at 0.1, but those on both NIST05 and NIST06achieves the best when it arrives at 0.06.
Thisphenomenon can also be interpreted in Table 4 asthe lowest similarity between the development andNIST08 datasets.Generally, the better performance may beachieved when more examples are retrieved.
Actu-ally, in Table 7 there seems to be little dependencybetween the numbers of examples retrieved and thetranslation qualities, although they are positively re-408Retrieval Size NIST05 NIST06 NIST0840 27.66 27.81 20.8770 27.77 27.93 21.08100 27.85 27.99 21.08Table 7: The performance comparison by varying re-trieval size in Algorithm 2 based on EBUU.Methods NIST05 NIST06 NIST08MERT 27.07 26.32 19.03EBUU 27.85 27.99 21.08Oracle 29.46 29.35 22.09Table 8: The performance of Oracle of 2-best resultswhich consist of 1-best resluts of MERT and 1-bestresluts of EBUU.lated approximately.Table 8 presents the performance of the oracletranslations selected from the 1-best translation re-sults of MERT and EBUU.
Clearly, there exists morepotential improvement for local training method.6 Related WorkSeveral works have proposed discriminative tech-niques to train log-linear model for SMT.
(Och andNey, 2002; Blunsom et al2008) used maximumlikelihood estimation to learn weights for MT.
(Och,2003; Moore and Quirk, 2008; Zhao and Chen,2009; Galley and Quirk, 2011) employed an eval-uation metric as a loss function and directly opti-mized it.
(Watanabe et al2007; Chiang et al2008;Hopkins and May, 2011) proposed other optimiza-tion objectives by introducing a margin-based andranking-based indirect loss functions.All the methods mentioned above train a singleweight for the whole development set, whereas ourlocal training method learns a weight for each sen-tence.
Further, our translation framework integratesthe training and testing into one unit, instead of treat-ing them separately.
One of the advantages is that itcan adapt the weights for each of the test sentences.Our method resorts to some translation exam-ples, which is similar as example-based translationor translation memory (Watanabe and Sumita, 2003;He et al2010; Ma et al2011).
Instead of usingtranslation examples to construct translation rulesfor enlarging the decoding space, we employed themto discriminatively learn local weights.Similar to (Hildebrand et al2005; Lu?
et al2007), our method also employes IR methods to re-trieve examples for a given test set.
Their methodsutilize the retrieved examples to acquire translationmodel and can be seen as the adaptation of trans-lation model.
However, ours uses the retrieved ex-amples to tune the weights and thus can be consid-ered as the adaptation of tuning.
Furthermore, sinceours does not change the translation model whichneeds to run GIZA++ and it incrementally trains lo-cal weights, our method can be applied for onlinetranslation service.7 Conclusion and Future WorkThis paper proposes a novel local training frame-work for SMT.
It has two characteristics, whichare different from global training methods such asMERT.
First, instead of training only one weight fordocument level, it trains a single weight for sentencelevel.
Second, instead of considering the trainingand testing as two separate units, we unify the train-ing and testing into one unit, which can employ theinformation of test sentences and perform sentence-wise local adaptation of weights.Local training can not only alleviate the prob-lem of the development data selection, but also re-duce the risk of sentence-wise bad translation re-sults, thus consistently improve the translation per-formance.
Experiments show gains up to 2.0 BLEUpoints compared with a MERT baseline.
With thehelp of incremental training methods, the time in-curred by local training was negligible and the localtraining and testing totally took 2.9 seconds for eachsentence.In the future work, we will further investigate thelocal training method, since there are more room forimprovements as observed in our experiments.
Wewill test our method on other translation models andlarger training data6.AcknowledgmentsWe would like to thank Hongfei Jiang and ShujieLiu for many valuable discussions and thank three6Intuitionally, when the corpus of translation examples islarger, the retrieval results in Algorithm 2 are much similar asthe test sentence.
Therefore our method may favor this.409anonymous reviewers for many valuable commentsand helpful suggestions.
This work was supportedby National Natural Science Foundation of China(61173073,61100093), and the Key Project of theNational High Technology Research and Develop-ment Program of China (2011AA01A207), and theFundamental Research Funds for Central Univer-sites (HIT.NSRIF.2013065).ReferencesAlexandr Andoni and Piotr Indyk.
2008.
Near-optimalhashing algorithms for approximate nearest neighborin high dimensions.
Commun.
ACM, 51(1):117?122,January.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisti-cal machine translation.
In Proceedings of ACL,pages 200?208, Columbus, Ohio, June.
Associationfor Computational Linguistics.Le?on Bottou and Vladimir Vapnik.
1992.
Local learningalgorithms.
Neural Comput., 4:888?900, November.G.
Cauwenberghs and T. Poggio.
2001.
Incrementaland decremental support vector machine learning.
InAdvances in Neural Information Processing Systems(NIPS*2000), volume 13.Stanley F Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
In Technical Report TR-10-98.
Harvard Univer-sity.Haibin Cheng, Pang-Ning Tan, and Rong Jin.
2010.
Ef-ficient algorithm for localized support vector machine.IEEE Trans.
on Knowl.
and Data Eng., 22:537?549,April.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?08, pages 224?233, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting on Association for Computa-tional Linguistics, ACL ?05, pages 263?270, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Koby Crammer and Yoram Singer.
2003.
Ultraconser-vative online algorithms for multiclass problems.
J.Mach.
Learn.
Res., 3:951?991, March.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
J. Mach.
Learn.
Res., 7:551?585, December.Michel Galley and Chris Quirk.
2011.
Optimal searchfor minimum error rate training.
In Proceedings ofthe 2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 38?49, Edinburgh, Scot-land, UK., July.
Association for Computational Lin-guistics.Yifan He, Yanjun Ma, Josef van Genabith, and AndyWay.
2010.
Bridging smt and tm with translationrecommendation.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 622?630, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.S.
Hildebrand, M. Eck, S. Vogel, and Alex Waibel.
2005.Adaptation of the translation model for statistical ma-chine translation based on information retrieval.
InProceedings of EAMT.
Association for ComputationalLinguistics.Mark Hopkins and Jonathan May.
2011.
Tuning as rank-ing.
In Proceedings of the 2011 Conference on Empir-ical Methods in Natural Language Processing, pages1352?1362, Edinburgh, Scotland, UK., July.
Associa-tion for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HLT-NAACL.
ACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, ACL ?07,pages 177?180, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
of EMNLP.ACL.Mu Li, Yinggong Zhao, Dongdong Zhang, and MingZhou.
2010.
Adaptive development data selection forlog-linear model in statistical machine translation.
InProceedings of the 23rd International Conference onComputational Linguistics, COLING ?10, pages 662?670, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Yajuan Lu?, Jin Huang, and Qun Liu.
2007.
Improvingstatistical machine translation performance by train-ing data selection and optimization.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages410343?350, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-abith.
2011.
Consistent translation using discrim-inative learning - a translation memory-inspired ap-proach.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies, pages 1239?1248, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of statistical natural language process-ing.
MIT Press, Cambridge, MA, USA.Robert C. Moore and Chris Quirk.
2008.
Randomrestarts in minimum error rate training for statisticalmachine translation.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics -Volume 1, COLING ?08, pages 585?592, Stroudsburg,PA, USA.
Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Meeting on Association for Computa-tional Linguistics, ACL ?00, pages 440?447, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native training and maximum entropy models for sta-tistical machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 295?302, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 160?167, Sapporo, Japan,July.
Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Adam Pauls, John Denero, and Dan Klein.
2009.
Con-sensus training for consensus decoding in machinetranslation.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 1418?1427, Singapore, August.
Association forComputational Linguistics.Alistair Shilton, Marimuthu Palaniswami, Daniel Ralph,and Ah Chung Tsoi.
2005.
Incremental training ofsupport vector machines.
IEEE Transactions on Neu-ral Networks, 16(1):114?131.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proc.
of ICSLP.Taro Watanabe and Eiichiro Sumita.
2003.
Example-based decoding for statistical machine translation.
InProc.
of MT Summit IX, pages 410?417.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for sta-tistical machine translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech Republic, June.
Association forComputational Linguistics.Hao Zhang, Alexander C. Berg, Michael Maire, and Ji-tendra Malik.
2006.
Svm-knn: Discriminative near-est neighbor classification for visual category recog-nition.
In Proceedings of the 2006 IEEE ComputerSociety Conference on Computer Vision and PatternRecognition - Volume 2, CVPR ?06, pages 2126?2136,Washington, DC, USA.
IEEE Computer Society.Bing Zhao and Shengyuan Chen.
2009.
A simplexarmijo downhill algorithm for optimizing statisticalmachine translation decoding parameters.
In Proceed-ings of Human Language Technologies: The 2009 An-nual Conference of the North American Chapter of theAssociation for Computational Linguistics, Compan-ion Volume: Short Papers, NAACL-Short ?09, pages21?24, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.411
