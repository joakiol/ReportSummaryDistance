Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 938?943,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsRecurrent Residual Learning for Sequence ClassificationYiren Wang?University of Illinois at Urbana-Champaignyiren@illinois.eduFei TianMicrosoft Researchfetia@microsoft.comAbstractIn this paper, we explore the possibility ofleveraging Residual Networks (ResNet), apowerful structure in constructing extremelydeep neural network for image understanding,to improve recurrent neural networks (RNN)for modeling sequential data.
We show thatfor sequence classification tasks, incorporat-ing residual connections into recurrent struc-tures yields similar accuracy to Long ShortTerm Memory (LSTM) RNN with much fewermodel parameters.
In addition, we proposetwo novel models which combine the bestof both residual learning and LSTM.
Experi-ments show that the new models significantlyoutperform LSTM.1 IntroductionRecurrent Neural Networks (RNNs) are powerfultools to model sequential data.
Among variousRNN models, Long Short Term Memory (LSTM)(Hochreiter and Schmidhuber, 1997) is one of themost effective structures.
In LSTM, gating mech-anism is used to control the information flow suchthat gradient vanishing problem in vanilla RNN isbetter handled, and long range dependency is bet-ter captured.
However, as empirically verified byprevious works and our own experiments, to obtainfairly good results, training LSTM RNN needs care-fully designed optimization procedure (Hochreiteret al, 2001; Pascanu et al, 2013; Dai and Le, 2015;Laurent et al, 2015; He et al, 2016; Arjovsky et?This work was done when the author was visiting Mi-crosoft Research Asia.al., 2015), especially when faced with unfolded verydeep architectures for fairly long sequences (Daiand Le, 2015).From another perspective, for constructing verydeep neural networks, recently Residual Networks(ResNet) (He et al, 2015) have shown their ef-fectiveness in quite a few computer vision tasks.By learning a residual mapping between layers withidentity skip connections (Jaeger et al, 2007),ResNet ensures a fluent information flow, leading toefficient optimization for very deep structures (e.g.,with hundreds of layers).
In this paper, we explorethe possibilities of leveraging residual learning toimprove the performances of recurrent structures, inparticular, LSTM RNN, in modeling fairly long se-quences (i.e., whose lengths exceed 100).
To sum-marize, our main contributions include:1.
We introduce residual connecting mechanisminto the recurrent structure and propose recur-rent residual networks for sequence learning.Our model achieves similar performances toLSTM in text classification tasks, whereas thenumber of model parameters is greatly reduced.2.
We present in-depth analysis of the strengthsand limitations of LSTM and ResNet in respectof sequence learning.3.
Based on such analysis, we further propose twonovel models that incorporate the strengths ofthe mechanisms behind LSTM and ResNet.
Wedemonstrate that our models outperform LSTMin many sequence classification tasks.9382 BackgroundRNN models sequences by taking sequential inputx = {x1, ?
?
?
, xT } and generating T step hiddenstates h = {h1, ?
?
?
, hT }.
At each time step t, RNNtakes the input vector xt ?
Rn and the previous hid-den state vector ht?1 ?
Rm to produce the next hid-den state ht.Based on this basic structure, LSTM avoids gradi-ent vanishing in RNN training and thus copes betterwith long range dependencies, by further augment-ing vanilla RNN with a memory cell vector ct ?
Rmand multiplicative gate units that regulate the infor-mation flow.
To be more specific, at each time stept, an LSTM unit takes xt, ct?1, ht?1 as input, gen-erates the input, output and forget gate signals (de-noted as it, ot and ft respectively), and produces thenext cell state ct and hidden state ht:c?t = tanh(Wc[ht?1, xt] + bc)it = ?
(Wi[ht?1, xt] + bi)ft = ?
(Wf [ht?1, xt] + bf )ot = ?
(Wo[ht?1, xt] + bo)ct = ft ?
Ct?1 + it ?
c?tht = ot ?
tanh(ct)(1)where ?
refers to element-wise product.
?
(x) is thesigmoid function ?
(x) = 1/(1+exp(?x)).
Wj(j ?
{i, o, f, c}) are LSTM parameters.
In the followingpart, such functions generating ht and ct are denotedas ht, ct = LSTM(xt, ht?1, ct?1).Residual Networks (ResNet) are among the pio-neering works (Szegedy et al, 2015; Srivastava etal., 2015) that utilize extra identity connections toenhance information flow such that very deep neuralnetworks can be effectively optimized.
ResNet (Heet al, 2015) is composed of several stacked resid-ual units, in which the lth unit takes the followingtransformation:hl+1 = f(g(hl) + F(hl;Wl)) (2)where hl and hl+1 are the input and output for thelth unit respectively.
F is the residual function withweight parametersWl.
f is typically the ReLU func-tion (Nair and Hinton, 2010).
g is set as identityfunction, i.e., g(hl) = hl.
Such an identity con-nection guarantees the direct propagation of signalsamong different layers, thereby avoids gradient van-ishing.
The recent paper (Liao and Poggio, 2016)talks about the possibility of using shared weights inResNet, similar to what RNN does.3 Recurrent Residual LearningThe basic idea of recurrent residual learning is toforce a direct information flow in different time stepsof RNNs by identity (skip) connections.
In this sec-tion, we introduce how to leverage residual learningto 1) directly construct recurrent neural network insubsection 3.1; 2) improve LSTM in subsection 3.2.3.1 Recurrent Residual Networks (RRN)The basic architecture of Recurrent Residual Net-work (RRN for short) is illustrated in Figure 1, inwhich orange arrows indicate the identity connec-tions from each ht?1 to ht, and blue arrows rep-resent the recurrent transformations taking both htand xt as input.
Similar to equation (2), the recur-rent transformation in RRN takes the following form(denoted as ht = RRN(xt, ht?1) in the followingsections):ht = f(g(ht?1) + F(ht?1, xt;W )), (3)where g is still the identity function s.t.
g(ht?1) =ht?1, corresponding to the orange arrows in Figure1.
f is typically set as tanh.
For function F withweight parameters W (corresponding to the blue ar-rows in Figure 1), inspired by the observation thathigher recurrent depth tends to lead to better perfor-mances (Zhang et al, 2016), we impose K deeptransformations in F :yt1 = ?
(xtW1 + ht?1U1 + b1)yt2 = ?
(xtW2 + yt1U2 + b2)?
?
?ytK = ?
(xtWK + ytK?1UK + bK)F(ht?1, xt) = ytK(4)where xt is taken at every layer such that the inputinformation is better captured, which works simi-larly to the mechanism of highway network (Sri-vastava et al, 2015).
K is the recurrent depth de-fined in (Zhang et al, 2016).
The weights Wm(m ?
{1, ?
?
?
,K}) are shared across different timesteps t.939Figure 1: The basic structure of Recurrent Residual Networks.RRN forces the direct propagation of hidden statesignals between every two consecutive time stepswith identity connections g. In addition, the mul-tiple non-linear transformations in F guarantees itscapability in modelling complicated recurrent rela-tionship.
In practice, we found that K = 2 yieldsfairly good performances, meanwhile leads to halfof LSTM parameter size when model dimensionsare the same.3.2 Gated Residual RNNIdentity connections in ResNet are important forpropagating the single input image information tohigher layers of CNN.
However, when it comes tosequence classification, the scenario is quite differ-ent in that there is a new input at every time step.Therefore, a forgetting mechanism to ?forget?
lesscritical historical information, as is employed inLSTM (controlled by the forget gate ft), becomesnecessary.
On the other hand, while LSTM benefitsfrom the flexible gating mechanism, its parametricnature brings optimization difficulties to cope withfairly long sequences, whose long range informa-tion dependencies could be better captured by iden-tity connections.Inspired by the success of the gating mechanismof LSTM and the residual connecting mechanismwith enhanced information flow of ResNet, we fur-ther propose two Gated Residual Recurrent modelsleveraging the strengths of the two mechanisms.3.2.1 Model 1: Skip-Connected LSTM(SC-LSTM)Skip-Connected LSTM (SC-LSTM for short) in-troduces identity connections into standard LSTM toenhance the information flow.
Note that in Figure 1,a straightforward approach is to replace F with anLSTM unit.
However, our preliminary experimentsdo not achieve satisfactory results.
Our conjecture isthat identity connections between consecutive mem-ory cells, which are already sufficient to maintainshort-range memory, make the unregulated informa-tion flow overly strong, and thus compromise themerit of gating mechanism.To reasonably enhance the information flow forLSTM network while keeping the advantage of gat-ing mechanism, starting from equation (1), we pro-pose to add skip connections between two LSTMhidden states with a wide range of distance L (e.g.,L = 20), such that ?t = {1, 1+L, 1+ 2L, ?
?
?
, 1+bT?L?1L cL}:ht+L = tanh(ct+L)?
ot+L + ?ht (5)Here ?
is a scalar that can either be fixed as 1(i.e., identity mapping) or be optimized during train-ing process as a model parameter (i.e., parametricskip connection).
We refer to these two variants asSC-LSTM-I and SC-LSTM-P respectively.
Notethat in SC-LSTM, the skip connections only existin time steps 1, 1 + L, 1 + 2L, ?
?
?
, 1 + bT?L?1L cL.The basic structure is shown in Figure 2.Figure 2: The basic structure of Skip-Connected LSTM.3.2.2 Model 2: Hybrid Residual LSTM (HRL)Since LSTM generates sequence representationsout of flexible gating mechanism, and RRN gener-ates representations with enhanced residual histori-cal information, it is a natural extension to combinethe two representations to form a signal that bene-fits from both mechanisms.
We denote this model asHybrid Residual LSTM (HRL for short).In HRL, two independent signals, hLSTMt gen-erated by LSTM (equation (1)) and hRRNt gener-ated by RRN (equation (3)), are propagated throughLSTM and RRN respectively:hLSTMt , ct = LSTM(xt, hLSTMt?1 , ct?1)hRRNt = RRN(xt, hRRNt?1 )(6)940The final representation hHRLT is obtained by themean pooling of the two ?sub?
hidden states:hHRLT =12(hLSTMT + hRRNT ) (7)hHRLT is then used for higher level tasks such as pre-dicting the sequence label.
Acting in this way, hHRLTcontains both the statically forced and dynamicallyadjusted historical signals, which are respectivelyconveyed by hRRNt and hLSTMt .4 ExperimentsWe conduct comprehensive empirical analysis onsequence classification tasks.
Listed in the ascend-ing order of average sequence lengths, several publicdatasets we use include:1.
AG?s news corpus1,a news article corpus withcategorized articles from more than 2, 000news sources.
We use the dataset with 4 largestclasses constructed in (Zhang et al, 2015).2.
IMDB movie review dataset2, a binary senti-ment classification dataset consisting of moviereview comments with positive/negative senti-ment labels (Maas et al, 2011).3.
20 Newsgroups (20NG for short), an emailcollection dataset categorized into 20 newsgroups.
Simiar to (Dai and Le, 2015), we usethe post-processed version3, in which attach-ments, PGP keys and some duplicates are re-moved.4.
Permuted-MNIST (P-MNIST for short).
Fol-lowing (Le et al, 2015; Arjovsky et al, 2015),we shuffle pixels of each MNIST image (Le-Cun et al, 1998) with a fixed random per-mutation, and feed all pixels sequentially intorecurrent network to predict the image label.Permuted-MNIST is assumed to be a goodtestbed for measuring the ability of modelingvery long range dependencies (Arjovsky et al,2015).1http://www.di.unipi.it/~gulli/AG corpus of newsarticles.html2http://ai.stanford.edu/~amaas/data/sentiment/3http://ana.cachopo.org/datasets-for-single-label-text-categorizationDetailed statistics of each dataset are listed inTable 1.
For all the text datasets, we take everyword as input and feed word embedding vectorspre-trained by Word2Vec (Mikolov et al, 2013) onWikipedia into the recurrent neural network.
Thetop most frequent words with 95% total frequencycoverage are kept, while others are replaced by thetoken ?UNK?.
We use the standard training/testsplit along with all these datasets and randomlypick 15% of training set as dev set, based on whichwe perform early stopping and for all modelstune hyper-parameters such as dropout ratio (onnon-recurrent layers) (Zaremba et al, 2014),gradient clipping value (Pascanu et al, 2013) andthe skip connection length L for SC-LSTM (cf.equation (5)).
The last hidden states of recurrentnetworks are put into logistic regression classifiersfor label predictions.
We use Adadelta (Zeiler,2012) to perform parameter optimization.
All ourimplementations are based on Theano (Theano De-velopment Team, 2016) and run on one K40 GPU.All the source codes and datasets can be down-loaded at https://publish.illinois.edu/yirenwang/emnlp16source/.We compare our proposed models mainly withthe state-of-art standard LSTM RNN.
In addition, tofully demonstrate the effects of residual learning inour HRL model, we employ another hybrid modelas baseline, which combines LSTM and GRU (Choet al, 2014), another state-of-art RNN variant, in asimilar way as HRL.
We use LSTM+GRU to de-note such a baseline.
The model sizes (word embed-ding size ?
hidden state size) configurations usedfor each dataset are listed in Table 2.
In Table 2,?Non-Hybrid?
refers to LSTM, RRN and SC-LSTMmodels, while ?Hybrid?
refers to two methods thatcombines two basic models: HRL and LSTM+GRU.The model sizes of all hybrid models are smallerthan the standard LSTM.
All models have only onerecurrent layer.4.1 Experimental ResultsAll the classification accuracy numbers are listed inTable 3.
From this table, we have the following ob-servations and analysis:1.
RRN achieves similar performances to stan-dard LSTM in all classification tasks with only941Dataset Ave. Len Max Len #Classes #Train : #TestAG?s News 34 211 4 120, 000 : 7, 600IMDB 281 2, 956 2 25, 000 : 25, 00020NG 267 11, 924 20 11, 293 : 7, 528P-MNIST 784 784 10 60, 000 : 10, 000Table 1: Classification Datasets.AG?s News IMDB 20NG P-MNISTNon-Hybird 256?
512 256?
512 500?
768 1?
100Hybrid 256?
384 256?
384 256?
512 1?
80Table 2: Model Sizes on Different Dataset.Model/Task AG?s News IMDB 20NG P-MNISTLSTM 91.76% 88.88% 79.21% 90.64%RRN 91.19% 89.13% 79.76% 88.63%SC-LSTM-P 92.01% 90.74% 82.98% 94.46%SC-LSTM-I 92.05% 90.67% 81.85% 94.80%LSTM+GRU 91.05% 89.23% 80.12% 90.28%HRL 91.90% 90.92% 81.73% 90.33%Table 3: Classification Results (Test Accuracy).half of the model parameters, indicating thatresidual network structure, with connectingmechanism to enhance the information flow, isalso an effective approach for sequence learn-ing.
However, the fact that it fails to sig-nificantly outperform other models (as it doesin image classification) implies that forgettingmechanism is desired in recurrent structures tohandle multiple inputs.2.
Skip-Connected LSTM performs much betterthan standard LSTM.
For tasks with shorter se-quences such as AG?s News, the improvementis limited.
However, the improvements getmore significant with the growth of sequencelengths among different datasets4, and the per-formance is particularly good in P-MNIST withvery long sequences.
This reveals the impor-tance of skip connections in carrying on histor-ical information through a long range of timesteps, and demonstrates the effectiveness of ourapproach that adopts the residual connectingmechanism to improve LSTM?s capability ofhandling long-term dependency.
Furthermore,SC-LSTM is robust with different hyperparam-4t-test on SC-LSTM-P and SC-LSTM-I with p value <0.001.eter values: we test L = 10, 20, 50, 75 in P-MNIST and find the performance is not sensi-tive w.r.t.
these L values.3.
HRL also outperforms standard LSTM withfewer model parameters5.
In comparison, thehybrid model of LSTM+GRU cannot achievesuch accuracy as HRL.
As we expected, the ad-ditional long range historical information prop-agated by RRN is proved to be good assistanceto standard LSTM.5 ConclusionIn this paper, we explore the possibility of lever-aging residual network to improve the performanceof LSTM RNN.
We show that direct adaptation ofResNet performs well in sequence classification.
Inaddition, when combined with the gating mecha-nism in LSTM, residual learning significantly im-prove LSTM?s performance.
As to future work,we plan to apply residual learning to other se-quence tasks such as language modeling, and RNNbased neural machine translation (Sutskever et al,2014) (Cho et al, 2014).5t-test on HRL with p value < 0.001.942ReferencesMartin Arjovsky, Amar Shah, and Yoshua Bengio.
2015.Unitary evolution recurrent neural networks.
arXivpreprint arXiv:1511.06464.Kyunghyun Cho, Bart Van Merrie?nboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014.
Learn-ing phrase representations using rnn encoder-decoderfor statistical machine translation.
arXiv preprintarXiv:1406.1078.Andrew M Dai and Quoc V Le.
2015.
Semi-supervisedsequence learning.
In Advances in Neural InformationProcessing Systems, pages 3061?3069.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun.
2015.
Deep residual learning for image recogni-tion.
arXiv preprint arXiv:1512.03385.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun.
2016.
Identity mappings in deep residual net-works.
arXiv preprint arXiv:1603.05027.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jr-gen Schmidhuber.
2001.
Gradient flow in recurrentnets: the difficulty of learning long-term dependen-cies.Herbert Jaeger, Mantas Lukos?evic?ius, Dan Popovici, andUdo Siewert.
2007.
Optimization and applicationsof echo state networks with leaky-integrator neurons.Neural Networks, 20(3):335?352.Ce?sar Laurent, Gabriel Pereyra, Phile?mon Brakel, YingZhang, and Yoshua Bengio.
2015.
Batch nor-malized recurrent neural networks.
arXiv preprintarXiv:1510.01378.Quoc V Le, Navdeep Jaitly, and Geoffrey E Hin-ton.
2015.
A simple way to initialize recurrentnetworks of rectified linear units.
arXiv preprintarXiv:1504.00941.Yann LeCun, Le?on Bottou, Yoshua Bengio, and PatrickHaffner.
1998.
Gradient-based learning applied todocument recognition.
Proceedings of the IEEE,86(11):2278?2324.Qianli Liao and Tomaso Poggio.
2016.
Bridging the gapsbetween residual learning, recurrent neural networksand visual cortex.
arXiv preprint arXiv:1604.03640.Andrew L. Maas, Raymond E. Daly, Peter T. Pham, DanHuang, Andrew Y. Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 142?150, Portland, Oregon, USA,June.
Association for Computational Linguistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositionality.In Advances in neural information processing systems,pages 3111?3119.Vinod Nair and Geoffrey E Hinton.
2010.
Rectified lin-ear units improve restricted boltzmann machines.
InProceedings of the 27th International Conference onMachine Learning (ICML-10), pages 807?814.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2013.
On the difficulty of training recurrent neuralnetworks.
In Proceedings of The 30th InternationalConference on Machine Learning, pages 1310?1318.Rupesh K Srivastava, Klaus Greff, and Ju?rgen Schmid-huber.
2015.
Training very deep networks.
InAdvances in Neural Information Processing Systems,pages 2368?2376.Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.Sequence to sequence learning with neural networks.In Advances in neural information processing systems,pages 3104?3112.Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-manet, Scott Reed, Dragomir Anguelov, Dumitru Er-han, Vincent Vanhoucke, and Andrew Rabinovich.2015.
Going deeper with convolutions.
In Proceed-ings of the IEEE Conference on Computer Vision andPattern Recognition, pages 1?9.Theano Development Team.
2016.
Theano: A Pythonframework for fast computation of mathematical ex-pressions.
arXiv e-prints, abs/1605.02688, May.Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.2014.
Recurrent neural network regularization.
arXivpreprint arXiv:1409.2329.Matthew D Zeiler.
2012.
Adadelta: an adaptive learningrate method.
arXiv preprint arXiv:1212.5701.Xiang Zhang, Junbo Zhao, and Yann LeCun.
2015.Character-level convolutional networks for text classi-fication.
In Advances in Neural Information Process-ing Systems, pages 649?657.Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin,Roland Memisevic, Ruslan Salakhutdinov, and YoshuaBengio.
2016.
Architectural complexity mea-sures of recurrent neural networks.
arXiv preprintarXiv:1602.08210.943
