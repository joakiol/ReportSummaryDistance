Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 172?180,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsProbabilistic Word Alignment under the L0-normThomas SchoenemannCenter for Mathematical SciencesLund University, SwedenAbstractThis paper makes two contributions to thearea of single-word based word alignment forbilingual sentence pairs.
Firstly, it integratesthe ?
seemingly rather different ?
works of(Bodrumlu et al, 2009) and the standard prob-abilistic ones into a single framework.Secondly, we present two algorithms to opti-mize the arising task.
The first is an iterativescheme similar to Viterbi training, able to han-dle large tasks.
The second is based on the in-exact solution of an integer program.
While itcan handle only small corpora, it allows moreinsight into the quality of the model and theperformance of the iterative scheme.Finally, we present an alternative way tohandle prior dictionary knowledge and dis-cuss connections to computing IBM-3 Viterbialignments.1 IntroductionThe training of single word based translation mod-els (Brown et al, 1993b; Vogel et al, 1996) is an es-sential building block for most state-of-the-art trans-lation systems.
Indeed, even more refined transla-tion models (Wang and Waibel, 1998; Sumita et al,2004; Deng and Byrne, 2005; Fraser and Marcu,2007a) are initialized by the parameters of singleword based ones.
The exception is here the jointapproach of Marcu and Wong (2002), but its refine-ment by Birch et al (2006) again relies on the well-known IBM models.Traditionally (Brown et al, 1993b; Al-Onaizanet al, 1999) single word based models are trainedby the EM-algorithm, which has the advantageousproperty that the collection of counts can be de-composed over the sentences.
Refinements that alsoallow symmetrized models are based on bipartitegraph matching (Matusov et al, 2004; Taskar et al,2005) or quadratic assignment problems (Lacoste-Julien et al, 2006).
Recently, Bodrumlu et al(2009) proposed the first method that treats a non-decomposable problem by handling all sentencepairs at once and via integer linear programming.Their (non-probabilistic) approach finds dictionarieswith a minimal number of entries.
However, the ap-proach does not include a position model.In this work we combine the two strategies intoa single framework.
That is, the dictionary sparsityobjective of Bodrumlu et al will become a regu-larity term in our framework.
It is combined withthe maximal alignment probability of every sentencepair, where we consider the models IBM-1, IBM-2and HMM.
This allows us to write dictionary spar-sity as the (non-convex) L0 norm of the dictionaryparameters of the respective models.For supervised training, regularity terms are quitecommon, e.g.
(Taskar et al, 2005; Lacoste-Julien etal., 2006).
For the unsupervised problem addressedin this paper they have recently been introduced inthe form of posterior constraints (Ganchev et al,2010).
In related fields of NLP lately Dirichlet pri-ors have been investigated, e.g.
(Johnson, 2007).We present two strategies to handle the objec-tive function addressed in this paper.
One of theseschemes relies, like (Germann et al, 2004; Lacoste-Julien et al, 2006; DeNero and Klein, 2008; Bo-drumlu et al, 2009), on integer linear programming172(see e.g.
(Schrijver, 1986; Achterberg, 2007)), butdue to the large-scale nature of our problem wesolve only the LP-relaxation, followed by successivestrengthening.
For the latter, we develop our own,exponentially large set of cuts and show that it canbe handled as a polynomially sized system, thoughin practice this is too inefficient.2 The ModelsBefore we introduce our objective function we givea brief description of the (standard) models we con-sider.
In all cases, one is given a set of bilin-gual sentence pairs containing a foreign languageand English.
The models formalize the probabil-ity of obtaining the foreign sentence from a givenEnglish sentence, by considering hidden variablescalled alignments:pd,l(fs|es) =?aspd,l(fs,as|es) .Here, the subscripts d and l denote two sets of pa-rameters: whereas the set l defines the probability ofan alignment without knowing any of the sentences,d describes the translational probability given analignment and a source sentence.For a source (English) sentence of length I and atarget (foreign) sentence of length J , the set of ad-missible alignments is generally the set of subsetsof {1, .
.
.
, I} ?
{1, .
.
.
, J}.
However, for compu-tational reasons the considered models only allowrestricted alignments, where each target word mayalign to at most one source word.
Any such align-ment is expressed as a vector aJ1 ?
{0, .
.
.
, I}J .2.1 Considered modelsFor a source sentence es = eI1 and a target sentencef s = fJ1 , the considered models all factor as follows:pd,l(fs,as|es) = (1)J?j=1pd(fj |eaj ) ?
pl(aj |aj?1, j, I)In all cases, the translational probability is non-parametric, i.e.
d contains one parameter for everyco-occurring pair of source and target words.
Sincethe model is probabilistic, the parameters of all f fora given e have to sum up to one.With respect to the alignment probability, themodels differ.
For the IBM-1 the set l is actuallyempty, so pl(aj |aj?1, j, I) = 1/(I+1).
The IBM-2models1 p(aj |j, I), with a respective set of parame-ters.
Finally, the HMM models p(aj |aj?1, I).It is common to further reduce the alignment pa-rameters.
In this paper we consider a nonparametricdistribution for the IBM-2, but both a nonparamet-ric and a parametric one for the HMM.
In contrastto GIZA++, we have a parameter for every possibledifference, i.e.
we do not group differences with ab-solutes greater than 5.
Also, commonly one uses adistribution p(i|i?, I) = r(i?
i?)/?Ii?
?=1 r(i??
?
i?
),but for technical reasons, we drop the denominatorand instead constrain the r(?
)-parameters to sum to1.
In future work we hope to implement both thenormalization and the grouping of bins.2.2 Word AlignmentOriginally the considered models were used for theactual translation problem.
Hence, the parametersd and l had to be inferred from a training corpus,which was based on maximizing the probabilitymaxd,l?s?apd,l(fs,as|es) .
(2)Today the major application of the models lies inword alignment.
Instead of estimating continuousparameters, one is now faced with the discrete opti-mization problem of assigning a single alignment toevery sentence pair in the corpus.
This lead to therecent innovative work of (Bodrumlu et al, 2009)where the alignments are the only unknown quanti-ties.Nevertheless, the use of probabilistic models re-mains attractive, in particular since they contributestatistics of likely alignments.
In this work, we com-bine the two concepts into the criterionmind,l?
log[?smaxaspd,l(fs,as|es)]+ ?
?d?0 ,where ?
?
0 is a weighting parameter and we nowestimate a single alignment for every sentence.The second term denotes the L0-norm of thetranslational parameters, i.e.
the number of non-zero1The original work considered a dependence on I and J , butit is common to drop J .173parameters.
Since we only consider a single align-ment per sentence, this term is equivalent to Bo-drumlu et al?s objective function.
Minimizing thefirst term is closely related to the common criterion(2).
For parameter estimation it is known as the max-imum approximation, but for word alignment it is aperfectly valid model.For the IBM-1 model the first term alone results ina convex, but not strictly convex minimization prob-lem2.
However, EM-like iterative methods generallydo not reach the minimum: they are doing block co-ordinate descent (Bertsekas, 1999, chap.
2.7) whichgenerally gives the optimum only for strictly convexfunctions.
Indeed, our experiments showed a strongdependence on initialization and lead to heavily lo-cal solutions.In the following we present two strategies to min-imize the new objective.
We start with an iterativemethod that also handles the regularity term.3 An Iterative SchemeTo derive our algorithms, we first switch the mini-mum and maximum in the objective and obtainmin{as}mind,l?
?slog[pd,l(fs,as|es)]+ ?
?d?0 ,where the notation{as} denotes the alignments ofall sentence pairs.
Ignoring the L0-term for the mo-ment, we now make use of a result of (Vicente et al,2009) in their recent work on histogram-based im-age segmentation: for any given set of alignments,the inner minimization over the parameters is solvedby relative frequencies.
When plugging this solutioninto the functional, one gets a model that does notdecompose over the sentences, but one that is stillreasonably easy to handle.Before we get into details, we observe that thisminimizer is valid even when including the L0 term:if two words are never linked, both terms will set therespective parameter to 0.
If they are linked, how-ever, then setting this parameter to 0 would makethe first term infinite.
All non-zero parameters aretreated equally by the L0 term, so the restrictionto relative frequencies does not change the optimalvalue.
In fact, this can be extended to weighted L02This is due to taking the maximizing alignment.
Summingover all alignments is strictly convex.terms, and later on we exploit this to derive an al-ternative way to handle a dictionary prior.
Note thatthe same principle could also be applied to the workof (Bodrumlu et al, 2009).3.1 Mathematical FormulationWe detail our scheme for the IBM-1 model, the ex-tension to other models is easy.
For given align-ments we introduce the countsNf,e({as}) =?s?j?
(f, fj) ?
?
(e, eaj )Ne({as}) =?s?j?
(e, eaj ) ,where ?
(?, ?)
is the Kronecker-delta.
The op-timal translation parameters are then given byNf,e({as})/Ne({as}), and plugging this into thefirst term in the objective gives (up to a constant)min{as}?f,e?Nf,e({as}) log(Nf,e({as})Ne({as})).The second term is simply ?
?f,e ?Nf,e({as})?0,and since N(e) =?f N(f, e), in total we getmin{as}?f,e?Nf,e({as}) log (Nf,e({as}))+?eNe({as}) log (Ne({as})) .+ ?
?f,e?Nf,e({as})?0 (3)In essence we are now dealing with the functionx log(x), where its value for 0 is defined as 0.3.2 AlgorithmFor the new objective, we were able to entirely getrid of the model parameters, leaving only alignmentvariables.
Nevertheless, the algorithm we presentmaintains these parameters, and it requires an initialchoice.
While we initialize the alignment parame-ters uniformly, for the translation parameters we useco-occurrence counts.
This performed much betterthan a uniform initialization.
The algorithm, calledAM (for alternating minimization), now iterates twosteps:1741.
Starting from the current setting of d andl, derive Viterbi alignments for all sentencepairs.
E.g.
for the IBM-1 we set asj =argmaxid(fj |ei).
For the IBM-2 the term issimilar, while for the HMM one can use dy-namic programming.Note that this step does not consider the L0-term.
This term can however not increase.2.
Run the Iterated Conditional Modes (Besag,1986), i.e.
go sequentially over all alignmentvariables and set them to their optimal valuewhen keeping the others fixed.Here, we need to keep track of the currentalignment counts.
In every step we need tocompute the objective cost associated to a countthat increases by 1, and (another) one thatdecreases by 1.
For the IBM-2 we need toconsider the alignment counts, too, and forthe HMM usually two alignment terms are af-fected.
In case of 0-alignments there can bemore than two.
We presently do not considerthese cases and hence do not find the exact op-timum there.Afterwards, reestimate the parameters d and lfrom the final counts.4 Integer Linear ProgrammingThe above algorithm is fast and can handle large cor-pora.
However, it still gets stuck in local minima,and there is no way of telling how close to the opti-mum one got.This motivates the second algorithm where wecast the objective function as an integer linear pro-gram (ILP).
In practice it is too large to be solvedexactly, so we solve its linear programming relax-ation, followed by successive strengthening.
Herewe derive our own set of cuts.
Now we also get alower bound on the problem and obtain lower en-ergy solutions in practice.
But we can handle onlysmall corpora.We limit this method to the models IBM-1 andIBM-2.
Handling an HMM would be possible,but its first order alignment model would introducemany more variables.
Handling the IBM-3, based on(Ravi and Knight, 2010; Schoenemann, 2010) seemsa more promising direction.4.1 An ILP for the Regularized IBM-1The basis of our ILP is the fact that the counts Nf,eand Ne can only assume a finite, a-priori known setof integral values, including 0.
We introduce a bi-nary variable ncf,e ?
{0, 1} for each possible valuec, where we want ncf,e = 1 if Nf,e(as) = c, oth-erwise ncf,e = 0.
This is analogous for the vari-ables nce and Ne(as).
Finally, since the counts de-pend on the alignments, we also need binary vari-ables xsi,j ?
{0, 1} that we want to be 1 if and onlyif asj = i.The cost function of (3) can now be written as alinear function in terms of the integer variables ncf,eand nce, with coefficientswce,f = ?c log(c) + ?
?c?0 , wce = c log(c) .However, we need additional constraints.
In particu-lar we need to ensure that for a given f and e exactlyone variable ncf,e is 1.
Equivalently we can postulatethat the sum of these variables is one.
We proceedanalogous for each e and the variables nce.Then, we need to ensure that for each source wordin each sentence f s an alignment is specified, i.e.that for each given s and j the variables xsi,j sumto 1.
Finally, the count variables have to reflect thecounts induced by the alignment variables.
For thecounts Nf,e this is expressed by?s,i,j:fsj =f,esi=exsi,j =?cc ?
ncf,e ?f, e ,and likewise for the counts Ne.Altogether, we arrive at the following system:min{xsi,j},{ncf,e},{nce}?e,cwce nce +?f,e,cwcf,e ncf,es.t.
?ixsi,j = 1 ?s, j?cncf,e = 1 ?f, e?cnce = 1 ?e?s,i,j:fj=f,ei=exsi,j =?cc ?
ncf,e ?f, e?s,i,j:ei=exsi,j =?cc ?
nce ?exsi,j ?
{0, 1}, nce ?
{0, 1}, nce,f ?
{0, 1} .1754.2 Handling the IBM-2The above mentioned system can be easily adaptedto the IBM-2 model.
To this end, we introduce vari-ables nci,j,I ?
{0, 1} to express how often sourceword j is aligned to target word i given that thereare I words in the target sentence.
Note that thenumber of times source word j is aligned given thatthe target sentence has I words is known a-prioriand does not depend on the alignment to be opti-mized.
We denote it Cj,I .
The cost function ofthe ILP is augmented by?i,j,I,cwci,j,I nci,j,I , withwci,j,I = c log(c/Cj,I).
In addition we add the fol-lowing constraints to the system:?s:Is=Ixsi,j =?cc ?
nci,j,I ?i, j, I .5 Cutting PlanesInteger linear programming is an NP-hard problem(see e.g.
(Schrijver, 1986)).
While for problemswith a few thousand variables it can often be solvedexactly via the branch and cut method, in our settingnone of the solvers we tried terminated.
Alreadysolving the linear programming relaxation requiresup to 4 GB of memory for corpora with roughly3000 sentence pairs.So instead of looking for an exact solution, wemake use of a few iterations of the cutting planesmethod (Gomory, 1958), where repeatedly an LP-relaxation is solved, then additionally valid inequal-ities, called cuts, are added to the system.
Everyround gives a tighter relaxation, i.e.
a better lowerbound on the optimal integral value.After solving each LP-relaxation we derive an in-tegral solution by starting the iterative method fromsection 3 from the fractional LP-solution.
In the endwe output the best found integral solution.For deriving cuts we tried all the methods imple-mented in the COIN Cut Generation Library CGL3,based on the solver Clp from the same project line.However, either the methods were very slow in pro-ducing cuts or they produced very few cuts only.
Soeventually we derived our own set of cuts that willnow be presented.
Note that they alone do not givean integral solution.3http://www.coin-or.org/projects/Cgl.xml5.1 A Set of Count-based CutsThe derived ILP contains several constraints of theform ?iyi =?cc ?
zc , (4)where all variables are binary.
Expressions of thiskind arise wherever we need to ensure consistencybetween alignment variables and count variables.Our cuts aim at strengthening each of these equa-tions individually.Assume that equation (4) involves the variablesy1, .
.
.
, yK and hence also the variables z0, .
.
.
, zK .The trouble with the equation is that even if theleft hand side is integral, the right-hand side is usu-ally not.
As an example, consider the case where?Ki=1 yi = 3.
Then the fractional assignment z0 =1?3/K, zK = 3/K and zc = 0 for all other c satis-fies (4).
Indeed, if the cost function for zc is concavein c, as is the function?c log(c) we use, this will bethe optimal solution for the given left hand side.Hence we want to enforce that for an integralvalue k of the left hand side, all variables zc for0 ?
c < k are zero.
This is ensured by the fol-lowing system of inequalities that is exponentiallylarge in k:?i?Kyi +k?1?c=0zc ?
k (5)?K ?
{1, .
.
.
,K} : |K| = k .It turns out that this system can be formulated quitecompactly.5.2 Polynomial FormulationWe now formalize the result for the compact formu-lation of (5).Proposition 1 The union of the systems (5) for all kcan be represented by polynomially many variablesand linear constraints.Proof: we first observe that it suffices to enforce[maxK:|K|=k?i?Kyi]+k?1?c=0zc ?
kfor all k. These are polynomially many equations(one for each k), but each involves a maximization176over exponentially many sets.
However, this maxi-mization can be expressed by the auxiliary variables?kl := maxK?
{1,...,l}:|K|?k?i?Kyi= max{?k?1l?1 + yl , ?kl?1}Now we only have to maximize over two linear ex-pressions for each of the new, polynomially many,variables.
We can enforce ?kl to be an upper boundon the maximum by postulating ?kl ?
?k?1l?1 + yl and?kl ?
?kl?1.
Since the original maximum occurred onthe left hand side of a less-than constraint, this upperbound will be tight wherever this is necessary.
2In practice the arising system is numerically hardto solve.
Since usually only polynomially many cutsof the form (5) are actually violated during the cut-ting planes process, we add them in passes and getsignificantly lower running times.
Moreover, eachround of cuts gives a new opportunity to derive aheuristic integral solution.5.3 Backward CutsWe call the cuts we have derived above forward cutsas they focus on variables that are smaller than k. Ifwe could be sure that the left-hand side of (4) wasalways integral, they should indeed suffice.
In prac-tice this is not the case, so we now also derive back-ward cuts where we focus on all variables that arelarger than k, with the following reasoning: once weknow that at least K ?
k variables yi are inactive(i.e.
yi = 0), we can conclude that all zc with c > kmust be zero, too.
This can be expressed by the setof inequalities?i?K(1?
yi) +K?c=k+1zc ?
K ?
k?K ?
{1, .
.
.
,K} : |K| = K ?
k ,or equivalently?i?K?yi +K?c=k+1zc ?
0 ?K : |K| = K ?
k .5.4 Other ApplicationsA related constraint system arises in recent work(Ravi and Knight, 2010; Schoenemann, 2010) oncomputing IBM-3 Viterbi alignments.
We imple-mented4 the polynomial formulation of the aboveforward cuts for this system, and got mild speed-ups(224 instead of 237 minutes for the Hansards taskreported in the second paper).
With an additionallyimproved fertility exclusion stage5 this is reduced to176 minutes.6 ExperimentsWe evaluate the proposed strategies on both smallscale and (where applicable) large scale tasks.
Wecompare to standard EM with sums over alignments,where for the IBM-1 and the HMM we use GIZA++.In addition, we evaluate several variants (our imple-mentations) of the HMM, with non-parametric andparametric alignment models.
Note that for the non-parametric variant we estimate distributions for thefirst aligned position, for the parametric all initialpositions are equally likely.
For the IBM-2 we con-sider the non-parametric variant and hence our im-plementation.
We also evaluate our schemes on thetask without regularization.All experiments in this work were executed on a3 GHz Core 2 Duo machine with 8 GB of memory,where up to 4 GB were actually used.
The itera-tive scheme was run for 15 iterations, where it wasnearly converged.
This setting was also used for ourown EM-implementations.
Solely for GIZA++ weused the standard setting of 5 iterations, and the im-plemented smoothing process.
For the IBM-2 andHMM we follow the standard strategy to first trainan IBM-1 with the same objective function.6.1 Large Scale TasksWe consider two well-known corpora with publiclyavailable gold alignments, and run both translationdirections for each of them.
The first task is theCanadian Hansards task (French and English) withroughly 1 million sentences.
The second task isEuroparl Spanish-English, where we take the first500000 sentences.
Our iterative scheme runs in4based on code available at www.maths.lth.se/matematiklth/personal/tosch/download.html.5In (Schoenemann, 2010) we stated that the difference be-tween cyif and the contribution of i to the bound has to exceedu ?
l3.
This can be tightened if one additionally adds the costof the best f alignments to i to the cost cyif .177Canadian HansardsFr?
En En?
FrHMM (Giza++) 0.918 0.918par.
HMM (our EM) 0.887 0.896par.
HMM (Viterbi) 0.873 0.897par.
HMM + L0 0.891 0.907nonpar.
HMM (our EM) 0.873 0.911nonpar.
HMM (Viterbi) 0.881 0.909nonpar.
HMM + L0 0.902 0.917EuroparlEs?
En En?
EsHMM (Giza++) 0.764 0.754nonpar.
HMM (our EM) 0.738 0.733nonpar.
HMM (Viterbi) 0.726 0.716nonpar.
HMM + L0 0.729 0.73Table 1: For large corpora, the proposed scheme outper-forms Viterbi training and sometimes even our EM.roughly 5 hours (with room for improvements), us-ing 2.5 GB memory.
We found that an L0-weight of?
= 5.0 performs very well.
Hence, we will use thisfor all our experiments.We compare to the standard GIZA++ implemen-tation and our own HMM implementations with EM.Here we ran 15 iterations for IBM-1 and HMM each.As shown in Table 1 adding the L0 term improvesthe standard Viterbi training.
Our method also some-times beats the simple EM implementation but notGIZA++.
This may be due to the special paramet-ric model of GIZA++, its smoothing process or thelower number of iterations.
Our deficient paramet-ric model is inferior for the Hansards task, so we didnot run it for Europarl.6.2 Small Scale TasksTo evaluate the ILP strategy we consider four smallscale tasks released by the European Corpus Ini-tiative6.
See (Schoenemann, 2010) for the corpusstatistics.
We report weighted f-measures (Fraserand Marcu, 2007b) on gold alignments (sure andpossible) specified by one annotator, for 144 and 110sentences respectively.
The number of cut roundswas selected so that the execution times remainedbelow 2 days for all tasks.
This was 50 rounds forthe IBM-1 and 2 rounds for the IBM-2.
In fact, with6http://www.elsnet.org/eci.htmlthese numbers the Avalanche task is processed in lit-tle less than a day.We tested a number of LP solvers and found thatmost of them need several hours to solve the root re-laxation.
This is different for the commercial solverFICO Xpress, which only needs around 15 minutes.However, it is slower in processing the subsequentcut iterations.
Hence, for the IBM-1 we use the opensource Clp7.The resulting f-measures for the tested strategiesare given in Table 2.
In all cases, adding the L0term greatly improves the standard Viterbi training.Moreover, for the small scale tasks, the parametricHMM is clearly the best choice when using the L0penalty.
In the majority of cases the ILP strategyperforms better than the iterative scheme.
In fact, italways found the lower energy solution.
The mostextreme difference we observed for the IBM-2 onthe UBS English to German task: here AM finds anenergy of 318147, where the ILP gives 303674.Finally, Table 3 evaluates the effectiveness ofthe cut strategy exemplarily on one of the tasks.Clearly, the gaps are reduced significantly comparedto the LP-relaxation.
However, except for the IBM-1 (which is convex for ?
= 0) the lower bounds arestill quite loose.6.3 Handling Dictionary KnowledgeThe presented L0 regularity is easily modified to in-clude dictionary knowledge8.
To this end, we intro-duce a weighted L0-norm: whenever a pair of sourceand target words is listed in the dictionary, the entryis not penalized.
All remaining entries are penalizedby ?.Note that this is different from the standard way(Brown et al, 1993a) of handling dictionary knowl-edge, which appends the dictionary to the corpus(with a proper weighting factor).
We tried bothschemes with several weighting factors, then chosethe best-performing for the UBS task.
For the UBSGerman to English task we get an accuracy of 0.445,which beats GIZA++ both with (0.438) and without(0.398) dictionary knowledge.
In the reverse direc-tion both schemes profit from the extra knowledge,7http://www.coin-or.org/projects/Clp.xml8Our data are based on www.dict.info/uddl.phpand www.ilovelanguages.com/idp and the stemmingalgorithms at snowball.tartarus.org.178Avalanche French?
GermanModel EM AM ILPIBM-1 0.603 0.619 0.591IBM-1 + L0 ?
0.64 0.625IBM-2 0.568 0.632 0.60IBM-2 + L0 ?
0.680 0.636par.
HMM 0.752 0.621 ?par.
HMM + L0 ?
0.779 ?nonpar.
HMM 0.752 0.655 ?nonpar.
HMM + L0 ?
0.714 ?Avalanche German?
FrenchModel EM AM ILPIBM-1 0.494 0.485 0.507IBM-1 + L0 ?
0.497 0.488IBM-2 0.428 0.459 0.526IBM-2 + L0 ?
0.483 0.55par.
HMM 0.606 0.49 ?par.
HMM + L0 ?
0.592 ?nonpar.
HMM 0.582 0.501 ?nonpar.
HMM + L0 ?
0.537 ?UBS German?
EnglishModel EM AM ILPIBM-1 0.381 0.359 0.335IBM-1 + L0 ?
0.350 0.442IBM-2 0.315 0.324 0.340IBM-2 + L0 ?
0.383 0.462par.
HMM 0.398 0.229 ?par.
HMM + L0 ?
0.383 ?nonpar.
HMM 0.421 0.29 ?nonpar.
HMM + L0 ?
0.371 ?UBS English?
GermanModel EM AM ILPIBM-1 0.515 0.435 0.489IBM-1 + L0 ?
0.444 0.504IBM-2 0.417 0.40 0.435IBM-2 + L0 ?
0.52 0.571par.
HMM 0.625 0.404 ?par.
HMM + L0 ?
0.537 ?nonpar.
HMM 0.623 0.436 ?nonpar.
HMM + L0 ?
0.524 ?Table 2: Alignment accuracy (weighted f-measure) fordifferent algorithms.
We use a dictionary penalty of?
= 5 and the standard EM (GIZA++ for IBM-1 andparametric HMM, our implementation otherwise) train-ing scheme with 5 iterations for each model.UBS English?
GermanL0-weight IBM-1 IBM-2root relaxation 0.0 1.098 7.697after cut rounds 0.0 1.081 5.67root relaxation 5.0 1.16 2.76after cut rounds 5.0 1.107 2.36Table 3: Ratios of the best known integer solution and thebest known lower bounds for all considered tasks.but GIZA++ remains the clear winner.
Applyingthe same weights to the above mentioned Hansardstask slightly improved GIZA++, whereas it slightlyworsened the performance of our scheme in the onedirection and slightly improved it in the other.
Weintend to investigate this more thoroughly in the fu-ture.7 DiscussionIn this paper we have shown that an L0 prior onthe dictionary parameters greatly improves Viterbitraining.
A simple iterative scheme often nearlymatches our EM-implementation of the HMM.We have also derived two algorithms to deal withthe new objective.
A simple iterative scheme givesquite accurate results on large scale tasks.
On smallscale tasks our inexact ILP strategy shows that theiterative scheme does not find the optimum in prac-tice, a point that may well carry over to other mod-els trained with the maximum approximation.
Thisstrategy also provides lower bounds, but at presentthey are quite loose.Moreover, we have presented an alternative wayof handling dictionary knowledge.
Finally, we havediscussed connections to computing IBM-3 Viterbialignments, where we got mild speed-ups.In future work we intend to investigate the effectof the generated alignments on the translation qual-ity of phrase-based approaches.
We also want to ex-plore strategies to determine the regularity weight.Finally, we want to handle a non-deficient paramet-ric HMM.Acknowledgements.
We thank Ben Taskar andJoa?o Grac?a for helpful discussions.
This work wasfunded by the European Research Council (Glob-alVision grant no.
209480).179ReferencesT.
Achterberg.
2007.
Constraint Integer Programming.Ph.D.
thesis, Zuse Institut, TU Berlin, Germany, July.Y.
Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf-ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.Smith, and D. Yarowsky.
1999.
Statisticalmachine translation, Final report, JHU workshop.http://www.clsp.jhu.edu/ws99/.D.P.
Bertsekas.
1999.
Nonlinear Programming, 2nd edi-tion.
Athena Scientific.J.
Besag.
1986.
On the statistical analysis of dirty pic-tures.
Journal of the Royal Statistical Society, SeriesB, 48(3):259?302.A.
Birch, C. Callison-Burch, and M. Osborne.
2006.Constraining the phrase-based, joint probability statis-tical translation model.
In Conference of the Associa-tion for Machine Translation in the Americas (AMTA),Cambridge, Massachusetts, August.T.
Bodrumlu, K. Knight, and S. Ravi.
2009.
A new ob-jective function for word alignment.
In Proceedings ofthe Workshop on Integer Linear Programming for Nat-ural Language Processing (ILP), Boulder, Colorado,June.P.F.
Brown, S.A. Della Pietra, V.J.
Della Pietra, M.J.Goldsmith, J. Hajic, R.L.
Mercer, and S. Mohanty.1993a.
But dictionaries are data too.
In HLT work-shop on Human Language Technology.P.F.
Brown, S.A. Della Pietra, V.J.
Della Pietra, and R.L.Mercer.
1993b.
The mathematics of statistical ma-chine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311, June.J.
DeNero and D. Klein.
2008.
The complexity of phrasealignment problems.
In Annual Meeting of the Associ-ation for Computational Linguistics (ACL), Columbus,Ohio, June.Y.
Deng and W. Byrne.
2005.
HMM word and phrasealignment for statistical machine translation.
In HLT-EMNLP, Vancouver, Canada, October.A.
Fraser and D. Marcu.
2007a.
Getting the structureright for word alignment: LEAF.
In Conference onEmpirical Methods in Natural Language Processing(EMNLP), Prague, Czech Republic, June.A.
Fraser and D. Marcu.
2007b.
Measuring word align-ment quality for statistical machine translation.
Com-putational Linguistics, 33(3):293?303, September.K.
Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.2010.
Posterior regularization for structured latentvariable models.
Journal of Machine Learning Re-search, 11:2001?2049, July.U.
Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-mada.
2004.
Fast decoding and optimal decoding formachine translation.
Artificial Intelligence, 154(1?2),April.R.E.
Gomory.
1958.
Outline of an algorithm for integersolutions to linear programs.
Bulletin of the AmericanMathematical Society, 64:275?278.M.
Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In Conference on Empirical Methodsin Natural Language Processing (EMNLP), Prague,Czech Republic, June.S.
Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan.2006.
Word alignment via quadratic assignment.
InHuman Language Technology Conference of the NorthAmerican Chapter of the Association of Computa-tional Linguistics, New York, New York, June.D.
Marcu and W. Wong.
2002.
A phrase-based, jointprobability model for statistical machine translation.In Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), Philadelphia, Pennsylva-nia, July.E.
Matusov, R. Zens, and H. Ney.
2004.
Symmetric wordalignments for statistical machine translation.
In In-ternational Conference on Computational Linguistics(COLING), Geneva, Switzerland, August.S.
Ravi and K. Knight.
2010.
Does GIZA++ make searcherrors?
Computational Linguistics, 36(3).T.
Schoenemann.
2010.
Computing optimal alignmentsfor the IBM-3 translation model.
In Conference onComputational Natural Language Learning (CoNLL),Uppsala, Sweden, July.A.
Schrijver.
1986.
Theory of Linear and IntegerProgramming.
Wiley-Interscience Series in DiscreteMathematics and Optimization.
John Wiley & Sons.E.
Sumita, Y. Akiba, T. Doi, A. Finch, K. Imamura,H.
Okuma, M. Paul, M. Shimohata, and T. Watanabe.2004.
EBMT, SMT, Hybrid and more: ATR spokenlanguage translation system.
In International Work-shop on Spoken Language Translation (IWSLT), Ky-oto, Japan, September.B.
Taskar, S. Lacoste-Julien, and D. Klein.
2005.
Adiscriminative matching approach to word alignment.In Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), Vancouver, Canada, Oc-tober.S.
Vicente, V.N.
Kolmogorov, and C. Rother.
2009.
Jointoptimization of segmentation and appearance models.In IEEE International Conference on Computer Vision(ICCV), Kyoto, Japan, September.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based word alignment in statistical translation.
In In-ternational Conference on Computational Linguistics(COLING), pages 836?841, Copenhagen, Denmark,August.Y.-Y.
Wang and A. Waibel.
1998.
Modeling withstructures in statistical machine translation.
In In-ternational Conference on Computational Linguistics(COLING), Montreal, Canada, August.180
