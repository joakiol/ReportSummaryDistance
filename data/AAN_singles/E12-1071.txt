Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 696?705,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsUnsupervised Detection of Downward-Entailing Operators ByMaximizing Classification CertaintyJackie CK Cheung and Gerald PennDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canada{jcheung,gpenn}@cs.toronto.eduAbstractWe propose an unsupervised, iterativemethod for detecting downward-entailingoperators (DEOs), which are important fordeducing entailment relations between sen-tences.
Like the distillation algorithm ofDanescu-Niculescu-Mizil et al(2009), theinitialization of our method depends on thecorrelation between DEOs and negative po-larity items (NPIs).
However, our methodtrusts the initialization more and aggres-sively separates likely DEOs from spuri-ous distractors and other words, unlike dis-tillation, which we show to be equivalentto one iteration of EM prior re-estimation.Our method is also amenable to a bootstrap-ping method that co-learns DEOs and NPIs,and achieves the best results in identifyingDEOs in two corpora.1 IntroductionReasoning about text has been a long-standingchallenge in NLP, and there has been consider-able debate both on what constitutes inference andwhat techniques should be used to support infer-ence.
One task involving inference that has re-cently received much attention is that of recog-nizing textual entailment (RTE), in which the goalis to determine whether a hypothesis sentence canbe entailed from a piece of source text (Bentivogliet al 2010, for example).An important consideration in RTE is whethera sentence or context produces an entailment re-lation for events that are a superset or subset ofthe original sentence (MacCartney and Manning,2008).
By default, contexts are upward-entailing,allowing reasoning from a set of events to a su-perset of events as seen in (1).
In the scope ofa downward-entailing operator (DEO), however,this entailment relation is reversed, such as inthe scope of the classical DEO not (2).
Thereare also operators which are neither upward- nordownward entailing, such as the expression ex-actly three (3).
(1) She sang in French.
?
She sang.
(upward-entailing)(2) She did not sing in French.
?
She did notsing.
(downward-entailing)(3) Exactly three students sang.
6?
Exactlythree students sang in French.
(neitherupward- nor downward-entailing)Danescu-Niculescu-Mizil et al(2009) (hence-forth DLD09) proposed the first computationalmethods for detecting DEOs from a corpus.
Theyproposed two unsupervised algorithms which relyon the correlation between DEOs and negativepolarity items (NPIs), which by the definition ofLadusaw (1980) must appear in the context ofDEOs.
An example of an NPI is yet, as in thesentence This project is not complete yet.
Thefirst baseline method proposed by DLD09 sim-ply calculates a ratio of the relative frequenciesof a word in NPI contexts versus in a generalcorpus, and the second is a distillation methodwhich appears to refine the baseline ratios using atask-specific heuristic.
Danescu-Niculescu-Miziland Lee (2010) (henceforth DL10) extend this ap-proach to Romanian, where a comprehensive listof NPIs is not available, by proposing a bootstrap-ping approach to co-learn DEOs and NPIs.DLD09 are to be commended for having iden-tified a crucial component of inference that nev-ertheless lends itself to a classification-based ap-696proach, as we will show.
However, as notedby DL10, the performance of the distillationmethod is mixed across languages and in thesemi-supervised bootstrapping setting, and thereis no mathematical grounding of the heuristic toexplain why it works and whether the approachcan be refined or extended.
This paper suppliesthe missing mathematical basis for distillation andshows that, while its intentions are fundamentallysound, the formulation of distillation neglects animportant requirement that the method not beeasily distracted by other word co-occurrencesin NPI contexts.
We call our alternative cer-tainty, which uses an unusual posterior classifica-tion confidence score (based on the max function)to favour single, definite assignments of DEO-hood within every NPI context.
DLD09 actuallyspeculated on the use of max as an alternative,but within the context of an EM-like optimizationprocedure that throws away its initial parametersettings too willingly.
Certainty iteratively anddirectly boosts the scores of the currently best-ranked DEO candidates relative to the alternativesin a Na?
?ve Bayes model, which thus pays more re-spect to the initial weights, constructively build-ing on top of what the model already knows.
Thismethod proves to perform better on two corporathan distillation, and is more amenable to the co-learning of NPIs and DEOs.
In fact, the bestresults are obtained by co-learning the NPIs andDEOs in conjunction with our method.2 Related workThere is a large body of literature in linguis-tic theory on downward entailment and polar-ity items1, of which we will only mention themost relevant work here.
The connection betweendownward-entailing contexts and negative polar-ity items was noticed by Ladusaw (1980), whostated the hypothesis that NPIs must be gram-matically licensed by a DEO.
However, DEOsare not the sole licensors of NPIs, as NPIs canalso be found in the scope of questions, certainnumeric expressions (i.e., non-monotone quanti-fiers), comparatives, and conditionals, among oth-ers.
Giannakidou (2002) proposes that the prop-erty shared by these constructions and downwardentailment is non-veridicality.
If F is a propo-1See van der Wouden (1997) for a comprehensive refer-ence.sitional operator for proposition p, then an oper-ator is non-veridical if Fp 6?
p. Positive opera-tors such as past tense adverbials are veridical (4),whereas questions, negation and other DEOs arenon-veridical (5, 6).
(4) She sang yesterday.
?
She sang.
(5) She denied singing.
6?
She sang.
(6) Did she sing?
6?
She sang.While Ladusaw?s hypothesis is thus acceptedto be insufficient from a linguistic perspective, itis nevertheless a useful starting point for compu-tational methods for detecting NPIs and DEOs,and has inspired successful techniques to detectDEOs, like the work by DLD09, DL10, and alsothis work.
In addition to this hypothesis, we fur-ther assume that there should only be one plausi-ble DEO candidate per NPI context.
While thereare counterexamples, this assumption is in prac-tice very robust, and is a useful constraint for ourlearning algorithm.
An analogy can be drawn tothe one sense per discourse assumption in wordsense disambiguation (Gale et al 1992).The related?and as we will argue, moredifficult?problem of detecting NPIs has alsobeen studied, and in fact predates the work onDEO detection.
Hoeksema (1997) performed thefirst corpus-based study of NPIs, predominantlyfor Dutch, and there has also been work on de-tecting NPIs in German which assumes linguisticknowledge of licensing contexts for NPIs (Lichteand Soehn, 2007).
Richter et al(2010) makethis assumption as well as use syntactic structureto extract NPIs that are multi-word expressions.Parse information is an especially important con-sideration in freer-word-order languages like Ger-man where a MWE may not appear as a contigu-ous string.
In this paper, we explicitly do not as-sume detailed linguistic knowledge about licens-ing contexts for NPIs and do not assume that aparser is available, since neither of these are guar-anteed when extending this technique to resource-poor languages.3 Distillation as EM Prior Re-estimationLet us first review the baseline and distillationmethods proposed by DLD09, then show that dis-tillation is equivalent to one iteration of EM prior697re-estimation in a Na?
?ve Bayes generative proba-bilistic model up to constant rescaling.
The base-line method assigns a score to each word-typebased on the ratio of its relative frequency withinNPI contexts to its relative frequency within ageneral corpus.
Suppose we are given a corpus Cwith extracted NPI contexts N and they containtokens(C) and tokens(N ) tokens respectively.Let y be a candidate DEO, countC(y) be the uni-gram frequency of y in a corpus, and countN (y)be the unigram frequency of y in N .
Then, wedefine S(y) to be the ratio between the relativefrequencies of y within NPI contexts and in theentire corpus2:S(y) = countN (y)/tokens(N )countC(y)/tokens(C) .
(7)The scores are then used as a ranking to de-termine word-types that are likely to be DEOs.This method approximately captures Ladusaw?shypothesis by highly ranking words that appearin NPI contexts more often than would be ex-pected by chance.
However, the problem withthis approach is that DEOs are not the only wordsthat co-occur with NPIs.
In particular, there existmany piggybackers, which, as defined by DLD09,collocate with DEOs due to semantic relatednessor chance, and would thus incorrectly receive ahigh S(y) score.Examples of piggybackers found by DLD09 in-clude the proper noun Milken, and the adverb vig-orously, which collocate with DEOs like deny inthe corpus they used.
DLD09?s solution to thepiggybacker problem is a method that they termdistillation.
Let Ny be the NPI contexts that con-tain word y; i.e., Ny = {c ?
N|c ?
y}.
In dis-tillation, each word-type is given a distilled scoreaccording to the following equation:Sd(y) =1|Ny|?p?NyS(y)?y?
?p S(y?).
(8)where p indexes the set of NPI contexts whichcontain y3, and the denominator is the number of2DLD09 actually use the number of NPI contexts con-taining y rather than countN (y), but we find that using theraw count works better in our experiments.3In DLD09, the corresponding equation does not indicatethat p should be the contexts that include y, but it is clearfrom the surrounding text that our version is the intendedmeaning.
If all the NPI contexts were included in the sum-mation, Sd(y) would reduce to inverse relative frequency.YLDEOContext wordsXFigure 1: Na?
?ve Bayes formulation of DEO detection.NPI contexts which contain y.DLD09 find that distillation seems to improvethe performance of DEO detection in BLLIP.Later work by DL10, however, shows that distil-lation does not seem to improve performance overthe baseline method in Romanian, and the authorsalso note that distillation does not improve perfor-mance in their experiments on co-learning NPIsand DEOs via bootstrapping.A better mathematical grounding of the distilla-tion method?s apparent heuristic in terms of exist-ing probabilistic models sheds light on the mixedperformance of distillation across languages andexperimental settings.
In particular, it turns outthat the distillation method of DLD09 is equiva-lent to one iteration of EM prior re-estimation ina Na?
?ve Bayes model.
Given a lexicon L of Lwords, let each NPI context be one sample gen-erated by the model.
One sample consists of alatent categorical (i.e., a multinomial with onetrial) variable Y whose values range over L, cor-responding to the DEO that licenses the context,and observed Bernoulli variables ~X = Xi=1...Lwhich indicate whether a word appears in the NPIcontext (Figure 1).
This method does not attemptto model the order of the observed words, nor thenumber of times each word appears.
Formally, aNa?
?ve Bayes model is given by the following ex-pression:P ( ~X, Y ) =L?i=1P (Xi|Y )P (Y ).
(9)The probability of a DEO given a particularNPI context isP (Y | ~X) ?L?i=1P (Xi|Y )P (Y ).
(10)698The probability of a set of observed NPI con-texts N is the product of the probabilities for eachsample:P (N ) =?~X?NP ( ~X) (11)P ( ~X) =?y?LP ( ~X, y).
(12)We first instantiate the baseline method ofDLD09 by initializing the parameters to themodel, P (Xi = 1|y) and P (Y = y), such thatP (Y = y) is proportional to S(y).
Recall that thisinitialization utilizes domain knowledge about thecorrelation between NPIs and DEOs, inspired byLadusaw?s hypothesis:P (Y = y) = S(y)/?y?S(y?)
(13)P (Xi = 1|y) ={1 if Xi corresponds to y0.5 otherwise.
(14)This initialization of P (Xi = 1|y) ensures thatthe the value of y corresponds to one of the wordsin the NPI context, and the initialization of P (Y )is simply a normalization of S(y).Since we are working in an unsupervised set-ting, there are no labels for Y available.
A com-mon and reasonable assumption about learningthe parameter settings in this case is to find the pa-rameters that maximize the likelihood of the ob-served training data; i.e., the NPI contexts:??
= argmax?P (N ; ?).
(15)The EM algorithm is a well-known iterative al-gorithm for performing this optimization.
Assum-ing that the prior P (Y = y) is a categorical distri-bution, the M-step estimate of these parametersafter one iteration through the corpus is as fol-lows:P t+1(Y = y) =?~X?NP t(y| ~X)?y?
P t(y?| ~X)(16)We do not re-estimate P (Xi = 1|y) becausetheir role is simply to ensure that the DEO re-sponsible for an NPI context exists in the context.Estimating these parameters would exacerbate theproblems with EM for this task which we will dis-cuss shortly.P (Y ) gives a prior probability that a certainword-type y is a DEO in an NPI context, withoutnormalizing for the frequency of y in NPI con-texts.
Since we are interested in estimating thecontext-independent probability that y is a DEO,we must calculate the probability that a word isa DEO given that it appears in an NPI context.Let Xy be the observed variable corresponding toy.
Then, the expression we are interested in isP (y|Xy = 1).
We now show that P (y|Xy =1) = P (y)/P (Xy = 1), and that this expressionis equivalent to (8).P (y|Xy = 1) =P (y,Xy = 1)P (Xy = 1)(17)Recall that P (y,Xy = 0) = 0 because of theassumption that a DEO appears in the NPI contextthat it generates.
Thus,P (y,Xy = 1) = P (y,Xy = 1) + P (y,Xy = 0)= P (y) (18)One iteration of EM to calculate this proba-bility is equivalent to the distillation method ofDLD09.
In particular, the numerator of (17),which we just showed to be equal to the estimateof P (Y ) given by (16), is exactly the sum of theresponsibilities for a particular y, and is propor-tional to the summation in (8) modulo normaliza-tion, because P ( ~X |y) is constant for all y in thecontext.
The denominator P (Xy = 1) is simplythe proportion of contexts containing y, which isproportional to |Ny|.
Since both the numeratorand denominator are equivalent up to a constantfactor, an identical ranking is produced by distil-lation and EM prior re-estimation.Unfortunately, the EM algorithm does not pro-vide good results on this task.
In fact, as moreiterations of EM are run, the performance dropsdrastically, even though the corpus likelihoodis increasing.
The reason is that unsupervisedEM learning is not constrained or biased towardslearning a good set of DEOs.
Rather, a higher datalikelihood can be achieved simply by assigninghigh prior probabilities to frequent word-types.This can be seen qualitatively by consider-ing the top-ranking DEOs after several itera-tions of EM/distillation (Figure 2).
The top-ranking words are simply function words or otherwords common in the corpus, which have noth-ing to do with downward entailment.
In effect,6991 iteration 2 iterations 3 iterationsdenies the thedenied to tounaware denied thatlongest than thanhardly that andlacking if hasdeny has ifnobody denies ofopposes and deniedhighest but deniesFigure 2: Top 10 DEOs after iterations of EM onBLLIP.EM/distillation overrides the initialization basedon Ladusaw?s hypothesis and finds another solu-tion with a higher data likelihood.
We will alsoprovide a quantitative analysis of the effects ofEM/distillation in Section 5.4 Alternative to EM: Maximizing thePosterior Classification CertaintyWe have seen that in trying to solve the piggy-backer problem, EM/distillation too readily aban-dons the initialization based on Ladusaw?s hy-pothesis, leading to an incorrect solution.
Insteadof optimizing the data likelihood, what we need isa measure of the number of plausible DEO candi-dates there are in an NPI context, and a methodthat refines the scores towards having only onesuch plausible candidate per context.
To this end,we define the classification certainty to be theproduct of the maximum posterior classificationprobabilities over the DEO candidates.
For a setof hidden variables yN for NPI contexts N , thisis the expression:Certainty(yN |N ) =?~X?NmaxyP (y| ~X).
(19)To increase this certainty score, we proposea novel iterative heuristic method for refiningthe baseline initializations of P (Y ).
UnlikeEM/distillation, our method biases learning to-wards trusting the initialization, but refines thescores towards having only one plausible DEOper context in the training corpus.
This is accom-plished by treating the problem as a DEO classi-fication problem, and then maximizing an objec-tive ratio that favours one DEO per context.
Ourmethod is not guaranteed to increase classificationcertainty between iterations, but we will show thatit does increase certainty very quickly in practice.The key observation that allows us to resolvethe tension between trusting the initialization andenforcing one DEO per NPI context is that thedistributions of words that co-occur with DEOsand piggybackers are different, and that this dif-ference follows from Ladusaw?s hypothesis.
Inparticular, while DEOs may appear with or with-out piggybackers in NPI contexts, piggybackersdo not appear without DEOs in NPI contexts, be-cause Ladusaw?s hypothesis stipulates that a DEOis required to license the NPI in the first place.Thus, the presence of a high-scoring DEO candi-date among otherwise low-scoring words is strongevidence that the high-scoring word is not a pig-gybacker and its high score from the initializationis deserved.
Conversely, a DEO candidate whichalways appears in the presence of other strongDEO candidates is likely a piggybacker whoseinitial high score should be discounted.We now describe our heuristic method that isbased on this intuition.
For clarity, we use scoresrather than probabilities in the following explana-tion, though it is equally applicable to either.
Asin EM/distillation, the method is initialized withthe baseline S(y) scores.
One iteration of themethod proceeds as follows.
Let the score of thestrongest DEO candidate in an NPI context p be:M(p) = maxy?pSth(y), (20)where Sth(y) is the score of candidate y at the tthiteration according to this heuristic method.Then, for each word-type y in each context p,we compare the current score of y to the scores ofthe other words in p. If y is currently the strongestDEO candidate in p, then we give y credit equalto the proportional change to M(p) if y were re-moved (Context p without y is denoted p \ y).
Alarge change means that y is the only plausibleDEO candidate in p, while a small change meansthat there are other plausible DEO candidates.
Ify is not currently the strongest DEO candidate, itreceives no credit:cred(p, y) ={M(p)?M(p\y)M(p) if Sth(y) = M(p)0 otherwise.
(21)700NPI contextsA B C,B C,B C,D COriginal scoresS(A) = 5, S(B) = 4, S(C) = 1, S(D) = 2Updated scoresSh(A) = 5?
(5?
4)/5 = 1Sh(B) = 4?
(0 + 2?
(4?
1)/4)/3 = 2Sh(C) = 1?
(0 + 0 + 0) = 0Sh(D) = 2?
(2?
1)/2 = 1Figure 3: Example of one iteration of the certainty-based heuristic on four NPI contexts with four wordsin the lexicon.Then, the average credit received by each y isa measure of how much we should trust the cur-rent score for y.
The updated score for each DEOcandidate is the original score multiplied by thisaverage:St+1h (y) =Sth(y)|Ny|?
?p?Nycred(p, y).
(22)The probability P t+1(Y = y) is then simplySt+1h (y) normalized:P t+1(Y = y) = St+1h (y)?y?
?LSt+1h (y?).
(23)We iteratively reduce the scores in this fashionto get better estimates of the relative suitability ofword-types as DEOs.An example of this method and how it solvesthe piggybacker problem is given in Figure 3.
Inthis example, we would like to learn that B andD are DEOs, A is a piggybacker, and C is a fre-quent word-type, such as a stop word.
Using theoriginal scores, piggybacker A would appear tobe the most likely word to be a DEO.
However,by noticing that it never occurs on its own withwords that are unlikely to be DEOs (in the exam-ple, word C), our heuristic penalizes A more thanB, and ranks B higher after one iteration.
EMprior re-estimation would not correctly solve thisexample, as it would converge on a solution whereC receives all of the probability mass because itappears in all of the contexts, even though it isunlikely to be a DEO according to the initializa-tion.5 ExperimentsWe evaluate the performance of these methods onthe BLLIP corpus (?30M words) and the AFPportion of the Gigaword corpus (?338M words).Following DLD09, we define an NPI context tobe all the words to the left of an NPI, up to theclosest comma or semi-colon, and removed NPIcontexts which contain the most common DEOslike not.
We further removed all empty NPI con-texts or those which only contain other punctua-tion.
After this filtering, there were 26696 NPIcontexts in BLLIP and 211041 NPI contexts inAFP, using the same list of 26 NPIs defined byDLD09.We first define an automatic measure of per-formance that is common in information retrieval.We use average precision to quantify how well asystem separates DEOs from non-DEOs.
Given alist of known DEOs, G, and non-DEOs, the aver-age precision of a ranked list of items, X, is de-fined by the following equation:AP (X) =?nk=1 P (X1...k)?
1(xk ?
G)|G| ,(24)where P (X1...k) is the precision of the first kitems and 1(xk ?
G) is an indicator functionwhich is 1 if x is in the gold standard list of DEOsand 0 otherwise.DLD09 simply evaluated the top 150 outputDEO candidates by their systems, and qualita-tively judged the precision of the top-k candidatesat various values of k up to 150.
Average preci-sion can be seen as a generalization of this evalu-ation procedure that is sensitive to the ranking ofDEOs and non-DEOs.
For development purposes,we use the list of 150 annotations by DLD09.
Ofthese, 90 were DEOs, 30 were not, and 30 wereclassified as ?other?
(they were either difficult toclassify, or were other types of non-veridical oper-ators like comparatives or conditionals).
We dis-carded the 30 ?other?
items and ignored all itemsnot in the remaining 120 items when evaluating aranked list of DEO candidates.
We call this mea-sure AP120.In addition, we annotated DEO candidates fromthe top-150 rankings produced by our certainty-701absolve, abstain, banish, bereft, boycott, cau-tion, clear, coy, delay, denial, desist, devoid,disavow, discount, dispel, disqualify, down-play, exempt, exonerate, foil, forbid, forego,impossible, inconceivable, irrespective, limit,mitigate, nip, noone, omit, outweigh, pre-condition, pre-empt, prerequisite, refute, re-move5, repel, repulse, scarcely, scotch, scuttle,seldom, sensitive, shy, sidestep, snuff, thwart,waive, zero-toleranceFigure 4: Lemmata of DEOs identified in this work notfound by DLD09.based heuristic on BLLIP and also by the dis-tillation and heuristic methods on AFP, in orderto better evaluate the final output of the meth-ods.
This produced an additional 68 DEOs (nar-rowly defined) (Figure 4), 58 non-DEOs, and 31?other?
items4.
Adding the DEOs and non-DEOswe found to the 120 items from above, we havean expanded list of 246 items to rank, and a corre-sponding average precision which we call AP246.We employ the frequency cut-offs used byDLD09 for sparsity reasons.
A word-type mustappear at least 10 times in an NPI context and150 times in the corpus overall to be considered.We treat BLLIP as a development corpus and useAP120 on AFP to determine the number of itera-tions to run our heuristic (5 iterations for BLLIPand 13 iterations for AFP).
We run EM/distillationfor one iteration in development and testing, be-cause more iterations hurt performance, as ex-plained in Section 3.We first report the AP120 results of our ex-periments on the BLLIP corpus (Table 1 sec-ond column).
Our method outperforms bothEM/distillation and the baseline method.
Theseresults are replicated on the final test set fromAFP using the full set of annotations AP246 (Ta-ble 1 third column).
Note that the scores are lowerwhen using all the annotations because there aremore non-DEOs relative to DEOs in this list, mak-ing the ranking task more challenging.A better understanding of the algorithms can4The complete list will be made publicly available.5We disagree with DLD09 that remove is not downward-entailing; e.g., The detergent removed stains from his cloth-ing.
?
The detergent removed stains from his shirts.Method BLLIP AP120 AFP AP246Baseline .879 .734Distillation .946 .785This work .955 .809Table 1: Average precision results on the BLLIP andAFP corpora.be obtained by examining the data likelihood andthe classification certainty at each iteration of thealgorithms (Figure 5).
Whereas EM/distillationmaximizes the former expression, the certainty-based heuristic method actually decreases datalikelihood for the first couple of iterations beforeincreasing it again.
In terms of classification cer-tainty, EM/distillation converges to a lower classi-fication certainty score compared to our heuristicmethod.
Thus, our method better captures the as-sumption of one DEO per NPI context.6 Bootstrapping to Co-Learn NPIs andDEOsThe above experiments show that the heuristicmethod outperforms the EM/distillation methodgiven a list of NPIs.
We would like to extendthis result to novel domains, corpora, and lan-guages.
DLD09 and DL10 proposed the follow-ing bootstrapping algorithm for co-learning NPIsand DEOs given a much smaller list of NPIs as aseed set.1.
Begin with a small set of seed NPIs2.
Iterate:(a) Use the current list of NPIs to learn alist of DEOs(b) Use the current list of DEOs to learn alist of NPIsInterestingly, DL10 report that while thismethod works in Romanian data, it does not workin the English BLLIP corpus.
They speculate thatthe reason might be due to the nature of the En-glish DEO any, which can occur in all classes ofDE contexts according to an analysis by Haspel-math (1997).
Further, they find that in Romanian,distillation does not perform better than the base-line method during Step (2a).
While this linguis-tic explanation may certainly be a factor, we raise7020 1 2 3 4 5 6 7 8 9 10-2.5-2-1.5-1-0.50x 106IterationsLogprobability(a) Data log likelihood.0 1 2 3 4 5 6 7 8 9 10-2.5-2-1.5-1-0.50x 105IterationsLogprobability(b) Log classification certainty probabilities.Figure 5: Log likelihood and classification certainty probabilities of NPI contexts in two corpora.
Thinner linesnear the top are for BLLIP; thicker lines for AFP.
Blue dotted: baseline; red dashed: distillation; green solid:our certainty-based heuristic method.
P ( ~X|y) probabilities are not included since they would only result in aconstant offset in the log domain.a second possibility that the distillation algorithmitself may be responsible for these results.
As ev-idence, we show that the heuristic algorithm isable to work in English with just the single seedNPI any, and in fact the bootstrapping approach inconjunction with our heuristic even outperformsthe above approaches when using a static list ofNPIs.In particular, we use the methods described inthe previous sections for Step (2a), and the follow-ing ratio to rank NPI candidates in Step (2b), cor-responding to the baseline method to detect DEOsin reverse:T (x) = countD(x)/tokens(D)countC(x)/tokens(C) .
(25)Here, countD(x) refers to the number of oc-currences of NPI candidate x in DEO contextsD, defined to be the words to the right of a DEOoperator up to a comma or semi-colon.
We donot use the EM/distillation or heuristic methods inStep (2b).
Learning NPIs from DEOs is a muchharder problem than learning DEOs from NPIs.Because DEOs (and other non-veridical opera-tors) license NPIs, the majority of occurrences ofNPIs will be in the context of a DEO, modulo am-biguity of DEOs such as the free-choice any andother spurious correlations such as piggybackersas discussed earlier.
In the other direction, it isnot the case that DEOs always or nearly alwaysappear in the context of an NPI.
Rather, the mostcommon collocations of DEOs are the selectionalpreferences of the DEO, such as common argu-ments to verbal DEOs, prepositions that are partof the subcategorization of the DEO, and wordsthat together with the surface form of the DEOcomprise an idiomatic expression or multi-wordexpression.
Further, NPIs are more likely to becomposed of multiple words, while many DEOsare single words, possibly with PP subcategoriza-tion requirements which can be filled in post hoc.Because of these issues, we cannot trust the ini-tialization to learn NPIs nearly as much as withDEOs, and cannot use the distillation or certaintymethods for this step.
Rather, the hope is thatlearning a noisy list of ?pseudo-NPIs?, which of-ten occur in negative contexts but may not actu-ally be NPIs, can still improve the performance ofDEO detection.There are a number of parameters to the methodwhich we tuned to the BLLIP corpus usingAP120.
At the end of Step (2a), we use the cur-rent top 25 DEOs plus 5 per iteration as the DEOlist for the next step.
To the initial seed NPI of703Method BLLIP AP120 AFP AP246Baseline .889 (+.010) .739 (?.005)Distillation .930 (?.016) .804 (+.019)This work .962 (+.007) .821 (+.012)Table 2: Average precision results with bootstrappingon the BLLIP and AFP corpora.
Absolute gain in av-erage precision compared to using a fixed list of NPIsgiven in brackets.anymore, anything, anytime, avail, bother,bothered, budge, budged, countenance, faze,fazed, inkling, iota, jibe, mince, nor, whatso-ever, whitFigure 6: Probable NPIs found by bootstrapping usingthe certainty-based heuristic method.any, we add the top 5 ranking NPI candidates atthe end of Step (2b) in each subsequent iteration.We ran the bootstrapping algorithm for 11 itera-tions for all three algorithms.
The final evaluationwas done on AFP using AP246.The results show that bootstrapping can indeedimprove performance, even in English (Table 2).Using bootstrapping to co-learn NPIs and DEOsactually results in better performance than spec-ifying a static list of NPIs.
The certainty-basedheuristic in particular achieves gains with boot-strapping in both corpora, in contrast to the base-line and distillation methods.
Another factor thatwe found to be important is to add a sufficientnumber of NPIs to the NPI list each iteration, asadding too few NPIs results in only a small changein the NPI contexts available for DEO detection.DL10 only added one NPI per iteration, whichmay explain why they did not find any improve-ment with bootstrapping in English.
It also ap-pears that learning the pseudo-NPIs does not hurtperformance in detecting DEO, and further, thata number of true NPIs are learned by our method(Figure 6).7 ConclusionWe have proposed a novel unsupervised methodfor discovering downward-entailing operatorsfrom raw text based on their co-occurrence withnegative polarity items.
Unlike the distilla-tion method of DLD09, which we show tobe an instance of EM prior re-estimation, ourmethod directly addresses the issue of piggyback-ers which spuriously correlate with NPIs but arenot downward-entailing.
This is achieved bymaximizing the posterior classification certaintyof the corpus in a way that respects the initializa-tion, rather than maximizing the data likelihoodas in EM/distillation.
Our method outperformsdistillation and a baseline method on two corporaas well as in a bootstrapping setting where NPIsand DEOs are jointly learned.
It achieves the bestperformance in the bootstrapping setting, ratherthan when using a fixed list of NPIs.
The perfor-mance of our algorithm suggests that it is suitablefor other corpora and languages.Interesting future research directions includedetecting DEOs of more than one word as well asdistinguishing the particular word sense and sub-categorization that is downward-entailing.
An-other problem that should be addressed is thescope of the downward entailment, generalizingwork being done in detecting the scope of nega-tion (Councill et al 2010, for example).AcknowledgmentsWe would like to thank Cristian Danescu-Niculescu-Mizil for his help with replicating hisresults on the BLLIP corpus.
This project wassupported by the Natural Sciences and Engineer-ing Research Council of Canada.ReferencesLuisa Bentivogli, Peter Clark, Ido Dagan, Hoa T.Dang, and Danilo Giampiccolo.
2010.
The sixthpascal recognizing textual entailment challenge.
InThe Text Analysis Conference (TAC 2010).Isaac G. Councill, Ryan McDonald, and Leonid Ve-likovich.
2010.
What?s great and what?s not:Learning to classify the scope of negation for im-proved sentiment analysis.
In Proceedings of theWorkshop on Negation and Speculation in NaturalLanguage Processing, pages 51?59.
Association forComputational Linguistics.Cristian Danescu-Niculescu-Mizil and Lillian Lee.2010.
Don?t ?have a clue??
: Unsupervised co-learning of downward-entailing operators.
In Pro-ceedings of the ACL 2010 Conference Short Papers,pages 247?252.
Association for Computational Lin-guistics.Cristian Danescu-Niculescu-Mizil, Lillian Lee, andRichard Ducott.
2009.
Without a ?doubt??
: Un-supervised discovery of downward-entailing oper-704ators.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One sense per discourse.
In Pro-ceedings of the Workshop on Speech and NaturalLanguage, pages 233?237.
Association for Compu-tational Linguistics.Anastasia Giannakidou.
2002.
Licensing and sensitiv-ity in polarity items: from downward entailment tononveridicality.
CLS, 38:29?53.Martin Haspelmath.
1997.
Indefinite pronouns.
Ox-ford University Press.Jack Hoeksema.
1997.
Corpus study of negative po-larity items.
IV-V Jornades de corpus linguistics1996?1997.William A. Ladusaw.
1980.
On the notion ?affective?in the analysis of negative-polarity items.
Journalof Linguistic Research, 1(2):1?16.Timm Lichte and Jan-Philipp Soehn.
2007.
The re-trieval and classification of negative polarity itemsusing statistical profiles.
Roots: Linguistics inSearch of Its Evidential Base, pages 249?266.Bill MacCartney and Christopher D. Manning.
2008.Modeling semantic containment and exclusion innatural language inference.
In Proceedings of the22nd International Conference on ComputationalLinguistics.Frank Richter, Fabienne Fritzinger, and Marion Weller.2010.
Who can see the forest for the trees?
ex-tracting multiword negative polarity items fromdependency-parsed text.
Journal for LanguageTechnology and Computational Linguistics, 25:83?110.Ton van der Wouden.
1997.
Negative Contexts: Col-location, Polarity and Multiple Negation.
Rout-ledge.705
