Symmetric Word Alignments for Statistical Machine TranslationEvgeny Matusov and Richard Zens and Hermann NeyLehrstuhl fu?r Informatik VI, Computer Science DepartmentRWTH Aachen UniversityD-52056 Aachen, Germany{matusov,zens,ney}@cs.rwth-aachen.deAbstractIn this paper, we address the wordalignment problem for statistical machinetranslation.
We aim at creating a sym-metric word alignment allowing for reli-able one-to-many and many-to-one wordrelationships.
We perform the iterativealignment training in the source-to-targetand the target-to-source direction withthe well-known IBM and HMM alignmentmodels.
Using these models, we robustlyestimate the local costs of aligning a sourceword and a target word in each sentencepair.
Then, we use efficient graph algo-rithms to determine the symmetric align-ment with minimal total costs (i. e. max-imal alignment probability).
We evalu-ate the automatic alignments created inthis way on the German?English Verb-mobil task and the French?English Cana-dian Hansards task.
We show statisticallysignificant improvements of the alignmentquality compared to the best results re-ported so far.
On the Verbmobil task,we achieve an improvement of more than1% absolute over the baseline error rate of4.7%.1 IntroductionWord-aligned bilingual corpora provide im-portant knowledge for many natural languageprocessing tasks, such as the extraction ofbilingual word or phrase lexica (Melamed,2000; Och and Ney, 2000).
The solutions ofthese problems depend heavily on the qualityof the word alignment (Och and Ney, 2000).Word alignment models were first introducedin statistical machine translation (Brown etal., 1993).
An alignment describes a mappingfrom source sentence words to target sentencewords.Using the IBM translation models IBM-1to IBM-5 (Brown et al, 1993), as well asthe Hidden-Markov alignment model (Vogel etal., 1996), we can produce alignments of goodquality.
However, all these models constrainthe alignments so that a source word can bealigned to at most one target word.
This con-straint is useful to reduce the computationalcomplexity of the model training, but makesit hard to align phrases in the target lan-guage (English) such as ?the day after tomor-row?
to one word in the source language (Ger-man) ?u?bermorgen?.
We will present a wordalignment algorithm which avoids this con-straint and produces symmetric word align-ments.
This algorithm considers the align-ment problem as a task of finding the edgecover with minimal costs in a bipartite graph.The parameters of the IBM models and HMM,in particular the state occupation probabili-ties, will be used to determine the costs ofaligning a specific source word to a targetword.We will evaluate the suggested alignmentmethods on the German?English Verbmo-bil task and the French?English CanadianHansards task.
We will show statistically sig-nificant improvements compared to state-of-the-art results in (Och and Ney, 2003).2 Statistical Word Alignment ModelsIn this section, we will give an overview ofthe commonly used statistical word alignmenttechniques.
They are based on the source-channel approach to statistical machine trans-lation (Brown et al, 1993).
We are givena source language sentence fJ1 := f1...fj ...fJwhich has to be translated into a target lan-guage sentence eI1 := e1...ei...eI .
Among allpossible target language sentences, we willchoose the sentence with the highest proba-bility:e?I1 = argmaxeI1{Pr(eI1|fJ1 )}= argmaxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)}This decomposition into two knowledgesources allows for an independent modeling oftarget language model Pr(eI1) and translationmodel Pr(fJ1 |eI1).
Into the translation model,the word alignment A is introduced as a hid-den variable:Pr(fJ1 |eI1) =?APr(fJ1 , A|eI1)Usually, the alignment is restricted in thesense that each source word is aligned to atmost one target word, i.e.
A = aJ1 .
The align-ment may contain the connection aj = 0 withthe ?empty?
word e0 to account for source sen-tence words that are not aligned to any tar-get word at all.
A detailed description of thepopular translation/alignment models IBM-1to IBM-5 (Brown et al, 1993), as well as theHidden-Markov alignment model (HMM) (Vo-gel et al, 1996) can be found in (Och and Ney,2003).
Model 6 is a loglinear combination ofthe IBM-4, IBM-1, and the HMM alignmentmodels.A Viterbi alignment A?
of a specific model isan alignment for which the following equationholds:A?
= argmaxA{Pr(fJ1 , A|eI1)} .3 State Occupation ProbabilitiesThe training of all alignment models is doneusing the EM-algorithm.
In the E-step, thecounts for each sentence pair (fJ1 , eI1) are cal-culated.
Here, we present this calculation onthe example of the HMM.
For its lexicon pa-rameters, the marginal probability of a targetword ei to occur at the target sentence posi-tion i as the translation of the source word fjat the source sentence position j is estimatedwith the following sum:pj(i, fJ1 |eI1) =?aJ1 :aj=iPr(fJ1 , aJ1 |eI1)This value represents the likelihood of aligningfj to ei via every possible alignment A = aJ1that includes the alignment connection aj = i.By normalizing over the target sentence posi-tions, we arrive at the state occupation proba-bility :pj(i|fJ1 , eI1) =pj(i, fJ1 |eI1)I?i?=1pj(i?, fJ1 |eI1)In the M-step of the EM training, the stateoccupation probabilities are aggregated for allwords in the source and target vocabulariesby taking the sum over all training sentencepairs.
After proper renormalization the lexi-con probabilities p(f |e) are determined.Similarly, the training can be performedin the inverse (target-to-source) direction,yielding the state occupation probabilitiespi(j|eI1, fJ1 ).The negated logarithms of the state occu-pation probabilitiesw(i, j; fJ1 , eI1) := ?
log pj(i|fJ1 , eI1) (1)can be viewed as costs of aligning the sourceword fj with the target word ei.
Thus, theword alignment task can be formulated as thetask of finding a mapping between the sourceand the target words, so that each source andeach target position is covered and the totalcosts of the alignment are minimal.Using state occupation probabilities forword alignment modeling results in a num-ber of advantages.
First of all, in calculationof these probabilities with the models IBM-1,IBM-2 and HMM the EM-algorithm is per-formed exact, i.e.
the summation over allalignments is efficiently performed in the E-step.
For the HMM this is done using theBaum-Welch algorithm (Baum, 1972).
So far,an efficient algorithm to compute the sum overall alignments in the fertility models IBM-3to IBM-5 is not known.
Therefore, this sumis approximated using a subset of promisingalignments (Och and Ney, 2000).
In bothcases, the resulting estimates are more pre-cise than the ones obtained by the maximumapproximation, i. e. by considering only theViterbi alignment.Instead of using the state occupation prob-abilities from only one training direction ascosts (Equation 1), we can interpolate thestate occupation probabilities from the source-to-target and the target-to-source training foreach pair (i,j) of positions in a sentence pair(fJ1 , eI1).
This will improve the estimation ofthe local alignment costs.
Having such sym-metrized costs, we can employ the graph align-ment algorithms (cf.
Section 4) to producereliable alignment connections which includemany-to-one and one-to-many alignment re-lationships.
The presence of both relation-ship types characterizes a symmetric align-ment that can potentially improve the trans-lation results (Figure 1 shows an example of asymmetric alignment).Another important advantage is the effi-ciency of the graph algorithms used to deter-Figure 1: Example of a symmetric alignmentwith one-to-many and many-to-one connec-tions (Verbmobil task, spontaneous speech).mine the final symmetric alignment.
They willbe discussed in Section 4.4 Alignment AlgorithmsIn this section, we describe the alignment ex-traction algorithms.
We assume that for eachsentence pair (fJ1 , eI1) we are given a cost ma-trix C.1 The elements of this matrix cij arethe local costs that result from aligning sourceword fj to target word ei.
For a given align-ment A ?
I ?
J , we define the costs of thisalignment c(A) as the sum of the local costsof all aligned word pairs:c(A) =?
(i,j)?Acij (2)Now, our task is to find the alignment with theminimum costs.
Obviously, the empty align-ment has always costs of zero and would be op-timal.
To avoid this, we introduce additionalconstraints.
The first constraint is source sen-tence coverage.
Thus each source word hasto be aligned to at least one target word oralternatively to the empty word.
The secondconstraint is target sentence coverage.
Similarto the source sentence coverage thus each tar-get word is aligned to at least one source wordor the empty word.Enforcing only the source sentence cover-age, the minimum cost alignment is a mappingfrom source positions j to target positions aj ,including zero for the empty word.
Each tar-get position aj can be computed as:aj = argmini{cij}This means, in each column we choose therow with the minimum costs.
This method re-sembles the common IBM models in the sense1For notational convenience, we omit the depen-dency on the sentence pair (fJ1 , eI1) in this section.that the IBM models are also a mapping fromsource positions to target positions.
There-fore, this method is comparable to the IBMmodels for the source-to-target direction.
Sim-ilarly, if we enforce only the target sentencecoverage, the minimum cost alignment is amapping from target positions i to source po-sitions bi.
Here, we have to choose in eachrow the column with the minimum costs.
Thecomplexity of these algorithms is in O(I ?
J).The algorithms for determining such a non-symmetric alignment are rather simple.
Amore interesting case arises, if we enforce bothconstraints, i.e.
each source word as well aseach target word has to be aligned at leastonce.
Even in this case, we can find the globaloptimum in polynomial time.The task is to find a symmetric alignmentA, for which the costs c(A) are minimal (Equa-tion 2).
This task is equivalent to findinga minimum-weight edge cover (MWEC) in acomplete bipartite graph2.
The two nodesets of this bipartite graph correspond to thesource sentence positions and the target sen-tence positions, respectively.
The costs of anedge are the elements of the cost matrix C.To solve the minimum-weight edge coverproblem, we reduce it to the maximum-weightbipartite matching problem.
As describedin (Keijsper and Pendavingh, 1998), this re-duction is linear in the graph size.
For themaximum-weight bipartite matching problem,well-known algorithm exist, e.g.
the Hungar-ian method.
The complexity of this algorithmis in O((I + J) ?
I ?
J).
We will call the solu-tion of the minimum-weight edge cover prob-lem with the Hungarian method ?the MWECalgorithm?.
In contrary, we will refer to the al-gorithm enforcing either source sentence cov-erage or target sentence coverage as the one-sided minimum-weight edge cover algorithm(o-MWEC).The cost matrix of a sentence pair (fJ1 , eI1)can be computed as a weighted linear interpo-lation of various cost types hm:cij =M?m=1?m ?
hm(i, j)In our experiments, we will use the negatedlogarithm of the state occupation probabilitiesas described in Section 3.
To obtain a moresymmetric estimate of the costs, we will inter-polate both the source-to-target direction and2An edge cover of G is a set of edges E?
such thateach node of G is incident to at least one edge in E?.the target-to-source direction (thus the stateoccupation probabilities are interpolated log-linearly).
Because the alignments determinedin the source-to-target training may substan-tially differ in quality from those produced inthe target-to-source training, we will use aninterpolation weight ?
:cij = ?
?w(i, j; fJ1 , eI1) + (1??)
?w(j, i; eI1, fJ1 )(3)Additional feature functions can be includedto compute cij ; for example, one could makeuse of a bilingual word or phrase dictionary.To apply the methods described in this sec-tion, we made two assumptions: first, the costsof an alignment can be computed as the sumof local costs.
Second, the features have to bestatic in the sense that we have to fix the costsbefore aligning any word.
Therefore, we can-not apply dynamic features such as the IBM-4 distortion model in a straightforward way.One way to overcome these restrictions lies inusing the state occupation probabilities; e.g.for IBM-4, they contain the distortion modelto some extent.5 Results5.1 Evaluation CriterionWe use the same evaluation criterion as de-scribed in (Och and Ney, 2000).
We comparethe generated word alignment to a referencealignment produced by human experts.
Theannotation scheme explicitly takes the am-biguity of the word alignment into account.There are two different kinds of alignments:sure alignments (S) which are used for unam-biguous alignments and possible alignments(P ) which are used for alignments that mightor might not exist.
The P relation is usedespecially to align words within idiomatic ex-pressions and free translations.
It is guaran-teed that the sure alignments are a subset ofthe possible alignments (S ?
P ).
The ob-tained reference alignment may contain many-to-one and one-to-many relationships.The quality of an alignment A is computedas appropriately redefined precision and recallmeasures.
Additionally, we use the alignmenterror rate (AER), which is derived from thewell-known F-measure.recall = |A ?
S||S| , precision =|A ?
P ||A|AER(S, P ;A) = 1?
|A ?
S|+ |A ?
P ||A|+ |S|Table 1: Verbmobil task: corpus statistics.Source/Target: German EnglishTrain Sentences 34 446Words 329 625 343 076Vocabulary 5 936 3 505Singletons 2 600 1 305Dictionary Entries 4 404Test Sentences 354Words 3 233 3 109S reference relations 2 559P reference relations 4 596Table 2: Canadian Hansards: corpus statistics.Source/Target: French EnglishTrain Sentences 128KWords 2.12M 1.93MVocabulary 37 542 29 414Singletons 12 986 9 572Dictionary Entries 28 701Test Sentences 500Words 8 749 7 946S reference relations 4 443P reference relations 19 779With these definitions a recall error can onlyoccur if a S(ure) alignment is not found and aprecision error can only occur if a found align-ment is not even P (ossible).5.2 Experimental SetupWe evaluated the presented lexicon sym-metrization methods on the Verbmobil andthe Canadian Hansards task.
The German?English Verbmobil task (Wahlster, 2000) is aspeech translation task in the domain of ap-pointment scheduling, travel planning and ho-tel reservation.
The French?English CanadianHansards task consists of the debates in theCanadian Parliament.The corpus statistics are shown in Table 1and Table 2.
The number of running wordsand the vocabularies are based on full-formwords including punctuation marks.
As in(Och and Ney, 2003), the first 100 sentencesof the test corpus are used as a developmentcorpus to optimize model parameters that arenot trained via the EM algorithm, e.g.
theinterpolation weights.
The remaining part ofthe test corpus is used to evaluate the models.We use the same training schemes (modelsequences) as presented in (Och and Ney,2003): 15H5334363 for the Verbmobil Task ,i.e.
5 iteration of IBM-1, 5 iterations of theHMM, 3 iteration of IBM-3, etc.
; for the Cana-dian Hansards task, we use 15H10334363.
Werefer to these schemes as the Model 6 schemes.For comparison, we also perform less sophisti-cated trainings, to which we refer as the HMMschemes (15H10 and 15H5, respectively), aswell as the IBM Model 4 schemes (15H103343and 15H53343).In all training schemes we use a conventionaldictionary (possibly containing phrases) as ad-ditional training material.
Because we use thesame training and testing conditions as (Ochand Ney, 2003), we will refer to the results pre-sented in that article as the baseline results.5.3 Non-symmetric AlignmentsIn the first experiments, we use the state oc-cupation probabilities from only one transla-tion direction to determine the word align-ment.
This allows for a fair comparison withthe Viterbi alignment computed as the resultof the training procedure.
In the source-to-target translation direction, we cannot esti-mate the probability for the target words withfertility zero and choose to set it to 0.
In thiscase, the minimum weight edge cover problemis solved by the one-sided MWEC algorithm.Like the Viterbi alignments, the alignmentsproduced by this algorithm satisfy the con-straint that multiple source (target) words canonly be aligned to one target (source) word.Tables 3 and 4 show the performance ofthe one-sided MWEC algorithm in compar-ison with the experiment reported by (Ochand Ney, 2003).
We report not only the finalalignment error rates, but also the intermedi-ate results for the HMM and IBM-4 trainingschemes.For IBM-3 to IBM-5, the Viterbi alignmentand a set of promising alignments are usedto determine the state occupation probabili-ties.
Consequently, we observe similar align-ment quality when comparing the Viterbi andthe one-sided MWEC alignments.We also evaluated the alignment quality af-ter applying alignment generalization meth-ods, i.e.
we combine the alignment of bothtranslation directions.
Experimentally, thebest generalization heuristic for the CanadianHansards task is the intersection of the source-to-target and the target-to-source alignments.For the Verbmobil task, the refined methodof (Och and Ney, 2003) is used.
Again, weobserved similar alignment error rates whenmerging either the Viterbi alignments or theo-MWEC alignments.Table 3: AER [%] for non-symmetric align-ment methods and for various models (HMM,IBM-4, Model 6) on the Canadian Hansardstask.Alignment method HMM IBM4 M6Baseline T?S 14.1 12.9 11.9S?T 14.4 12.8 11.7intersection 8.4 6.9 7.8o-MWEC T?S 14.0 13.1 11.9S?T 14.3 13.0 11.7intersection 8.2 7.1 7.8Table 4: AER [%] for non-symmetric align-ment methods and for various models (HMM,IBM-4, Model 6) on the Verbmobil task.Alignment method HMM IBM4 M6Baseline T?S 7.6 4.8 4.6S?T 12.1 9.3 8.8refined 7.1 4.7 4.7o-MWEC T?S 7.3 4.8 4.5S?T 12.0 9.3 8.5refined 6.7 4.6 4.65.4 Symmetric AlignmentsThe heuristically generalized Viterbi align-ments presented in the previous section canpotentially avoid the alignment constraints3.However, the choice of the optimal general-ization heuristic may depend on a particularlanguage pair and may require extensive man-ual optimization.
In contrast, the symmetricMWEC algorithm is a systematic and theo-retically well-founded approach to the task ofproducing a symmetric alignment.In the experiments with the symmetricMWEC algorithm, the optimal interpolationparameter ?
(see Equation 3) for the Verbmo-bil corpus was empirically determined as 0.8.This shows that the model parameters can beestimated more reliably in the direction fromGerman to English.
In the inverse English-to-German alignment training, the mappingsof many English words to one German wordare not allowed by the modeling constraints,although such alignment mappings are signif-icantly more frequent than mappings of manyGerman words to one English word.The experimentally best interpolation pa-rameter for the Canadian Hansards corpus was?
= 0.5.
Thus the model parameters esti-mated in the translation direction from Frenchto English are as reliable as the ones estimated3Consequently, we will use them as baseline for theexperiments with symmetric alignments.in the direction from English to French.Lines 2a and 2b of Table 5 show the perfor-mance of the MWEC algorithm.
The align-ment error rates are slightly lower if the HMMor the full Model 6 training scheme is usedto train the state occupation probabilities onthe Canadian Hansards task.
On the Verbmo-bil task, the improvement is more significant,yielding an alignment error rate of 4.1%.Columns 4 and 5 of Table 5 contain the re-sults of the experiments, in which the costscij were determined as the loglinear interpola-tion of state occupation probabilities obtainedfrom the HMM training scheme with thosefrom IBM-4 (column 4) or from Model 6 (col-umn 5).
We set the interpolation parametersfor the two translation directions proportionalto the optimal values determined in the previ-ous experiments.
On the Verbmobil task, weobtain a further improvement of 19% relativeover the baseline result reported in (Och andNey, 2003), reaching an AER as low as 3.8%.The improvements of the alignment qual-ity on the Canadian Hansards task are lesssignificant.
The manual reference alignmentsfor this task contain many possible connec-tions and only a few sure connections (cf.
Ta-ble 2).
Thus automatic alignments consistingof only a few reliable alignment points are fa-vored.
Because the differences in the numberof words and word order between French andEnglish are not as dramatic as e.g.
betweenGerman and English, the probability of theempty word alignment is not very high.
There-fore, plenty of alignment points are producedby the MWEC algorithm, resulting in a highrecall and low precision.
To increase the preci-sion, we replaced the empty word connectioncosts (previously trained as state occupationprobabiliities using the EM algorithm) by theglobal, word- and position-independent costsdepending only on one of the involved lan-guages.
The alignment error rates for theseexperiments are given in lines 3a and 3b of Ta-ble 5.
The global empty word probability forthe Canadian Hansards task was empiricallyset to 0.45 for French and for English, and,for the Verbmobil task, to 0.6 for German and0.1 for English.
On the Canadian Hansardstask, we achieved further significant reductionof the AER.
In particular, we reached an AERof 6.6% by performing only the HMM training.In this case the effectiveness of the MWEC al-gorithm is combined with the efficiency of theHMM training, resulting in a fast and robustalignment training procedure.We also tested the more simple one-sidedMWEC algorithm.
In contrast to the exper-iments presented in Section 5.3, we used theloglinear interpolated state occupation prob-abilities (given by the Equation 3) as costs.Thus, although the algorithm is not able toproduce a symmetric alignment, it operateswith symmetrized costs.
In addition, we useda combination heuristic to obtain a symmetricalignment.
The results of these experimentsare presented in Table 5, lines 4-6 a/b.The performance of the one-sided MWECalgorithm turned out to be quite robust onboth tasks.
However, the o-MWEC align-ments are not symmetric and the achieved lowAER depends heavily on the differences be-tween the involved languages, which may fa-vor many-to-one alignments in one translationdirection only.
That is why on the Verbmobiltask, when determining the mininum weight ineach row for the translation direction from En-glish to German, the alignment quality deteri-orates, because the algorithm cannot producealignments which map several English wordsto one German word (line 5b of Table 5).Applying the generalization heuristics(line 6a/b of Table 5), we achieve an AER of6.0% on the Canadian Hansards task wheninterpolating the state occupation probabil-ities trained with the HMM and with theIBM-4 schemes.
On the Verbmobil task, theinterpolation of the HMM and the Model 6schemes yields the best result of 3.7% AER.In the latter experiment, we reached 97.3%precision and 95.2% recall.6 Related WorkA description of the IBM models for statisticalmachine translation can be found in (Brown etal., 1993).
The HMM-based alignment modelwas introduced in (Vogel et al, 1996).
Anoverview of these models is given in (Och andNey, 2003).
That article also introduces theModel 6; additionally, state-of-the-art resultsare presented for the Verbmobil task and theCanadian Hansards task for various configura-tions.
Therefore, we chose them as baseline.Additional linguistic knowledge sources suchas dependeny trees or parse trees were used in(Cherry and Lin, 2003; Gildea, 2003).
Bilin-gual bracketing methods were used to producea word alignment in (Wu, 1997).
(Melamed,2000) uses an alignment model that enforcesone-to-one alignments for nonempty words.Table 5: AER[%] for different alignment symmetrization methods and for various alignmentmodels on the Canadian Hansards and the Verbmobil tasks (MWEC: minimum weight edgecover, EW: empty word).Symmetrization Method HMM IBM4 M6 HMM + IBM4 HMM + M6Canadian 1a.
Baseline (intersection) 8.4 6.9 7.8 ?
?Hansards 2a.
MWEC 7.9 9.3 7.5 8.2 7.43a.
MWEC (global EW costs) 6.6 7.4 6.9 6.4 6.44a.
o-MWEC T?S 7.3 7.9 7.4 6.7 7.05a.
S?T 7.7 7.6 7.2 6.9 6.96a.
S?T (intersection) 7.2 6.6 7.6 6.0 7.1Symmetrization Method HMM IBM4 M6 HMM + IBM4 HMM + M6Verbmobil 1b.
Baseline (refined) 7.1 4.7 4.7 ?
?2b.
MWEC 6.4 4.4 4.1 4.3 3.83b.
MWEC (global EW costs) 5.8 5.8 6.6 6.0 6.74b.
o-MWEC T?S 6.8 4.4 4.1 4.5 3.75b.
S?T 9.3 7.2 6.8 7.5 6.96b.
S?T (refined) 6.7 4.3 4.1 4.6 3.77 ConclusionsIn this paper, we addressed the task of au-tomatically generating symmetric word align-ments for statistical machine translation.
Weexploited the state occupation probabilties de-rived from the IBM and HMM translationmodels.
We used the negated logarithms ofthese probabilities as local alignment costs andreduced the word alignment problem to find-ing an edge cover with minimal costs in abipartite graph.
We presented efficient algo-rithms for the solution of this problem.
Weevaluated the performance of these algorithmsby comparing the alignment quality to man-ual reference alignments.
We showed that in-terpolating the alignment costs of the source-to-target and the target-to-source translationdirections can result in a significant improve-ment of the alignment quality.In the future, we plan to integrate the graphalgorithms into the iterative training proce-dure.
Investigating the usefulness of addi-tional feature functions might be interestingas well.AcknowledgmentThis work has been partially funded by theEU project TransType 2, IST-2001-32091.ReferencesL.
E. Baum.
1972.
An inequality and associatedmaximization technique in statistical estimationfor probabilistic functions of markov processes.Inequalities, 3:1?8.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra,and R. L. Mercer.
1993.
The mathematics ofstatistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311, June.C.
Cherry and D. Lin.
2003.
A probability modelto improve word alignment.
In Proc.
of the 41thAnnual Meeting of the Association for Compu-tational Linguistics (ACL), pages 88?95, Sap-poro, Japan, July.D.
Gildea.
2003.
Loosely tree-based alignment formachine translation.
In Proc.
of the 41th An-nual Meeting of the Association for Computa-tional Linguistics (ACL), pages 80?87, Sapporo,Japan, July.J.
Keijsper and R. Pendavingh.
1998.
An effi-cient algorithm for minimum-weight bibranch-ing.
Journal of Combinatorial Theory Series B,73(2):130?145, July.I.
D. Melamed.
2000.
Models of translationalequivalence among words.
Computational Lin-guistics, 26(2):221?249.F.
J. Och and H. Ney.
2000.
Improved statisticalalignment models.
In Proc.
of the 38th AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 440?447, Hong Kong,October.F.
J. Och and H. Ney.
2003.
A systematic com-parison of various statistical alignment models.Computational Linguistics, 29(1):19?51, March.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based word alignment in statistical translation.In COLING ?96: The 16th Int.
Conf.
on Com-putational Linguistics, pages 836?841, Copen-hagen, Denmark, August.W.
Wahlster, editor.
2000.
Verbmobil: Founda-tions of speech-to-speech translations.
SpringerVerlag, Berlin, Germany, July.D.
Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3):377?403, September.
