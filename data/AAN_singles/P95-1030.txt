New Techniques for Context ModelingEr ic  Sven  R is tad  and  Rober t  G .
ThomasDepar tment  of Computer  Sc iencePr inceton  Un ivers i ty{ristad, rgt }~cs.
princeton, eduAbst ractWe introduce three new techniques for sta-tistical language models: extension mod-eling, nonmonotonic contexts, and the di-vergence heuristic.
Together these tech-niques result in language models that havefew states, even fewer parameters, and lowmessage ntropies.1 In t roduct ionCurrent approaches to automatic speech and hand-writing transcription demand a strong languagemodel with a small number of states and an evensmaller number of parameters.
If the model entropyis high, then transcription results are abysmal.
Ifthere are too many states, then transcription be-comes computationally infeasible.
And if there aretoo many parameters; then "overfitting" occurs andpredictive performance degrades.In this paper we introduce three new techniquesfor statistical language models: extension modeling,nonmonotonic contexts, and the divergence heuris-tic.
Together these techniques result in languagemodels that have few states, even fewer parameters,and low message ntropies.
For example, our tech-niques achieve a message ntropy of 1.97 bits/charon the Brown corpus using only 89,325 parameters.By modestly increasing the number of model param-eters in a principled manner, our techniques are ableto further reduce the message ntropy of the BrownCorpus to 1.91 bits/char.
1 In contrast, the charac-ter 4-gram model requires 250 times as many pa-rameters in order to achieve a message ntropy ofonly 2.47 bits/char.
Given the logarithmic natureof codelengths, a savings of 0.5 bits/char is quitesignificant.
The fact that our model performs ignif-icantly better using vastly fewer parameters argues1The only change to our model selection procedure isto replace the incremental cost formula ALe(w, ~', a)with a constant cost of 2 bits/extension.
This smallchange reduces the test message ntropy from 1.97 to1.91 bits/char but it also quadruples the number ofmodel parameters and triples the total codelength.that it is a much better probability model of naturallanguage text.Our first two techniques - nonmono lon ic  contextsand ex lens ion  mode l ing  - are generalizations of thetraditional context model (Cleary and Witten 1984;Rissanen 1983,1986).
Our third technique - the di-vergence heur i s t i c  - is an incremental model selec-tion criterion based directly on Rissanen's (1978)minimum description length (MDL) principle.
TheMDL principle states that the best model is the sim-plest model that provides a compact description ofthe observed ata.In the traditional context model, every prefix andevery suffix of a context is also a context.
Threeconsequences follow from this property.
The firstconsequence is that the context dictionary is un-necessarily large because most of these contexts areredundant.
The second consequence is to attenu-ate the benefits of context blending, because mostcontexts are equivalent to their maximal proper suf-fixes.
The third consequence is that the length of thelongest candidate context can increase by at mostone symbol at each time step, which impairs themodel's ability to model complex sources.
In a non-monotonic model, this constraint is relaxed to allowcompact dictionaries, discontinuous backoff, and ar-bitrary context switching.The traditional context model maps every historyto a unique context.
All symbols are predicted us-ing that context, and those predictions are estimatedusing the same set of histories.
In contrast, an exten-sion model maps every history to a sel  of contexts,one for each symbol in the alphabet.
Each symbol ispredicted in its own context, and the model's currentpredictions need not be estimated using the sameset of histories.
This is a form of parameter tyingthat increases the accuracy of the model's predic-tions while reducing the number of free parametersin the model.As a result of these two generalizations, nonmono-tonic extension models can outperform their equiv-alent context models using significantly fewer pa-rameters.
For example, an order 3 n-gram (ie., the4-gram) requires more than 51 times as many con-220texts and 787 times as many parameters as the order3 nonmonotonic extension model, yet aleady per-forms worse on the Brown corpus by 0.08 bits/char.Our third contribution is the divergence heuris-tic, which adds a more specific context o the modelonly when it reduces the codelength of the past datamore than it increases the codelength of the model.In contrast, the traditional selection heuristic adds amore specific context o the model only if it's entropyis less than the entropy of the more general context(Rissanen 1983,1986).
The traditional minimum en-tropy heuristic is a special case of the more effectiveand more powerful divergence heuristic.
The diver-gence heuristic allows our models to generalize fromthe training corpus to the testing corpus, even fornonstationary sources uch as the Brown corpus.The remainder of our article is organized intothree sections.
In section 2, we formally define theclass of extension models and present a heuristicmodel selection algorithm for that model class basedon the divergence criterion.
Next, in section 3, wedemonstrate the efficacy of our techniques on theBrown Corpus, an eclectic ollection of English prosecontaining approximately one million words of text.Section 4 discusses possible improvements to themodel class.2 Extens ion  Mode l  C lassThis section consists of four parts.
In 2.1, we for-mally define the class of extension models and provethat they satisfy the axioms of probability.
In 2.2,we show to estimate the parameters of an exten-sion model using Moffat's (1990) "method C." In 2.3,we provide codelength formulas for our model class,based on efficient enumerative codes.
These code-length formulas will be used to match the complexityof the model to the complexity of the data.
In 2.4,we present a heuristic model selection algorithm thatadds parameters to an extension model only whenthey reduce the codelength of the data more thanthey increase the codelength of the model.2.1 Mode l  Class Def in i t ionFormally, an extension model ?
: (E, D, E, A) con-sists of a finite alphabet E, \[E\[ = m, a dictionaryD of contexts, D C E*, a set of available contextextensions E, E C D x E, and a probability func-tion I : E ---* \[0, 1\].
For every context w in D, E(w)is the set of symbols available in the context w andA(~rlw ) is the conditional probability of the symbolc~ in the context w. Note that )--\]o~ A(c~\[w) < 1 forall contexts w in the dictionary D.The probability /5(h1? )
of a string h given themodel ?, h ?
E ' ,  is calculated as a chain of con-ditional probabilities (1)/5(h{?)
--" ~(hnlhl...hn_l,?)~(hl...h,~_ll?)
(1while the conditional probability ih(elh, ?)
of a singlesymbol ~r after the history h is defined as (2).
{ ~(~rlh ) if (h,a) ~ E/3(a\]h, ?)
- 5(h)~(a\]h2h3...h,, ?)
otherwise(2)The expansion factor 6(h) ensures that/5(.\]h, ?)
is aprobability function if/5(-Ih2.., h,~, ?)
is a probabil-ity function.1 - )~(E(h)\[h) (3)6(h)-  1 -  ~(E(h)Ih2...h,~,?
)Note that E(h) represents a set of symbols, andso by a slight abuse of notation )~(E(h)Ih ) denotes~\]~eE(h) A(a\[h), ie., the sum of A(alh ) over all ~ inE(h).Example l .
Let E :{0 ,1} ,D:  {e , "0"  },E(e)- {0, 1}, E("0")  -= {0}.
Suppose A(010 = ?, X(lle)= ?, and A(01"0" ) = 3 Then 6("0") = 1/1 _ 1 ~.
~ - yand i6(11"0",? )
: 5("0") (l\[e) - 1The fundamental difference between a contextmodel and an extension model lies in the inputsto the context selection rule, not its outputs.
Thetraditional context model includes a selection rules : E* --~ D whose only input is the history.
In con-trast, an extension model includes a selection rules : E* x E --+ D whose inputs include the pasthistory and the symbol to be predicted.
This dis-tinction is preserved even if we generalize the selec-tion rule to select a set of candidate contexts.
Un-der such a generalization, the context model wouldmap every history to a set of candidate contexts,ie., s : E* ---* 2 D , while an extension model wouldmap every history and symbol to a set of candidatecontexts, ie., s : E* x E --* 2 D.Our extension selection rule s : E* x E --+ D is de-fined implicitly by the set E of extensions currentlyin the model.
The recursion in (2) says that eachsymbol should be predicted in its longest candidatecontext, while the expansion factor 6(h) says thatlonger contexts in the model should be trusted morethan shorter contexts when combining the predic-tions from different contexts.An extension model ?
is valid iff it satisfies thefollowing constraints:a. eC DAE(c )  :Ec.
Vw ?
D \[E(w) : E =?, ~oez(~o) A(~iw) : 1\](4)These constraints suffice to ensure that the model ?defines a probability function.
Constraint (4a) statesthat every symbol has the empty string as a context.This guarantees that every symbol will always haveat least one context in every history and that the re-cursion in (2) will terminate.
Constraint (45) statesthat the sum of the probabilities of the extensionsE(w) available in in a given context w cannot sum221to more than unity.
The third constraint (4c) statesthat the sum of the probabilities of the extensionsE(w) must sum exactly to unity when every symbolis available in that context (ie., when E(w) : E).Lemma 2.1 VyEE*  Vcr62E\[ fi(~\]lY) : 1 :~/\](EIqy) = 1 \]P roof .
By the definition of 6(~ry).Theorem 1 If an exlension model ?
is valid, thenvn \]S,es,, = 1.Proof .
By induction on n. For the base case,n : 1 and the statement is true by the definition ofvalidity (constraints 4a and 4c).
The induction stepis true by lemma 2.1 and definition (1).
\[\]2.2 Parameter  Es t imat ionLet us now estimate the conditional probabilitiesA(.\[-) required for an extension model.
Traditionally,these conditional probabilities are estimated usingstring frequencies obtained from a training corpus.Let c(c~\[w) be the number of times that the symbolfollowed the string w in the training corpus, and letc(w) be the sum ~es  c(crlw) of all its conditionalfrequencies.Following Moffat (1990), we first partition theconditional event space E in a given context winto two subevents: the symbols q(w) that havepreviously occurred in context w and those thatq(w) that have not.
Formally, q(w) - {(r :c(,r\[w) > 0} and ~(w) - E - q(w).
We estimate)~c(q(w)lw ) as e(w)/(c(w) + #(w)) and )~c(4(w)\[w)as #(w) / (c (w)+ #(w)) where #(w) is the to-tal weight assigned to the novel events q(w) inthe context w. Currently, we calculate #(w)as min(\[q(w)l, Iq(w)\[) so that highly variable con-texts receive more flattening, but no novel symbolin ~(w) receives more than unity weight.
Next,)~c(alq(w ), w) is estimated as c(alw)/c(w ) for thepreviously seen symbols c~ e q(w) and Ac((r\]4(w), w)is estimated uniformly as 1/\[4(w)\[ for the novel sym-bols ~r ?
4(w).
Combining these estimates, we ob-tain our overall estimate (5).c( lw)c(w) + #(w) if c~ ?
q(w)Ae (alw) = #(w) otherwise+O)Unlike Moffat, our estimate (5) does not use escapeprobabilities or any other form of context blending.All novel events 4(w) in the context w are assigneduniform probability.
This is suboptimal but simpler.We note that our frequencies are incorrect whenused in an extension model that contains contextsthat are proper suffixes of each other.
In such a sit-uation, the shorter context is only used when thelonger context was not used.
Let y and xy be twodistinct contexts in a model ?.
Then the context ywill never be used when the history is E*xy.
There-fore, our estimate of A(.ly ) should be conditioned onthe fact that the longer context xy did not occur.The interaction between candidate contexts can be-come quite complex, and we consider this problemin other work (Ristad and Thomas, 1995).Parameter estimation is only a small part of theoverall model estimation problem.
Not only do wehave to estimate the parameters for a model, we haveto find the right parameters to use!
To do this, weproceed in two steps.
First, in section 2.3, we usethe minimum description length (MDL) principle toquantify the total merit of a model with respect oa training corpus.
Next, in section 2.4, we use ourMDL codelengths to derive a practical model selec-tion algorithm with which to find a good model inthe vast class of all extension models.2.3 Code length  FormulasThe goal of this section is to establish the proper ten-sion between model complexity and data complexity,in the fundamental units of information.
Althoughthe MDL framework obliges us to propose particu-lar encodings for the model and the data, our goalis not to actually encode the data or the model.Given an extension model ?
and a text corpus T,ITI = t, we define the total codelength L(T,?I(I))relative to the model class ~ using a 2-part code.L(T, ?\[(I)) : L(?I~ ) + L(TI?
, ~)Since conditioning on the model class (I) is alwaysunderstood, we will henceforth suppress it in ournotation.Firstly, we will encode the text T using the prob-ability model ?
and an arithmetic ode, obtainingthe following codelength.L(T\[?)
= - logif(Tl?
)Next, we encode the model ?
in three parts: the con-text dictionary as L(D), the extensions as L(EID),and the conditional frequencies c(.\[-) as L(e\[D, E).The dictionary D of contexts forms a suffix treecontaining ni vertices with branching factor i.
Them tree contains n = )--~i=l ni internal vertices andno leaf vertices.
There are (no + nl + .
.
.
+ nm -1)!/no!nl!...nm!
such trees (Knuth, 1986:587).
Ac-cordingly, this tree may be encoded with an enumer-ative code using L(D) bits.L ID) :  Lz (n)+ log(  n+m-lm_l )+log (no + nl + .
.
.+ nm - 1)!no!nl!...nm!rn -1i +Lz<(\[\[DJ\[,n)i= l+ log ( n + I LDJl 1 JLDJI-7 ) \222where \[DJ is the set of all contexts in D that areproper suffixes of another context in D. The firstterm encodes the number n of internal vertices usingthe Elias code.
The second term encodes the counts{nl, n2 , .
.
.
,  am}.
Given the frequencies of these in-ternal vertices, we may calculate the number no ofleaf vertices as no = 1 + n2 + 2n3 + 3n4 +.
.
.
+ (m -1)am.
The third term encodes the actual tree (with-out labels) using an enumerative code.
The fourthterm assigns labels (ie., symbols from E) to the edgesin the tree.
At this point the decoder knows all con-texts which are not proper suffixes of other contexts,ie., D - LD\].
The fourth term encodes the magni-tude of \[D\] as an integer bounded by the number nof internal vertices in the suffix tree.
The fifth termidentifies the contexts \[DJ as interior vertices in thetree that are proper suffices of another context in D.Now we encode the symbols available in each con-text.
Let mi be the number of contexts that haveexactly i extensions, ie., mi - J{w: JE(w)l = i}l.7"n Observe that ~i=1 mi = IDI.
( )  E m -F rni log ii--1The first term represents he encoding of {mi } whilethe second term represents the encoding IE(w)l foreach w in D. The third term represents he encodingof E(w) as a subset of E for each w in D.Finally, we encode the frequencies c(~rlw) used toestimate the model parameterswED+ g ,o, ( C(?)
+ )IE(w)lwhere \[y\] consists of all contexts that have y as theirmaximal proper suffix, ie., all contexts that y imme-diately dominates, and \[y\] is the maximal propersuffix of y in D, ie., the unique context hat imme-diately dominates y.
The first term encodes ITI withan Elias code and the second term recursively parti-tions c(w) into c(\[w\]) for every context w. The thirdterm partitions the context frequency c(w) into theavailable xtensions c(E(w)lw ) and the "unallocatedfrequency" c (E -  E(w)lw) = c(w) - c(E(w)\[w) in thecontext w.2.4 Mode l  Select ionThe final component of our contribution is a modelselection algorithm for the extension model class ~.Our algorithm repeatedly refines the accuracy of ourmodel in increasingly long contexts.
Adding a newparameter to the model will decrease the codelengthof the data and increase the codelength of the model.Accordingly, we add a new parameter to the modelonly if doing so will decrease the total codelength ofthe data and the model.The incremental cost and benefit of adding a sin-gle parameter to a given context cannot be accu-rately approximated in isolation from any other pa-rameters that might be added to that context.
Ac-cordingly, the incremental cost of adding the set E'of extensions to the context w is defined as (6) whilethe incremental benefit is defined as (7).ALe(w, E') - L(?
U ({w} ?
E')) - L(?)
(6)ALT(W, E') - L(TI? )
- L(T\[?
U ({w} x E')) (7)Keeping only significant erms that are monoton-ically nondecreasing, we approximate the incremen-tal cost ALe(w, E') asloglDl+log IS'l+ log c(Lwj) + log ( c(w)ls, i + C 'I )The first term represents the incremental increasein the size of the context dictionary D. The secondterm represents the cost of encoding the candidateextensions E(w) = E ~.
The third term represents(an upper bound on) the cost of encoding c(w).
Thefourth term represents the cost of encoding c(.Iw )for E(w).
Only the second and fourth terms aresignficant.Let us now consider the incremental benefit ofadding the extensions E' to a given context w. Theaddition of a single parameter (w, ~r) to the model?
will immediately change A(alw), by definition ofthe model class.
Any change to A(.Iw ) will alsochange the expansion factor 5(w) in that context,which may in turn change the conditional probabili-ties ~(E-E(w)lw, ?)
of symbols not available in thatcontext.
Thus the incremental benefit of adding theextensions E' to the context w may be calculated asALT(w,E') -- c(E - E' lw)log 1 - A(E'Iw)1 - ~(S ' l~ ,  ?
)+ ~ c('/Iwll?g~(~,lw,?
)a' E FdThe first term represents the incremental benefit (inbits) for evaluating E - E' in the context w usingthe more accurate xpansion factor 5(w).
The sec-ond term represents the incremental benefit (in bits)of using the direct estimate A(a'lw ) instead of themodel probability/5(cr'lw, ?)
in the context w. Notethat A(a'lw) may be more or less than/~(cr'lw , ?
).Now the incremental cost and benefit of addinga single extension (w, cr) to a model that alreadycontains the extensions (w, El/ may be defined asfollows.ALe(w, E', a) -- ALe(w, E' U {a}) - ALe(w, E')223ALT(w, ~', a) - ALT(w, ~' U {a}) - ALT(W, ~')Let us now use these incremental cost/benefit for-mulas to design a simple heuristic estimation algo-rithm for the extension model.
The algorithm con-sists of two subroutines.
Refine(D,E,n) augmentsthe model with all individually profitable xtensionsof contexts of length n. It rests on the assump-tion that adding a new context does not changethe model's performance in the shorter contexts.Extend(w) determines all profitable xtensions of thecandidate context w, if any exist.
Since it is notfeasible to evaluate the incremental profit of everysubset of E, Extend(w) uses a greedy heuristic thatrepeatedly augments the set of profitable xtensionsof w by the single most profitable xtension until itis not longer profitable to do so.Refine( D,E,n)1.
D,  := {};E,  := {};2.
Cn := {w:  w e Cn-1 ~'\]~ A c(w) > Cmi.}
;3. if (( n > nm~=) V (ICnl = 0)) then return;4. for w E Cn5.
~' := Extend(w);6. if ISI > o then D.  :-- Dn U {w}; En(w) := S;7.
D :=DUDn;E :=EUEn;8.
Refine( D,E,n -F 1);Cn is the set of candidate contexts of length n,obtained from the training corpus.
Dn is the set ofprofitable contexts of length n, while En is the setof profitable xtensions of those contexts.Extend(w)1.
S : :  {};2. o" := argmaxoe~.
{AL(w, {at})}3. while (AL(w,S ,~)  > O)4.
S := S U {a};S. o" := argrnax.e\]g_ s {AL(w, ,S', ?r)}6. return(S);The loop in lines 3-5 repeatedly finds the singlemost profitable symbol a with which to augmentthe set S of profitable xtensions.
The incrementalprofit AL( .
.
. )
is the incremental benefit ALT(..
.
)minus the incremental cost ALe( .
.
. )
.Our breadth-first search considers shorter con-texts before longer ones, and consequently the deci-sion to add a profitable context y may significantlydecrease the benefit of a more profitable context xy,particularly when c(xy) ~ c(y).
For example, con-sider a source with two hidden states.
In the firststate, the source generates the alphabet E = {0, 1,2}uniformly.
In the second state, the source generatesthe string "012" with certainty.
With appropriatestate transition probabilities, the source generatesstrings where c(0) ~ c(1) ~ e(2), c(211)/c(1 ) >>c(21e)/c(c), and c(2101)/c(01 ) > c(211)/c(1 ).
In sucha situation, the best context model includes the con-texts "0" and "01" along with the empty contextc.
However, the divergence heuristic will first deter-mine that the context "1" is profitable relative to theempty context, and add it to the model.
Now theprofitability of the better context ?
'01" is reduced,and the divergence heuristic may therefore not in-clude it in the model.
This problem is best solvedwith a best first search.
Our current implementationuses a breadth first search to limit the computationalcomplexity of model selection.Finally, we note that our parameter estimationtechniques and model selection criteria are compara-ble in computational complexity to Rissanen's con-text models (1983, 1986).
For that reason, extensionmodels should be amendable to efficient online im-plementation.3 Empi r i ca l  Resu l tsBy means of the following experiments, we hopeto demonstrate the utility of our context modelingtechniques.
All results are based on the Brown cor-pus, an eclectic collection of English prose drawnfrom 500 sources across 15 genres (Francis andKucera, 1982).
The irregular and nonstationary na-ture of this corpus poses an exacting test for sta-tistical language models.
We use the first 90% ofeach file in the corpus to estimate our models, andthen use the remaining 10% of each file in the corpusto evaluate the models.
Each file contains approx-imately 2000 words.
Due to limited computationalresources, we set nmax = 10, Cmin -~- 8, and restrictour our alphabet size to 70 (ie., all printing asciicharacters, ignoring case distinction).Our results are summarized in the following ta-ble.
Message entropy (in bits/symbol) is for thetesting corpus only, as per traditional model vali-dation methodology.
The nonmonotonic extensionmodel (NEM) outperforms all other models for allorders using vastly fewer parameters.
Its perfor-mance all the more impressive when we consider thatno context blending or escaping is performed, evenfor novel events.We note that the test message ntropy of the n-gram model class is minimized by the 5-gram at 2.38bits/char.
This result for the 5-gram is not honestbecause knowledge of the test set was used to selectthe optimal model order.
Jelinek and Mercer (1980)have shown to interpolate n-grams of different or-der using mixing parameters that are conditionedon the history.
Such interpolated Markov sourcesare considerably more powerful than traditional n-grams but contain even more parameters.The best reported results on the Brown Corpusare 1.75 bits/char using a large interpolated trigramword model whose parameters are estimated usingover 600,000,000 words of proprietary training data(Brown et.al., 1992).
The use of proprietary trainingdata means that these results are not independentlyrepeatable.
In contrast, our results were obtainedusing only 900,000 words of generally available train-ing data and may be independently verified by any-224Mode lNEMNCMMCM1MCM2n-gramParameters Entropy89,325 1.97687,276 2.1988,945,904 2.4388,945,904 3.12506,352,021,176,052 3.74Table 1: Results for the nonmonotonic extensionmodel (NEM), the nonmonotonic ontext model(NCM), Rissanen's (1983,1986) monotonic ontextmodels (MCM1, MCM2) and the n-gram model.
Allmodels are order 7.
The rightmost column containstest message ntropy in bits/symbol.
NEM outper-forms all other model classes for all orders using sig-nificantly fewer parameters.
It is possible to reducethe test message ntropy of the NEM and NCM to1.91 and 1.99, respectively, by quadrupling the num-ber of model parameters.one with the inclination to do so.
The amount oftraining data is known to be a significant factor inmodel performance.
Given a sufficiently rich dictio-nary of words and a sufficiently large training corpus,a model of word sequences i likely to outperform anotherwise quivalent model of character sequences.For these three reasons - repeatability, training cor-pus size, and the advantage of word models overcharacter models - the results reported by Brownet.al (1992) are not directly comparable to those re-ported here.Section 3.1 compares the statistical efficiency ofthe various context model classes.
Next, sec-tion 3.2 anecodatally examines the complex interac-tions among the parameters of an extension model.3.1 Mode l  Class Compar i sonGiven the tremendous risk of overfitting, the mostimportant property of a model class is arguably itsstatistical efficiency.
Informally, statistical efficiencymeasures the effectiveness of individual parametersin a given model class.
A high efficiency indicatesthat our model class provides a good description ofthe data.
Conversely, a low efficiency indicates thatthe model class does not adequately describe the ob-served data.In this section, we compare the statistical effi-ciency of three model classes: context models, ex-tension models, and fixed-length Markov processes(ie., n-grams).
Our model class comparison is basedon three criteria of statistical efficiency: total code-length, bits/parameter on the test message, andbits/order on the test message.
The context andextension models are all of order 9, and were es-timated using the true incremental benefit and arange of fixed incremental costs (between 5 and 25bits/extension for the extension model and between25 and 150 bits/context for the context model).According to the first criteria of statistical effi-ciency, the best model is the one that achieves thesmallest otal codelength L(T, ?)
of the training cor-pus T and model ?
using the fewest parameters.This criteria measures the statistical efficiency of amodel class according to the MDL framework, wherewe would like each parameter to be as cheap as pos-sible and do as much work as possible.
Figure 1graphs the number of model parameters required toachieve a given total codelength for the training cor-pus and model.
The extension model class is theoverwhelming winner.. .
.
.
.
.
.
.
.
N. um ,be.
r, of Param,et?rs.. vs: Codele, ng~ .
.
.
.
.
.
.
.
3,5"l t ..... M-... 2,3,4 ngrarn?
exle~lslon model .-~ E- - ~--  context mod~ ....'"? """
\[..'""" i? '""
I'"'""'"'"" i15 .m" ~- Btm ~10000 100000 1000000 I(XX)OO(X)Number of parametersFigure 1: The relationship between the number ofmodel parameters and the total codelength L(T, ?
)of the training corpus T and the model ?.
By thiscriteria of statistical efficiency, the extension modelscompletely dominate context models and n-grams.According to the second criteria of statistical effi-ciency, the best model is the one that achieves thelowest test message ntropy using the fewest param-eters.
This criteria measures the statistical efficiencyof a model class according to traditional model vali-dation methodology, tempered by a healthy concernfor overfitting.
Figure 2 graphs the number of modelparameters required to achieve a given test messageentropy for each of the three model classes.
Again,the extension model class is the clear winner.
(Thisis particularly striking when the number of parame-ters is plotted on a linear scale.)
For example, one ofour extension models saves 0.98 bits/char over thetrigram while using less than 1/3 as many param-eters.
Given the logarithmic nature of codelengthand the scarcity of training data, this is a significantimprovement.According to the third criteria of statistical effi-ciency, the best model is one that achieves the low-est test message ntropy for a given model order.This criteria is widely used in the language model-ing community, in part because model order is typi-225"C"&i ' s3 .0 .
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
= .i e~s lon  mo~l2.8 22 27.
=~?~"~ ..... 3,4  gram "'"'",........" ' .
.
.2.6' .
.
.
.
"'x2.42.22.0L810 4 10  s 10  6 10  7Number  o f  Parameters  vs .
Message Entropy.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
= .
.
.
.
.
.
.
.N ~ r ~  Pa~emto ~Figure 2: The relationship between the number ofmodel parameters and test message ntropy.
Themost striking fact about this graph is the tremen-dous efficiency of the extension model.cally -- although not necessarily - -  related to boththe number of model parameters and the amount ofcomputation required to estimate the model.
Fig-ure 3 compares model order to test message ntropyfor each of the three model classes.
As the orderof the models increases from 0 (ie., unigram) to 10,we naturally expect he test message ntropy to ap-proach a lower bound, which is itself bounded belowby the true source entropy.
By this criteria, the ex-tension model class is better than the context modelclass, and both are significantly better than the n-gram.4.44 .24 .03 .83 .63 .43 .23 .02 .82 .62 .42 .22 .01 .8Mode l  Order  vs .
Message  Entropy..... ?
.. ...... .
- - : - -  :~Z~m=~2 4 6 8 10Mode l  OrderFigure 3: The relationship between model order andtest message ntropy.
The extension model class isthe clear winner by this criteria as well.3.2 AnecdotesIt is also worthwhile to interpret he parameters ofthe extension model estimated from the Brown Cor-pus, to better understand the interaction betweenour model class and our heuristic model selection al-gorithm.
According to the divergence heuristic, thedecision to add an extension (w, ~) is made relativeto that context's maximal proper suffix LwJ in D aswell as any other extensions in the context w. Anextension (w, ~) will be added only if the direct es-timate of its conditional probability is significantlydifferent from its conditional probability in its maxi-mal proper suffix after scaling by the expansion fac-tor in the context w, ie., if A(alw ) is significantlydifferent han 6(w)~(c~ I LwJ).This is illusrated by the three contexts and sixextensions shown immediately below, where +E(w)includes all symbols in E(w) that are more likelyin w than they were in \[wJ and -E(w)  includes allsymbols in E(w) that are less likely in w than theywere  in L J.W"blish""ouestablish""euestablish"+E(w) -E(w)e,i,mUm eThe substring blish is most often followed by thecharacters 'e', 5', and 'm', corresponding to the rel-atively frequent word forms publish{ ed, er, ing} andestablish{ ed, ing, ment}.
Accordingly, the context"b l i sh"  has three positive extensions {e, i ,m}, ofwhich e has by far the greatest probability.
Thecontext "b l i sh"  is the maximal proper suffix of twoother contexts in the model, "ouestab l i sh"  and"euestablish".The substring o establish occurs most frequentlyin the gerund to establish, which is nearly alwaysfollowed by a space.
Accordingly, the context"ouestab l i sh"  has a single positive extension "u".The substring o establish is also found before thecharacters 'm', 'e', and 'i' in sequences uch asto establishments, {who, ratio, also} established, and{ to, into, also} establishing.
Accordingly, the context"ouestab l i sh"  does not have any negative exten-sions.In contrast, the substring e establish is overwhelm-ingly followed by the character 'm', rarely followedby 'e', and never followed by either 'i' or space.
Forthis reason, the context "euestab l i sh"  has a sin-gle positive extension {m} corresponding to the greatfrequency of the string the establishment.
This con-text also has single negative extension {e}, corre-sponding to the fact that the character 'e' is still pos-sible in the context "euestab l i sh"  but considerablyless likely than in that context's maximal proper suf-fix "bl ish".Since 'i' is reasonably likely in the context"b l i sh"  but completely unlikely in the context"euestab l i sh" ,  we  may well wonder  why  the mode l226does not include the negative xtension 'i' in addi-tion to 'e' or even instead of 'e'.
This puzzle is ex-plained by the expansion factor as follows.
Sincethe substring e establish is only followed by 'm' and'e', the expansion factor ~("e,,establish") is essen-tially zero after 'm' and 'e' are added to that con-text, and therefore ~(~-  {m, e}l "euestabl ish")is also essentially zero.
Thus, 'i' and space areboth assigned nearly zero probability in the con-text "e, ,establ ish",  simply because 'm' and 'e' getnearly all the probability in that context.4 Conc lus ionIn ongoing work, we are investigating extension mix-ture models as well as improved model selection al-gorithms.
An extension mixture model is an exten-sion model whose ~(~lw) parameters are estimatedby linearly interpolating the empirical probabilityestimates for all extensions that dominate w withrespect o c~, ie., all extensions whose symbol isand whose context is a suffix of w. Extension mix-ing allows us to remove the uniform flattening ofzero frequency symbols in our parameter estimates(5).
Preliminary results are promising.
The idea ofcontext mixing is due to Jelinek and Mercer (1980).Our results highlight he fundamental tension be-tween model complexity and data complexity.
If themodel complexity does not match the data complex-ity, then both the total codelength ofthe past obser-vations and the predictive rror increase.
In otherwords, simply increasing the number of parametersin the model does not necessarily increase predictivepower of the model.
Therefore, it is necessary tohave a a fine-grained model along with a heuristicmodel selection algorithm to guide the expansion ofthe model in a principled manner.Acknowledgements.
Thanks to Andrew Appel,Carl de Marken, and Dafna Scheinvald for their cri-tique.
The paper has benefited from discussions withthe participants of DCC95.
Both authors are par-tially supported by Young Investigator Award IRI-0258517 to the first author from the National ScienceFoundation.
The second author is additionally sup-ported by a tuition award from the Princeton Uni-versity Research Board.
The research was partiallysupported by NSF SGER IRI-9217208.FRANCIS, W. N., AND KUCERA, H. Frequencyanalysis of English usage: lexicon and grammar.Houghton Mifflin, Boston, 1982.JELINEK, F., AND MERCER, a.  L. Interpolated es-timation of Markov source parameters from sparsedata.
In Pattern Recognition i  Practice (Amster-dam, May 21-23 1980), E. S. Gelsema nd L. N.Kanal, Eds., North Holland, pp.
381-397.KNUTH, D. E. The Art of Computer Programming,1 ed., vol.
1.
Addison-Wesley, Reading, MA, 1968.MOFFAT, A.
Implementing the PPM data compre-sion scheme.
IEEE Trans.
Communications 38,11 (1990), 1917-1921.RISSANEN, J.
Modeling by shortest data descrip-tion.
Automatica 14 (1978), 465-471.RISSANEN, J.
A universal data compression system.IEEE Trans.
Information Theory IT-29, 5 (1983),656-664.RISSANEN, J.
Complexity of strings in the class ofMarkov sources.
IEEE Trans.
Information The-ory IT-32, 4 (1986), 526-532.RISTAD, E. S., AND THOMAS, R. G. Contextmodels in the MDL framework.
In Proceedings of5th Data Compression Conference (Los Alamitos,CA, March 28-30 1995), J. Storer and M.
Cohn,Eds., IEEE Computer Society Press, pp.
62-71.ReferencesBROWN, P., PIETRA, V. D., PIETRA, S. D., LAI,J., AND MERCER, R. An estimate of an upperbound for the entropy of English.
ComputationalLinguistics 18 (1992), 31-40.CLEARY, J., AND WITTEN, I.
Data compressionusing adaptive coding and partial string matching.IEEE Trans.
Comm.
COM-32, 4 (1984), 396-402.227
