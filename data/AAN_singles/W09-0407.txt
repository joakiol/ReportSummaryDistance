Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 51?55,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsThe RWTH System Combination System for WMT 2009Gregor Leusch, Evgeny Matusov, and Hermann NeyRWTH Aachen UniversityAachen, GermanyAbstractRWTH participated in the System Combi-nation task of the Fourth Workshop on Sta-tistical Machine Translation (WMT 2009).Hypotheses from 9 German?English MTsystems were combined into a consen-sus translation.
This consensus transla-tion scored 2.1% better in BLEU and 2.3%better in TER (abs.)
than the best sin-gle system.
In addition, cross-lingualoutput from 10 French, German, andSpanish?English systems was combinedinto a consensus translation, which gavean improvement of 2.0% in BLEU/3.5% inTER (abs.)
over the best single system.1 IntroductionThe RWTH approach to MT system combinationis a refined version of the ROVER approach inASR (Fiscus, 1997), with additional steps to copewith reordering between different hypotheses, andto use true casing information from the input hy-potheses.
The basic concept of the approach hasbeen described by Matusov et al (2006).
Severalimprovements have been added later (Matusov etal., 2008).
This approach includes an enhancedalignment and reordering framework.
In con-trast to existing approaches (Jayaraman and Lavie,2005; Rosti et al, 2007), the context of the wholecorpus rather than a single sentence is consideredin this iterative, unsupervised procedure, yieldinga more reliable alignment.
Majority voting on thegenerated lattice is performed using the prior prob-abilities for each system as well as other statisticalmodels such as a special n-gram language model.2 System Combination AlgorithmIn this section we present the details of our systemcombination method.
Figure 1 gives an overviewof the system combination architecture describedin this section.
After preprocessing the MT hy-potheses, pairwise alignments between the hy-potheses are calculated.
The hypotheses are thenreordered to match the word order of a selectedprimary hypothesis.
From this, we create a confu-sion network (CN), which we then rescore usingFigure 1: The system combination architecture.system prior weights and a language model (LM).The single best path in this CN then constitutes theconsensus translation.2.1 Word AlignmentThe proposed alignment approach is a statisticalone.
It takes advantage of multiple translations fora whole corpus to compute a consensus translationfor each sentence in this corpus.
It also takes ad-vantage of the fact that the sentences to be alignedare in the same language.For each source sentence F in the test corpus,we select one of its translations En, n=1, .
.
.
,M,as the primary hypothesis.
Then we align the sec-ondary hypotheses Em(m = 1, .
.
.
,M ;n 6= m)with En to match the word order in En.
Since it isnot clear which hypothesis should be primary, i. e.has the ?best?
word order, we let every hypothesisplay the role of the primary translation, and alignall pairs of hypotheses (En, Em); n 6= m.The word alignment is trained in analogy tothe alignment training procedure in statistical MT.The difference is that the two sentences that haveto be aligned are in the same language.
We use theIBM Model 1 (Brown et al, 1993) and the Hid-den Markov Model (HMM, (Vogel et al, 1996))to estimate the alignment model.The alignment training corpus is created from atest corpus1 of effectively M ?
(M ?
1) ?
N sen-tences translated by the involved MT engines.
Thesingle-word based lexicon probabilities p(e|e?)
areinitialized from normalized lexicon counts col-lected over the sentence pairs (Em, En) on thiscorpus.
Since all of the hypotheses are in the samelanguage, we count co-occurring identical words,i.
e. whether em,j is the same word as en,i for somei and j.
In addition, we add a fraction of a countfor words with identical prefixes.1A test corpus can be used directly because the align-ment training is unsupervised and only automatically pro-duced translations are considered.51The model parameters are trained iteratively us-ing the GIZA++ toolkit (Och and Ney, 2003).
Thetraining is performed in the directions Em ?
Enand En ?
Em.
After each iteration, the updatedlexicon tables from the two directions are interpo-lated.
The final alignments are determined usinga cost matrix C for each sentence pair (Em, En).Elements of this matrix are the local costs C(j, i)of aligning a word em,j from Em to a word en,ifrom En.
Following Matusov et al (2004), wecompute these local costs by interpolating thenegated logarithms of the state occupation proba-bilities from the ?source-to-target?
and ?target-to-source?
training of the HMM model.
Two differ-ent alignments are computed using the cost matrixC: the alignment a?
used for reordering each sec-ondary translation Em, and the alignment a?
usedto build the confusion network.In addition to the GIZA++ alignments, we havealso conducted preliminary experiments follow-ing He et al (2008) to exploit character-basedsimilarity, as well as estimating p(e|e?)
:=?f p(e|f)p(f |e?)
directly from a bilingual lexi-con.
But we were not able to find improvementsover the GIZA++ alignments so far.2.2 Word Reordering and ConfusionNetwork GenerationAfter reordering each secondary hypothesis Emand the rows of the corresponding alignment costmatrix according to a?, we determine M?1 mono-tone one-to-one alignments between En as the pri-mary translation and Em,m = 1, .
.
.
,M ;m 6= n.We then construct the confusion network.
In caseof many-to-one connections in a?
of words in Emto a single word from En, we only keep the con-nection with the lowest alignment costs.The use of the one-to-one alignment a?
impliesthat some words in the secondary translation willnot have a correspondence in the primary transla-tion and vice versa.
We consider these words tohave a null alignment with the empty word ?.
Inthe corresponding confusion network, the emptyword will be transformed to an ?-arc.M ?
1 monotone one-to-one alignments canthen be transformed into a confusion network.
Wefollow the approach of Bangalore et al (2001)with some extensions.
Multiple insertions with re-gard to the primary hypothesis are sub-aligned toeach other, as described by Matusov et al (2008).Figure 2 gives an example for the alignment.2.3 Voting in the confusion networkInstead of choosing a fixed sentence to define theword order for the consensus translation, we gen-erate confusion networks for all hypotheses as pri-mary, and unite them into a single lattice.
In ourexperience, this approach is advantageous in termsof translation quality, e.g.
by 0.7% in BLEU com-pared to a minimum Bayes risk primary (Rosti etal., 2007).
Weighted majority voting on a singleconfusion network is straightforward and analo-gous to ROVER (Fiscus, 1997).
We sum up theprobabilities of the arcs which are labeled with thesame word and have the same start state and thesame end state.
To exploit the true casing abilitiesof the input MT systems, we sum up the scores ofarcs bearing the same word but in different cases.Here, we leave the decision about upper or lowercase to the language model.2.4 Language ModelsThe lattice representing a union of several confu-sion networks can then be directly rescored withan n-gram language model (LM).
A transforma-tion of the lattice is required, since LM history hasto be memorized.We train a trigram LM on the outputs of the sys-tems involved in system combination.
For LMtraining, we took the system hypotheses for thesame test corpus for which the consensus trans-lations are to be produced.
Using this ?adapted?LM for lattice rescoring thus gives bonus to n-grams from the original system hypotheses, inmost cases from the original phrases.
Presum-ably, many of these phrases have a correct word or-der, since they are extracted from the training data.Previous experimental results show that using thisLM in rescoring together with a word penalty (tocounteract any bias towards short sentences) no-tably improves translation quality.
This even re-sults in better translations than using a ?classical?LM trained on a monolingual training corpus.
Weattribute this to the fact that most of the systemswe combine are phrase-based systems, which al-ready include such general LMs.
Since we are us-ing a true-cased LM trained on the hypotheses, wecan exploit true casing information from the in-put systems by using this LM to disambiguate be-tween the separate arcs generated for the variants(see Section 2.3).After LM rescoring, we add the probabilities ofidentical partial paths to improve the estimationof the score for the best hypothesis.
This is donethrough determinization of the lattice.2.5 Extracting Consensus TranslationsTo generate our consensus translation, we extractthe single-best path within the rescored confusionnetwork.
With our approach, we could also extractN -best hypotheses.
In a subsequent step, these N -best lists could be rescored with additional statis-tical models (Matusov et al, 2008).
But as we didnot have the resources in the WMT 2009 evalua-tion, this step was dropped for our submission.3 Tuning system weightsSystem weights, LM factor, and word penaltyneed to be tuned to produce good consensus trans-lations.
We optimize these parameters using the520.25 would your like coffee or teasystem 0.35 have you tea or Coffeehypotheses 0.10 would like your coffee or0.30 I have some coffee tea would you likealignment have|would you|your $|like Coffee|coffee or|or tea|teaand would|would your|your like|like coffee|coffee or|or $|teareordering I|$ would|would you|your like|like have|$ some|$ coffee|coffee $|or tea|tea$ would your like $ $ coffee or teaconfusion $ have you $ $ $ Coffee or teanetwork $ would your like $ $ coffee or $I would you like have some coffee $ tea$ would you $ $ $ coffee or teavoting 0.7 0.65 0.65 0.35 0.7 0.7 0.5 0.7 0.9(normalized) I have your like have some Coffee $ $0.3 0.35 0.35 0.65 0.3 0.3 0.5 0.3 0.1consensus translation would you like coffee or teaFigure 2: Example of creating a confusion network from monotone one-to-one word alignments (denotedwith symbol |).
The words of the primary hypothesis are printed in bold.
The symbol $ denotes a nullalignment or an ?-arc in the corresponding part of the confusion network.Table 1: Systems combined for the WMT 2009task.
Systems written in oblique were also used inthe Cross Lingual task (rbmt3 for FR?EN).DE?EN google, liu, rbmt3, rwth, stutt-gart, systran, uedin, uka, umdES?EN google, nict, rbmt4, rwth,talp-upc, uedinFR?EN dcu, google, jhu, limsi, lium-systran, rbmt4, rwth, uedin, ukapublicly available CONDOR optimization toolkit(Berghen and Bersini, 2005).
For the WMT2009 Workshop, we selected a linear combina-tion of BLEU (Papineni et al, 2002) and TER(Snover et al, 2006) as optimization criterion,??
:= argmax?
{(2 ?
BLEU)?
TER}, based onprevious experience (Mauser et al, 2008).
Weused the whole dev set as a tuning set.
For morestable results, we used the case-insensitive variantsfor both measures, despite the explicit use of caseinformation in our approach.4 Experimental resultsDue to the large number of submissions (71 intotal for the language pairs DE?EN, ES?EN,FR?EN), we had to select a reasonable numberof systems to be able to tune the parameters ina reliable way.
Based on previous experience,we manually selected the systems with the bestBLEU/TER score, and tried different variations ofthis selection, e.g.
by removing systems whichhad low weights after optimization, or by addingpromising systems, like rule based systems.Table 1 lists the systems which made it intoour final submission.
In our experience, if a largenumber of systems is available, using n-best trans-lations does not give better results than using sin-gle best translations, but raises optimization timesignificantly.
Consequently, we only used singlebest translations from all systems.The results also confirm another observation:Even though rule-based systems by itself mayhave significantly lower automatic evaluationscores (e.g.
by 2% or more in BLEU on DE?EN),they are often very important in system combina-tion, and can improve the consensus translatione.g.
by 0.5% in BLEU.Having submitted our translations to the WMTworkshop, we calculated scores on the WMT 2009test set, to verify the results on the tuning data.Both the results on the tuning set and on the testset can be found in the following tables.4.1 The Google ProblemOne particular thing we noticed is that in the lan-guage pairs of FR?EN and ES?EN, the trans-lations from one provided single system (Google)were much better in terms of BLEU and TER thanthose of all other systems ?
in the former caseby more than 4% in BLEU.
In our experience,our system combination approach requires at leastthree ?comparably good?
systems to be able toachieve significant improvements.
This was con-firmed in the WMT 2009 task as well: Neither inFR?EN nor in ES?EN we were able to achievean improvement over the Google system.
For thisreason, we did not submit consensus translationsfor these two language pairs.
On the other hand,we would have achieved significant improvementsover all (remaining) systems leaving out Google.4.2 German?English (DE?EN)Table 2 lists the scores on the tuning and test setfor the DE?EN task.
We can see that the bestsystems are rather close to each other in termsof BLEU.
Also, the rule-based translation system(RBMT), here SYSTRAN, scores rather well.
Asa consequence, we find a large improvement usingsystem combination: 2.9%/2.7% abs.
on the tun-ing set, and still 2.1%/2.3% on test, which meansthat system combination generalizes well here.4.3 Spanish?English (ES?EN),French?English (FR?EN)In Table 3, we see that on the ES?EN andFR?EN tasks, a single system ?
Google ?
scoressignificantly better on the TUNE set than any other53Table 2: German?English task: case-insensitivescores.
Best single system was Google, secondbest UKA, best RBMT Systran.
SC stands for sys-tem combination output.TUNE TESTGerman?English BLEU TER BLEU TERBest single 23.2 59.5 21.3 61.3Second best single 23.0 58.8 21.0 61.7Best RBMT 21.3 61.3 18.9 63.7SC (9 systems) 26.1 56.8 23.4 59.0w/o RBMT 24.5 57.3 22.5 59.2w/o Google 24.9 57.4 23.0 59.1Table 3: Spanish?English and French?Englishtask: scores on the tuning set after system combi-nation weight tuning (case-insensitive).
Best sin-gle system was Google, second best was Uedin(Spanish) and UKA (French).
No results on TESTwere generated.ES?EN FR?ENSpanish?English BLEU TER BLEU TERBest single 29.5 53.6 32.2 50.1Second best single 26.9 56.1 28.0 54.6SC (6/9 systems) 28.7 53.6 30.7 52.5w/o Google 27.5 55.6 30.0 52.8system, namely by 2.6%/4.2% resp.
in BLEU.
Asa result, a combination of these systems scoresbetter than any other system, even when leavingout the Google system.
But it gives worse scoresthan the single best system.
This is explainable,because system combination is trying to find aconsensus translation.
For example, in one case,the majority of the systems leave the French term?wagon-lit?
untranslated; spurious translations in-clude ?baggage car?, ?sleeping car?, and ?alive?.As a result, the consensus translation also contains?wagon-lit?, not the correct translation ?sleeper?which only the Google system provides.
Even tun-ing all other system weights to zero would not re-sult in pure Google translations, as these weightsneither affect the LM nor the selection of the pri-mary hypothesis in our approach.4.4 Cross-Lingual?English (XX?EN)Finally, we have conducted experiments on cross-lingual system combination, namely combiningthe output from DE?EN, ES?EN, and FR?ENsystems to a single English consensus transla-tion.
Some interesting results can be found inTable 4.
We see that this consensus translationscores 2.0%/3.5% better than the best single sys-tem, and 4.4%/5.6% better than the second bestsingle system.
While this is only 0.8%/2.5% bet-ter than the combination of only the three Googlesystems, the combination of the non-Google sys-Table 4: Cross-lingual task: combinationof German?English, Spanish?English, andFrench?English.
Case-insensitive scores.
Bestsingle system was Google for all language pairs.Cross-lingual TUNE TEST?
English BLEU TER BLEU TERBest single German 23.2 59.5 21.3 61.3Best single Spanish 29.5 53.6 28.7 53.8Best single French 32.2 50.1 31.1 51.7SC (10 systems) 35.5 46.4 33.1 48.2w/o RBMT 35.1 46.5 32.7 48.3w/o Google 32.3 48.8 29.9 50.53 Google systems 34.2 48.0 32.3 49.2w/o German 34.0 49.3 31.5 50.9w/o Spanish 33.4 49.8 31.0 51.9w/o French 30.5 51.4 28.6 52.3tems leads to translations that could compete withthe FR?EN Google system.
Again, we see thatRBMT systems lead to a small improvement of0.4% in BLEU, although their scores are signif-icantly worse than those of the competing SMTsystems.Regarding languages, we see that despite thelarge differences in the quality of the systems (10points between DE?EN and FR?EN), all lan-guages seem to provide significant information tothe consensus translation: While FR?EN cer-tainly has the largest influence (?4.5% in BLEUwhen left out), even DE?EN ?contributes?
1.6BLEU points to the final submission.5 ConclusionsWe have shown that our system combination sys-tem can lead to significant improvements oversingle best MT output where a significant num-ber of comparably good translations is availableon a single language pair.
For cross-lingual sys-tem combination, we observe even larger improve-ments, even if the quality in terms of BLEU orTER between the systems of different languagepairs varies significantly.
While the input of high-quality SMT systems has the largest weight for theconsensus translation quality, we find that RBMTsystems can give important additional informationleading to better translations.AcknowledgmentsThis work was partly realized as part of theQuaero Programme, funded by OSEO, FrenchState agency for innovation.
This work waspartly supported by the Defense Advanced Re-search Projects Agency (DARPA) under ContractNo.
HR0011-06-C-0023.54ReferencesS.
Bangalore, G. Bordel, and G. Riccardi.
2001.Computing consensus translation from multiple ma-chine translation systems.
In IEEE AutomaticSpeech Recognition and Understanding Workshop,Madonna di Campiglio, Italy, December.F.
V. Berghen and H. Bersini.
2005.
CONDOR,a new parallel, constrained extension of Powell?sUOBYQA algorithm: Experimental results andcomparison with the DFO algorithm.
Journal ofComputational and Applied Mathematics, 181:157?175.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Compu-tational Linguistics, 19(2):263?311, June.J.
Fiscus.
1997.
A post-processing system to yield re-duced word error rates: Recognizer output voting er-ror reduction (ROVER).
In IEEE Workshop on Au-tomatic Speech Recognition and Understanding.X.
He, M. Yang, J. Gao, P. Nguyen, and R. Moore.2008.
Indirect-HMM-based hypothesis alignmentfor combining outputs from machine translation sys-tems.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 98?107, Honolulu, Hawaii, October.S.
Jayaraman and A. Lavie.
2005.
Multi-engine ma-chine translation guided by explicit word matching.In Proc.
of the 10th Annual Conf.
of the EuropeanAssociation for Machine Translation (EAMT), pages143?152, Budapest, Hungary, May.E.
Matusov, R. Zens, and H. Ney.
2004.
Symmetricword alignments for statistical machine translation.In COLING ?04: The 20th Int.
Conf.
on Computa-tional Linguistics, pages 219?225, Geneva, Switzer-land, August.E.
Matusov, N. Ueffing, and H. Ney.
2006.
Computingconsensus translation from multiple machine trans-lation systems using enhanced hypotheses align-ment.
In Conference of the European Chapter of theAssociation for Computational Linguistics (EACL),pages 33?40, Trento, Italy, April.E.
Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,D.
Dechelotte, M. Federico, M. Kolss, Y. S. Lee,J.
B. Marino, M. Paulik, S. Roukos, H. Schwenk,and H. Ney.
2008.
System combination for machinetranslation of spoken and written language.
IEEETransactions on Audio, Speech and Language Pro-cessing, 16(7):1222?1237, September.A.
Mauser, S. Hasan, and H. Ney.
2008.
Automaticevaluation measures for statistical machine transla-tion system optimization.
In International Confer-ence on Language Resources and Evaluation, Mar-rakech, Morocco, May.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51, March.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
In Proc.
of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 311?318, Philadelphia, PA, July.A.
V. Rosti, S. Matsoukas, and R. Schwartz.
2007.Improved word-level system combination for ma-chine translation.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics (ACL), pages 312?319, Prague, Czech Re-public, June.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A Study of Translation ErrorRate with Targeted Human Annotation.
In Proc.
ofthe 7th Conf.
of the Association for Machine Trans-lation in the Americas (AMTA), pages 223?231,Boston, MA, August.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based word alignment in statistical translation.
InCOLING ?96: The 16th Int.
Conf.
on ComputationalLinguistics, pages 836?841, Copenhagen, Denmark,August.55
