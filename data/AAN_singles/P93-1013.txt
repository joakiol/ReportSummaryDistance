PLANNING MULT IMODAL D ISCOURSEWolfgang Wahls terGerman Research Center for Artificial Intelligence (DFKI)Stuh lsatzenhausweg 3D-6600 Saarbr/ icken 11, GermanyInternet :  wahlster@dfk i .un i -sb .deAbst rac tIn this talk, we will, show how techniques for plan-ning text and discourse can be generalized to planthe structure and content of multimodal communi-cations, that integrate natural anguage, pointing,graphics, and animations.
The central claim ofthis talk is that the generation of multimodal dis-course can be considered as an incremental plan-ning process that aims to achieve a given commu-nicative goal.One of the surprises from our research is thatit is actually possible to extend and adapt manyof the fundamental concepts developed to datein computatational linguistics in such a way thatthey become useful for multimodal discourse aswell.
This means that an interesting methodologi-cal transfer from the area of natural anguage pro-cessing to a much broader computational model ofmultimodal communication is possible.
In partic-ular, semantic and pragmatic oncepts like speechacts, coherence, focus, communicative act, dis-course model, reference, implicature, anaphora,rhetorical relations and scope ambiguity take anextended meaning in the context of multimodaldiscourse.It is an important goal of this research notsimply to merge the verbalization and visualiza-tion results of mode-specific generators, but tocarefully coordinate them in such a way that theygenerate a multiplieative improvement in commu-nication capabilities.
Allowing all of the modali-ties to refer to and depend upon each other is akey to the richness of multimodal communication.A basic principle underlying our model is thatthe various constituents of a multimodal commu-nication should be generated from a common rep-resentation of what is to be conveyed.
This raisesthe question of how to decompose a given com-municative goal into subgoals to be realized bythe mode-specific generators, so that they com-plement each other.
To address this problem, weexplore computational models of the cognitive de-eision process, coping with questions uch as whatshould go into text, what should go into graphics,and which kinds of links between the verbal andnon-verbal fragments are necessary.
In addition,we deal with layout as a rhetorical force, influ-encing the intentional and attentional state of thediscourse participants.We have been engaged in work in the area ofmultimodal communication for several years now,starting with the HAM-ANS (Wahlster et al 1983)and VITRA systems (Wahlster 1989), which auto-matically create natural language descriptions ofpictures and image sequences shown on the screen.These projects resulted in a better understandingof how perception interacts with language produc-tion.
Since then, we have been investigating waysof integrating tactile pointing and graphics withnatural language understanding and generationin the XTRA (Wahlster 1991) and WIP projects(Wahlster et al 1991).The task of the knowledge-based presentationsystem WIP is the context-sensitive g neration ofa variety of multimodal communications from aninput including a presentation goal (Wahlster etal.
1993a).
The presentation goal is a formal repre-sentation of the communicative intent specified bya back-end application system.
WIP is currentlyable to generate simple multimodal explanationsin German and English on using an espresso ma-chine, assembling a lawn-mower, or installing amodem, demonstrating our claim of language andapplication independence.
WIP is a highly adap-tive multimodal presentation system, since all ofits output is generated on the fly and customizedfor the intended iscourse situation.
The quest foradaptation is based on the fact that it is impos-sible to anticipate the needs and requirements ofeach potential dialog partner in an infinite numberof discourse situations.
Thus all presentation deci-sions are postponed until runtime.
In contrast ohypermedia-based approaches, WIP does not useany preplanned texts or graphics.
That is, eachpresentation is designed from scratch by reasoning95from first principles using commonsense presenta-tion knowledge.We approximate the fact that multimodalcommunication is always situated by introducingseven discourse parameters in our model.
The cur-rent system includes a choice between user stereo-types (e.g.
novice, expert), target languages (Ger-man vs. English), layout formats (e.g.
paper hard-copy, slide, screen display), output modes (incre-mental output vs. complete output only), pre-ferred mode (e.g.
text, graphics, or no prefer-ence), and binary switches for space restrictionsand speech output.
This set of parameters i  usedto specify design constraints that must be satisfiedby the final presentation.
The combinatorics ofWIP's contextual parameters can generate 576 al-ternate multimodal presentations of the same con-tent.At the heart of the multimodal presentationsystem WIP is a parallel top-down planner (Andr6and Rist 1993) and a constraint-based layout man-ager.
While the root of the hierarchical plan struc-ture for a particular multimodal communicationcorresponds to a complex communicative act suchas describing a process, the leaves are elementaryacts that verbalize and visualize information spec-ified in the input from the back-end applicationsystem.In multimodal generation systems, three dif-ferent processes are distinguished: a content plan-ning process, a mode selection process and a con-tent realization process.
A sequential architec-ture in which data only flow from the "what topresent" to the "how to present" part has proveninappropriate because the components responsiblefor selecting the contents would have to anticipateall decisions of the realization components.
Thisproblem is compounded if content realization isdone by separate components (e.g.
for language,pointing, graphics and animations) of which thecontent planner has only limited knowledge.It seems even inappropriate to sequentializecontent planning and mode selection.
Selecting amode of presentation depends to a large extent onthe nature of the information to be conveyed.On the other hand, content planning isstrongly influenced by previously selected modecombinations.
E.g., to graphically refer to a phys-ical object (Rist and Andr6 1992), we need visualinformation that may be irrelevant o textual ref-erences.
In the WIP system, we interleave contentand mode selection.
In contrast o this, presen-tation planning and content realization are per-formed by separate components o enable parallelprocessing (Wahlster et al 1993b).In a follow-up project to WIP called PPP(Personalized Plan-Based Presenter), we are cur-rently addressing the additional problem of plan-ning presentation acts such as pointing and coor-dinated speech output during the display of themultimodal material synthesized by WIP.The insights and experience we gained fromthe design and implementation f the multimodalsystems IIAM-ANS, VITRA, XTRA and WIPprovide a good starting point for a deeper un-derstanding of the interdependencies of language,graphics, pointing, and animations in coordinatedmultimodal discourse.REFERENCESAndre, Elisabeth; and Rist, Thomas.
1993.
TheDesign of Illustrated Documents as a PlanningTask.
Maybury, Mark (ed.).
Intelligent Multime-dia Interfaces, AAAI Press (to appear).Rist, Thomas; and Andr6, Elisabeth.
1992.From Presentation Tasks to Pictures: Towards anApproach to Automatic Graphics Design.
Pro-ceedings European Conference on AI (ECAI-92),Vienna, Austria (1992) 764-768.Wahlster, Wolfgang.
1989.
One Word Saysmore than a Thousand Pictures.
On the Auto-matic Verbalization of the Results of Image Se-quence Analysis Systems.
Computers and Artifi-cial Intelligence, 8, 5:479-492Wahlster, Wolfgang.
1991.
User and Dis-course Models for Multimodal Communication.in: Sullivan, J.W.
; and Tyler, S.W.(eds.).
Intel-ligent User Interfaces, Reading: Addison-Wesley(1991): 45-67.Wahlster, Wolfgang; Marburger, Heinz; Jame-son, Anthony; Busemann, Stephan.
1983.
Over-answering Yes-No Questions: Extended Responsesin a NL Interface to a Vision System.
Proceedingsof IJCAI-83, Karlsruhe: 643-646.Wahlster, Wolfgang; Andr6, Elisabeth; Graf,Winfried; and Rist, Thomas.
1991.
Designing I1-lustrated Texts: How Language Production is In-fluenced by Graphics Generation.
Proceedings Eu-ropean ACL Conference, Berlin, Germany: 8-14.Wahlster, Wolfgang; Andr6, Elisabeth; Ban-dyopadhyay, Som; Graf, Winfried; and Rist,Thomas.
1993a.
WIP: The Coordinated Gener-ation of Multimodal Presentations from a Com-mon Representation, i : Ortony, A.; Slack, J.; andStock, O.(eds.).
Communication from an Artifi-cial Intelligence Perspective: Theoretical and Ap-plied Issues, Springer: Heidelberg: 121-144.Wahlster, Wolfgang; Andr6, Elisabeth; Fin-kler, Wolfgang; Profitlich, Hans-Jiirgen; and Rist,Thomas.
1993b.
Plan-Based Integration of Natu-ral Language and Graphics Generation.
ArtificialIntelligence Journal 26(3), (to appear).96
