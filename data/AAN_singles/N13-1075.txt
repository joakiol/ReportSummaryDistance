Proceedings of NAACL-HLT 2013, pages 655?660,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsTranslation Acquisition Using Synonym SetsDaniel Andrade Masaaki Tsuchida Takashi Onishi Kai IshikawaKnowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan{s-andrade@cj, m-tsuchida@cq,t-onishi@bq, k-ishikawa@dq}.jp.nec.comAbstractWe propose a new method for translation ac-quisition which uses a set of synonyms to ac-quire translations from comparable corpora.The motivation is that, given a certain queryterm, it is often possible for a user to specifyone or more synonyms.
Using the resultingset of query terms has the advantage that wecan overcome the problem that a single queryterm?s context vector does not always reliablyrepresent a terms meaning due to the contextvector?s sparsity.
Our proposed method usesa weighted average of the synonyms?
contextvectors, that is derived by inferring the meanvector of the von Mises-Fisher distribution.We evaluate our method, using the synsetsfrom the cross-lingually aligned Japanese andEnglish WordNet.
The experiments show thatour proposed method significantly improvestranslation accuracy when compared to a pre-vious method for smoothing context vectors.1 IntroductionAutomatic translation acquisition is an importanttask for various applications.
For example, findingterm translations can be used to automatically up-date existing bilingual dictionaries, which are an in-dispensable resource for tasks such as cross-lingualinformation retrieval and text mining.Various previous research like (Rapp, 1999; Fung,1998) has shown that it is possible to acquire wordtranslations from comparable corpora.We suggest here an extension of this approachwhich uses several query terms instead of a singlequery term.
A user who searches a translation fora query term that is not listed in an existing bilin-gual dictionary, might first try to find a synonymof that term.
For example, the user might look upa synonym in a thesaurus1 or might use methodsfor automatic synonym acquisition like describedin (Grefenstette, 1994).
If the synonym is listed inthe bilingual dictionary, we can consider the syn-onym?s translations as the translations of the queryterm.
Otherwise, if the synonym is not listed in thedictionary either, we use the synonym together withthe original query term to find a translation.We claim that using a set of synonymous queryterms to find a translation is better than using a singlequery term.
The reason is that a single query term?scontext vector is, in general, unreliable due to spar-sity.
For example, a low frequent query term tends tohave many zero entries in its context vector.
To mit-igate this problem it has been proposed to smootha query?s context vector by its nearest neighbors(Pekar et al 2006).
However, nearest neighbors,which context vectors are close the query?s contextvector, can have different meanings and thereforemight introduce noise.The contributions of this paper are two-fold.
First,we confirm experimentally that smoothing a query?scontext vector with its synonyms leads in deed tohigher translation accuracy, compared to smoothingwith nearest neighbors.
Second, we propose a sim-ple method to combine a set of context vectors thatperforms in this setting better than a method previ-ously proposed by (Pekar et al 2006).Our approach to combine a set of context vec-1Monolingual thesauri are, arguably, easier to construct thanbilingual dictionaries.655tors is derived by learning the mean vector of a vonMises-Fisher distribution.
The combined contextvector is a weighted-average of the original context-vectors, where the weights are determined by theword occurrence frequencies.In the following section we briefly show the rela-tion to other previous work.
In Section 3, we explainour method in detail, followed by an empirical eval-uation in Section 4.
We summarize our results inSection 6.2 Related WorkThere are several previous works on extract-ing translations from comparable corpora rangingfrom (Rapp, 1999; Fung, 1998), and more re-cently (Haghighi et al 2008; Laroche and Langlais,2010), among others.
Essentially, all these meth-ods calculate the similarity of a query term?s contextvector with each translation candidate?s context vec-tor.
The context vectors are extracted from the com-parable corpora, and mapped to a common vectorspace with the help of an existing bilingual dictio-nary.The work in (De?jean et al 2002) uses cross-lingually aligned classes in a multilingual thesaurusto improve the translation accuracy.
Their methoduses the probability that the query term and a trans-lation candidate are assigned to the same class.
Incontrast, our method does not need cross-linguallyaligned classes.Ismail and Manandhar (2010) proposes a methodthat tries to improve a query?s context vector by us-ing in-domain terms.
In-domain terms are the termsthat are highly associated to the query, as well ashighly associated to one of the query?s highly asso-ciated terms.
Their method makes it necessary thatthe query term has enough highly associated contextterms.2 However, a low-frequent query term mightnot have enough highly associated terms.In general if a query term has a low-frequency inthe corpus, then its context vector is sparse.
In thatcase, the chance of finding a correct translation isreduced (Pekar et al 2006).
Therefore, Pekar et al(2006) suggest to use distance-based averaging tosmooth the context vector of a low-frequent query2In their experiments, they require that a query word has atleast 100 associated terms.term.
Their smoothing strategy is dependent on theoccurrence frequency of a query term and its closeneighbors.
Let us denote q the context vector of thequery word, and K be the set of its close neighbors.The smoothed context vector q?
is then derived byusing:q?
= ?
?
q + (1 ?
?)
?
?x?Kwx ?
x , (1)where wx is the weight of neighbor x, and allweights sum to one.
The context vectors q and xare interpreted as probability vectors and thereforeL1-normalized.
The weight wx is a function of thedistance between neighbor x and query q.
The pa-rameter ?
determines the degree of smoothing, andis a function of the frequency of the query term andits neighbors:?
= log f(q)logmaxx?K?
{q} f(x)(2)where f(x) is the frequency of term x.
Their methodforms the baseline for our proposed method.3 Proposed MethodOur goal is to combine the context vectors to onecontext vector which is less sparse and more reli-able than the original context vector of query wordq.
We assume that for each occurrence of a word,its corresponding context vector was generated bya probabilistic model.
Furthermore, we assume thatsynonyms are generated by the same probability dis-tribution.
Finally we use the mean vector of that dis-tribution to represent the combined context vector.By using the assumption that each occurrence of aword corresponds to one sample of the probabilitydistribution, our model places more weight on syn-onyms that are highly-frequent than synonyms thatoccur infrequently.
This is motivated by the assump-tion that context vectors of synonyms that occur withhigh frequency in the corpus, are more reliable thanthe ones of low-frequency synonyms.When comparing context vectors, worklike Laroche and Langlais (2010) observedthat often the cosine similarity performs superiorto other distance-measures, like, for example, theeuclidean distance.
This suggests that contextvectors tend to lie in the spherical vector space,656and therefore the von Mises-Fisher distribution isa natural choice for our probabilistic model.
Thevon Mises-Fisher distribution was also successfullyused in the work of (Basu et al 2004) to clustertext data.The von Mises-Fisher distribution with locationparameter ?, and concentration parameter ?
is de-fined as:p(x|?, ?)
= c(?)
?
e??x?
?T ,where c(?)
is a normalization constant, and ||x|| =||?|| = 1, and ?
?
0.
|| denotes here the L2-norm.The cosine-similarity measures the angle betweentwo vectors, and the von Mises distribution definesa probability distribution over the possible angles.The parameter ?
of the von Mises distribution is es-timated as follows (Jammalamadaka and Sengupta,2001): Given the words x1, ..., xn, we denote thecorresponding context vectors as x1, ...,xn, and as-sume that each context vector is L2-normalized.Then, the mean vector ?
is calculated as:?
= 1Zn?i=1xinwhere Z ensures that the resulting context vector isL2-normalized, i.e.
Z is ||?ni=1xin ||.
For our pur-pose, ?
is irrelevant and is assumed to be any fixedpositive constant.Since we assume that each occurrence of a word xin the corpus corresponds to one observation of thecorresponding word?s context vector x, we get thefollowing formula:?
= 1Z ?
?n?i=1f(xi)?nj=1 f(xj)?
xiwhere Z ?
is now ||?ni=1f(xi)?nj=1 f(xj)?
xi||.
We thenuse the vector ?
as the combined vector of thewords?
context vectors xi.Our proposed procedure to combine the contextvector of query word q and its synonyms can be sum-marized as follows:1.
Denote the context vectors of q and its syn-onyms as x1, ...,xn, and L2-normalize eachcontext vector.2.
Calculate the weighted average of the vectorsx1, ...,xn, whereas the weights correspond tothe frequencies of each word xi.3.
L2-normalize the weighted average.4 ExperimentsAs source and target language corpora we use a cor-pus extracted from a collection of complaints con-cerning automobiles compiled by the Japanese Min-istry of Land, Infrastructure, Transport and Tourism(MLIT)3 and the USA National Highway TrafficSafety Administration (NHTSA)4, respectively.
TheJapanese corpus contains 24090 sentences that werePOS tagged using MeCab (Kudo et al 2004).
TheEnglish corpus contains 47613 sentences, that werePOS tagged using Stepp Tagger (Tsuruoka et al2005), and use the Lemmatizer (Okazaki et al2008) to extract and stem content words (nouns,verbs, adjectives, adverbs).For creating the context vectors, we calculate theassociation between two content words occurringin the same sentence, using the log-odds-ratio (Ev-ert, 2004).
It was shown in (Laroche and Langlais,2010) that the log-odds-ratio in combination withthe cosine-similarity performs superior to severalother methods like PMI5 and LLR6.
For comparingtwo context vectors we use the cosine similarity.To transform the Japanese and English contextvectors into the same vector space, we use a bilin-gual dictionary with around 1.6 million entries.7To express all context vectors in the same vectorspace, we map the context vectors in English to con-text vectors in Japanese.8 First, for all the wordswhich are listed in the bilingual dictionary we calcu-late word translation probabilities.
These translationprobabilities are calculated using the EM-algorithmdescribed in (Koehn and Knight, 2000).
We thencreate a translation matrix T which contains in each3http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html4http://www-odi.nhtsa.dot.gov/downloads/index.cfm5point-wise mutual information6log-likelihood ratio7The bilingual dictionary was developed in the course of ourJapanese language processing efforts described in (Sato et al2003).8Alternatively, we could, for example, use canonical corre-lation analysis to match the vectors to a common latent vectorspace, like described in (Haghighi et al 2008).657column the translation probabilities for a word inEnglish into any word in Japanese.
Each contextvector in English is then mapped into Japanese us-ing the linear transformation described by the trans-lation matrix T .
For word x with context vector x inEnglish, let x?
be its context vector after transforma-tion into Japanese, i.e.
x?
= T ?
x.The gold-standard was created by consideringall nouns in the Japanese and English WordNetwhere synsets are aligned cross-lingually.
This waywe were able to create a gold-standard with 215Japanese nouns, and their respective English trans-lations that occur in our comparable corpora.9 Notethat the cross-lingual alignment is needed only forevaluation.
For evaluation, we consider only thetranslations that occur in the corresponding Englishsynset as correct.Because all methods return a ranked list of trans-lation candidates, the accuracy is measured using therank of the translation listed in the gold-standard.The inverse rank is the sum of the inverse ranks ofeach translation in the gold-standard.In Table 1, the first row shows the results when us-ing no smoothing.
Next, we smooth the query?s con-text vector by using Equation (1) and (2).
The set ofneighbors K is defined as the k-terms in the sourcelanguage that are closest to the query word, with re-spect to the cosine similarity (sim).
The weight wxfor a neighbor x is set to wx = 100.13?sim(x,q) inaccordance to (Pekar et al 2006).
For k we triedvalues between 1 and 100, and got the best inverserank when using k=19.
The resulting method (Top-k Smoothing) performs consistently better than themethod using no smoothing, see Table 1, secondrow.
Next, instead of smoothing the query word withits nearest neighbors, we use as the set K the set ofsynonyms of the query word (Syn Smoothing).
Ta-ble 1 shows a clear improvement over the methodthat uses nearest neighbor-smoothing.
This confirmsour claim that using synonyms for smoothing canlead to better translation accuracy than using nearestneighbors.
In the last row of Table 1, we compareour proposed method to combine context vectors ofsynonyms (Syn Mises-Combination), with the pre-9The resulting synsets in Japanese and English, contain inaverage 2.2 and 2.8 words, respectively.
The ambiguity of aquery term in our gold-standard is low, since, in average, aquery term belongs to only 1.2 different synsets.vious method (Syn Smoothing).
A pair-wise com-parison of our proposed method with Syn Smooth-ing shows a statistically significant improvement (p< 0.01).10Finally, we also show the result when simplyadding each synonym vector to the query?s contextvector to form a new combined context vector (SynSum).11 Even though, this approach does not use thefrequency information of a word, it performs bet-ter than Syn Smoothing.
We suppose that this isdue to the fact that it actually indirectly uses fre-quency information, since the log-odds-ratio tendsto be higher for words which occur with high fre-quency in the corpus.Method Top1 Top5 Top10 MIRNo Smoothing 0.14 0.30 0.36 0.23Top-k Smoothing 0.16 0.33 0.43 0.26Syn Smoothing 0.18 0.35 0.46 0.28Syn Sum 0.23 0.46 0.57 0.35Syn Mises-Combination 0.31 0.46 0.55 0.40Table 1: Shows Top-n accuracy and mean inverse rank(MIR) for baseline methods which use no synonyms(No Smoothing, Top-k Smoothing), the proposed method(Syn Mises-Combination) which uses synonyms, and al-ternative methods that also use synonyms (Syn Smooth-ing, Syn Sum).5 DiscussionWe first discuss an example where the query termsare????
(cruise) and??
(cruise).
Both wordscan have the same meaning.
The resulting trans-lation candidates suggested by the baseline meth-ods and the proposed method is shown in Table 2.Using no smoothing, the baseline method outputsthe correct translation for ????
(cruise) and ??
(cruise) at rank 10 and 15, respectively.
Whencombining both queries to form one context vectorour proposed method (Syn Mises-Combination) re-trieves the correct translation at rank 2.
Note that weconsidered all nouns that occur three or more timesas possible translation candidates.
As can be seenin Table 2, this also includes spelling mistakes like?sevice?
and ?infromation?.10We use the sign-test (Wilcox, 2009) to test the hypothesisthat the proposed method ranks higher than the baseline.11No normalization is performed before adding the contextvectors.658Method Query Output RankNo Smoothing ????
..., affinity, delco, cruise, sevice, sentrum,... 10No Smoothing ??
..., denali, attendant, cruise, abs, tactic,... 15Top-k Smoothing ????
pillar, multi, cruise, star, affinity,... 3Top-k Smoothing ??
..., burnout, dipstick, cruise, infromation, speed, ... 8Syn Smoothing ????
smoothed with??
..., affinity, delco, cruise, sevice, sentrum,... 10Syn Smoothing ??
smoothed with????
..., alldata, mode, cruise, expectancy, mph,... 8Syn Sum ????,??
assumption, level, cruise, reimbursment, infromation,... 3Syn Mises-Combination ????,??
pillar, cruise, assumption, level, speed,... 2Table 2: Shows the results for ????
and ??
which both have the same meaning ?cruise?.
The third columnshows part of the ranked translation candidates separated by comma.
The last column shows the rank of the correcttranslation ?cruise?.
Syn Smoothing uses Equation (1) with q corresponding to the context vector of the query word,andK contains only the context vector of the term that is used for smoothing.Finally, we note that some terms in our test setare ambiguous, and the ambiguity is not resolved byusing the synonyms of only one synset.
For exam-ple, the term ??
(steering, guidance) belongs tothe synset ?steering, guidance?
which includes theterms???
(steering, guidance) and???
(guid-ance), ??
(guidance).
Despite this conflation ofsenses in one synset, our proposed method can im-prove the finding of (one) correct translation.
Thebaseline system using only??
(steering, guidance)outputs the correct translation ?steering?
at rank 4,whereas our method using all four terms outputs itat rank 2.6 ConclusionsWe proposed a new method for translation acquisi-tion which uses a set of synonyms to acquire transla-tions.
Our approach combines the query term?s con-text vector with all the context vectors of its syn-onyms.
In order to combine the vectors we use aweighted average of each context vector, where theweights are determined by a term?s occurrence fre-quency.Our experiments, using the Japanese and EnglishWordNet (Bond et al 2009; Fellbaum, 1998), showthat our proposed method can increase the transla-tion accuracy, when compared to using only a singlequery term, or smoothing with nearest neighbours.Our results suggest that instead of directly search-ing for a translation, it is worth first looking for syn-onyms, for example by considering spelling varia-tions or monolingual resources.ReferencesS.
Basu, M. Bilenko, and R.J. Mooney.
2004.
A prob-abilistic framework for semi-supervised clustering.
InProceedings of the ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining,pages 59?68.F.
Bond, H. Isahara, S. Fujita, K. Uchimoto, T. Kurib-ayashi, and K. Kanzaki.
2009.
Enhancing thejapanese wordnet.
In Proceedings of the 7th Workshopon Asian Language Resources, pages 1?8.
Associationfor Computational Linguistics.H.
De?jean, E?.
Gaussier, and F. Sadat.
2002.
An approachbased on multilingual thesauri and model combinationfor bilingual lexicon extraction.
In Proceedings of theInternational Conference on Computational Linguis-tics, pages 1?7.
International Committee on Computa-tional Linguistics.S.
Evert.
2004.
The statistics of word cooccurrences:word pairs and collocations.
Doctoral dissertation, In-stitut fu?r maschinelle Sprachverarbeitung, Universita?tStuttgart.C.
Fellbaum.
1998.
Wordnet: an electronic lexicaldatabase.
Cambrige, MIT Press, Language, Speech,and Communication.P.
Fung.
1998.
A statistical view on bilingual lexicon ex-traction: from parallel corpora to non-parallel corpora.Lecture Notes in Computer Science, 1529:1?17.G.
Grefenstette.
1994.
Explorations in automatic the-saurus discovery.
Springer.A.
Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.2008.
Learning bilingual lexicons from monolingualcorpora.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics, pages771?779.
Association for Computational Linguistics.A.
Ismail and S. Manandhar.
2010.
Bilingual lexiconextraction from comparable corpora using in-domainterms.
In Proceedings of the International Conferenceon Computational Linguistics, pages 481 ?
489.659S.R.
Jammalamadaka and A. Sengupta.
2001.
Topics incircular statistics, volume 5.
World Scientific Pub CoInc.P.
Koehn and K. Knight.
2000.
Estimating word trans-lation probabilities from unrelated monolingual cor-pora using the em algorithm.
In Proceedings of theNational Conference on Artificial Intelligence, pages711?715.
Association for the Advancement of Artifi-cial Intelligence.T.
Kudo, K. Yamamoto, and Y. Matsumoto.
2004.
Ap-plying conditional random fields to Japanese morpho-logical analysis.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 230?237.
Association for Computational Lin-guistics.A.
Laroche and P. Langlais.
2010.
Revisiting context-based projection methods for term-translation spottingin comparable corpora.
In Proceedings of the In-ternational Conference on Computational Linguistics,pages 617 ?
625.N.
Okazaki, Y. Tsuruoka, S. Ananiadou, and J. Tsujii.2008.
A discriminative candidate generator for stringtransformations.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 447?456.
Association for Computational Lin-guistics.V.
Pekar, R. Mitkov, D. Blagoev, and A. Mulloni.
2006.Finding translations for low-frequency words in com-parable corpora.
Machine Translation, 20(4):247?266.R.
Rapp.
1999.
Automatic identification of word transla-tions from unrelated English and German corpora.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics, pages 519?526.
Asso-ciation for Computational Linguistics.K.
Sato, T. Ikeda, T. Nakata, and S. Osada.
2003.
In-troduction of a Japanese language processing mid-dleware used for CRM.
In Annual Meeting of theJapanese Association for Natural Language Process-ing (in Japanese), pages 109?112.Y.
Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,S.
Ananiadou, and J. Tsujii.
2005.
Developing a ro-bust part-of-speech tagger for biomedical text.
LectureNotes in Computer Science, 3746:382?392.R.R.
Wilcox.
2009.
Basic Statistics: UnderstandingConventional Methods and Modern Insights.
OxfordUniversity Press.660
