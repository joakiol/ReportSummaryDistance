Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 58?68, Dublin, Ireland, August 23-29 2014.Identification of Implicit Topics in Twitter Data Not Containing ExplicitSearch QueriesSuzi Park Hyopil ShinDepartment of Linguistics, Seoul National University1 Gwanak-ro, Gwanak-gu, Seoul, 151-745, Republic of Korea{mam3b,hpshin}@snu.ac.krAbstractThis study aims at retrieving tweets with an implicit topic, which cannot be identified by thecurrent query-matching system employed by Twitter.
Such tweets are relevant to a given querybut do not explicitly contain the term.
When these tweets are combined with a relevant tweetcontaining the overt keyword, the ?serialized?
tweets can be integrated into the same discoursecontext.
To this end, features like reply relation, authorship, temporal proximity, continuationmarkers, and discourse markers were used to build models for detecting serialization.
Accordingto our experiments, each one of the suggested serializing methods achieves higher means ofaverage precision rates than baselines such as the query matching model and the tf-idf weightingmodel, which indicates that considering an individual tweet within a discourse context is helpfulin judging its relevance to a given topic.1 Introduction1.1 Limits of the Twitter Query-Matching SearchTwitter search was not a very crucial thing in the past (Stone, 2009a), at least for users in its earlystages who read and wrote tweets only within their curated timelines real-time (Dorsey, 2007; Stone,2009b; Stone, 2009c).
Users?
personal interests became one of the motivations to explore a large bodyof tweets only after commercial, political and academic demands, but it triggered the current extensionof the Twitter search service.
The domain of Twitter search was widened, for example, from tweets inthe recent week to older ones (Burstein, 2013), and from accounts that have a specific term in their nameor username to those that are relevant to that particular subject (Stone, 2007; Stone, 2008; Twitter, 2011;Kozak, Novermber 19, 2013).
However, the standard Twitter search mechanism is based only on thepresence of query terms.Even though the Twitter Search API provides many operators, the current query matching search doesnot guarantee retrieving a complete list of all relevant tweets.12The 140-character limit sometimes forcesa tweet not to contain a term, not because of its lack of relevance to the topic represented by the term,but due to one of the following:Reduction the query term is written in an abbreviated form or in form of Internet slang,Expansion the query term is in external text that can be expanded through other services such as Twit-Longer (http://twitlonger.com) and twtkr (http://twtkr.olleh.com), while thepart exceeding 140 characters is shown only as a link on twitter.com, orSerialization the query term is contained as an antecedent in some previous tweet.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1?
[T]he Search API is focused on relevance and not completeness.?
2 October 2013.
Using the Twitter Search API.https://dev.twitter.com/docs/using-search2?
[T]he Search API is not meant to be an exhaustive source of Tweets.?
7 March 2013.
GET search/tweets https://dev.twitter.com/docs/api/1.1/get/search/tweets58If these cases are frequent enough, the current query matching search in Twitter will get a low recall rate.Considering that tweets are usually used to obtain as various views on a topic as possible, in addition toaccurate and reliable information about it, this setback would block attempts to collect diverse opinionsin Twitter.These three different cases require different approaches.
First, reduction, one of the most significantcharacteristics of Twitter data in natural language processing, can be solved by building a dictionaryof Internet slang terms or learning them.
Second, in case of expansion tweets are always accompaniedwith short URLs (http://tl.gd for TwitLonger and http://dw.am for twtkr) and the full textis reachable through them.
In these two cases, tweets correspond one-on-one with documents, whetherreduced internally or expanded externally.
This study will focus on the third case, serialization, whereseveral tweets may be interpreted as a single document.1.2 Serialization of Tweets: An Overlooked Aspect of TwitterThough little reported before, serialization of tweets is frequently observed in Korean data.3Influentialusers like famous journalists, columnists and critics as well as ordinary users often publish multipletweets over a short period of time instead of using other media such as blogs or web magazines.
Types oftweets published in this way by Korean users include reports, reviews, and analysis on political or socialaffairs, news articles, books, films and dramas.
The content users intend to express is longer than a tweetbut shorter than a typical blog post.
Examples from our dataset will be introduced in Section 3.This study aims at retrieving tweets on a topic, which cannot be found by the current query-matchingsystem.
Such tweets are relevant to a given query but do not contain the necessary words.
Under thehypothesis that a considerable number of these tweets not containing the query term are serialized withone containing it overtly, and that serialized segments are integrated into the same discourse context, webuilt a model that allows us, when given a tweet that includes a query or a mentioned topic, to find theother tweets serialized with it and count them as relevant to the topic.
We primarily focused on KoreanTwitter data, but we believe that the methods developed here are also applicable to other languages withsimilar phenomena.2 Previous StudiesOur study is based on the observation that a tweet in a ?serialization?
does not necessarily correspondto a full document.
In fact, it has already been reported (Hong and Davison, 2010; Weng et al., 2010;Mehrotra et al., 2013) that a single tweet is too short to be treated as an individual document, especiallyconsidering that word co-occurrence in a tweet is hardly found.
Studies proved that performance ofLatent Dirichlet Allocation (LDA) models for Twitter topic discovery can be improved by aggregatingtweets into a document.
In these studies, a ?document ?
consists either of all tweets under the sameauthorship (Hong and Davison, 2010; Weng et al., 2010), all tweets published in a particular period, orall tweets sharing a hashtag (Mehrotra et al., 2013).
These criteria are useful for finding topics, intowhich tweets can be classified, but our purpose requires a different degree of ?documentness.?
Ourstudy deals with a fixed topic and is interested in whether or not only tweets relevant to the topic canbe pooled.
All tweets merged into the same document as constructed in the previous studies are notnecessarily coherent or related to the same topic because it is not usually expected that ordinary usersdevote their Twitter accounts to a single topic.
In this study, we will develop more detailed criteria forthe aggregation of tweets by combining authorship with time intervals and adopting features such assentiment consistency and discourse markers.A method of using discourse markers for microblog data was proposed by Mukherjee and Bhat-tacharyya (2012).
They noted that a dependency parser, on which opinion analyses using discourseinformation (Somasundaran, 2010) are usually based, is inadequate for small microblog data, and in-stead used a ?lightweight?
discourse analysis, considering the existence of a discourse marker on eachtweet.
The list of discourse markers used in their study was based on the list of conjunctions repre-senting discourse relations presented by Wolf et al.
(2004).
This method was successful for sentiment3Some Korean users sarcastically call this a ?saga?
of tweets.59analysis on Twitter data assuming that the relevance of each tweet to a certain topic was already known.We will take a similar approach of using discourse markers, but with a different assumption and for adifferent purpose.
In our study, we treat unknown topic relevance of tweets with missing query terms byaggregating them with a topic-marked tweet using discourse markers.3 Features3.1 Properties of Tweet SerializationMultiple tweets are likely to be consistent with a topic if they form a discourse as in the followingsituations, with examples of tweets in Korean translated into English.
In each tweet, topic words are inboldface.Conversation This is the most typical case.U1: Wow the neighborhood theater is packed; will Snowpiercer hit ten million?U2: @U1 My parents and my boss are all gonna watch, and they watch only one film a year.
Thisis the measure for ten million.Comment after retweet Users retweet and comment.U3 RT @U4: Today?s quote.
?It is stupid to concentrate on symbolic meaning in Wang Kar Wai?sHappy Together.
That would be like trying to find political messages and signs in Snow-piercer.?
?
Jung Sung-IlU3 Master Jung?s sarcasm........?On-the-spot addition Because a published tweet cannot be edited, users can elaborate or correct it onlyby writing a new tweet or deleting the existing tweet.U5 Is Curtis the epitome of Director Bong?s4sinserityU5 Sincerity, shitTrue (intentional) serialization Some users begin to write tweets with a text of more than 140 charac-ters in mind.
They arrive at the length limit and continue to write in a new tweet.U6 (1) Watched Snowpiercer.
It was more interesting than I thought.
It felt more like black comedythan SF.
On another note, I was surprised by several oddities, making the film feel more like aKorean film with foreign actors in it rather than Director Bong?s Hollywood debut.U6 (2) In many ways the film was ?nineties?...
like watching The City of Lost Children all overagain... and the trip from the tail-car to the first car, though I expected some kind of level-upfor each car,U6 (3) the world connected car to car was not an organic world (a sideways pyramid?)
but worldstoo separate car by car, and the front-car people were so lifeless that I was surprised.
The scaleof the ?charge?
after 17 years felt shrunken.If this is a characteristic feature of Korean Twitter data, this may be due to reasons such as personalwriting style, the writing system of the Korean language, and Korean Web platforms.
First, it may besimply because these users prefer formal language and are reluctant to use short informal expressionseven in Web writing.
Second, it is possibly because CJK writing systems including Hangul, the Koreanalphabet, have more information per character than the Roman alphabet (Neubig and Duh, 2013).
Sincea 140-character text in Hangul has generally more information than that in the Roman alphabet, a Korean(or Japanese) user can more readily tweet about content which an English (or other European) languageuser would consider too long to write about on Twitter.
Third, for many Korean users Twitter is the mostavailable medium for publishing their opinions online, as a number of standard blogs have been replaced4Director of the film Snowpiercer60by microblogs.
Some users divide a long public text into multiple length-limited tweets simply becausethey do not have a blog to write in.While Internet slang and abbreviations are common in tweets, ?Serializers?
tend to use 1) fully-spelled forms (unlike ?reducers?
), 2) usually without hashtags and emoticons, 3) which are all visible ontwitter.com itself (unlike ?expanders?
), so it is not guaranteed that all serialized tweets will containthe topic word, as in the examples above.
This implies that some tweet segments in a single discoursemay not be retrieved even if the discourse is relevant to a given query.
Search results may include apartial document for which it is difficult the full version of which is difficult to find.3.2 Extralinguistic CriteriaTwo tweets are more likely to be a part of a larger document consisting of a series of tweets ifReply-relation one of them is a reply to the other,Temporal proximity they are published immediately one after the other, orContinuation markers they share such markers as numbers, arrows>> and continuation marker ?(con-tinued).
?Figure 1 shows examples of each case.Figure 1: Serialized tweets with numbers, an arrow, or a continuation marker ?
(continued)?3.3 Linguistic CluesSemantic similarity to the query In order to determine the relatedness of two documents, the similaritybetween their term distributions is mainly considered.
Based on this idea, one of our baselinemethods will represent each tweet as a bag-of-words vector and retrieve a tweet containing noquery term if its tf-idf weighted vector has a high cosine similarity with at least one vector from atweet containing a query term.Discourse markers Users may add a discourse marker when writing a new sentence in a new tweet.
Ifa tweet begins with a marker that indicates continuation of a discourse, it is likely to be a part of alarger document.
A sentiment analysis in Twitter by Mukherjee and Bhattacharyya (2012) adopteddiscourse relations from Wolf et al.
(2004).
In this paper, we use linguistic characteristics describedby Kang (1999) in order to classify Korean texts, listing their English translations in Table 1.
Thediscourse marker feature refers to whether or not any marker on the list occurs in the first N words(set N = 5) of the tweet.4 Experiments4.1 DataWe collected 173,271 tweets posted or retweeted by 105 Korean users, including film critics, film stu-dents, and amateur cinephiles from 27 July to 26 September 2013.
Out of the 105 users, 17 users whohad mentioned the film Snowpiercer5most often were singled out.
In addition, the highest overall oc-currence of the keyword was found to be between 1 to 15 August, probably due the film?s release on 315http://www.imdb.com/title/tt1706620/61Demonstratives this, that, it, here, thereProverbs be so, do soDiscourse well, nowConj-Reasoning because, so, therfore, thus, henceConj-Conditional then, as long as, in the case, underConj-Coordinate and, norConj-Adversative but, yet, however, still, by contrastConj-Discourse meanwhile, anyway, by the wayTable 1: List of selected Korean discourse markers used for classifying text types in Kang (1999), trans-lated into EnglishJuly in South Korea.
Then we kept all 8,543 tweets posted by those 17 users from the period between 1to 15 August 2013, in order to construct a labeled data set.
This set includes 189 tweets that explicitlycontain the word Snowpiercer.
Each tweet in the filtered set was labeled as related or not related to themovie by three annotators who were Twitter users already following most of the above 17 users and thusaware of the context of most tweets, and a tweet was considered relevant if two or more of the annota-tors agreed.
Inter-annotator agreement was evaluated by using Fleiss?s kappa statistic flei:71, which was?
= 0.749 (p ?
0).
Table 2 shows the annotation results.Related Not related TotalExplicit 173 15 188Not explicit 207 8,148 8,355Total 380 8,163 8,543Table 2: The number of annotated tweets classified by explicitness and relatednessTable 2 shows that 8163/8543 = 95.55% of the tweets in the dataset are not relevant to the movieSnowpiercer.
Additional topics are induced from 7?9 manually collected seed words among the 200most frequently occurring nouns in the dataset, in which each tweet text was POS-tagged by the Koreanmorphological and POS tagger Hannanum6.
Induced topics and their seed words are listed in Table 3.Topic Seed wordsMovie Movie, Snowpiercer, director, The Terror Live, actor, stage, audience, film, theaterLiterature Story, book, writing, author, novel, character, workGender/relationship Men, women, female, marriage, male, wife, loverPolitics Politics, state, Park Geun-hye, government, president, party, Ahn Cheol-sooTable 3: Four topics from manually collected seed wordsAs described in 3.1, it should be noted again that hashtags are not always useful for finding informationin Korean tweets, particularly in this dataset.
Among the seed words above, only Snowpiercer was everused as a hashtag, and happened only three times (twice in English and once in Korean).
Only ninetypes of hashtags occurred more than twice in the full dataset (they are presented in Table 4 with theirrespective frequencies).
This predicts that hashtag-based tweet aggregation would not be very useful tofind tweets relevant to Snowpiercer or one of the four induced topics.Table 5 shows the number of tweets containing seed words for each topic, where a tweet is allowed tobelong to more than one topic.
Since only 1853/8543 = 21.69% of the tweets explicitly contain a topicor seed word, it is not plausible that each of the remaining 80% tweets belongs to one of the four topics.Many of the tweets may be related to a topic which was of a too small portion to be induced, or to notopic at all.
So, instead of classifying all of the tweets into the given topics, the experiment seeks toretrieve any tweet that is relevant to a certain topic, which allows each tweet to belong to more than onetopic at once.
In every experiment we regarded tweets that contain a topic or seed word as relevant to thetopic, and restricted the test set to those tweets which did not contain them.6http://sourceforge.net/projects/hannanum/62#make people cry with a story of two words 13#lgtwins 10#quote 7#changing zero0 to fatty makes things totally depressing 6#EBSbookcafe 4#today i feel 4#blow the whistle on chun doo-hwan 3#chosundotcom 3#the name of your bias followed by the name of the food you just ate feels nice 3Table 4: Korean hashtags occurring more than twice in the dataset, translated into EnglishMovie Literature Gender Politics Total716 452 379 306 1853Table 5: Number of tweets including at least one of the seed words for each induced topic4.2 MeasuresFor all models, the authors judged the relevance of each of the retrieved tweets for induced topics until tenrelevant tweets were retrieved.
In the Snowpiercer case, precision scores were calculated for all recallscores.
We built a ranking retrieval system for each model and evaluated its performance by averageprecision.
For models including a randomizing process, we used the mean of average precisions over1,000 replicated samples.
Precision was computed at every percentile of recall levels for Snowpiercercase and after each retrieved relevant tweet (up to top 10) for induced topics.
In sum, the performance ofa model m was defined in two ways asmeanAP@percent(m) :=110001000?i=1AP@percent(mi)andmeanAP@10(m) :=110001000?i=1AP@10(mi), where m has 1,000 replicates m1, ?
?
?m1000whose measures areAP@percent(mi) :=1100100?j=1prec@j%(mi)andAP@10(mi) :=11010?k=1prec@k%(mi).When m is a tf-idf model, which has a unique ranking without replication, average precision was used.4.3 BaselinesQuery matching method The most obvious baseline method for this study is the current Twitter searchsystem that treats topic words and seed words as queries and finds documents, or tweets, that arerelevant to the topic.
Since only tweets not containing the query terms remained in the test set, thereare no tweets matching them.
As the set of retrieved tweets is empty, relevance rank is randomlyassigned to each tweet of the test set.Tf-idf weighting method One may predict that a tweet is likely to be relevant to a topic if it shows asimilar word distribution to some explicitly relevant tweets.
Under this assumption, we representedeach tweet as a tf-idf weighted vector (Salton and Buckley, 1988) after removing all punctuationmarks and user-mention markers (@username).
Stopwords were not removed and tf-idf valueswere length-normalized.
Relevance of each tweet in the test set was defined as the maximum of itscosine-similarities with all tweets containing a query term.634.4 Tweet SerializationExamples of Tweet Serialization in Section 3 indicate clues between related tweets other than distribu-tional similarity.
When 1) a tweet is a reply to another one, 2) two tweets are written one after anotherby the same user, 3) one tweet following another includes some discourse marker, or 4) two tweets sharea marker, such as numbers, they can be considered to be serialized into a single document rather thanbeing two separate ones.
Tweets serialized together are treated as a single document, and if this docu-ment contains a a tweet with a query term, then all tweets lacking it but belonging to the same the samedocument are retrieved.
All retrieved tweets are first ranked in random order, followed by the others alsoin random order.We suggest four criteria for Tweet Serialization:Reply Two tweets are serialized if one is a reply to the other.Continuation markers Two tweets are serialized if they are written successively by the same user andshare a marker, such as a number or a phrase ?(cont.
)?Discourse markers Two tweets are serialized if they are written successively by the same user, the lattercontains one of the discourse markers listed in Table 1 in its first 5 words, and neither of them is areply to another user.Time Two tweets are serialized if they are written successively by the same user within a given intervaland neither of them is a reply to another user.
The upper boundary for intervals is set in one of thefollowing ways:Constant 30 or 60 secondsUser-specific Users may show different densities in their tweets, depending on their tweeting en-vironment.
Distribution of time intervals between successive tweets over users is presentedin Table 6.
The smallest 5% and 15% quantiles were selected, corresponding to 30 and 60seconds respectively.Quantile U1 U2 U3 U4 U5 U6 U7 U8 U9 U10 U11 U12 U13 U14 U15 U16 U170% 3 19 2 1 1 5 10 3 2 3 9 3 2 3 3 2 165% 20 42 18 16 13 30 21 18 13 8 43 23 18 15 13 12 11010% 33 45 25 35 20 43 38 38 28 13 71 35 28 35 21 21 13015% 47 52 33 57 30 56 67 57 40 23 89 51 37 61 27 40 16120% 62 67 41 79 41 73 92 74 53 31 111 65 50 84 33 58 19725% 81 86 55 100 55 92 145 95 69 43 138 84 68 105 38 77 27550% 237 298 164 322 151 242 1060 297 167 159 297 317 178 258 90 266 725Table 6: Time intervals (in seconds) by cumulative percentile between consecutive pairs of tweets foreach userFor all criteria, Tweet Serialization is transitive, that is, if tiand tjare serialized and tjand tkareserialized, then tiand tkare serialized.
Table 7 shows the distribution of serialization sizes (numberof serialized tweets) over criteria.
Time value of 60 seconds serializes most tweets, as many as (8543-6464)/8543=24.33%, while continuation markers serialize only (8543-8511)/8543 = .37%.
Assuming allserializations are correct, the relevance of retrieved documents is judged.4.5 ResultsThe average precision values of all models are summarized in Table 8 (means calculated over recalllevels) and Figure 2 (means calculated over 1,000 replications).
In both Tables 8 and 9, differencesbetween the tf-idf weighting model and each of the Serialization methods were statistically significantaccording to t-test.
Figure 2 compares the results of the serialization methods, among which continuationmarker model has the highest precision over 0.8 at the 1% recall level, and Time with 15% quantile has theaverage precision score showing the slowest decrease.
Even though for all serialization methods averageprecision values converge to zero as recall levels increase, each of the method gets higher precision ratesthan baselines until some part of relevant tweets are retrieved.64Size Repl.
Disc.
Cohe.
T:30s T:60s T:5% T:15%1 8137 8169 8511 7314 6464 7845 68492 88 166 6 465 664 298 6103 34 14 2 76 149 31 1094 9 0 0 6 40 1 195 3 0 1 5 12 1 86 5 0 0 1 6 0 27 3 0 0 0 3 0 08 1 0 0 2 2 0 19 2 0 1 0 0 0 010 0 0 0 0 0 0 011 0 0 0 0 1 0 1Table 7: Distribution of serialization size (number of serialized tweets) under each criterionRecall level (%)MeanofAPover1,000replications0.00.20.40.60.80 20 40 60 80 100tf-idf match reply disc.
cont.
30sec 60sec 5% 15%Figure 2: Means of average precision rates of all methods for the topic SnowpiercerRecall Baselines: Repl.
Disc.
Cont.
Time difference thresholdlevel ?
Match tf-idf rela.
mark.
mark.
30sec 60sec 5% 15%5% .0342 .0518 .3019 .1266 .2313 .5158 .4178 .6804 .472010% .0309 .0588 .1798 .0801 .1324 .3916 .3459 .4050 .397625% .0284 .0695 .0920 .0494 .0702 .1824 .1665 .1847 .189450% .0273 .0685 .0602 .0382 .0486 .1062 .0986 .1070 .1103100% .0268 .0556 .0434 .0322 .0375 .0666 .0628 .0669 .0687Table 8: Means of average precision rates (at recall level up to 5%, 50%, and 100%) on various se-rialization criteria for the topic Snowpiercer (Results in boldface represent the best results among themethods.
)Serialization methods also perform better than the tf-idf baseline for induced topics, as shown in Figure3 and Table 9.
In particular, Reply and Discourse markers, which were far from the best for Snowpiercer,serve well for other topics such as Movie in general, Politics, and Gender/Relationships.The precision of Reply for the topic Movie is exceptionally high, partly because the data were initiallycollected from users who were interested in films.
Reply relation is dependent on the choice of the data,in that it is determined by interaction between users, not by a single user?s tweets.
If data are collectedfrom users friendly with each other, Reply will serialize many tweets.
On the contrary, if data containssome users while leaving out their friends, replies to these friends are not serialized by Reply criteria.Discourse markers give a precision of higher than 50% for the topic Politics, which is likely to bediscussed in more formal expressions using various conjunctions.65Figure 3: Means of average precision rates of all methods for the induced topicsBaselines: Repl.
Disc.
Cont.
Time difference thresholdMatch tf-idf rela.
mark.
mark.
30sec 60sec 5% 15%Mov.
.0134 .1139 .8855 .3925 .1791 .3787 .3123 .4161 .3435Lit.
.0026 .0759 .1804 .0171 .1293 .2005 .1287 .1719 .1601Gen.
.0048 .1287 .0653 .2424 .0050 .1092 .2476 .0187 .2297Pol.
.0090 .2176 .2135 .5762 .0625 .5072 .4453 .5234 .4948Table 9: Means of Average Precision rates at cutoff k = 10 of baselines and different serializationcriteria for induced topics (Results in boldface represent the most accurate results of the topic among themethods.
)In the topics Literature and Gender/Relationships, average precision scores are at most 25%, whichpossibly results from the fact that the seed words for these topics consist of general terms only, whilethose of the other two topics include proper nouns such as movie titles or politicians?
names.
This isless a problem of the topic itself but rather one of data selection, which focused on users tweeting aboutfilms, and so the set of seed words will vary according to differences in data collection.5 ConclusionIn this paper, we found that tweets with an implicit topic can be found more effectively by consideringwhether or not they are serialized with some tweet containing the overt keyword.
Our experiments showthat Tweet Serialization can be detected using various criteria such as reply relations between users,presence of discourse or continuation markers, and temporal proximity under the same authorship.
Our66original purpose was to find as various opinions on a given topic as possible, but we expect the methodsused here will be helpful for other tasks, including topic discovery and sentiment analysis, by settingmore exact document boundaries in microblog data.
The method we proposed is for Korean Twitterdata, where tweet serialization is observed frequently, particularly among influential users, but it is alsoapplicable to other languages with similar phenomena.In future work, we will investigate methods for the evaluation of the results of Tweet Serializationand combine tf-idf methods with Tweet Serialization criteria.
Furthermore, we aim at verifying theapplicability of the results of this study with regard to more various users and more topics.ReferencesPaul Burstein.
February 7, 2013.
Older Tweets in search results.
The Official Twitter Blog.
https://blog.twitter.com/2013/now-showing-older-tweets-in-search-results.Jack Dorsey.
September 25, 2007.
Tracking Twitter.
The Official Twitter Blog.
https://blog.twitter.com/2007/tracking-twitter.Joseph L. Fleiss.
1971.
Measuring nominal scale agreement among many raters.
Psychological Bulletin.
76(5):378?382.Liangjie Hong and Brian D. Davison.
2010.
Empirical study of topic modeling in Twitter.
SOMA 2010: TheProceedings of the First Workshop on Social Media Analytics.
80?88.Beom-mo Kang.
1999.
Hankukeui theksuthu cangluwa ene thukseng [Text genres and linguistic characteristics inKorean].
Korea University Press, Seoul, Korea.Esteban Kozak.
November 19, 2013.
New ways to search on Twitter.
The Official Twitter Blog.
https://blog.twitter.com/2013/new-ways-to-search-on-twitter.Subhabrata Mukherjee and Pushpak Bhattacharyya.
2012.
Sentiment analysis in Twitter with lightweight dis-course analysis.
COLING 2012: The 24th International Conference on Computational Linguistics, Proceedingsof the Conference: Technical Papers.
1847?1864.Rishabh Mehrotra, Scoot Sanner, Wray Buntine and Lexing Xie.
2013.
Improving LDA topic models for mi-croblogs via tweet pooling and automatic labeling.
SIGIR ?13; The 36th International ACM SIGIR Conferenceon Research and Development in Information Retrieval.
889?892.Graham Neubig and Kevin Duh.
2013.
How much is said in a Tweet?
A multilingual, information-theoreticperspective.
AAAI Spring Symposium: Analyzing Microtext, Volume SS-13-01 of AAAI Technical Report.Gerard Salton and Christopher Buckley.
1988.
Term-weighting approaches in automatic text retrieval.
InformationProcessing & Management.
24(5): 513?523.Swapna Somasundaran.
2010.
Discourse-level Relations for Opinion Analysis.
Ph.D Thesis, University of Pitts-burgh.Biz Stone.
August 22, 2007.
Searching Twitter.
The Official Twitter Blog.
https://blog.twitter.com/2007/searching-twitter.Biz Stone.
December 23, 2008.
Finding Nemo ?
Or, name search is back!
The Official Twitter Blog.
https://blog.twitter.com/2008/finding-nemo%E2%80%94or-name-search-back.Biz Stone.
February 18, 2009.
Testing a more integrated search experience.
The Official Twitter Blog.
https://blog.twitter.com/2009/testing-more-integrated-search-experience.Biz Stone.
April 03, 2009.
The discovery engine is coming.
The Official Twitter Blog.
https://blog.twitter.com/2009/discovery-engine-coming.Biz Stone.
April 30, 2009.
Twitter search for everyone!
The Official Twitter Blog.
https://blog.twitter.com/2009/twitter-search-everyone.Twitter.
April 4, 2011.
Discover new accounts and search like a pro.
The Official Twitter Blog.
https://blog.twitter.com/2011/discover-new-accounts-and-search-pro.67Jianshu Weng, Ee-Peng Lim, Jing Jiang, and Qi He.
2010.
TwitterRank: Finding topic-sensitive influentialtwitterers.
WSDM ?10: Proceedings of the Third ACM International Conference on Web Search and DataMining.
261?270.Florian Wolf, Edward Gibson and Timothy Desmet.
2004.
Discourse coherence and pronoun resolution.
Lan-guage and Cognitive Processes, 19(6): 665?675.68
