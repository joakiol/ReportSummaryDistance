Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431?437,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAutomatic Discrimination between Cognates and BorrowingsAlina Maria Ciobanu, Liviu P. DinuFaculty of Mathematics and Computer Science, University of BucharestCenter for Computational Linguistics, University of Bucharestalina.ciobanu@my.fmi.unibuc.ro,ldinu@fmi.unibuc.roAbstractIdentifying the type of relationship be-tween words provides a deeper insight intothe history of a language and allows a bet-ter characterization of language related-ness.
In this paper, we propose a com-putational approach for discriminating be-tween cognates and borrowings.
We showthat orthographic features have discrimi-native power and we analyze the underly-ing linguistic factors that prove relevant inthe classification task.
To our knowledge,this is the first attempt of this kind.1 IntroductionNatural languages are living eco-systems.
Theyare subject to continuous change due, in part, tothe natural phenomena of language contact andborrowing (Campbell, 1998).
According to Hall(1960), there is no such thing as a ?pure language??
a language ?without any borrowing from a for-eign language?.
Although admittedly regardedas relevant factors in the history of a language(McMahon et al, 2005), borrowings bias the ge-netic classification of the languages, characteriz-ing them as being closer than they actually are(Minett and Wang, 2003).
Thus, the need fordiscriminating between cognates and borrowingsemerges.
Heggarty (2012) acknowledges the ne-cessity and difficulty of the task, emphasizing therole of the ?computerized approaches?.In this paper we address the task of automati-cally distinguishing between borrowings and cog-nates: given a pair of words, the task is to de-termine whether one is a historical descendant ofthe other, or whether they both share a commonancestor.
A borrowing (also called loanword), isdefined by Campbell (1998) as a ?lexical item (aword) which has been ?borrowed?
from anotherlanguage, a word which originally was not part ofthe vocabulary of the recipient language but wasadopted from some other language and made partof the borrowing language?s vocabulary?.
The no-tion of cognate is much more relaxed, and vari-ous NLP tasks and applications use different def-initions of the cognate pairs.
In some situations,cognates and borrowings are considered together,and are referred to as historically connected words(Kessler, 2001) or denoted by the term correlates(Heggarty, 2012; McMahon et al, 2005).
In sometasks, such as statistical machine translation (Kon-drak et al, 2003) and sentence alignment, or whenstudying the similarity or intelligibility of the lan-guages, cognates are seen as words that have sim-ilar spelling and meaning, their etymology beingcompletely disregarded.
However, in problemsof language classification, distinguishing cognatesfrom borrowings is essential.
Here, we accountfor the etymology of the words, and we adopt thefollowing definition: two words form a cognatepair if they share a common ancestor and havethe same meaning.
In other words, they derive di-rectly from the same word, have a similar meaningand, due to various (possibly language-specific)changes across time, their forms might differ.2 Related WorkIn a natural way, one of the most investigatedproblems in historical linguistics is to determinewhether similar words are related or not (Kondrak,2002).
Investigating pairs of related words is veryuseful not only in historical and comparative lin-guistics, but also in the study of language relat-edness (Ng et al, 2010), phylogenetic inference(Atkinson et al, 2005) and in identifying how andto what extent languages changed over time or in-fluenced each other.Most studies in this area focus on automaticallyidentifying pairs of cognates.
For measuring theorthographic or phonetic proximity of the cog-nate candidates, string similarity metrics (Inkpen431et al, 2005; Hall and Klein, 2010) and algo-rithms for string alignment (Delmestri and Cris-tianini, 2010) have been applied, both in cognatedetection (Koehn and Knight, 2000; Mulloni andPekar, 2006; Navlea and Todirascu, 2011) and incognate production (Beinborn et al, 2013; Mul-loni, 2007).
Minett and Wang (2003) focus onidentifying borrowings within a family of genet-ically related languages and propose, to this end,a distance-based and a character-based technique.Minett and Wang (2005) address the problem ofidentifying language contact, building on the ideathat borrowings bias the lexical similarities amonggenetically related languages.According to the regularity principle, the dis-tinction between cognates and borrowings benefitsfrom the regular sound changes that generate reg-ular phoneme correspondences in cognates (Kon-drak, 2002).
In turn, sound correspondences arerepresented, to a certain extent, by alphabetic char-acter correspondences (Delmestri and Cristianini,2010).3 Our ApproachIn light of this, we investigate whether cognatescan be automatically distinguished from borrow-ings based on their orthography.
More specifically,our task is as follows: given a pair of words in twodifferent languages (x, y), we want to determinewhether x and y are cognates or if y is borrowedfrom x (in other words, x is the etymon of y).Our starting point is a methodology that haspreviously proven successful in discriminating be-tween related and unrelated words (Ciobanu andDinu, 2014b).
Briefly, the method comprises thefollowing steps:1) Aligning the pairs of related words using astring alignment algorithm;2) Extracting orthographic features from thealigned words;3) Training a binary classifier to discriminatebetween the two types of relationship.To align the pairs of related words, we em-ploy the Needleman-Wunsch global alignment al-gorithm (Needleman and Wunsch, 1970), which isequivalent to the weighted edit distance algorithm.We consider words as input sequences and we usea very simple substitution matrix1, which assigns1In our future work, we intend to also experiment withmore informed language-specific substitution matrices.Lang.Cognates Borrowingslen1len2edit len1len2editIT-RO 7.95 8.78 0.26 7.58 8.41 0.29ES-RO 7.91 8.33 0.26 5.78 6.14 0.52PT-RO 7.99 8.35 0.28 5.35 5.42 0.52TR-RO 7.35 6.88 0.31 6.49 6.09 0.44Table 2: Statistics for the dataset of related words.Given a pair of languages (L1, L2), the len1andlen2columns represent the average word length ofthe words in L1and L2.
The edit column rep-resents the average normalized edit distance be-tween the words.
The values are computed onlyon the training data, to keep the test data unseen.equal scores to all substitutions, disregarding dia-critics (e.g., we ensure that e and `e are matched).As features, we use characters n-grams extractedfrom the alignment2.
We mark word boundarieswith $ symbols.
For example, the Romanian wordfunct?ie (meaning function) and its Spanish cognatepair funci?on are aligned as follows:$ f u n c t?
i e - $$ f u n c - i ?o n $The features for n = 2 are:$f$f, fufu, unun, ncnc, ct?
c-,t?i-i, iei?o, e-?on, -$n$.For the prediction task, we experiment withtwo models, Naive Bayes and Support Vector Ma-chines.
We extend the method by introducing ad-ditional linguistic features and we conduct an anal-ysis on their predictive power.4 Experiments and ResultsIn this section we present and analyze the experi-ments we run for discriminating between cognatesand borrowings.4.1 DataOur experiments revolve around Romanian, a Ro-mance language belonging to the Italic branchof the Indo-European language family.
It is sur-rounded by Slavic languages and its relationshipwith the big Romance kernel was difficult.
Its ge-ographic position, at the North of the Balkans, put2While the original methodology proposed features ex-tracted around mismatches in the alignment, we now comparetwo approaches: 1) features extracted around mismatches,and 2) features extracted from the entire alignment.
The latterapproach leads to better results, as measured on the test set.432Lang.
Borrowings CognatesIT-RO baletto ?
balet (ballet) vittoria - victorie (victory) ?
victoria (LAT)PT-RO selva ?
selv?a(selva) instinto - instinct (instinct) ?
instinctus (LAT)ES-RO machete ?
macet?a (machete) castillo - castel (castle) ?
castellum (LAT)TR-RO t?ut?un ?
tutun (tobacco) aranjman - aranjament (arrangement) ?
arrangement (FR)Table 1: Examples of borrowings and cognates.
For cognates we also report the common ancestor.it in contact not only with the Balkan area, but alsowith the vast majority of Slavic languages.
Polit-ical and administrative relationships with the Ot-toman Empire, Greece (the Phanariot domination)and the Habsburg Empire exposed Romanian toa wide variety of linguistic influences.
We applyour method on four pairs of languages extractedfrom the dataset proposed by Ciobanu and Dinu(2014c):?
Italian - Romanian (IT-RO);?
Portuguese - Romanian (PT-RO);?
Spanish - Romanian (ES-RO);?
Turkish - Romanian (TR-RO).For the first three pairs of languages, whichare formed of sister languages3, most cognatepairs have a Latin common ancestor, while for thefourth pair, formed of languages belonging to dif-ferent families (Romance and Turkic), most of thecognate pairs have a common French etymology,and date back to the end of the 19thcentury, whenboth Romanian and Turkish borrowed massivelyfrom French.
In Table 1 we provide examples ofborrowings and cognates.The dataset contains borrowings4and cognatesthat share a common ancestor.
The words (and in-formation about their origins) were extracted fromelectronic dictionaries and their relationships weredetermined based on their etymology.
We use astratified dataset of 2,600 pairs of related wordsfor each pair of languages.
In Table 2 we providean initial analysis of our dataset.
We report statis-tics regarding the length of the words and the editdistance between them.
The difference in lengthbetween the related words shows what operationsto expect when aligning the words.
Romanianwords are almost in all situations shorter, in av-erage, than their pairs.
For TR-RO len1is higher3Sister languages are ?languages which are related to oneanother by virtue of having descended from the same com-mon ancestor (proto-language)?
(Campbell, 1998).4Romanian is always the recipient language in our dataset(i.e., the language that borrowed the words).than len2, so we expect more deletions for this pairof languages.
The edit columns show how muchwords vary from one language to another based ontheir relationship (cognates or borrowings).
ForIT-RO both distances are small (0.26 and 0.29), asopposed to the other languages, where there is amore significant difference between the two (e.g.,0.26 and 0.52 for ES-RO).
The small differencefor IT-RO might make the discrimination betweenthe two classes more difficult.4.2 BaselinesGiven the initial analysis presented above, wehypothesize that the distance between the wordsmight be indicative of the type of relationshipbetween them.
Previous studies (Inkpen et al,2005; Gomes and Lopes, 2011) show that relatedand non-related words can be distinguished basedon the distance between them, but a finer-grainedtask, such as determining the type of relationshipbetween the words, is probably more subtle.
Wecompare our method with two baselines:?
A baseline which assigns a label based on thenormalized edit distance between the words:given a test instance pair word1- word2, wesubtract the average normalized edit distancebetween word1and word2from the aver-age normalized edit distance of the cognatepairs and from the average normalized editdistance between the borrowings and their et-ymons (computed on the training set; see Ta-ble 2), and assign the label which yields asmaller difference (in absolute value).
In caseof equality, the label is chosen randomly.?
A decision tree classifier, following the strat-egy proposed by Inkpen et al (2005): weuse the normalized edit distance as single fea-ture, and we fit a decision tree classifier withthe maximum tree depth set to 1.
We per-form 3-fold cross-validation in order to se-lect the best threshold for discriminating be-tween borrowings and cognates.
Using the433best threshold selected for each language, wefurther assign one of the two classes to thepairs of words in our test set.4.3 Task SetupWe experiment with Naive Bayes and SupportVector Machines (SVMs) to learn orthographicchanges.
We put our system together using theWeka5workbench (Hall et al, 2009).
For SVM,we employ the radial basis function kernel (RBF)and we use the wrapper provided by Weka forLibSVM (Chang and Lin, 2011).
For each lan-guage pair, we split the dataset in two stratifiedsubsets, for training and testing, with a 3:1 ra-tio.
We experiment with different values for then-gram size (n ?
{1, 2, 3}) and we perform gridsearch and 3-fold cross validation over the train-ing set in order to optimize hyperparameters c and?
for SVM.
We search over {1, 2, ..., 10} for c andover {10?2, 10?1, 100, 101, 102} for ?.4.4 Results AnalysisTable 3 and Table 4 show the results of our ex-periment.
The two baselines produce comparableresults.
For all pairs of languages, our method sig-nificantly improves over the baselines (99% con-fidence level)6with values between 7% and 29%for the F1score, suggesting that the n-grams ex-tracted from the alignment of the words are bet-ter indicators of the type of relationship than theedit distance between them.
The best results areobtained for TR-RO, with an F1score of 92.1,followed closely by PT-RO with 90.1 and ES-ROwith 85.5.
These results show that, for these pairsof languages, the orthographic cues are differentwith regard to the relationship between the words.For IT-RO we obtain the lowest F1score, 69.0.In this experiment, we know beforehand thatthere is a relationship between the words, and ouraim is to identify the type of relationship.
How-ever, in many situations this kind of a-priori in-formation is not available.
In a real scenario, wewould have either to add an intermediary clas-sifier for discriminating between related and un-related words, or to discriminate between threeclasses: cognates, borrowings, and unrelated.
Weaugment our dataset with unrelated words (deter-mined based on their etymology), building a strat-5www.cs.waikato.ac.nz/ml/weka6All the statistical significance tests reported in this paperare performed on 1,000 iterations of paired bootstrap resam-pling (Koehn, 2004).Lang.Baseline #1 Baseline #2P R F1P R F1IT-RO 50.7 50.7 50.7 64.4 54.5 45.0PT-RO 79.3 79.0 79.2 80.1 80.0 80.0ES-RO 78.6 78.4 78.5 78.6 78.5 78.4TR-RO 61.1 61.0 61.1 62.5 59.8 57.6Table 3: Weighted average precision (P ), recall(R) and F1score (F1) for automatic discrimina-tion between cognates and borrowings.ified dataset annotated with three classes, and werepeat the previous experiment.
The performancedecreases7, but the results are still significantlybetter than chance (99% confidence level).4.5 Linguistic FactorsTo gain insight into the factors with high predictivepower, we perform several further experiments.Part of speech.
We investigate whether addingknowledge about the part of speech of thewords leads to performance improvements.Verbs, nouns, adverbs and adjectives havelanguage-specific endings, thus we assume thatpart of speech might be useful when learningorthographic patterns.
We obtain POS tags fromthe DexOnline8machine-readable dictionary.We employ the POS feature as an additionalcategorical feature for the learning algorithm.
Itturns out that, except for PT-RO (F1score 92.3),the additional POS feature does not improve theperformance of our method.Syllabication.
We analyze whether the systembenefits from using the syllabified form of thewords as input to the alignment algorithm.
Weare interested to see if marking the boundaries be-tween the syllables improves the alignment (and,thus, the feature extraction).
We obtain the syl-labication for the words in our dataset from theRoSyllabiDict dictionary (Barbu, 2008) for Roma-nian words and several available Perl modules9forthe other languages.
For PT-RO and ES-RO theF1score increases by about 1%, reaching a valueof 93.4 for the former and 86.7 for the latter.7Weighted average F1score on the test set for SVM:IT-RO 63.8, PT-RO 77.6, ES-RO 74.0, TR-RO 86.1.8www.dexonline.ro9Lingua::ID::Hyphenate modules where ID ?
{IT, PT,ES, TR}, available on the Comprehensive Perl Archive Net-work: www.cpan.org.434Lang.Naive Bayes SVMP R F1n P R F1n c ?IT-RO 68.6 68.2 68.3 3 69.2 69.1 69.0 3 10 0.10PT-RO 92.6 91.7 92.1 3 90.1 90.0 90.0 3 3 0.10ES-RO 85.3 84.5 84.9 3 85.7 85.5 85.5 2 2 0.10TR-RO 89.7 89.4 89.5 3 90.3 90.2 90.1 3 6 0.01Table 4: Weighted average precision (P ), recall (R), F1score (F1) and optimal n-gram size for automaticdiscrimination between cognates and borrowings.
For SVM we also report the optimal values for c and ?.Consonants.
We examine the performance ofour system when trained and tested only on thealigned consonant skeletons of the words (i.e., aversion of the words where vowels are discarded).According to Ashby and Maidment (2005), conso-nants change at a slower pace than vowels acrosstime; while the former are regarded as referencepoints, the latter are believed to carry less infor-mation useful for identifying the words (Gooskenset al, 2008).
The performance of the systemdecreases when vowels are removed (95% confi-dence level).
We also train and test the decisiontree classifier on this version of the dataset, andits performance is lower in this case as well (95%confidence level), indicating that, for our task, theinformation carried by the vowels is helpful.Stems.
We repeat the first experiment usingstems as input, instead of lemmas.
What we seekto understand is whether the aligned affixes are in-dicative of the type of relationship between thewords.
We use the Snowball Stemmer10and wefind that the performance decreases when stemsare used instead of lemmas.
Performing a ?2fea-ture ranking on the features extracted from mis-matches in the alignment of the related words re-veals further insight into this matter: for all pairsof languages, at least one feature containing the $character (indicating the beginning or the end of aword) is ranked among the 10 most relevant fea-tures, and over 50 are ranked among the 500 mostrelevant features.
This suggests that prefixes andsuffixes (usually removed by the stemmer) varywith the type of relationship between the words.Diacritics.
We explore whether removing dia-critics influences the performance of the system.Many words have undergone transformations bythe augmentation of language-specific diacritics10http://snowball.tartarus.orgwhen entering a new language (Ciobanu and Dinu,2014a).
For this reason, we expect diacritics toplay a role in the classification task.
We observethat, when diacritics are removed, the F1scoreon the test set is lower in almost all situations.Analyzing the ranking of the features extractedfrom mismatches in the alignment provides evenstronger evidence in this direction: for all pairs oflanguages, more than a fifth of the top 500 featurescontain diacritics.5 ConclusionsIn this paper, we propose a computational methodfor discriminating between cognates and borrow-ings based on their orthography.
Our results showthat it is possible to identify the type of rela-tionship with fairly good performance (over 85.0F1score) for 3 out of the 4 pairs of languages weinvestigate.
Our predictive analysis shows that theorthographic cues are different for cognates andborrowings, and that underlying linguistic factorscaptured by our model, such as affixes and diacrit-ics, are indicative of the type of relationship be-tween the words.
Other insights, such as the syl-labication or the part of speech of the words, areshown to have little or no predictive power.
Weintend to further account for finer-grained char-acteristics of the words and to extend our exper-iments to more languages.
The method we pro-pose is language-independent, but we believe thatincorporating language-specific knowledge mightimprove the system?s performance.AcknowledgementsWe thank the anonymous reviewers for their help-ful and constructive comments.
The contribu-tion of the authors to this paper is equal.
LiviuP.
Dinu was supported by UEFISCDI, PNII-ID-PCE-2011-3-0959.435ReferencesMichael Ashby and John Maidment.
2005.
Introduc-ing Phonetic Science.
Cambridge University Press.Quentin D. Atkinson, Russell D. Gray, Geoff K.Nicholls, and David J. Welch.
2005.
From Wordsto Dates: Water into Wine, Mathemagic or Phylo-genetic Inference?
Transactions of the PhilologicalSociety, 103(2):193?219.Ana-Maria Barbu.
2008.
Romanian Lexical DataBases: Inflected and Syllabic Forms Dictionaries.
InProceedings of the 6th International Conference onLanguage Resources and Evaluation, LREC 2008,pages 1937?1941.Lisa Beinborn, Torsten Zesch, and Iryna Gurevych.2013.
Cognate Production using Character-basedMachine Translation.
In Proceedings of the 6th In-ternational Joint Conference on Natural LanguageProcessing, IJCNLP 2013, pages 883?891.Lyle Campbell.
1998.
Historical Linguistics.
An Intro-duction.
MIT Press.Chih-Chung Chang and Chih-Jen Lin.
2011.LIBSVM: A Library for Support Vector Ma-chines.
ACM Transactions on Intelligent Sys-tems and Technology, 2(3):27:1?27:27.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Alina Maria Ciobanu and Liviu P. Dinu.
2014a.
AnEtymological Approach to Cross-Language Ortho-graphic Similarity.
Application on Romanian.
InProceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2014, pages 1047?1058.Alina Maria Ciobanu and Liviu P. Dinu.
2014b.
Au-tomatic Detection of Cognates Using OrthographicAlignment.
In Proceedings of the 52st Annual Meet-ing of the Association for Computational Linguis-tics, ACL 2014, pages 99?105.Alina Maria Ciobanu and Liviu P. Dinu.
2014c.
Build-ing a Dataset of Multilingual Cognates for the Ro-manian Lexicon.
In Proceedings of the 9th Interna-tional Conference on Language Resources and Eval-uation, LREC 2014.Antonella Delmestri and Nello Cristianini.
2010.String Similarity Measures and PAM-like Matricesfor Cognate Identification.
Bucharest Working Pa-pers in Linguistics, 12(2):71?82.Lu?
?s Gomes and Jos?e Gabriel Pereira Lopes.
2011.Measuring Spelling Similarity for Cognate Identifi-cation.
In Proceedings of the 15th Portugese Con-ference on Progress in Artificial Intelligence, EPIA2011, pages 624?633.
Software available at http://research.variancia.com/spsim.Charlotte Gooskens, Wilbert Heeringa, and Karin Bei-jering.
2008.
Phonetic and Lexical Predictors ofIntelligibility.
International Journal of Humanitiesand Arts Computing, 2(1-2):63?81.David Hall and Dan Klein.
2010.
Finding CognateGroups Using Phylogenies.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, ACL 2010, pages 1030?1039.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Up-date.
SIGKDD Explorations, 11(1):10?18.Robert Anderson Hall.
1960.
Linguistics and YourLanguage.
Doubleday New York.Paul Heggarty.
2012.
Beyond Lexicostatistics: Howto Get More out of ?Word List?
Comparisons.
InQuantitative Approaches to Linguistic Diversity:Commemorating the Centenary of the Birth of Mor-ris Swadesh, pages 113?137.
Benjamins.Diana Inkpen, Oana Frunza, and Grzegorz Kondrak.2005.
Automatic Identification of Cognates andFalse Friends in French and English.
In Proceed-ings of the International Conference on Recent Ad-vances in Natural Language Processing, RANLP2005, pages 251?257.Brett Kessler.
2001.
The Significance of Word Lists.Stanford: CSLI Publications.Philipp Koehn and Kevin Knight.
2000.
Estimat-ing Word Translation Probabilities from UnrelatedMonolingual Corpora Using the EM Algorithm.
InProceedings of the 17th National Conference on Ar-tificial Intelligence and 12th Conference on Inno-vative Applications of Artificial Intelligence, pages711?715.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods in Natu-ral Language Processing, EMNLP 2004, pages 388?395.Grzegorz Kondrak, Daniel Marcu, and Keven Knight.2003.
Cognates Can Improve Statistical TranslationModels.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, HLT-NAACL 2003, pages 46?48.Grzegorz Kondrak.
2002.
Algorithms For LanguageReconstruction.
Ph.D. thesis, University of Toronto.April McMahon, Paul Heggarty, Robert McMahon,and Natalia Slaska.
2005.
Swadesh Sublistsand the Benefits of Borrowing: an Andean CaseStudy.
Transactions of the Philological Society,103(2):147?170.James W. Minett and William S.-Y.
Wang.
2003.On Detecting Borrowing: Distance-based andCharacter-based Approaches.
Diachronica,20(2):289?331.436James W. Minett and William S.-Y.
Wang.
2005.
Ver-tical and Horizontal Transmission in Language Evo-lution.
Transactions of the Philological Society,103(2):121?146.Andrea Mulloni and Viktor Pekar.
2006.
AutomaticDetection of Orthographic Cues for Cognate Recog-nition.
In In Proceedings of the 5th InternationalConference on Language Resources and Evaluation,LREC 2006, pages 2387?2390.Andrea Mulloni.
2007.
Automatic Prediction of Cog-nate Orthography Using Support Vector Machines.In Proceedings of the 45th Annual Meeting of theACL: Student Research Workshop, ACL 2007, pages25?30.Mirabela Navlea and Amalia Todirascu.
2011.
UsingCognates in a French-Romanian Lexical AlignmentSystem: A Comparative Study.
In Proceedings ofthe International Conference on Recent Advances inNatural Language Processing, RANLP 2011, pages247?253.Saul B. Needleman and Christian D. Wunsch.
1970.
AGeneral Method Applicable to the Search for Sim-ilarities in the Amino Acid Sequence of Two Pro-teins.
Journal of Molecular Biology, 48(3):443 ?453.Ee-Lee Ng, Beatrice Chin, Alvin W. Yeo, and BaliRanaivo-Malanc?on.
2010.
Identification of Closely-Related Indigenous Languages: An OrthographicApproach.
International Journal of Asian LanguageProcessing, 20(2):43?62.437
