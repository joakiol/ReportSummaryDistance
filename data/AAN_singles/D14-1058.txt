Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523?533,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLearning to Solve Arithmetic Word Problems with Verb CategorizationMohammad Javad Hosseini1, Hannaneh Hajishirzi1, Oren Etzioni2, and Nate Kushman31{hosseini, hannaneh}@washington.edu,2OrenE@allenai.org,3nkushman@csail.mit.edu1University of Washington,2Allen Institute for AI,3Massachusetts Institute of TechnologyAbstractThis paper presents a novel approach tolearning to solve simple arithmetic wordproblems.
Our system, ARIS, analyzeseach of the sentences in the problem state-ment to identify the relevant variables andtheir values.
ARIS then maps this infor-mation into an equation that representsthe problem, and enables its (trivial) so-lution as shown in Figure 1.
The pa-per analyzes the arithmetic-word problems?genre?, identifying seven categories ofverbs used in such problems.
ARIS learnsto categorize verbs with 81.2% accuracy,and is able to solve 77.7% of the problemsin a corpus of standard primary school testquestions.
We report the first learning re-sults on this task without reliance on pre-defined templates and make our data pub-licly available.11 IntroductionDesigning algorithms to automatically solve mathand science problems is a long-standing AI chal-lenge (Feigenbaum and Feldman, 1963).
For NLP,mathematical word problems are particularly at-tractive because the text is concise and relativelystraightforward, while the semantics reduces tosimple equations.Arithmetic word problems begin by describinga partial world state, followed by simple updatesor elaborations and end with a quantitative ques-tion.
For a child, the language understanding partis trivial, but the reasoning may be challenging;for our system, the opposite is true.
ARIS needs to1Our data is available at https://www.cs.washington.edu/nlp/arithmetic.Arithmetic word ProblemLiz had 9 black kittens.
She gave some of her kittens toJoan.
Joan now has 11 kittens.
Liz has 5 kittens left and 3have spots.
How many kittens did Joan get?State Transitions1LizN: 9E: KittenA: BlackLiz gave some of her kittens to Joan.s2LizN: 9-L1E: KittenA: BlackJoanN:  J0+L1E: KittenA: BlackgiveEquation: 9?
x = 5Solution: x = 4 kittensFigure 1: Example problem and solution.make sense of multiple sentences, as shown in Fig-ure 2, without a priori restrictions on the syntax orvocabulary used to describe the problem.
Figure1 shows an example where ARIS is asked to inferhow many kittens Joan received based on facts andconstraints expressed in the text, and representedby the state diagram and corresponding equation.While the equation is trivial, the text could haveinvolved assembling toy aircraft, collecting coins,eating cookies, or just about any activity involvingchanges in the quantities of discrete objects.This paper investigates the task of learning tosolve such problems by mapping the verbs in theproblem text into categories that describe their im-pact on the world state.
While the verbs categoryis crucial (e.g., what happens if ?give?
is replacedby ?receive?
in Figure 1?
), some elements of theproblem are irrelevant.
For instance, the fact thatthree kittens have spots is immaterial to the solu-tion.
Thus, ARIS has to determine what informa-tion is relevant to solving the problem.To abstract from the problem text, ARIS mapsthe text to a state representation which consists of523a set of entities, their containers, attributes, quan-tities, and relations.
A problem text is split intofragments where each fragment corresponds to anobservation or an update of the quantity of an en-tity in one or two containers.
For example in Fig-ure 1, the sentence ?Liz has 5 kittens left and 3have spots?
has two fragments of ?Liz has 5 kit-tens left?
and ?3 have spots?.The verb in each sentence is associated with oneor two containers, and ARIS has to classify eachverb in a sentence into one of seven categoriesthat describe the impact of the verb on the con-tainers (Table 1).
ARIS learns this classifier basedon training data as described in section 4.2.To evaluate ARIS, we compiled a corpus ofabout 400 arithmetic (addition and subtraction)word problems and utilized cross validation toboth train ARIS and evaluate its performanceover this corpus.
We compare its performanceto the template-based learning method developedindependently and concurrently by Kushman etal.
(2014).
We find that our approach is muchmore robust to domain diversity between the train-ing and test sets.Our contributions are three-fold: (a) We presentARIS, a novel, fully automated method that learnsto solve arithmetic word problems; (b) We intro-duce a method to automatically categorize verbsfor sentences from simple, easy-to-obtain train-ing data; our results refine verb senses in Word-Net (Miller, 1995) for arithmetic word problems;(c) We introduce a corpus of arithmetic word prob-lems, and report on a series of experiments show-ing high efficacy in solving addition and subtrac-tion problems based on verb categorization.2 Related WorkUnderstanding semantics of a natural languagetext has been the focus of many researchers in nat-ural language processing (NLP).
Recent work fo-cus on learning to align text with meaning repre-sentations in specific, controlled domains.
A fewmethods (Zettlemoyer and Collins, 2005; Ge andMooney, 2006) use an expensive supervision inthe form of manually annotated formal representa-tions for every sentence in the training data.
Morerecent work (Eisenstein et al., 2009; Kate andMooney, 2007; Goldwasser and Roth, 2011; Poonand Domingos, 2009; Goldwasser et al., 2011;Kushman and Barzilay, 2013) reduce the amountof required supervision in mapping sentences tomeaning representations while taking advantageof special properties of the domains.
Our method,on the other hand, requires small, easy-to-obtaintraining data in the form of verb categories thatare shared among many different problem types.Our work is also closely related to the groundedlanguage acquisition research (Snyder and Barzi-lay, 2007; Branavan et al., 2009; Branavan et al.,2012; Vogel and Jurafsky, 2010; Chen et al., 2010;Hajishirzi et al., 2011; Chambers and Jurafsky,2009; Liang et al., 2009; Bordes et al., 2010)where the goal is to align a text into underlying en-tities and events of an environment.
These meth-ods interact with an environment to obtain super-vision from the real events and entities in the envi-ronment.
Our method, on the other hand, groundsthe problem into world state transitions by learn-ing to predict verb categories in sentences.
In addi-tion, our method combines the representations ofindividual sentences into a coherent whole to formthe equations.
This is in contrast with the previouswork that study each sentence in isolation from theother sentences.Previous work on studying math word and logicproblems uses manually aligned meaning repre-sentations or domain knowledge where the seman-tics for all the words is provided (Lev, 2007; Levet al., 2004).
Most recently, Kushman et al.
(2014)introduced an algorithm that learns to align al-gebra problems to equations through the use oftemplates.
This method applies to broad range ofmath problems, including multiplication, division,and simultaneous equations, while ARIS only han-dles arithmetic problems (addition and subtrac-tion).
However, our empirical results show thatfor the problems it handles, ARIS is much morerobust to diversity in the problem types betweenthe training and test data.3 Arithmetic Problem RepresentationWe address solving arithmetic word problems thatinclude addition and subtraction.
A problem textis split into fragments where each fragment is rep-resented as a transition between two world statesin which the quantities of entities are updated orobserved (Figure 2).
We refer to these fragmentsas sentences.
We represent the world state as a tu-ple ?E,C,R?
consisting of entities E, containersC, and relations R among entities, containers, at-tributes, and quantities.Entities: An entity is a mention in the text corre-524N: W0-13E: treeA: walnutLiz had 9 black kittens.
She gave some of her kittens to Joan.
Joan has now 11 kittens.
Liz has 5 kitten left and 3 has spots.
How many kittens did Joan get?Liz had 9black kittenss0s1LizN: 9E: KittenA: BlackShe gave some of her kittens to Joans2LizN: 9-L1E: KittenA: BlackJoanN:  J0+L1E: KittenA: BlackJoan has now 11 kittensLiz has 5 kitten leftAnd 3 has spotsLizN: 9-L1E: KittenA: BlackJoanN: 11E: KittenA: Blacks3LizN: 5E: KittenA: BlackJoanN: 11E: KittenA: Blacks4LizN: 5E: KittenA: BlackJoanN: 11E: KittenA: BlackunknownN:3E: Kittens5There are 42 walnut trees and 12 orange trees currently in the park.
Park workers cut down 13 walnut trees that were damaged.
How many walnut trees will be in the park when the workers are finished?There are 42 walnut trees and 12 orange trees currently in the park.s0s1ParkN: 42E: treeA: walnutPark workers cut down 13 walnut trees that were damagedN: 12E: treeA: oranges2ParkN: 42-13E: treeA: walnutN: 12E: treeA: orangeWorkersFigure 2: A figure sketching different steps of our method ?
a sequence of states.sponding to an object whose quantity is observedor is changing throughout the problem.
For in-stance, kitten and tree are entities in Fig-ure 2.
In addition, every entity has attributes thatmodify the entity.
For instance, black is an at-tribute of kittens, and walnut is an attributeof tree (more details on attributes in section 4.1).Relations describing attributes are invariant to thestate changes.
For instance kittens stay blackthroughout the problem of Figure 1.Containers: A container is a mention in thetext representing a set of entities.
For instance,Liz, Joan, park, and workers are containersin Figure 2.
Containers usually correspond to theperson possessing entities or a location contain-ing entities.
For example, in the sentence ?Thereare 43 blue marbles in the basket.
John found 32marbles.
?, basket and John are containers ofmarbles.Quantities: Containers include entities with theircorresponding quantities in a particular worldstate.
Quantities can be known numbers (e.g.
9),unknown variables (e.g.
L1), or numerical expres-sions over unknown quantities and numbers (e.g.9?L1).
For instance, in state 2 of Figure 2, the nu-merical expression corresponding to Liz is 9?L1and corresponding to Joan is J0+ L1, where J0is a variable representing the number of kittensthat Joan has started with.Hereinafter, we will refer to a generic entity ase, container as c, number as num, attribute as a.We represent the relation between a container, anentity, and a number in the form of a quantity ex-Category ExampleObservation There were 28 bales of hay in the barn.Positive Joan went to 4 football games this year.Negative John lost 3 of the violet balloons.PositiveTransferMike?s dad borrowed 7 nickels fromMike.NegativeTransferJason placed 131 erasers in the drawer.Construct Karen added 1/4 of a cup of walnuts to abatch of trail mix.Destroy The rabbits ate 4 of Dan?s potatoes.Table 1: Examples for different verb categories in sen-tences.
Entities are underlined; containers are italic, andverbs are bolded.pression N(c,e).
Figure 2 shows the quantityrelations in different world states.State transitions: Sentences depict progressionof the world state (Figure 2) in the form of ob-servations of updates of quantities.
We assumethat every sentence w consists of a verb v, an en-tity e, a quantity num (might be unknown), oneor two containers c1, c2, and attributes a. Thepresence of the second container, c2, will be dic-tated by the category of the verb, as we discussbelow.
Sentences abstract transitions (st?
st+1)between states in the form of an algebraic opera-tion of addition or subtraction.
For every sentence,we model the state transition according to the verbcategory and containers in the sentence.
There arethree verb categories for sentences with one con-tainer: Observation: the quantity is initialized inthe container, Positive: the quantity is increasedin the container, and Negative: the quantity is de-creased in the container.
Moreover, there are fourcategories for sentences with two containers: Pos-525itive transfer: the quantity is transferred from thesecond container to the first one, Negative trans-fer: the quantity is transferred from the first con-tainer to the second one, Construct: the quantityis increased for both containers, and Destroy: thequantity is decreased for both containers.Figure 2 shows how the state transitions aredetermined by the verb categories.
The sen-tence ?Liz has 9 black kittens?
initializes thequantity of kittens in the container Lizto 9.
In addition, the sentence ?She gavesome of her kittens to Joan.?
shows thenegative transfer of L1kittens from Liz toJoan represented as N(Liz,kitten)=9-L1and N(Joan,kitten)=J0+ L1.Given a math word problem, ARIS grounds theworld state into entities (e.g., kitten), contain-ers (e.g., Liz), attributes (e.g., black), and quan-tities (e.g., 9) (Section 4.1).
In addition, ARISlearns state transitions by classifying verb cate-gories in sentences (Section 4.2).
Finally, from theworld state and transitions, it generates an arith-metic equation which can be solved to generate thenumeric answer to the word problem.4 Our MethodIn this section we describe how ARIS maps anarithmetic word problem into an equation (Fig-ure 2).
ARIS consists of three main steps (Fig-ure 3): (1) grounding the problem into entities andcontainers, (2) training a model to classify verbcategories in sentences, and (3) solving the prob-lem by updating the world states with the learnedverb categories and forming equations.4.1 Grounding into Entities and ContainersARIS automatically identifies entities, attributes,containers, and quantities corresponding to everysentence fragment (details in Figure 3 step 1).
Forevery problem, this module returns a sequence ofsentence fragments ?w1, .
.
.
, wT, wx?where everywtconsists of a verb vt, an entity et, its quantitynumt, its attributes at, and up to two containersct1, ct2.
wxcorresponds to the question sentenceinquiring about an unknown entity.
ARIS appliesthe Stanford dependency parser, named entity rec-ognizer and coreference resolution system to theproblem text (de Marneffe et al., 2006; Finkel etal., 2005; Raghunathan et al., 2010).
It uses thepredicted coreference relationships to replace pro-nouns (including possessive pronouns) with theircoreferenent links.
The named entity recognitionoutput is used to identify numbers and people.Entities: Entities are references to some objectwhose quantity is observed or changing through-out the problem.
So to determine the set ofentities, we define h as the set of noun typeswhich have a dependent number (in the depen-dency parse) somewhere in the problem text.
Theset of entities is then defined as all noun phraseswhich are headed by a noun type in h. For in-stance kitten in the first sentence of Figure 1is an entity because it is modified by the number9, while kitten in the second sentence of Fig-ure 1 is an entity because kitten was modifiedby a number in the first sentence.
Every numberin the text is associated with one entity.
Num-bers which are dependents of a noun are associ-ated with its entity.
Bare numbers (not dependenton a noun) are associated with the previous entityin the text.
The entity in the last sentence is identi-fied as the question entity ex.
Finally, ARIS splitsthe problem text into T + 1 sentence fragments?w1, .
.
.
wT, wx?
such that each fragment containsa single entity and it?s containers.
For simplicitywe refer to these fragments as a sentences.Containers: Each entity is associated with oneor two container noun phrases using the algorithmdescribed in in Figure 3 step 1c.
As we saw earlierwith numbers, arithmetic problems often includesentences with missing information.
For examplein Figure 2, the second container in the the sen-tence ?Park workers had to cut down 13 walnuttrees that were damaged.?
is not explicitly men-tioned.
To handle this missing information, weuse the circumscription assumption (McCarthy,1980).
The circumscription assumption formal-izes the commonsense assumption that things areas expected unless otherwise specified.
In this set-ting, we assume that the set of containers are fixedin a problem.
Thus if the container(s) for a givenentity cannot be identified they are set to the con-tainer(s) for the previous entity with the same headword.
For example in Figure 2 we know from theprevious sentence that trees were in the park.Therefore, we assume that the unmentioned con-tainer is the park.Attributes: ARIS selects attributes A as modifiersfor every entity from the dependency parser (de-tails in Figure 3 step 1a).
For example black isan attribute of the entity kitten and is an ad-jective modifier in the parser.
These attributes are5261.
Grounding into entities and containers: for every problem p in dataset (Section 4.1)(a) ?e1, .
.
.
, eT, ex?p?
extract all entities and the question entityi.
Extract all numbers and noun phrases (NP).ii.
h ?
all noun types which appear with a number as a dependant (in the dependency parse tree) somewherein the problem text.iii.
et?
all NPs which are headed by a noun type in h.iv.
numt?
the dependant number of etif one exists.
Bare numbers (not directly dependant on any nounphrase) are associated with the previous entity in the text.
All other numtare set to unknown.v.
ex?
the last identified entity.vi.
at?
adjective and noun modifiers of et.
Update implicit attributes using the previously observed attributes.vii.
vt?
the verb with the shortest path to etin the dependency parse tree.
(b) ?w1, .
.
.
, wT, wx?p?
split the problem text into fragments based on the entities and verbs(c) ?ct1, ct2, .
.
.
, cT1, cT2, cx?p?
the list of containers for each entityi.
ct1?
the subject of wt.If wtcontains There is/are, ct1is the first adverb of place to the verb.ii.
ct2?
An NP that is direct object of the verb.
If not found, ct2is the object of the first adverbial phrase ofthe verb.iii.
Circumscription assumption: When ct1or ct2are not found, they are set to the previous containers.2.
Training for sentence categorization (Section 4.2)(a) instances1, instances2?
?
(b) for every sentence wt?
?w1, .
.
.
, wT, wx?pin the training set:i. featurest?
extract features (similarity based, WordNet based, structural) (Section 4.2.1)ii.
lt1, lt2?
determine labels for containers ct1and ct2based on the verb category of wt.iii.
append ?featurest, lt,1?, ?featurest, lt,2?
to instances1, instances2.
(c) M1,M2?
train two SVMs for instances1, instances23.
Solving: for every problem p in the test set (Section 4.3)(a) Identifying verb categories in sentencesi.
for every sentence wt?
?w1, .
.
.
, wT, wx?p:A. featurest?
extract features (similarity based, WordNet based, structural).B.
lt1, lt2?
classify wtfor both containers ct1and ct2using models M1,M2.
(b) State progression: Form ?s0, .
.
.
, sT?
(Section 4.3.1)i. s0?
null.ii.
for t ?
?1, .
.
.
, T ?
: st?
progress(st?1, wt).A.
if et= exand at= ax:if wtis an observation: Nt(ct1, et) = numt.else: update Nt(ct1, et) and Nt(ct2, et) given verb categories lt1, lt2.B.
copy Nt?1(c, e) to Nt(c, e) for all other (c, e) pairs.
(c) Forming equations and solution (Section 4.3.2)i.
Mark each wtthat matches with wxif:a) ct1matches with cxand verb categories are equal or verbs are similar.b) ct2matches with cxand the verbs are in opposite categories.ii.
x?
the unknown quantity if wxmatches with a sentence introducing an unknown numberiii.
If the question asks about an unknown variable x or a start variable (wxcontains ?begin?
or ?start?
):For some container c, find two states st(quantity expression contains x) and st+1(quantity is a knownnumber).
Then, form an equation for x: Nt(c, ex) = Nt+1(c, ex).iv.
else: form equation as x = Nt(cx, ex).v.
Solve the equation and return the absolute value of x.Figure 3: ARIS: a method for solving arithmetic word problems.used to prune the irrelevant information in pro-gressing world states.Arithmetic problems usually include sentenceswith no attributes for the entities.
For example,the attribute black has not been explicitly men-tioned for the kitten in the second sentence.
Inparticular, ARIS updates an implicit attribute usingthe previously observed attribute.
For example, in?Joan went to 4 football games this year.
She wentto 9 games last year.
?, ARIS assigns football asan attribute of the game in both sentences.4.2 Training for Verb CategoriesThis step involves training a model to identify verbcategories for sentences.
This entails predictingone label (increasing, decreasing) for each (verb,container) pair in the sentence.
Each possible set-ting of these binary labels corresponds to one ofthe seven verb categories discussed earlier.
For ex-ample, if c1is increasing and c2is decreasing thisis a positive transfer verb.Our dataset includes word problems from dif-ferent domains (more details in Section 5.2).
Eachverb in our dataset is labeled with one of the 7 cat-527egories from Table 1.For training, we compile a list of sentences fromall the problems in the dataset and split sentencesinto training and test sets in two settings.
In thefirst setting no instance from the same domainappears in the training and test sets in order tostudy the robustness of our method to new prob-lem types.
In the second setting no verb is re-peated in the training and test sets in order to studyhow well our method predicts categories of unseenverbs.For every sentence wtin the problems, we buildtwo data instances, (wt, c1) and (wt, c2), where c1and c2are containers extracted from the sentence.For every instance in the training data, we assigntraining labels using the verb categories of the sen-tences instead of labeling every sentence individu-ally.
The verb can be increasing or decreasing cor-responding to every container in the sentence.
Forpositive (negative) and construction (destruction)verbs, both instances are labeled positive (nega-tive).
For transfer positive (negative) verbs, thefirst instance is labeled positive (negative) and thesecond instance is labeled negative (positive).
Forobservation verbs, both instances are labeled pos-itive.
We assume that the observation verbs areknown (total of 5 verbs).
Finally, we train SupportVector Machines given the extracted features andtraining labels explained above (Figure 3 step 2).In the following, we describe the features used fortraining.4.2.1 FeaturesThere are three sets of features: similarity based,Wordnet-based, and structural features.
The firsttwo sets of features focus on the verb and the thirdset focuses on the dependency structure of the sen-tence.
All of our features are unlexicalized.
Thisallows ARIS to handle verbs in the test questionswhich are completely different from those seen inthe training data.Similarity-based Features: For every instance(w, c), the feature vector includes similarity be-tween the verb of the sentence w and a list of seedverbs.
The list of seed verbs is automatically se-lected from a set V containing the 2000 most com-mon English verbs using `1regularized feature se-lection technique.
We select a small set of seedverbs to avoid dominating the other feature types(structural and WordNet-based features).The goal is to automatically select verbs fromV that are most discriminative for each of the 7verb categories in Table 1.
We define 7 classifi-cation tasks: ?Is a verb a member of each cate-gory??
Then, we select the three most represen-tative verbs for each category.
To do so, we ran-domly select a set of 65 verbs Vl, from all the verbsin our dataset (118 in total) and manually anno-tate the verb categories.
For every classificationtask, the feature vector X includes the similarityscores (Equation 1) between the verb v and all theverbs in the V .
We train an `1regularized regres-sion model (Park and Hastie, 2007) over the fea-ture vector X to learn each category individually.The number of original (similarity based) featuresin X is relatively large, but `1regularization pro-vides a sparse weight vector.
ARIS then selects thethree most common verbs (without replacement)among the features (verbs) with non-zero weights.This accounts for 21 total seed verbs to be used forthe main classification task.
We find that in prac-tice using this selection technique leads to betterperformance than using either all the verbs in V orusing just the 65 randomly selected verbs.Our method computes the similarity betweentwo verbs v1and v2from the similarity between allthe senses (from WordNet) of these verbs (Equa-tion 1).
We compute the similarity between twosenses using linear similarity (Lin, 1998).
Thesimilarity between two synsets sv1and sv2are pe-nalized according to the order of each sense for thecorresponding verb.
Intuitively, if a synset appearsearlier in the set of synsets of a verb, it is morelikely to be considered as the correct meaning.Therefore, later occurrences of a synset should re-sult in reduced similarity scores.
The similaritybetween two verbs v1and v2is the maximum sim-ilarity between two synsets of the verbs:sim(v1, v2) = maxsv:synsets(v)lin-sim(sv1, sv2)log(p1+ p2)(1)where sv1, sv2are two synsets, p1, p2are the posi-tion of each synset match, and lin-sim is the linearsimilarity.
Our experiments show better perfor-mance using linear similarity compared to othercommon similarity metrics (e.g., WordNet pathsimilarity and Resnik similarity (Resnik, 1995)).WordNet-based Features: We use WordNetverb categories in the feature vector.
For eachpart of speech in WordNet, the synsets are or-ganized into different categories.
There are15 categories for verbs.
Some examples in-528clude ?verb.communication?, ?verb.possession?,and ?verb.creation?.
In addition, WordNet in-cludes the frequency measure fcsvindicating howoften the sense sv has appeared in a reference cor-pus.
For each category i, we define the feature fias the ratio of the frequency of the sense svioverthe total frequency of the verb i.e., fi= fcsvi/fcv.Structural Features: For structural features, weuse the dependency relations between the verb andthe sentence elements since they can be a goodproxy of the sentence structure.
ARIS uses a bi-nary vector including 35 dependency relations be-tween the verb and other elements.
For example,in the sentence ?Joan picked 2 apples from the ap-ple tree?, the dependency between (?picked?
and?tree?)
and (?picked?
and ?apples?)
are depicted as?prep-from?
and ?dobj?
relations in the dependencyparser, respectively.
In addition, we include thelength of the path in the dependency parse fromthe entity to the verb.4.3 Solving the ProblemSo far, ARIS grounds every problem into entities,containers, and attributes, and learns verb cate-gories in sentences.
Solving the problem consistsof two main steps: (1) progressing states based onverb categories in sentences and (2) forming theequation.4.3.1 State Progression with Verb CategoriesThis step (Figure 3 step 3b) involves formingstates ?s1, .
.
.
, sT?
by updating quantities in everycontainer using learned verb categories (Figure 3step 3a).
ARIS initializes s0to an empty state.
Itthen iteratively updates the state stby progressingthe state st?1given the sentence wtwith the verbv, entity e, number num, and containers c1and c2.For a given sentence t, ARIS attempts to matchetand ctto entities and categories in st?1.
Anentity/category is matched if has the same headword and same set of attributes as an existing en-tity/category.
If an entity or category cannot bematching to one in st?1, then a new one is createdin st.The progress subroutine prunes the irrelevantsentences by checking if the entity e and its at-tributes a agree with the question entity exand itsattributes axin the question.
For example bothgame entities agree with the question entity in theproblem ?Joan went to 4 football games this year.She went to 9 games last year.
How many footballgames did Joan go??.
The first entity has an ex-plicit football attribute, and the second entityhas been assigned the same attribute (Section 4.1).Even if the question asks about games withoutmentioning football, the two sentences willmatch the question.
Note that the second sentencewould have not been matched if there was an ex-plicit mention of the ?basketball game?
in the sec-ond sentence.For the matched entities, ARIS initializes or up-dates the values of the containers c1, c2in the statest.
ARIS uses the learned verb categories in sen-tences (Section 4.2) to update the values of con-tainers.
For an observation sentence wt, the valueof c1in the state stis assigned to the observedquantity num.
For other sentence types, if thecontainer c does not match to a container the pre-vious state, its value is initialized with a start vari-able C0.
For example, the container Joan is ini-tialized with J0at the state s1(Figure 2).
Other-wise, the values of c1and c2are updated accordingto the verb category in the sentence.
For instance,if the verb category in the sentence is a positivetransfer then Nt(c1, e) = Nt?1(c1, e)?
num andNt(c2, e) = Nt?1(c2, e) + num where Nt(c, e)represents the quantity of e in the container c atstate st(Figure 2).4.3.2 Forming Equations and SolutionThe question entity excan match either to an en-tity in the final state, or to some unknown gener-ated during the state progression.
Concretely, thequestion sentence wxasks about the quantity x ofthe entity exin a container cxat a particular statesuor a transition after the sentence wu(Figure 3step 3c).To determine if exmatches to an unknown vari-able, we define a matching subroutine betweenthe question sentence wxand every sentence wtto check entities, containers, and verbs (Figure 3step 3(c)i).
We consider two cases.
1) Whenwxcontains the words ?begin?, or ?start?, the un-known variable is about the initial value of an en-tity, and it is set to the start variable of the con-tainer cx(Figure 3 step 3(c)iii).
For example, in?Bob had balloons.
He gave 9 to his friends.
Henow has 4 balloons.
How many balloons did hehave to start with?
?, the unknown variable is set tothe start variable B0.
2) When the question verbis not one of the defined set of observation verbs,ARIS attempts to match exwith an unknown in-troduced by one of the state transitions (Figure 3529step 3(c)iii).
For example, the second sentencein Figure 1 introduces an unknown variable overkittens.
The matching subroutine matches thisentity with the question entity since the questioncontainer, i.e.
Joan, matches with the secondcontainer and verb categories are complementary.In order to solve for the unknown variable x,ARIS searches through consecutive states standst+1, where in st, the quantity of exfor a containerc is an expression over x, and in st+1, the quan-tity is a known number for a container matchedto c. It then forms an equation by comparing thequantities for containers matched between the twostates.
In the previous example, the equation willbe B0?
9 = 4 by comparing states s2and s3,where the numerical expression over balloonsis B0?9 in the state s2, and the quantity is a knownnumber in the state s3.When neither of the two above cases apply,ARIS matches exto an entity in the final state,sTand returns its quantity, (Figure 3 step 3(c)iv).In the football example of the previous sec-tion, the equation will be x = Nt(cx, ex), whereNt(cx, ex) is the quantity in the final state.Finally, the equation will be solved for the un-known variable x and the absolute value of the un-known variable is returned.5 ExperimentsTo experimentally evaluate our method we builda dataset of arithmetic word problems along withtheir correct solutions.
We test our method on theaccuracy of solving arithmetic word problems andidentifying verb categories in sentences.5.1 Experimental SetupDatasets: We compiled three diverse datasetsMA1, MA2, IXL (Table 2) of Arithmetic wordproblems on addition and subtraction for third,fourth, and fifth graders.
These datasets have sim-ilar problem types, but have different characteris-tics.
Problem types include combinations of ad-ditions, subtractions, one unknown equations, andU.S.
money word problems.
Problems in MA2 in-clude more irrelevant information compared to theother two datasets, and IXL includes more infor-mation gaps.
In total, they include 395 problems,13,632 words, 118 verbs, and 1,483 sentences.Tasks and Baselines: We evaluate ARIS on twotasks: 1) solving arithmetic word problems in thethree datasets and 2) classifying verb categories inSource #Tests Avg.# SentencesMA1 math-aids.com 134 3.5IXL ixl.com 140 3.36MA2 math-aids.com 121 4.48Table 2: Properties of the datasets.MA1 IXL MA2 Total3-fold Cross validationARIS 83.6 75.0 74.4 77.7ARIS283.9 75.4+69.8+76.5+KAZB 89.6 51.1 51.2 64.0Majority 45.5 71.4 23.7 48.9Gold sentence categorizationGold ARIS 94.0 77.1 81.0 84.0Table 3: Accuracy of solving arithmetic word problems inthree datasets MA1, IXL, and MA2.
This table comparesour method, ARIS, ARIS2with the state-of-the-art KAZB.
Allmethods are trained on two (out of three) datasets and testedon the other one.
ARIS2is trained when no verb is repeatedin the training and test sets.
Gold ARIS uses gold verb cat-egories.
The improvement of ARIS (boldfaced) and ARIS2(denoted by+) are significant over KAZB and the majoritybaseline with p < 0.05.sentences.
We use the percentage of correct an-swers to the problems as the evaluation metric forthe first task and accuracy as the evaluation metricfor the second task.
We use Weka?s SVM (Wit-ten et al., 1999) with default parameters for clas-sification which is trained with verb categories insentences (as described in Section 4.2).For the first task, we compare ARIS withKAZB (Kushman et al., 2014), majority baseline,ARIS2, and Gold ARIS.
KAZB requires trainingdata in the form of equation systems and numeri-cal answers to the problems.
The majority base-line classifies every instance as increasing.
InARIS2(a variant of ARIS) the system is trained ina way that no verb is repeated in the training andtest sets.
Gold ARIS uses the ground-truth sen-tence categories instead of predicted ones.
For thesecond task, we compare ARIS with a baseline thatuses WordNet verb senses.5.2 ResultsWe evaluate ARIS in solving arithmetic wordproblems in the three datasets and then evaluate itsability in classifying verb categories in sentences.5.2.1 Solving Arithmetic ProblemsTable 3 shows the accuracy of ARIS in solv-ing problems in each dataset (when trained onthe other two datasets).Table 3 shows that ARIS530significantly outperforms KAZB and the major-ity baseline.
As expected, ARIS shows a largergain on the two more complex datasets MA2 andIXL; our method shows promising results in deal-ing with irrelevant information (dataset MA2) andinformation gaps (dataset IXL).
This is becauseARIS learns to classify verb categories in sen-tences and does not require observing similar pat-terns/templates in the training data.
Therefore,ARIS is more robust to differences between thetraining and test datasets and can generalize acrossdifferent dataset types.
As discussed in the ex-perimental setup, the datasets have mathematicallysimilar problems, but differ in the natural languageproperties such as in the sentence length and irrel-evant information (Table 2).Table 3 also shows that the sentence categoriza-tion is performed with high accuracy even if theproblem types and also the verbs are different.
Inparticular, there are a total of 118 verbs amongwhich 64 verbs belong to MA datasets and 54 arenew to IXL.
To further study this, we train ourmethod ARIS2in which no verb can be repeatedin the training and test sets.
ARIS2still signifi-cantly outperforms KAZB.
In addition, we observeonly a slight change in accuracy between ARISand ARIS2.To further understand our method, we study theeffect of verb categorization in sentences in solv-ing problems.
Table 3 shows the results of GoldARIS in solving arithmetic word problems withgold sentence categorizations.
In addition, com-paring ARIS with Gold ARIS suggests that ourmethod is able to reliably identify verb categoriesin sentences.We also perform an experiment where we poolall of the problems in the three datasets andrandomly choose 3 folds for the data (insteadof putting each original dataset into it?s ownfold).
We compare our method with KAZBinthis scenario.
In this setting, our method?s accu-racy is 79.5% while KAZB?s accuracy is 81.8%.As expected, our method?s performance has notchanged significantly from the previous setting,while KAZB?s performance significantly improvesbecause of the reduced diversity between the train-ing and test sets in this scenario.5.2.2 Sentence CategorizationTable 4 compares accuracy scores of sentencecategorization for our method with different fea-tures, a baseline that uses WordNet verb senses,and the majority baseline that assigns every (verb,container) pair as increasing.
Similar to ARIS2,we randomly split verbs into three equal foldsand assign the corresponding sentences to eachfold.
No verb is shared between training and testsets.
We then directly evaluate the accuracy ofthe SVM?s verb categorization (explained in Sec-tion 4.2).
This table shows that ARIS performswell in classifying sentence categories even withnew verbs in the test set.
This suggests that ourmethod can generalize well to predict verb cate-gories for unseen verbs.Table 4 also details the performance of fourvariants of our method that ablate various featuresof ARIS.
The table shows that similarity, contex-tual, and WordNet features are all important tothe performance of ARIS in verb categorization,whereas the WordNet features are less importantfor solving the problems.
In addition, it shows thatsimilarity features play more important roles.
Wealso performed another experiment to study the ef-fect of the proposed feature selection method forsimilarity-based features.
The accuracy of ARISin classifying sentence categories is 69.7% whenwe use all the verbs in V in the similarity featurevector.
This shows that our feature selection algo-rithm for selecting seed verbs is important towardscategorizing verbs.Finally, Table 4 shows that our method signif-icantly outperforms the baseline that only usesWordNet verb sense.
An interesting observationis that the majority baseline in fact outperformsWordNet verb senses in verb categorization, butis significantly worse in solving arithmetic wordproblems.
In addition, we evaluate the accuracyof predicting only verb categories by assigning theverb label according to the majority of its labelsin the sentence categories.
The accuracy of verbcategories is 78.2% confirming that ARIS is ableto successfully categorize verbs.5.2.3 Error AnalysisWe analyzed all 63 errors of Gold ARIS andpresent our findings in Table 5.
There are five ma-jor classes of errors.
In the first category, some in-formation is not mentioned explicitly and shouldbe entailed.
For example, ?washing cars?
is thesource of ?making money?.
Despite the improve-ments that come from ARIS, a large portion of theerrors can still be attributed to irrelevant informa-tion.
For example, ?short?
is not a ?toy?.
The thirdcategory refers to errors that require knowledge531Categorization SolutionARIS 81.2+76.5+No similarity features 68.8 65.4No WordNet features 75.3 78.0+No structural features 75.5 72.4+Baseline (WordNet) 67.8 68.4Majority Baseline 73.4 48.9Table 4: Ablation study and baseline comparisons: this ta-ble reports the accuracy of verb categorization in sentencesand solutions for ARIS with ablating features.
It also pro-vides comparisons to WordNet and majority baselines.
Theimprovement of ARIS (boldfaced) and ablations denoted by+are statistically significant over the baselines (with p < 0.05)for both tasks.Error type ExampleEntailment,ImplicitAction (26%)Last week Tom had $74.
He washed carsover the weekend and now has $86.
Howmuch money did he make washing cars?IrrelevantInformation(19%)Tom bought a skateboard for $9.46, andspent $9.56 on marbles.
Tom also spent$14.50 on shorts.
In total, how much didTom spend on toys?Set Comple-tion (13%)Sara?s school played 12 games this year.They won 4 games.
How many games didthey lose?ParsingIssues (21%)Sally had 27 Pokemon cards.
Dan gaveher 41 new Pokemon cards.
How manyPokemon cards does Sally have now?Others (21%) In March it rained 0.81 inches.
It rained0.35 inches less in April than in March.How much did it rain in April?Table 5: Examples of different error categories and relativefrequencies.
The cause of error is bolded.about set completions.
For example, the ?played?games can be split into ?win?
and ?lost?
games.Finally, parsing and coreference mistakes are an-other source of errors for ARIS.6 Discussions and ConclusionIn this paper we introduce ARIS, a method forsolving arithmetic word problems.
ARIS learnsto predict verb categories in sentences using syn-tactic and (shallow) semantic features from small,easy-to-obtain training data.
ARIS grounds theworld state into entities, sets, quantities, attributes,and their relations and takes advantage of the cir-cumscription assumption and successfully fills inthe information gaps.
Finally, ARIS makes useof attributes and discards irrelevant information inthe problems.
Together these provide a new rep-resentation and a learning algorithm for solvingarithmetic word problems.This paper is one step toward building a sys-tem that can solve any math and logic wordproblem.
Our empirical evaluations show thatour method outperforms a template-based learn-ing method (developed recently by Kushman et al.
(2014)) on solving addition and subtraction prob-lems with diversity between the training and testsets.
In particular, our method generalizes bet-ter to data from different domains because ARISonly relies on learning verb categories which al-leviates the need for equation templates for arith-metic problems.
In this paper, we have focusedon addition and subtraction problems.
However,KAZB can deal with more general types of prob-lems such as multiplication, division, and simulta-neous equations.We have observed a complementary behaviorbetween our method and that of Kushman et al.This suggests a hybrid approach that can bene-fit from the strengths of both methods while be-ing applicable to more general problems while ro-bust to the errors specific to each.
In addition, weplan to focus on incrementally collecting domainknowledge to deal with missing information gaps.Another possible direction is to improve parsingand coreference resolution.AcknowledgmentsThe research was supported by the Allen Institutefor AI, and grants from the NSF (IIS-1352249)and UW-RRF (65-2775).
We thank Ben Hixonand the anonymous reviewers for helpful com-ments and the feedback on the work.ReferencesAntoine Bordes, Nicolas Usunier, and Jason Weston.
2010.Label ranking under ambiguous supervision for learningsemantic correspondences.
In Proc.
International Confer-ence on Machine Learning (ICML).SRK Branavan, Harr Chen, Luke S. Zettlemoyer, and ReginaBarzilay.
2009.
Reinforcement learning for mapping in-structions to actions.
In Proc.
of the Annual Meeting of theAssociation for Computational Linguistics and the Inter-national Joint Conference on Natural Language Process-ing of the AFNLP (ACL-AFNLP).SRK Branavan, Nate Kushman, Tao Lei, and Regina Barzi-lay.
2012.
Learning high-level planning from text.
InProc.
of the Annual Meeting of the Association for Com-putational Linguistics (ACL).Nathanael Chambers and Dan Jurafsky.
2009.
Unsupervisedlearning of narrative schemas and their participants.
InProc.
of the Annual Meeting of the Association for Com-putational Linguistics and the International Joint Con-ference on Natural Language Processing of the AFNLP(ACL-AFNLP).532David Chen, Joohyun Kim, and Raymond Mooney.
2010.Training a multilingual sportscaster: Using perceptualcontext to learn language.
Journal of Artificial Intelli-gence Research, 37.Marie-Catherine de Marneffe, Bill MacCartney, and Christo-pher D. Manning.
2006.
Generating typed dependencyparses from phrase structure parses.
In Proc.
LanguageResources and Evaluation Conference (LREC).Jacob Eisenstein, James Clarke, Dan Goldwasser, and DanRoth.
2009.
Reading to learn: Constructing featuresfrom semantic abstracts.
In Proc.
Conference on Empiri-cal Methods in Natural Language Processing (EMNLP).Edward A. Feigenbaum and Julian Feldman, editors.
1963.Computers and Thought.
McGraw Hill, New York.Jenny Rose Finkel, Trond Grenager, and Christopher Man-ning.
2005.
Incorporating non-local information intoinformation extraction systems by gibbs sampling.
InProc.
of the Annual Meeting of the Association for Com-putational Linguistics (ACL).Ruifang Ge and Raymond J. Mooney.
2006.
Discriminativereranking for semantic parsing.
In Proc.
of the AnnualMeeting of the Association for Computational Linguistics(ACL).Dan Goldwasser and Dan Roth.
2011.
Learning from naturalinstructions.
In Proceedings of International Joint Con-ference on Artificial Intelligence (IJCAI).Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth.2011.
Confidence driven unsupervised semantic parsing.In Proc.
of the Annual Meeting of the Association for Com-putational Linguistics (ACL).Hannaneh Hajishirzi, Julia Hockenmaier, Erik T. Mueller,and Eyal Amir.
2011.
Reasoning about robocup soccernarratives.
In Proc.
Conference on Uncertainty in Artifi-cial Intelligence (UAI).Rohit J. Kate and Raymond J. Mooney.
2007.
Learn-ing language semantics from ambiguous supervision.
InProc.
Conference of the Association for the Advancementof Artificial Intelligence (AAAI).Nate Kushman and Regina Barzilay.
2013.
Using seman-tic unification to generate regular expressions from natu-ral language.
In Proceeding of the Annual Meeting of theNorth American Chapter of the Association for Computa-tional Linguistics.Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and ReginaBarzilay.
2014.
Learning to automatically solve algebraword problems.
In Proc.
of the Annual Meeting of theAssociation for Computational Linguistics (ACL).Iddo Lev, Bill MacCartney, Christopher D. Manning, , andRoger Levy.
2004.
Solving logic puzzles: From robustprocessing to precise semantics.
In Workshop on TextMeaning and Interpretation at Association for Computa-tional Linguistics (ACL).Iddo Lev.
2007.
Packed Computation of Exact Meaning Rep-resentations.
Ph.D. thesis, CS, Stanford University.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.
Learn-ing semantic correspondences with less supervision.
InProc.
of the Annual Meeting of the Association for Com-putational Linguistics and the International Joint Con-ference on Natural Language Processing of the AFNLP(ACL-AFNLP).Dekang Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proc.
International Conference on MachineLearning (ICML).John McCarthy.
1980.
Circumscription?a form of non-monotonic reasoning.
Artificial Intelligence, 13.George A Miller.
1995.
Wordnet: a lexical database for en-glish.
Communications of the ACM, 38.Mee Young Park and Trevor Hastie.
2007.
L1-regularizationpath algorithm for generalized linear models.
Journal ofthe Royal Statistical Society: Series B (Statistical Method-ology), 69.Hoifung Poon and Pedro Domingos.
2009.
Unsupervised se-mantic parsing.
In Proc.
Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangara-jan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky,and Christopher Manning.
2010.
A multi-pass sieve forcoreference resolution.
In Proc.
Conference on EmpiricalMethods in Natural Language Processing (EMNLP).Philip Resnik.
1995.
Using information content to evaluatesemantic similarity in a taxonomy.
In International jointconference on Artificial intelligence (IJCAI).Benjamin Snyder and Regina Barzilay.
2007.
Database-textalignment via structured multilabel classification.
In Pro-ceedings of International Joint Conference on ArtificialIntelligence (IJCAI).Adam Vogel and Daniel Jurafsky.
2010.
Learning to follownavigational directions.
In Proc.
of the Annual Meeting ofthe Association for Computational Linguistics (ACL).Ian H Witten, Eibe Frank, Leonard E Trigg, Mark A Hall, Ge-offrey Holmes, and Sally Jo Cunningham.
1999.
Weka:Practical machine learning tools and techniques with javaimplementations.Luke S. Zettlemoyer and Michael Collins.
2005.
Learningto map sentences to logical form: Structured classificationwith probabilistic categorial grammars.
In Proc.
Confer-ence on Uncertainty in Artificial Intelligence (UAI).533
