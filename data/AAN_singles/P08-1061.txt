Proceedings of ACL-08: HLT, pages 532?540,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSemi-supervised Convex Training for Dependency ParsingQin Iris WangDepartment of Computing ScienceUniversity of AlbertaEdmonton, AB, Canada, T6G 2E8wqin@cs.ualberta.caDale SchuurmansDepartment of Computing ScienceUniversity of AlbertaEdmonton, AB, Canada, T6G 2E8dale@cs.ualberta.caDekang LinGoogle Inc.1600 Amphitheatre ParkwayMountain View, CA, USA, 94043lindek@google.comAbstractWe present a novel semi-supervised trainingalgorithm for learning dependency parsers.By combining a supervised large margin losswith an unsupervised least squares loss, a dis-criminative, convex, semi-supervised learningalgorithm can be obtained that is applicableto large-scale problems.
To demonstrate thebenefits of this approach, we apply the tech-nique to learning dependency parsers fromcombined labeled and unlabeled corpora.
Us-ing a stochastic gradient descent algorithm, aparsing model can be efficiently learned fromsemi-supervised data that significantly outper-forms corresponding supervised methods.1 IntroductionSupervised learning algorithms still represent thestate of the art approach for inferring dependencyparsers from data (McDonald et al, 2005a; McDon-ald and Pereira, 2006; Wang et al, 2007).
How-ever, a key drawback of supervised training algo-rithms is their dependence on labeled data, whichis usually very difficult to obtain.
Perceiving thelimitation of supervised learning?in particular, theheavy dependence on annotated corpora?many re-searchers have investigated semi-supervised learn-ing techniques that can take both labeled and unla-beled training data as input.
Following the commontheme of ?more data is better data?
we also use botha limited labeled corpora and a plentiful unlabeleddata resource.
Our goal is to obtain better perfor-mance than a purely supervised approach withoutunreasonable computational effort.
Unfortunately,although significant recent progress has been madein the area of semi-supervised learning, the perfor-mance of semi-supervised learning algorithms stillfall far short of expectations, particularly in chal-lenging real-world tasks such as natural languageparsing or machine translation.A large number of distinct approaches to semi-supervised training algorithms have been investi-gated in the literature (Bennett and Demiriz, 1998;Zhu et al, 2003; Altun et al, 2005; Mann andMcCallum, 2007).
Among the most prominent ap-proaches are self-training, generative models, semi-supervised support vector machines (S3VM), graph-based algorithms and multi-view algorithms (Zhu,2005).Self-training is a commonly used techniquefor semi-supervised learning that has been ap-532plied to several natural language processing tasks(Yarowsky, 1995; Charniak, 1997; Steedman et al,2003).
The basic idea is to bootstrap a supervisedlearning algorithm by alternating between inferringthe missing label information and retraining.
Re-cently, McClosky et al (2006a) successfully appliedself-training to parsing by exploiting available un-labeled data, and obtained remarkable results whenthe same technique was applied to parser adaptation(McClosky et al, 2006b).
More recently, Haffariand Sarkar (2007) have extended the work of Abney(2004) and given a better mathematical understand-ing of self-training algorithms.
They also show con-nections between these algorithms and other relatedmachine learning algorithms.Another approach, generative probabilistic mod-els, are a well-studied framework that can be ex-tremely effective.
However, generative models usethe EM algorithm for parameter estimation in thepresence of missing labels, which is notoriouslyprone to getting stuck in poor local optima.
More-over, EM optimizes a marginal likelihood score thatis not discriminative.
Consequently, most previouswork that has attempted semi-supervised or unsu-pervised approaches to parsing have not producedresults beyond the state of the art supervised results(Klein and Manning, 2002; Klein and Manning,2004).
Subsequently, alternative estimation strate-gies for unsupervised learning have been proposed,such as Contrastive Estimation (CE) by Smith andEisner (2005).
Contrastive Estimation is a general-ization of EM, by defining a notion of learner guid-ance.
It makes use of a set of examples (its neighbor-hood) that are similar in some way to an observedexample, requiring the learner to move probabilitymass to a given example, taking only from the ex-ample?s neighborhood.
Nevertheless, CE still suf-fers from shortcomings, including local minima.In recent years, SVMs have demonstrated stateof the art results in many supervised learning tasks.As a result, many researchers have put effort ondeveloping algorithms for semi-supervised SVMs(S3VMs) (Bennett and Demiriz, 1998; Altun etal., 2005).
However, the standard objective of anS3VM is non-convex on the unlabeled data, thusrequiring sophisticated global optimization heuris-tics to obtain reasonable solutions.
A number ofresearchers have proposed several efficient approx-imation algorithms for S3VMs (Bennett and Demi-riz, 1998; Chapelle and Zien, 2005; Xu and Schu-urmans, 2005).
For example, Chapelle and Zien(2005) propose an algorithm that smoothes the ob-jective with a Gaussian function, and then performsa gradient descent search in the primal space toachieve a local solution.
An alternative approach isproposed by Xu and Schuurmans (2005) who formu-late a semi-definite programming (SDP) approach.In particular, they present an algorithm for multi-class unsupervised and semi-supervised SVM learn-ing, which relaxes the original non-convex objectiveinto a close convex approximation, thereby allowinga global solution to be obtained.
However, the com-putational cost of SDP is still quite expensive.Instead of devising various techniques for cop-ing with non-convex loss functions, we approach theproblem from a different perspective.
We simply re-place the non-convex loss on unlabeled data with analternative loss that is jointly convex with respectto both the model parameters and (the encoding of)the self-trained prediction targets.
More specifically,for the loss on the unlabeled data part, we substi-tute the original unsupervised structured SVM losswith a least squares loss, but keep constraints onthe inferred prediction targets, which avoids trivial-ization.
Although using a least squares loss func-tion for classification appears misguided, there isa precedent for just this approach in the early pat-tern recognition literature (Duda et al, 2000).
Thisloss function has the advantage that the entire train-ing objective on both the labeled and unlabeled datanow becomes convex, since it consists of a convexstructured large margin loss on labeled data and aconvex least squares loss on unlabeled data.
Aswe will demonstrate below, this approach admits anefficient training procedure that can find a globalminimum, and, perhaps surprisingly, can systemat-ically improve the accuracy of supervised trainingapproaches for learning dependency parsers.Thus, in this paper, we focus on semi-supervisedlanguage learning, where we can make use of bothlabeled and unlabeled data.
In particular, we in-vestigate a semi-supervised approach for structuredlarge margin training, where the objective is a com-bination of two convex functions, the structuredlarge margin loss on labeled data and the leastsquares loss on unlabeled data.
We apply the result-533fundsInvestorscontinuetopourcashintomoneyFigure 1: A dependency treeing semi-supervised convex objective to dependencyparsing, and obtain significant improvement overthe corresponding supervised structured SVM.
Notethat our approach is different from the self-trainingtechnique proposed in (McClosky et al, 2006a),although both methods belong to semi-supervisedtraining category.In the remainder of this paper, we first reviewthe supervised structured large margin training tech-nique.
Then we introduce the standard semi-supervised structured large margin objective, whichis non-convex and difficult to optimize.
Next wepresent a new semi-supervised training algorithm forstructured SVMs which is convex optimization.
Fi-nally, we apply this algorithm to dependency pars-ing and show improved dependency parsing accu-racy for both Chinese and English.2 Dependency Parsing ModelGiven a sentence X = (x1, ..., xn) (xi denoteseach word in the sentence), we are interested incomputing a directed dependency tree, Y , over X.As shown in Figure 1, in a dependency structure,the basic units of a sentence are the syntactic re-lationships (aka.
head-child or governor-dependentor regent-subordinate relations) between two indi-vidual words, where the relationships are expressedby drawing links connecting individual words (Man-ning and Schutze, 1999).
The direction of each linkpoints from a head word to a child word, and eachword has one and only one head, except for the headof the sentence.
Thus a dependency structure is ac-tually a rooted, directed tree.
We assume that a di-rected dependency tree Y consists of ordered pairs(xi ?
xj) of words in X such that each word ap-pears in at least one pair and each word has in-degreeat most one.
Dependency trees are assumed to beprojective here, which means that if there is an arc(xi ?
xj), then xi is an ancestor of all the wordsbetween xi and xj .1 Let ?
(X) denote the set of allthe directed, projective trees that span on X. Theparser?s goal is then to find the most preferred parse;that is, a projective tree, Y ?
?
(X), that obtainsthe highest ?score?.
In particular, one would assumethat the score of a complete spanning tree Y for agiven sentence, whether probabilistically motivatedor not, can be decomposed as a sum of local scoresfor each link (a word pair) (Eisner, 1996; Eisner andSatta, 1999; McDonald et al, 2005a).
Given thisassumption, the parsing problem reduces to findY ?
= arg maxY ??
(X)score(Y |X) (1)= arg maxY ??(X)?
(xi?xj)?Yscore(xi ?
xj)where the score(xi ?
xj) can depend on any mea-surable property of xi and xj within the sentence X.This formulation is sufficiently general to capturemost dependency parsing models, including proba-bilistic dependency models (Eisner, 1996; Wang etal., 2005) as well as non-probabilistic models (Mc-Donald et al, 2005a).For standard scoring functions, particularly thoseused in non-generative models, we further assumethat the score of each link in (1) can be decomposedinto a weighted linear combination of featuresscore(xi ?
xj) = ?
?
f(xi ?
xj) (2)where f(xi ?
xj) is a feature vector for the link(xi ?
xj), and ?
are the weight parameters to beestimated during training.3 Supervised Structured Large MarginTrainingSupervised structured large margin training ap-proaches have been applied to parsing and producepromising results (Taskar et al, 2004; McDonald etal., 2005a; Wang et al, 2006).
In particular, struc-tured large margin training can be expressed as min-imizing a regularized loss (Hastie et al, 2004), asshown below:1We assume all the dependency trees are projective in ourwork (just as some other researchers do), although in the realword, most languages are non-projective.534min?
?2 ???
+ (3)?imaxLi,k(?
(Li,k, Yi)?
diff(?, Yi, Li,k))where Yi is the target tree for sentence Xi; Li,kranges over all possible alternative k trees in ?
(Xi);diff(?, Yi, Li,k) = score(?, Yi) ?
score(?, Li,k);score(?, Yi) =?
(xm?xn)?Yi ?
?
f(xm ?
xn), asshown in Section 2; and ?
(Li,k, Yi) is a measure ofdistance between the two trees Li,k and Yi.
This isan application of the structured large margin trainingapproach first proposed in (Taskar et al, 2003) and(Tsochantaridis et al, 2004).Using the techniques of Hastie et al (2004) onecan show that minimizing the objective (3) is equiv-alent to solving the quadratic programmin?,?
?2 ???
+ e??
subject to?i,k ?
?
(Li,k, Yi)?
diff(?, Yi, Li,k)?i,k ?
0for all i, Li,k ?
?
(Xi) (4)where e denotes the vector of all 1?s and ?
representsslack variables.
This approach corresponds to thetraining problem posed in (McDonald et al, 2005a)and has yielded the best published results for En-glish dependency parsing.To compare with the new semi-supervised ap-proach we will present in Section 5 below, we re-implemented the supervised structured large margintraining approach in the experiments in Section 7.More specifically, we solve the following quadraticprogram, which is based on Equation (3)min?
?2 ???
+?imaxLk?m=1k?n=1?
(Li,m,n, Yi,m,n)?
diff(?, Yi,m,n, Li,m,n) (5)where diff(?, Yi,m,n, Li,m,n) = score(?, Yi,m,n) ?score(?, Li,m,n) and k is the sentence length.
Werepresent a dependency tree as a k ?
k adjacencymatrix.
In the adjacency matrix, the value of Yi,m,nis 1 if the word m is the head of the word n, 0 oth-erwise.
Since both the distance function ?
(Li, Yi)and the score function decompose over links, solv-ing (5) is equivalent to solve the original constrainedquadratic program shown in (4).4 Semi-supervised Structured LargeMargin ObjectiveThe objective of standard semi-supervised struc-tured SVM is a combination of structured large mar-gin losses on both labeled and unlabeled data.
It hasthe following form:min?
?2 ???
+N?i=1structured loss (?,Xi, Yi)+ minYjU?j=1structured loss (?,Xj , Yj) (6)wherestructured loss (?,Xi, Yi)= maxLk?m=1k?n=1?
(Li,m,n, Yi,m,n) (7)?diff(?, Yi,m,n, Li,m,n)N and U are the number of labeled and unlabeledtraining sentences respectively, and Yj ranges overguessed targets on the unsupervised data.In the second term of the above objective shown in(6), both ?
and Yj are variables.
The resulting lossfunction has a hat shape (usually called hat-loss),which is non-convex.
Therefore the objective as awhole is non-convex, making the search for globaloptimal difficult.
Note that the root of the optimiza-tion difficulty for S3VMs is the non-convex propertyof the second term in the objective function.
We willpropose a novel approach which can deal with thisproblem.
We introduce an efficient approximation?least squares loss?for the structured large marginloss on unlabeled data below.5 Semi-supervised Convex Training forStructured SVMAlthough semi-supervised structured SVM learninghas been an active research area, semi-supervisedstructured SVMs have not been used in many realapplications to date.
The main reason is that mostavailable semi-supervised large margin learning ap-proaches are non-convex or computationally expen-sive (e.g.
(Xu and Schuurmans, 2005)).
These tech-niques are difficult to implement and extremely hardto scale up.
We present a semi-supervised algorithm535for structured large margin training, whose objectiveis a combination of two convex terms: the super-vised structured large margin loss on labeled dataand the cheap least squares loss on unlabeled data.The combined objective is still convex, easy to opti-mize and much cheaper to implement.5.1 Least Squares Convex ObjectiveBefore we introduce the new algorithm, we first in-troduce a convex loss which we apply it to unlabeledtraining data for the semi-supervised structured largemargin objective which we will introduce in Sec-tion 5.2 below.
More specifically, we use a struc-tured least squares loss to approximate the struc-tured large margin loss on unlabeled data.
The cor-responding objective is:min?,Yj?2 ???
+ (8)?2U?j=1k?m=1k?n=1(?
?f(Xj,m ?
Xj,n)?
Yj,m,n)2subject to constraints on Y (explained below).The idea behind this objective is that for each pos-sible link (Xj,m ?
Xj,n), we intend to minimize thedifference between the link and the correspondingestimated link based on the learned weight vector.Since this is conducted on unlabeled data, we needto estimate both ?
and Yj to solve the optimizationproblem.
As mentioned in Section 3, a dependencytree Yj is represented as an adjacency matrix.
Thuswe need to enforce some constraints in the adjacencymatrix to make sure that each Yj satisfies the depen-dency tree constraints.
These constraints are criticalbecause they prevent (8) from having a trivial solu-tion in Y.
More concretely, suppose we use rows todenote heads and columns to denote children.
Thenwe have the following constraints on the adjacencymatrix:?
(1) All entries in Yj are between 0 and 1(convex relaxation of discrete directed edge in-dicators);?
(2) The sum over all the entries on each col-umn is equal to one (one-head rule);?
(3) All the entries on the diagonal are zeros(no self-link rule);?
(4) Yj,m,n + Yj,n,m ?
1 (anti-symmetricrule), which enforces directedness.One final constraint that is sufficient to ensure thata directed tree is obtained, is connectedness (i.e.acyclicity), which can be enforced with an addi-tional semidefinite constraint.
Although convex, thisconstraint is more expensive to enforce, therefore wedrop it in our experiments below.
(However, addingthe semidefinite connectedness constraint appears tobe feasible on a sentence by sentence level.
)Critically, the objective (8) is jointly convex inboth the weights ?
and the edge indicator variablesY.
This means, for example, that there are no localminima in (8)?any iterative improvement strategy,if it converges at all, must converge to a global min-imum.5.2 Semi-supervised Convex ObjectiveBy combining the convex structured SVM loss onlabeled data (shown in Equation (5)) and the con-vex least squares loss on unlabeled data (shown inEquation (8)), we obtain a semi-supervised struc-tured large margin lossmin?,Yj?2 ???
+N?i=1structured loss (?,Xi, Yi) +U?j=1least squares loss (?,Xj , Yj) (9)subject to constraints on Y (explained above).Since the summation of two convex functions isalso convex, so is (9).
Replacing the two losses withthe terms shown in Equation (5) and Equation (8),we obtain the final convex objective as follows:min?,Yj?2N ???
+N?i=1maxLk?m=1k?n=1?
(Li,m,n, Yi,m,n)?diff(?, Yi,m,n, Li,m,n) + ?2U ???
+ (10)?2U?j=1k?m=1k?n=1(?
?f(Xj,m ?
Xj,n)?
Yj,m,n)2subject to constraints on Y (explained above),where diff(?, Yi,m,n, Li,m,n) = score(?, Yi,m,n) ?536score(?, Li,m,n), N and U are the number of labeledand unlabeled training sentences respectively, as wementioned before.
Note that in (10) we have splitthe regularizer into two parts; one for the supervisedcomponent of the objective, and the other for theunsupervised component.
Thus the semi-supervisedconvex objective is regularized proportionally to thenumber of labeled and unlabeled training sentences.6 Efficient Optimization StrategyTo solve the convex optimization problem shown inEquation (10), we used a gradient descent approachwhich simply uses stochastic gradient steps.
Theprocedure is as follows.?
Step 0, initialize the Yj variables of eachunlabeled sentence as a right-branching (left-headed) chain model, i.e.
the head of each wordis its left neighbor.?
Step 1, pass through all the labeled training sen-tences one by one.
The parameters ?
are up-dated based on each labeled sentence.?
Step 2, based on the learned parameter weightsfrom the labeled data, update ?
and Yj on eachunlabeled sentence alternatively:?
treat Yj as a constant, update ?
on eachunlabeled sentence by taking a local gra-dient step;?
treat ?
as a constant, update Yj by call-ing the optimization software packageCPLEX to solve for an optimal local so-lution.?
Repeat the procedure of step 1 and step 2 untilmaximum iteration number has reached.This procedure works efficiently on the task oftraining a dependency parser.
Although ?
andYj are updated locally on each sentence, progressin minimizing the total objective shown in Equa-tion (10) is made in each iteration.
In our experi-ments, the objective usually converges within 30 it-erations.7 Experimental ResultsGiven a convex approach to semi-supervised struc-tured large margin training, and an efficient trainingalgorithm for achieving a global optimum, we nowinvestigate its effectiveness for dependency parsing.In particular, we investigate the accuracy of the re-sults it produces.
We applied the resulting algorithmto learn dependency parsers for both English andChinese.7.1 Experimental DesignData SetsSince we use a semi-supervised approach, both la-beled and unlabeled training data are needed.
Forexperiment on English, we used the English PennTreebank (PTB) (Marcus et al, 1993) and the con-stituency structures were converted to dependencytrees using the same rules as (Yamada and Mat-sumoto, 2003).
The standard training set of PTBwas spit into 2 parts: labeled training data?thefirst 30k sentences in section 2-21, and unlabeledtraining data?the remaining sentences in section2-21.
For Chinese, we experimented on the PennChinese Treebank 4.0 (CTB4) (Palmer et al, 2004)and we used the rules in (Bikel, 2004) for conver-sion.
We also divided the standard training set into2 parts: sentences in section 400-931 and sentencesin section 1-270 are used as labeled and unlabeleddata respectively.
For both English and Chinese,we adopted the standard development and test setsthroughout the literature.As listed in Table 1 with greater detail, weexperimented with sets of data with different sen-tence length: PTB-10/CTB4-10, PTB-15/CTB4-15,PTB-20/CTB4-20, CTB4-40 and CTB4, whichcontain sentences with up to 10, 15, 20, 40 and allwords respectively.FeaturesFor simplicity, in current work, we only used twosets of features?word-pair and tag-pair indicatorfeatures, which are a subset of features used byother researchers on dependency parsing (McDon-ald et al, 2005a; Wang et al, 2007).
Althoughour algorithms can take arbitrary features, by onlyusing these simple features, we already obtainedvery promising results on dependency parsingusing both the supervised and semi-supervisedapproaches.
Using the full set of features describedin (McDonald et al, 2005a; Wang et al, 2007) andcomparing the corresponding dependency parsing537EnglishPTB-10Training(l/ul) 3026/1016Dev 163Test 270PTB-15Training 7303/2370Dev 421Test 603PTB-20Training 12519/4003Dev 725Test 1034ChineseCTB4-10Training(l/ul) 642/347Dev 61Test 40CTB4-15Training 1262/727Dev 112Test 83CTB4-20Training 2038/1150Dev 163Test 118CTB4-40Training 4400/2452Dev 274Test 240CTB4Training 5314/2977Dev 300Test 289Table 1: Size of Experimental Data (# of sentences)results with previous work remains a direction forfuture work.Dependency Parsing AlgorithmsFor simplicity of implementation, we use a stan-dard CKY parser in the experiments, althoughEisner?s algorithm (Eisner, 1996) and the SpanningTree algorithm (McDonald et al, 2005b) are alsoapplicable.7.2 ResultsWe evaluate parsing accuracy by comparing the di-rected dependency links in the parser output againstthe directed links in the treebank.
The parameters?
and ?
which appear in Equation (10) were tunedon the development set.
Note that, during training,we only used the raw sentences of the unlabeleddata.
As shown in Table 2 and Table 3, for eachdata set, the semi-supervised approach achieves asignificant improvement over the supervised one independency parsing accuracy on both Chinese andEnglish.
These positive results are somewhat sur-prising since a very simple loss function was used onTraining Test length Supervised Semi-supTrain-10 ?
10 82.98 84.50Train-15 ?
10 84.80 86.93?
15 76.96 80.79Train-20?
10 84.50 86.32?
15 78.77 80.57?
20 74.89 77.85Train-40?
10 84.19 85.71?
15 78.03 81.21?
20 76.25 77.79?
40 68.17 70.90Train-all?
10 82.67 84.80?
15 77.92 79.30?
20 77.30 77.24?
40 70.11 71.90all 66.30 67.35Table 2: Supervised and Semi-supervised DependencyParsing Accuracy on Chinese (%)Training Test length Supervised Semi-supTrain-10 ?
10 87.77 89.17Train-15 ?
10 88.06 89.31?
15 81.10 83.37Train-20?
10 88.78 90.61?
15 83.00 83.87?
20 77.70 79.09Table 3: Supervised and Semi-supervised DependencyParsing Accuracy on English (%)538the unlabeled data.
A key benefit of the approach isthat a straightforward training algorithm can be usedto obtain global solutions.
Note that the results ofour model are not directly comparable with previousparsing results shown in (McClosky et al, 2006a),since the parsing accuracy is measured in terms ofdependency relations while their results are f -scoreof the bracketings implied in the phrase structure.8 Conclusion and Future WorkIn this paper, we have presented a novel algorithmfor semi-supervised structured large margin training.Unlike previous proposed approaches, we introducea convex objective for the semi-supervised learningalgorithm by combining a convex structured SVMloss and a convex least square loss.
This new semi-supervised algorithm is much more computationallyefficient and can easily scale up.
We have proved ourhypothesis by applying the algorithm to the signifi-cant task of dependency parsing.
The experimentalresults show that the proposed semi-supervised largemargin training algorithm outperforms the super-vised one, without much additional computationalcost.There remain many directions for future work.One obvious direction is to use the whole Penn Tree-bank as labeled data and use some other unannotateddata source as unlabeled data for semi-supervisedtraining.
Next, as we mentioned before, a muchricher feature set can be used in our model to getbetter dependency parsing results.
Another direc-tion is to apply the semi-supervised algorithm toother natural language problems, such as machinetranslation, topic segmentation and chunking.
Inthese areas, there are only limited annotated dataavailable.
Therefore semi-supervised approachesare necessary to achieve better performance.
Theproposed semi-supervised convex training approachcan be easily applied to these tasks.AcknowledgmentsWe thank the anonymous reviewers for their usefulcomments.
Research is supported by the Alberta In-genuity Center for Machine Learning, NSERC, MI-TACS, CFI and the Canada Research Chairs pro-gram.
The first author was also funded by the QueenElizabeth II Graduate Scholarship.ReferencesS.
Abney.
2004.
Understanding the yarowsky algorithm.Computational Linguistics, 30(3):365?395.Y.
Altun, D. McAllester, and M. Belkin.
2005.
Max-imum margin semi-supervised learning for structuredvariables.
In Proceedings of Advances in Neural In-formation Processing Systems 18.K.
Bennett and A. Demiriz.
1998.
Semi-supervised sup-port vector machines.
In Proceedings of Advances inNeural Information Processing Systems 11.D.
Bikel.
2004.
Intricacies of Collins?
parsing model.Computational Linguistics, 30(4).O.
Chapelle and A. Zien.
2005.
Semi-supervised clas-sification by low density separation.
In Proceedingsof the Tenth International Workshop on Artificial In-teligence and Statistics.E.
Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In Proceedings ofthe Association for the Advancement of Artificial In-telligence, pages 598?603.R.
Duda, P. Hart, and D. Stork.
2000.
Pattern Classifica-tion.
Wiley, second edition.J.
Eisner and G. Satta.
1999.
Efficient parsing for bilexi-cal context-free grammars and head-automaton gram-mars.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proceedings ofthe International Conference on Computational Lin-guistics.G.
Haffari and A. Sarkar.
2007.
Analysis of semi-supervised learning with the yarowsky algorithm.
InProceedings of the Conference on Uncertainty in Arti-ficial Intelligence.T.
Hastie, S. Rosset, R. Tibshirani, and J. Zhu.
2004.The entire regularization path for the support vectormachine.
Journal of Machine Learning Research,5:1391?1415.D.
Klein and C. Manning.
2002.
A generativeconstituent-context model for improved grammar in-duction.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics.D.
Klein and C. Manning.
2004.
Corpus-based inductionof syntactic structure: Models of dependency and con-stituency.
In Proceedingsof the Annual Meeting of theAssociation for Computational Linguistics.G.
S. Mann and A. McCallum.
2007.
Simple, robust,scalable semi-supervised learning via expectation reg-ularization.
In Proceedings of International Confer-ence on Machine Learning.C.
Manning and H. Schutze.
1999.
Foundations of Sta-tistical Natural Language Processing.
MIT Press.539M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics, 19(2):313?330.D.
McClosky, E. Charniak, and M. Johnson.
2006a.
Ef-fective self-training for parsing.
In Proceedings of theHuman Language Technology: the Annual Conferenceof the North American Chapter of the Association forComputational Linguistics.D.
McClosky, E. Charniak, and M. Johnson.
2006b.Reranking and self-training for parser adaptation.
InProceedings of the International Conference on Com-putational Linguistics and the Annual Meeting of theAssociation for Computational Linguistics.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Pro-ceedings of European Chapter of the Annual Meetingof the Association for Computational Linguistics.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line large-margin training of dependency parsers.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005b.Non-projective dependency parsing using spanningtree algorithms.
In Proceedings of Human LanguageTechnologies and Conference on Empirical Methodsin Natural Language Processing.M.
Palmer et al 2004.
Chinese Treebank 4.0.
LinguisticData Consortium.N.
Smith and J. Eisner.
2005.
Contrastive estimation:Training log-linear models on unlabeled data.
In Pro-ceedings of the Annual Meeting of the Association forComputational Linguistics.M.
Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,J.
Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In Proceedings of the European Chapter ofthe Annual Meeting of the Association for Computa-tional Linguistics, pages 331?338.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-margin Markov networks.
In Proceedings of Advancesin Neural Information Processing Systems 16.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.2004.
Support vector machine learning for interdepen-dent and structured output spaces.
In Proceedings ofInternational Conference on Machine Learning.Q.
Wang, D. Schuurmans, and D. Lin.
2005.
Strictlylexical dependency parsing.
In Proceedings of the In-ternational Workshop on Parsing Technologies, pages152?159.Q.
Wang, C. Cherry, D. Lizotte, and D. Schuurmans.2006.
Improved large margin dependency parsing vialocal constraints and Laplacian regularization.
In Pro-ceedings of The Conference on Computational NaturalLanguage Learning, pages 21?28.Q.
Wang, D. Lin, and D. Schuurmans.
2007.
Simpletraining of dependency parsers via structured boosting.In Proceedings of the International Joint Conferenceon Artificial Intelligence, pages 1756?1762.L.
Xu and D. Schuurmans.
2005.
Unsupervised andsemi-supervised multi-class support vector machines.In Proceedings the Association for the Advancement ofArtificial Intelligence.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InProceedings of the International Workshop on ParsingTechnologies.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proceed-ings of the Annual Meeting of the Association for Com-putational Linguistics, pages 189?196, Cambridge,Massachusetts.X.
Zhu, Z. Ghahramani, and J. Lafferty.
2003.
Semi-supervised learning using Gaussian fields and har-monic functions.
In Proceedings of International Con-ference on Machine Learning.X.
Zhu.
2005.
Semi-supervised learning literature sur-vey.
Technical report, Computer Sciences, Universityof Wisconsin-Madison.540
