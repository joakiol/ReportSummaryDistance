Word Translation Disambiguation Using Bilingual BootstrappingCong LiMicrosoft Research Asia5F Sigma Center, No.49 Zhichun Road, HaidianBeijing, China, 100080i-congl@microsoft.comHang LiMicrosoft Research Asia5F Sigma Center, No.49 Zhichun Road, HaidianBeijing, China, 100080hangli@microsoft.comAbstractThis paper proposes a new method forword translation disambiguation usinga machine learning technique called?Bilingual Bootstrapping?.
BilingualBootstrapping makes use of  inlearning a small number of classifieddata and a large number of unclassifieddata in the source and the targetlanguages in translation.
It constructsclassifiers in the two languages inparallel and repeatedly boosts theperformances of the classifiers byfurther classifying data in each of thetwo languages and by exchangingbetween the two languagesinformation regarding the classifieddata.
Experimental results indicate thatword translation disambiguation basedon Bilingual Bootstrappingconsistently and significantlyoutperforms the existing methodsbased on ?MonolingualBootstrapping?.1 IntroductionWe address here the problem of word translationdisambiguation.
For instance, we are concernedwith an ambiguous word in English (e.g., ?plant?
),which has multiple translations in Chinese (e.g.,? (gongchang)?
and ? (zhiwu)?).
Ourgoal is to determine the correct Chinesetranslation of the ambiguous English word, givenan English sentence which contains the word.Word translation disambiguation is actually aspecial case of word sense disambiguation (in theexample above, ?gongchang?
corresponds to thesense of ?factory?
and ?zhiwu?
corresponds to thesense of ?vegetation?
).1Yarowsky (1995) proposes a method for wordsense (translation) disambiguation that is basedon a bootstrapping technique, which we refer tohere as ?Monolingual Bootstrapping (MB)?.In this paper, we propose a new method for wordtranslation disambiguation using a bootstrappingtechnique we have developed.
We refer to thetechnique as ?Bilingual Bootstrapping (BB)?.In order to evaluate the performance of BB, weconducted some experiments on word translationdisambiguation using the BB technique and theMB technique.
All of the results indicate that BBconsistently and significantly outperforms MB.2 Related WorkThe problem of word translation disambiguation(in general, word sense disambiguation) can beviewed as that of classification and can beaddressed by employing a supervised learningmethod.
In such a learning method, for instance,an English sentence containing an ambiguousEnglish word corresponds to an example, and theChinese translation of the word under the contextcorresponds to a classification decision (a label).Many methods for word sense disambiguationusing a supervised learning technique have beenproposed.
They include those using Na?ve Bayes(Gale et al 1992a), Decision List (Yarowsky1994), Nearest Neighbor (Ng and Lee 1996),Transformation Based Learning (Mangu andBrill 1997), Neural Network (Towell and1In this paper, we take English-Chinese translation asexample; it is a relatively easy process, however, toextend the discussions to translations between otherlanguage pairs.Computational Linguistics (ACL), Philadelphia, July 2002, pp.
343-351.Proceedings of the 40th Annual Meeting of the Association forVoorhess 1998), Winnow (Golding and Roth1999), Boosting (Escudero et al 2000), andNa?ve Bayesian Ensemble (Pedersen 2000).Among these methods, the one using Na?veBayesian Ensemble (i.e., an ensemble of Na?veBayesian Classifiers) is reported to perform thebest for word sense disambiguation with respectto a benchmark data set (Pedersen 2000).The assumption behind the proposed methods isthat it is nearly always possible to determine thetranslation of a word by referring to its context,and thus all of the methods actually manage tobuild a classifier (i.e., a classification program)using features representing context information(e.g., co-occurring words).Since preparing supervised learning data isexpensive (in many cases, manually labeling datais required), it is desirable to develop abootstrapping method that starts learning with asmall number of classified data but is still able toachieve high performance under the help of alarge number of unclassified data which is notexpensive anyway.Yarowsky (1995) proposes a method for wordsense disambiguation, which is based onMonolingual Bootstrapping.
When applied to ourcurrent task, his method starts learning with asmall number of English sentences which containan ambiguous English word and which arerespectively assigned with the correct Chinesetranslations of the word.
It then uses theclassified sentences as training data to learn aclassifier (e.g., a decision list) and uses theconstructed classifier to classify someunclassified sentences containing the ambiguousword as additional training data.
It also adopts theheuristics of ?one sense per discourse?
(Gale et al1992b) to further classify unclassified sentences.By repeating the above processes, it can create anaccurate classifier for word translationdisambiguation.For other related work, see, for example, (Brownet al 1991; Dagan and Itai 1994; Pedersen andBruce 1997; Schutze 1998; Kikui 1999;Mihalcea and Moldovan 1999).3 Bilingual Bootstrapping3.1 OverviewInstead of using Monolingual Bootstrapping, wepropose a new method for word translationdisambiguation using Bilingual Bootstrapping.In translation from English to Chinese, forinstance, BB makes use of not only unclassifieddata in English, but also unclassified data inChinese.
It also uses a small number of classifieddata in English and, optionally, a small numberof classified data in Chinese.
The data in Englishand in Chinese are supposed to be not in parallelbut from the same domain.BB constructs classifiers for English to Chinesetranslation disambiguation by repeating thefollowing two steps: (1) constructing classifiersfor each of the languages on the basis of theclassified data in both languages, (2) using theconstructed classifiers in each of the languages toclassify some unclassified data and adding themto the classified training data set of the language.The reason that we can use classified data in bothlanguages at step (1) is that words in onelanguage generally have translations in the otherand we can find their translation relationship byusing a dictionary.3.2 AlgorithmLet E denote a set of words in English, C a set ofwords in Chinese, and T a set of links in atranslation dictionary as shown in Figure 1.
(Anytwo linked words can be translation of each other.
)Mathematically, T is defined as a relationbetween E and C , i.e., CET ??
.Let ?
stand for a random variable on E, ?
arandom variable on C. Also let e stand for arandom variable on E, c a random variable on C,and t a random variable on T. While ?
and?
represent words to be translated, e and crepresent context words.For an English word ?,}),,(|{ TtttT ?
?== ???
represents the linksMMMMMMFigure 1: Example of translation dictionaryfrom it, and }),(|{ TC ??
?= ????
represents theChinese words which are linked to it.
For aChinese word ?, let }),,(|{ TtttT ?
?== ???
and}),(|{ TE ??
?= ????
.
We can define eC  and cEsimilarly.Let e denote a sequence of words (e.g., a sentenceor a text) in English),,2,1(   },,,,{ 21 miEeeee im LL =?=  e .Let c denote a sequence of words in Chinese),,2,1(   },,,,{ 21 niCcccc in LL =?=  c .We view e and c as examples representingcontext information for translationdisambiguation.For an English word ?, we define a binaryclassifier for resolving each of its translationambiguities in ?T  in a general form as:},{ ),|(   &     ),|( tTttPTttP ???
????
eewhere e denotes an example in English.
Similarly,for a Chinese word ?, we define a classifier as:},{  ),|(   &     ),|( tTttPTttP ???
????
ccwhere c denotes an example in Chinese.Let ?L  denote a set of classified examples inEnglish, each representing one context of ?
),,,2,1(},),(,,),(,),{( 2211kiTttttLikkLL=?=?????
eeeand ?U  a set of unclassified examples in English,each representing one context of ?}.
)(,,)(,){( 21 ????
lU eee L=Similarly, we denote the sets of classified andunclassified examples with respect to ?
inChinese as ?L  and ?U  respectively.Furthermore, we have.,,, ????????UUUULLLLCCEECCEE???
?==== UUUUWe perform Bilingual Bootstrapping asdescribed in Figure 2.
Hereafter, we will onlyexplain the process for English (left-hand side);the process for Chinese (right-hand side) can beconducted similarly.3.3 Na?ve Bayesian ClassifierInput :  CCEE ULULTCE ,,,,,, ,  Parameter : ?,bRepeat  in parallel the following processes for English (left) and Chinese (right), until unable to continue :1. for each ( E?? )
{ for each ( C?? )
{for each (?Tt ? )
{use ?L  and )( ??
?
CL ?
to create classifier:??
TttP ?
),|( e   &  };{   ),|( tTttP ??
??
e }}for each ( ?Tt ? )
{use ?L  and )( ??
?
EL ?
to create classifier:??
TttP ?
),|( c   &  };{   ),|( tTttP ??
??
c }}2. for each ( E?? )
{{};{}; ??
NLNUfor each (?Tt ? )
{ }{};{}; ??
tt QSfor each ( C?? )
{{};{}; ??
NLNUfor each ( ?Tt ? )
{ }{};{}; ??
tt QSfor each (?U?e ){calculate)|()|(max)(*eeetPtPTt ????
?= ;let)|()|(maxarg)(*eeetPtPtTt ???
?= ;if ( tt => )(  &  )( ** ee ??
)put e into tS ;}for each ( ?U?c ){calculate)|()|(max)(*ccctPtPTt????
?= ;let)|()|(maxarg)(*ccctPtPtTt ???
?=;if ( tt => )(  &  )( ** cc ??
)put c into tS ;}for each (?Tt ?
){sort tS?e in descending order of )(* e?
andput the top b elements into tQ ;}for each ( ?Tt ?
){sort tS?c in descending order of )(* c?
andput the top b elements into tQ ;}for each ( ttQU?e ){put e into NU and put ))(,( ee ?t  into NL;}for each (ttQU?c ){put c into NU and put ))(,( cc ?t  into NL;}NLLL U??
?
; NUUU ??
??
;} NLLL U??
?
; NUUU ??
??
;}Output: classifiers in English and ChineseFigure 2: Bilingual BootstrappingWhile we can in principle employ any kind ofclassifier in BB, we use here a Na?ve BayesianClassifier.
At step 1 in BB, we construct theclassifier as described in Figure 3.
At step 2, foreach example e, we calculate with the Na?veBayesian Classifier:.
)|()()|()(max)|()|(max)(*tPtPtPtPtPtPTtTt eeeee??????????
?==The second equation is based on Bayes?
rule.In the calculation, we assume that the contextwords in e (i.e.,meee ,,, 21 L ) are independentlygenerated from )|( teP?
and thus we have.
)|()|(1?==mii tePtP ??
eWe can calculate )|( tP e?
similarly.For )|( teP?
, we calculate it at step 1 by linearlycombining )|()( teP E?
estimated from Englishand )|()( teP C?
estimated from Chinese:),( )|()|()1()|()()()(ePtePtePtePUCE???????++?
?=(1)where 10 ??
?
, 10 ??
?
, 1?+ ??
, and)()( eP U  is a uniform distribution over E , whichis used for avoiding zero probability.
In this way,we estimate )|( teP?
using information from notonly English but also Chinese.For )|()( teP E?
, we estimate it with MLE(Maximum Likelihood Estimation) using ?L  asdata.
For )|()( teP C?
, we estimate it as isdescribed in Section 3.4.3.4 EM AlgorithmFor the sake of readability, we rewrite )|()( teP C?as )|( teP .
We define a finite mixture model ofthe form ?
?=EetePtecPtcP )|(),|()|(  and for aspecific ?
we assume that the data in???????
ChiTttttLihh?
?=?=),,,1(},),(,,),(,),{( 2211LL cccare independently generated on the basis of themodel.
We can, therefore, employ theExpectation and Maximization Algorithm (EMAlgorithm) (Dempster et al 1977) to estimate theparameters of the model including )|( teP .
Wealso use the relation T in the estimation.Initially, we set??????
?=eeeCcCcCtecPif          , 0if     , ||1),|( ,.
, ||1)|( EeEteP ?=We next estimate the parameters by iterativelyupdating them ass described in Figure 4 untilthey converge.
Here ),( tcf  stands for thefrequency of c related to t. The contextinformation in Chinese is then ?translated?
intothat in English through the links in T.4  Comparison between BB and MBWe note that Monolingual Bootstrapping is aspecial case of Bilingual Bootstrapping (considerthe situation in which ?
equals 0 in formula (1)).Moreover, it seems safe to say that BB canalways perform better than MB.The many-to-many relationship between thewords in the two languages stands out as key tothe higher performance of BB.Suppose that the classifier with respect to ?plant?has two decisions (denoted as A and B in Figure5).
Further suppose that the classifiers withestimate )|()( teP E?
with MLE using  ?L   as data;estimate )|()( teP C?
with EM Algorithm using  ?Lfor each ??
C?
as data;calculate )|( teP?
as a linear combination of)|()( teP E?
and )|()( teP C?
;estimate )(tP?
with MLE using ?L ;calculate )|( teP?
and )(tP?
similarly.Figure 3: Creating Na?ve Bayesian ClassifierE-step:      ??
?EetePtecPtePtecPtceP)|(),|()|(),|(),|(M-step:      ???CctcePtcftcePtcftecP),|(),(),|(),(),|(????
?CcCctcftcePtcfteP),(),|(),()|(Figure 4: EM Algorithmrespect to ?gongchang?
and ?zhiwu?
in Chinesehave two decisions respectively, (C and D) (Eand F).
A and D are equivalent to each other (i.e.,they represent the same sense), and so are B andE.Assume that examples are classified after severaliterations in BB as depicted in Figure 5.
Here,circles denote the examples that are correctlyclassified and crosses denote the examples thatare incorrectly classified.Since A and D are equivalent to each other, wecan ?translate?
the examples with D and use themto boost the performance of classification to A.This is because the misclassified examples(crosses) with D are those mistakenly classifiedfrom C and they will not have much negativeeffect on classification to A, even though thetranslation from Chinese into English canintroduce some noises.
Similar explanations canbe stated to other classification decisions.In contrast, MB only uses the examples in A andB to construct a classifier, and when the numberof misclassified examples increases (this isinevitable in bootstrapping), its performance willstop improving.5 Word Translation Disambiguation5.1 Using Bilingual BootstrappingWhile it is possible to straightforwardly apply thealgorithm of BB described in Section 3 to wordtranslation disambiguation, we use here a variantof it for a better adaptation to the task and for afairer comparison with existing technologies.The variant of BB has four modifications.
(1)  It actually employs an ensemble of the Na?veBayesian Classifiers (NBC), because anensemble of NBCs generally performs betterthan a single NBC (Pedersen 2000).
In anensemble, it creates different NBCs using as datathe words within different window sizessurrounding the word to be disambiguated (e.g.,?plant?
or ?zhiwu?)
and further constructs a newclassifier by linearly combining the NBCs.
(2) It employs the heuristics of ?one sense perdiscourse?
(cf., Yarowsky 1995) after using anensemble of NBCs.
(3) It uses only classified data in English at thebeginning.
(4) It individually resolves ambiguities onselected English words such as ?plant?, ?interest?.As a result, in the case of ?plant?
; for example, theclassifiers with respect to ?gongchang?
and?zhiwu?
only make classification decisions to Dand E but not C and F (in Figure 5).
It calculates)(* c?
as )|()(* tP cc =?
and sets 0=?
at theright-hand side of step 2.5.2 Using Monolingual BootstrappingWe consider here two implementations of MBfor word translation disambiguation.In the first implementation, in addition to thebasic algorithm of MB, we also use (1) anensemble of Na?ve Bayesian Classifiers, (2) theheuristics of ?one sense per discourse?, and (3) asmall number of classified data in English at thebeginning.
We will denote this implementationas MB-B hereafter.The second implementation is different from thefirst one only in (1).
That is, it employs as aclassifier a decision list instead of an ensemble ofNBCs.
This implementation is exactly the oneproposed in (Yarowsky 1995), and we willdenote it as MB-D hereafter.MB-B and MB-D can be viewed as thestate-of-the-art methods for word translationdisambiguation using bootstrapping.6 Experimental ResultsMMMMooooo oo oo oooo ooo oooooooo???????
??
??
?Figure 5: Example of BBWe conducted two experiments onEnglish-Chinese translation disambiguation.6.1 Experiment 1: WSD Benchmark DataWe first applied BB, MB-B, and MB-D totranslation of the English words ?line?
and?interest?
using a benchmark data 2 .
The datamainly consists of articles in the Wall StreetJournal and it is designed for conducting Word2http://www.d.umn.edu/~tpederse/data.html.Sense Disambiguation (WSD) on the two words(e.g., Pedersen 2000).We adopted from the HIT dictionary 3  theChinese translations of the two English words, aslisted in Table 1.
One sense of the wordscorresponds to one group of translations.We then used the benchmark data as our test data.
(For the word ?interest?, we only used its fourmajor senses, because the remaining two minorsenses occur in only 3.3% of the data)3The dictionary is created by Harbin Institute ofTechnology.Table 1: Data descriptions in Experiment 1Words Chinese translations Corresponding English senses Seed wordsreadiness to give attention showmoney paid for the use of money rate,  a share in company or business holdinterestadvantage, advancement or favor conflict, 	 a thin flexible object cut,written or spoken text writetelephone connection telephone,  formation of people or things wait,  an artificial division betweenline,  product productTable 2: Data sizes in Experiment 1Unclassified sentences Words English ChineseTestsentencesinterest 1927 8811 2291line 3666 5398 4148Table 3: Accuracies in Experiment 1Words Major  (%)MB-D(%)MB-B(%)BB(%)interest 54.6 54.7 69.3 75.5line 53.5 55.6 54.1 62.7  	 Figure 6: Learning curves with ?interest?   Figure 7: Learning curves with ?line?  	  ?Figure 8: Accuracies of BB with different ?Table 4: Accuracies of supervised methodsinterest (%) line (%)Ensembles of NBC 89 88Na?ve Bayes 74 72Decision Tree 78 -Neural Network - 76Nearest Neighbor 87 -As classified data in English, we defined a ?seedword?
for each group of translations based on ourintuition (cf., Table 1).
Each of the seed wordswas then used as a classified ?sentence?.
This wayof creating classified data is similar to that in(Yarowsky, 1995).
As unclassified data inEnglish, we collected sentences in news articlesfrom a web site (www.news.com), and asunclassified data in Chinese, we collectedsentences in news articles from another web site(news.cn.tom.com).
We observed that thedistribution of translations in the unclassifieddata was balanced.Table 2 shows the sizes of the data.
Note thatthere are in general more unclassified sentencesin Chinese than in English because an Englishword usually has several Chinese words astranslations (cf., Figure 5).As a translation dictionary, we used the HITdictionary, which contains about 76000 Chinesewords, 60000 English words, and 118000 links.We then used the data to conduct translationdisambiguation with BB, MB-B, and MB-D, asdescribed in Section 5.For both BB and MB-B, we used an ensemble offive Na?ve Bayesian Classifiers with the windowsizes being ?1, ?3, ?5, ?7, ?9 words.
For bothBB and MB-B, we set the parameters of ?, b, and?
to 0.2, 15, and 1.5 respectively.
Theparameters were tuned based on our preliminaryexperimental results on MB-B, they were nottuned, however, for BB.
For the BB specificparameter ?, we set it to 0.4, which meant that wetreated the information from English and thatfrom Chinese equally.Table 3 shows the translation disambiguationaccuracies of the three methods as well as that ofa baseline method in which we always choose themajor translation.
Figures 6 and 7 show thelearning curves of MB-D, MB-B, and BB.
Figure8 shows the accuracies of BB with different?
values.From the results, we see that BB consistently andsignificantly outperforms both MB-D and MB-B.The results from the sign test are statisticallysignificant (p-value < 0.001).Table 4 shows the results achieved by someexisting supervised learning methods withrespect to the benchmark data (cf., Pedersen2000).
Although BB is a method nearlyequivalent to one based on unsupervised learning,it still performs favorably well when comparedwith the supervised methods (note that since theexperimental settings are different, the resultscannot be directly compared).6.2 Experiment 2: Yarowsky?s WordsWe also conducted translation on seven of thetwelve English words studied in (Yarowsky,1995).
Table 5 shows the list of the words.For each of the words, we extracted about 200sentences containing the word from the Encarta4English corpus and labeled those sentences withChinese translations ourselves.
We used thelabeled sentences as test data and the remainingsentences as unclassified data in English.
Wealso used the sentences in the GreatEncyclopedia 5  Chinese corpus as unclassifieddata in Chinese.
We defined, for each translation,4http://encarta.msn.com/default.asp5http://www.whlib.ac.cn/sjk/bkqs.htmTable 5: Data descriptions and data sizes in Experiment 2Unclassified sentences Words Chinese translationsEnglish ChineseSeed words Testsentencesbass,  / ,  142 8811 fish / music 200drug,  /  3053 5398 treatment / smuggler 197duty!, "  / #, #$ 1428 4338 discharge / export 197palm%&', %& / () 366 465 tree / hand 197plant*+, + / , 7542 24977 industry / life 197space-., ./ / 0-, 12-.
3897 14178 volume / outer 197tank34 / 56, 76 417 1400 combat / fuel 199Total - 16845 59567 - 1384a seed word in English as a classified example(cf., Table 5).We did not, however, conduct translationdisambiguation on the words ?crane?, ?sake?,?poach?, ?axes?, and ?motion?, because the firstfour words do not frequently occur in the Encartacorpus, and the accuracy of choosing the majortranslation for the last word has already exceeded98%.We next applied BB, MB-B, and MB-D to wordtranslation disambiguation.
The experimentsettings were the same as those in Experiment 1.From Table 6, we see again that BB significantlyoutperforms MB-D and MB-B.
(We will describethe results in detail in the full version of thispaper.)
Note that the results of MB-D here cannotbe directly compared with those in (Yarowsky,1995), mainly because the data used are different.6.3 DiscussionsWe investigated the reason of BB?soutperforming MB and found that theexplanation on the reason in Section 4 appears tobe true according to the following observations.
(1) In a Na?ve Bayesian Classifier, words havinglarge values of probability ratio )|()|(tePtePhavestrong influence on the classification of t whenthey occur, particularly, when they frequentlyoccur.
We collected the words having largevalues of probability ratio for each t in both BBand MB-B and found that BB obviously has more?relevant words?
than MB-B.
Here ?relevantwords?
for t refer to the words which are stronglyindicative to t on the basis of human judgments.Table 7 shows the top ten words in terms ofprobability ratio for the ? ?
translation(?money paid for the use of money?)
with respectto BB and MB-B, in which relevant words areunderlined.
Figure 9 shows the numbers ofrelevant words for the four translations of?interest?
with respect to BB and MB-B.
(2) From Figure 8, we see that the performance ofBB remains high or gets higher when ?
becomeslarger than 0.4 (recall that ?
was fixed to 0.2).This result strongly indicates that the informationfrom Chinese has positive effects ondisambiguation.
(3) One may argue that the higher performance ofBB might be attributed to the larger unclassifieddata size it uses, and thus if we increase theTable 6: Accuracies in Experiment 2Words Major (%)MB-D(%)MB-B(%)BB(%)bass 61.0 57.0 87.0 89.0drug 77.7 78.7 79.7 86.8duty 86.3 86.8 72.0 75.1palm 82.2 80.7 83.3 92.4plant 71.6 89.3 95.4 95.9space 64.5 71.6 84.3 87.8tank 60.3 62.8 76.9 84.4Total 71.9 75.2 82.6 87.4Table 7: Top words for ??
of ?interest?MB-B BBpaymentcutearnshortshort-termyieldu.s.marginbenchmarkregardsavingpaymentbenchmarkwhosebasepreferfixeddebtannualdividendFigure 9: Number of relevant words    Figure 10: When more unlabeled data availableunclassified data size for MB, it is likely that MBcan perform as well as BB.We conducted an additional experiment andfound that this is not the case.
Figure 10 showsthe accuracies achieved by MB-B when datasizes increase.
Actually, the accuracies of MB-Bcannot further improve when unlabeled datasizes increase.
Figure 10 plots again the results ofBB as well as those of a method referred to asMB-C.
In MB-C, we linearly combine two MB-Bclassifiers constructed with two differentunlabeled data sets and we found that althoughthe accuracies get some improvements in MB-C,they are still much lower than those of BB.7 ConclusionThis paper has presented a new word translationdisambiguation method using a bootstrappingtechnique called Bilingual Bootstrapping.Experimental results indicate that BBsignificantly outperforms the existingMonolingual Bootstrapping technique in wordtranslation disambiguation.
This is because BBcan effectively make use of information from twosources rather than from one source as in MB.AcknowledgementsWe thank Ming Zhou, Ashley Chang and YaoMeng for their valuable comments on an earlydraft of this paper.ReferencesP.
Brown, S. D. Pietra, V. D. Pietra, and R. Mercer,1991.
Word Sense Disambiguation UsingStatistical Methods.
In Proceedings of the 29thAnnual Meeting of the Association forComputational Linguistics, pp.
264-270.I.
Dagan and A. Itai, 1994.
Word SenseDisambiguation Using a Second LanguageMonolingual Corpus.
Computational Linguistics,vol.
20, pp.
563-596.A.
P. Dempster, N. M. Laird, and D. B. Rubin, 1977.Maximum Likelihood from Incomplete Data viathe EM Algorithm.
Journal of the Royal StatisticalSociety B, vol.
39, pp.
1-38.G.
Escudero, L. Marquez, and G. Rigau, 2000.Boosting Applied to Word Sense Disambiguation.In Proceedings of the 12th European Conferenceon Machine Learning.W.
Gale, K. Church, and D. Yarowsky, 1992a.
AMethod for Disambiguating Word Senses in aLarge Corpus.
Computers and Humanities, vol.
26,pp.
415-439.W.
Gale, K. Church, and D. Yarowsky, 1992b.
Onesense per discourse.
In Proceedings of DARPAspeech and Natural Language Workshop.A.
R. Golding and D. Roth, 1999.
A Winnow-BasedApproach to Context-Sensitive SpellingCorrection.
Machine Learning, vol.
34, pp.107-130.G.
Kikui, 1999.
Resolving Translation AmbiguityUsing Non-parallel Bilingual Corpora.
InProceedings of ACL ?99 Workshop onUnsupervised Learning in Natural LanguageProcessing.L.
Mangu and E. Brill, 1997.
Automatic ruleacquisition for spelling correction.
In Proceedingsof the 14th International Conference on MachineLearning.R.
Mihalcea and D. Moldovan, 1999.
A method forWord Sense Disambiguation of unrestricted text.In Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics.H.
T. Ng and H. B. Lee, 1996.
Integrating MultipleKnowledge Sources to Disambiguate Word Sense:An Exemplar-based Approach.
In Proceedings ofthe 34th Annual Meeting of the Association forComputational Linguistics, pp.
40-47.T.
Pedersen and R. Bruce, 1997.
Distinguishing WordSenses in Untagged Text.
In Proceedings of the2nd Conference on Empirical Methods in NaturalLanguage Processing, pp.
197-207.T.
Pedersen, 2000.
A Simple Approach to BuildingEnsembles of Na?ve Bayesian Classifiers for WordSense Disambiguation.
In Proceedings of the 1stMeeting of the North American Chapter of theAssociation for Computational Linguistics.H.
Schutze, 1998.
Automatic Word SenseDiscrimination.
In Computational Linguistics, vol.24, no.
1, pp.
97-124.G.
Towell and E. Voothees, 1998.
DisambiguatingHighly Ambiguous Words.
ComputationalLinguistics, vol.
24, no.
1, pp.
125-146.D.
Yarowsky, 1994.
Decision Lists for LexicalAmbiguity Resolution: Application to AccentRestoration in Spanish and French.
In Proceedingsof the 32nd Annual Meeting of the Association forComputational Linguistics, pp.
88-95.D.
Yarowsky, 1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Methods.
InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics, pp.189-196.
