Proceedings of the 43rd Annual Meeting of the ACL, pages 427?434,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsExploring Various Knowledge in Relation ExtractionZHOU GuoDong   SU Jian  ZHANG Jie  ZHANG MinInstitute for Infocomm research21 Heng Mui Keng Terrace, Singapore 119613Email: {zhougd, sujian, zhangjie, mzhang}@i2r.a-star.edu.sgAbstractExtracting semantic relationships between en-tities is challenging.
This paper investigatesthe incorporation of diverse lexical, syntacticand semantic knowledge in feature-based rela-tion extraction using SVM.
Our study illus-trates that the base phrase chunkinginformation is very effective for relation ex-traction and contributes to most of the per-formance improvement from syntactic aspectwhile additional information from full parsinggives limited further enhancement.
This sug-gests that most of useful information in fullparse trees for relation extraction is shallowand can be captured by chunking.
We alsodemonstrate how semantic information such asWordNet and Name List, can be used in fea-ture-based relation extraction to further im-prove the performance.
Evaluation on theACE corpus shows that effective incorporationof diverse features enables our system outper-form previously best-reported systems on the24 ACE relation subtypes and significantlyoutperforms tree kernel-based systems by over20 in F-measure on the 5 ACE relation types.1 IntroductionWith the dramatic increase in the amount of textualinformation available in digital archives and theWWW, there has been growing interest in tech-niques for automatically extracting informationfrom text.
Information Extraction (IE) systems areexpected to identify relevant information (usuallyof pre-defined types) from text documents in a cer-tain domain and put them in a structured format.According to the scope of the NIST AutomaticContent Extraction (ACE) program, currentresearch in IE has three main objectives: EntityDetection and Tracking (EDT), Relation Detectionand Characterization (RDC), and Event Detectionand Characterization (EDC).
The EDT task entailsthe detection of entity mentions and chaining themtogether by identifying their coreference.
In ACEvocabulary, entities are objects, mentions arereferences to them, and relations are semanticrelationships between entities.
Entities can be offive types: persons, organizations, locations,facilities and geo-political entities (GPE:geographically defined regions that indicate apolitical boundary, e.g.
countries, states, cities,etc.).
Mentions have three levels: names, nomialexpressions or pronouns.
The RDC task detectsand classifies implicit and explicit relations1between entities identified by the EDT task.
Forexample, we want to determine whether a person isat a location, based on the evidence in the context.Extraction of semantic relationships betweenentities can be very useful for applications such asquestion answering, e.g.
to answer the query ?Whois the president of the United States?
?.This paper focuses on the ACE RDC task andemploys diverse lexical, syntactic and semanticknowledge in feature-based relation extractionusing Support Vector Machines (SVMs).
Ourstudy illustrates that the base phrase chunkinginformation contributes to most of the performanceinprovement from syntactic aspect while additionalfull parsing information does not contribute much,largely due to the fact that most of relationsdefined in ACE corpus are within a very shortdistance.
We also demonstrate how semantic in-formation such as WordNet (Miller 1990) andName List can be used in the feature-based frame-work.
Evaluation shows that the incorporation ofdiverse features enables our system achieve bestreported performance.
It also shows that our fea-1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),explicit relations occur in text with explicit evidencesuggesting the relationships.
Implicit relations need nothave explicit supporting evidence in text, though theyshould be evident from a reading of the document.427ture-based approach outperforms tree kernel-basedapproaches by 11 F-measure in relation detectionand more than 20 F-measure in relation detectionand classification on the 5 ACE relation types.The rest of this paper is organized as follows.Section 2 presents related work.
Section 3 andSection 4 describe our approach and variousfeatures employed respectively.
Finally, we presentexperimental setting and  results in Section 5 andconclude with some general observations inrelation extraction in Section 6.2 Related WorkThe relation extraction task was formulated at the7th Message Understanding Conference (MUC-71998) and is starting to be addressed more andmore within the natural language processing andmachine learning communities.Miller et al(2000) augmented syntactic fullparse trees with semantic information correspond-ing to entities and relations, and built generativemodels for the augmented trees.
Zelenko et al(2003) proposed extracting relations by computingkernel functions between parse trees.
Culotta et al(2004) extended this work to estimate kernel func-tions between augmented dependency trees andachieved 63.2 F-measure in relation detection and45.8 F-measure in relation detection and classifica-tion on the 5 ACE relation types.
Kambhatla(2004) employed Maximum Entropy models forrelation extraction with features derived fromword, entity type, mention level, overlap, depend-ency tree and parse tree.
It achieves 52.8 F-measure on the 24 ACE relation subtypes.
Zhang(2004) approached relation classification by com-bining various lexical and syntactic features withbootstrapping on top of Support Vector Machines.Tree kernel-based approaches proposed by Ze-lenko et al(2003) and Culotta et al(2004) are ableto explore the implicit feature space without muchfeature engineering.
Yet further research work isstill expected to make it effective with complicatedrelation extraction tasks such as the one defined inACE.
Complicated relation extraction tasks mayalso impose a big challenge to the modeling ap-proach used by Miller et al(2000) which integratesvarious tasks such as part-of-speech tagging,named entity recognition, template element extrac-tion and relation extraction, in a single model.This paper will further explore the feature-basedapproach with a systematic study on the extensiveincorporation of diverse lexical, syntactic and se-mantic information.
Compared with Kambhatla(2004), we separately incorporate the base phrasechunking information, which contributes to mostof the performance improvement from syntacticaspect.
We also show how semantic informationlike WordNet and Name List can be equipped tofurther improve the performance.
Evaluation onthe ACE corpus shows that our system outper-forms Kambhatla (2004) by about 3 F-measure onextracting 24 ACE relation subtypes.
It also showsthat our system outperforms tree kernel-based sys-tems (Culotta et al2004) by over 20 F-measure onextracting 5 ACE relation types.3 Support Vector MachinesSupport Vector Machines (SVMs) are a supervisedmachine learning technique motivated by the sta-tistical learning theory (Vapnik 1998).
Based onthe structural risk minimization of the statisticallearning theory, SVMs seek an optimal separatinghyper-plane to divide the training examples intotwo classes and make decisions based on supportvectors which are selected as the only effectiveinstances in the training set.Basically, SVMs are binary classifiers.Therefore, we must extend SVMs to multi-class(e.g.
K) such as the ACE RDC task.
For efficiency,we apply the one vs. others strategy, which buildsK classifiers so as to separate one class from allothers, instead of the pairwise strategy, whichbuilds K*(K-1)/2 classifiers considering all pairs ofclasses.
The final decision of an instance in themultiple binary classification is determined by theclass which has the maximal SVM output.Moreover, we only apply the simple linear kernel,although other kernels can peform better.The reason why we choose SVMs for thispurpose is that SVMs represent the state-of?the-artin  the machine learning research community, andthere are good implementations of the algorithmavailable.
In this paper, we use the binary-classSVMLight2 deleveloped by Joachims (1998).2 Joachims has just released a new version of SVMLightfor multi-class classification.
However, this paper onlyuses the binary-class version.
For details aboutSVMLight, please see http://svmlight.joachims.org/4284 FeaturesThe semantic relation is determined between twomentions.
In addition, we distinguish the argumentorder of the two mentions (M1 for the first mentionand M2 for the second mention), e.g.
M1-Parent-Of-M2 vs. M2-Parent-Of-M1.
For each pair ofmentions3, we compute various lexical, syntacticand semantic features.4.1 WordsAccording to their positions, four categories ofwords are considered: 1) the words of both thementions, 2) the words between the two mentions,3) the words before M1, and 4) the words after M2.For the words of both the mentions, we also differ-entiate the head word4 of a mention from otherwords since the head word is generally much moreimportant.
The words between the two mentionsare classified into three bins: the first word in be-tween, the last word in between and other words inbetween.
Both the words before M1 and after M2are classified into two bins: the first word next tothe mention and the second word next to the men-tion.
Since a pronominal mention (especially neu-tral pronoun such as ?it?
and ?its?)
contains littleinformation about the sense of the mention, the co-reference chain is used to decide its sense.
This isdone by replacing the pronominal mention with themost recent non-pronominal antecedent when de-termining the word features, which include:?
WM1: bag-of-words in M1?
HM1: head word of M13 In ACE, each mention has a head annotation and anextent annotation.
In all our experimentation, we onlyconsider the word string between the beginning point ofthe extent annotation and the end point of the head an-notation.
This has an effect of choosing the base phrasecontained in the extent annotation.
In addition, this alsocan reduce noises without losing much of information inthe mention.
For example, in the case where the nounphrase ?the former CEO of McDonald?
has the headannotation of ?CEO?
and the extent annotation of ?theformer CEO of McDonald?, we only consider ?the for-mer CEO?
in this paper.4 In this paper, the head word of a mention is normallyset as the last word of the mention.
However, when apreposition exists in the mention, its head word is set asthe last word before the preposition.
For example, thehead word of the name mention ?University of Michi-gan?
is ?University?.?
WM2: bag-of-words in M2?
HM2: head word of M2?
HM12: combination of HM1 and HM2?
WBNULL: when no word in between?
WBFL: the only word in between when onlyone word in between?
WBF: first word in between when at least twowords in between?
WBL: last word in between when at least twowords in between?
WBO: other words in between except first andlast words when at least three words in between?
BM1F: first word before M1?
BM1L: second word before M1?
AM2F: first word after M2?
AM2L: second word after M24.2 Entity TypeThis feature concerns about the entity type of boththe mentions, which can be PERSON,ORGANIZATION, FACILITY, LOCATION andGeo-Political Entity or GPE:?
ET12: combination of mention entity types4.3 Mention LevelThis feature considers the entity level of both thementions, which can be NAME, NOMIAL andPRONOUN:?
ML12: combination of mention levels4.4 OverlapThis category of features includes:?
#MB: number of other mentions in between?
#WB: number of words in between?
M1>M2 or M1<M2: flag indicating whetherM2/M1is included in M1/M2.Normally, the above overlap features are toogeneral to be effective alone.
Therefore, they arealso combined with other features: 1)ET12+M1>M2; 2) ET12+M1<M2; 3)HM12+M1>M2; 4) HM12+M1<M2.4.5 Base Phrase ChunkingIt is well known that chunking plays a critical rolein the Template Relation task of the 7th MessageUnderstanding Conference (MUC-7 1998).
Therelated work mentioned in Section 2 extended toexplore the information embedded in the full parsetrees.
In this paper, we separate the features of base429phrase chunking from those of full parsing.
In thisway, we can separately evaluate the contributionsof base phrase chunking and full parsing.
Here, thebase phrase chunks are derived from full parsetrees using the Perl script5 written by SabineBuchholz from Tilburg University and the Collins?parser (Collins 1999) is employed for full parsing.Most of the chunking features concern about thehead words of the phrases between the two men-tions.
Similar to word features, three categories ofphrase heads are considered: 1) the phrase heads inbetween are also classified into three bins: the firstphrase head in between, the last phrase head inbetween and other phrase heads in between; 2) thephrase heads before M1 are classified into twobins: the first phrase head before and the secondphrase head before; 3) the phrase heads after M2are classified into two bins: the first phrase headafter and the second phrase head after.
Moreover,we also consider the phrase path in between.?
CPHBNULL when no phrase in between?
CPHBFL: the only phrase head when only onephrase in between?
CPHBF: first phrase head in between when atleast two phrases in between?
CPHBL: last phrase head in between when atleast two phrase heads in between?
CPHBO: other phrase heads in between exceptfirst and last phrase heads when at least threephrases in between?
CPHBM1F: first phrase head before M1?
CPHBM1L: second phrase head before M1?
CPHAM2F: first phrase head after M2?
CPHAM2F: second phrase head after M2?
CPP: path of phrase labels connecting the twomentions in the chunking?
CPPH: path of phrase labels connecting the twomentions in the chunking augmented with headwords, if at most two phrases in between4.6 Dependency TreeThis category of features includes informationabout the words, part-of-speeches and phrase la-bels of the words on which the mentions are de-pendent in the dependency tree derived from thesyntactic full parse tree.
The dependency tree isbuilt by using the phrase head information returnedby the Collins?
parser and linking all the other5 http://ilk.kub.nl/~sabine/chunklink/fragments in a phrase to its head.
It also includesflags indicating whether the two mentions are inthe same NP/PP/VP.?
ET1DW1: combination of the entity type andthe dependent word for M1?
H1DW1: combination of the head word and thedependent word for M1?
ET2DW2: combination of the entity type andthe dependent word for M2?
H2DW2: combination of the head word and thedependent word for M2?
ET12SameNP: combination of ET12 andwhether M1 and M2 included in the same NP?
ET12SamePP: combination of ET12 andwhether M1 and M2 exist in the same PP?
ET12SameVP: combination of ET12 andwhether M1 and M2 included in the same VP4.7 Parse TreeThis category of features concerns about the in-formation inherent only in the full parse tree.?
PTP: path of phrase labels (removing dupli-cates) connecting M1 and M2 in the parse tree?
PTPH: path of phrase labels (removing dupli-cates) connecting M1 and M2 in the parse treeaugmented with the head word of the top phrasein the path.4.8 Semantic ResourcesSemantic information from various resources, suchas WordNet, is used to classify important wordsinto different semantic lists according to their indi-cating relationships.Country Name ListThis is to differentiate the relation subtype?ROLE.Citizen-Of?, which defines the relationshipbetween a person and the country of the person?scitizenship, from other subtypes, especially?ROLE.Residence?, where defines the relationshipbetween a person and the location in which theperson lives.
Two features are defined to includethis information:?
ET1Country: the entity type of M1 when M2 isa country name?
CountryET2: the entity type of M2 when M1 isa country name430Personal Relative Trigger Word ListThis is used to differentiate the six personal socialrelation subtypes in ACE: Parent, Grandparent,Spouse, Sibling, Other-Relative and Other-Personal.
This trigger word list is first gatheredfrom WordNet by checking whether a word has thesemantic class ?person|?|relative?.
Then, all thetrigger words are semi-automatically6 classifiedinto different categories according to their relatedpersonal social relation subtypes.
We also extendthe list by collecting the trigger words from thehead words of the mentions in the training dataaccording to their indicating relationships.
Twofeatures are defined to include this information:?
ET1SC2: combination of the entity type of M1and the semantic class of M2 when M2 triggersa personal social subtype.?
SC1ET2: combination of the entity type of M2and the semantic class of M1 when the firstmention triggers a personal social subtype.5 ExperimentationThis paper uses the ACE corpus provided by LDCto train and evaluate our feature-based relation ex-traction system.
The ACE corpus is gathered fromvarious newspapers, newswire and broadcasts.
Inthis paper, we only model explicit relations be-cause of poor inter-annotator agreement in the an-notation of implicit relations and their limitednumber.5.1 Experimental SettingWe use the official ACE corpus from LDC.
Thetraining set consists of 674 annotated text docu-ments (~300k words) and 9683 instances of rela-tions.
During development, 155 of 674 documentsin the training set are set aside for fine-tuning thesystem.
The testing set is held out only for finalevaluation.
It consists of 97 documents (~50kwords) and 1386 instances of relations.
Table 1lists the types and subtypes of relations for theACE Relation Detection and Characterization(RDC) task, along with their frequency of occur-rence in the ACE training set.
It shows that the6 Those words that have the semantic classes ?Parent?,?GrandParent?, ?Spouse?
and ?Sibling?
are automati-cally set with the same classes without change.
How-ever, The remaining words that do not have above fourclasses are manually classified.ACE corpus suffers from a small amount of anno-tated data for a few subtypes such as the subtype?Founder?
under the type ?ROLE?.
It also showsthat the ACE RDC task defines some difficult sub-types such as the subtypes ?Based-In?, ?Located?and ?Residence?
under the type ?AT?, which aredifficult even for human experts to differentiate.Type Subtype FreqAT(2781) Based-In 347Located 2126Residence 308NEAR(201) Relative-Location 201PART(1298) Part-Of 947Subsidiary 355Other 6ROLE(4756) Affiliate-Partner 204Citizen-Of 328Client 144Founder 26General-Staff 1331Management 1242Member 1091Owner 232Other 158SOCIAL(827) Associate 91Grandparent 12Other-Personal 85Other-Professional 339Other-Relative 78Parent 127Sibling 18Spouse 77Table 1: Relation types and subtypes in the ACEtraining dataIn this paper, we explicitly model the argumentorder of the two mentions involved.
For example,when comparing mentions m1 and m2, we distin-guish between m1-ROLE.Citizen-Of-m2 and m2-ROLE.Citizen-Of-m1.
Note that only 6 of these 24relation subtypes are symmetric: ?Relative-Location?, ?Associate?, ?Other-Relative?, ?Other-Professional?, ?Sibling?, and ?Spouse?.
In thisway, we model relation extraction as a multi-classclassification problem with 43 classes, two foreach relation subtype (except the above 6 symmet-ric subtypes) and a ?NONE?
class for the casewhere the two mentions are not related.5.2 Experimental ResultsIn this paper, we only measure the performance ofrelation extraction on ?true?
mentions with ?true?chaining of coreference (i.e.
as annotated by thecorpus annotators) in the ACE corpus.
Table 2measures the performance of our relation extrac-431tion system over the 43 ACE relation subtypes onthe testing set.
It shows that our system achievesbest performance of 63.1%/49.5%/ 55.5 in preci-sion/recall/F-measure when combining diverselexical, syntactic and semantic features.
Table 2also measures the contributions of different fea-tures by gradually increasing the feature set.
Itshows that:Features P R FWords 69.2 23.7 35.3+Entity Type 67.1 32.1 43.4+Mention Level 67.1 33.0 44.2+Overlap 57.4 40.9 47.8+Chunking 61.5 46.5 53.0+Dependency Tree 62.1 47.2 53.6+Parse Tree 62.3 47.6 54.0+Semantic Resources 63.1 49.5 55.5Table 2: Contribution of different features over 43relation subtypes in the test data?
Using word features only achieves the perform-ance of 69.2%/23.7%/35.3 in precision/recall/F-measure.?
Entity type features are very useful and improvethe F-measure by 8.1 largely due to the recallincrease.?
The usefulness of mention level features is quitelimited.
It only improves the F-measure by 0.8due to the recall increase.?
Incorporating the overlap features gives somebalance between precision and recall.
It in-creases the F-measure by 3.6 with a big preci-sion decrease and a big recall increase.?
Chunking features are very useful.
It increasesthe precision/recall/F-measure by 4.1%/5.6%/5.2 respectively.?
To our surprise, incorporating the dependencytree and parse tree features only improve the F-measure by 0.6 and 0.4 respectively.
This maybe due to the fact that most of relations in theACE corpus are quite local.
Table 3 shows thatabout 70% of relations exist where two men-tions are embedded in each other or separatedby at most one word.
While short-distance rela-tions dominate and can be resolved by abovesimple features, the dependency tree and parsetree features can only take effect in the remain-ing much less long-distance relations.
However,full parsing is always prone to long distance er-rors although the Collins?
parser used in oursystem represents the state-of-the-art in fullparsing.?
Incorporating semantic resources such as thecountry name list and the personal relative trig-ger word list further increases the F-measure by1.5 largely due to the differentiation of the rela-tion subtype ?ROLE.Citizen-Of?
from ?ROLE.Residence?
by distinguishing country GPEsfrom other GPEs.
The effect of personal relativetrigger words is very limited due to the limitednumber of testing instances over personal socialrelation subtypes.Table 4 separately measures the performance ofdifferent relation types and major subtypes.
It alsoindicates the number of testing instances, the num-ber of correctly classified instances and the numberof wrongly classified instances for each type orsubtype.
It is not surprising that the performanceon the relation type ?NEAR?
is low because it oc-curs rarely in both the training and testing data.Others like ?PART.Subsidary?
and ?SOCIAL.Other-Professional?
also suffer from their low oc-currences.
It also shows that our system performsbest on the subtype ?SOCIAL.Parent?
and ?ROLE.Citizen-Of?.
This is largely due to incorporation oftwo semantic resources, i.e.
the country name listand the personal relative trigger word list.
Table 4also indicates the low performance on the relationtype ?AT?
although it frequently occurs in both thetraining and testing data.
This suggests the diffi-culty of detecting and classifying the relation type?AT?
and its subtypes.Table 5 separates the performance of relationdetection from overall performance on the testingset.
It shows that our system achieves the perform-ance of 84.8%/66.7%/74.7 in precision/recall/F-measure on relation detection.
It also shows thatour system achieves overall performance of77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in preci-sion/recall/F-measure on the 5 ACE relation typesand the best-reported systems on the ACE corpus.It shows that our system achieves better perform-ance by ~3 F-measure largely due to its gain inrecall.
It also shows that feature-based methodsdramatically outperform kernel methods.
This sug-gests that feature-based methods can effectivelycombine different features from a variety ofsources (e.g.
WordNet and gazetteers) that can bebrought to bear on relation extraction.
The treekernels developed in Culotta et al(2004) are yet tobe effective on the ACE RDC task.Finally, Table 6 shows the distributions of er-rors.
It shows that 73% (627/864) of errors results432from relation detection and 27% (237/864) of er-rors results from relation characterization, amongwhich 17.8% (154/864) of errors are from misclas-sification across relation types and 9.6% (83/864)of errors are from misclassification of relation sub-types inside the same relation types.
This suggeststhat relation detection is critical for relation extrac-tion.# of other mentions in between # of relations0 1 2 3 >=4 Overall0 3991 161 11 0 0 41631 2350 315 26 2 0 26932 465 95 7 2 0 5693 311 234 14 0 0 5594 204 225 29 2 3 4635 111 113 38 2 1 265>=6 262 297 277 148 134 1118#ofthe wordsinbetweenOverall 7694 1440 402 156 138 9830Table 3: Distribution of relations over #words and #other mentions in between in the training dataType Subtype #Testing Instances #Correct #Error P R FAT  392 224 105 68.1 57.1 62.1Based-In 85 39 10 79.6 45.9 58.2Located 241 132 120 52.4 54.8 53.5Residence 66 19 9 67.9 28.8 40.4NEAR  35 8 1 88.9 22.9 36.4Relative-Location 35 8 1 88.9 22.9 36.4PART  164 106 39 73.1 64.6 68.6Part-Of 136 76 32 70.4 55.9 62.3Subsidiary 27 14 23 37.8 51.9 43.8ROLE  699 443 82 84.4 63.4 72.4Citizen-Of 36 25 8 75.8 69.4 72.6General-Staff 201 108 46 71.1 53.7 62.3Management 165 106 72 59.6 64.2 61.8Member 224 104 36 74.3 46.4 57.1SOCIAL  95 60 21 74.1 63.2 68.5Other-Professional 29 16 32 33.3 55.2 41.6Parent 25 17 0 100 68.0 81.0Table 4: Performance of different relation types and major subtypes in the test dataRelation Detection RDC on Types RDC on Subtypes SystemP R F P R F P R FOurs: feature-based 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5Kambhatla (2004):feature-based - - - - - - 63.5 45.2 52.8Culotta et al(2004):tree kernel 81.2 51.8 63.2 67.1 35.0 45.8 - - -Table 5: Comparison of our system with other best-reported systems on the ACE corpusError Type #ErrorsFalse Negative 462 Detection ErrorFalse Positive 165Cross Type Error 154 CharacterizationError Inside Type Error 83Table 6: Distribution of errors6 Discussion and ConclusionIn this paper, we have presented a feature-basedapproach for relation extraction where diverselexical, syntactic and semantic knowledge are em-ployed.
Instead of exploring the full parse tree in-formation directly as previous related work, weincorporate the base phrase chunking informationfirst.
Evaluation on the ACE corpus shows thatbase phrase chunking contributes to most of theperformance improvement from syntactic aspectwhile further incorporation of the parse tree anddependence tree information only slightly im-proves the performance.
This may be due to threereasons: First, most of relations defined in ACEhave two mentions being close to each other.While short-distance relations dominate and can beresolved by simple features such as word andchunking features, the further dependency tree andparse tree features can only take effect in the re-maining much less and more difficult long-distancerelations.
Second, it is well known that full parsing433is always prone to long-distance parsing errors al-though the Collins?
parser used in our systemachieves the state-of-the-art performance.
There-fore, the state-of-art full parsing still needs to befurther enhanced to provide accurate enough in-formation, especially PP (Preposition Phrase) at-tachment.
Last, effective ways need to be exploredto incorporate information embedded in the fullparse trees.
Besides, we also demonstrate how se-mantic information such as WordNet and NameList, can be used in feature-based relation extrac-tion to further improve the performance.The effective incorporation of diverse featuresenables our system outperform previously best-reported systems on the ACE corpus.
Althoughtree kernel-based approaches facilitate the explora-tion of the implicit feature space with the parse treestructure, yet the current technologies are expectedto be further advanced to be effective for relativelycomplicated relation extraction tasks such as theone defined in ACE where 5 types and 24 subtypesneed to be extracted.
Evaluation on the ACE RDCtask shows that our approach of combining variouskinds of evidence can scale better to problems,where we have a lot of relation types with a rela-tively small amount of annotated data.
The ex-periment result also shows that our feature-basedapproach outperforms the tree kernel-based ap-proaches by more than 20 F-measure on the extrac-tion of 5 ACE relation types.In the future work, we will focus on exploringmore semantic knowledge in relation extraction,which has not been covered by current research.Moreover, our current work is done when the En-tity Detection and Tracking (EDT) has been per-fectly done.
Therefore, it would be interesting tosee how imperfect EDT affects the performance inrelation extraction.ReferencesAgichtein E. and Gravano L. (2000).
Snowball: Extract-ing relations from large plain text collections.
In Pro-ceedings of 5th ACM International Conference onDigital Libraries.
4-7 June 2000.
San Antonio, TX.Brin S. (1998).
Extracting patterns and relations fromthe World Wide Web.
In Proceedings of WebDBworkshop at 6th International Conference on Extend-ing DataBase Technology (EDBT?1998).23-27March 1998, Valencia, SpainCollins M. (1999).
Head-driven statistical models fornatural language parsing.
Ph.D. Dissertation, Univer-sity of Pennsylvania.Collins M. and Duffy N. (2002).
Covolution kernels fornatural language.
In Dietterich T.G., Becker S. andGhahramani Z. editors.
Advances in Neural Informa-tion Processing Systems 14.
Cambridge, MA.Culotta A. and Sorensen J.
(2004).
Dependency treekernels for relation extraction.
In Proceedings of 42thAnnual Meeting of the Association for ComputationalLinguistics.
21-26 July 2004.
Barcelona, SpainCumby C.M.
and Roth D. (2003).
On kernel methodsfor relation learning.
In Fawcett T. and Mishra N.editors.
In Proceedings of 20th International Confer-ence on Machine Learning (ICML?2003).
21-24 Aug2003.
Washington D.C. USA.
AAAI Press.Haussler D. (1999).
Covention kernels on discrete struc-tures.
Technical Report UCS-CRL-99-10.
Universityof California, Santa Cruz.Joachims T. (1998).
Text categorization with SupportVector Machines: Learning with many relevant fea-tures.
In Proceedings of European Conference onMachine Learning(ECML?1998).
21-23 April 1998.Chemnitz, GermanyMiller G.A.
(1990).
WordNet: An online lexical data-base.
International Journal of Lexicography.3(4):235-312.Miller S., Fox H., Ramshaw L. and Weischedel R.(2000).
A novel use of statistical parsing to extractinformation from text.
In Proceedings of 6th AppliedNatural Language Processing Conference.
29 April- 4 May 2000, Seattle, USAMUC-7.
(1998).
Proceedings of the 7th Message Under-standing Conference (MUC-7).
Morgan Kaufmann,San Mateo, CA.Kambhatla N. (2004).
Combining lexical, syntactic andsemantic features with Maximum Entropy models forextracting relations.
In Proceedings of 42th AnnualMeeting of the Association for Computational Lin-guistics.
21-26 July 2004.
Barcelona, Spain.Roth D. and Yih W.T.
(2002).
Probabilistic reasoningfor entities and relation recognition.
In Proceedingsof 19th International Conference on ComputationalLinguistics(CoLING?2002).
Taiwan.Vapnik V. (1998).
Statistical Learning Theory.
Whiley,Chichester, GB.Zelenko D., Aone C. and Richardella.
(2003).
Kernelmethods for relation extraction.
Journal of MachineLearning Research.
pp1083-1106.Zhang Z.
(2004).
Weekly-supervised relation classifica-tion for Information Extraction.
In Proceedings ofACM 13th Conference on Information and Knowl-edge Management (CIKM?2004).
8-13 Nov 2004.Washington D.C., USA.434
