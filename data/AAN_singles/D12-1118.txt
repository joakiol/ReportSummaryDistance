Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1290?1301, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsBesting the Quiz Master:Crowdsourcing Incremental Classification GamesJordan Boyd-GraberiSchool and UMIACSUniversity of Marylandjbg@umiacs.umd.eduBrianna Satinoff, He He, and Hal Daume?
IIIDepartment of Computer ScienceUniversity of Maryland{bsonrisa, hhe, hal}@cs.umd.eduAbstractCost-sensitive classification, where the featuresused in machine learning tasks have a cost, hasbeen explored as a means of balancing knowl-edge against the expense of incrementally ob-taining new features.
We introduce a settingwhere humans engage in classification withincrementally revealed features: the collegiatetrivia circuit.
By providing the community witha web-based system to practice, we collectedtens of thousands of implicit word-by-wordratings of how useful features are for elicitingcorrect answers.
Observing humans?
classifi-cation process, we improve the performanceof a state-of-the art classifier.
We also use thedataset to evaluate a system to compete in theincremental classification task through a reduc-tion of reinforcement learning to classification.Our system learns when to answer a question,performing better than baselines and most hu-man players.1 IntroductionA typical machine learning task takes as input a setof features and learns a mapping from features to alabel.
In such a setting, the objective is to minimizethe error of the mapping from features to labels.
Wecall this traditional setting, where all of the featuresare consumed, rapacious machine learning.1This not how humans approach the same task.They do not exhaustively consider every feature.
Af-ter a certain point, a human has made a decisionand no longer needs additional features.
Even in-defatigable computers cannot always exhaustivelyconsider every feature.
This is because the result1Earlier drafts called this ?batch?
machine learning, whichconfused the distinction between batch and online learning.
Wegladly adopt ?rapacious?
to make this distinction clearer andto cast traditional machine learning?that always examines allfeatures?as a resource hungry approach.is time sensitive, such as in interactive systems, orbecause processing time is limited by the sheer quan-tity of data, as in sifting e-mail for spam (Pujara etal., 2011).
In such settings, often the best solutionis incremental: allow a decision to be made withoutseeing all of an instance?s features.
We discuss theincremental classification framework in Section 2.Our understanding of how humans conduct incre-mental classification is limited.
This is because com-plicating an already difficult annotation task is oftenan unwise tradeoff.
Instead, we adapt a real worldsetting where humans are already engaging (eagerly)in incremental classification?trivia games?and de-velop a cheap, easy method for capturing humanincremental classification judgments.After qualitatively examining how humans con-duct incremental classification (Section 3), we showthat knowledge of a human?s incremental classifi-cation process improves state-of-the-art rapaciousclassification (Section 4).
Having established thatthese data contain an interesting signal, we buildBayesian models that, when embedded in a Markovdecision process, can engage in effective incrementalclassification (Section 5), and develop new hierar-chical models combining local and thematic contentto better capture the underlying content (Section 7).Finally, we conclude in Section 8 and discuss exten-sions to other problem areas.2 Incremental ClassificationIn this section, we discuss previous approaches thatexplore how much effort or resources a classifierneeds to come to a decision, a problem not as thor-oughly examined as the question of whether the de-cision is right or not.2 Incremental classification is2When have an externally interrupted feature stream, thesetting is called ?any time?
(Boddy and Dean, 1989; Horsch andPoole, 1998).
Like ?budgeted?
algorithms (Wang et al 2010),these are distinct but related problems.1290not equivalent to missing features, which have beenstudied at training time (Cesa-Bianchi et al 2011),test time (Saar-Tsechansky and Provost, 2007), andin an online setting (Rostamizadeh et al 2011).
Incontrast, incremental classification allows the learnerto decide whether to acquire additional features.A common paradigm for incremental classificationis to view the problem as a Markov decision process(MDP) (Zubek and Dietterich, 2002).
The incremen-tal classifier can either request an additional featureor render a classification decision (Chai et al 2004;Ji and Carin, 2007; Melville et al 2005), choosingits actions to minimize a known cost function.
Here,we assume that the environment chooses a featurein contrast to a learner, as in some active learningsettings (Settles, 2011).
In Section 5, we use a MDPto decide whether additional features need to be pro-cessed in our application of incremental classificationto a trivia game.2.1 Trivia as Incremental ClassificationA real-life setting where humans classify documentsincrementally is quiz bowl, an academic competitionbetween schools in English-speaking countries; hun-dreds of teams compete in dozens of tournamentseach year (Jennings, 2006).
Note the distinction be-tween quiz bowl and Jeopardy, a recent applicationarea (Ferrucci et al 2010).
While Jeopardy also usessignaling devices, these are only usable after a ques-tion is completed (interrupting Jeopardy?s questionswould make for bad television).
Thus, Jeopardy israpacious classification followed by a race to see?among those who know the answer?who can puncha button first.
Moreover, buzzes before the question?send are penalized.Two teams listen to the same question.3 In thiscontext, a question is a series of clues (features) re-ferring to the same entity (for an example question,see Figure 1).
We assume a fixed feature orderingfor a test sequence (i.e., you cannot request specificfeatures).
Teams interrupt the question at any pointby ?buzzing in?
; if the answer is correct, the teamgets points and the next question is read.
Otherwise,the team loses points and the other team can answer.3Called a ?starter?
(UK) or ?tossup?
(US) in the lingo, as itoften is followed by a ?bonus?
given to the team that answers thestarter; here we only concern ourselves with tossups answerableby both teams.After losing a race for the Senate, this politician edited the Om-aha World-Herald.
This man resigned 3 from one of his postswhen the President sent a letter to Germany protesting the Lusi-tania 3 sinking, and 3 he advocated 3 coining 3 silver at a 163 to 1 33 rate 3 compared to 3 gold.
He was the 3 three-timeDemocratic 3 Party 333 nominee for 3 President 3 but 333lost to McKinley twice 33 and then Taft, although he served asSecretary of State 33 under Woodrow Wilson, 3 and he laterargued 3 against Clarence Darrow 3 in the Scopes 33 MonkeyTrial.
For ten points, name this 3 man who famously declaredthat ?we shall not be crucified on a Cross of 3 Gold?.
3Figure 1: Quiz bowl question on William Jennings Bryan,a late nineteenth century American politician; obscureclues are at the beginning while more accessible clues areat the end.
Words (excluding stop words) are shaded basedon the number of times the word triggered a buzz from anyplayer who answered the question (darker means morebuzzes; buzzes contribute to the shading of the previousfive words).
Diamonds (3) indicate buzz positions.The answers to quiz bowl questions are well-known entities (e.g., scientific laws, people, battles,books, characters, etc.
), so the answer space is rel-atively limited; there are no open-ended questionsof the form ?why is the sky blue??
However, thereare no multiple choice questions?as there are inWho Wants to Be a Millionaire (Lam et al 2003)?or structural constraints?as there are in crosswordpuzzles (Littman et al 2002).Now that we introduced the concepts of questions,answers, and buzzes, we pause briefly to define themmore formally and explicitly connect to machinelearning.
In the sequel, we will refer to: questions,sequences of words (tokens) associated with a singleanswer; features, inputs used for decisions (derivedfrom the tokens in a question); labels, a question?scorrect response; answers, the responses (either cor-rect or incorrect) provided; and buzzes, positions ina question where users halted the stream of featuresand gave an answer.Quiz bowl is not a typical problem domain for natu-ral language processing; why should we care about it?First, it is a real-world instance of incremental classi-fication that happens hundreds of thousands of timesmost weekends.
Second, it is a classification problemintricately intertwined with core computational lin-guistics problems such as anaphora resolution, onlinesentence processing, and semantic priming.
Finally,quiz bowl?s inherent fun makes it easy to acquirehuman responses, as we describe in the next section.1291Number of Tokens RevealedAccuracy0.40.60.81.040 60 80 100Total50010001500200025003000Figure 2: Users plotted based on accuracy vs. the numberof tokens?on average?the user took to give an answer.Dot size and colour represent the total number of ques-tions answered.
Users that answered questions later in thequestion had higher accuracy.
However, there were usersthat were able to answer questions relatively early withoutsacrificing accuracy.3 Getting a Buzz through CrowdsourcingWe built a corpus with 37,225 quiz bowl questionswith 25,498 distinct labels from 121 tournamentswritten for tournaments between 1999 and 2010.
Wecreated a webapp4 that simulates the experience ofplaying quiz bowl.
Text is incrementally revealed(at a pace adjustable by the user) until users pressthe space bar to ?buzz?.
Users then answer, and thewebapp judges correctness using a string matchingalgorithm.
Players can override the automatic checkif the system mistakenly judged an answer incorrect.Answers of previous users are displayed after answer-ing a question; this enhances the sense of communityand keeps users honest (e.g., it?s okay to say that ?wjbryan?
is an acceptable answer for the label ?williamjennings bryan?, but ?asdf?
is not).
We did not seeexamples of nonsense answers from malicious users;in contrast, users were stricter than we expected, per-haps because protesting required effort.To collect a set of labels with many buzzes, wefocused on the 1186 labels with more than four dis-tinct questions.
Thus, we shuffled the labels into acanonical order shown to all users (e.g., everyonesaw a question on ?Jonathan Swift?
and then a ques-tion on ?William Jennings Bryan?, but because theselabels have many questions the specific questions4Play online or download the datasets at http://umiacs.umd.edu/?jbg/qb.Figure 3: A screenshot of the webapp used to collect data.Users see a question revealed one word at a time.
Theysignal buzzes by clicking on the answer button and inputan answer.were different for each user).
Participants were ea-ger to answer questions; over 7000 questions wereanswered in the first day, and over 43000 questionswere answered in two weeks by 461 users.To represent a ?buzz?, we define a function b(q, f)(?b?
for buzz) as the number of times that featuref occurred in question q at most five tokens beforea user correctly buzzed on that question.5 Aggre-gating buzzes across questions (summing over q)shows different features useful for eliciting a buzz(Figure 4(a)).
Some features coarsely identify thetype of answer sought, e.g., ?author?, ?opera?, ?city?,?war?, or ?god?.
Other features are relational, con-necting the answer to other clues, e.g., ?namesake?,?defeated?, ?husband?, or ?wrote?.
The set of buzzeshelp narrow which words are important for matchinga question to its answer; for an example, see howthe word cloud for all of the buzzes on ?Wuthering5This window was chosen qualitatively by examining thepatterns of buzzes; this is person-dependent, based on readingcomprehension, reaction time, and what reveal speed the userchose.
We leave explicitly modeling this for future work.1292(a) Buzzes over all Questions (b) Wuthering Heights Question Text (c) Buzzes on Wuthering HeightsFigure 4: Word clouds representing all words that were a part of a buzz (a), the original text appearing in seven questionson the book ?Wuthering Heights?
by Emily Bro?nte (b), and the buzzes of users on those questions (c).
The buzzesreflect what users remember about the work and is more focused than the complete question text.Heights?
(Figure 4(c)) is much more focused than theword cloud for all of the words from the questionswith that label (Figure 4(b)).4 Buzzes Reveal Useful FeaturesIf we restrict ourselves to a finite set of labels, theprocess of answering questions is a multiclass clas-sification problem.
In this section, we show that in-formation gleaned from humans making a similar de-cision can help improve rapacious machine learningclassification.
This validates that our crowdsourcingtechnique is gathering useful information.We used a state-of-the-art maximum entropy clas-sification model, MEGAM (Daume?
III, 2004), thataccepts a per-class mean prior for feature weightsand applied MEGAM to the 200 most frequent labels(11,663 questions, a third of the dataset).
The priormean of the feature weight is a convenient, simpleway to incorporate human feature utility; apart fromthe mean, all default options are used.Specifying the prior requires us to specify a weightfor each pair of label and feature.
The weight com-bines buzz information (described in Section 3) andtf-idf (Salton, 1968).
The tf-idf value is computed bytreating the training set of questions with the samelabel as a single document.Buzzes and tf-idf information were combined intothe prior ?
for label a and feature f as ?a,f =[?b(a, f) + ?I [b(a, f) > 0] + ?
]tf-idf(a, f).
(1)We describe our weight strategies in increasing orderof human knowledge.
If ?, ?, and ?
are zero, thisis a na?
?ve zero prior.
If ?
only is nonzero, this is alinear transformation of features?
tf-idf.
If only ?is nonzero, this is a linear transformation of buzzedWeighting ?
?
?
Errorzero - - - 0.37tf-idf - - 3.5 0.14buzz-binary 7.1 - - 0.10buzz-linear - 1.5 - 0.16buzz-tier - 1.1 0.1 0.09Table 1: Classification error of a rapacious classifier ableto draw on human incremental classification.
The bestweighting scheme for each dataset is in bold.
Missingparameter values (-) mean that the parameter is fixed tozero for that weighting scheme.words?
tf-idf weights.
If only ?
is non-zero, num-ber of buzzes is now a linear multiplier of the tf-idfweight (buzz-linear).
Finally we allow unbuzzedwords to have a separate linear transformation if both?
and ?
are non-zero (buzz-tier).Grid search (width of 0.1) on development set errorwas used to set parameters.
Table 1 shows test errorfor weighting schemes and demonstrates that addinghuman information as a prior improves classificationerror, leading to a 36% error reduction over tf-idfalone.
While not directly comparable (this classifieris rapacious, not incremental, and has a predefinedanswer space), the average user had an error rate of16.7%.5 Building an Incremental ClassifierIn the previous section we improved rapacious classi-fication using humans?
incremental classification.
Amore interesting problem is how to compete againsthumans in incremental classification.
While in theprevious section we used human data for a trainingset, here we use human data as an evaluation set.Doing so requires us to formulate an incremental rep-resentation of the contents of questions and to learna strategy to decide when to buzz.1293Because this is the first machine learning algo-rithm for quiz bowl, we attempt to provide reason-able rapacious baselines and compare against our newstrategies.
We believe that our attempts represent areasonable explanation of the problem space, but ad-ditional improvements could improve performance,as discussed in Section 8.A common way to represent state-dependent strate-gies is via a Markov decision process (MDP).
Themost salient component of a MDP is the policy, i.e., amapping from the state space to an action.
In our con-text, a state is a sequence of (thus far revealed) tokens,and the action is whether to buzz or not.
To learn apolicy, we use a standard reinforcement learning tech-nique (Langford and Zadrozny, 2005; Abbeel andNg, 2004; Syed et al 2008): given a representationof the state space, learn a classifier that can map froma state to an action.
This is also a common paradigmfor other incremental tasks, e.g., shift-reduce pars-ing (Nivre, 2008).Given examples of the correct answer given a con-figuration of the state space, we can learn a MDPwithout explicitly representing the reward function.In this section, we define our method of definingactions and our representation of the state space.5.1 Action SpaceWe assume that there are only two possible actions:buzz now or wait.
An alternative would be a moreexpressive action space (e.g., an action for every pos-sible answer).
However, this conflates the questionof when to buzz with what to answer.
Instead, we callthe distinct component that provides what to answerthe content model.
We describe an initial contentmodel in Section 5.2, below, and improve the modelsfurther in Section 7.
For the moment, assume thata content model maintains a posterior distributionover labels and when needed can provide its bestguess (e.g., given the features seen, the best answeris ?William Jennings Bryan?
).Given the action space, we need to specify whereexamples of state space and action come from.
Inthe language of classification, we need to provide(x, y) pairs to learn a mapping x 7?
y.
The clas-sifier attempts to learn that action y is (?buzz?)
inall states where the content model gave a correct re-sponse given state x.
Negative examples (?wait?
)are applied to states where the content model gavea wrong answer.
Every token in our training set cor-responds to a classification example; both states areprevalent enough that we do not to explicitly need toaddress class imbalance.
This resembles approachesthat merge different classifiers (Riedel et al 2011) orattempt to estimate confidence of models (Blatz et al2004).
However, here we use partial observations.This is a simplification of the problem and corre-sponds to a strategy of ?buzz as soon as you know theanswer?, ignoring all other factors.
While reasonable,this is not always optimal.
For example, if you knowyour opponent is unlikely to answer a question, it isbetter to wait until you are more confident.
Incorrectanswers might also help your opponent, e.g., by elim-inating an incorrect answer.
Moreover, strategies in agame setting (rather than a single question) are morecomplicated.
For example, if a right answer is worth+10 points and the penalty for an incorrect questionis ?5, then a team leading by 15 points on the lastquestion should never attempt to answer.
Investigat-ing such gameplay strategies would require a ?rollout?
of game states (Tesauro and Galperin, 1996) toexplore the efficacy of such strategies.
While inter-esting, we leave these issues to future work.We also investigated learning a policy directlyfrom users?
buzzes directly (Abbeel and Ng, 2004),but this performed poorly because the content modelis incompatible with the players?
abilities and thehigh variation in players?
ability and styles (compareFigure 2).5.2 State SpaceRecall that our goal is to learn a classifier that mapsstates to actions; above, we defined the action space(the classifier?s output) but not the state space, theclassifier?s input.
The straightforward parameteriza-tion of the state space would be all of the words thathave been revealed.
However, such a feature set isvery sparse.We use three components to form the state space:what information has been observed, what the contentmodel believes is the correct answer, how confidentthe content model is, and whether the content model?sconfidence is changing.
We describe each in moredetail below.Text In addition to the obvious, sparse parameter-ization that contains all of the features thus far ob-1294served, we also include the total number of tokensrevealed and whether the phrase ?for ten points?
hasappeared.6Guess An additional feature that we used to repre-sent the state space is the current guess of the contentmodel; i.e., the argmax of the posterior.Posterior The posterior feature (Pos for short) cap-tures the shape of the posterior distribution: the prob-ability of the current guess (the max of the poste-rior), the difference between the top two probabilitiesand the probabilities associated with the fifteen mostprobable labels under the posterior.Change As features are revealed, there is oftena rapid transition from a state of confusion?whenthere are many candidates with no clear best choice?to a state of clarity with the posterior pointing to onlyone probable label.
To capture when this happens,we add a binary feature to reflect when the best guesshas changed when a single feature has been revealed.Other Features We thought that other featureswould be useful.
While useful on their own, nofeatures that we tried were useful when the contentmodel?s posterior was also used as a feature.
Fea-tures that we attempted to use were: a logistic re-gression model attempting to capture the probabilitythat any player would answer (Silver et al 2008), aregression predicting how many individuals wouldbuzz in the next n words, the year the question waswritten, the category of the question, etc.5.3 Na?
?ve Content ModelThe action space is only deciding when to answer,having abdicated responsibility for what to answer.So where does do the answers come from?
We as-sume that at any point we can ask ?what is the highestprobability label given my current feature observa-tions??
We call the component of our model thatanswers this question the content model.Our first content model is a na?
?ve Bayesmodel (Lewis, 1998) trained over a text collection.This generative model assumes labels for questionscome from a multinomial distribution ?
?
Dir(?
)6The phrase ?for ten points?
(abbreviated FTP) appears inall quiz bowl questions to signal the question?s last sentence orclause.
It is a signal to answer soon, as the final ?giveaway?
clueis next.and assumes that label l has a word distribution?l ?
Dir(?).
Each question n has a label zn andits words are generated from ?zn .
Given labeled ob-servations, we use the maximum a posteriori (MAP)estimate of ?l.Why use a generative model when a discriminativeclassifier could use a richer feature space?
The mostimportant reason is that, by definition, it makes senseto ask a generative model the probability of a labelgiven a partial observation; such a question is notwell-formed for discriminative models, which expecta complete feature set.
Another important consid-eration is that generative models can predict future,unrevealed features (Chai et al 2004); however, wedo not make use of that capability here.In addition to providing our answers, the contentmodel also provides an additional, critically impor-tant feature for our state space: its posterior (posfor short) probability.
With every revealed feature,the content model updates its posterior distributionover labels given that t tokens have been revealed inquestion n,p(zn |w1 .
.
.
wt, ?,?).
(2)To train our na?
?ve Bayes model, we semi-automatically associate labels with a Wikipedia page(correcting mistakes manually) and then form theMAP estimate of the class multinomial distributionfrom the Wikipedia page?s text.
We did this for the1065 labels that had at least three human answers,excluding ambiguous labels associated with multipleconcepts (e.g., ?europa?, ?steppenwolf?, ?georgia?,?paris?, and ?v?
).Features were taken to be the 25,000 most frequenttokens and bigrams7 that were not stop words; fea-tures were extracted from the Wikipedia text in thesame manner as from the question tokens.8After demonstrating our ability to learn an incre-mental classifier using this simple content model, weextend the content model to capture local context andcorrelations between similar labels in Section 7.7We used NLTK (Loper and Bird, 2002) to filter stop wordsand we used a ?2 test to identify bigrams with that rejected thenull hypothesis at the 0.01 level.8The Dirichlet scaling parameter ?
was set to 10,000 givenour relatively large vocabulary (25,000) and to not penalize alabel?s posterior probability if there were unseen features; thiscorresponds to a pseudocount of 0.4. ?
was set to 1.0.12956 Pitting the Algorithm Against HumansWith a state space and a policy, we now have all thenecessary ingredients to have our algorithm competeagainst humans.
Classification, which allows us tolearn a policy, was done using the default settings ofLIBLINEAR (Fan et al 2008).
To determine wherethe algorithm buzzes, we provide a sequence of statespaces until the policy classifier determines that it istime to buzz.We simulate competition by taking the human an-swers and buzzes as a given and ask our algorithm(independently) to provide its decision on when tobuzz on a test set.
We compare the two buzz positions.If the algorithm buzzed earlier with the right answer,we consider it to have ?won?
the question; equiva-lently, if the algorithm buzzed later, we consider it tohave ?lost?
that question.
Ties are rare (less than 1%of cases), as the algorithm had significantly differentbehavior from humans; in the case where there was atie, ties were broken in favor of the machine.Because we have a large, diverse population an-swering questions, we need aggregate measures ofhuman performance to get a comprehensive view ofalgorithm performance.
We use the following metricsfor each question in the test set:?
best: the earliest anyone buzzed correctly?
median: the first buzz after 50% of human buzzes?
mean: for each recorded buzz compute a reward andwe average over all rewardsWe compare the algorithm against baseline strategies:?
rap The rapacious strategy waits until the end of thequestion and answers the best answer possible.?
ftp Waiting until when ?for 10 points?
is said, thengiving the best answer possible.?
indexn Waiting until the first feature after the nth to-ken has been processed, then giving the best answerpossible.
The indices were chosen as the quartilesfor question length (by convention, most questionsare of similar length).We compare these baselines against policies that de-cide when to buzz based on the state.Recall that the non-oracle algorithms were un-aware of the true reward function.
To best simulateconventional quiz bowl settings, a correct answerwas +10 and the incorrect answer was ?5.
The fullpayoff matrix for the computer is shown in Table 2.Cases where the opponent buzzes first but is wrongare equivalent to rapacious classification, as there isno longer any incentive to answer early.
Thus weexclude such situations (Outcomes 3, 5, 6 in Table 2)from the dataset to focus on the challenge of process-ing clues incrementally.Computer Human Payoff1 first and wrong right ?152 ?
first and correct ?103 first and wrong wrong ?54 first and correct ?
+105 wrong first and wrong +56 right first and wrong +15Table 2: Payoff matrix (from the computer?s perspective)for when agents ?buzz?
during a question.
To focus onincremental classification, we exclude instances where thehuman interrupts with an incorrect answer, as after anopponent eliminates themselves, the answering reduces torapacious classification.Table 3 shows the algorithm did much better whenit had access to the posterior.
While incrementalalgorithms outperform rapacious baselines, they loseto humans.
Against the median and average players,they lose between three and four points per question,and nearly twice that against the best players.Although the content model is simple, this poorperformance is not from the content model neverproducing the correct answer.
To see this, we alsocomputed the optimal actions that could be executed.We called this strategy the oracle strategy; it was ableto consistently win against its opponents.
Thus, whilethe content model was able to come up with correctanswers often enough to on average win against oppo-nents (even the best human players), we were unableto consistently learn winning policies.There are two ways to solve this problem: createdeeper, more nuanced policies (or the features thatfeed into them) or refine content models that providethe signal needed for our policies to make sounddecisions.
We chose to refine the content model, aswe felt we had added all of the obvious features forlearning effective policies.7 Expanding the Content ModelWhen we asked quiz bowlers how they answer ques-tions, they said that they first determine the category1296Strategy Features Mean Best Median IndexClassifytext -8.72 -10.04 -6.50 40.36+guess -5.71 -8.40 -3.95 66.02+pos -4.13 -7.56 -2.70 67.97+change -4.02 -7.41 -2.63 77.33Oracle text 3.36 0.61 4.35 49.90all -6.61 -9.03 -4.42 100.19ftp -5.22 -8.62 -4.23 88.65Rapacious index30 -7.89 -8.71 -6.41 32.23Baseline index60 -5.16 -7.56 -3.71 61.90index90 -5.02 -8.62 -3.50 87.13Table 3: Performance of strategies against users.
Thehuman scoring columns show the average points per ques-tion (positive means winning on average, negative meanslosing on average) that the algorithm would expect to ac-cumulate per question versus each human amalgam metric.The index column notes the average index of the tokenwhen the strategy chose to buzz.of a question, which substantially narrows the an-swer space.
Ideally, the content model should con-duct the same calculus?if a question seems to beabout mathematics, all answers related with mathe-matics should be more likely in the posterior.
Thiswas consistent with our error analysis; many errorswere non-sensical (e.g., answering ?entropy?
for ?Jo-hannes Brahms?, when an answer such as ?RobertSchumann?, another composer, would be better).In addition, assuming independence between fea-tures given a label causes us to ignore potentiallyinformative multiword expressions such as quota-tions, titles, or dates.
Adding a language model toour content model allows us to capture some of thesephenomena.To create a model that jointly models categoriesand local context, we propose the following model:1.
Draw a distribution over labels ?
?
Dir(?)2.
Draw a background distribution over words ?0 ?Dir(?0~1)(a) For each category c of questions, draw a distributionover words ?c ?
Dir(?1?0).i.
For each label l in category c, draw a distribu-tion over words ?l,c ?
Dir(?2?c)A.
For each type v, draw a bigram distribution?l,c,v ?
Dir(?3?l,c)3.
Draw a distribution over labels ?
?
Dir(?).4.
For each question with category c and N words, drawanswer l ?
Mult(?
):(a) Assume w0 ?
START(b) Draw wn ?
Mult(?l,c,wn?1) for n ?
{1 .
.
.
N}This creates a language model over categories, la-bels, and observed words (we use ?words?
loosely, asbigrams replace some word pairs).
By constructingthe word distributions using hierarchical distributionsbased on domain and ngrams (a much simpler para-metric version of more elaborate methods (Wood andTeh, 2009)), we can share statistical strength acrossrelated contexts.
We assume that labels are (only)associated with their majority category as seen in ourtraining data and that category assignments are ob-served.
All scaling parameters ?
were set to 10,000,?
was 1.0, and the vocabulary was still 25,000.We used the maximal seating assignment (Wallach,2008) for propagating counts through the Dirichlethierarchy.
Thus, if the word v appeared Bl,u,v timesin label l following a preceding word u, Sl,v times inlabel l, Tc,v times in category c, andGv times in total,we estimate the probability of a word v appearingin label k, category t, and after word u as p(wn =v | lab = l, cat = c, wn?1 = u;~?)
=Bl,u,v + ?3Sl,v+?2Tc,v+?1Gv+?0/VG?+?0Tc,?+?2Sl,?+?2Bl,u,?
+ ?3, (3)where we use ?
to represent marginalization, e.g.Tc,?
=?v?
Tc,v?
.
As with na?
?ve Bayes, Bayes?
ruleprovides posterior label probabilities (Equation 2).We compare the na?
?ve model with models thatcapture more of the content in the text in Table 4;these results also include intermediate models be-tween na?
?ve Bayes and the full content model: ?cat?
(omit 2.a.i.A) and ?bigram?
(omit 2.a).
These modelsperform much better than the na?
?ve Bayes modelsseen in Table 3.
They are about even against themean and median players and lose four points perquestion against top players.7.1 Qualitative AnalysisIn this section, we explore what defects are prevent-ing the model presented here from competing withtop players, exposing challenges in reinforcementlearning, interpreting pragmatic cues, and large data.Three examples of failures of the model are in Fig-ure 5.
This model is the best performing model ofthe previous section.Too Slow The first example is a question on Mau-rice Ravel, a French composer known for Bole?ro.
Thequestion leads off with Ravel?s orchestral version of1297Strategy Model Mean Best Median IndexClassifyna?
?ve -4.02 -7.41 -2.63 77.33cat -1.69 -5.22 0.12 67.97bigram -3.80 -7.66 -2.51 78.69bgrm+cat -0.86 -4.46 0.83 63.42Oraclenaive 3.36 0.61 4.35 49.90cat 4.48 1.64 5.47 47.88bigram 3.58 0.87 4.61 49.34bgrm+cat 4.67 1.99 5.74 46.49Table 4: As in Table 3, performance of strategies againstusers, but with enhanced content models.
Modeling bothbigrams and label categories improves overall perfor-mance.Mussorgsky?s piano piece ?Pictures at an Exhibition?.Based on that evidence, the algorithm considers ?Pic-tures at an Exhibition?
the most likely but does notyet buzz.
When it receives enough information to besure about the correct answer, over half of the playershad already buzzed.
Correcting this problem wouldrequire a more aggressive strategy, perhaps incorpo-rating the identity of the opponent or estimating thedifficulty of the question.Mislead by the Content Model The second ex-ample is a question on Enrico Fermi, an Italian-American physicist.
The first clues are about mag-netic fields near a Fermi surface, which causes thecontent model to view ?magnetic field?
as the mostlikely answer.
The question?s text, however, haspragmatic cues ?this man?
and ?this Italian?
whichwould have ruled out the abstract answer ?magneticfield?.
Correcting this would require a model thatjointly models content and bigrams (Hardisty etal., 2010), has a coreference system as its contentmodel (Haghighi and Klein, 2007), or determines thecorrect question type (Moldovan et al 2000).Insufficient Data The third example is where ourapproach had no chance.
The question is a very diffi-cult question about George Washington, America?sfirst president.
As a sign of its difficulty, only halfthe players answered correctly, and only near the endof the question.
The question concerns lesser knownepisodes from Washington?s life, including a mistresscaught in the elements.
To the content model, of theseveral hypotheses it considers, the closest matchit can find is ?Yasunari Kawabata?, who wrote thenovel Snow Country, whose plot matches some ofthese keywords.
To answer these types of question,george washingtonTokens Revealed0.00.20.40.60.80 10 20 30 40 50 60maurice ravelTokens Revealed0.00.20.40.60.81.00 10 20 30 40 50 60 70enrico fermiTokens Revealed0.00.20.40.60.81.00 20 40 60 80 100charlemagne yasunari kawabatageneralschillmistresslanguagemagnetic field neutrinomagnetic fieldmagneticparadoxzerothis_manthis_italianpicturesorchestratedpictures at an exhibitionmaurice ravelthis_frenchcomposerboleroPredictionanswerObservationfeatureBuzzPosterior OpponentFigure 5: Three questions where our algorithm performedpoorly.
It gets ?Maurice Ravel?
(top) right but only afterover half the humans had answered correctly (i.e., thebuzz?s hexagon appears when the cyan line is above 0.6);on ?Enrico Fermi?
(middle) it confuses the correct typeof answer (person vs. concept); on ?George Washington?
(bottom) it lacks information to answer correctly.
Linesrepresent the current estimate posterior probability of theanswer (red) and the proportion of opponents who haveanswered the question correctly (cyan).
The label of eachof the three questions is above each chart.
Words are inblack with arrows and arrows, and the current argmaxanswer is at the bottom of the graph in red.
The buzzlocation is the hexagon.1298the repository used to train the content model wouldhave to be orders of magnitude larger to be able tolink the disparate clues in the question to a consistenttarget.
The content model would also benefit fromweighting later (more informative) features higher.7.2 AssumptionsWe have made assumptions to solve a problem thatis subtly different that the game of quiz bowl thata human would play.
Some of these were simpli-fying assumptions, such as our assumption that thealgorithm has a closed set of possible answers (Sec-tion 5.3).
Even with this advantage, the algorithm isunable to compete with human players, who chooseanswers from an unbounded set.
On the other hand,to focus on incremental classification, we idealizedour human opponents so that they never give incor-rect answers (Section 6).
This causes our estimatesof our performance to be lower than they would beagainst real players.8 Conclusion and Future WorkWe make three contributions.
First, we introduce anew setting for exploring the problem of incrementalclassification: trivia games.
This problem is intrin-sically interesting because of its varied topics andcompetitive elements, has a great quantity of stan-dardized, machine-readable data, and also has theboon of being cheaply and easily annotated.
We tookadvantage of that ease and created a framework forquickly and efficiently gathering examples of humansdoing incremental classification.There are other potential uses for the dataset; theprogression of clues from obscure nuggets to couldhelp determine how ?known?
a particular aspect ofan entity is (e.g., that William Jennings Bryant gavethe ?Cross of Gold?
speech is better known his resig-nation after the Lusitania sinking, Figure 1).
Whichcould be used in educational settings (Smith et al2008) or summarization (Das and Martins, 2007).The second contribution shows that humans?
incre-mental classification improves state-of-the-art rapa-cious classification algorithms.
While other frame-works (Zaidan et al 2008) have been proposed toincorporate user clues about features, the system de-scribed here provides analogous features without theneed for explicit post-hoc reflection, has faster anno-tation throughput, and is much cheaper.The problem of answering quiz bowl questions isitself a challenging task that combines issues fromlanguage modeling, large data, coreference, and re-inforcement learning.
While we do not address allof these problems, our third contribution is a sys-tem that learns a policy in a MDP for incrementalclassification even in very large state spaces; it cansuccessfully compete with skilled human players.Incorporating richer content models is one of ournext steps.
This would allow us to move beyond theclosed-set model and use a more general coreferencemodel (Haghighi and Klein, 2007) for identifyinganswers and broader corpora for training.
In addi-tion, using larger corpora would allow us to havemore comprehensive doubly-hierarchical languagemodels (Wood and Teh, 2009).
We are also inter-ested in adding richer models of opponents to thestate space that would adaptively adjust strategies asit learned more about the strengths and weaknessesof its opponent (Waugh et al 2011).Further afield, our presentation of sentencesclosely resembles paradigms for cognitive experi-ments in linguistics (Thibadeau et al 1982) but aremuch cheaper to conduct.
If online processing ef-fects (Levy et al 2008; Levy, 2011) could be ob-served in buzzing behavior; e.g., if a confusinglyworded phrase depresses buzzing probability, it couldhelp validate cognitively-inspired models of onlinesentence processing.Incremental classification is a natural problem,both for humans and resource-limited machines.While our data set is trivial (in a good sense), learn-ing how humans process data and make decisionsin a cheap, easy crowdsourced application can helpus apply new algorithms to improve performance insettings where features aren?t free, either because ofcomputational or annotation cost.1299AcknowledgmentsWe thank the many players who played our onlinequiz bowl to provide our data (and hopefully had fundoing so) and Carlo Angiuli, Arnav Moudgil, andJerry Vinokurov for providing access to quiz bowlquestions.
This research was supported by NSF grant#1018625.
Jordan Boyd-Graber is also supported bythe Army Research Laboratory through ARL Cooper-ative Agreement W911NF-09-2-0072.
Any opinions,findings, conclusions, or recommendations expressedare the authors?
and do not necessarily reflect thoseof the sponsors.ReferencesPieter Abbeel and Andrew Y. Ng.
2004.
Apprentice-ship learning via inverse reinforcement learning.
InProceedings of International Conference of MachineLearning.J.
Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,A.
Kulesza, A. Sanchis, and N. Ueffing.
2004.
Confi-dence estimation for machine translation.
In Proceed-ings of the Association for Computational Linguistics.Mark Boddy and Thomas L. Dean.
1989.
Solving time-dependent planning problems.
In International JointConference on Artificial Intelligence, pages 979?984.Morgan Kaufmann Publishers, August.Nicolo` Cesa-Bianchi, Shai Shalev-Shwartz, and OhadShamir.
2011.
Efficient learning with partially ob-served attributes.
Journal of Machine Learning Re-search, 12:2857?2878.Xiaoyong Chai, Lin Deng, Qiang Yang, and Charles X.Ling.
2004.
Test-cost sensitive naive bayes classi-fication.
In IEEE International Conference on DataMining.Dipanjan Das and Andre Martins.
2007.
A survey onautomatic text summarization.
Engineering and Tech-nology, 4:192?195.Hal Daume?
III.
2004.
Notes on CG and LM-BFGS optimization of logistic regression.
Pa-per available at http://pub.hal3.name/?daume04cg-bfgs, implementation available athttp://hal3.name/megam/.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.David Ferrucci, Eric Brown, Jennifer Chu-Carroll, JamesFan, David Gondek, Aditya A. Kalyanpur, Adam Lally,J.
William Murdock, Eric Nyberg, John Prager, NicoSchlaefer, and Chris Welty.
2010.
Building Watson:An Overview of the DeepQA Project.
AI Magazine,31(3).Aria Haghighi and Dan Klein.
2007.
Unsupervised coref-erence resolution in a nonparametric bayesian model.In Proceedings of the Association for ComputationalLinguistics.Eric Hardisty, Jordan Boyd-Graber, and Philip Resnik.2010.
Modeling perspective using adaptor grammars.In Proceedings of Emperical Methods in Natural Lan-guage Processing.Michael C. Horsch and David Poole.
1998.
An anytimealgorithm for decision making under uncertainty.
InProceedings of Uncertainty in Artificial Intelligence.Ken Jennings.
2006.
Brainiac: adventures in the curious,competitive, compulsive world of trivia buffs.
Villard.Shihao Ji and Lawrence Carin.
2007.
Cost-sensitive fea-ture acquisition and classification.
Pattern Recognition,40:1474?1485, May.Shyong K. Lam, David M. Pennock, Dan Cosley, andSteve Lawrence.
2003.
1 billion pages = 1 milliondollars?
mining the web to play ?who wants to be a mil-lionaire??.
In Proceedings of Uncertainty in ArtificialIntelligence.John Langford and Bianca Zadrozny.
2005.
Relatingreinforcement learning performance to classificationperformance.
In Proceedings of International Confer-ence of Machine Learning.Roger P. Levy, Florencia Reali, and Thomas L. Griffiths.2008.
Modeling the effects of memory on human on-line sentence processing with particle filters.
In Pro-ceedings of Advances in Neural Information ProcessingSystems.Roger Levy.
2011.
Integrating surprisal and uncertain-input models in online sentence comprehension: formaltechniques and empirical results.
In Proceedings of theAssociation for Computational Linguistics.David D. Lewis.
1998.
Naive (Bayes) at forty: The inde-pendence assumption in information retrieval.
In ClaireNe?dellec and Ce?line Rouveirol, editors, Proceedingsof European Conference of Machine Learning, number1398.Michael L. Littman, Greg A. Keim, and Noam Shazeer.2002.
A probabilistic approach to solving crosswordpuzzles.
Artif.
Intell., 134(1-2):23?55, January.Edward Loper and Steven Bird.
2002.
NLTK: the natu-ral language toolkit.
In Tools and methodologies forteaching.Prem Melville, Maytal Saar-Tsechansky, Foster Provost,and Raymond J. Mooney.
2005.
An expected utilityapproach to active feature-value acquisition.
In Inter-national Conference on Data Mining, November.Dan Moldovan, Sanda Harabagiu, Marius Pasca, RadaMihalcea, Roxana Girju, Richard Goodrum, and Vasile1300Rus.
2000.
The structure and performance of an open-domain question answering system.
In Proceedings ofthe Association for Computational Linguistics.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Comput.
Linguist.,34(4):513?553, December.Jay Pujara, Hal Daume III, and Lise Getoor.
2011.
Usingclassifier cascades for scalable e-mail classification.In Collaboration, Electronic Messaging, Anti-Abuseand Spam Conference, ACM International ConferenceProceedings Series.Sebastian Riedel, David McClosky, Mihai Surdeanu, An-drew McCallum, and Christopher D. Manning.
2011.Model combination for event extraction in bionlp 2011.In Proceedings of the BioNLP Workshop.Afshin Rostamizadeh, Alekh Agarwal, and Peter L.Bartlett.
2011.
Learning with missing features.
InProceedings of Uncertainty in Artificial Intelligence.Maytal Saar-Tsechansky and Foster Provost.
2007.
Han-dling missing values when applying classification mod-els.
Journal of Machine Learning Research, 8:1623?1657, December.Gerard.
Salton.
1968.
Automatic Information Organiza-tion and Retrieval.
McGraw Hill Text.Burr Settles.
2011.
Closing the loop: Fast, interactivesemi-supervised annotation with queries on featuresand instances.
In Proceedings of Emperical Methodsin Natural Language Processing.David Silver, Richard S. Sutton, and Martin Mu?ller.
2008.Sample-based learning and search with permanent andtransient memories.
In International Conference onMachine Learning.Noah A. Smith, Michael Heilman, and Rebecca Hwa.2008.
Question generation as a competitive under-graduate course project.
In Proceedings of the NSFWorkshop on the Question Generation Shared Task andEvaluation Challenge.Umar Syed, Michael Bowling, and Robert E. Schapire.2008.
Apprenticeship learning using linear program-ming.
In Proceedings of International Conference ofMachine Learning.Gerald Tesauro and Gregory R. Galperin.
1996.
On-linepolicy improvement using monte-carlo search.
In Pro-ceedings of Advances in Neural Information ProcessingSystems.Robert Thibadeau, Marcel A.
Just, and Patricia A. Carpen-ter.
1982.
A model of the time course and content ofreading.
Cognitive Science, 6.Hanna M Wallach.
2008.
Structured Topic Models forLanguage.
Ph.D. thesis, University of Cambridge.Lidan Wang, Donald Metzler, and Jimmy Lin.
2010.Ranking Under Temporal Constraints.
In Proceedingsof the ACM International Conference on Informationand Knowledge Management.Kevin Waugh, Brian D. Ziebart, and J. Andrew Bagnell.2011.
Computational rationalization: The inverse equi-librium problem.
In Proceedings of International Con-ference of Machine Learning.F.
Wood and Y. W. Teh.
2009.
A hierarchical nonpara-metric Bayesian approach to statistical language modeldomain adaptation.
In Proceedings of Artificial Intelli-gence and Statistics.Omar F. Zaidan, Jason Eisner, and Christine Piatko.
2008.Machine learning with annotator rationales to reduceannotation cost.
In Proceedings of the NIPS*2008Workshop on Cost Sensitive Learning.Valentina Bayer Zubek and Thomas G. Dietterich.
2002.Pruning improves heuristic search for cost-sensitivelearning.
In International Conference on MachineLearning.1301
