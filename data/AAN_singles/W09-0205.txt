Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 33?40,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsBagPack: A general framework to represent semantic relationsAma?
Herdag?delenCIMEC, University of TrentoRovereto, Italyamac@herdagdelen.comMarco BaroniCIMEC, University of TrentoRovereto, Italymarco.baroni@unitn.itAbstractWe introduce a way to represent word pairsinstantiating arbitrary semantic relations thatkeeps track of the contexts in which the wordsin the pair occur both together and indepen-dently.
The resulting features are of sufficientgenerality to allow us, with the help of a stan-dard supervised machine learning algorithm,to tackle a variety of unrelated semantic taskswith good results and almost no task-specifictailoring.1 IntroductionCo-occurrence statistics extracted from corpora leadto good performance on a wide range of tasks thatinvolve the identification of the semantic relation be-tween two words or concepts (Sahlgren, 2006; Turney,2006).
However, the difficulty of such tasks and thefact that they are apparently unrelated has led to thedevelopment of largely ad-hoc solutions, tuned to spe-cific challenges.
For many practical applications, this isa drawback: Given the large number of semantic rela-tions that might be relevant to one or the other task, weneed a multi-purpose approach that, given an appropri-ate representation and training examples instantiatingan arbitrary target relation, can automatically mine newpairs characterized by the same relation.
Building on arecent proposal in this direction by Turney (2008), wepropose a generic method of this sort, and we test iton a set of unrelated tasks, reporting good performanceacross the board with very little task-specific tweaking.There has been much previous work on corpus-basedmodels to extract broad classes of related words.
Theliterature on word space models (Sahlgren, 2006) hasfocused on taxonomic similarity (synonyms, antonyms,co-hyponyms.
.
. )
and general association (e.g., find-ing topically related words), exploiting the idea thattaxonomically or associated words will tend to occurin similar contexts, and thus share a vector of co-occurring words.
The literature on relational similar-ity, on the other hand, has focused on pairs of words,devising various methods to compare how similar thecontexts in which target pairs appear are to the contextsof other pairs that instantiate a relation of interest (Tur-ney, 2006; Pantel and Pennacchiotti, 2006).
Beyondthese domains, purely corpus-based methods play anincreasingly important role in modeling constraints oncomposition of words, in particular verbal selectionalpreferences ?
finding out that, say, children are morelikely to eat than apples, whereas the latter are morelikely to be eaten (Erk, 2007; Pad?
et al, 2007).
Tasksof this sort differ from relation extraction in that weneed to capture productive patterns: we want to findout that shabu shabu (a Japanese meat dish) is eatenwhereas ink is not, even if in our corpus neither noun isattested in proximity to forms of the verb to eat.Turney (2008) is the first, to the best of our knowl-edge, to raise the issue of a unified approach.
In par-ticular, he treats synonymy and association as specialcases of relational similarity: in the same way in whichwe might be able to tell that hands and arms are ina part-of relation by comparing the contexts in whichthey co-occur to the contexts of known part-of pairs,we can guess that cars and automobiles are synonymsby comparing the contexts in which they co-occur tothe contexts linking known synonym pairs.Here, we build on Turney?s work, adding two mainmethodological innovations that allow us further gen-eralization.
First, merging classic approaches to taxo-nomic and relational similarity, we represent conceptpairs by a vector that concatenates information aboutthe contexts in which the two words occur indepen-dently, and the contexts in which they co-occur (Mirkinet al 2006 also integrate information from the lexi-cal patterns in which two words co-occur and simi-larity of the contexts in which each word occurs onits own, to improve performance in lexical entailmentacquisition).
Second, we represent contexts as bag ofwords and bigrams, rather than strings of words (?pat-terns?)
of arbitrary length: we leave it to the machinelearning algorithm to zero in on the most interestingwords/bigrams.Thanks to the concatenated vector, we can tackletasks in which the two words are not expected toco-occur even in very large corpora (such as selec-tional preference).
Concatenation, together with un-igram/bigram representation of context, allows us toscale down the approach to smaller training corpora(Turney used a corpus of more than 50 billion words),since we do not need to see the words directly co-occurring, and the unigram/bigram dimensions of the33vectors are less sparse than dimensions based on longerstrings of words.
We show that our method producesreasonable results also on a corpus of 2 billion words,with many unseen pairs.
Moreover, our bigram andunigram representation is general enough that we donot need to extract separate statistics nor perform ad-hoc feature selection for each task: we build the co-occurrence matrix once, and use the same matrix in allexperiments.
The bag-of-words assumption also makesfor faster and more compact model building, since thenumber of features we extract from a context is linearin the number of words in the context, whereas it is ex-ponential for Turney.
On the other hand, our methodis currently lagging behind Turney?s in terms of perfor-mance, suggesting that at least some task-specific tun-ing will be necessary.Following Turney, we focus on devising a suitablygeneral featural representation, and we see the spe-cific machine learning algorithm employed to performthe various tasks as a parameter.
Here, we use Sup-port Vector Machines since they are a particularly ef-fective general-purpose method.
In terms of empiricalevaluation of the model, besides experimenting withthe ?classic?
SAT and TOEFL datasets, we show howour algorithm can tackle the selectional preference taskproposed in Pad?
(2007) ?
a regression task ?
and weintroduce to the corpus-based semantics community achallenge from the ConceptNet repository of common-sense knowledge (extending such repository by auto-mated means is the original motivation of our project).In the next section, we will present our proposedmethod along with the corpora and model parameterchoices used in the implementation.
In Section 3, wedescribe the tasks that we use to evaluate the model.Results are reported in Section 4 and we conclude inSection 5, with a brief overview of the contributions ofthis paper.2 Methodology2.1 ModelThe central idea in BagPack (Bag-of-words represen-tation of Paired concept knowledge) is to construct avector-based representation of a pair of words in such away that the vector represents both the contexts wherethe two words co-occur and the contexts where the sin-gle words occur on their own.
A straightforward ap-proach is to construct three different sub-vectors, onefor the first word, one for the second word, and one forthe co-occurring pair.
The concatenation of these threesub-vectors is the final vector that represents the pair.This approach provides us a graceful fall back mech-anism in case of data scarcity.
Even if the two words arenot observed co-occurring in the corpus ?
no syntag-maic information about the pair ?, the correspondingvector will still represent the individual contexts wherethe words are observed on their own.
Our hypothesis(and hope) is that this information will be representa-tive of the semantic relation between the pair, in thesense that, given pairs characterized by same relation,there should be paradigmatic similarity across the first,resp.
second elements of the pairs (e.g., if the relationis between professionals and the typical tool of theirtrade, it is reasonable to expect that that both profes-sionals and tools will tend to share similar contexts).Before going into further details, we need to describewhat a ?co-occurrence?
precisely means, define the no-tion of context, and determine how to structure our vec-tor.
For a single word W , the following pseudo regularexpression identifies an observation of occurrence:?C W D?
(1)where C and D can be empty strings or concatena-tions of up to 4 words separated by whitespace (i.e.C1, .
.
.
, Ci and D1, .
.
.
, Dj where i, j ?
4).
Each ob-servation of this pattern constitutes a single context ofW .
The pattern is matched with the longest possiblesubstring without crossing sentence boundaries.Let (W1,W2) denote an ordered pair of words W1and W2.
We say the two words occur as a pair when-ever one of the following pseudo regular expressions isobserved in the corpus:?C W1 DW2 E?
(2)?C W2 D W1 E?
(3)where C and E can be empty strings or concatena-tions of up to 2 words and similarly, D can be ei-ther an empty string or concatenation of up to 5 words(i.e.
C1, .
.
.
, Ci, D1, .
.
.
, Dj , and E1, .
.
.
, Ek wherei, j ?
2 and k ?
5).
Together, patterns 2 and 3 con-stitute the pair context for W1 and W2.
The pattern ismatched with the longest possible substring while mak-ing sure that D does not contain neither W1 nor W2.The number of context words allowed before, after,and between the targets are actually model parametersbut for the experiments reported in this study, we usedthe aforementioned values with no attempt at tuning.The vector representing (W1,W2) is a concatenationv1v2v1,2, where, the sub-vectors v1 and v2 are con-structed by using the single contexts of W1 and W2correspondingly (i.e.
by pattern 1) and the sub-vectorv1,2 is built by using the pair contexts identified bythe patterns 2 and 3.
We refer to the components assingle-occurrence vectors and pair-occurrence vectorrespectively.The population of BagPack starts by identifying theb most frequent unigrams and the b most frequent bi-grams as basis terms.
Let T denote a basis term.
Forthe construction of v1, we create two features for eachterm T : tpre corresponds to the number of observationsof T in the single contexts of W1 occurring before W1and tpost corresponds to the number of observations ofT in the single occurrence of W1 where T occurs afterW1 (i.e.
number of observations of the pattern 1 whereT ?
C and T ?
D correspondingly).
The construc-tion of v2 is identical except that this time the features34correspond to the number of times the basis term is ob-served before and after the target word W2 in singlecontexts.
The construction of the pair-occurrence sub-vector v1,2 proceeds in a similar fashion but in addi-tion, we incorporate also the order of W1 and W2 asthey co-occur in the pair context: The number of ob-servations of the pair contexts where W1 occurs beforeW2 and T precedes (follows) the pair, are representedby feature t+pre (t+post).
The number of cases wherethe basis term is in between the target words is repre-sented by t+betw.
The number of cases where W2 oc-curs before W1 and T precedes the pair is representedby the feature t?pre.
Similarly the number of caseswhere T follows (is in between) the pair is representedby the feature t?post (t?betw).Assume that the words "only" and "that" are our ba-sis terms and consider the following context for theword pair ("cat", "lion"): "Lion is the only cat thatlives in large social groups."
The observation of the ba-sis terms should contribute to the pair-occurrence sub-vector v1,2 and since the target words occur in reverseorder, this context results in the incrementation of thefeatures only?betw and that?post by one.To sum up, we have 2b basis terms (b unigrams andb bigrams).
Each of the single-occurrence sub-vectorsv1 and v2 consists of 4b features: Each basis termgives rise to 2 features incorporating the relative posi-tion of basis term with respect to the single word.
Thepair-occurrence sub-vector, v1,2, consists of 12b fea-tures: Each basis term gives rise to 6 new features; ?3for possible relative positions of the basis term with re-spect to the pair and ?2 for the order of the words.Importantly, the 2b basis terms are picked only once,and the overall co-occurrence matrix is built once andfor all for all the tasks: unlike Turney, we do not needto go back to the corpus to pick basis terms and collectseparate statistics for different tasks.The specifics of the adaptation to each task will bedetailed in Section 3.
For the moment, it should sufficeto note that the vectors v1 and v2 represent the con-texts in which the two words occur on their own, thusencode paradigmatic information.
However, v1,2 rep-resents the contexts in which the two words co-occur,thus encode sytagmatic information.The model training and evaluation is done in a 10-fold cross-validation setting whenever applicable.
Thereported performance measures are the averages overall folds and the confidence intervals are calculated byusing the distribution of fold-specific results.
The onlyexception to this setting is the SAT analogy questionstask simply because we consider each question as aseparate mini dataset as described in Section 3.2.2 Source CorporaWe carried out our tests on two different corpora:ukWaC, a Web-derived, POS-tagged and lemmatizedcollection of about 2 billion tokens,1 and the Yahoo!1http://wacky.sslmit.unibo.itdatabase queried via the BOSS service.2 We will referto these corpora as ukWaC and Yahoo from now on.In ukWaC, we limited the number of occurrence andco-occurrence queries to the first 5000 observationsfor computational efficiency.
Since we collect cor-pus statistics at the lemma level, we construct Yahoo!queries using disjunctions of inflected forms that wereautomatically generated with the NodeBox Linguisticslibrary.3 For example, the query to look for ?lion?
and?cat?
with 4 words in the middle is: ?
(lion OR lions) ** * * (cat OR cats OR catting OR catted)?.
Each pairrequires 14 Yahoo!
queries (one for W1, one for W2,6 for (W1,W2), in that order, with 0-to-5 interveningwords, 6 analogous queries for (W2,W1)).
Yahoo!
re-turns maximally 1,000 snippets per query, and the latterare lemmatized with the TreeTagger4 before feature ex-traction.2.3 Model implementationWe did not carry out a search for ?good?
parameter val-ues.
Instead, the model parameters are generally pickedat convenience to ease memory requirements and com-putational efficiency.
For instance, in all experiments,b is set to 1500 unless noted otherwise in order to fitthe vectors of all pairs at our hand into the computermemory.Once we construct the vectors for a set of word pairs,we get a co-occurrence matrix with pairs on the rowsand the features on the columns.
In all of our exper-iments, the same normalization method and classifi-cation algorithm is used with the default parameters:First, a TF-IDF feature weighting is applied to the co-occurrence matrix (Salton and Buckley, 1988).
Thenfollowing the suggestion of Hsu and Chang (2003),each feature t?s [??t?2?
?t, ??t+2?
?t] interval is scaled to[0, 1], trimming the exceeding values from upper andlower bounds (the symbols ?
?t and ?
?t denote the av-erage and standard deviation of the feature values re-spectively).
For the classification algorithm, we use theC-SVM classifier and for regression the -SVM regres-sor, both implemented in the Matlab toolbox of Canuet al (2005).
We employed a linear kernel.
The costparameter C is set to 1 for all experiments; for the re-gressor,  = 0.2.
For other pattern recognition relatedcoding (e.g., cross validation, scaling, etc.)
we madeuse of the Matlab PRTools (Duin, 2001).For each task that will be defined in the next section,we evaluated our algorithm on the following represen-tations: 1) Single-occurrence vectors (v1v2 condition)2) Pair-occurrence vectors (v1,2 condition) 3) Entireco-occurrence matrix (v1v2v1,2 condition).2http://developer.yahoo.com/search/boss/3http://nodebox.net/code/index.php/Linguistics4http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/353 Tasks3.1 SAT Analogy QuestionsThe first task we evaluated our algorithm on is theSAT analogy questions task introduced by Turney et al(2003).
In this task, there are 374 multiple choice ques-tions with a pair of related words like (lion,cat) as thestem and 5 other pairs as the choices.
The correct an-swer is the choice pair which has the relationship mostsimilar to that in the stem pair.We adopt a similar approach to the one used in Tur-ney (2008) and consider each question as a separate bi-nary classification problem with one positive traininginstance and 5 unknown pairs.
For a question, we picka pair at random from the stems of other questions as apseudo negative instance and train our classifier on thistwo-instance training set.
Then the trained classifier isevaluated on the choice pairs and the pair with the high-est posterior probability for the positive class is calledthe winner.
The procedure is repeated 10 times pick-ing a different pseudo-negative instance each time andthe choice pair which is selected as the winner most of-ten is taken as the answer to that question.
The perfor-mance measure on this task is defined as the percent-age of correctly answered questions.
The mean scoreand confidence intervals are calculated over the perfor-mance scores obtained for all folds.3.2 TOEFL Synonym QuestionsThis task, introduced by Landauer and Dumais (1997),consists of 80 multiple choice questions in which aword is given as the stem and the correct choice is theword which has the closest meaning to that of the stem,among 4 candidates.
To fit the task into our frame-work, we pair each choice with the stem word and ob-tain 4 word pairs for each question.
The word pairconstructed with the stem and the correct choice is la-beled as positive and the other pairs are labeled as neg-ative.
We consider all 320 pairs constructed for all 80questions as our dataset.
Thus, the problem is turnedinto a binary classification problem where the task isto discriminate the synonymous word pairs (i.e.
pos-itive class) from the other pairs (i.e.
negative class).We made sure that the pairs constructed for the samequestion were never split between training and test set,so that no question-specific learning is performed.
Thereason for this precaution is that the evaluation is doneon a per-question basis.
The estimated posterior classprobabilities of the pairs constructed for the same ques-tion are compared to each other and the pair with thehighest probability for the positive class is selected asthe answer for the question.
By keeping the pairs ofa question in the same set we make sure their posteri-ors are calculated by the same trained classifier.
Theperformance measure is the percentage of correctly an-swered questions and we report the mean performanceover all 10 folds.3.3 Selectional Preference JudgmentsLinguists have long been interested in the semanticconstraints that verbs impose on their arguments, abroad area that has also attracted computational mod-eling, with increasing interest in purely corpus-basedmethods (Erk, 2007; Pad?
et al, 2007).
This task isof particular interest to us as an example of a broaderclass of linguistic problems that involve productiveconstraints on composition.
As has been stressed atleast since Chomsky?s early work (Chomsky, 1957), nomatter how large a corpus is, if a phenomenon is pro-ductive there will always be new well-formed instancesthat are not in the corpus.
In the domain of selectionalrestrictions this is particularly obvious: we would notsay that an algorithm learned the constraints on the pos-sible objects/patients of eating simply by producing thelist of all the attested objects of this verb in a very largecorpus; the interesting issue is whether the algorithmcan detect if an unseen object is or is not a plausible?eatee?, like humans do without problems.
Specifi-cally, we test selectional preferences on the dataset con-structed by Pad?
(2007), that collects average plausi-bility judgments (from 20 speakers) for nouns as eithersubjects or objects of verbs (211 noun-verb pairs).We formulate this task as a regression problem.
Wetrain the -SVM regressor with 18-fold cross valida-tion: Since the pair instances are not independent butgrouped according to the verbs, one fold is constructedfor each of the 18 verbs used in the dataset.
In eachfold, all instances sharing the corresponding verb areleft out as the test set.
The performance measure forthis task is the Spearman correlation between the hu-man judgments and our algorithm?s estimates.
Thereare two possible ways to calculate this measure.
One isto get the overall correlation between the human judg-ments and our estimates obtained by concatenating theoutput of each cross-validation fold.
That measure al-lows us to compare our method with the previously re-ported results.
However, it cannot control for a possi-ble verb-effect on the human judgment values: If theaverage judgment values of the pairs associated with aspecific verb is significantly higher (or lower) than theaverage of the pairs associated with another verb, thenany regressor which simply learns to assign the aver-age value to all pairs associated with that verb (regard-less of whether there is a patient or agent relation be-tween the pairs) will still get a reasonably high correla-tion because of the variation of judgment scores acrossthe verbs.
To control for this effect, we also calculatedthe correlation between the human judgments and ourestimates for each verb?s plausibility values separately,and we report averages across these separate correla-tions (the ?mean?
results reported below).3.4 Common-sense Relations from ConceptNetOpen Mind Common Sense5 is an ongoing project ofacquisition of common-sense knowledge from ordinary5http://commons.media.mit.edu/en/36Relation Pairs Relation PairsIsA 316 PartOf 139UsedFor 198 LocationOf 1379CapableOf 228 Total 1943Table 1: ConceptNet relations after filtering.people by letting them carry out simple semantic andlinguistics tasks.
An end result of the project is Con-ceptNet 3, a large scale semantic network consisting ofrelations between concept pairs (Havasi et al, 2007).
Itis possible to view this network as a collection of se-mantic assertions, each of which can be represented bya triple involving two concepts and a relation betweenthem, e.g.
UsedFor(piccolo, make music).
One moti-vation for this project is the fact that common-senseknowledge is assumed to be known by both parties ina communication setting and usually is not expressedexplicitly.
Thus, corpus-based approaches may haveserious difficulties in capturing these relations (Havasiet al, 2007), but there are reasons to believe that theycould still be useful: Eslick (2006) uses the assertionsof ConceptNet as seeds to parse Web search results andaugment ConceptNet by new candidate relations.We use the ConceptNet snapshot released in June2008, containing more than 200.000 assertions witharound 20 semantic relations like UsedFor, Desirious-EffectOf, or SubEventOf.
Each assertion has a confi-dence rating based on the number of people who ex-pressed or confirmed that assertion.
For simplicity welimited ourselves to single word concepts and the re-lations between them.
Furthermore, we eliminated theassertions with a confidence score lower than 3 in anattempt to increase the "quality" of the assertions andfocused on the most populated 5 relations of the re-maining set, as given in Table 3.4.
There may be morethan one relation between a pair of concepts, so the to-tal number is less than the sum of the size of the indi-vidual relation sets.4 ResultsFor the multiple choice question tasks (i.e.
SAT andTOEFL), we say a question is complete when all of therelated pairs (stem and choice) are represented by vec-tors with at least one non-zero component.
If a ques-tion has at least one pair represented by a zero-vector(missing pairs), then we say that the question is partial.For these tasks, we report the worst-case performancescores where we assume that a random guessing per-formance is obtained on the partial questions.
This isa strict lower bound because it discards all informationwe have about a partial question even if it has only onemissing pair.
We define coverage as the percentage ofcomplete questions.4.1 SATIn Yahoo, the coverage is quite high.
In the v1,2 onlycondition, 4 questions had at least some choice/stempairs with all zero components.
In all other cases, all ofthe pairs were represented by vectors with at least onenon-zero component.
The highest score is obtained forthe v1v2v1,2 condition with a 44.1% of correct ques-tions, that is not significantly above the 42.5% perfor-mance of v1,2 (paired t-test, ?
= 0.05).
The v1v2 onlycondition results in a poorer performance of 33.9% cor-rect questions, statistically lower than the former twoconditions.For ukWaC, the v1,2 only condition provides a rel-atively low coverage.
Only 238 questions out of 374were complete.
For the other conditions, we get a com-plete coverage.
The performances are statistically in-distinguishable from each other and are 38.0%, 38.2%,and 39.6% for v1,2, v1v2, and v1v2v1,2 respectively.Condition Yahoo ukWaCv1,2 42.5% 38.0%v1v2 33.9% 38.2%v1v2v1,2 44.1% 39.6%Table 2: Percentage of correctly answered questions inSAT analogy task, worst-case scenario.In Fig.
1, the best performances we get for Yahooand ukWaC are compared to previous studies with 95%binomial confidence intervals plotted.
The reportedvalues are taken from the ACL wiki page on the state ofthe art for SAT analogy questions6.
The algorithm pro-posed by Turney (2008) is labeled as Turney-PairClass.35404550556065Percentage ofcorrect answersBagPack?ukWaCMangalath et alVeale?KNOW?BESTBicici and YuretBagPack?YahooTurney and LittmanTurney?PairClassTurney?PERTTurney?LRAFigure 1: Comparison with previous algorithms onSAT analogy questions.Overall, the performance of BagPack is not at thelevel of the state of the art but still provides a reasonablelevel even in the v1v2 only condition for which we donot utilize the contexts where the two words co-occur.This aspect is most striking for ukWaC where the cov-erage is low and by only utilizing the single-occurrencesub-vectors we obtain a performance of 38.2% cor-rect answers (the comparable ?attributional?
models re-6See http://aclweb.org/aclwiki/ for furtherinformation and references37ported in Turney, 2006, have an average performance of31%).4.2 TOEFLFor the v1,2 sub-vector calculated for Yahoo, we havetwo partial questions out of 80 and the system answers80.0% of the questions correctly.
The single occur-rence case v1v2 instead provides a correct percentageof 41.2% which is significantly above the random per-formance of 25% but still very poor.
The combinedcase v1v2v1,2 provides a score of 75.0% with no sta-tistically significant difference from the v1,2 case.
Thereason of the low performance for v1v2 is an openquestion.For ukWaC, the coverage for the v1v2 case is prettylow.
Out of 320 pairs, 70 were represented by zero-vectors, resulting in 34 partial questions out of 80.The performance is at 33.8%.
The v1v2 case on itsown does not lead to a performance better than randomguessing (27.5%) but the combined case v1v2v1,2provides the highest ukWaC score of 42.5%.Condition Yahoo ukWaCv1,2 80.0% 33.8%v1v2 41.2% 27.5%v1v2v1,2 75.0% 42.5%Table 3: Percentage of correctly answered questions inTOEFL synonym task, worst-case scenario.To our knowledge, the best performance with apurely corpus-based approach is that of Rapp (2003)who obtained a score of 92.5% with SVD.
Fig.
2 re-ports our results and a list of other corpus-based sys-tems which achieve scores higher than 70%, along with95% confidence interval values.
The results are takenfrom the ACL wiki page on the state of the art forTOEFL synonym questions.30405060708090100Percentage ofcorrect answersBagPack?ukWaCPado andLapataTurney?PMI?IRTurney?PairClassBagPack?YahooTerra and ClarkeBullinariaand LevyMatveeva et al RappFigure 2: Comparison with previous algorithms onTOEFL synonym questions with 95% confidence in-tervals.We note that our results obtained for Yahoo are com-parable to the results of Turney but even the best re-sults obtained for ukWaC and the Yahoo?s results forv1v2 only condition are very poor.
Whether this isbecause of the inability of the sub-vectors to capturesynonymity or because the default parameter values ofSVM are not adequate is an open question.
Notice thatour concatenated v1v2 vector does not exploit infor-mation about the similarity of v1 to v2, that, presum-ably, should be of great help in solving the synonymtask.4.3 Selectional PreferenceThe coverage for this dataset is quite high.
All pairswere represented by non-zero vectors for Yahoo whileonly two pairs had zero-vectors for ukWaC.
The twopairs are discarded in our experiments.
For Yahoo, thebest results are obtained for the v1,2 case.
The single-occurrence case, v1v2, provides an overall correlationof 0.36 and mean correlation of 0.26.
However low, incase of rarely co-occurring word pairs this data couldbe the only data we have in our hands and it is impor-tant that it provides reasonable judgment estimates.For the ukWaC corpus, the best results we get arean overall correlation of 0.60 and a mean correlation of0.52 for the combined case v1v2v1,2.
The results forv1,2 and v1v2v1,2 are statistically indistinguishable.Yahoo ukWaCCondition Overall Mean Overall Meanv1,2 0.60 0.45 0.58 0.48v1v2 0.36 0.26 0.33 0.22v1v2v1,2 0.55 0.42 0.60 0.52Table 4: Spearman correlations between the targets andestimations for selectional preference task.In Fig.
3, we present a comparison of our results withsome previous studies reported in Pad?
et al (2007).The best result reported so far is a correlation of 0.52.Our results for Yahoo and ukWaC are currently thehighest correlation values reported.
Even the verb-effect-controlled correlations achieve competitive per-formance.0.150.20.250.30.350.40.450.50.550.60.65SpearmancorrelationResnikBagPack?Yahoo (mean)Pado, Pado, & Erk(parsed cosine)BagPack?ukWaC(mean)Pado, Keller, & CrockerBagPack?Yahoo (overall)BagPack?ukWaC(overall)Figure 3: Comparison of algorithms on selectionalpreference task.384.4 ConceptNetOnly for this task, (because of practical memory limita-tions) we reduced the model parameter b to 500, whichmeans we used the 500 most frequent unigrams and500 most frequent bigrams as our basis terms.
For eachof the 5 relations at our hand, we trained a differentSVM classifier by labeling the pairs with the corre-sponding relation as positive and the rest as negative.To eliminate the issue of unbalanced number of nega-tive and positive instances we randomly down-sampledthe positive or negative instances set (whichever islarger).
For the IsA, UsedFor, CapableOf, and PartOfrelations, the down-sampling procedure means keep-ing some of the negative instances out of the trainingand test sets while for the LocationOf relation it meanskeeping a subset of the positive instances out.
We per-formed 5 iterations of the down-sampling procedureand for each iteration we carried out a 10-fold cross-validation to train and test our classifier.
The results aretest set averages over all iterations and folds.
The per-formance measure we use is the area under the receiveroperating characteristic (AUC in short for area underthe curve).
The AUC of a classifier is the area under thecurve defined by the corresponding true positive rateand false positive rate values obtained for varying thethreshold of the classifier to accept an instance as posi-tive.
Intuitively, AUC is the probability that a randomlypicked positive instance?s estimated posterior probabil-ity is higher than a randomly picked negative instance?sestimated posterior probability (Fawcett, 2006).The coverage is quite high for both corpora: Out of1943 pairs,only 3 were represented by a zero-vector inYahoo while in ukWaC this number is 68.
For sim-plicity, we discarded missing pairs from our analysis.We report only the results obtained for the entire co-occurrence matrix.
The results are virtually identi-cal for the other conditions too: Both for Yahoo andukWaC, almost all of the AUC values obtained for allrelations and for all conditions are above 95%.
Onlythe PartOf relation has AUC values above 90% (whichis still a very good result).Relation Yahoo ukWaCIsA 99.0% 98.0%UsedFor 98.2% 98.5%CapableOf 98.9% 99.1%PartOf 97.6% 95.0%LocationOf 99.0% 98.8%Table 5: AUC scores for 5 relations of ConceptNet,classifier trained for v1v2v1,2 condition.The very high performance we observe for the Con-ceptNet task is surprising when compared to the mod-erate performance we observe for other tasks.
Our ex-tensive filtering of the assertions could have resultedin a biased dataset which might have made the job ofthe classifier easy while reducing its generalization ca-pacity.
To investigate this, we decided to use the pairscoming from the SAT task as a validation set.Again, we trained an SVM classifier on the Concept-Net data for each of the 5 relations like we did previ-ously, but this time without cross-validation (i.e.
afterthe down-sampling, we used the entire set as the train-ing dataset in each iteration).
Then we evaluated theclassifiers on the 2224 pairs of the SAT analogy task(removing pairs that were in the training data) and av-eraged the posterior probability reported by each SVMover each down-sampling iteration.
The 5 pairs whichare assigned the highest posterior probability for eachrelation are reported in Table 6.
We have not yet quan-tified the performance of BagPack in this task but thepreliminary results in this table are, qualitatively, ex-ceptionally good.5 ConclusionsWe presented a general way to build a vector-basedspace to represent the semantic relations between wordpairs and showed how that representation can be usedto solve various tasks involving semantic similarity.For SAT and TOEFL, we obtained reasonable perfor-mances comparable to the state of the art.
For the es-timation of selective preference judgments about verb-noun pairs, we achieved state of the art performance.Perhaps more importantly, our representation formatallows us to provide meaningful estimates even whenthe verb and noun are not observed co-occurring in thecorpus ?
which is an obvious advantage over the mod-els which rely on sytagmatic contexts alone and cannotprovide estimates for word pairs that are not seen di-rectly co-occurring.
We also obtained very promisingresults for the automated augmentation of ConceptNet.The generality of the proposed method is also re-flected in the fact that we built a single feature spacebased on frequent basis terms and used the same fea-tures for all pairs coming from different tasks.
Theuse of the same feature set for all pairs makes it pos-sible to build a single database of word-pair vectors.For example, we were able to re-use the vectors con-structed for SAT pairs as a validation set in the Con-ceptNet task.
Furthermore, the results reported here areobtained for the same machine learning model (SVM)without any parameter tweaking, which renders themvery strict lower bounds.Another contribution is that the proposed methodprovides a way to represent the relations betweenwords even if they are not observed co-occurring in thecorpus.
Employing a larger corpus can be an alternativesolution for some cases but this is not always possibleand some tasks, like estimating selectional preferencejudgments, inherently call for a method that does notexclusively depends on paired co-occurrence observa-tions.Finally, we introduced ConceptNet, a common-sensesemantic network, to the corpus-based semantics com-munity, both as a new challenge and as a repository we39Rank IsA UsedFor PartOf CapableOf LocationOf1 watch,timepiece pencil,draw vehicle,wheel motorist,drive spectator,arena2 emerald,gem blueprint,build spider,leg volatile,vaporize water,riverbed3 cherry,fruit detergent,clean keyboard,finger concrete,harden bovine,pasture4 dinosaur,reptile guard,protect train,caboose parasite,contribute benediction,church5 ostrich,bird buttress,support hub,wheel immature,develop byline,newspaperTable 6: Top 5 SAT pairs classified as positive for ConceptNet relations, classifier trained for v1v2v1,2 condition.can benefit from.In future work, one of the most pressing issue wewant to explore is how to better exploit the informa-tion in the single occurrence vectors: currently, we donot make any use of the overlap between v1 and v2.In this way, we are missing the classic intuition thattaxonomically similar words tend to occur in similarcontexts, and it is thus not surprising that v1v2 flunksthe TOEFL.
We are currently looking at ways to aug-ment our concatenated vector with ?meta-information?about vector overlap.ReferencesS.
Canu, Y. Grandvalet, V. Guigue and A. Rakotoma-monjy.
2005.
SVM and Kernel Methods MatlabToolbox, Perception Syst?mes et Information, INSAde Rouen, Rouen, FranceN.
Chomsky.
1957.
Syntactic structures.
Mouton, TheHague.R.
P. W. Duin.
2001.
PRTOOLD (Version 3.1.7),A Matlab toolbox for pattern recognition.
PatternRecognition Group.
Delft University of Technology.K.
Erk.
2007.
A simple, similarity-based model forselectional preferences.
Proceedings of ACL 2007.K.
Erk and S. Pad?.
2008.
A structured vector spacemodel for word meaning in context.
Proceedings ofEMNLP 2008.I.
Eslick.
2006.
Searching for commonsense.
Master?sthesis, Massachusetts Institute of Technology.T.
Fawcett.
2006.
An introduction to roc analysis.Pattern Recogn.
Lett., 27(8):861?874.C.
Havasi, R. Speer and J. Alonso.
2007.
Concept-net 3: a flexible, multilingual semantic network forcommon sense knowledge.
In Recent Advances inNatural Language Processing, Borovets, Bulgaria,September.C.-W. Hsu, C.-C Chang.
2003.
A practical guideto support vector classification.
Technical report,Department of Computer Science, National TaiwanUniversity.T.K.
Landauer and S.T.
Dumais.
1997.
A solutionto Plato?s problem: The Latent Semantic Analysistheory of acquisition, induction and representationof knowledge.
Psychological Review, 104(2): 211?240.H.
Liu and P. Singh.
2004.
ConceptNet ?
A practi-cal commonsense reasoning tool-kit.
BT TechnologyJournal, 22(4) 211?226.S.
Mirkin, I. Dagan and M. Geffet.
2006.
Integrat-ing pattern-based and distributional similarity meth-ods for lexical entailment acquisition.
Proceedingsof COLING/ACL 2006, 579?586.S.
Pad?, S. Pad?
and K. Erk.
2007.
Flexible, corpus-based modelling of human plausibility judgements.Proceedings EMNLP 2007, 400?409.U.
Pad?.
2007.
The Integration of Syntax and SemanticPlausibility in a Wide-Coverage Model of SentenceProcessing.
Ph.D. thesis, Saarland University.P.
Pantel and M. Pennacchiotti.
2006.
Espresso:Leveraging generic patterns for automatically har-vesting semantic relations.
Proceedings of COL-ING/ACL 2006, 113?120.R.
Rapp.
2003.
Word sense discovery based on sensedescriptor dissimilarity.
Proceedings of MT SummitIX: 315?322.M.
Sahlgren.
2006.
The Word-space model.
Ph.D. dis-sertation, Stockholm University, Stockholm.G.
Salton and C. Buckley.
1988.
Term-weightingapproaches in automatic text retrieval.
InformationProcessing and Management, 24(5): 513?523.R.
Speer, C. Havasi and H. Lieberman.
2008.
Anal-ogyspace: Reducing the dimensionality of commonsense knowledge.
In Dieter Fox and Carla P. Gomes,editors, AAAI, pages 548?553.
AAAI Press.P.
Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3): 379?416.P.
Turney.
2008.
A uniform approach to analogies,synonyms, antonyms and associations.
Proceedingsof COLING 2008, 905?912.40
