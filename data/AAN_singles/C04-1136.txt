Significance tests for the evaluation of ranking methodsStefan EvertInstitut fu?r maschinelle SprachverarbeitungUniversita?t StuttgartAzenbergstr.
12, 70174 Stuttgart, Germanyevert@ims.uni-stuttgart.deAbstractThis paper presents a statistical model that in-terprets the evaluation of ranking methods asa random experiment.
This model predicts thevariability of evaluation results, so that appro-priate significance tests for the results can bederived.
The paper concludes with an empiricalvalidation of the model on a collocation extrac-tion task.1 IntroductionMany tools in the area of natural-language process-ing involve the application of ranking methods tosets of candidates, in order to select the most use-ful items from an all too often overwhelming list.Examples of such tools range from syntactic parsers(where alternative analyses are ranked by their plau-sibility) to the extraction of collocations from textcorpora (where a ranking according to the scores as-signed by a lexical association measure is the essen-tial component of an extraction ?pipeline?
).To this end, a scoring function g is applied to thecandidate set, which assigns a real number g(x) ?R to every candidate x.1 Conventionally, higherscores are assigned to candidates that the scoringfunction considers more ?useful?.
Candidates canthen be selected in one of two ways: (i) by compar-ison with a pre-defined threshold ?
?
R (i.e.
x isaccepted iff g(x) ?
?
), resulting in a ?-acceptanceset; (ii) by ranking the entire candidate set accord-ing to the scores g(x) and selecting the n highest-scoring candidates, resulting in an n-best list (wheren is either determined by practical constraints or in-teractively by manual inspection).
Note that an n-best list can also be interpreted as a ?-acceptance setwith a suitably chosen cutoff threshold ?g(n) (deter-mined from the scores of all candidates).Ranking methods usually involve various heuris-tics and statistical guesses, so that an empirical eval-1Some systems may directly produce a sorted candidate listwithout assigning explicit scores.
However, unless this opera-tion is (implicitly) based on an underlying scoring function, theresult will in most cases be a partial ordering (where some pairsof candidates are incomparable) or lead to inconsistencies.uation of their performance is necessary.
Even whenthere is a solid theoretical foundation, its predictionsmay not be borne out in practice.
Often, the maingoal of an evaluation experiment is the comparisonof different ranking methods (i.e.
scoring functions)in order to determine the most useful one.A widely-used evaluation strategy classifies thecandidates accepted by a ranking method into?good?
ones (true positives, TP) and ?bad?
ones(false positives, FP).
This is sometimes achieved bycomparison of the relevant ?-acceptance sets or n-best lists with a gold standard, but for certain ap-plications (such as collocation extraction), manualinspection of the candidates leads to more clear-cutand meaningful results.
When TPs and FPs havebeen identified, the precision ?
of a ?-acceptanceset or an n-best list can be computed as the pro-portion of TPs among the accepted candidates.
Themost useful ranking method is the one that achievesthe highest precision, usually comparing n-best listsof a given size n. If the full candidate set has beenannotated, it is also possible to determine the recallR as the number of accepted TPs divided by the to-tal number of TPs in the candidate set.
While theevaluation of extraction tools (e.g.
in informationretrieval) usually requires that both precision andrecall are high, ranking methods often put greaterweight on high precision, possibly at the price ofmissing a considerable number of TPs.
Moreover,when n-best lists of the same size are compared,precision and recall are fully equivalent.2 For thesereasons, I will concentrate on the precision ?
here.As an example, consider the identification of col-locations from text corpora.
Following the method-ology described by Evert and Krenn (2001), Ger-man PP-verb combinations were extracted from achunk-parsed version of the Frankfurter RundschauCorpus.3 A cooccurrence frequency threshold of2Namely, ?
= nTP ?
R/n, where nTP stands for the totalnumber of TPs in the candidate set.3The Frankfurter Rundschau Corpus is a German newspa-per corpus, comprising ca.
40 million words of text.
It is part ofthe ECI Multilingual Corpus 1 distributed by ELSNET.
For thisf ?
30 was applied, resulting in a candidate setof 5 102 PP-verb pairs.
The candidates were thenranked according to the scores assigned by fourassociation measures: the log-likelihood ratio G2(Dunning, 1993), Pearson?s chi-squared statistic X2(Manning and Schu?tze, 1999, 169?172), the t-scorestatistic t (Church et al, 1991), and mere cooccur-rence frequency f .4 TPs were identified accordingto the definition of Krenn (2000).
The graphs inFigure 1 show the precision achieved by these mea-sures, for n ranging from 100 to 2 000 (lists withn < 100 were omitted because the graphs becomehighly unstable for small n).
The baseline precisionof 11.09% corresponds to a random selection of ncandidates.0 500 1000 1500 200001020304050n?best listprecision(%)baseline = 11.09%G2tX2fFigure 1: Evaluation example: candidates for Ger-man PP-verb collocations are ranked by four differ-ent association measures.From Figure 1, we can see that G2 and t are themost useful ranking methods, t being marginallybetter for n ?
800 and G2 for n ?
1 500.
Both mea-sures are by far superior to frequency-based rank-ing.
The evaluation results also confirm the argu-ment of Dunning (1993), who suggested G2 as amore robust alternative to X2.
Such results cannotbe taken at face value, though, as they may simplybe due to chance.
When two equally useful rank-ing methods are compared, method A might justhappen to perform better in a particular experiment,with B taking the lead in a repetition of the experi-experiment, the corpus was annotated with the partial parserYAC (Kermes, 2003).4See Evert (2004) for detailed information about these as-sociation measures, as well as many further alternatives.ment under similar conditions.
The causes of suchrandom variation include the source material fromwhich the candidates are extracted (what if a slightlydifferent source had been used?
), noise introducedby automatic pre-processing and extraction tools,and the uncertainty of human annotators manifestedin varying degrees of inter-annotator agreement.Most researchers understand the necessity of test-ing whether their results are statistically significant,but it is fairly unclear which tests are appropriate.For instance, Krenn (2000) applies the standard ?2-test to her comparative evaluation of collocation ex-traction methods.
She is aware, though, that thistest assumes independent samples and is hardly suit-able for different ranking methods applied to thesame candidate set: Krenn and Evert (2001) sug-gest several alternative tests for related samples.
Awide range of exact and asymptotic tests as well ascomputationally intensive randomisation tests (Yeh,2000) are available and add to the confusion aboutan appropriate choice.The aim of this paper is to formulate a statisti-cal model that interprets the evaluation of rankingmethods as a random experiment.
This model de-fines the degree to which evaluation results are af-fected by random variation, allowing us to deriveappropriate significance tests.
After formalising theevaluation procedure in Section 2, I recast the pro-cedure as a random experiment and make the under-lying assumptions explicit (Section 3.1).
On the ba-sis of this model, I develop significance tests for theprecision of a single ranking method (Section 3.2)and for the comparison of two ranking methods(Section 3.3).
The paper concludes with an empiri-cal validation of the statistical model in Section 4.2 A formal account of ranking methodsand their evaluationIn this section I present a formalisation of rankingsand their evaluation, giving ?-acceptance sets a ge-ometrical interpretation that is essential for the for-mulation of a statistical model in Section 3.The scores computed by a ranking method arebased on certain features of the candidates.
Eachcandidate can therefore be represented by its featurevector x ?
?, where ?
is an abstract feature space.For all practical purposes, ?
can be equated with asubset of the (possibly high-dimensional) real Eu-clidean space Rm.
The complete set of candidatescorresponds to a discrete subset C ?
?
of the fea-ture space.5 A ranking method is represented by5More precisely, C is a multi-set because there may be mul-tiple candidates with identical feature vectors.
In order to sim-plify notation I assume that C is a proper subset of ?, whicha real-valued function g : ?
?
R on the featurespace, called a scoring function (SF).
In the follow-ing, I assume that there are no candidates with equalscores, and hence no ties in the rankings.6The ?-acceptance set for a SF g contains all can-didates x ?
C with g(x) ?
?.
In a geomet-rical interpretation, this condition is equivalent tox ?
Ag(?)
?
?, whereAg(?)
:= {x ?
?
| g(x) ?
?
}is called the ?-acceptance region of g. The ?-acceptance set of g is then given by the intersectionAg(?
)?C =: Cg(?).
The selection of an n-best listis based on the ?-acceptance region Ag(?g(n)) fora suitably chosen n-best threshold ?g(n).7As an example, consider the collocation extrac-tion task introduced in Section 1.
The feature vec-tor x associated with a collocation candidate rep-resents the cooccurrence frequency information forthis candidate: x = (O11, O12, O21, O22), whereOij are the cell counts of a 2 ?
2 contingencytable (Evert, 2004).
Therefore, we have a four-dimensional feature space ?
?
R4, and each as-sociation measure defines a SF g : ?
?
R. Theselection of collocation candidates is usually madein the form of an n-best list, but may also be basedon a pre-defined threshold ?.8For an evaluation in terms of precision and re-call, the candidates in the set C are classified intotrue positives C+ and false positives C?.
The pre-cision corresponding to an acceptance region A isthen given by?A := |C+ ?A| / |C ?A| , (1)i.e.
the proportion of TPs among the accepted candi-dates.
The precision achieved by a SF g with thresh-old ?
is ?Cg(?).
Note that the numerator in Eq.
(1)reduces to n for an n-best list (i.e.
?
= ?g(n)),yielding the n-best precision ?g,n.
Figure 1 showsgraphs of ?g,n for 100 ?
n ?
2 000, for the SFsg1 = G2, g2 = t, g3 = X2, and g4 = f .can be enforced by adding a small amount of random jitter tothe feature vectors of candidates.6Under very general conditions, random jittering (cf.
Foot-note 5) ensures that no two candidates have equal scores.
Thisprocedure is (almost) equivalent to breaking ties in the rankingsrandomly.7Since I assume that there are no ties in the rankings, ?g(n)can always be determined in such a way that the acceptance setcontains exactly n candidates.8For instance, Church et al (1991) use a threshold of ?
=1.65 for the t-score measure corresponding to a nominal sig-nificance level of ?
= .05.
This threshold is obtained from thelimiting distribution of the t statistic.3 Significance tests for evaluation results3.1 Evaluation as a random experimentWhen an evaluation experiment is repeated, the re-sults will not be exactly the same.
There are manycauses for such variation, including different sourcematerial used by the second experiment, changes inthe tool settings, changes in the evaluation criteria,or the different intuitions of human annotators.
Sta-tistical significance tests are designed to account fora small fraction of this variation that is due to ran-dom effects, assuming that all parameters that mayhave a systematic influence on the evaluation resultsare kept constant.
Thus, they provide a lower limitfor the variation that has to be expected in an actualrepetition of the experiment.
Only when results aresignificant can we expect them to be reproducible,but even then a second experiment may draw a dif-ferent picture.In particular, the influence of qualitatively differ-ent source material or different evaluation criteriacan never be predicted by statistical means alone.In the example of the collocation extraction task,randomness is mainly introduced by the selectionof a source corpus, e.g.
the choice of one partic-ular newspaper rather than another.
Disagreementbetween human annotators and uncertainty aboutthe interpretation of annotation guidelines may alsolead to an element of randomness in the evaluation.However, even significant results cannot be gener-alised to a different type of collocation (such asadjective-noun instead of PP-verb), different eval-uation criteria, a different domain or text type, oreven a source corpus of different size, as the resultsof Krenn and Evert (2001) show.A first step in the search for an appropriate sig-nificance test is to formulate a (plausible) modelfor random variation in the evaluation results.
Be-cause of the inherent randomness, every repetitionof an evaluation experiment under similar condi-tions will lead to different candidate sets C+ andC?.
Some elements will be entirely new candidates,sometimes the same candidate appears with a differ-ent feature vector (and thus represented by a differ-ent point x ?
?
), and sometimes a candidate thatwas annotated as a TP in one experiment may beannotated as a FP in the next.
In order to encapsu-late all three kinds of variation, let us assume thatC+ and C?
are randomly selected from a large setof hypothetical possibilities (where each candidatecorresponds to many different possibilities with dif-ferent feature vectors, some of which may be TPsand some FPs).For any acceptance region A, both the number ofTPs in A, TA := |C+ ?A|, and the number of FPsin A, FA := |C?
?A|, are thus random variables.We do not know their precise distributions, but it isreasonable to assume that (i) TA and FA are alwaysindependent and (ii) TA and TB (as well as FA andFB) are independent for any two disjoint regions Aand B.
Note that TA and TB cannot be indepen-dent for A ?
B 6= ?
because they include the samenumber of TPs from the region A ?
B.
The totalnumber of candidates in the region A is also a ran-dom variable NA := TA+FA, and the same followsfor the precision ?A, which can now be written as?A = TA/NA.9Following the standard approach, we may nowassume that ?A approximately follows a normaldistribution with mean piA and variance ?2A, i.e.
?A ?
N(piA, ?2A).
The mean piA can be interpretedas the average precision of the acceptance regionA (obtained by averaging over many repetitions ofthe evaluation experiment).
However, there are twoproblems with this assumption.
First, while ?A isan unbiased estimator for pia, the variance ?2A can-not be estimated from a single experiment.10 Sec-ond, ?A is a discrete variable because both TA andNA are non-negative integers.
When the numberof candidates NA is small (as in Section 3.3), ap-proximating the distribution of ?A by a continuousnormal distribution will not be valid.It is reasonable to assume that the distribution ofNA does not depend on the average precision piA.
Inthis case, NA is called an ancillary statistic and canbe eliminated without loss of information by condi-tioning on its observed value (see Lehmann (1991,542ff) for a formal definition of ancillary statisticsand the merits of conditional inference).
Instead ofprobabilities P (?A) we will now consider the con-ditional probabilities P (?A |NA).
Because NA isfixed to the observed value, ?A is proportional toTA and the conditional probabilities are equivalentto P (TA |NA).
When we choose one of the NAcandidates at random, the probability that it is a TP(averaged over many repetitions of the experiment)9In the definition of the n-best precision ?g,n, i.e.
forA = Cg(?g(n)), the number of candidates in A is constant:NA = n. At first sight, this may seem to be inconsistent withthe interpretation of NA as a random variable.
However, onehas to keep in mind that ?g(n), which is determined from thecandidate set C, is itself a random variable.
Consequently, A isnot a fixed acceptance region and its variation counter-balancesthat of NA.10Sometimes, cross-validation is used to estimate the vari-ability of evaluation results.
While this method is appropri-ate e.g.
for machine learning and classification tasks, it is notuseful for the evaluation of ranking methods.
Since the cross-validation would have to be based on random samples from asingle candidate set, it would not be able to tell us anythingabout random variation between different candidate sets.should be equal to the average precision piA.
Conse-quently, P (TA |NA) should follow a binomial dis-tribution with success probability piA, i.e.P (TA = k |NA) =(NAk)?
(piA)k ?
(1?
piA)NA?k (2)for k = 0, .
.
.
, NA.
We can now make inferencesabout the average precision piA based on this bino-mial distribution.11As a second step in our search for an appropriatesignificance test, it is essential to understand exactlywhat question this test should address: What does itmean for an evaluation result (or result difference)to be significant?
In fact, two different questionscan be asked:A: If we repeat an evaluation experiment underthe same conditions, to what extent will the ob-served precision values vary?
This question isaddressed in Section 3.2.B: If we repeat an evaluation experiment underthe same conditions, will method A again per-form better than method B?
This question isaddressed in Section 3.3.3.2 The stability of evaluation resultsQuestion A can be rephrased in the following way:How much does the observed precision value foran acceptance region A differ from the true aver-age precision piA?
In other words, our goal hereis to make inferences about piA, for a given SF gand threshold ?.
From Eq.
(2), we obtain a bino-mial confidence interval for the true value piA, giventhe observed values of TA and NA (Lehmann, 1991,89ff).
Using the customary 95% confidence level,piA should be contained in the estimated interval inall but one out of twenty repetitions of the experi-ment.
Binomial confidence intervals can easily becomputed with standard software packages such asR (R Development Core Team, 2003).
As an ex-ample, assume that an observed precision of ?A =40% is based on TA = 200 TPs out of NA = 500accepted candidates.
Precision graphs as those inFigure 1 display ?A as a maximum-likelihood es-timate for piA, but its true value may range from35.7% to 44.4% (with 95% confidence).1211Note that some of the assumptions leading to Eq.
(2) arefar from self-evident.
As an example, (2) tacitly assumes thatthe success probability is equal to piA regardless of the particu-lar value of NA on which the distribution is conditioned, whichneed not be the case.
Therefore, an empirical validation is nec-essary (see Section 4).12This confidence interval was computed with the R com-mand binom.test(200,500).Figure 2 shows binomial confidence intervals forthe association measures G2 and X2 as shaded re-gions around the precision graphs.
It is obviousthat a repetition of the evaluation experiment maylead to quite different precision values, especiallyfor n < 1 000.
In other words, there is a consider-able amount of uncertainty in the evaluation resultsfor each individual measure.
However, we can beconfident that both ranking methods offer a substan-tial improvement over the baseline.0 500 1000 1500 200001020304050n?best listprecision(%)baseline = 11.09%G2X2Figure 2: Precision graphs for the G2 and X2 mea-sures with 95% confidence intervals.For an evaluation based on n-best lists (as in thecollocation extraction example), it has to be notedthat the confidence intervals are estimates for theaverage precision piA of a fixed ?-acceptance re-gion (with ?
= ?g(n) computed from the observedcandidate set).
While this region contains exactlyNA = n candidates in the current evaluation, NAmay be different from n when the experiment is re-peated.
Consequently, piA is not necessarily identi-cal to the average precision of n-best lists.3.3 The comparison of ranking methodsQuestion B can be rephrased in the following way:Does the SF g1 on average achieve higher precisionthan the SF g2?
(This question is normally askedwhen g1 performed better than g2 in the evaluation.
)In other words, our goal is to test whether piA > piBfor given acceptance regions A of g1 and B of g2.The confidence intervals obtained for two SF g1and g2 will often overlap (cf.
Figure 2, where theconfidence intervals of G2 and X2 overlap for alllist sizes n), suggesting that there is no significantdifference between the two ranking methods.
Bothobserved precision values are consistent with an av-erage precision piA = piB in the region of overlap,so that the observed differences may be due to ran-dom variation in opposite directions.
However, thisconclusion is premature because the two rankingsare not independent.
Therefore, the observed pre-cision values of g1 and g2 will tend to vary in thesame direction, the degree of correlation being de-termined by the amount of overlap between the tworankings.
Given acceptance regions A := Ag1(?1)and B := Ag2(?2), both SF make the same decisionfor any candidates in the intersection A ?
B (bothSF accept) and in the ?complement?
?
\ (A ?
B)(both SF reject).
Therefore, the performance of g1and g2 can only differ in the regions D1 := A \ B(g1 accepts, but g2 rejects) and B \ A (vice versa).Correspondingly, the counts TA and TB are corre-lated because they include the same number of TPsfrom the region A?B (namely, the set C+?A?B),Indisputably, g1 is a better ranking method thang2 iff piD1 > piD2 and vice versa.13 Our goal is thusto test the null hypothesis H0 : piD1 = piD2 on thebasis of the binomial distributions P (TD1 |ND1)and P (TD2 |ND2).
I assume that these distribu-tions are independent because D1 ?
D2 = ?
(cf.Section 3.1).
The number of candidates in thedifference regions, ND1 and ND2 , may be small,especially for acceptance regions with large over-lap (this was one of the reasons for using condi-tional inference rather than a normal approximationin Section 3.1).
Therefore, it is advisable to useFisher?s exact test (Agresti, 1990, 60?66) insteadof an asymptotic test that relies on large-sample ap-proximations.
The data for Fisher?s test consist ofa 2?
2 contingency table with columns (TD1 , FD1)and (TD2 , FD2).
Note that a two-sided test is calledfor because there is no a priori reason to assumethat g1 is better than g2 (or vice versa).
Althoughthe implementation of a two-sided Fisher?s test isnot trivial, it is available in software packages suchas R.Figure 3 shows the same precision graphs asFigure 2.
Significant differences between the G2and X2 measures according to Fisher?s test (at a95% confidence level) are marked by grey triangles.13Note that piD1 > piD2 does not necessarily entail piA >piB if NA and NB are vastly different and piA?B ?
piDi .
Inthis case, the winner will always be the SF that accepts thesmaller number of candidates (because the additional candi-dates only serve to lower the precision achieved in A ?
B).This example shows that it is ?unfair?
to compare acceptancesets of (substantially) different sizes just in terms of their over-all precision.
Evaluation should therefore either be based onn-best lists or needs to take recall into account.Contrary to what the confidence intervals in Fig-ure 2 suggested, the observed differences turn outto be significant for all n-best lists up to n = 1250(marked by a thin vertical line).0 500 1000 1500 200001020304050n?best listprecision(%)baseline = 11.09%G2X2Figure 3: Significant differences between the G2and X2 measures at 95% confidence level.4 Empirical validationIn order to validate the statistical model and the sig-nificance tests proposed in Section 3, it is neces-sary to simulate the repetition of an evaluation ex-periment.
Following the arguments of Section 3.1,the conditions should be the same for all repetitionsso that the amount of purely random variation canbe measured.
To achieve this, I divided the Frank-furter Rundschau Corpus into 80 contiguous, non-overlapping parts, each one containing approx.
500kwords.
Candidates for PP-verb collocations wereextracted as described in Section 1, with a frequencythreshold of f ?
4.
The 80 samples of candidatesets were ranked using the association measures G2,X2 and t as scoring functions, and true positiveswere manually identified according to the criteriaof (Krenn, 2000).14 The true average precision piAof an acceptance set A was estimated by averagingover all 80 samples.Both the confidence intervals of Section 3.2 andthe significance tests of Section 3.3 are based onthe assumption that P (TA |NA) follows a binomialdistribution as given by Eq.
(2).
Unfortunately, it14I would like to thank Brigitte Krenn for making her annota-tion database of PP-verb collocations (Krenn, 2000) available,and for the manual annotation of 1 913 candidates that were notcovered by the existing database.is impossible to test the conditional distribution di-rectly, which would require that NA is the same forall samples.
Therefore, I use the following approachbased on the unconditional distribution P (?A).
IfNA is sufficiently large, P (?A |NA) can be approx-imated by a normal distribution with mean ?
= piAand variance ?2 = piA(1?
piA)/NA (from Eq.
(2)).Since ?
does not depend on NA and the standarddeviation ?
is proportional to (NA)?1/2, it is validto make the approximationP (?A |NA) ?
P (?A) (3)as long as NA is relatively stable.
Eq.
(3) allows usto pool the data from all samples, predicting thatP (?A) ?
N(?, ?2) (4)with ?
= piA and ?2 = piA(1 ?
piA)/N .
Here, Nstands for the average number of TPs in A.These predictions were tested for the measuresg1 = G2 and g2 = t, with cutoff thresholds ?1 =32.5 and ?2 = 2.09 (chosen so that N = 100 candi-dates are accepted on average).
Figure 4 comparesthe empirical distribution of ?A with the expecteddistribution according to Eq.
(4).
These histogramsshow that the theoretical model agrees quite wellwith the empirical results, although there is a lit-tle more variation than expected.15 The empiricalstandard deviation is between 20% and 40% largerthan expected, with s = 0.057 vs. ?
= 0.044 for G2and s = 0.066 vs. ?
= 0.047 for t. These findingssuggest that the model proposed in Section 3.1 mayindeed represent a lower bound on the true amountof random variation.Further evidence for this conclusion comes froma validation of the confidence intervals defined inSection 3.2.
For a 95% confidence interval, the trueproportion piA should fall within the confidence in-terval in all but 4 of the 80 samples.
For G2 (with?
= 32.5) and X2 (with ?
= 239.0), piA was out-side the confidence interval in 9 cases each (threeof them very close to the boundary), while the con-fidence interval for t (with ?
= 2.09) failed in 12cases, which is significantly more than can be ex-plained by chance (p < .001, binomial test).5 ConclusionIn the past, various statistical tests have been usedto assess the significance of results obtained in theevaluation of ranking methods.
There is much con-fusion about their validity, though, mainly due to15The agreement is confirmed by the Kolmogorov test ofgoodness-of-fit, which does not reject the theoretical model (4)in either case.Histogram for G2precisionnumber of samples0.0 0.1 0.2 0.3 0.4 0.5 0.605101520 observedexpectedHistogram for tprecisionnumber of samples0.0 0.1 0.2 0.3 0.4 0.5 0.605101520 observedexpectedFigure 4: Distribution of the observed precision ?A for ?-acceptance regions of the association measuresG2 (left panel) and t (right panel).
The solid lines indicate the expected distribution according to Eq.
(2).the fact that assumptions behind the applicationof a test are seldom made explicit.
This paperis an attempt to remedy the situation by interpret-ing the evaluation procedure as a random experi-ment.
The model assumptions, motivated by intu-itive arguments, are stated explicitly and are openfor discussion.
Empirical validation on a colloca-tion extraction task has confirmed the usefulnessof the model, indicating that it represents a lowerbound on the variability of evaluation results.
Onthe basis of this model, I have developed appro-priate significance tests for the evaluation of rank-ing methods.
These tests are implemented in theUCS toolkit, which was used to produce the graphsin this paper and can be downloaded from http://www.collocations.de/.ReferencesAlan Agresti.
1990.
Categorical Data Analysis.John Wiley & Sons, New York.Kenneth Church, William Gale, Patrick Hanks, andDonald Hindle.
1991.
Using statistics in lexicalanalysis.
In Lexical Acquisition: Using On-lineResources to Build a Lexicon, pages 115?164.Lawrence Erlbaum.Ted Dunning.
1993.
Accurate methods for thestatistics of surprise and coincidence.
Computa-tional Linguistics, 19(1):61?74.Stefan Evert and Brigitte Krenn.
2001.
Methodsfor the qualitative evaluation of lexical associa-tion measures.
In Proceedings of the 39th AnnualMeeting of the Association for ComputationalLinguistics, pages 188?195, Toulouse, France.Stefan Evert.
2004.
An on-line reposi-tory of association measures.
http://www.collocations.de/AM/.Hannah Kermes.
2003.
Off-line (and On-line) TextAnalysis for Computational Lexicography.
Ph.D.thesis, IMS, University of Stuttgart.
Arbeitspa-piere des Instituts fu?r Maschinelle Sprachverar-beitung (AIMS), volume 9, number 3.Brigitte Krenn and Stefan Evert.
2001.
Can wedo better than frequency?
a case study on ex-tracting pp-verb collocations.
In Proceedings ofthe ACL Workshop on Collocations, pages 39?46,Toulouse, France, July.Brigitte Krenn.
2000.
The Usual Suspects: Data-Oriented Models for the Identification and Rep-resentation of Lexical Collocations., volume 7 ofSaarbru?cken Dissertations in Computational Lin-guistics and Language Technology.
DFKI & Uni-versita?t des Saarlandes, Saarbru?cken, Germany.E.
L. Lehmann.
1991.
Testing Statistical Hypothe-ses.
Wadsworth, 2nd edition.Christopher D. Manning and Hinrich Schu?tze.1999.
Foundations of Statistical Natural Lan-guage Processing.
MIT Press, Cambridge, MA.R Development Core Team, 2003.
R: A languageand environment for statistical computing.
RFoundation for Statistical Computing, Vienna,Austria.
ISBN 3-900051-00-3.
See also http://www.r-project.org/.Alexander Yeh.
2000.
More accurate tests for thestatistical significance of result differences.
InProceedings of the 18th International Conferenceon Computational Linguistics (COLING 2000),Saarbru?cken, Germany.
