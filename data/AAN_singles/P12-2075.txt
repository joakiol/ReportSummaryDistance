Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 383?387,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsUnsupervized Word Segmentation:the case for Mandarin ChinesePierre MagistryAlpage, INRIA & Univ.
Paris 7,175 rue du Chevaleret,75013 Paris, Francepierre.magistry@inria.frBeno?t SagotAlpage, INRIA & Univ.
Paris 7,175 rue du Chevaleret,75013 Paris, Francebenoit.sagot@inria.frAbstractIn this paper, we present an unsupervized seg-mentation system tested on Mandarin Chi-nese.
Following Harris's Hypothesis in Kempe(1999) and Tanaka-Ishii's (2005) reformulation,we base our work on the Variation of BranchingEntropy.
We improve on (Jin and Tanaka-Ishii,2006) by adding normalization and viterbi-decoding.
This enable us to remove most ofthe thresholds and parameters from their modeland to reach near state-of-the-art results (Wanget al, 2011) with a simpler system.
We provideevaluation on different corpora available fromthe Segmentation bake-off II (Emerson, 2005)and define a more precise topline for the taskusing cross-trained supervized system availableoff-the-shelf (Zhang and Clark, 2010; Zhao andKit, 2008; Huang and Zhao, 2007)1 IntroductionThe Chinese script has no explicit ?word?
bound-aries.
Therefore, tokenization itself, although thevery first step of many text processing systems, isa challenging task.
Supervized segmentation sys-tems exist but rely on manually segmented corpora,which are often specific to a genre or a domain anduse many different segmentation guidelines.
In orderto deal with a larger variety of genres and domains,or to tackle more theoretic questions about linguisticunits, unsupervized segmentation is still an impor-tant issue.
After a short review of the correspondingliterature in Section 2, we discuss the challenging is-sue of evaluating unsupervized word segmentationsystems in Section 3.
Section 4 and Section 5 presentthe core of our system.
Finally, in Section 6, we de-tail and discuss our results.2 State of the ArtUnsupervized word segmentation systems tend tomake use of three different types of information: thecohesion of the resulting units (e.g., Mutual Infor-mation, as in (Sproat and Shih, 1990)), the degree ofseparation between the resulting units (e.g., Acces-sor Variety, see (Feng et al, 2004)) and the proba-bility of a segmentation given a string (Goldwater etal., 2006; Mochihashi et al, 2009).A recently published work by Wang et al (2011)introduce ESA: ?Evaluation, Selection, Adjust-ment.?
This method combines cohesion and separa-tion measures in a ?goodness?
metric that is maxi-mized during an iterative process.
This work is thecurrent state-of-the-art in unsupervized segmenta-tion of Mandarin Chinese data.The main drawbacks of ESA are the need to iteratethe process on the corpus around 10 times to reachgood performance levels and the need to set a param-eter that balances the impact of the cohesion measurew.r.t.
the separation measure.
Empirically, a corre-lation is found between the parameter and the size ofthe corpus but this correlation depends on the scriptused in the corpus (it changes if Latin letters andArabic numbers are taken into account during pre-processing or not).
Moreover, computing this cor-relation and finding the best value for the parameter(i.e., what the authors call the proper exponent) re-quires a manually segmented training corpus.
There-fore, this proper exponent may not be easily availablein all situations.
However, if we only consider theirexperiments using settings similar to ours, their re-sults consistently lie around an f-score of 0.80.An older approach, introduced by Jin and Tanaka-Ishii (2006), solely relies on a separation measure383that is directly inspired by a linguistic hypothesis for-mulated by Harris (1955).
In Tanaka-Ishii (2005)(following Kempe (1999)) who use Branching En-tropy (BE), this hypothesis goes as follows: if se-quences produced by human language were random,we would expect the Branching Entropy of a se-quence (estimated from the n-grams in a corpus)to decrease as we increase the length of the se-quence.
Therefore the variation of the branching en-tropy (VBE) should be negative.
When we observethat it is not the case, Harris hypothesizes that weare at a linguistic boundary.
Following this hypoth-esis, (Jin and Tanaka-Ishii, 2006) propose a systemthat segments when BE is rising or when it reach acertain maximum.The main drawback of Jin and Tanaka-Ishii (2006)model is that segmentation decisions are taken verylocally1 and do not depend on neighboring cuts.Moreover, this system also also relies on parameters,namely the threshold on the VBE above which thesystem decides to segment (in their system, this iswhen VBE?
0).
In theory, we could expect a de-creasing BE and look for a less decreasing value (oron the contrary, rising at least to some extent).
Athreshold of 0 can be seen as a default value.
Fi-nally, Jin and Tanaka-Ishii do not take in account thatVBE of n-gram may not be directly comparable tothe VBE of m-grams if m ?= n. A normalization isneeded (as in (Cohen et al, 2002)).Due to space constraints, we shall not describehere other systems than those by Wang et al (2011)and Jin and Tanaka-Ishii (2006).
A more compre-hensive state of the art can be found in (Zhao andKit, 2008) and (Wang et al, 2011).In this paper we will show that we can correct thedrawbacks of Jin and Tanaka-Ishii (2006) model andreach performances comparable to those of Wang etal.
(2011) with as simpler system.3 EvaluationIn this paper, in order to be comparable withWang et al (2011), we evaluate our system againstthe corpora from the Second International Chi-nese Word Segmentation Bakeoff (Emerson, 2005).These corpora cover 4 different segmentation guide-lines from various origins: Academia Sinica (AS),City-University of Hong-Kong (CITYU), MicrosoftResearch (MSR) and Peking University (PKU).1Jin (2007) uses self-training with MDL to address this issue.Evaluating unsupervized systems is a challenge byitself.
As an agreement on the exact definition ofwhat a word is remains hard to reach, various seg-mentation guidelines have been proposed and fol-lowed for the annotation of different corpora.
Theevaluation of supervized systems can be achieved onany corpus using any guidelines: when trained ondata that follows particular guidelines, the resultingsystem will follow as well as possible these guide-lines, and can be evaluated on data annotated accord-ingly.
However, for unsupervized systems, there isno reason why a system should be closer to one ref-erence than another or even not to lie somewherein between the different existing guidelines.
Huangand Zhao (2007) propose to use cross-training of asupervized segmentation system in order to have anestimation of the consistency between different seg-mentation guidelines, and therefore an upper boundof what can be expected from an unsupervized sys-tem (Zhao and Kit, 2008).
The average consistencyis found to be as low as 0.85 (f-score).
Thereforethis figure can be considered as a sensible topline forunsupervized systems.
The standard baseline whichconsists in segmenting each character leads to a base-line around 0.35 (f-score) ?
almost half of the to-kens in a manually segmented corpus are unigrams.Per word-length evaluation is also important asunits of various lengths tend to have different distri-butions.
We used ZPAR (Zhang and Clark, 2010) onthe four corpora from the Second Bakeoff to repro-duce Huang and Zhao's (2007) experiments, but alsoto measure cross-corpus consistency at a per-word-length level.
Our overall results are comparable towhat Huang and Zhao (2007) report.
However, theconsistency is quickly falling for longer words: onunigrams, f-scores range from 0.81 to 0.90 (the sameas the overall results).
We get slightly higher figureson bigrams (0.85?0.92) but much lower on trigramswith only 0.59?0.79.
In a segmented Chinese text,most of the tokens are uni- and bigrams but most ofthe types are bi- and trigrams (as unigrams are oftenhigh frequency grammatical words and trigrams theresult of more or less productive affixations).
There-fore the results of evaluations only based on tokensdo not suffer much from poor performances on tri-grams even if a large part of the lexicon may be in-correctly processed.Another issue about the evaluation and compari-son of unsupervized systems is to try and remain fair384in terms of preprocessing and prior knowledge givento the systems.
For example, Wang et al (2011)used different levels of preprocessing (which theycall ?settings?).
In their settings 1 and 2, Wang etal.
(2011) try not to rely on punctuation and char-acter encoding information (such as distinguishingLatin and Chinese characters).
However, they opti-mize their parameter for each setting.
We thereforeconsider that their system does take into account thelevel of processing which is performed on Latin char-acters and Arabic numbers, and therefore ?knows?whether to expect such characters or not.
In set-ting 3 they add the knowledge of punctuation as clearboundaries and in setting 4 they preprocess Arabicand Latin and obtain better, more consistent and lessquestionable results.As we are more interested in reducing the amountof human labor needed than in achieving by allmeans fully unsupervized learning, we do not re-frain from performing basic and straightforward pre-processing such as detection of punctuation marks,Latin characters and Arabic numbers.2 Therefore,our experiments rely on settings similar to their set-tings 3 and 4, and are evaluated against the samecorpora.4 Normalized Variation of BranchingEntropy (nVBE)Our system builds upon Harris's (1955) hypothesisand its reformulation by Kempe (1999) and Tanaka-Ishii (2005).
Let us now define formally the notionsunderlying our system.Given an n-gram x0..n = x0..1 x1..2 .
.
.
xn?1..nwith a left context ?
?, we define its Right BranchingEntropy (RBE) as:h?
(x0..n) = H(??
| x0..n)= ??x??
?P (x | x0..n) logP (x | x0..n).The Left Branching Entropy (LBE) is defined in asymmetric way: if we note ??
the right context ofx0..n, its LBE is defined as:h?
(x0..n) = H(??
| x0..n).The RBE (resp.
LBE) can be considered as x0..n'sBranching Entropy (BE) when reading from left toright (resp.
right to left).2Simple regular expressions could also be considered to dealwith unambiguous cases of numbers and dates in Chinese script.From h?
(x0..n) and h?
(x0..n?1) on the one hand,and from h?
(x0..n) and h?
(x1..n) we estimate theVariation of Branching Entropy (VBE) in both direc-tions, defined as follows:?h?
(x0..n) = h?
(x0..n) ?
h?(x0..n?1)?h?
(x0..n) = h?
(x0..n) ?
h?
(x1..n).The VBEs are not directly comparable for stringsof different lengths and need to be normalized.
Inthis work, we recenter them around 0 with respect tothe length of the string by substracting the mean ofthe VBEs of the strings of the same length.
Writing??h?
(x) and ??h?(x).
The normalized VBEs for thestring x, or nVBEs, are then defined as follow (weonly defined ??h?
(x) for clarity reasons): for eachlength k and each k-gram x such that len(x) = k,??h?
(x) = ?h?(x)??
?,k, where ?
?,k is the meanof the values of ?h?
(x) of all k-grams x.Note that we use and normalize the variation ofbranching entropy and not the branching entropy it-self.
Doing so would break the Harris's hypothesis aswe would not expect h?
(x0..n) < h?
(x0..n?1) in non-boundary situation anymore.
Many studies use di-rectly the branching entropy (normalized or not) andreport results that are below state-of-the-art systems(Cohen et al, 2002).5 Decoding algorithmIf we follow Harris's hypothesis and consider com-plex morphological word structures, we expect alarge VBE at the boundaries of interesting units andmore unstable variations inside ?words.?
This expec-tation was confirmed by empirical data visualization.For different lengths of n-grams, we compared thedistributions of the VBEs at different positions insidethe n-gram and at its boundaries.
By plotting densitydistributions for words vs. non-words, we observedthat the VBE at both boundaries were the most dis-criminative value.
Therefore, we decided to take inaccount the VBE only at the word-candidate bound-aries (left and right) and not to consider the inner val-ues.
Two interesting consequences of this decisionare: first, all ?
?h(x) can be precomputed as they donot depend on the context.
Second, best segmenta-tion can be computed using dynamic programming.Since we consider the VBE only at words bound-ary, we can define for any n-gram w its autonomy asa(x) = ??
?h(x) + ??h?(x).
The more an n-gram isautonomous, the more likely it is to be a word.385With this measure, we can redefine the sentencesegmentation problem as the maximization of the au-tonomy measure of its words.
For a character se-quence s, if we call Seg(s) the set of all the possiblesegmentations, then we are looking for:arg maxW?Seg(s)?wi?Wa(wi) ?
len(wi),where W is the segmentation corresponding to thesequence of words w0w1 .
.
.
wm, and len(wi) is thelength of a word wi used here to be able to com-pare segmentations resulting in a different numberof words.
This best segmentation can be computedeasily using dynamic programming.6 Results and discussionWe tested our system against the data from the 4 cor-pora of the Second Bakeoff, in both settings 3 and 4,as described in Section 3.
Overall results are givenin Table 1 and per-word-length results in Table 2.Our results (nVBE) show significant improve-ments over Jin's (2006) strategy (VBE > 0) andare closely competing with ESA.
But contrarily toESA (Wang et al, 2011), it does not require multi-ple iterations on the corpus and it does not rely onany parameters.
This shows that we can rely solelyon a separation measure and get high segmentationscores.
When maximized over a sentence, this mea-sure captures at least in part what can be modeled bya cohesion measure without the need for fine-tuningthe balance between the two.The evolution of the results w.r.t.
word length isconsistent with the supervized cross-evaluation re-sults of the various segmentation guidelines as per-formed in Section 3.Due to space constraints, we cannot detail here aqualitative analysis of the results.
We can simplymention that the errors we observed are consistentwith previous systems based on Harris's hypothesis(see (Magistry and Sagot, 2011) and Jin (2007) for alonger discussion).
Many errors are related to datesand Chinese numbers.
This could and should bedealt with during preprocessing.
Other errors ofteninvolve frequent grammatical morphemes or produc-tive affixes.
These errors are often interesting for lin-guists and could be studied as such and/or correctedin a post-processing stage that would introduce lin-guistic knowledge.
Indeed, unlike content words,grammatical morphemes belongs to closed classes,System AS CITYU PKU MSRSetting 3ESA worst 0.729 0.795 0.781 0.768ESA best 0.782 0.816 0.795 0.802nVBE 0.758 0.775 0.781 0.798Setting 4VBE > 0 0.63 0.640 0.703 0.713ESA worst 0.732 0.809 0.784 0.784ESA best 0.786 0.829 0.800 0.818nVBE 0.766 0.767 0.800 0.813Table 1: Evaluation on the Second Bakeoff data withWang et al's (2011) settings.
?Worst?
and ?best?
give therange of the reported results with differents values of theparameter in Wang et al's system.
VBE > 0 correspondto a cut whenever BE is raising.
nVBE corresponds to ourproposal, based on normalized VBE with maximization atword boundaries.
Recall that the topline is around 0.85Corpus overall unigrams bigrams trigramsAS 0.766 0.741 0.828 0.494CITYU 0.767 0.739 0.834 0.555PKU 0.800 0.789 0.855 0.451MSR 0.813 0.823 0.856 0.482Table 2: Per word-length details of our results with ournVBE algorithm and setting 4.
Recall that the toplinesare respectively 0.85, 0.81, 0.85 and 0.59 (see Section 3)therefore introducing this linguistic knowledge intothe system may be of great help without requiringto much human effort.
A sensible way to go in thatdirection would be to let unsupervized system dealwith open classes and process closed classes with asymbolic or supervized module.One can also observe that our system performs bet-ter on PKU and MSR corpora.
As PKU is the small-est corpus and AS the biggest, size alone cannot ex-plain this result.
However, PKU is more consistentin genre as it contains only articles from the Peo-ple's Daily.
On the other end, AS is a balanced cor-pus with a greater variety in many aspects.
CITYUCorpus is almost as small as PKU but contains arti-cles from newspapers of various Mandarin Chinesespeaking communities where great variation is to beexpected.
This suggest that consistency of the inputdata is as important as the amount of data.
This hy-pothesis has to be confirmed in futur studies.
If it is,automatic clustering of the input data may be an im-portant pre-processing step for this kind of systems.386ReferencesPaul Cohen, Brent Heeringa, and Niall Adams.
2002.An unsupervised algorithm for segmenting categoricaltimeseries into episodes.
Pattern Detection and Dis-covery, page 117?133.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedings of theFourth SIGHAN Workshop on Chinese Language Pro-cessing, volume 133.Haodi Feng, Kang Chen, Xiaotie Deng, and WeimingZheng.
2004.
Accessor variety criteria for Chi-nese word extraction.
Computational Linguistics,30(1):75?93.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006.
Contextual dependencies in unsupervisedword segmentation.
In Proceedings of the 21st Inter-national Conference on Computational Linguistics andthe 44th annual meeting of the Association for Compu-tational Linguistics, page 673?680.Zellig S. Harris.
1955.
From phoneme to morpheme.Language, 31(2):190?222.Changning.
Huang and Hai Zhao.
2007.
????????
(Chinese word segmentation: A decade review).Journal of Chinese Information Processing, 21(3):8?20.Zhihui Jin and Kumiko Tanaka-Ishii.
2006.
Unsuper-vised segmentation of Chinese text by use of branchingentropy.
In Proceedings of the COLING/ACL on Mainconference poster sessions, page 428?435.Zhihui Jin.
2007.
A Study On Unsupervised Segmenta-tion Of Text Using Contextual Complexity.
Ph.D. the-sis, University of Tokyo.Andr?
Kempe.
1999.
Experiments in unsupervisedentropy-based corpus segmentation.
In Workshop ofEACL in Computational Natural Language Learning,page 7?13.Pierre Magistry and Beno?t Sagot.
2011.
Segmentationet induction de lexique non-supervis?es du mandarin.In TALN'2011 - Traitement Automatique des LanguesNaturelles, Montpellier, France, June.
ATALA.Daichi Mochihashi, Takeshi.
Yamada, and Naonori Ueda.2009.
Bayesian unsupervised word segmentation withnested Pitman-Yor language modeling.
In Proceedingsof the Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Volume1-Volume 1, page 100?108.Richard W. Sproat and Chilin Shih.
1990.
A statis-tical method for finding word boundaries in Chinesetext.
Computer Processing of Chinese and OrientalLanguages, 4(4):336?351.Kumiko Tanaka-Ishii.
2005.
Entropy as an indicator ofcontext boundaries: An experiment using a web searchengine.
In IJCNLP, page 93?105.Hanshi Wang, Jian Zhu, Shiping Tang, and XiaozhongFan.
2011.
A new unsupervised approach to wordsegmentation.
Computational Linguistics, 37(3):421?454.Yue Zhang and Stephen Clark.
2010.
A fast decoderfor joint word segmentation and POS-tagging using asingle discriminative model.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, page 843?852.Hai Zhao and Chunyu Kit.
2008.
An empirical compar-ison of goodness measures for unsupervised Chineseword segmentation with a unified framework.
In TheThird International Joint Conference on Natural Lan-guage Processing (IJCNLP2008), Hyderabad, India.387
