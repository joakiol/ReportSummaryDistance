Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 72?79,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsA Probabilistic Rich Type Theory for Semantic InterpretationRobin Cooper1, Simon Dobnik1, Shalom Lappin2, and Staffan Larsson11University of Gothenburg,2King?s College London{cooper,sl}@ling.gu.se, simon.dobnik@gu.se, shalom.lappin@kcl.ac.ukAbstractWe propose a probabilistic type theory in which asituation s is judged to be of a type T with probabil-ity p. In addition to basic and functional types it in-cludes, inter alia, record types and a notion of typ-ing based on them.
The type system is intensionalin that types of situations are not reduced to setsof situations.
We specify the fragment of a com-positional semantics in which truth conditions arereplaced by probability conditions.
The type sys-tem is the interface between classifying situationsin perception and computing the semantic interpre-tations of phrases in natural language.1 IntroductionClassical semantic theories (Montague, 1974), aswell as dynamic (Kamp and Reyle, 1993) and un-derspecified (Fox and Lappin, 2010) frameworksuse categorical type systems.
A type T identifiesa set of possible denotations for expressions in T ,and the system specifies combinatorial operationsfor deriving the denotation of an expression fromthe values of its constituents.These theories cannot represent the gradienceof semantic properties that is pervasive in speak-ers?
judgements concerning truth, predication, andmeaning relations.
In general, predicates do nothave determinate extensions (or intensions), andso, in many cases, speakers do not make categor-ical judgements about the interpretation of an ex-pression.
Attributing gradience effects to perfor-mance mechanisms offers no help, unless one canshow precisely how these mechanisms produce theobserved effects.Moreover, there is a fair amount of evidence in-dicating that language acquisition in general cru-cially relies on probabilistic learning (Clark andLappin, 2011).
It is not clear how a reasonableaccount of semantic learning could be constructedon the basis of the categorical type systems that ei-ther classical or revised semantic theories assume.Such systems do not appear to be efficiently learn-able from the primary linguistic data (with weaklearning biases), nor is there much psychologicaldata to suggest that they provide biologically de-termined constraints on semantic learning.A semantic theory that assigns probabilityrather than truth conditions to sentences is in abetter position to deal with both of these issues.Gradience is intrinsic to the theory by virtue ofthe fact that speakers assign values to declarativesentences in the continuum of real numbers [0,1],rather than Boolean values in {0,1}.
In addition,a probabilistic account of semantic learning is fa-cilitated if the target of learning is a probabilisticrepresentation of meaning.
Both semantic repre-sentation and learning are instances of reasoningunder uncertainty.Probability theorists working in AI often de-scribe probability judgements as involving distri-butions over worlds.
In fact, they tend to limitsuch judgements to a restricted set of outcomesor events, each of which corresponds to a par-tial world which is, effectively, a type of situa-tion (Halpern, 2003).
A classic example of the re-duction of worlds to situation types in probabilitytheory is the estimation of the likelihood of headsvs tails in a series of coin tosses.
Here the worldis held constant except along the dimension of abinary choice between a particular set of possi-ble outcomes.
A slightly more complex case isthe probability distribution for possible results ofthrowing a single die, which allows for six pos-sibilities corresponding to each of its numberedfaces.
This restricted range of outcomes consti-tutes the sample space.We are making explicit the assumption, com-mon to most probability theories used in AI, withclearly defined sample spaces, that probabilityis distributed over situation types (Barwise andPerry, 1983), rather than over sets of entire worlds.An Austinian proposition is a judgement that a72situation is of a particular type, and we treat itas probabilistic.
In fact, it expresses a subjec-tive probability in that it encodes the belief of anagent concerning the likelihood that a situation isof that type.
The core of an Austinian proposi-tion is a type judgement of the form s : T , whichstates that a situation s is of type T .
On our ac-count this judgement is expressed probabilisticallyas p(s : T ) = r, where r ?
[0,1].1On the probabilistic type system that we pro-pose situation types are intensional objects overwhich probability distributions are specified.
Thisallows us to reason about the likelihood of alter-native states of affairs without invoking possibleworlds.Complete worlds are not tractably repre-sentable.
Assume that worlds are maximal con-sistent sets of propositions (Carnap, 1947).
Ifthe logic of propositions is higher-order, then theproblem of determining membership in such a setis not complete.
If the logic is classically first-order, then the membership problem is complete,but undecidable.Alternatively, we could limit ourselves topropositional logic, and try to generate a maxi-mally consistent set of propositions from a singlefinite proposition P in Conjunctive Normal Form(CNF, a conjunction of disjunctions), by simplyadding conjuncts to P .
But it is not clear what(finite) set of rules or procedures we could use todecide which propositions to add in order to gen-erate a full description of a world in a systematicway.
Nor is it obvious at what point the conjunc-tion will constitute a complete description of theworld.Moreover, all the propositions that P entailsmust be added to it, and all the propositions withwhich P is inconsistent must be excluded, in or-der to obtain the maximal consistent set of propo-sitions that describe a world.
But then testing thesatisfiability of P is an instance of the ksat prob-lem, which, in the general case, is NP-complete.21Beltagy et al.
(2013) propose an approach on which clas-sical logic-based representations are combined with distribu-tional lexical semantics and a probabilistic Markov logic, inorder to select among the set of possible inferences from asentence.
Our concern here is more foundational.
We seek toreplace classical semantic representations with a rich proba-bilistic type theory as the basis of both lexical and composi-tional interpretation.2The ksat problem is to determine whether a formula inpropositional logic has a satisfying set of truth-value assign-ments.
For the complexity results of different types of ksatproblem see Papadimitriou (1995).By contrast situation types can be as large or assmall as we need them to be.
They are not max-imal in the way that worlds are, and so the issueof completeness of specification does not arise.Therefore, they can, in principle, be tractably rep-resented.2 Rich Type Theory and ProbabilityCentral to standard formulations of rich type the-ories (for example, (Martin-L?of, 1984)) is the no-tion of a judgement a : T , that object a is of typeT .
We represent the probability of this judgementas p(a : T ).
Our system (based on Cooper (2012))includes the following types.Basic Types are not constructed out of other ob-jects introduced in the theory.
If T is a basic type,p(a : T ) for any object a is provided by a probabil-ity model, an assignment of probabilities to judge-ments involving basic types.PTypes are constructed from a predicate andan appropriate sequence of arguments.
An exam-ple is the predicate ?man?
with arity ?Ind ,Time?where the types Ind and Time are the basic typeof individuals and of time points respectively.Thus man(john,18:10) is the type of situation (oreventuality) where John is a man at time 18:10.A probability model provides probabilities p(e :r(a1, .
.
.
, an)) for ptypes r(a1, .
.
.
, an).
We takeboth common nouns and verbs to provide the com-ponents out of which PTypes are constructed.Meets and Joins give, for T1and T2, the meet,T1?
T2and the join T1?
T2, respectively.
a :T1?
T2just in case a : T1and a : T2.
a : T1?T2just in case either a : T1or a : T2(possiblyboth).3The probabilities for meet and joint typesare defined by the classical (Kolmogorov, 1950)equations p(a : T1?
T2) = p(a : T1)p(a : T2| a : T1)(equivalently, p(a : T1?
T2) = p(a : T1, a : T2)), andp(a : T1?
T2) = p(a : T1) + p(a : T2) ?
p(a : T1?
T2),respectively.Subtypes A type T1is a subtype of type T2,T1v T2, just in case a : T1implies a : T2no mat-ter what we assign to the basic types.
If T1v T2then a : T1?T2iff a : T1and a : T1?T2iff a : T2.Similarly, if T2v T1then a : T1?
T2iff a : T2and a : T1?
T2iff a : T1.If T2v T1, then p(a : T1?
T2) = p(a : T2),and p(a : T1?
T2) = p(a : T1).
If T1v T2,3This use of intersection and union types is not standard inrich type theories, where product and disjoint union are pre-ferred following the Curry-Howard correspondence for con-junction and disjunction.73then p(a : T1) ?
p(a : T2).
These definitionsalso entail that p(a : T1?
T2) ?
p(a : T1), andp(a : T1) ?
p(a : T1?
T2).We generalize probabilistic meet and join typesto probabilities for unbounded conjunctive anddisjunctive type judgements, again using the clas-sical equations.Let?p(a0: T0, .
.
.
, an: Tn) be the conjunctiveprobability of judgements a0: T0, .
.
.
, an: Tn.Then?p(a0: T0, .
.
.
, an: Tn) =?p(a0: T0, .
.
.
, an?1:Tn?1)p(an: Tn| a0: T0, .
.
.
, an?1: Tn?1).
If n = 0,?p(a0: T0, .
.
.
, an: Tn) = 1.We interpret universal quantification as an un-bounded conjunctive probability, which is true ifit is vacuously satisfied (n = 0) (Paris, 2010).Let?p(a0: T0, a1: T1, .
.
.)
be the disjunctiveprobability of judgements a0: T0, a1: T1, .
.
..It is computed by?p(a0: T0, .
.
.
, an: Tn) =?p(a0: T0, .
.
.
, an?1: Tn?1) + p(an: Tn) ?
?p(a0:T0, .
.
.
, an?1: Tn?1)p(an: Tn| a0: T0, .
.
.
, an?1:Tn?1).
If n = 0,?p(a0: T0, .
.
.
, an: Tn) = 0.We take existential quantification to be an un-bounded disjunctive probability, which is false if itlacks a single non-nil probability instance (n = 0).Conditional Conjunctive Probabilities arecomputed by?p(a0: T0, .
.
.
, an: Tn| a : T ) =?p(a0: T0, .
.
.
, an?1: Tn?1| a : T )p(an: Tn|a0: T0, .
.
.
, an?1: Tn?1, a : T )).
If n = 0,?p(a0:T0, .
.
.
, an: Tn| a : T ) = 1.Function Types give, for any types T1and T2,the type (T1?
T2).
This is the type of total func-tions with domain the set of all objects of typeT1and range included in objects of type T2.
Theprobability that a function f is of type (T1?
T2)is the probability that everything in its domain is oftype T1and that everything in its range is of typeT2, and furthermore that everything not in its do-main which has some probability of being of typeT1is not in fact of type T1.
p(f : (T1?
T2)) =?a?dom(f)p(a : T1, f(a) : T2)(1?
?a6?dom(f)p(a : T1))Suppose that T1is the type of event where thereis a flash of lightning and T2is the type of eventwhere there is a clap of thunder.
Suppose that fmaps lightning events to thunder events, and thatit has as its domain all events which have beenjudged to have probability greater than 0 of beinglightning events.
Let us consider that all the puta-tive lightning events were clear examples of light-ning (i.e.
judged with probability 1 to be of typeT1) and are furthermore associated by f with clearevents of thunder (i.e.
judged with probability 1 tobe of type T2).
Suppose there were four such pairsof events.
Then the probability of f being of type(T1?
T2) is (1?
1)4, that is, 1.Suppose, alternatively, that for one of the fourevents f associates the lightning event with a silentevent, that is, one whose probability of being ofT2is 0.
Then the probability of f being of type(T1?
T2) is (1 ?
1)3?
(1 ?
0) = 0.
One clearcounterexample is sufficient to show that the func-tion is definitely not of the type.In cases where the probabilities of the an-tecedent and the consequent type judgements arehigher than 0, the probability of the entire judge-ment on the existence of a functional type f willdecline in proportion to the size of dom(f).
As-sume, for example that there are k elements a ?dom(f), where for each such a p(a : T1) =p(f(a) : T2) ?
.5.
Every aithat is added todom(f) will reduce the value of p(f : (T1?T2)), even if it yields higher values for p(a : T1)and p(f(a) : T2).
This is due to the fact that weare treating the probability of p(f : (T1?
T2))as the likelihood of there being a function that issatisfied by all objects in its domain.
The largerthe domain, the less probable that all elements init fulfill the functional relation.We are, then, interpreting a functional typejudgement of this kind as a universally quantifiedassertion over the pairing of objects in dom(f)and range(f).
The probability of such an asser-tion is given by the conjunction of assertions cor-responding to the co-occurrence of each element ain f ?s domain as an instance of T1with f(a) as aninstance of T2.
This probability is the product ofthe probabilities of these individual assertions.This seems reasonable, but it only deals withfunctions whose domain is all objects which havebeen judged to have some probability, howeverlow, of being of type T1.
Intuitively, functionswhich leave out some of the objects with lowerlikelihood of being of type T1should also have aprobability of being of type (T1?
T2).
This fac-tor in the probability is represented by the secondelement of the product in the formula.74Negation ?T , of type T , is the function type(T ?
?
), where ?
is a necessarily empty typeand p(?)
= 0.
It follows from our rules for func-tion types that p(f : ?T ) = 1 if dom(f) = ?, thatis T is empty, and 0 otherwise.We also assign probabilities to judgements con-cerning the (non-)emptiness of a type, p(T ).
wepass over the details of how we compute the prob-abilities of such judgements, but we note that ouraccount of negation entails that p(T ?
?T ) = 1,and (ii) p(?
?T ) = p(T ).
Therefore, we sustainclassical Boolean negation and disjunction, in con-trast to Martin-L?of?s (1984) intuitionistic type the-ory.Dependent Types are functions from objects totypes.
Given appropriate arguments as functionsthey will return a type.
Therefore, the account ofprobabilities associated with functions above ap-plies to dependent types.Record Types A record in a type system asso-ciated with a set of labels is a set of ordered pairs(fields) whose first member is a label and whosesecond member is an object of some type (possiblya record).
Records are required to be functional onlabels (each label in a record can only occur oncein the record?s left projection).A dependent record type is a set of fields (or-dered pairs) consisting of a label ` followed by Tas above.
The set of record types is defined by:1.
[], that is the empty set or Rec, is a record type.
r : Recjust in case r is a record.2.
If T1is a record type, ` is a label not occurring in T1,and T2is a type, then T1?
{?`, T2?}
is a record type.r : T1?
{?`, T2?}
just in case r : T1, r.` is defined (`occurs as a label in r) and r.` : T2.3.
If T is a record type, ` is a label not occuring inT , T is a dependent type requiring n arguments, and?pi1, .
.
.
, pin?
is an n-place sequence of paths in T ,4then T ?
{?`, ?T , ?pi1, .
.
.
, pin???}
is a record type.r : T ?
{?`, ?T , ?pi1, .
.
.
, pin???}
just in case r : T ,r.` is defined and r.` : T (r.pi1, .
.
.
, r.pin).The probability that an object r is of a recordtype T is given by the following clauses:1. p(r : Rec) = 1 if r is a record, 0 otherwise2.
p(r : T1?
{?`, T2?})
=?p(r : T1, r.` : T2)3.
If T : (T1?
(.
.
.
?
(Tn?
Type) .
.
.
)), thenp(r : T ?
{?`, ?T , ?pi1, .
.
.
, pin???})
=?p(r : T, r.` :T (r.pi1, .
.
.
, r.pin) | r.pi1: T1, .
.
.
, r.pin: Tn)4In the full version of TTR we also allow absolute pathswhich point to particular records, but we will not includethem here.3 Compositional SemanticsMontague (1974) determines the denotation of acomplex expression by applying a function to anintensional argument (as in [[ NP ]]([[?VP ]])).
Weemploy a variant of this general strategy by ap-plying a probabilistic evaluation function [[ ?
]]ptoa categorical (non-probabilistic) semantic value.For semantic categories that are interpreted asfunctions, [[ ?
]]pyields functions from categoricalvalues to probabilities.
For sentences it producesprobability values.The probabilistic evaluation function [[ ?
]]ppro-duces a probabilistic interpretation based on aclassical compositional semantics.
For sentencesit will return the probability that the sentence istrue.
For categories that are interpreted as func-tions it will return functions from (categorical) in-terpretations to probabilities.
We are not propos-ing strict compositionality in terms of probabili-ties.
Probabilities are like truth-values (or rather,truth-values are the limit cases of probabilities).We would not expect to be able to compute theprobability associated with a complex constituenton the basis of the probabilities associated with itsimmediate constituents, any more than we wouldexpect to be able to compute a categorical inter-pretation entirely in terms of truth-functions andextensions.
However, the simultaneous computa-tion of categorical and probabilistic interpretationsprovides us with a compositional semantic systemthat is closely related to the simultaneous com-putation of intensions and extensions in classicalMontague semantics.The following definition of [[ ?
]]pfor a fragmentof English is specified on the basis of our proba-bilistic type system and a non-probabilistic inter-pretation function [[ ?
]], which we do not give inthis version of the paper.
(It?s definition is givenby removing the probability p from the definitionbelow.
)[[ [SS1and S2] ]]p= p([e1:[[ S1]]e2:[[ S2]]])[[ [SS1or S2] ]]p= p([e:[[ S1]]?
[[ S2]]])[[ [SNeg S] ]]p= [[ Neg ]]p([[ S ]])[[ [SNP VP] ]]p= [[ NP ]]p([[ VP ]])[[ [NPDet N] ]]p= [[ Det ]]p([[ N ]])[[ [NPNprop] ]]p= [[ Nprop]]p[[ [VPVtNP] ]]p= [[ Vt]]p([[ NP ]])[[ [VPVi] ]]p= [[ Vi]]p[[ [Neg?it?s not true that?]
]]p= ?T :RecType(p([e:?T]))[[ [Det?some?]
]]p= ?Q:Ppty(?P :Ppty(p([e:some(Q, P )])))[[ [Det?every?]
]]p= ?Q:Ppty(?P :Ppty(p([e:every(Q, P )])))[[ [Det?most?]
]]p= ?Q:Ppty(?P :Ppty(p([e:most(Q, P )])))75[[ [N?boy?]
]]p= ?r:[x:Ind](p([e:boy(r.x)]))[[ [N?girl?]
]]p= ?r:[x:Ind](p([e:girl(r.x)]))[[ [Adj?green?]
]]p=?P :Ppty(?r:[x:Ind](p(([e:green(r.x,P )])))))[[ [Adj?imaginary?]
]]p=?P :Ppty(?r:[x:Ind](p(([e:imaginary(r.x,P )])))))5[[ [Nprop?Kim?]
]]p= ?P :Ppty(p(P ([x=kim])))[[ [Nprop?Sandy?]
]]p= ?P :Ppty(p(P ([x=sandy])))[[ [Vt?knows?]
]]p=?P:Quant(?r1:[x:Ind](p(P(?r2:([e:know(r1.x,r2.x)])))))[[ [Vt?sees?]
]]p=?P:Quant(?r1:[x:Ind](p(P(?r2:([e:see(r1.x,r2.x)])))))[[ [Vi?smiles?]
]]p= ?r:[x:Ind](p([e:smile(r.x)]))[[ [Vi?laughs?]
]]p= ?r:[x:Ind](p([e:laugh(r.x)]))A probability distribution d for this fragment,based on a set of situations S, is such that:pd(a : Ind) = 1 if a is kim or sandy6pd(s : T ) ?
[0, 1] if s ?
S and T is a ptypepd(s : T ) = 0 if s 6?
S and T is a ptype7pd(a : [?P ]) = pd(P ([x=a]))pd(some(P,Q)) = pd([?P ] ?
[?Q])pd(every(P,Q)) = pd([?P ]?
[?Q])pd(most(P,Q)) = min(1,pd([?P ]?
[?Q]?mostpd([?P ]))The probability that an event e is of the type inwhich the relation some holds of the properties PandQ is the probability that e is of the conjunctivetype P ?Q.
The probability that e is of the everytype for P and Q is the likelihood that it instanti-ates the functional type P ?
Q.
As we have de-fined the probabilities associated with functionaltypes in terms of universal quantification (an un-bounded conjunction of the pairings between theelements of the domain P of the function and itsrange Q), this definition sustains the desired read-ing of every.
The likelihood that e is of the typemost for P and Q is the likelihood that e is oftype P ?Q, factored by the product of the contex-tually determined parameter ?mostand the likeli-hood that e is of type P , where this fraction is lessthan 1, and 1 otherwise.Consider a simple example.
[[ [S[NP[NpropKim]] [VP[Vismiles]]] ]]p=?P :Ppty(p(P ([x=kim])))(?r:[x:Ind]([e:smile(r.x)])) =p(?r:[x:Ind]([e:smile(r.x)])([x=kim])) =p([e:smile(kim)])5Notice that we characterize adjectival modifiers as rela-tions between records of individuals and properties.
We canthen invoke subtyping to capture the distinction between in-tersective and non-intersective modifier relations.6This seems an intuitive assumption, though not a neces-sary one.7Again this seems an intuitive, though not a necessary as-sumption.Suppose that pd(s1:smile(kim)) = .7,pd(s2:smile(kim)) = .3, pd(s3:smile(kim)) =.4, and there are no other situations sisuchthat pd(si:smile(kim)) > 0.
Furthermore, letus assume that these probabilities are indepen-dent of each other, that is, pd(s3:smile(kim)) =pd(s3:smile(kim) | s1:smile(kim), s2:smile(kim))and so on.
Thenpd(smile(kim))=?pd(s1: smile(kim), s2: smile(kim), s3: smile(kim)) =?pd(s1: smile(kim), s2: smile(kim)) + .4 ?
.4?pd(s1:smile(kim), s2: smile(kim)) =(.7 + .3?
.7?
.3) + .4?
.4(.7 + .3?
.7?
.3) =.874This means that pd([e:smile(kim)]) = .874.Hence [[ [S[NP[NpropKim]] [VP[Vismiles]]] ]]pd= .874(where [[ ?
]]pdis the result of computing [[ ?
]]pwith respect to the probability distribution d).Just as for categorical semantics, we can con-struct type theoretic objects corresponding toprobabilistic judgements.
We call these proba-bilistic Austinian propositions.
These are recordsof type?
?sit : Sitsit-type : Typeprob : [0,1]?
?where [0,1] is used to represent the type of realnumbers between 0 and 1.
They assert that theprobability that a situation s is of type Type is thevalue of prob.The definition of [[ ?
]]pspecifies a compositionalprocedure for generating an Austinian proposition(record) of this type from the meanings of the syn-tactic constituents of a sentence.4 An Outline of Semantic LearningWe outline a schematic theory of semantic learn-ing on which agents acquire classifiers that formthe basis for our probabilistic type system.
Forsimplicity and ease of presentation we take theseto be Naive Bayes classifiers, which an agent ac-quires from observation.
In future developmentsof this theory we will seek to extend the approachto Bayesian networks (Pearl, 1990).We assume that agents keep records of observedsituations and their types, modelled as probabilis-tic Austinian propositions.
For example, an obser-vation of a man running might yield the followingAustinian proposition for some a:Ind, s1:man(a),s2:run(a):76????????
?sit =?
?ref = acman= s1crun= s2?
?sit-type =?
?ref : Indcman: man(ref)crun: run(ref)?
?prob = 0.7????????
?An agent, A, makes judgements based on afinite string of probabilistic Austinian proposi-tions, J, corresponding to prior judgements heldin memory.
For a type, T , JTrepresents that set ofAustinian propositions j such that j.sit-type v T .If T is a type and J a finite string of probabilis-tic Austinian propositions, then || T ||Jrepresentsthe sum of all probabilities associated with T in J(?j?JTj.prob).
P(J) is the sum of all probabilitiesin J (?j?Jj.prob).We use priorJ(T ) to represent the prior proba-bility that anything is of type T given J, that is||T ||JP(J)if P(J) > 0, and 0 otherwise.pA,J(s : T ) denotes the probability that agent Aassigns with respect to prior judgements J to s be-ing of type T .
Similarly, pA,J(s : T1| s : T2) isthe probability that agent A assigns with respectto prior judgements J to s being of type T1, giventhat A judges s to be of type T2.When an agent A encounters a new situations and considers whether it is of type T , he/sheuses probabilistic reasoning to determine the valueof pA,J(s : T ).
A uses conditional probabilitiesto calculate this value, where A computes theseconditional probabilities with the equation pA,J(s :T1| s : T2) =||T1?T2||J||T2||J, if || T2||J6= 0.
Otherwise,pA,J(s : T1| s : T2) = 0.This is our type theoretic variant of the stan-dard Bayesian formula for conditional probabili-ties: p(A | B) =|A&B||B|.
But instead of countingcategorical instances, we sum the probabilities ofjudgements.
This is because our ?training data?
isnot limited to categorical observations.
Instead itconsists of probabilistic observational judgementsthat situations are of particular types.8Assume that we have the following types:Tman=[ref : Indcman: man(ref)]andTrun=[ref : Indcrun: run(ref)]8As a reviewer observes, by using an observer?s previousjudgements for the probability of an event being of a partic-ular type, as the prior for the rule that computes the proba-bility of a new event being of that type, we have, in effect,compressed information that properly belongs in a Bayesiannetwork into our specification of a naive Bayesian classifier.This is a simplification that we adopt here for ease of expo-sition.
In future work, we will characterise classifier learningthrough full Bayesian networks.Assume also that JTman?Trunhas three members,corresponding to judgements by A that a man wasrunning in three observed situations s1, s3, ands4, and that these Austinian propositions have theprobabilities 0.6, 0.6. and 0.5 respectively.Take JTmanto have five members correspond-ing to judgements by A that there was a man ins1, .
.
.
, s5, and that the Austinian propositions as-signing Tmanto s1, .
.
.
, s5all have probability 0.7.Given these assumptions, the conditional probabil-ity that A will assign on the basis of J to someoneruns, given that he is a man is pA,J(r : Trun| r :Tman) =||Tman?Trun||J||Tman||J=0.6+0.6+0.50.7+0.7+0.7+0.7+0.7= .486We use conditional probabilities to construct aNaive Bayes classifier.
A classifies a new situa-tion s based on the prior judgements J, and what-ever evidence A can acquire about s. This evi-dence has the form pA,J(s : Te1), .
.
., pA,J(s : Ten),where Te1, .
.
.
, Tenare the evidence types.
TheNaive Bayes classifier assumes that the evidence isindependent, in that the probability of each pieceof evidence is independent of every other piece ofevidence.We first formulate Bayes?
rule of conditionalprobability.
This rule defines the conditional prob-ability of a conclusion r : Tc, given evidence r :Te1, r : Te2, .
.
.
, r : Ten, in terms of conditional prob-abilities of the form p(si: Tei| si: Tc), 1 ?
i ?
n,and priors for conclusion and evidence:pA,J(r : Tc| r : Te1, .
.
.
, r : Ten) =priorJ(Tc)||Te1?Tc||J||Tc||J...||Ten?Tc||J||Tc||JpriorJ(Te1)...priorJ(Ten)The conditional probabilities are computedfrom observations as indicated above.
The rule ofconditional probability allows the combination ofseveral pieces of evidence, without requiring pre-vious observation of a situation involving all theevidence types.We formulate a Naive Bayes classifier as a func-tion from evidence types Te1, Te2, .
.
.
, Ten(i.e.
froma record of type Te1?
Te2?
.
.
.
?
Ten) to conclusiontypes Tc1, Tc2, .
.
.
, Tcm.
The conclusion is a disjunc-tion of one or more T ?
{Tc1, Tc2, .
.
.
, Tcm}, wherem ranges over all possible non-disjunctive conclu-sions distinguished by the classifier.
This functionis specified as follows.?
: (Te1?
.
.
.?Ten)?
(Tc1?
.
.
.
?Tcm) such that ?
(r) =(?argmaxT?
?Tc1,...,Tcm?pA,J(r : T | r : Te1, .
.
.
, r : Ten)The classifier returns the type T which max-imises the conditional probability of r : T given77the evidence provided by r. The argmax operatorhere takes a sequence of arguments and a func-tion and yields a sequence containing the argu-ments which maximise the function (if there aremore than one).The classifier will output a disjunction in caseboth possibilities have the same probability.
The?operator takes a sequence and returns the dis-junction of all elements of the sequence.In addition to computing the conclusion whichreceives the highest probability given the evi-dence, we also want the posterior probability ofthe judgement above, i.e.
the probability of thejudgement in light of the evidence.
We obtain thenon-normalised probabilities (pnnA,J) of the differentpossible conclusions by factoring in the probabili-ties of the evidence:pnnA,J(r : ?
(r)) =?T???1?
(r)pA,J(r : T | r : Te1, .
.
.
, r : Ten)pA,J(r :Te1) .
.
.
pA,J(r : Ten)where?
?1is the inverse of?, i.e.
a function thattakes a disjunction and returns the set of disjuncts.We then take the probability of r : ?
(r) andnormalise over the sum of the probabilities ofall the possible conclusions.
This gives us thenormalised probability of the judgement resultingfrom classification p(r : ?
(r)) =pnnA,J(r:?
(r))?1?i?mpnnA,J(r:Tci).However, since the probabilities of the evidenceare identical for all possible conclusions, we canignore them and instead compute the normalisedprobability with the following equation (where mranges over all possible non-disjunctive conclu-sions distinguished by the classifier, as above).pA,J(r : ?
(r)) =?T???1?
(r)pA,J(r:T |r:Te1,...,r:Ten)?1?i?mpA,J(r:Tci|r:Te1,...,r:Ten)The result of classification can be represented asan Austinian proposition?
?sit = ssit-type = ?
(s)prob = pA,J(s : ?(s))?
?which A adds to J as a result of observing andclassifying s, and is thus made available for sub-sequent probabilistic reasoning.5 Conclusions and Future WorkWe have presented a probabilistic version of a richtype theory with records, relying heavily on classi-cal equations for types formed with meet, join, andnegation.
This has permitted us to sustain classi-cal equivalences and Boolean negation for com-plex types within an intensional type theory.
Wehave replaced the truth of a type judgement withthe probability of it being the case, and we haveapplied this approach to judgements that a situa-tion if of type T .Our probabilistic formulation of a rich type the-ory with records provides the basis for a compo-sitional semantics in which functions apply to cat-egorical semantic objects in order to return eitherfunctions from categorical interpretations to prob-abilistic judgements, or, for sentences, to proba-bilistic Austinian propositions.
One of the inter-esting ways in which this framework differs fromclassical model theoretic semantics is that the ba-sic types and type judgements at the foundation ofthe type system correspond to perceptual judge-ments concerning objects and events in the world,rather than to entities in a model and set theoreticconstructions defined on them.We have offered a schematic view of semanticlearning.
On this account observations of situa-tions in the world support the acquisition of naiveBayesian classifiers from which the basic proba-bilistic types of our type theoretical semantics areextracted.
Our type theory is, then, the interfacebetween observation-based learning of classifiersfor objects and the situations in which they figureon one hand, and the computation of complex se-mantic values for the expressions of a natural lan-guage from these simple probabilistic types andtype judgements on the other.
Therefore our gen-eral model of interpretation achieves a highly in-tegrated bottom-up treatment of linguistic mean-ing and perceptually-based cognition that situatesmeaning in learning how to make observationaljudgements concerning the likelihood of situationsobtaining in the world.The types of our semantic theory are inten-sional.
They constitute ways of classifying situ-ations, and they cannot be reduced to set of situa-tions.
The theory achieves fine-grained intension-ality through a rich and articulated type system,where the foundation of this system is anchored inperceptual observation.The meanings of expressions are acquired onthe basis of speakers?
experience in the applica-tion of classifiers to objects and events that theyencounter.
Meanings are dynamic and updated inlight of subsequent experience.78Probability is distributed over alternative situ-ation types.
Possible worlds, construed as maxi-mal consistent sets of propositions (ultrafilters in aproof theoretic lattice of propositions) play no rolein this framework.Bayesian reasoning from observation providesthe incremental basis for learning and refiningpredicative types.
These types feed the combina-torial semantic procedures for interpreting the sen-tences of a natural language.In future work we will explore implementationsof our learning theory in order to study the viabil-ity of our probabilistic type theory as an interfacebetween perceptual judgement and compositionalsemantics.
We hope to show that, in addition toits cognitive and theoretical interest, our proposedframework will yield results in robotic languagelearning, and dialogue modelling.AcknowledgmentsWe are grateful to two anonymous reviewers forvery helpful comments on an earlier draft of thispaper.
We also thank Alex Clark, JekaterinaDenissova, Raquel Fern?andez, Jonathan Ginzburg,Noah Goodman, Dan Lassiter, Michiel van Lam-balgen, Poppy Mankowitz, Aarne Ranta, and Pe-ter Sutton for useful discussion of ideas presentedin this paper.
Shalom Lappin?s participation inthe research reported here was funded by grantES/J022969/1 from the Economic and Social Re-search Council of the UK, and a grant from theWenner-Gren Foundations.
We also gratefully ac-knowledge the support of Vetenskapsr?adet, project2009-1569, Semantic analysis of interaction andcoordination in dialogue (SAICD); the Depart-ment of Philosophy, Linguistics, and Theory ofScience; and the Centre for Language Technologyat the University of Gothenburg.ReferencesJon Barwise and John Perry.
1983.
Situations andAttitudes.
Bradford Books.
MIT Press, Cambridge,Mass.I.
Beltagy, C. Chau, G. Boleda, D. Garrette, K. Erk,and R. Mooney.
2013.
Montague meets markov:Deep semantics with probabilistic logical form.
InSecond Joint Conference on Lexical and Computa-tional Semantics, Vol.
1, pages 11?21.
Associationof Computational Linguistics, Atlanta, GA.R.
Carnap.
1947.
Meaning and Necessity.
Universityof Chicago Press, Chicago.A.
Clark and S. Lappin.
2011.
Linguistic Nativismand the Poverty of the Stimulus.
Wiley-Blackwell,Chichester, West Sussex, and Malden, MA.Robin Cooper.
2012.
Type theory and semantics influx.
In Ruth Kempson, Nicholas Asher, and TimFernando, editors, Handbook of the Philosophy ofScience, volume 14: Philosophy of Linguistics.
El-sevier BV, 271?323.
General editors: Dov M. Gab-bay, Paul Thagard and John Woods.C.
Fox and S. Lappin.
2010.
Expressiveness andcomplexity in underspecified semantics.
LinguisticAnalysis, Festschrift for Joachim Lambek, 36:385?417.J.
Halpern.
2003.
Reasoning About Uncertainty.
MITPress, Cambridge MA.H.
Kamp and U. Reyle.
1993.
From Discourse toLogic: Introduction to Modeltheoretic Semanticsof Natural Language, Formal Logic and DiscourseRepresentation Theory.
Kluwer, Dordrecht.A.N.
Kolmogorov.
1950.
Foundations of Probability.Chelsea Publishing, New York.Per Martin-L?of.
1984.
Intuitionistic Type Theory.
Bib-liopolis, Naples.Richard Montague.
1974.
Formal Philosophy: Se-lected Papers of Richard Montague.
Yale UniversityPress, New Haven.
ed.
and with an introduction byRichmond H. Thomason.C.
Papadimitriou.
1995.
Computational Complexity.Addison-Wesley Publishing Co., Readin, MA.J.
Paris.
2010.
Pure inductive logic.
Winter School inLogic, Guangzhou, China.J.
Pearl.
1990.
Bayesian decision methods.
InG.
Shafer and J. Pearl, editors, Readings in Uncer-tain Reasoning, pages 345?352.
Morgan Kaufmann.79
