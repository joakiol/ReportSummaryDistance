Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 205?208, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSemantic Role Labeling using libSVMNecati Ercan Ozgencil Nancy McCrackenCenter for Natural Language Processing Center for Natural Language ProcessingSchool of Engineering and Computer Science School of Information StudiesSyracuse University Syracuse Universityneozgenc@ecs.syr.edu njm@ecs.syr.eduAbstractWe describe a system for the CoNLL-2005 shared task of Semantic Role Label-ing.
The system implements a two-layerarchitecture to first identify the argumentsand then to label them for each predicate.The components are implemented asSVM classifiers using libSVM.
Featureswere adapted and tuned for the system,including a reduced set for the identifierclassifier.
Experiments were conducted tofind kernel parameters for the Radial Ba-sis Function (RBF) kernel.
An algorithmwas defined to combine the results of theargument labeling classifier according tothe constraints of the argument labelingproblem.1 Introduction and StrategyThe Semantic Role Labeling (SRL) problem hasbeen the topic of the both the CoNLL-2004 and theCoNLL-2005 Shared Tasks (Carreras andM?rquez, 2005).
The SRL system described heredepends on a full syntactic parse from the Charniakparser, and investigates aspects of using SupportVector Machines (SVMs) as the machine learningtechnique for the SRL problem, using the libSVMpackage.In common with many other systems, this sys-tem uses the two-level strategy of first identifyingwhich phrases can be arguments to predicates ingeneral, and then labeling the arguments accordingto that predicate.
The argument identificationphase is a binary classifier that decides whethereach constituent in the full syntax tree of the sen-tence is a potential argument.
These potential ar-guments are passed into the argument labelingclassifier, which uses binary classifiers for eachlabel to decide if that label should be given to thatargument.
A post-processing phase picks the bestlabeling that satisfies the constraints of labeling thepredicate arguments.For overall classification strategy and forsuggestions of features, we are indebted to thework of Pradhan et al(2005) and to the work ofmany authors in both the CoNLL-2004 shared taskand the similar semantic roles task of Senseval-3.We used the results of their experiments withfeatures, and worked primarily on features for theidentifying classifier and with the constraintsatisfaction problem on the final argument output.2 System Description2.1 Input DataIn this system, we chose to use full syntax treesfrom the Charniak parser, as the constituents ofthose trees more accurately represented argumentphrases in the training data at the time of the datarelease.
Within each sentence, we first map thepredicate to a constituent in the syntax tree.
In thecases that the predicate is not represented by a con-stituent, we found that these were verb phrases oflength two or more, where the first word was themain verb (carry out, gotten away, served up, etc.
).In these cases, we used the first word constituent asthe representation of the predicate, for purposes ofcomputing other features that depended on a rela-tive position in the syntax tree.205We next identify every constituent in the tree asa potential argument, and label the training dataaccordingly.
Although approximately 97% of thearguments in the training data directly matchedconstituents in the Charniak tree, only 91.3% of thearguments in the development set match constitu-ents.
Examination of the sentences with incorrectparses show that almost all of these are due tosome form of incorrect attachment, e.g.
preposi-tional attachment, of the parser.
Heuristics can bederived to correct constituents with quotes, but thisonly affected a small fraction of a percent of theincorrect arguments.
Experiments with correctionsto the punctuation in the Collins parses were alsounsuccessful in identifying additional constituents.Our recall results on the development directory arebounded by the 91.3% alignment figure.We also did not use the the partial syntax,named entities or the verb senses in thedevelopment data.2.2 Learning Components:  SVM classifiersFor our system, we chose to use libSVM, an opensource SVM package (Chang and Lin, 2001).In the SRL problem, the features are nominal,and we followed the standard practice of represent-ing a nominal feature with n discrete values as nbinary features.
Many of the features in the SRLproblem can take on a large number of values, forexample, the head word of a constituent may takeon as many values as there are different words pre-sent in the training set, and these large number offeatures can cause substantial performance issues.The libSVM package has several kernel func-tions available, and we chose to use the radial basisfunctions (RBF).
For the argument labeling prob-lem, we used the binary classifiers in libSVM, withprobability estimates of how well the label fits thedistribution.
These are normally combined usingthe ?one-against-one?
approach into a multi-classclassifier.
Instead, we combined the binary classi-fiers in our own post-processing phase to get a la-beling satisfying the constraints of the problem.2.3 The Identifier Classifier FeaturesOne aspect of our work was to use fewer featuresfor the identifier classifier than the basic feature setfrom (Gildea and Jurafsky, 2002).
The intuitionbehind the reduction is that whether a constituentin the tree is an argument depends primarily on thestructure and is independent of the lexical items ofthe predicate and headword.
This reduced featureset is:Phrase Type: The phrase label of the argument.Position:  Whether the phrase is before or afterthe predicate.Voice:  Whether the predicate is in active orpassive voice.
Passive voice is recognized if a pastparticiple verb is preceded by a form of the verb?be?
within 3 words.Sub-categorization:  The phrase labels of thechildren of the predicate?s parent in the syntax tree.Short Path: The path from the parent of theargument position in the syntax tree to the parentof the predicate.The first four features are standard, and the shortpath feature is defined as a shorter version of thestandard path feature that does not use theargument phrase type on one end of the path, northe predicate type on the other end.The use of this reduced set of features wasconfirmed experimentally by comparing the effectof this reduced feature set on the F-measure of theidentifier classifier, compared to feature sets thatalso added the predicate, the head word and thepath features, as normally defined.Reduced + Pred + Head + PathF-measure 81.51 81.31 72.60 81.19Table 1:  Additional features reduce F-measure for theidentifier classifier.2.4 Using the Identifier Classifier for Train-ing and TestingTheoretically, the input for training the identifierclassifier is that, for each predicate, all constituentsin the syntax tree are training instances, labeledtrue if it is any argument of that predicate, andfalse otherwise.
However, this leads to too manynegative (false) instances for the training.
To cor-rect this, we experimented with two filters fornegative instances.
The first filter is simply a ran-dom filter; we randomly select a percentage of ar-guments for each argument label.
Experimentswith the percentage showed that 30% yielded thebest F-measure for the identifier classifier.The second filter is based on phrase labels fromthe syntax tree.
The intent of this filter was to re-move one word constituents of a phrase type thatwas never used.
We selected only those phrase206labels whose frequency in the training was higherthan a threshold.
Experiments showed that the bestthreshold was 0.01, which resulted in approxi-mately 86% negative training instances.However, in the final experimentation, compari-son of these two filters showed that the randomfilter was best for F-measure results of the identi-fier classifier.The final set of experiments for the identifierclassifier was to fine tune the RBF kernel trainingparameters, C and gamma.
Although we followedthe standard grid strategy of finding the best pa-rameters, unlike the built-in grid program oflibSVM with its accuracy measure, we judged theresults based on the more standard F-measure ofthe classifier.
The final values are that C = 2 andgamma = 0.125.The final result of the identifier classifier trainedon the first 10 directories of the training set is:Precision:  78.27%         Recall:  89.01%(F-measure:  83.47)Training on more directories did not substan-tially improve these precision and recall figures.2.5   Labeling Classifier FeaturesThe following is a list of the features used in thelabeling classifiers.Predicate:  The predicate lemma from thetraining file.Path:  The syntactic path through the parse treefrom the argument constituent to the predicate.Head Word:  The head word of the argumentconstituent, calculated in the standard way, butalso stemmed.
Applying stemming reduces thenumber of unique values of this featuresubstantially, 62% in one directory of training data.Phrase Type, Position, Voice, and Sub-categorization:  as in the identifier classifier.In addition, we experimented with the followingfeatures, but did not find that they increased thelabeling classifier scores.Head Word POS:  the part of speech tag of thehead word of the argument constituent.Temporal Cue Words:  These words werecompiled by hand from ArgM-TMP phrases in thetraining data.Governing Category:  The phrase label of theparent of the argument.Grammatical Rule:  The generalization of thesubcategorization feature to show the phrase labelsof the children of the node that is the lowest parentof all arguments of the predicate.In the case of the temporal cue words, wenoticed that using our definition of this featureincreased the number of false positives for theARGM-TMP label; we guess that our temporal cuewords included too many words that occured inother labels.
Due to lack of time, we were notable to more fully pursue these features.2.6  Using the Labeling Classifier for Train-ing and TestingOur strategy for using the labeling classifier isthat in the testing, we pass only those arguments tothe labeling classifier that have been marked astrue by the identifier classifier.
Therefore, fortraining the labeling classifier, instances were con-stituents that were given argument labels in thetraining set, i.e.
there were no ?null?
training ex-amples.For the labeling classifier, we also found thebest parameters for the RBF kernel of the classi-fier.
For this, we used the grid program of libSVMthat uses the multi-class classifier, using the accu-racy measure to tune the parameters, since thiscombines the precision of the binary classifiers foreach label.
The final values are that C = 0.5 andgamma = 0.5.In order to show the contribution of the labelingclassifier to the entire system, a final test was doneon the development set, but passing it the correctarguments.
We tested this with a labeling classi-fier trained on 10 directories and one trained on 20directories, showing the final F-measure:10 directories:  83.2720 directories:  84.512.7 Post-processing the classifier labelsThe final part of our system was to use the resultsof the binary classifiers for each argument label toproduce a final labeling subject to the labeling con-straints.For each predicate, the constraints are:  two con-stituents cannot have the same argument label, aconstituent cannot have more than one label, if twoconstituents have (different) labels, they cannothave any overlap, and finally, no argument canoverlap the predicate.207Precision Recall F?=1Development 73.57% 71.87% 72.71Test WSJ 74.66% 74.21% 74.44Test Brown 65.52% 62.93% 64.20Test WSJ+Brown 73.48% 72.70% 73.09Test WSJ Precision Recall F?=1Overall 74.66% 74.21% 74.44A0 83.59% 85.07% 84.32A1 77.00% 74.35% 75.65A2 66.97% 66.85% 66.91A3 66.88% 60.69% 63.64A4 77.66% 71.57% 74.49A5 80.00% 80.00% 80.00AM-ADV 55.13% 50.99% 52.98AM-CAU 52.17% 49.32% 50.70AM-DIR 27.43% 56.47% 36.92AM-DIS 73.04% 72.81% 72.93AM-EXT 57.69% 46.88% 51.72AM-LOC 50.00% 49.59% 49.79AM-MNR 54.00% 54.94% 54.47AM-MOD 92.02% 94.19% 93.09AM-NEG 96.05% 95.22% 95.63AM-PNC 35.07% 40.87% 37.75AM-PRD 50.00% 20.00% 28.57AM-REC 0.00% 0.00% 0.00AM-TMP 68.69% 63.57% 66.03R-A0 77.61% 89.73% 83.23R-A1 71.95% 75.64% 73.75R-A2 87.50% 43.75% 58.33R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 100.00% 50.00% 66.67R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 66.67% 85.71% 75.00R-AM-MNR 8.33% 16.67% 11.11R-AM-TMP 66.67% 88.46% 76.03V 97.32% 97.32% 97.32Table 2:  Overall results (top) and detailed results on theWSJ test (bottom).To achieve these constraints, we used the prob-abilities produced by libSVM for each of the bi-nary argument label classifiers.
We produced aconstraint satisfaction module that uses a greedyalgorithm that uses probabilities from the matrix ofpotential labeling for each constituent and label.The algorithm iteratively chooses a label for a nodewith the highest probability and removes any po-tential labeling that would violate constraints withthat chosen label.
It continues to choose labels fornodes until all probabilities in the matrix are lowerthan a threshold, determined by experiments to be.3.
In the future, it is our intent to replace thisgreedy algorithm with a dynamic optimization al-gorithm.3 Experimental Results3.1     Final System and ResultsThe final system used an identifier classifiertrained on (the first) 10 directories, in approxi-mately 7 hours, and a labeling classifier trained on20 directories, in approximately 23 hours.
Testingtook approximately 3.3 seconds per sentence.As a further test of the final system, we trainedboth the identifier classifier and the labeling classi-fier on the first 10 directories and used the second10 directories as development tests.
Here are someof the results, showing the alignment and F-measure on each directory, compared to 24.Directory: 12 14 16 18 20 24Alignment 95.7 96.1 95.9 96.5 95.9 91.3F-measure 80.4 79.6 79.0 80.5 79.7 71.1Table 3:  Using additional directories for testingFinally, we note that we did not correctly antici-pate the final notation for the predicates in the testset for two word verbs.
Our system assumed thattwo word verbs would be given a start and an end,whereas the test set gives just the one word predi-cate.ReferencesXavier Carreras and Llu?s M?rquez, 2005.
Introductionto the CoNLL-2005 Shared Task:  Semantic RoleLabeling, Proceedings of CoNLL-2005.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM :a library for support vector machines.
Softwareavailable at http://www.csie.ntu.edu.tw/~cjlin/libsvmDaniel Gildea and Daniel Jurafsky, 2002.
AutomaticLabeling of Semantic Roles.
Computational Linguis-tics 28(3):245-288.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin, and Daniel Jurafsky,2005.
Support Vector Learning for Semantic Argu-ment Classification,  To appear in Machine Learningjournal, Special issue on Speech and Natural Lan-guage Processing.208
