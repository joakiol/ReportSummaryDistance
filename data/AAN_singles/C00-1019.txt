Automated Generalization of Translation ExamplesRa l f  D .
Brown()a.lnc'g'c Mellon Univclsity, l~a.nguage Technologies 111stil;ul:('.l)ittsl)urgh, I)A \]5213-3890ralf+ ~,/cs.
(;1 n u. cduAbst ractl~t:ovious work has shown thai adding gen-era.liza.tion of the exa.ml)les in the corpus ofa.n exa.ml)le-1)ased machine tra.nsla.tion (I'31LMT)system ea, n reduce 1;he re(ltfire.d amount  o\[' pre-tra.nsla.ted exa.ml)le text l)y as \[iltl(;\]l }is a.ii ordero\[' magnitude for Spa.nish-l';nglish and l,'rench-l~;nglish I+',I~Mrl '.
Using word clusto.t:itlg to a.tt-toma.ticaJly generalize the example eorl>uS ca.nprovide the majority o\[' this inlprovement forl,'rench-l'hlglish wil;h no nlanuaI illtervelltioll;the prior work required a. la.rge I)iliugual dic-lionary ta.gged wil;}l 1)a.rls of speech aud themanual crea.tion of gl'.%llllll.
:ll" rules.
/~y seedingthe clustering with a. small a.mou nt of manually-crea.ted iM'orma.tion, even t)el;ter t)erl'ornla.nceea.n be a.chieved.
This pa.l)ev descril)es a. methodwhereby bilingual word clustering ca.n 1)e per-\[brined using sta.nda.rd 'nto,zoli'n.qttal documentcl ustering techniques, a,nd its e\[l'ectiveness at re-d ucing the.
size of the exam l)le corpus ,'eq u ire(I.1 h l t roduct ionI';xanq)le-I{ased Machine 'l'ranslaLion (I';I{M'I')relies on a. collection of textual units (usuallysentences) and their  tra, nsla, l,ions.
New text  |,o1)e tra, nsla, ted is nla,tched a,ga, inst the source-langua.ge ha.If of the colh'x;tion, and the corre-sponding tra.nsla.tions from the ta.rget-langua.gehalf axe used to generate a. l;ra.nsh~tion of thenew text.l~xperience with several language pairs hasshown that producing a.n EBMT system whichprovides reasomt.ble t, ra.nsla.tion coverage of un-restricted texts using simple textual matchingrequires on the order of two million words ofpre-translated texts (one million words in eachl;mguage); if either la.nguage is highly in\[letting,polysynthetic, or (worse yet) a.gglu tina.tive, evenllloro text will be required.
It ma.y I)e difficult,time-consuming, and expensive to obtain tha.tmuch pa.rallel text, pa.rtieula.rly for lesser-usedla.nguage pairs.
Thus, it' one' wishes to developa.
new tr,~nslator ra.pidly a.nd a.t low cost, tech-niques are needed which permit the 131~MT sys~tom to 1)erform just as well using substantia.llyless example text.lk~th the C,a.ijin Ii;I~MT system 1)y Veale and~ " r \Va.y (\]997) and 1,he a.uthor's l~\]~h/l I sySteln(I999) COllVel't {;he examples in the corpus intoteml)la.tes against which the new texts ea.n I)ema.tched.
(la.ijin va.ria.I)lizes the well-formedsegment mappings between source a.nd ta.rgetsentences 1;}ta.t i  is able to find, using a. closedset o\[' markers to segment 1.he input into l)hrasos.q'he a.utllor'.~ syslem i)er\['orms its generaliza.tiollusing equix,a.lence classes (both syntactic a.nd se-ma.ntic) a.nd a. production-rule grammar.
First,any occurrences of terms conta,ined in a,n equiv-alence class are replaced l)y a. token giving 1.hename of the equiwdence (:lass, a.nd then thegramma.r ules a~re used to replace l)a.tterns ofwords a.nd tokens I)y more genera.l tokens (suchas <NI '> for noun phrases).
(\]{town, 1999)showed t\]la.t one ca.n reduce the corpus size byas much as a.\]l order o\[' ma.gnitude in this way.
(liven l;ha.t, explicit, ma.llua.lly-gom~ra.ted equi-va.lence classes red uce the need for exam l)le text,an obvious extelmion would l)e I;o ;~tte\]nl)t logelleral.e tll(~se classes a.ul;olna.tica.l\[y frolll thecorpus of pre-tra.nslated exanlples.
This pa.-1)or describes ()lie ~q)l)roa,ch to a.utoma.ted ex-1;racl;ioll of equiva.lence classes, using clusteringteclmiques.The rema.inder of this l)aper describes howto 1)erform bilingua.1 word clustering using stan-dard monoh;ngual document clustering tech-niques 1)y converting the problem space; theva.rious clustering algorithms which were inves-tiga.ted; mid the effectiveness of generaliza.tionusing the derived clusters a.t reducing the re-quired amount of example text.2 Conver t ing  the  Prob lemThe task of clustering words a.ccording to theiroccurrence pa, tterns ca, n 1)e testa,ted as a, sta, n-dard document-clustering task by convertingthe l)rol)lem sl)a.ce.
For each unique word to beclllstered, crea.te a. l)seudo-doculnent conta.iningthe words of the contexts in which theft word N)-125pears, and use the word itself as tile documentidentifier.
After the pseudo-documents are clus-tered, retrieving the identitier for each docu-ment in a particular cluster l)roduces tile list ofwords occurring in su\[\[iciently similar contextsto be considered equivalent \['or the l)urposes ofgeneralizing an EBM(1 ~ system.By itself, this approach only produces amonolingual clustering, but we require a, bilin-gum clustering fox" proper generalization sincedifferent senses of a word will appear in differingcontexts.
The method of Barrachina and Vilar(1999) provides the means for injecting bilingualinformation into the clustering process.Using a bilingual dictionary - -  which may becreated fl'om the corl)us using statistical meth-()<Is, such as those of Peter \]~rown el al (71990) orthe author's own l)r(~viotls?
work (Brown, 11997)and the parallel text, create a rough ma.pping1)etween the words in the source-language half ofeach translation example in tile corpus and tiletarget-language half el'that example.
Wheneverthere is exactly one l)ossible translation candi-date listed for a word by the mapping, generatea bilingual word pair consisting of the word andits translation.
This word pair will be treatedas an indivisible token in further processing,adding bilingual information to the clusteringprocess.
\]eorming 1)airs in this manner causeseach distinct translation of a. word to be treatedas a separate sense; although translation pairsdo not exactly correspond to word senses, pairscan be formed without any additional knowl-edge sonrces and are what tile EBM:I' systernrequires for its equivalence classes.1,'or every unique word pair found in the 1)re-vious step, we a.ccurnulate counts for each wordin the surrounding context of its occurrences.The context of ~n occurrence is defined to betile N words immediately prior to and the Nwords immediately following the occurrence; Ncurrently is set to 3.
Because word order is im-portant, counts are accumulated separately foreach position within the context, i.e.
for N = 3,a particular context word may contribute to anyof six different counts, depending on its loca-tion relative to the occurrence.
Further, as thedistance ffoln the occurrence increases, the sur-rounding words become less likely to be a truepart of the word-pair's context, so tile countsare weighted to give the greatest importanceto the words immediately adjacent o the wordpair being examined.
Currently, a silnple lineardecay fl'om 1.0 to -~ is used, but other decayfunctions such as the reciprocal of the distanceare also possible.
Tile resulting weighted set ofword counts tbrms the above-mentioned I)seudo-document which is converted into a term vectorIbr cosine similarity computations (a standaMmeasure in information retrieval, defined as thedot product of two term vectors normalized tounit length),If the clustering is seeded with a. set of ini-tial equivalence classes (which will be discussedbelow), then the equivalences will be used togeneralize the contexts as they are added to tileoverall counts \['or tile word pair.
Any words inthe context for which a unique correspondencecan be found (and f'or which the word and itscorresponding translation are one of the pah:sin an equivalence class) will be counted as if thename of the equivMence class had been l)resentin the text rather than the original word.
Forexample, if days of the week are an equivalenceclass, then ':(lid he come on Fridas:' and "didhe leave on Mends3:' will yield identical con-text vectors for "come" and "leave", maldng iteasier \['or those two terms to chlster together.To illustrate the conversion process, considertile li'rench word "('inq" in two examl)les whereit translates into English as ::five" (thus formingtile word pair "cinq_fi ve") :<NUt> <NI/L> Le ci,zq jours dcpuis la<NUL> <NUL> 73e five dags si~zce lheellcs com'me~,cc~w~,t c~z cinq jours .<NUL>they will begin i~), five days .<NUL>where <NUt> is used as a placeholder whenthe word pair is too near the beginning or endof the sentence for the flfll context o be present.Note that the word order on the target-languageside \]s not considered when building the termvector, so it need llOt be the same as on thesource-language side; the examples were chosenwith the same word order merely for clarity.The resulting ternl vector for "cinqJive" isa.s follows, where the numbers in parenthesesindicate the context word's position relative tothe word pair under consideration:Word Occur Weight<NWl.>(-3) 1 0.333elles(-3) 1 0.3331 0.667commenceront(-2) 1 0.667Le(q) 1 1.oooen(-1) 1 1.000jours(J) 2 2.000depuis(2) 1 0.667.
(2) 1 0.667la(3) 1 0.333<NUL>(3)  1 0.333Term vectors such as tile above are then clus-tered to determine equivalent usages amongwords.1263 Cluster ing  ApproachesA tota.l of six clustering a.lgoHthms ha.v(~ I)oen1.ested; th roe variants of grout)-a.vora.go.
('\]tlsl.
('.,'-ins a.nd i, hree of agglomera.tive clustering.
In-cl'omental group-a.vera.ge clustering was ilnple-mented tirst, to provide a. proof of concopt,borore the COml)uta.tiona.lly more expensive a.g-glomerative (bottom-up) clusteril~g was i lnple-mented.The incremental groul)-a.vera.ge a.lgoril;hms allexa.mine each word pair in turn, computing asimilsu:ity measure to evory existing clustor.
Ifth(; 1)(;st siinila.rity measur(; is a l)ov(~ a. l)r(;del;er -nfin('d threshold, the new word pair is i)lacedin tile corresponding cluster; otherwis% a now(;\]usi;er is crea.ted.
The th roe varianl;s diltT, r onlyin tile simila.rity moasure eml)loyed::1. cosin(; s imi lar i ty 1)(;1;w(~(;n 1,h(~ i)s(;u(lo<loc-umonl, a.nd the centroid o1" the oxisting clus-ter (standard grOUl)-a.vera.ge clusto.rillg;)2. a.verage of' i;\]lo cosine similaril;ies l)otwe(;nthe l)seudo-docuni(;nl; a.nd all nl(;nll)ers o\['the 0xisting (:lust(;,' (a.voragc-link clustor-ing)3. square root of' 1;h(; a.vcrag(; of 1;lie S(luaredcosine simila.r\]l;io.s I)ctweon l;he l)seudo-( locuinent an(\] all molnl)(~,'s or l he existing('hlster (rool.-nloa.n-sqllar(, nlo(lifical.ion ofaverage-liNl?
clustering)Thoso i;hro(~ vnria.tiol,S give hlc,'eas\]ngly IIl()l'(':weight to 1,ho nea.rer mcml)ers of' tho oxist.ingcl ust;cr.Tim t)o(;1;oin-u 1) a.gglomera.tive algoril;hms allfuncl;ion I)y (;tea.tills a. clustor For each I)Seudo-(\[o(:unlenl,, t;hon r(;i)(;a.1;(;(lly ln(u:ging l:li(; twoclusl;ors wit l i  the \]iighesl; siinila.ril,y score unl,il110 (,WO C\]tlS|,orH \]lSt,vo ,% ,q\] i i l i la,r i l ;y .~(:Ol'(~ (~x('.
(~(;d-ing a l)re(Iol;ornlino(\] 1;hl:eshold.
The three vari--;/,IIi;S }/,ga, ill differ ()lily ill 1;lio S\]liiilaril,y lllO}lStll'OO llll)loyc(l:\].
cosine simila.rity between clustor centroids(st~ul(la.rd agglomei:a.tivo clustering)2. a.vera.ge of cosine sitnilariLy 1)etween men>l)ers of the two clusters (a.vera.ge-tink)3. nia.xilnal cosino similarity betweon a.ny pairOf ni('.nll)oi:s of l,\]ie i;wo clusl;(',rs (single-lin\]{)l"oi: (;acli of the va.i:ia.tions a.bovc, the l)r(~(l(;1,er -niincd (;hreshol(I is a. funci;ion of word \['r(xluoncy.Two words wliich each a.l)l)ea.r only onc(Y in theentire tra.ining text a.nd ha.re a. high simila.rib,score a.ro more likely to ha.re a.l)l)ea.red in siniila.rcontexl;s I)y cohicide.nce l:ha.n 1;wo wor(ls whicheach a,1)pea.r ill 1;he traJli i l/g 1;(;xi; lifty tin-its.l,'ro( t UO I / cy5(J78 -10 -\ ]2  - \] 5>16Thresho ld -1 \] .002 0.853 0.804 0.750.700.650.609 0.551 \] 0.500.450.40I,'igure \] : Chtslel ing 'l'hro.shold t unctionI~br exa.ml)le ~ when using threo words on ei-thor side as context, a.nd a. linca.r dcca.y in t;ermweights, two singleton words achievo a. sinlitar-it; 5, scor(', of ().321 (1.000 is the ma.ximum t)os-siblc) if just one o\[" the immodia,tely a(lja,ccntwords is the sa.mc for 1)oth, evon if none of' 1;hoother five context words axe the sa, mc'.
/ks thenumber  o\[' occul 'renc('s increases,  l;ho contr i \ ])u-l,ion t,o the simila.rit,y score o\[' hidividua.l wordsdecreases, ma.king it less likely 1;o encounter ahigh score by chance.
Ilencc, we wish to seta.
si;ricl;er 1;hres\],ol(l \['or clustering low-frequollcywords i;hati higho,'-l'roquelmy words.The thr(~shold Function is exI)ressc(l in 1,(~rmsof tim fr('(lU(mcy o1" occurrence in th(~ 1,ra.il,ing1.exl.s.
I"or si,,gle, ull('lus(;ere(\[ vord pairs, I, hot'requollcy is sinll)ly 1,11o numb(~r ol' 1;hnos I, hewor(I 1)a.ir was (m(:ounl,(u'(,d. When I)e,'\['orn>ing groul)-a.\,erag(; ;lu.qlx;ring, the l'requoncy as-signod l;() a.
('\]/ml;('.r is tim sum o\[' (;h(; frequenciosof a.ll the members; for agglomera.l.ive (:lust('.ri)lg,the \['re(ltten(;y is the sum when using cent;roidsand 1,he lnaximunl fre(lucn('y <tnlong the m(;m-I)oJ'S wllen using l;he average or lmarest-,,(;ighl)or,~imila.rity.
The va.lu(~ of' the (;hr(>shold \['or a. givenpair of ('lusi,('ms is the va.lue of tim thr(~,~holdI'unction a.t the lower word frequency.
\]:igure 1sl,ows l,h(', threshold tunction used in the (,Xl)Cr-iments whose results a, rc rel)ortcd here; cluster-ins is only allowed if the simila, rity measure isa.1)ove the indicated threshold vahm.On its own, clustering is quite suc(:essfill forgeneralizing EBMT ('Xaml)les, I)ut the fully-a.utomated t)roducl;ion of clusters is not com-t)a.tible with adding a, l)roduction-rule gra.mma.ras (lcscril)od in (l~rown, \]999).
Therel'ore, theclustering process may 1)e seeded with a. set ofm an u a.lly-gc'nera.ted clusters.VVhell seed clusters m'e a.va.ilablo., the cluster-ins process is moditied in two ways.
First, l;hegrOUl)-avera.ge a.pl)roa.clms a.dd an initiaJ clusl;erfor o.a,('h soed cluslcr and the a.gglolnera.tive a p-127proaches add an initial cluster for each wordpair; these initial clusters are tagged with thename of the seed cluster.
Second, whenever atagged chister is merged with an untagged oneor another cluster with the same tag, the com-bination inherits the tag; further, merging twoclusters with different ags is disallowed.
As aresult, the initial seed chlsters are expanded byadding additional word pairs while preventingany of the seed clusters from themselves inerg-ing with each other.One special case is handled sepa.rately,namely numeric strings.
If both the source-language and target-l~mguage words of a wordpair are numeric strings, the word pair is treatedas if it had been specified ill the seed class<number>.
Word pairs not containing a digitin either word can optionally be prevented fi'ombeing added to the <number> chlster unlessexplicitly seeded in that cluster.
The formerfeature eusures that nunibers will apl)ear in a.single cluster, rather than in multiple chlsters.The latter avoids the inclusion of the many non-numeric word pairs (primarily adjectives) whichwould otherwise tend to cluster with numbers,because both they and numbers are used asmodifiers.Once clustering is completed, any clusterswhich have inherited the same tag (which ispossible when using agglomerative clustering)are merged.
Those clusters which contain morethan one pseudo-document areoutput,  togetherwith any inherited label, a.nd can be used as aset of equivalence classes for EBMT.Agglomerative chlstering using the maximalcosine sinfila.rity (single-link) produced the sub-jectively best clusters, and was used for the ex-periments described here.4 Exper imentThe Inethod described in the previous twosections was tested on French-English EBMT.The training corpus was a subset of the 1BMIlansard corpns of Canadian parliamentary pro-ceedings (Linguistic Data Consortium, 1997),containing a total of slightly more than onemillion words, approximately half in each lan-guage.
Word-level alignment between Frenchand English was pertbrmed using a dictio-nary containing entries derived statisticallyfrom the full Hansard corpus, auglnented bythe ARTH, French-English did;iona.ry (ARTFLProject, 1998).
This dictionary was used for allEBMT and chlstering runs.The efl'ects of varying the amount of train-ing texts were determined by further sl)littingthe training corl)us into smaller seglnents aMusing differing numbers of segments.
For eachClust I238260348522535137513861528;1563;1.652200821.8224723539M e lnberslJ.IS'l'OIl{E HISTOIWECONOMIE ECONOMYCERTAINI!~MENT CEI{TAI NLYCERTAINEMENT SURELYCERTES SURELYJAMAIS NEVERPAS NOTI~EUT-F, TRE MAYH~OI~ABLEMENT PROBAI~LYQUE ONLYl.
{lfl';N NOTItINGS\[JREMENT CERTAINLYSUREMENT SURELYVRAIMENT REALLYCONSERVATEUR CONSEIWATI \q~JCQNSERVATEUII TORYI)EMOCIi,NI.
'IQUE DEMOCtl, ATICI)I~IVl OCRATIQUIE NDPLIBI~RAL LII3ERA Ll)l A{NII, A%LS LAS \]\])ERNIIjEI{ES PASTI)ERNIIERIDS I{h;CENTPI{OCI\]A INF, S NEXTQ UELQUES FEWQUF, LQUh;S SOMEAVONS HA\q';SOMMES ARI'~p p t r ,  p 1 ,LLC 10RALL CAMPAIGNEM~2CTOIi,A ILF~ EIAECTIONFI~I)I~RAM:,S-I)I {OXq N C IAL1,;SFEI)ERA L-PllOVINCIAI,INDUS'FRIEM3~S INI)US'I'IIIAI,OUVRIERES LA BOURFA(,J()N h;VENTP ?
17' I~VIDLNCL CLEARLYEVIDh;NC\]'; OBVIO USINHOMMF, S POIATICIANSPRISONNIFJ{S PR/SONEI{SRETOUR, BA.CII(,REVENIR BACKCONVENU AGREEDSIGNE SIGNEI)VU SEENAGRJCOLE AGR1C UL'I'UREENT'IER AROUN\])E N T I ER T Ill RO U G I\] O U TOCCIDENTAL WESTERNAVIDUGLI~S BI,INDCIIA.USSURI'2S SI-IOESCONSTRUC;I'EURS BUILDh;RSPENSIONN, F,S PENSIONERSRISTRAITES PENSIONERSVETEMENTS CLOTHINGPOISSON FISI\]PORC IK)RKFigure 2: Sanli)le Chlsters128run using clustering, the first K segments ofthe corl)uS a.re cones.Lena.ted into a. single file,which is used as inl)ut \['or both the clusteringl)t:ogra, m a.nd the EI{M:I.'
system.
The clust;er-ltlg 1)rogranl is rtltt (;o deternfine a. set o1" equiv-alence classes, a.nd these classes a.re then pro-vkled to tile I';I{M:I' systetn a Jest  with the tra.in-ing exa, mples to be indexed, lleld-out lla.nsa.rdtext (a,1)I)roxima.lsely d5,0()O words)is then tra.ns-laLed, +tnd tile l)ercenta.ge of tile words in thetest text for which the I~;I~M~.I ' system couldlind ma,tches a.nd generate a. tl'a.lasla.tion is de-termined.To test the efl'ects of adding seed ('lttsters+a set of' initia.1 clusters was generated withthe \]te.lp of the A I{:I'I"I, dict;iona.ry.
First, the500 most frequ(:nt words in the milliou-word\]\]~msa.rd sul)se.t (excluding pun('.\[;uation) wereextracted.
These terms were then nmtcheda.gMnst the AI~.TFI, dictionary, removing thosewords which had multi-word transla.tions aswell a.s severaJ which listed multil)le parts el"sl)eech For the same tra,nslation (multil>le l>a, rtsof speech can only 1)e used i\[' the corresi>on(I-ing tra.tlsla.tiolls are distinct f'rom each <)ther).The remaining d20 tra.nslal.ion pairs, tagged forl)a.rt o\[' speech, were then convert:e(l inl,o se(~(Iclusters a.nd l)rovided to the clustering t)rogra.nl.To fa.cilita.te xperiments using the t)re-existingl)roduction-rule grammar, tire a.d(litiona,I tra.ns-la?ion I)a,h's from the lna,nually-gelmra, ix~(1 equiv-aJe.n(:e ('la.sses were a.dded t;o l)rovide seeds forfive equiva.\]ence lasses which a.re not, l)resent inthe dictiona.ry.5 Resu l tsThe nlethod (les('ril>ed i,I this l)a, per does (Sttl)jectively) a, very good jol> of clustering likewords toget\]wx, a lid using the clusters to getl-era.lize EI{MT gives a.
(;onsidera.I)le boost, to the.l)etTVol'ltl~-Lt,ce+ of' the  l<\]\]\]\]\/l~\[ ' SySl;(':lll.l"igure 2 shows a, sa.ml)ling of tile sma.llerclusters generated from 1.\] million words o\['Hansard text.
While the nmmbers of a, clus-ter are o f ten semant ica, l ly  l inked (a,s in c luster848, which cotltains types of politica.1 paxl;ies, orcluster stag), they need not be.
Those clusterswhose members a.re not semantically linked gen-eraJly contain words which a.17e all the sa.me l)a.rtof sl)eech , numl)er, a.nd gender (a.s in (:luster2472, which costa.ins exclusively plural nouns)1)ut a.s will be discussed in t;he next section,even those chlsters whose ,neml)ers a.re tota.llyunrela.ted may 1)e useful a.nd correct.. One J'a.h:lycotl l t l \ ]Otl  occur re l tce  a, l l lOl lg the smaller clustersis that various synonymous 1;ra.nslnt;ions o\[ aword (from either source or target language)will chlster together, as in cluster \]652.
Thisis pa.rticula.rly useful when tile ta.rget-languageword is the sa.me, a.s this a.llows va.rious wa.ys ofexpressing t.he same thing to be tra.nsla.ted when~l.lly Of" those  \['OFtlIS ~/l'e present in the tra.ining('orpl.t s.Figure 3 shows how adding a.utoma.tically-generated equiva.lence classes sul)sta.ntially in-c reases  the covers,we of the EI3MT system.
A1-terna.tively, lnuch less text is required to rea.cha.
specific level of coverage.
The lowest curve inthe.
graph is tile percentage of the d5,000-wordtest text for which the EI{M:J' system was ableto genera.te tra.nsla.tions when using strict lexi-c+d matching against the trahling corpus.
Thelop-most curve shows the best performa.nce, pre-viously achieved using 1)oth a, la.rge set of eqttiva-lento classes (in t;he fornt of tagged entries fromthe \]\ItYI'II+'I, dicl;iona.rv) a.nd a. production-rulegra.nlntar (\]{rows, J999).
Of the two centercurves, the lower is the performs.nee when gen-era.lizing the tra.ining corl)us using the equiv-alence classes which were autolna.tica.lly goner-ated from that same text, a.nd tim upper showsthe t)erforma.tlce using ('lustering with the d25seed pairs./ks can b~, seen in Figure 3, 80% cover-age of the test text is achieved with less than300,000 words using nta.ntta.lly-crea.te(l gener-alizat, ion information a.nd with approxima.te-ly 300,000 words wllen using a.utonmtically-creaJ;ed genera.liza.tion i forma.tion, but requires1.2 million words when not using genera.liza.-ties.
90% covers.we is reached with less than500,000 words using lna.nua.lly-ereat.ed informa.lion a.nd should I>e reached with less t.ha.n 1.2tnillion words using a.utonm.tically-crealed gen-era.lization informa.tk)n, versus T million wordswithout genera.liza.tion.
Tiffs reduction I)y a. tim-(or of f  our to live in tile amount of text is accom-1)lishe(I with lit;tie o)' no degradation in the qual-ity of the tra.nsla.tions.
Adding a. small amountof kt,owle(lge in the f'ornt o1" 425 seed pairs re-(lutes the required trahling text; even further;this ca.n la.rgely be attril)uted to the merging ofclusters which would otherwise have rema.ineddistinct, thus increasing the level of generaliza.-ties.Adding the production-rule gratnma.r to theseeded clustering had little effect.
When usirtgmore than 50,000 words of tra.ining text, the in-crease in coverage from adding the gram m a,r wasnegligible, and even with the sma.llest rainingcorl)ora, (,he+ increase wa.s very modest.Using the sa.me thresltolds tha.t were used intile fully-~mtonla.tic case, clustering on 1.\] mil-lion words expands the initial 425 word pairsin 37 clusters to a200 word pairs, a.nd adds a.nadditions.1 555 word pairs in \]d() further non-(;t:ivia,1 clusters.
This (:Oral)ares very fa.vorably129c-O o(DC1)CDOO9080706050403020' si- -x -  .
.
.
.
: .
.
.
.
.
.
x .
.
.
.
.
.
.
.
.
.
.
x .
.
.
.
i .
.
.
.
.
- x  .
.
.
.
.
.
.
.
.
.
* .
.
.
.
.
.
.
.
.~- .~+", zzV3 .
.
.
.
i ~ , (a" /, / ///iI0.2 0.4 0.6Corpus Size (millions of words)lexical matching only -~- -w i th  automatic clustering -4--clustering w/425 seeds -D--,full manual g~neralization --x---0.8l i'igure 3: BI3MT \]~el'formance with and without Generalizationto the 3506 word 1)airs in 221 clusters tbundwithout seeding.
'l'he 1)rogram also runs reasonably quickly.The step of creating context term vectors con-verts approximately 500,000 words of raw textper minute on a 300 MHz processor.
1,'or ag-glomerative clustering, the processing time isroughly quadratic in the number of word \])airs,with a theoretical cubic worst case; the 17,527distinct word pairs found from the million-wordtraining corpus require about 25 minutes tocluster.6 D iscuss ionOne statement made earlier deserves cla.rifica-tion: l;he members of ~ cluster need not be re-lated to each other in any way, either syntacti-cally or semantically, for a cluster to be usefuland correct.
This is because (absent a gram-mar) we do not care about the features of tilewords in the cluster, only wh, cthc~" their tr(msla-lion,s Jbllow the same pattcrT~.An illustration based on actual experienceis useful here.
In early testing of the group-average clustering algorithm with seeding, the<con junct ion> seed class of "and" and "or"was used.
Clustering augmented this seed classwith "," (comma.
), "in", and %y".
One can eas-ily see tha.t the comma is a valid member of theclass, since it takes the place of "and" in listsof items.
13ut wllat about ':in" and "135;", wlfichare prepositions rather than conjunctions2 11'one considers the tra.nsbttion t)attern__  7~ C \[ ?
__  Fr'eNl~ I'>cNP2 --+ EW/A 1 1 F-~:/NI):2it becomes clear that all of the terms in theexpanded class give a correct translation whenplaced in the blank in this pattern, lndeed,one could imagine a production-rule grammargeared toward taking advmltage of such com-mon translation patterns regardless of conven-tional linguistic features.7 Conc lus ion  and  Future  WorkUsing word clustering to automatically gener-alize the example corpus of an I;BM'I?
systemcan provide the majority of tile improvementwhich can be achieved using both ~ manually-generated set of equivalence ('lasses and a pro-duct;ion rule grammar.
The use of a set of smallinitial equivalence classes produces a substan-tial further reduction in training text at a verylow cost (a few hours) in lal)or.130An obvious {'~xtension to using st,.e{\] clusl;orsiS (;(} 1+180 (,110 I'Osllll; ()\[' a, ClU,'ql;{':l'illg 1"I+111 ;IS l;tl{?i\]lil;ia\] seed \['or a second it{;ra,l;io\]t o1' chlsl,er-ing, sin('{', th{; additional g{,neralization of lo-{;a.i COlll;{!xl;s cnabl(;d 1)y the la.rgcr s{,e(1 clusl,(,J'swill l)ormit a.
(l(litional ex\])allSiOll O\['LIlo clusl,(Brs.l:or such itera.tivo {:lustoring, a.II but the lastrou n(1 shouI(1 l)l'(2Slllllal)ly USe sl;ri(;Ler 1,hresh-ol(Is, to avoi(1 adding goo many irr{;l{,A,ant inonl-t)ers Lo tim clusLers.
I ) rd iminary OXl)erinmntshay{ Bbeen inconclusive --although ihc result o\['a second it{wation {'onta.ins more {,{'.rms ill the{;lusl;ers, IBBMT l}erforma.nce {toes not seem tolint)rove.More sophistica.ted {;hlsl;o.l'illg; a.lg(}rithms suchas k-lneans and (l('+terlninLqtic a.nnealing l\]lay'1)rovi(lo \])etter-qua.lity clust{ws for bcl, ter t)ei't"oflllall{;e} :-1+,{; the  (~xi)ens(; of  illCl'Oas(;(\] t)ro{'eHsill~tim{'..This a.i)l}Z:oach to gelWXa.l,ing e(luival('Jw(~cla.sses hould worl( j usl; as well \['or l)h rases as I'orsingle words, simply hy mo(lil~qng {;he conver-Si()ll SLOp 1;O el'oat;(; C(}lltOXt VeCl;ors l"or phrases.This enhancenmnt would elimi,lal;{'~ i;he currentlimitation t, hat trat,slal;ion \]):q,il:S l,O 1)O. clust(;red\]\]lUSt )O single words in 1)oth languages.
\Vot:kor, this n\]o(lifi{;al;ion is {:urP(~ll|;ly ttn(ler way.An inleresting \['ui, ur{~ (;xI)eriment would 1)(~tbr{'going gratnnlar rules based {)n standa.rdgl :a l l l l l l : - / , l ; ica l  \['{'.
:-1+,l;tll'(~s Sl l{:h as \]).~l,rl, o\[' st){"(':{:\]l ,and inst{,ad crea,tinp; a gran~ma, r guid(,{I I} 3 ,{;1~{; ('lusters I'oun(l fully aul,o~tati{'ally (wil, houl,sce{liug) fronl th{~ exa.nll}lc re\l,.
'File r{,{:(;ntwoH{ I)y +\(lcTait and 'l?..iil lo (I 999) {}, OXtl':dcl.,-ing tra1~slal, ion t}al;l;{'+rn,q woul(l a.t)poa.r t,o 1}o. al){;rfe{:l; {;oml)lc'nmnt, as 1;h{'5 are it, e\[t'ect lind-i ,g {:ont;ext strings wit\], (}l}e. slots, while thework descril)ed h('.re lit,(ls {,he fillers I'(}1' tJ~{)s('slots.
(liv{;n the al)ility to learn such +1+.
gra.mmarwithout l\]\]a.nual interv{mtion, it would \])e(:onl{'.I)ossil)l{~ to ere'at{; an I!
'I:~MT 8yst{m\] usillg g{:ql-era, liz{,(l e,:aml)les from nol, hi\]~g ~n{)r{; than l)ar-allel l;ext~ which for n~any hulguag(, pairs couldalso 1)c acquired a hnost fully a, utom~tical ly 1)ycrawling the World Wide VVel) (Resnil{, :1.998).ReferencesA\]~.TFL Project.
:1998.
I"re'nch-?'nglish.
Dic-lionarg.
I}roject for American and Frenchl{esearch {}i\] the Treasury of {,he l:renchl,anguage, University o\[" Chicago.
h t tp  : / / -human?t i es .
uch?cago, edu/kgTFL, html.Sergio Barrachina a.nd Juan Miguel \:ilar.
1999.Bilingual Clustering Using Monolingual Algo-rithms.
Ill 15vcecdings of lhe l@hlh \]'n.lcrna-~ional (7onJ?rence on Theoretical and Method-ological Issues in Machinc 7)'anslatio'n (~1341-99), pages 77 87, Chester, England, August.I)ctor l~,roxvH, .l.
(;ocke, S. l)ella \])iotra, V. I)ollal}h:l, ra, I:.
dolinel<, 3. l,afl'erl.y, R. Mercer, andI }.
I~.oossh,.
1990.
A Statistical Approachto Machine Translation.
COmlmtaHonal Lin-.qui.slic% 16:79 85.l{alF \]).
\]h:own.
1997.
Autonlated \]) ictionaryl';xLracLion lot "l(nowlcdge-l,'ree:' Example-1},ascd Translation.
In l)~vccedin:l s of ihc,5'cvcnlh h,l(rnaiional (.
'o,@Tvncc on "IJm-orclical and A4clhodolo:lical l~ue.s in Ma-chi'~c 7}'an.~lalion (TM1-97), F, ages \ ]  11 1 \]8,Sant~t \]:c, New Mexico, July.
ht;1;p://-www.
cs.
cmu.
edu/Oral: f /papers,  html.Rail" I).
lh'own.
1999.
Adding l,ingttistic l (nowl-edge Co a l,exica\] \]';xamp\]e-\]~ased Tra, nsla-tion System.
In I)rocccding.s of thc 12iEM, hhzlcrnational (,'onj)rencc on 7'hcorel, ical mzdMclhodoh::lical fss.uc.~ in Machine 7)'anslation.
(7341-99), pages 22-32, Chesl:er, I~;ngland,August.
http ://www.
cs.
cmu.
edu/~rat f / -papers .html.I,illguistic I)al, a. Con.~orLium.
1997. lla~>'ard(,'ou~u.~ of I)aralld ?
'Rqlish (rod t'Yc,~ch.IAnguistic l )ata  Consorl;ium, \])ecember.http  ://wwu.
ldc .
upenn, edu/.l(evin McTl'a.it and Arturo Trujil\]o.
1999.
Al,atlguag{>Neutral Sl)arse-\])ata Algorithnl fbrI:x tracti n g 'I'ranslation I}atterns.
\] II \])ro(:ccd-in:l s of lhc l'/i:lhlh hztcrlzational Co~@rcnccon 77~colv:ical and Mcthodoh:gical Is.sues inMachMc 7~rmtslatio~t (7'A41-99), l)ages 981()8, Ch{~ster, l'3ngla,(1, August.I}hilil } l{(,snik.
\]998.
I)ara.llel Strait(Is: A 1)re -l iminary lnvesCiga.tion into Mining the \V(;br 1 { for l~\]lingua.1 e\  .
In I)a.vid l'a.rweI1, l,a.urle(:left)or, and Edua.rd llovy, editors, Mac.hine7}'a~.~laZion a d the hdbrm.alion Soup: 7'hird(;'on, fcrcncc of Ihc A.~..s'ociation for A'lachi~c'l}.anslation in Ihe Americas (A A4'F4-9S), vol-un\]e 1529 o\[' Lccl'mv No*ca in ArZi./icial lnlcl-ligc:~zcc, i)ages 72 82, l,anghorne, I}ennsylva -ida, ()ct{}l}er.
Springer.Tony Vcalc and An(ly Wa.y.
1997.
Gaijin: A'lhml)la.te-I)riven Bootstrapl)ing AI)l)roacllto Exa, ml)Ie-Ba.sed Ma,chin(; Tra.nsla.tion.
:It\] 1}roecedin:p of the NeMNLl~'97.
NewMel.hods in Natural Langauge 1)rocessessin9,Sofia., lhdgaria., Sel)teml)er.
h t tp : / / -wwu.
compapp, dcu.
ie / " tonyv /papers / -ga i j  in .
html.Ellen M. \:oorhees.
1986. lmplel\]mnting Ag-glomera.tive ll ierarchical Clustering Algo-r ithms for Use in \ ] )ocument l/etrieval.I'nJbrmation l~rocessi'ng and Mana.qeme'nt,22(6):d65 ,176.131
