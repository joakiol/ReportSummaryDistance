Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 68?74,Sydney, July 2006. c?2006 Association for Computational LinguisticsEvaluation of the Bible as a Resourcefor Cross-Language Information RetrievalPeter A. Chew Steve J. Verzi Travis L. Bauer Jonathan T. McClainSandia National LaboratoriesP.
O.
Box 5800Albuquerque, NM 87185, USA{pchew,sjverzi,tlbauer,jtmccl}@sandia.govAbstractAn area of recent interest in cross-language information retrieval (CLIR)is the question of which parallel corporamight be best suited to tasks in CLIR, oreven to what extent parallel corpora canbe obtained or are necessary.
One pro-posal, which in our opinion has beensomewhat overlooked, is that the Bibleholds a unique value as a multilingualcorpus, being (among other things)widely available in a broad range oflanguages and having a high coverageof modern-day vocabulary.
In this pa-per, we test empirically whether thisclaim is justified through a series ofvalidation tests on various informationretrieval tasks.
Our results appear to in-dicate that our methodology may sig-nificantly outperform others recentlyproposed.1 IntroductionThis paper describes an empirical evaluation ofthe Bible as a resource for cross-language infor-mation retrieval (CLIR).
The paper is organizedas follows: section 2 describes the background tothis project and explains our need for CLIR.
Sec-tion 3 sets out the various alternatives available(as far as multilingual corpora are concerned) forthe type of textual CLIR which we want to per-form, and details in qualitative terms why theBible would appear to be a good candidate.
Insection 4, we outline the mechanics behind the'Rosetta-Stone' type method we use for cross-language comparison.
The manner in which boththis method, and the reliability of using the Bibleas the basis for cross-language comparison, arevalidated is outlined in section 5, together withthe results of our tests.
Finally, we conclude onand discuss these results in section 6.2 BackgroundThis paper describes a project which is part of alarger, ongoing, undertaking, the goal of which isto harvest a representative sample of materialfrom the internet and determine, on a very broadscale, the answers to such questions as:?
what ideas in the global public discourseenjoy most currency;?
how the popularity of ideas changes overtime.Ideas are, of course, expressed in words; or, toput it another way, a document's vocabulary islikely to reveal something about the author's ide-ology (Lakoff, 2002).
In view of this, and sinceultimately we are interested in clustering thedocuments harvested from the internet by theirideology (and we understand 'ideology' in thebroadest possible sense), we approach the prob-lem as a textual information retrieval (IR) task.There is another level of complexity to theproblem, however.
The language of the internetis not, of course, confined to English; on the con-trary, the representation of other languages isprobably increasing (Hill and Hughes, 1998;Nunberg, 2000).
Thus, for our results to be repre-sentative, we require a way to compare docu-ments in one language to those in potentially anyother language.
Essentially, we would like toanswer the question of how ideologically alignedtwo documents are, regardless of their respectivelanguages.
In cross-language IR, this must beapproached by the use of a parallel multilingualcorpus, or at least some kind of appropriate train-ing material available in multiple languages.3 Parallel multilingual corpora: avail-able alternativesOne collection of multilingual corpora gatheredwith a specific view towards CLIR has been de-68veloped by the Cross-Language Evaluation Fo-rum (CLEF); see, for example, Gonzalo (2001).This collection, and its most recent revision (atthe CLEF website, www.clef-campaign.org), arebased on news documents or governmentalcommunications.
Use of such corpora is wide-spread in much recent CLIR work; one such ex-ample is Nie, Simard, Isabelle and Durand(1999), which uses the Hansard corpus, parallelFrench-English texts of eight years of the Cana-dian parliamentary proceedings, to train a CLIRmodel.It should be noted that the stated objective ofCLEF is to 'develop and maintain an infrastruc-ture for the testing and evaluation of informationretrieval systems operating on European lan-guages' (Peters 2001:1).
Indeed, there is goodreason for this: CLEF is an activity under theauspices of the European Commission.
Likewise,the Canadian Hansard corpus covers only Eng-lish and French, the most widespread languagesof Canada.
It is to be expected that governmentalinstitutions would have most interest in promot-ing resources and research in the languages fal-ling most within their respective domains.But in many ways, not least for the computa-tional linguistics community, nor for anyone in-terested in understanding trends in globalopinion, this represents an inherent limitation.Since many of the languages of interest for ourproject are not European ?
Arabic is a good ex-ample ?
resources such as the CLEF collectionwill be insufficient by themselves.
The output ofglobal news organizations is a more promisingavenue, because many such organizations makean effort to provide translations in a wide varietyof languages.
For example, the BBC news web-site (http://news.bbc.co.uk/) provides translationsin 34 languages, as follows:Albanian, Arabic, Azeri, Bengali, Bur-mese, Chinese, Czech, English, French,Hausa, Hindi, Indonesian, Kinyarwanda,Kirundi, Kyrgyz, Macedonian, Nepali,Pashto, Persian, Portuguese, Romanian,Russian, Serbian, Sinhala, Slovene, So-mali, Spanish, Swahili, Tamil, Turkish,Ukrainian, Urdu, Uzbek, VietnameseHowever, there is usually no assurance that anews article in one language will be translatedinto any, let alne all, of the other languages.In view of this, even more promising still as aparallel corpus for our purposes is the Bible.
Res-nik, Olsen and Diab (1999) elaborate on some ofthe reasons for this: it is the world's most trans-lated book, with translations in over 2,100 lan-guages (often, multiple translations per language)and easy availability, often in electronic formand in the public domain; it covers a variety ofliterary styles including narrative, poetry, andcorrespondence; great care is taken over thetranslations; it has a standard structure whichallows parallel alignment on a verse-by-versebasis; and, perhaps surprisingly, its vocabularyappears to have a high rate of coverage (as muchas 85%) of modern-day language.
Resnik, Olsenand Diab note that the Bible is small compared tomany corpora currently used in computationallinguistics research, but still falls within therange of acceptability based on the fact that othercorpora of similar size are used; and as previ-ously noted, the breadth of languages covered issimply not available elsewhere.
This in itselfmakes the Bible attractive to us as a resource forour CLIR task.
It is an open question whether,because of the Bible's content, relatively smallsize, or some other attribute, it can successfullybe used for the type of CLIR we envisage.
Therest of this paper describes our attempt to estab-lish a definitive answer to this question.4 Methods for Cross-Language Com-parisonAll of the work described in this section was im-plemented using the Sandia Text Analysis Exten-sible Library (STANLEY).
STANLEY allowsfor information retrieval based on a standard vec-tor model (Baeza-Yates and Ribeiro-Neto, 1999:27-30) with term weighting based on log en-tropy.
Previous work (Bauer et al2005) hasshown that the precision-recall curve forSTANLEY is better than many other publishedalgorithms; Dumais (1991) finds specifically thatthe precision-recall curve for information re-trieval based on log-entropy weighting comparesfavorably to that for other weighting schemes.Two distinct methods for cross-language com-parison are described in this section, and theseare as follows.The first method (Method 1) involves creatinga separate textual model for each 'minimal unit'of each translation of the Bible.
A 'minimal unit'could be as small as a verse (e.g.
Genesis 1:1),but it could be a group of verses (e.g.
Genesis1:1-10); the key is that alignment is possible be-cause of the chapter-and-verse structure of theBible, and that whatever grouping is used shouldbe the same in each translation.
Thus, for each69language ?we end up with a set of models (m1,?,m2,?, ?
mn,?).
If the Bible is used as the parallelcorpus and the 'minimal unit' is the verse, then n= 31,102 (the number of verses in the Bible).Let us suppose now that we wish to comparedocument di with document dj, and that we hap-pen to know that di is in English and dj is in Rus-sian.
In order to assess to what extent di and djare 'about' the same thing, we treat the text ofeach document as a query against all of the mod-els in its respective language.
So, di is evaluatedagainst m1,English, m2,English, ?, mn,English to givesimi,1, simi,2, ?, simi,n, where simx,y (a value be-tween 0 and 1) represents the similarity of docu-ment dx in language?to model mn in language?,based on the cosine of the angle between the vec-tor for dx and the vector for mn.
Similar evalua-tions are performed for dj against the set ofmodels in Russian.
Now, each set of n results fora particular document can itself be thought of ann-dimensional vector.
Thus, di is associated with(simi,1, simi,2, ?, simi,n) and dj with (simj,1, simj,2,?, sim j,n).
To quantify the similarity between diand dj, we now compute the cosine betweenthese two vectors to yield a single measure, alsoa value between 0 and 1.
In effect, we have usedthe multilingual corpus ?
the Bible, in this case ?in 'Rosetta-Stone' fashion to bridge the languagegap between di and dj.
Method 1 is summarizedgraphically in Figure 1, for two hypotheticaldocuments.The second method of comparison (Method 2)is quite similar.
This time, however, instead ofbuilding one set of textual models for each trans-lation in language?
(m1,?, m2,?, ?
mn,?
), we builda single set of textual models for all translations,with each language represented at least once (m1,m2, ?
mn).
Thus, m1 might represent a modelbased on the concatenation of Genesis 1:1 inEnglish, Russian, Arabic, and so on.
In a fashionsimilar to that of Method 1, each incomingdocument di is evaluated as a query against m1,m2, ?, mn, to give an n-dimensional vectorwhere each cell is a value between 0 and 1.Method 2 is summarized graphically in Figure 2,for just English and Russian.There are at least two features of Method 2which make it attractive, from a linguist's pointof view, for CLIR.
The first is that it allows forthe possibility that a single input document maybe multilingual.
In Figure 2, document dj is rep-resented by an symbol with a mainly light-colored background, but with a small dark-colored section.
This is intended to represent adocument with mainly English content, but somesmall subsection in Russian.
Under Method 1, inwhich dj is compared to an English-languagemodel, the Russian content would have been ef-fectively ignored, but under Method 2 this is nolonger the case.
Accordingly, the hypotheticalsimilarity measure for the first 'minimal unit' haschanged very slightly, as has the overall measureof similarity between document di and dj.The second linguistic attraction of Method 2 isthat it is not necessary to know a priori the lan-guage of di or dj, providing that the language isone of those for which we have textual data inthe model set.
Since, as already stated, the Biblecovers over 2,100 languages, this should not be asignificant theoretical impediment.The theoretical advantages of Method 1 haveprincipally to do with the ease of technical im-plementation.
New model sets for additional lan-guages can be easily added as they becomeavailable, whereas under Method 2 the entiremodel set must be rebuilt (statistics recomputed,etc.)
each time a new language is added.5 Validation of the Bible as a resourcefor CLIRIn previous sections, we have rehearsed some ofthe qualitative arguments for our choice of the70Bible as the basis for CLIR.
In this section, weconsider how this choice may be validated em-pirically.
We would like to know how reliablethe cross-language comparison methods outlinedin the previous section are at identifying docu-ments in different languages but which happen tobe similar in content.
This reliability will be inpart a function of the particular text analysismodel we employ, but it will also be a functionof our choice of parallel text used to train themodel.
The Bible has some undeniable qualita-tive advantages for our purposes, but are theCLIR results based on it satisfactory in practice?Three tests are described in this section; the aimof these is to provide an answer to this question.5.1 Preliminary analysisIn order to obtain a preliminary idea of whetherthis method was likely to work, we populated theentire matrix of similarity measures, verse byverse, for each language pair.
There are 31,102verses in the Bible (allowing for some variationin versification between different translations,which we carefully controlled for by adopting acommon versification schema).
Thus, this stepinvolved building a 31,102 by 31,102 matrix foreach language pair, in which the cell in row mand column n contains a number between 0 and 1representing the similarity of verse m in one lan-guage to verse n in the other language.
If use ofthe Bible for CLIR is a sound approach, wewould expect to see the highest similarity meas-ures in what we will call the matrix's diagonalvalues ?
the values occurring down the diagonalof the matrix from top-left to bottom-right ?meaning that verse n in one language is mostsimilar to verse n in the other, for all n.Here, we would simply like to note an inciden-tal finding.
We found that for certain languagepairs, the diagonal values were significantlyhigher than for other language pairs, as shown inTable 1.Language pair Mean similarity,verse by verseEnglish-Russian 0.3728English-Spanish 0.5421English-French 0.5508Spanish-French 0.5691Table 1.
Mean similarities by language pairOne hypothesis we have is that the lower overallsimilarity for English-Russian is at least partlydue to the fact that Russian is a much morehighly inflected language then any of English,French, or Spanish.
That many verses containingnon-dictionary forms are the ones that score thehighest for similarity, and many of those that donot score lowest, appears to confirm this.
How-ever, there appear to be other factors at play aswell, since many of the highest-scoring versescontain proper names or other infrequently oc-curring lexical items (examples are Esther 9:9:'and Parmashta, and Arisai, and Aridai, and Vai-zatha', and Exodus 37:19: 'three cups made likealmond-blossoms in one branch, a bud and aflower, and three cups made like almond-blossoms in the other branch, a bud and a flower:so for the six branches going out of the lamp-stand').
A third possibility, consistent with thefirst, is that Table 1 actually reflects more gen-eral measures of similarity between languages,the Western European languages (for example)all being more closely related to Latin than theirSlavic counterparts.
At any rate, if our hypothesisabout inflection being an important factor is cor-rect, then this would seem to underline the im-portance of stemming for highly-inflectedlanguages.5.2 Simple validationIn this test, the CLIR algorithm is trained on theentire Bible, and validation is performed againstavailable extra-Biblical multilingual corporasuch as the FQS (2006) and RALI (2006) cor-pora.
This test, together with the tests alreadydescribed, should provide a reliable measure ofhow well our CLIR model will work when ap-plied to our target domain (documents collectedfrom the internet).For this test, five abstracts in the FQS (2006)were selected.
These abstracts are in both Span-ish and English, and the five are listed in Table 2below.Eng.
1 PerspectivesEng.
2 Public and Private NarrativesEng.
3 Qualitative ResearchEng.
4 How Much Culture is PsychologyAble to Deal WithEng.
5 Conference ReportSp.
1 PerspectivasSp.
2 Narrativas p?blicas y privadasSp.
3 Cu?nta cultura es capaz de abordar laPsicolog?aSp.
4 Investigaci?n cualitativaSp.
5 Nota sobre la conferenciaTable 2.
Documents selected for analysis71The results based on these five abstracts, wherecomparison was performed between Spanish andEnglish and vice-versa, are as shown in Table 3.The results shown in Table 3 are the actual (raw)similarity values provided by our CLIR frame-work using the FQS corpus.Eng.
1 Eng.
2 Eng.
3 Eng.
4 Eng.
5Sp.
1 0.6067 0.0430 0.0447 0.0821 0.1661Sp.
2 0.0487 0.3969 0.0377 0.0346 0.0223Sp.
3 0.1018 0.0956 0.0796 0.1887 0.1053Sp.
4 0.0303 0.0502 0.0450 0.1013 0.0493Sp.
5 0.0354 0.1314 0.0387 0.0425 0.1682Table 3.
Raw similarity values of Spanish andEnglish documents from FQS corpusIn this table, 'Eng.
1', 'Sp.
1', etc., refer to thedocuments as listed in Table 2.In four out of five cases, the CLIR engine cor-rectly predicted which English document wasrelated to which Spanish document, and in fourout of five cases it also correctly predicted whichSpanish document was related to which Englishdocument.
We can relate these results to tradi-tional IR measures such as precision-recall andmean average precision by using a query thatreturns the top-most similar document.
Thus, our?right?
answer set as well as our CLIR answerswill consist of a single document.
For the FQScorpus, this represents a mean average precision(MAP) of 0.8 at a recall point of 1 (the firstdocument recalled).
The incorrect cases wereEng.
4, where Sp.
3 was predicted, and Sp.
3,where Eng.
4 was predicted.
(By way of possibleexplanation, both these two documents includedthe keywords 'qualitative research' with the ab-stract.)
Furthermore, in most of the cases wherethe prediction was correct, there is a clear marginbetween the score for the correct choice and thescores for the incorrect choices.
This leads us tobelieve that our general approach to CLIR is atvery least promising.5.3 Validation on a larger test setTo address the question of whether the CLIRapproach performs as well on larger test sets,where the possibility of an incorrect prediction isgreater simply because there are more documentsto select from, we trained the CLIR engine on theBible and validated it against the 114 suras of theQuran, performing a four-by-four-way test usingthe original Arabic (AR) text plus English (EN),Russian (RU) and Spanish (ES) translations.
TheMAP at a recall point of 1 is shown for each lan-guage pair in Table 4.Language of predicted documentAR EN RU ESAR 1.0000 0.2193 0.2281 0.2105EN 0.2632 1.0000 0.3333 0.5263RU 0.2719 0.3860 1.0000 0.4386Languageof inputES 0.2105 0.4912 0.4035 1.0000Table 4.
Results based on Quran testThis table shows, for example, that for 52.63%(or 60) of the 114 English documents used asinput, the correct Spanish document was re-trieved first.
As with the results in the previoussection, we can relate these results to MAP at arecall of 1.
If we were to consider more than justthe top-most similar document in our CLIR out-put, we would expect the chance of seeing thecorrect document to increase.
However, since inthis experiment the number of relevant docu-ments can never exceed 1, the precision will bediluted as more documents are retrieved (exceptat the point when the one correct document isretrieved).
The values shown in the table are, ofcourse, greater by a couple of orders of magni-tude than that expected of random retrieval, of0.0088 (1/114).
Our methodology appears sig-nificantly to outperform that proposed byMcNamee and Mayfield (2004), who report anMAP of 0.3539, and a precision of 0.4520 at arecall level of 10, for English-to-Spanish CLIRbased on 5-gram tokenization.
(We have not yetbeen able to compare our results to McNameeand Mayfield's using the same corpora that theyuse, but we intend to do this later.
We do not ex-pect our results to differ significantly from thosewe report above.)
Perhaps not surprisingly, ourresults appear to be better for more closely-related languages, with pairs including Arabicbeing consistently those with the lowest averagepredictive precision across all suras.6 DiscussionIn this paper, we have presented a non-language-specific framework for cross-language informa-tion retrieval which appears promising at leastfor our purposes, and potentially for many others.It has the advantages of being easily extensible,and, with the results we have presented, it is em-pirically benchmarked.
It is extensible in twodimensions; first, by language (substantially any72human language which might be represented onthe internet can be covered, and the cost of add-ing resources for each additional language isrelatively small), secondly, by extending thetraining set with additional corpora, for availablelanguage pairs.
Doubtless, also, the methodologycould be further tuned for better performance.It is perhaps surprising that the Bible has notbeen more widely used as a multilingual corpusby the computational linguistics and informationretrieval community.
In fact, it usually appears tobe assumed by researchers that parallel texts,particularly those which have been as carefullytranslated as the Bible and are easy to align, arescarce and hard to come by (for two examples,see McNamee and Mayfield 2004 and Munteanuand Marcu 2006).
The reason for the Bible beingignored may be the often unspoken assumptionthat the domain of the Bible is too limited (beinga religious document) or that its content is tooarchaic.
Yet, the truth is that much of the Bible'scontent has to do with enduring human concerns(life, death, war, love, etc.
), and if the language isarchaic, that may have more a matter of transla-tion style than of content.There are a number of future research direc-tions in computational linguistics we would liketo pursue, besides those which may be of interestin other disciplines.
The first is to use thisframework to evaluate the relative faithfulness ofdifferent translations.
For example, we wouldexpect to see similar statistical relationshipswithin the model for a translation of the Bible asare seen in its original languages (Hebrew andGreek).
Statistical comparisons could thus beused as the basis for evaluating a translation'sfaithfulness to the original.
Such an analysiscould be of theological, as well as linguistic, in-terest.Secondly, we would like to examine whetherthe model's performance can be improved byintroducing more sophisticated morphologicalanalysis, so that the units of analysis are mor-phemes instead of words, or possibly morphemesas well as words.Third, we intend to investigate further whichof the two methods outlined in section 4 per-forms better in cross-language comparison, par-ticularly when the language of the sourcedocument is unknown.
In particular, we are in-terested in the extent to which homographic cog-nates across languages (e.g.
French coin 'corner'versus English coin), may affect the performanceof the CLIR engine.AcknowledgementSandia is a multiprogram laboratory operatedby Sandia Corporation, a Lockheed Martin Com-pany, for the United States Department of En-ergy?s National Nuclear Security Administrationunder contract DE-AC04-94AL85000.ReferencesLars Asker.
2004.
Building Resources: Experiencesfrom Amharic Cross Language Information Re-trieval.
Paper presented at Cross-Language Infor-mation Retrieval and Evaluation: Workshop of theCross-Language Evaluation Forum, CLEF 2004.Ricardo Baeza-Yates and Berthier Ribeiro-Neto.1999.
Modern Information Retrieval.
New York:ACM Press.Travis Bauer, Steve Verzi, and Justin Basilico.
2005.Automated Context Modeling through Text Analy-sis.
Paper presented at Cognitive Systems: HumanCognitive Models in System Design.Susan Dumais.
1991.
Improving the Retrieval of In-formation from External Sources.
Behavior Re-search Methods, Instruments, and Computers23(2):229-236.Forum: Qualitative Social research (FQS).
2006.
Pub-lished Conference Reports.
(Conference reportsavailable on-line in multiple languages.)
Accessedat http://www.qualitative-research.net/fqs/conferences/conferences-pub-e.htm on February 22, 2006.Julio Gonzalo.
2001.
Language Resources in Cross-Language Text Retrieval: a CLEF Perspective.
InCarol Peters (ed.).
Cross-Language InformationRetrieval and Evaluation: Workshop of the Cross-Language Evaluation Forum, CLEF 2000: 36-47.Berlin: Springer-Verlag.George Lakoff.
2002.
Moral politics : how liberalsand conservatives think.
Chicago : University ofChicago Press.Paul McNamee and James Mayfield.
2004.
CharacterN-Gram Tokenization for European Language TextRetrieval.
Information Retrieval 7: 73-97.Dragos Munteanu and Daniel Marcu.
2006.
Improv-ing Machine Translation Performance by Exploit-ing Non-Parallel Corpora.
ComputationalLinguistics 31(4):477-504.Geoffrey Nunberg.
2000.
Will the Internet AlwaysSpeak English?
The American Prospect 11(10).Carol Peters (ed.).
2001.
Cross-Language InformationRetrieval and Evaluation: Workshop of the Cross-Language Evaluation Forum, CLEF 2000.
Berlin:Springer-Verlag.73Recherche appliqu?e en linguistique informatique(RALI).
2006.
Corpus align?
bilingue anglais-fran?ais.
Accessed at http://rali.iro.umontreal.ca/on February 22, 2006.Philip Resnik, Mari Broman Olsen, and Mona Diab.1999.
The Bible as a Parallel Corpus: Annotatingthe "Book of 2000 Tongues".
Computers and theHumanities, 33: 129-153.74
