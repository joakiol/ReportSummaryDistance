Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 945?955,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsTwo Is Bigger (and Better) Than One: the Wikipedia Bitaxonomy ProjectTiziano Flati, Daniele Vannella, Tommaso Pasini and Roberto NavigliDipartimento di InformaticaSapienza Universit`a di Roma{flati,vannella,navigli}@di.uniroma1.itp.tommaso@gmail.comAbstractWe present WiBi, an approach to theautomatic creation of a bitaxonomy forWikipedia, that is, an integrated taxon-omy of Wikipage pages and categories.We leverage the information available ineither one of the taxonomies to reinforcethe creation of the other taxonomy.
Ourexperiments show higher quality and cov-erage than state-of-the-art resources likeDBpedia, YAGO, MENTA, WikiNet andWikiTaxonomy.
WiBi is available athttp://wibitaxonomy.org.1 IntroductionKnowledge has unquestionably become a keycomponent of current intelligent systems in manyfields of Artificial Intelligence.
The creation anduse of machine-readable knowledge has not onlyentailed researchers (Mitchell, 2005; Mirkin et al,2009; Poon et al, 2010) developing huge, broad-coverage knowledge bases (Hovy et al, 2013;Suchanek and Weikum, 2013), but it has alsohit big industry players such as Google (Singhal,2012) and IBM (Ferrucci, 2012), which are mov-ing fast towards large-scale knowledge-orientedsystems.The creation of very large knowledge baseshas been made possible by the availability ofcollaboratively-curated online resources such asWikipedia and Wiktionary.
These resources areincreasingly becoming enriched with new con-tent in many languages and, although they areonly partially structured, they provide a great dealof valuable knowledge which can be harvestedand transformed into structured form (Medelyanet al, 2009; Hovy et al, 2013).
Prominentexamples include DBpedia (Bizer et al, 2009),BabelNet (Navigli and Ponzetto, 2012), YAGO(Hoffart et al, 2013) and WikiNet (Nastase andStrube, 2013).
The types of semantic relationin these resources range from domain-specific, asin Freebase (Bollacker et al, 2008), to unspec-ified relations, as in BabelNet.
However, un-like the case with smaller manually-curated re-sources such as WordNet (Fellbaum, 1998), inmany large automatically-created resources thetaxonomical information is either missing, mixedacross resources, e.g., linking Wikipedia cate-gories to WordNet synsets as in YAGO, or coarse-grained, as in DBpedia whose hypernyms link to asmall upper taxonomy.Current approaches in the literature have mostlyfocused on the extraction of taxonomies from thenetwork of Wikipedia categories.
WikiTaxonomy(Ponzetto and Strube, 2007), the first approachof this kind, is based on the use of heuristicsto determine whether is-a relations hold betweena category and its subcategories.
Subsequent ap-proaches have also exploited heuristics, but haveextended them to any kind of semantic relationexpressed in the category names (Nastase andStrube, 2013).
But while the aforementioned at-tempts provide structure for categories that sup-ply meta-information for Wikipedia pages, sur-prisingly little attention has been paid to the ac-quisition of a full-fledged taxonomy for Wikipediapages themselves.
For instance, Ruiz-Casado etal.
(2005) provide a general vector-based methodwhich, however, is incapable of linking pageswhich do not have a WordNet counterpart.
Highercoverage is provided by de Melo and Weikum(2010) thanks to the use of a set of effective heuris-tics, however, the approach also draws on Word-Net and sense frequency information.In this paper we address the task of taxono-mizing Wikipedia in a way that is fully indepen-dent of other existing resources such as WordNet.We present WiBi, a novel approach to the cre-ation of a Wikipedia bitaxonomy, that is, a tax-onomy of Wikipedia pages aligned to a taxonomyof categories.
At the core of our approach lies theidea that the information at the page and category945level are mutually beneficial for inducing a wide-coverage and fine-grained integrated taxonomy.2 WiBi: A Wikipedia BitaxonomyWe induce a Wikipedia bitaxonomy, i.e., a taxon-omy of pages and categories, in 3 phases:1.
Creation of the initial page taxonomy: wefirst create a taxonomy for the Wikipediapages by parsing textual definitions, ex-tracting the hypernym(s) and disambiguatingthem according to the page inventory.2.
Creation of the bitaxonomy: we leveragethe hypernyms in the page taxonomy, to-gether with their links to the correspondingcategories, so as to induce a taxonomy overWikipedia categories in an iterative way.
Ateach iteration, the links in the page taxonomyare used to identify category hypernyms and,conversely, the new category hypernyms areused to identify more page hypernyms.3.
Refinement of the category taxonomy: fi-nally we employ structural heuristics to over-come inherent problems affecting categories.The output of our three-phase approach is a bitax-onomy of millions of pages and hundreds of thou-sands of categories for the English Wikipedia.3 Phase 1: Inducing the Page TaxonomyThe goal of the first phase is to induce a taxonomyof Wikipedia pages.
Let P be the set of all thepages and let TP= (P,E) be the page taxonomydirected graph whose nodes are pages and whoseedge set E is initially empty (E := ?).
For eachp ?
P our aim is to identify the most suitable gen-eralization ph?
P so that we can create the edge(p, ph) and add it to E. For instance, given thepage APPLE, which represents the fruit meaningof apple, we want to determine that its hypernymis FRUIT and add the hypernym edge connectingthe two pages (i.e., E := E?
{(APPLE, FRUIT)}).To do this, we perform a syntactic step, in whichthe hypernyms are extracted from the page?s tex-tual definition, and a semantic step, in which theextracted hypernyms are disambiguated accordingto the Wikipedia inventory.3.1 Syntactic step: hypernym extractionIn the syntactic step, for each page p ?
P , weextract zero, one or more hypernym lemmas, thatis, we output potentially ambiguous hypernymsfor the page.
The first assumption, which followsJulia Fiona Roberts isanAmerican actressNNP NNP NNP VBZ DT JJNNnnnnnsubjcopdetamodFigure 1: A dependency tree example with copula.the Wikipedia guidelines and is validated in theliterature (Navigli and Velardi, 2010; Navigli andPonzetto, 2012), is that the first sentence of eachWikipedia page p provides a textual definition forthe concept represented by p. The second assump-tion we build upon is the idea that a lexical tax-onomy can be obtained by extracting hypernymsfrom textual definitions.
This idea dates back tothe early 1970s (Calzolari et al, 1973), with laterdevelopments in the 1980s (Amsler, 1981; Calzo-lari, 1982) and the 1990s (Ide and V?eronis, 1993).To extract hypernym lemmas, we draw on thenotion of copula, that is, the relation between thecomplement of a copular verb and the copular verbitself.
Therefore, we apply the Stanford parser(Klein and Manning, 2003) to the definition of apage in order to extract all the dependency rela-tions of the sentence.
For example, given the def-inition of the page JULIA ROBERTS, i.e., ?JuliaFiona Roberts is an American actress.
?, the Stan-ford parser outputs the set of dependencies shownin Figure 1.
The noun involved in the copula re-lation is actress and thus it is taken as the page?shypernym lemma.
However, the extracted hyper-nym is sometimes overgeneral (one, kind, type,etc.).
For instance, given the definition of thepage APOLLO, ?Apollo is one of the most impor-tant and complex of the Olympian deities in an-cient Greek and Roman religion [...].
?, the onlycopula relation extracted is between is and one.To cope with this problem we use a list of stop-words.1When such a term is extracted as hyper-nym, we replace it with the rightmost noun of thefirst following noun sequence (e.g., deity in theabove example).
If the resulting lemma is again astopword we repeat the procedure, until a valid hy-pernym or no appropriate hypernym can be found.Finally, to capture multiple hypernyms, we iter-atively follow the conj and and conj or relationsstarting from the initially extracted hypernym.
Forexample, consider the definition of ARISTOTLE:?Aristotle was a Greek philosopher and polymath,a student of Plato and teacher of Alexander theGreat.?
Initially, the philosopher hypernym isselected thanks to the copula relation, then, fol-1E.g., species, genus, one, etc.
Full list available online.946lowing the conjunction relations, also polymath,student and teacher are extracted as hypernyms.While more sophisticated approaches like Word-Class Lattices could be applied (Navigli and Ve-lardi, 2010), we found that, in practice, our hy-pernym extraction approach provides higher cov-erage, which is critical in our case.3.2 Semantic step: hypernym disambiguationSince our aim is to connect pairs of pages viahypernym relations, our second step consists ofdisambiguating the obtained hypernym lemmas ofpage p by associating the most suitable page witheach hypernym.
Following previous work (Ruiz-Casado et al, 2005; Navigli and Ponzetto, 2012),as the inventory for a given lemma we consider theset of pages whose main title is the lemma itself,except for the sense specification in parenthesis.For instance, given fruit as the hypernym for AP-PLE we would like to link APPLE to FRUIT as op-posed to, e.g., FRUIT (BAND) or FRUIT (ALBUM).3.2.1 Hypernym linkersTo disambiguate hypernym lemmas, we exploitthe structural features of Wikipedia through apipeline of hypernym linkers L = {Li}, appliedin cascade order (cf.
Section 3.3.1).
We start withthe set of page-hypernym pairs H = {(p, h)} asobtained from the syntactic step.
The successfulapplication of a linker to a pair (p, h) ?
H yieldsa page phas the most suitable sense of h, result-ing in setting isa(p, h) = ph.
At step i, the i-th linker Li?
L is applied to H and all the hy-pernyms which the linker could disambiguate areremoved from H .
This prevents lower-precisionlinkers from overriding decisions taken by moreaccurate ones.We now describe the hypernym linkers.
In whatfollows we denote with ph?
phthe fact that thedefinition of a Wikipedia page p contains an oc-currence of h linked to page ph.
Note that phisnot necessarily a sense of h.Crowdsourced linker If ph?
ph, i.e., the hyper-nym h is found to have been manually linked to phin p by Wikipedians, we assign isa(p, h) = ph.For example, because capital was linked in theBRUSSELS page definition to CAPITAL CITY, weset isa(BRUSSELS, capital) = CAPITAL CITY.Category linker Given the set W ?
P ofWikipedia pages which have at least one categoryin common with p, we select the majority senseof h, if there is one, as hyperlinked across all thedefinitions of pages in W :isa(p, h) = argmaxph?p??W1(p?h?
ph)where 1(p?h?
ph) is the characteristic functionwhich equals 1 if h is linked to phin pagep?, 0 otherwise.
For example, the linker setsisa(EGGPLANT, plant) = PLANT because most ofthe pages associated with TROPICAL FRUIT, a cat-egory of EGGPLANT, contain in their definitionsthe term plant linked to the PLANT page.Multiword linker If pm?
phand m is amultiword expression containing the lemma has one of its words, set isa(p, h) = ph.
Forexample, we set isa(PROTEIN, compound) =CHEMICAL COMPOUND, as chemical compoundis linked to CHEMICAL COMPOUND in the defini-tion of PROTEIN.Monosemous linker If h is monosemous inWikipedia (i.e., there is only a single page phforthat lemma), link it to its only sense by settingisa(p, h) = ph.
For example, we extract thehypernym businessperson from the definition ofMERCHANT and, as it is unambiguous, we linkit to BUSINESSPERSON.Distributional linker Finally, we provide a dis-tributional approach to hypernym disambiguation.We represent the textual definition of page p as adistributional vector ~vpwhose components are allthe English lemmas in Wikipedia.
The value ofeach component is the occurrence count of the cor-responding content word in the definition of p.The goal of this approach is to find the bestlink for hypernym h of p among the pages h islinked to, across the whole set of definitions inWikipedia.
Formally, for each phsuch that his linked to phin some definition, we define theset of pages P (ph) whose definitions contain alink to ph, i.e., P (ph) = {p??
P |p?h?
ph}.We then build a distributional vector ~vp?for eachp??
P (ph) as explained above and create an ag-gregate vector ~vph=?p?~vp?.
Finally, we de-termine the similarity of p to each phby calcu-lating the dot product between the two vectorssim(p, ph) = ~vp?
~vph.
If sim(p, ph) > 0 for anyphwe perform the following association:isa(p, h) = argmaxphsim(p, ph)For example, thanks to this linker we setisa(VACUUM CLEANER, device) = MACHINE.947Figure 2: Distribution of linked hypernyms.3.3 Page Taxonomy EvaluationStatistics We applied the above linkers to theOctober 2012 English Wikipedia dump.
Out ofthe 3,829,058 total pages, 4,270,232 hypernymlemmas were extracted in the syntactic step for3,697,113 pages (covering more than 96% of thetotal).
Due to illformed definitions, though, itwas not always possible to extract the hypernymlemma: for example, 6 APRIL 2010 BAGHDADBOMBINGS is defined as ?A series of bomb ex-plosions destroyed several buildings in Baghdad?,which only implicitly provides the hypernym.The semantic step disambiguated 3,718,612 hy-pernyms for 3,294,562 Wikipedia pages, i.e., cov-ering more than 86% of the English pages with atleast one disambiguated hypernym.
Figure 2 plotsthe number and distribution of hypernyms disam-biguated by our hypernym linkers.Taxonomy quality To evaluate the quality ofour page taxonomy we randomly sampled 1,000Wikipedia pages.
For each page we provided: i)a list of suitable hypernym lemmas for the page,mainly selected from its definition; ii) for eachlemma the correct hypernym page(s).
We calcu-lated precision as the average ratio of correct hy-pernym lemmas (senses) to the total number oflemmas (senses) returned for all the pages in thedataset, recall as the number of correct lemmas(senses) over the total number of lemmas (senses)in the dataset, and coverage as the fraction ofpages for which at least one lemma (sense) wasreturned, independently of its correctness.
Results,both at lemma- and sense-level, are reported in Ta-ble 1.
Not only does our taxonomy show high pre-cision and recall in extracting ambiguous hyper-nyms, it also disambiguates more than 3/4 of thehypernyms with high precision.3.3.1 Hypernym linker orderThe optimal order of application of the abovelinkers is the same as that presented in Section3.2.1.
It was established by selecting the combina-tion, among all possible permutations, which max-imized precision on a tuning set of 100 randomlysampled pages, disjoint from our page dataset.Prec.
Rec.
Cov.Lemma 94.83 90.20 98.50Sense 82.77 75.10 89.20Table 1: Page taxonomy performance.4 Phase 2: Inducing the BitaxonomyThe page taxonomy built in Section 3 will serveas a stable, pivotal input to the second phase, theaim of which is to build our bitaxonomy, that is, ataxonomy of pages and categories.
Our key ideais that the generalization-specialization informa-tion available in each of the two taxonomies ismutually beneficial.
We implement this idea byexploiting one taxonomy to update the other, andvice versa, in an iterative way, until a fixed pointis reached.
The final output of this phase is, on theone hand, a page taxonomy augmented with addi-tional hypernymy relations and, on the other hand,a category taxonomy which is built from scratch.4.1 InitializationOur bitaxonomy B = {TP, TC} is a pair consist-ing of the page taxonomy TP= (P,E), as ob-tained in Section 3, and the category taxonomyTC= (C, ?
), which initially contains all the cate-gories as nodes but does not include any hypernymedge between category nodes.
In the followingwe describe the core algorithm of our approach,which iteratively and mutually populates and re-fines the edge sets E(TP) and E(TC).4.2 The Bitaxonomy AlgorithmPreliminaries Before proceeding, we definesome basic concepts that will turn out to be use-ful for presenting our algorithm.
We denote bysuperT(t) the set of all ancestors of a node t in thetaxonomy T (be it TPor TC).
We further define averification function t;Tt?which, in the case ofTC, returns true if there is a path in the Wikipediacategory network between t and t?, false other-wise, and, in the case of TP, returns true if t?isa sense, i.e., a page, of a hypernym h of t (thatis, (t, h) ?
H , cf.
Section 3.2.1).
For instance,SPORTSMEN ;TCMEN BY OCCUPATION holdsfor categories because the former is a sub-categoryof the latter in Wikipedia, and RADIOHEAD ;TPBAND (MUSIC) for pages, because band is a hy-pernym extracted from the textual definition ofRADIOHEAD and BAND (MUSIC) is a sense ofband in Wikipedia.
Note that, while the superfunction returns information that we have alreadylearned, i.e., it is in TPand TC, the ; operator948holds just for candidate is-a relations, as it usesknowledge from Wikipedia itself which is poten-tially incorrect.
For instance, SPORTSMEN ;TCMEN?S SPORTS in the Wikipedia category net-work, and RADIOHEAD ;TPBAND (RADIO) be-tween the two Wikipedia pages, both hold accord-ing to our definition of ;, while connecting thewrong hypernym candidates.
At the core of ouralgorithm, explained below, is the mutual lever-aging of the super function from one of the twotaxonomies (pages or categories) to decide aboutwhich candidates (for which a ; relation holds)in the other taxonomy are real hypernyms.Finally, we define the projection operator pi,such that pi(c), c ?
C, is the set of pagescategorized with c, and pi(p), p ?
P , is theset of categories associated with page p inWikipedia.
For instance, the pages which belongto the category OLYMPIC SPORTS are given bypi(OLYMPIC SPORTS) = {BASEBALL, BOXING,.
.
.
, TRIATHLON}.
Vice versa, pi(TRIATHLON) ={MULTISPORTS, OLYMPIC SPORTS, .
.
.
, OPENWATER SWIMMING}.
The projection operator pienables us to jump from one taxonomy to the otherand expresses the mutual membership relation be-tween pages and categories.Algorithm We now describe in detail the bitax-onomy algorithm, whose pseudocode is given inAlgorithm 1.
The algorithm takes as input the twotaxonomies, initialized as described in Section 4.1.Starting from the category taxonomy (line 1), thealgorithm updates the two taxonomies in turn, un-til convergence is reached, i.e., no more edges canbe added to any side of the bitaxonomy.
Let T bethe current taxonomy considered at a given mo-ment and T?its dual taxonomy.
The algorithmproceeds by selecting a node t ?
V (T ) for whichno hypernym edge (t, th) could be found up untilthat moment (line 3), and then tries to infer sucha relation by drawing on the dual taxonomy T?
(lines 5-12).
This is the core of the bitaxonomy al-gorithm, in which hypernymy knowledge is trans-ferred from one taxonomy to the other.
By apply-ing the projection operator pi to t, the algorithmconsiders those nodes t?aligned to t in the dualtaxonomy (line 5) and obtains their hypernyms t?husing the superT?function (line 6).
The nodesreached in T?act as a clue for discovering the suit-able hypernyms for the starting node t ?
V (T ).To perform the discovery, the algorithm projectseach such hypernym node t?h?
S and incrementsthe count of each projection th?
pi(t?h) (lineAlgorithm 1 The Bitaxonomy AlgorithmInput: TP, TC1: T := TC, T?
:= TP2: repeat3: for all t ?
V (T ) s.t.
@(t, th) ?
E(T ) do4: reset count5: for all t??
pi(t) do6: S := superT?(t?
)7: for all t?h?
S do8: for all th?
pi(t?h) do count(th)++ end for9: end for10: end for11:?th:= argmaxth: t;Tthcount(th)12: if count(?th) > 0 thenE(T ) := E(T )?
{(t,?th)}13: end for14: swap T and T?15: until convergence16: return {T, T?}8).
Finally, the node?th?
V (T ) with maximumcount, and such that t ;T?thholds, if one exists,is promoted as hypernym of t and a new hypernymedge (t,?th) is added toE(T ) (line 12).
Finally, therole of the two taxonomies is swapped and the pro-cess is repeated until no more change is possible.Example Let us illustrate the algorithm by wayof an example.
Assume we are in the first iteration(T = TC) and consider the Wikipedia categoryt = OLYMPICS (line 3) and its super-categories{MULTI-SPORT EVENTS, SPORT AND POLITICS,INTERNATIONAL SPORTS COMPETITIONS}.
Thiscategory has 27 pages associated with it (line5), 23 of which provide a hypernym page in TP(line 6): e.g., PARALYMPIC GAMES, associatedwith the category OLYMPICS, is a MULTI-SPORTEVENT and is therefore contained in S. By con-sidering and counting the categories of each pagein S (lines 7-8), we end up counting the categoryMULTI-SPORT EVENTS four times and othercategories, such as AWARDS and SWIMSUITS,once.
As MULTI-SPORT EVENTS has the highestcount and is connected to OLYMPICS by a pathin the Wikipedia category network (line 11),the hypernym edge (OLYMPICS, MULTI-SPORTEVENTS) is added to TC(line 12).5 Phase 3: Category taxonomyrefinementAs the final phase, we refine and enrich the cate-gory taxonomy.
The goal of this phase is to pro-vide broader coverage to the category taxonomyTCcreated as explained in Section 4.
We applythree enrichment heuristics which add hypernymsto those categories c for which no hypernym couldbe found in phase 2, i.e., @c?s.t.
(c, c?)
?
E(TC).949Single super-category As a first structural re-finement, we automatically link an uncovered cat-egory c to c?if c?is the only direct super-categoryof c in Wikipedia.Sub-categories We increase the hypernym cov-erage by exploiting the sub-categories of each un-covered category c (see Figure 3a).
In detail,for each uncovered category c we consider theset sub(c) of all the Wikipedia subcategories ofc (nodes c1, c2, .
.
.
, cnin Figure 3a) and then leteach category vote, according to its direct hyper-nym categories in TC(the vote is as in Algo-rithm 1).
Then we proceed in decreasing orderof vote and select the highest-ranking category c?which is connected to c via a path in TC, i.e.,c ;TCc?.
We then pick up the direct ancestorc?
?of c which lies in the path from c to c?andadd the hypernym edge (c, c??)
to E(TC).
For ex-ample, consider the category FRENCH TELEVI-SION PEOPLE; since this category has no asso-ciated pages, in phase 2 no hypernym could befound.
However, by applying the sub-categoriesheuristic, we discover that TELEVISION PEOPLEBY COUNTRY is the hypernym most voted by ourtarget category?s descendants, such as FRENCHTELEVISION ACTORS and FRENCH TELEVISIONDIRECTORS.
Since TELEVISION PEOPLE BYCOUNTRY is at distance 1 in the Wikipediacategory network from FRENCH TELEVISIONPEOPLE, we add (FRENCH TELEVISION PEOPLE,TELEVISION PEOPLE BY COUNTRY) to E(TC).Super-categories We then apply a similarheuristic involving super-categories (see Figure3b).
Given an uncovered category c, we considerits direct Wikipedia super-categories and let themvote, according to their hypernym categories inTC.
Then we proceed in decreasing order of voteand select the highest-ranking category c?which isconnected to c in TC, i.e., c;TCc?.
We then pickup the direct ancestor c?
?of c which lies in the pathfrom c to c?and add the edge (c, c??)
to E(TC).5.1 Bitaxonomy EvaluationCategory taxonomy statistics We appliedphases 2 and 3 to the output of phase 1, whichwas evaluated in Section 3.3.
In Figure 4a weshow the increase in category coverage at eachiteration throughout the execution of the twophases (1SUP, SUB and SUPER correspond tothe three above heuristics of phase 3).
The finaloutcome is a category taxonomy which includes594,917 hypernymy links between categories,c?dec??cc1c2.
.
.cn(a) Sub categ.
heuristic.hypernym in TCWikipedia super-categoryc?c???c1c??cm.
.
.c(b) Super categ.
heuristic.Figure 3: Heuristic patterns for the coverage re-finement of the category taxonomy.covering more than 96% of the 618,641 categoriesin the October 2012 English Wikipedia dump.The graph shows the steepest slope in the firstiterations of phase 2, which converges around400k categories at iteration 30, and a significantboost due to phase 3 producing another 175khypernymy edges, with the super-category heuris-tic contributing most.
78.90% of the nodes inTCbelong to the same connected component.The average height of the biggest component ofTCis 23.26 edges and the maximum height is49.
We note that the average height of TCismuch greater than that of TP, which reflects thecategory taxonomy distinguishing between verysubtle classes, such as ALBUMS BY ARTISTS,ALBUMS BY RECORDING LOCATION, etc.Category taxonomy quality To estimate thequality of the category taxonomy, we ran-domly sampled 1,000 categories and, for each ofthem, we manually associated the super-categorieswhich were deemed to be appropriate hypernyms.Figure 4b shows the performance trend as the al-gorithm iteratively covers more and more cate-gories.
Phase 2 is particularly robust across it-erations, as it leads to increased recall while re-taining very high precision.
As regards phase 3,the super-categories heuristic leads to only a slightprecision decrease, while improving recall consid-erably.
Overall, the final taxonomy TCachieves85.80% precision, 83.40% recall and 97.20% cov-erage on our dataset.Page taxonomy improvement As a result ofphase 2, 141,105 additional hypernymy links werealso added to the page taxonomy, resulting inan overall 82.99% precision, 77.90% recall and92.10% coverage, with a non-negligible 3% boostfrom phase 1 to phase 2 in terms of recall and cov-erage on our Wikipedia page dataset.We also calculated some statistics for the result-ing taxonomy obtained by aggregating the 3.8M950Figure 4: Category taxonomy evaluation.hypernym links in a single directed graph.
Over-all, 99% of nodes belong to the same connectedcomponent, with a maximum height of 29 and anaverage height on the biggest component of 6.98.6 Related WorkAlthough the extraction of taxonomies frommachine-readable dictionaries was already beingstudied in the early 1970s (Calzolari et al, 1973),pioneering work on large amounts of data onlyappeared in the 1990s (Hearst, 1992; Ide andV?eronis, 1993).
Approaches based on hand-crafted patterns and pattern matching techniqueshave been developed to provide a supertype forthe extracted terms (Etzioni et al, 2004; Blohm,2007; Kozareva and Hovy, 2010; Navigli and Ve-lardi, 2010; Velardi et al, 2013, inter alia).
How-ever, these methods do not link terms to existingknowledge resources such as WordNet, whereasthose that explicitly link do so by adding newleaves to the existing taxonomy instead of acquir-ing wide-coverage taxonomies from scratch (Pan-tel and Ravichandran, 2004; Snow et al, 2006).The recent upsurge of interest in collabo-rative knowledge curation has enabled severalapproaches to large-scale taxonomy acquisition(Hovy et al, 2013).
Most approaches initiallyfocused on the Wikipedia category network, anentangled set of generalization-containment rela-tions between Wikipedia categories, to extract thehypernymy taxonomy as a subset of the network.The first approach of this kind was WikiTaxonomy(Ponzetto and Strube, 2007; Ponzetto and Strube,2011), based on simple, yet effective lightweightheuristics, totaling more than 100k is-a relations.Other approaches, such as YAGO (Suchanek etal., 2008; Hoffart et al, 2013), yield a taxonom-ical backbone by linking Wikipedia categories toWordNet.
However, the categories are linked tothe first, i.e., most frequent, sense of the categoryhead in WordNet, involving only leaf categories inthe linking.Interest in taxonomizing Wikipedia pages, in-stead, developed with DBpedia (Auer et al, 2007),which pioneered the current stream of work aimedat extracting semi-structured information fromWikipedia templates and infoboxes.
In DBpedia,entities are mapped to a coarse-grained ontologywhich is collaboratively maintained and containsonly about 270 classes corresponding to popularnamed entity types, in contrast to our goal of struc-turing the full set of Wikipedia articles in a largerand finer-grained taxonomy.A few notable efforts to reconcile the two sidesof Wikipedia, i.e., pages and categories, havebeen put forward very recently: WikiNet (Nas-tase et al, 2010; Nastase and Strube, 2013) is aproject which heuristically exploits different as-pects of Wikipedia to obtain a multilingual con-cept network by deriving not only is-a relations,but also other types of relations.
A second project,MENTA (de Melo and Weikum, 2010), createsone of the largest multilingual lexical knowledgebases by interconnecting more than 13M articlesin 271 languages.
In contrast to our work, hy-pernym extraction is supervised in that decisionsare made on the basis of labelled training exam-ples and requires a reconciliation step owing tothe heterogeneous nature of the hypernyms, some-thing that we only do for categories, due to theirnoisy network.
While WikiNet and MENTA bringtogether the knowledge available both at the pageand category level, like we do, they either achievelow precision and coverage of the taxonomicalstructure or exhibit overly general hypernyms, aswe show in our experiments in the next section.Our work differs from the others in at least threerespects: first, in marked contrast to most other re-sources, but similarly to WikiNet and WikiTaxon-omy, our resource is self-contained and does notdepend on other resources such as WordNet; sec-ond, we address the taxonomization task on bothsides, i.e., pages and categories, by providing analgorithm which mutually and iteratively transfersknowledge from one side of the bitaxonomy to theother; third, we provide a wide coverage bitaxon-omy closer in structure and granularity to a manualWordNet-like taxonomy, in contrast, for example,to DBpedia?s flat entity-focused hierarchy.22Note that all the competitors on categories have averageheight between 1 and 3.69 on their biggest component, whilewe have 23.26, while on pages their height is between 1.9 and4.22, while ours is 6.98.
Since WordNet?s average height is8.07 we deem WiBi to be the resource structurally closest toWordNet.951Dataset System Prec.
Rec.
Cov.PagesWiBi 84.11 79.40 92.57WikiNet 57.29??71.45?
?82.01DBpedia 87.06 51.50?
?55.93MENTA 81.52 72.49?88.92CategoriesWiBi 85.18 82.88 97.31WikiTax 88.50 54.83?
?59.43YAGO 94.13 53.41?
?56.74MENTA 87.11 84.63 97.15MENTA?ENT85.18 71.95?
?84.47Table 2: Page and category taxonomy evaluation.?(??)
denotes statistically significant difference,using ?2test, p < 0.02 (p < 0.01) between WiBiand the daggered resource.7 Comparative Evaluation7.1 Experimental SetupWe compared our resource (WiBi) against theWikipedia taxonomies of the major knowledge re-sources in the literature providing hypernym links,namely DBpedia, WikiNet, MENTA, WikiTax-onomy and YAGO (see Section 6).
As datasets,we used our gold standards of 1,000 randomly-sampled pages (see Section 3.3) and categories(see Section 5.1).
In order to ensure a level playingfield, we detected those pages (categories) whichdo not exist in any of the above resources and re-moved them to ensure full coverage of the datasetacross all resources.
For each resource we cal-culated precision, by manually marking each hy-pernym returned for each page (category) as cor-rect or not.
As regards recall, we note that intwo cases (i.e., DBpedia returning page super-types from its upper taxonomy, YAGO linking cat-egories to WordNet synsets) the generalizationsare neither pages nor categories and that MENTAreturns heterogeneous hypernyms as mixed sets ofWordNet synsets, Wikipedia pages and categories.Given this heterogeneity, standard recall across re-sources could not be calculated.
For this reason wecalculated recall as described in Section 3.3.7.2 ResultsWikipedia pages We first report the results ofthe knowledge resources which provide page hy-pernyms, i.e., we compare against WikiNet, DB-pedia and MENTA.
We use the original outputsfrom the three resources: the first two are basedon dumps which are from the same year as the oneused in WiBi (cf.
Section 3.3), while MENTA isbased on a dump dating back to 2010 (consistingof 3.25M pages and 565k categories).
We decidedto include the latter for comparison purposes, as ituses knowledge from 271 Wikipedias to build thefinal taxonomy.
However, we recognize its perfor-mance might be relatively higher on a 2012 dump.We show the results on our page hypernymdataset in Table 2 (top).
As can be seen, WikiNetobtains the lowest precision, due to the high num-ber of hypernyms provided, many of which areincorrect, with a recall between that of DBpe-dia and MENTA.
WiBi outperforms all other re-sources with 84.11% precision, 79.40% recall and92.57% coverage.
MENTA seems to be the clos-est resource to ours, however, we remark that thehypernyms output by MENTA are very heteroge-neous: 48% of answers are represented by a Word-Net synset, 37% by Wikipedia categories and 15%are Wikipedia pages.
In contrast to all other re-sources, WiBi outputs page hypernyms only.Wikipedia categories We then compared all theknowledge resources which deal with categories,i.e., WikiTaxonomy, YAGO and MENTA.
For thelatter two, the above considerations about the 2012dump hold, whereas we reimplemented WikiTax-onomy, which was based on a 2009 dump, to run iton the same dump as WiBi.
We excluded WikiNetfrom our comparison because it turned out to havelow coverage of categories (i.e., less than 1%).We show the results on our category datasetin Table 2 (bottom).
Despite other systems ex-hibiting higher precision, WiBi generally achieveshigher recall, thanks also to its higher categorycoverage.
YAGO obtains the lowest recall andcoverage, because only leaf categories are consid-ered.
MENTA is the closest system to ours, ob-taining slightly higher precision and recall.
No-tably, however, MENTA outputs the first WordNetsense of entity for 13.17% of all the given answers,which, despite being correct and accounted in pre-cision and recall, is uninformative.
Since a systemwhich always outputs entity would maximise allthe three measures, we also calculated the perfor-mance for MENTA when discarding entity as ananswer; as Table 2 shows (bottom, MENTA?ENT),recall drops to 71.95%.
Further analysis, pre-sented below, shows that the specificity of its hy-pernyms is considerably lower than that of WiBi.7.3 Analysis of the resultsTo get further insight into our results we per-formed two additional analyses of the data.
First,we estimated the level of specialization of thehypernyms in the different resources on our twodatasets.
The idea is that a hypernym should be952Dataset System (X) WiBi=X WiBi>X WiBi<XPagesWikiNet 33.38 34.94 31.68DBpedia 31.68 56.71 11.60MENTA 19.04 50.85 30.12CategoriesWikiTax 43.11 38.51 18.38YAGO 12.36 81.14 6.50MENTA 12.36 73.69 13.95Table 3: Specificity comparison.valid while at the same time being as specific aspossible (e.g., SINGER should be preferred overPERSON).
We therefore calculated a measure,which we called specificity, that computes the per-centage of times a system outputs a more specificanswer than another system.
To do this, we anno-tated each hypernym returned by a system as fol-lows: ?1 if the answer was wrong, 0 if missing, >0 if correct; more specific answers were assignedhigher scores.
When comparing two systems, weselect the respective most specific answers a1, a2and say the first system is more specific than thelatter whenever score(a1) > score(a2).
Table 3shows the results for all the resources and for boththe page and category taxonomies: WiBi consis-tently provides considerably more specific hyper-nyms than any other resource (middle column).A second important aspect that we analyzed wasthe granularity of each taxonomy, determined bydrawing each resource on a bidimensional planewith the number of distinct hypernyms on thex axis and the total number of hypernyms (i.e.,edges) in the taxonomy on the y axis.
Figures 5aand 5b show the position of each resource for thepage and the category taxonomies, respectively.As can be seen, WiBi, as well as the page tax-onomy of MENTA, is the resource with the bestgranularity, as not only does it attain high cover-age, but it also provides a larger variety of classesas generalizations of pages and categories.
Specif-ically, WiBi provides over 3M hypernym pageschosen from a range of 94k distinct hypernyms,while others exhibit a considerably smaller rangeof distinct hypernyms (e.g., DBpedia by design,but also WikiNet, with around 11k distinct pagehypernyms).
The large variety of classes providedby MENTA, however, is due to including morethan 100k Wikipedia categories (among which,categories about deaths and births alone repre-sent about 2% of the distinct hypernyms).
As re-gards categories, while the number of distinct hy-pernyms of WiBi and WikiTaxonomy is approxi-mately the same (around 130k), the total numberof hypernyms (around 580k for both taxonomies)is distributed over half of the categories in Wiki-(a) Page taxonomies (b) Category taxonomiesFigure 5: Hypernym granularity for the resources.Taxonomy compared to WiBi, resulting in a dou-ble number of hypernyms per category, but lowercoverage (cf.
Table 2).8 ConclusionsIn this paper we have presented WiBi, an auto-matic 3-phase approach to the construction of abitaxonomy for the English Wikipedia, i.e., a full-fledged, integrated page and category taxonomy:first, using a set of high-precision linkers, the pagetaxonomy is populated; next, a fixed point algo-rithm populates the category taxonomy while en-riching the page taxonomy iteratively; finally, thecategory taxonomy undergoes structural refine-ments.
Coverage, quality and granularity of thebitaxonomy are considerably higher than the tax-onomy of state-of-the-art resources like DBpedia,YAGO, MENTA, WikiNet and WikiTaxonomy.Our contributions are three-fold: i) we proposea unified, effective approach to the construction ofa Wikipedia bitaxonomy, a richer structure thanthose produced in the literature; ii) our method forbuilding the bitaxonomy is self-contained, thanksto its independence from external resources (likeWordNet) and the virtual absence of supervision,making WiBi replicable on any new version ofWikipedia; iii) the taxonomy provides nearly fullcoverage of pages and categories, encompassingthe entire encyclopedic knowledge in Wikipedia.We will apply our video games with a purpose(Vannella et al, 2014) to validate WiBi.
We alsoplan to integrate WiBi into BabelNet (Navigli andPonzetto, 2012), so as to fully taxonomize it, andexploit its high quality for improving semanticpredicates (Flati and Navigli, 2013).AcknowledgmentsThe authors gratefully acknowledgethe support of the ERC StartingGrant MultiJEDI No.
259234.We thank Luca Telesca for his implementation ofWikiTaxonomy and Jim McManus for his com-ments on the manuscript.953ReferencesRobert A. Amsler.
1981.
A Taxonomy for EnglishNouns and Verbs.
In Proceedings of Association forComputational Linguistics (ACL ?81), pages 133?138, Stanford, California, USA.S?oren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary Ive.2007.
DBpedia: A nucleus for a web of open data.In Proceedings of 6th International Semantic WebConference joint with 2nd Asian Semantic Web Con-ference (ISWC+ASWC 2007), pages 722?735, Bu-san, Korea.Christian Bizer, Jens Lehmann, Georgi Kobilarov,S?oren Auer, Christian Becker, Richard Cyganiak,and Sebastian Hellmann.
2009.
DBpedia - a crystal-lization point for the Web of Data.
Web Semantics,7(3):154?165.Sebastian Blohm.
2007.
Using the web to reduce datasparseness in pattern-based information extraction.In Proceedings of the 11th European Conference onPrinciples and Practice of Knowledge Discovery inDatabases (PKDD), pages 18?29, Warsaw, Poland.Springer.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: A collab-oratively created graph database for structuring hu-man knowledge.
In Proceedings of the InternationalConference on Management of Data (SIGMOD ?08),SIGMOD ?08, pages 1247?1250, New York, NY,USA.Nicoletta Calzolari, Laura Pecchia, and Antonio Zam-polli.
1973.
Working on the Italian Machine Dictio-nary: a Semantic Approach.
In Proceedings of the5th Conference on Computational Linguistics (COL-ING ?73), pages 49?70, Pisa, Italy.Nicoletta Calzolari.
1982.
Towards the organization oflexical definitions on a database structure.
In Proc.of the 9th Conference on Computational Linguistics(COLING ?82), pages 61?64, Prague, Czechoslo-vakia.Gerard de Melo and Gerhard Weikum.
2010.
MENTA:Inducing Multilingual Taxonomies from Wikipedia.In Proceedings of Conference on Information andKnowledge Management (CIKM ?10), pages 1099?1108, New York, NY, USA.Oren Etzioni, Michael Cafarella, Doug Downey, Stan-ley Kok, Ana-Maria Popescu, Tal Shaked, StephenSoderland, Daniel S. Weld, and Alexander Yates.2004.
Web-scale information extraction in know-ItAll: (preliminary results).
In Proceedings of the13th International Conference on World Wide Web(WWW ?04), pages 100?110, New York, NY, USA.ACM.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.David A. Ferrucci.
2012.
Introduction to ?This is Wat-son?.
IBM Journal of Research and Development,56(3):1.Tiziano Flati and Roberto Navigli.
2013.
SPred:Large-scale Harvesting of Semantic Predicates.
InProceedings of the 51st Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages1222?1232, Sofia, Bulgaria.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedingsof the International Conference on ComputationalLinguistics (COLING ?92), pages 539?545, Nantes,France.Johannes Hoffart, Fabian M. Suchanek, KlausBerberich, and Gerhard Weikum.
2013.
YAGO2: Aspatially and temporally enhanced knowledge basefrom Wikipedia.
Artificial Intelligence, 194:28?61.Eduard H. Hovy, Roberto Navigli, and Simone PaoloPonzetto.
2013.
Collaboratively built semi-structured content and Artificial Intelligence: Thestory so far.
Artificial Intelligence, 194:2?27.Nancy Ide and Jean V?eronis.
1993.
Extractingknowledge bases from machine-readable dictionar-ies: Have we wasted our time?
In Proceedings ofthe Workshop on Knowledge Bases and KnowledgeStructures, pages 257?266, Tokyo, Japan.Dan Klein and Christopher D. Manning.
2003.
FastExact Inference with a Factored Model for NaturalLanguage Parsing.
In Advances in Neural Infor-mation Processing Systems 15 (NIPS), pages 3?10,Vancouver, British Columbia, Canada.Zornitsa Kozareva and Eduard H. Hovy.
2010.
ASemi-Supervised Method to Learn and ConstructTaxonomies Using the Web.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP ?10), pages 1110?1118,Seattle, WA, USA.Olena Medelyan, David Milne, Catherine Legg, andIan H. Witten.
2009.
Mining meaning fromWikipedia.
International Journal of Human-Computer Studies, 67(9):716?754.Shachar Mirkin, Ido Dagan, and Eyal Shnarch.
2009.Evaluating the inferential utility of lexical-semanticresources.
In Proceedings of the 12th Conferenceof the European Chapter of the Association forComputational Linguistics (EACL), pages 558?566,Athens, Greece.Tom Mitchell.
2005.
Reading the Web: A Break-through Goal for AI.
AI Magazine.Vivi Nastase and Michael Strube.
2013.
Transform-ing Wikipedia into a large scale multilingual conceptnetwork.
Artificial Intelligence, 194:62?85.954Vivi Nastase, Michael Strube, Benjamin Boerschinger,Caecilia Zirn, and Anas Elghafari.
2010.
WikiNet:A Very Large Scale Multi-Lingual Concept Net-work.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10), Valletta, Malta.Roberto Navigli and Simone Paolo Ponzetto.
2012.BabelNet: The automatic construction, evaluationand application of a wide-coverage multilingual se-mantic network.
Artificial Intelligence, 193:217?250.Roberto Navigli and Paola Velardi.
2010.
LearningWord-Class Lattices for Definition and HypernymExtraction.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics(ACL 2010), pages 1318?1327, Uppsala, Sweden,July.
Association for Computational Linguistics.Patrick Pantel and Deepak Ravichandran.
2004.
Au-tomatically labeling semantic classes.
In Proceed-ings of the Human Language Technology Confer-ence of the North American Chapter of the Asso-ciation for Computational Linguistics (NAACL HLT2013), Boston, Massachusetts, 2?7 May 2004, pages321?328.Simone Paolo Ponzetto and Michael Strube.
2007.Deriving a large scale taxonomy from Wikipedia.In Proceedings of the 22nd Conference on the Ad-vancement of Artificial Intelligence (AAAI ?07), Van-couver, B.C., Canada, 22?26 July 2007, pages1440?1445.Simone Paolo Ponzetto and Michael Strube.
2011.Taxonomy induction based on a collaboratively builtknowledge repository.
Artificial Intelligence, 175(9-10):1737?1756.Hoifung Poon, Janara Christensen, Pedro Domingos,Oren Etzioni, Raphael Hoffmann, Chloe Kiddon,Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Ste-fan Schoenmackers, Stephen Soderland, Dan Weld,Fei Wu, and Congle Zhang.
2010.
Machine Read-ing at the University of Washington.
In Proceedingsof the 1st International Workshop on Formalismsand Methodology for Learning by Reading in con-junction with NAACL-HLT 2010, pages 87?95, LosAngeles, California, USA.Maria Ruiz-Casado, Enrique Alfonseca, and PabloCastells.
2005.
Automatic assignment of Wikipediaencyclopedic entries to WordNet synsets.
In Ad-vances in Web Intelligence, volume 3528 of Lec-ture Notes in Computer Science, pages 380?386.Springer Verlag.Amit Singhal.
2012.
Introducing the KnowledgeGraph: Things, Not Strings.
Technical report, Of-ficial Blog (of Google).
Retrieved May 18, 2012.Rion Snow, Dan Jurafsky, and Andrew Ng.
2006.
Se-mantic taxonomy induction from heterogeneous ev-idence.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the Association for Computa-tional Linguistics (COLING-ACL 2006), pages 801?808.Fabian Suchanek and Gerhard Weikum.
2013.
Knowl-edge harvesting from text and Web sources.
In IEEE29th International Conference on Data Engineer-ing (ICDE 2013), pages 1250?1253, Brisbane, Aus-tralia.
IEEE Computer Society.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2008.
YAGO: A large ontology fromWikipedia and WordNet.
Journal of Web Semantics,6(3):203?217.Daniele Vannella, David Jurgens, Daniele Scarfini,Domenico Toscani, and Roberto Navigli.
2014.Validating and Extending Semantic KnowledgeBases using Video Games with a Purpose.
In Pro-ceedings of the 52nd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2014),Baltimore, USA.Paola Velardi, Stefano Faralli, and Roberto Navigli.2013.
OntoLearn Reloaded: A graph-based algo-rithm for taxonomy induction.
Computational Lin-guistics, 39(3):665?707.955
