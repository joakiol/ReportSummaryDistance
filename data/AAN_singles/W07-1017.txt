BioNLP 2007: Biological, translational, and clinical language processing, pages 129?136,Prague, June 2007. c?2007 Association for Computational LinguisticsAutomatic Code Assignment to Medical TextKoby Crammer and Mark Dredze and Kuzman Ganchev and Partha Pratim TalukdarDepartment of Computer and Information Science, University of Pennsylvania, Philadelphia, PA{crammer|mdredze|kuzman|partha}@seas.upenn.eduSteven CarrollDivision of Oncology, The Children?s Hospital of Philadelphia, Philadelphia, PAcarroll@genome.chop.eduAbstractCode assignment is important for handlinglarge amounts of electronic medical data inthe modern hospital.
However, only expertannotators with extensive training can as-sign codes.
We present a system for theassignment of ICD-9-CM clinical codes tofree text radiology reports.
Our system as-signs a code configuration, predicting one ormore codes for each document.
We com-bine three coding systems into a single learn-ing system for higher accuracy.
We compareour system on a real world medical datasetwith both human annotators and other auto-mated systems, achieving nearly the maxi-mum score on the Computational MedicineCenter?s challenge.1 IntroductionThe modern hospital generates tremendous amountsof data: medical records, lab reports, doctor notes,and numerous other sources of information.
As hos-pitals move towards fully electronic record keeping,the volume of this data only increases.
While manymedical systems encourage the use of structured in-formation, including assigning standardized codes,most medical data, and often times the most impor-tant information, is stored as unstructured text.This daunting amount of medical text createsexciting opportunities for applications of learningmethods, such as search, document classification,data mining, information extraction, and relation ex-traction (Shortliffe and Cimino, 2006).
These ap-plications have the potential for considerable bene-fit to the medical community as they can leverageinformation collected by hospitals and provide in-centives for electronic record storage.
Much of thedata generated by medical personnel is unused pastthe clinical visit, often times because there is no wayto simply and quickly apply the wealth of informa-tion.
Medical NLP holds the promise of both greatercare for individual patients and enhanced knowledgeabout health care.In this work we explore the assignment of ICD-9-CM codes to clinical reports.
We focus on this prac-tical problem since it is representative of the typeof task faced by medical personnel on a daily ba-sis.
Many hospitals organize and code documentsfor later retrieval using different coding standards.Often times, these standards are extremely complexand only trained expert coders can properly performthe task, making the process of coding documentsboth expensive and unreliable since a coder must se-lect from thousands of codes a small number for agiven report.
An accurate automated system wouldreduce costs, simplify the task for coders, and createa greater consensus and standardization of hospitaldata.This paper addresses some of the challenges asso-ciated with ICD-9-CM code assignment to clinicalfree text, as well as general issues facing applica-tions of NLP to medical text.
We present our auto-mated system for code assignment developed for theComputational Medicine Center?s challenge.
Ourapproach uses several classification systems, eachwith the goal of predicting the exact code configu-ration for a medical report.
We then use a learning129system to combine our predictions for superior per-formance.This paper is organized as follows.
First, we ex-plain our task and difficulties in detail.
Next we de-scribe our three automated systems and features.
Wecombine the three approaches to create a single su-perior system.
We evaluate our system on clinicalreports and show accuracy approaching human per-formance and the challenge?s best score.2 Task OverviewThe health care system employs a large number ofcategorization and classification systems to assistdata management for a variety of tasks, includingpatient care, record storage and retrieval, statisticalanalysis, insurance, and billing.
One of these sys-tems is the International Classification of Diseases,Ninth Revision, Clinical Modification (ICD-9-CM)which is the official system of assigning codes to di-agnoses and procedures associated with hospital uti-lization in the United States.
1 The coding systemis based on World Health Organization guidelines.An ICD-9-CM code indicates a classification of adisease, symptom, procedure, injury, or informationfrom the personal history.
Codes are organized hier-archically, where top level entries are general group-ings (e.g.
?diseases of the respiratory system?)
andbottom level codes indicate specific symptoms ordiseases and their location (e.g.
?pneumonia in as-pergillosis?).
Each specific, low-level code consistsof 4 or 5 digits, with a decimal after the third.
Higherlevel codes typically include only 3 digits.
Overall,there are thousands of codes that cover a broad rangeof medical conditions.Codes are assigned to medical reports by doc-tors, nurses and other trained experts based on com-plex coding guidelines (National Center for HealthStatistics, 2006).
A particular medical report can beassigned any number of relevant codes.
For exam-ple, if a patient exhibits a cough, fever and wheez-ing, all three codes should be assigned.
In addi-tion to finding appropriate codes for each condition,complex rules guide code assignment.
For exam-ple, a diagnosis code should always be assigned if adiagnosis is reached, a diagnosis code should never1http://www.cdc.gov/nchs/about/otheract/icd9/abticd9.htmbe assigned when the diagnosis is unclear, a symp-tom should never be assigned when a diagnosis ispresent, and the most specific code is preferred.
Thismeans that codes that seem appropriate to a reportshould be omitted in specific cases.
For example,a patient with hallucinations should be coded 780.1(hallucinations) but for visual hallucinations, thecorrect code is 368.16.
The large number of codesand complexity of assignment rules make this a diffi-cult problem for humans (inter-annotator agreementis low).
Therefore, an automated system that sug-gested or assigned codes could make medical datamore consistent.These complexities make the problem difficultfor NLP systems.
Consider the task as multi-class,multi-label.
For a given document, many codes mayseem appropriate but it may not be clear to the algo-rithm how many to assign.
Furthermore, the codesare not independent and different labels can inter-act to either increase or decrease the likelihood ofthe other.
Consider a report that says, ?patient re-ports cough and fever.?
The presence of the wordscough and fever indicate codes 786.2 (cough) and780.6 (fever).
However, if the report continues tostate that ?patient has pneumonia?
then these codesare dropped in favor of 486 (pneumonia).
Further-more, if the report then says ?verify clinically?, thenthe diagnosis is uncertain and only codes 786.2 and780.6 apply.
Clearly, this is a challenging problem,especially for an automated system.2.1 CorpusWe built and evaluated our system in accordancewith the Computational Medicine Center?s (CMC)2007 Medical Natural Language Processing Chal-lenge.2 Since release of medical data must strictlyfollow HIPAA standards, the challenge corpus un-derwent extensive treatment for disambiguation,anonymization, and careful scrubbing.
A detaileddescription of data preparation is found in Compu-tational Medicine Center (2007).
We describe thecorpus here to provide context for our task.The training corpus is comprised of 978 radiolog-ical reports taken from real medical records.
A testcorpus contains 976 unlabeled documents.
Radiol-ogy reports have two text fields, clinical history and2www.computationalmedicine.org/challenge130impression.
The physician ordering the x-ray writesthe clinical history, which contains patient informa-tion for the radiologist, including history and currentsymptoms.
Sometimes a guess as to the diagnosisappears (?evaluate for asthma?).
The descriptionsare sometimes whole sentences and other times sin-gle words (?cough?).
The radiologist writes the im-pression to summarize his or her findings.
It con-tains a short analysis and often times a best guess asto the diagnosis.
At times this field is terse, (?pneu-monia?
or ?normal kidneys?)
and at others it con-tains an entire paragraph of text.
Together, these twofields are used to assign ICD-9-CM codes, whichjustify a certain procedure, possibly for reimburse-ment by the insurance company.Only a small percentage of ICD-9-CM codes ap-pear in the challenge.
In total, the reports include 45different codes arranged in 94 configurations (com-binations).
Some of these codes appear frequently,while others are rare, appearing only a single time.The test set is restricted so that each configurationappears at least once in the training set, althoughthere is no further guarantee as to the test set?s distri-bution over codes.
Therefore, in addition to a largenumber of codes, there is variability in the amountof data for each code.
Four codes have over 100examples each and 24 codes have 10 or fewer doc-uments, with 10 of these codes having only a singledocument.Since code annotation is a difficult task, each doc-ument in the corpus was evaluated by three expertannotators.
A gold annotation was created by tak-ing the majority of the annotators; if two of the threeannotators provided a code, that code is used in thegold configuration.
This approach means that a doc-ument?s configuration may be a construction of mul-tiple annotators and may not match any of the threeannotators exactly.
Both the individual and the ma-jority annotations are included with the training cor-pus.While others have attempted ICD-9 code classi-fication, our task differs in two respects (Section 7provides an overview of previous work).
First, pre-vious work has used discharge reports, which aretypically longer with more text fields.
Second, whilemost systems are evaluated as a recommendationsystem, offering the top k codes and then scoringrecall at k, our task is to provide the exact configu-ration.
The CMC challenge evaluated systems usingan F1 score, so we are penalized if we suggest anylabel that does not appear in the majority annotation.To estimate task difficulty we measured the inter-annotator score for the training set using the threeannotations provided.
We scored two annotationswith the micro average F1, which weighs each codeassignment equally (see Section 5 for details onevaluation metrics).
If an annotator omitted a codeand included an extra code, he or she is penalizedwith a false positive (omitting a code) and a falsenegative (adding an extra code).
We measured anno-tators against each other; the average f-measure was74.85 (standard deviation of .06).
These scores werelow since annotators chose from an unrestricted setof codes, many of which were not included in the fi-nal majority annotation.
However, these scores stillindicate the human accuracy for this task using anunrestricted label set.
33 Code Assignment SystemWe developed three automated systems guided byour above analysis.
First, we designed a learningsystem that used natural language features from theofficial code descriptions and the text of each re-port.
It is general purpose and labels all 45 codesand 94 configurations (labels).
Second, we built arule based system that assigned codes based on theoverlap between the reports and code descriptions,similar to how an annotator may search code de-scriptions for appropriate labels.
Finally, a special-ized system aimed at the most common codes imple-mented a policy that mimics the guidelines a medicalstaffer would use to assign these codes.3.1 Learning SystemWe begin with some notational definitions.
In whatfollows, x denotes the generic input document (ra-diology report), Y denotes the set of possible label-ings (code configurations) of x, and y?
(x) the cor-rect labeling of x.
For each pair of document xand labeling y ?
Y , we compute a vector-valuedfeature representation f(x, y).
A linear model is3We also measured each annotator with the majority codes,taking the average score (87.48), and the best annotator withthe majority label (92.8).
However, these numbers are highlybiased since the annotator influences the majority labeling.
Weobserve that our final system still exceeds the average score.131given by a weight vector w. Given this weight vec-tor w, the score w ?
f(x, y) ranks possible labelingsof x, and we denote by Yk,w(x) the set of k topscoring labelings for x.
For some structured prob-lems, a factorization of f(x, y) is required to enablea dynamic program for inference.
For our problem,we know all the possible configurations in advance(there are 94 of them) so we can pick the highestscoring y ?
Y by trying them all.
For each docu-ment x and possible labeling y, we compute a scoreusing w and the feature representation f(x, y).
Thetop scoring y is output as the correct label.
Section3.1.1 describes our feature function f(x, y) whileSection 3.1.2 describes how we find a good weightvector w.3.1.1 FeaturesProblem representation is one of the most impor-tant aspects of a learning system.
In our case, thisis defined by the set of features f(x, y).
Ideally wewould like a linear combination of our features to ex-actly specify the true labeling of all the instances, butwe want to have a small total number of features sothat we can accurately estimate their values.
We sep-arate our features into two classes: label specific fea-tures and transfer features.
For simplicity, we indexfeatures by their name.
Label specific features areonly present for a single label.
For example, a simpleclass of label specific features is the conjunction of aword in the document with an ICD-9-CM code in thelabel.
Thus, for each word we create 94 features, i.e.the word conjoined with every label.
These featurestend to be very powerful, since weights for them canencode very specific information about the way doc-tors talk about a disease, such as the feature ?con-tains word pneumonia and label contains code 486?.Unfortunately, the cost of this power is that there area large number of these features, making parameterestimation difficult for rare labels.
In contrast, trans-fer features can be present in multiple labels.
Anexample of a transfer feature might be ?the impres-sion contains all the words in the code descriptionsof the codes in this label?.
Transfer features allow usto generalize from one label to another by learningthings like ?if all the words of the label descriptionoccur in the impression, then this label is likely?
buthave the drawback that we cannot learn specific de-tails about common labels.
For example, we cannotlearn that the word ?pneumonia?
in the impressionis negatively correlated with the code cough.
Theinclusion of both label specific and transfer featuresallows us to learn specificity where we have a largenumber of examples and generality for rare codes.Before feature extraction we normalized the re-ports?
text by converting it to lower case and byreplacing all numbers (and digit sequences) with asingle token ?NUM?.
We also prepared a synonymdictionary for a subset of the tokens and n-gramspresent in the training data.
The synonym dictionarywas based onMeSH4, the Medical Subject Headingsvocabulary, in which synonyms are listed as termsunder the same concept.
All ngrams and tokensin the training data which had mappings defined inthe synonym dictionary were then replaced by theirnormalized token; e.g.
all mentions of ?nocturnalenuresis?
or ?nighttime urinary incontinence?
werereplaced by the token ?bedwetting?.
Additionally,we constructed descriptions for each code automati-cally from the official ICD-9-CM code descriptionsin National Center for Health Statistics (2006).
Wealso created a mapping between code and code type(diagnosis or symptom) using the guidelines.Our system used the following features.
The de-scriptions of particular features are in quotes, whileschemes for constructing features are not.?
?this configuration contains a disease code?,?this configuration contains a symptom code?,?this configuration contains an ambiguouscode?
and ?this configuration contains both dis-ease and symptom codes?.5?
With the exception of stop-words, all words ofthe impression and history conjoined with eachlabel in the configuration; pairs of words con-joined with each label; words conjoined withpairs of labels.
For example, ?the impressioncontains ?pneumonia?
and the label containscodes 786.2 and 780.6?.?
A feature indicating when the history or im-pression contains a complete code description4www.nlm.nih.gov/mesh5We included a feature for configurations that had both dis-ease and symptom codes because they appeared in the trainingdata, even though coding guidelines prohibit these configura-tions.132for the label; one for a word in common withthe code description for one of the codes in thelabel; a common word conjoined with the pres-ence of a negation word nearby (?no?, ?not?,etc.
); a word in common with a code descrip-tion not present in the label.
We applied similarfeatures using negative words associated witheach code.?
A feature indicating when a soft negation wordappears in the text (?probable?, ?possible?,?suspected?, etc.)
conjoined with words thatfollow; the token length of a text field (?im-pression length=3?
); a conjunction of a featureindicating a short text field with the words inthe field (?impression length=1 and ?pneumo-nia?
?)?
A feature indicating each n-gram sequence thatappears in both the impression and clinical his-tory; the conjunction of certain terms whereone appears in the history and the other in theimpression (e.g.
?cough in history and pneu-monia in impression?
).3.1.2 Learning TechniqueUsing these feature representations, we now learna weight vector w that scores the correct labelingsof the data higher than incorrect labelings.
We useda k-best version of the MIRA algorithm (Crammer,2004; McDonald et al, 2005).
MIRA is an onlinelearning algorithm that for each training documentx updates the weight vector w according to the rule:wnew = argminw?w ?
wold?s.t.
?y ?
Yk,wold(x) :w ?
f(x, y?
(x)) ?
w ?
f(x, y) ?
L(y?
(x), y)where L(y?
(x), y) is a measure of the loss of label-ing y with respect to the correct labeling y?(x).
Forour experiments, we set k to 30 and iterated over thetraining data 10 times.
Two standard modificationsto this approach also helped.
First, rather than usingjust the final weight vector, we average all weightvectors.
This has a smoothing effect that improvesperformance on most problems.
The second modifi-cation is the introduction of slack variables:wnew = argminw?w ?
wold?
+ ??i?is.t.
?y ?
Yk,wold(x) :w ?
f(x, y?
(x)) ?
w ?
f(x, y) ?
L(y?
(x), y) ?
?i?i ?
{1 .
.
.
k} : ?i ?
0.We used a ?
of 10?3 in our experiments.The most straightforward loss function is the 0/1loss, which is one if y does not equal y?
(x) and zerootherwise.
Since we are evaluated based on the num-ber of false negative and false positive ICD-9-CMcodes assigned to all the documents, we used a lossthat is the sum of the number of false positive and thenumber of false negative labels that y assigns withrespect to y?
(x).Finally, we only used features that were possi-ble for some labeling of the test data by using onlythe test data to construct our feature alphabet.
Thisforced the learner to focus on hypotheses that couldbe used at test time and resulted in a 1% increase inF-measure in our final system on the test data.3.2 Rule Based SystemSince some of the configurations appear a smallnumber of times in our corpus (some only once),we built a rule based system that requires no train-ing.
The system uses a description of the ICD-9-CMcodes and their types, similar to the list used by ourlearning system (Section 3.1.1).
The code descrip-tions include between one and four short descrip-tions, such as ?reactive airway disease?, ?asthma?,and ?chronic obstructive pulmonary disease?.
Wetreat each of these descriptions as a bag of words.For a given report, the system parses both the clini-cal history and impression into sentences, using ?.
?as a sentence divider.
Each sentence is the checkedto see if all of the words in a code description appearin the sentence.
If a match is found, we set a flagcorresponding to the code.
However, if the code isa disease, we search for a negation word in the sen-tence, removing the flag if a negation word is found.Once all code descriptions have been evaluated, wecheck if there are any flags set for disease codes.
Ifso, we remove all symptom code flags.
We then emita code corresponding to each set flag.
This simplesystem does not enforce configuration restrictions;133we may predict a code configuration that does notappear in our training data.
Adding this restrictionimproved precision but hurt recall, leading to a slightdecrease in F1 score.
We therefore omitted the re-striction from our system.3.3 Automatic Coding PoliciesAs we described in Section 2, enforcing codingguidelines can be a complex task.
While a learningsystem may have trouble coding a document, a hu-man may be able to define a simple policy for cod-ing.
Since some of the most frequent codes in ourdataset have this property, we decided to implementsuch an automatic coding policy.
We selected tworelated sets of codes to target with a rule based sys-tem, a set of codes found in pneumonia reports anda set for urinary tract infection/reflux reports.Reports related to pneumonia are the most com-mon in our dataset and include codes for pneumo-nia, asthma, fever, cough and wheezing; we handlethem with a single policy.
Our policy is as follows:?
Search for a small set of keywords (e.g.
?cough?, ?fever?)
to determine if a code shouldbe applied.?
If ?pneumonia?
appears unnegated in the im-pression and the impression is short, or if it oc-curs in the clinical history and is not precededby phrases such as ?evaluate for?
or ?historyof?, apply pneumonia code and stop.?
Use the same rule to code asthma by lookingfor ?asthma?
or ?reactive airway disease?.?
If no diagnosis is found, code all non-negatedsymptoms (cough, fever, wheezing).We selected 80% of the training set to evaluate in theconstruction of our rules.
We then ran the finishedsystem on both this training set and the held out 20%of the data.
The system achieved F1 scores of 87%on the training set and 84% on the held out data forthese five codes.
The comparable scores indicatesthat we did not over-fit the training data.We designed a similar policy for two other relatedcodes, urinary tract infection and vesicoureteral re-flux.
We found these codes to be more complex asthey included a wide range of kidney disorders.
Onthese two codes, our system achieved 78% on thetrain set and 76% on the held out data.
Overall, au-tomatically applying our two policies yielded highconfidence predictions for a significant subset of thecorpus.4 Combined SystemSince our three systems take complimentary ap-proaches to the problem, we combined them to im-prove performance.
First, we took our automaticpolicy and rule based systems and cascaded them; ifthe automatic policy system does not apply a code,the rule based system classifies the report.
We useda cascaded approach since the automatic policy sys-tem was very accurate when it was able to assigna code.
Therefore, the rule based system defers tothe policy system when it is triggered.
Next, we in-cluded the prediction of the cascaded system as afeature for our learning system.
We used two fea-ture rules: ?cascaded-system predicted exactly thislabel?
and ?cascaded-system predicted one of thecodes in this label?.
As we show, this yielded ourmost accurate system.
While we could have used ameta-classifier to combine the three systems, includ-ing the rule based systems as features to the learningsystem allowed it to learn the appropriate weightsfor the rule based predictions.5 Evaluation MetricEvaluation metrics for this task are often based onrecommendation systems, where the system returnsa list of the top k codes for selection by the user.
Asa result, typical metrics are ?recall at k?
and aver-age precision (Larkey and Croft, 1995).
Instead, ourgoal was to predict the exact configuration, returningexactly the number of codes predicted to be on thereport.
The competition used a micro-averaged F1score to evaluate predictions.
A contingency table(confusion matrix) is computed by summing overeach predicted code for each document by predic-tion type (true positive, false positive, false negative)weighing each code assignment equally.
F1 scoreis computed based on the resultant table.
If specificcodes or under-coding is favored, we can modify ourlearning loss function as described in Section 3.1.2.A detailed treatment of this evaluation metric can befound in Computational Medicine Center (2007).134System Precision Recall F1BL 61.86 72.58 66.79RULE 81.9 82.0 82.0CASCADE 86.04 84.56 85.3LEARN 85.5 83.6 84.6CASCADE+LEARN 87.1 85.9 86.5Table 1: Performance of our systems on the providedlabeled training data (F1 score).
The learning sys-tems (CASCADE+LEARN and LEARN ) were eval-uated on ten random split of the data while RULEwas evaluated on all of the training data.
We includea simple rule based system (BL ) as a baseline.6 ResultsWe evaluated our systems on the labeled trainingdata of 978 radiology reports.
For each report, eachsystem predicted an exact configuration of codes(i.e.
one of 94 possible labels).
We score each sys-tem using a micro-averaged F1 score.
Since we onlyhad labels for the training data, we divided the datausing an 80/20 training test split and averaged resultsover 10 runs for our learning systems.
We evaluatedthe following systems:?
RULE : The rule based system based on ICD-9-CM code descriptions (Section 3.2).?
CASCADE : The automatic code policy system(Section 3.3) cascaded with RULE (Section 4).?
LEARN : The learning system with both labelspecific and transfer features (Section 3.1).?
CASCADE+LEARN : Our combined systemthat incorporates CASCADE predictions as afeature to LEARN (Section 4).For a baseline, we built a simple system that ap-plies the official ICD-9-CM code descriptions to findthe correct labels (BL ).
For each code in the train-ing set, the system generates text-segments related toit.
During testing, for each new document, the sys-tem checks if any text-segment (as discovered dur-ing training) appears in the document.
If so, the cor-responding code is predicted.
The results from ourfour systems and baseline are shown in Table 1.System Train TestCASCADE 85.3 84CASCADE+LEARN 86.5 87.60Average - 76.6Best - 89.08Table 2: Performance of two systems on the trainand test data.
Results obtained from the web sub-mission interface were rounded.
Average and Bestare the average and best f-measures of the 44 sub-mitted systems (standard deviation 13.40).Each of our systems easily beats the baseline, andthe average inter-annotator score for this task.
Ad-ditionally, we were able to evaluate two of our sys-tems on the test data using a web interface as pro-vided by the competition.
The test set contains 976documents (about the same as the training set) andis drawn the from same distribution as the trainingdata.
Our test results were comparable to perfor-mance on the training data, showing that we didnot over-fit to the training data (Table 2).
Addi-tionally, our combined system (CASCADE+LEARN) achieved a score of 87.60%, beating our trainingdata performance and exceeding the average inter-annotator score.
Out of 44 submitted systems, theaverage score on test data was 76.7% (standard devi-ation of 13.40) and the maximum score was 89.08%.Our system scored 4th overall and was less than1.5% behind the best system.
Overall, in comparisonwith our baselines and over 40 systems, we performvery well on this task.7 Related WorkThere have been several attempts at ICD-9-CMcode classification and related problems for med-ical records.
The specific problem of ICD-9-CMcode assignment was studied by Lussier et al (2000)through an exploratory study.
Larkey and Croft(1995) designed classifiers for the automatic assign-ment of ICD-9 codes to discharge summaries.
Dis-charge summaries tend to be considerably longerthan our data and contain multiple text fields.
Ad-ditionally, the number of codes per document hasa larger range, varying between 1 and 15 codes.Larkey and Croft use three classifiers: K-nearestneighbors, relevance feedback, and bayesian inde-135pendence.
Similar to our approach, they tag itemsas negated and try to identify diagnosis and symp-tom terms.
Additionally, their final system combinesall three models.
A direct comparison is not possi-ble due to the difference in data and evaluation met-rics; they use average precision and recall at k. Ona comparable metric, ?principal code is top candi-date?, their best system achieves 59.9% accuracy.
deLima et al (1998) rely on the hierarchical nature ofmedical codes to design a hierarchical classificationscheme.
This approach is likely to help on our taskas well but we were unable to test this since the lim-ited number of codes removes any hierarchy.
Otherapproaches have used a variety of NLP techniques(Satomura and Amaral, 1992).Others have used natural language systems for theanalysis of medical records (Zweigenbaum, 1994).Chapman and Haug (1999) studied radiology re-ports looking for cases of pneumonia, a goal sim-ilar to that of our automatic coding policy system.Meystre and Haug (2005) processed medical recordsto harvest potential entries for a medical problemlist, an important part of electronic medical records.Chuang et al (2002) studied Charlson comorbidi-ties derived from processing discharge reports andchest x-ray reports and compared them with admin-istrative data.
Additionally, Friedman et al (1994)applies NLP techniques to radiology reports.8 ConclusionWe have presented a learning system that processesradiology reports and assigns ICD-9-CM codes.Each of our systems achieves results comparablewith an inter-annotator baseline for our training data.A combined system improves over each individ-ual system.
Finally, we show that on test data un-available during system development, our final sys-tem continues to perform well, exceeding the inter-annotator baseline and achieving the 4th best scoreout of 44 systems entered in the CMC challenge.9 AcknowledgementsWe thank Andrew Lippa for his extensive medicalwisdom.
Dredze is supported by an NDSEG fel-lowship; Ganchev and Talukdar by NSF ITR EIA-0205448; and Crammer by DARPA under ContractNo.
NBCHD03001.
Any opinions, findings, andconclusions or recommendations expressed in thismaterial are those of the author(s) and do not nec-essarily reflect the views of the DARPA or the De-partment of Interior-National Business Center (DOI-NBC).ReferencesW.W.
Chapman and P.J.
Haug.
1999.
Comparing expert sys-tems for identifying chest x-ray reports that support pneu-monia.
In AMIA Symposium, pages 216?20.JH Chuang, C Friedman, and G Hripcsak.
2002.
A com-parison of the charlson comorbidities derived from medicallanguage processing and administrative data.
AMIA Sympo-sium, pages 160?4.Computational Medicine Center.
2007.
Thecomputational medicine center?s 2007 med-ical natural language processing challenge.http://computationalmedicine.org/challenge/index.php.Koby Crammer.
2004.
Online Learning of Complex CategorialProblems.
Ph.D. thesis, Hebrew Univeristy of Jerusalem.Luciano R. S. de Lima, Alberto H. F. Laender, and Berthier A.Ribeiro-Neto.
1998.
A hierarchical approach to the auto-matic categorization of medical documents.
In CIKM.C Friedman, PO Alderson, JH Austin, JJ Cimino, and SB John-son.
1994.
A general natural-language text processor forclinical radiology.
Journal of the American Medical Infor-matics Association, 1:161?74.Leah S. Larkey and W. Bruce Croft.
1995.
Automatic assign-ment of icd9 codes to discharge summaries.
Technical re-port, University of Massachusetts at Amherst, Amherst, MA.YA Lussier, C Friedman, L Shagina, and P Eng.
2000.
Au-tomating icd-9-cm encoding using medical language pro-cessing: A feasibility study.Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005.Flexible text segmentation with structured multilabel classi-fication.
In HLT/EMNLP.Stephane Meystre and Peter J Haug.
2005.
Automation of aproblem list using natural language processing.
BMC Medi-cal Informatics and Decision Making.National Center for Health Statistics.
2006.
Icd-9-cm official guidelines for coding and reporting.http://www.cdc.gov/nchs/datawh/ftpserv/ftpicd9/ftpicd9.htm.Y Satomura and MB Amaral.
1992.
Automated diagnostic in-dexing by natural language processing.
Medical Informat-ics, 17:149?163.Edward H. Shortliffe and James J. Cimino, editors.
2006.Biomedical Informatics: Computer Applications in HealthCare and Biomedicine.
Springer.P.
Zweigenbaum.
1994.
Menelas: an access system for medicalrecords using natural language.
Comput Methods ProgramsBiomed, 45:117?20.136
