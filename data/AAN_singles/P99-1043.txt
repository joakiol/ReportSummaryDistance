Mixed Language Query DisambiguationPasca le  FUNG,  L IU  X iaohu  and CHEUNG Ch i  ShunHKUSTHuman Language Technology CenterDepartment  of Electrical and Electronic EngineeringUniversity of Science and Technology, HKUSTClear Water Bay, Hong Kong{pascale, Ixiaohu, eepercy}@ee, ust.
hkAbst rac tWe propose a mixed language query disam-biguation approach by using co-occurrence in-formation from monolingual data only.
Amixed language query consists of words in aprimary language and a secondary language.Our method translates the query into mono-lingual queries in either language.
Two novelfeatures for disambiguation, amely contextualword voting and 1-best contextual word, are in-troduced and compared to a baseline feature,the nearest neighbor.
Average query transla-tion accuracy for the two features are 81.37%and 83.72%, compared to the baseline accuracyof 75.50%.1 In t roduct ionOnline information retrieval is now prevalentbecause of the ubiquitous World Wide Web.The Web is also a powerful platform for anotherapplication--interactive spoken language querysystems.
Traditionally, such systems were im-plemented on stand-alone kiosks.
Now we caneasily use the Web as a platform.
Informationsuch as airline schedules, movie reservation, cartrading, etc., can all be included in HTML files,to be accessed by a generic spoken interface tothe Web browser (Zue, 1995; DiDio, 1997; Ray-mond, 1997; Fung et al, 1998a).
Our teamhas built a multilingual spoken language inter-face to the Web, named SALSA (Fung et al,1998b; Fung et al, 1998a; Ma and Fung, 1998).Users can use speech to surf the net via vari-ous links as well as issue search commands suchas "Show me the latest movie of Jacky Chan'.The system recognizes commands and queriesin English, Mandarin and Cantonese, as well asmixed language sentences.Until recently, most of the search engines han-dle keyword based queries where the user typesin a series of strings without syntactic structure.The choice of key words in this case determinesthe success rate of the search.
In many situa-tions, the key words are ambiguous.To resolve ambiguity, query expansion is usu-ally employed to look for additional keywords.We believe that a more useful search engineshould allow the user to input natural lan-guage sentences.
Sentence-based queries areuseful because (1) they are more natural to theuser and (2) more importantly, they providemore contextual information which are impor-tant for query understanding.
To date, the fewsentence-based search engines do not seem totake advantage of context information in thequery, but merely extracting key words from thequery sentence (AskJeeves, 1998; ElectricMonk,1998).In addition to the need for better query un-derstanding methods for a large variety of do-mains, it has also become important o han-dle queries in different languages.
Cross-language in fo rmat ion  retr ieval  has emergedas an important area as the amount of non-English material is ever increasing (Oard, 1997;Grefenstette, 1998; Ballesteros and Croft, 1998;Picchi and Peters, 1998; Davis, 1998; Hull andGrefenstette, 1996).
One of the important tasksof cross-language IR is to translate queries fromone language to another.
The original queryand the translated query are then used to matchdocuments in both the source and target lan-guages.
Target language documents are eitherglossed or translated by other systems.
Accord-ing to (Grefenstette, 1998), three main prob-lems of query translations are:1. generating translation candidates,2.
weighting translation candidates, and3333.
pruning translation alternatives for docu-ment matching.In cross-language IR, key word disambigua-tion is even more critical than in monolin-gual IR (Ballesteros and Croft, 1998) since thewrong translation can lead to a large amountof garbage documents in the target language, inaddition to the garbage documents in the sourcelanguage.
Once again, we believe that sentence-based queries provide more information thanmere key words in cross-language IR.In both monolingual IR and cross-languageIR, the query sentence or key words are as-sumed to be consistently in one language only.This makes ense in cases where the user is morelikely to be a monolingual person who is lookingfor information in any language.
It is also eas-ier to implement a monolingual search engine.However, we suggest hat the typical user of across-language IR system is likely to be bilin-gual to some extent.
Most Web users in theworld know some English.
In fact, since En-glish still constitutes 88% of the current webpages, speakers of another language would liketo find English contents as well as contents intheir own language.
Likewise, English speakersmight want to find information in another lan-guage.
A typical example is a Chinese user look-ing for the information of an American movie,s/he might not know the Chinese name of thatmovie.
His/her query for this movie is likely tobe in mixed language.Mixed language query is also prevalent inspoken language.
We have observed this tobe a common phenomenon among users of ourSALSA system.
The colloquial Hong Kong lan-guage is Cantonese with mixed English words.In general, a mixed language consists of a sen-tence mostly in the primary language with somewords in a secondary language.
We are inter-ested in translating such mixed language queriesinto monolingual queries unambiguously.In this paper, we propose a mixed languagequery disambiguation approach which makesuse of the co-occurrence information of wordsbetween those in the primary language andthose in the secondary language.
We describethe overall methodology in Section 2.
In Sec-tions 2.1-3, we present he solutions to the threedisambiguation problems.
In Section 2.3 wepresent three different discriminative featuresfor disambiguation, ranging from the baselinemodel (Section 2.3.1), to the voting scheme(Section 2.3.2), and finally the 1-best model(Section 2.3.3).
We describe our evaluation ex-periments in Section 3, and present he resultsin Section 4.
We then conclude in Section 5.2 Methodo logyMixed language query translation is halfway be-tween query translation and query disambigua-tion in that not all words in the query need tobe translated.There are two ways to use the disambiguatedmixed language queries.
In one scenario, allsecondary language words are translated unam-biguously into the primary language, and theresulting monolingual query is processed by ageneral IR system.
In another scenario, theprimary language words are converted into sec-ondary language and the query is passed toanother IR system in the secondary language.Our methods allows for both general and cross-language IR from a mixed language query.To draw a parallel to the three problems ofquery translation, we suggest that the threemain problems of mixed language disambigua-tion are:1. generating translation candidates in theprimary language,2.
weighting translation candidates, and3.
pruning translation alternatives for querytranslation.Co-occurrence information between eighbor-ing words and words in the same sentencehas been used in phrase extraction (Smadja,1993; Fung and Wu, 1994), phrasal translation(Smadja et al, 1996; Kupiec, 1993; Wu, 1995;Dagan and Church, 1994), target word selection(Liu and Li, 1997; Tanaka and Iwasaki, 1996),domain word translation (Fung and Lo, 1998;Fung, 1998), sense disambiguation (Brown etal., 1991; Dagan et al, 1991; Dagan and Itai,1994; Gale et al, 1992a; Gale et al, 1992b; Galeet al, 1992c; Shiitze, 1992; Gale et al, 1993;Yarowsky, 1995), and even recently for querytranslation in cross-language IR as well (Balles-teros and Croft, 1998).
Co-occurrence statisticsis collected from either bilingual parallel and334non-parallel corpora (Smadja et al, 1996; Ku-piec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996;Fung and Lo, 1998), or monolingual corpora(Smadja, 1993; Fung and Wu, 1994; Liu andLi, 1997; Shiitze, 1992; Yarowsky, 1995).
Aswe noted in (Fung and Lo, 1998; Fung, 1998),parallel corpora are rare in most domains.
Wewant to devise a method that uses only mono-lingual data in the primary language to trainco-occurrence information.2.1 Translation candidate generationWithout loss of generality, we suppose themixed language sentence consists of the wordsS = (E1 ,E2 , .
.
.
,C , .
.
.
,En} ,  where C is theonly secondary language word 1.
Since in ourmethod we want to find the co-occurrence in-formation between all Ei and C from a mono-lingual corpus, we need to translate the lat-ter into the primary language word Ec.
Thiscorresponds to the first problem in querytranslation--translation candidate generation.We generate translation candidates of C via anonline bilingual dictionary.
All translations ofsecondary language word C ,  comprising of mul-tiple senses, are taken together as a set {Eci }.2.2 Translation candidate weight ingProblem two in query translation is to weightall translation candidates for C. In our method,the weights are based on co-occurrence informa-tion.
The hypothesis i that the correct ransla-tions of C should co-occur frequently with thecontextual words Ei and incorrect ranslationof C should co-occur rarely with the contex-tual words.
Obviously, other information suchas syntactical relationship between words or thepart-of-speech tags could be used as weights too.However, it is difficult to parse and tag a mixedlanguage sentence.
The only information wecan use to disambiguate C is the co-occurrenceinformation between its translation candidates{ Ec, } and El, E2, .
.
.
, En.Mutual information is a good measure of theco-occurrence relationship between two words(Gale and Church, 1993).
We first compute themutual information between any word pair froma monolingual corpus in the primary language 21In actual experiments, each sentence can containmultiple secondary language words2This corpus does not need to be in the same domainas the testing datausing the following formula, where E is a wordand f (E) is the frequency of word E.MI(Ei ,  Ej) = log f(Ei ,  Ej)f(Ei)  * f (S j )  (1)Ei and Ej can be either neighboring words orany two words in the sentence.2.3 Translation candidate prun ingThe last problem in query translation is select-ing the target translation.
In our approach, weneed to choose a particular Ec from Ec~.
Wecall this pruning process t rans la t ion  d isam-b iguat ion.We present and compare three unsupervisedstatistical methods in this paper.
The first base-line method is similar to (Dagan et al, 1991;Dagan and Itai, 1994; Ballesteros and Croft,1998; Smadja et al, 1996), where we use thenearest neighboring word of the secondary lan-guage word C as feature for disambiguation.In the second method, we chQose all contex-tual words as disambiguating feature.
In thethird method, the most discriminative contex-tual word is selected as feature.2.3.1 Baseline: single neighboring wordas d isambiguat ing featureThe first disambiguating feature we present hereis similar to the statistical feature in (Dagan etal., 1991; Smadja et al, 1996; Dagan and Itai,1994; Ballesteros and Croft, 1998), namely theco-occurrence with neighboring words.
We donot use any syntactic relationship as in (Daganand Itai, 1994) because such relationship is notavailable for mixed-language s ntences.
The as-sumption here is that the most powerful wordfor disambiguating a word is the one next to it.Based on mutual information, the primary lan-guage target word for C is chosen from the set{Ec~}.
Suppose the nearest neighboring wordfor C in S is Ey,  we select he target word Ecr,such that the mutual information between Ec~and Ev is maximum.r = argmaxiMI(Ec,, Ey) (2)Ev is taken to be either the left or the rightneighbor of our target word.This idea is illustrated in Figure 1.
MI1, rep-resented by the solid line, is greater than MI2,335660Word in the pr~ I~guaguQ ord in th?
secondary languageSelected translation wordMII > MI2Figure 1: The neighboring word as disambiguat-ing featurerepresented by the dotted line.
Ey is the neigh-boring word for C. Since MI1 is greater thanMI2, Ecl is selected as the translation of C.2.3.2 Vot ing:  mul t ip le  contextualwords as d isambiguat ing featureThe baseline method uses only the neighboringword to disambiguate C. Is one or two neigh-boring word really sufficient for disambigua-tion?The intuition for choosing the nearest neigh-boring word Ey as the disambiguating featurefor C is based on the assumption that they arepart of a phrase or collocation term, and thatthere is only one sense per collocation (Daganand Itai, 1994; Yarowsky, 1993).
However, inmost cases where C is a single word, there mightbe some other words which are more useful fordisambiguating C. In fact, such long-distancedependency occurs frequently in natural lan-guage (Rosenfeld, 1995; Huang et al, 1993).Another eason against using single neighbor-ing word comes from (Gale and Church, 1994)where it is argued that as many as 100,000 con-text words might be needed to have high disam-biguation accuracy.
(Shfitze, 1992; Yarowsky,1995) all use multiple context words as discrim-inating features.
We have also demonstrated inour domain translation task that multiple con-text words are useful (Fung and Lo, 1998; Fungand McKeown, 1997).Based on the above arguments, we enlargethe disambiguation window to be the entire sen-tence instead of only one word to the left orright.
We use all the contextual words in thequery sentence.
Each contextual word "votes"by its mutual information with all translationcandidates.Suppose there are n primary language wordsin S = E1 ,E2 , .
.
.
,C , .
.
.
,En ,  as shown in Fig-ure 2, we compute mutual information scoresbetween all Ec~ and all Ej where Eci is oneof the translation candidates for C and Ej isone of all n words in S. A mutual informationscore matrix is shown in Table 1. whereMIjc~is the mutual information score between contex-tual word Ej and translation candidate Eel.E1E2?o .EjEnEel Ec2MI lc l  MIlc2MI2cl MI2c2Ml jc l  Mljc2MIncl MInc2?oo  Ec~... MIlcm... MI2cm.
.
.
MXjc... MlncmTable 1: Mutual information between all trans-lation candidates and words in the sentenceFor each row j in Table 1, the largest scoringMIjci receives a vote.
The rest of the row getzero's.
At the end, we sum up all the one'sin each column.
The column i receiving thehighest vote is chosen as the one representingthe real translation.m m L~ c 00 Selected tramlntionFigure 2: Voting for the best translationTo illustrate this idea, Table 2 shows thatcandidate 2 is the correct translation for C.There are four candidates of C and four con-textual words to disambiguate C.E1 0 1 0 0E2 1 0 0 0E3 0 0 0 1E4 0 1 0 0Table 2: Candidate 2 is the correct ranslation2.3.3 1-best contextua l  word asd isambiguat ing featureIn the above voting scheme, a candidate receiveseither a one vote or a zero vote from all contex-336tual words equally no matter how these wordsaxe related to C. As an example, in the query"Please show me the latest dianying/movie ofJacky Chan", the and Jacky are considered tobe equally important.
We believe however, thatif the most powerful word is chosen for disam-biguation, we can expect better performance.This is related to the concept of "trigger pairs"in (Rosenfeld, 1995) and Singular Value Decom-position in (Shfitze, 1992).In (Dagan and Itai, 1994), syntactic relation-ship is used to find the most powerful "triggerword".
Since syntactic relationship is unavail-able in a mixed language sentence, we have touse other type of information.
In this method,we want to choose the best trigger word amongall contextual words.
Referring again to Table1, Mljci is the mutual information score be-tween contextual word Ej and translation can-didate Ec~.We compute the disambiguation contributionratio for each context word Ej.
For each rowj in Table 1, the largest MI score Mljc~ andthe second largest MI score Mljc~ are chosen toyield the contribution for word Ej, which is theratio between the two scoresMljc/Contribution(Ej, Eci) = Mljc~ (3)If the ratio between MIjc/and MIjc~ is closeto one, we reason that Ej is not discriminativeenough as a feature for disambiguating C. Onthe other hand, if the ratio between MIie/i andMIie.~ is noticeably greater than one, we can useEj as the feature to disambiguate {Ec~} withhigh confidence.
We choose the word Ey withmaximum contribution as the disambiguatingfeature, and select the target word Ecr , whosemutual information score with Ey is the highest,as the translation for C.r = arg max MI(Ey, Ec,) (4)This method is illustrated in Figure 3.
SinceE2 is the contextual word with highest contri-bution score, the candidate Ei is chosen thatthe mutual information between E2 and Eci isthe largest.3 Eva luat ion  exper imentsThe mutual information between co-occurringwords and its contribution weight is ob-i? "
' - .
~iI!j/J /Q Word ia the primary languageWord in die seconda~ languageS?lectcd mutslalion of CFigure 3: The best contextual word as disam-biguating featuretained from a monolingual training corpus--Wall Street Journal from 1987-1992.
The train-ing corpus size is about 590MB.
We evaluateour methods for mixed language query disam-biguation on an automatically generated mixed-language test set.
No bilingual corpus, parallelor comparable, is needed for training.To evaluate our method, a mixed-languagesentence set is generated from the monolingualATIS corpus.
The primary language is Englishand the secondary language is chosen to be Chi-nese.
Some English words in the original sen-tences are selected randomly and translated intoChinese words manually to produce the test-ing data.
These axe the mixed language sen-tences.
500 testing sentences are extracted fromthe ARPA ATIS corpus.
The ratio of Chinesewords in the sentences varies from 10% to 65%.We carry out three sets of experiments usingthe three different features we have presented inthis paper.
In each experiment, he percentageof primary language words in the sentence isincrementally increased at 5% steps, from 35%to 90%.
We note the accuracy of unambiguoustranslation at each step.
Note that at the 35%stage, the primary language is in fact Chinese.4 Eva luat ion  resu l t sOne advantage of using the artificially gener-ated mixed-language t st set is that it becomesvery easy to evaluate the performance of thedisambiguation/translation algorithm.
We justneed to compare the translation output with theoriginal ATIS sentences.The experimental results are shown in Fig-ure 4.
The horizontal axis represents the per-centage of English words in the testing data andthe vertical axis represents the translation ac-curacy.
Translation accuracy is the ratio of thenumber of secondary language (Chinese) wordsdisambiguated correctly over the number of all337secondary language (Chinese) words present inthe testing sentences.
The three different curvesrepresent the accuracies obtained from the base-line feature, the voting model, and the 1-bestmodel.O.85 1i0,8VoOng ~- .ba~ine  .e.-m B"".
.u .
.
.
.i i i i i i~ i a ~  o f  p r imary  l .a~u i i ta  WordsFigure 4: 1-best is the most discriminating fea-tureWe can see that both voting contextual wordsand the 1-best contextual words are more pow-erful discriminant than the baseline neighboringword.
The 1-best feature is most effective fordisambiguating secondary language words in amixed-language s ntence.5 Conc lus ion  and  D iscuss ionMixed-language query occurs very often in bothspoken and written form, especially in Asia.Such queries are usually in complete sentencesinstead of concatenated word strings becausethey are closer to the spoken language and morenatural for user.
A mixed-language s ntenceconsists of words mostly in a primary languageand some in a secondary language.
However,even though mixed-languages are in sentenceform, they are difficult to parse and tag be-cause those secondary language words introducean ambiguity factor.
To understand a query canmean finding the matched ocument, in the caseof Web search, or finding the corresponding se-mantic classes, in the case of an interactive sys-tem.
In order to understand a mixed-languagequery, we need to translate the secondary lan-guage words into primary language unambigu-ously.In this paper, we present an approach ofmixed,language query disambiguation by us-ing co-occurrence information obtained from amonolingual corpus.
Two new types of dis-ambiguation features are introduced, namelyvoting contextual words and 1-best contextualword.
These two features are compared to thebaseline feature of a single neighboring word.Assuming the primary language is English andthe secondary language Chinese, our experi-ments on English-Chinese mixed language showthat the average translation accuracy for thebaseline is 75.50%, for the voting model is81.37% and for the 1-best model, 83.72%.The baseline method uses only the neighbor-ing word to disambiguate C. The assumption isthat the neighboring word is the most semanticrelevant.
This method leaves out an importantfeature of nature language: long distance de-pendency.
Experimental results show that it isnot sufficient o use only the nearest neighbor-ing word for disambiguation.The performance of the voting method is bet-ter than the baseline because more contextualwords are used.
The results are consistent withthe idea in (Gale and Church, 1994; Shfitze,1992; Yarowsky, 1995).In our experiments, it is found that 1-bestcontextual word is even better than multiplecontextual words.
This seemingly counter-intuitive result leads us to believe that choos-ing the most discriminative single word is evenmore powerful than using multiple contextualword equally.
We believe that this is consistentwith the idea of using "trigger pairs" in (Rosen-feld, 1995) and Singular Value Decompositionin (Shiitze, 1992).We can conclude that sometimes long-distance contextual words are more discrimi-nant than immediate neighboring words, andthat multiple contextual words can contributeto better disambiguation.Our results supportour belief that natural sentence-based queriesare less ambiguous than keyword based queries.Our method using multiple disambiguating con-textual words can take advantage of syntacticinformation even when parsing or tagging is notpossible, such as in the case of mixed-languagequeries.Other advantages of our approach include:(1) the training is unsupervised and no domain-dependent data is necessary, (2) neither bilin-gual corpora or mixed-language corpora isneeded for training, and (3) it can generate338monolingual queries in both primary and sec-ondary languages, enabling true cross-languageIR.In our future work, we plan to analyze thevarious "discriminating words" contained in amixed language or monolingual query to findout which class of words contribute more tothe final disambiguation.
We also want to testthe significance of the co-occurrence informa-tion of all contextual words between themselvesin the disambiguation task.
Finally, we planto develop a general mixed-language and cross-language understanding framework for bothdocument retrieval and interactive tasks.Re ferencesAskJeeves.
1998. http://www.askjeeves.com.Lisa Ballesteros and W. Bruce Croft.
1998.Resolving ambiguity for cross-language r -trieval.
In Proceedings of the 21st Annual In-ternational ACM SIGIR Conference on Re-search and Development in Information Re-trieval, pages 64-:71, Melbourne, Australia,August.P.
Brown, J. Lai, and R. Mercer.
1991.
Aligningsentences in parallel corpora.
In Proceedingsof the 29th Annual Conference of the Associ-ation for Computational Linguistics.Ido Dagan and Kenneth W. Church.
1994.
Ter-might: Identifying and translating technicalterminology.
In Proceedings of the 4th Con-ference on Applied Natural Language Process-ing, pages 34-40, Stuttgart, Germany, Octo-ber.Ido Dagan and Alon Itai.
1994.
Word sense dis-ambiguation using a second language mono-lingual corpus.
In Computational Linguistics,pages 564-596.Ido Dagan, Alon Itai, and Ulrike Schwall.
1991.Two languages are more informative thanone.
In Proceedings of the 29th Annual Con-ference of the Association for ComputationalLinguistics, pages 130-137, Berkeley, Califor-nia.M.
Davis.
1998.
Free resources and advancedalignment for cross-language t xt retrieval.In Proceedings of the 6th Text Retrieval Con-ference (TREC-6), NIST, Gaithersburg, MD,November.Laura DiDio.
1997.
Os/2 let users talk back to'net.
page 12.ElectricMonk.
1998.http://www.electricmonk.com.Pascale Fung and Yuen Yee Lo.
1998.
An IRapproach for translating new words from non-parallel, comparable texts.
In Proceedings ofthe 36th Annual Conference of the Associ-ation for Computational Linguistics, pages414-420, Montreal,Canada, August.Pascale Fung and Kathleen McKeown.
1997.Finding terminology translations from non-parallel corpora.
In The 5th Annual Work-shop on Very Large Corpora, pages 192-202,Hong Kong, Aug.Pascale Fung and Dekai Wu.
1994.
Statisticalaugmentation of a Chinese machine-readabledictionary.
In Proceedings of the Second An-nual Workshop on Very Large Corpora, pages69-85, Kyoto, Japan, June.Pascale Fung, CHEUNG Chi Shuen,LAM Kwok Leung, LIU Wai Kat, andLO Yuen Yee.
1998a.
A speech assistedonline search agent (salsa).
In ICSLP.Pascale Fung, CHEUNG Chi Shuen,LAM Kwok Leung, LIU Wai Kat, LO YuenYee, and MA Chi Yuen.
1998b.
SALSA, amultilingual speech-based web browser.
InThe First AEARU Web Technolgy Workshop,Nov.Pascale Fung.
1998.
A statistical view of bilin-gual lexicon extraction: from parallel corporato non-parallel corpora.
In Proceedings of theThird Conference of the Association for Ma-chine Translation in the Americas, Pennsyl-vania, October.William A. Gale and Kenneth W. Church.1993.
A program for aligning sentences inbilingual corpora.
Computational Linguis-tics, 19(1):75-102.William A. Gale and Kenneth W. Church.
1994.Discrimination decisions in 100,000 dimen-sional spaces.
Current Issues in Computa-tional Linguistics: In honour of Don Walker,pages 429-550.W.
Gale, K. Church, and D. Yarowsky.
1992a.Estimating upper and lower bounds on theperformance of word-sense disambiguationprograms.
In Proceedings of the 30th Con-ference of the Association for ComputationalLinguistics.
Association for ComputationalLinguistics.W.
Gale, K. Church, and D. Yarowsky.
1992b.339Using bilingual materials to develop wordsense disambiguation methods.
In Proceed-ings of TMI 92.W.
Gale, K. Church, and D. Yarowsky.
1992c.Work on statistical methods for word sensedisambiguation.
I  Proceedings of AAAI 92.W.
Gale, K. Church, and D. Yarowsky.
1993.
Amethod for disambiguating word senses in alarge corpus.
In Computers and Humanities,volume 26, pages 415-439.Gregory Grefenstette, editor.
1998.
Cross-language Information Retrieval.
Kluwer Aca-demic Publishers.Xuedong Huang, Fileno Alleva, Hisao-WuenHong, Mei-Yuh Hwang, Kai-Fu Lee, andRonald Rosenfeld.
1993.
The SPHINX-II speech recognition system: an overview.Computer, Speech and Language, pages 137-148.David A.
Hull and Gregory Grefenstette.
1996.A dictionary-based approach to multilingualinformaion retrieval.
In Proceedings of the19th International Conference on Researchand Development in Information Retrieval,pages 49-57.Julian Kupiec.
1993.
An algorithm for findingnoun phrase correspondences in bilingual cor-pora.
In Proceedings of the 31st Annual Con-ference of the Association for ComputationalLinguistics, pages 17-22, Columbus, Ohio,June.Xiaohu Liu and Sheng Li.
1997.
Statistic-basedtarget word selection in English-Chinese ma-chine translation.
Journal of Harbin Instituteof Technology, May.Chi Yuen Ma and Pascale Fung.
1998.
UsingEnglish phoneme models for Chinese speechrecognition.
In International Symposium onChinese Spoken language processing.D.W.
Oard.
1997.
Alternative approaches forcross-language text retrieval.
In AAAI Sym-posium on cross-language t xt and speech re-trieval.
American Association for ArtificialIntelligence, Mar.Eugenio Picchi and Carol Peters.
1998.
Cross-language information retrieval: a systemfor comparable corpus querying.
In GregoryGrefenstette, ditor, Cross-language Infor-mation Retrieval, pages 81-92.
Kluwer Aca-demic Publishers.Lau Raymond.
1997.
Webgalaxy : Beyondpoint and click - a conversational interface toa browser.
In Computer Netowrks ~ ISDNSystems, pages 1385-1393.Rony Rosenfeld.
1995.
A Corpus-Based Ap-proach to Language Learning.
Ph.D. thesis,Carnegie Mellon University.Hinrich Shfitze.
1992.
Dimensions of meaning.In Proceedings of Supercomputing '92.Frank Smadja, Kathleen McKeown, andVasileios Hatzsivassiloglou.
1996.
Translat-ing collocations for bilingual lexicons: A sta-tistical approach.
Computational Linguistics,21(4):1-38.Frank Smadja.
1993.
Retrieving collocationsfrom text: Xtract.
Computational Linguis-tics, 19(1):143-177.Kumiko Tanaka and Hideya Iwasaki.
1996.Extraction of lexical translations from non-aligned corpora.
In Proceedings of COLING96, Copenhagan, Danmark, July.Dekai Wu.
1995.
Grammarless extraction ofphrasal translation examples from paralleltexts.
In Proceedings of TMI 95, Leuven, Bel-gium, July.
Submitted.D.
Yarowsky.
1993.
One sense per collocation.In Proceedings of ARPA Human LanguageTechnology Workshop, Princeton.D.
Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.In Proceedings of the 33rd Conference ofthe Association for Computational Linguis-tics, pages 189-196.
Association for Compu-tational Linguistics.Victor Zue.
1995.
Spoken language interfaces tocomputers: Achievements and challenges.
InThe 33rd Annual Meeting of the Associationof Computational Linguistics, Boston, June.340
