Dual Decompositionfor Natural Language ProcessingAlexander M. Rush and Michael CollinsDecoding complexityfocus: decoding problem for natural language tasksy?= arg maxyf (y)motivation:?
richer model structure often leads to improved accuracy?
exact decoding for complex models tends to be intractableDecoding tasksmany common problems are intractable to decode exactlyhigh complexity?
combined parsing and part-of-speech tagging (Rush et al,2010)?
?loopy?
HMM part-of-speech tagging?
syntactic machine translation (Rush and Collins, 2011)NP-Hard?
symmetric HMM alignment (DeNero and Macherey, 2011)?
phrase-based translation?
higher-order non-projective dependency parsing (Koo et al,2010)in practice:?
approximate decoding methods (coarse-to-fine, beam search,cube pruning, gibbs sampling, belief propagation)?
approximate models (mean field, variational models)Motivationcannot hope to find exact algorithms (particularly when NP-Hard)aim: develop decoding algorithms with formal guaranteesmethod:?
derive fast algorithms that provide certificates of optimality?
show that for practical instances, these algorithms often yieldexact solutions?
provide strategies for improving solutions or findingapproximate solutions when no certificate is founddual decomposition helps us develop algorithms of this formDual Decomposition (Komodakis et al, 2010; Lemare?chal, 2001)goal: solve complicated optimization problemy?= arg maxyf (y)method: decompose into subproblems, solve iterativelybenefit: can choose decomposition to provide ?easy?
subproblemsaim for simple and efficient combinatorial algorithms?
dynamic programming?
minimum spanning tree?
shortest path?
min-cut?
bipartite match?
etc.Related workthere are related methods used NLP with similar motivationrelated methods:?
belief propagation (particularly max-product) (Smith andEisner, 2008)?
factored A* search (Klein and Manning, 2003)?
exact coarse-to-fine (Raphael, 2001)aim to find exact solutions without exploring the full search spaceTutorial outlinefocus:?
developing dual decomposition algorithms for new NLP tasks?
understanding formal guarantees of the algorithms?
extensions to improve exactness and select solutionsoutline:1. worked algorithm for combined parsing and tagging2.
important theorems and formal derivation3.
more examples from parsing, sequence labeling, MT4.
practical considerations for implementing dual decomposition5.
relationship to linear programming relaxations6.
further variations and advanced examples1.
Worked exampleaim: walk through a dual decomposition algorithm for combinedparsing and part-of-speech tagging?
introduce formal notation for parsing and tagging?
give assumptions necessary for decoding?
step through a run of the dual decomposition algorithmCombined parsing and part-of-speech taggingSNPNUnitedVPVfliesNPDsomeAlargeNjetgoal: find parse tree that optimizesscore(S ?
NP VP) + score(VP ?
V NP) +...+ score(United1,N) + score(V,N) + ...Constituency parsingnotation:?
Y is set of constituency parses for input?
y ?
Y is a valid parse?
f (y) scores a parse treegoal:arg maxy?Yf (y)example: a context-free grammar for constituency parsingSNPNUnitedVPVfliesNPDsomeAlargeNjetPart-of-speech taggingnotation:?
Z is set of tag sequences for input?
z ?
Z is a valid tag sequence?
g(z) scores of a tag sequencegoal:arg maxz?Zg(z)example: an HMM for part-of speech taggingUnited1 flies2 some3 large4 jet5N V D A NIdentifying tagsnotation: identify the tag labels selected by each model?
y(i , t) = 1 when parse y selects tag t at position i?
z(i , t) = 1 when tag sequence z selects tag t at position iexample: a parse and tagging with y(4,A) = 1 and z(4,A) = 1SNPNUnitedVPVfliesNPDsomeAlargeNjetyUnited1 flies2 some3 large4 jet5N V D A NzCombined optimizationgoal:arg maxy?Y,z?Zf (y) + g(z)such that for all i = 1 .
.
.
n, t ?
T ,y(i , t) = z(i , t)i.e.
find the best parse and tagging pair that agree on tag labelsequivalent formulation:arg maxy?Yf (y) + g(l(y))where l : Y ?
Z extracts the tag sequence from a parse treeDynamic programming intersectioncan solve by solving the product of the two modelsexample:?
parsing model is a context-free grammar?
tagging model is a first-order HMM?
can solve as CFG and finite-state automata intersectionreplace S ?
NP VP withSN,N ?
NPN,V VPV ,NSNPNUnitedVPVfliesNPDsomeAlargeNjetParsing assumptionthe structure of Y is open (could be CFG, TAG, etc.
)assumption: optimization with u can be solved efficientlyarg maxy?Yf (y) +?i ,tu(i , t)y(i , t)generally benign since u can be incorporated into the structure of fexample: CFG with rule scoring function hf (y) =?X?Y Z?yh(X ?
Y Z ) +?
(i ,X )?yh(X ?
wi )wherearg maxy?Y f (y) +?i ,tu(i , t)y(i , t) =arg maxy?Y?X?Y Z?yh(X ?
Y Z ) +?
(i ,X )?y(h(X ?
wi ) + u(i ,X ))Tagging assumptionwe make a similar assumption for the set Zassumption: optimization with u can be solved efficientlyarg maxz?Zg(z)?
?i ,tu(i , t)z(i , t)example: HMM with scores for transitions T and observations Og(z) =?t?t?
?zT (t ?
t ?)
+?
(i ,t)?zO(t ?
wi )wherearg maxz?Z g(z)?
?i ,tu(i , t)z(i , t) =arg maxz?Z?t?t?
?zT (t ?
t ?)
+?
(i ,t)?z(O(t ?
wi )?
u(i , t))Dual decomposition algorithmSet u(1)(i , t) = 0 for all i , t ?
TFor k = 1 to Ky(k) ?
arg maxy?Yf (y) +?i ,tu(k)(i , t)y(i , t) [Parsing]z(k) ?
arg maxz?Zg(z)?
?i ,tu(k)(i , t)z(i , t) [Tagging]If y (k)(i , t) = z(k)(i , t) for all i , t Return (y (k), z(k))Else u(k+1)(i , t)?
u(k)(i , t)?
?k(y(k)(i , t)?
z(k)(i , t))Algorithm step-by-step[Animation]Main theoremtheorem: if at any iteration, for all i , t ?
Ty(k)(i , t) = z(k)(i , t)then (y(k), z(k)) is the global optimumproof: focus of the next section2.
Formal propertiesaim: formal derivation of the algorithm given in the previoussection?
derive Lagrangian dual?
prove three propertiesI upper boundI convergenceI optimality?
describe subgradient methodLagrangiangoal:arg maxy?Y,z?Zf (y) + g(z) such that y(i , t) = z(i , t)Lagrangian:L(u, y , z) = f (y) + g(z) +?i ,tu(i , t) (y(i , t)?
z(i , t))redistribute termsL(u, y , z) =?
?f (y) +?i ,tu(i , t)y(i , t)??+??g(z)?
?i ,tu(i , t)z(i , t)?
?Lagrangian dualLagrangian:L(u, y , z) =?
?f (y) +?i ,tu(i , t)y(i , t)??+??g(z)?
?i ,tu(i , t)z(i , t)?
?Lagrangian dual:L(u) = maxy?Y,z?ZL(u, y , z)= maxy?Y?
?f (y) +?i ,tu(i , t)y(i , t)??+maxz?Z??g(z)?
?i ,tu(i , t)z(i , t)?
?Theorem 1.
Upper bounddefine:?
y?, z?
is the optimal combined parsing and tagging solutionwith y?
(i , t) = z?
(i , t) for all i , ttheorem: for any value of uL(u) ?
f (y?)
+ g(z?
)L(u) provides an upper bound on the score of the optimal solutionnote: upper bound may be useful as input to branch and bound orA* searchTheorem 1.
Upper bound (proof)theorem: for any value of u, L(u) ?
f (y?)
+ g(z?
)proof:L(u) = maxy?Y,z?ZL(u, y , z) (1)?
maxy?Y,z?Z:y=zL(u, y , z) (2)= maxy?Y,z?Z:y=zf (y) + g(z) (3)= f (y?)
+ g(z?)
(4)Formal algorithm (reminder)Set u(1)(i , t) = 0 for all i , t ?
TFor k = 1 to Ky(k) ?
arg maxy?Yf (y) +?i ,tu(k)(i , t)y(i , t) [Parsing]z(k) ?
arg maxz?Zg(z)?
?i ,tu(k)(i , t)z(i , t) [Tagging]If y (k)(i , t) = z(k)(i , t) for all i , t Return (y (k), z(k))Else u(k+1)(i , t)?
u(k)(i , t)?
?k(y(k)(i , t)?
z(k)(i , t))Theorem 2.
Convergencenotation:?
u(k+1)(i , t)?
u(k)(i , t) + ?k(y(k)(i , t)?
z(k)(i , t)) is update?
u(k)is the penalty vector at iteration k?
?k is the update rate at iteration ktheorem: for any sequence ?1, ?2, ?3, .
.
.
such thatlimt??
?t = 0 and?
?t=1?t =?,we havelimt?
?L(ut) = minuL(u)i.e.
the algorithm converges to the tightest possible upper boundproof: by subgradient convergence (next section)Dual solutionsdefine:?
for any value of uyu = arg maxy?Y?
?f (y) +?i ,tu(i , t)y(i , t)?
?andzu = arg maxz?Z??g(z)?
?i ,tu(i , t)z(i , t)???
yu and zu are the dual solutions for a given uTheorem 3.
Optimalitytheorem: if there exists u such thatyu(i , t) = zu(i , t)for all i , t thenf (yu) + g(zu) = f (y?)
+ g(z?)i.e.
if the dual solutions agree, we have an optimal solution(yu, zu)Theorem 3.
Optimality (proof)theorem: if u such that yu(i , t) = zu(i , t) for all i , t thenf (yu) + g(zu) = f (y?)
+ g(z?
)proof: by the definitions of yu and zuL(u) = f (yu) + g(zu) +?i ,tu(i , t)(yu(i , t)?
zu(i , t))= f (yu) + g(zu)since L(u) ?
f (y?)
+ g(z?)
for all values of uf (yu) + g(zu) ?
f (y?)
+ g(z?
)but y?and z?are optimalf (yu) + g(zu) ?
f (y?)
+ g(z?
)Dual optimizationLagrangian dual:L(u) = maxy?Y,z?ZL(u, y , z)= maxy?Y?
?f (y) +?i ,tu(i , t)y(i , t)??+maxz?Z??g(z)?
?i ,tu(i , t)z(i , t)?
?goal: dual problem is to find the tightest upper boundminuL(u)Dual subgradientL(u) = maxy?Y?
?f (y) +?i,tu(i , t)y(i , t)?
?+ maxz?Z??g(z)?
?i,tu(i , t)z(i , t)??properties:?
L(u) is convex in u (no local minima)?
L(u) is not differentiable (because of max operator)handle non-differentiability by using subgradient descentdefine: a subgradient of L(u) at u is a vector gu such that for all vL(v) ?
L(u) + gu ?
(v ?
u)Subgradient algorithmL(u) = maxy?Y?
?f (y) +?i,tu(i , t)y(i , t)?
?+ maxz?Z??g(z)?
?i,ju(i , t)z(i , t)?
?recall, yu and zu are the argmax?s of the two termssubgradient:gu(i , t) = yu(i , t)?
zu(i , t)subgradient descent: move along the subgradientu?
(i , t) = u(i , t)?
?
(yu(i , t)?
zu(i , t))guaranteed to find a minimum with conditions given earlier for ?3.
More examplesaim: demonstrate similar algorithms that can be applied to otherdecoding applications?
context-free parsing combined with dependency parsing?
corpus-level part-of-speech tagging?
combined translation alignmentCombined constituency and dependency parsingsetup: assume separate models trained for constituency anddependency parsingproblem: find constituency parse that maximizes the sum of thetwo modelsexample:?
combine lexicalized CFG with second-order dependency parserLexicalized constituency parsingnotation:?
Y is set of lexicalized constituency parses for input?
y ?
Y is a valid parse?
f (y) scores a parse treegoal:arg maxy?Yf (y)example: a lexicalized context-free grammarS(flies)NP(United)NUnitedVP(flies)VfliesNP(jet)DsomeAlargeNjetDependency parsingdefine:?
Z is set of dependency parses for input?
z ?
Z is a valid dependency parse?
g(z) scores a dependency parseexample:*0 United1 flies2 some3 large4 jet5Identifying dependenciesnotation: identify the dependencies selected by each model?
y(i , j) = 1 when constituency parse y selects word i as amodifier of word j?
z(i , j) = 1 when dependency parse z selects word i as amodifier of word jexample: a constituency and dependency parse with y(3, 5) = 1and z(3, 5) = 1S(flies)NP(United)NUnitedVP(flies)VfliesNP(jet)DsomeAlargeNjety*0 United1 flies2 some3 large4 jet5zCombined optimizationgoal:arg maxy?Y,z?Zf (y) + g(z)such that for all i = 1 .
.
.
n, j = 0 .
.
.
n,y(i , j) = z(i , j)Algorithm step-by-step[Animation]Corpus-level taggingsetup: given a corpus of sentences and a trained sentence-leveltagging modelproblem: find best tagging for each sentence, while at the sametime enforcing inter-sentence soft constraintsexample:?
test-time decoding with a trigram tagger?
constraint that each word type prefer a single POS tagCorpus-level taggingfull model for corpus-level taggingHe saw an American manThe smart man stood outsideMan is the best measureNSentence-level decodingnotation:?
Yi is set of tag sequences for input sentence i?
Y = Y1?
.
.
.
?Ym is set of tag sequences for the input corpus?
Y ?
Y is a valid tag sequence for the corpus?
F (Y ) =?if (Yi ) is the score for tagging the whole corpusgoal:arg maxY?YF (Y )example: decode each sentence with a trigram taggerHePsawVanDAmericanAmanNTheDsmartAmanNstoodVoutsideRInter-sentence constraintsnotation:?
Z is set of possible assignments of tags to word types?
z ?
Z is a valid tag assignment?
g(z) is a scoring function for assignments to word types(e.g.
a hard constraint - all word types only have one tag)example: an MRF model that encourages words of the same typeto choose the same tagz1manNmanNmanNNz2manNmanNmanANg(z1) > g(z2)Identifying word tagsnotation: identify the tag labels selected by each model?
Ys(i , t) = 1 when the tagger for sentence s at position iselects tag t?
z(s, i , t) = 1 when the constraint assigns at sentence sposition i the tag texample: a parse and tagging with Y1(5,N) = 1 andz(1, 5,N) = 1He saw an American manThe smart man stood outsideYman man manzCombined optimizationgoal:arg maxY?Y,z?ZF (Y ) + g(z)such that for all s = 1 .
.
.m, i = 1 .
.
.
n, t ?
T ,Ys(i , t) = z(s, i , t)Algorithm step-by-step[Animation]Combined alignment (DeNero and Macherey, 2011)setup: assume separate models trained for English-to-French andFrench-to-English alignmentproblem: find an alignment that maximizes the score of bothmodels with soft agreementexample:?
HMM models for both directional alignments (assume correctalignment is one-to-one for simplicity)English-to-French alignmentdefine:?
Y is set of all possible English-to-French alignments?
y ?
Y is a valid alignment?
f (y) scores of the alignmentexample: HMM alignmentThe1 ugly2 dog3 has4 red5 fur61 3 2 4 6 5French-to-English alignmentdefine:?
Z is set of all possible French-to-English alignments?
z ?
Z is a valid alignment?
g(z) scores of an alignmentexample: HMM alignmentLe1 chien2 laid3 a4 fourrure5 rouge61 2 3 4 6 5Identifying word alignmentsnotation: identify the tag labels selected by each model?
y(i , j) = 1 when e-to-f alignment y selects French word i toalign with English word j?
z(i , j) = 1 when f-to-e alignment z selects French word i toalign with English word jexample: two HMM alignment models with y(6, 5) = 1 andz(6, 5) = 1The1 ugly2 dog3 has4 red5 fur61 3 2 4 6 5yLe1 chien2 laid3 a4 fourrure5 rouge61 2 3 4 6 5zCombined optimizationgoal:arg maxy?Y,z?Zf (y) + g(z)such that for all i = 1 .
.
.
n, j = 1 .
.
.
n,y(i , j) = z(i , j)Algorithm step-by-step[Animation]4.
Practical issuesaim: overview of practical dual decomposition techniques?
tracking the progress of the algorithm?
extracting solutions if algorithm does not converge?
lazy update of dual solutionsTracking progressat each stage of the algorithm there are several useful valuestrack:?
y(k), z(k)are current dual solutions?
L(u(k)) is the current dual value?
y(k), l(y(k)) is a potential primal feasible solution?
f (y(k)) + g(l(y(k))) is the potential primal valueuseful signals:?
L(u(k))?
L(u(k?1)) is the dual change (may be positive)?
minkL(u(k)) is the best dual value (tightest upper bound)?
maxkf (y(k)) + g(l(y(k))) is the best primal valuethe optimal value must be between the best dual and primal valuesApproximate solutionupon agreement the solution is exact, but this may not occurotherwise, there is an easy way to find an approximate solutionchoose: the structure y (k?
)wherek?= arg maxkf (y(k)) + g(l(y(k)))is the iteration with the best primal scoreguarantee: the solution yk?is non-optimal by at most(mintL(ut))?
(f (y (k?))
+ g(l(y(k ?
))))there are other methods to estimate solutions, for instance byaveraging solutions (see Nedic?
and Ozdaglar (2009))Lazy decodingidea: don?t recompute y (k) or z(k) from scratch each iterationlazy decoding: if subgradient u(k) is sparse, then y (k) may bevery easy to compute from y(k?1)use:?
very helpful if y or z factors naturally into several parts?
decompositions with this property are very fast in practiceexample:?
in corpus-level tagging, only need to recompute sentenceswith a word type that received an update5.
Linear programmingaim: explore the connections between dual decomposition andlinear programming?
basic optimization over the simplex?
formal properties of linear programming?
full example with fractional optimal solutions?
tightening linear program relaxationsSimplexdefine:?
?y is the simplex over Y where ?
?
?y implies?y ?
0 and?y?y = 1?
?z is the simplex over Z?
?y : Y ?
?y maps elements to the simplexexample:Y = {y1, y2, y3}vertices?
?y (y1) = (1, 0, 0)?
?y (y2) = (0, 1, 0)?
?y (y3) = (0, 0, 1)?y (y1)?y (y2) ?y (y3)?yLinear programmingoptimize over the simplices ?y and ?z instead of the discrete setsY and Zgoal: optimize linear programmax??
?y ,??
?z?y?y f (y) +?z?zg(z)such that for all i , t?y?yy(i , t) =?z?zz(i , t)LagrangianLagrangian:M(u, ?, ?)
=?y?y f (y) +?z?zg(z) +?i,tu(i , t)(?y?yy(i , t)?
?z?zz(i , t))=(?y?y f (y) +?i,tu(i , t)?y?yy(i , t))+(?z?zg(z)?
?i,tu(i , t)?z?zz(i , t))Lagrangian dual:M(u) = max??
?y ,??
?zM(u, ?, ?
)Strong dualitydefine:?
?
?, ??
is the optimal assignment to ?, ?
in the linear programtheorem:minuM(u) =?y?
?y f (y) +?z?
?zg(z)proof: by linear programming dualityDual relationshiptheorem: for any value of u,M(u) = L(u)note: solving the original Lagrangian dual also solves dual of thelinear programPrimal relationshipdefine:?
Q ?
?y ?
?z corresponds to feasible solutions of the originalproblemQ = {(?y (y), ?z(z)): y ?
Y, z ?
Z,y(i , t) = z(i , t) for all (i , t)}?
Q?
?
?y ?
?z is the set of feasible solutions to the LPQ?
= {(?, ?
): ?
?
?Y , ?
?
?Z ,?y ?yy(i , t) =?z ?zz(i , t) for all (i , t)}?
Q ?
Q?solutions:maxq?Qh(q) ?
maxq?Q?h(q) for any hConcrete example?
Y = {y1, y2, y3}?
Z = {z1, z2, z3}?
?y ?
R3, ?z ?
R3YxaHeaisy1xbHebisy2xcHecisy3Z aHebisz1bHeaisz2cHecisz3Simple solutionYxaHeaisy1xbHebisy2xcHecisy3Z aHebisz1bHeaisz2cHecisz3choose:?
?
(1) = (0, 0, 1) ?
?y is representation of y3?
?
(1) = (0, 0, 1) ?
?z is representation of z3confirm: ?y?
(1)y y(i , t) =?z?
(1)z z(i , t)?
(1) and ?
(1) satisfy agreement constraintFractional solutionYxaHeaisy1xbHebisy2xcHecisy3Z aHebisz1bHeaisz2cHecisz3choose:?
?
(2) = (0.5, 0.5, 0) ?
?y is combination of y1 and y2?
?
(2) = (0.5, 0.5, 0) ?
?z is combination of z1 and z2confirm: ?y?
(2)y y(i , t) =?z?
(2)z z(i , t)?
(2) and ?
(2) satisfy agreement constraint, but not integralOptimal solutionweights:?
the choice of f and g determines the optimal solution?
if (f , g) favors (?
(2), ?
(2)), the optimal solution is fractionalexample: f = [1 1 2] and g = [1 1 ?
2]?
f ?
?
(1) + g ?
?
(1) = 0 vs f ?
?
(2) + g ?
?
(2) = 2?
?
(2), ?
(2) is optimal, even though it is fractionalAlgorithm run[Animation]Tightening (Sherali and Adams, 1994; Sontag et al, 2008)modify:?
extend Y, Z to identify bigrams of part-of-speech tags?
y(i , t1, t2) = 1 ?
y(i , t1) = 1 and y(i + 1, t2) = 1?
z(i , t1, t2) = 1 ?
z(i , t1) = 1 and z(i + 1, t2) = 1all bigram constraints: valid to add for all i , t1, t2 ?
T?y?yy(i , t1, t2) =?z?zz(i , t1, t2)however this would make decoding expensivesingle bigram constraint: cheaper to implement?y?yy(1, a, b) =?z?zz(1, a, b)the solution ?
(1), ?
(1) trivially passes this constraint, while?
(2), ?
(2) violates itDual decomposition with tighteningtightened decomposition includes an additional Lagrange multiplieryu,v = arg maxy?Yf (y) +?i ,tu(i , t)y(i , t) + v(1, a, b)y(1, a, b)zu,v = arg maxz?Zg(z)?
?i ,tu(i , t)z(i , t)?
v(1, a, b)z(1, a, b)in general, this term can make the decoding problem more difficultexample:?
for small examples, these penalties are easy to compute?
for CFG parsing, need to include extra states that maintaintag bigrams (still faster than full intersection)Tightening step-by-step[Animation]6.
Advanced examplesaim: demonstrate some different relaxation techniques?
higher-order non-projective dependency parsing?
syntactic machine translationHigher-order non-projective dependency parsingsetup: given a model for higher-order non-projective dependencyparsing (sibling features)problem: find non-projective dependency parse that maximizes thescore of this modeldifficulty:?
model is NP-hard to decode?
complexity of the model comes from enforcing combinatorialconstraintsstrategy: design a decomposition that separates combinatorialconstraints from direct implementation of the scoring functionNon-projective dependency parsingstructure:?
starts at the root symbol *?
each word has a exactly one parent word?
produces a tree structure (no cycles)?
dependencies can crossexample:*0 John1 saw2 a3 movie4 today5 that6 he7 liked8*0 John1 saw2 a3 movie4 today5 that6 he7 liked8Arc-Factored*0 John1 saw2 a3 movie4 today5 that6 he7 liked8f (y) = score(head =?0,mod =saw2) +score(saw2, John1)+score(saw2,movie4) +score(saw2, today5)+score(movie4, a3) + ...e.g.
score(?0, saw2) = log p(saw2|?0) (generative model)or score(?0, saw2) = w ?
?
(saw2, ?0) (CRF/perceptron model)y?= arg maxyf (y) ?
Minimum Spanning Tree AlgorithmSibling models*0 John1 saw2 a3 movie4 today5 that6 he7 liked8f (y) = score(head = ?0, prev = NULL,mod = saw2)+score(saw2,NULL, John1)+score(saw2,NULL,movie4)+score(saw2,movie4, today5) + ...e.g.
score(saw2,movie4, today5) = log p(today5|saw2,movie4)or score(saw2,movie4, today5) = w ?
?
(saw2,movie4, today5)y?= arg maxyf (y) ?
NP-HardThought experiment: individual decoding*0 John1 saw2 a3 movie4 today5 that6 he7 liked8score(saw2,NULL, John1) + score(saw2,NULL,movie4)+score(saw2,movie4, today5)score(saw2,NULL, John1) + score(saw2,NULL, that6)score(saw2,NULL, a3) + score(saw2, a3,he7)2n?1possibilitiesunder sibling model, can solve for each word with Viterbi decoding.Thought experiment continued*0 John1 saw2 a3 movie4 today5 that6 he7 liked8idea: do individual decoding for each head word using dynamicprogrammingif we?re lucky, we?ll end up with a valid final treebut we might violate some constraintsDual decomposition structuregoal:y?= arg maxy?Yf (y)rewrite:arg maxy?
Y z?
Z,f (y) + g(z)such that for all i , jy(i , j) = z(i , j)Algorithm step-by-step[Animation]Syntactic translation decodingsetup: assume a trained model for syntactic machine translationproblem: find best derivation that maximizes the score of thismodeldifficulty:?
need to incorporate language model in decoding?
empirically, relaxation is often not tight, so dualdecomposition does not always convergestrategy:?
use a different relaxation to handle language model?
incrementally add constraints to find exact solutionSyntactic translation example[Animation]Summarypresented dual decomposition as a method for decoding in NLPformal guarantees?
gives certificate or approximate solution?
can improve approximate solutions by tightening relaxationefficient algorithms?
uses fast combinatorial algorithms?
can improve speed with lazy decodingwidely applicable?
demonstrated algorithms for a wide range of NLP tasks(parsing, tagging, alignment, mt decoding)References IJ.
DeNero and K. Macherey.
Model-Based Aligner CombinationUsing Dual Decomposition.
In Proc.
ACL, 2011.D.
Klein and C.D.
Manning.
Factored A* Search for Models overSequences and Trees.
In Proc IJCAI, volume 18, pages1246?1251.
Citeseer, 2003.N.
Komodakis, N. Paragios, and G. Tziritas.
Mrf energyminimization and beyond via dual decomposition.
IEEETransactions on Pattern Analysis and Machine Intelligence,2010.
ISSN 0162-8828.Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola,and David Sontag.
Dual decomposition for parsing withnon-projective head automata.
In EMNLP, 2010.
URLhttp://www.aclweb.org/anthology/D10-1125.B.H.
Korte and J. Vygen.
Combinatorial Optimization: Theory andAlgorithms.
Springer Verlag, 2008.References IIC.
Lemare?chal.
Lagrangian Relaxation.
In ComputationalCombinatorial Optimization, Optimal or Provably Near-OptimalSolutions [based on a Spring School], pages 112?156, London,UK, 2001.
Springer-Verlag.
ISBN 3-540-42877-1.Angelia Nedic?
and Asuman Ozdaglar.
Approximate primalsolutions and rate analysis for dual subgradient methods.
SIAMJournal on Optimization, 19(4):1757?1780, 2009.Christopher Raphael.
Coarse-to-fine dynamic programming.
IEEETransactions on Pattern Analysis and Machine Intelligence, 23:1379?1390, 2001.A.M.
Rush and M. Collins.
Exact Decoding of SyntacticTranslation Models through Lagrangian Relaxation.
In Proc.ACL, 2011.A.M.
Rush, D. Sontag, M. Collins, and T. Jaakkola.
On DualDecomposition and Linear Programming Relaxations for NaturalLanguage Processing.
In Proc.
EMNLP, 2010.References IIIHanif D. Sherali and Warren P. Adams.
A hierarchy of relaxationsand convex hull characterizations for mixed-integer zero?oneprogramming problems.
Discrete Applied Mathematics, 52(1):83?
106, 1994.D.A.
Smith and J. Eisner.
Dependency Parsing by BeliefPropagation.
In Proc.
EMNLP, pages 145?156, 2008.
URLhttp://www.aclweb.org/anthology/D08-1016.D.
Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss.Tightening LP relaxations for MAP using message passing.
InProc.
UAI, 2008.
