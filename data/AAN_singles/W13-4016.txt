Proceedings of the SIGDIAL 2013 Conference, pages 112?116,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsReinforcement Learning of Two-Issue Negotiation Dialogue PoliciesKallirroi GeorgilaInstitute for Creative Technologies, University of Southern California12015 Waterfront Drive, Playa Vista, CA 90094, USAkgeorgila@ict.usc.eduAbstractWe use hand-crafted simulated negotiators(SNs) to train and evaluate dialogue poli-cies for two-issue negotiation between twoagents.
These SNs differ in their goals andin the use of strong and weak argumentsto persuade their counterparts.
They mayalso make irrational moves, i.e., moves notconsistent with their goals, to generate avariety of negotiation patterns.
Differentversions of these SNs interact with eachother to generate corpora for Reinforce-ment Learning (RL) of argumentation di-alogue policies for each of the two agents.We evaluate the learned policies againsthand-crafted SNs similar to the ones usedfor training but with the modification thatthese SNs no longer make irrational movesand thus are harder to beat.
The learnedpolicies generally do as well as, or bet-ter than the hand-crafted SNs showing thatRL can be successfully used for learningargumentation dialogue policies in two-issue negotiation scenarios.1 IntroductionThe dialogue policy of a dialogue system decideson what dialogue move (also called action) thesystem should make given the dialogue context(also called dialogue state).
Building hand-craftedpolicies is a hard task, and there is no guaranteethat the resulting policies will be optimal.
This is-sue has motivated the dialogue community to usestatistical methods for automatically learning dia-logue policies, the most popular of which is Rein-forcement Learning (RL) (Szepesva?ri, 2010).To date, RL has been used mainly for learn-ing dialogue policies for slot-filling applicationssuch as restaurant recommendations (Williamsand Young, 2007; Chandramohan et al 2010;Jurc??
?c?ek et al 2012; Gas?ic?
et al 2012), flightreservations (Henderson et al 2008), sightsee-ing recommendations (Misu et al 2010), appoint-ment scheduling (Georgila et al 2010), techni-cal support (Janarthanam and Lemon, 2010), etc.,largely ignoring other types of dialogue.
RL hasalso been applied to question-answering (Misu etal., 2012) and tutoring domains (Tetreault and Lit-man, 2008; Chi et al 2011).
There has also beensome work on applying RL to the more difficultproblem of learning negotiation policies (Heeman,2009; Paruchuri et al 2009; Georgila and Traum,2011a; Georgila and Traum, 2011b; Nouri et al2012), which is the topic of this paper.In negotiation dialogue the system and the userhave opinions about the optimal outcomes and tryto reach a joint decision.
Dialogue policy deci-sions are typically whether to present, accept, orreject a proposal, whether to compromise, etc.
Re-wards may depend on the type of policy that wewant to learn.
For example, a cooperative policyshould be rewarded for accepting proposals.Recently, Georgila and Traum (2011a; 2011b)learned argumentation dialogue policies for nego-tiation against users of different cultural norms ina one-issue negotiation scenario.
We extend thiswork by learning argumentation policies in a two-issue negotiation setting.
We aim to learn system(or agent) policies that will persuade their inter-locutor (a human user or another agent) to agreeon the system?s preferences.Our research contribution is two-fold: First, toour knowledge this is the first study that uses RLfor learning argumentation policies in a two-issuenegotiation scenario and one of the few studies onusing RL for negotiation.
Second, for the firsttime, we learn policies for agents with differentdegrees of persuasion skills, i.e., agents that pro-vide strong or weak arguments.Section 2 introduces RL, and section 3 de-scribes our two-issue negotiation domain and ourlearning methodology.
Section 4 presents ourevaluation results and section 5 concludes.1122 Reinforcement LearningReinforcement Learning (RL) is a machine learn-ing technique used to learn the policy of anagent (Szepesva?ri, 2010).
RL is used inthe framework of Markov Decision Processes(MDPs) (Szepesva?ri, 2010) or Partially Observ-able Markov Decision Processes (Williams andYoung, 2007).
In this paper we use MDPs.An MDP is defined as a tuple (S, A, P , R, ?
)where S is the set of states that the agent may bein, A is the set of actions of the agent, P : S ?
A?
P (S, A) is the set of transition probabilities be-tween states after taking an action, R : S ?
A ?< is the reward function, and ?
?
[0, 1] a discountfactor weighting long-term rewards.
At any giventime step i the agent is in a state si ?
S. When theagent performs an action ?i ?
A following a pol-icy pi : S ?
A, it receives a reward ri(si, ?i) ?
<and transitions to state s?i according to P (s?i|si, ?i)?
P .
The quality of the policy pi followed by theagent is measured by the expected future rewardalso called Q-function, Qpi : S ?
A?<.To estimate the Q-function we use Least-Squares Policy Iteration (LSPI) (Lagoudakis andParr, 2003; Li et al 2009).
LSPI can learn directlyfrom a corpus of dialogues and is sample efficient.We use linear function approximation of the Q-function.
Thus Q(s, ?)
= ?ki=1wi?i(s, ?)
wheres is the state that the agent is in and ?
the actionthat it performs in this state, and w?
is a vector ofweights wi for the feature functions ?i(s, ?).
Themagnitude of a weight wi shows the contributionof the feature ?i(s, ?)
to the Q(s, ?)
value.3 Learning Argumentation PoliciesIn our experiments, two agents negotiate on two is-sues that are independent of each other.
Each issuemay have three possible outcomes.
Our approachcan be applied to any such issues.
For the sakeof readability, from now on we will use a negoti-ation scenario in which Agents 1 and 2 are hav-ing a party and need to agree on the type of foodthat will be served (Thai, Italian, Mexican) and theday of the week that the party will be held (Friday,Saturday, Sunday).
Agents 1 and 2 have differentgoals.
Table 1 shows the points that Agents 1 and 2earn for each negotiation outcome.We build hand-crafted simulated negotiators(SNs) for the two agents that interact with eachother to generate simulated corpora.
The SNs dif-fer not only in their goals but also in whetherthey use strong or weak arguments to persuadeAgent 1 Agent 2Food typeThai 200 0Italian 100 40Mexican 0 80Day of the weekFriday 80 0Saturday 40 100Sunday 0 200Table 1: Rewards for Agents 1 and 2.their counterparts, and sometimes make irrationalmoves, i.e., moves not consistent with their goals.For example, Agent 1 may reject an offer for?Thai?
food, and Agent 2 may offer or accept ?Fri-day?.
This is to generate a variety of negotiationpatterns.
There is also some randomness regard-ing whether the SN will start the conversation bya direct offer or by providing an argument.The SNs for Agents 1 and 2 can chooseamong 13 actions: ?offer-Thai?, ?offer-Italian?, ?offer-Mexican?, ?offer-Friday?,?offer-Saturday?, ?offer-Sunday?, ?provide-argument-Thai?, ?provide-argument-Mexican?,?provide-argument-Friday?, ?provide-argument-Sunday?, ?accept?, ?reject?, ?release-turn?.
In oursetup Agents 1 and 2 do not provide arguments for?Italian?
or ?Saturday?
since these are acceptableoptions for both agents.
Because Agent 1 caresmore about the food type and Agent 2 cares moreabout the day there is potential for trade-offs,i.e., ?I?ll give you the food type that you want ifyou agree on the day that I want?.
So we haveone more action ?trade-off?
which is basically acombined action ?offer-Thai, offer-Sunday?.
Thetwo agents have to agree on both issues for thedialogue to end.
If there is no agreement in 40turns then the dialogue stops.Note that for testing our learned policies (seesection 4) we use a rationalized version of theseSNs.
For example, Agent 1 never offers ?Sunday?and never accepts ?Mexican?.
We will refer to theSNs that exhibit some degree of randomness andirrationality as ?semi-rational?
and the SNs that al-ways behave rationally as ?rational?.For training, 4 corpora are generated (50,000 di-alogues each) using different SNs, each of whichis limited to using either strong or weak argu-ments: SN for Agent 1 with strong arguments vs.SN for Agent 2 with strong arguments, SN forAgent 1 with strong arguments vs. SN for Agent 2113with weak arguments, SN for Agent 1 with weakarguments vs. SN for Agent 2 with strong argu-ments, and SN for Agent 1 with weak argumentsvs.
SN for Agent 2 with weak arguments.We use LSPI to learn policies directly from the4 corpora.
Each agent is rewarded only at the endof the dialogue based on the agreement.
So if theoutcome is ?Thai?
and ?Saturday?
Agent 1 willearn 240 points and Agent 2 100 points.
We set asmall reward +1 point for each policy action taken.Table 2 shows our state representation.The first 10 state variables are self-explanatory.Below we explain how the ?counter?
variableswork.
Initially the counter for ?Thai?
argumentsis set to 0 and Agent 2 supports food type ?Mexi-can?.
Every time the policy of Agent 1 providesan argument in favor of ?Thai?, the counter for?Thai?
arguments is increased by 1 and the counterfor ?Mexican?
arguments is decreased by 1 (likea penalty).
Every time the policy of Agent 1 ar-gues in favor of ?Mexican?
the counter for ?Thai?arguments is decreased by 1 and the counter for?Mexican?
arguments is increased by 1.
Whenthe counter for ?Thai?
arguments becomes 3,then the state variable ?Thai-argument-counter-reached-threshold?
becomes ?yes?
and Agent 2 isready to yield to the demands of Agent 1.
Thisthreshold of 3 was set empirically after experimen-tation.
Likewise for the rest of the ?counter?
vari-ables.
We also account for both strong and weakarguments.
When the arguments of an agent areweak, even if the corresponding counters exceedthe predefined threshold and the associated statevariables change from ?no?
to ?yes?, the behav-ior of their interlocutor will not change.
This isto simulate the fact that weak arguments cannot bepersuasive.
The release action counter works simi-larly.
Initially it is 0 but after 4 consecutive actionsof the same speaker it is set to 1 to ensure that theturns are not very long.There are 786,432 possible states and11,010,048 possible Q-values (state-actionpairs).
We use linear function approximationwith 1,680 manually selected features.
Therationale for selecting these features is as follows:We associate the action ?offer-Thai?
with thestate variables ?current-day-accepted?, ?Thai-rejected?, ?Italian-rejected?, ?Mexican-rejected?,?Thai-argument-counter-reached-threshold?, and?Mexican-argument-counter-reached-threshold?.Thus we assume that the values of the other statevariables are irrelevant.
This is an approximation(to keep the number of features manageable) thatCurrent offer on the table (null/Thai/Italian/Mexican/Friday/Saturday/Sunday/trade-off)By whom is the current offer on the table(null/Agent1/Agent2)Currently accepted food type(null/Thai/Italian/Mexican)Currently accepted day (null/Friday/Saturday/Sunday)Has food type Thai been rejected?
(no/yes)Has food type Italian been rejected?
(no/yes)Has food type Mexican been rejected?
(no/yes)Has day Friday been rejected?
(no/yes)Has day Saturday been rejected?
(no/yes)Has day Sunday been rejected?
(no/yes)Has counter for food type Thai argumentsreached threshold?
(no/yes)Has counter for food type Mexican argumentsreached threshold?
(no/yes)Has counter for day Friday argumentsreached threshold?
(no/yes)Has counter for day Sunday argumentsreached threshold?
(no/yes)Has release action counter reachedthreshold (no/yes)Table 2: State variables that we keep track of andall the possible values they can take.has drawbacks, e.g., we may have an ?offer-Thai?action even though the food type agreed so far is?Thai?
(because there is no feature to associate thecurrently accepted food type value with a ?Thai?offer).
With this configuration we end up having4 ?
25 = 128 binary features just for the action?offer-Thai?.
Similarly, features are selected forthe rest of the actions.We partition each one of our 4 simulated cor-pora into 5 subsets of 10,000 dialogues each.
Eachpartition is processed independently and will bereferred to as trial.
We train policies for eachtrial of each corpus type (20 policies for eachagent).
Thus we end up with the following 4types of policies for Agent 1 (and likewise for thepolicies of Agent 2): Agent 1 with strong argu-ments trained against Agent 2 with strong argu-ments (Agent 1 S(S)); Agent 1 with strong argu-ments trained against Agent 2 with weak argu-ments (Agent 1 S(W)); Agent 1 with weak argu-ments trained against Agent 2 with strong argu-ments (Agent 1 W(S)); and Agent 1 with weakarguments trained against Agent 2 with weak ar-guments (Agent 1 W(W)).114Policy Opponent Policy Opponent Policy OpponentScore Score #Actions #Actions #Turns #TurnsAgent 1 S(S) vs.
Agent 2 S 214.3 164.3 7.6 6.2 2.0 1.6Agent 1 S(S) vs.
Agent 2 W 214.1 164.5 7.4 6.1 2.0 1.6Agent 1 S(W) vs.
Agent 2 S 213.9 165.1 7.6 6.2 2.0 1.6Agent 1 S(W) vs.
Agent 2 W 214.1 164.7 7.4 6.1 2.0 1.6Agent 1 W(S) vs.
Agent 2 S 192.4 196.5 9.1 8.5 2.5 2.4Agent 1 W(S) vs.
Agent 2 W 197.9 198.9 7.6 7.0 2.1 1.9Agent 1 W(W) vs.
Agent 2 S 195.0 197.9 8.8 8.5 2.5 2.4Agent 1 W(W) vs.
Agent 2 W 198.1 199.0 7.7 7.0 2.2 2.0Table 3: Results of different training and testing combinations for learned policies of Agent 1 and rationalSNs for Agent 2.4 EvaluationEach policy of Agent 1 resulting from a trialis evaluated against two hand-crafted SNs forAgent 2, one where Agent 2 provides strong ar-guments (Agent 2 S) and one where Agent 2 pro-vides weak arguments (Agent 2 W).
So for thecondition ?Agent 1 with strong arguments trainedagainst Agent 2 with strong arguments (Agent 1S(S))?
we have 5 policies, each of which interactswith ?Agent 2 S?
(or ?Agent 2 W?).
We calcu-late the averages of the earned points for each ofthe agents, of the number of actions per dialogueof each agent, and of the number of turns per di-alogue of each agent, over 10,000 dialogues perpolicy.
Likewise for the policies of Agent 2.
Notethat the SNs used in the evaluation do not behaveirrationally like the ones used for training, and thusare harder to beat.In Table 3 we can see the results for the policyof Agent 1.
Results for the policy of Agent 2 aresimilar given that the goals of Agent 2 mirror thegoals of Agent 1.
As we can see, the policy ofAgent 1 with strong arguments learned to providethe appropriate arguments and make Agent 2 agreeon ?Thai?
and ?Friday?
or ?Saturday?.
When thepolicy of Agent 1 provides only weak arguments itcannot get day ?Friday?
but it can secure a trade-off.
This is because both the learned policies andthe SNs usually accept trade-off offers (due to theway the hand-crafted SNs were constructed).
Wealso performed tests with SNs that did not proposeor accept as many trade-offs.
This arrangement fa-vored the policy of Agent 1 with strong arguments,and hurt the performance of the policy of Agent 1with weak arguments playing against Agent 2 withstrong arguments.
This shows that trade-offs helpthe weaker negotiators.Furthermore, we experimented with testing onsemi-rational SNs similar to the ones used fortraining and the results were better for the policyof Agent 1 with weak arguments and worse for thepolicy of Agent 1 with strong arguments.
So liketrade-offs a semi-rational SN favors the weakernegotiators.5 ConclusionWe learned argumentation dialogue policies fortwo-issue negotiation, using simulated corporagenerated from the interaction of two hand-craftedSNs that differed in their goals and in the use ofstrong and weak arguments to persuade their coun-terparts.
These SNs sometimes made random orirrational moves to generate a variety of negotia-tion patterns.We used these simulated corpora and RL tolearn argumentation dialogue policies for each ofthe two agents.
Each of the learned policies wasevaluated against hand-crafted SNs similar to theones used for training but with the modificationthat these SNs no longer made irrational movesand thus were harder to beat.
The policies gener-ally did as well as, or better than the hand-craftedSNs showing that RL can be successfully used forlearning argumentation dialogue policies in two-issue negotiation scenarios.For future work we would like to use automaticfeature selection (Li et al 2009; Misu and Kash-ioka, 2012) and learn policies for more than twoissues and more than three outcomes per issue.Selecting features manually is a difficult processthat requires a lot of experimentation and trial-and-error.AcknowledgmentsThis work was funded by the NSF grant#1117313.115ReferencesSenthilkumar Chandramohan, Matthieu Geist, andOlivier Pietquin.
2010.
Sparse approximate dy-namic programming for dialog management.
InProc.
of the Annual SIGdial Meeting on Discourseand Dialogue, pages 107?115, Tokyo, Japan.Min Chi, Kurt VanLehn, Diane Litman, and PamelaJordan.
2011.
Empirically evaluating the ap-plication of reinforcement learning to the induc-tion of effective and adaptive pedagogical strategies.User Modeling and User-Adapted Interaction, 21(1-2):137?180.Milica Gas?ic?, Matthew Henderson, Blaise Thomson,Pirros Tsiakoulis, and Steve Young.
2012.
Pol-icy optimisation of POMDP-based dialogue systemswithout state space compression.
In Proc.
of theIEEE Workshop on Spoken Language Technology(SLT), pages 31?36, Miami, FL, USA.Kallirroi Georgila and David Traum.
2011a.
Learningculture-specific dialogue models from non culture-specific data.
In Proc.
of HCI International, LectureNotes in Computer Science Vol.
6766, pages 440?449, Orlando, FL, USA.Kallirroi Georgila and David Traum.
2011b.
Rein-forcement learning of argumentation dialogue poli-cies in negotiation.
In Proc.
of Interspeech, pages2073?2076, Florence, Italy.Kallirroi Georgila, Maria K. Wolters, and Johanna D.Moore.
2010.
Learning dialogue strategies fromolder and younger simulated users.
In Proc.
ofthe Annual SIGdial Meeting on Discourse and Di-alogue, pages 103?106, Tokyo, Japan.Peter A. Heeman.
2009.
Representing the rein-forcement learning state in a negotiation dialogue.In Proc.
of the IEEE Automatic Speech Recogni-tion and Understanding Workshop (ASRU), Merano,Italy.James Henderson, Oliver Lemon, and KallirroiGeorgila.
2008.
Hybrid reinforcement/supervisedlearning of dialogue policies from fixed datasets.Computational Linguistics, 34(4):487?511.S.
Janarthanam and O.
Lemon.
2010.
Adaptive refer-ring expression generation in spoken dialogue sys-tems: Evaluation with real users.
In Proc.
of the An-nual SIGdial Meeting on Discourse and Dialogue,pages 124?131, Tokyo, Japan.Filip Jurc??
?c?ek, Blaise Thomson, and Steve Young.2012.
Reinforcement learning for parameter esti-mation in statistical spoken dialogue systems.
Com-puter Speech and Language, 26(3):168?192.Michail G. Lagoudakis and Ronald Parr.
2003.
Least-squares policy iteration.
Journal of Machine Learn-ing Research, 4:1107?1149.Lihong Li, Jason D. Williams, and Suhrid Balakrish-nan.
2009.
Reinforcement learning for dialog man-agement using least-squares policy iteration and fastfeature selection.
In Proc.
of Interspeech, pages2475?2478, Brighton, United Kingdom.Teruhisa Misu and Hideki Kashioka.
2012.
Simul-taneous feature selection and parameter optimiza-tion for training of dialogue policy by reinforcementlearning.
In Proc.
of the IEEE Workshop on SpokenLanguage Technology (SLT), pages 1?6, Miami, FL,USA.Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake,Chiori Hori, Hideki Kashioka, Hisashi Kawai, andSatoshi Nakamura.
2010.
Modeling spoken de-cision making dialogue and optimization of its di-alogue strategy.
In Proc.
of the Annual SIGdialMeeting on Discourse and Dialogue, pages 221?224, Tokyo, Japan.Teruhisa Misu, Kallirroi Georgila, Anton Leuski, andDavid Traum.
2012.
Reinforcement learning ofquestion-answering dialogue policies for virtual mu-seum guides.
In Proc.
of the Annual SIGdial Meet-ing on Discourse and Dialogue, pages 84?93, Seoul,South Korea.Elnaz Nouri, Kallirroi Georgila, and David Traum.2012.
A cultural decision-making model for nego-tiation based on inverse reinforcement learning.
InProc.
of the Annual Meeting of the Cognitive Sci-ence Society (CogSci), pages 2097?2102, Sapporo,Japan.P.
Paruchuri, N. Chakraborty, R. Zivan, K. Sycara,M.
Dudik, and G. Gordon.
2009.
POMDP basednegotiation modeling.
In Proc.
of the IJCAI Work-shop on Modeling Intercultural Collaboration andNegotiation (MICON).Csaba Szepesva?ri.
2010.
Algorithms for Reinforce-ment Learning.
Morgan & Claypool Publishers.Joel R. Tetreault and Diane J. Litman.
2008.
A rein-forcement learning approach to evaluating state rep-resentations in spoken dialogue systems.
SpeechCommunication, 50(8-9):683?696.Jason D. Williams and Steve Young.
2007.
Scal-ing POMDPs for spoken dialog management.
IEEETrans.
on Audio, Speech, and Language Processing,15(7):2116?2129.116
