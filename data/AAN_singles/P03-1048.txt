Evaluation challenges in large-scale document summarizationDragomir R. RadevU.
of Michiganradev@umich.eduWai LamChinese U. of Hong Kongwlam@se.cuhk.edu.hkArda C?elebiUSC/ISIardax@isi.eduSimone TeufelU.
of Cambridgesimone.teufel@cl.cam.ac.ukJohn BlitzerU.
of Pennsylvaniablitzer@seas.upenn.eduDanyu LiuU.
of Alabamaliudy@cis.uab.eduHoracio SaggionU.
of Sheffieldh.saggion@dcs.shef.ac.ukHong QiU.
of Michiganhqi@umich.eduElliott DrabekJohns Hopkins U.edrabek@cs.jhu.eduAbstractWe present a large-scale meta evaluationof eight evaluation measures for bothsingle-document and multi-documentsummarizers.
To this end we built acorpus consisting of (a) 100 Million auto-matic summaries using six summarizersand baselines at ten summary lengths inboth English and Chinese, (b) more than10,000 manual abstracts and extracts, and(c) 200 Million automatic document andsummary retrievals using 20 queries.
Wepresent both qualitative and quantitativeresults showing the strengths and draw-backs of all evaluation methods and howthey rank the different summarizers.1 IntroductionAutomatic document summarization is a field thathas seen increasing attention from the NLP commu-nity in recent years.
In part, this is because sum-marization incorporates many important aspects ofboth natural language understanding and natural lan-guage generation.
In part it is because effective auto-matic summarization would be useful in a variety ofareas.
Unfortunately, evaluating automatic summa-rization in a standard and inexpensive way is a diffi-cult task (Mani et al, 2001).
Traditional large-scaleevaluations are either too simplistic (using measureslike precision, recall, and percent agreement which(1) don?t take chance agreement into account and (2)don?t account for the fact that human judges don?tagree which sentences should be in a summary) ortoo expensive (an approach using manual judge-ments can scale up to a few hundred summaries butnot to tens or hundreds of thousands).In this paper, we present a comparison of sixsummarizers as well as a meta-evaluation includingeight measures: Precision/Recall, Percent Agree-ment, Kappa, Relative Utility, Relevance Correla-tion, and three types of Content-Based measures(cosine, longest common subsequence, and wordoverlap).
We found that while all measures tendto rank summarizers in different orders, measureslike Kappa, Relative Utility, Relevance Correlationand Content-Based each offer significant advantagesover the more simplistic methods.2 Data, Annotation, and ExperimentalDesignWe performed our experiments on the Hong KongNews corpus provided by the Hong Kong SAR ofthe People?s Republic of China (LDC catalog num-ber LDC2000T46).
It contains 18,146 pairs of par-allel documents in English and Chinese.
The textsare not typical news articles.
The Hong Kong News-paper mainly publishes announcements of the localadministration and descriptions of municipal events,such as an anniversary of the fire department, or sea-sonal festivals.
We tokenized the corpus to iden-tify headlines and sentence boundaries.
For the En-glish text, we used a lemmatizer for nouns and verbs.We also segmented the Chinese documents using thetool provided at http://www.mandarintools.com.Several steps of the meta evaluation that we per-formed involved human annotator support.
First, weCluster 2 Meetings with foreign leadersCluster 46 Improving Employment OpportunitiesCluster 54 Illegal immigrantsCluster 60 Customs staff doing good job.Cluster 61 Permits for charitable fund raisingCluster 62 Y2K readinessCluster 112 Autumn and sports carnivalsCluster 125 Narcotics RehabilitationCluster 199 Intellectual Property RightsCluster 241 Fire safety, building management concernsCluster 323 Battle against disc piracyCluster 398 Flu results in Health ControlsCluster 447 Housing (Amendment) Bill Brings Assorted ImprovementsCluster 551 Natural disaster victims aidedCluster 827 Health education for youngstersCluster 885 Customs combats contraband/dutiable cigarette operationsCluster 883 Public health concerns cause food-business closingsCluster 1014 Traffic Safety EnforcementCluster 1018 Flower showsCluster 1197 Museums: exhibits/hoursFigure 1: Twenty queries created by the LDC forthis experiment.asked LDC to build a set of queries (Figure 1).
Eachof these queries produced a cluster of relevant doc-uments.
Twenty of these clusters were used in theexperiments in this paper.Additionally, we needed manual summaries or ex-tracts for reference.
The LDC annotators producedsummaries for each document in all clusters.
In or-der to produce human extracts, our judges also la-beled sentences with ?relevance judgements?, whichindicate the relevance of sentence to the topic of thedocument.
The relevance judgements for sentencesrange from 0 (irrelevant) to 10 (essential).
As in(Radev et al, 2000), in order to create an extract ofa certain length, we simply extract the top scoringsentences that add up to that length.For each target summary length, we produce anextract using a summarizer or baseline.
Then wecompare the output of the summarizer or baselinewith the extract produced from the human relevancejudgements.
Both the summarizers and the evalua-tion measures are described in greater detail in thenext two sections.2.1 Summarizers and baselinesThis section briefly describes the summarizers weused in the evaluation.
All summarizers take as inputa target length (n%) and a document (or cluster) splitinto sentences.
Their output is an n% extract of thedocument (or cluster).?
MEAD (Radev et al, 2000): MEAD isa centroid-based extractive summarizer thatscores sentences based on sentence-level andinter-sentence features which indicate the qual-ity of the sentence as a summary sentence.
Itthen chooses the top-ranked sentences for in-clusion in the output summary.
MEAD runs onboth English documents and on BIG5-encodedChinese.
We tested the summarizer in both lan-guages.?
WEBS (Websumm (Mani and Bloedorn,2000)): can be used to produce generic andquery-based summaries.
Websumm uses agraph-connectivity model and operates underthe assumption that nodes which are connectedto many other nodes are likely to carry salientinformation.?
SUMM (Summarist (Hovy and Lin, 1999)):an extractive summarizer based on topic signa-tures.?
ALGN (alignment-based): We ran a sentencealignment algorithm (Gale and Church, 1993)for each pair of English and Chinese stories.We used it to automatically generate Chinese?manual?
extracts from the English manual ex-tracts we received from LDC.?
LEAD (lead-based): n% sentences are chosenfrom the beginning of the text.?
RAND (random): n% sentences are chosen atrandom.The six summarizers were run at ten different tar-get lengths to produce more than 100 million sum-maries (Figure 2).
For the purpose of this paper, weonly focus on a small portion of the possible experi-ments that our corpus can facilitate.3 Summary Evaluation TechniquesWe used three general types of evaluation measures:co-selection, content-based similarity, and relevancecorrelation.
Co-selection measures include preci-sion and recall of co-selected sentences, relative util-ity (Radev et al, 2000), and Kappa (Siegel andCastellan, 1988; Carletta, 1996).
Co-selection meth-ods have some restrictions: they only work for ex-tractive summarizers.
Two manual summaries of thesame input do not in general share many identicalsentences.
We address this weakness of co-selectionLengths #dj05W 05S 10W 10S 20W 20S 30W 30S 40W 40S FDE-FD - - - - - - - - - - x 40E-LD X X X X x x X X X X - 440E-RA X X X X x x X X X X - 440E-MO x x X x x x X x X x - 540E-M2 - - - - - X - - - - - 20E-M3 - - - - - X - - - - - 8E-S2 - - - - - X - - - - - 8E-WS - X - X x x - X - X - 160E-WQ - - - - - X - - - - - 10E-LC - - - - - - x - - - - 40E-CY - X - X - x - X - X - 120E-AL X X X X X X X X X X - 200E-AR X X X X X X X X X X - 200E-AM X X X X X X X X X X - 200C-FD - - - - - - - - - - x 40C-LD X X X X x x X X X X - 240C-RA X X X X x x X X X X - 240C-MO X x X x x x X x X x - 320C-M2 - - - - - X - - - - - 20C-CY - X - X - x - X - X - 120C-AL X X X X X X X X X X - 180C-AR X X X X X X X X X X - 200C-AM - X X X X X X X X - 120X-FD - - - - - - - - - - x 40X-LD X X X X x x X X X X - 240X-RA X X X X x x X X X X - 240X-MO X x X x x x X x X x - 320X-M2 - - - - - X - - - - - 20X-CY - X - X - x - X - X - 120X-AL X X X X X X X X X X - 140X-AR X X X X X X X X X X - 160X-AM - X X X X X X X - X - 120Figure 2: All runs performed (X = 20 clusters, x = 10 clusters).
Language: E = English, C = Chinese,X = cross-lingual; Summarizer: LD=LEAD, RA=RAND, WS=WEBS, WQ=WEBS-query based, etc.
; S =sentence-based, W = word-based; #dj = number of ?docjudges?
(ranked lists of documents and summaries).Target lengths above 50% are not shown in this table for lack of space.
Each run is available using twodifferent retrieval schemes.
We report results using the cross-lingual retrievals in a separate paper.measures with several content-based similarity mea-sures.
The similarity measures we use are wordoverlap, longest common subsequence, and cosine.One advantage of similarity measures is that theycan compare manual and automatic extracts withmanual abstracts.
To our knowledge, no system-atic experiments about agreement on the task ofsummary writing have been performed before.
Weuse similarity measures to measure interjudge agree-ment among three judges per topic.
We also ap-ply the measures between human extracts and sum-maries, which answers the question if human ex-tracts are more similar to automatic extracts or tohuman summaries.The third group of evaluation measures includesrelevance correlation.
It shows the relative perfor-mance of a summary: how much the performanceof document retrieval decreases when indexing sum-maries rather than full texts.Task-based evaluations (e.g., SUMMAC (Maniet al, 2001), DUC (Harman and Marcu, 2001), or(Tombros et al, 1998) measure human performanceusing the summaries for a certain task (after thesummaries are created).
Although they can be avery effective way of measuring summary quality,task-based evaluations are prohibitively expensive atlarge scales.
In this project, we didn?t perform anytask-based evaluations as they would not be appro-priate at the scale of millions of summaries.3.1 Evaluation by sentence co-selectionFor each document and target length we producethree extracts from the three different judges, whichwe label throughout as J1, J2, and J3.We used the rates 5%, 10%, 20%, 30%, 40% formost experiments.
For some experiments, we alsoconsider summaries of 50%, 60%, 70%, 80% and90% of the original length of the documents.
Figure3 shows some abbreviations for co-selection that wewill use throughout this section.3.1.1 Precision and RecallPrecision and recall are defined as:PJ2 (J1) =AA+ C,RJ2 (J1) =AA+ BJ2Sentence inExtractSentence notin ExtractSentence inExtractA B A+ BJ1 Sentence notin ExtractC D C +DA+ C B +D N = A +B+C+DFigure 3: Contingency table comparing sentencesextracted by the system and the judges.In our case, each set of documents which is com-pared has the same number of sentences and alsothe same number of sentences are extracted; thusP = R.The average precision Pavg(SY STEM) and re-call Ravg(SY STEM) are calculated by summingover individual judges and normalizing.
The aver-age interjudge precision and recall is computed byaveraging over all judge pairs.However, precision and recall do not take chanceagreement into account.
The amount of agreementone would expect two judges to reach by chance de-pends on the number and relative proportions of thecategories used by the coders.
The next section onKappa shows that chance agreement is very high inextractive summarization.3.1.2 KappaKappa (Siegel and Castellan, 1988) is an evalua-tion measure which is increasingly used in NLP an-notation work (Krippendorff, 1980; Carletta, 1996).Kappa has the following advantages over P and R:?
It factors out random agreement.
Randomagreement is defined as the level of agreementwhich would be reached by random annotationusing the same distribution of categories as thereal annotators.?
It allows for comparisons between arbitrarynumbers of annotators and items.?
It treats less frequent categories as more im-portant (in our case: selected sentences), simi-larly to precision and recall but it also consid-ers (with a smaller weight) more frequent cate-gories as well.The Kappa coefficient controls agreement P (A)by taking into account agreement by chance P (E) :K =P (A)?
P (E)1?
P (E)No matter how many items or annotators, or howthe categories are distributed, K = 0 when there isno agreement other than what would be expected bychance, and K = 1 when agreement is perfect.
Iftwo annotators agree less than expected by chance,Kappa can also be negative.We report Kappa between three annotators in thecase of human agreement, and between three hu-mans and a system (i.e.
four judges) in the next sec-tion.3.1.3 Relative UtilityRelative Utility (RU) (Radev et al, 2000) is testedon a large corpus for the first time in this project.RU takes into account chance agreement as a lowerbound and interjudge agreement as an upper boundof performance.
RU allows judges and summarizersto pick different sentences with similar content intheir summaries without penalizing them for doingso.
Each judge is asked to indicate the importanceof each sentence in a cluster on a scale from 0 to10.
Judges also specify which sentences subsume orparaphrase each other.
In relative utility, the scoreof an automatic summary increases with the impor-tance of the sentences that it includes but goes downwith the inclusion of redundant sentences.3.2 Content-based Similarity measuresContent-based similarity measures compute the sim-ilarity between two summaries at a more fine-grained level than just sentences.
For each automaticextract S and similarity measure M we compute thefollowing number:sim(M,S, {J1, J2, J3}) =M(S, J1) +M(S, J2) +M(S, J3)3We used several content-based similarity mea-sures that take into account different properties ofthe text:Cosine similarity is computed using the follow-ing formula (Salton, 1988):cos(X,Y ) =?xi ?
yi??
(xi)2 ???
(yi)2where X and Y are text representations based onthe vector space model.Longest Common Subsequence is computed asfollows:lcs(X,Y ) = (length(X) + length(Y )?
d(X,Y ))/2where X and Y are representations based onsequences and where lcs(X,Y ) is the length ofthe longest common subsequence between X andY , length(X) is the length of the string X , andd(X,Y ) is the minimum number of deletion and in-sertions needed to transform X into Y (Crochemoreand Rytter, 1994).3.3 Relevance CorrelationRelevance correlation (RC) is a new measure for as-sessing the relative decrease in retrieval performancewhen indexing summaries instead of full documents.The idea behind it is similar to (Sparck-Jones andSakai, 2001).
In that experiment, Sparck-Jones andSakai determine that short summaries are good sub-stitutes for full documents at the high precision end.With RC we attempt to rank all documents given aquery.Suppose that given a queryQ and a corpus of doc-uments Di, a search engine ranks all documents inDi according to their relevance to the query Q. Ifinstead of the corpus Di, the respective summariesof all documents are substituted for the full docu-ments and the resulting corpus of summaries Si isranked by the same retrieval engine for relevance tothe query, a different ranking will be obtained.
Ifthe summaries are good surrogates for the full docu-ments, then it can be expected that rankings will besimilar.There exist several methods for measuring thesimilarity of rankings.
One such method is Kendall?stau and another is Spearman?s rank correlation.
Bothmethods are quite appropriate for the task that wewant to perform; however, since search engines pro-duce relevance scores in addition to rankings, wecan use a stronger similarity test, linear correlationbetween retrieval scores.
When two identical rank-ings are compared, their correlation is 1.
Two com-pletely independent rankings result in a score of 0while two rankings that are reverse versions of oneanother have a score of -1.
Although rank correla-tion seems to be another valid measure, given thelarge number of irrelevant documents per query re-sulting in a large number of tied ranks, we opted forlinear correlation.
Interestingly enough, linear cor-relation and rank correlation agreed with each other.Relevance correlation r is defined as the linearcorrelation of the relevance scores (x and y) as-signed by two different IR algorithms on the sameset of documents or by the same IR algorithm ondifferent data sets:r =?i(xi ?
x)(yi ?
y)?
?i(xi ?
x)2?
?i(yi ?
y)2Here x and y are the means of the relevance scoresfor the document sequence.We preprocess the documents and use Smart toindex and retrieve them.
After the retrieval process,each summary is associated with a score indicatingthe relevance of the summary to the query.
Therelevance score is actually calculated as the innerproduct of the summary vector and the query vec-tor.
Based on the relevance score, we can produce afull ranking of all the summaries in the corpus.In contrast to (Brandow et al, 1995) who run 12Boolean queries on a corpus of 21,000 documentsand compare three types of documents (full docu-ments, lead extracts, and ANES extracts), we mea-sure retrieval performance under more than 300 con-ditions (by language, summary length, retrieval pol-icy for 8 summarizers or baselines).4 ResultsThis section reports results for the summarizers andbaselines described above.
We relied directly on therelevance judgements to create ?manual extracts?
touse as gold standards for evaluating the English sys-tems.
To evaluate Chinese, we made use of a ta-ble of automatically produced alignments.
Whilethe accuracy of the alignments is quite high, wehave not thoroughly measured the errors producedwhen mapping target English summaries into Chi-nese.
This will be done in future work.4.1 Co-selection resultsCo-selection agreement (Section 3.1) is reported inFigures 4, and 5).
The tables assume human perfor-mance is the upper bound, the next rows comparethe different summarizers.Figure 4 shows results for precision and recall.We observe the effect of a dependence of the nu-merical results on the length of the summary, whichis a well-known fact from information retrieval eval-uations.Websumm has an advantage over MEAD forlonger summaries but not for 20% or less.
Leadsummaries perform better than all the automaticsummarizers, and better than the human judges.This result usually occurs when the judges choosedifferent, but early sentences.
Human judgementsovertake the lead baseline for summaries of length50% or more.5% 10% 20% 30% 40%Humans .187 .246 .379 .467 .579MEAD .160 .231 .351 .420 .519WEBS .310 .305 .358 .439 .543LEAD .354 .387 .447 .483 .583RAND .094 .113 .224 .357 .432Figure 4: Results in precision=recall (averaged over20 clusters).Figure 5 shows results using Kappa.
Randomagreement is 0 by definition between a random pro-cess and a non-random process.While the results are overall rather low, the num-bers still show the following trends:?
MEAD outperforms Websumm for all but the5% target length.?
Lead summaries perform best below 20%,whereas human agreement is higher after that.?
There is a rather large difference between thetwo summarizers and the humans (except forthe 5% case for Websumm).
This numericaldifference is relatively higher than for any otherco-selection measure treated here.?
Random is overall the worst performer.?
Agreement improves with summary length.Figures 6 and 7 summarize the results obtainedthrough Relative Utility.
As the figures indicate,random performance is quite high although all non-random methods outperform it significantly.
Fur-ther, and in contrast with other co-selection evalua-tion criteria, in both the single- and multi-document5% 10% 20% 30% 40%Humans .127 .157 .194 .225 .274MEAD .109 .136 .168 .192 .230WEBS .138 .128 .146 .159 .192LEAD .180 .198 .213 .220 .261RAND .064 .081 .097 .116 .137Figure 5: Results in kappa, averaged over 20 clus-ters.case MEAD outperforms LEAD for shorter sum-maries (5-30%).
The lower bound (R) represents theaverage performance of all extracts at the given sum-mary length while the upper bound (J) is the inter-judge agreement among the three judges.5% 10% 20% 30% 40%R 0.66 0.68 0.71 0.74 0.76RAND 0.67 0.67 0.71 0.75 0.77WEBS 0.72 0.73 0.76 0.79 0.82LEAD 0.72 0.73 0.77 0.80 0.83MEAD 0.78 0.79 0.79 0.81 0.83J 0.80 0.81 0.83 0.85 0.87Figure 6: RU per summarizer and summary length(Single-document).5% 10% 20% 30% 40%R 0.64 0.66 0.69 0.72 0.74RAND 0.63 0.65 0.71 0.72 0.74LEAD 0.71 0.71 0.76 0.79 0.82MEAD 0.73 0.75 0.78 0.79 0.81J 0.76 0.78 0.81 0.83 0.85Figure 7: RU per summarizer and summary length(Multi-document).4.2 Content-based resultsThe results obtained for a subset of target lengthsusing content-based evaluation can be seen in Fig-ures 8 and 9.
In all our experiments with tf ?
idf -weighted cosine, the lead-based summarizer ob-tained results close to the judges in most of the targetlengths while MEAD is ranked in second position.In all our experiments using longest common sub-sequence, no system obtained better results in themajority of the cases.10% 20% 30% 40%LEAD 0.55 0.65 0.70 0.79MEAD 0.46 0.61 0.70 0.78RAND 0.31 0.47 0.60 0.69WEBS 0.52 0.60 0.68 0.77Figure 8: Cosine (tf?idf ).
Average over 10 clusters.10% 20% 30% 40%LEAD 0.47 0.55 0.60 0.70MEAD 0.37 0.52 0.61 0.70RAND 0.25 0.38 0.50 0.58WEBS 0.39 0.45 0.53 0.64Figure 9: Longest Common Subsequence.
Averageover 10 clusters.The numbers obtained in the evaluation of Chi-nese summaries for cosine and longest common sub-sequence can be seen in Figures 10 and 11.
Bothmeasures identify MEAD as the summarizer thatproduced results closer to the ideal summaries (theseresults also were observed across measures and textrepresentations).10% 20% 30% 40%SUMM 0.44 0.65 0.71 0.78LEAD 0.54 0.63 0.68 0.77MEAD 0.49 0.65 0.74 0.82RAND 0.31 0.50 0.65 0.71Figure 10: Chinese Summaries.
Cosine (tf ?
idf ).Average over 10 clusters.
Vector space of Words asText Representation.10% 20% 30% 40%SUMM 0.32 0.53 0.57 0.65LEAD 0.42 0.49 0.54 0.64MEAD 0.35 0.50 0.60 0.70RAND 0.21 0.35 0.49 0.54Figure 11: Chinese Summaries.
Longest CommonSubsequence.
Average over 10 clusters.
ChineseWords as Text Representation.We have based this evaluation on target sum-maries produced by LDC assessors, although otheralternatives exist.
Content-based similarity mea-sures do not require the target summary to be a sub-set of sentences from the source document, thus,content evaluation based on similarity measurescan be done using summaries published with thesource documents which are in many cases available(Teufel and Moens, 1997; Saggion, 2000).4.3 Relevance Correlation resultsWe present several results using Relevance Correla-tion.
Figures 12 and 13 show how RC changes de-pending on the summarizer and the language used.RC is as high as 1.0 when full documents (FD) arecompared to themselves.
One can notice that evenrandom extracts get a relatively high RC score.
It isalso worth observing that Chinese summaries scorelower than their corresponding English summaries.Figure 14 shows the effects of summary length andsummarizers on RC.
As one might expect, longersummaries carry more of the content of the full doc-ument than shorter ones.
At the same time, the rel-ative performance of the different summarizers re-mains the same across compression rates.C112 C125 C241 C323 C551 AVG10FD 1.00 1.00 1.00 1.00 1.00 1.000MEAD 0.91 0.92 0.93 0.92 0.90 0.903WEBS 0.88 0.82 0.89 0.91 0.88 0.843LEAD 0.80 0.80 0.84 0.85 0.81 0.802RAND 0.80 0.78 0.87 0.85 0.79 0.800SUMM 0.77 0.79 0.85 0.88 0.81 0.775Figure 12: RC per summarizer (English 20%).C112 C125 C241 C323 C551 AVG10FD 1.00 1.00 1.00 1.00 1.00 1.000MEAD 0.78 0.87 0.93 0.66 0.91 0.850SUMM 0.76 0.75 0.85 0.84 0.75 0.755RAND 0.71 0.75 0.85 0.60 0.74 0.744ALGN 0.74 0.72 0.83 0.95 0.72 0.738LEAD 0.72 0.71 0.83 0.58 0.75 0.733Figure 13: RC per summarizer (Chinese, 20%).5% 10% 20% 30% 40%FD 1.000 1.000 1.000 1.000 1.000MEAD 0.724 0.834 0.916 0.946 0.962WEBS 0.730 0.804 0.876 0.912 0.936LEAD 0.660 0.730 0.820 0.880 0.906SUMM 0.622 0.710 0.820 0.848 0.862RAND 0.554 0.708 0.818 0.884 0.922Figure 14: RC per summary length and summarizer.5 ConclusionThis paper describes several contributions to textsummarization:First, we observed that different measures ranksummaries differently, although most of themshowed that ?intelligent?
summarizers outperformlead-based summaries which is encouraging giventhat previous results had cast doubt on the ability ofsummarizers to do better than simple baselines.Second, we found that measures like Kappa, Rel-ative Utility, Relevance Correlation and Content-Based, each offer significant advantages over moresimplistic methods like Precision, Recall, and Per-cent Agreement with respect to scalability, applica-bility to multidocument summaries, and ability toinclude human and chance agreement.
Figure 15Property Prec, recall Kappa Normalized RU Word overlap, cosine, LCS Relevance CorrelationIntrinsic (I)/extrinsic (E) I I I I EAgreement between human extracts X X X X XAgreement human extracts and automatic extracts X X X X XAgreement human abstracts and human extracts XNon-binary decisions X XTakes random agreement into account by design X XFull documents vs. extracts X XSystems with different sentence segmentation X XMultidocument extracts X X X XFull corpus coverage X XFigure 15: Properties of evaluation measures used in this project.presents a short comparison of all these evaluationmeasures.Third, we performed extensive experiments usinga new evaluation measure, Relevance Correlation,which measures how well a summary can be usedto replace a document for retrieval purposes.Finally, we have packaged the code used for thisproject into a summarization evaluation toolkit andproduced what we believe is the largest and mostcomplete annotated corpus for further research intext summarization.
The corpus and related softwareis slated for release by the LDC in mid 2003.ReferencesRon Brandow, Karl Mitze, and Lisa F. Rau.
1995.
Auto-matic Condensation of Electronic Publications by Sen-tence Selection.
Information Processing and Manage-ment, 31(5):675?685.Jean Carletta.
1996.
Assessing Agreement on Classifica-tion Tasks: The Kappa Statistic.
CL, 22(2):249?254.Maxime Crochemore and Wojciech Rytter.
1994.
TextAlgorithms.
Oxford University Press.William A. Gale and Kenneth W. Church.
1993.
Aprogram for aligning sentences in bilingual corpora.Computational Linguistics, 19(1):75?102.Donna Harman and Daniel Marcu, editors.
2001.
Pro-ceedings of the 1st Document Understanding Confer-ence.
New Orleans, LA, September.Eduard Hovy and Chin Yew Lin.
1999.
Automated TextSummarization in SUMMARIST.
In Inderjeet Maniand Mark T. Maybury, editors, Advances in AutomaticText Summarization, pages 81?94.
The MIT Press.Klaus Krippendorff.
1980.
Content Analysis: An Intro-duction to its Methodology.
Sage Publications, Bev-erly Hills, CA.Inderjeet Mani and Eric Bloedorn.
2000.
Summariz-ing Similarities and Differences Among Related Doc-uments.
Information Retrieval, 1(1).Inderjeet Mani, The?re`se Firmin, David House, GaryKlein, Beth Sundheim, and Lynette Hirschman.
2001.The TIPSTER SUMMAC Text Summarization Evalu-ation.
In Natural Language Engineering.Dragomir R. Radev, Hongyan Jing, and MalgorzataBudzikowska.
2000.
Centroid-Based Summarizationof Multiple Documents: Sentence Extraction, Utility-Based Evaluation, and User Studies.
In Proceedingsof the Workshop on Automatic Summarization at the6th Applied Natural Language Processing Conferenceand the 1st Conference of the North American Chap-ter of the Association for Computational Linguistics,Seattle, WA, April.Horacio Saggion.
2000.
Ge?ne?ration automatiquede re?sume?s par analyse se?lective.
Ph.D. the-sis, De?partement d?informatique et de rechercheope?rationnelle.
Faculte?
des arts et des sciences.
Uni-versite?
de Montre?al, August.Gerard Salton.
1988.
Automatic Text Processing.Addison-Wesley Publishing Company.Sidney Siegel and N. John Jr. Castellan.
1988.
Non-parametric Statistics for the Behavioral Sciences.McGraw-Hill, Berkeley, CA, 2nd edition.Karen Sparck-Jones and Tetsuya Sakai.
2001.
GenericSummaries for Indexing in IR.
In Proceedings of the24th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 190?198, New Orleans, LA, September.Simone Teufel and Marc Moens.
1997.
Sentence Ex-traction as a Classification Task.
In Proceedings of theWorkshop on Intelligent Scalable Text Summarizationat the 35th Meeting of the Association for Computa-tional Linguistics, and the 8th Conference of the Eu-ropean Chapter of the Assocation for ComputationalLinguistics, Madrid, Spain.Anastasios Tombros, Mark Sanderson, and Phil Gray.1998.
Advantages of Query Biased Summaries in In-formation Retrieval.
In Eduard Hovy and Dragomir R.Radev, editors, Proceedings of the AAAI Symposiumon Intelligent Text Summarization, pages 34?43, Stan-ford, California, USA, March 23?25,.
The AAAIPress.
