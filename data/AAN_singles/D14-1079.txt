Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708?719,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsEvaluating Neural Word Representations inTensor-Based Compositional SettingsDmitrijs Milajevs1Dimitri Kartsaklis2Mehrnoosh Sadrzadeh1Matthew Purver11Queen Mary University of LondonSchool of Electronic Engineeringand Computer ScienceMile End Road, London, UK{d.milajevs,m.sadrzadeh,m.purver}@qmul.ac.uk2University of OxfordDepartment of Computer ScienceParks Road, Oxford, UKdimitri.kartsaklis@cs.ox.ac.ukAbstractWe provide a comparative study be-tween neural word representations andtraditional vector spaces based on co-occurrence counts, in a number of com-positional tasks.
We use three differ-ent semantic spaces and implement seventensor-based compositional models, whichwe then test (together with simpler ad-ditive and multiplicative approaches) intasks involving verb disambiguation andsentence similarity.
To check their scala-bility, we additionally evaluate the spacesusing simple compositional methods onlarger-scale tasks with less constrainedlanguage: paraphrase detection and di-alogue act tagging.
In the more con-strained tasks, co-occurrence vectors arecompetitive, although choice of composi-tional method is important; on the larger-scale tasks, they are outperformed by neu-ral word embeddings, which show robust,stable performance across the tasks.1 IntroductionNeural word embeddings (Bengio et al., 2006;Collobert and Weston, 2008; Mikolov et al.,2013a) have received much attention in the dis-tributional semantics community, and have shownstate-of-the-art performance in many natural lan-guage processing tasks.
While they have beencompared with co-occurrence based models insimple similarity tasks at the word level (Levy etal., 2014; Baroni et al., 2014), we are aware ofonly one work that attempts a comparison of thetwo approaches in compositional settings (Blacoeand Lapata, 2012), and this is limited to additiveand multiplicative composition, compared againstcomposition via a neural autoencoder.The purpose of this paper is to provide a morecomplete picture regarding the potential of neu-ral word embeddings in compositional tasks, andmeaningfully compare them with the traditionaldistributional approach based on co-occurrencecounts.
We are especially interested in investi-gating the performance of neural word vectors incompositional models involving general mathe-matical composition operators, rather than in themore task- or domain-specific deep-learning com-positional settings they have generally been usedwith so far (for example, by Socher et al.
(2012),Kalchbrenner and Blunsom (2013) and many oth-ers).In particular, this is the first large-scale studyto date that applies neural word representations intensor-based compositional distributional modelsof meaning similar to those formalized by Coeckeet al.
(2010).
We test a range of implementationsbased on this framework, together with additiveand multiplicative approaches (Mitchell and Lap-ata, 2008), in a variety of different tasks.
Specif-ically, we use the verb disambiguation task ofGrefenstette and Sadrzadeh (2011a) and the tran-sitive sentence similarity task of Kartsaklis andSadrzadeh (2014) as small-scale focused experi-ments on pre-defined sentence structures.
Addi-tionally, we evaluate our vector spaces on para-phrase detection (using the Microsoft ResearchParaphrase Corpus of Dolan et al.
(2005)) and di-alogue act tagging using the Switchboard Corpus(see e.g.
(Stolcke et al., 2000)).In all of the above tasks, we compare the neuralword embeddings of Mikolov et al.
(2013a) withtwo vector spaces both based on co-occurrencecounts and produced by standard distributionaltechniques, as described in detail below.
The gen-eral picture we get from the results is that in almostall cases the neural vectors are more effective thanthe traditional approaches.We proceed as follows: Section 2 provides aconcise introduction to distributional word repre-sentations in natural language processing.
Section7083 takes a closer look to the subject of composi-tionality in vector space models of meaning anddescribes the range of compositional operators ex-amined here.
In Section 4 we provide details aboutthe vector spaces used in the experiments.
Our ex-perimental work is described in detail in Section 5,and the results are discussed in Section 6.
Finally,Section 7 provides conclusions.2 Meaning representationThere are several approaches to the representationof word, phrase and sentence meaning.
As nat-ural languages are highly creative and it is veryrare to see the same sentence twice, any practicalapproach dealing with large text segments mustbe compositional, constructing the meaning ofphrases and sentences from their constituent parts.The ideal method would therefore express notonly the similarity in meaning between those con-stituent parts, but also between the results of theircomposition, and do this in ways which fit withlinguistic structure and generalisations thereof.Formal semantics Formal approaches to thesemantics of natural language have long builtupon the classical idea of compositionality ?that the meaning of a sentence is a functionof the meanings of its parts (Frege, 1892).
Incompositional type-logical approaches, predicate-argument structures representing phrases and sen-tences are built from their constituent parts by ?-reduction within the lambda calculus framework(Montague, 1970): for example, given a represen-tation of John as john?and sleeps as ?x.sleep?
(x),the meaning of the sentence ?John sleeps?can be constructed as ?x.sleep?(x)(john?)
=sleep?(john?).
Given a suitable pairing betweenwords and semantic representations of them, thismethod can produce structured sentential repre-sentations with broad coverage and good gener-alisability (see e.g.
(Bos, 2008)).
The above logi-cal approach is extremely powerful because it cancapture complex aspects of meaning such as quan-tifiers and their interaction (see e.g.
(Copestake etal., 2005)), and enables inference using well stud-ied and developed logical methods (see e.g.
(Bosand Gabsdil, 2000)).Distributional hypothesis However, such for-mal approaches are less able to express similar-ity in meaning.
We would like to capture theintuition that while John and Mary are distinct,they are rather similar to each other (both of themare humans) and dissimilar to words such as dog,pavement or idea.
The same applies at the phraseand sentence level: ?dogs chase cats?
is similar inmeaning to ?hounds pursue kittens?, but less so to?cats chase dogs?
(despite the lexical overlap).Distributional methods provide a way to addressthis problem.
By representing words and phrasesas vectors or tensors in a (usually highly dimen-sional) vector space, one can express similarityin meaning via a suitable distance metric withinthat space (usually cosine distance); furthermore,composition can be modelled via suitable linear-algebraic operations.Co-occurrence-based word representationsOne way to produce such vectorial representa-tions is to directly exploit Harris (1954)?s intuitionthat semantically similar words tend to appear insimilar contexts.
We can construct a vector spacein which the dimensions correspond to contexts,usually taken to be words as well.
The wordvector components can then be calculated fromthe frequency with which a word has co-occurredwith the corresponding contexts in a window ofwords, with a predefined length.Table 1 shows 5 3-dimensional vectors for thewords Mary, John, girl, boy and idea.
The wordsphilosophy, book and school signify vector spacedimensions.
As the vector for John is closer toMary than it is to idea in the vector space?a di-rect consequence of the fact that John?s contextsare similar to Mary?s and dissimilar to idea?s?wecan infer that John is semantically more similar toMary than to idea.Many variants of this approach exist: perfor-mance on word similarity tasks has been shownto be improved by replacing raw counts withweighted values (e.g.
mutual information)?see(Turney et al., 2010) and below for discussion, and(Kiela and Clark, 2014) for a detailed comparison.philosophy book schoolMary 0 10 22John 4 60 59girl 0 19 93boy 0 12 164idea 10 47 39Table 1: Word co-occurrence frequencies ex-tracted from the BNC (Leech et al., 1994).709Neural word embeddings Deep learning tech-niques exploit the distributional hypothesis dif-ferently.
Instead of relying on observed co-occurrence frequencies, a neural language modelis trained to maximise some objective function re-lated to e.g.
the probability of observing the sur-rounding words in some context (Mikolov et al.,2013b):1TT?t=1?
?c?j?c,j 6=0log p(wt+j|wt) (1)Optimizing the above function, for example, pro-duces vectors which maximise the conditionalprobability of observing words in a context aroundthe target word wt, where c is the size of thetraining window, and w1w2, ?
?
?wTa sequence ofwords forming a training instance.
Therefore, theresulting vectors will capture the distributional in-tuition and can express degrees of lexical similar-ity.This method has an obvious advantage com-pared to co-occurrence method: since now thecontext is predicted, the model in principle canbe much more robust in data sparsity prob-lems, which is always an important issue for co-occurrence word spaces.
Additionally, neural vec-tors have also proven successful in other tasks(Mikolov et al., 2013c), since they seem to en-code not only attributional similarity (the degree towhich similar words are close to each other), butalso relational similarity (Turney, 2006).
For ex-ample, it is possible to extract the singular:pluralrelation (apple:apples, car:cars) using vector sub-traction:???
?apple ?????
?apples ??
?car ???
?carsPerhaps even more importantly, semantic relation-ships are preserved in a very intuitive way:??
?king ???
?man ????
?queen ?????
?womanallowing the formation of analogy queries similarto??
?king ???
?man +????
?woman = ?, obtaining???
?queen asthe result.1Both neural and co-occurrence-based ap-proaches have advantages over classical formalapproaches in their ability to capture lexical se-mantics and degrees of similarity; their success at1Levy et al.
(2014) improved Mikolov et al.
(2013c)?smethod of retrieving relational similarities by changing theunderlying objective function.extending this to the sentence level and to morecomplex semantic phenomena, though, dependson their applicability within compositional mod-els, which is the subject of the next section.3 Compositional modelsCompositional distributional models representmeaning of a sequence of words by a vector, ob-tained by combining meaning vectors of the wordswithin the sequence using some vector composi-tion operation.
In a general classification of thesemodels, one can distinguish between three broadcases: simplistic models which combine wordvectors irrespective of their order or relation to oneanother, models which exploit linear word order,and models which use grammatical structure.The first approach combines word vectorsby vector addition or point-wise multiplication(Mitchell and Lapata, 2008)?as this is indepen-dent of word order, it cannot capture the differ-ence between the two sentences ?dogs chase cats?and ?cats chase dogs?.
The second approach hasgenerally been implemented using some form ofdeep learning, and captures word order, but not bynecessarily caring about the grammatical structureof the sentence.
Here, one works by recursivelybuilding and combining vectors for subsequencesof words within the sentence using e.g.
autoen-coders (Socher et al., 2012) or convolutional fil-ters (Kalchbrenner et al., 2014).
We do not con-sider this approach in this paper.
This is because,as mentioned in the introduction, their vectors andcomposition operators are task-specific.
These aretrained directly to achieve specific objectives incertain pre-determined tasks.
We are interestedin vector and composition operators that work forany compositional task, and which can be com-bined with results in linguistics and formal se-mantics to provide generalisable models that cancanonically extend to complex semantic phenom-ena.
The third (i.e.
the grammatical) approachpromises a way to achieve this, and has been in-stantiated in various ways in the work of Baroniand Zamparelli (2010),Grefenstette and Sadrzadeh(2011a), and Kartsaklis et al.
(2012).General framework Formally, we can spec-ify the vector representation of a word sequencew1w2?
?
?wnas the vector?
?s =??w1???w2?
?
?
???
?wn,where ?
is a vector operator, such as addition +,point-wise multiplication , tensor product ?, ormatrix multiplication ?.710In the simplest compositional models (the firstapproach described above), ?
is + or , e.g.
see(Mitchell and Lapata, 2008).
Grammar-basedcompositional models (the third approach) arebased on a generalisation of the notion of vectors,known as tensors.
Whereas a vector?
?v is an ele-ment of an atomic vector space V , a tensor z is anelement of a tensor space V ?W ?
?
?
?
?
Z. Thenumber of tensored spaces is referred to by the or-der of the space.
Using a general duality theoremfrom multi-linear algebra (Bourbaki, 1989), it fol-lows that tensors are in one-one correspondencewith multi-linear maps, that is we have:z ?
V ?W??
?
?
?Z?=fz: V ?W ?
?
?
?
?
ZIn such a tensor-based formalism, meanings ofnouns are vectors and meanings of predicates suchas adjectives and verbs are tensors.
Meaning of astring of words is obtained by applying the compo-sitions of multi-linear map duals of the tensors tothe vectors.
For the sake of demonstration, takethe case of an intransitive sentence ?Sbj Verb?
;the meaning of the subject is a vector?
?Sbj ?
Vand the meaning of the intransitive verb is a ten-sor Verb ?
V ?W .
Meaning of the sentence isobtained by applying fV erbto?
?Sbj, as follows:?????
?Sbj Verb = fV erb(?
?Sbj)By tensor-map duality, the above becomesequivalent to the following, where compositionhas now become the familiar notion of matrix mul-tiplication, that is ?
is ?:Verb??
?SbjIn general and for words with tensors of orderhigher than two, ?
becomes a generalisation of ?,referred to by tensor contraction, see e.g.
Kartsak-lis and Sadrzadeh (2013).
Since the creation andmanipulation of tensors of order higher than 2 isdifficult, one can work with simplified versions oftensors, faithful to their underlying mathematicalbasis; these have found intuitive interpretations,e.g.
see Grefenstette and Sadrzadeh (2011a), Kart-saklis and Sadrzadeh (2014).
In such cases, ?
be-comes a combination of a range of operations suchas ?, ?, , and +.Specific models In the current paper we will ex-periment with a variety of models.
In Table 2, wepresent these models in terms of their composi-tion operators and a reference to the main paper inwhich each model was introduced.
For the sim-ple compositional models the sentence is a stringof any number of words; for the grammar-basedmodels, we consider simple transitive sentences?Sbj Verb Obj?
and introduce the following abbre-viations for the concrete method used to build atensor for the verb:1.
Verb is a verb matrix computed using the for-mula?i???Sbji???
?Obji, where???Sbjiand??
?Objiarethe subjects and objects of the verb across thecorpus.
These models are referred to by rela-tional (Grefenstette and Sadrzadeh, 2011a);they are generalisations of predicate seman-tics of transitive verbs, from pairs of individ-uals to pairs of vectors.
The models reducethe order 3 tensor of a transitive verb to anorder 2 tensor (i.e.
a matrix).2.?Verb is a verb matrix computed using the for-mula??
?Verb ???
?Verb, where??
?Verb is the distri-butional vector of the verb.
These models arereferred to by Kronecker, which is the termsometimes used to denote the outer prod-uct of tensors (Grefenstette and Sadrzadeh,2011b).
This models also reduces the order3 tensor of a transitive verb to an order 2 ten-sor.3.
The models of the last five lines of the tableuse the so-called Frobenius operators fromcategorical compositional distributional se-mantics (Kartsaklis et al., 2012) to expandthe relational matrices of verbs from order 2to order 3.
The expansion is obtained by ei-ther copying the dimension of the subject intothe space provided by the third tensor, hencereferred to by Copy-Sbj, or copying the di-mension of the object in that space, hence re-ferred to by Copy-Obj; furthermore, we cantake addition, multiplication, or outer productof these, which are referred to by Frobenius-Add, Frobenius-Mult, and Frobenius-Outer(Kartsaklis and Sadrzadeh, 2014).4 Semantic word spacesCo-occurrence-based vector space instantiationshave received a lot of attention from the scientificcommunity (refer to (Kiela and Clark, 2014; Pola-jnar and Clark, 2014) for recent studies).
We in-stantiate two co-occurrence-based vectors spaceswith different underlying corpora and weightingschemes.711Method Sentence Linear algebraic formula ReferenceAddition w1w2?
?
?wn??w1+?
?w2+ ?
?
?+?
?wnMitchell and Lapata (2008)Multiplication w1w2?
?
?wn??w1??w2?
?
??
?wnMitchell and Lapata (2008)Relational Sbj Verb Obj Verb (??Sbj??
?Obj) Grefenstette and Sadrzadeh (2011a)Kronecker Sbj Verb Obj V?erb (??Sbj??
?Obj) Grefenstette and Sadrzadeh (2011b)Copy object Sbj Verb Obj?
?Sbj (Verb??
?Obj) Kartsaklis et al.
(2012)Copy subject Sbj Verb Obj?
?Obj (VerbT??
?Sbj) Kartsaklis et al.
(2012)Frob.
add.
Sbj Verb Obj (?
?Sbj (Verb??
?Obj)) + (?
?Obj (VerbT??
?Sbj)) Kartsaklis and Sadrzadeh (2014)Frob.
mult.
Sbj Verb Obj (?
?Sbj (Verb??
?Obj)) (?
?Obj (VerbT??
?Sbj)) Kartsaklis and Sadrzadeh (2014)Frob.
outer Sbj Verb Obj (?
?Sbj (Verb???Obj))?
(?
?Obj (VerbT??
?Sbj)) Kartsaklis and Sadrzadeh (2014)Table 2: Compositional methods.GS11 Our first word space is based on a typ-ical configuration that has been used in the pastextensively for compositional distributional mod-els (see below for details), so it will serve as auseful baseline for the current work.
In this vec-tor space, the co-occurrence counts are extractedfrom the British National Corpus (BNC) (Leech etal., 1994).
As basis words, we use the most fre-quent nouns, verbs, adjectives and adverbs (POStags SUBST, VERB, ADJ and ADV in the BNCXML distribution2).
The vector space is lemma-tized, that is, it contains only ?canonical?
forms ofwords.In order to weight the raw co-occurrence counts,we use positive point-wise mutual information(PPMI).
The component value for a target wordt and a context word c is given by:PPMI(t, c) = max(0, logp(c|t)p(c))where p(c|t) is the probability of word c given tin a symmetric window of length 5 and p(c) is theprobability of c overall.Vector spaces based on point-wise mutual in-formation (or variants thereof) have been success-fully applied in various distributional and compo-sitional tasks; see e.g.
Grefenstette and Sadrzadeh(2011a), Mitchell and Lapata (2008), Levy et al.
(2014) for details.
PPMI has been shown toachieve state-of-the-art results (Levy et al., 2014)and is suggested by the review of Kiela and Clark(2014).
Our use here of the BNC as a corpusand the window length of 5 is based on previ-ous use and better performance of these param-eters in a number of compositional experiments(Grefenstette and Sadrzadeh, 2011a; Grefenstette2http://www.natcorp.ox.ac.uk/and Sadrzadeh, 2011b; Mitchell and Lapata, 2008;Kartsaklis et al., 2012).KS14 In this variation, we train a vector spacefrom the ukWaC corpus3(Ferraresi et al., 2008),originally using as a basis the 2,000 content wordswith the highest frequency (but excluding a list ofstop words as well as the 50 most frequent contentwords since they exhibit low information content).The vector space is again lemmatized.
As contextwe consider a 5-word window from either side ofthe target word, while as our weighting scheme weuse local mutual information (i.e.
point-wise mu-tual information multiplied by raw counts).
In afurther step, the vector space was normalized andprojected onto a 300-dimensional space using sin-gular value decomposition (SVD).In general, dimensionality reduction producesmore compact word representations that are robustagainst potential noise in the corpus (Landauer andDumais, 1997; Sch?utze, 1997).
SVD has beenshown to perform well on a variety of tasks similarto ours (Baroni and Zamparelli, 2010; Kartsaklisand Sadrzadeh, 2014).Neural word embeddings (NWE) For our neu-ral setting, we used the skip-gram model ofMikolov et al.
(2013b) trained with negative sam-pling.
The specific implementation that was testedin our experiments was a 300-dimensional vec-tor space learned from the Google News corpusand provided by the word2vec4toolkit.
Fur-thermore, the gensim library (?Reh?u?rek and So-jka, 2010) was used for accessing the vectors.On the contrary with the previously described co-3http://wacky.sslmit.unibo.it/4https://code.google.com/p/word2vec/712occurrence vector spaces, this version is not lem-matized.The negative sampling method improves the ob-jective function of Equation 1 by introducing neg-ative examples to the training algorithm.
Assumethat the probability of a specific (c, t) pair of words(where t is a target word and c another word inthe same context with t), coming from the trainingdata, is denoted as p(D = 1|c, t).
The objectivefunction is then expressed as follows:?
(c,t)?Dp(D = 1|c, t) (2)That is, the goal is to set the model parameters ina way that maximizes the probability of all obser-vations coming from the training data.
Assumenow that D?is a set of randomly selected incorrect(c?, t?)
pairs that do not occur in D, then Equation2 above can be recasted in the following way:?
(c,t)?Dp(D = 1|c, t)?(c?,t?
)?D?p(D = 0|c?, t?
)(3)In other words, the model tries to distinguish a tar-get word t from random draws that come from anoise distribution.
In the implementation we usedfor our experiments, c is always selected froma 5-word window around t. More details aboutthe negative sampling approach can be found in(Mikolov et al., 2013b); the note of Goldberg andLevy (2014) also provides an intuitive explanationof the underlying setting.5 ExperimentsOur experiments explore the use of the vectorspaces above, together with the compositional op-erators described in Section 3, in a range of tasksall of which require semantic composition: verbsense disambiguation; sentence similarity; para-phrasing; and dialogue act tagging.5.1 DisambiguationWe use the transitive verb disambiguation datasetdescribed in Grefenstette and Sadrzadeh (2011a)5.This dataset consists of ambiguous transitive verbstogether with their arguments, landmark verbsthat identify one of the verb senses, and humanjudgements that specify how similar is the disam-biguated sense of the verb in the given context to5This and the sentence similarity dataset are avail-able at http://www.cs.ox.ac.uk/activities/compdistmeaning/one of the landmarks.
This is similar to the in-transitive dataset described in (Mitchell and Lap-ata, 2008).
Consider the sentence ?system meetsspecification?
; here, meets is the ambiguous tran-sitive verb, and system and specification are its ar-guments in this context.
Possible landmarks formeet are satisfy and visit; for this sentence, thehuman judgements show that the disambiguatedmeaning of the verb is more similar to the land-mark satisfy and less similar to visit.The task is to estimate the similarity of the senseof a verb in a context with a given landmark.
Toget our similarity measures, we compose the verbwith its arguments using one of our compositionalmodels; we do the same for the landmark and thencompute the cosine similarity of the two vectors.We evaluate the performance by averaging the hu-man judgements for the same verb, argument andlandmark entries, and calculating the Spearman?scorrelation between the average values and the co-sine scores.
As a baseline, we compare this withthe correlation produced by using only the verbvector, without composing it with its arguments.Table 3 shows the results of the experiment.NWE copy-object composition yields the best cor-relation with the human judgements, and top per-formance across all vector spaces and models witha Spearman ?
of 0.456.
For the KS14 space, thebest result comes from Frobenius outer (0.350),Method GS11 KS14 NWEVerb only 0.212 0.325 0.107Addition 0.103 0.275 0.149Multiplication 0.348 0.041 0.095Kronecker 0.304 0.176 0.117Relational 0.285 0.341 0.362Copy subject 0.089 0.317 0.131Copy object 0.334 0.331 0.456Frobenius add.
0.261 0.344 0.359Frobenius mult.
0.233 0.341 0.239Frobenius outer 0.284 0.350 0.375Table 3: Spearman ?
correlations of models withhuman judgements for the word sense disam-biguation task.
The best result (NWE Copy ob-ject) outperforms the nearest co-occurrence-basedcompetitor (KS14 Frobenius outer) with a statisti-cally significant difference (p < 0.05, t-test).713while the best operator for the GS11 space ispoint-wise multiplication (0.348).For simple point-wise composition, only mul-tiplicative GS11 and additive NWE improve overtheir corresponding verb-only baselines (but bothperform worse than the KS14 baseline).
Withtensor-based composition in co-occurrence basedspaces, copy subject yields lower results thanthe corresponding baselines.
Other compositionmethods, except Kronecker for KS14, improveover the verb-only baselines.
Finally we shouldnote that, despite the small training corpus, theGS11 vector space performs comparatively well:for instance, Kronecker model improves the pre-viously reported score of 0.28 (Grefenstette andSadrzadeh, 2011b).5.2 Sentence similarityIn this experiment we use the transitive sen-tence similarity dataset described in Kartsaklis andSadrzadeh (2014).
The dataset consists of transi-tive sentence pairs and a human similarity judge-ment6.
The task is to estimate a similarity measurebetween two sentences.
As in the disambiguationtask, we first compose word vectors to obtain sen-tence vectors, then compute cosine similarity ofthem.
We average the human judgements for iden-tical sentence pairs to compute a correlation withcosine scores.Table 4 shows the results.
Again, the bestperforming vector space is KS14, but this timewith addition: the Spearman ?
correlation scorewith averaged human judgements is 0.732.
Addi-tion was the means for the other vector spaces toachieve top performance as well: GS11 and NWEgot 0.682 and 0.689 respectively.None of the models in tensor-based composi-tion outperformed addition.
KS14 performs worsewith tensor-based methods here than in the othervector spaces.
However, GS11 and NWE, exceptcopy subject for both of them and Frobenius multi-plication for NWE, improved over their verb-onlybaselines.5.3 ParaphrasingIn this experiment we evaluate our vector spaceson a mainstream paraphrase detection task.6The textual content of this dataset is the same as that of(Kartsaklis and Sadrzadeh, 2013), the difference is that thedataset of (Kartsaklis and Sadrzadeh, 2014) has updated hu-man judgements whereas the previous dataset used the orig-inal annotations of the intransitive dataset of (Mitchell andLapata, 2010).Method GS11 KS14 NWEVerb only 0.491 0.602 0.561Addition 0.682 0.732 0.689Multiplication 0.597 0.321 0.341Kronecker 0.581 0.408 0.561Relational 0.558 0.437 0.618Copy subject 0.370 0.448 0.405Copy object 0.571 0.306 0.655Frobenius add.
0.566 0.460 0.585Frobenius mult.
0.525 0.226 0.387Frobenius outer 0.560 0.439 0.622Table 4: Results for sentence similarity.
Thereis no statistically significant difference betweenKS14 addition and NWE addition (the second bestresult).Specifically, we get classification results on theMicrosoft Research Paraphrase Corpus paraphrasecorpus (Dolan et al., 2005) working in the follow-ing way: we construct vectors for the sentencesof each pair; if the cosine similarity between thetwo sentence vectors exceeds a certain threshold,the pair is classified as a paraphrase, otherwise asnot a paraphrase.
For this experiment and that ofSection 5.4 below, we investigate only the addi-tion and point-wise multiplication compositionalmodels, since at their current stage of developmenttensor-based models can only efficiently handlesentences of fixed structure.
Nevertheless, thesimple point-wise compositional models still al-low for a direct comparison of the vector spaces,which is the main goal of this paper.For each vector space and model, a number ofdifferent thresholds were tested on the first 2000pairs of the training set, which we used as a de-velopment set; in each case, the best-performedthreshold was selected for a single run of our?classifier?
on the test set (1726 pairs).
Addition-ally, we evaluate the NWE model with a lemma-tized version of the corpus, so that the experimen-tal setup is maximally similar for all vector spaces.The results are shown in the first part of Table 5.Additive NWE gives the highest performance,with both lemmatized and un-lemmatized versionsoutperforming the GS11 and KS14 spaces.
Inthe un-lemmatized case, the accuracy of our sim-ple ?classifier?
(0.73) is close to state-of-the-artrange.
The state-of-the art result (0.77 accuracy714Co-occurrence Neural word embeddingsBaseline GS11 KS14 Unlemmatized LemmatizedModel Accuracy F-Score Accuracy F-Score Accuracy F-Score Accuracy F-Score Accuracy F-ScoreMSR addition0.65 0.750.62 0.79 0.70 0.80 0.73 0.82 0.72 0.81MSR multiplication 0.52 0.58 0.66 0.80 0.42 0.34 0.41 0.36SWDA addition0.60 0.580.35 0.35 0.40 0.35 0.63 0.60 0.44 0.40SWDA multiplication 0.32 0.16 0.39 0.33 0.58 0.53 0.43 0.38Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks.
All top resultssignificantly outperform corresponding nearest competitors (for accuracy): p < 0.05, ?2test.and 0.84 F-score7) by the time of this writing hasbeen obtained using 8 machine translation metricsand three constituent classifiers (Madnani et al.,2012).The multiplicative model gives lower resultsthan the additive model across all vector spaces.The KS14 vector space shows the steadiest per-formance, with a drop in accuracy of only 0.04and no drop in F-score, while for the GS11 andNWE spaces both accuracy and F-score experi-enced drops by more than 0.20.5.4 Dialogue act taggingAs our last experiment, we evaluate the wordspaces on a dialogue act tagging task (Stolcke etal., 2000) over the Switchboard corpus (Godfreyet al., 1992).
Switchboard is a collection of ap-proximately 2500 dialogs over a telephone line by500 speakers from the U.S. on predefined topics.8The experiment pipeline follows (Milajevs andPurver, 2014).
The input utterances are prepro-cessed so that the parts of interrupted utterancesare concatenated (Webb et al., 2005).
Disfluencymarkers and commas are removed from the utter-ance raw texts.
For GS11 and KS14 the utterancetokens are POS-tagged and lemmatized; for NWE,we test the vectors in both a lemmatized and anun-lemmatized version of the corpus.9We splitthe training and testing utterances as suggested byStolcke et al.
(2000).
Utterance vectors are thenobtained as in the previous experiments; they arereduced to 50 dimensions using SVD and a k-nearest-neighbour classifier is trained on these re-duced utterance vectors (the 5 closest neighboursby Euclidean distance are retrieved to make a clas-7F-scores use the standard definition F = 2(precision ?recall)/(precision + recall).8The dataset and a Python interface to it are availableat http://compprag.christopherpotts.net/swda.html9We use WordNetLemmatizer of the NLTK library(Bird, 2006).sification decision).
The results are shown in thesecond part of Table 5.Un-lemmatized NWE addition gave the best ac-curacy (0.63) and F-score (0.60) (averaged overtag classes), i.e.
similar results to (Milajevs andPurver, 2014)?although note that the dimension-ality of our NWE vectors is 10 times lower thantheirs.
Multiplicative NWE outperformed the cor-responding model in (Milajevs and Purver, 2014).In general, addition consistently outperforms mul-tiplication for all the models.
Lemmatizationdramatically lowers tagging accuracy: the lem-matized GS11, KS14 and NWE models performmuch worse than un-lemmatized NWE, suggest-ing that morphological features are important forthis task.6 DiscussionPrevious comparisons of co-occurrence-based andneural word vector representations vary widelyin their conclusions.
While Baroni et al.
(2014)conclude that ?context-predicting models obtaina thorough and resounding victory against theircount-based counterparts?, this seems to contra-dict, at least at the first consideration, the moreconservative conclusion of Levy et al.
(2014) that?analogy recovery is not restricted to neural wordembeddings [.
.
. ]
a similar amount of relationalsimilarities can be recovered from traditional dis-tributional word representations?
and the findingsof Blacoe and Lapata (2012) that ?shallow ap-proaches are as good as more computationally in-tensive alternatives?
on phrase similarity and para-phrase detection tasks.It seems clear that neural word embeddingshave an advantage when used in tasks for whichthey have been trained; our main questions hereare whether they outperform co-occurrence basedalternatives across the board; and which ap-proach lends itself better to composition usinggeneral mathematical operators.
To partially an-715swer this question, we can compare model be-haviour against the baselines in isolation.For the disambiguation and sentence similaritytasks the baseline is the similarity between verbsonly, ignoring the context?see above.
For theparaphrase task, we take the global vector-basedsimilarity reported in (Mihalcea et al., 2006): 0.65accuracy and 0.75 F-score.
For the dialogue acttagging task the baseline is the accuracy of thebag-of-unigrams model in (Milajevs and Purver,2014): 0.60.Sections 5.1 and 5.2 show that although the bestchoice of vector representation might vary, forsmall-scale tasks all methods give fairly compet-itive results.
The choice of compositional oper-ator seems to be more important and more task-specific: while a tensor-based operation (Frobe-nius copy-object) performs best for verb disam-biguation, the best result for sentence similarityis achieved by a simple additive model, with allother compositional methods behaving worse thanthe verb-only baseline in the KS14 case.
GS11 andNWE, on the other hand, outperform their base-lines with a number of compositional methods, al-though both of them achieve lower performancethan KS14 overall.Based on only small-scale experiment results,one could conclude that there is little significantdifference between the two ways of obtaining vec-tors.
GS11 and NWE show similar behaviour incomparison to their baselines, while it is possibleto tune a co-occurrence based vector space (KS14)and obtain the best result.
Large scale tasks revealanother pattern: the GS11 vector space, which be-haves stably on the small scale, drags behind theKS14 and NWE spaces in the paraphrase detec-tion task.
In addition, NWE consistently yieldsbest results.
Finally, only the NWE space was ableto provide adequate results on the dialogue act tag-ging task.
Table 6 summarizes model performancewith regard to baselines.7 ConclusionIn this work we compared the performance of twoco-occurrence-based semantic spaces with vectorslearned by a neural network in compositional set-tings.
We carried out two small-scale tasks (wordsense disambiguation and sentence similarity) andtwo large-scale tasks (paraphrase detection and di-alogue act tagging).Task GS11 KS14 NWEDisambiguation + + +Sentence similarity + ?
+Paraphrase ?
+ +Dialog act tagging ?
?
+Table 6: Summary of vector space performanceagainst baselines.
General improvement (caseswhere more than a half of the models perform bet-ter) and decrease with regard to a correspondingbaseline is respectively marked by + and ?.
Abold value means that the model gave the best re-sult in the task.On small-scale tasks, where the sentence struc-tures are predefined and relatively constrained,NWE gives better or similar results to count-basedvectors.
Tensor-based composition does not al-ways outperform simple compositional operators,but for most of the cases gives results within thesame range.On large-scale tasks, neural vectors are moresuccessful than the co-occurrence based alterna-tives.
However, this study does not reveal whetherthis is because of their neural nature, or just be-cause they are trained on a larger amount of data.The question of whether neural vectors outper-form co-occurrence vectors therefore requires fur-ther detailed comparison to be entirely resolved;our experiments suggest that this is indeed the casein large-scale tasks, but the difference in size andnature of the original corpora may be a confound-ing factor.
In any case, it is clear that the neuralvectors of word2vec package perform steadilyoff-the-shelf across a large variety of tasks.
Thesize of the vector space (3 million words) and theavailable code-base that simplifies the access tothe vectors, makes this set a good and safe choicefor experiments in the future.
Of course, even bet-ter performances can be achieved by training neu-ral language models specifically for a given task(see e.g.
Kalchbrenner et al.
(2014)).The choice of compositional operator (tensor-based or a simple point-wise operation) dependsstrongly on the task and dataset: tensor-basedcomposition performed best with the verb dis-ambiguation task, where the verb senses dependstrongly on the arguments of the verb.
However, itseems to depend less on the nature of the vectorsitself: in the disambiguation task, tensor-based716composition proved best for both co-occurrence-based and neural vectors; in the sentence similar-ity task, where point-wise operators proved best,this was again true across vector spaces.AcknowledgementsWe would like to thank the three anonymousreviewers for their fruitful comments.
Sup-port by EPSRC grant EP/F042728/1 is grate-fully acknowledged by Milajevs, Kartsaklis andSadrzadeh.
Purver is partly supported by Con-CreTe: the project ConCreTe acknowledges the fi-nancial support of the Future and Emerging Tech-nologies (FET) programme within the SeventhFramework Programme for Research of the Eu-ropean Commission, under FET grant number611733.ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages1183?1193.
Association for Computational Linguis-tics.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proceedingsof the 52nd Annual Meeting of the Association forComputational Linguistics, volume 1.Yoshua Bengio, Holger Schwenk, Jean-S?ebastienSen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
InInnovations in Machine Learning, pages 137?186.Springer.Steven Bird.
2006.
NLTK: the natural languagetoolkit.
In Proceedings of the COLING/ACL on In-teractive presentation sessions, pages 69?72.
Asso-ciation for Computational Linguistics.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for semanticcomposition.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 546?556.
Association for Compu-tational Linguistics.Johan Bos and Malte Gabsdil.
2000.
First-order infer-ence and the interpretation of questions and answers.Proceedings of Gotelog, pages 43?50.Johan Bos.
2008.
Wide-coverage semantic analy-sis with boxer.
In Johan Bos and Rodolfo Del-monte, editors, Semantics in Text Processing.
STEP2008 Conference Proceedings, Research in Compu-tational Semantics, pages 277?286.
College Publi-cations.N.
Bourbaki.
1989.
Commutative Algebra: Chapters1-7.
Srpinger Verlag, Berlin/New York.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical foundations for a com-positional distributional model of meaning.
CoRR,abs/1003.4394.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th international conference onMachine learning, pages 160?167.
ACM.Ann Copestake, Dan Flickinger, Carl Pollard, andIvan A Sag.
2005.
Minimal recursion semantics:An introduction.
Research on Language and Com-putation, 3(2-3):281?332.Bill Dolan, Chris Brockett, and Chris Quirk.
2005.
Mi-crosoft research paraphrase corpus.
Retrieved May,29:2013.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukWaC, a very large web-derived corpus of English.In Proceedings of the 4th Web as Corpus Workshop(WAC-4) Can we beat Google, pages 47?54.Gottlob Frege.
1892.
On sense and reference.
Ludlow(1997), pages 563?584.John J Godfrey, Edward C Holliman, and Jane Mc-Daniel.
1992.
Switchboard: Telephone speech cor-pus for research and development.
In Acoustics,Speech, and Signal Processing, 1992.
ICASSP-92.,1992 IEEE International Conference on, volume 1,pages 517?520.
IEEE.Yoav Goldberg and Omer Levy.
2014. word2vecExplained: deriving Mikolov et al.
?s negative-sampling word-embedding method.
arXiv preprintarXiv:1402.3722.Edward Grefenstette and Mehrnoosh Sadrzadeh.2011a.
Experimental support for a categorical com-positional distributional model of meaning.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 1394?1404.Association for Computational Linguistics.Edward Grefenstette and Mehrnoosh Sadrzadeh.2011b.
Experimenting with transitive verbs in a Dis-CoCat.
In Proceedings of the GEMS 2011 Work-shop on GEometrical Models of Natural LanguageSemantics, pages 62?66, Edinburgh, UK, July.
As-sociation for Computational Linguistics.Z.S.
Harris.
1954.
Distributional structure.
Word.717Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentconvolutional neural networks for discourse compo-sitionality.
In Proceedings of the Workshop on Con-tinuous Vector Space Models and their Composition-ality, pages 119?126, Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
Proceedings of the 52nd An-nual Meeting of the Association for ComputationalLinguistics, June.Dimitri Kartsaklis and Mehrnoosh Sadrzadeh.
2013.Prior disambiguation of word tensors for construct-ing sentence vectors.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing (EMNL), pages 1590?1601, Seat-tle, USA, October.
Association for ComputationalLinguistics.Dimitri Kartsaklis and Mehrnoosh Sadrzadeh.
2014.
Astudy of entanglement in a categorical framework ofnatural language.
In Proceedings of the 11th Work-shop on Quantum Physics and Logic (QPL), Kyoto,Japan, June.Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and StephenPulman.
2012.
A unified sentence space forcategorical distributional-compositional semantics:Theory and experiments.
In Proceedings of COL-ING 2012: Posters, pages 549?558, Mumbai, India,December.
The COLING 2012 Organizing Commit-tee.Douwe Kiela and Stephen Clark.
2014.
A systematicstudy of semantic vector space model parameters.In Proceedings of the 2nd Workshop on Continu-ous Vector Space Models and their Compositionality(CVSC), pages 21?30, Gothenburg, Sweden, April.Association for Computational Linguistics.T.
Landauer and S. Dumais.
1997.
A Solutionto Plato?s Problem: The Latent Semantic AnalysisTheory of Acquision, Induction, and Representationof Knowledge.
Psychological Review.Geoffrey Leech, Roger Garside, and Michael Bryant.1994.
Claws4: the tagging of the british nationalcorpus.
In Proceedings of the 15th conferenceon Computational linguistics-Volume 1, pages 622?628.
Association for Computational Linguistics.Omer Levy, Yoav Goldberg, and Israel Ramat-Gan.2014.
Linguistic regularities in sparse and explicitword representations.
In Proceedings of the Eigh-teenth Conference on Computational Natural Lan-guage Learning, Baltimore, Maryland, USA, June.Association for Computational Linguistics.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Re-examining machine translation metricsfor paraphrase identification.
In Proceedings of the2012 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 182?190.
Asso-ciation for Computational Linguistics.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In AAAI, vol-ume 6, pages 775?780.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013c.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL-HLT, pages 746?751.Dmitrijs Milajevs and Matthew Purver.
2014.
Inves-tigating the contribution of distributional semanticinformation for dialogue act classification.
In Pro-ceedings of the 2nd Workshop on Continuous VectorSpace Models and their Compositionality (CVSC),pages 40?47, Gothenburg, Sweden, April.
Associa-tion for Computational Linguistics.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedingsof ACL-08: HLT, pages 236?244.
Association forComputational Linguistics.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1439.Richard Montague.
1970.
Universal grammar.
Theo-ria, 36(3):373?398.Tamara Polajnar and Stephen Clark.
2014.
Improv-ing distributional semantic vectors through contextselection and normalisation.
In Proceedings of the14th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 230?238, Gothenburg, Sweden, April.
Association forComputational Linguistics.Radim?Reh?u?rek and Petr Sojka.
2010.
SoftwareFramework for Topic Modelling with Large Cor-pora.
In Proceedings of the LREC 2010 Workshopon New Challenges for NLP Frameworks, pages 45?50, Valletta, Malta, May.
ELRA.
http://is.muni.cz/publication/884893/en.Hinrich Sch?utze.
1997.
Ambiguity resolution in natu-ral language learning.
csli.
Stanford, CA, 4:12?36.718Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211.
Association for Computational Linguis-tics.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Carol Van Ess-Dykema, Rachel Martin, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26(3):339?373.Peter D Turney, Patrick Pantel, et al.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of artificial intelligence research,37(1):141?188.Peter D Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.Nick Webb, Mark Hepple, and Yorick Wilks.
2005.Dialogue act classification based on intra-utterancefeatures.
In Proceedings of the AAAI Workshop onSpoken Language Understanding.
Citeseer.719
