Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 112?119, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsRepresentational Bias in Unsupervised Learning of Syllable StructureSharon Goldwater and Mark JohnsonDepartment of Cognitive and Linguistic SciencesBrown UniversityProvidence, RI 02912{Sharon Goldwater,Mark Johnson}@brown.eduAbstractUnsupervised learning algorithms basedon Expectation Maximization (EM) areoften straightforward to implement andprovably converge on a local likelihoodmaximum.
However, these algorithms of-ten do not perform well in practice.
Com-mon wisdom holds that they yield poorresults because they are overly sensitiveto initial parameter values and easily getstuck in local (but not global) maxima.We present a series of experiments indi-cating that for the task of learning sylla-ble structure, the initial parameter weightsare not crucial.
Rather, it is the choice ofmodel class itself that makes the differ-ence between successful and unsuccess-ful learning.
We use a language-universalrule-based algorithm to find a good set ofparameters, and then train the parameterweights using EM.
We achieve word ac-curacy of 95.9% on German and 97.1% onEnglish, as compared to 97.4% and 98.1%respectively for supervised training.1 IntroductionThe use of statistical methods in computational lin-guistics has produced advances in tasks such as pars-ing, information retrieval, and machine translation.However, most of the successful work to date hasused supervised learning techniques.
Unsupervisedalgorithms that can learn from raw linguistic data,as humans can, remain a challenge.
In a statisticalframework, one method that can be used for unsu-pervised learning is to devise a probabilistic modelof the data, and then choose the values for the modelparameters that maximize the likelihood of the dataunder the model.If the model contains hidden variables, there isoften no closed-form expression for the maximumlikelihood parameter values, and some iterative ap-proximation method must be used.
ExpectationMaximization (EM) (Neal and Hinton, 1998) isone way to find parameter values that at least lo-cally maximize the likelihood for models with hid-den variables.
EM is attractive because at eachiteration, the likelihood of the data is guaranteednot to decrease.
In addition, there are efficientdynamic-programming versions of the EM algo-rithm for several classes of models that are importantin computational linguistics, such as the forward-backward algorithm for training Hidden MarkovModels (HMMs) and the inside-outside algorithmfor training Probabilistic Context-Free Grammars(PCFGs).Despite the advantages of maximum likelihoodestimation and its implementation via various in-stantiations of the EM algorithm, it is widely re-garded as ineffective for unsupervised languagelearning.
Merialdo (1994) showed that with onlya tiny amount of tagged training data, supervisedtraining of an HMM part-of-speech tagger outper-formed unsupervised EM training.
Later results (e.g.Brill (1995)) seemed to indicate that other methodsof unsupervised learning could be more effective (al-though the work of Banko and Moore (2004) sug-gests that the difference may be far less than previ-112ously assumed).
Klein and Manning (2001; 2002)recently achieved more encouraging results using anEM-like algorithm to induce syntactic constituentgrammars, based on a deficient probability model.It has been suggested that EM often yield poorresults because it is overly sensitive to initial param-eter values and tends to converge on likelihood max-ima that are local, but not global (Carroll and Char-niak, 1992).
In this paper, we present a series ofexperiments indicating that for the task of learninga syllable structure grammar, the initial parameterweights are not crucial.
Rather, it is the choice ofthe model class, i.e., the representational bias, thatmakes the difference between successful and unsuc-cessful learning.In the remainder of this paper, we first describethe task itself and the structure of the two differ-ent classes of models we experimented with.
Wethen present a deterministic algorithm for choosinga good set of parameters for this task.
The algo-rithm is based on language-universal principles ofsyllabification, but produces different parameters foreach language.
We apply this algorithm to Englishand German data, and describe the results of exper-iments using EM to learn the parameter weights forthe resulting models.
We conclude with a discussionof the implications of our experiments.2 Statistical Parsing of Syllable StructureKnowledge of syllable structure is important forcorrect pronunciation of spoken words, since cer-tain phonemes may be pronounced differently de-pending on their position in the syllable.
A num-ber of different supervised machine learning tech-niques have been applied to the task of automaticsyllable boundary detection, including decision-treeclassifiers (van den Bosch et al, 1998), weightedfinite state transducers (Kiraz and Mo?bius, 1998),and PCFGs (Mu?ller, 2001; Mu?ller, 2002).
The re-searchers presenting these systems have generallyargued from the engineering standpoint that sylla-ble boundary detection is useful for pronunciation ofunknown words in text-to-speech systems.
Our mo-tivation is a more scientific one: we are interested inthe kinds of procedures and representations that canlead to successful unsupervised language learning inboth computers and humans.Our work has some similarity to that of Mu?ller,who trains a PCFG of syllable structure from acorpus of words with syllable boundaries marked.We, too, use a model defined by a grammar to de-scribe syllable structure.1 However, our work dif-fers from Mu?ller?s in that it focuses on how to learnthe model?s parameters in an unsupervised manner.Several researchers have worked on unsupervisedlearning of phonotactic constraints and word seg-mentation (Elman, 2003; Brent, 1999; Venkatara-man, 2001), but to our knowledge there is no pre-viously published work on unsupervised learning ofsyllable structure.In the work described here, we experimented withtwo different classes of models of syllable structure.Both of these model classes are presented as PCFGs.The first model class, described in Mu?ller (2002),encodes information about the positions within aword or syllable in which each phoneme is likelyto appear.
In this positional model, each syllableis labeled as initial (I), medial (M), final (F), or asthe one syllable in a monosyllabic word (O).
Syl-lables are broken down into an optional onset (theinitial consonant or consonant cluster) followed by arhyme.
The rhyme consists of a nucleus (the vowel)followed by an optional coda consonant or cluster.Each phoneme is labeled with a preterminal cate-gory of the form CatPos.x.y, where Cat ?
{Ons,Nuc, Cod}, Pos ?
{I, M, F, O}, x is the positionof a consonant within its cluster, and y is the totalnumber of consonants in the cluster.
x and y are un-used when Cat = Nuc, since all nuclei consist of asingle vowel.
See Fig.
1 for an example parse.Rather than directly encoding positional infor-mation, the second model class we investigate (thebigram model) models statistical dependencies be-tween adjacent phonemes and adjacent syllables.In particular, each onset or coda expands directlyinto one or more terminal phonemes, thus capturingthe ordering dependencies between consonants in acluster.
Also, the shape of each syllable (whether itcontains an onset or coda) depends on the shape ofthe previous syllable, so that the model can learn,for example, that syllables ending in a coda shouldbe followed by syllables with an onset.2 This kind1We follow Mu?ller in representing our models as PCFGs be-cause this representation is easy to present.
The languages gen-erated by these PCFGs are in fact regular, and it is straightfor-ward to transform the PCFGs into equivalent regular grammars.2 Many linguists believe that, cross-linguistically, a poten-113WordSylIRhyINucI@SylMOnsMOnsM.1.2gOnsM.2.2rRhyMNucMiSylFOnsFOnsF.1.1mRhyFNucF@CodFCodF.1.2nCodF.2.2tWordWdNSylNNuc@WdONSylONOnsg rNuciWdONSylONCOnsmNuc@Codn tFigure 1: Positional analysis (left) and bigram analysis (right) of the word agreement.
Groups of terminalsdominated by a Syl* node constitute syllables.
Terminals appear in the SAMPA encoding of IPA used byCELEX.of bigram dependency between syllables is modeledusing rules of the form WdX ?
SylX WdY , whereX and Y are drawn from the set of possible combi-nations of onset, nucleus, and coda in a syllable: {N,ON, NC, ONC}.
Each SylX category has only oneexpansion.
See Fig.
1 for an example.With respect to either of these two model classes,each way of assigning syllable boundaries to a wordcorresponds to exactly one parse of that word.
Thismakes it simple to train the models from a corpus inwhich syllable boundaries are provided, as in Mu?ller(2001).
We used two different corpora for our exper-iments, one German (from the ECI corpus of news-paper text) and one English (from the Penn WSJcorpus).
Each corpus was created by convertingthe orthographic forms in the original text into theirphonemic transcriptions using the CELEX database(Baayen et al, 1995).
CELEX includes syllableboundaries, which we used for supervised trainingand for evaluation.
Any words in the original textsthat were not listed in CELEX were discarded, sinceone of our goals is to compare supervised and un-supervised training.3 From the resulting phonemiccorpora, we created a training set of 20,000 tokensand a test set of 10,000 tokens.
Using standard max-imum likelihood supervised training procedures, weobtained similar results for models from the twomodel classes.
In German, word accuracy (i.e.
thetially ambiguous consonant, such as the b in saber, is alwayssyllabified as the onset of the second syllable rather than thecoda of the first.
We discuss this point further in Section 3.3Due to the nature of the corpora, the percentage of wordsdiscarded was fairly high: 35.6% of the English tokens (pri-marily proper nouns, acronyms, and numerals, with a smallernumber of morphologically complex words) and 26.7% of theGerman tokens (with compound words making up a somewhatlarger portion of these discards).percentage of words with no syllabification errors)was 97.4% for the bigram model and 97.2% for thepositional model,4 while in English it was 98.1%and 97.6% respectively.
These results for Englishare in line with previous reported results using othersupervised learning techniques, e.g.
van den Boschet al (1998).
Since many of the words in the data aremonosyllabic (49.1% in German, 61.2% in English)and therefore contain no ambiguous syllable bound-aries, we also calculated the multisyllabic word ac-curacy.
This was 94.9% (bigram) and 94.5% (posi-tional) in German, and 95.2% (bigram) and 93.8%(positional) in English.3 Categorical Parsing of Syllable StructureIn the previous section, we described two differentmodel classes and showed that the maximum like-lihood estimates with supervised training data yieldgood models of syllable structure.
In moving to un-supervised learning, however, there are two prob-lems that need to be addressed: exactly what class ofmodels do we want to consider (i.e., what kinds ofrules should the model contain), and how should weselect a particular model from that class (i.e., whatweights should the rules have)?
We take as our so-lution to the latter problem the most straightforwardapproach; namely, maximum likelihood estimationusing EM.
This leaves us with the question of howto choose a set of parameters in the first place.
In thissection, we describe an algorithm based on two fun-damental phonological principles that, when given aset of data from a particular language, will produce a4Mu?ller reports slightly lower results of 96.88% on Germanusing the same positional model.
We have no explanation forthis discrepancy.114set of rules appropriate to that language.
These rulescan then be trained using EM.Given a particular rule schema, it is not imme-diately clear which of the possible rules should ac-tually be included in the model.
For example, inthe bigram model, should we start off with the ruleOns ?
k n?
This rule is unnecessary for English,and could lead to incorrect parses of words suchas weakness.
But /kn/ is a legal onset in German,and since we want an algorithm that is prepared tolearn any language, disallowing /kn/ as an onset outof hand is unacceptable.
On the other hand, the setof all combinatorially possible consonant clusters isinfinite, and even limiting ourselves to clusters actu-ally seen in the data for a particular language yieldsextremely unlikely-sounding onsets like /lkj/ (calcu-late) and /bst/ (substance).
Ideally, we should limitthe set of rules to ones that are likely to actually beused in the language of interest.The algorithm we have developed for produc-ing a set of language-appropriate rules is essentiallya simple categorical (i.e., non-statistical) syllableparser based on the principles of onset maximiza-tion and sonority sequencing (Blevins, 1995).
Onsetmaximization is the idea that in word-medial conso-nant clusters, as many consonants as possible (giventhe phonotactics of the language) should be assignedto onset position.
This idea is widely accepted andhas been codified in Optimality Theory (Prince andSmolensky, 1993) by proposing the existence of auniversal preference for syllables with onsets.5In addition to onset maximization, our categoricalparser follows the principle of sonority sequencingwhenever possible.
This principle states that, withina syllable, segments that are closer to the nucleusshould be higher in sonority than segments that arefurther away.
Vowels are considered to be the mostsonorous segments, followed by glides (/j/, /w/), liq-uids (/l/, /r/), nasals (/n/, /m/, /N/), fricatives (/v/,/s/, /T/, .
.
.
), and stops (/b/, /t/, /k/, .
.
.
).
Given a5An important point, which we return to in Section 5, isthat exceptions to onset maximization may occur at morphemeboundaries.
Some linguists also believe that there are addi-tional exceptions in certain languages (including English andGerman), where stressed syllables attract codas.
Under this the-ory, the correct syllabification for saber would not be sa.ber, butrather sab.er, or possibly sa[b]er, where the [b] is ambisyllabic.Since the syllable annotations in the CELEX database followsimple onset maximization, we take that as our approach as welland do not consider stress when assigning syllable boundaries.cluster of consonants between two syllable nuclei,sonority sequencing states that the syllable boundaryshould occur either just before or just after the con-sonant with lowest sonority.
Combining this princi-ple with onset maximization predicts that the bound-ary should fall before the lowest-sonority segment.Predicting syllable boundaries in this way is notfoolproof.
In some cases, clusters that are predictedby sonority sequencing to be acceptable are in factillegal in some languages.
The illegal English on-set cluster kn is a good example.
In other cases,such as the English onset str, clusters are alloweddespite violating sonority sequencing.
These mis-matches between universal principles and language-specific phonotactics lead to errors in the predic-tions of the categorical parser, such as wea.kness andins.tru.ment.
In addition, certain consonant clusterslike bst (as in substance) may contain more thanone minimum sonority point.
To handle these cases,the categorical parser follows onset maximizationby adding any consonants occurring between thetwo minima to the onset of the second syllable:sub.stance.Not surprisingly, the categorical parser does notperform as well as the supervised statistical parser:only 92.7% of German words and 94.9% of Englishwords (85.7% and 86.8%, respectively, of multisyl-labic words) are syllabified correctly.
However, amore important result of parsing the corpus usingthe categorical parser is that its output can be usedto define a model class (i.e., a set of PCFG rules)from which a model can be learned using EM.Specifically, our model class contains the set ofrules that were proposed at least once by the cat-egorical parser in its analysis of the training cor-pus; in the EM experiments described below, therule probabilities are initialized to their frequencyin the categorical parser?s output.
Due to the mis-takes made by the categorical parser, there will besome rules, like Ons ?
k n in English, that are notpresent in the model trained on the true syllabifica-tion, but many possible but spurious rules, such asOns ?
b s t, will be avoided.
Although clusters thatviolate sonority sequencing tend to be avoided bythe categorical parser, it does find examples of thesetypes of clusters at the beginnings and endings ofwords, as well as occasionally word-medially (as insub.stance).
This means that many legal clusters that115Bigram Positionalall multi all multiCP 92.7 85.7 92.7 85.7CP + EM 95.9 91.9 91.8 84.0CP-U + EM 95.9 91.9 92.0 84.4supervised 97.4 94.9 97.2 94.5SP + EM 71.6 44.3 94.4 89.1SP-U + EM 71.6 44.3 94.4 89.0Table 1: Results for German: % of all words (ormultisyllabic words) correctly syllabified.violate sonority sequencing will also be included inthe set of rules found by this procedure, althoughtheir probabilities may be considerably lower thanthose of the supervised model.
In the following sec-tion, we show that these differences in rule probabil-ities are unimportant; in fact, it is not the rule prob-abilities estimated from the categorical parser?s out-put, but only the set of rules itself that matters forsuccessful task performance.4 ExperimentsIn this section, we present a series of experiments us-ing EM to learn a model of syllable structure.
All ofour experiments use the same German and English20,000-word training corpora and 10,000-word test-ing corpora as described in Section 2.6For our first experiment, we ran the categoricalparser on the training corpora and estimated a modelfrom the parse trees it produced, as described in theprevious section.
This is essentially a single stepof Viterbi EM training.
We then continued to trainthe model by running (standard) EM to convergence.Results of this experiment with Categorical Pars-ing + EM (CP + EM) are shown in Tables 1 and2.
For both German and English, using this learn-ing method with the bigram model yields perfor-mance that is much better than the categorical parseralone, though not quite as good as the fully super-vised regime.
On the other hand, training a posi-tional model from the categorical parser?s output andthen running EM causes performance to degrade.To determine whether the good performance of6Of course, for unsupervised learning, it is not necessary touse a distinct testing corpus.
We did so in order to use the sametesting corpus for both supervised and unsupervised learningexperiments, to ensure fair comparison of results.Bigram Positionalall multi all multiCP 94.9 86.8 94.9 86.8CP + EM 97.1 92.6 94.1 84.9CP-U + EM 97.1 92.6 94.1 84.9supervised 98.1 95.2 97.6 93.8SP + EM 86.0 64.0 96.5 90.9SP-U + EM 86.0 64.0 67.6 16.5Table 2: Results for English.the bigram model was simply due to good initial-ization of the parameter weights, we performed asecond experiment.
Again starting with the set ofrules output by the categorical parser, we initializedthe rule weights to the uniform distribution.
The re-sults of this experiment (CP-U + EM) show that forthe class of bigram models, the performance of thefinal model found by EM does not depend on theinitial rule probabilities.
Performance within the po-sitional model framework does depend on the initialrule probabilities, since accuracy in German is dif-ferent for the two experiments.As we have pointed out, the rules found by thecategorical parser are not exactly the same as therules found using supervised training.
This raisesthe question of whether the difference in perfor-mance between the unsupervised and supervised bi-gram models is due to differences in the rules.
Toaddress this question, we performed two additionalexperiments.
First, we simply ran EM starting fromthe model estimated from supervised training data.Second, we kept the set of rules from the supervisedtraining data, but reinitialized the probabilities to auniform distribution before running EM.
The resultsof these experiments are shown as SP + EM and SP-U + EM, respectively.
Again, performance of thebigram model is invariant with respect to initial pa-rameter values, while the performance of the posi-tional model is not.
Interestingly, the performanceof the bigram model in these two experiments is farworse than in the CP experiments.
This result iscounterintuitive, since it would seem that the modelrules found by the supervised system are the opti-mal rules for this task.
In the following section, weexplain why these rules are not, in fact, the optimalrules for unsupervised learning, as well as why webelieve the bigram model performs so much better116than the positional model in the unsupervised learn-ing situation.5 DiscussionThe results of our experiments raise two interestingquestions.
First, when starting from the categoricalparser?s output, why does the bigram model improveafter EM training, while the positional model doesnot?
And second, why does applying EM to the su-pervised bigram model lead to worse performancethan applying it to the model induced from the cate-gorical parser?To answer the first question, notice that one dif-ference between the bigram model and the posi-tional model is that onsets and codas in the bigrammodel are modeled using the same set of parame-ters regardless of where in the word they occur.
Thismeans that the bigram model generalizes whatever itlearns about clusters at word edges to word-medialclusters (and, of course, vice versa).
Since the cate-gorical parser only makes errors word-medially, in-correct clusters are only a small percentage of clus-ters overall, and the bigram model can overcomethese errors by reanalyzing the word-medial clus-ters.
The errors that are made after EM trainingare mostly due to overgeneralization from clustersthat are very common at word edges, e.g.
predictingle.gi.sla.tion instead of le.gis.la.tion.In contrast to the bigram model, the positionalmodel does not generalize over different positionsof the word, which means that it learns and repeatsthe word-medial errors of the categorical parser.
Forexample, this model predicts /E.gzE.kju.tIv/ for ex-ecutive, just as the categorical parser does, although/gz/ is never attested in word-initial position.
In ad-dition, each segment in a cluster is generated in-dependently, which means clusters like /tl/ may beplaced together in an onset because /t/ is commonas the first segment of an onset, and /l/ is commonas the second.
While this problem exists even inthe supervised positional model, it is compoundedin the unsupervised version because of the errors ofthe categorical parser.The differences between these two models are anexample of the bias-variance trade-off in probabilis-tic modeling (Geman et al, 1992): models with lowbias will be able to fit a broad range of observationsfairly closely, but slight changes in the observed datawill cause relatively large changes in the inducedmodel.
On the other hand, models with high biasare less sensitive to changes in the observed data.Here, the bigram model induced from the categor-ical parser has a relatively high bias: regardless ofthe parameter weights, it will be a poor model ofdata where word-medial onsets and codas are verydifferent from those at word edges, and it cannotmodel data with certain onsets such as /vp/ or /tz/at all because the rules Ons ?
v p and Ons ?
t zare simply absent.
The induced positional modelcan model both of these situations, and can fit thetrue parses more closely as well (as evidenced bythe fact that the likelihood of the data under the su-pervised positional model is higher than the like-lihood under the supervised bigram model).
As aresult, however, it is more sensitive to the initialparameter weights and learns to recreate the errorsproduced by the categorical parser.
This sensitiv-ity to initial parameter weights also explains the ex-tremely poor performance of the positional modelin the SP-U + EM experiment on English.
Becausethe model is so unconstrained, in this case it finds acompletely different local maximum (not the globalmaximum) which more or less follows coda max-imization rather than onset maximization, yieldingsyllabifications like synd.ic.ate and tent.at.ive.ly.The concept of representational bias can also ex-plain why applying EM to the supervised bigrammodel performs so poorly.
Examining the model in-duced from the categorical parser reveals that, notsurprisingly, it contains more rules than the super-vised bigram model.
This is because the categori-cal parser produces a wider range of onsets and co-das than there are in the true parses.
However, theinduced model is not a superset of the supervisedmodel.
There are four rules (three in English) thatoccur in the supervised model but not the inducedmodel.
These are the rules that allow words whereone syllable contains a coda and the following syl-lable has no onset.
These are never produced by thecategorical parser because of its onset-maximizationprinciple.
However, it turns out that a very small per-centage of words do follow this pattern (about .14%of English tokens and 1.1% of German tokens).
InEnglish, these examples seem to consist entirely ofwords where the unusual syllable boundary occurs ata morpheme boundary (e.g.
un.usually, dis.appoint,117week.end, turn.over).
In German, all but a handful ofexamples occur at morpheme boundaries as well.7The fact that the induced bigram model is unableto model words with codas followed by no onset isa very strong bias, but these words are so infrequentthat the model can still fit the data quite well.
Themissing rules have no effect on the accuracy of theparser, because in the supervised model the proba-bilities on the rules allowing these kinds of wordsare so low that they are never used in the Viterbiparses anyway.
The problem is that if these rules areincluded in the model prior to running EM, they addseveral extra free parameters, and suddenly EM isable to reanalyze many of the words in the corpus tomake better use of these parameters.
It ends up pre-ferring certain segments and clusters as onsets andothers as codas, which raises the likelihood of thecorpus but leads to very poor performance.
Essen-tially, it seems that the presence of a certain kind ofmorpheme boundary is an additional parameter ofthe ?true?
model that the bigram model doesn?t in-clude.
Trying to account for the few cases where thisparameter matters requires introducing extra param-eters that allow EM too much freedom of analysis.It is far better to constrain the model, disallowingcertain rare analyses but enabling the model to learnsuccessfully in a way that is robust to variations ininitial conditions and idiosyncracies of the data.6 ConclusionWe make no claims that our learning system em-bodies a complete model of syllabification.
A fullmodel would need to account for the effects of mor-phological boundaries, as well as the fact that somelanguages allow resyllabification over word bound-aries.
Nevertheless, we feel that the results presentedhere are significant.
We have shown that, despiteprevious discouraging results (Carroll and Charniak,1992; Merialdo, 1994), it is possible to achieve goodresults using EM to learn linguistic structures in anunsupervised way.
However, the choice of modelparameters is crucial for successful learning.
Car-roll and Charniak, for example, generated all pos-7The exceptions in our training data were auserkoren ?cho-sen?, erobern ?capture?, and forms of erinnern ?remind?, all ofwhich were listed in CELEX as having a syllable boundary, butno morpheme boundary, after the first consonant.
Our knowl-edge of German is not sufficient to determine whether there issome other factor that can explain these cases.sible rules within a particular framework and reliedon EM to remove the ?unnecessary?
rules by lettingtheir probabilities go to zero.
We suggest that thisprocedure tends to yield models with low bias buthigh variance, so that they are extremely sensitiveto the small variations in expected rule counts thatoccur with different initialization weights.Our work suggests that using models with higherbias but lower variance may lead to much moresuccessful results.
In particular, we used univer-sal phonological principles to induce a set of ruleswithin a carefully chosen grammatical framework.We found that there were several factors that en-abled our induced bigram model to learn success-fully where the comparison positional model didnot:1.
The bigram model encodes bigram dependen-cies of syllable shape and disallows onset-lesssyllables following syllables with codas.2.
The bigram model does not distinguish be-tween different positions in a word, so it cangeneralize onset and coda sequences from wordedges to word-medial position.3.
The bigram model learns specific sequencesof legal clusters rather than information aboutwhich positions segments are likely to occur in.Notice that each of these factors imposes a con-straint on the kinds of data that can be modeled.
Wehave already discussed the fact that item 1 rules outthe correct syllabification of certain morphologicallycomplex words, but since our system currently hasno way to determine morpheme boundaries, it is bet-ter to do so than to introduce extra free parameters.One possible extension to this work would be to tryto incorporate morphological boundary information(either annotated or induced) into the model.A more interesting constraint is the one imposedby item 2, since in fact most languages do have somedifferences between the onsets and (especially) co-das allowed at word edges and within words.
How-ever, the proper way to handle this fact is not byintroducing completely independent parameters forinitial, medial, and final positions, since this allowsfar too much freedom.
It would be extremely sur-prising to find a language with one set of codas al-lowed word-internally, and a completely disjoint set118allowed word-finally.
In fact, the usual situation isthat word-internal onsets and codas are a subset ofthose allowed at word edges, and this is exactly whyusing word edges to induce our rules was successful.Considering language more broadly, it is com-mon to find patterns of linguistic phenomena withmany similarities but some differences as well.
Forsuch cases, adding extra parameters to a supervisedmodel often yields better performance, since theaugmented model can capture both primary and sec-ondary effects.
But it seems that, at least for thecurrent state of unsupervised learning, it is better tolimit the number of parameters and focus on thosethat capture the main effects in the data.
In our taskof learning syllable structure, we were able to usejust a few simple principles to constrain the modelsuccessfully.
For more complex tasks such as syn-tactic parsing, the space of linguistically plausiblemodels is much larger.
We feel that a research pro-gram integrating results from the study of linguisticuniversals, human language acquisition, and compu-tational modeling is likely to yield the most insightinto the kinds of constraints that are needed for suc-cessful learning.Ultimately, of course, we will want to be able tocapture not only the main effects in the data, butsome of the subtler effects as well.
However, webelieve that the way to do this is not by introducingcompletely free parameters, but by using a Bayesianprior that would enforce a degree of similarity be-tween certain parameters.
In the meantime, we haveshown that employing linguistic universals to deter-mine which set of parameters to include in a lan-guage model for syllable parsing allows us to useEM for learning the parameter weights in a success-ful and robust way.AcknowledgmentsWe would like to thank Eugene Charniak and ourcolleagues in BLLIP for their support and helpfulsuggestions.
This research was partially supportedby NSF awards IGERT 9870676 and ITR 0085940and NIMH award 1R0-IMH60922-01A2.ReferencesR.
Baayen, R. Piepenbrock, and L. Gulikers.
1995.
TheCELEX lexical database (release 2) [cd-rom].M.
Banko and R. Moore.
2004.
A study of unsupervised part-of-speech tagging.
In Proceedings of COLING ?04.J.
Blevins.
1995.
The syllable in phonological theory.
InJ.
Goldsmith, editor, the Handbook of Phonological Theory.Blackwell, Oxford.M.
Brent.
1999.
An efficient, probabilistically sound algorithmfor segmentation and word discovery.
Machine Learning,34:71?105.E.
Brill.
1995.
Unsupervised learning of disambiguation rulesfor part of speech tagging.
In Proceedings of the 3rd Work-shop on Very Large Corpora, pages 1?13.G.
Carroll and E. Charniak.
1992.
Two experiments on learningprobabilistic dependency grammars from corpora.
In Pro-ceedings of the AAAI Workshop on Statistically-Based Natu-ral Language Processing Techniques, San Jose, CA.J.
Elman.
2003.
Generalization from sparse input.
In Proceed-ings of the 38th Annual Meeting of the Chicago LinguisticSociety.S.
Geman, E. Bienenstock, and R. Doursat.
1992.
Neural net-works and the bias/variance dilemma.
Neural Computation,4:1?58.G.
A. Kiraz and B. Mo?bius.
1998.
Multilingual syllabifica-tion using weighted finite-state transducers.
In Proceedingsof the Third European Speech Communication AssociationWorkshop on Speech Synthesis.D.
Klein and C. Manning.
2001.
Distributional phrase struc-ture induction.
In Proceedings of the Conference on NaturalLanguage Learning, pages 113?120.D.
Klein and C. Manning.
2002.
A generative constituent-context model for improved grammar induction.
In Proceed-ings of the ACL.B.
Merialdo.
1994.
Tagging english text with a probabilisticmodel.
Computational Linguistics, 20(2):155?172.K.
Mu?ller.
2001.
Automatic detection of syllable boundariescombining the advantages of treebank and bracketed corporatraining.
In Proceedings of the ACL.K.
Mu?ller.
2002.
Probabilistic context-free grammars forphonology.
In Proceedings of the Workshop on Morpholog-ical and Phonological Learning at ACL.R.
Neal and G. Hinton, 1998.
A New View of the EM AlgorithmThat Justifies Incremental and Other Variants, pages 355?368.
Kluwer.A.
Prince and P. Smolensky.
1993.
Optimality theory: Con-straint interaction in generative grammar.
Technical ReportTR-2, Rutgers Center for Cognitive Science, Rutgers Univ.A.
van den Bosch, T. Weijters, and W. Daelemans.
1998.
Mod-ularity in inductively-learned word pronunciation systems.In New Methods in Language Processing and ComputationalLanguage Learning (NeMLaP3/CoNLL98).A.
Venkataraman.
2001.
A statistical model for word dis-covery in transcribed speech.
Computational Linguistics,27(3):351?372.119
