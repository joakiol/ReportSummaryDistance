Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 793?803,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLarge-Scale Cross-Document Coreference UsingDistributed Inference and Hierarchical ModelsSameer Singh?
Amarnag Subramanya?
Fernando Pereira?
Andrew McCallum??
Department of Computer Science, University of Massachusetts, Amherst MA 01002?
Google Research, Mountain View CA 94043sameer@cs.umass.edu, asubram@google.com, pereira@google.com, mccallum@cs.umass.eduAbstractCross-document coreference, the task ofgrouping all the mentions of each entity in adocument collection, arises in information ex-traction and automated knowledge base con-struction.
For large collections, it is clearlyimpractical to consider all possible groupingsof mentions into distinct entities.
To solvethe problem we propose two ideas: (a) a dis-tributed inference technique that uses paral-lelism to enable large scale processing, and(b) a hierarchical model of coreference thatrepresents uncertainty over multiple granular-ities of entities to facilitate more effective ap-proximate inference.
To evaluate these ideas,we constructed a labeled corpus of 1.5 milliondisambiguated mentions in Web pages by se-lecting link anchors referring to Wikipedia en-tities.
We show that the combination of thehierarchical model with distributed inferencequickly obtains high accuracy (with error re-duction of 38%) on this large dataset, demon-strating the scalability of our approach.1 IntroductionGiven a collection of mentions of entities extractedfrom a body of text, coreference or entity resolu-tion consists of clustering the mentions such thattwo mentions belong to the same cluster if andonly if they refer to the same entity.
Solutions tothis problem are important in semantic analysis andknowledge discovery tasks (Blume, 2005; Mayfieldet al, 2009).
While significant progress has beenmade in within-document coreference (Ng, 2005;Culotta et al, 2007; Haghighi and Klein, 2007;Bengston and Roth, 2008; Haghighi and Klein,2009; Haghighi and Klein, 2010), the larger prob-lem of cross-document coreference has not receivedas much attention.Unlike inference in other language processingtasks that scales linearly in the size of the corpus,the hypothesis space for coreference grows super-exponentially with the number of mentions.
Conse-quently, most of the current approaches are devel-oped on small datasets containing a few thousandmentions.
We believe that cross-document coref-erence resolution is most useful when applied to avery large set of documents, such as all the news ar-ticles published during the last 20 years.
Such a cor-pus would have billions of mentions.
In this paperwe propose a model and inference algorithms thatcan scale the cross-document coreference problemto corpora of that size.Much of the previous work in cross-documentcoreference (Bagga and Baldwin, 1998; Ravin andKazi, 1999; Gooi and Allan, 2004; Pedersen et al,2006; Rao et al, 2010) groups mentions into entitieswith some form of greedy clustering using a pair-wise mention similarity or distance function basedon mention text, context, and document-level statis-tics.
Such methods have not been shown to scale up,and they cannot exploit cluster features that cannotbe expressed in terms of mention pairs.
We providea detailed survey of related work in Section 6.Other previous work attempts to address some ofthe above concerns by mapping coreference to in-ference on an undirected graphical model (Culottaet al, 2007; Poon et al, 2008; Wellner et al, 2004;Wick et al, 2009a).
These models contain pair-wise factors between all pairs of mentions captur-ing similarity between them.
Many of these mod-els also enforce transitivity and enable features over793FilmmakerRapperBEIJING, Feb. 21?
Kevin Smith, who played the god of war in the "Xena"......
The Physiological Basis of Politics,?
by Kevin B. Smith, Douglas Oxley, Matthew Hibbing...The filmmaker Kevin Smith returns to the role of Silent Bob...Like Back in 2008, the Lions drafted Kevin Smith, even though Smith was badly...Firefighter Kevin Smith spent almost 20 years preparing for Sept. 11.
When he......shorthanded backfield in the wake of Kevin Smith's knee injury, and the addition of Haynesworth......were coming,'' said Dallas cornerback Kevin Smith.
''We just didn't know when......during the late 60's and early 70's, Kevin Smith worked with several local......the term hip-hop is attributed to Lovebug Starski.
What does it actually mean...Nothing could be more irrelevant to Kevin Smith's audacious ''Dogma'' than ticking off...CornerbackFirefighterActorRunning backAuthorFigure 1: Cross-Document Coreference Problem: Example mentions of ?Kevin Smith?
from New YorkTimes articles, with the true entities shown on the right.entities by including set-valued variables.
Exact in-ference in these models is intractable and a numberof approximate inference schemes (McCallum et al,2009; Rush et al, 2010; Martins et al, 2010) maybe used.
In particular, Markov chain Monte Carlo(MCMC) based inference has been found to workwell in practice.
However as the number of men-tions grows to Web scale, as in our problem of cross-document coreference, even these inference tech-niques become infeasible, motivating the need fora scalable, parallelizable solution.In this work we first distribute MCMC-based in-ference for the graphical model representation ofcoreference.
Entities are distributed across the ma-chines such that the parallel MCMC chains on thedifferent machines use only local proposal distribu-tions.
After a fixed number of samples on each ma-chine, we redistribute the entities among machinesto enable proposals across entities that were pre-viously on different machines.
In comparison tothe greedy approaches used in related work, ourMCMC-based inference provides better robustnessproperties.As the number of mentions becomes large, high-quality samples for MCMC become scarce.
Tofacilitate better proposals, we present a hierarchi-cal model.
We add sub-entity variables that repre-sent clusters of similar mentions that are likely tobe coreferent; these are used to propose compositejumps that move multiple mentions together.
Wealso introduce super-entity variables that representclusters of similar entities; these are used to dis-tribute entities among the machines such that similarentities are assigned to the same machine.
These ad-ditional levels of hierarchy dramatically increase theprobability of beneficial proposals even with a largenumber of entities and mentions.To create a large corpus for evaluation, we iden-tify pages that have hyperlinks to Wikipedia, and ex-tract the anchor text and the context around the link.We treat the anchor text as the mention, the con-text as the document, and the title of the Wikipediapage as the entity label.
Using this approach, 1.5million mentions were annotated with 43k entity la-bels.
On this dataset, our proposed model yields aB3 (Bagga and Baldwin, 1998) F1 score of 73.7%,improving over the baseline by 16% absolute (corre-sponding to 38% error reduction).
Our experimen-tal results also show that our proposed hierarchicalmodel converges much faster even though it containsmany more variables.2 Cross-document CoreferenceThe problem of coreference is to identify the sets ofmention strings that refer to the same underlying en-tity.
The identities and the number of the underlyingentities is not known.
In within-document corefer-ence, the mentions occur in a single document.
Thenumber of mentions (and entities) in each documentis usually in the hundreds.
The difficulty of the taskarises from a large hypothesis space (exponential inthe number of mentions) and challenge in resolv-ing nominal and pronominal mentions to the correctnamed mentions.
In most cases, named mentions794are not ambiguous within a document.
In cross-document coreference, the number of mentions andentities is in the millions, making the combinatoricseven more daunting.
Furthermore, naming ambigu-ity is much more common as the same string canrefer to multiple entities in different documents, anddistinct strings may refer to the same entity in differ-ent documents.We show examples of ambiguities in Figure 1.Resolving the identity of individuals with the samename is a common problem in cross-documentcoreference.
This problem is further complicatedby the fact that in some situations, these individ-uals may belong to the same field.
Another com-mon ambiguity is that of alternate names, in whichthe same entity is referred to by different names oraliases (e.g.
?Bill?
is often used as a substitute for?William?).
The figure also shows an example ofthe renaming ambiguity ?
?Lovebug Starski?
refersto ?Kevin Smith?, and this is an extreme form of al-ternate names.
Rare singleton entities (like the fire-fighter) that may appear only once in the whole cor-pus are also often difficult to isolate.2.1 Pairwise Factor ModelFactor graphs are a convenient representation for aprobability distribution over a vector of output vari-ables given observed variables.
The model that weuse for coreference represents mentions (M) and en-tities (E) as random variables.
Each mention cantake an entity as its value, and each entity takes a setof mentions as its value.
Each mention also has afeature vector extracted from the observed text men-tion and its context.
More precisely, the probabilityof a configuration E = e is defined byp(e) ?
exp?e?e{?m,n?e,n 6=m ?a(m,n)+?m?e,n/?e ?r(m,n)}where factor ?a represents affinity between men-tions that are coreferent according to e, and factor?r represents repulsion between mentions that arenot coreferent.
Different factors are instantiated fordifferent predicted configurations.
Figure 2 showsthe model instantiated with five mentions over a two-entity hypothesis.For the factor potentials, we use cosine sim-ilarity of mention context pairs (?mn) such thatm1m2m3m4m5e1e2Figure 2: Pairwise Coreference Model: Factorgraph for a 2-entity configuration of 5 mentions.Affinity factors are shown with solid lines, and re-pulsion factors with dashed lines.
?a(m,n) = ?mn ?
b and ?r(m,n) = ?
(?mn ?
b),where b is the bias.
While one can certainly makeuse of a more sophisticated feature set, we leave thisfor future work as our focus is to scale up inference.However, it should be noted that this approach isagnostic to the particular set of features used.
Aswe will note in the next section, we do not need tocalculate features between all pairs of mentions (aswould be prohibitively expensive for large datasets);instead we only compute the features as and whenrequired.2.2 MCMC-based InferenceGiven the above model of coreference, we seek themaximum a posteriori (MAP) configuration:e?
= argmaxe p(e)= argmaxe?e?e{?m,n?e,n 6=m ?a(m,n)+?m?e,n/?e ?r(m,n)}Computing e?
exactly is intractable due to thelarge space of possible configurations.1 Instead,we employ MCMC-based optimization to discoverthe MAP configuration.
A proposal function q isused to propose a change e?
to the current config-uration e. This jump is accepted with the followingMetropolis-Hastings acceptance probability:?
(e, e?)
= min(1,(p(e?
)p(e))1/t q(e)q(e?
))(1)1Number of possible entities is Bell(n) in the number ofmentions, i.e.
number of partitions of n items795where t is the annealing temperature parameter.MCMC chains efficiently explore the high-density regions of the probability distribution.
Byslowly reducing the temperature, we can decreasethe entropy of the distribution to encourage con-vergence to the MAP configuration.
MCMC hasbeen used for optimization in a number of relatedwork (McCallum et al, 2009; Goldwater and Grif-fiths, 2007; Changhe et al, 2004).The proposal function moves a randomly chosenmention l from its current entity es to a randomlychosen entity et.
For such a proposal, the log-modelratio is:logp(e?
)p(e)=?m?et?a(l,m) +?n?es?r(l, n)?
?n?es?a(l, n)?
?m?et?r(l,m) (2)Note that since only the factors between mention land mentions in es and et are involved in this com-putation, the acceptance probability of each proposalis calculated efficiently.In general, the model may contain arbitrarilycomplex set of features over pairs of mentions, withparameters associated with them.
Given labeleddata, these parameters can be learned by Percep-tron (Collins, 2002), which uses the MAP config-uration according to the model (e?).
There also existmore efficient training algorithms such as SampleR-ank (McCallum et al, 2009; Wick et al, 2009b) thatupdate parameters during inference.
However, weonly focus on inference in this work, and the onlyparameter that we set manually is the bias b, whichindirectly influences the number of entities in e?.
Un-less specified otherwise, in this work the initial con-figuration for MCMC is the singleton configuration,i.e.
all entities have a size of 1.This MCMC inference technique, which has beenused in McCallum and Wellner (2004), offers sev-eral advantages over other inference techniques: (a)unlike message-passing-methods, it does not requirethe full ground graph, (b) we only have to exam-ine the factors that lie within the changed entitiesto evaluate a proposal, and (c) inference may bestopped at any point to obtain the current best con-figuration.
However, the super exponential nature ofthe hypothesis space in cross-doc coreference ren-ders this algorithm computationally unsuitable forlarge scale coreference tasks.
In particular, fruit-ful proposals (that increase the model score) are ex-tremely rare, resulting in a large number of propos-als that are not accepted.
We describe methods tospeed up inference by 1) evaluating multiple pro-posal simultaneously (Section 3), and 2) by aug-menting our model with hierarchical variables thatenable better proposal distributions (Section 4).3 Distributed MAP InferenceThe key observation that enables distribution is thatthe acceptance probability computation of a pro-posal only examines a few factors that are not com-mon to the previous and next configurations (Eq.
2).Consider a pair of proposals, one that moves men-tion l from entity es to entity et, and the other thatmoves mention l?
from entity e?s to entity e?t.
Theset of factors to compute acceptance of the first pro-posal are factors between l and mentions in es andet, while the set of factors required to compute ac-ceptance of the second proposal lie between l?
andmentions in e?s and e?t.
Since these set of factorsare completely disjoint from each other, and the re-sulting configurations do not depend on each other,these two proposals are mutually-exclusive.
Differ-ent orders of evaluating such proposals are equiv-alent, and in fact, these proposals can be proposedand evaluated concurrently.
This mutual-exclusivityis not restricted only to pairs of proposals; a set ofproposals are mutually-exclusive if no two propos-als require the same factor for evaluation.Using this insight, we introduce the following ap-proach to distributed cross-document coreference.We divide the mentions and entities among multiplemachines, and propose moves of mentions betweenentities assigned to the same machine.
These jumpsare evaluated exactly and accepted without commu-nication between machines.
Since acceptance of amention?s move requires examining factors that liebetween other mentions in its entity, we ensure thatall mentions of an entity are assigned the same ma-chine.
Unless specified otherwise, the distribution isperformed randomly.
To enable exploration of thecomplete configuration space, rounds of samplingare interleaved by redistribution stages, in which theentities are redistributed among the machines (seeFigure 3).
We use MapReduce (Dean and Ghe-796DistributorInferenceInferenceFigure 3: Distributed MCMC-based Inference:Distributor divides the entities among the machines,and the machines run inference.
The process is re-peated by the redistributing the entities.mawat, 2004) to manage the distributed computa-tion.This approach to distribution is equivalent to in-ference with all mentions and entities on a singlemachine with a restricted proposer, but is fastersince it exploits independencies to propose multiplejumps simultaneously.
By restricting the jumps asdescribed above, the acceptance probability calcu-lation is exact.
Partitioning the entities and propos-ing local jumps are restrictions to the single-machineproposal distribution; redistribution stages ensurethe equivalent Markov chains are still irreducible.See Singh et al (2010) for more details.4 Hierarchical Coreference ModelThe proposal function for MCMC-based MAP infer-ence presents changes to the current entities.
Sincewe use MCMC to reach high-scoring regions of thehypothesis space, we are interested in the changesthat improve the current configuration.
But as thenumber of mentions and entities increases, thesefruitful samples become extremely rare due to theblowup in the possible space of configurations, re-sulting in rejection of a large number of proposals.By distributing as described in the previous section,we propose samples in parallel, improving chancesof finding changes that result in better configura-tions.
However, due to random redistribution and anaive proposal function within each machine, a largefraction of proposals are still wasted.
We addressthese concerns by adding hierarchy to the model.4.1 Sub-EntitiesConsider the task of proposing moves of mentions(within a machine).
Given the large number ofmentions and entities, the probability that a ran-domly picked mention that is moved to a randomentity results in a better configuration is extremelysmall.
If such a move is accepted, this gives us ev-idence that the mention did not belong to the pre-vious entity, and we should also move similar men-tions from the previous entity simultaneously to thesame entity.
Since the proposer moves only a sin-gle mention at a time, a large number of samplesmay be required to discover these fruitful moves.To enable block proposals that move similar men-tions simultaneously, we introduce latent sub-entityvariables that represent groups of similar mentionswithin an entity, where the similarity is defined bythe model.
For inference, we have stages of sam-pling sub-entities (moving individual mentions) in-terleaved with stages of entity sampling (moving allmentions within a sub-entity).
Even though our con-figuration space has become larger due to these ex-tra variables, the proposal distribution has also im-proved since it proposes composite moves.4.2 Super-EntitiesAnother issue faced during distributed inference isthat random redistribution is often wasteful.
For ex-ample, if dissimilar entities are assigned to a ma-chine, none of the proposals may be accepted.
For alarge number of entities and machines, the probabil-ity that similar entities will be assigned to the samemachine is extremely small, leading to a larger num-ber of wasted proposals.
To alleviate this problem,we introduce super-entities that represent groups ofsimilar entities.
During redistribution, we ensure allentities in the same super-entity are assigned to thesame machine.
As for sub-entities above, inferenceswitches between regular sampling of entities andsampling of super-entities (by moving entities).
Al-though these extra variables have made the config-uration space larger, they also allow more efficientdistribution of entities, leading to useful proposals.4.3 Combined Hierarchical ModelEach of the described levels of the hierarchy are sim-ilar to the initial model (Section 2.1): mentions/sub-entities have the same structure as the entities/super-entities, and are modeled using similar factors.
Torepresent the ?context?
of a sub-entity we take theunion of the bags-of-words of the constituent men-tion contexts.
Similarly, we take the union of sub-797Super-EntitiesEntitiesMentionsSub-EntitiesFigure 4: Combined Hierarchical Model with factors instantiated for a hypothesis containing 2 super-entities, 4 entities, and 8 sub-entities, shown as colored circles, over 16 mentions.
Dotted lines representrepulsion factors and solid lines represent affinity factors (the color denotes the type of variable that thefactor touches).
The boxes on factors were excluded for clarity.entity contexts to represent the context of an entity.The factors are instantiated in the same manner asSection 2.1 except that we change the bias factorb for each level (increasing it for sub-entities, anddecreasing it for super-entities).
The exact valuesof these biases indirectly determines the number ofpredicted sub-entities and super-entities.Since these two levels of hierarchy operate atseparate granularities from each other, we combinethem into a single hierarchical model that containsboth sub- and super-entities.
We illustrate this hi-erarchical structure in Figure 4.
Inference for thismodel takes a round-robin approach by fixing twoof the levels of the hierarchy and sampling the third,cycling through these three levels.
Unless specifiedotherwise, the initial configuration is the singletonconfiguration, in which all sub-entities, entities, andsuper-entities are of size 1.5 ExperimentsWe evaluate our models and algorithms on a numberof datasets.
First, we compare performance on thesmall, publicly-available ?John Smith?
dataset.
Sec-ond, we run the automated Person-X evaluation toobtain thousands of mentions that we use to demon-strate accuracy and scalability improvements.
Mostimportantly, we create a large labeled corpus usinglinks to Wikipedia to explore the performance in thelarge-scale setting.5.1 John Smith CorpusTo compare with related work, we run an evalua-tion on the ?John Smith?
corpus (Bagga and Bald-win, 1998), containing 197 mentions of the name?John Smith?
from New York Times articles (la-beled to obtain 35 true entities).
The bias b forour approach is set to result in the correct numberof entities.
Our model achieves B3 F1 accuracy of66.4% on this dataset.
In comparison, Rao et al(2010) obtains 61.8% using the model most similarto ours, while their best model (which uses sophis-ticated topic-model features that do not scale easily)achieves 69.7%.
It is encouraging to note that ourapproach, using only a subset of the features, per-forms competitively with related work.
However,due to the small size of the dataset, we require fur-ther evaluation before reaching any conclusions.5.2 Person-X EvaluationThere is a severe lack of labeled corpora for cross-document coreference due to the effort requiredto evaluate the coreference decisions.
Relatedapproaches have used automated Person-X evalu-ation (Gooi and Allan, 2004), in which uniqueperson-name strings are treated as the true entitylabels for the mentions.
Every mention string isreplaced with an ?X?
for the coreference system.We use this evaluation methodology on 25k person-name mentions from the New York Times cor-pus (Sandhaus, 2008) each with one of 50 uniquestrings.
As before, we set the bias b to achieve thesame number of entities.
We use 1 million samplesin each round of inference, followed by random re-distribution in the flat model, and super-entities inthe hierarchical model.
Results are averaged overfive runs.798Figure 5: Person-X Evaluation of Pairwise model:Performance as number of machines is varied, aver-aged over 5 runs.Number of Entities 43,928Number of Mentions 1,567,028Size of Largest Entity 6,096Average Mentions per Entity 35.7Variance of Mentions per Entity 5191.7Table 1: Wikipedia Link Corpus Statistics.
Sizeof an entity is the number of mentions of that entity.Figure 5 shows accuracy compared to relativewallclock running time for distributed inference onthe flat, pairwise model.
Speed and accuracy im-prove as additional machines are added, but largernumber of machines lead to diminishing returns forthis small dataset.
Distributed inference on our hi-erarchical model is evaluated in Figure 6 against in-ference on the pairwise model from Figure 5.
Wesee that the individual hierarchical models performmuch better than the pairwise model; they achievethe same accuracy as the pairwise model in approx-imately 10% of the time.
Moreover, distributed in-ference on the combined hierarchical model is bothfaster and more accurate than the individual hierar-chical models.5.3 Wikipedia Link CorpusTo explore the application of the proposed approachto a larger, realistic dataset, we construct a corpusbased on the insight that links to Wikipedia that ap-pear on webpages can be treated as mentions, andsince the links were added manually by the page au-thor, we use the destination Wikipedia page as theFigure 6: Person-X Evaluation of HierarchicalModels: Performance of inference on hierarchicalmodels compared to the pairwise model.
Experi-ments were run using 50 machines.entity the link refers to.The dataset is created as follows: First, we crawlthe web and select hyperlinks on webpages that linkto an English Wikipedia page.2 The anchors ofthese links form our set of mentions, with the sur-rounding block of clean text (obtained after remov-ing markup, etc.)
around each link being its con-text.
We assign the title of the linked Wikipediapage as the entity label of that link.
Since this setof mentions and labels can be noisy, we use thefollowing filtering steps.
All links that have lessthan 36 words in their block, or whose anchor texthas a large string edit distance from the title of theWikipedia page, are discarded.
While this results incases in which ?President?
is discarded when linkedto the ?Barack Obama?
Wikipedia page, it was nec-essary to reduce noise.
Further, we also discardlinks to Wikipedia pages that are concepts (such as?public_domain?)
rather than entities.
All enti-ties with less than 6 links to them are also discarded.Table 1 shows some statistics about our automat-ically generated data set.
We randomly sampled 5%of the entities to create a development set, treatingthe remaining entities as the test set.
Unlike theJohn Smith and Person-X evaluation, this data setalso contains non-person entities such as organiza-tions and locations.For our models, we augment the factor potentialswith mention-string similarity:2e.g.
http://en.wikipedia.org/Hillary_Clinton799?a/r(m,n) = ?
(?mn ?
b+ wSTREQ(m,n))where STREQ is 1 if mentions m and n are stringidentical (0 otherwise), and w is the weight to thisfeature.3 In our experiments we found that settingw = 0.8 and b = 1e?
4 gave the best results on thedevelopment set.Due to the large size of the corpus, existing cross-document coreference approaches could not be ap-plied to this dataset.
However, since a majorityof related work consists of using clustering afterdefining a similarity function (Section 6), we pro-vide a baseline evaluation of clustering with Sub-Square (Bshouty and Long, 2010), a scalable, dis-tributed clustering method.
Subsquare takes as in-put a weighted graph with mentions as nodes andsimilarity between mentions used as edge weights.Subsquare works by stochastically assigning a ver-tex to the cluster of one its neighbors if they havesignificant neighborhood overlap.
This algorithmis an efficient form of approximate spectral cluster-ing (Bshouty and Long, 2010), and since it is giventhe same distances between mentions as our models,we expect it to get similar accuracy.
We also gen-erate another baseline clustering by assigning men-tions with identical strings to the same entity.
Thismention-string clustering is also used as the initialconfiguration of our inference.Figure 7: Wikipedia Link Evaluation: Perfor-mance of inference for different number of machines(N = 100, 500).
Mention-string match clustering isused as the initial configuration.3Note that we do not use mention-string similarity for JohnSmith or Person-X as the mention strings are all identical.MethodPairwise B3 ScoreP/ R F1 P/ R F1String-Match 30.0 / 66.7 41.5 82.7 / 43.8 57.3Subsquare 38.2 / 49.1 43.0 87.6 / 51.4 64.8Our Model 44.2 / 61.4 51.4 89.4 / 62.5 73.7Table 2: F1 Scores on the Wikipedia Link Data.The results are significant at the 0.0001 level overSubsquare according to the difference of proportionssignificance test.Inference is run for 20 rounds of 10 million sam-ples each, distributed over N machines.
We useN = 100, 500 and the B3 F1 score results obtainedset for each case are shown in Figure 7.
It canbe seen that N = 500 converges to a better solu-tion faster, showing effective use of parallelism.
Ta-ble 2 compares the results of our approach (at con-vergence for N = 500), the baseline mention-stringmatch and the Subsquare algorithm.
Our approachsignificantly outperforms the competitors.6 Related WorkAlthough the cross-document coreference problemis challenging and lacks large labeled datasets, itsubiquitous role as a key component of many knowl-edge discovery tasks has inspired several efforts.A number of previous techniques use scoringfunctions between pairs of contexts, which are thenused for clustering.
One of the first approachesto cross-document coreference (Bagga and Bald-win, 1998) uses an idf-based cosine-distance scor-ing function for pairs of contexts, similar to the onewe use.
Ravin and Kazi (1999) extend this work tobe somewhat scalable by comparing pairs of con-texts only if the mentions are deemed ?ambiguous?using a heuristic.
Others have explored multiplemethods of context similarity, and concluded thatagglomerative clustering provides effective meansof inference (Gooi and Allan, 2004).
Pedersen etal.
(2006) and Purandare and Pedersen (2004) inte-grate second-order co-occurrence of words into thesimilarity function.
Mann and Yarowsky (2003) usebiographical facts from the Web as features for clus-tering.
Niu et al (2004) incorporate information ex-traction into the context similarity model, and anno-tate a small dataset to learn the parameters.
A num-ber of other approaches include various forms of800hand-tuned weights, dictionaries, and heuristics todefine similarity for name disambiguation (Blume,2005; Baron and Freedman, 2008; Popescu et al,2008).
These approaches are greedy and differ in thechoice of the distance function and the clustering al-gorithm used.
Daume?
III and Marcu (2005) proposea generative approach to supervised clustering, andHaghighi and Klein (2010) use entity profiles to as-sist within-document coreference.Since many related methods use clustering, thereare a number of distributed clustering algorithmsthat may help scale these approaches.
Datta etal.
(2006) propose an algorithm for distributed k-means.
Chen et al (2010) describe a parallel spectralclustering algorithm.
We use the Subsquare algo-rithm (Bshouty and Long, 2010) as baseline becauseit works well in practice.
Mocian (2009) presents asurvey of distributed clustering algorithms.Rao et al (2010) have proposed an online deter-ministic method that uses a stream of input mentionsand assigns them greedily to entities.
Although itcan resolve mentions from non-trivial sized datasets,the method is restricted to a single machine, whichis not scalable to the very large number of mentionsthat are encountered in practice.Our representation of the problem as an undi-rected graphical model, and performing distributedinference on it, provides a combination of advan-tages not available in any of these approaches.
First,most of the methods will not scale to the hundredsof millions of mentions that are present in real-worldapplications.
By utilizing parallelism across ma-chines, our method can run on very large datasetssimply by increasing the number of machines used.Second, approaches that use clustering are limitedto using pairwise distance functions for which ad-ditional supervision and features are difficult to in-corporate.
In addition to representing features fromall of the related work, graphical models can alsouse more complex entity-wide features (Culotta etal., 2007; Wick et al, 2009a), and parameters canbe learned using supervised (Collins, 2002) or semi-supervised techniques (Mann and McCallum, 2008).Finally, the inference for most of the related ap-proaches is greedy, and earlier decisions are not re-visited.
Our technique is based on MCMC inferenceand simulated annealing, which are able to escapelocal maxima.7 ConclusionsMotivated by the problem of solving the corefer-ence problem on billions of mentions from all of thenewswire documents from the past few decades, wemake the following contributions.
First, we intro-duce distributed version of MCMC-based inferencetechnique that can utilize parallelism to enable scal-ability.
Second, we augment the model with hierar-chical variables that facilitate fruitful proposal distri-butions.
As an additional contribution, we use linksto Wikipedia pages to obtain a high-quality cross-document corpus.
Scalability and accuracy gains ofour method are evaluated on multiple datasets.There are a number of avenues for future work.Although we demonstrate scalability to more than amillion mentions, we plan to explore performanceon datasets in the billions.
We also plan to examineinference on complex coreference models (such aswith entity-wide factors).
Another possible avenuefor future work is that of learning the factors.
Sinceour approach supports parameter estimation, we ex-pect significant accuracy gains with additional fea-tures and supervised data.
Our work enables cross-document coreference on very large corpora, and wewould like to explore the downstream applicationsthat can benefit from it.AcknowledgmentsThis work was done when the first author was anintern at Google Research.
The authors wouldlike to thank Mark Dredze, Sebastian Riedel, andanonymous reviewers for their valuable feedback.This work was supported in part by the Centerfor Intelligent Information Retrieval, the Univer-sity of Massachusetts gratefully acknowledges thesupport of Defense Advanced Research ProjectsAgency (DARPA) Machine Reading Program underAir Force Research Laboratory (AFRL) prime con-tract no.
FA8750-09-C-0181., in part by an awardfrom Google, in part by The Central IntelligenceAgency, the National Security Agency and NationalScience Foundation under NSF grant #IIS-0326249,in part by NSF grant #CNS-0958392, and in partby UPenn NSF medium IIS-0803847.
Any opin-ions, findings and conclusions or recommendationsexpressed in this material are those of the authorsand do not necessarily reflect those of the sponsor.801ReferencesAmit Bagga and Breck Baldwin.
1998.
Entity-basedcross-document coreferencing using the vector spacemodel.
In International Conference on ComputationalLinguistics, pages 79?85.A.
Baron and M. Freedman.
2008. Who is who and whatis what: experiments in cross-document co-reference.In Empirical Methods in Natural Language Process-ing (EMNLP), pages 274?283.Eric Bengston and Dan Roth.
2008.
Understandingthe value of features for coreference resolution.
InEmpirical Methods in Natural Language Processing(EMNLP).Matthias Blume.
2005.
Automatic entity disambigua-tion: Benefits to NER, relation extraction, link anal-ysis, and inference.
In International Conference onIntelligence Analysis (ICIA).Nader H. Bshouty and Philip M. Long.
2010.
Find-ing planted partitions in nearly linear time using ar-rested spectral clustering.
In Johannes Fu?rnkranzand Thorsten Joachims, editors, Proceedings of the27th International Conference on Machine Learning(ICML-10), pages 135?142, Haifa, Israel, June.
Omni-press.Yuan Changhe, Lu Tsai-Ching, and Druzdzel Marek.2004.
Annealed MAP.
In Uncertainty in Artificial In-telligence (UAI), pages 628?635, Arlington , Virginia.AUAI Press.Wen-Yen Chen, Yangqiu Song, Hongjie Bai, Chih-JenLin, and Edward Y. Chang.
2010.
Parallel spectralclustering in distributed systems.
IEEE Transactionson Pattern Analysis and Machine Intelligence.Michael Collins.
2002.
Discriminative training methodsfor hidden markov models: Theory and experimentswith perceptron algorithm.
In Annual Meeting of theAssociation for Computational Linguistics (ACL).Aron Culotta, Michael Wick, and Andrew McCallum.2007.
First-order probabilistic models for coreferenceresolution.
In North American Chapter of the Associa-tion for Computational Linguistics - Human LanguageTechnologies (NAACL HLT).S.
Datta, C. Giannella, and H. Kargupta.
2006.
K-MeansClustering over a Large, Dynamic Network.
In SIAMData Mining Conference (SDM).Hal Daume?
III and Daniel Marcu.
2005.
A Bayesianmodel for supervised clustering with the Dirichlet pro-cess prior.
Journal of Machine Learning Research(JMLR), 6:1551?1577.Jeffrey Dean and Sanjay Ghemawat.
2004.
Mapreduce:Simplified data processing on large clusters.
Sympo-sium on Operating Systems Design & Implementation(OSDI).Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speech tag-ging.
In Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 744?751.Chung Heong Gooi and James Allan.
2004.
Cross-document coreference on a large scale corpus.
InNorth American Chapter of the Association for Com-putational Linguistics - Human Language Technolo-gies (NAACL HLT), pages 9?16.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric bayesianmodel.
In Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 848?855.Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.
InEmpirical Methods in Natural Language Processing(EMNLP), pages 1152?1161.Aria Haghighi and Dan Klein.
2010.
Coreference reso-lution in a modular, entity-centered model.
In NorthAmerican Chapter of the Association for Computa-tional Linguistics - Human Language Technologies(NAACL HLT), pages 385?393.Gideon S. Mann and Andrew McCallum.
2008.
General-ized expectation criteria for semi-supervised learningof conditional random fields.
In Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 870?878.Gideon S. Mann and David Yarowsky.
2003.
Unsuper-vised personal name disambiguation.
In North Amer-ican Chapter of the Association for ComputationalLinguistics - Human Language Technologies (NAACLHLT), pages 33?40.Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,and Mario Figueiredo.
2010.
Turbo parsers: Depen-dency parsing by approximate variational inference.In Empirical Methods in Natural Language Process-ing (EMNLP), pages 34?44, Cambridge, MA, October.Association for Computational Linguistics.J.
Mayfield, D. Alexander, B. Dorr, J. Eisner, T. Elsayed,T.
Finin, C. Fink, M. Freedman, N. Garera, P. Mc-Namee, et al 2009.
Cross-document coreference res-olution: A key technology for learning by reading.
InAAAI Spring Symposium on Learning by Reading andLearning to Read.Andrew McCallum and Ben Wellner.
2004.
Conditionalmodels of identity uncertainty with application to nouncoreference.
In Neural Information Processing Sys-tems (NIPS).Andrew McCallum, Karl Schultz, and Sameer Singh.2009.
FACTORIE: Probabilistic programming via im-peratively defined factor graphs.
In Neural Informa-tion Processing Systems (NIPS).Horatiu Mocian.
2009.
Survey of Distributed ClusteringTechniques.
Ph.D. thesis, Imperial College of London.802Vincent Ng.
2005.
Machine learning for coreference res-olution: From local classification to global ranking.
InAnnual Meeting of the Association for ComputationalLinguistics (ACL).Cheng Niu, Wei Li, and Rohini K. Srihari.
2004.
Weaklysupervised learning for cross-document person namedisambiguation supported by information extraction.In Annual Meeting of the Association for Computa-tional Linguistics (ACL), page 597.Ted Pedersen, Anagha Kulkarni, Roxana Angheluta, Zor-nitsa Kozareva, and Thamar Solorio.
2006.
Anunsupervised language independent method of namediscrimination using second order co-occurrence fea-tures.
In International Conference on Intelligent TextProcessing and Computational Linguistics (CICLing),pages 208?222.Hoifung Poon, Pedro Domingos, and Marc Sumner.2008.
A general method for reducing the complexityof relational inference and its application to MCMC.In AAAI Conference on Artificial Intelligence.Octavian Popescu, Christian Girardi, Emanuele Pianta,and Bernardo Magnini.
2008.
Improving cross-document coreference.
Journe?es Internationalesd?Analyse statistique des Donne?es Textuelles, 9:961?969.A.
Purandare and T. Pedersen.
2004.
Word sense dis-crimination by clustering contexts in vector and simi-larity spaces.
In Conference on Computational Natu-ral Language Learning (CoNLL), pages 41?48.Delip Rao, Paul McNamee, and Mark Dredze.
2010.Streaming cross document entity coreference reso-lution.
In International Conference on Computa-tional Linguistics (COLING), pages 1050?1058, Bei-jing, China, August.
Coling 2010 Organizing Commit-tee.Yael Ravin and Zunaid Kazi.
1999.
Is Hillary RodhamClinton the president?
disambiguating names acrossdocuments.
In Annual Meeting of the Association forComputational Linguistics (ACL), pages 9?16.Alexander M Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decomposition andlinear programming relaxations for natural languageprocessing.
In Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 1?11, Cambridge,MA, October.
Association for Computational Linguis-tics.Evan Sandhaus.
2008.
The New York Times annotatedcorpus.
Linguistic Data Consortium.Sameer Singh, Amarnag Subramanya, Fernando Pereira,and Andrew McCallum.
2010.
Distributed map in-ference for undirected graphical models.
In NeuralInformation Processing Systems (NIPS), Workshop onLearning on Cores, Clusters and Clouds.Ben Wellner, Andrew McCallum, Fuchun Peng, andMichael Hay.
2004.
An integrated, conditional modelof information extraction and coreference with appli-cation to citation matching.
In Uncertainty in ArtificialIntelligence (UAI), pages 593?601.Michael Wick, Aron Culotta, Khashayar Rohanimanesh,and Andrew McCallum.
2009a.
An entity-basedmodel for coreference resolution.
In SIAM Interna-tional Conference on Data Mining (SDM).Michael Wick, Khashayar Rohanimanesh, Aron Culotta,and Andrew McCallum.
2009b.
Samplerank: Learn-ing preferences from atomic gradients.
In Neural In-formation Processing Systems (NIPS), Workshop onAdvances in Ranking.803
