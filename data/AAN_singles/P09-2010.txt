Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 37?40,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPImproving data-driven dependency parsingusing large-scale LFG grammarsLilja ?vrelid, Jonas Kuhn and Kathrin SpreyerDepartment of LinguisticsUniversity of Potsdam{lilja,kuhn,spreyer}@ling.uni-potsdam.deAbstractThis paper presents experiments whichcombine a grammar-driven and a data-driven parser.
We show how the con-version of LFG output to dependencyrepresentation allows for a technique ofparser stacking, whereby the output of thegrammar-driven parser supplies featuresfor a data-driven dependency parser.
Weevaluate on English and German and showsignificant improvements stemming fromthe proposed dependency structure as wellas various other, deep linguistic featuresderived from the respective grammars.1 IntroductionThe divide between grammar-driven and data-driven approaches to parsing has become less pro-nounced in recent years due to extensive work onrobustness and efficiency for the grammar-drivenapproaches (Riezler et al, 2002; Cahill et al,2008b).
The linguistic generalizations captured insuch knowledge-based resources are thus increas-ingly available for use in practical applications.The NLP-community has in recent years wit-nessed a surge of interest in dependency-basedapproaches to syntactic parsing, spurred by theCoNLL shared tasks of dependency parsing(Buchholz and Marsi, 2006; Nivre et al, 2007).Nivre and McDonald (2008) show how two differ-ent approaches to dependency parsing, the graph-based and transition-based approaches, may becombined and subsequently learn to complementeach other to achieve improved parse results for arange of different languages.In this paper, we show how a data-driven depen-dency parser may straightforwardly be modified tolearn directly from a grammar-driven parser.
Weevaluate on English and German and show signifi-cant improvements for both languages.
Like Nivreand McDonald (2008), we supply a data-drivendependency parser with features from a differentparser to guide parsing.
The additional parser em-ployed in this work, is not however, a data-drivenparser trained on the same data set, but a grammar-driven parser outputing a deep LFG analysis.
Wefurthermore show how a range of other features ?morphological, structural and semantic ?
from thegrammar-driven analysis may be employed dur-ing data-driven parsing and lead to significant im-provements.2 Grammar-driven LFG-parsingThe XLE system (Crouch et al, 2007) performsunification-based parsing using hand-crafted LFGgrammars.
It processes raw text and assigns to itboth a phrase-structural (?c-structure?)
and a fea-ture structural, functional (?f-structure?
).In the work described in this paper, we employthe XLE platform using the grammars availablefor English and German from the ParGram project(Butt et al, 2002).
In order to increase the cover-age of the grammars, we employ the robustnesstechniques of fragment parsing and ?skimming?available in XLE (Riezler et al, 2002).3 Dependency conversion and featureextractionIn extracting information from the output of thedeep grammars we wish to capture as much of theprecise, linguistic generalizations embodied in thegrammars as possible, whilst keeping with the re-quirements posed by the dependency parser.
Theprocess is illustrated in Figure 1.3.1 DataThe English data set consists of the Wall StreetJournal sections 2-24 of the Penn treebank (Mar-cus et al, 1993), converted to dependency format.The treebank data used for German is the Tiger37f1???????????
?PRED ?halte?.
.
.?
?VTYPE predicativeSUBJ ?pro?OBJf2?
?PRED ?Verhalten?CASE accSPEC f3?das?ADJUNCT{f4?damalige?}??XCOMP-PRED?
?PRED ?fu?r?.
.
.?
?PTYPE nosemOBJ[PRED ?richtig?SUBJ]?????????????
?SUBJconverted:SPECXCOMP-PREDADJCTSUBJ-OBJOBJIch halte das damalige Verhalten fu?r richtig.1sg pred.
acc nosemgSBold:NKOANKMONKFigure 1: Treebank enrichment with LFG output; German example: I consider the past behaviour cor-rect.treebank (Brants et al, 2004), where we employthe version released with the CoNLL-X sharedtask on dependency parsing (Buchholz and Marsi,2006).3.2 LFG to dependency structureWe start out by converting the XLE output to adependency representation.
This is quite straight-forward since the f-structures produced by LFGparsers can be interpreted as dependency struc-tures.
The conversion is performed by a set ofrewrite rules which are executed by XLE?s built-in extraction engine.
We employ two strategies forthe extraction of dependency structures from out-put containing multiple heads.
We attach the de-pendent to the closest head and, i) label it with thecorresponding label (Single), ii) label it with thecomplex label corresponding to the concatenationof the labels from the multiple head attachments(Complex).
The converted dependency analysis inFigure 1 shows the f-structure and the correspond-ing converted dependency output of a German ex-ample sentence, where a raised object Verhaltenreceives the complex SUBJ-OBJ label.
Followingthe XLE-parsing of the treebanks and the ensu-ing dependency conversion, we have a grammar-based analysis for 95.2% of the English sentence,45238 sentences altogether, and 96.5% of the Ger-man sentences, 38189 sentences altogether.3.3 Deep linguistic featuresThe LFG grammars capture linguistic generaliza-tions which may not be reduced to a dependencyrepresentation.
For instance, the grammars con-tain information on morphosyntactic propertiessuch as case, gender and tense, as well as more se-mantic properties detailing various types of adver-bials, specifying semantic conceptual categoriessuch as human, time and location etc., see Fig-ure 1.
Table 1 presents the features extracted foruse during parsing from the German and EnglishXLE-parses.4 Data-driven dependency parsingMaltParser (Nivre et al, 2006a) is a language-independent system for data-driven dependencyparsing which is freely available.1 MaltParser isbased on a deterministic parsing strategy in com-bination with treebank-induced classifiers for pre-dicting parse transitions.
MaltParser constructsparsing as a set of transitions between parse con-figurations.
A parse configuration is a triple?S, I,G?, where S represents the parse stack, I isthe queue of remaining input tokens, and G repre-sents the dependency graph defined thus far.The feature model in MaltParser defines the rel-evant attributes of tokens in a parse configuration.Parse configurations are represented by a set offeatures, which focus on attributes of the top of thestack, the next input token and neighboring tokensin the stack, input queue and dependency graphunder construction.
Table 2 shows an example ofa feature model.2For the training of baseline parsers we employfeature models which make use of the word form(FORM), part-of-speech (POS) and the dependencyrelation (DEP) of a given token, exemplified inTable 2.
For the baseline parsers and all subse-quent parsers we employ the arg-eager algorithmin combination with SVM learners with a polyno-mial kernel.31http://maltparser.org2Note that the feature model in Table 2 is an example fea-ture model and not the actual model employed in the parseexperiments.
The details or references for the English andGerman models are provided below.3For training of the baseline parsers we also em-ploy some language-specific settings.
For English weuse learner and parser settings, as well as feature modelfrom the English pretrained MaltParser-model available fromhttp://maltparser.org.
For German, we use the learner andparser settings from the parser employed in the CoNLL-X38POS XFeatsVerb CLAUSETYPE, GOVPREP, MOOD, PASSIVE, PERF,TENSE, VTYPENoun CASE, COMMON, GOVPREP, LOCATIONTYPE, NUM,NTYPE, PERS, PROPERTYPEPronoun CASE, GOVPREP, NUM, NTYPE, PERSPrep PSEM, PTYPEConj COORD, COORD-FORM, COORD-LEVELAdv ADJUNCTTYPE, ADVTYPEAdj ATYPE, DEGREEEnglish DEVERBAL, PROG, SUBCAT, GENDSEM, HUMAN,TIMEGerman AUXSELECT, AUXFLIP, COHERENT, FUT, DEF, GEND,GENITIVE, COUNTTable 1: Features from XLE output, common forboth languages and language-specifficFORM POS DEP XFEATS XDEPS:top + + + + +I:next + + + +I:next?1 + +G:head of top + +G:leftmost dependent of top + +InputArc(XHEAD)Table 2: Example feature model; S: stack, I: input,G: graph; ?n = n positions to the left(?)
or right(+).5 Parser stackingThe procedure to enable the data-driven parser tolearn from the grammar-driven parser is quite sim-ple.
We parse a treebank with the XLE platform.We then convert the LFG output to dependencystructures, so that we have two parallel versionsof the treebank ?
one gold standard and one withLFG-annotation.
We extend the gold standardtreebank with additional information from the cor-responding LFG analysis, as illustrated by Figure1 and train the data-driven dependency parser onthe enhanced data set.We extend the feature model of the baselineparsers in the same way as Nivre and McDon-ald (2008).
The example feature model in Table2 shows how we add the proposed dependencyrelation (XDEP) top and next as features for theparser.
We furthermore add a feature which looksat whether there is an arc between these two tokensin the dependency structure (InputArc(XHEAD)),with three possible values: Left, Right, None.
Inorder to incorporate further information suppliedby the LFG grammars we extend the feature mod-els with an additional, static attribute, XFEATS.This is employed for the range of deep linguisticfeatures, detailed in section 3.3 above.5.1 Experimental setupAll parse experiments are performed using 10-foldcross-validation for training and testing.
Overallparsing accuracy will be reported using the stan-dard metrics of labeled attachment score (LAS)and unlabeled attachment score (UAS).Statisticalsignificance is checked using Dan Bikel?s random-ized parsing evaluation comparator.4shared task (Nivre et al, 2006b).
For both languages, we em-ploy so-called ?relaxed?
root handling.4http://www.cis.upenn.edu/?dbikel/software.html6 ResultsWe experiment with the addition of two types offeatures: i) the dependency structure proposed byXLE for a given sentence ii) other morphosyntac-tic, structural or lexical semantic features providedby the XLE grammar.
The results are presented inTable 3.For English, we find that the addition of pro-posed dependency structure from the grammar-driven parser causes a small, but significant im-provement of results (p<.0001).
In terms of la-beled accuracy the results improve with 0.15 per-centage points, from 89.64 to 89.79.
The introduc-tion of complex dependency labels to account formultiple heads in the LFG output causes a smallerimprovement of results than the single labelingscheme.
The corresponding results for German arepresented in Table 3.
We find that the addition ofgrammar-driven dependency structures with sin-gle labels (Single) improves the parse results sig-nificantly (p<.0001), both in terms of unlabeledand labeled accuracy.
For labeled accuracy we ob-serve an improvement of 1.45 percentage points,from 85.97 to 87.42.
For the German data, wefind that the addition of dependency structure withcomplex labels (Complex) gives a further small,but significant (p<.03) improvement over the ex-periment with single labels.The results following the addition of thegrammar-extracted features in Table 1 (Feats) arepresented in Table 3.5 We observe significant im-provements of overall parse results for both lan-guages (p<.0001).5We experimented with several feature models for the in-clusion of the additional information, however, found no sig-nificant differences when performing a forward feature selec-tion.
The simple feature model simply adds the XFEATS ofthe top and next tokens of the parse configuration.39English GermanUAS LAS UAS LASBaseline 92.48 89.64 88.68 85.97Single 92.61 89.79 89.72 87.42Complex 92.58 89.74 89.76 87.46Feats 92.55 89.77 89.63 87.30Single+Feats 92.52 89.69 90.01 87.77Complex+Feats 92.53 89.70 90.02 87.78Table 3: Overall results in experiments expressed as unlabeled and labeled attachment scores.We also investigated combinations of the dif-ferent sources of information ?
dependency struc-tures and deep features.
These results are pre-sented in the final lines of Table 3.
We findthat for the English parser, the combination ofthe features do not cause a further improve-ment of results, compared to the individual ex-periments.
The combined experiments (Sin-gle+Feats, Complex+Feats) for German, on theother hand, differ significantly from the base-line experiment, as well as the individual ex-periments (Single,Complex,Feats) reported above(p<.0001).
By combination of the grammar-derived features we improve on the baseline by1.81 percentage points.A comparison with the German results obtainedusing MaltParser with graph-based dependencystructures supplied by MSTParser (Nivre and Mc-Donald, 2008) shows that our results using agrammar-driven parser largely corroborate the ten-dencies observed there.
Our best results for Ger-man, combining dependency structures and addi-tional features, slightly improve on those reportedfor MaltParser (by 0.11 percentage points).67 Conclusions and future workThis paper has presented experiments in the com-bination of a grammar-driven LFG-parser and adata-driven dependency parser.
We have shownhow the use of converted dependency structuresin the training of a data-driven dependency parser,MaltParser, causes significant improvements inoverall parse results for English and German.
Wehave furthermore presented a set of additional,deep features which may straightforwardly be ex-tracted from the grammar-based output and causeindividual improvements for both languages and acombined effect for German.In terms of future work, a more extensive er-ror analysis will be performed to locate the pre-6English was not among the languages investigated in-Nivre and McDonald (2008).cise benefits of the parser combination.
We willalso investigate the application of the method di-rectly to raw text and application to a task whichmay benefit specifically from the combined anal-yses, such as semantic role labeling or semanticverb classification.It has recently been shown that automaticallyacquired LFG grammars may actually outperformhand-crafted grammars in parsing (Cahill et al,2008a).
These results add further to the relevanceof the results shown in this paper, bypassing thebottleneck of grammar hand-crafting as a prereq-uisite for the applicability of our results.ReferencesSabine Brants, Stefanie Dipper, Peter Eisenberg, Silvia Hansen-Schirra, EstherKnig, Wolfgang Lezius, Christian Rohrer, George Smith, and Hans Uszko-reit.
2004.
Tiger: Linguistic interpretation of a German corpus.
Researchon Language and Computation, 2:597?620.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilin-gual dependency parsing.
In Proceedings of CoNLL-X).Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Masuichi, andChristian Rohrer.
2002.
The Parallel Grammar Project.
In Proceedingsof COLING-2002 Workshop on Grammar Engineering and Evaluation.Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan Riezler, Josef van Gen-abith, and Andy Way.
2008a.
Wide-coverage deep statistical parsing usingautomatic dependency structure annotation.
Computational Linguistics.Aoife Cahill, John T. Maxwell, Paul Meurer, Christian Rohrer, and VictoriaRosen.
2008b.
Speeding up LFG parsing using c-structure pruning.
InProceedings of the Workshop on Grammar Engineering Across Frame-works.D.
Crouch, M. Dalrymple, R. Kaplan, T. King, J. Maxwell, and P. Newman,2007.
XLE Documentation.
http://www2.parc.com/isl/.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.
Building a largeannotated corpus for English: The Penn treebank.
Computational Linguis-tics, 19(2):313?330.Joakim Nivre and Ryan McDonald.
2008.
Integrating graph-based andtransition-based dependency parsers.
In Proceedings of ACL-HLT 2008.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006a.
Maltparser: A data-drivenparser-generator for dependency parsing.
In Proceedings of LREC.Joakim Nivre, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, and Svetoslav Mari-nov. 2006b.
Labeled pseudo-projective dependency parsing with SupportVector Machines.
In Proceedings of CoNLL.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald, Jens Nilsson, Se-bastian Riedel, and Deniz Yuret.
2007.
CoNLL 2007 Shared Task onDependency Parsing.
In Proceedings of the CoNLL Shared Task Sessionof EMNLP-CoNLL 2007, pages 915?932.Stefan Riezler, Tracy King, Ronald Kaplan, Richard Crouch, John T. Maxwell,and Mark Johnson.
2002.
Parsing the Wall Street journal using a lexical-functional grammar and discriminative estimation techniques.
In Proceed-ings of ACL.40
