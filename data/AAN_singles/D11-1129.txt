Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394?1404,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsExperimental Support for a Categorical CompositionalDistributional Model of MeaningEdward GrefenstetteUniversity of OxfordDepartment of Computer ScienceWolfson Building, Parks RoadOxford OX1 3QD, UKedward.grefenstette@cs.ox.ac.ukMehrnoosh SadrzadehUniversity of OxfordDepartment of Computer ScienceWolfson Building, Parks RoadOxford OX1 3QD, UKmehrs@cs.ox.ac.ukAbstractModelling compositional meaning for sen-tences using empirical distributional methodshas been a challenge for computational lin-guists.
We implement the abstract categoricalmodel of Coecke et al (2010) using data fromthe BNC and evaluate it.
The implementationis based on unsupervised learning of matricesfor relational words and applying them to thevectors of their arguments.
The evaluation isbased on the word disambiguation task devel-oped by Mitchell and Lapata (2008) for intran-sitive sentences, and on a similar new experi-ment designed for transitive sentences.
Ourmodel matches the results of its competitorsin the first experiment, and betters them in thesecond.
The general improvement in resultswith increase in syntactic complexity show-cases the compositional power of our model.1 IntroductionAs competent language speakers, we humans can al-most trivially make sense of sentences we?ve neverseen or heard before.
We are naturally good at un-derstanding ambiguous words given a context, andforming the meaning of a sentence from the mean-ing of its parts.
But while human beings seemcomfortable doing this, machines fail to deliver.Search engines such as Google either fall back onbag of words models?ignoring syntax and lexicalrelations?or exploit superficial models of lexicalsemantics to retrieve pages with terms related tothose in the query (Manning et al, 2008).However, such models fail to shine when it comesto processing the semantics of phrases and sen-tences.
Discovering the process of meaning as-signment in natural language is among the mostchallenging and foundational questions of linguis-tics and computer science.
The findings thereof willincrease our understanding of cognition and intelli-gence and shall assist in applications to automatinglanguage-related tasks such as document search.Compositional type-logical approaches (Mon-tague, 1974; Lambek, 2008) and distributional mod-els of lexical semantics (Schutze, 1998; Firth, 1957)have provided two partial orthogonal solutions to thequestion.
Compositional formal semantic modelsstem from classical ideas from mathematical logic,mainly Frege?s principle that the meaning of a sen-tence is a function of the meaning of its parts (Frege,1892).
Distributional models are more recent andcan be related to Wittgenstein?s later philosophy of?meaning is use?, whereby meanings of words can bedetermined from their context (Wittgenstein, 1953).The logical models relate to well known and robustlogical formalisms, hence offering a scalable theoryof meaning which can be used to reason inferen-tially.
The distributional models have found theirway into real world applications such as thesaurusextraction (Grefenstette, 1994; Curran, 2004) or au-tomated essay marking (Landauer, 1997), and haveconnections to semantically motivated informationretrieval (Manning et al, 2008).
This two-sortednessof defining properties of meaning: ?logical form?versus ?contextual use?, has left the quest for ?what isthe foundational structure of meaning??
even moreof a challenge.Recently, Coecke et al (2010) used high levelcross-disciplinary techniques from logic, category1394theory, and physics to bring the above two ap-proaches together.
They developed a unified mathe-matical framework whereby a sentence vector is bydefinition a function of the Kronecker product of itsword vectors.
A concrete instantiation of this the-ory was exemplified on a toy hand crafted corpusby Grefenstette et al (2011).
In this paper we imple-ment it by training the model over the entire BNC.The highlight of our implementation is that wordswith relational types, such as verbs, adjectives, andadverbs are matrices that act on their arguments.
Weprovide a general algorithm for building (or indeedlearning) these matrices from the corpus.The implementation is evaluated against the taskprovided by Mitchell and Lapata (2008) for disam-biguating intransitive verbs, as well as a similar newexperiment for transitive verbs.
Our model improveson the best method evaluated in Mitchell and Lapata(2008) and offers promising results for the transitivecase, demonstrating its scalability in comparison tothat of other models.
But we still feel there is needfor a different class of experiments to showcase mer-its of compositionality in a statistically significantmanner.
Our work shows that the categorical com-positional distributional model of meaning permitsa practical implementation and that this opens theway to the production of large scale compositionalmodels.2 Two Orthogonal Semantic ModelsFormal Semantics To compute the meaning of asentence consisting of n words, meanings of thesewords must interact with one another.
In formal se-mantics, this further interaction is represented as afunction derived from the grammatical structure ofthe sentence, but meanings of words are amorphousobjects of the domain: no distinction is made be-tween words that have the same type.
Such modelsconsist of a pairing of syntactic interpretation rules(in the form of a grammar) with semantic interpreta-tion rules, as exemplified by the simple model pre-sented in Figure 1.The parse of a sentence such as ?cats like milk?typically produces its semantic interpretation bysubstituting semantic representation for their gram-matical constituents and applying ?-reduction whereneeded.
Such a derivation is shown in Figure 2.Syntactic Analysis Semantic InterpretationS?
NP VP |V P |(|NP |)NP?
cats, milk, etc.
|cats|, |milk|, .
.
.VP?
Vt NP |V t|(|NP |)Vt?
like, hug, etc.
?yx.|like|(x, y), .
.
.Figure 1: A simple model of formal semantics.|like|(|cats|, |milk|)|cats| ?x.|like|(x, |milk|)|milk| ?yx.|like|(x, y)Figure 2: A parse tree showing a semantic derivation.This methodology is used to translate sentencesof natural language into logical formulae, then usecomputer-aided automation tools to reason aboutthem (Alshawi, 1992).
One major drawback is thatthe result of such analysis can only deal with truthor falsity as the meaning of a sentence, and saysnothing about the closeness in meaning or topic ofexpressions beyond their truth-conditions and whatmodels satisfy them, hence do not perform well onlanguage tasks such as search.
Furthermore, an un-derlying domain of objects and a valuation functionmust be provided, as with any logic, leaving openthe question of how we might learn the meaning oflanguage using such a model, rather than just use it.Distributional Models Distributional models ofsemantics, on the other hand, dismiss the interactionbetween syntactically linked words and are solelyconcerned with lexical semantics.
Word meaningis obtained empirically by examining the contexts1in which a word appears, and equating the meaningof a word with the distribution of contexts it shares.The intuition is that context of use is what we ap-peal to in learning the meaning of a word, and thatwords that frequently have the same sort of contextin common are likely to be semantically related.For instance, beer and sherry are both drinks, al-coholic, and often cause a hangover.
We expectthese facts to be reflected in a sufficiently large cor-pus: the words ?beer?
and ?sherry?
occur within the1E.g.
words which appear in the same sentence or n-wordwindow, or words which hold particular grammatical or depen-dency relations to the word being learned.1395context of identifying words such as ?drink?, ?alco-holic?
and ?hangover?
more frequently than they oc-cur with other content words.Such context distributions can be encoded as vec-tors in a high dimensional space with contexts asbasis vectors.
For any word vector ??
?word, the scalarweight cwordi associated with each context basis vec-tor ?
?ni is a function of the number of times theword has appeared in that context.
Semantic vectors(cword1 , cword2 , ?
?
?
, cwordn ) are also denoted by sumsof such weight/basis vector pairs:??
?word =?icwordi ?
?niLearning a semantic vector is just learning its ba-sis weights from the corpus.
This setting offers ge-ometric means to reason about semantic similarity(e.g.
via cosine measure or k-means clustering), asdiscussed in Widdows (2005).The principal drawback of such models is theirnon-compositional nature: they ignore grammaticalstructure and logical words, and hence cannot com-pute the meanings of phrases and sentences in thesame efficient way that they do for words.
Com-mon operations discussed in (Mitchell and Lapata,2008) such as vector addition (+) and component-wise multiplication (, cf.
?4 for details) are com-mutative, hence if ?
?vw = ?
?v + ?
?w or ?
?v  ?
?w , then?
?vw = ?
?wv, leading to unwelcome equalities such as?????????????
?the dog bit the man = ?????????????
?the man bit the dogNon-commutative operations, such as the Kroneckerproduct (cf.
?4 for definition) can take word-orderinto account (Smolensky, 1990) or even some morecomplex syntactic relations, as described in Clarkand Pulman (2007).
However, the dimensionality ofsentence vectors produced in this manner differs forsentences of different length, barring all sentencesfrom being compared in the same vector space, andgrowing exponentially with sentence length hencequickly becoming computationally intractable.3 A Hybrid Logico-Distributional ModelWhereas semantic compositional mechanisms forset-theoretic constructions are well understood,there are no obvious corresponding methods for vec-tor spaces.
To solve this problem, Coecke et al(2010) use the abstract setting of category theory toturn the grammatical structure of a sentence into amorphism compatible with the higher level logicalstructure of vector spaces.One pragmatic consequence of this abstract ideais as follows.
In distributional models, there is ameaning vector for each word, e.g.
?
?cats, ?
?like, and???milk.
The logical recipe tells us to apply the mean-ing of the verb to the meanings of subject and object.But how can a vector apply to other vectors?
The so-lution proposed above implies that one needs to havedifferent levels of meaning for words with differenttypes.
This is similar to logical models where verbsare relations and nouns are atomic sets.
So verb vec-tors should be built differently from noun vectors,for instance as matrices.The general information as to which words shouldbe matrices and which words atomic vectors is infact encoded in the type-logical representation of thegrammatical structure of the sentence.
This is thelinear map with word vectors as input and sentencevectors as output.
Hence, at least theoretically, oneshould be able to build sentence vectors and com-pare their synonymity in exactly the same way asone measures word synonymity.Pregroup Grammars The aforementioned linearmaps turn out to be the grammatical reductionsof a type-logic called a Lambek pregroup gram-mar (Lambek, 2008)2.
Pregroups and vector spacesshare the same high level mathematical structure, re-ferred to as a compact closed category, for a proofand details of this claim see Coecke et al (2010); fora friendly introduction to category theory, see Co-ecke and Paquette (2011).
One consequence of thisparity is that the grammatical reductions of a pre-group grammar can be directly transformed into lin-ear maps that act on vectors.In a nutshell, pregroup types are either atomicor compound.
Atomic types can be simple (e.g.
nfor noun phrases, s for statements) or left/rightsuperscripted?referred to as adjoint types (e.g.
nrand nl).
An example of a compound type is that ofa verb nrsnl.
The superscripted types express thatthe verb is a relation with two arguments of type n,2The usage of pregroup types is not essential, the types ofany other logic, for instance CCG can be used, but should betranslated into the language of pregroups.1396which have to occur to the right and to the left ofit, and that it outputs an argument of the type s. Atransitive sentence has types as shown in Figure 3.Each type n cancels out with its right adjoint nrfrom the right and its left adjoint nl from the left;mathematically speaking these mean3nln ?
1 and nnr ?
1Here 1 is the unit of concatenation: 1n = n1 =n.
The corresponding grammatical reduction of atransitive sentence is nnrsnl ?
1s1 = s. Each suchreduction can be depicted as a wire diagram.
Thediagram of a transitive sentence is shown in Figure 3.Catsnlikenr s nlmilk.nFigure 3: The pregroup types and reduction diagram fora transitive sentence.Syntax-guided Semantic Composition Accord-ing to Coecke et al (2010) and based on a generalcompleteness theorem between compact categories,wire diagrams, and vector spaces, the meaning ofsentences can be canonically reduced to linear alge-braic formulae.
The following is the meaning vectorof our transitive sentence:????????
?cats like milk = (f)(??cats???like???
?milk)(I)Here f is the linear map that encodes the grammati-cal structure.
The categorical morphism correspond-ing to it is denoted by the tensor product of 3 compo-nents: V ?1S?W , where V andW are subject andobject spaces, S is the sentence space, the ?s are thecups, and 1S is the straight line in the diagram.
Thecups stand for taking inner products, which whendone with the basis vectors imitate substitution.
Thestraight line stands for the identity map that doesnothing.
By the rules of the category, equation (I) re-duces to the following linear algebraic formula with3The relation?
is the partial order of the pregroup.
It corre-sponds to implication =?
in a logical reading thereof.
If theseinequalities are replaced by equalities, i.e.
if nln = 1 = nnr ,then the pregroup collapses into a group where nl = nr .lower dimensions, hence the dimensional explosionproblem for Kronecker products is avoided:?itjcitj???cats|?
?vi ??
?st ???wj|???milk?
?
S (II)?
?vi ,?
?wj are basis vectors of V and W .
The innerproduct ??
?cats | ?
?vi ?
substitutes the weights of ?
?catsinto the first argument place of the verb (similarlyfor object and second argument place).
?
?st is a basisvector of the sentence space S in which meanings ofsentences live, regardless of their grammatical struc-ture.The degree of synonymity of sentences is ob-tained by taking the cosine measure of their vectors.S is an abstract space: it needs to be instantiatedto provide concrete meanings and synonymity mea-sures.
For instance, a truth-theoretic model is ob-tained by taking the sentence space S to be the 2-dimensional space with basis vectors |1?
(True) and|0?
(False).4 Building Matrices for Relational WordsIn this section we present a general scheme to buildmatrices for relational words.
Recall that givena vector space A with basis {?
?ni}i, the Kroneckerproduct of two vectors ?
?v = ?i cai?
?ni and ?
?w =?i cbi?
?ni is defined as follows:?
?v ??
?w =?ijcai cbj (?
?ni ??
?nj)where (?
?ni ??
?nj) is just the pairing of the basis of A,i.e.
(?
?ni ,??nj).
The Kronecker product vectors belongin the tensor product of A with itself: A?A, henceifA has dimension r, these will be of dimensionalityr?r.
The point-wise multiplication of these vectorsis defined as follows?
?v ?
?w =?icai cbi ?
?niThe intuition behind having a matrix for a rela-tional word is that any relation R on sets X and Y ,i.e.
R ?
X ?
Y can be represented as a matrix,namely one that has as row-bases x ?
X and ascolumn-bases y ?
Y , with weight cxy = 1 where(x, y) ?
R and 0 otherwise.
In a distributional set-ting, the weights, which are natural or real numbers,1397will represent more: ?the extent according to whichx and y are related?.
This can be determined in dif-ferent ways.Suppose X is the set of animals, and ?chase?
is arelation on it: chase ?
X ?
X .
Take x = ?dog?and y = ?cat?
: with our type-logical glasses on, theobvious choice would be to take cxy to be the num-ber of times ?dog?
has chased ?cat?, i.e.
the numberof times the sentence ?the dog chases the cat?
hasappeared in the corpus.
But in the distributional set-ting, this method will be too syntactic and dismissiveof the actual meaning of ?cat?
and ?dog?.
If insteadthe corpus contains the sentence ?the hound huntedthe wild cat?, cxy will be 0, restricting us to onlyassign meaning to sentences that have directly ap-peared in the corpus.
We propose to, instead, use alevel of abstraction by taking words such as verbs tobe distributions over the semantic information in thevectors of their context words, rather than over thecontext words themselves.Start with an r-dimensional vector space N withbasis {?
?n i}i, in which meaning vectors of atomicwords, such as nouns, live.
The basis vectors of Nare in principle all the words from the corpus, how-ever in practice and following Mitchell and Lapata(2008) we had to restrict these to a subset of themost occurring words.
These basis vectors are notrestricted to nouns: they can as well be verbs, adjec-tives, and adverbs, so that we can define the mean-ing of a noun in all possible contexts?as is usualin context-based models?and not only in the con-text of other nouns.
Note that basis words with re-lational types are treated as pure lexical items ratherthan as semantic objects represented as matrices.
Inshort, we count how many times a noun has occurredclose to words of other syntactic types such as ?elect?and ?scientific?, rather than count how many times ithas occurred close to their corresponding matrices:it is the lexical tokens that form the context, not theirmeaning.Each relational word P with grammatical type piand m adjoint types ?1, ?2, ?
?
?
, ?m is encoded asan (r ?
.
.
.?
r) matrix with m dimensions.
Sinceour vector space N has a fixed basis, each such ma-trix is represented in vector form as follows:?
?P =?ij ?
?
?
??
??
?mcij????
(?
?n i ??
?n j ?
?
?
?
?
?
?n ?)?
??
?mThis vector lives in the tensor spaceN ?N ?
?
?
?
?N?
??
?m.
Each cij????
is computedaccording to the procedure described in Figure 4.1) Consider a sequence of words containing a re-lational word ?P?
and its arguments w1, w2, ?
?
?
,wm, occurring in the same order as described inP?s grammatical type pi.
Refer to these sequencesas ?P?-relations.
Suppose there are k of them.2) Retrieve the vector ?
?w l of each argument wl.3) Suppose w1 has weight c1i on basis vector ?
?n i,w2 has weight c2j on basis vector ?
?n j , ?
?
?
, andwm has weight cm?
on basis vector ?
?n ?
.
Multiplythese weightsc1i ?
c2j ?
?
?
?
?
cm?4) Repeat the above steps for all the k ?P?-relations, and suma the corresponding weightscij????
=?k(c1i ?
c2j ?
?
?
?
?
cm?
)kaWe also experimented with multiplication, but the spar-sity of noun vectors resulted in most verb matrices beingempty.Figure 4: Procedure for learning weights for matrices ofwords ?P?
with relational types pi of m arguments.Linear algebraically, this procedure corresponds tocomputing the following?
?P =?k(?
?w 1 ??
?w 2 ?
?
?
?
?
?
?wm)kType-logical examples of relational words areverbs, adjectives, and adverbs.
A transitive verb isrepresented as a 2 dimensional matrix since its typeis nrsnl with two adjoint types nr and nl.
The cor-responding vector of this matrix is??
?verb =?ijcij (?
?n i ??
?n j)1398The weight cij corresponding to basis vector?
?n i??
?n j , is the extent according to which words that haveco-occurred with ?
?n i have been the subject of the?verb?
and words that have co-occurred with ?
?n jhave been the object of the ?verb?.
This examplecomputation is demonstrated in Figure 5.1) Consider phrases containing ?verb?, its subjectw1 and object w2.
Suppose there are k of them.2) Retrieve vectors ?
?w 1 and ?
?w 2.3) Suppose ?
?w 1 has weight c1i on ?
?n i and ?
?w 2 hasc2j on ?
?n j .
Multiply these weights c1i ?
c2j .4) Repeat the above steps for all k ?verb?-relations and sum the corresponding weights?k(c1i ?
c2j )kFigure 5: Procedure for learning weights for matrices oftransitive verbs.Linear algebraically, we are computing??
?verb =?k(?
?w 1 ??
?w 2)kAs an example, consider the verb ?show?
and sup-pose there are two ?show?-relations in the corpus:s1 = table show results2 = map show locationThe vector of ?show?
is??
?show = ???table????
?result + ???map?????
?locationConsider an N space with four basis vectors ?far?,?room?, ?scientific?, and ?elect?.
The TF/IDF-weighted values for vectors of the above four nouns(built from the BNC) are as shown in Table 1.i ?
?ni table map result location1 far 6.6 5.6 7 5.92 room 27 7.4 0.99 7.33 scientific 0 5.4 13 6.14 elect 0 0 4.2 0Table 1: Sample weights for selected noun vectors.Part of the matrix of ?show?
is presented in Table 2.As a sample computation, the weight c11 forvector (1, 1), i.e.
(??far,?
?far) is computed by multiply-ing weights of ?table?
and ?result?
on?
?far, i.e.
6.6?7,far room scientific electfar 79.24 47.41 119.96 27.72room 232.66 80.75 396.14 113.2scientific 32.94 31.86 32.94 0elect 0 0 0 0Table 2: Sample semantic matrix for ?show?.multiplying weights of ?map?
and ?location?
on ??far,i.e.
5.6 ?
5.9 then adding these 46.2 + 33.04 andobtaining the total weight 79.24.The same method is applied to build matrices for di-transitive verbs, which will have 3 dimensions, andadjectives and adverbs, which will be of 1 dimensioneach.5 Computing Sentence VectorsMeaning of sentences are vectors computed by tak-ing the variables of the categorical prescription ofmeaning (the linear map f obtained from the gram-matical reduction of the sentence) to be determinedby the matrices of the relational words.
For instancethe meaning of the transitive sentence ?sub verb obj?is:????????
?sub verb obj =?itj??
?sub | ?
?v i???
?w j | ??obj?
citj?
?s tWe take V := W := N and S = N ?
N , then?itj citj?
?s t is determined by the matrix of the verb,i.e.
substitute it by ?ij cij(?
?n i ?
?
?n j)4.
Hence????????
?sub verb obj becomes:?ij??
?sub | ?
?n i???
?n j | ??obj?cij(?
?n i ??
?n j) =?ijcsubi cobjj cij(?
?n i ??
?n j)This can be decomposed to point-wise multiplica-tion of two vectors as follows:(?ijcsubi cobjj (?
?n i??
?n j))(?ijcij(?
?n i??
?n j))4Note that by doing so we are also reducing the verb spacefrom N ?
(N ?N)?N to N ?N , since for our constructionwe only need tuples of the form ?
?n i ??
?n i ??
?n j ??
?n j whichare isomorphic to pairs (?
?n i ??
?n j).1399The left argument is the Kronecker product of sub-ject and object vectors and the right argument is thevector of the verb, so we obtain(??sub???obj)??
?verbSince  is commutative, this provides us with a dis-tributional version of the type-logical meaning of thesentence: point-wise multiplication of the meaningof the verb to the Kronecker product of its subjectand object:????????
?sub verb obj = ???verb(??sub??
?obj)This mathematical operation can be informally de-scribed as a structured ?mixing?
of the informationof the subject and object, followed by it being ?fil-tered?
through the information of the verb appliedto them, in order to produce the information of thesentence.In the transitive case, S = N ?
N , hence ?
?s t =?
?n i ?
?
?n j .
More generally, the vector space cor-responding to the abstract sentence space S is theconcrete tensor space (N ?
.
.
.
?N) for m the di-mension of the matrix of the ?verb?.
As we haveseen above, in practice we do not need to build thistensor space, as the computations thereof reduce topoint-wise multiplications and summations.Similar computations yield meanings of sentenceswith adjectives and adverbs.
For instance the mean-ing of a transitive sentence with a modified subjectand a modified verb we have?????????????
?adj sub verb obj adv =(?
?adv???verb)((?
?adj??sub)??
?obj)After building vectors for sentences, we can com-pare their meaning and measure their degree of syn-onymy by taking their cosine measure.6 EvaluationEvaluating such a framework is no easy task.
Whatto evaluate depends heavily on what sort of applica-tion a practical instantiation of the model is gearedtowards.
In (Grefenstette et al, 2011), it is sug-gested that the simplified model we presented andexpanded here could be evaluated in the same way aslexical semantic models, measuring compositionallybuilt sentence vectors against a benchmark datasetsuch as that provided by Mitchell and Lapata (2008).In this section, we briefly describe the evaluation ofour model against this dataset.
Following this, wepresent a new evaluation task extending the experi-mental methodology of Mitchell and Lapata (2008)to transitive verb-centric sentences, and compare ourmodel to those discussed by Mitchell and Lapata(2008) within this new experiment.First Dataset Description The first experiment,described in detail by Mitchell and Lapata (2008),evaluates how well compositional models disam-biguate ambiguous words given the context of a po-tentially disambiguating noun.
Each entry of thedataset provides a noun, a target verb and landmarkverb (both intransitive).
The noun must be com-posed with both verbs to produce short phrase vec-tors the similarity of which is measured by the can-didate.
Also provided with each entry is a classifi-cation (?High?
or ?Low?)
indicating whether or notthe verbs are indeed semantically close within thecontext of the noun, as well as an evaluator-set simi-larity score between 1 and 7 (along with an evaluatoridentifier), where 1 is low similarity and 7 is high.Evaluation Methodology Candidate models pro-vide a similarity score for each entry.
The scoresof high similarity entries and low similarity entriesare averaged to produce a mean High score andmean Low score for the model.
The correlation ofthe model?s similarity judgements with the humanjudgements is also calculated using Spearman?s ?, ametric which is deemed to be more scrupulous, andultimately that by which models should be ranked,by Mitchell and Lapata (2008).
The mean for eachmodel is on a [0, 1] scale, except for UpperBoundwhich is on the same [1, 7] scale the annotators used.The ?
scores are on a [?1, 1] scale.
It is assumedthat inter-annotator agreement provides the theoret-ical maximum ?
for any model for this experiment.The cosine measure of the verb vectors, ignoring thenoun, is taken to be the baseline (no composition).Other Models The other models we compareours to are those evaluated by Mitchell and Lap-ata (2008).
We provide a selection of the results1400from that paper for the worst (Add) and best5 (Mul-tiply) performing models, as well as the previoussecond-best performing model (Kintsch).
The ad-ditive and multiplicative models are simply applica-tions of vector addition and component-wise multi-plication.
We invite the reader to consult (Mitchelland Lapata, 2008) for the description of Kintsch?sadditive model and parametric choices.Model Parameters To provide the most accuratecomparison with the existing multiplicative model,and exploiting the aforementioned feature that thecategorical model can be built ?on top of?
existinglexical distributional models, we used the parame-ters described by Mitchell and Lapata (2008) to re-produce the vectors evaluated in the original exper-iment as our noun vectors.
All vectors were builtfrom a lemmatised version of the BNC.
The nounbasis was the 2000 most common context words,basis weights were the probability of context wordsgiven the target word divided by the overall proba-bility of the context word.
Intransitive verb function-vectors were trained using the procedure presentedin ?4.
Since the dataset only contains intransitiveverbs and nouns, we used S = N .
The cosine mea-sure of vectors was used as a similarity metric.First Experiment Results In Table 3 we presentthe comparison of the selected models.
Our categor-ical model performs significantly better than the ex-isting second-place (Kintsch) and obtains a ?
quasi-identical to the multiplicative model, indicating sig-nificant correlation with the annotator scores.There is not a large difference between the meanHigh score and mean Low score, but the distri-bution in Figure 6 shows that our model makes anon-negligible distinction between high similarityphrases and low similarity phrases, despite the ab-solute scores not being different by more than a fewpercentiles.5The multiplicative model presented here is what is quali-fied as best in (Mitchell and Lapata, 2008).
However, they alsopresent a slightly better performing (?
= 0.19) model whichis a combination of their multiplicative model and a weightedadditive model.
The difference in ?
is qualified as ?not sta-tistically significant?
in the original paper, and furthermore themixed model requires parametric optimisation hence was notevaluated against the entire test set.
For these reasons, we chosenot to include it in the comparison.Model High Low ?Baseline 0.27 0.26 0.08Add 0.59 0.59 0.04Kintsch 0.47 0.45 0.09Multiply 0.42 0.28 0.17Categorical 0.84 0.79 0.17UpperBound 4.94 3.25 0.40Table 3: Selected model means for High and Low similar-ity items and correlation coefficients with human judge-ments, first experiment (Mitchell and Lapata, 2008).
p <0.05 for each ?.High Low0.40.50.60.70.80.91.0Figure 6: Distribution of predicted similarities for the cat-egorical distributional model on High and Low similarityitems.Second Dataset Description The second dataset6,developed by the authors, follows the format of the(Mitchell and Lapata, 2008) dataset used for the firstexperiment, with the exception that the target andlandmark verbs are transitive, and an object nounis provided in addition to the subject noun, henceforming a small transitive sentence.
The datasetcomprises 200 entries consisting of sentence pairs(hence a total of 400 sentences) constructed by fol-lowing the procedure outlined in ?4 of (Mitchell andLapata, 2008), using transitive verbs from CELEX7.For examples of these sentences, see Table 4.
Thedataset was split into four sections of 100 entrieseach, with guaranteed 50% exclusive overlap with6http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt7http://celex.mpi.nl/1401exactly two other datasets.
Each section was givento a group of evaluators, with a total of 25, who wereasked to form simple transitive sentence pairs fromthe verbs, subject and object provided in each entry;for instance ?the table showed the result?
from ?tableshow result?.
The evaluators were then asked to ratethe semantic similarity of each verb pair within thecontext of those sentences, and offer a score between1 and 7 for each entry.
Each entry was given an arbi-trary classification of HIGH or LOW by the authors,for the purpose of calculating mean high/low scoresfor each model.
For example, the first two pairs intable 4 were classified as HIGH, whereas the secondtwo pairs as LOW.Sentence 1 Sentence 2table show result table express resultmap show location map picture locationtable show result table picture resultmap show location map express locationTable 4: Example entries from the transitive dataset with-out annotator score, second experiment.Evaluation Methodology The evaluationmethodology for the second experiment wasidentical to that of the first, as are the scales formeans and scores.
Here also, Spearman?s ?
isdeemed a more rigorous way of determining howwell a model tracks difference in meaning.
This isboth because of the imprecise nature of the classifi-cation of verb pairs as HIGH or LOW; and since theobjective similarity scores produced by a model thatdistinguishes sentences of different meaning fromthose of similar meaning can be renormalised inpractice.
Therefore the delta between HIGH meansand LOW mean cannot serve as a definite indicationof the practical applicability (or lack thereof) ofsemantic models; the means are provided just to aidcomparison with the results of the first experiment.Model Parameters As in the first experiment, thelexical vectors from (Mitchell and Lapata, 2008)were used for the other models evaluated (additive,multiplicative and baseline)8 and for the noun vec-8Kintsch was not evaluated as it required optimising modelparameters against a held-out segment of the test set, and wecould not replicate the methodology of Mitchell and Lapatators of our categorical model.
Transitive verb vec-tors were trained as described in ?4 with S = N?N .Second Experiment Results The results for themodels evaluated against the second dataset are pre-sented in Table 5.Model High Low ?Baseline 0.47 0.44 0.16Add 0.90 0.90 0.05Multiply 0.67 0.59 0.17Categorical 0.73 0.72 0.21UpperBound 4.80 2.49 0.62Table 5: Selected model means for High and Low similar-ity items and correlation coefficients with human judge-ments, second experiment.
p < 0.05 for each ?.We observe a significant (according to p < 0.0.5)improvement in the alignment of our categoricalmodel with the human judgements, from 0.17 to0.21.
The additive model continues to make lit-tle distinction between senses of the verb duringcomposition, and the multiplicative model?s align-ment does not change, but becomes statistically in-distinguishable from the non-compositional baselinemodel.Once again we note that the high-low means arenot very indicative of model performance, as the dif-ference between high mean and the low mean of thecategorical model is much smaller than that of theboth the baseline model and multiplicative model,despite better alignment with annotator judgements.7 DiscussionIn this paper, we described an implementation of thecategorical model of meaning (Coecke et al, 2010),which combines the formal logical and the empiri-cal distributional frameworks into a unified seman-tic model.
The implementation is based on build-ing matrices for words with relational types (ad-jectives, verbs), and vectors for words with atomictypes (nouns), based on data from the BNC.
Wethen show how to apply verbs to their subject/object,in order to compute the meaning of intransitive andtransitive sentences.
(2008) with full confidence.1402Other work uses matrices to model meaning (Ba-roni and Zamparelli, 2010; Guevara, 2010), but onlyfor adjective-noun phrases.
Our approach easily ap-plies to such compositions, as well as to sentencescontaining combinations of adjectives, nouns, verbs,and adverbs.
The other key difference is that theylearn their matrices in a top-down fashion, i.e.
by re-gression from the composite adjective-noun contextvectors, whereas our model is bottom-up: it learnssentence/phrase meaning compositionally from thevectors of the compartments of the composites.
Fi-nally, very similar functions, for example a verb withargument alternations such as ?break?
in ?Y breaks?and ?X breaks Y?, are not treated as unrelated.
Thematrix of the intransitive ?break?
uses the corpus-observed information about the subject of break, in-cluding that of ?Y?, similarly the matrix of the tran-sitive ?break?
uses information about its subject andobject, including that of ?X?
and ?Y?.
We leave athorough study of these phenomena, which fall un-der providing a modular representation of passive-active similarities, to future work.We evaluated our model in two ways: first againstthe word disambiguation task of Mitchell and Lap-ata (2008) for intransitive verbs, and then against asimilar new experiment for transitive verbs, whichwe developed.Our findings in the first experiment show thatthe categorical method performs on par with theleading existing approaches.
This should not sur-prise us given that the context is so small and ourmethod becomes similar to the multiplicative modelof Mitchell and Lapata (2008).
However, our ap-proach is sensitive to grammatical structure, lead-ing us to develop a second experiment taking thisinto account and differentiating it from models withcommutative composition operations.The second experiment?s results deliver the ex-pected qualitative difference between models, withour categorical model outperforming the others andshowing an increase in alignment with human judge-ments in correlation with the increase in sentencecomplexity.
We use this second evaluation princi-pally to show that there is a strong case for the devel-opment of more complex experiments measuring notonly the disambiguating qualities of compositionalmodels, but also their syntactic sensitivity, which isnot directly measured in the existing experiments.These results show that the high level categori-cal distributional model, uniting empirical data withlogical form, can be implemented just like any otherconcrete model.
Furthermore it shows better resultsin experiments involving higher syntactic complex-ity.
This is just the tip of the iceberg: the mathe-matics underlying the implementation ensures thatit uniformly scales to larger, more complicated sen-tences and enables it to compare synonymity of sen-tences that are of different grammatical structure.8 Future WorkTreatment of function words such as ?that?, ?who?,as well as logical words such as quantifiers and con-junctives are left to future work.
This will buildalongside the general guidelines of Coecke et al(2010) and concrete insights from the work of Wid-dows (2005).
It is not yet entirely clear how ex-isting set-theoretic approaches, for example that ofdiscourse representation and generalised quantifiers,apply to our setting.
Preliminary work on integrationof the two has been presented by Preller (2007) andmore recently also by Preller and Sadrzadeh ( 2009).As mentioned by one of the reviewers, our pre-group approach to grammar flattens the sentencerepresentation, in that the verb is applied to its sub-ject and object at the same time; whereas in otherapproaches such as CCG, it is first applied to theobject to produce a verb phrase, then applied to thesubject to produce the sentence.
The advantages anddisadvantages of this method and comparisons withother systems, in particular CCG, constitutes ongo-ing work.9 AcknowledgementWe wish to thank P. Blunsom, S. Clark, B. Coecke,S.
Pulman, and the anonymous EMNLP review-ers for discussions and comments.
Support fromEPSRC grant EP/F042728/1 is gratefully acknowl-edged by M. Sadrzadeh.ReferencesH.
Alshawi (ed).
1992.
The Core Language Engine.MIT Press.M.
Baroni and R. Zamparelli.
2010.
Nouns are vectors,adjectives are matrices.
Proceedings of Conference1403on Empirical Methods in Natural Language Processing(EMNLP).S.
Clark and S. Pulman.
2007.
Combining Symbolicand Distributional Models of Meaning.
Proceedingsof AAAI Spring Symposium on Quantum Interaction.AAAI Press.B.
Coecke, and E. Paquette.
2011.
Categories for thePracticing Physicist.
New Structures for Physics, 167-271.
B. Coecke (ed.).
Lecture Notes in Physics 813.Springer.B.
Coecke, M. Sadrzadeh and S. Clark.
2010.
Mathemat-ical Foundations for Distributed Compositional Modelof Meaning.
Lambek Festschrift.
Linguistic Analysis36, 345?384.
J. van Benthem, M. Moortgat and W.Buszkowski (eds.).J.
Curran.
2004.
From Distributional to Semantic Simi-larity.
PhD Thesis, University of Edinburgh.K.
Erk and S. Pado?.
2004.
A Structured Vector SpaceModel for Word Meaning in Context.
Proceedingsof Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), 897?906.G.
Frege 1892.
U?ber Sinn und Bedeutung.
Zeitschriftfu?r Philosophie und philosophische Kritik 100.J.
R. Firth.
1957.
A synopsis of linguistic theory 1930-1955.
Studies in Linguistic Analysis.E.
Grefenstette, M. Sadrzadeh, S. Clark, B. Coecke,S.
Pulman.
2011.
Concrete Compositional SentenceSpaces for a Compositional Distributional Model ofMeaning.
International Conference on ComputationalSemantics (IWCS?11).
Oxford.G.
Grefenstette.
1994.
Explorations in Automatic The-saurus Discovery.
Kluwer.E.
Guevara.
2010.
A Regression Model of Adjective-Noun Compositionality in Distributional Semantics.Proceedings of the ACL GEMS Workshop.Z.
S. Harris.
1966.
A Cycling Cancellation-Automatonfor Sentence Well-Formedness.
International Compu-tation Centre Bulletin 5, 69?94.R.
Hudson.
1984.
Word Grammar.
Blackwell.J.
Lambek.
2008.
From Word to Sentence.
Polimetrica,Milan.T.
Landauer, and S. Dumais.
2008.
A solution to Platosproblem: The latent semantic analysis theory of ac-quisition, induction, and representation of knowledge.Psychological review.C.
D. Manning, P. Raghavan, and H. Schu?tze.
2008.
In-troduction to information retrieval.
Cambridge Uni-versity Press.J.
Mitchell and M. Lapata.
2008.
Vector-based mod-els of semantic composition.
Proceedings of the 46thAnnual Meeting of the Association for ComputationalLinguistics, 236?244.R.
Montague.
1974.
English as a formal language.
For-mal Philosophy, 189?223.J.
Nivre.
2003.
An efficient algorithm for projectivedependency parsing.
Proceedings of the 8th Interna-tional Workshop on Parsing Technologies (IWPT).A.
Preller.
Towards Discourse Representation via Pre-group Grammars.
Journal of Logic Language Infor-mation 16 173?194.A.
Preller and M. Sadrzadeh.
Semantic Vector Mod-els and Functional Models for Pregroup Grammars.Journal of Logic Language Information.
DOI:10.1007/s10849-011-9132-2.
to appear.J.
Saffron, E. Newport, R. Asling.
1999.
Word Segmenta-tion: The role of distributional cues.
Journal of Mem-ory and Language 35, 606?621.H.
Schuetze.
1998.
Automatic Word Sense Discrimina-tion.
Computational Linguistics 24, 97?123.P.
Smolensky.
1990.
Tensor product variable bindingand the representation of symbolic structures in con-nectionist systems.
Computational Linguistics 46, 1?2, 159?216.M.
Steedman.
2000.
The Syntactic Process.
MIT Press.D.
Widdows.
2005.
Geometry and Meaning.
Universityof Chicago Press.L.
Wittgenstein.
1953.
Philosophical Investigations.Blackwell.1404
