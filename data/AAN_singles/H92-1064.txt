NEAL-MONTGOMERY NLP SYSTEM EVALUATION METHODOLOGYSharon M. WalterRome LaboratoryRL/C3CAGriffiss AFB, NY 13441-5700walter@ aivax.rl.af.milABSTRACTOn what basis are the input processing capabilities of NaturalLanguage software judged?
That is, what are the capabilities tobe described and measured, and what are the standards againstwhich we measure them?
Rome Laboratory is currentlysupporting an effort to develop a concise terminology fordescribing the linguistic processing capabilities of NaturalLanguage Systems, and a uniform methodology forappropriately applying the terminology.
This methodology ismeant o produce quantitative, objective profiles of NL systemcapabilities without requiring system adaptation to a new testdomain or text corpus.
The effort proposes to develop arepeatable procedure that produces consistent results forindependent evaluators.1.
INTRODUCTIONAn appreciable drawback to current corpus-based (eg.,\[BBN; 1988\], \[Flickinger, et al 1987\], \[Hendrix, et al1976\], \[Malhotra; 1975\]) and task-based (eg.,\["Proceedings"; 1991\]) methodologies for evaluatingNatural Language Processing Systems is the requirementfor transportation of the system to a test domain.
Theexpense and time consumption are sizable and, as the portmay be minimal or incomplete, the evaluation may bebased on a demonstration f less than the full potential ofthe system.
Further, current evaluation methodologies donot fully elucidate NLP system capabilities for possiblefuture applications.Under contract o Rome Laboratory, Dr. Jeannette Neal(Calspan Corporation) and Dr. Christine Montgomery(Language Systems Incorporated) are in the final months ofdeveloping an NLP system evaluation methodology thatproduces descriptive, objective profiles of system linguisticcapabilities without a requirement for system adaptation toa new domain.
The evaluation methodology is meant oproduce consistent results for varied haman users.1.1.
Evaluation Methodology DescriptionWithin the Neal-Montgomery NLP System EvaluationMethodology each identified linguistic (lexical, syntactic,semantic, or discourse) feature is first carefully defined andexplained in order to establish a standard elimitation of thefeature.
Illustrative language patterns and sample sentencesthen guide the human evaluator to the formulation of aninput that tests the feature on the NLP system within thesystem's native domain.Based on clear and specific evaluation criteria for test iteminputs, NLP system responses are scored as follows:S: The system successfully met the stated criteria nddemonstrated understanding with respect to the feature undertest.C: The system responded in a way that was correct(that is, correctly answered the question posed), but thecriteria were not met.P: The system responded in a way that was onlypartially correct.F: The system responded in a way that was incorrect,failing to meet he criteria.N: The system was unable to accept he input or forma response (for example, the system vocabulary lacksappropriate words to complete a test inpu0.Each linguistic feature is tested by more than onemethodology item to make sure that results are not basedon spurious responses, and each item examines only one as-yet-untested capability, or one as-yet-untested combinationof capabilities.
Test inputs that are dependent oncapabilities previously shown to be unsuccessful areavoided.
Scores are then aggregated into percentages forhierarchically-structured classes of linguistic capabilitieswhich produce descriptive profiles of NLP systems.
Theprofiles can be viewed at varying levels of granularity.Figure 1 shows a sample system profile from the top levelof the hierarchy.Note that he scoring nomenclature (above) has been refinedand expanded since project experiments produced the profilesand results presented in this paper.
In Figures 1 and 2,"Unable to Compose Input" is equivalent to an 'N' in thenewer nomenclature.
A score of "Indeterminate" arliermeant he human evaluator could not determine if the NLP323System XYZI.
Basic SentencesII.
Simple Verb PhrasesIIL Noun PhrasesIV.
AdverbialsV.
Verbs and Verb PhrasesVI.
QuantifiersVII.
ComparativesVIII.
ConnectivesIX.
Embedded SentencesX.
ReferenceXI.
EllipsisXI/.
Semantics of EventsSuccesses# %18 81.825.5 78.5763 56.253.5 70.0012.5 65.7936 45.0025 39.0628.5 83.822 40.006 50.005 29.4114.5 37.18Failures# % #2 9.091.5 21A334 30.361.5 30.003.5 18.4239 48.7538 59.385.5 16.183 60.005 41.6710 58.8217.5 44.87Unable toComposeInput%0 0.000 0.006 5.360 0.000 0.001 1.251 1.560 0.000 0.001 8.332 11.762 5.13Indeterminate# %2 9.090 0.009 8.030 0.003 15.794 5.000 0.000 0.000 0.000 0.000 0.005 12.82TotalTime0:500:364:310:231:403:043:002:100:201:181:052:17AverageTimePer Item0:02:160:05:090:02:250:04:360:05:160:02:180:02:490:03:490:04:000:06:300:03:490:03:31Figure 1: A Top Level Evaluation Profile of an NLP Systemsystem correctly processed the test input.
The new systemof scores will be applied for the final project self-assessment activities.The columns at the far fight of Figure 1 display the totaltime (in hours and minutes) the user required to completethat section of the evaluation, and the average time per item(hours:minutes:seconds) for the section.Figure 2 displays part of the evaluation to themethodology's most detailed level of granularity.2.
PROJECT SELF-ASSESSMENTIn March and September of 1991 rigorous projectassessments provided valuable feedback into the design ofthe Neal-Montgomery NLP System EvaluationMethodology.
For each assessment, three people appliedthe methodology toeach of three NLP systems, for a totalof eighteen applications.
Assessment personnel,knowledgeable with respect to interface technology but nottrained linguists, were distinct from the methodologydevelopment team.The consistency of system profiles resulting from theseapplications, the examination of test inputs composedduring the assessments, records of oral commentary byevaluators, and responses to a post-evaluation questionnairehave been used as measures of the accuracy of methodologyresults.
For the September assessment phase, Figure 3shows, for each section of the methodology, the percentageof items for which the assessment team gave the same scoreto each system.
For example: the data points for theadverbial section indicate that all three people gave the sameassessment of System 2's skills for adverbials (they agreedin every instance), they agreed 60% of the time on Systeml's adverbial skills, and they agreed only 20% of the timefor System 3's adverbial skills.
The inconsistency ofscores in this section has prompted the development teamto refine the methodology's adverbial section.NLP systems used for assessments to date have includedthree NL database query systems and two MUC-3 systems.Focusing on reliability rather than feedback intomethodology design, four people will apply the Neal-Montgomery NLP Evaluation Methodology to each of twosystems for the third (and final) project self-assessment iApril 1992.3.
TOWARD THE FUTUREEvaluation "standards" are not developed and adoptedwithout a period of review, rumination, and tweaking bythe relevant user community.
It is our hope therefore, indistributing the Neal-Montgomery NLP System EvaluationMethodology to the technical community, to stir interestthat may lead to the eventual consideration of themethodology as the basis for a standard evaluation tool forNLP system capabilities.The Neal-Montgomery NLP System EvaluationMethodology is due for completion and delivery to RomeLaboratory in May of 1992.
It will become immediatelyavailable at that time to all interested parties.
Requestsshould be made to the author of this paper.
Reviewercomment, critique, and suggestions for the methodology areinvited.324System XYZI.
Basic Sentences1 Declarative Sentences2 Imperative Sentences...3...kn.~.o.
~.a..a.y..e...S..e.n..~.n..c.e..s. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.3.1 What-questions3.1.1 What as Pronouna) with BEb) with DO3.1.2 What as Determinera) with verbb) with BE.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.c.)...w.~.~....~...
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.3.2 Who-questionsa) with verb.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..b)....w.!.t.h.. ~ .O.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.3.3 Where-questionsa) with BE.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..bL.w.!~....~...
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.3.4 When-questionsa) with BE.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..b)....w.!~.Eg.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.3.5 Which-questionsa) with BE npb) with verbc) with BE adj.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..~)....w.!~....~... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.3.6 How-questions3.6.1 How \[Adj\] [BE-Verb\] \[NP\]?......
3....6=2 .H.o...w.. \[ a..a.
~!.t.BE:y.~ .bL .~.. J ?
....3.7 Yes/No questionsa) with BE npb) with BE adjc) with DOSuccesses1817.?
.~,0.50.5..o9.,1.50.5.?.1o,3.511.0.
:.s1.51,0.~.531i 1I1# % #81.82 20 0.00 01 100.00 0. .
.
.
.
.
.
.
,.8..5:.~ .
.
.
.
.
.
,.2.5 100.00 02 100.00 01 01 03 100.00 01 01 0.1. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
9.2 100.00 01 00Failures%9.090.000.00.1..o=~0.000.000.000.00Unable toComposeInput# %0 0.000 0.000 0.00.9.
..... .o.
:~,0 0.000 0.00000 0.0000P.*.**o..
.
.
.*.
.0 0.0000Indeterminate# %2 9 .~1 1~.00 0 .~.
.
.
.
.
.
.
~ :~0 0 .~0 0 .~000 0 .~00~.
.
.
?
l , ?
.
.
.
?
?
.
.
,0 0 .~0025.00, .??.
.?
.
.
.75.00. .
.
.
.
.
.
?
?.87.5075.00100.00,...5.9:.~100.000.5 25.000.5,.., .0.??.
.?
.?
.?
.
.???.
.0.5 25.000.5, .?o.0.?.
.
.
.
.
.
.
.
.
.
???
?0.5 12.50000,0~5o.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0.5 25.000 0.00o..5.
.. .
.
.
.
.
.5..o.,~0 0.000000 0.0000????
.
.
.
?
.
.
.
.
.
,0 0.000P.??
.
.?
.o?
.
.?
.
,0 0.00000Po??
.?
.
.
.
.
.
.
.
.
.0 0.000 0.009.
..... .o...~.0 0.0000010.100900009.009.000050.00. .
.
.
?
.
.
* .
.0.00?????..?.?0.00.
.
?
?
.
.
.
.
.
.0.000.00. .
.
.o..~.0.00TotalTime0:50AverageTimePer Item0:02:16Figure 2: Detailed Evaluation Profile for 'Basic Sentences'My sincere thanks to Jeannette Neal of the CalspanCorporation and to Beth Sundheim for their valuablecritique on early versions of this paper.REFERENCES1.
BBN Systems and Technologies Corporation, DraftCorpus for Testing NL Data Base Query Interfaces,2.3.NL Evaluation Workshop, Wayne, PA, December1988.Flickinger, D., Nerbonne, J., Sag, I., and Wasow T."Toward Evaluation of Natural Language ProcessingSystems", Hewlett-Packard Laboratories TechnicalReport, 1987.Hendrix, G.G., Sacerdoti, E.D.
and Slocum, J.
"Developing a Natural Language Interface toComplex Data", Artificial Intelligence CenterTechnical Report, SRI International, 1976.325..Malhotra, A., "Design Criteria for a Knowledge- 6.Based Language System for Management: AnExperimental Analysis", MIT/LCS/TR-146, 1975.Neal, J. G., Feit, E.L., and Montgomery, C.A., 7.
"An Application-Independent Approach to NaturalLanguage Evaluation", submitted to ACL-92..Neal, J.G.
and Walter, S.M.
(ed.)
"Natural LanguageProcessing Systems Evaluation Workshop", RomeLaboratory Technical Report, 1991.Read, W., Quilici, A., Reeves, J., Dyer, M., andBaker, E.; "Evaluating Natural Language Systems:A Sourcebook Approach", Coling-88.
"Proceedings of the Third Message UnderstandingConference", Morgan Kaufmann Publishers, 1991.100%cl90%80%70%60%50%40%30%20%10%0%System 31 0 System 2IBasicSentenceII IV VI VIII X XIISimple Adverbials Quanfifiers Connectives IX Reference SemanticsVerb IlI V VII XI of EventsNoun Verbs & Comparat ives Embedded Ellipsis Phrases Phrases Verb Phrases SentencesFigure 3: Percentage of Agreement Among Evaluators for Each System326
