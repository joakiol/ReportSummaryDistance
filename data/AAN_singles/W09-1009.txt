Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 58?65,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsUpper Bounds for Unsupervised Parsing with UnambiguousNon-Terminally Separated GrammarsFranco M. Luque and Gabriel Infante-LopezGrupo de Procesamiento de Lenguaje NaturalUniversidad Nacional de C?rdoba & CONICETArgentina{francolq|gabriel}@famaf.unc.edu.arAbstractUnambiguous Non-Terminally Separated(UNTS) grammars have properties thatmake them attractive for grammatical in-ference.
However, these properties do notstate the maximal performance they canachieve when they are evaluated against agold treebank that is not produced by anUNTS grammar.
In this paper we inves-tigate such an upper bound.
We developa method to find an upper bound for theunlabeled F1 performance that any UNTSgrammar can achieve over a given tree-bank.
Our strategy is to characterize allpossible versions of the gold treebank thatUNTS grammars can produce and to findthe one that optimizes a metric we define.We show a way to translate this score intoan upper bound for the F1.
In particular,we show that the F1 parsing score of anyUNTS grammar can not be beyond 82.2%when the gold treebank is the WSJ10 cor-pus.1 IntroductionUnsupervised learning of natural language has re-ceived a lot of attention in the last years, e.g., Kleinand Manning (2004), Bod (2006a) and Seginer(2007).
Most of them use sentences from a tree-bank for training and trees from the same treebankfor evaluation.
As such, the best model for un-supervised parsing is the one that reports the bestperformance.Unambiguous Non-Terminally Separated(UNTS) grammars have properties that makethem attractive for grammatical inference.
Thesegrammars have been shown to be PAC-learnablein polynomial time (Clark, 2006), meaning thatunder certain circumstances, the underlyinggrammar can be learned from a sample of theunderlying language.
Moreover, UNTS grammarshave been successfully used to induce grammarsfrom unannotated corpora in competitions oflearnability of formal languages (Clark, 2007).UNTS grammars can be used for modeling nat-ural language.
They can be induced using anytraining material, the induced models can be eval-uated using trees from a treebank, and their per-formance can be compared against state-of-the-art unsupervised models.
Different learning al-gorithms might produce different grammars and,consequently, different scores.
The fact that theclass of UNTS grammars is PAC learnable doesnot convey any information on the possible scoresthat different UNTS grammars might produce.From a performance oriented perspective it mightbe possible to have an upper bound over the setof possible scores of UNTS grammars.
Knowingan upper bound is complementary to knowing thatthe class of UNTS grammars is PAC learnable.Such upper bound has to be defined specificallyfor UNTS grammars and has to take into accountthe treebank used as test set.
The key questionis how to compute it.
Suppose that we want toevaluate the performance of a given UNTS gram-mar using a treebank.
The candidate grammar pro-duces a tree for each sentence and those trees arecompared to the original treebank.
We can thinkthat the candidate grammar has produced a newversion of the treebank, and that the score of thegrammar is a measure of the closeness of the newtreebank to the original treebank.
Finding the bestupper bound is equivalent to finding the closestUNTS version of the treebank to the original one.Such bounds are difficult to find for most classesof languages because the search space is theset of all possible versions of the treebank thatmight have been produced by any grammar in theclass under study.
In order to make the problemtractable, we need the formalism to have an easyway to characterize all the versions of a treebank58it might produce.
UNTS grammars have a specialcharacterization that makes the search space easyto define but whose exploration is NP-hard.In this paper we present a way to characterizeUNTS grammars and a metric function to mea-sure the closeness between two different versionof a treebank.
We show that the problem of find-ing the closest UNTS version of the treebank canbe described as Maximum Weight Independent Set(MWIS) problem, a well known NP-hard problem(Karp, 1972).
The exploration algorithm returnsa version of the treebank that is the closest to thegold standard in terms of our own metric.We show that the F1-measure is related to ourmeasure and that it is possible to find and upperbound of the F1-performance for all UNTS gram-mars.
Moreover, we compute this upper bound forthe WSJ10, a subset of the Penn Treebank (Mar-cus et al, 1994) using POS tags as the alphabet.The upper bound we found is 82.2% for the F1measure.
Our result suggest that UNTS grammarsare a formalism that has the potential to achievestate-of-the-art unsupervised parsing performancebut does not guarantee that there exists a grammarthat can actually achieve the 82.2%.To the best of our knowledge, there is no pre-vious research on finding upper bounds for perfor-mance over a concrete class of grammars.
In Kleinand Manning (2004), the authors compute an up-per bound for parsing with binary trees a gold tree-bank that is not binary.
This upper bound, that is88.1% for the WSJ10, is for any parser that returnsbinary trees, including the concrete models devel-oped in the same work.
But their upper bound doesnot use any specific information of the concretemodels that may help them to find better ones.The rest of the paper is organized as follows.Section 2 presents our characterization of UNTSgrammars.
Section 3 introduces the metric we op-timized and explains how the closest version of thetreebank is found.
Section 4 explains how the up-per bound for our metric is translated to an up-per bound of the F1 score.
Section 5 presents ourbound for UNTS grammars using the WSJ10 andfinally Section 6 concludes the paper.2 UNTS Grammars and LanguagesFormally, a context free grammar G =(?, N, S, P ) is said to be Non-Terminally Sepa-rated (NTS) if, for all X,Y ?
N and ?, ?, ?
?(?
?
N)?
such that X ??
???
and Y ??
?, wehave that X ??
?Y ?
(Clark, 2007).
UnambiguousNTS (UNTS) grammars are those NTS grammarsthat parses unambiguously every instance of thelanguage.Given any grammar G, a substring s of r ?L(G) is called a constituent of r if and only if thereis an X in N such that S ??
uXv ??
usv = r.In contrast, a string s is called a non-constituent ordistituent of r ?
L(G) if s is not a constituent of r.We say that s is a constituent of a language L(G)if for every r that contains s, s is a constituent ofr.
In contrast, s is a distituent of L(G) if for everyr where s occurs, s is a distituent of r.An interesting characterization of finite UNTSgrammars is that every substring that appear insome string of the language is always a constituentor always a distituent.
In other words, if there is astring r in L(G) for which s is a constituent, thens is a constituent of L(G).
By means of this prop-erty, if we ignore the non-terminal labels, a finiteUNTS language is fully determined by its set ofconstituents C. We can show this property for fi-nite UNTS languages.
We believe that it can alsobe shown for non-finite cases, but for our purposesthe finite cases suffices, because we use grammarsto parse finite sets of sentences, specifically, thesentences of test treebanks.
We know that for ev-ery finite subset of an infinite language producedby a UNTS grammar G, there is a UNTS gram-mar G?
whose language is finite and that parsesthe finite subset as G. If we look for the upperbound among the grammars that produce a finitelanguage, this upper bound is also an upper boundfor the class of infinite UNTS grammars.The UNTS characterization plays a very im-portant role in the way we look for the upperbound.
Our method focuses on how to determinewhich of the constituents that appear in the goldare actually the constituents that produce the up-per bound.
Suppose that a given gold treebankcontains two strings ?
and ?
such that they occuroverlapped.
That is, there exist non-empty strings?
?, ?, ??
such that ?
= ???
and ?
= ???
and?????
occurs in the treebank.
If C is the set ofconstituents of a UNTS grammar it can not haveboth ?
and ?.
It might have one or the other, butif both belong to C the resulting language can notbe UNTS.
In order to find the closest UNTS gram-mar we design a procedure that looks for the sub-set of all substrings that occur in the sentences ofthe gold treebank that can be the constituent set C59of a grammar.
We do not explicitly build a UNTSgrammar, but find the set C that produces the bestscore.We say that two strings ?
and ?
are compatiblein a language L if they do not occur overlappedin L, and hence they both can be members of C.If we think of L as a subset of an infinite lan-guage, it is not possible to check that two overlap-ping strings do not appear overlapped in the ?real?language and hence that they are actually com-patible.
Nevertheless, we can guarantee compat-ibility between two strings ?, ?
by requiring thatthey do not overlap at all, this is, that there areno non-empty strings ?
?, ?, ??
such that ?
= ??
?and ?
= ???.
We call this type of compatibilitystrong compatibility.
Strong compatibility ensuresthat two strings can belong to C regardless of L.In our experiments we focus on finding the best setC of compatible strings.Any set of compatible strings C extracted fromthe gold treebank can be used to produce a newversion of the treebank.
For example, Figure 1shows two trees from the WSJ Penn Treebank.The string ?in the dark?
occurs as a constituent in(a) and as a distituent in (b).
If C contains ?in thedark?, it can not contain ?the dark clouds?
giventhat they overlap in the yield of (b).
As a con-sequence, the new treebank correctly contains thesubtree in (a) but not the one in (b).
Instead, theyield of (b) is described as in (c) in the new tree-bank.C defines a new version of the treebank that sat-isfies the UNTS property.
Our goal is to obtain atreebank T ?
such that (a) T ?
and T are treebanksover the same set of sentences, (b) T ?
is UNTS,and (c) T ?
is the closest treebank to T in terms ofperformance.
The three of them imply that anyother UNTS grammar is not as similar as the onewe found.3 Finding the Best UNTS GrammarAs our goal is to find the closest grammar in termsof performance, we need to define first a weightfor each possible grammar and second, an algo-rithm that searches for the grammar with the bestweight.
Ideally, the weight of a candidate gram-mar should be in terms of F1, but we can showthat optimization of this particular metric is com-putationally hard.
Instead of defining F1 as theirscore, we introduce a new metric that is easier tooptimize, we find the best grammar for this met-ric, and we show that the possible values of F1can be bounded by a function that takes this scoreas argument.
In this section we present our metricand the technique we use to find a grammar thatreports the best value for our metric.If the original treebank T is not produced by anyUNTS grammar, then there are strings in T thatare constituents in some sentences and that are dis-tituents in some other sentences.
For each one ofthem we need a procedure to decide whether theyare members of C or not.
If a string ?
appears asignificant number of times more as a constituentthan as a distituent the procedure may choose toinclude it in C at the price of being wrong a fewtimes.
That is, the new version of T has all occur-rences of ?
either as constituents or as distituents.The treebank that has all of its occurrences as con-stituents differs from the original in that there aresome occurrences of ?
that were originally dis-tituents and are marked as constituents.
Similarly,if ?
is marked as distituent in the new treebank, ithas occurrences of ?
that were constituents in T .The decision procedure becomes harder whenall the substrings that appear in the treebank areconsidered.
The increase in complexity is a con-sequence of the number of decisions the procedureneeds to take and the way these decisions interfereone with another.
We show that the problem ofdetermining the set C is naturally embedded in agraph NP-hard problem.
We define a way to lookfor the optimal grammars by translating our prob-lem to a well known graph problem.
Let L be thethe set of sentences in a treebank, and let S(L) beall the possible non-empty proper substrings of L.We build a weighted undirected graph G in termsof the treebank as follows.
Nodes in G correspondto strings in S(L).
The weight of a node is a func-tion w(s) that models our interest of having s se-lected as a constituent; w(s) is defined in terms ofsome information derived from the gold treebankT and we discuss it later in this section.
Finally,two nodes a and b are connected by an edge if theirtwo corresponding strings conflict in a sentence ofT (i.e., they are not compatible in L).Not all elements of L are in S(L).
We did notinclude L in S(L) for two practical reasons.
Thefirst one is that to require L in S(L) is too re-strictive.
It states that all strings in L are in factconstituents.
If two string ab and bc of L oc-cur overlapped in a third string abc then there isno UNTS grammar capable of having the three of60(a)PRPweVBP?reINinDTtheJJdark(b)INin DTtheJJdarkNNSclouds(c)INinDTtheJJdarkNNScloudsFigure 1: (a) and (b) are two subtrees that show ?in the dark?
as a constituent and as a distituent respec-tively.
(c) shows the result of choosing ?in the dark?
as a constituent.them as constituents.
The second one is that in-cluding them produces graphs that are too sparse.If they are included in the graph, we know thatany solution should contain them, consequently,all their neighbors do not belong to any solutionand they can be removed from the graph.
Our ex-periments show that the graph that results from re-moving nodes related to nodes representing stringsin L are too small to produce any interesting result.By means of representing the treebank as agraph, selecting a set of constituents C ?
S(L)is equivalent to selecting an independent set ofnodes in the graph.
An independent set is a sub-set of the set of nodes that do not have any pairof nodes connected by an edge.
Clearly, there areexponentially many possible ways to select an in-dependent set, and each of these sets represents aset of constituents.
But, since we are interested inthe best set of constituents, we associate to eachindependent set C the weight W (C) defined as?s?C w(s).
Our aim is then to find a set Cmaxthat maximizes this weight.
This problem is a wellknown problem of graph theory known in the lit-erature as the Maximum Weight Independent Set(MWIS) problem.
This problem is also known tobe NP-hard (Karp, 1972).We still have to choose a definition for w(s).We want to find the grammar that maximizes F1.Unfortunately, F1 can not be expressed in terms ofa sum of weights.
Maximization of F1 is beyondthe expressiveness of our model, but our strategyis to define a measure that correlates with F1 andthat can be expressed as a sum of weights.In order to introduce our measure, we first de-fine c(s) and d(s) as the number of times a strings appears in the gold treebank T as a constituentand as a distituent respectively.
Observe that ifwe choose to include s as a constituent of C, theresulting treebank T ?
contains all the c(s) + d(s)occurrences of s as a constituent.
c(s) of the s oc-currences in T ?
are constituents as they are in Tand d(s) of the occurrences are constituents in T ?but are in fact distituents in T .
We want to max-imize c(s) and minimize d(s) at the same time.This can be done by defining the contribution of astring s to the overall score asw(s) = c(s)?
d(s).With this definition of w, the weight W (C) =?s?C w(s) becomes the number of constituentsof T ?
that are in T minus the number of con-stituents that do not.
If we define the number ofhits to be H(C) =?s?C c(s) and the number ofmisses to be M(C) =?s?C d(s) we have thatW (C) = H(C)?M(C).
(1)As we confirm in Section 5, graphs tend to bevery big.
In order to reduce the size of the graphs,if a string s has w(s) ?
0, we do not include itscorresponding node in the graph.
An independentset that does not include s has an equal or higherW than the same set including s.For example, let T be the treebank in Fig-ure 2 (a).
The sets of substrings such thatw(c) ?
0 is {da, cd, bc, cda, ab, bch}.
Thegraph that corresponds to this set of strings isgiven in Figure 3.
Nodes corresponding tostrings {dabch, bcda, abe, abf, abg, bci, daj} arenot shown in the figure because the strings donot belong to S(L).
The figure also shows theweights associated to the substrings according totheir counts in Figure 2 (a).
The shadowed nodescorrespond to the independent set that maximizesW .
The trees in the Figure 2 (b) are the sentencesof the treebank parsed according the optimal inde-pendent set.4 An Upper Bound for F1Even though finding the independent set that max-imizes W is an NP-Hard problem, there are in-stances where it can be effectively computed, aswe show in the next section.
The set Cmax max-imizes W for the WSJ10 and we know that allothers C produces a lower value of W .
In otherwords, the set Cmax produce a treebank Tmax that61(a)d ab c h(da)((bc)h)bc d ab((cd)a)a b e(ab)ea b f(ab)fa b g(ab)gb c i(bc)id a j(da)j(b)da b c hd(ab)chbc d ab((cd)a)a b e(ab)ea b f(ab)fa b g(ab)gb c ibcid a jdajFigure 2: (a) A gold treebank.
(b) The treebank generated by the grammar C = L ?
{cd, ab, cda}.Figure 3: Graph for the treebank of Figure 2.is the closest UNTS version to the WSJ10 in termsof W .
We can compute the precision, recall andF1 for Cmax but there is no warranty that the F1score is the best for all the UNTS grammars.
Thisis the case because F1 and W do not define thesame ordering over the family of candidate con-stituent sets C: there are gold treebanks T (usedfor computing the metrics), and sets C1, C2 suchthat F1(C1) < F1(C2) and W (C1) > W (C2).For example, consider the gold treebank T in Fig-ure 4 (a).
The table in Figure 4 (b) displays twosets C1 and C2, the treebanks they produce, andtheir values of F1 and W .
Note that C2 is the re-sult of adding the string ef to C1, also note thatc(ef) = 1 and d(ef) = 2.
This improves the F1score but produces a lower W .The F1 measure we work with is the one de-fined in the recent literature of unsupervised pars-ing (Klein and Manning, 2004).
F1 is defined interms of Precision and Recall as usual, and the lasttwo measures are micro-averaged measures thatinclude full-span brackets, and that ignore bothunary branches and brackets of span one.
For sim-plicity, the previous example does not count thefull-span brackets.As the example shows, the upper bound for Wmight not be an upper bound of F1, but it is pos-sible to find a way to define an upper bound ofF1 using the upper bound of W .
In this sectionwe define a function f with the following prop-erty.
Let X and Y be the sets of W -weights andF1-weights for all possible UNTS grammars re-spectively.
Then, if w is an upper bound of X ,then f(w) is an upper bound of Y .
The function fis defined as follows:f(w) = F1( 12?
wK, 1)(2)where F1(p, r) = 2prp+r , and K =?s?ST c(s) isthe total number of constituents in the gold tree-bank T .
From it, we can also derive values forprecision and recall: precision 12?
wK and recall 1.A recall of 1 is clearly an upper bound for all thepossible values of recall, but the value given forprecision is not necessarily an upper bound for allthe possible values of precision.
It might exist agrammar having a higher value of precision butwhose F1 has to be below our upper bound.The rest of section shows that f(W ) is an up-per bound for F1, the reader not interested in thetechnicalities can skip it.The key insight for the proof is that both metricsF1 and W can be written in terms of precision andrecall.
Let T be the treebank that is used to com-pute all the metrics.
And let T ?
be the treebankproduced by a given constituent set C. If a strings belongs to C, then its c(s) + d(s) occurrencesin T ?
are marked as constituents.
Moreover, s iscorrectly tagged a c(s) number of times while itis incorrectly tagged a d(s) number of times.
Us-ing this, P , R and F1 can be computed for C asfollows:P (C) =Ps?C c(s)Ps?C c(s)+d(s)= H(C)H(C)+M(C) (3)R(C) =Ps?C c(s)K= H(C)K (4)F1(C) = 2P (C)R(C)P (C)+R(C)= 2H(C)K+H(C)+M(C)62(a)a b c(ab)ca b da(bd)e f g(ef)ge f hefhe f iefi(b)C T ?C P R F1 WC1 = {abc, abd, efg, efh, efi, ab} {(ab)c, (ab)d, efg, efh, efi} 50% 33% 40% 1 ?
1 = 0C2 = {abc, abd, efg, efh, efi, ab, ef} {(ab)c, (ab)d, (ef)g, (ef)h, (ef)i} 40% 67% 50% 2 ?
3 = ?1Figure 4: (a) A gold treebank.
(b) Two grammars, the treebanks they generate, and their scores.W can also be written in terms of P and R asW (C) = (2?
1P (C))R(C)K (5)This formula is proved to be equivalent to Equa-tion (1) by replacing P (C) and R(C) with equa-tions (3) and (4) respectively.
Using the last twoequations, we can rewrite F1 and W taking p andr, representing values of precision and recall, asparameters:F1(p, r) = 2prp+ rW (p, r) = (2?
1p)rK (6)Using these equations, we can prove that fcorrectly translates upper bounds of W to upperbounds of F1 using calculus.
In contrast to F1,W not necessarily take values between 0 and 1.
In-stead, it takes values between K and ??.
More-over, it is negative when p < 12 , and goes to ?
?when p goes to 0.
Let C be an arbitrary UNTSgrammar, and let pC , rC and wC be its precision,recall and W -weight respectively.
Let w be ourupper bound, so that wC ?
w. If f1C is definedas F1(pC , rC) we need to show that f1C ?
f(w).We bound f1C in two steps.
First, we show thatf1C ?
f(wC)and second, we show thatf(wC) ?
f(w).The first inequality is proved by observing thatf1C and f(wC) are the values of the functionf1(r) = F1( 12?
wCKr, r)at the points r = rC and r = 1 respectively.This function corresponds to the line defined bythe F1 values of all possible models that have afixed weight W = wC .
The function is monoton-ically increasing in r, so we can apply it to bothsides of the following inequality rC ?
1, which istrivially true.
As result, we get f1C ?
f(wC) asrequired.
The second inequality is proved by ob-serving that f(w) is monotonically increasing inw, and by applying it to both sides of the hypothe-sis wc ?
w.5 UNTS Bounds for the WSJ10 TreebankIn this section we focus on trying to find real upperbounds building the graph for a particular treebankT .
We find the best independent set, we build theUNTS version Tmax of T and we compute the up-per bound for F1.
The treebank we use for exper-iments is the WSJ10, which consists of the sen-tences of the WSJ Penn Treebank whose lengthis at most 10 words after removing punctuationmarks (Klein and Manning, 2004).
We also re-moved lexical entries transforming POS tags intoour terminal symbols as it is usually done (Kleinand Manning, 2004; Bod, 2006a).We start by finding the best independent set.
Tosolve the problem in the practice, we convert itinto an Integer Linear Programming (ILP) prob-lem.
ILP is also NP-hard (Karp, 1972), but thereis software that implements efficient strategies forsolving some of its instances (Achterberg, 2004).ILP problems are defined by three parameters.First, there is a set of variables that can take val-ues from a finite set.
Second, there is an objectivefunction that has to be maximized, and third, thereis a set of constraints that must be satisfied.
In ourcase, we define a binary variable xs ?
{0, 1} forevery node s in the graph.
Its value is 1 or 0, thatrespectively determines the presence or absence ofs in the set Cmax.
The objective function is?s?S(L)xsw(s)The constraints are defined using the edges of the63graph.
For every edge (s1, s2) in the graph, weadd the following constraint to the problem:xs1 + xs2 ?
1The 7422 trees of the WSJ10 treebank have atotal of 181476 substrings of length ?
2, thatform the set S(L) of 68803 different substrings.The number of substrings in S(L) does not growtoo much with respect to the number of strings inL because substrings are sequences of POS tags,meaning that each substring is very frequent in thecorpus.
If substrings were made out of words in-stead of POS tags, the number of substrings wouldgrow much faster, making the problem harder tosolve.
Moreover, removing the strings s such thatw(s) ?
0 gives a total of only 7029 substrings.Since there is a node for each substring, the result-ing graph contains 7029 nodes.
Recall that thereis an edge between two strings if they occur over-lapped.
Our graph contains 1204 edges.
The ILPversion has 7029 variables, 1204 constraints andthe objective function sums over 7029 variables.These numbers are summarized in Table 1.The solution of the ILP problem is a set of6583 variables that are set to one.
This set corre-sponds to a set Cmax of nodes in our graph of thesame number of elements.
Using Cmax we builda new version Tmax of the WSJ10, and computeits weight W , precision, recall and F1.
Their val-ues are displayed in Table 2.
Since the elementsof L were not introduced in S(L), elements of Lare not necessarily in Cmax, but in order to com-pute precision and recall, we add them by hand.Strictly speaking, the set of constituents that weuse for building Tmax is Cmax plus the full spanbrackets.We can, using equation (2), compute the up-per bound of F1 for all the possible scores of allUNTS grammars that use POS tags as alphabet:f(wmax) = F1( 12?
wmaxK, 1)= 82.2%The precision for this upper bound isP (wmax) =12?
wmaxK= 69.8%while its recall is R = 100%.
Note from the pre-vious section that P (wmax) is not an upper boundfor precision but just the precision associated tothe upper bound f(wmax).Gold constituents K 35302Strings |S(L)| 68803Nodes 7029Edges 1204Table 1: Figures for the WSJ10 and its graph.Hits H 22169Misses M 2127Weight W 20042Precision P 91.2%Recall R 62.8%F1 F1 74.4%Table 2: Summary of the scores for Cmax.Table 3 shows results that allow us to com-pare the upper bounds with state-of-the-art pars-ing scores.
BestW corresponds to the scores ofTmax and UBoundF1 is the result of our transla-tion function f .
From the table we can see thatan unsupervised parser based on UNTS grammarsmay reach a sate-of-the-art performance over theWSJ10.
RBranch is a WSJ10 version where alltrees are binary and right branching.
DMV, CCMand DMV+CCM are the results reported in Kleinand Manning (2004).
U-DOP and UML-DOPare the results reported in Bod (2006b) and Bod(2006a) respectively.
Incremental refers to the re-sults reported in Seginer (2007).We believe that our upper bound is a generousone and that it might be difficult to achieve it fortwo reasons.
First, since the WSJ10 corpus isa rather flat treebank, from the 68803 substringsonly 10% of them are such that c(s) > d(s).
Ourprocedure has to decide among this 10% whichof the strings are constituents.
An unsupervisedmethod has to choose the set of constituents fromthe set of all 68803 possible substrings.
Second,we are supposing a recall of 100% which is clearlytoo optimistic.
We believe that we can find atighter upper bound by finding an upper bound forrecall, and by rewriting f in equation (2) in termsof the upper bound for recall.It must be clear the scope of the upper boundwe found.
First, note that it has been computedover the WSJ10 treebank using the POS tags asthe alphabet.
Any other alphabet we use, like forexample words, or pairs of words and POS tags,changes the relation of compatibility among thesubstrings, making a completely different universe64Model UP UR F1RBranch 55.1 70.0 61.7DMV 46.6 59.2 52.1CCM 64.2 81.6 71.9DMV+CCM 69.3 88.0 77.6U-DOP 70.8 88.2 78.5UML-DOP 82.9Incremental 75.6 76.2 75.9BestW(UNTS) 91.2 62.8 74.4UBoundF1(UNTS) 69.8 100.0 82.2Table 3: Performance on the WSJ10 of the mostrecent unsupervised parsers, and our upper boundson UNTS.of UNTS grammars.
Second, our computation ofthe upper bound was not made for supersets of theWSJ10.
Supersets such as the entire Penn Tree-bank produce bigger graphs because they containlonger sentences and various different sequencesof substrings.
As the maximization of W is anNP-hard problem, the computational cost of solv-ing bigger instances grows exponentially.
A thirdlimitation that must be clear is about the modelsaffected by the bound.
The upper bound, and ingeneral the method, is only applicable to the classof formal UNTS grammars, with only some veryslight variants mentioned in the previous sections.Just moving to probabilistic or weighted UNTSgrammars invalidates all the presented results.6 ConclusionsWe present a method for assessing the potential ofUNTS grammars as a formalism for unsupervisedparsing of natural language.
We assess their po-tential by finding an upper bound of their perfor-mance when they are evaluated using the WSJ10treebank.
We show that any UNTS grammars canachieve at most 82.2% of F1 measure, a valuecomparable to most state-of-the-art models.
In or-der to compute this upper bound we introduceda measure that does not define the same orderingamong UNTS grammars as the F1, but that hasthe advantage of being computationally easier tooptimize.
Our measure can be used, by means ofa translation function, to find an upper bound forF1.
We also showed that the optimization proce-dure for our metric maps into an NP-Hard prob-lem, but despite this fact we present experimen-tal results that compute the upper bound for theWSJ10 when POS tags are treated as the grammaralphabet.From a more abstract perspective, we intro-duced a different approach to assess the usefulnessof a grammatical formalism.
Usually, formalismare proved to have interesting learnability proper-ties such as PAC-learnability or convergence of aprobabilistic distribution.
We present an approachthat even though it does not provide an effectiveway of computing the best grammar in an unsu-pervised fashion, it states the upper bound of per-formance for all the class of UNTS grammars.AcknowledgmentsThis work was supported in part by grant PICT2006-00969, ANPCyT, Argentina.
We would liketo thank Pablo Rey (UDP, Chile) for his helpwith ILP, and Demetrio Mart?n Vilela (UNC, Ar-gentina) for his detailed review.ReferencesTobias Achterberg.
2004.
SCIP - a framework to in-tegrate Constraint and Mixed Integer Programming.Technical report.Rens Bod.
2006a.
An all-subtrees approach to unsu-pervised parsing.
In Proceedings of COLING-ACL2006.Rens Bod.
2006b.
Unsupervised parsing with U-DOP.In Proceedings of CoNLL-X.Alexander Clark.
2006.
PAC-learning unambiguousNTS languages.
In Proceedings of ICGI-2006.Alexander Clark.
2007.
Learning deterministic con-text free grammars: The Omphalos competition.Machine Learning, 66(1):93?110.Richard M. Karp.
1972.
Reducibility among com-binatorial problems.
In R. E. Miller and J. W.Thatcher, editors, Complexity of Computer Compu-tations, pages 85?103.
Plenum Press.Dan Klein and Christopher D. Manning.
2004.Corpus-based induction of syntactic structure: Mod-els of dependency and constituency.
In Proceedingsof ACL 42.Mitchell P. Marcus, Beatrice Santorini, and Mary A.Marcinkiewicz.
1994.
Building a large annotatedcorpus of english: The Penn treebank.
Computa-tional Linguistics, 19(2):313?330.Yoav Seginer.
2007.
Fast unsupervised incrementalparsing.
In Proceedings of ACL 45.65
