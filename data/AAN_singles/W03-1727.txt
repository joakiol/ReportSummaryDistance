Chinese Word Segmentation in MSR-NLPAndi WuMicrosoft ResearchOne Microsoft Way, Redmond, WA 98052andiwu@microsoft.comAbstractWord segmentation in MSR-NLP is an in-tegral part of a sentence analyzer whichincludes basic segmentation, derivationalmorphology, named entity recognition,new word identification, word latticepruning and parsing.
The final segmenta-tion is produced from the leaves of parsetrees.
The output can be customized tomeet different segmentation standardsthrough the value combinations of a set ofparameters.
The system participated infour tracks of the segmentation bakeoff --PK-open, PK-close, CTB-open and CTB-closed ?
and ranked #1, #2, #2 and #3 re-spectively in those tracks.
Analysis of theresults shows that each component of thesystem contributed to the scores.1 System DescriptionThe MSR-NLP Chinese system that participated inthe current segmentation bakeoff is not a stand-alone word segmenter.
It is a Chinese sentenceanalyzer where the leaves of parse trees are dis-played as the output of word segmentation.
Thecomponents of this system are described below.1.1 Basic segmentationEach input sentence is first segmented into indi-vidual characters.
1   These characters and theircombinations are then looked up in a dictionary2and a word lattice containing lexicalized wordsonly is formed.
Each node in the lattice is a feature1 If an input line contains more than one sentence, a sentenceseparator is applied to break the line into individual sentences,which are then processed one by one and the results are con-catenated to form a single output.2 The lookup is optimized so that not all possible combinationsare tried.matrix that contains the part of speech and othergrammatical attributes.
Multiple-character wordsmay also have information for resolving segmenta-tion ambiguities.
In general, multiple-characterwords are assigned higher scores than the wordsthey subsume, but words like ????
are excep-tions and such exceptional cases are usuallymarked in the dictionary.
For some of the wordsthat tend to overlap with other words, there is alsoinformation as to what the preferred segmentationis.
For instance, the preferred segmentation for?????
is ??+???
rather than ???+?
?.Such information was collected from segmentedcorpora and stored in the dictionary.
The scoresare later used in word lattice pruning and parseranking (Wu and Jiang 1998).1.2 Derivational morphology and named en-tity recognitionAfter basic segmentation, a set of augmentedphrase structure rules are applied to the word lat-tice to form larger word units which include:?
Words derived from morphological proc-esses such as reduplication, affixation,compounding, merging, splitting, etc.?
Named entities such as person names,place names, company names, productnames, numbers, dates, monetary units, etc.Each of these units is a tree that reflects the historyof rule application.
They are added to the existingword lattice as single nodes and treated as singlewords by the parser.
The internal structures areuseful for various purposes, one of which is thecustomization of word segmentation:  words withsuch structures can all be displayed as single wordsor multiple words depending on where the ?cuts?are made in the word tree (Wu 2003).1.3 New word identificationThe expanded word lattice built in 1.2 is inspectedto detect spots of possible OOV new words.
Typi-cal spots of this kind are sequences of single char-acters that are not subsumed by longer words.
Wethen use the following information to propose newwords (Wu and Jiang, 2000).?
The probability of the character string be-ing a sequence of independent words;?
The morphological and syntactic proper-ties of the characters;?
Word formation rules;?
Behavior of each character in existingwords (e.g.
how likely is this character tobe used as the second character of a two-character verb).?
The context in which the characters appear.The proposed new words are added to the wordlattice and they will get used if no successful parsecan be obtained without them.
When a new wordproposed this way has been verified by the parser(i.e.
used in a successful parse) more than n times,it will automatically become an entry in the dic-tionary.
From then on, this word can be looked updirectly from the dictionary instead of being pro-posed online.
This kind of dynamic lexical acquisi-tion has been presented in Wu et al(2002).1.4 Word lattice pruningNow that all the possible words are in the wordlattice, both statistical and linguistic methods areapplied to eliminate certain paths.
For instance,those paths that contain one or more bound mor-phemes are pruned away.
Single characters thatare subsumed by longer words are also thrown outif their independent word probabilities are verylow.
The result is a much smaller lattice that re-sembles the n-best paths produced by a statisticalword segmenter.
Because the final resolution ofambiguities is expected to be done during parsing,the lattice pruning is non-greedy so that no plausi-ble path will be excluded prematurely.
Many ofthe ambiguities that are eliminated here can also beresolved by the parser, but the pruning greatly re-duces the complexity of the parsing process, mak-ing the parser much faster and more accurate.1.5 ParsingThe cleaned-up word lattice is then submitted tothe parser as the initial entries in the parsing chart.With the assumption that a successful parse of thesentence requires a correct segmentation of thesentence, many segmentation ambiguities are ex-pected to be resolved here.
This assumption doesnot always hold, of course.
A sentence can oftenbe parsed in multiple ways and the top-rankingparse is not always the correct one.
There are alsosentences that are not covered by the grammar andtherefore cannot be parsed at all.
In this latter case,we back off to partial parsing and use dynamicprogramming to assemble a tree that consists of thelargest sub-trees in the chart.In most cases, the use of the parser results inbetter segmentation, but the parser can also mis-lead us.
One of the problems is that the parsertreats every input as a sentence and tries to con-struct an S out of it.
As a result, even a name like?????
can be analyzed as a sentence with ?
asthe subject, ?
as the verb and ?
as the object, if itappears in the wrong context (or no context).1.6 Segmentation parametersDue to the differences in segmentation standards,the leaves of a parse tree do not always correspondto the words in a particular standard.
For instance,a Chinese full name is a single leaf in our trees, butit is supposed to be two words (family name +given name) according to the PK standard.
Fortu-nately, most of the words whose segmentation iscontroversial are built dynamically in our systemwith their internal structures preserved.
A Chinesefull name, for example, is a word tree where thetop node dominates two nodes: the family nameand the given name.
Each non-terminal node in aword tree as described in 1.2 is associated with aparameter whose value determines whether thedaughters of this node are to be displayed as asinge word or multiple words.
Since all the dy-namic words are built by phrase structure rules andtheir word trees reflect the derivational history ofrule application, there is a one-to-one correspon-dence between the types of words and the word-internal structures of those words.
A segmentationparameter is associated with each type of words3and the value of this parameter determines how thegiven type of words should be segmented.
Thismakes it possible for the system to quickly adapt todifferent standards (Wu 2003).3 There are about 50 parameters in our system.1.7 SpeedOur system is not optimized for word segmentationin terms of speed.
As we have seen, the system isa sentence analyzer and word segmentation is justthe by-product of a parser.
The speed we reporthere is in fact the speed of parsing.On a single 997 MHz Pentium III machine, thesystem is able to process 28,740 characters perminute.
The speed may vary according to sentencelengths: given texts of the same size, those contain-ing longer sentences will take more time.
Thenumber reported here is an average of the timetaken to process the test sets of the four tracks weparticipated in.We have the option of turning off the parserduring word segmentation.
When the parser isturned off, segmentation is produced directly fromthe word lattice with dynamic programming whichselects the shortest path.
The speed in this case isabout 60,000 characters per minute.2 EvaluationWe participated in the four GB tracks in the firstinternational Chinese word segmentation bakeoff -PK-open, PK-closed, CTB-open and CTB-closed ?and ranked #1, #2, #2, and #3 respectively in thosetracks.
In what follows, we discuss how we got theresults: what dictionaries we used, how we usedthe training data, how much each component con-tributed to the scores, and the problems that af-fected our performance.2.1 DictionariesIn the open tracks, we used our proprietary dic-tionary of 89,845 entries, which includes the en-tries of 7,017 single characters.
In the closedtracks, we removed from the dictionary all thewords that did not appear in the training data, butkept all the single characters.
This resulted in adictionary of 34,681 entries in the PK track and18,207 entries in the CTB track.
It should be notedthat not all the words in the training data are in ourdictionary.
This explains why the total numbers ofentries in those reduced dictionaries are smallerthan the vocabulary sizes of the respective trainingsets even with all the single-character entries in-cluded in them.The dictionary we use in each case is not a sim-ple word list.
Every word has one or more parts-of-speech and a number of other grammatical fea-tures.
No word can be used by the parser unless ithas those features.
This made it very difficult forus to add all the words in the training data to thedictionary.
We did use a semi-automatic processto add as many words as possible, but both the ac-curacy and coverage of the added grammatical fea-tures are questionable due to the lack of manualverification.2.2 Use of the training dataWe used the training data mainly to tune the seg-mentation parameters of our system.
As has beenmentioned in 1.6, there are about 50 types of mor-phologically derived words that are built online inour system and each type has a parameter to de-termine whether a given unit should be displayedas a single word or separate words.
Since our de-fault segmentation is very different from PK orCTB, and PK and CTB also follow different guide-lines, we had to try different value combinations ofthe parameters in each case until we got the opti-mal settings.The main problem in the tuning is that manymorphologically derived words have been lexical-ized in our dictionary and therefore do not have theword-internal structures that they would have ifthey had been constructed dynamically.
As a re-sult, their segmentation is beyond the control ofthose parameters.
To solve this problem, we usedthe training data to automatically identify all suchcases, create a word-internal structure for each ofthem, and store the word tree in their lexical en-tries.4  This made it possible for the parameter val-ues to apply to both the lexicalized and non-lexicalized words.
This process can be fairlyautomatic if the annotation of the training data iscompletely consistent.
However, as we have dis-covered, the training data is not as consistent asexpected, which made total automation impossible.2.3 Contribution of each componentAfter we received our individual scores and thereference testing data, we did some ablation ex-4 The work is incomplete, since the trees were created only forthose words that are in the training data provided.periments to find out the contribution of each sys-tem component in this competition.
We turned offthe components one at a time (except basic seg-mentation) and recorded the scores of each ablatedsystem.
The results are summarized in the follow-ing table, where ?DM-NER?
stands for ?deriva-tional morphology and named entity recognition?,?NW-ID?
for ?new word identification and lexical-ization?, ?pruning?
for ?lattice pruning?
and ?tun-ing?
for ?tuning of parameter values?.
Each cell inthe table has two percentages.
The top one is theF-measure and the bottom one is the OOV wordrecall rate.PKOpenPKclosedCTBopenCTBclosedCompleteSystem95.9 %79.9 %94.7 %68.0 %90.1 %73.8 %83.1 %43.1 %WithoutDM-NER90.2 %44.4 %88.9 %33.9 %86.6 %66.6 %79.2 %33.5 %WithoutNW-ID95.8 %77.3 %94.0 %61.2 %88.7 %69.0 %79.2 %28.2 %WithoutPruning92.0 %77.5 %90.9 %65.9 %85.5 %69.0 %78.8 %39.5 %WithoutParsing95.5 %79.9 %94.4 %68.5 %89.8 %75.0 %84.0 %48.1 %WithoutTuning84.8 %43.4 %83.9 %33.3 %84.8 %72.3 %78.4 %43.3 %Several interesting facts are revealed in thisbreak-down:?
The tuning of parameter values has the big-gest impact on the scores across the board.?
Derivational morphology and NE recogni-tion is also a main contributor, especially inthe PK sets, which presumably containsmore named entities.?
The impact of new word identification isminimal when the OOV word rate is low,such as in the PK-open case, but becomesmore and more significant as the OOV rateincreases.?
Lattice pruning makes a big difference aswell.
Apparently it cannot be replaced bythe parser in terms of the disambiguatingfunction it performs.
Another fact, which isnot represented in the table, is that parsingis three times slower when lattice pruning isturned off.?
The parser has very limited impact on thescores.
Looking at the data, we find thatparsing did help to resolve some of themost difficult cases of ambiguities and wewould not be able to get the last few pointswithout it.
But it seems that most of thecommon problems can be solved withoutthe parser.
In one case (CTB closed), thescore is higher when the parser is turned off.This is because the parser may prefer astructure where those dynamically recog-nized OOV words are broken up intosmaller units.
For practical purposes, there-fore, we may choose to leave out the parser.2.4 Problems that affected our performanceThe main problem is the definition of new words.While our system is fairly aggressive in recogniz-ing new words, both PK and CTB are quite con-servative in this respect.
Expressions such as ???
?, ???
?, ????
?, ??????
areconsidered single words in our system but not so inPK or CTB.
This made our new word recognitiondo more harm than good in many cases, though theoverall impact is positive.
Consistency in the an-notated corpora is another problem, but this affectsevery participant.
We also had a technical problemwhere some sentences remained unsegmented sim-ply because some characters are not in our diction-ary.ReferencesWu, Andi.
2003.
Customizable segmentation of mor-phologically derived Words in Chinese, to appear inComputational Linguistics and Chinese LanguageProcessing., 8(2).Wu, Andi, J. Pentheroudakis and Z. Jiang, 2002.
Dy-namic lexical acquisition in Chinese sentence analy-sis.
In Proceedings of the 19th InternationalConference on Computational Linguistics, pp.
1308-1312, Taipei, Taiwan.Wu, Andi, J. and Z. Jiang, 2000.
Statistically-enhancednew word identification in a rule-based Chinese sys-tem, in Proceedings of the 2nd Chinese LanguageProcessing Workshop, pp.
46-51, HKUST, HongKong.Wu, Andi, J. and Z. Jiang, 1998.
Word segmentation insentence analysis, in Proceedings of 1998 Interna-tional Conference on Chinese Information Process-ing, pp.
46-51.169-180, Beijing, China.
