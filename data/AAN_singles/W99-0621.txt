A Learning Approach to Shallow Parsing*Marcia Mufioz t Vasin Punyakanok* Dan Roth* Day ZimakDepartment  of Computer  ScienceUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801 USAAbstractA SNoW based learning approach to shallow pars-ing tasks is presented and studied experimentally.The approach learns to identify syntactic patterns bycombining simple predictors to produce a coherentinference.
Two instantiations of this approach arestudied and experimental results for Noun-Phrases(NP) and Subject-Verb (SV) phrases that comparefavorably with the best published results are pre-sented.
In doing that, we compare two ways of mod-eling the problem of learning to recognize patternsand suggest that shallow parsing patterns are bet-ter learned using open/close predictors than usinginside/outside predictors.1 I n t roduct ionShallow parsing is studied as an alternative tofull-sentence parsers.
Rather than producing acomplete analysis of sentences, the alternative isto perform only partial analysis of the syntacticstructures in a text (Harris, 1957; Abney, 1991;Greffenstette, 1993).
Shallow parsing informa-tion such as NPs and other syntactic sequenceshave been found useful in many large-scale lan-guage processing applications including infor-mation extraction and text summarization.
Alot of the work on shallow parsing over the pastyears has concentrated on manual constructionof rules.
The observation that shallow syntacticinformation can be extracted using local infor-mation - by examining the pattern itself, itsnearby context and the local part-of-speech in-formation - has motivated the use of learningmethods to recognize these patterns (Church,1988; Ramshaw and Marcus, 1995; Argamon etal., 1998; Cardie and Pierce, 1998).
* Research supported by NSF grants IIS-9801638 andSBR-9873450.t Research supported by NSF grant CCR-9502540.This paper presents a general learning ap-proach for identifying syntactic patterns, basedon the SNoW learning architecture (Roth, 1998;Roth, 1999).
The SNoW learning architecture isa sparse network of linear ftmctions over a pre-defined or incrementally learned feature space.SNoW is specifically tailored for learning in do-mains in which the potential number of infor-mation sources (features) taking part in deci-sions is very large - of which NLP  is a princi-pal example.
Preliminary versions of it have al-ready been used successfully on several tasks innatural language processing (Roth, 1998; Gold-ing and Roth, 1999; Roth and Zelenko, 1998).In particular, SNoW's sparse architecture sup-ports well chaining and combining predictors toproduce a coherent inference.
This property ofthe architecture is the base for the learning ap-proach studied here in the context of shallowparsing.Shallow parsing tasks often involve the iden-tification of syntactic phrases or of words thatparticipate in a syntactic relationship.
Com-putationally, each decision of this sort involvesmultiple predictions that interact in some way.For example, in identifying a phrase, one canidentify the beginning and end of the phrasewhile also making sure they are coherent.Our computational paradigm suggests using aSNoW based predictor as a building block thatlearns to perform each of the required predic-tions, and writing a simple program that acti-vates these predictors with the appropriate in-put, aggregates their output and controls theinteraction between the predictors.
Two instan-tiations of this paradigm are studied and eval-uated on two different shallow parsing tasks -identifying base NPs  and SV phrases.
The firstinstantiation of this para4igm uses predictorsto decide whether each word belongs to the in-168Iterior of a phrase or not, and then groups thewords into phrases.
The second instantiationfinds the borders of phrases (beginning and end)and then pairs !them in an "optimal" way intodifferent phrases.
These problems formulationsare similar to those studied in (Ramshaw andMarcus, 1995) and (Church, 1988; Argamon etal., 1998), respectively.The experimental results presented using theSNoW based approach compare favorably withpreviously published results, both for NPs andSV phrases.
A s important, we present a fewexperiments hat shed light on some of the is-sues involved in using learned predictors thatinteract to produce the desired inference.
Inparticular, we exhibit the contribution of chain-ing: features that are generated as the outputof one of the predictors contribute to the per-formance of another predictor that uses them asits input.
Also, the comparison between the twoinstantiations 0fthe learning paradigm - the In-side/Outside and the Open/Close - shows theadvantages of the Open/Close model over theInside/Outside, specially for the task of iden-tifying long sequences.The contribtition of this work is in improvingthe state of the art in learning to perform shal-low parsing tasks, developing a better under-standing for how to model these tasks as learn-ing problems and in further studying the SNoWbased computational paradigm that, we believe,can be used in many other related tasks in NLP.The rest of this paper is organized as follows:The SNoW architecture is presented in Sec.
2.Sec.
3 presents the shallow parsing tasks stud-led and provides details on the computationalapproach.
Sec.
4 describes the data used andthe experimental pproach, and Sec.
5 presentsand discusses the experimental results.2 SNoWThe SNoW (Sparse Network of Winnows 1)learning architecture is a sparse network of lin-ear units over: a common pre-defined or incre-mentally learned feature space.
Nodes in theinput layer of t:he network represent simple rela-tions over the input sentence and are being usedas the input features.
Each linear unit is calleda target node and represents relations which areof interest ove~r the input sentence; in the cur-1To winnow: to separate chaff rom grain.rent application, target nodes may represent apotential prediction with respect o a word inthe input sentence, .g., inside a phrase, outsidea phrase, at the beginning off a phrase, etc.
Aninput sentence, along with a designated wordof interest in it, is mapped into a set of fea-tures which are active in it; this representationis presented to the input layer of SNoW andpropagates to the target nodes.
Target nodesare linked via weighted edges to (some of the)input features.
Let ,At = (Q , .
.
.
,ira} be the setof features that are active in an example andare linked to the target node t. Then the lineart t is the unit is active iff ~ieAt  wi > 9t, where w iweight on the edge connecting the ith featureto the target node t, and 9t is the threshold forthe target node t.Each SNoW unit may include a collection ofsubnetworks, one for each of the target rela-tions.
A given example is treated autonomouslyby each target subnetwork; an example labeledt may be treated as a positive example by thesubnetwork for t and as a negative xample bythe rest of the target nodes.The learning policy is on-line and mistake-driven; several update rules can be used withinSNOW.
The most successful update rule, andthe only one used in this work is a variant ofLittlestone's (1988) Winnow update rule, a mul-tiplicative update rule tailored to the situationin which the set of input features is not known apriori, as in the infinite attribute model (Blum,1992).
This mechanism is implemented via thesparse architecture of SNOW.
That is, (1) inputfeatures are allocated in a data driven way - aninput node for the feature i is allocated only ifthe feature i was active in any input sentenceand (2) a link (i.e., a non-zero weight) existsbetween a target node t and a feature i if andonly if i was active in an example labeled t.The Winnow update rule has, in addition tothe threshold 9t at the target , two update pa-rameters: a promotion parameter a > 1 anda demotion parameter 0 < j3 < 1.
Theseare being used to update the current represen-tation of the target t (the set of weights w~)only when a mistake in prediction is made.
Let.At -- (Q , .
.
.
,ira} be the set of active featuresthat are linked to the target node t. If the algo-t < St) and rithm predicts 0 (that is, ~ieAt wi -the received label is 1, the active weights in169the current example are promoted in a multi-t If the plicative fashion: Vi E ,At, wl t +-- o~ ?
w i.t 0t) and the algorithm predicts 1 (~ie.~t wi >received label is 0, the active weights in the cur-t t rent example are demoted Vi E .At, wi ~ 8 " wi.All other weights are unchanged.The key feature of the Winnow update rule isthat the number of examples required to learna linear function grows linearly with the num-ber of relevant features and only logarithmicallywith the total number of features.
This prop-erty seems crucial in domains in which the num-ber of potential features is vast, but a relativelysmall number of them is relevant.
Winnow isknown to learn efficiently any linear thresholdfunction and to be robust in the presence ofvarious kinds of noise and in cases where nolinear-threshold function can make perfect clas-sifications, while still maintaining its abovemen-tioned dependence on the number of total andrelevant attributes (Littlestone, 1991; Kivinenand Warmuth, 1995).Once target subnetworks have been learnedand the network is being evaluated, a decisionsupport mechanism is employed, which selectsthe dominant active target node in the SNoWunit via a winner-take-all mechanism toproducea final prediction.
The decision support mech-anism may also be cached and processed alongwith the output of other SNoW units to producea coherent output.3 Mode l ing  Sha l low Pars ing3.1 Task Definit ionThis section describes how we model the shallowparsing tasks studied here as learning problems.The goal is to detect NPs and SV phrases.
Ofthe several slightly different definitions of a baseNP in the literature we use for the purposes ofthis work the definition presented in (Ramshawand Marcus, 1995) and used also by (Argamonet al, 1998)and others.
That is, a base NPis a non-recursive NP that includes determin-ers but excludes post-modifying prepositionalphrases or clauses.
For example:?
.
.presented \ [ las t  year \] in\ [ I l l i no i s \ ]  in  f ront  of .
.
.SV phrases, following the definition suggestedin (Argamon et al, 1998), are word phrasesstarting with the subject of the sentence andending with the first verb, excluding modalverbs 2.
For example, the SV phrases are brack-eted in the following:?..presented \[ a theory that claims \]that \[the algorithm runs \] andperforms...Both tasks can be viewed as sequence recog-nition problems.
This can be modeled as a col-lection of prediction problems that interact ina specific way.
For example, one may predictthe first and last word in a target sequence.Moreover, it seems plausible that informationproduced by one predictor (e.g., predicting thebeginning of the sequence) may contribute toothers (e.g., predicting the end of the sequence).Therefore, our computational paradigm sug-gests using SNoW predictors that learn sepa-rately to perform each of the basic predictions,and chaining the resulting predictors at evalu-ation time.
Chaining here means that the pre-dictions produced by one of the predictors maybe used as (a part of the) input to others 3.Two instantiations of this paradigm - each ofwhich models the problems using a different setof predictors - are described below.3.2 Ins ide/Outs ide PredictorsThe predictors in this case are used to decide,for each word, whether it belongs to the inte-rior of a phrase or not; this information is thenused to group the words into phrases.
Sinceannotating words only with Inside/Outside in-formation is ambiguous in cases of two consec-utive phrases, an additional predictor is used.Specifically, each word in the sentence may beannotated using one of the following labels: O- the current word is outside the pattern.
I -the current word is inside the pattern.
B - thecurrent word marks the beginning of a patternthat immediately follows another pattern 4.2Notice that according to this definition the identifiedverb may not correspond to the subject, but this phrasestill contains meaningful information; in any case, thelearning method presented is independent of the specificdefinition used.3The input data used in all the experimentspresented here consists of part-of-speech taggeddata.
In the demo of the system (available fromhttp ://12r.
cs.
uiuc.
edu/'cogcomp/eoh/index, html),an additional layer of chaining is used.
Raw sentencesare supplied as input and are processed using a SNoWbased POS tagger (Roth and Ze!enko, 1998) first.4There are other ways to define the B annotation,e.g., as always marking the beginning of a phrase.
The170(iOIB PredictionSecondPredictor(Final ~edicfion\[FeatureExt actor ISentence +POS tags + OIB))OIB PredictionPredictorSentence +POS tagsFigure 1: Architecture for Inside/Outside method.For example, the sentence I went toCa l i fo rn ia  las t  May would be marked forbase NPs as:I went to California last MayI 0 0 I B Iindicating that the NPs are I, Ca l i fo rn ia  andlas t  May.
This approach has been studiedin (Ramshaw and Marcus, 1995).3.2.1 Arch i tec tureSNoW is used in order to learn the OIB an-notations both for NPs and SV phrases.
Ineach case, two:predictors are learned, which dif-fer in the type of information they receive intheir input.
A first predictor takes as input amodeling used, however, turns out best experimentally.sentence along with the corresponding part-of-speech (POS) tags.
The features extracted fromthis input represent the local context of eachword in terms of POS tags (with the possibleaddition of lexical information), as described inSec 3.4.
The SNoW predictor in this case con-sists of three targets - O, I and B.
Figure 1depicts the feature xtraction module which ex-tracts the local features and generates an exam-ple for each word in the sentence.
Each exampleis labeled with one of 0 ,  I or B.The second predictor takes as input a sen-tence along with the corresponding POS tagsas well as the Ins ide /Outs ide  in format ion .The hope is that representing the local contextof a word using the Inside/Outside informationfor its neighboring words, in addition to thePOS and lexical information, will enhance the171Final Prediction )Combinatort OlinFFeature Extractor \],d?
-~\[ Feature ExtractorT TSentence + POS tagsFigure 2: Architecture for Open/Close Method.performance of the predictor.
While this in-formation is available during training, since thedata is annotated with the OIB information, itis not available in the input sentence at evalua-tion time.
Therefore, at evaluation time, givena sentence (represented as a sequence of POStags), we first need to evaluate the first pre-dictor on it, generate an Inside/Outside repre-sentation of the sentence, and then use this togenerate new features that feed into the secondpredictor.3.3 Open/Close PredictorsThe predictors in this case are used to decide,for each word, whether it is the first in a phrase,the last in a phrase, both of these, or noneof these.
In this way, the phrase boundariesare determined; this is annotated by placing anopen bracket ( \ [ )  before the first word and aclose bracket ( \ ] )  after the last word of eachphrase.
Our earlier example would be markedfor base NPs as: \[I\] wont to \[California\]\[last May\].
This approach has been studiedin (Church, 1988; Argamon et al, 1998).3.3.1 ArchitectureThe architecture used for the Open/Close pre-dictors is shown in Figure 2.
Two SNoW pre-dictors are used, one to predict if the word cur-rently in consideration is the first in the phrase(an open bracket), and the other to predict if itis the last (a close bracket).
Each of the two pre-dictors is a SNoW network with two competingtarget nodes: one predicts if the current posi-tion is an open (close) bracket and the otherpredicts if it is not.
In this case, the actualactivation value (sum of weights of the activefeatures for a given target) of the SNoW pre-dictors is used to compute a confidence in theprediction.
Let ty be the activation value forthe yes-bracket target and t N for the no-brackettarget.
Normally, the network would predictthe target corresponding to the higher activa-tion value.
In this case, we prefer to cache thesystem preferences for each of the open (close)brackets predictors o that several bracket pair-ings can be considered when all the informationis available.
The confidence, '7, of a candidate isdefined by '7 = tr / (tr  + t,v).
Normally, SNoWwill predict that there is a bracket if "7 /> 0.5,17281 82 83 84 85 W6\[;.8 \];.3 \]0.6\[;.5 \]0.3 \];.9 \]0.3\[04 \]05\[ 81 \] 82 \ [  s3 84 \] 85 86\]03Figure 3: Example of combinator assignment.
Subscripts denote the confidence of the bracketcandidates.
Bracket candidates that would be chosen by the combinator are marked with a *but this system employs an threshold ~-.
Wewill consider any bracket hat has '7 >t r as acandidate.
The lower ~- is, the more candidateswill be considered.The input to: the open bracket predictor isa sentence and the  POS tags associated witheach word in the sentence.
For each positionin the sentence,: the open bracket predictor de-cides if it is a candidate for an open bracket.
Foreach open bracket candidate, features that cor-respond to this information are generated; theclose bracket predictor can (potentially) receivethis information in addition to the sentence andthe POS information, and use it in its decisionon whether a given position in the sentence isto be a candiddte for a close bracket predictor(to be paired with the open bracket candidate).3 .3 .2  CombinatorFinding the final phrases by pairing the openand close bracket candidates is crucial to theperformance of the system; even given goodprediction performance choosing an inadequatepairing would severely lower the overall perfor-mance.
We use 'a graph based method that usesthe confidence Of the SNoW predictors to gener-ate the consistent pairings, at only a linear timecomplexity.We call p = (o, c) a pair, where o is an openbracket and c is any close bracket hat was pre-dicted with respect to o.
The position of abracket at the: ith word is defined to be i ifit is an open bracket and i + 1 if it is a closebracket.
Clearly, a pair (o, c) is possible onlywhen pos(o) <: po8(c).
The confidence of abracket is thei weight '7(t).
The value of a pairp = (o,c) is defined to be v(p) = 7(o) * 7(c).The pair Pl occurs before the pair P2 if po8(cl) ~.pos(o2).
Pl and P2 are compatible if either Pl oc-curs before p2 or P2 occurs before Pl.
A pairingis a set of pair s P = {pl ,p2, .
.
.pn} such that Plis compatible with pj for all i and j where i ~ j.The value of the pairing is the sum of all of thevalues of the pairs within the pairing.Our combinator finds the pairing with themaximum value.
Note that while there may beexponentially many pairings, by modeling theproblem of finding the maximum wlued pairingas a shortest path problem on a directed acyclicgraph, we provide a linear time solution.
Fig-ure 3 gives an example of pairing bracket can-didates of the sentence S = 818283848586, wherethe confidence of each candidate is written inthe subscript.3.4 FeaturesThe features used in our system are relationalfeatures over the sentence and the POS informa-tion, which can be defined by a pair of numbers,k and w. Specifically, features are either wordconjunctions or POS tags conjunctions.
All con-junctions of size up to k and within a symmetricwindow that includes the w words before andafter the designated word are generated.An example is shown in Figure 4 where(w, k) = (3, 4) for POS tags, and (w, k) = (1, 2)for words.
In this example the word "how" isthe designated word with POS tag "WRB".
"0"marks the position of the current word (tag)if it is not part of the feature, and "(how)"or "(WI:tB)" marks the position of the currentword (tag) if it is part of the current feature.The distance of a conjunction from the currentword (tag) can be induced by the placement ofthe special character %" in the feature.
We donot consider mixed features between words andPOS tags as in (l:tamshaw and Marcus, 1995),that is, a single feature consists of either wordsor tags.Additionally, in the Inside/Outside model,the second predictor incorporates as featuresthe OIB status of the w words before and af-ter the designated word, and the conjunctionsof size 2 of the words surrounding it.173This is an exampleDT VBZ DT NNof howIN WRBtoTOgenerateVBfeaturesNNSConj.
SizeFor Tagsw=3k=4For Wordsw=lk=21 2DT __ 0 DT NN _ 0NN _ 0 NN IN 0IN 0 IN (WRB)(WRB) (WRB) TO0 TO 0 TO VB0 -VB 0 - VB NNS0 - - NNSof 0 of (how)(how) (how) to0 to3DT NN IN 0NN IN (WRB)IN (Wl:tB) TO(WRB) TO VB0 TO VB NNS4DT NN IN (WRB)NN IN (WRB) TOIN (WRB) TO VB(WRB) TO VB NNSFigure 4: An example of feature xtraction.I  ni?
l 8?3?12117271 54758 ITest 2012 47377 12335Table 1: Sizes of the training and test data sets for NP Patterns.\ [Data \]Sentences \[ Words I SV Patterns J l  io,o \[Test 1921 46451 3044Table 2: Sizes of the training and test data sets for SV Patterns.4 Methodo logy4.1  DataIn order to be able to compare our results withthe results obtained by other researchers, weworked with the same data sets already usedby (Ramshaw and Marcus, 1995; Argamon etal., 1998) for NP and SV detection.
These datasets were based on the Wall Street Journal cor-pus in the Penn Treebank (Marcus et al, 1993).For NP, the training and test corpus was pre-pared from sections 15 to 18 and section 20,respectively; the SV corpus was prepared fromsections 1 to 9 for training and section 0 fortesting.
Instead of using the NP bracketing in-formation present in the tagged Treebank data,Ramshaw and Marcus modified the data so asto include bracketing information related onlyto the non-recursive, base NPs present in eachsentence while the subject verb phrases weretaken as is.
The data sets include POS taginformation generated by Ramshaw and Mar-cus using Brill's transformational part-of-speechtagger (Brill, 1995).The sizes of the training and test data aresummarized in Table 1 and Table 2.4.2  ParametersThe Open/Close system has two adjustable pa-rameters, r\[ and v\], the threshold for the openand close bracket predictors, respectively.
Forall experiments, the system is first trained on90% of the training data and then tested on theremaining 10%.
The r\] and r\[ that provide the174best performance are used on the real test file.After the best parameters are found, the systemis trained on the whole training data set.
Re-sults are reported in terms of recall, precision,and Fa.
F# is always used as the single value tocompare the performance.For all the experiments, we use 1 as the initialweight, 5 as the .threshold, 1.5 as a, and 0.7 as~3 to train SNOW, and it is always trained for 2cycles.4.3 Evaluat ion TechniqueTo evaluate the results, we use the followingmetrics:Number of correct proposed patternsRecall = Number of correct patternsPrecision = Number of correct proposed patternsNumber of proposed patterns(/~2 + 1) ?
Recall.
PrecisionF~ = ~32.
Precision + RecallNumber of words labeled correctlyAccuracy = Total number of wordsWe use ~ = 1.
Note that, for the Open/Closesystem, we must measure the accuracy for theopen predictor and the close predictor sepa-rately since each word can be labeled as "Open"or "Not Open" and, at the same time, "Close"or "Not Close".5 Exper imenta l  Resu l ts5.1 Ins ide /Outs ideThe results o f  each of the predictors usedin the Inside/0utside method are presentedin Table 3.
The results are comparable toother results reported using the Inside/Outsidemethod (Ramshaw and Marcus, 1995) (see Ta-ble 7.
We have observed that most of the mis-taken predictions of base NPs involve predic-tions with respect o conjunctions, gerunds, ad-verbial NPs and some punctuation marks.
Asreported in (Argamon et al, 1998), most baseNPs present in ~he data are less or equal than4 words long.
This implies that our predictorstend to break up long base NPs into smallerones .The results also show that lexical informationimproves the performance by nearly 2%.
Thisis similar to results in the literature (Ramshawand Marcus, 1995).
What we found surprising isthat the second predictor, that uses additionalinformation about the OIB status of the localcontext, did not do much better than the firstpredictor, which relies only on POS and lexicalinformation.
A control experiment has verifiedthat this is not due to the noisy features that thefirst predictor supplies to the second predictor.Finally, the Inside/Outside method was alsotested on predicting SV phrases, yielding poorresults that are not shown here.
An attempt atexplaining this phenomena by breaking downperformance according to the length of thephrases is discussed in Sec.
5.3.5.2 Open/C loseThe results of the Open/Close method for NPand SV phrases are presented in Table 4.
In ad-dition to the good overall performance, the re-sults show significant improvement by incorpo-rating the lexical information into the features.In addition to the recall/precision results wehave also presented the accuracy of each of theOpen and Close predictors.
These are impor-tant since they determine the overall accuracyin phrase detection.
It is evident hat the pre-dictors perform very well, and that the overallperformance degrades due to inconsistent pair-ings.An important question in the learning ap-proach presented here is investigating the gainachieved ue to chaining.
That is, whether thefeatures extracted from open brackets can im-prove the performance of the the close bracketpredictor.
To this effect, we measured the ac-curacy of the close bracket predictor itself, on aword basis, by supplying it features generatedfrom correct open brackets.
We compared thiswith the same experiment, only this time with-out incorporating the features from open brack-ets to the close bracket predictor.
The results,shown in Table 5 indicate a significant contribu-tion due to chaining the features.
Notice thatthe overall accuracy for the close bracket pre-dictor is very high.
This is due to the fact that,as shown in Table 2, there are many more neg-ative examples than positive examples.
Thus, a175MethodFirst PredictorSecond PredictorFirst Predictor + lexicalSecond Predictor + lexical\[Recall\[ Precision\[ Eft= 1 Accuracy90.5 89.8 90.1 96.990.5 90.4 90.4 97.092.5 92.2 92.4 97.692.5 92.1 92.3 97.6Table 3: Results for NP detection using Inside/Outside method.Method PrecisionSV~ lexical ~ 87.9SV with lexical ~ 92.2NP w/o lexical 90.3NP with lexical 92.4\[Fts=I \[ AccuracY \]Table 4: Results for SV Phrase and NP detection using Open/Close method.I II without Openio o IIOverall I Positive OnlyWith Open bracket info IOverall I Positive Only \[\[ 0"5 II 99"3 I 92.7 II 99 .41  95.0 ITable 5: Accuracy of close bracket predictor when using features created on local information aloneversus using additional features created from the open bracket candidate.
Overall performance andperformance on positive xamples only is shown.predictor that always predicts "no" would havean accuracy of 93.4%.
Therefore, we consideredalso the accuracy over positive xamples, whichindicates the significant role of the chaining.5.3 DiscussionBoth methods we study here - Inside/Outsideand Open/Close - have been evaluated before(using different learning methods) on similartasks.
However, in this work we have allowed fora fair comparison between two different modelsby using the same basic learning method andthe same features.Our main conclusion is with respect o therobustness of the methods to sequences of dif-ferent lengths.
While both methods give goodresults for the base NP problem, they differ sig-nificantly on the SV tasks.
Furthermore, ourinvestigation revealed that the Inside/Outsidemethod is very sensitive to the length of thephrases.
Table 6 shows a breakdown of the per-formance of the two methods on SV phrases ofdifferent lengths.
Perhaps this was not observedearlier since (Ramshaw and Marcus, 1995) stud-ied only base NPs, most of which are short.The conclusion is therefore that the Open/Closemethod is more robust, especially when the tar-get sequences are longer than a few tokens.Finally, Tables 7 and 8 present a comparisonof our methods to some of the best NP and SVresults published on these tasks.6 Conc lus ionWe have presented a SNoW based learning ap-proach to shallow parsing tasks.
The learningapproach suggests to identify a syntactic pat-terns is performed by writing a simple programin which several instantiations of SNoW learn-ing units are chained and combined to producea coherent inference.
Two instantiations of thisapproach ave been described and shown to per-form very well on NP and SV phrase detection.In addition to exhibiting ood results on shallow176Length Patterns4 I 22124 < I ~ 8 i 5O9> 8 323I Inside/Outside Open/Close'Recall lPrecision I Fp=l Recall lPrecision I Fp=t90.5 61.5 73.2 94.1 93.5 93.861.4 44.1 51.3 72.3 79.7 75.830.3 15.0 20.0 74.0 64.4 68.9Table 6: Comparison of Inside/Outside and Open/Close on SV patterns of varying lengths.MethodInside/OutsideInside/Outside + lexicalOpen/CloseOpen/Close + lexicalRamshaw & MarcusRamshaw & Marcus + lexicalArgamon et alI Recall Precision Fp=l I Accuracy90.5 90.4 90.4 97.092.5 92.2 92.4 97.690.9 90.3 90.6 O: 97.4, C: 97.893.1 92.4 92.8 O: 98.1, C: 98.290.7 90.5 90.6 97.092.3 91.8 92.0 97.491.6 91.6 91.6 N/ATable 7: Comparison of Results for NP.
In the accuracy column, OOpen predictor and C indicates the accuracy of the Close predictor.indicates the accuracy of theMethod Recall Precision I Fp=l I0Pen/Close 88.3 87.9 , 88.10pen/Close + lexical 91.9 92.2 92.0Argamon et al 84.5 88.6 I 86.5AccuracyO: 98.6, C: 99.4O: 99.2, C: 99.4N/ATable 8: Comparison of Results for SV.
In the accuracy column, O indicates the accuracy of theOpen predictor and C indicates the accuracy of the Close predictor.parsing tasks, we have made some observationson the sensitivity of modeling the task.
We be-lieve that the paradigm described here, as wellas the basic learning system, can be used in thisway in many problems that are of interest to theNLP community.AcknowledgmentsWe would like to thank Yuval Krymolowski andthe reviewers for their helpful comments on thepaper.
We als0 thank Mayur Khandelwal forthe suggestion to model the combinator as agraph problem.ReferencesS.
P. Abney.
:1991.
Parsing by chunks.
InS.
P. AbneylR.
C. Berwick and C. Tenny,editors, Principle-based parsing: Computa-tion and Psycholinguistics, pages 257-278.Kluwer, Dordrecht.S.
Argamon, I. Dagan, and Y. Krymolowski.1998.
A memory-based approach to learn-ing shallow natural language patterns.
InCOLING-ACL 98, The 17th InternationalConference on Computational Linguistics.A.
Blum.
1992.
Learning boolean functions inan infinite attribute space.
Machine Learn-ing, 9(4):373-386, October.E.
BriU.
1995.
Transformation-based rror-driven learning and natural language process-ing: A case study in part of speech tagging.Computational Linguistics, 21(4):543-565.C.
Cardie and D. Pierce.
1998.
Error-drivenpruning of treebanks grammars for base nounphrase identification.
In Proceedings of A CL-98, pages 218-224.177Kenneth W. Church.
1988.
A stochastic partsprogram and noun phrase parser for unre-stricted text.
In Proc.
of A CL Conference onApplied Natural Language Processing.A.
R. Golding and D. Roth.
1999.
A winnowbased approach to context-sensitive spellingcorrection.
Machine Learning.
Special Issueon Machine Learning and Natural Language.Preliminary version appeared in ICML-96.G.
Greffenstette.
1993.
Evaluation techniquesfor automatic semantic extraction: compar-ing semantic and window based approaches.In ACL'93 workshop on the Acquisition ofLexical Knowledge from TexLZ.
S. Harris.
1957.
Co-occurrence and trans-formation in linguistic structure.
Language,33(3):283-340.J.
Kivinen and M. K. Warmuth.
1995.
Expo-nentiated gradient versus gradient descent forlinear predictors.
In Proceedings of the An-nual A CM SSymp.
on the Theory of Comput-ing.N.
Littlestone.
1988.
Learning quickly whenirrelevant attributes abound: A new linear-threshold algorithm.
Machine Learning,2:285-318.N.
Littlestone.
1991.
Redundant noisy at-tributes, attribute rrors, and linear thresh-old learning using Winnow.
In Proc.
~thAnnu.
Workshop on Comput.
Learning The-ory, pages 147-156, San Mateo, CA.
MorganKaufmann.M.
P. Marcus, B. Santorini, andM.
Marcinkiewicz.
1993.
Building alarge annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313-330, June.L.
A. Ramshaw and M. P. Marcus.
1995.
Textchunking using transformation-based learn-ing.
In Proceedings of the Third AnnualWorkshop on Very Large Corpora.D.
Roth and D. Zelenko.
1998.
Part of speechtagging using a network of linear separa-tors.
In COLING-ACL 98, The 17th Inter-national Conference on Computational Lin-guistics, pages 1136-1142.D.
Roth.
1998.
Learning to resolve natural lan-guage ambiguities: A unified approach.
InProc.
National Conference on Artificial Intel-ligence, pages 806---813.D.
Roth.
1999.
The SNoW learning archi-tecture.
Technical Report UIUCDCS-R-99-2101, UIUC Computer Science Department,May.178
