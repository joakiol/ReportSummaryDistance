A Multi-Neuro Tagger Using Variable Lengths of ContextsQing  Ma and H i tosh i  IsaharaCommunications Research LaboratoryMinistry of Posts and Telecommunications588-2, Iwaoka, Nishi-ku, Kobe, 651-2401, Japan{qma, isahara}@crl.go.jpAbstractThis paper presents a multi-neuro tagger thatuses variable lengths of contexts and weightedinputs (with information gains) for part ofspeech tagging.
Computer experiments showthat it has a correct rate of over 94% for tag-ging ambiguous words when a small Thai corpuswith 22,311 ambiguous words is used for train-ing.
This result is better than any of the resultsobtained using the single-neuro taggers withfixed but different lengths of contexts, whichindicates that the multi-neuro tagger can dy-namically find a suitable length of contexts intagging.1 In t roduct ionWords are often ambiguous in terms of theirpart of speech (POS).
POS tagging disam-biguates them, i.e., it assigns to each word thecorrect POS in the context of the sentence.Several kinds of POS taggers using rule-based(e.g., Brill et al, 1990), statistical (e.g., Meri-aldo, 1994), memory-based (e.g., Daelemans,1996), and neural network (e.g., Schmid, 1994)models have been proposed for some languages.The correct rate of tagging of these modelshas reached 95%, in part by using a very largeamount of training data (e.g., 1,000,000 wordsin Schmid, 1994).
For many other languages(e.g., Thai, which we deal with in this paper),however, the corpora have not been preparedand there is not a large amount of training dataavailable.
It is therefore important o constructa practical tagger using as few training data aspossible.In most of the statistical and neural networkmodels proposed so far, the length of the con-texts used for tagging is fixed and has to beselected empirically.
In addition, all words inthe input are regarded to have the same rele-vance in tagging.
An ideal model would be onein which the length of the contexts can be au-tomatically selected as needed in tagging andthe words used in tagging can be given differentrelevances.
A simple but effective solution is tointroduce a multi-module tagger composed ofmultiple modules (basic taggers) with fixed butdifferent lengths of contexts in the input anda selector (a selecting rule) to obtain the finalanswer.
The tagger should also have a set ofweights reflecting the different relevances of theinput elements.
If we construct such a multi-module tagger with statistical methods (e.g., n-gram models), however, the size of the n-gramtable would be extremely large, as mentioned inSec.
4.4.
On the other hand, in memory-basedmodels such as IGtree (Daelemans, 1996), thenumber of features used in tagging is actuallyvariable, within the maximum length (i.e., thenumber of features panning the tree), and thedifferent relevances of the different features aretaken into account in tagging.
Tagging by thisapproach, however, may be computationally ex-pensive if the maximum length is large.
Actu-ally, the maximum length was set at 4 in Daele-mans's model, which can therefore be regardedas one using fixed length of contexts.This paper presents a multi-neuro taggerthat is constructed using multiple neural net-works, all of which can be regarded as single-neuro taggers with fixed but different lengths ofcontexts in inputs.
The tagger performs POStagging in different lengths of contexts based onlongest context priority.
Given that the targetword is more relevant han any of the wordsin its context and that the words in contextmay have different relevances in tagging, each802element of the input is weighted with informa-tion gains, i.e., numbers expressing the averageamount of reduction of training set informa-tion entropy when the POSs of the element areknown (Quinlan 1993).
By using the trained re-sults (weights) of the single-neuro taggers withshort inputs as initial weights of those with longinputs, the training time for the latter ones canbe greatly reduced and the cost to train a multi-neuro tagger is almost he same as that to traina single-neuro tagger.2 POS Tagg ing  Prob lemsSince each input Thai text can be segmentedinto individual words that can be further taggedwith all possible POSs using an electronic Thaidictionary, the POS tagging tasks can be re-garded as a kind of POS disambiguation prob-lem using contexts as follows:I PT  : ( iptdt, .
.
.
, ipt-ll, ipt_t, ipt_r l , .
.
.
,  ipt_rr)OPT  : POS_t, (1)where ipt_t is the element related to the possiblePOSs of the target word, (ipt_lt,..., ipt_ll) and(ipt_rl, .
.
.
, ipt_rr) are the elements related tothe contexts, i.e., the POSs of the words to theleft and right of the target word, respectively,and POS_t is the correct POS of the target wordin the contexts.3 In fo rmat ion  Ga inSuppose each element, ipt_x (x = li,t, or rj),in (1) has a weight, w_z, which can be obtainedusing information theory as follows.
Let S bethe training set and Ci be the ith class, i.e.,the ith POS (i = 1 , .
.
.
,n ,  where n is the totalnumber of POSs).
The entropy of the set S,i.e., the average amount of information eededto identify the class (the POS) of an example in5', isin f o( S) = _ ~-~ f req( Ci, S) ~(~\]i, S) ), ISl x In( f re(2)where ISl is the number of examples in S andfreq(Ci, S) is the number of examples belong-ing to class Ci.
When S has been partitionedto h subset Si (i = 1 , .
.
.
,h )  according to theelement ipt.x, the new entropy can be found asthe weighted sum over these subsets, orinfox(S) = ~ ?
info(Si).
(3)i=1Thus, the quantity of information gained by thispartitioning, or by knowing the POSs of elementipt_x, can be obtained bygain(x) = info(S)  - in fox(S),  (4)which is used as the weight, w_T, i.e.,w_x= gain(x).
(5)4 Mul t i -Neuro  Tagger4.1 S ing le -Neuro  TaggerFigure 1 shows a single-neuro tagger (SNT)which consists of a 3-layer feedforward neuralnetwork.
The SNT can disambiguate he POSof each word using a fixed length of the con-text by training it in a supervised manner witha well-known error back-propagation algorithm(for details see e.g., Haykin, 1994).OPTipt  l I - -  ip t_ l  I ipt__t ip t_ r  I - "  ipt  r rI PTFig.
1.
The single-neuro tagger (SNT).When word x is given in position y (y = t, li,or rj), element ipt-y of input I PT  is a weightedpattern defined asipt_y = w_y.
(ezl,ex2," -,ezn),= (Ix,, I~2,--', I~n) (6)where w_y is the weight obtained in (5), n isthe total number of POSs defined in Thai, and803Izi = w_y.e~i ( i = 1, .
.
.
,n  ).
I f x  is aknownword, i.e., it appears in the training data, eachbit ezi is obtained as follows:e~i = Prob(PO&lx).
(7)Here tile Prob(POSi\[x) is the prior probabilityof POSi that the word x can be and is estimatedfrom tile training data asProb(PO&\[x) - IPOSi,xlIxl ' (8 )where IPOSi,x\[ is the number of times bothPOSi and x appear and Ixl is the number oftimes x appears in all the training data.
If x isan unknown word, i.e., it does not appear in thetraining data, each bit e,:i is obtained as follows:1__ if POSi is a candidate = n, '  (9) exi 0, otherwise,where nx is the number of POSs that the wordx can be (this number can be simply obtainedfrom an electronic Thai dictionary).
The OPTis a pattern defined as follows:OPT = (O1 ,O2, "  ' '  ,On) .
(10)The OPT is decoded to obtain a final resultRST for the POS of the target word as follows:RST = ~ POSi, i fO i= 1~ Oj =0for j~ i\[ Unknown.
otherwise(11)There is more information available for con-structing the input for the words on the left be-cause they have already been tagged.
In thetagging phase, instead of using (6)-(9), the in-put may be constructed simply as follows:ipt_li(t) = wdi.
OPT(t - i), (12)where t is the position of the target word in asentence and i = 1,2, .
.
.
,1  for t - i > 0.
How-ever, in the training process the output of thetagger is not correct and cannot be fed back tothe inputs directly.
Instead, a weighted averageof the actual output and the desired output isused as follows:iptdi(t) = wdi.
(WOPT.O PT( t -  i)+WDEs'DES),(13)where DES is the desired outputDES = (D1, D2, - .
.
,  D,~), (14)whose bits are defined as follows:1 i fPOS i  is a desired answerDi = 0. otherwise(15)and WOPT and WDES are respectively defined asandEOBd- (16)  WOPT EACTWDE S = 1 - WOPT,  (17)where EOBJ and EACT are the objective andactual errors, respectively.
Thus, the weightingof the desired output is large at the beginning ofthe training, and decreases to zero during train-ing.4.2 Mult i -Neuro TaggerFigure 2 shows the structure of the multi-neurotagger.
The individual SNTi has input IPTiwith length (the number of input elements: l +1 + r) l(IPTi), for which the following relationshold: l(IPTi) < l(IPTj) for i < j.i ~!II~ Rsr,.IFig.
2.
The multi-neuro tagger.When a sequence of words (word_ll, ...,word_ll, word_t, word_r1, ..., word_r~), whichhas a target word word_t in the center and amaximum length l(IPTm ), is inputed, its subse-quence of words, which also has the target wordword_t in the center and length l(IPTi), will beencoded into IPTi in the same way as describedin the previous ection.
The outputs OPTi (for804i = 1,- - .
,  m) of the single-neuro taggers are de-coded into RSTi by (11).
The RSTi are nextinputed into the longest-context-priority selec-tor which obtains the final result as follows:RSTi, if RSTj = Unknown(for j > i)POS_t = .
and RSTi ?
UnknownUnknown.
otherwise(18)This means that the output of the single-neurotagger that gives a result being not unknownand has the largest length of input is regardedas a final answer.4.3 TrainingIf we use the weights trained by the single-neurotaggers with short inputs as the initial values ofthose with long inputs, the training time for thelatter ones can be greatly reduced and the costto train multi-neuro taggers would be almostthe same as that to train the single-neuro tag-gers.
Figure 3 shows an example of training atagger with four input elements.
The trainedweights, w\] and w2, of the tagger with threeinput elements are copied to the correspondingpart of the tagger and used as initial values forits training.Output Layer I"- II .W Hidden A Layer-?}"
I _ 0~ ?
WlI-%,-7, 711 , , , ,  II *,-, II , , r ,  IFig.
3.
How to train single-neuro tagger.4.4 Feat  uresSuppose that at most seven elements areadopted in the inputs for tagging and that thereare 50 POSs.
The n-gram models must es-tin\]ate 50 T = 7.8e + 11 n-grams, while thesingle-neuro tagger with the longest input usesonly 70,000 weights, which can be calculatedby nipt ?
nhid q- nhid ?
nopt where  nipt,  nhid,  andnopt are, respectively, the number of units inthe input, the hidden, and the output layers,and nhid is set to be nipt/2.
That neuro modelsrequire few parameters may offer another ad-vantage: their performance is less affected by asmall amount of training data than that of thestatistical methods (Schmid, 1994).
Neuro tag-gers also offer fast tagging compared to othermodels, although its training stage is longer.5 Exper imenta l  Resu l tsThe Thai corpus used in the computer experi-ments contains 10,452 sentences that are ran-domly divided into two sets: one with 8,322sentences for training and another with 2,130sentences for testing.
The training and test-ing sets contain, respectively, 22,311 and 6,717ambiguous words that serve as more than onePOS and were used for training and testing.Because there are 47 types of POSs in Thai(Charoenporn et al, 1997), n in (6), (10), and(14) was set at 47.
The single neuro-taggersare 3-layer neural networks whose input length,l(IPT) (=l+ l+r ) ,  is set to 3-7 and whose sizeis p x 2 a x n, where p = n x I(IPT).
The multi-neuro tagger is constructed by five (i.e., rn = .5)single-neuro taggers, SNTi (i = 1,.. .
, .5), inwhich l(IPTi) = 2 + i.Table 1 shows that no matter whether theinformation gain (IG) was used or not, themulti-neuro tagger has a correct rate of over94%, which is higher than that of any of thesingle-neuro taggers.
This indicates that by us-ing the multi-neuro tagger the length of the con-text need not be chosen empirically; it can beselected ynamically instead.
If we focus on thesingle-neuro taggers with inputs greater thanfour, we can see that the taggers with informa-tion gain are superior to those without informa-tion gain.
Note that the correct rates shown inthe table were obtained when only counting theambiguous words in the testing set.
The correctrate of the multi-neuro tagger is 98.9% if all thewords in the testing set (the ratio of ambigu-ous words was 0.19) are counted.
Moreover, al-though the overall performance is not improved805Table 1.
Results of POS Tagging for Testing DataTaggers "single-neuro" "multi-neuro"l(IPTi) 3 4 5 6 7with IG 0.915 0.920 0.929 0.930 0.933 0.943without IG 0.924 0.927 0.922 0.926 0.926 0.941much by adopting the information gains, thetraining can be greatly speeded up.
It takes1024 steps to train the first tagger, SNT1, whenthe information gains are not used and only 664steps to train the same tagger when the infor-mation gains are used.Figure 4 shows learning (training) curves indifferent cases for the single-neuro tagger withsix input elements.
Thick line shows the casein which the tagger is trained by using trainedweights of the tagger with five input elements asinitial values.
The thin line shows the case inwhich the tagger is trained independently.
Thedashed line shows the case in which the taggeris trained independently and does not use theinformation gain.
From this figure, we knowthat the training time can be greatly reducedby using the previous result and the informationgain.0.0250.02~ 0.015LT.I0.010.005~Learn ing  using previous result- -  Learning with IG....... Learning without IG0 10 20 30 40 50 60 70 80 90Number of learning stepsFig.
4.
Learning curves.1006 Conc lus ionThis paper described a multi-neuro tagger thatuses variable lengths of contexts and weightedinputs for part of speech tagging.
Computer ex-periments howed that the multi-neuro taggerhas a correct rate of over 94% for tagging am-biguous words when a small Thai corpus with22,311 ambiguous words is used for training.This result is better than any of the results ob-tained by the single-neuro taggers, which indi-cates that that the multi-neuro tagger can dy-namically find suitable lengths of contexts fortagging.
The cost to train a multi-neuro tag-ger was almost the same as that to train asingle-neuro tagger using new learning methodsin which the trai~ed results (weights) of the pre-vious taggers are used as initial weights for thelatter ones.
It was also shown that while theperformance of tagging can be improved onlyslightly, the training time can be greatly re-duced by using information gain to weight inputelements.ReferencesBrill, E., Magerman, D., and Santorini, B.: De-ducing linguistic structure from the statis-tics of large corpora, Proc.
DARPA Speechand Natural Language Workshop, HiddenValley PA, pp.
275-282, 1990.Charoenporn, T., Sornlertlamvanich, V., andIsahara, H.: Building a large Thai text cor-pus - part of speech tagged corpus: OR-CHID, Proc.
Natural Language Process-ing Pacific Rim Symposium 1997, Thailand,1997.Daelemans, W., Zavrel, J., Berck, P., and Gillis,S.
: MBT: A memory-based part of speechtagger-generator, Proc.
4th Workshop onVery Large Corpora, Denmark, 1996.Haykin, S.: Neural Networks, Macmillan Col-lege Publishing Company, Inc., 1994.Merialdo, B.: Tagging English text with a prob-abilistic model, Computational Linguistics,vol.
20, No.
2, pp.
155-171, 1994.Quinlan, J.: C4.5: Programs for MachineLearning, San Mateo, CA: Morgan Kauf-mann, 1993.Schmid, H.: Part-of-speech tagging with neuralnetworks, Proc.
Int.
Conf.
on Computa-tional Linguistics, Japan, pp.
172-176, 1994.806
