Proceedings of the Fifth Law Workshop (LAW V), pages 74?81,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsCrowdsourcing Word Sense DefinitionAnna Rumshisky???
Computer Science and Artificial Intelligence LaboratoryMassachusetts Institute of TechnologyCambridge, MAarum@csail.mit.edu?Department of Computer ScienceBrandeis UniversityWaltham, MAAbstractIn this paper, we propose a crowdsourcingmethodology for a single-step construction ofboth an empirically-derived sense inventoryand the corresponding sense-annotated cor-pus.
The methodology taps the intuitions ofnon-expert native speakers to create an expert-quality resource, and natively lends itself tosupplementing such a resource with additionalinformation about the structure and reliabil-ity of the produced sense inventories.
The re-sulting resource will provide several ways toempirically measure distances between relatedword senses, and will explicitly address thequestion of fuzzy boundaries between them.1 IntroductionA number of recent initiatives has focused on cre-ating sense-annotated gold standards for word sensedisambiguation and induction algorithms.
However,such work has frequently come under criticism overthe lack of a satisfactory set of standards for creat-ing consistent, task-independent sense inventories.More systematic efforts to replace ad hoc lexico-graphic procedures for sense inventory constructionhave often focused on working with existing senseinventories, attempting to resolve the specific asso-ciated problems (e.g.
sense granularity, overlappingsenses, etc.)
Methodologically, defining a robustprocedure for sense definition has remained an elu-sive task.In this paper, we propose a method for creatinga sense inventory from scratch for any polysemousword, simultaneously with the corresponding sense-annotated lexical sample.
The methodology wepropose explicitly addresses the question of relatedword senses and fuzzy boundaries between them,without trying to establish hard divisions where em-pirically there are none.The proposed method uses Amazon?s Mechani-cal Turk for sense annotation.
Over the last severalof years, Mechanical Turk, introduced by Amazonas ?artificial artificial intelligence?, has been usedsuccessfully for a number of NLP tasks, includingrobust evaluation of machine translation systems byreading comprehension (Callison-Burch, 2009), andother tasks explored in the recent NAACL workshop(Callison-Burch and Dredze, 2010b).
MechanicalTurk has also been used to create labeled data setsfor word sense disambiguation (Snow et al, 2008)and even to modify sense inventories.
But the origi-nal sense inventory construction has always been leftto the experts.
In contrast, in the annotation methodwe describe, the expert is eliminated from the an-notation process.
As has been the case with usingMechanical Turk for other NLP tasks, the proposedannotation is quite inexpensive and can be done veryquickly, while maintaining expert-level annotationquality.The resulting resource will produce several waysto empirically measure distances between senses,and should help to address some open research ques-tions regarding word sense perceptions by nativespeakers.
We describe a set of pilot annotation stud-ies needed to ensure reliability of this methodologyand test the proposed quality control mechanisms.The outcome will be a lexicon where sense inven-tories are represented as clusters of instances, andan explicit quantitative representation of sense con-74sistency, distance between senses, and sense overlapis associated with the senses for each word.
The goalis to provide a more accurate representation the wayspeakers of a language conceptualize senses, whichcan be used for training and testing of the automatedWSD systems, as well as to automatically induce se-mantic and syntactic context patterns that representusage norms and permit native speakers to performsense disambiguation.2 The Problem of Sense DefinitionThe quality of the annotated corpora depends di-rectly on the selected sense inventory, so, for ex-ample, SemCor (Landes et al, 1998), which usedWordNet synsets, inherited all the associated prob-lems, including using senses that are too fine-grained and in many cases poorly distinguished.
Atthe Senseval competitions (Mihalcea et al, 2004;Snyder and Palmer, 2004; Preiss and Yarowsky,2001), the choice of a sense inventory also fre-quently presented problems, spurring the efforts tocreate coarser-grained sense inventories (Navigli,2006; Hovy et al, 2006; Palmer et al, 2007).
Inven-tories derived from WordNet by using small-scalecorpus analysis and by automatic mapping to topentries in Oxford Dictionary of English were usedin the recent workshops on semantic evaluation, in-cluding Semeval-2007 and Semeval-2010 (Agirre etal., 2007; Erk and Strapparava, 2010).Several current resource-oriented projects attemptto formalize the procedure of creating a sense inven-tory.
FrameNet (Ruppenhofer et al, 2006) attemptsto organize lexical information in terms of script-like semantic frames, with semantic and syntacticcombinatorial possibilities specified for each frame-evoking lexical unit (word/sense pairing).
CorpusPattern Analysis (CPA) (Hanks and Pustejovsky,2005) attempts to catalog norms of usage for in-dividual words, specifying them in terms of con-text patterns.
Other large-scale resource-buildingprojects also use corpus analysis techniques.
InPropBank (Palmer et al, 2005), verb senses weredefined based on their use in Wall Street Journal cor-pus and specified in terms of framesets which con-sist of a set of semantic roles for the arguments of aparticular sense.
In the OntoNotes project (Hovy etal., 2006), annotators use small-scale corpus anal-ysis to create sense inventories derived by group-ing together WordNet senses, with the procedure re-stricted to maintain 90% inter-annotator agreement.Importantly, most standard WSD resources con-tain no information about the clarity of distinctionsbetween different senses in the sense inventory.
Forexample, OntoNotes, which was used for evaluationin the word sense disambiguation and sense induc-tion tasks in the latest SemEval competitions con-tains no information about sense hierarchy, relatedsenses, or difficulty and consistency of a given set ofsenses.3 Characteristics of the Proposed LexicalResourceThe lexical resource we propose to build is a sense-disambiguated lexicon which will consist of anempirically-derived sense inventory for each word inthe language, and a sense-tagged corpus annotatedwith the derived inventories.
The resource will beassembled from ?the ground up?
using the intuitionsof non-expert native speakers about the similaritybetween different uses of the same word.
Each sensewill be represented as a cluster of instances groupedtogether in annotation.
The following informationwill be associated with each sense cluster:1.
Consistency rating for each sense cluster, in-cluding several of the following measures:?
Annotator agreement, using the inter-annotator agreement measures for thesense cluster (e.g.
Fleiss?
Kappa);?
Cluster tightness, determined from thedistributional contextual features associ-ated with instance comprising the cluster;2.
Distances to other sense clusters derived for thesame word, using several distance measures,including:?
Cluster overlap, determined from the per-centage of instances associated with bothclusters;?
Translation similarity, determined as thenumber existing different lexicalizationsin an aligned multilingual parallel corpus,using a measurement methodology similarto Resnik and Yarowsky (1999).75The resource would also include a Membershiprating for each instance within a given sense clus-ter, which would represent how typical this exam-ple is for the associated sense cluster.
The instanceswhose membership in the cluster was establishedwith minimal disagreement between the annotators,and which do not have multiple sense cluster mem-bership will be designated as the core of the sensecluster.
The membership ratings would be based on(1) inter-annotator agreement for that instance (2)distance from the core elements of the cluster.Presently, the evaluation of automated WSD andWSI systems does not take into account the rela-tive difficulty of sense distinctions made within agiven sense inventories.
In the proposed resource,for every lexical item, annotator agreement valueswill be associated with each sense separately, as wellas with the full sense inventory for that word, provid-ing an innate measure of disambiguation difficultyfor every lexical item.Given that the fluidity of senses is such a perva-sive problem for lexical resources and that it cre-ates severe problems for the usability of the systemstrained using these resources, establishing the relia-bility and consistency of each sense cluster and the?prototypicality?
of each example associated withthat sense is crucial for any lexical resource.
Simi-larly crucial is the information about the overlap be-tween senses in a sense inventory as well as the sim-ilarity between senses.
And yet, none of the exist-ing resources contain this information.1 As a result,the systems trained on sense-tagged corpora usingthe existing sense inventories attempt to make sensedistinctions where empirically no hard division be-tween senses exist.
And since the information aboutconsistency and instance typicality is not available,the standard evaluation paradigm currently used inthe field for the automated WSD/WSI systems doesnot take it into account.
In contrast, the methodologywe propose here lends itself naturally to quantitativeanalysis needed to explicitly address the question ofrelated word senses and fuzzy boundaries betweenthem.1One notable exception is the sense-based inter-annotatoragreement available in OntoNotes.4 Annotation MethodologyIn traditional annotation settings, the quality of an-notation directly depends on how well the annota-tion task is defined.
The effects of felicitous or poortask design are greatly amplified when one is target-ing untrained non-expert annotators.Typically for the tasks performed using Mechan-ical Turk, complex annotation is split into simplersteps.
Each step is farmed out to the non-expert an-notators employed via Mechanical Turk (henceforth,MTurkers) in a form of a HIT (Human IntelligenceTask), a term used to refer to the tasks that are hardto perform automatically, yet very easy to do for hu-mans.4.1 Prototype-Based ClusteringWe propose a simple HIT design intended to imi-tate the work done by a lexicographer in corpus-based dictionary construction, of the kind used inCorpus Pattern Analysis (CPA, 2009).
The task isdesigned as a sequence of annotation rounds, witheach round creating a cluster corresponding to onesense.
MTurkers are first given a set of sentencescontaining the target word, and one sentence that israndomly selected from this set as a target sentence.They are then asked to identify, for each sentence,whether the target word is used in the same way asin the target sentence.
If the sense is unclear or itis impossible to tell, they are instructed to pick the?unclear?
option.
After the first round of annota-tion is completed, the sentences that are judged assimilar to the target sentence by the majority voteare set apart into a separate cluster corresponding toone sense, and excluded from the set used in furtherrounds.
The procedure is repeated with the remain-ing set, i.e.
a new target sentence is selected, and theremaining examples are presented to the annotators.This cycle is repeated until all the remaining exam-ples are classified as ?unclear?
by the majority vote,or no examples remain.4.2 Proof-of-Concept StudyA preliminary proof-of-concept study for this taskdesign has been reported on previously (Rumshiskyet al, 2009).
In that study, the proposed task designwas tested on a chosen polysemous verb of mediumdifficulty.
The results were then evaluated against76the groupings created by a professional lexicogra-pher, giving the set-matching F-score of 93.0 and theentropy of the two clustering solutions of 0.3.
Theexample sentences were taken from the CPA verblexicon for crush.
Figure 1 shows the first screendisplayed to MTurkers for the HIT, with ten exam-ples presented on each screen.
Each example wasannotated by 5 MTurkers.The prototype sentences associated with eachcluster obtained for the verb crush are shown below:C1 By appointing Majid as Interior Minister, Pres-ident Saddam placed him in charge of crushingthe southern rebellion.C2 The lighter woods such as balsa can be crushedwith the finger.C3 This time the defeat of his hopes didn?t crushhim for more than a few days.Each round took approximately 30 minutes to anhour to complete, depending on the number of sen-tences in that round.
Each set of 10 sentences tookon the average 1 minute, and the annotator received$0.03 USD as compensation.
The experiment wasconducted using 5-way annotation, and the total sumspent was less than $10 USD.
It should be notedthat in a large-scale annotation effort, the cost of theannotation for a single word will certainly vary de-pending on the number of senses it has.
However,time is less of an issue, since the annotators can workin parallel on many words at the same time.4.3 Removing Prototype ImpactPrototype-based clustering produces hard clus-ters, without explicit information about the originof boundary cases or the potentially overlappingsenses.
One of the possible alternatives to having in-stances judged against a single prototype, with mul-tiple iterations, is to have pairs of concordance linesevaluated against each other.
This is in effect morerealistic, since (1) each sentence is effectively a pro-totype, and (2) there is no limitation on the types ofsimilarity judgments allowed; ?cross-cluster?
con-nections can be retained.Whether obtained in a prototype-based setup, orin pairs, the obtained data lends itself well to agraph representation.
The pairwise judgments in-duce an undirected graph, in which judgments canbe thought of as edges connecting the instancenodes, and interconnected clusters of nodes corre-spond to the derived sense inventory (cf.
Figure 2).In the pairwise setup, results do not depend on theselection of a prototype sentence, so it provides anatural protection against a single unclear sentencehaving undue impact on cluster results, and does sowithout having to introduce an additional step intothe annotation process.
It also protects against di-rectional similarity evaluation bias.
However, oneof the disadvantages is the number of judgments re-quired to collect.
The prototype-based clusteringof N instances requires between N(N ?
1)/2 andN ?
1 judgments (depending on the way instancessplit between senses), which gives O(N2) for 1 clus-ter 1 instance case vs. O(N) for 1 cluster 1 wordcase.
A typical sense inventory has < 10 senses, sothat gives us an estimate of about 10N judgmentsto cluster N concordance lines, to be multiplied bythe number of annotators for each pair.
In order tobypass prototyping, we must allow same/differentjudgments for every pair of examples.
For N ex-amples, this gives O(N2) judgments, which makescollecting all pair judgments, from multiple annota-tors, too expensive.One of the alternatives for reducing the numberof judgments is to use a partial graph approxima-tion.
The idea behind it is that rather than collectingrepeat judgments (multiple annotations) of the sameinstance, one would collect a random subset of edgesfrom the full graph, and then perform clustering onthe obtained sparse graph.
Full pairwise annotationwill need to be performed on a small cross-sectionof English vocabulary in order to get an idea of howsparse the judgment graph can be to obtain resultscomparable to those we obtained with prototype-based clustering using good prototypes.Some preliminary experiments using MarkovClustering (MCL) on a sparse judgment graph sug-gest that the number of judgments collected in theproof-of-concept experiment above by Rumshisky etal.
(2009) in order to cluster 350 concordance lineswould only be sufficient to reliably cluster about 140concordance lines.77Figure 1: Task interface and instructions for the HIT presented to the non-expert annotators in proof-of-conceptexperiment.5 Pilot AnnotationsIn this section, we outline the pilot studies thatneed to be conducted prior to applying the describedmethodology in a large-scale annotation effort.
Thegoal of the pilot studies we propose is to establishthe best MTurk annotation practice that would en-sure the reliability of obtained results while mini-mizing the required time and cost of the annotation.The anticipated outcome of these studies is a robustmethodology which can be applied to unseen dataduring the construction of the proposed lexical re-source.5.1 Testing the validity of obtained resultsThe goal of the first set of studies is to establishthe validity of sense groupings obtained using non-expert annotators.
We propose to use the procedureoutlined in Sec 4 on the data from existing sense-tagged corpora, in particular, OntoNotes, PropBank,NomBank, and CPA.This group of pilot studies would involve per-forming prototype-based annotation for a selectedset of words representing a cross-section of Englishvocabulary.
A concordance for each selected wordwill be extracted from the gold standard provided byan expert-tagged sense-annotated corpus.
The initialset of selected content words would be evenly splitbetween verbs and nouns.
Each group will consistof a set of words with different degrees of polysemy.The lexical items would need to be prioritized ac-cording to corpus frequencies, with more frequentwords from each group being given preference.For example, for verbs, a preliminary study donewithin the framework of the CPA project suggestedthat out of roughly 6,000 verbs in a language, 30%have one sense, with the rest evenly split betweenverbs having 2-3 senses and verbs having more than4 senses.
About 20 light verbs have roughly 100senses each.
The chosen lexical sample will there-fore need to include low-polysemy verbs, mid-rangeverbs with 3-10 senses, lighter verbs with 10-20senses, and several light verbs.
Degree of polysemywould need to be obtained from the existing lexi-cal resource used as a gold standard.
The annota-tion procedure could also be tested additionally on asmall number of adjectives and adverbs.78Figure 2: Similarity judgment graphA smaller subset of the re-annotated data wouldthen need to be annotated using full pairwiseannotation.
The results of this annotation wouldneed to be used to investigate the quality of theclusters obtained using a partial judgment graph, in-duced by a subset of collected judgments.
The re-sults of both types of annotation could then be usedto evaluate different measures of sense consistencyand as well as for evaluation of distance between dif-ferent senses of a lexical item.5.2 Testing quality control mechanismsThe goal of this set of studies is to establish reliablequality control mechanisms for the annotation.
Anumber of mechanisms for quality control have beenproposed for use with Mechanical Turk annotation(Callison-Burch and Dredze, 2010a).
We propose toinvestigate the following mechanisms:?
Multiple annotation.
A subset of the datafrom existing resources would need to be an-notated by a larger number of annotators, (e.g.10 MTurkers.
The obtained clustering resultswould need to be compared to the gold standarddata from the existing resource, while varyingthe number of annotators producing the clus-tering through majority voting.
Results fromdifferent subsets of annotators for each subsetsize would need to be aggregated to evaluatethe consistency of annotation for each value.For example, for 3-way annotation, the cluster-ings obtained from by the majority vote withinall possible triads of annotators would be eval-uated and the results averaged.?
Checking annotator work against goldstandard.
Using the same annotated data set,we could investigate the effects of eliminatingthe annotators performing poorly on the judg-ments of similarity for the first 50 examplesfrom the gold standard.
The judgments of theremaining annotators would need to be aggre-gated to produce results through a majorityvote.?
Checking annotator work against the majorityvote.
Using a similar approach, we can inves-tigate the effects of eliminating the annotatorsperforming poorly against the majority vote.The data set obtained above would allow us toexperiment with different thresholds for elim-inating annotators, in each case evaluating theresulting improvement in cluster quality.?
Using prototype-quality control step.
Wewould need to re-annotate a subset of words us-ing an additional step, during which poor qual-ity prototype sentences will be eliminated.
Thisstep would be integrated with the main annota-tion as follows.
For each candidate prototypesentence, we would collect the first few similar-ity judgments from the selected number of an-notators.
If a certain percentage of judgmentsare logged as unclear, the sentence is elimi-79nated from the set, and another prototype sen-tence is selected.
We would evaluate the resultsof this modification, using different thresholdsfor the number of judgments collected and thepercentage of ?unclear?
ratings.5.3 Using translation equivalents to computedistances between sensesThe goal of this set of studies is to investigate theviability of computing distances between the senseclusters obtained for a given word by using its trans-lation equivalents in other languages.
If this method-ology proves viable, then the proposed lexical re-source can be designed to include some data frommultilingual parallel corpora.
This would provideboth a methodology for measuring relatedness of de-rived senses and a ready set of translation equiva-lents for every sense.Resnik and Yarowsky (1999) used human anno-tators to produce cross-lingual data in order to mea-sure distances between different senses in a mono-lingual sense inventory and derive a hierarchy ofsenses, at different levels of sense granularity.
Twomethods were tested, where the first one involvedasking human translators for the ?best?
translationfor a given polysemous word in a monolingualsense-annotated lexical sample data set.
The sec-ond method involved asking the human translators,for each pair of examples in the lexical sample, toprovide different lexicalizations for the target word,if they existed in their language.
The distances be-tween different senses were then determined fromthe number of languages in which different lexi-calizations were preferred (or existed) for differentsenses of the target word.In the present project, we propose to obtain simi-lar information by using the English part of a word-aligned multilingual parallel corpus for sense anno-tation.
The degree of cross-lingual lexicalization ofthe target word in instances associated with differ-ent sense classes could then be used to evaluate thedistance between these senses.
We propose the fol-lowing to be done as a part of this pilot study.
For aselected sample of polysemous words:?
Extract several hundred instances for eachword from the English part of a multilingualcorpus, such as the Europarl (Koehn, 2005); 2?
Use the best MTurk annotation procedure as es-tablished in Sec 5.2 to cluster the extracted in-stances;?
Obtain translation equivalents for each instanceof the target word using word-alignment pro-duced with Giza++ (Och and Ney, 2000);?
Compute the distances between the obtainedclusters by estimating the probability of differ-ent lexicalization of the two senses from theword-aligned parallel corpus.The distances would then be computed using a mul-tilingual cost function similar to the one used byResnik and Yarowsky (1999), shown in Figure 5.3.The Europarl corpus contains Indo-European lan-guages (except for Finnish), predominantly of theRomanic and Germanic family.
These languages of-ten have parallel sense distinctions.
If that proves tobe the case, a small additional parallel corpus withthe data from other non-European languages wouldneed to be used to supplement the data from Eu-roparl.6 ConclusionIn this paper, we have presented a proposal for a newannotation strategy for obtaining sense-annotateddata WSD/WSI applications, together with the cor-responding sense inventories, using non-expert an-notators.
We have described a set of pilot studies thatwould need to be conducted prior to applying thisstrategy in a large-scale annotation effort.
We out-lined the provisional design of the lexical resourcethat can be constructed using this strategy, includingthe native measures for sense consistency and diffi-culty, distance between related senses, sense over-lap, and other parameters necessary for the hierar-chical organization of sense inventories.AcknowledgmentsI would like to thank James Pustejovsky and DavidTresner-Kirsch for their contributions to this project.2If necessary, the instance set for selected words may be sup-plemented with the data from other corpora, such as the JRC-Acquis corpus (Steinberger et al, 2006).80Cost(sensei, sensej) =1|Languages|?L?LanguagesPL(diff-lexicalization|sensei, sensej)Figure 3: Multilingual cost function for distances between senses.ReferencesE.
Agirre, L. Ma`rquez, and R. Wicentowski, editors.2007.
Proceedings of the Fourth International Work-shop on Semantic Evaluations (SemEval-2007).
ACL,Prague, Czech Republic, June.Chris Callison-Burch and Mark Dredze.
2010a.
Creatingspeech and language data with amazon?s mechanicalturk.
In Proceedings of the NAACL HLT 2010 Work-shop on Creating Speech and Language Data withAmazon?s Mechanical Turk, pages 1?12, Los Angeles,June.
ACL.Chris Callison-Burch and Mark Dredze, editors.
2010b.Proceedings of the NAACL HLT 2010 Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk.
ACL, Los Angeles, June.Chris Callison-Burch.
2009.
Fast, cheap, and cre-ative: Evaluating translation quality using amazon?smechanical turk.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP 2009).CPA.
2009.
Corpus Pattern Analysis.Katrin Erk and Carlo Strapparava, editors.
2010.
Pro-ceedings of the 5th International Workshop on Seman-tic Evaluation.
ACL, Uppsala, Sweden, July.P.
Hanks and J. Pustejovsky.
2005.
A pattern dictionaryfor natural language processing.
Revue Franc?aise deLinguistique Applique?e.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
OntoNotes: The 90% solu-tion.
In Proceedings of the Human Language Technol-ogy Conference of the NAACL, Companion Volume:Short Papers, pages 57?60, New York City, USA,June.
ACL.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In MT summit, volume 5.Citeseer.S.
Landes, C. Leacock, and R.I. Tengi.
1998.
Buildingsemantic concordances.
In C. Fellbaum, editor, Word-net: an electronic lexical database.
MIT Press, Cam-bridge (Mass.).R.
Mihalcea, T. Chklovski, and A. Kilgarriff.
2004.
TheSenseval-3 English lexical sample task.
In Rada Mi-halcea and Phil Edmonds, editors, Senseval-3: ThirdInternational Workshop on the Evaluation of Sys-tems for the Semantic Analysis of Text, pages 25?28,Barcelona, Spain, July.
ACL.R.
Navigli.
2006.
Meaningful clustering of senseshelps boost word sense disambiguation performance.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 105?112, Sydney, Australia, July.
ACL.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
pages 440?447, Hongkong, China, Oc-tober.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.M.
Palmer, H. Dang, and C. Fellbaum.
2007.
Mak-ing fine-grained and coarse-grained sense distinctions,both manually and automatically.
Journal of NaturalLanguage Engineering.J Preiss and D. Yarowsky, editors.
2001.
Proceedings ofthe Second Int.
Workshop on Evaluating WSD Systems(Senseval 2).
ACL2002/EACL2001.P.
Resnik and D. Yarowsky.
1999.
Distinguishing sys-tems and distinguishing senses: new evaluation meth-ods for word sense disambiguation.
Natural LanguageEngineering, 5(2):113?134.A.
Rumshisky, J. Moszkowicz, and M. Verhagen.
2009.The holy grail of sense definition: Creating a sense-disambiguated corpus from scratch.
In Proceedingsof 5th International Conference on Generative Ap-proaches to the Lexicon (GL2009), Pisa, Italy.J.
Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,and J. Scheffczyk.
2006.
FrameNet II: Extended The-ory and Practice.R.
Snow, B. OConnor, D. Jurafsky, and A.Y.
Ng.
2008.Cheap and fastbut is it good?
evaluating non-expertannotations for natural language tasks.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2008).B.
Snyder and M. Palmer.
2004.
The english all-words task.
In Rada Mihalcea and Phil Edmonds,editors, Senseval-3: Third International Workshop onthe Evaluation of Systems for the Semantic Analysis ofText, pages 41?43, Barcelona, Spain, July.
ACL.R.
Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-javec, D. Tufis, and D. Varga.
2006.
The JRC-Acquis:A multilingual aligned parallel corpus with 20+ lan-guages.
Arxiv preprint cs/0609058.81
