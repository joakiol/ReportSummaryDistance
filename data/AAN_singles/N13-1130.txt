Proceedings of NAACL-HLT 2013, pages 1100?1109,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsPaving the Way to a Large-scale Pseudosense-annotated DatasetMohammad Taher Pilehvar and Roberto NavigliDepartment of Computer ScienceSapienza University of Rome{pilehvar,navigli}@di.uniroma1.itAbstractIn this paper we propose a new approach tothe generation of pseudowords, i.e., artificialwords which model real polysemous words.Our approach simultaneously addresses thetwo important issues that hamper the gener-ation of large pseudosense-annotated datasets:semantic awareness and coverage.
We eval-uate these pseudowords from three differentperspectives showing that they can be used asreliable substitutes for their real counterparts.1 IntroductionA fundamental problem in computational linguis-tics is the paucity of manually annotated data, suchas part-of-speech tagged sentences, treebanks, andlogical forms, which exist only for few languages(Ide et al 2010).
A case in point is the lack ofabundant sense annotated data, which hampers theperformance and coverage of lexical semantic taskssuch as Word Sense Disambiguation (Navigli, 2009;Navigli, 2012, WSD) and semantic role labeling(Gildea and Jurafsky, 2002).
A possible way tobreak this bottleneck is to use pseudowords, i.e., arti-ficial words constructed by conflating a set of unam-biguous words, with the aim of modeling polysemyin real ambiguous words.
The idea of pseudowordswas originally proposed by Gale et al(1992) andSchu?tze (1992) for WSD evaluation, but later foundapplication in other tasks such as selectional prefer-ences (Erk, 2007; Bergsma et al 2008; Chambersand Jurafsky, 2010), Word Sense Induction (Bor-dag, 2006; Di Marco and Navigli, 2013) or studiesconcerning the effects of the amount of data on ma-chine learning for natural language disambiguation(Banko and Brill, 2001).
Being made up of monose-mous words, pseudowords can potentially be used tocreate large amounts of pseudosense-annotated dataat virtually no cost, hence enabling large-scale stud-ies in lexical semantics.
Unfortunately, though, theextent of their usability for such a purpose is ham-pered by two main issues: semantic awareness andwide coverage.Semantic awareness corresponds to the constraintthat pseudowords, in order to be realistic, are ex-pected to have senses which are in a semantic rela-tionship (thus modeling systematic polysemy).
Re-cent work has focused on this issue and, by exploit-ing either specific lexical hierarchies (Nakov andHearst, 2003; Lu et al 2006), or the WordNet struc-ture (Otrusina and Smrz, 2010), have succeeded ingenerating pseudowords which are comparable toreal words in terms of disambiguation difficulty.
Thesecond challenge is coverage, which corresponds tothe number of distinct pseudowords an algorithmcan generate.
When coupled with the semanticawareness issue, wide coverage is hampered by thedifficulty in generating thousands of pseudowordswhich mimic existing polysemous words.Unfortunately, none of the existing approaches tothe generation of pseudowords can meet both thesechallenges simultaneously, and this has hinderedthe generation of a large pseudosense-annotateddataset.
For instance, approaches which exploit themonosemous neighbors of a target sense in Word-Net (Otrusina and Smrz, 2010) can be used to gener-ate pseudowords with good semantic awareness, but1100they have low coverage of ambiguous nouns whenmany pseudosense-tagged sentences are needed (cf.Section 2.1.1).In this paper we propose a new approach, basedon Personalized PageRank, which simultaneouslyaddresses the two above-mentioned issues concern-ing the generation of pseudowords (i.e., seman-tic awareness and coverage), and hence enablesthe generation of large-scale pseudosense-annotateddatasets.
We perform three different experiments toshow that our pseudowords are good at modeling ex-isting ambiguous words in terms of disambiguationdifficulty, representativeness of real senses and dis-tinguishability of the artificial senses.
As a byprod-uct of this work, we generate a large dataset that pro-vides 1000 tagged sentences for each of the 15,935pseudowords modeled after real ambiguous nouns inWordNet 3.0.2 PseudowordsA pseudoword p = w1*w2*.
.
.
*wn is an artificially-generated ambiguous word of polysemy degree nwhich is usually created by conflating n unique un-ambiguous words wi called pseudosenses.
For in-stance, airplane*river is a pseudoword with twomeanings explicitly identified by its pseudosenses:airplane and river.
Pseudowords are particularly in-teresting as they can be used to introduce controlledartificial ambiguity into a corpus.
Given a pseu-doword p and an untagged corpus C, this artificialtagging is achieved by substituting all occurrences ofwi in C with p for each pseudosense i ?
{1, .
.
.
, n}.As a result, each occurrence of the pseudoword p istagged with the underlying sense wi.
As an example,consider the following two sentences:a1.
The Wright brothers invented the airplane.a2.
The Nile is the longest river in the world.If we replace the individual occurrences of air-plane and river with the pseudoword airplane*riverwhile noting the replaced term as the correspondingsense, we obtain the following pseudosense-taggedsentences:b1.
The Wright brothers invented the airplane*river.b2.
The Nile is the longest airplane*river in the world.As a result of this procedure, we obtain a corpusof sentences containing the occurrences of an arti-ficially ambiguous word p, for each of which weknow its correct sense annotation wi.
Virtually anynumber of pseudowords can be created, resulting ina large pseudosense-annotated corpus.
An obviousrestriction on the choice of pseudosenses is that theyneed to be unambiguous, so as to avoid the introduc-tion of uncontrolled ambiguity.
Another constraintis that the constituent wi must satisfy a minimum oc-currence frequency in the corpus C. This minimumfrequency corresponds to the number of annotatedsentences that are requested for the task of interestwhich will exploit the resulting annotated corpus.An immediate way of generating a pseudowordwould be to randomly select its constituents fromthe set of all monosemous words given by a lexi-con (e.g., WordNet).
However, constructing a pseu-doword by merely combining a random set of unam-biguous words selected on the basis of their fallingin the same range of occurrence frequency (Schu?tze,1992), or leveraging homophones and OCR ambi-guities (Yarowsky, 1993), does not provide a suit-able model of a real polysemous word (Gaustad,2001; Nakov and Hearst, 2003).
This is becausein the real world different senses, unless they arehomonymous, share some semantic or pragmatic re-lation.
Therefore, random pseudowords will typ-ically model only homonymous distinctions (suchas the centimeter vs. curium senses of cm), whilethey will fall short of modeling systematic polysemy(such as the lack vs. insufficiency senses of defi-ciency).2.1 Semantically-aware PseudowordsIn order to cope with the above-mentioned lim-its of random pseudowords, an artificial word hasto model an existing word by providing a one-to-one correspondence between each pseudosense anda corresponding sense of the modeled word.
Forinstance, the pseudoword lack*shortfall is a goodmodel of the real word deficiency in that its pseu-dosenses preserve the meanings of their correspond-ing real word?s senses.
We call this kind of artificialwords semantically-aware pseudowords.In the next two subsections, we will describe twotechniques (the second of which is presented forthe first time in this paper) for the generation of1101Minimum PolysemyOverallFrequency 2 3 4 5 6 7 8 9 10 11 12 >120 87 82 74 71 67 70 60 64 45 46 44 28 83500 41 31 24 15 12 13 10 7 7 0 0 0 351000 31 20 16 7 4 6 4 3 0 0 0 0 25Table 1: Ambiguous noun coverage percentage of vicinity-based pseudowords by degree of polysemy for differentvalues of minimum pseudosense occurrence frequency in Gigaword.semantically-aware pseudowords.
In what followswe focus on nominal pseudowords, and leave the ex-tension to other parts of speech to future work.2.1.1 Vicinity-based PseudowordsA computational lexicon such as WordNet (Fell-baum, 1998) can be used as the basis for theautomatic generation of semantically-aware pseu-dowords, an idea which was first proposed byOtrusina and Smrz (2010).
WordNet can be viewedas a graph in which synsets act as nodes and the lexi-cal and semantic relationships among them as edges.Given a sense, the approach looks into its surround-ing synsets in the WordNet graph in order to finda related monosemous term that can represent thatsense.
As search space, the approach considers: theother literals in the same synset, the genus phrasefrom its textual definition, direct siblings, and di-rect hyponyms.
If no monosemous candidate can befound, this space is further extended to hypernymsand meronyms.
Hereafter, we term this approach asvicinity-based.For example, consider the generation process ofthe vicinity-based pseudoword corresponding to theterm coke, which has three senses in WordNet 3.0.There exist multiple monosemous candidates foreach sense: dozens of candidates (such as biomassand butane) in the direct siblings?
vicinity of thefirst sense, coca cola, pepsi, and pepsi cola for thesecond sense, and nose candy and coca cola forthe third sense.
Among these candidates Otrusinaand Smrz (2010) select those whose occurrence fre-quency ratio in a given text corpus is most similar tothat of the senses of the corresponding real word asgiven by a sense-annotated corpus.
Clearly, a suffi-ciently large sense-tagged corpus is required for cal-culating the occurrence frequency of the individualsenses of a word.
This is a limitation of the vicinity-based approach.In addition, as we mentioned earlier, we needpseudowords that can enable the generation of large-scale pseudosense-tagged corpora.
For this to beachieved, each pseudosense is required to occur witha relatively high frequency in a given text corpus.The vicinity-based approach can, however, identifyat best only a few representatives for each pseu-dosense, thus undermining its ability to cover manyambiguous nouns.
Table 1 shows the percentageof ambiguous nouns in WordNet that can be mod-eled using the vicinity-based approach when differ-ent minimum numbers of annotated sentences arerequested, i.e.
each pseudosense is required to oc-cur in at least 0 (i.e., no minimum frequency restric-tion), 500, or 1000 unique sentences in the referencecorpus (we use Gigaword (Graff and Cieri, 2003)in our experiments).
In the Table, beside the over-all coverage percentage, we present the coverage bydegree of polysemy and for three different values ofminimum pseudosense occurrence frequency.
Eventhough the overall coverage is over 80% when no re-striction on minimum frequency is considered (firstrow in the Table), this high coverage drops rapidlywhen we request some hundred sentences per sense.For instance, only 25% of the ambiguous nouns inWordNet can be modeled using this approach whena minimum frequency of 1000 noun occurrences isrequired (last row of Table 1), with most of the cov-ered words having low polysemy (in fact about 93%of them are either 2- or 3-sense nouns).
This se-vere limitation of the vicinity-based approach hin-ders a wide-coverage modeling of ambiguous nounsin WordNet, thus preventing it from being an op-tion for the generation of a large-scale pseudosense-annotated dataset.With a view to addressing the above-mentionedissues and to enable wide coverage, in the next sub-section we propose a flexible approach for the gen-eration of semantically-aware pseudowords.11022.1.2 Similarity-based PseudowordsThe vicinity-based pseudoword generation ap-proach works on local subgraphs of WordNet, con-sidering mostly all those candidates which are in adirect relationship with a real sense si, and treatingthem as potentially good representatives of si.
Wepropose an extension to this approach which exploitsthe WordNet semantic network in its entirety, henceenabling us to determine a graded degree of similar-ity between si and all the senses of all other wordsin WordNet.We chose a graph-based similarity measure fortwo reasons: firstly, it comes as a natural exten-sion of the vicinity-based method, and, secondly, al-ternative context-based methods such as Lin?s mea-sure (Lin, 1998) have been shown to require a wide-coverage sense-tagged dataset in order to calculatesimilarities on a sense-by-sense basis for all words inthe lexicon (Otrusina and Smrz, 2010).
As our sim-ilarity measure we selected the Personalized PageR-ank (Haveliwala, 2002, PPR) algorithm.
PPR basi-cally computes the probability according to which arandom walker at a specific node in a graph wouldvisit an arbitrary node in the same graph.
The al-gorithm estimates, for a specific node in a graph,a probability distribution (called PPR vector) whichdetermines the importance of any given node in thegraph for that specific node.
When applied to asemantic graph, this importance can be interpretedas semantic similarity.
PPR has previously beenused as a core component for semantic similarity1(Hughes and Ramage, 2007; Agirre et al 2009)and Word Sense Disambiguation (Agirre and Soroa,2009).Algorithm 1 shows the procedure for the genera-tion of our similarity-based pseudowords.
The algo-rithm takes an ambiguous word w as input, and out-puts its corresponding similarity-based pseudowordPw whose ith pseudosense models the ith sense ofw, together with a confidence score which we detailbelow.Given w, the algorithm iterates over the synsetscorresponding to its individual senses (lines 4-13)and finds the most suitable pseudosenses for Pw.
For1Top-ranking synsets will contain words which are mostlikely similar to the target sense, whereas we move to a gradednotion of relatedness as far as lower-ranking ones are concerned(Agirre et al 2009).Algorithm 1 Generate a similarity-based pseudowordInput: an ambiguous word w in WordNetOutput: a ?similarity-based?
pseudoword Pwa confidence score averageRank1: Pw ?
?2: totalRank?
03: i?
14: for each s ?
Synsets(w)5: similarSynsets?
PersonalizedPageRank(s)6: sort similarSynsets in descending order7: for each s?
?
similarSynsets8: totalRank?
totalRank + 19: for each w?
?
SynsetLiterals(s?
)10: if |Synsets(w?
)|=1 & Freq(w?
)>minFreq then11: Pw ?
Pw ?
{(i, w?
)}12: break13: i?
i + 114: averageRank?
totalRank/|Synsets(w)|15: return (Pw, averageRank)each synset s of w, we start the PPR algorithm froms (line 5) and collect the probability distribution vec-tor output by PPR (similarSynsets in the algorithm),which determines the probability of reaching eachsynset in WordNet starting from s. We then sortthis vector (line 6) and check if each of its nomi-nal synsets (s?)
contains a monosemous word (line10).
This search continues until a suitable candi-date is found that satisfies a certain minimum oc-currence frequency minFreq.
When this occurs, theselected monosemous candidate w?
is saved as thecorresponding pseudosense for the ith sense of Pw(line 11).
We iterate these steps for all synsets of w.In line 14 we calculate the averageRank, a valuegiven by the average of synset positions in the simi-larSynsets lists from which the pseudosenses of Pware picked out.
We later use this value as a confi-dence score while evaluating our pseudowords.
Fi-nally, the algorithm returns the corresponding pseu-doword Pw along with its averageRank score (line15).
We show in Table 2 some examples of ambigu-ous words together with their similarity-based pseu-dowords.Thanks to the large search space of our similarity-based approach, we are always able to select amonosemous candidate for each pseudosense, thusresolving the coverage issue regarding vicinity-based pseudowords.
A question that arises here isthat of how often our algorithm needs to resort tolower-ranking items in the similarSynsets list.
To1103Word Similarity-based Pseudowordbernoulli physicist*mathematician*astronomercoach football coach*tutor*passenger car*clarence*public transportgreen greenery*common*labor leader*green party*river*golf course*greens*maxhoroscope forecast*diagramsunray sunbeam*vine*sunlightlifter athlete*thiefTable 2: Similarity-based pseudowords generated forsix different nouns in WordNet 3.0 (with minimum fre-quency of 1000 occurrences in Gigaword).
Pseudosenseswhich could not be modeled using the vicinity-based ap-proach are shown in bold.verify this, we analyzed the averageRank values out-put by Algorithm 1.
Table 3 shows for each poly-semy degree and for three different values of min-Freq, the mean and mode statistics of the averageR-ank scores of the generated similarity-based pseu-dowords for all the 15,935 polysemous nouns inWordNet 3.0.
As expected, the higher the number ofrequired sentences per pseudosense (minFreq), thefurther the algorithm descends through the list simi-larSynsets to select a pseudosense.
However, as canbe seen from the mode statistics in the Table, evenwhen minFreq is set to a large value, most of thepseudosenses are picked from the highest-rankingpositions in the similarSynsets list.3 EvaluationOur novel similarity-based algorithm for the gen-eration of pseudowords inherently tackles the cov-erage issue.
To test whether our generated pseu-dowords also cope with the issue of semantic aware-ness we carried out three separate evaluations soas to assess their strength in modeling semanticproperties of their corresponding real senses fromdifferent perspectives.
These will be described inthe next three subsections.
Since our aim was toleverage pseudowords for the creation of a large-scale pseudosense-annotated dataset, we performedevaluations on pseudowords generated with minFreqper pseudosense set to 1000 (i.e., we can gener-ate at least 1000 annotated sentences for each pseu-dosense).minFreq 0 500 1000poly.
mean mode mean mode mean mode2 2.0 1.0 14.8 2.0 25.4 4.03 2.3 1.7 13.4 2.7 21.0 5.54 2.3 1.8 12.3 5.8 19.8 6.85 2.3 1.8 12.9 5.6 20.0 10.06 2.4 2.0 13.7 4.5 18.7 8.87 2.3 2.1 11.5 6.3 16.0 6.18 2.2 1.8 11.3 9.6 17.2 10.89 2.4 2.0 10.7 10.9 15.6 15.110 2.2 2.0 10.1 7.0 14.3 12.111 2.4 2.1 10.2 7.1 14.2 17.312 2.5 2.4 11.0 4.4 14.4 14.4>12 2.6 1.0 9.3 2.0 13.7 4.0overall 2.1 1.0 14.1 2.0 23.4 4.0Table 3: Statistics of averageRank scores for the full setof 15,935 similarity-based pseudowords modeled afterambiguous nouns in WordNet 3.0: we show mean andmode statistics for three different values of minimum oc-currence frequency (0, 500, and 1000).
We show the av-erage value in the case of multiple modes.3.1 Disambiguation Difficulty of PseudowordsOur first experiment is an extrinsic evaluation ofpseudowords.
Ideally, pseudowords are expected toshow a similar degree of difficulty to real ambigu-ous words in a disambiguation task (Otrusina andSmrz, 2010; Lu et al 2006).
We thus experimen-tally tested this assumption on similarity-based andrandom pseudowords.
Given its low coverage, weexcluded the vicinity-based approach from this ex-periment.Starting from a sense-tagged lexical sampledataset for a set of ambiguous nouns, for each suchnoun and for each kind of pseudoword, we automat-ically generated a pseudosense-annotated dataset byenforcing the same sense distribution as the cor-responding real ambiguous noun.
This constraintwas particularly important for random pseudowordssince they do not model the corresponding real am-biguous words (see Section 2).
An analysis was thenperformed to compare the disambiguation perfor-mance of a supervised WSD system on a given am-biguous word against its corresponding pseudoword.Specifically, for our manually sense-tagged cor-pus we used the Senseval-3 English lexical sampledataset (Mihalcea et al 2004), which contains 3593and 1807 sense-tagged sentences for 20 ambiguousnouns (with an average polysemy degree of 5.8) inits training and test sets, respectively.
We generated,1104with minFreq = 1000, the similarity-based pseu-dowords corresponding to these 20 nouns, as wellas a set of 20 random pseudowords with the samepolysemy degrees.
We note that, in this setting, thevicinity-based approach could only generate pseu-dowords corresponding to 5 of the 20 nouns.In order to create the datasets for our experiments,for each of our similarity-based and random pseu-dowords, we sampled unique sentences from the En-glish Gigaword corpus (Graff and Cieri, 2003) ac-cording to the same sense distributions given by theSenseval-3 training and test datasets for the corre-sponding real word.
Next, we performed WSD onour three datasets, namely: the Senseval-3 datasetof real words, and the two artificially sense-taggeddatasets for the similarity-based and random pseu-dowords.
As our WSD system for this experiment,we used It Makes Sense (IMS), a state-of-the-art su-pervised WSD system (Zhong and Ng, 2010).WSD recall2 performance values on the above-mentioned datasets are shown in Table 4.
For therandom setting, in order to ensure stability, the re-sults are averaged on a set of 25 different pseu-dowords modeling a given ambiguous noun.
Wecan see from the Table that the overall systemperformance with the similarity-based pseudowords(75.14%) is much closer to the real setting (73.26%)than it is with random pseudowords (78.80%).
Forrandom pseudowords, the overall recall over 25 runsranges from 75.40% to 80.80%.Moreover, the similarity-based approach exhibitsa closer WSD recall performance to that of real data(|RE?SB| column in the table) for 15 of the 20nouns (shown in bold in the Table).
Accordingly,the overall sum of the differences (distance) betweenthe recall values is 129.3 for similarity-based pseu-dowords, which is considerably lower than the 196.4for random pseudowords (averaged over 25 runswhose distances range from 158.3 to 262.0).To further corroborate our findings, we calculatedthe Pearson?s r correlation between recall values onreal words with those obtained on the correspondingpseudowords.
Similarity-based pseudowords obtainthe high correlation of 0.74, whereas this value dropsto 0.54 for random pseudowords.
Even worse, we2Since in our experiments the WSD system always providesan answer for each item in the test set, the values of precision,recall and F1 will be equal.Word RE SB RND |RE?SB| |RE?RND|argument 50.44 68.79 77.15 18.35 26.71arm 92.30 85.69 88.11 6.61 4.19atmosphere 70.52 69.15 80.44 1.37 10.32audience 81.28 73.74 83.76 7.54 4.22bank 85.76 83.07 82.46 2.69 3.99degree 78.42 81.58 80.59 3.16 4.35difference 62.46 61.43 75.17 1.03 12.90difficulty 52.72 51.82 67.23 0.90 14.97disc 78.62 76.48 78.07 2.14 6.18image 71.78 75.76 81.50 3.98 10.02interest 77.34 73.19 71.70 4.15 6.85judgment 55.64 66.87 59.64 11.23 9.01organization 80.36 72.86 78.65 7.50 3.65paper 60.84 66.29 73.14 5.45 12.59party 82.94 80.00 81.04 2.94 3.74performance 58.56 64.76 73.86 6.20 15.52plan 88.42 85.41 87.39 3.01 3.12shelter 58.48 74.75 80.21 16.27 21.73sort 67.64 88.15 77.37 20.51 9.73source 63.46 67.74 66.26 4.28 7.03overall 73.26 75.14 78.80 129.31 196.35Table 4: Recall percentage of IMS on the 20 nouns of theSenseval-3 lexical-sample test set (RE) compared to thecorresponding similarity-based (SB) and random (RND)pseudowords.
The last 2 columns show absolute differ-ences between the real and the two pseudoword settings.observed a high variation of correlation (in the rangeof [0.18, 0.67]) over the 25 sets of random pseu-dowords (0.54 being the average).3.2 Representative Power of PseudosensesThe ideal case for pseudosenses would be that ofbeing in a synonymous relationship with the cor-responding real sense, i.e., selected from the sameWordNet synset.
But given that many of the Word-Net synsets do not contain monosemous terms, thesimilarity-based approach often needs to look fur-ther into other related synsets to find a suitable pseu-dosense.
To get a clear idea of the exact statistics, wewent through all our similarity-based pseudowordsand, for each pseudosense wi, checked the relation-ship in WordNet between the synset containing wiand the corresponding real sense.
Table 5 showsfor three values of minFreq the distribution of pseu-dosenses across different types of WordNet relation-ships, also including indirect ones.
As can be seenin the Table, when minFreq is set to 0, a large por-tion of pseudosenses (around 75%) are selected fromsynonyms or generalization/specialization relations1105minFreq 0 500 1000RelationtypeSynonyms 33.0 7.6 5.4Hypernyms 33.4 16.1 13.0Hyponyms 9.1 6.1 4.9Meronyms 0.2 0.2 0.2Siblings 8.2 17.2 16.6Indirect relations 16.1 52.8 59.9Table 5: Percentage of similarity-based pseudosenses ob-tained from different types of WordNet relations.
(hypernym and hyponyms).
However, this percent-age drops to about 23% when minFreq = 1000.
Thissuggests that many of our pseudosenses are mod-eled from indirect relations when higher values ofminFreq are used.
This can potentially increase therisk of an undesirable modeling in which meaningsare not properly preserved.
For this reason, we car-ried out another experiment to assess the representa-tive power of similarity-based pseudosenses.
To thisend, we randomly sampled 110 pseudowords (fromthe entire set of 15,935 pseudowords generated withminimum frequency of 1000), 10 for each degree ofpolysemy, from 2 to 12, totaling 770 pseudosenses.Then we presented each of these pseudowords3 totwo annotators who were asked to judge the degreeof representativeness of its pseudosenses based onthe following scores: 1: completely unrelated, 2:somewhat related, 3: good substitute, or 4: perfectsubstitute.As an example, the scores assigned by the twoannotators to different pseudosenses of the pseu-doword generated for the noun representative areshown in Table 6.
The overall representativenessscore for each pseudoword is calculated by aver-aging the scores assigned to its individual pseu-dosenses.
For instance, the overall scores calculatedfor the pseudoword representative are 3.75 and 3.50(as given by the two annotators).
The first row inTable 7 shows the average representativeness scoresfor each degree of polysemy on the full set of 770pseudosenses.
It can be seen that the score remainsaround 3.0 for all polysemy degrees from 2 to 12.Despite the fact that only one fifth of pseudosensesare taken from synonyms, hypernyms and hyponyms(when minFreq is 1000, cf.
Table 5), the overall3For each pseudoword, we provided annotators with the cor-responding real word, as well as its synsets and glosses as givenby WordNet.Sense Definition (in short)Score1Score2{Synset}> Corresponding Pseudosensea person who represents others3 3{representative}> negotiatoran advocate who represents someone else?s policy4 4{spokesperson, interpreter, representative, voice}> spokespersona member of the U.S. House of Representatives4 4{congressman, congresswoman, representative}> congressmanan item of information that is typical of a group4 3{example, illustration, instance, representative}> case in pointaverage score 3.75 3.50Table 6: Examples of representativeness scores assignedby the annotators to pseudosenses of the term representa-tive.representativeness score of 3.12 shows that most ofthese pseudosenses can be considered as good sub-stitutes for their corresponding real senses.
There-fore we conclude that not only does our similarity-based pseudoword generation approach extend thecoverage of the vicinity-based method from 25% to100% (when minFreq = 1000), but also that thepseudosenses coming from more distant synsets asranked by PPR are still good representatives on av-erage.3.3 Distinguishability of PseudosensesIn addition to assessing the representativeness ofpseudosenses, their degree of distinguishability hasto be determined.
In other words, we have to de-termine how easily each pseudosense can be distin-guished from the others in a pseudoword.
Our rea-son for having such an experiment is readily illus-trated by way of an example: consider the similarity-based pseudoword philanthropist*benefactor4 cor-responding to the noun donor5.
Even though bothpseudosenses are good representatives for their cor-responding senses, the distinguishability of the two4From WordNet: ?Philanthropist: someone who makescharitable donations intended to increase human well-being?
;?Benefactor: a person who helps people or institutions (espe-cially with financial help)?.5donor has 2 senses according to WordNet 3.0: (1) ?personwho makes a gift of property?
; (2) ?
(medicine) someone whogives blood or tissue or an organ to be used in another person?.1106Polysemy 2 3 4 5 6 7 8 9 10 11 12 OverallRepresentativeness score 3.3 3.4 3.1 3.1 2.9 3.1 2.9 2.8 3.3 3.1 3.3 3.12Distinguishability score 0.90 0.83 0.83 0.82 0.81 0.77 0.75 0.73 0.80 0.71 0.70 0.79Table 7: Average representativeness and distinguishability scores for pseudosenses of different polysemy classes(scores range from 1 to 4 for representativeness and from 0 to 1 for distinguishability evaluation).real senses is not preserved in the pseudoword.
Forinstance, benefactor is a suitable pseudosense forboth senses of donor, whereas philanthropist cannotbe used in the blood donation sense.Therefore we carried out another manual evalua-tion to test the efficacy of pseudowords in preservingthe distinguishability of senses of real words.
To thisend, for each pseudoword Pw (from the same set of110 sampled pseudowords used in Section 3.2) wepresented its corresponding pseudosenses in randomorder to two annotators and asked them to associateeach pseudosense with the most appropriate Word-Net sense of the real word w. Then we calculateda distinguishability score for each polysemy degreeby dividing the number of correct mappings by thetotal number of senses.For instance, for the similarity-based pseudowordcorresponding to the word representative (shownin Table 6), we provided the shuffled list of pseu-dosenses [spokesperson, case in point, negotiator,congressman] to each annotator and asked them tosort the list according to the WordNet sense inven-tory of representative (i.e., map each pseudosense toits most suitable real sense).
Both annotators cor-rectly mapped all pseudosenses of this pseudoword;hence, the distinguishability score given by each an-notator for this pseudoword was 4/4 = 1.The average distinguishability scores for each de-gree of polysemy, as well as the overall score, isshown in Table 7 (second row).
Each value is anaverage of the scores obtained from the two an-notators.
It can be seen that the distinguishabilityscore decreases for higher degrees of polysemy.
Thescore, however, remains above 0.70 with highly-polysemous pseudowords.
The overall score of 0.79shows that similarity-based pseudowords effectivelypreserve the distinguishability of senses of their realcounterparts.
In other words, they do not tendto have over-generalized pseudosenses which covermore than one sense.4 Related WorkThe idea of pseudowords dates back to 1992, whenit was first proposed as a means of generating largeamounts of artificially annotated evaluation data forWSD algorithms (Gale et al 1992; Schu?tze, 1992).However, as mentioned earlier in Section 2, con-structing a pseudoword by combining a random setof unambiguous words, as was done in these earlyworks, can not model systematic polysemy (Gaus-tad, 2001; Nakov and Hearst, 2003), since differ-ent senses of a real ambiguous word, unless it ishomonymous, share some semantic or pragmatic re-lation.Several researchers addressed the issue of produc-ing semantically-aware pseudowords that can modelsemantic relationships between senses.
Nakovand Hearst (2003) used lexical category mem-bership from a medical term hierarchy (extractedfrom MeSH6 (Medical Subject Headings)) to cre-ate ?more plausibly-motivated?
pseudowords.
Byconsidering the frequency distributions from lexi-cal category co-occurrence, they produced a set ofpseudowords which were closer to real ambiguouswords in terms of disambiguation difficulty thanrandom pseudowords.
However, this approach re-quires a specific hierarchical lexicon and falls shortof creating many pseudowords with high polysemy(the authors report generating pseudowords with twosenses only).More recent work has focused on the identifica-tion of monosemous representatives in the surround-ing of a sense, i.e., selected among concepts directlyrelated to the given sense.
Lu et al(2006) mod-eled senses of a real ambiguous word by pickingout the most similar monosemous morpheme from aChinese hierarchical lexicon.
Pseudowords are thenconstructed by conflating these morphemes accord-ingly.
However, this method leverages a specificChinese hierarchical lexicon, in which different lev-6http://www.nlm.nih.gov/mesh1107els of the hierarchy correspond to different levels ofsense granularity.
A more flexible technique is pro-posed by Otrusina and Smrz (2010) who model am-biguous words in WordNet.
Their vicinity-based ap-proach searches the surroundings of each particularsense in the WordNet graph in order to find an un-ambiguous representative for that sense.
However,as we described in Section 2.1.1, while the approachaddresses the semantic awareness issue, it falls shortof providing a high coverage, an issue which wetackle in our novel similarity-based approach.5 Conclusion and Future WorkIn this paper we proposed a new technique for thegeneration of pseudowords which, in contrast toexisting work, can simultaneously tackle the twomajor issues associated with pseudowords, i.e., se-mantic awareness and coverage.
Our approach canbe used to model any given ambiguous noun inWordNet, hence enabling the generation of large-scale pseudosense-annotated datasets for thousandsof pseudowords.
We performed three experimentsto evaluate the reliability of our pseudowords.
Weshowed that the similarity-based pseudowords arehighly correlated with their real counterparts interms of disambiguation difficulty.
Further evalua-tions demonstrated that this approach is able to pro-vide a good semantic modeling of individual sensesof real words while preserving their distinguishabil-ity.We are releasing to the research communitythe entire set of 15,935 pseudowords, i.e., forall WordNet polysemous nouns (http://lcl.uniroma1.it/pseudowords/).
This set ofpseudowords (together with the English Gigawordcorpus) can be used to generate a large pseudosense-tagged dataset containing ?1000 annotated sen-tences for every sense of all the pseudowords mod-eled after real ambiguous nouns in WordNet.
Theresulting dataset could be a good complement forMASC (Ide et al 2010) which, being human-created, can provide 1000 sense-annotated sentencesfor just a few words.We hope that the availability of this resource willenable large-scale experiments in tasks such as se-mantic role labeling, semantic parsing, and WordSense Disambiguation.
Specifically, as future work,we plan to utilize the generated pseudosense-taggeddataset to perform an in-depth study of differentWSD paradigms.
We also plan to extend our workto other part-of-speech tags.AcknowledgmentsThe authors gratefully acknowledgethe support of the ERC StartingGrant MultiJEDI No.
259234.ReferencesEneko Agirre and Aitor Soroa.
2009.
PersonalizingPageRank for word sense disambiguation.
In Proceed-ings of the 12th Conference of the European Chap-ter of the Association for Computational Linguistics,EACL ?09, pages 33?41, Athens, Greece.Eneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and WordNet-based approaches.
In Proceed-ings of Human Language Technologies: The 2009 An-nual Conference of the North American Chapter of theAssociation for Computational Linguistics, NAACL?09, pages 19?27, Boulder, Colorado.Michele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigua-tion.
In Proceedings of the 39th Annual Meeting onAssociation for Computational Linguistics, ACL ?01,pages 26?33, Toulouse, France.Shane Bergsma, Dekang Lin, and Randy Goebel.
2008.Discriminative learning of selectional preference fromunlabeled text.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,EMNLP ?08, pages 59?68, Honolulu, Hawaii.Stefan Bordag.
2006.
Word Sense Induction: Triplet-based clustering and automatic evaluation.
In Pro-ceedings of the 11th Conference on European chap-ter of the Association for Computational Linguistics,EACL ?06, pages 137?144, Trento, Italy.Nathanael Chambers and Dan Jurafsky.
2010.
Improv-ing the use of pseudo-words for evaluating selectionalpreferences.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,ACL ?10, pages 445?453, Uppsala, Sweden.Antonio Di Marco and Roberto Navigli.
2013.
Cluster-ing and diversifying Web search results with graph-based Word Sense Induction.
Computational Linguis-tics, 39(4).Katrin Erk.
2007.
A simple, similarity-based model forselectional preferences.
In Proceedings of the 45th1108Annual Meeting of the Association of ComputationalLinguistics, ACL ?07, pages 216?223, Prague, CzechRepublic.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.William Gale, Kenneth Church, and David Yarowsky.1992.
Work on statistical methods for Word SenseDisambiguation.
In Proceedings of the AAAI FallSymposium on Probabilistic Approaches to NaturalLanguage, pages 54?60, Menlo Park, CA.Tanja Gaustad.
2001.
Statistical corpus-based WordSense Disambiguation: Pseudowords vs real ambigu-ous words.
In Companion Volume to the Proceed-ings of the 39th Annual Meeting of the Associationfor Computational Linguistics Proceedings ot the Stu-dent Research Workshop, ACL/EACL ?01, pages 61?66, Toulouse, France.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.David Graff and Christopher Cieri.
2003.
English Giga-word, LDC2003T05.
In Linguistic Data Consortium,Philadelphia.Taher H. Haveliwala.
2002.
Topic-sensitive PageRank.In Proceedings of 11th International Conference onWorld Wide Web, WWW ?02, pages 517?526, Hon-olulu, Hawaii, USA.Thad Hughes and Daniel Ramage.
2007.
Lexical seman-tic relatedness with random graph walks.
In Proceed-ings of the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning, EMNLP-CoNLL?07, pages 581?589, Prague, Czech Republic.Nancy Ide, Collin F. Baker, Christiane Fellbaum, and Re-becca J. Passonneau.
2010.
The manually annotatedsub-corpus: A community resource for and by the peo-ple.
In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics (Short Pa-pers), pages 68?73, Uppsala, Sweden.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proceedings of the Fifteenth Interna-tional Conference on Machine Learning, ICML ?98,pages 296?304, Madison, USA.Zhimao Lu, Haifeng Wang, Jianmin Yao, Ting Liu, andSheng Li.
2006.
An equivalent pseudoword solutionto Chinese Word Sense Disambiguation.
In Proceed-ings of the 21st International Conference on Computa-tional Linguistics and the 44th annual meeting of theAssociation for Computational Linguistics, ACL ?06,pages 457?464, Sydney, Australia.Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-riff.
2004.
The Senseval-3 English lexical sampletask.
In Proceedings of Senseval-3: The Third Inter-national Workshop on the Evaluation of Systems forthe Semantic Analysis of Text, pages 25?28, Barcelona,Spain.Preslav I. Nakov and Marti A. Hearst.
2003.
Category-based pseudowords.
In Proceedings of the Confer-ence of the North American Chapter of the Associationof Computational Linguistics ?
short papers, HLT-NAACL ?03, pages 67?69, Edmonton, Canada.Roberto Navigli.
2009.
Word Sense Disambiguation: Asurvey.
ACM Computing Surveys, 41(2):1?69.Roberto Navigli.
2012.
A quick tour of word sensedisambiguation, induction and related approaches.
InProceedings of the 38th Conference on Current Trendsin Theory and Practice of Computer Science, SOF-SEM ?12, pages 115?129, Spindleruv Mlyn, CzechRepublic.Lubomir Otrusina and Pavel Smrz.
2010.
A new ap-proach to pseudoword generation.
In Proceedings ofthe International Conference on Language Resourcesand Evaluation, LREC?10, pages 1195?1199, Valletta,Malta.Hinrich Schu?tze.
1992.
Dimensions of meaning.In Supercomputing ?92: Proceedings of the 1992ACM/IEEE conference on Supercomputing, pages787?796, Minneapolis, Minnesota, USA.David Yarowsky.
1993.
One sense per collocation.In Proceedings of the 3rd DARPA Workshop on Hu-man Language Technology, pages 266?271, Princeton,New Jersey.Zhi Zhong and Hwee Tou Ng.
2010.
It makes sense:A wide-coverage Word Sense Disambiguation systemfor free text.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,ACL?10, pages 78?83, Uppsala, Sweden.1109
