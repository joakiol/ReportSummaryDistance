Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 270?278,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsThe Story of the Characters, the DNA and the Native LanguageMarius PopescuUniversity of BucharestDepartment of Computer ScienceAcademiei 14, Bucharest, Romaniapopescunmarius@gmail.comRadu Tudor IonescuUniversity of BucharestDepartment of Computer ScienceAcademiei 14, Bucharest, Romaniaraducu.ionescu@gmail.comAbstractThis paper presents our approach to the 2013Native Language Identification shared task,which is based on machine learning methodsthat work at the character level.
More pre-cisely, we used several string kernels and akernel based on Local Rank Distance (LRD).Actually, our best system was a kernel combi-nation of string kernel and LRD.
While stringkernels have been used before in text analysistasks, LRD is a distance measure designed towork on DNA sequences.
In this work, LRD isapplied with success in native language iden-tification.Finally, the Unibuc team ranked third in theclosed NLI Shared Task.
This result is moreimpressive if we consider that our approachis language independent and linguistic theoryneutral.1 IntroductionThis paper presents our approach to the shared taskon Native Language Identification, NLI 2013.
Weapproached this task with machine learning methodsthat work at the character level.
More precisely, wetreated texts just as sequences of symbols (strings)and used different string kernels in conjunction withdifferent kernel-based learning methods in a seriesof experiments to assess the best performance levelthat can be achieved.
Our aim was to investigate ifidentifying native language is possible with machinelearning methods that work at the character level.By disregarding features of natural language such aswords, phrases, or meaning, our approach has an im-portant advantage in that it is language independent.Using words is natural in text analysis tasks liketext categorization (by topic), authorship identifi-cation and plagiarism detection.
Perhaps surpris-ingly, recent results have proved that methods han-dling the text at character level can also be veryeffective in text analysis tasks (Lodhi et al 2002;Sanderson and Guenter, 2006; Popescu and Dinu,2007; Grozea et al 2009; Popescu, 2011; Popescuand Grozea, 2012).
In (Lodhi et al 2002) stringkernels were used for document categorization withvery good results.
Trying to explain why treatingdocuments as symbol sequences and using stringkernels led to such good results the authors sup-pose that: ?the [string] kernel is performing some-thing similar to stemming, hence providing seman-tic links between words that the word kernel mustview as distinct?.
String kernels were also suc-cessfully used in authorship identification (Sander-son and Guenter, 2006; Popescu and Dinu, 2007;Popescu and Grozea, 2012).
For example, the sys-tem described in (Popescu and Grozea, 2012) rankedfirst in most problems and overall in the PAN 2012Traditional Authorship Attribution tasks.
A possiblereason for the success of string kernels in authorshipidentification is given in (Popescu and Dinu, 2007):?the similarity of two strings as it is measured bystring kernels reflects the similarity of the two textsas it is given by the short words (2-5 characters)which usually are function words, but also takes intoaccount other morphemes like suffixes (?ing?
for ex-ample) which also can be good indicators of the au-thor?s style?.Even more interesting is the fact that two meth-ods, that are essentially the same, obtained very270good results for text categorization (by topic) (Lodhiet al 2002) and authorship identification (Popescuand Dinu, 2007).
Both are based on SVM and astring kernel of length 5.
How is this possible?Traditionally, the two tasks, text categorization (bytopic) and authorship identification are viewed asopposite.
When words are considered as features,for text categorization the (stemmed) content wordsare used (the stop words being eliminated), while forauthorship identification the function words (stopwords) are used as features, the others words (con-tent words) being eliminated.
Then, why did thesame string kernel (of length 5) work well in bothcases?
In our opinion the key factor is the kernel-based learning algorithm.
The string kernel im-plicitly embeds the texts in a high dimensional fea-ture space, in our case the space of all (sub)stringsof length 5.
The kernel-based learning algorithm(SVM or another kernel method), aided by regu-larization, implicitly assigns a weight to each fea-ture, thus selecting the features that are importantfor the discrimination task.
In this way, in thecase of text categorization the learning algorithm(SVM) enhances the features (substrings) represent-ing stems of content words, while in the case of au-thorship identification the same learning algorithmenhances the features (substrings) representing func-tion words.Using string kernels will make the correspond-ing learning method completely language indepen-dent, because the texts will be treated as sequencesof symbols (strings).
Methods working at the wordlevel or above very often restrict their feature spaceaccording to theoretical or empirical principles.
Forexample, they select only features that reflect var-ious types of spelling errors or only some type ofwords, such as function words, for example.
Thesefeatures prove to be very effective for specific tasks,but other, possibly good features, depending on theparticular task, may exist.
String kernels embed thetexts in a very large feature space (all substringsof length k) and leave it to the learning algorithm(SVM or others) to select important features for thespecific task, by highly weighting these features.A method that considers words as features can notbe language independent.
Even a method that usesonly function words as features is not completelylanguage independent because it needs a list of func-tion words (specific to a language) and a way to seg-ment a text into words which is not an easy task forsome languages, like Chinese.Character n-grams were already used in nativelanguage identification (Brooke and Hirst, 2012;Tetreault et al 2012).
The reported performancewhen only character n-grams were used as featureswas modest compared with other type of features.But, in the above mentioned works, the authors in-vestigated only the bigrams and trigrams and notlonger n-grams.
Particularly, we have obtained sim-ilar results with (Tetreault et al 2012) when usingcharacter bigrams, but we have achieved the bestperformance using a range of 5 to 8 n-grams (seesection 4.3).
We have used with success a similarapproach for the related task of identifying transla-tionese (Popescu, 2011).The first application of string kernel ideas came inthe field of text categorization, with the paper (Lodhiet al 2002), followed by applications in bioinfor-matics (Leslie et al 2002).
Computer science re-searchers have developed a wide variety of methodsthat can be applied with success in computationalbiology.
Such methods range from clustering tech-niques used to analyze the phylogenetic trees of dif-ferent organisms (Dinu and Sgarro, 2006; Dinu andIonescu, 2012b), to genetic algorithms used to findmotifs or common patterns in a set of given DNAsequences (Dinu and Ionescu, 2012a).
Most of thesemethods are based on a distance measure for strings,such as Hamming (Chimani et al 2011; Vezzi etal., 2012), edit (Shapira and Storer, 2003), Kendall-tau (Popov, 2007), or rank distance (Dinu, 2003).
Asimilar idea to character n-grams was introduced inthe early years of bioinformatics, where k-mers areused instead of single characters 1.
There are recentstudies that use k-mers for the phylogenetic analy-sis of organisms (Li et al 2004), or for sequencealignment (Melsted and Pritchard, 2011).
Analyz-ing DNA at substring level is also more suited froma biological point of view, because DNA substringsmay contain meaningful information.
For example,genes are encoded by a number close to 100 basepairs, or codons that encode the twenty standardamino acids are formed of 3-mers.
Local Rank Dis-1In biology, single DNA characters are also referred to asnucleotides or monomers.
Polymers are also known as k-mers.271tance (LRD) (Ionescu, 2013) has been recently pro-posed as an extension of rank distance.
LRD dropsthe annotation step of rank distance, and uses k-mersinstead of single characters.
The work (Ionescu,2013) shows that LRD is a distance function and thatit has very good results in phylogenetic analysis andDNA sequence comparison.
But, LRD can be ap-plied to any kind of string sequences, not only toDNA.
Thus, LRD was transformed into a kernel andused for native language identification.
Despite thefact it has no linguistic motivation, LRD gives sur-prisingly good results for this task.
Its performancelevel is lower than string kernel, but LRD can con-tribute to the improvement of string kernel when thetwo methods are combined.The paper is organized as follows.
In the nextsection, the kernel methods we used are briefly de-scribed.
Section 3 presents the string kernels andthe LRD, and shows how to transform LRD into akernel.
Section 4 presents details about the experi-ments.
It gives details about choosing the learningmethod, parameter tuning, combining kernels andresults of submitted systems.
Finally, conclusionsare given in section 5.2 Kernel Methods and String KernelsKernel-based learning algorithms work by embed-ding the data into a feature space (a Hilbert space),and searching for linear relations in that space.
Theembedding is performed implicitly, that is by speci-fying the inner product between each pair of pointsrather than by giving their coordinates explicitly.Given an input set X (the space of examples), andan embedding vector space F (feature space), let ?
:X ?
F be an embedding map called feature map.A kernel is a function k, such that for all x, z ?X , k(x, z) =< ?
(x), ?
(z) >, where < ?, ?
> de-notes the inner product in F .In the case of binary classification problems,kernel-based learning algorithms look for a discrim-inant function, a function that assigns +1 to exam-ples belonging to one class and ?1 to examples be-longing to the other class.
This function will be a lin-ear function in the space F , that means it will havethe form:f(x) = sign(< w,?
(x) > +b),for some weight vector w. The kernel can beexploited whenever the weight vector can be ex-pressed as a linear combination of the trainingpoints,n?i=1?i?
(xi), implying that f can be ex-pressed as follows:f(x) = sign(n?i=1?ik(xi, x) + b).Various kernel methods differ by the way in whichthey find the vector w (or equivalently the vector?).
Support Vector Machines (SVM) try to find thevector w that defines the hyperplane that maximallyseparates the images in F of the training examplesbelonging to the two classes.
Mathematically, SVMschoose the w and the b that satisfy the following op-timization criterion:minw,b1nn?i=1[1?
yi(< w,?
(xi) > +b)]+ + ?||w||2where yi is the label (+1/?1) of the training ex-ample xi, ?
a regularization parameter and [x]+ =max(x, 0).Kernel Ridge Regression (KRR) selects the vec-tor w that simultaneously has small empirical er-ror and small norm in Reproducing Kernel HilbertSpace generated by kernel k. The resulting mini-mization problem is:minw1nn?i=1(yi?
< w,?
(xi) >)2 + ?||w||2where again yi is the label (+1/?1) of the trainingexample xi, and ?
a regularization parameter.Details about SVM and KRR can be foundin (Taylor and Cristianini, 2004).
The important factis that the above optimization problems are solvedin such a way that the coordinates of the embeddedpoints are not needed, only their pairwise inner prod-ucts which in turn are given by the kernel functionk.3 String Kernels and Local Rank DistanceThe kernel function offers to the kernel methods thepower to naturally handle input data that are not inthe form of numerical vectors, for example strings.The kernel function captures the intuitive notion of272similarity between objects in a specific domain andcan be any function defined on the respective do-main that is symmetric and positive definite.
Forstrings, many such kernel functions exist with vari-ous applications in computational biology and com-putational linguistics (Taylor and Cristianini, 2004).3.1 String KernelsPerhaps one of the most natural ways to measure thesimilarity of two strings is to count how many sub-strings of length p the two strings have in common.This gives rise to the p-spectrum kernel.
Formally,for two strings over an alphabet ?, s, t ?
?
?, thep-spectrum kernel is defined as:kp(s, t) =?v?
?pnumv(s) ?
numv(t)where numv(s) is the number of occurrences ofstring v as a substring in s 2.
The feature map de-fined by this kernel associates to each string a vectorof dimension |?|p containing the histogram of fre-quencies of all its substrings of length p (p-grams).A variant of this kernel can be obtained if theembedding feature map is modified to associate toeach string a vector of dimension |?|p containingthe presence bits (instead of frequencies) of all itssubstrings of length p. Thus the character p-gramspresence bits kernel is obtained:k0/1p (s, t) =?v?
?pinv(s) ?
inv(t)where inv(s) is 1 if string v occurs as a substring ins and 0 otherwise.Normalized versions of these kernels ensure a faircomparison of strings of different lengths:k?p(s, t) =kp(s, t)?kp(s, s) ?
kp(t, t)k?0/1p (s, t) =k0/1p (s, t)?k0/1p (s, s) ?
k0/1p (t, t).Taking into account p-grams of different lengthand summing up the corresponding kernels, newkernels (called blended spectrum kernels) can be ob-tained.2Note that the notion of substring requires contiguity.
See(Taylor and Cristianini, 2004) for a discussion about the ambi-guity between the terms substring and subsequence across dif-ferent traditions: biology, computer science.3.2 Local Rank DistanceLocal Rank Distance is an extension of rank distancethat drops the annotation step and uses n-grams in-stead of single characters.
Thus, characters in onestring are simply matched with the nearest similarcharacters in the other string.
To compute the LRDbetween two strings, the idea is to sum up all the off-sets of similar n-grams between the two strings.
Forevery n-gram in one string, we search for a similarn-gram in the other string.
First, look for similar n-grams in the same position in both strings.
If thosen-grams are similar, sum up 0 since there is no offsetbetween them.
If the n-grams are not similar, startlooking around the initial n-gram position in the sec-ond string to find an n-gram similar to the one in thefirst string.
If a similar n-gram is found during thisprocess, sum up the offset between the two n-grams.The search goes on until a similar n-gram is found oruntil a maximum offset is reached.
LRD is formallydefined next.Definition 1 Let S1, S2 ?
??
be two strings withsymbols (n-grams) from the alphabet ?.
Local RankDistance between S1 and S2 is defined as:?LRD(S1, S2) = ?left + ?right=?xs?S1minxs?S2{|posS1(xs)?
posS2(xs)|,m}++?ys?S2minys?S1{|posS1(ys)?
posS2(ys)|,m},where xs and ys are occurrences of symbol s ?
?
instrings S1 and S2, posS(xs) represents the position(or the index) of the occurrence xs of symbol s ?
?in string S, and m ?
1 is the maximum offset.A string may contain multiple occurrences of asymbol s ?
?.
LRD matches each occurrence xsof symbol s ?
?
from a string, with the nearest oc-currence of symbol s in the other string.
A sym-bol can be defined either as a single character, oras a sequence of characters (n-grams).
Overlappingn-grams are also permitted in the computation ofLRD.
Notice that in order to be a symmetric distancemeasure, LRD must consider every n-gram in bothstrings.
The complexity of an algorithm to computeLRD can be reduced to O(l ?
m) using advancedstring searching algorithms, where l is the maximumlength of the two strings involved in the computationof LRD, and m is the maximum offset.273To understand how LRD actually works, considerexample 1 where LRD is computed between stringss1 and s2 using 1-grams (single characters).Example 1 Let s1 = CCBAADACB, s2 =DBACDCA, and m = 10 be the maximum offset.The LRD between s1 and s2 is given by:?LRD(s1, s2) = ?left + ?rightwhere the two sums ?left and ?right are computedas follows:?left =?xs?s1minxs?s2{|poss1(xs)?
poss2(xs)|, 10}= |1?
4|+ |2?
4|+ |3?
2|+ |4?
3|+ |5?
3|++ |6?
5|+ |7?
7|+ |8?
6|+ |9?
2| = 19?right =?ys?s2minys?s1{|poss1(ys)?
poss2(ys)|, 10}= |1?
6|+ |2?
3|+ |3?
4|+ |4?
2|+ |5?
6|++ |6?
8|+ |7?
7| = 12.In other words, ?left considers every symbolfrom s1, while ?right considers every symbol froms2.
Observe that ?LRD(s1, s2) = ?LRD(s2, s1).LRD measures the distance between two strings.Knowing the maximum offset (used to stop sim-ilar n-gram searching), the maximum LRD valuebetween two strings can be computed as the prod-uct between the maximum offset and the number ofpairs of compared n-grams.
Thus, LRD can be nor-malized to a value in the [0, 1] interval.
By normal-izing, LRD is transformed into a dissimilarity mea-sure.
LRD can be also used as a kernel, since kernelmethods are based on similarity.
The classical wayto transform a distance or dissimilarity measure intoa similarity measure is by using the Gaussian-likekernel (Taylor and Cristianini, 2004):k(s1, s2) = e?LRD(s1, s2)2?2where s1 and s2 are two strings.
The parameter?
is usually chosen to match the number of fea-tures (characters) so that values of k(s1, s2) are wellscaled.4 Experiments4.1 DatasetThe dataset for the NLI shared task is the TOEFL11corpus (Blanchard et al 2013).
This corpus con-tains 9900 examples for training, 1100 examples fordevelopment (or validation) and another 1100 ex-amples for testing.
Each example is an essay writ-ten in English by a person that is a non-native En-glish speaker.
The people that produced the essayshave one of the following native languages: German,French, Spanish, Italian, Chinese, Korean, Japanese,Turkish, Arabic, Telugu, Hindi.
For more detailssee (Blanchard et al 2013).We participated only in the closed NLI sharedtask, where the goal of the task is to predict thenative language of testing examples, only by us-ing the training and development data.
In our ap-proach, documents or essays from this corpus aretreated as strings.
Thus, when we refer to stringsthroughout this paper, we really mean documentsor essays.
Because we work at the character level,we didn?t need to split the texts into words, or to doany NLP-specific preprocessing.
The only editingdone to the texts was the replacing of sequences ofconsecutive space characters (space, tab, new line,etc.)
with a single space character.
This normaliza-tion was needed in order to not artificially increaseor decrease the similarity between texts as a resultof different spacing.
Also all uppercase letters wereconverted to the corresponding lowercase ones.
Wedidn?t use the additional information from promptsand English language proficiency level.4.2 Choosing the Learning MethodSVM and KRR produce binary classifiers and nativelanguage identification is a multi-class classificationproblem.
There are a lot of approaches for com-bining binary classifiers to solve multi-class prob-lems.
Typically, the multiclass problem is brokendown into multiple binary classification problemsusing common decomposing schemes such as: one-versus-all (OVA) and one-versus-one (OVO).
Thereare also kernel methods that directly take into ac-count the multiclass nature of the problem such asthe kernel partial least squares regression (KPLS).We conducted a series of preliminary experimentsin order to select the learning method.
In these ex-2740 1 2 3 4 5 6 7 8 9 10 1101020304050607090100n?gramsaccuracyn?grams frequenciesn?grams presenceFigure 1: 10-fold cross-validation accuracy on the train set for different n-grams.Method AccuracyOVO SVM 72.72%OVA SVM 74.94%OVO KRR 73.99%OVA KRR 77.74%KPLS 74.99%Table 1: Accuracy rates using 10-fold cross-validation onthe train set for different kernel methods with k?5 kernel.periments we fixed the kernel to the p-spectrum nor-malized kernel of length 5 (k?5) and plugged it inthe following learning methods: OVO SVM, OVASVM, OVO KRR, OVA KRR and KPLS.
Note thatin this stage we were interested only in selecting thelearning method and not in finding the best kernel.We chose the k?5 because it was reported to workwell in the case of the related task of identifyingtranslationese (Popescu, 2011).We carried out a 10-fold cross-validation on thetraining set and the result obtained (with the best pa-rameters setting) are shown in Table 1.The results show that for native language identi-fication the one-vs-all scheme performs better thanthe one-versus-one scheme.
The same fact was re-ported in (Brooke and Hirst, 2012).
See also (Rifkinand Klautau, 2004) for arguments in favor of one-vs-all.
The best result was obtained by one-vs-allKernel Ridge Regression and we selected it as ourlearning method.4.3 Parameter Tuning for String KernelTo establish the type of kernel, (blended) p-spectrumkernel or (blended) p-grams presence bits kernel,and the length(s) of of n-grams that must be used,we performed another set of experiments.
For bothp-spectrum normalized kernel and p-grams presencebits normalized kernel, and for each value of p from2 to 10, we carried out a 10-fold cross-validation onthe train set.
The results are summarized in Figure 1.As can be seen, both curves have similar shapes,both achieve their maximum at 8, but the accuracy ofthe p-grams presence bits normalized kernel is gen-erally better than the accuracy of the p-spectrum nor-malized kernel.
It seem that in native language iden-tification the information provided by the presenceof an n-gram is more important than the the infor-mation provided by the frequency of occurrence ofthe respective n-gram.
This phenomenon was alsonoticed in the context of sexual predator identifica-tion (Popescu and Grozea, 2012).We also experimented with different blended ker-nels to see if combining n-grams of different lengthscan improve the accuracy.
The best result was ob-tained when all the n-grams with the length in therange 5-8 were used, that is the 5-8-grams presencebits normalized kernel (k?0/15?8).
The 10-fold cross-validation accuracy on the train set for this kernel275Method AccuracyKRR + KLRD6 42.1%KRR + KnLRD4 70.8%KRR + KnLRD6 74.4%KRR + KnLRD8 74.8%Table 2: Accuracy rates, using 10-fold cross-validationon the training set, of LRD with different n-grams, withand without normalization.
Normalized LRD is muchbetter.was 80.94% and was obtained for the KRR param-eter ?
set to 10?5.
The authors of (Bykh and Meur-ers, 2012) also obtained better results using n-gramswith the length in a range than using n-grams of afixed length.4.4 Parameter Tuning for LRD KernelParameter tuning for LRD kernel (KLRD) was alsodone by using 10-fold cross validation on the train-ing data.
First, we observed that the KRR based onLRD works much better with the normalized versionof LRD (KnLRD).
Another concern was to choosethe right length of n-grams.
We tested with severaln-grams such as 4-grams, 6-grams and 8-grams thatare near the mean English word length of 5-6 let-ters.
The tests show that the LRD kernels based on6-grams (KnLRD6) and 8-grams (KnLRD8) give thebest results.
In the end, the LRD kernels based on 6-grams and 8-grams are combined to obtain even bet-ter results (see section 4.5).
Finally, the maximumoffset parameter m involved in the computation ofLRD was chosen so that it generates search windowsize close to the average number of letters per docu-ment from the TOEFL 11 set.
There are 1802 char-acters per document on average, and m was chosento be 700.
This parameter was also chosen with re-spect to the computational time of LRD, which isproportional to the parameter value.
Table 2 showsthe results of the LRD kernel with different parame-ters cross validated on the training set.
For KnLRD,the ?
parameter of the Gaussian-like kernel was setto 1.
The reported accuracy rates were obtained withthe KRR parameter ?
set to 10?5.Regarding the length of strings, we observed thatLRD is affected by the variation of string lengths.When comparing two documents with LRD, wetried to cut the longer one to match the length ofMethod AccuracyKRR + KnLRD6+8 75.4%KRR + k?0/15?8 + KnLRD6+8 81.6%KRR + (k?0/1 +KnLRD)6+8 80.9%Table 3: Accuracy rates of different kernel combinationsusing 10-fold cross-validation on the training set.the shorter.
This made the accuracy even worse.
Itseems that the parts cut out from longer documentscontain valuable information for LRD.
We decidedto use the entire strings for LRD, despite the noisebrought by the variation of string lengths.4.5 Combining KernelsTo improve results, we thought of combining thekernels in different ways.
First, notice that theblended string kernels presented in section 4.3 areessentially a sum of the string kernels with differentn-grams.
This combination improves the accuracy,being more stable and robust.
In the same manner,the LRD kernels based on 6-grams and 8-grams, re-spectively, were summed up to obtain the kernel de-noted by KnLRD6+8 .
Indeed, the KnLRD6+8 kernelworks better (see Table 3).There are other options to combine the string ker-nels with LRD kernels, besides summing them up.One option is by kernel alignment (Cristianini et al2001).
Instead of simply summing kernels, kernelalignment assigns weights for each to the two ker-nels based on how well they are aligned with theideal kernel Y Y ?
obtained from labels.
Thus, the 5-8-grams presence bits normalized kernel (k?0/15?8) wascombined with the LRD kernel based on sum of 6,8-grams (KnLRD6+8), by kernel alignment.
From ourexperiments, kernel alignment worked slightly bet-ter than the sum of the two kernels.
This also sug-gests that kernels can be combined only by kernelalignment.
The string kernel of length 6 was alignedwith the LRD kernel based on 6-grams.
In the sameway, the string kernel of length 8 was aligned withthe LRD kernel based on 8-grams.
The two kernelsobtained by alignment are combined together, againby kernel alignment, to obtain the kernel denoted by(k?0/1 +KnLRD)6+8.
The results of all kernel com-binations are presented in Table 3.
The reported ac-curacy rates were obtained with the KRR parameter276Method Submission CV Tr.
Dev.
CV Tr.+Dev.
TestKRR + k?0/15?8 Unibuc-1 80.9% 85.4% 82.5% 82.0%KRR + KnLRD6+8 Unibuc-2 75.4% 76.3% 75.7% 75.8%KRR + k?0/15?8 + KnLRD6+8 Unibuc-3 81.6% 85.7% 82.6% 82.5%KRR + (k?0/1 +KnLRD)6+8 Unibuc-4 80.9% 85.6% 82.0% 81.4%KRR + k?0/15?8 + KnLRD6+8 + heuristic Unibuc-5 - - - 82.7%Table 4: Accuracy rates of submitted systems on different evaluation sets.
The Unibuc team ranked third in the closedNLI Shared Task with the kernel combination improved by the heuristic to level the predicted class distribution.?
set to 10?5.4.6 Results and DiscussionFor the closed NLI Shared Task we submitted thetwo main systems, namely the 5-8-grams presencebits normalized kernel and the LRD kernel based onsum of 6,8-grams, separately.
Another two submis-sions are the kernel combinations discussed in sec-tion 4.5.
These four systems were tested using sev-eral evaluation procedures, with results shown in Ta-ble 4.
First, they were tested using 10-fold cross val-idation on the training set.
Next, the systems weretested on the development set.
In this case, the sys-tems were trained on the entire training corpus.
An-other 10-fold cross validation procedure was doneon the corpus obtained by combining the trainingand the development sets.
The folds were providedby the organizers.
Finally, the results of our systemson the NLI Shared Task test set are given in the lastcolumn of Table 4.
For testing, the systems weretrained on the entire training and development set,with the KRR parameter ?
set to 2 ?
10?5.We didn?t expect KnLRD6+8 kernel to performvery well on the test set.
This system was submittedjust to be compared with systems submitted by otherparticipants.
Considering that LRD is inspired frombiology and that it has no ground in computationallinguistics, it performed very well, by standing in thetop half of the ranking of all submitted systems.The kernel obtained by aligning the k?0/15?8 andKnLRD6+8 kernels gives the best results, no matterthe evaluation procedure.
It is followed closely bythe other two submitted systems.We thought of exploiting the distribution of thetesting set in our last submitted system.
We knewthat there should be exactly 100 examples per classfor testing.
We took the kernel obtained by com-bining the k?0/15?8 and KnLRD6+8 kernels, and tried toadjust its output to level the predicted class distribu-tion.
We took all the classes with more than 100 ex-amples and ranked the examples by their confidencescore (returned by regression) to be part of the pre-dicted class.
The examples ranked below 100 werechosen to be redistributed to the classes that had lessthan 100 examples per class.
Examples were redis-tributed only if their second most confident class hadless than 100 examples.
This heuristic improved theresults on the test set by 0.2%, enough to put us onthird place in the closed NLI Shared Task.5 ConclusionIn this paper, we have presented our approach tothe 2013 NLI Shared Task.
What makes our sys-tem stand out is that it works at the character level,making the approach completely language indepen-dent and linguistic theory neutral.
The results ob-tained were very good.
A standard approach basedon string kernels, that proved to work well in manytext analysis tasks, obtained an accuracy of 82% ontest data with a difference of only 1.6% between itand the top performing system.
A second systembased on a new kernelKLRD, inspired from biologywith no ground in computational linguistics, per-formed also unexpectedly well, by standing in thetop half of the ranking of all submitted systems.
Thecombination of the two kernels obtained an accuracyof 82.5% making it to the top ten, while an heuristicimprovement of this combination ranked third withan accuracy of 82.7%.
Obviously, an explanationfor these results was needed.
It will be adressed infuture work.277ReferencesDaniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: ACorpus of Non-Native English.
Technical report, Ed-ucational Testing Service.Julian Brooke and Graeme Hirst.
2012.
Robust, Lexical-ized Native Language Identification.
In Proceedingsof COLING 2012, pages 391?408, Mumbai, India, De-cember.
The COLING 2012 Organizing Committee.Serhiy Bykh and Detmar Meurers.
2012.
Native Lan-guage Identification using Recurring n-grams ?
In-vestigating Abstraction and Domain Dependence.
InProceedings of COLING 2012, pages 425?440, Mum-bai, India, December.
The COLING 2012 OrganizingCommittee.Markus Chimani, Matthias Woste, and Sebastian Bocker.2011.
A Closer Look at the Closest String and ClosestSubstring Problem.
Proceedings of ALENEX, pages13?24.Nello Cristianini, John Shawe-Taylor, Andre?
Elisseeff,and Jaz S. Kandola.
2001.
On kernel-target aln-ment.
In Thomas G. Dietterich, Suzanna Becker, andZoubin Ghahramani, editors, NIPS, pages 367?373.MIT Press.Liviu P. Dinu and Radu Tudor Ionescu.
2012a.
An Ef-ficient Rank Based Approach for Closest String andClosest Substring.
PLoS ONE, 7(6):e37576, 06.Liviu P. Dinu and Radu Tudor Ionescu.
2012b.
Clus-tering based on Rank Distance with Applications onDNA.
Proceedings of ICONIP, 7667:722?729.Liviu P. Dinu and Andrea Sgarro.
2006.
A Low-complexity Distance for DNA Strings.
FundamentaInformaticae, 73(3):361?372.Liviu P. Dinu.
2003.
On the classification and aggrega-tion of hierarchies with different constitutive elements.Fundamenta Informaticae, 55(1):39?50.C.
Grozea, C. Gehl, and M. Popescu.
2009.
EN-COPLOT: Pairwise Sequence Matching in LinearTime Applied to Plagiarism Detection.
In 3rdPAN WORKSHOP.
UNCOVERING PLAGIARISM,AUTHORSHIP AND SOCIAL SOFTWARE MISUSE,page 10.Radu Tudor Ionescu.
2013.
Local Rank Distance and itsApplications on DNA.
Submitted to PKDD.Christina S. Leslie, Eleazar Eskin, and William StaffordNoble.
2002.
The spectrum kernel: A string kernelfor svm protein classification.
In Pacific Symposiumon Biocomputing, pages 566?575.Ming Li, Xin Chen, Xin Li, Bin Ma, and Paul M. B. Vi-tanyi.
2004.
The similarity metric.
IEEE Transac-tions on Information Theory, 50(12):3250?3264.Huma Lodhi, Craig Saunders, John Shawe-Taylor, NelloCristianini, and Christopher J. C. H. Watkins.
2002.Text classification using string kernels.
Journal of Ma-chine Learning Research, 2:419?444.Pall Melsted and Jonathan Pritchard.
2011.
Efficientcounting of k-mers in DNA sequences using a bloomfilter.
BMC Bioinformatics, 12(1):333.Marius Popescu and Liviu P. Dinu.
2007.
Kernel meth-ods and string kernels for authorship identification:The federalist papers case.
In Proceedings of the In-ternational Conference on Recent Advances in Natu-ral Language Processing (RANLP-07), Borovets, Bul-garia, September.Marius Popescu and Cristian Grozea.
2012.
Ker-nel methods and string kernels for authorship analy-sis.
In Pamela Forner, Jussi Karlgren, and ChristaWomser-Hacker, editors, CLEF (Online WorkingNotes/Labs/Workshop).Marius Popescu.
2011.
Studying translationese at thecharacter level.
In Proceedings of the InternationalConference Recent Advances in Natural LanguageProcessing 2011, pages 634?639, Hissar, Bulgaria,September.
RANLP 2011 Organising Committee.V.
Yu.
Popov.
2007.
Multiple genome rearrangement byswaps and by element duplications.
Theoretical Com-puter Science, 385(1-3):115?126.Ryan Rifkin and Aldebaro Klautau.
2004.
In defense ofone-vs-all classification.
Journal of Machine LearningResearch, 5(January):101?141.Conrad Sanderson and Simon Guenter.
2006.
Short textauthorship attribution via sequence kernels, markovchains and author unmasking: An investigation.
InProceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 482?491, Sydney, Australia, July.
Association for Compu-tational Linguistics.Dana Shapira and James A. Storer.
2003.
Large Edit Dis-tance with Multiple Block Operations.
Proceedings ofSPIRE, 2857:369?377.J.
S. Taylor and N. Cristianini.
2004.
Kernel Methods forPattern Analysis.
Cambridge University Press, NewYork, NY, USA.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native Tongues, Lost andFound: Resources and Empirical Evaluations in Na-tive Language Identification.
In Proceedings of COL-ING 2012, pages 2585?2602, Mumbai, India, Decem-ber.
The COLING 2012 Organizing Committee.Francesco Vezzi, Cristian Del Fabbro, Alexandru I.Tomescu, and Alberto Policriti.
2012. rNA: a fast andaccurate short reads numerical aligner.
Bioinformat-ics, 28(1):123?124.278
