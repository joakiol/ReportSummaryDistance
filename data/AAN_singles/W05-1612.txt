Explorations in Sentence Fusion?Erwin Marsi and Emiel KrahmerCommunication and CognitionFaculty of Arts, Tilburg UniversityP.O.Box 90153, NL-5000 LE Tilburg, The Netherlands{e.c.marsi, e.j.krahmer}@uvt.nlAbstractSentence fusion is a text-to-text (revision-like) gen-eration task which takes related sentences as inputand merges these into a single output sentence.
Inthis paper we describe our ongoing work on de-veloping a sentence fusion module for Dutch.
Wepropose a generalized version of alignment whichnot only indicates which words and phrases shouldbe aligned but also labels these in terms of a smallset of primitive semantic relations, indicating howwords and phrases from the two input sentences re-late to each other.
It is shown that human label-ers can perform this task with a high agreement (F-score of .95).
We then describe and evaluate ouradaptation of an existing automatic alignment al-gorithm, and use the resulting alignments, plus thesemantic labels, in a generalized fusion and gen-eration algorithm.
A small-scale evaluation studyreveals that most of the resulting sentences are ad-equate to good.1 IntroductionTraditionally, Natural Language Generation (NLG) is definedas the automatic production of ?meaningful texts in (...) hu-man language from some underlying non-linguistic represen-tation of information?
[Reiter and Dale, 2000, xvii].
Re-cently, there is an increased interest in NLG applicationsthat produce meaningful text from meaningful text rather thanfrom abstract meaning representations.
Such applicationsare sometimes referred to as text-to-text generation applica-tions (e.g., [Chandrasekar and Bangalore, 1997], [Knight andMarcu, 2002], [Lapata, 2003]), and may be likened to ear-lier revision-based generation strategies, e.g.
[Robin, 1994][Callaway and Lester, 1997].
Text-to-text generation is oftenmotivated from practical applications such as summarization,sentence simplification, and sentence compression.
One rea-son for the interest in such generation systems is the possi-bility to automatically learn text-to-text generation strategiesfrom corpora of parallel text.
?This work was carried out within the IMIX-IMOGEN (Inter-active Multimodal Output Generation) project, sponsored by theNetherlands Organization of Scientific Research (NWO).In this paper, we take a closer look at sentence fusion[Barzilay, 2003][Barzilay et al, 1999], one of the interestingvariants in text-to-text generation.
A sentence fusion moduletakes related sentences as input, and generates a single sen-tence summarizing the input sentences.
The general strategydescribed in [Barzilay, 2003] is to first align the dependencystructures of the two input sentences to find the common in-formation in both sentences.
On the basis of this alignment,the common information is framed into an fusion tree (i.e.,capturing the shared information), which is subsequently re-alized in natural language by generating all traversals of thefusion tree and scoring their probability using an n-gram lan-guage model.
Of the sentences thus generated the one withthe lowest (length normalized) entropy is selected.Barzilay and co-workers apply sentence fusion in the con-text of multi-document summarization, where the input sen-tences typically come from multiple documents describingthe same event, but sentence fusion seems to be useful forother applications as well.
In question-answering, for in-stance, sentence fusion could be used to generate more com-plete answers.
Many current QA systems use various parallelanswer-finding strategies, each of which may produce an N-best list of answers (e.g., [Maybury, 2004]) In response to aquestion like ?What causes RSI??
one potential answer sen-tence could be:RSI can be caused by repeating the same sequenceof movements many times an hour or day.And another might be:RSI is generally caused by a mixture of poor er-gonomics, stress and poor posture.These two incomplete answers might be fused into a morecomplete answer such as:RSI can be caused by a mixture of poor er-gonomics, stress, poor posture and by repeating thesame sequence of movements many times an houror day.The same process of sentence fusion can of course be appliedto the whole list of N-best answers in order to derive a morespecific, or even the most specific, answer, akin to taking theunion of a number of sets.
Likewise, we can rely on sentencefusion to derive a more general answer, or even the most gen-eral one (cf.
intersection), in the hope that this will filter outirrelevant parts of the answer.Arguably, such applications call for a generalized versionof sentence fusion, which may have consequences for the var-ious components (alignment, fusion and generation) of thesentence fusion pipeline.
At the alignment level, we wouldlike to have a better understanding of how words and phrasesin the input sentences relate to each other.
Rather than a bi-nary choice (align or not), one might want to distinguish morefine-grained relations such as overlap (if two phrases sharesome but not all of their content), paraphrases (if two phrasesexpress the same information in different ways), entailments(if one phrase entails the other, but not vice versa), etc.
Suchan alignment strategy would be especially useful for applica-tions such as question answering and information extraction,where it is often important to know whether two sentencesare paraphrases or stand in an entailment relation [Dagan andGlickman, 2004].
In the fusion module, we are interested inthe possibilities to generate various kinds of fusions depend-ing on the relations between the respective sentences, e.g., se-lecting the more specific or the more general phrase depend-ing on whether the fusion tree is an intersection or a unionone.
Finally, the generation may be more complicated in thegeneralized version, and it is an interesting question whetherthe use of language models is equally suitable for differentkinds of fusion.In this paper, we will explore some of these issues re-lated to a generalized version of sentence fusion.
We startwith the basic question whether it is possible at all to reli-ably align sentences, including different potential relationsbetween words and phrases (section 2).
We then present ourongoing work on sentence fusion, describing the current sta-tus and performance of the alignment algorithm (section 3),as well as the fusion and generation components (section 4).We end with discussion and description of future plans in sec-tion 5.2 Data collection and Annotation2.1 General approachAlignment has become standard practice in data-driven ap-proaches to machine translation (e.g.
[Och and Ney, 2000]).Initially work focused on word-based alignment, but more re-cent research also addresses alignment at the higher levels(substrings, syntactic phrases or trees), e.g.,[Gildea, 2003].The latter approach seems most suitable for current purposes,where we want to express that a sequence of words in onesentence is related to a non-identical sequence of words inanother sentence (a paraphrase, for instance).
However, ifwe allow alignment of arbitrary substrings of two sentences,then the number of possible alignments grows exponentiallyto the number of tokens in the sentences, and the process ofalignment ?
either manually or automatically ?
may becomeinfeasible.
An alternative, which seems to occupy the middleground between word alignment on the one hand and align-ment of arbitrary substrings on the other, is to align syntac-tic analyses.
Here, following [Barzilay, 2003], we will alignsentences at the level of dependency structures.
Unlike to[Barzilay, 2003], we are interested in a number of differentalignment relations between sentences, and pay special atten-tion to the feasibility of this alignment task.verb:hebbenverb:hebbenhd/vcpron:ikhd/suadv:zohd/mod hd/sunoun:contacthd/obj1prep:methd/pcprep:in de loop vanhd/moddet:veelhd/detadv:heelhd/modnoun:persoonhd/obj1adj:serieushd/moddet:veelhd/detadv:heelhd/modnoun:levenhd/obj1det:mijnhd/detFigure 1: Example dependency structure for the sentence Zoheb ik in the loop van mijn leven heel veel contacten gehadmet heel veel serieuze personen.
(lit.
?Thus have I in thecourse of my life very many contacts had with very manyserious persons?
).2.2 CorpusFor evaluation and parameter estimation we have developeda parallel monolingual corpus consisting of two differentDutch translations of the French book ?Le petit prince?
(thelittle prince) by Antoine de Saint-Exupe?ry (published 1943),one by Laetitia de Beaufort-van Hamel (1966) and one byErnst Altena (2000).
The texts were automatically tokenizedand split into sentences, after which errors were manuallycorrected.
Corresponding sentences from both translationswere manually aligned; in most cases this was a one-to-onemapping but occasionally a single sentence in one versionmapped onto two sentences in the other: Next, the Alpinoparser for Dutch (e.g., [Bouma et al, 2001]) was used forpart-of-speech tagging and lemmatizing all words, and forassigning a dependency analysis to all sentences.
The POSlabels indicate the major word class (e.g.
verb, noun, pron,and adv).
The dependency relations hold between tokensand are the same as used in the Spoken Dutch Corpus (seee.g., [van der Wouden et al, 2002]).
These include depen-dencies such as head/subject, head/modifier and coordina-tion/conjunction.
See Figure 1 for an example.
If a full parsecould not be obtained, Alpino produced partial analyses col-lected under a single root node.
Errors in lemmatization, POStagging, and syntactic dependency parsing were not subject tomanual correction.2.3 Task definitionA dependency analysis of a sentence S yields a labeled di-rected graph D = ?V,E?, where V (vertices) are the nodes,and E (edges) are the dependency relations.
For each nodev in the dependency structure for a sentence S, we defineSTR(v) as the substring of all tokens under v (i.e., the com-position of the tokens of all nodes reachable from v).
Forexample, the string associated with node persoon in Figure 1is heel veel serieuze personen (?very many serious persons?
).An alignment between sentences S and S?
pairs nodes fromthe dependency graphs for both sentences.
Aligning node vfrom the dependency graph D of sentence S with node v?from the graph D?
of S?
indicates that there is a relation be-tween STR(v) and STR(v?
), i.e., between the respective sub-strings associated with v and v?.
We distinguish five potential,mutually exclusive, relations between nodes (with illustrativeexamples):1. v equals v?
iff STR(v) and STR(v?)
are literally identical(abstracting from case and word order)Example: ?a small and a large boa-constrictor?
equals?a large and a small boa-constrictor?;2.
v restates v?
iff STR(v) is a paraphrase of STR(v?)
(sameinformation content but different wording),Example: ?a drawing of a boa-constrictor snake?
re-states ?a drawing of a boa-constrictor?;3.
v specifies v?
iff STR(v) is more specific than STR(v?
),Example: ?the planet B 612?
specifies ?the planet?;4.
v generalizes v?
iff STR(v?)
is more specific thanSTR(v),Example: ?the planet?
generalizes ?the planet B 612?;5.
v intersects v?
iff STR(v) and STR(v?)
share some in-formational content, but also each express some piece ofinformation not expressed in the other,Example: ?Jupiter and Mars?
intersects ?Mars andVenus?Note that there is an intuitive relation with entailment here:both equals and restates can be understood as mutual entail-ment (i.e., if the root nodes of the analyses corresponding Sand S?
stand in an equal or restate relation, S entails S?
andS?
entails S), if S specifies S?
then S also entails S?
and if Sgeneralizes S?
then S is entailed by S?.An alignment between S and S?
can now formally bedefined on the basis of the respective dependency graphsD = ?V,E?
and D?
= ?V ?, E??
as a graph A = ?VA, EA?,such thatEA = {?v, l, v??
| v ?
V & v?
?
V ?
& l(STR(v), STR(v?
))},where l is one of the five relations defined above.
The nodesof A are those nodes from D en D?
which are aligned, for-mally defined asVA = {v | ?v?
?l?v, l, v??
?
EA}?{v?
| ?v?l?v, l, v??
?
EA}A complete example alignment can be found in the Appendix,Figure 3.
(A1, A2) (A1?
, A2?)
(Ac, A1?)
(Ac, A2?
)#real: 322 323 322 322#pred: 312 321 323 321#correct: 293 315 317 318precision: .94 .98 .98 .99recall: .91 .98 .98 .99F-score: .92 .98 .98 .99Table 1: Interannotator agreement with respect to align-ment between annotators 1 and 2 before (A1, A2) and after(A1?
, A2?)
revision , and between the consensus and annota-tor 1 (Ac, A1?)
and annotator 2 (Ac, A2?)
respectively.2.4 Alignment toolFor creating manual alignments, we developed a special-purpose annotation tool called Gadget (?Graphical Aligner ofDependency Graphs and Equivalent Tokens?).
It shows, sideby side, two sentences, as well as their respective dependencygraphs.
When the user clicks on a node v in the graph, the cor-responding string (STR(v)) is shown at the bottom.
The toolenables the user to manually construct an alignment graph onthe basis of the respective dependency graphs.
This is doneby focusing on a node in the structure for one sentence, andthen selecting a corresponding node (if possible) in the otherstructure, after which the user can select the relevant align-ment relation.
The tool offers additional support for foldingparts of the graphs, highlighting unaligned nodes and hidingdependency relation labels.
See Figure 4 in the Appendix fora screen shot of Gadget.2.5 ResultsAll text material was aligned by the two authors.
They starteddoing the first ten sentences of chapter one together in orderto get a feel for the task.
They continued with the remainingsentences from chapter one individually.
The total numberof nodes in the two translations of the chapter was 445 and399 respectively.
Inter-annotator agreement was calculatedfor two aspects: alignment and relation labeling.
With respectto alignment, we calculated the precision, recall and F-score(with ?
= 1) on aligned node pairs as follows:precision(Areal, Apred) = | Areal ?Apred || Apred | (1)recall(Areal, Apred) = | Areal ?Apred || Areal | (2)F -score = 2?
precision?
recallprecision+ recall (3)where Areal is the set of all real alignments (the reference orgolden standard), Apred is the set of all predicted alignments,and Apred?Areal is the set al correctly predicted alignments.For the purpose of calculating inter-annotator agreement, oneof the annotations (A1) was considered the ?real?
alignment,the other (A2) the ?predicted?.
The results are summarized inTable 1 in column (A1, A2).Next, both annotators discussed the differences in align-ment, and corrected mistaken or forgotten alignments.
Thisimproved their agreement as shown in column (A1?
, A2?).
In(A1, A2) (A1?
, A2?)
(Ac, A1?)
(Ac, A2?
)precision: .86 .96 .98 .97recall: .86 .95 .97 .97F-score: .85 .95 .97 .97?
: .77 .92 .96 .96Table 2: Inter-annotator agreement with respect to alignmentrelation labeling between annotators 1 and 2 before (A1, A2)and after (A1?
, A2?)
revision , and between the consensus andannotator 1 (Ac, A1?)
and annotator 2 (Ac, A2?)
respectively.addition, they agreed on a single consensus annotation (Ac).The last two columns of Table 1 show the results of evalu-ating each of the revised annotations against this consensusannotation.
The F-score of .96 can therefore be regarded asthe upper bound on the alignment task.In a similar way, the agreement was calculated for the taskof labeling the alignment relations.
Results are shown in Ta-ble 2, where the measures are weighted precision, recall andF-score.
For instance, the precision is the weighted sum ofthe separate precision scores for each of the five relations.The table also shows the ?-score, which is another commonlyused measure for inter-annotator agreement [Carletta, 1996].Again, the F-score of .97 can be regarded as the upper boundon the relation labeling task.We think these numbers indicate that the labeled alignmenttask is well defined and can be accomplished with a high levelof inter-annotator agreement.3 Automatic alignmentIn this section, we describe the alignment algorithm that weuse (section 3.1), and evaluate its performance (section 3.2).3.1 Tree alignment algorithmThe tree alignment algorithm is based on [Meyers et al,1996], and similar to that used in [Barzilay, 2003].
It cal-culates the match between each node in dependency tree Dagainst each node in dependency tree D?.
The score for eachpair of nodes only depends on the similarity of the wordsassociated with the nodes and, recursively, on the scores ofthe best matching pairs of their descendants.
For an efficientimplementation, dynamic programming is used to build up ascore matrix, which guarantees that each score will be calcu-lated only once.Given two dependency trees D and D?, the algorithmbuilds up a score function S(v, v?)
for matching each nodev in D against each node v?
in D?, which is stored in a ma-trix M .
The value S(v, v?)
is the score for the best matchbetween the two subtrees rooted at v in D and at v?
in D?.When a value for S(v, v?)
is required, and is not yet in thematrix, it is recursively computed by the following formula:S(v, v?)
= max??
?TREEMATCH(v, v?
)maxi=1,...,n S(vi, v?
)maxj=1,...,m S(v, v?j)(4)where v1, .
.
.
, vn denote the children of v and v?1, .
.
.
, v?m de-note the children of v?.
The three terms correspond to thethree ways that nodes can be aligned: (1) v can be directlyaligned to v?
; (2) any of the children of v can be aligned to v?
;(3) v can be aligned to any of the children of v?.
Notice thatthe last two options imply skipping one or more edges, andleaving one or more nodes unaligned.1The function TREEMATCH(v, v?)
is a measure of how wellthe subtrees rooted at v and v?
match:TREEMATCH(v, v?)
= NODEMATCH(v, v?)
+maxp ?
P(v,v?)??
?
(i,j) ?
p(RELMATCH(?
?v i,?
?v ?j) + S(vi, v?j))?
?Here ?
?v i denotes the dependency relation from v to vi.P(v, v?)
is the set of all possible pairings of the n childrenof v against the m children of v?, which is the power set of{1, .
.
.
, n} ?
{1, .
.
.
,m}.
The summation in (5) ranges overall pairs, denoted by (i, j), which appear in a given pairingp ?
P(v, v?).
Maximizing this summation thus amounts tofinding the optimal alignment of children of v to children ofv?.NODEMATCH(v, v?)
?
0 is a measure of how well thelabel of node v matches the label of v?.RELMATCH(?
?v i,?
?v ?j) ?
0 is a measure for how well thedependency relation between node v and its child vi matchesthat of the dependency relation between node v?
and its childvj .Since the dependency graphs delivered by the Alpinoparser were usually not trees, they required some modifica-tion in order to be suitable input for the tree alignment al-gorithm.
We first determined a root node, which is definedas a node from which all other nodes in the graph can bereached.
In the rare case of multiple root nodes, an arbi-trary one was chosen.
Starting from this root node, any cyclicedges were temporarily removed during a depth-first traver-sal of the graph.
The resulting directed acyclic graphs maystill have some amount of structure sharing, but this poses noproblem for the algorithm.3.2 Evaluation of automatic alignmentWe evaluated the automatic alignment of nodes, abstractingfrom relation labels, as we have no algorithm for automaticlabeling of these relations yet.
The baseline is achieved byaligning those nodes with stand in an equals relation to eachother, i.e., a node v in D is aligned to a node v?
in D?
iffSTR(v) =STR(v?).
This alignment can be constructed rela-tively easy.The alignment algorithm is tested with the followingNODEMATCH function:NODEMATCH(v, v?)
=????????????
?10 if STR(v) = STR(v?
)5 if LABEL(v) = LABEL(v?
)2 if LABEL(v) is a synonymhyperonym or hyponymof LABEL(v?
)0 otherwise1In the original formulation of the algorithm by [Meyers et al,1996], there is a penalty for skipping edges.Alignment : Prec : Rec : F-score:baseline .87 .41 .56algorithm without wordnet .84 .82 .83algorithm with wordnet .86 .84 .85Table 3: Precision, recall and F-score on automatic alignmentIt reserves the highest value for a literal string match, a some-what lower value for matching lemmas, and an even lowervalue in case of a synonym, hyperonym or hyponym relation.The latter relations are retrieved from the Dutch part of Eu-roWordnet [Vossen, 1998].
For the RELMATCH function, wesimply used a value of 1 for identical dependency relations,and 0 otherwise.
These values were found to be adequate in anumber of test runs on two other, manually aligned chapters(these chapters were not used for the actual evaluation).
In thefuture we intend to experiment with automatic optimizations.We measured the alignment accuracy defined as the per-centage of correctly aligned node pairs, where the consen-sus alignment of the first chapter served as the golden stan-dard.
The results are summarized in Table 3.
In order to testthe contribution of synonym and hyperonym information fornode matching, performance is measured with and withoutthe use of Eurowordnet.
The results show that the algorithmimproves substantially on the baseline.
The baseline alreadyachieves a relatively high score (an F-score of .56), whichmay be attributed to the nature of our material: the translatedsentence pairs are relatively close to each other and may showa sizeable amount of literal string overlap.
The alignment al-gorithm (without use of EuroWordnet) loses a few points onprecision, but improves a lot on recall (a 200% increase withrespect to the baseline), which in turn leads to a substantialimprovement on the overall F-score.
The use of Euroword-net leads to a small increase (two points) on both precisionand recall (and thus to small increase on F-score).
Yet, incomparison with the gold standard human score for this task(.95), there is clearly room for further improvement.4 Merging and generationThe remaining two steps in the sentence fusion process aremerging and generation.
In general, merging amounts to de-ciding which information from either sentence should be pre-served, whereas generation involves producing a grammat-ically correct surface representation.
In order to get an ideaabout the baseline performance, we explored a simple, some-what naive string-based approach.
Below, the pseudocodeis shown for merging two dependency trees in order to getrestatements.
Given a labeled alignment A between depen-dency graphs D and D?, if there is a restates relation betweennode v from D and node v?
from D?, we add the string real-ization of v?
as an alternative to those of v.RESTATE(A)1 for each edge ?v, l, v??
?
EA2 do if l = restates3 then STR(v) ?
STR(v) ?
STR(v?
)The same procedure is followed in order to get specifications:SPECIFY(A)1 for each edge ?v, l, v??
?
EA2 do if l = generalizes3 then STR(v) ?
STR(v) ?
STR(v?
)The generalization procedure adds the option to omit the re-alization of a modifier that is not aligned:GENERALIZE(D,A)1 for each edge ?v, l, v??
?
EA2 do if l = specifies3 then STR(v) ?
STR(v) ?
STR(v?
)4 for each edge ?v, l, v??
?
ED5 do if l ?
MOD-DEP-RELS and v /?
EA6 then STR(v) ?
STR(v) ?
NILwhere MOD-DEP-REL is the set of dependency relations be-tween a node and a modifier (e.g.
head/mod and head/predm).Each procedure is repeated twice, once adding substringsfrom D into D?
and once the other way around.
Next, wetraverse the dependency trees and generate all string realiza-tions, extending the list of variants for each node that has mul-tiple realizations.
Finally, we filter out multiple copies of thesame string, as well as strings that are identical to the inputsentences.This procedure for merging and generation was applied tothe 35 sentence pairs from the consensus alignment of chapterone of ?Le Petit Prince?.
Overall this gave rise to 194 restate-ment, 62 specifications and 177 generalizations, with somesentence pairs leading to many variants and others to none atall.
Some output showed only minor variations, for instance,substitution of a synonym.
However, others revealed surpris-ingly adequate generalizations or specifications.
Examples ofgood and bad output are given in Figure 2.As expected, many of the resulting variants are ungram-matical, because constraints on word order, agreement or sub-categorisation are violated.
Following work on statistical sur-face generation [Langkilde and Knight, 1998] and other workon sentence fusion [Barzilay, 2003], we tried to filter un-grammatical variants with an n-gram language model.
TheCambridge-CMU Statistical Modeling Toolkit v2 was used totrain a 3-gram model on over 250M words from the TwenteNewscorpus , using back-off and Good-Turing smoothing.Variants were ranked in order of increasing entropy.
Wefound, however, that the ranking was often inadequate, show-ing ungrammatical variants at the top and grammatical vari-ants in the lower regions.To gain some insight into the general performance of themerging and generation strategy, we performed a small eval-uation test in which the two authors independently judged allgenerated variants in terms of three categories:1.
Perfect: no problems in either semantics or syntax;2.
Acceptable: understandable, but with some minor flawsin semantics or grammar;3.
Nonsense: serious problems in semantics or grammarTable 4 shows the number of sentences in each of the threecategories per judge, broken down in restatements, general-ization and specifications.
The ?-score on this classificationInput1: ZoThushebhaveikIinindetheloopcoursevanofmijnmylevenlifeheelveryveelmanycontactencontactsgehadhadmetwithheelveryveelmanyserieuzeseriouspersonenpersonsInput2: OpIndiethatmanierwaykwamcameikIininhetthelevenlifemetwithmassa?smasses-ofgewichtigeweighty/importantmensenpeopleininaanrakingtouchRestate: opindiethatmanierwayhebhaveikIinindetheloopcoursevanofmijnmylevenlifeheelveryveelmanycontactencontactsgehadhadmetwithheelveryveelmanyserieuzeseriouspersonenpersonsSpecific: opindiethatmanierwaykwamhaveikIinindetheloopcoursevanofmijnmylevenlifemetwithmassa?smasses-ofgewichtigeweighty/importantmensenpeopleininaanrakingtouchGeneral: zothushebhaveikIininhetthelevenlifeveelmanycontactencontactsgehadhadmetwithveelmanyserieuzeseriouspersonenpersonsInput1: EnAndzosohebhaveikIopatmijnmyzesdesixthjaaryeareenaprachtigewonderfulloopbaancareeralsaskunstschilderart-painterlatenletvarensail.Input2: ZoThuskwamcamehetit,,datthatikIopatzesjarigesix-yearleeftijdageeenaschitterendebrightschildersloopbaanpainter-careerlietletvarensail.Specific: enandzosohebhaveikIopatmijnmyzesdesixthjaaryearalsaskunstschilderart-painterlatenleteenaschitterendebrightschildersloopbaanpainter-careervarensailGeneral: zosokwamcamehetitdatthatikIopatleeftijdageeenaprachtigewonderfulloopbaancareerlietletvarensailFigure 2: Examples of good (top) and bad (bottom) sentence fusion outputRestate: Specific: General:J1 J2 J1 J2 J1 J2Perfect: 109 104 28 22 89 86Acceptable: 44 58 15 16 34 24Nonsense: 41 32 19 24 54 67Total: 194 62 177Table 4: Results of the evaluation of the sentence fusion out-put as the number of sentences in each of the three categoriesperfect, acceptable and nonsense per judge (J1 and J2), bro-ken down in restatements, generalizations and specifications.task is .75, indicating a moderate to good agreement betweenthe judges.
Roughly half of the generated restatements andgeneralization are perfect, while this is not the case for spec-ifications.
We have no plausible explanation for this yet.We think we can conclude from this evaluation that sen-tence fusion is a viable and interesting approach for produc-ing restatements, generalization and specifications.
However,there is certainly further work to do; the procedure for merg-ing dependency graphs should be extended, and the realiza-tion model clearly requires more linguistic sophistication inparticular to deal with word order, agreement and subcate-gorisation constraints.5 Discussion and Future workIn this paper we have described our ongoing work on sen-tence fusion for Dutch.
Starting point was the sentence fusionmodel proposed by [Barzilay et al, 1999; Barzilay, 2003]in which dependency analyses of pairs of sentences are firstaligned, after which the aligned parts (representing the com-mon information) are fused.
The resulting fused dependencytree is subsequently transfered into natural language.
Ournew contributions are primarily in two areas.
First, we carriedout an explicit evaluation of the alignment ?
both human andautomatic alignment ?
whereas [Barzilay, 2003] only evalu-ates the output of the complete sentence fusion process.
Wefound that annotators can reliably align phrases and assignrelation labels to them, and that good results can be achievedwith automatic alignment, certainly above an informed base-line, albeit still below human performance.
Second, Barzi-lay and co-workers developed their sentence fusion model inthe context of multi-document summarization, but arguablythe approach could also be applicable for applications suchas question answering or information extraction.
This seemsto call for a more refined version of sentence fusion, whichhas consequences for alignment, merging and realization.
Wehave therefore introduced five different types of semantic re-lations between strings, namely equals, restates, specifies,generalizes and intersects.
This increases the expressivenessof the representation, and supports generating restatements,generalizations and specifications.
Finally, we described andevaluated our first results on sentence realization based onthese refined alignments, with promising results.Similar work is described in [Pang et al, 2003], who de-scribe a syntax-based algorithm that builds word lattices fromparallel translations which can be used to generate new para-phrases.
Their alignment algorithm is less refined, and thereis only type of alignment and hence output (only restate-ments), but their mapping of aligned trees to a word lattice(or FSA) seems worthwhile to explore in combination withthe approach we have proposed here.One of the issues that remains to be addressed in futurework is the effect of parsing errors.
Such errors were notmanually corrected, but during manual alignment, however,we sometimes found that substrings could not be properlyaligned because the parser failed to identify them as syntac-tic constituents.
The repercussions of this for the generationshould be investigated by comparing the results obtained herewith alignments on perfect parses.
Furthermore, our work onautomatic alignment so far only concerned the alignment ofnodes, not the determination of the relation type.
We intendto address this task with machine learning, initially relyingon shallow features such as the length of the respective tokenstrings and the amount of overlap.
It is also clear that morework is needed on merging and surface realization.
One pos-sible direction here is to exploit the relatively rich linguisticrepresentation of the input sentences (POS tags, lemmas anddependency structures), for instance, along the lines of [Ban-galore and Rambow, 2000].
Yet another issue concerns thetype of text material.
The sentence pairs from our current cor-pus are relatively close, in the sense that there is usually a 1-to-1 mapping between sentences, and both translations moreor less convey the same information.
Although this seems agood starting point to study alignment, we intend to continuewith other types of text material in future work.
For instance,in extending our work to the actual output of a QA system,we expect to encounter sentences with far less overlap.
Ofparticular interest to us is also whether sentence fusion canbe shown to improve the quality of QA system output.References[Bangalore and Rambow, 2000] Srinivas Bangalore andOwen Rambow.
Exploiting a probabilistic hierar-chical model for generation.
In Proceedings of the17th conference on Computational linguistics, pages42?48, Morristown, NJ, USA, 2000.
Association forComputational Linguistics.
[Barzilay et al, 1999] R. Barzilay, K. McKeown, and M. El-haded.
Information fusion in the context of multi-document summarization.
In Proceedings of the 37th An-nual Meeting of the Association for Computational Lin-guistics (ACL-99), Maryland, 1999.
[Barzilay, 2003] R. Barzilay.
Information Fusion for Multi-document Summarization.
Ph.D. Thesis, Columbia Uni-versity, 2003.
[Bouma et al, 2001] Gosse Bouma, Gertjan van Noord, andRobert Malouf.
Alpino: Wide-coverage computationalanalysis of dutch.
In Computational Linguistics in TheNetherlands 2000.
2001.
[Callaway and Lester, 1997] C. Callaway and J. Lester.
Dy-namically improving explanations: A revision-based ap-proach to explanation generation.
In Proceedings of the15th International Joint Conference on Artificial Intelli-gence (IJCAI 1997), pages 952?958, Nagoya, Japan, 1997.
[Carletta, 1996] Jean Carletta.
Assessing agreement on clas-sification tasks: the kappa statistic.
Comput.
Linguist.,22(2):249?254, 1996.
[Chandrasekar and Bangalore, 1997] R. Chandrasekar andS.
Bangalore.
Automatic induction of rules for text simpli-fication.
Knowledge-based Systems, 10(3):183?190, 1997.
[Dagan and Glickman, 2004] I. Dagan and O. Glickman.Probabilistic textual entailment: Generic applied mod-elling of language variability.
In Learning Methods forText Understanding and Mining, Grenoble, 2004.
[Gildea, 2003] D. Gildea.
Loosely tree-based alignment formachine translation.
In Proceedings of the 41st AnnualMeeting of the ACL, Sapporo, Japan, 2003.
[Imamura, 2001] K. Imamura.
Hierarchical phrase align-ment harmonized with parsing.
In Proceedings of the6th Natural Language Processing Pacific Rim Symposium(NLPRS 2001), pages 377?384, Tokyo, Japan, 2001.
[Knight and Marcu, 2002] K. Knight and D. Marcu.
Sum-marization beyond sentence extraction: A probabilistic ap-proach to sentence compression.
Artificial Intelligence,139(1):91?107, 2002.
[Langkilde and Knight, 1998] Irene Langkilde and KevinKnight.
Generation that exploits corpus-based statisticalknowledge.
In Proceedings of the 36th conference on As-sociation for Computational Linguistics, pages 704?710,Morristown, NJ, USA, 1998.
Association for Computa-tional Linguistics.
[Lapata, 2003] M. Lapata.
Probabilistic text structuring: Ex-periments with sentence ordering.
In Proceedings of the41st Annual Meeting of the Association for ComputationalLinguistics, pages 545?552, Sapporo, 2003.
[Maybury, 2004] M. Maybury.
New Directions in QuestionAnswering.
AAAI Press, 2004.
[Meyers et al, 1996] A. Meyers, R. Yangarber, and R. Gr-isham.
Alignment of shared forests for bilingual cor-pora.
In Proceedings of 16th International Conference onComputational Linguistics (COLING-96), pages 460?465,Copenhagen, Denmark, 1996.
[Och and Ney, 2000] Franz Josef Och and Hermann Ney.Statistical machine translation.
In EAMT Workshop, pages39?46, Ljubljana, Slovenia, 2000.
[Pang et al, 2003] Bo Pang, Kevin Knight, and DanielMarcu.
Syntax-based alignment of multiple translations:Extracting paraphrases and generating new sentences.
InHLT-NAACL, 2003.
[Reiter and Dale, 2000] E. Reiter and R. Dale.
Building Nat-ural Language Generation Systems.
Cambridge UniversityPress, Cambridge, 2000.
[Robin, 1994] J. Robin.
Revision-based generation of Nat-ural Language Summaries Providing Historical Back-ground.
Ph.D. Thesis, Columbia University, 1994.
[van der Wouden et al, 2002] T. van der Wouden, H. Hoek-stra, M. Moortgat, B. Renmans, and I. Schuurman.
Syntac-tic analysis in the spoken dutch corpus.
In Proceedings ofthe third International Conference on Language Resourcesand Evaluation, pages 768?773, Las Palmas, Canary Is-lands, Spain, 2002.
[Vossen, 1998] Piek Vossen, editor.
EuroWordNet: a multi-lingual database with lexical semantic networks.
KluwerAcademic Publishers, Norwell, MA, USA, 1998.6 Appendixhebbenkomenhebbenikikopininaanrakingmetzocontactmetindeloopvanveelheelpersoonserieusveelmassagewichtigheelleven mijnlevenhetmanierdiemensFigure3:DependencystructuresandalignmentforthesentencesZohebikindeloopvanmijnlevenheelveelcontactengehadmetheelveelserieuzepersonen.(lit.?ThushaveIinthecourseofmylifeverymanycontactshadwithverymanyseriouspersons?)andOpdiemanierkwamikinhetlevenmetmassa?sgewichtigemenseninaanraking..(lit.?InthatwaycameIinthelifewithmass-ofweighty/importantpeopleintouch?
).Thealignmentrelationsareequals(dottedgray),restates(solidgray),specifies(dottedblack),andintersects(dashedgray).Forthesakeoftransparency,dependencyrelationshavebeenomitted.Figure4:ScreenshotofGadget,thetoolusedforaligningdependencystructuresofsentences.
