Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 30?34,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLearning Entity Representation for Entity DisambiguationZhengyan He?
Shujie Liu?
Mu Li?
Ming Zhou?
Longkai Zhang?
Houfeng Wang???
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China?
Microsoft Research Asiahezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.comzhlongk@qq.com wanghf@pku.edu.cnAbstractWe propose a novel entity disambigua-tion model, based on Deep Neural Net-work (DNN).
Instead of utilizing simplesimilarity measures and their disjoint com-binations, our method directly optimizesdocument and entity representations for agiven similarity measure.
Stacked Denois-ing Auto-encoders are first employed tolearn an initial document representation inan unsupervised pre-training stage.
A su-pervised fine-tuning stage follows to opti-mize the representation towards the simi-larity measure.
Experiment results showthat our method achieves state-of-the-artperformance on two public datasets with-out any manually designed features, evenbeating complex collective approaches.1 IntroductionEntity linking or disambiguation has recently re-ceived much attention in natural language process-ing community (Bunescu and Pasca, 2006; Hanet al, 2011; Kataria et al, 2011; Sen, 2012).
It isan essential first step for succeeding sub-tasks inknowledge base construction (Ji and Grishman,2011) like populating attribute to entities.
Givena sentence with four mentions, ?The [[Python]] of[[Delphi]] was a creature with the body of a snake.This creature dwelled on [[Mount Parnassus]], incentral [[Greece]].?
How can we determine thatPython is an earth-dragon in Greece mythologyand not the popular programming language, Del-phi is not the auto parts supplier, and Mount Par-nassus is in Greece, not in Colorado?A most straightforward method is to comparethe context of the mention and the definition ofcandidate entities.
Previous work has exploredmany ways of measuring the relatedness of context?Corresponding authord and entity e, such as dot product, cosine similar-ity, Kullback-Leibler divergence, Jaccard distance,or more complicated ones (Zheng et al, 2010;Kulkarni et al, 2009; Hoffart et al, 2011; Bunescuand Pasca, 2006; Cucerzan, 2007; Zhang et al,2011).
However, these measures are often dupli-cate or over-specified, because they are disjointlycombined and their atomic nature determines thatthey have no internal structure.Another line of work focuses on collective dis-ambiguation (Kulkarni et al, 2009; Han et al,2011; Ratinov et al, 2011; Hoffart et al, 2011).Ambiguous mentions within the same context areresolved simultaneously based on the coherenceamong decisions.
Collective approaches often un-dergo a non-trivial decision process.
In fact, (Rati-nov et al, 2011) show that even though global ap-proaches can be improved, local methods based ononly similarity sim(d, e) of context d and entity eare hard to beat.
This somehow reveals the impor-tance of a good modeling of sim(d, e).Rather than learning context entity associa-tion at word level, topic model based approaches(Kataria et al, 2011; Sen, 2012) can learn it inthe semantic space.
However, the one-topic-per-entity assumption makes it impossible to scale tolarge knowledge base, as every entity has a sepa-rate word distribution P (w|e); besides, the train-ing objective does not directly correspond withdisambiguation performances.To overcome disadvantages of previous ap-proaches, we propose a novel method to learn con-text entity association enriched with deep architec-ture.
Deep neural networks (Hinton et al, 2006;Bengio et al, 2007) are built in a hierarchical man-ner, and allow us to compare context and entityat some higher level abstraction; while at lowerlevels, general concepts are shared across entities,resulting in compact models.
Moreover, to makeour model highly correlated with disambiguationperformance, our method directly optimizes doc-30ument and entity representations for a fixed simi-larity measure.
In fact, the underlying representa-tions for computing similarity measure add inter-nal structure to the given similarity measure.
Fea-tures are learned leveraging large scale annotationof Wikipedia, without any manual design efforts.Furthermore, the learned model is compact com-pared with topic model based approaches, and canbe trained discriminatively without relying on ex-pensive sampling strategy.
Despite its simplicity,it beats all complex collective approaches in ourexperiments.
The learned similarity measure canbe readily incorporated into any existing collectiveapproaches, which further boosts performance.2 Learning Representation forContextual DocumentGiven a mention string m with its context docu-ment d, a list of candidate entities C(m) are gen-erated form, for each candidate entity ei ?
C(m),we compute a ranking score sim(dm, ei) indicat-ing how likely m refers to ei.
The linking result ise = argmaxei sim(dm, ei).Our algorithm consists of two stages.
In the pre-training stage, Stacked Denoising Auto-encodersare built in an unsupervised layer-wise fashion todiscover general concepts encoding d and e. In thesupervised fine-tuning stage, the entire networkweights are fine-tuned to optimize the similarityscore sim(d, e).2.1 Greedy Layer-wise Pre-trainingStacked Auto-encoders (Bengio et al, 2007) isone of the building blocks of deep learning.
As-sume the input is a vector x, an auto-encoder con-sists of an encoding process h(x) and a decod-ing process g(h(x)).
The goal is to minimize thereconstruction error L(x, g(h(x))), thus retainingmaximum information.
By repeatedly stackingnew auto-encoder on top of previously learnedh(x), stacked auto-encoders are obtained.
Thisway we learn multiple levels of representation ofinput x.One problem of auto-encoder is that it treats allwords equally, no matter it is a function word ora content word.
Denoising Auto-encoder (DA)(Vincent et al, 2008) seeks to reconstruct x givena random corruption x?
of x. DA can capture globalstructure while ignoring noise as the author showsin image processing.
In our case, we input eachdocument as a binary bag-of-words vector (Fig.1).
DA will capture general concepts and ignorenoise like function words.
By applying maskingnoise (randomly mask 1 with 0), the model alsoexhibits a fill-in-the-blank property (Vincent etal., 2010): the missing components must be re-covered from partial input.
Take ?greece?
for ex-ample, the model must learn to predict it with?python?
?mount?, through some hidden unit.
Thehidden unit may somehow express the concept ofGreece mythology.h(x)g(h(x))pythondragon delphicoding ... greecemountsnake phdreconstruct inputreconstruct randomzero nodenot reconstructinactiveactive, but mask outactiveFigure 1: DA and reconstruction sampling.In order to distinguish between a large num-ber of entities, the vocabulary size must be largeenough.
This adds considerable computationaloverhead because the reconstruction process in-volves expensive dense matrix multiplication.
Re-construction sampling keeps the sparse propertyof matrix multiplication by reconstructing a smallsubset of original input, with no loss of quality ofthe learned representation (Dauphin et al, 2011).2.2 Supervised Fine-tuningThis stage we optimize the learned representation(?hidden layer n?
in Fig.
2) towards the rankingscore sim(d, e), with large scale Wikipedia an-notation as supervision.
We collect hyperlinks inWikipedia as our training set {(di, ei,mi)}, wheremi is the mention string for candidate generation.The network weights below ?hidden layer n?
areinitialized with the pre-training stage.Next, we stack another layer on top of thelearned representation.
The whole network istuned by the final supervised objective.
The reasonto stack another layer on top of the learned rep-resentation, is to capture problem specific struc-tures.
Denote the encoding of d and e as d?
ande?
respectively, after stacking the problem-specificlayer, the representation for d is given as f(d) =sigmoid(W ?
d?
+ b), where W and b are weightand bias term respectively.
f(e) follows the same31encoding process.The similarity score of (d, e) pair is defined asthe dot product of f(d) and f(e) (Fig.
2):sim(d, e) = Dot(f(d), f(e)) (1)<.,.>f(d) f(e)hidden layer nstacked auto-encodersim(d,e)Figure 2: Network structure of fine-tuning stage.Our goal is to rank the correct entity higherthan the rest candidates relative to the context ofthe mention.
For each training instance (d, e), wecontrast it with one of its negative candidate pair(d, e?).
This gives the pairwise ranking criterion:L(d, e) = max{0, 1?
sim(d, e) + sim(d, e?
)}(2)Alternatively, we can contrast with all its candi-date pairs (d, ei).
That is, we raise the similarityscore of true pair sim(d, e) and penalize all therest sim(d, ei).
The loss function is defined asnegative log of softmax function:L(d, e) = ?
log exp sim(d, e)?ei?C(m) exp sim(d, ei)(3)Finally, we seek to minimize the following train-ing objective across all training instances:L =?d,eL(d, e) (4)The loss function is closely related to con-trastive estimation (Smith and Eisner, 2005),which defines where the positive example takesprobability mass from.
We find that by penaliz-ing more negative examples, convergence speedcan be greatly accelerated.
In our experiments, thesoftmax loss function consistently outperformspairwise ranking loss function, which is taken asour default setting.However, the softmax training criterion addsadditional computational overhead when per-forming mini-batch Stochastic Gradient Descent(SGD).
Although we can use a plain SGD (i.e.mini-batch size is 1), mini-batch SGD is faster toconverge and more stable.
Assume the mini-batchsize ism and the number of candidates is n, a totalof m ?
n forward-backward passes over the net-work are performed to compute a similarity ma-trix (Fig.
3), while pairwise ranking criterion onlyneeds 2?m.
We address this problem by groupingtraining pairs with same mentionm into one mini-batch {(d, ei)|ei ?
C(m)}.
Observe that if candi-date entities overlap, they share the same forward-backward path.
Only m + n forward-backwardpasses are needed for each mini-batch now.Python (programming language)PythonidaePython (mythology)... ...... ...... ...d0d1 ...dm ... =sim(d,e)e0 e1 e2 enFigure 3: Sharing path within mini-batch.The re-organization of mini-batch is similarin spirit to Backpropagation Through Structure(BTS) (Goller and Kuchler, 1996).
BTS is a vari-ant of the general backpropagation algorithm forstructured neural network.
In BTS, parent nodeis computed with its child nodes at the forwardpass stage; child node receives gradient as the sumof derivatives from all its parents.
Here (Fig.
2),parent node is the score node sim(d, e) and childnodes are f(d) and f(e).
In Figure 3, each rowshares forward path of f(d) while each columnshares forward path of f(e).
At backpropagationstage, gradient is summed over each row of scorenodes for f(d) and over each column for f(e).Till now, our input simply consists of bag-of-words binary vector.
We can incorporate anyhandcrafted feature f(d, e) as:sim(d, e) = Dot(f(d), f(e)) + ~?~f(d, e) (5)In fact, we find that with only Dot(f(d), f(e))as ranking score, the performance is sufficientlygood.
So we leave this as our future work.323 Experiments and AnalysisTraining settings: In pre-training stage, inputlayer has 100,000 units, all hidden layers have1,000 units with rectifier functionmax(0, x).
Fol-lowing (Glorot et al, 2011), for the first recon-struction layer, we use sigmoid activation func-tion and cross-entropy error function.
For higherreconstruction layers, we use softplus (log(1 +exp(x))) as activation function and squared lossas error function.
For corruption process, we use amasking noise probability in {0.1,0.4,0.7} for thefirst layer, a Gaussian noise with standard devi-ation of 0.1 for higher layers.
For reconstructionsampling, we set the reconstruction rate to 0.01.
Infine-tuning stage, the final layer has 200 units withsigmoid activation function.
The learning rate isset to 1e-3.
The mini-batch size is set to 20.We run all our experiments on a Linux ma-chine with 72GB memory 6 core Xeon CPU.
Themodel is implemented in Python with C exten-sions, numpy configured with Openblas library.Thanks to reconstruction sampling and refinedmini-batch arrangement, it takes about 1 day toconverge for pre-training and 3 days for fine-tuning, which is fast given our training set size.Datasets: We use half of Wikipedia 1 plain text(?1.5M articles split into sections) for pre-training.We collect a total of 40M hyperlinks grouped byname string m for fine-tuning stage.
We holdouta subset of hyperlinks for model selection, and wefind that 3 layers network with a higher maskingnoise rate (0.7) always gives best performance.We select TAC-KBP 2010 (Ji and Grishman,2011) dataset for non-collective approaches, andAIDA 2 dataset for collective approaches.
For bothdatasets, we evaluate the non-NIL queries.
TheTAC-KBP and AIDA testb dataset contains 1020and 4485 non-NIL queries respectively.For candidate generation, mention-to-entity dic-tionary is built by mining Wikipedia structures,following (Cucerzan, 2007).
We keep top 30 can-didates by prominence P (e|m) for speed consid-eration.
The candidate generation recall are 94.0%and 98.5% for TAC and AIDA respectively.Analysis: Table 1 shows evaluation resultsacross several best performing systems.
(Han etal., 2011) is a collective approach, using Person-alized PageRank to propagate evidence between1available at http://dumps.wikimedia.org/enwiki/, we usethe 20110405 xml dump.2available at http://www.mpi-inf.mpg.de/yago-naga/aida/different decisions.
To our surprise, our methodwith only local evidence even beats several com-plex collective methods with simple word similar-ity.
This reveals the importance of context model-ing in semantic space.
Collective approaches canimprove performance only when local evidence isnot confident enough.
When embedding our sim-ilarity measure sim(d, e) into (Han et al, 2011),we achieve the best results on AIDA.A close error analysis shows some typical er-rors due to the lack of prominence feature andname matching feature.
Some queries acciden-tally link to rare candidates and some link to en-tities with completely different names.
We willadd these features as mentioned in Eq.
5 in future.We will also add NIL-detection module, which isrequired by more realistic application scenarios.A first thought is to construct pseudo-NIL withWikipedia annotations and automatically learn thethreshold and feature weight as in (Bunescu andPasca, 2006; Kulkarni et al, 2009).Methods microP@1macroP@1TAC 2010 evalLcc (2010) (top1, noweb) 79.22 -Siel 2010 (top2, noweb) 71.57 -our best 80.97 -AIDA dataset (collective approaches)AIDA (2011) 82.29 82.02Shirakawa et al (2011) 81.40 83.57Kulkarni et al (2009) 72.87 76.74wordsim (cosine) 48.38 37.30Han (2011) +wordsim 78.97 75.77our best (non-collective) 84.82 83.37Han (2011) + our best 85.62 83.95Table 1: Evaluation on TAC and AIDA dataset.4 ConclusionWe propose a deep learning approach that auto-matically learns context-entity similarity measurefor entity disambiguation.
The intermediate rep-resentations are learned leveraging large scale an-notations of Wikipedia, without any manual effortof designing features.
The learned representationof entity is compact and can scale to very largeknowledge base.
Furthermore, experiment revealsthe importance of context modeling in this field.By incorporating our learned measure into collec-tive approach, performance is further improved.33AcknowledgmentsWe thank Nan Yang, Jie Liu and Fei Wang for helpful discus-sions.
This research was partly supported by National HighTechnology Research and Development Program of China(863 Program) (No.
2012AA011101), National Natural Sci-ence Foundation of China (No.91024009) and Major Na-tional Social Science Fund of China(No.
12&ZD227).ReferencesY.
Bengio, P. Lamblin, D. Popovici, and H. Larochelle.2007.
Greedy layer-wise training of deep networks.Advances in neural information processing systems,19:153.R.
Bunescu and M. Pasca.
2006.
Using encyclope-dic knowledge for named entity disambiguation.
InProceedings of EACL, volume 6, pages 9?16.S.
Cucerzan.
2007.
Large-scale named entity disam-biguation based on wikipedia data.
In Proceedingsof EMNLP-CoNLL, volume 6, pages 708?716.Y.
Dauphin, X. Glorot, and Y. Bengio.
2011.Large-scale learning of embeddings with recon-struction sampling.
In Proceedings of the Twenty-eighth International Conference on Machine Learn-ing (ICML11).X.
Glorot, A. Bordes, and Y. Bengio.
2011.
Domainadaptation for large-scale sentiment classification: Adeep learning approach.
In Proceedings of the 28thInternational Conference on Machine Learning.Christoph Goller and Andreas Kuchler.
1996.
Learn-ing task-dependent distributed representations bybackpropagation through structure.
In Neural Net-works, 1996., IEEE International Conference on,volume 1, pages 347?352.
IEEE.X.
Han, L. Sun, and J. Zhao.
2011.
Collective en-tity linking in web text: a graph-based method.
InProceedings of the 34th international ACM SIGIRconference on Research and development in Infor-mation Retrieval, pages 765?774.
ACM.G.E.
Hinton, S. Osindero, and Y.W.
Teh.
2006.
A fastlearning algorithm for deep belief nets.
Neural com-putation, 18(7):1527?1554.J.
Hoffart, M.A.
Yosef, I. Bordino, H. Fu?rstenau,M.
Pinkal, M. Spaniol, B. Taneva, S. Thater, andG.
Weikum.
2011.
Robust disambiguation ofnamed entities in text.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 782?792.
Association for Com-putational Linguistics.Heng Ji and Ralph Grishman.
2011.
Knowledgebase population: Successful approaches and chal-lenges.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 1148?1158, Portland, Oregon, USA, June.
Association forComputational Linguistics.S.S.
Kataria, K.S.
Kumar, R. Rastogi, P. Sen, and S.H.Sengamedu.
2011.
Entity disambiguation with hier-archical topic models.
In Proceedings of KDD.S.
Kulkarni, A. Singh, G. Ramakrishnan, andS.
Chakrabarti.
2009.
Collective annotation ofwikipedia entities in web text.
In Proceedings ofthe 15th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 457?466.
ACM.J.
Lehmann, S. Monahan, L. Nezda, A. Jung, andY.
Shi.
2010.
Lcc approaches to knowledge basepopulation at tac 2010.
In Proc.
TAC 2010 Work-shop.L.
Ratinov, D. Roth, D. Downey, and M. Anderson.2011.
Local and global algorithms for disambigua-tion to wikipedia.
In Proceedings of the AnnualMeeting of the Association of Computational Lin-guistics (ACL).P.
Sen. 2012.
Collective context-aware topic mod-els for entity disambiguation.
In Proceedings of the21st international conference on World Wide Web,pages 729?738.
ACM.M.
Shirakawa, H. Wang, Y.
Song, Z. Wang,K.
Nakayama, T. Hara, and S. Nishio.
2011.
Entitydisambiguation based on a probabilistic taxonomy.Technical report, Technical Report MSR-TR-2011-125, Microsoft Research.N.A.
Smith and J. Eisner.
2005.
Contrastive estima-tion: Training log-linear models on unlabeled data.In Proceedings of the 43rd Annual Meeting on Asso-ciation for Computational Linguistics, pages 354?362.
Association for Computational Linguistics.P.
Vincent, H. Larochelle, Y. Bengio, and P.A.
Man-zagol.
2008.
Extracting and composing robustfeatures with denoising autoencoders.
In Proceed-ings of the 25th international conference on Ma-chine learning, pages 1096?1103.
ACM.Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,Yoshua Bengio, and Pierre-Antoine Manzagol.2010.
Stacked denoising autoencoders: Learninguseful representations in a deep network with a localdenoising criterion.
The Journal of Machine Learn-ing Research, 11:3371?3408.W.
Zhang, Y.C.
Sim, J. Su, and C.L.
Tan.
2011.
Entitylinking with effective acronym expansion, instanceselection and topic modeling.
In Proceedings ofthe Twenty-Second international joint conference onArtificial Intelligence-Volume Volume Three, pages1909?1914.
AAAI Press.Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xi-aoyan Zhu.
2010.
Learning to link entities withknowledge base.
In Human Language Technolo-gies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 483?491, Los Ange-les, California, June.
Association for ComputationalLinguistics.34
