Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 923?930, Vancouver, October 2005. c?2005 Association for Computational LinguisticsMulti-Perspective Question Answering Using the OpQA CorpusVeselin Stoyanov and Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14850, USA{ves,cardie}@cs.cornell.eduJanyce WiebeDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260, USAwiebe@cs.pitt.eduAbstractWe investigate techniques to support theanswering of opinion-based questions.We first present the OpQA corpus of opin-ion questions and answers.
Using the cor-pus, we compare and contrast the proper-ties of fact and opinion questions and an-swers.
Based on the disparate characteris-tics of opinion vs. fact answers, we arguethat traditional fact-based QA approachesmay have difficulty in an MPQA settingwithout modification.
As an initial steptowards the development of MPQA sys-tems, we investigate the use of machinelearning and rule-based subjectivity andopinion source filters and show that theycan be used to guide MPQA systems.1 IntroductionMuch progress has been made in recent years inautomatic, open-domain question answering (e.g.,Voorhees (2001), Voorhees (2002), Voorhees andBuckland (2003)).
The bulk of the research in thisarea, however, addresses fact-based questions like:?When did McDonald?s open its first restaurant?
?or ?What is the Kyoto Protocol??.
To date, how-ever, relatively little research been done in the areaof Multi-Perspective Question Answering (MPQA),which targets questions of the following sort:?
How is Bush?s decision not to ratify the Kyoto Protocollooked upon by Japan and other US allies??
How do the Chinese regard the human rights record of theUnited States?In comparison to fact-based question answering(QA), researchers understand far less about the prop-erties of questions and answers in MPQA, and haveyet to develop techniques to exploit knowledge ofthose properties.
As a result, it is unclear whetherapproaches that have been successful in the domainof fact-based QA will work well for MPQA.We first present the OpQA corpus of opinion ques-tions and answers.
Using the corpus, we compareand contrast the properties of fact and opinion ques-tions and answers.
We find that text spans identi-fied as answers to opinion questions: (1) are approx-imately twice as long as those of fact questions, (2)are much more likely (37% vs. 9%) to represent par-tial answers rather than complete answers, (3) varymuch more widely with respect to syntactic cate-gory ?
covering clauses, verb phrases, prepositionalphrases, and noun phrases; in contrast, fact answersare overwhelming associated with noun phrases, and(4) are roughly half as likely to correspond to a sin-gle syntactic constituent type (16-38% vs. 31-53%).Based on the disparate characteristics of opinionvs.
fact answers, we argue that traditional fact-basedQA approaches may have difficulty in an MPQAsetting without modification.
As one such modifi-cation, we propose that MPQA systems should relyon natural language processing methods to identifyinformation about opinions.
In experiments in opin-ion question answering using the OpQA corpus, wefind that filtering potential answers using machinelearning and rule-based NLP opinion filters substan-tially improves the performance of an end-to-endMPQA system according to both a mean reciprocalrank (MRR) measure (0.59 vs. a baseline of 0.42)923and a metric that determines the mean rank of thefirst correct answer (MRFA) (26.2 vs. a baseline of61.3).
Further, we find that requiring opinion an-swers to match the requested opinion source (e.g.,does <source> approve of the Kyoto Protocol) dra-matically improves the performance of the MPQAsystem on the hardest questions in the corpus.The remainder of the paper is organized as fol-lows.
In the next section we summarize relatedwork.
Section 3 describes the OpQA corpus.
Sec-tion 4 uses the OpQA corpus to identify poten-tially problematic issues for handling opinion vs.fact questions.
Section 5 briefly describes an opin-ion annotation scheme used in the experiments.
Sec-tions 6 and 7 explore the use of opinion informationin the design of MPQA systems.2 Related WorkThere is a growing interest in methods for the auto-matic identification and extraction of opinions, emo-tions, and sentiments in text.
Much of the relevantresearch explores sentiment classification, a text cat-egorization task in which the goal is to assign toa document either positive (?thumbs up?)
or nega-tive (?thumbs down?)
polarity (e.g.
Das and Chen(2001), Pang et al (2002), Turney (2002), Dave etal.
(2003), Pang and Lee (2004)).
Other researchhas concentrated on analyzing opinions at, or below,the sentence level.
Recent work, for example, indi-cates that systems can be trained to recognize opin-ions, their polarity, their source, and their strengthto a reasonable degree of accuracy (e.g.
Dave etal.
(2003), Riloff and Wiebe (2003), Bethard et al(2004), Pang and Lee (2004), Wilson et al (2004),Yu and Hatzivassiloglou (2003), Wiebe and Riloff(2005)).Related work in the area of corpus developmentincludes Wiebe et al?s (2005) opinion annotationscheme to identify subjective expressions ?
expres-sions used to express opinions, emotions, sentimentsand other private states in text.
Wiebe et al haveapplied the annotation scheme to create the MPQAcorpus consisting of 535 documents manually an-notated for phrase-level expressions of opinion.
Inaddition, the NIST-sponsored TREC evaluation hasbegun to develop data focusing on opinions ?
the2003 Novelty Track features a task that requires sys-tems to identify opinion-oriented documents w.r.t.
aspecific issue (Voorhees and Buckland, 2003).While all of the above work begins to bridgethe gap between text categorization and questionanswering, none of the approaches have been em-ployed or evaluated in the context of MPQA.3 OpQA CorpusTo support our research in MPQA, we created theOpQA corpus of opinion and fact questions and an-swers.
Additional details on the construction of thecorpus as well as results of an interannotator agree-ment study can be found in Stoyanov et al (2004).3.1 Documents and QuestionsThe OpQA corpus consists of 98 documents that ap-peared in the world press between June 2001 andMay 2002.
All documents were taken from theaforementioned MPQA corpus (Wilson and Wiebe,2003)1 and are manually annotated with phrase-level opinion information, following the annotationscheme of Wiebe et al (2005), which is brieflysummarized in Section 5.
The documents coverfour general (and controversial) topics: PresidentBush?s alternative to the Kyoto protocol (kyoto); theUS annual human rights report (humanrights); the2002 coup d?etat in Venezuela (venezuela); and the2002 elections in Zimbabwe and Mugabe?s reelec-tion (mugabe).
Each topic is covered by between 19and 33 documents that were identified automaticallyvia IR methods.Both fact and opinion questions for each topicwere added to the OpQA corpus by a volunteer notassociated with the current project.
The volunteerwas provided with a set of instructions for creat-ing questions together with two documents on eachtopic selected at random.
He created between sixand eight questions on each topic, evenly split be-tween fact and opinion.
The 30 questions are givenin Table 1 sorted by topic.3.2 Answer annotationsAnswer annotations were added to the corpus by twoannotators according to a set of annotation instruc-1The MPQA corpus is available athttp://nrrc.mitre.org/NRRC/publications.htm.The OpQA corpus is available upon request.924Kyoto1 f What is the Kyoto Protocol about?2 f When was the Kyoto Protocol adopted?3 f Who is the president of the Kiko Network?4 f What is the Kiko Network?5 o Does the president of the Kiko Network approve of the US action concerning the Kyoto Protocol?6 o Are the Japanese unanimous in their opinion of Bush?s position on the Kyoto Protocol?7 o How is Bush?s decision not to ratify the Kyoto Protocol looked upon by Japan and other US allies?8 o How do European Union countries feel about the US opposition to the Kyoto protocol?Human Rights1 f What is the murder rate in the United States?2 f What country issues an annual report on human rights in the United States?3 o How do the Chinese regard the human rights record of the United States?4 f Who is Andrew Welsdan?5 o What factors influence the way in which the US regards the human rights records of other nations?6 o Is the US Annual Human Rights Report received with universal approval around the world?Venezuela1 f When did Hugo Chavez become President?2 f Did any prominent Americans plan to visit Venezuela immediately following the 2002 coup?3 o Did anything surprising happen when Hugo Chavez regained power in Venezuela after he wasremoved by a coup?4 o Did most Venezuelans support the 2002 coup?5 f Which governmental institutions in Venezuela were dissolved by the leaders of the 2002 coup?6 o How did ordinary Venezuelans feel about the 2002 coup and subsequent events?7 o Did America support the Venezuelan foreign policy followed by Chavez?8 f Who is Vice-President of Venezuela?Mugabe1 o What was the American and British reaction to the reelection of Mugabe?2 f Where did Mugabe vote in the 2002 presidential election?3 f At which primary school had Mugabe been expected to vote in the 2002 presidential election?4 f How long has Mugabe headed his country?5 f Who was expecting Mugabe at Mhofu School for the 2002 election?6 o What is the basis for the European Union and US critical attitude and adversarial action towardMugabe?7 o What did South Africa want Mugabe to do after the 2002 election?8 o What is Mugabe?s opinion about the West?s attitude and actions towards the 2002 Zimbabwe elec-tion?Table 1: Questions in the OpQA collection by topic.f in column 1 indicates a fact question; o, an opinionquestion.tions.2 Every text segment that contributes to ananswer to any of the 30 questions is annotated asan answer.
In particular, answer annotations includesegments that constitute a partial answer.
Partial an-swers either (1) lack the specificity needed to consti-tute a full answer (e.g., ?before May 2004?
partiallyanswers the question When was the Kyoto protocolratified?
when a specific date is known) or (2) needto be combined with at least one additional answersegment to fully answer the question (e.g., the ques-tion Are the Japanese unanimous in their oppositionof Bush?s position on the Kyoto protocol?
is an-swered only partially by a segment expressing a sin-gle opinion).
In addition, annotators mark the min-imum answer spans (e.g., ?a Tokyo organization,?vs.
?a Tokyo organization representing about 150Japanese groups?
).4 Characteristics of opinion answersNext, we use the OpQA corpus to analyze and com-pare the characteristics of fact vs. opinion questions.Based on our findings, we believe that QA systemsbased solely on traditional QA techniques are likely2The annotation instructions are availableat http://www.cs.cornell.edu/ ves/Publications/publications.htm.to be less effective at MPQA than they are at tradi-tional fact-based QA.4.1 Traditional QA architecturesDespite the wide variety of approaches implied bymodern QA systems, almost all systems rely on thefollowing two steps (subsystems), which have em-pirically proven to be effective:?
IR module.
The QA system invokes an IR subsystem thatemploys traditional text similarity measures (e.g., tf/idf)to retrieve and rank document fragments (sentences orparagraphs) w.r.t.
the question (query).?
Linguistic filters.
QA systems employ a set of filtersand text processing components to discard some docu-ment fragments.
The following filters have empiricallyproven to be effective and are used universally:Semantic filters prefer an answer segment that matchesthe semantic class(es) associated with the question type(e.g., date or time for when questions; person or organi-zation for who questions).Syntactic filters are also configured on the type of ques-tion.
The most common and effective syntactic filters se-lect a specific constituent (e.g., noun phrase) according tothe question type (e.g., who question).QA systems typically interleave the above twosubsystems with a variety of different processingsteps of both the question and the answer.
The goalof the processing is to identify text fragments thatcontain an answer to the question.
Typical QA sys-tems do not perform any further text processing;they return the text fragment as it occurred in thetext.
34.2 Corpus-based analysis of opinion answersWe hypothesize that QA systems that conform tothis traditional architecture will have difficulty han-dling opinion questions without non-trivial modifi-cation.
In support of this hypothesis, we providestatistics from the OpQA corpus to illustrate some ofthe characteristics that distinguish answers to opin-ion vs. fact questions, and discuss their implicationsfor a traditional QA system architecture.Answer length.
We see in Table 2 that the aver-age length of opinion answers in the OpQA corpus3This architecture is seen mainly in QA systems designedfor TREC?s ?factoid?
and ?list?
QA tracks.
Systems competingin the relatively new ?definition?
or ?other?
tracks have begunto introduce new approaches.
However, most such systems stillrely on the IR step and return the text fragment as it occurred inthe text.925Number of answers Length Number of partialsfact 124 5.12 12 (9.68%)opinion 415 9.24 154 (37.11%)Table 2: Number of answers, average answer length(in tokens), and number of partial answers forfact/opinion questions.is 9.24 tokens, almost double that of fact answers.Unfortunately, longer answers could present prob-lems for some traditional QA systems.
In particu-lar, some of the more sophisticated algorithms thatperform additional processing steps such as logi-cal verifiers (Moldovan et al, 2002) may be less ac-curate or computationally infeasible for longer an-swers.
More importantly, longer answers are likelyto span more than a single syntactic constituent, ren-dering the syntactic filters, and very likely the se-mantic filters, less effective.Partial answers.
Table 2 also shows that over 37%of the opinion answers were marked as partial vs.9.68% of the fact answers.
The implications of par-tial answers for the traditional QA architecture aresubstantial: an MPQA system will require an an-swer generator to (1) distinguish between partialand full answers; (2) recognize redundant partial an-swers; (3) identify which subset of the partial an-swers, if any, constitutes a full answer; (4) determinewhether additional documents need to be examinedto find a complete answer; and (5) asemble the finalanswer from partial pieces of information.Syntactic constituent of the answer.
As discussedin Section 4.1, traditional QA systems rely heav-ily on the predicted syntactic and semantic class ofthe answer.
Based on answer lengths, we specu-lated that opinion answers are unlikely to span a sin-gle constituent and/or semantic class.
This specula-tion is confirmed by examining the phrase type as-sociated with OpQA answers using Abney?s (1996)CASS partial parser.4 For each question, we countthe number of times an answer segment for the ques-tion (in the manual annotations) matches each con-stituent type.
We consider four constituent types?
noun phrase (n), verb phrase (v), prepositionalphrase (p), and clause (c) ?
and three matching cri-teria:4The parser is available fromhttp://www.vinartus.net/spa/.Fact OpinionQues- # of Matching Criteria syn Ques- # of Matching Criteria syntion answers ex up up/dn type tion answers ex up up/dn typeH 1 1 0 0 0 H 3 15 5 5 5 cH 2 4 2 2 2 n H 5 24 5 5 10 nH 4 1 0 0 0 H 6 123 17 23 52 nK 1 48 13 14 24 n K 5 3 0 0 1K 2 38 13 13 19 n K 6 34 6 5 12 cK 3 1 1 1 1 c n K 7 55 9 8 19 cK 4 2 1 1 1 n K 8 25 4 4 10 vM 2 3 0 0 1 M 1 74 10 12 29 vM 3 1 0 0 1 M 6 12 3 5 7 nM 4 10 2 2 5 n M 7 1 0 0 0M 5 3 1 1 2 c M 8 3 0 0 1V 1 4 3 3 4 n V 3 1 1 0 1 cV 2 1 1 1 1 n V 4 13 2 2 2 cV 5 3 0 1 1 V 6 9 2 2 5 c nV 8 4 2 4 4 n V 7 23 3 1 5Cov- 124 39 43 66 Cov- 415 67 70 159erage 31% 35% 53% erage 16% 17% 38%Table 3: Syntactic Constituent Type for Answers inthe OpQA Corpus1.
The exact match criterion is satisfied only by answer seg-ments whose spans exactly correspond to a constituent inthe CASS output.2.
The up criterion considers an answer to match a CASSconstituent if the constituent completely contains the an-swer and no more than three additional (non-answer) to-kens.3.
The up/dn criterion considers an answer to match aCASS constituent if it matches according to the up crite-rion or if the answer completely contains the constituentand no more than three additional tokens.The counts for the analysis of answer segmentsyntactic type for fact vs. opinion questions are sum-marized in Table 3.
Results for the 15 fact ques-tions are shown in the left half of the table, andfor the 15 opinion questions in the right half.
Theleftmost column in each half provides the questiontopic and number, and the second column indicatesthe total number of answer segments annotated forthe question.
The next three columns show, for eachof the ex, up, and up/dn matching criteria, respec-tively, the number of annotated answer segmentsthat match the majority syntactic type among an-swer segments for that question/criterion pair.
Us-ing a traditional QA architecture, the MPQA sys-tem might filter answers based on this majority type.The syn type column indicates the majority syntac-tic type using the exact match criterion; two valuesin the column indicate a tie for majority syntactictype, and an empty syntactic type indicates that noanswer exactly matched any of the four constituenttypes.
With only a few exceptions, the up and up/dnmatching criteria agreed in majority syntactic type.Results in Table 3 show a significant disparity be-tween fact and opinion questions.
For fact ques-926tions, the syntactic type filter would keep 31%, 35%,or 53% of the correct answers, depending on thematching criterion.
For opinion questions, there isunfortunately a two-fold reduction in the percentageof correct answers that would remain after filtering?
only 16%, 17% or 38%, depending on the match-ing criterion.
More importantly, the majority syntac-tic type among answers for fact questions is almostalways a noun phrase, while no single constituenttype emerges as a useful syntactic filter for opinionquestions (see the syn phrase columns in Table 3).Finally, because semantic class information is gener-ally tied to a particular syntactic category, the effec-tiveness of traditional semantic filters in the MPQAsetting is unclear.In summary, identifying answers to questions inan MPQA setting within a traditional QA architec-ture will be difficult.
First, the implicit and explicitassumptions inherent in standard linguistic filters areconsistent with the characteristics of fact- rather thanopinion-oriented QA.
In addition, the presence ofrelatively long answers and partial answers will re-quire a much more complex answer generator thanis typically present in current QA systems.In Sections 6 and 7, we propose initial steps to-wards modifying the traditional QA architecture foruse in MPQA.
In particular, we propose and evaluatetwo types of opinion filters for MPQA: subjectiv-ity filters and opinion source filters.
Both types oflinguistic filters rely on phrase-level and sentence-level opinion information, which has been manuallyannotated for our corpus; the next section briefly de-scribes the opinion annotation scheme.5 Manual Opinion AnnotationsDocuments in our OpQA corpus come from thelarger MPQA corpus, which contains manual opin-ion annotations.
The annotation framework is de-scribed in detail in (Wiebe et al, 2005).
Here wegive a high-level overview.The annotation framework provides a basis forsubjective expressions: expressions used to expressopinions, emotions, and sentiments.
The frameworkallows for the annotation of both directly expressedprivate states (e.g., afraid in the sentence ?John isafraid that Sue might fall,?)
and opinions expressedby the choice of words and style of language (e.g.,it is about time and oppression in the sentence ?It isabout time that we end Saddam?s oppression?).
Inaddition, the annotations include several attributes,including the intensity (with possible values low,medium, high, and extreme) and the source of theprivate state.
The source of a private state is the per-son or entity who holds or experiences it.6 Subjectivity Filters for MPQA SystemsThis section describes three subjectivity filtersbased on the above opinion annotation scheme.
Be-low (in Section 6.3), the filters are used to removefact sentences from consideration when answeringopinion questions, and the OpQA corpus is used toevaluate their effectiveness.6.1 Manual Subjectivity FilterMuch previous research on automatic extraction ofopinion information performed classifications at thesentence level.
Therefore, we define sentence-levelopinion classifications in terms of the phrase-levelannotations.
For our gold standard of manual opin-ion classifications (dubbed MANUAL for the rest ofthe paper) we will follow Riloff and Wiebe?s (2003)convention (also used by Wiebe and Riloff (2005))and consider a sentence to be opinion if it containsat least one opinion of intensity medium or higher,and to be fact otherwise.6.2 Two Automatic Subjectivity FiltersAs discussed in section 2, several research effortshave attempted to perform automatic opinion clas-sification on the clause and sentence level.
We in-vestigate whether such information can be useful forMPQA by using the automatic sentence level opin-ion classifiers of Riloff and Wiebe (2003) and Wiebeand Riloff (2005).Riloff and Wiebe (2003) use a bootstrapping al-gorithm to perform a sentence-based opinion classi-fication on the MPQA corpus.
They use a set of highprecision subjectivity and objectivity clues to iden-tify subjective and objective sentences.
This datais then used in an algorithm similar to AutoSlog-TS (Riloff, 1996) to automatically identify a set ofextraction patterns.
The acquired patterns are thenused iteratively to identify a larger set of subjectiveand objective sentences.
In our experiments we use927precision recall FMPQA corpus RULEBASED 90.4 34.2 46.6NAIVE BAYES 79.4 70.6 74.7Table 4: Precision, recall, and F-measure for the twoclassifiers.the classifier that was created by the reimplemen-tation of this bootstrapping process in Wiebe andRiloff (2005).
We will use RULEBASED to denotethe opinion information output by this classifier.In addition, Wiebe and Riloff used the RULE-BASED classifier to produce a labeled data set fortraining.
They trained a Naive Bayes subjectivityclassifier on the labeled set.
We will use NAIVEBAYES to refer to Wiebe and Riloff?s naive Bayesclassifier.5 Table 4 shows the performance of thetwo classifiers on the MPQA corpus as reported byWiebe and Riloff.6.3 ExperimentsWe performed two types of experiments using thesubjectivity filters.6.3.1 Answer rank experimentsOur hypothesis motivating the first type of exper-iment is that subjectivity filters can improve the an-swer identification phase of an MPQA system.
Weimplement the IR subsystem of a traditional QA sys-tem, and apply the subjectivity filters to the IR re-sults.
Specifically, for each opinion question in thecorpus 6 , we do the following:1.
Split all documents in our corpus into sentences.2.
Run an information retrieval algorithm7 on the set of allsentences using the question as the query to obtain aranked list of sentences.3.
Apply a subjectivity filter to the ranked list to remove allfact sentences from the ranked list.We test each of the MANUAL, RULEBASED, andNAIVE BAYES subjectivity filters.
We compare therank of the first answer to each question in the5Specifically, the one they label Naive Bayes 1.6We do not evaluate the opinion filters on the 15 fact ques-tions.
Since opinion sentences are defined as containing at leastone opinion of intensity medium or higher, opinion sentencescan contain factual information and sentence-level opinion fil-ters are not likely to be effective for fact-based QA.7We use the Lemur toolkit?s standard tf.idf implementationavailable from http://www.lemurproject.org/.Topic Qnum Baseline Manual NaiveBayes RulebasedKyoto 5 1 1 1 16 5 4 4 37 1 1 1 18 1 1 1 1Human 3 1 1 1 1Rights 5 10 6 7 56 1 1 1 1Venezuela 3 106 81 92 354 3 2 3 16 1 1 1 17 3 3 3 2Mugabe 1 2 2 2 26 7 5 5 47 447 291 317 1538 331 205 217 182MRR : 0.4244 0.5189 0.5078 0.5856MRFA: 61.3333 40.3333 43.7333 26.2Table 5: Results for the subjectivity filters.ranked list before the filter is applied, with the rankof the first answer to the question in the ranked listafter the filter is applied.Results.
Results for the opinion filters are comparedto a simple baseline, which performs the informa-tion retrieval step with no filtering.
Table 5 gives theresults on the 15 opinion questions for the baselineand each of the three subjectivity filters.
The tableshows two cumulative measures ?
the mean recip-rocal rank (MRR) 8 and the mean rank of the firstanswer (MRFA).
9Table 5 shows that all three subjectivity filters out-perform the baseline: for all three filters, the firstanswer in the filtered results for all 15 questions isranked at least as high as in the baseline.
As a result,the three subjectivity filters outperform the baselinein both MRR and MRFA.
Surprisingly, the best per-forming subjectivity filter is RULEBASED, surpass-ing the gold standard MANUAL, both in MRR (0.59vs.
0.52) and MRFA (40.3 vs. 26.2).
Presum-ably, the improvement in performance comes fromthe fact that RULEBASED identifies subjective sen-tences with the highest precision (and lowest recall).Thus, the RULEBASED subjectivity filter discardsnon-subjective sentences most aggressively.6.3.2 Answer probability experimentsThe second experiment, answer probability, be-gins to explore whether opinion information can be8The MRR is computed as the average of 1/r, where r isthe rank of the first answer.9MRR has been accepted as the standard performance mea-sure in QA, since MRFA can be strongly affected by outlierquestions.
However, the MRR score is dominated by the resultsin the high end of the ranking.
Thus, MRFA may be more ap-propriate for our experiments because the filters are an interme-diate step in the processing, the results of which other MPQAcomponents may improve.928sentencefact opinionManual fact 56 (46.67%) 64 (53.33%)opinion 42 (10.14%) 372 (89.86%)question Naive Bayes fact 49 (40.83%) 71 (59.17%)opinion 57 (13.77%) 357 (86.23%)Rulebased fact 96 (80.00%) 24 (20.00%)opinion 184 (44.44%) 230 (55.56%)Table 6: Answer probability results.used in an answer generator.
This experiment con-siders correspondences between (1) the classes (i.e.,opinion or fact) assigned by the subjectivity filters tothe sentences containing answers, and (2) the classesof the questions the answers are responses to (ac-cording to the OpQA annotations).
That is, we com-pute the probabilities (where ans = answer):P(ans is in a C1 sentence | ans is the answer to a C2 ques-tion) for all four combinations of C1=opinion, fact andC2=opinion, fact.Results.
Results for the answer probability experi-ment are given in Table 6.
The rows correspond tothe classes of the questions the answers respond to,and the columns correspond to the classes assignedby the subjectivity filters to the sentences contain-ing the answers.
The first two rows, for instance,give the results for the MANUAL criterion.
MANUALplaced 56 of the answers to fact questions in factsentences (46.67% of all answers to fact questions)and 64 (53.33%) of the answers to fact questions inopinion sentences.
Similarly, MANUAL placed 42(10.14%) of the answers to opinion questions in factsentences, and 372 (89.86%) of the answers to opin-ion questions in opinion sentences.The answer probability experiment sheds somelight on the subjectivity filter experiments.
All threesubjectivity filters place a larger percentage of an-swers to opinion questions in opinion sentences thanthey place in fact sentences.
However, the differ-ent filters exhibit different degrees of discrimination.Answers to opinion questions are almost alwaysplaced in opinion sentences by MANUAL (89.86%)and NAIVE BAYES (86.23%).
While that aspect oftheir performance is excellent, MANUAL and NAIVEBAYES place more answers to fact questions in opin-ion rather than fact sentences (though the percent-ages are in the 50s).
This is to be expected, becauseMANUAL and NAIVE BAYES are more conservativeand err on the side of classifying sentences as opin-ions: for MANUAL, the presence of any subjectiveexpression makes the entire sentence opinion, evenif parts of the sentence are factual; NAIVE BAYESshows high recall but lower precision in recognizingopinion sentences (see Table 4).
Conversely, RULE-BASED places 80% of the fact answers in fact sen-tences and only 56% of the opinion answers in opin-ion sentences.
Again, the lower number of assign-ments to opinion sentences is to be expected, giventhe high precision and low recall of the classifier.But the net result is that, for RULEBASED, the off-diagonals are all less than 50%: it places more an-swers to fact questions in fact rather than opinionsentences (80%), and more answers to opinion ques-tions in opinion rather than fact sentences (56%).This is consistent with its superior performance inthe subjectivity filtering experiment.In addition to explaining the performance ofthe subjectivity filters, the answer rank experimentshows that the automatic opinion classifiers can beused directly in an answer generator module.
Thetwo automatic classifiers rely on evidence in the sen-tence to predict the class (the information extractionpatterns used by RULEBASED and the features usedby NAIVE BAYES).
In ongoing work we investigateways to use this evidence to extract and summarizethe opinions expressed in text, which is a task simi-lar to that of an answer generator module.7 Opinion Source Filters for MPQASystemsIn addition to subjectivity filters, we also define anopinion source filter based on the manual opinionannotations.
This filter removes all sentences thatdo not have an opinion annotation with a source thatmatches the source of the question10.
For this filterwe only used the MANUAL source annotations sincewe did not have access to automatically extractedsource information.
We employ the same AnswerRank experiment as in 6.3.1, substituting the sourcefilter for a subjectivity filter.Results.
Results for the source filter are mixed.The filter outperforms the baseline on some ques-tions and performs worst on others.
As a result theMRR for the source filter is worse than the base-10We manually identified the sources of each of the 15 opin-ion questions.929line (0.4633 vs. 0.4244).
However, the source fil-ter exhibits by far the best results using the MRFAmeasure, a value of 11.267.
The performance im-provement is due to the filter?s ability to recognizethe answers to the hardest questions, for which theother filters have the most trouble (questions mu-gabe 7 and 8).
For these questions, the rank of thefirst answer improves from 153 to 21, and from 182to 11, respectively.
With the exception of questionvenezuela 3, which does not contain a clear source(and is problematic altogether because there is onlya single answer in the corpus and the question?squalification as opinion is not clear) the source filteralways ranked an answer within the first 25 answers.Thus, source filters can be especially useful in sys-tems that rely on the presence of an answer withinthe first few ranked answer segments and then in-voke more sophisticated analysis in the additionalprocessing phase.8 ConclusionsWe began by giving a high-level overview of theOpQA corpus.
Using the corpus, we compared thecharacteristics of answers to fact and opinion ques-tions.
Based on the different characteristics, we sur-mise that traditional QA approaches may not be aseffective for MPQA as they have been for fact-basedQA.
Finally, we investigated the use of machinelearning and rule-based opinion filters and showedthat they can be used to guide MPQA systems.Acknowledgments We would like to thank Di-ane Litman for her work eliciting the questions forthe OpQA corpus, and the anonymous reviewersfor their helpful comments.This work was supportedby the Advanced Research and Development Activ-ity (ARDA), by NSF Grants IIS-0208028 and IIS-0208798, by the Xerox Foundation, and by a NSFGraduate Research Fellowship to the first author.ReferencesSteven Abney.
1996.
Partial parsing via finite-state cascades.In Proceedings of the ESSLLI ?96 Robust Parsing Workshop.S.
Bethard, H. Yu, A. Thornton, V. Hativassiloglou, and D. Ju-rafsky.
2004.
Automatic extraction of opinion propositionsand their holders.
In 2004 AAAI Spring Symposium on Ex-ploring Attitude and Affect in Text.S.
Das and M. Chen.
2001.
Yahoo for amazon: Extracting mar-ket sentiment from stock message boards.
In Proceedings ofthe 8th Asia Pacific Finance Association Annual Conference.Kushal Dave, Steve Lawrence, and David Pennock.
2003.
Min-ing the peanut gallery: Opinion extraction and semantic clas-sification of product reviews.
In International World WideWeb Conference, pages 519?528.D.
Moldovan, S. Harabagiu, R. Girju, P. Morarescu, F. Laca-tusu, A. Novischi, A. Badulescu, and O. Bolohan.
2002.LCC tools for question answering.
In Proceedings of TREC2002.Bo Pang and Lillian Lee.
2004.
A sentimental education: Sen-timent analysis using subjectivity summarization based onminimum cuts.
In Proceedings of the ACL, pages 271?278.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002.Thumbs up?
sentiment classification using machine learningtechniques.
In Proceedings of EMNLP.E.
Riloff and J. Wiebe.
2003.
Learning extraction patterns forsubjective expressions.
In Proceesings of EMNLP.Ellen Riloff.
1996.
Automatically generating extraction pat-terns from untagged text.
Proceedings of AAAI.V.
Stoyanov, C. Cardie, J. Wiebe, and D. Litman.
2004.
Eval-uating an opinion annotation scheme using a new Multi-Perspective Question and Answer corpus.
In 2004 AAAISpring Symposium on Exploring Attitude and Affect in Text.Peter Turney.
2002.
Thumbs up or thumbs down?
Semanticorientation applied to unsupervised classification of reviews.In Proceedings of the ACL, pages 417?424.E.
Voorhees and L. Buckland.
2003.
Overview of theTREC 2003 Question Answering Track.
In Proceedings ofTREC 12.Ellen Voorhees.
2001.
Overview of the TREC 2001 QuestionAnswering Track.
In Proceedings of TREC 10.Ellen Voorhees.
2002.
Overview of the 2002 Question Answer-ing Track.
In Proceedings of TREC 11.Janyce Wiebe and Ellen Riloff.
2005.
Creating subjectiveand objective sentence classifiers from unannotated texts.
InProceedings of CICLing.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.
An-notating expressions of opinions and emotions in language.Language Resources and Evaluation, 1(2).Theresa Wilson and Janyce Wiebe.
2003.
Annotating opinionsin the world press.
4th SIGdial Workshop on Discourse andDialogue (SIGdial-03).T.
Wilson, J. Wiebe, and R. Hwa.
2004.
Just how mad are you?Finding strong and weak opinion clauses.
In Proceedings ofAAAI.H.
Yu and V. Hatzivassiloglou.
2003.
Towards answering opin-ion questions: Separating facts from opinions and identi-fying the polarity of opinion sentences.
In Proceedings ofEMNLP.930
