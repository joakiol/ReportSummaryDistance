Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 41?46,Beijing, China, July 28, 2015.c?2015 Association for Computational LinguisticsLearning to Map Dependency Parses to Abstract MeaningRepresentationsWei-Te ChenDepartment of Computer ScienceUniversity of Colorado at BoulderWeite.Chen@colorado.eduAbstractAbstract Meaning Representation (AMR)is a semantic representation language usedto capture the meaning of English sen-tences.
In this work, we propose an AMRparser based on dependency parse rewriterules.
This approach transfers dependencyparses into AMRs by integrating the syn-tactic dependencies, semantic arguments,named entity and co-reference informa-tion.
A dependency parse to AMR graphaligner is also introduced as a preliminarystep for designing the parser.1 IntroductionAbstract Meaning Representation (AMR) (Ba-narescu et al., 2013) is a semantic formalism thatexpresses the logical meanings of English sen-tences in the form of a directed, acyclic graph.AMR focuses on the semantic concepts (nodeson the graph), and relations (labeled edges on thegraph) between those concepts.
AMR relies heav-ily on predicate-argument structures defined in thePropBank (PB) (Palmer et al., 2005).
The repre-sentation encodes rich information, including se-mantic roles, named entities, and co-reference in-formation.
Fig.
1 shows an example AMR.In this proposal, we focus on the design of anautomatic AMR parser in a supervised fashionfrom dependency parses.
In contrast with recentsemantic parsing algorithms, we start the parsingprocess from the dependency parses rather thanthe sentences.
A dependency parse provides boththe semantic dependency information for the sen-tence, and the structure of the relations betweenthe head word and their dependencies.
These canprovide strong features for semantic parsing.
Byusing a binary-branching bottom-up shift-reducedalgorithm, the statistical model for the rewriterules can be learned discriminatively.
AlthoughFigure 1: The AMR annotation of sentence ?PierreVinken, 61 years old, will join the board as anonexecutive director Nov.
29.?the AMR parser is my thesis topic, in this proposalwe will pay more attention to preliminary work -the AMR -Dependency Parse aligner.To extract the rewrite rules and the statisticalmodel, we need the links between AMR con-cepts and the word nodes within the dependencyparse.
An example alignment is shown in Fig.2.
Alignment between an AMR concept and de-pendency node is needed because 1) it representsthe meaning of the sub-graph of the concept andits child concepts corresponding to the phrase ofthe head word node, and 2) the dependency nodecontains sufficient information for the extractionof rewrite rules.
For example, the word node?Vinken?
on the dependency parse side in Fig.
2links to the lexical concept ?Vinken?
and, further-more, links to the ?p2/name?
and the ?p/person?concepts since ?Vinken?
is the head of the namedentity (NE) ?Pierre Vinken?
and the head of thenoun phrase ?Pierre Vinken, 61 years old.?
Thesecondary aim of this proposal is to design analignment model between AMR concepts and de-pendency parses.
We use EM to search the hiddenderivations by combining the features of lexicalform, relation label, NE, semantic role, etc.
Af-ter EM processing, both the alignments and all thefeature probabilities can be estimated.The design of a rewrite-based AMR parser isdescribed in Sec.
2, and the aligner is in Sec.
3.Our preliminary experiments and results are pre-41Figure 2: The alignment between an AMR sub-graph and a dependency parse.
A red line linksthe corresponding concept and dependency node.sented in Sec.
4, followed by future work.2 Rewrite Based AMR ParserAMR is a rooted, directed, acyclic graph.
For ex-ample, the concept join-01 in Fig.
1 is the rootmeaning of the sentence, which links to the childconcepts Arg0, Arg1, and time.
AMR adheres tothe following principles (Banarescu et al., 2013):?
AMRs are rooted acyclic graphs with labels (re-lations) on edges.
These labels indicate the di-rected relation between two concepts.?
AMRs abstract away from syntactic idiosyncra-cies of a language, and instead attempt to captureonly the core meaning of a sentence.?
AMRs use the PB framesets as relation la-bels (Palmer et al., 2005).
For example, the rela-tion labels (i.e., ARG0, ARG1) of ?join-01?
con-cept in Fig.
1 correspond to the roles of the PBframe ?join-v.??
AMRs combine multiple layers of linguistic an-notation, like coreference, NE, semantic role,etc., in a single structure.The above basic characteristics make the parsingof AMRs a difficult task.
First, because AMRabstracts away from syntactic idiosyncrasies, weneed a model to link the AMR concepts to wordsin the original sentence, in order to obtain exter-nal lexical, syntactic and semantic features.
Sec-ondly, the parser should learn the different featuretransformation probabilities jointly since AMRscombine several linguistic annotations.
Moreover,(x)/NP?
:op(x) -r1(x) nn (y)?
:name (name(x, y)) -r2(x)/CD?
:quant(x) -r3(x)/NNS?
:unit(x) -r4npadvmod (x)(y)?
temporal-quanity(x, y) -r5old/JJ (x)?
:age(x) -r6NE PERSON(x)(y)?
person(x, y) -r7Table 1: Sample Rewrite RulesAMR uses graph variables and reentrancy to ex-press coreference (e.g., ?p?
variable in Fig 1 ap-pears twice ?
in :ARG0 of join-01 and :ARG0 ofhave-org-role-91).
The reentrancy prevents theAMR graph from begin a tree structure.
Dur-ing parsing decoding, a polynomial time algorithmshould be replaced by alternative algorithms, likebeam search, to avoid an exponential running time.JAMR (Flanigan et al., 2014) is the first sys-tem for AMR parsing, which identifies the con-cepts, and then searches for a maximum span-ning connected subgraph (MSCG) on a fully con-nected graph to identify the relations between con-cepts.
The search algorithm is similar to the maxi-mum spanning tree algorithms.
To assure the fi-nal connected graph conforms to linguistic con-straints, JAMR uses Lagrangian relaxation (Geof-frion, 2010) to supplement the MSCG algorithm.JAMR reaches a 58% Smatch score (Cai andKnight, 2013) on automatic concept and relationidentification data, and 80% on gold concept andautomatic relation identification data.2.1 Our Shift-Reduce Rewrite Rule ParserRewrite rule based parser is a bottom-up converterfrom dependency parses to AMRs.
The processstarts from the leaf word node on the dependencyparse.
By applying rewrite-rules to each wordnode, we obtain and assemble the sub-graphs ofour target AMR.
Sample rewrite rules are listed inTable 1.
In these rules, the left hand side containsthe dependency information (e.g.
word lemma,POS, relation label, NE tag, etc).
The right handside is the AMR concept and its template for fillingvariables from previous parsing steps.
The samplederivation steps are listed in Table 2.
For everystep, it shows the derivation rule applied (in Table1), and the concept name, c1-c8.This approach to parsing could be implementedwith a shift-reduce algorithm (Wang et al., 2015).We define a stack and a list of tokens, which storesthe dependency words in the order of tree traver-sal.
Several actions are defined to operate on thelist(L) and the stack(S):42DerivationApplyRuleConceptNamePierre/NNP?
:op Pierre r1 c1Vinken/NNP?
:op Vinken r1 c2(c1) nn (c2)?
:name (name :op1 Pierre :op2 Vinekn)r2 c361/CD?
:quant 61 r3 c4years/NNS?
:unit year r4 c5npadvmod (c4)(c5) r5 c6?
temporal-quanity :quant 61 :unit yearold/JJ (c6)?
:age (temporal-quanity :quant 61 :unit year)r6 c7NE PERSON (c3)(c7)?
person :name (name :op1 Pierre :op2 Vinekn):age (temporal-quanity :quant 61 :unit year)r7 c8Table 2: The derivation for parsing ?PierreVinken, 61 years old?
from dep.
parse to AMR?
Shift Remove the dependency word from L, ap-ply the rules, and push the new concept to S.?
Reduce Move the two top sub-concepts from S,apply the rules, and push it back to S.?
Unary Move the top sub-concept from S, applythe rules, and push it back to S.?
Finish If no more dependency words are in thelist, and one concept is in S, then return.The final AMR concept would be stored at the topof the stack.
It is guaranteed that all the AMRexpressions can be derived from the dependencyparses by using the shift-reduce algorithm.3 Dependency Parses to AMR AlignerA preliminary step for our rewrite-based parser isthe alignment between the AMR and the depen-dency parse.
JAMR (Flanigan et al., 2014) pro-vides a heuristic aligner between an AMR conceptand the word or phrase of a sentence.
They usea set of aligner rules, like NE, fuzzy NE, data en-tity, etc., with a greedy strategy to match the align-ments.
This aligner achieves a 90% F1score onhand aligned AMR-sentence pairs.
On the otherhand, Pourdamghani et al.
(2014) present a gen-erative model to align from AMR graphs to sen-tence strings.
They raises concerns about the lackof sufficient data for learning derivation rules.
In-stead, they propose a string-to-string alignmentmodel, which transfers the AMR expression to alinearized string representation.
Then they useseveral IBM word alignment models (Brown etal., 1993) on this task.
IBM Model-4 with a sym-metric method reaches the highest F1score of83.1%.
Separately analyzing the alignments ofroles and non-roles (lexical leaf on AMR), the F1scores are 49.3% and 89.8%, respectively.In comparison to previous work, our aligner es-timates the alignments by learning the transforma-tion probability of lexical form, relations, namedentities and semantic roles features jointly.
Boththe alignment and transformation probabilities areinitialized for the training of parser.3.1 Our Aligner Model with EM AlgorithmOur approach, based on the existing IBM Model(Brown et al., 1993), is an AMR-to-Dependencyparse aligner, which represents one AMR as a listof Concepts C = ?c1, c2, .
.
.
, c|C|?, and the cor-responding dependency parse as a list of depen-dency word nodes D = ?d1, d2, .
.
.
, d|D|?.
ThealignmentA is a set of mapping functions a, whichlink Concept cjto dependency word node di, a :cj?
di.
Our model adopts an asymmetric EMapproach, instead of the standard symmetric one.We can always find the dependency label path be-tween any pair of dependency word nodes.
How-ever, the number of concept relation label paths isnot deterministic.
Thus, we select the alignmentdirection of AMR to dependency parse only, andone-to-one mapping, in our model.The objective function is to learn the parameter?
in the AMR-to-Dependency Parse of EM:?
= argmaxL?
(AMR|DEP )L?
(AMR|DEP ) =|S|?k=1?AP (C(k), A|D(k); t, q)where L?is the likelihood that we would like tomaximize, S is the training data set.
We willexplain the transformation probability t and thealignment probability q below.Expectation-StepThe E-Step estimates the likelihood of the inputAMR and dependency parse by giving the trans-formation probability t and alignment probabilityq.
The likelihood can be calculated using:P (A|C,D) =|C|?j=1P (cj|a(cj))P (cj|di, |C|, |D|) = t(cj|di) ?
q(di|cj, |C|, |D|)We would like to calculate all the probabilitiesof possible alignments A between cjand di.The transformation probability t is a combination(multiple) probability of several different features:?
Plemma(cj|di): the lemma probability is theprobability of the concept name of cj, condi-tioned on the dependency word of di.43?
Prel(Label(cj, cpj)|RelPathdep(a(cj), a(cpj))):the relation probability is the probability of therelation label between ciand its parent conceptcpi, given the relation path between the depen-dency word nodes a(ci) and a(cpi).
e.g., therelation probability of cj= 61 and a(cj) = 61in Fig.
2 is P (quant|npadvmod ?
num ?).?
PNE(Name(cj)|TypeNE(a(cj))): the NEprobability is the probability of the name of cj,given the NE type (e.g., PERSON, DATE, ORG,etc.)
contained by a(cj)).?
PSR(Label(cj, cpj)|Pred(a(cpj)), Arg(a(cj))):the semantic role probability is the probabilityof relation label between cjand its parent cpj,conditioned on the predicate word of a(cpj) andargument type of a(cj) if a(cj) is semanticargument of predicate a(cpj).On the other hand, the alignment probabilityq(Dist(a(cj), a(cpj))|cj, |C|, |D|) can be inter-preted as the probability of the distance betweena(cj) and a(cpj) on dependency parse D, condi-tioned on cj, the lengths of D and C.Maximization-StepIn the M-Step, the parameter ?ris updated fromthe previous round of ?r?1, in order to maximizethe likelihood L?
(AMR|DEP ):t(C|D;AMR,DEP ) =?
(AMR,DEP )cnt(C|D;AMR,DEP )?C?
(AMR,DEP )cnt(C|D;AMR,DEP )q(D|C;AMR,DEP ) =?
(AMR,DEP )cnt(D|C;AMR,DEP )?D?
(AMR,DEP )cnt(D|C;AMR,DEP )where cnt is the normalized count that is collectedfrom the accumulating probability of all possiblealignment from the E-step.
EM iterates the E-stepand M-step until convergence.InitializationBefore iterating, the transformation probability tand alignment probability q must be initialized.We use these steps to initialize the parameters:1.
Assign a fixed value, say 0.9, to Plemma(cj|di)if the concept name of cjis identical or a partialmatch to the dependency word node di.
Other-wise, initialize it uniformly;2.
Run the EM algorithm with the initializedPlemmaonly (Similar to IBM Model 1, whichis only concerned with translation probability);3.
Initialize all the other parameters, i.e., Prel,PNE, PSR, and q with the development data;4.
Run the completed EM algorithm with thePlemmawe obtained from Step 2 and other prob-abilities from Step 3.The extra EM for the initialization of Plemmais toestimate a more reasonable Plemma, and to speedup the convergence of the second round of EM.DecodingTo find the alignment of ?C,D?, we define thesearch for alignments as follows:argmaxAP (A|C,D)= argmaxA|C|?j=1t(cj|a(cj)) ?
q(a(cj)|cj, |C|, |D|)This decoding problem finds the alignmentA withthe maximum likelihood.
A dynamic program-ming (DP) algorithm is designed to extract the tar-get alignment without exhaustively searching allcandidate alignments, which will take O(|D||C|).This DP algorithm starts from the leaf conceptsand then walks through parent concepts.
In cj, weneed to produce the following likelihoods:1.
Accumulated likelihood for aligning to any difrom all the child concepts of cj2.
Likelihood of Plemmaand PNE3.
Likelihood of Preland PSRfor parent conceptcpjaligned to any dependency word node dl.In step (3), we need to find the dl, aligned by cpj,that maximizes the likelihood.
The accumulatedlikelihood is then stored in a list with size=|D|.
Wecan trace back and find the most likely alignmentsin the end.
The running time of this algorithmis O(|C||D|2).
This algorithm does not includereentrancy cases.
One solution to be explored infuture work is to use a beam-search algorithm in-stead.4 Preliminary Experiments and ResultsHere, we describe a preliminary experiment forthe AMR-Dependency Parse aligner, including thedata description, experimental setup, and results.4.1 DataThe LDC AMR release 1.0 consists of 13,051AMR-English sentence pairs1.
To match an AMR1LDC AMR release 1.0, Release date: June 16, 2014https://catalog.ldc.upenn.edu/LDC2014T1244Split Sent.
Tokens# ofNE# ofPred.# ofArgsTrain 1,000 19,923 1,510 4,231 7,739Dev.
100 2,328 239 235 526Test 100 1,672 80 199 445Table 3: The data split of train/dev./test set.
?# ofNE?, ?# of Pred.?
and ?# of Args?
stand for thenumber of named entities, predicate and argumentannotations in the data set, respectively.P R F1Plemma56.7 50.5 53.4Combination 61.1 53.4 57.0Table 4: Experiment Resultswith its corresponding dependency parse, we se-lect the sentences which appear in the OntoNotes5.0 release2as well, then randomly select 1,000 ofthem as our training set.
The OntoNotes data con-tains TreeBank, PB, and NE annotations.
Statis-tics about the AMR and OntoNotes corpus and thetrain/dev./test splits are given in Table 3.
We man-ually align the AMR concepts and dependencyword nodes in the dev.
and test sets.
We initial-ize Prel, PNE, and PSRwith the dev.
set.4.2 ResultsWe run our first round of EM (Step 2 in Initializa-tion of Sec.
3.1) for 100 iterations, then use thesecond round (Step 4 in Initialization of Sec.
3.1)for another 100 iterations.
We run our decoding al-gorithm and evaluation on the test set after the firstand second round of EM.
Due to time constraints,we did not train the q here.The experimental results are listed in Table 4.We evaluate the performance on the precision, re-call, and F1score.
Using just the Plemma(a simi-lar approach to (Pourdamghani et al., 2014)), weachieve 53.4% F1score on the test set.
On theother hand, our aligner reaches 57.0% F1scorewith the full aligner.5 Conclusion and Future WorkIn this research, we briefly introduce AMR.
Wedescribe the design principles and characteristicsof AMR, and show how the AMR parser taskis important, yet difficult.
We present the basicidea for a proposed AMR parser, based on theshift-reduce algorithm.
We also present an AMR-Dependency Parse aligner, because such an aligner2LDC OntoNotes Release 5.0, Release date: October 16,2013 https://catalog.ldc.upenn.edu/LDC2013T19will be a necessary first step before parsing.
Thealignment and the estimated feature probabilitiesare obtained by running the EM algorithm, whichcould be use directly for the AMR parser.In the future, we will be following these steps todevelop the proposed rewrite-based parser:Implemention of our rewrite-based AMRparser: We would like to implement the proposedrewrite-based AMR parser.
In comparison to theparser of Flanigan (2014) , we believe our parsercould perform better on the runtime.
We also planto experiment with the data generated by an auto-matic dependency parser.Expand the experimental data of aligner:One problem discovered in our preliminary ex-periments was that of data sparsity, especially forPlemma.
The LDC AMR Release contains 18,779AMR/English sentences, and 8,996 of them arecontained in the OntoNotes release as well.
There-fore, increasing the training data size from the re-lease is one solution to improve the performanceof our aligner from the unsatisfactory results.
Us-ing external lexical resources, like WordNet, is an-other promissing solution to extend to snyonyms.Evaluation of the aligner with existingparser: Since our aligner provides the align-ment between the dependency word node and boththe AMR leaf concept and role concept, we as-sume that our aligner could improve not only ourrewrite-based parser but other parsers as well.
Toverify this, we hope to submit our improved align-ment results to a state-of-the-art AMR parser, andevaluate the parsing results.AcknowledgmentsWe gratefully acknowledge the support of the Na-tional Science Foundation Grants IIS-1116782,A Bayesian Approach to Dynamic LexicalResources for Flexible Language Processing,0910992 IIS:RI: Richer Representations for Ma-chine Translation, and NSF IIA-0530118 PIRE(a subcontract from Johns Hopkins) for the 2014Frederick Jelinek Memorial Workshop for Mean-ing Representations in Language and SpeechProcessing, and funding under the BOLT andMachine Reading programs, HR0011-11-C-0145(BOLT) FA8750-09-C-0179 (M.R.).
Any opin-ions, findings, and conclusions or recommenda-tions expressed in this material are those of theauthors and do not necessarily reflect the views ofthe National Science Foundation.45ReferencesLaura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract meaning representationfor sembanking.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Comput.
Linguist., 19(2):263?311, June.Shu Cai and Kevin Knight.
2013.
Smatch: an evalua-tion metric for semantic feature structures.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 2: ShortPapers), pages 748?752.
Association for Computa-tional Linguistics.J.
Flanigan, S. Thomson, J. Carbonell, C. Dyer, andN.
A. Smith.
2014.
A discriminative graph-basedparser for the abstract meaning representation.
InProc.
of ACL, Baltimore, Maryland, June.
Associa-tion for Computational Linguistics.ArthurM.
Geoffrion.
2010.
Lagrangian relaxation forinteger programming.
In Michael Jnger, Thomas M.Liebling, Denis Naddef, George L. Nemhauser,William R. Pulleyblank, Gerhard Reinelt, GiovanniRinaldi, and Laurence A. Wolsey, editors, 50 Yearsof Integer Programming 1958-2008, pages 243?281.Springer Berlin Heidelberg.Martha Palmer, Dan Guildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?105, March.Nima Pourdamghani, Yang Gao, Ulf Hermjakob, andKevin Knight.
2014.
Aligning english strings withabstract meaning representation graphs.
In Proceed-ings of the 2014 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages425?429.
Association for Computational Linguis-tics.Chuan Wang, Xue Nianwen, and Pradhan Sameer.2015.
A transition-based algorithm for amr parsing.In Proceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies.Association for Computational Linguistics.46
