Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 81?86,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsDeveloping Methodology for Korean Particle Error DetectionMarkus DickinsonIndiana Universitymd7@indiana.eduRoss IsraelIndiana Universityraisrael@indiana.eduSun-Hee LeeWellesley Collegeslee6@wellesley.eduAbstractWe further work on detecting errors in post-positional particle usage by learners of Koreanby improving the training data and develop-ing a complete pipeline of particle selection.We improve the data by filtering non-Koreandata and sampling instances to better matchthe particle distribution.
Our evaluation showsthat, while the data selection is effective, thereis much work to be done with preprocessingand system optimization.1 IntroductionA growing area of research in analyzing learner lan-guage is to detect errors in function words, namelycategories such as prepositions and articles (see Lea-cock et al, 2010, and references therein).
This workhas mostly been for English, and there are issues,such as greater morphological complexity, in mov-ing to other languages (see, e.g., de Ilarraza et al,2008; Dickinson et al, 2010).
Our goal is to build amachine learning system for detecting errors in post-positional particles in Korean, a significant source oflearner errors (Ko et al, 2004; Lee et al, 2009b).Korean postpositional particles are morphemesthat attach to a preceding nominal to indicate a rangeof linguistic functions, including grammatical func-tions, e.g., subject and object; semantic roles; anddiscourse functions.
In (1), for instance, ka marksthe subject (function) and agent (semantic role).1Similar to English prepositions, particles can alsohave modifier functions, adding meanings of time,location, instrument, possession, and so forth.1We use the Yale Romanization scheme for writing Korean.
(1) Sumi-kaSumi-SBJJohn-uyJohn-GENcip-eysehouse-LOCku-lulhe-OBJtwutwosikan-ulhours-OBJkitaly-ess-ta.wait-PAST-END?Sumi waited for John for (the whole) two hours inhis house.
?We treat the task of particle error detection asone of particle selection, and we use machine learn-ing because it has proven effective in similar tasksfor other languages (e.g., Chodorow et al, 2007;Oyama, 2010).
Training on a corpus of well-formedKorean, we predict which particle should appear af-ter a given nominal; if this is different from thelearner?s, we have detected an error.
Using a ma-chine learner has the advantage of being able to per-form well without a researcher having to specifyrules, especially with the complex set of linguisticrelationships motivating particle selection.2We build from Dickinson et al (2010) in twomain ways: first, we implement a presence-selectionpipeline that has proven effective for English prepo-sition error detection (cf.
Gamon et al, 2008).
Asthe task is understudied, the work is preliminary, butit nonetheless is able to highlight the primary ar-eas of focus for future work.
Secondly, we improveupon the training data, in particular doing a betterjob of selecting relevant instances for the machinelearner.
Obtaining better-quality training data is amajor issue for machine learning applied to learnerlanguage, as the domain of writing is different fromnews-heavy training domains (Gamon, 2010).2See Dickinson and Lee (2009); de Ilarraza et al (2008);Oyama (2010) for related work in other languages.812 Particle error detection2.1 Pre-processingKorean is an agglutinative language: Korean words(referred to as ecels) are usually composed of aroot with a number of functional affixes.
We thusfirst segment and POS tag the text, for both train-ing and testing, using a hybrid (trigram + rule-based) morphological tagger for Korean (Han andPalmer, 2004).
The tagger is designed for nativelanguage and is not optimized to make guesses forill-formed input.
While the POS tags assigned to thelearner corpus are thus often incorrect (see Lee et al,2009a), there is the more primary problem of seg-mentation, as discussed in more detail in section 4.2.2 Machine learningWe use the Maximum Entropy Toolkit (Le, 2004)for machine learning.
Training on a corpus of well-formed Korean, we predict which particle should ap-pear after a given nominal; if this is different fromwhat the learner used, we have detected an error.
Itis important that the data represent the relationshipsbetween specific lexical items: in the comparableEnglish case, for example, interest is usually foundwith in: interest in/*with learning.Treating the ends of nominal elements as possibleparticle slots, we break classification into two steps:1) Is there a particle?
(Yes/No); and 2) What is theexact particle?
Using two steps eases the task of ac-tual particle prediction: with a successful classifica-tion of negative and positive instances, there is noneed to handle nominals that have no particle in step2.
To evaluate our parameters for obtaining the mostrelevant instances, we keep the task simple and per-form only step 1, as this step provides informationabout the usability of the training data.
For actualsystem performance, we evaluate both steps.In selecting features for Korean, we have to ac-count for relatively free word order (Chung et al,2010).
We follow our previous work (Dickinsonet al, 2010) in our feature choices, using a five-word window that includes the target stem and twowords on either side for context (see also Tetreaultand Chodorow, 2008).
Each word is broken downinto: stem, affixes, stem POS, and affixes POS.
Wealso have features for the preceding and followingnoun and verb, thereby approximating relevant se-lectional properties.
Although these are relativelyshallow features, they provide enough lexical andgrammatical context to help select better or worsetraining data (section 3) and to provide a basis for apreliminary system (section 4).3 Obtaining the most relevant instancesWe need well-formed Korean data in order to traina machine learner.
To acquire this, we use web-based corpora, as this allows us to find data similarto learner language, and using web as corpus (WaC)tools allows us to adjust parameters for new data(Dickinson et al, 2010).
However, the methodologyoutlined in Dickinson et al (2010) can be improvedin at least three ways, outlined next.3.1 Using sub-corporaWeb corpora can be built by searching for a set ofseed terms, extracting documents with those terms(Baroni and Bernardini, 2004).
One way to improvesuch corpora is to use better seeds, namely, thosewhich are: 1) domain-appropriate (e.g., about trav-eling), and 2) of an appropriate level.
In Dickinsonet al (2010), we show that basic terms result in poorquality Korean, but slightly more advanced terms onthe same topics result in better-formed data.Rather than use all of the seed terms to create asingle corpus, we divide the seed terms into 13 sep-arate sets, based on the individual topics from ourlearner corpus.
The sub-corpora are then combinedto create a cohesive corpus covering all the topics.For example, we use 10 Travel words to build asubcorpus, 10 Learning Korean words for a differ-ent subcorpus, and so forth.
This means that termsappropriate for one topic are not mixed with termsfor a different topic, ensuring more coherent webdocuments.
Otherwise, we might obtain a HealthManagement word, such as pyengwen (?hospital?
),mixed with a Generation Gap word, such as kaltung(?conflict?
)?in this case, leading to webpages onwar, a topic not represented in our learner corpus.3.2 FilteringOne difficulty with our web corpora is that some ofthem have large amounts of other languages alongwith Korean.
The keywords are in the corpora, butthere is additional text, often in Chinese, English, orJapanese.
These types of pages are unreliable for82our purposes, as they may not exhibit natural Ko-rean.
By using a simple filter, we check whether amajority of the characters in a webpage are indeedfrom the Korean writing system, and remove pagesbeneath a certain threshold.3.3 Instance samplingParticles are often dropped in colloquial and evenwritten Korean, whereas learners are more oftenrequired to use them.
It is not always the casethat the web pages contain the same ratio of par-ticles as learners are expected to use.
To alleviatethis over-weighting of having no particle attachedto a noun, we propose to downsample our corporafor the machine learning experiments, by remov-ing a randomly-selected proportion of (negative) in-stances.
Instance sampling has been effective forother NLP tasks, e.g., anaphora resolution (Wunschet al, 2009), when the number of negative instancesis much greater than the positive ones.
In our webcorpora, nouns have a greater than 50% chance ofhaving no particle; in section 3.4, we thus downsam-ple to varying amounts of negative instances fromabout 45% to as little as 10% of the total corpus.3.4 Training data selectionIn Dickinson et al (2010), we used a Korean learnerdata set from Lee et al (2009b) for development.
Itcontains 3198 ecels, 1842 of which are nominals,and 1271 (?70%) of those have particles.
We usethis same corpus for development, to evaluate filter-ing and down-sampling.
Evaluating on (yes/no) par-ticle presence, in tables 1 and 2, recall is the percent-age of positive instances we correctly find and pre-cision is the percentage of instances that we classifyas positive that actually are.
A baseline of alwaysguessing a particle gives 100% recall, 69% preci-sion, and 81.7% F-score.Table 1 shows the results of the MaxEnt systemfor step 1, using training data built for the topics inthe data with filter thresholds of 50%, 70%, 90%,and 100%?i.e., requiring that percentage of Koreancharacters?as well as the unfiltered corpus.
Thebest F-score is with the filter set at 90%, despite thesize of the filtered corpus being smaller than the fullcorpus.
Accordingly, we use the 90% filter on ourtraining corpus for the experiments described below.Threshold 100% 90% 70% 50% FullEcel 67k 9.6m 10.3m 11.1m 12.7mInstances 37k 5.8m 6.3m 7.1m 8.4mAccuracy 74.75 81.11 74.64 80.29 80.46Precision 80.03 86.14 79.65 85.41 85.56Recall 84.50 86.55 84.97 86.15 86.23F-score 82.20 86.34 82.22 85.78 85.89Table 1: Step 1 (particle presence) results with filtersThe results for instance sampling are given in ta-ble 2.
We experiment with positive to negative sam-pling ratios of 1.3/1 (?43% negative instances), 2/1(?33%), 4/1 (?20%), and 10/1 (?10%).
We selectthe 90% filter, 1.3/1 downsampling settings and ap-ply them to the training corpus (section 3.1) for allexperiments below.P/N ratio 10/1 4/1 2/1 1.3/1 1/1.05Instances 3.1m 3.5m 4.3m 5m 5.8mAccuracy 74.75 77.85 80.23 81.59 81.11Precision 73.38 76.72 80.75 84.26 86.14Recall 99.53 97.48 93.71 90.17 86.55F-score 84.47 85.86 86.74 87.12 86.34Table 2: Step 1 (presence) results with instance samplingOne goal has been to improve the web as corpuscorpus methodology for training a machine learningsystem.
The results in tables 1 and 2 reinforce ourearlier finding that size is not necessarily the mostimportant variable in determining the usefulness oroverall quality of data collected from the web forNLP tasks (Dickinson et al, 2010).
Indeed, the cor-pus producing best results (90% filter, 1.3:1 down-sampling) is more than 3 million instances smallerthan the unfiltered, unsampled corpus.4 Initial system evaluationWe have obtained an annotated corpus of 25 essaysfrom heritage intermediate learners,3 with 299 sen-tences and 2515 ecels (2676 ecels after correctingspacing errors).
There are 1138 nominals, with 93particle errors (5 added particles, 35 omissions, 53substitutions)?in other words, less than 10% of par-ticles are errors.
There are 979 particles after cor-rection.
We focus on 38 particles that intermediate3Heritage learners have had exposure to Korean at a youngage, such as growing up with Korean spoken at home.83students can be reasonably expected to use.
A parti-cle is one of three types (cf.
Nam and Ko, 2005): 1)case markers, 2) adverbials (cf.
prepositions), and3) auxiliary particles.4Table 3 gives the results for the entire system onthe test corpus, with separate results for each cat-egory of particle, (Case, Adv., and Aux.)
as wellas the concatenation of the three (All).
The ac-curacy presented here is in terms of only the par-ticle in question, as opposed to the full form ofroot+particle(s).
Step 2 is presented in 2 ways: Clas-sified, meaning that all of the instances classified asneeding a particle by step 1 are processed, or Gold,in which we rely on the annotation to determine par-ticle presence.
It is not surprising, then, that Goldexperiments are more accurate than Classified ex-periments, due to step 1 errors and also preprocess-ing issues, discussed next.Step 1 Step 2Data # Classified GoldCase 504 95.83% 71.23% 72.22%Adv.
205 82.43% 30.24% 32.68%Aux.
207 89.37% 31.41% 35.74%All 916 91.37% 53.05% 55.13%Table 3: Accuracy for step 1 (particle presence) & step 2(particle selection), with number (#) of instancesPreprocessing For the particles we examine, thereare 135 mis-segmented nominals.
The problem ismore conspicuous if we look at the entire corpus:the tagger identifies 1547 nominal roots, but thereare only 1138.
Some are errors in segmentation, i.e.,mis-identifying the proper root of the ecel, and someare problems with tagging the root, e.g., a nominalmistagged as a verb.
Table 4 provides results dividedby cases with only correctly pre-processed ecels andwhere the target ecel has been mis-handled by thetagger.
This checks whether the system particle iscorrect, ignoring whether the whole form is correct;if full-form accuracy is considered, we have no wayto get the 135 inaccurate cases correct.Error detection While our goal now is to estab-lish a starting point, the ultimate, on-going goal of4Full corpus details will be made available at: http://cl.indiana.edu/?particles/.Step 1 Step 2Data # Classified GoldAccurate 781 94.24% 55.95% 58.13%Inaccurate 135 74.81% 36.29% 38.51%Table 4: Overall accuracy divided by accurate and inac-curate preprocessingCase Adv.
Aux.
AllPrecision 28.82% 7.69% 5.51% 15.45%Recall 87.50% 100% 77.78% 88.00%Table 5: Error detection (using Gold step 1)this work is to develop a robust system for automati-cally detecting errors in learner data.
Thus, it is nec-essary to measure our performance at actually find-ing the erroneous instances extracted from our testcorpus.
Table 5 provides results for step 2 in termsof our ability to detect erroneous instances.
We re-port precision and recall, calculated as in figure 1.From the set of erroneous instances:True Positive (TP) ML class 6= student classFalse Negative (FN) ML class = student classFrom the set of correct instances:False Positive (FP) ML class 6= student classTrue Negative (TN) ML class = student classPrecision (P) TPTP+FPRecall (R) TPTP+FNFigure 1: Precision and recall for error detection4.1 Discussion and OutlookOne striking aspect about the results in table 3 is thegap in accuracy between case particles and the othertwo categories, particularly in step 2.
This points ata need to develop independent systems for each typeof particle, each relying on different types of linguis-tic information.
Auxiliary particles, for example, in-clude topic particles which?similar to English arti-cles (Han et al, 2006)?require discourse informa-tion to get correct.
Still, as case particles comprisemore than half of all particles in our corpus, the sys-tem is already potentially useful to learners.Comparing the rows in table 4, the dramatic dropin accuracy when moving to inaccurately-processed84cases shows a clear need for preprocessing adaptedto learner data.
While it is disconcerting that nearly15% (135/916) of the cases have no chance of re-sulting in a correct full form, the results indicate thatwe can obtain reliable accuracy (cf.
94.24%) for pre-dicting particle presence across all types of particles,assuming good morphological tagging.From table 5, it is apparent that we are overguess-ing errors; recall that only 10% of particles are er-roneous, whereas we more often guess a differentparticle.
While this tendency results in high recall,a tool for learners should have higher precision, sothat correct usage is not flagged.
However, this isa first attempt at error detection, and simply know-ing that precision is low means we can take stepsto solve this deficiency.
Our training data may havetoo many possible classes in it, and we have not yetaccounted for phonological alternations; e.g.
if thesystem guesses ul when lul is correct, we count amiss, even though they are different realizations ofthe same morpheme.To try and alleviate the over-prediction of errors,we have begun to explore implementing a confi-dence filter.
As a first pass, we use a simple fil-ter that compares the probability of the best parti-cle to the probability of the particle the learner pro-vided; the absolute difference in probabilities mustbe above a certain threshold.
Table 6 provides the er-ror detection results for each type of particle, incor-porating confidence filters of 10%, 20%, 30%, 40%,50%, and 60%.
The results show that increasing thethreshold at which we accept the classifier?s answercan significantly increase precision, at the cost of re-call.
As noted above, higher precision is desirable,so we plan on further developing this confidence fil-ter.
We may also include heuristic-based filters, suchas the ones implemented in Criterion (see Leacocket al, 2010), as well as a language model approach(Gamon et al, 2008).Finally, we are currently working on improvingthe POS tagger, testing other taggers in the pro-cess, and developing optimal feature sets for differ-ent kinds of particles.AcknowledgmentsWe would like to thank the IU CL discussion groupand Joel Tetreault for feedback at various points.Adv Aux Case All10% P 10.0% 6.3% 29.9% 16.3%R 100% 77.8% 67.8% 73.3%20% P 13.5% 7.8% 32.6% 18.0%R 100% 77.8% 50.0% 60.0%30% P 20.0% 8.3% 36.1% 20.8%R 100% 66.7% 39.3% 50.7%40% P 19.4% 14.3% 48.6% 26.9%R 60.0% 66.7% 30.4% 38.7%50% P 23.1% 16.7% 57.9% 32.1%R 30.0% 44.4% 19.6% 24.0%60% P 40.0% 26.7% 72.3% 45.2%R 20.0% 44.4% 14.3% 18.7%Table 6: Error detection with confidence filtersReferencesMarco Baroni and Silvia Bernardini.
2004.
Bootcat:Bootstrapping corpora and terms from the web.
InProceedings of LREC 2004, pages 1313?1316.Martin Chodorow, Joel Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involv-ing prepositions.
In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 25?30.Prague.Tagyoung Chung, Matt Post, and Daniel Gildea.2010.
Factors affecting the accuracy of koreanparsing.
In Proceedings of the NAACL HLT2010 First Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 49?57.Los Angeles, CA, USA.Arantza D?
?az de Ilarraza, Koldo Gojenola, andMaite Oronoz.
2008.
Detecting erroneous usesof complex postpositions in an agglutinative lan-guage.
In Proceedings of COLING-08.
Manch-ester.Markus Dickinson, Ross Israel, and Sun-Hee Lee.2010.
Building a korean web corpus for analyz-ing learner language.
In Proceedings of the 6thWorkshop on the Web as Corpus (WAC-6).
LosAngeles.Markus Dickinson and Chong Min Lee.
2009.
Mod-ifying corpus annotation to support the analysis oflearner language.
CALICO Journal, 26(3).Michael Gamon.
2010.
Using mostly native data85to correct errors in learners?
writing.
In HumanLanguage Technologies: The 2010 Annual Con-ference of the North American Chapter of theAssociation for Computational Linguistics, pages163?171.
Los Angeles, California.Michael Gamon, Jianfeng Gao, Chris Brockett,Alexander Klementiev, William Dolan, DmitriyBelenko, and Lucy Vanderwende.
2008.
Usingcontextual speller techniques and language mod-eling for esl error correction.
In Proceedings ofIJCNLP.
Hyderabad, India.Chung-Hye Han and Martha Palmer.
2004.
A mor-phological tagger for korean: Statistical taggingcombined with corpus-based morphological ruleapplication.
Machine Translation, 18(4):275?297.Na-Rae Han, Martin Chodorow, and Claudia Lea-cock.
2006.
Detecting errors in english article us-age by non-native speakers.
Natural LanguageEngineering, 12(2).S.
Ko, M. Kim, J. Kim, S. Seo, H. Chung, andS.
Han.
2004.
An analysis of Korean learner cor-pora and errors.
Hanguk Publishing Co.Zhang Le.
2004.
Maximum Entropy Mod-eling Toolkit for Python and C++.
URLhttp://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html.Claudia Leacock, Martin Chodorow, Michael Ga-mon, and Joel Tetreault.
2010.
Automated Gram-matical Error Detection for Language Learners.Synthesis Lectures on Human Language Tech-nologies.
Morgan & Claypool.Chong Min Lee, Soojeong Eom, and Markus Dick-inson.
2009a.
Towards analyzing korean learnerparticles.
Talk given at CALICO ?09 Pre-Conference Workshop on Automatic Analysis ofLearner Language.
Tempe, AZ.Sun-Hee Lee, Seok Bae Jang, and Sang kyu Seo.2009b.
Annotation of korean learner corpora forparticle error detection.
CALICO Journal, 26(3).Ki-shim Nam and Yong-kun Ko.
2005.
KoreanGrammar (phyocwun kwuke mwunpeplon).
TopPublisher, Seoul.Hiromi Oyama.
2010.
Automatic error detectionmethod for japanese particles.
Polyglossia, 18.Joel Tetreault and Martin Chodorow.
2008.
The upsand downs of preposition error detection in eslwriting.
In Proceedings of COLING-08.
Manch-ester.Holger Wunsch, Sandra Ku?bler, and RachaelCantrell.
2009.
Instance sampling methods forpronoun resolution.
In Proceedings of RANLP2009.
Borovets, Bulgaria.86
