Combining Deep and Shallow Approaches in Parsing GermanMichael SchiehlenInstitute for Computational Linguistics, University of Stuttgart,Azenbergstr.
12, D-70174 Stuttgartmike@adler.ims.uni-stuttgart.deAbstractThe paper describes two parsing schemes:a shallow approach based on machinelearning and a cascaded finite-state parserwith a hand-crafted grammar.
It dis-cusses several ways to combine them andpresents evaluation results for the two in-dividual approaches and their combina-tion.
An underspecification scheme forthe output of the finite-state parser is intro-duced and shown to improve performance.1 IntroductionIn several areas of Natural Language Processing, acombination of different approaches has been foundto give the best results.
It is especially rewarding tocombine deep and shallow systems, where the for-mer guarantees interpretability and high precisionand the latter provides robustness and high recall.This paper investigates such a combination consist-ing of an n-gram based shallow parser and a cas-caded finite-state parser1 with hand-crafted gram-mar and morphological checking.
The respectivestrengths and weaknesses of these approaches arebrought to light in an in-depth evaluation on a tree-bank of German newspaper texts (Skut et al, 1997)containing ca.
340,000 tokens in 19,546 sentences.The evaluation format chosen (dependency tuples)is used as the common denominator of the systems1Although not everyone would agree that finite-stateparsers constitute a ?deep?
approach to parsing, they still areknowledge-based, require efforts of grammar-writing, a com-plex linguistic lexicon, manage without training data, etc.in building a hybrid parser with improved perfor-mance.
An underspecification scheme allows thefinite-state parser partially ambiguous output.
It isshown that the other parser can in most cases suc-cessfully disambiguate such information.Section 2 discusses the evaluation format adopted(dependency structures), its advantages, but alsosome of its controversial points.
Section 3 formu-lates a classification problem on the basis of theevaluation format and applies a machine learner toit.
Section 4 describes the architecture of the cas-caded finite-state parser and its output in a novelunderspecification format.
Section 5 explores sev-eral combination strategies and tests them on severalvariants of the two base components.
Section 6 pro-vides an in-depth evaluation of the component sys-tems and the hybrid parser.
Section 7 concludes.2 Parser EvaluationThe simplest method to evaluate a parser is to countthe parse trees it gets correct.
This measure is, how-ever, not very informative since most applications donot require one hundred percent correct parse trees.Thus, an important question in parser evaluation ishow to break down parsing results.In the PARSEVAL evaluation scheme (Black etal., 1991), partially correct parses are gauged by thenumber of nodes they produce and have in com-mon with the gold standard (measured in precisionand recall).
Another figure (crossing brackets) onlycounts those incorrect nodes that change the partialorder induced by the tree.
A problematic aspect ofthe PARSEVAL approach is that the weight given toparticular constructions is again grammar-specific,since some grammars may need more nodes to de-scribe them than others.
Further, the approach doesnot pay sufficient heed to the fact that parsing de-cisions are often intricately twisted: One wrong de-cision may produce a whole series of other wrongdecisions.Both these problems are circumvented whenparsing results are evaluated on a more abstractlevel, viz.
dependency structure (Lin, 1995).Dependency structure generally follows predicate-argument structure, but departs from it in that thebasic building blocks are words rather than predi-cates.
In terms of parser evaluation, the first propertyguarantees independence of decisions (every link isrelevant also for the interpretation level), while thesecond property makes for a better empirical justifi-cation.
for evaluation units.
Dependency structurecan be modelled by a directed acylic graph, withword tokens at the nodes.
In labelled dependencystructure, the links are furthermore classified into acertain set of grammatical roles.Dependency can be easily determined from con-stituent structure if in every phrase structure rulea constituent is singled out as the head (Gaifman,1965).
To derive a labelled dependency structure, allnon-head constituents in a rule must be labelled withthe grammatical role that links their head tokens tothe head token of the head constituent.There are two cases where the divergence be-tween predicates and word tokens makes trouble: (1)predicates expressed by more than one token, and(2) predicates expressed by no token (as they occurin ellipsis).
Case 1 frequently occurs within the verbcomplex (of both English and German).
The solu-tion proposed in the literature (Black et al, 1991;Lin, 1995; Carroll et al, 1998; K?bler and Telljo-hann, 2002) is to define a normal form for depen-dency structure, where every adjunct or argumentattaches to some distinguished part of the verb com-plex.
The underlying assumption is that those caseswhere scope decisions in the verb complex are se-mantically relevant (e.g.
with modal verbs) are notresolvable in syntax anyway.
There is no generallyaccepted solution for case 2 (ellipsis).
Most authorsin the evaluation literature neglect it, perhaps dueto its infrequency (in the NEGRA corpus, ellipsisonly occurs in 1.2% of all dependency relations).Robinson (1970, 280) proposes to promote one ofthe dependents (preferably an obligatory one) (1a)or even all dependents (1b) to head status.
(1) a. the very braveb.
John likes tea and Harry coffee.A more sweeping solution to these problems is toabandon dependency structure at all and directlygo for predicate-argument structure (Carroll et al,1998).
But as we argued above, moving to amore theoretical level is detrimental to comparabil-ity across grammatical frameworks.3 A Direct Approach: LearningDependency StructureAccording to the dependency structure approach toevaluation, the task of the parser is to find the cor-rect dependency structure for a string, i.e.
to as-sociate every word token with pairs of head tokenand grammatical role or else to designate it as inde-pendent.
To make the learning task easier, the num-ber of classes should be reduced as much as possi-ble.
For one, the task could be simplified by focus-ing on unlabelled dependency structure (measuredin ?unlabelled?
precision and recall (Eisner, 1996;Lin, 1995)), which is, however, in general not suffi-cient for further semantic processing.3.1 Tree PropertyAnother possibility for reduction is to associate ev-ery word with at most one pair of head token andgrammatical role, i.e.
to only look at dependencytrees rather than graphs.
There is one case wherethe tree property cannot easily be maintained: co-ordination.
Conceptually, all the conjuncts are headconstituents in coordination, since the conjunctioncould be missing, and selectional restrictions workon the individual conjuncts (2).
(2) John ate (fish and chips|*wish and ships).But if another word depends on the conjoined heads(see (4a)), the tree property is violated.
A way outof the dilemma is to select a specific conjunct asmodification site (Lin, 1995; K?bler and Telljohann,2002).
But unless care is taken, semantically vi-tal information is lost in the process: Example (4)shows two readings which should be distinguishedin dependency structure.
A comparison of the tworeadings shows that if either the first conjunct orthe last conjunct is unconditionally selected certainreadings become undistinguishable.
Rather, in or-der to distinguish a maximum number of readings,pre-modifiers must attach to the last conjunct andpost-modifiers and coordinating conjunctions to thefirst conjunct2 .
The fact that the modifier refers toa conjunction rather than to the conjunct is recordedin the grammatical role (by adding c to it).
(4) a. the [fans and supporters] of Arsenalb.
[the fans] and [supporters of Arsenal]Other constructions contradicting the tree propertyare arguably better treated in the lexicon anyway(e.g.
control verbs (Carroll et al, 1998)) or couldbe solved by enriching the repertory of grammati-cal roles (e.g.
relative clauses with null relative pro-nouns could be treated by adding the dependency re-lation between head verb and missing element to theone between head verb and modified noun).In a number of linguistic phenomena, dependencytheorists disagree on which constituent should bechosen as the head.
A case in point are PPs.
Fewgrammars distinguish between adjunct and subcate-gorized PPs at the level of prepositions.
In predicate-argument structure, however, the embedded NP isin one case related to the preposition, in the otherto the subcategorizing verb.
Accordingly, some ap-proaches take the preposition to be the head of a PP(Robinson, 1970; Lin, 1995), others the NP (K?blerand Telljohann, 2002).
Still other approaches (Tes-ni?re, 1959; Carroll et al, 1998) conflate verb,preposition and head noun into a triple, and thusonly count content words in the evaluation.
Forlearning, the matter can be resolved empirically:2Even in this setting some readings cannot be distinguished(see e.g.
(3) where a conjunction of three modifiers wouldbe retrieved).
Nevertheless, the proposed scheme fails in only0.0017% of all dependency tuples.
(3) In New York, we never meet, but in Boston.Note that by this move we favor interpretability over projectiv-ity, but example (4a) is non-projective from the start.Taking prepositions as the head somewhat improvesperformance, so we took PPs to be headed by prepo-sitions.3.2 Encoding Head TokensAnother question is how to encode the head to-ken.
The simplest method, encoding the word by itsstring position, generates a large space of classes.
Amore efficient approach uses the distance in stringposition between dependent and head token.
Finally,Lin (1995) proposes a third type of representation:In his work, a head is described by its word type, anindication of the direction from the dependent (leftor right) and the number of tokens of the same typethat lie between head and dependent.
An illustrativerepresentation would be ?paperwhich refers to thesecond nearest token paper to the right of the cur-rent token.
Obviously there are far too many wordtokens, but we can use Part-Of-Speech tags instead.Furthermore information on inflection and type ofnoun (proper versus common nouns) is irrelevant,which cuts down the size even more.
We will callthis approach nth-tag.
A further refinement of thenth-tag approach makes use of the fact that depen-dency structures are acylic.
Hence, only those wordswith the same POS tag as the head between depen-dent and head must be counted that do not dependdirectly or indirectly on the dependent.
We will callthis approach covered-nth-tag.pos dist nth-tag coverlabelled 1,924 1,349 982 921unlabelled 97 119 162 157Figure 1: Number of Classes in NEGRA TreebankFigure 1 shows the number of classes the individ-ual approaches generate on the NEGRA Treebank.Note that the longest sentence has 115 tokens (withpunctuation marks) but that punctuation marks donot enter dependency structure.
The original tree-bank exhibits 31 non-head syntactic3 grammaticalroles.
We added three roles for marker comple-ments (CMP), specifiers (SPR), and floating quanti-fiers (NK+), and subtracted the roles for conjunctionmarkers (CP) and coreference with expletive (RE).3i.e.
grammatical roles not merely used for tokenization22 roles were copied to mark reference to conjunc-tion.
Thus, all in all there was a stock of 54 gram-matical roles.3.3 ExperimentsWe used   -grams (3-grams and 5-grams) of POStags as context and C4.5 (Quinlan, 1993) for ma-chine learning.
All results were subjected to 10-foldcross validation.The learning algorithm always returns a result.We counted a result as not assigned, however, if itreferred to a head token outside the sentence.
SeeFigure 2 for results4 of the learner.
The left columnshows performance with POS tags from the treebank(ideal tags, I-tags), the right column values obtainedwith POS tags as generated automatically by a tag-ger with an accuracy of 95% (tagger tags, T-tags).I-tags T-tagsF-val prec rec F-val prec recdist, 3 .6071 .6222 .5928 .5902 .6045 .5765dist, 5 .6798 .6973 .6632 .6587 .6758 .6426nth-tag, 3 .7235 .7645 .6866 .6965 .7364 .6607nth-tag, 5 .7716 .7961 .7486 .7440 .7682 .7213cover, 3 .7271 .7679 .6905 .7009 .7406 .6652cover, 5 .7753 .7992 .7528 .7487 .7724 .7264Figure 2: Results for C4.5The nth-tag head representation outperforms thedistance representation by 10%.
Consideringacyclicity (cover) slightly improves performance,but the gain is not statistically significant (t-test with99%).
The results are quite impressive as they stand,in particular the nth-tag 5-gram version seems toachieve quite good results.
It should, however, bestressed that most of the dependencies correctly de-termined by the n-gram methods extend over nomore than 3 tokens.
With the distance method, such?short?
dependencies make up 98.90% of all depen-dencies correctly found, with the nth-tag methodstill 82%, but only 79.63% with the finite-stateparser (see section 4) and 78.91% in the treebank.4If the learner was given a chance to correct its errors, i.e.if it could train on its training results in a second round, therewas a statistically significant gain in F-value with recall risingand precision falling (e.g.
F-value .7314, precision .7397, recall.7232 for nth-tag trigrams, and F-value .7763, precision .7826,recall .7700 for nth-tag 5-grams).4 Cascaded Finite-State ParserIn addition to the learning approach, we used a cas-caded finite-state parser (Schiehlen, 2003), to extractdependency structures from the text.
The layoutof this parser is similar to Abney?s parser (Abney,1991): First, a series of transducers extracts nounchunks on the basis of tokenized and POS-taggedtext.
Since center-embedding is frequent in Germannoun phrases, the same transducer is used severaltimes over.
It also has access to inflectional informa-tion which is vital for checking agreement and deter-mining case for subsequent phases (see (Schiehlen,2002) for a more thorough description).
Second, aseries of transducers extracts verb-final, verb-first,and verb-second clauses.
In contrast to Abney, theseare full clauses, not just simplex clause chunks, sothat again recursion can occur.
Third, the result-ing parse tree is refined and decorated with gram-matical roles, using non-deterministic ?interpreta-tion?
transducers (the same technique is used byAbney (1991)).
Fourth, verb complexes are exam-ined to find the head verb and auxiliary passive orraising verbs.
Only then subcategorization framescan be checked on the clause elements via a non-deterministic transducer, giving them more specificgrammatical roles if successful.
Fifth, dependencytuples are extracted from the parse tree.4.1 UnderspecificationSome parsing decisions are known to be not resolv-able by grammar.
Such decisions are best handedover to subsequent modules equipped with the rel-evant knowledge.
Thus, in chart parsing, an under-specified representation is constructed, from whichall possible analyses can be easily and efficientlyread off.
Elworthy et al (2001) describe a cascadedparser which underspecifies PP attachment by allow-ing modifiers to be linked to several heads in a de-pendency tree.
Example (5) illustrates this scheme.
(5) I saw a man in a car on the hill.The main drawback of this scheme is its overgener-ation.
In fact, it allows six readings for example (5),which only has five readings (the speaker could nothave been in the car, if the man was asserted to beon the hill).
A similar clause with 10 PPs at theend would receive 39,916,800 readings rather than58,786.
So a more elaborate scheme is called for,but one that is just as easy to generate.A device that often comes in handy for under-specification are context variables (Maxwell III andKaplan, 1989; D?rre, 1997).
First let us give everysequence of prepositional phrases in every clause aspecific name (e.g.
1B for the second sequence inthe first clause).
Now we generate the ambiguousdependency relations (like (Elworthy et al, 2001))but label them with context variables.
Such contextvariables consist of the sequence name   , a num-ber  designating the dependent in left-to-right or-der (e.g.
0 for in, 1 for on in example (5)), and anumber  designating the head in left-to-right (e.g.0 for saw, 1 for man, 2 for hill in (5)).
If the linksare stored with the dependents, the number  can beleft implicit.
Generation of such a representation isstraightforward and, in particular, does not lead to ahigher class of complexity of the full system.
Ex-ample (6) shows a tuple representation for the twoprepositions of sentence (5).
(6) in [1A00] saw ADJ, [1A01] man ADJon [1A10] saw ADJ, [1A11] man ADJ,[1A12] car ADJIn general, a dependent  can modify  heads,viz.
the heads numbered 	 .
Now weput the following constraint on resolution: A depen-dent  can only modify a head  if no previousdependent  which could have attached to  (i.e. ) chose some head   to the left of  rather than  .
The condition is formally expressedin (7).
In example (6) there are only two dependents( ff in, fifl on).
If in attaches to saw, on cannotattach to a head between saw and in; conversely ifon attaches to man, in cannot attach to a head beforeman.
Nothing follows if on attaches to car.
(7) Constraint: ffi    !"$#   %&#'(*)+fi&#),-.0/ for all PP sequences The cascaded parser described adopts this under-specification scheme for right modification.
Leftmodification (see (8)) is usually not stacked so thesimpler scheme of Elworthy et al (2001) suffices.
(8) They are usually competent people.German is a free word order language, so that sub-categorization can be ambiguous.
Such ambiguitiesshould also be underspecified.
Again we introduce acontext variable   for every ambiguous subcatego-rization frame (e.g.
1 in (9)) and count the individualreadings 1 (with letters a,b in (9)).
(9) Peter kennt Karl.
(Peter knows Karl / Karlknows Peter.
)Peter kennt [1a] SBJ/[1b] OAkennt TOPKarl kennt [1a] OA/[1b] SBJSince subcategorization ambiguity interacts with at-tachment ambiguity, context variables sometimesneed to be coupled: In example (10) the attachmentambiguity only occurs if the PP is read as adjunct.
(10) Karl f?gte einige Gedanken zu dem Werkhinzu.
(Karl added some thoughts on/to thework.
)Gedanken f?gte [1a] OA/[1b] OAzu [1A0] f?gte [1a] PP:zu/[1b] ADJ[1A1] Gedanken PP:zu1A1 < 1b4.2 Evaluation of the UnderspecifiedRepresentationIn evaluating underspecified representations,Riezler et al (2002) distinguish upper and lowerbound, standing for optimal performance in disam-biguation and average performance, respectively.
InI-tags T-tagsF-val prec rec F-val prec recupper .8816 .9137 .8517 .8377 .8910 .7903direct .8471 .8779 .8183 .8073 .8588 .7617lower .8266 .8567 .7986 .7895 .8398 .7449Figure 3: Results for Cascaded ParserFigure 3, values are also given for the performanceof the parser without underspecification, i.e.
alwaysfavoring maximal attachment and word order with-out scrambling (direct).
Interestingly this methodperforms significantly better than average, an effectmainly due to the preference for high attachment.5 Combining the ParsersWe considered several strategies to combine the re-sults of the diverse parsing approaches: simple vot-ing, weighted voting, Bayesian learning, MaximumEntropy, and greedy optimization of F-value.Simple Voting.
The result predicted by the ma-jority of base classifiers is chosen.
The finite-stateparser, which may give more than one result, dis-tributes its vote evenly on the possible readings.Weighted Voting.
In weighted voting, the resultwhich gets the most votes is chosen, where the num-ber of votes given to a base classifier is correlatedwith its performance on a training set.Bayesian Learning.
The Bayesian approach ofXu et al (1992) chooses the most probable predic-tion.
The probability of a prediction   is computedby the product       / of the probability of  given the predictions  made by the individual baseclassifiers 	 .
The probability      / of a correctprediction    given a learned prediction    is ap-proximated by relative frequency in a training set.Maximum Entropy.
Combining the results canalso be seen as a classification task, with base pre-dictions added to the original set of features.
Weused the Maximum Entropy approach5 (Berger etal., 1996) as a machine learner for this task.
Un-derspecified features were assigned multiple values.Greedy Optimization of F-value.
Anothermethod uses a decision list of prediction?classifierpairs to choose a prediction by a classifier.
The listis obtained by greedy optimization: In each step,the prediction?classifier pair whose addition resultsin the highest gain in F-value for the combinedmodel on the training set is appended to the list.The algorithm terminates when F-value cannot beimproved by any of the remaining candidates.
Afiner distinction is possible if the decision is madedependent on the POS tag as well.
For greedyoptimization, the predictions of the finite-stateparser were classified only in grammatical roles, nothead positions.
We used 10-fold cross validation todetermine the decision lists.5More specifically, the OpenNLP implementation(http://maxent.sourceforge.net/) was used with 10 iterationsand a cut-off frequency for features of 10.F-val prec recsimple voting .7927 .8570 .7373weighted voting .8113 .8177 .8050Bayesian learning .8463 .8509 .8417Maximum entropy .8594 .8653 .8537greedy optim .8795 .8878 .8715greedy optim+tag .8849 .8957 .8743Figure 4: Combination StrategiesWe tested the various combination strategies forthe combination Finite-State parser (lower bound)and C4.5 5-gram nth-tag on ideal tags (results in Fig-ure 4).
Both simple and weighted voting degradethe results of the base classifiers.
Greedy optimiza-tion outperforms all other strategies.
Indeed it comesnear the best possible choice which would give anF-score of .9089 for 5-gram nth-tag and finite-stateparser (upper bound) (cf.
Figure 5).without POS tag with POS tagI-tags F-val prec rec F-val prec recupp, nth 5 .9008 .9060 .8956 .9068 .9157 .8980low, nth 5 .8795 .8878 .8715 .8849 .8957 .8743low, dist 5 .8730 .8973 .8499 .8841 .9083 .8612low, nth 3 .8722 .8833 .8613 .8773 .8906 .8644low, dist 3 .8640 .9034 .8279 .8738 .9094 .8410dir, nth 5 .8554 .8626 .8483 .8745 .8839 .8653Figure 5: Combinations via OptimizationFigure 5 shows results for some combinationswith the greedy optimization strategy on ideal tags.All combinations listed yield an improvement ofmore than 1% in F-value over the base classifiers.It is striking that combination with a shallow parserdoes not help the Finite-State parser much in cov-erage (upper bound), but that it helps both in dis-ambiguation (pushing up the lower bound to almostthe level of upper bound) and robustness (remedy-ing at least some of the errors).
The benefit of un-derspecification is visible when lower bound and di-rect are compared.
The nth-tag 5-gram method wasthe best method to combine the finite-state parserwith.
Even on T-tags, this combination achieved anF-score of .8520 (lower, upper: .8579, direct: .8329)without POS tag and an F-score of .8563 (lower, up-per: .8642, direct: .8535) with POS tags.6 In-Depth EvaluationFigure 6 gives a survey of the performance of theparsing approaches relative to grammatical role.These figures are more informative than overall F-score (Preiss, 2003).
The first column gives thename of the grammatical role, as explained below.The second column shows corpus frequency in per-cent.
The third column gives the standard devia-tion of distance between dependent and head.
Thethree last columns give the performance (recall) ofC4.5 with distance representation and 5-grams, C4.5with nth-tag representation and 5-grams, and thecascaded finite-state parser, respectively.
For thefinite-state parser, the number shows performancewith optimal disambiguation (upper bound) and, ifthe grammatical role allows underspecification, thenumber for average disambiguation (lower bound)in parentheses.Relations between function words and contentwords (e.g.
specifier (SPR), marker complement(CMP), infinitival zu marker (PM)) are frequent andeasy for all approaches.
The cascaded parser has anedge over the learners with arguments (subject (SB),clausal (OC), accusative (OA), second accusative(OA2), genitive (OG), dative object (DA)).
For allthese argument roles a slight amount of ambigu-ity persists (as can be seen from the divergence be-tween upper and lower bound), which is due to freeword order.
No ambiguity is found with reportedspeech (RS).
The cascaded parser also performsquite well where verb complexes are concerned(separable verb prefix (SVP), governed verbs (OC),and predicative complements (PD, SP)).
Anotherclearly discernible complex are adjuncts (modifier(MO), negation (NG), passive subject (SBP); one-place coordination (JUnctor) and discourse markers(DM); finally postnominal modifier (MNR), geni-tive (GR), or von-phrase (PG)), which all exhibit at-tachment ambiguities.
No attachment ambiguitiesare attested for prenominal genitives (GL).
Sometypes of adjunction have not yet been implementedin the cascaded parser, so that it performs badly onthem (e.g.
relative clauses (RC), which are usu-ally extraposed to the right (average distance is -11.6) and thus quite difficult also for the learn-ers; comparative constructions (CC, CM), measurephrases (AMS), floating quantifiers (NK+)).
Attach-ment ambiguities also occur with appositions (APP,NK6).
Notoriously difficult is coordination (attach-role freq dev dist nth-t FS-parserMO 24.922 4.5 65.4 75.2 86.9(75.7)SPR 14.740 1.0 97.4 98.5 99.4CMP 13.689 2.7 83.4 93.4 98.7SB 9.682 5.7 48.4 64.7 84.5(82.6)TOP 7.781 0.0 47.6 46.7 49.8OC 4.859 7.4 43.9 85.1 91.9(91.2)OA 4.594 5.8 24.1 37.7 83.5(70.6)MNR 3.765 2.8 76.2 73.9 89.0(48.1)CD 2.860 4.6 67.7 74.8 77.4GR 2.660 1.3 66.9 65.6 95.0(92.8)APP 2.480 3.4 72.6 72.5 81.6(77.4)PD 1.657 4.6 31.3 39.7 55.1RC 0.899 5.8 5.5 1.6 19.1c 0.868 7.8 13.1 13.3 34.4(26.1)SVP 0.700 5.8 29.2 96.0 97.4DA 0.693 5.4 1.9 1.8 77.1(71.9)NG 0.672 3.1 63.1 73.8 81.7(70.7)PM 0.572 0.0 99.7 99.9 99.2PG 0.381 1.5 1.9 1.4 94.9(53.2)JU 0.304 4.6 35.8 47.3 62.1(45.5)CC 0.285 4.4 22.3 20.9 4.0( 3.1)CM 0.227 1.4 85.8 85.8 0GL 0.182 1.1 70.3 67.2 87.5SBP 0.177 4.1 4.7 3.6 93.7(77.0)AC 0.110 2.5 63.9 60.6 91.9AMS 0.078 0.7 63.6 60.5 1.5( 0.9)RS 0.076 8.9 0 0 25.0NK 0.020 3.4 0 0 46.2(40.4)OG 0.019 4.5 0 0 57.4(54.3)DM 0.017 3.1 9.1 18.2 63.6(59.1)NK+ 0.013 3.3 16.1 16.1 0VO 0.010 3.2 50.0 25.0 0OA2 0.005 5.7 0 0 33.3(29.2)SP 0.004 7.0 0 0 55.6(33.3)Figure 6: Grammatical Rolesment of conjunction to conjuncts (CD), and depen-dency on multiple heads (c)).
Vocatives (VO) arenot treated in the cascaded parser.
AC is the relationbetween parts of a circumposition.6Other relations classified as NK in the original tree-bank have been reclassified: prenominal determiners to SPR,prenominal adjective phrases to MO.7 ConclusionThe paper has presented two approaches to Germanparsing (n-gram based machine learning and cas-caded finite-state parsing), and evaluated them onthe basis of a large amount of data.
A new represen-tation format has been introduced that allows under-specification of select types of syntactic ambiguity(attachment and subcategorization) even in the ab-sence of a full-fledged chart.
Several methods havebeen discussed for combining the two approaches.It has been shown that while combination with theshallow approach can only marginally improve per-formance of the cascaded parser if ideal disambigua-tion is assumed, a quite substantial rise is registeredin situations closer to the real world where POS tag-ging is deficient and resolution of attachment andsubcategorization ambiguities less than perfect.In ongoing work, we look at integrating a statis-tic context-free parser called BitPar, which was writ-ten by Helmut Schmid and achieves .816 F-score onNEGRA.
Interestingly, the performance goes up to.9474 F-score when BitPar is combined with the FSparser (upper bound) and .9443 for the lower bound.So at least for German, combining parsers seems tobe a pretty good idea.
Thanks are due to HelmutSchmid and Prof. C. Rohrer for discussions, and tothe reviewers for their detailed comments.ReferencesSteven Abney.
1991.
Parsing by Chunks.
In Robert C.Berwick, Steven P. Abney, and Carol Tenny, editors,Principle-based Parsing: computation and psycholin-guistics, pages 257?278.
Kluwer, Dordrecht.Adam Berger, Stephen Della Pietra, and VincentDella Pietra.
1996.
A maximum entropy approach tonatural language processing.
Computational Linguis-tics, 22(1):39?71, March.E.
Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A procedurefor quantitatively comparing the syntactic coverageof English grammars.
In Proceedings of the DARPASpeech and Natural Language Workshop 1991, PacificGrove, CA.John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998.Parser Evaluation: a Survey and a New Proposal.
InProceedings of LREC, pages 447?454, Granada.Jochen D?rre.
1997.
Efficient Construction of Un-derspecified Semantics under Massive Ambiguity.ACL?97, pages 386?393, Madrid, Spain.Jason M. Eisner.
1996.
Three new probabilistic mod-els for dependency parsing: An exploration.
COLING?96, Copenhagen.David Elworthy, Tony Rose, Amanda Clare, and AaronKotcheff.
2001.
A natural language system for re-trieval of captioned images.
Journal of Natural Lan-guage Engineering, 7(2):117?142.Haim Gaifman.
1965.
Dependency Systems andPhrase-Structure Systems.
Information and Control,8(3):304?337.Sandra K?bler and Heike Telljohann.
2002.
Towardsa Dependency-Oriented Evaluation for Partial Parsing.In Beyond PARSEVAL ?
Towards Improved EvaluationMeasures for Parsing Systems (LREC Workshop).Dekang Lin.
1995.
A Dependency-based Method forEvaluating Broad-Coverage Parsers.
In Proceedingsof the IJCAI-95, pages 1420?1425, Montreal.John T. Maxwell III and Ronald M. Kaplan.
1989.
Anoverview of disjunctive constraint satisfaction.
In Pro-ceedings of the International Workshop on ParsingTechnologies, Pittsburgh, PA.Judita Preiss.
2003.
Using Grammatical Relations toCompare Parsers.
EACL?03, Budapest.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, CA.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell III, and Mark John-son.
2002.
Parsing the Wall Street Journal using aLexical-Functional Grammar and Discriminative Esti-mation Techniques.
ACL?02, Philadelphia.Jane J. Robinson.
1970.
Dependency Structures andTransformational Rules.
Language, 46:259?285.Michael Schiehlen.
2002.
Experiments in German NounChunking.
COLING?02, Taipei.Michael Schiehlen.
2003.
A Cascaded Finite-StateParser for German.
Research Note in EACL?03.Wojciech Skut, Brigitte Krenn, Thorsten Brants, andHans Uszkoreit.
1997.
An Annotation Scheme forFree Word Order Languages.
ANLP-97, Washington.Lucien Tesni?re.
1959.
Elements de syntaxe structurale.Librairie Klincksieck, Paris.Lei Xu, Adam Krzyzak, and Ching Y. Suen.
1992.
Sev-eral Methods for Combining Multiple Classifiers andTheir Applications in Handwritten Character Recog-nition.
IEEE Trans.
on System, Man and Cybernetics,SMC-22(3):418?435.
