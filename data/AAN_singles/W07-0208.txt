TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 53?60,Rochester, April 2007 c?2007 Association for Computational LinguisticsLearning to Transform Linguistic GraphsValentin Jijkoun and Maarten de RijkeISLA, University of AmsterdamKruislaan 403, 1098 SJ Amsterdam, The Netherlandsjijkoun,mdr@science.uva.nlAbstractWe argue in favor of the the use of la-beled directed graph to represent varioustypes of linguistic structures, and illustratehow this allows one to view NLP tasks asgraph transformations.
We present a gen-eral method for learning such transforma-tions from an annotated corpus and de-scribe experiments with two applicationsof the method: identification of non-localdepenencies (using Penn Treebank data)and semantic role labeling (using Propo-sition Bank data).1 IntroductionAvailability of linguistically annotated corpora suchas the Penn Treebank (Bies et al, 1995), PropositionBank (Palmer et al, 2005), and FrameNet (John-son et al, 2003) has stimulated much research onmethods for automatic syntactic and semantic anal-ysis of text.
Rich annotations of corpora has al-lowed for the development of techniques for recov-ering deep linguistic structures: syntactic non-localdependencies (Johnson, 2002; Hockenmaier, 2003;Dienes, 2004; Jijkoun and de Rijke, 2004) and se-mantic arguments (Gildea, 2001; Pradhan et al,2005; Toutanova et al, 2005; Giuglea andMoschitti,2006).
Most state-of-the-art methods for the lattertwo tasks use a cascaded architecture: they employsyntactic parsers and re-cast the corresponding tasksas pattern matching (Johnson, 2002) or classifica-tion (Pradhan et al, 2005) problems.
Other meth-ods (Jijkoun and de Rijke, 2004) use combinationsof pattern matching and classification.The method presented in this paper belongs tothe latter category.
Specifically, we propose (1) touse a flexible and expressive graph-based represen-tation of linguistic structures at different levels; and(2) to view NLP tasks as graph transformation prob-lems: namely, problems of transforming graphs ofone type into graphs of another type.
An exam-ple of such a transformation is adding a level ofthe predicate argument structure or semantic argu-ments to syntactically annotated sentences.
Further-more, we describe a general method to automati-cally learn such transformations from annotated cor-pora.
Our method combines pattern matching ongraphs and machine learning (classification) and canbe viewed as an extension of the Transformation-Based Learning paradigm (Brill, 1995).
After de-scribing the method for learning graph transforma-tions we demonstrate its applicability on two tasks:identification of non-local dependencies (using PennTreebank data) and semantic roles labeling (usingProposition Bank data).The paper is organized as follows.
In Section 2we give our motivations for using graphs to encodelinguistic data.
In Section 3 we describe our methodfor learning graph transformations and in Section 4we report on experiments with applications of ourmethod.
We conclude in Section 5.2 Graphs for linguistic structures andlanguage processing tasksTrees and graphs are natural and common ways ofencoding linguistic information, in particular, syn-53VPto seek NPseatsVPplannedSdirectors SNP?SBJthis monthNP?TMP*NP?SBJFigure 1: Local and non-local syntantic relations.VPusing NPSstoppedLorillard Incincigarette filtersNP in NP1956ARG0 head ARG1 ARGMfeature=TMPpredcrocidoliteSPP PPNP VPFigure 2: Syntactic structure and semantic roles.tactic structures (phrase trees, dependency struc-tures).
In this paper we use node- and edge-labeleddirected graphs as our representational formalism.Figures 1 and 2 give informal examples of such rep-resentations.Figure 1 shows a graph encoding of the PennTreebank annotation of the local (solid edges) andnon-local (dashed edges) syntantic structure of thesentence directors this month planned to seek moreseats.
In this example, the co-indexing-based im-plicit annotation of the non-local dependency (sub-ject control) in the Penn Treebank (Bies et al, 1995)is made explicit in the graph-based encoding.Figure 2 shows a graph encoding of linguisticstructures for the sentence Lorillard Inc stopped us-ing crocodolite in sigarette filters in 1956.
Here,solid lines correspond to surface syntactic structure,produced by Charniak?s parser (Charniak, 2000),and dashed lines are an encoding of the PropositionBank annotation of the semantic roles with respectto the verb stopped.Graph-based representations allow for a uniformview on the linguistic structures on different layers.An advantage of such a uniform view is that ap-parently different NLP tasks can be considered asVPto seek NPseatsVPplannedSdirectors Sthis monthNPNPFigure 3: Output of a syntactic parser.manipulations with graphs, in other words, as graphtransformation problems.Consider the task of recovering non-local depen-dencies (such as control, WH-extraction, topicaliza-tion) in the surface syntactic phrase trees producedby the state-of-the-art parser of (Charniak, 2000).Figure 3 shows a graph-based encoding of the outputof the parser, and the task in question would consistin transforming the graph in Figure 3 into the graphin Figure 1.
We notice that this transformation canbe realised as a sequence of independent and rela-tively simple graph transformations: adding nodesand edges to the graph or changing their labels (e.g.,from NP to NP-SBJ).Similarly, for the example in Figure 2, adding asemantic layer (dashed edges) to the syntactic struc-ture can also be seen as transforming a graph.In general, we can view NLP tasks as adding ad-ditional linguistic information to text, based on theinformation already present: e.g., syntactic pars-ing taking part-of-speech tagged sentences as in-put (Collins, 1999), or anaphora resolution tak-ing sequences of syntactically analysed and named-entity-tagged sentences.
If both input and output lin-guistic structures are encoded as graphs, such NLPtasks become graph transformation problems.In the next section we describe our generalmethod for learning graph transformations from anannotated corpus.3 Learning graph transformationsWe start with a few basic definitions.
Similarto (Schu?rr, 1997), we define ?emphgraph as a rela-tional structure, i.e., a set of objects and relationsbetween them; we represent such structures as setsof first-order logic atomic predicates defining nodes,54directed edges and their attributes (labels).
Con-stants used in the predicates represent objects (nodesand edges) of graphs, as well as attribute names andvalues.
Atomic predicates node(?
), edge(?, ?, ?)
andattr(?, ?, ?)
define nodes, edges and their attributes.We refer to (Schu?rr, 1997; Jijkoun, 2006) for formaldefinitions and only illustrate these concepts with anexample.
The following set of predicates:node(n1), node(n2), edge(e, n1, n2),attr(n1, label, Src), attr(n2, label,Dst)defines a graph with two nodes, n1 and n2, hav-ing labels Src and Dst (encoded as attributes namedlabel), and an (unlabelled) edge e going from n1 ton2.A pattern is an arbitrary graph and an occurenceof a pattern P in graph G is a total injective homo-morphism ?
from P to G, i.e., a mapping that asso-ciates each object of P with one object G and pre-serves the graph structure (relations between nodes,edges, attribute names and values).
We will also usethe term occurence to refer to the graph?
(P ), a sub-graph of G, the image of the mapping ?
on P .A graph rewrite rule is a triple r =?lhsr, Cr, rhsr?
: the left-hand side, the constraintand the right-hand side of r, respectively, where lhsrand rhsr are graphs and Cr is a function that returns0 or 1 given a graphG, pattern lhsr and its occurencein G (i.e., Cr specifies a constraint on occurences ofa pattern in a graph).To apply a rewrite rule r = ?lhsr, Cr, rhsr?
toa graph G means finding all occurences of lhsr inG for which Cr evaluates to 1, and replacing suchoccurences of lhsr with occurences of rhsr.
Effec-tively, objects and relations present in lhsr but not inrhsr will be removed from G, objects and relationsin rhsr but not in lhsr will be added to G, and com-mon objects and relations will remain intact.
Again,we refer to (Jijkoun, 2006) for formal definitions.As will be discussed below, our method for learn-ing graph transformations is based on the ability tocompare pairs of graphs, identifying where the twographs are similar and where they differ.
An align-ment of two graphs is a partial one-to-one homomor-phism between their nodes and edges, such that iftwo edges of the two graphs are aligned, their re-spective endpoints are aligned as well.
A maximalalignment of two graphs is an alignment that maxi-mizes the sum of (1) the number of aligned objects(nodes and edges), and (2) the number of match-ing attribute values of all aligned objects.
In otherwords, a maximal alignment identifies as many sim-ilarities between two graphs as possible.
Given analignment of two graphs, it is possible to extract alist of rewrite rules that can transform one graph intoanother.
For a maximal alignment such a list willconsist of rules with the smallest possible left- andright-hand sides.
See (Jijkoun, 2006) for details.As stated above, we view NLP applications asgraph transformation modules.
Our supervisedmethod for learning graph transformation requirestwo corpora: input graphs In = {Ink} and corre-sponding output graphs Out = {Outk}, such thatOutk is the desired output of the NLP module onthe input Ink.The result of the method is an ordered list of graphrewrite rules R = ?r1, .
.
.
rn?, that can be applied insequence to input graphs to produce the output of theNLP module.Our method for learning graph transforma-tions follows the structure of Transformation-BasedLearning (Brill, 1995) and proceeds iteratively, asshown in Figure 4.
At each iteration, we compareand align pairs of input and output graphs, identifypossible rewrite rules and select rules with the mostfrequent left-hand sides.
For each selected rewriterule r, we extract all occurences of its left-handside and use them to train a two-class classifier im-plementing the constraint Cr: the classifier, givenan encoding of an occurence of the left-hand sidepredicts whether this particular occurence shouldbe replaced with the corresponding right-hand side.When encoding an occurence as a feature vector, weadd as features all paths and all attributes of nodesand edges in the one-edge neighborhood from thenodes of the occurence.
For the experiments de-scribed in this paper we used the SVM Light classi-fier (Joachims, 1999) with a standard linear kernel.See (Jijkoun, 2006) for details.4 ApplicationsHaving presented a general method for learninggraph transformations, we now illustrate the methodat work and describe two applications to concrete55CompareApplyExtract rulesAligned graphsCompareApplyExtract rulesAligned graphsrules rulesrulesIdeal output graphsInput graphs...Iteration 1 Iteration 2 Iteration NCompareApplyExtract rulesAligned graphs...Figure 4: Structure of our method for learning graph transformations.NLP problems: identification of non-local depen-dencies (with the Penn Treebank data) and semanticrole labeling (with the Proposition Bank data).4.1 Non-local dependenciesState-of-the-art statistical phrase structure parsers,e.g., Charniak?s and Collins?
parsers trained onthe Penn Treebank, produce syntactic parse treeswith bare phrase labels, (NP, PP, S, see Figure 3),i.e., providing surface grammatical analysis of sen-tences, even though the training corpus, the PennTreebank, is richer and contains additional gram-matical and semantic information: it distinguishesvarious types of modifiers, complements, subjects,objects and annotates non-local dependencies, i.e.,relations between phrases not adjacent in the parsetree (see Figure 1).
The task of recovering this in-formation in the parser?s output has received a gooddeal of attention.
(Campbell, 2004) presents a rule-based algorithm for empty node identification insyntactic trees, competitive with the machine learn-ing methods we mention next.
In (Johnson, 2002)a simple pattern-matching algorithm was proposedfor inserting empty nodes into syntactic trees, withpatterns extracted from the Penn Treebank.
(Dienes,2004) used a preprocessor that identified surface lo-cation of empty nodes and a syntactic parser incor-porating non-local dependencies into its probabilis-tic model.
(Jijkoun and de Rijke, 2004) describedan extension of the pattern-matching method with aclassifier trained on the dependency graphs derivedfrom the Penn Treebank data.In order to apply our graph transformation methodto the task of identifying non-local dependencies,we need to encode the information provided in thePenn Treebank annotations and in the output of asyntactic parser using directed labeled graphs.
Weused a straightforward encoding of syntactic trees,with nodes representing terminals and non-terminalsand edges defining the parent-child relationship.
Foreach node, we used the attribute type to specifywhether it is a terminal or a non-terminal.
Ter-minals corresponding to Penn empty nodes weremarked with the attribute empty = 1.
For eachterminal (i.e., each word), the values of attributespos, word and lemma provided the part-of-speech tag,the actual form and the lemma of the word.
Fornon-terminals, the attribute label contained the la-bel of the corresponding syntactic phrase.
The co-indexing of empty nodes and non-terminals used inthe Penn Treebank to annotate non-local dependen-cies was encoded using explicit edges with a distincttype attribute, connecting empty nodes with their an-tecedents (e.g., the dashed edge in Figure 1).
Foreach non-terminal node, its head child was markedby attaching attribute head with value 1 to the corre-56sponding parent-child edge, and the lexical head ofeach non-terminal was explicitly indicated using ad-ditional edges with the attribute type = lexhead.
Weused a heuristic method of (Collins, 1999) for headidentification.When Penn Treebank sentences and the output ofthe parser are encoded as directed labeled graphsas described above, the task of identifying non-local dependencies can be formulated as transform-ing phrase structure graphs produced by a parser intographs of the type used in Penn Treebank annota-tions.We parsed the strings of the Penn Treebank withCharniak?s parser and then used the data from sec-tions 02?21 of the Penn Treebank for training: en-coding of the parser?s output was used as the cor-pus of input graphs for our learning method, andthe encoding of the original Penn annotations wasused as the corpus of output graphs.
Similarly, weused the data of sections 00?01 for development andsection 23 for testing.
Using the input and outputcorpora, we ran the learning method as describedabove, at each iteration considering 20 most frequentleft-hand sides of rewrite rules.
At each iteration,the learned rewrite rules were applied to the currenttraining and development corpora to create a cor-pus of input graphs for the next iteration (see Fig-ure 4) and to estimate the performance of the systemat the current iteration.
The system was evaluatedon the development corpus with respect to non-localdependencies using the ?strict?
evaluation measureof (Johnson, 2002): the F1 score of precision andrecall of correctly identified empty nodes and an-tecedents.
If the absolute improvement of the F1score for the evaluation measure was smaller than0.1, the learning cycle was terminated, otherwise anew iteration was started.The learning cycle terminated after 12 iterations.The resulting sequence of 12 ?
20 = 240 graphrewrite rules was applied to the test corpus of in-put graphs: Charniak?s parser output on the stringsof section 23 of the Penn Treebank.
The resultwas evaluated against the original annotations of thePenn Treebank.The results of the evaluation of the system onempty nodes and non-local dependencies and thePARSEVAL F1 score on local syntactic phrasestructure against the test corpus at each iteration areStage P R F1 PARSEVAL F1Initial 0.0 0.0 0.0 88.71 88.2 38.6 53.7 88.42 87.2 48.6 62.5 88.43 87.5 51.9 65.2 88.44 86.7 52.1 65.1 88.45 86.1 56.3 68.1 88.36 86.0 57.2 68.7 88.47 86.3 61.3 71.7 88.48 86.6 63.4 73.2 88.49 86.7 64.6 74.0 88.410 86.7 64.9 74.2 88.411 86.6 65.1 74.3 88.412 86.7 65.2 74.4 88.4Table 1: Evaluation of our method for identificationof empty nodes and their antecedents (12 first itera-tions).shown in Table 1.As one can expect, at each iteration the methodextracts graph rewrite rules that introduce emptynodes and non-local relations into syntactic struc-tures, increasing the recall.
The performance of thefinal system (P/R/F1 = 86.7/65.2/74.4) for the taskof identifying non-local dependencies is compara-ble to the performance of the best model of (Di-enes, 2004): P/R/F1=82.5/70.1/75.8.
The PARSE-VAL score for the present system (88.4) is, however,higher than the 87.3 for the system of Dienes.Another effect of the learned transformations ischanging node labels of non-terminals, specifically,modifying labels to include Penn functional tags(e.g., changing NP in the input graph in Figure 3 toNP-SBJ in the output graph in Figure 1).
In fact, 17%of all learned rewrite rules involved only changinglabels of non-terminal nodes.
Analysis of the resultsshowed that the system is capable of assigning Pennfunction tags to constituents produced by Charniak?sparser with F1 = 91.4 (we use here the evalua-tion measure of (Blaheta, 2004): the F1 score of theprecision and recall for assigning function tags toconstituents with surface spans correctly identifiedby Charniak?s parser).
Comparison to the evalua-tion results of the function tagging method presentedin (Blaheta, 2004) is shown in Table 2.The present system outperforms the system ofBlaheta on semantic tags such as -TMP or -MNRmarking temporal and manner adjuncts, respec-tively, but performs worse on syntactic tags suchas -SBJ or -PRD marking subjects and predicatives,57(Blaheta, 2004) HereType Count P / R / F1 P / R / F1All tags 8480 - 93.3 / 89.6 / 91.4Syntactic 4917 96.5 / 95.3 / 95.9 95.4 / 95.5 / 95.5Semantic 3225 86.7 / 80.3 / 83.4 89.7 / 82.5 / 86.0Table 2: Evaluation of adding Penn Treebank func-tion tags.respectively.
Note that the present method was notspecifically designed to add functional tags to con-stituent labels.
The method is not even ?aware?
thatfunctional tags exists: it simply treats NP and NP-SBJas different labels and tries to correct labels compar-ing input and output graphs in the training corpora.In general, of the 240 graph rewrite rules ex-tracted during the 12 iterations of the method, 25%involved only one graph node in the left-hand side,16% two nodes, 12% three nodes, etc.
The twomost complicated extracted rewrite rules involvedleft-hand sides with ten nodes.We now switch to the second application of ourgraph transformation method.4.2 Semantic role labelingPut very broadly, the task of semantic role labelingconsists in detecting and labeling simple predicates:Who did what to whom, where, when, how, why, etc.There is no single definition of a universal set ofsemantic roles and moreover, different NLP appli-cations may require different specificity of role la-bels.
In this section we apply the graph transforma-tion method to the task of identification of semanticroles as annotated in the Proposition Bank (Palmeret al, 2005), PropBank for short.
In PropBank, forall verbs (except copular) of the syntactically anno-tated sentences of the Wall Street Journal section ofthe Penn Treebank, semantic arguments are markedusing references to the syntactic constituents of thePenn Treebank.
For the 49,208 syntactically anno-tated sentences of the Penn Treebank, the PropBankannotated 112,917 verb predicates (2.3 predicatesper sentence on average), with a total of 292,815 se-mantic arguments (2.6 arguments per predicate onaverage).PropBank does not aim at cross-verb semanticallyconsistent labeling of arguments, but rather at anno-tating the different ways arguments of a verb canbe realized syntactically in the corpus, which re-sulted in the choice of theory-neutral numbered la-bels (e.g., Arg0, Arg1, etc.)
for semantic arguments.Figure 2 shows an example of a PropBank annota-tion (dashed edges).In this section we address a specific NLP task:identifying and labeling semantic arguments in theoutput of a syntactic parser.
For the example inFigure 2 this task corresponds to adding ?semantic?nodes and edges to the syntactic tree.As before, in order to apply our graph transfor-mation method, we need to encode the available in-formation using graphs.
Our encoding of syntacticphrase structure is the same as in Section 4.1 and theencoding of the semantic annotations of PropBankis straightforward.
For each PropBank predicate, anew node with attributes type = propbank and label =pred is added.
Another node with label = head andnodes for all semantic arguments of the predicate(with labels indicating PropBank argument names)are added and connected to the predicate node.
Ar-gument nodes with label ARGM (adjunct) addition-ally have a feature attribute with values TMP, LOC,etc., as specified in PropBank.
The head node andall argument nodes are linked to their respective syn-tactic constituents, as specified in the PropBank an-notation.
All introduced semantic edges are markedwith the attribute type = propbank.As before, we used section 02?21 of the Prop-Bank (which annotates the same text as the PennTreebank) to train our graph transformation system,section 00-01 for development and section 23 fortesting.
We ran three experiments, taking three dif-ferent corpora of input graphs:1. the original syntactic structures of the PennTreebank containing function tags, emptynodes, non-local dependencies, etc.;2.
the output of Charniak?s parser (i.e., bare syn-tactic trees) on the strings of sections 02?21;and3.
the output of Charniak?s parser processedwith the graph transformation system describedin 4.1.For all three experiments we used the gold stan-dard syntactic and semantic annotations from the58Penn Treebank Charniak Charniak +Iter.
P R P R P R1 90.0 70.7 79.5 58.6 79.9 59.12 90.7 76.5 81.2 63.9 81.0 64.23 90.7 78.1 81.3 65.6 81.1 65.84 90.6 78.9 81.4 66.5 81.2 66.75 90.5 80.4 81.4 67.0 81.2 68.36 90.4 81.2 81.4 68.3 81.1 68.87 90.3 81.9 81.3 68.9 81.0 69.38 90.3 82.2 81.3 69.3 81.0 69.89 90.3 82.5 81.3 69.6 81.0 70.110 90.3 82.8 81.4 69.8 81.0 70.311 90.3 83.0 81.3 69.9 81.0 70.412 90.3 83.2Table 3: Evaluation of our method for semantic roleidentification with Propbank: with Charniak parsesand with parses processed by the system of Sec-tion 4.1.Penn Treebank and PropBank as the corpora of out-put graphs (for the experiment with bare Charniakparses, we dropped function tags, empty nodes andnon-local dependencies from the syntactic annota-tion of the output graphs: we did not want our sys-tem to start recovering these annotations, but wereinterested in the identification of PropBank informa-tion alone).For each of the experiments, we used the corporaof input and output graphs as before, at each itera-tion extracting 20 rewrite rules with most frequentleft-hand sides, applying the rules to the develop-ment data to measure the current performance of thesystem.
We stopped the learning in case the perfor-mance improvement was less than a threshold and,otherwise, continued the learning loop.
As our per-formance measure we used the F1 score of precisionand recall of the correctly identified and labeled non-empty constituents?semantic arguments.In all experiments, the learning stopped after 11or 12 iterations.
The results of the evaluation of thesystem at each iteration on the test section of Prop-Bank are shown in Table 3.As one may expect, the performance of our se-mantic role labeler is substantially higher on thegold Penn Treebank syntactic structures than on theparser?s output.
Surprisingly, however, adding extrainformation to the parser?s output (i.e., processing itwith the system of Section 4.1) does not significantlyimprove the performance of the resulting system.In Table 4 we compare our system for semanticSystem P R F1(Pradhan et al, 2005) 80.9 76.8 78.8Here 81.0 70.4 75.3Table 4: Evaluation of our methods for semantic roleidentification with Propbank (12 first iterations).roles labeling with the output of Charniak?s parser tothe state-of-the-art system of (Pradhan et al, 2005).While showing good precision, our system per-forms worse than state-of-the-art with respect to re-call.
Taking into account the iterative nature ofthe method and imperfect rule selection criteria (wesimply take the most frequent left-hand sides), webelieve that it is the rule selection and learning termi-nation condition that account for the relatively lowrecall values.
Indeed, in all three experiments de-scribed above the learning loop stops while the recallis still on the rise, albeit very slowly.
It seems thata more careful rule selection mechanism and looptermination criteria are needed to address the recallproblem.5 ConclusionsIn this paper we argued that encoding diverse andcomplex linguistic structures as directed labeledgraphs allows one to view many NLP tasks as graphtransformation problems.
We proposed a generalmethod for learning graph transformation from an-notated corpora and described experiments with twoNLP applications.For the task of identifying non-local dependen-cies and for function tagging our general methoddemonstrates performance similar to the state-of-the-art systems, designed specifically for these tasks.For the PropBank semantic role labeling the methodshows a relatively low recall, which can be explainedby our sub-optimal ?rule of thumb?
heuristics (suchas selecting 20 most frequent rewrite rules at eachiteration of the learning method).
We see two waysof avoiding such heuristics.
First, one can defineand fine-tune the heuristics for each specific appli-cation.
Second, one can use more informed rewriterule selection methods, based on graph-based rela-tional learning and frequent subgraph detection al-gorithms (Cook and Holder, 2000; Yan and Han,2002).
Furthermore, more experiments are required59to see how the details of encoding linguistic in-formation in graphs affect the performance of themethod.AcknowledgementsThis research was supported by the NetherlandsOrganization for Scientific Research (NWO) un-der project numbers 017.001.190, 220-80-001,264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006,640.001.501, 640.002.501, and by the E.U.
ISTprogramme of the 6th FP for RTD under projectMultiMATCH contract IST-033104.ReferencesAnn Bies, Mark Ferguson, Karen Katz, and Robert Mac-Intyre.
1995.
Bracketing guidelines for Treebank IIstyle Penn Treebank project.
Technical report, Uni-versity of Pennsylvania.Don Blaheta.
2004.
Function Tagging.
Ph.D. thesis,Brown University.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Computational Lin-guistics, 21(4):543?565.Richard Campbell.
2004.
Using linguistic principlesto recover empty categories.
In Proceedings of the42nd Annual Meeting on Association for Computa-tional Linguistics, pages 645?653.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st Meeting of NAACL,pages 132?139.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Diane J. Cook and Lawrence B.
Holder.
2000.Graph-based data mining.
IEEE Intelligent Systems,15(2):32?41.Pe?ter Dienes.
2004.
Statistical Parsing with Non-localDependencies.
Ph.D. thesis, Universita?t des Saarlan-des, Saarbru?cken, Germany.Daniel Gildea.
2001.
Statistical Language Understand-ing Using Frame Semantics.
Ph.D. thesis, Universityof California, Berkeley.Ana-Maria Giuglea and Alessandro Moschitti.
2006.
Se-mantic role labeling via framenet, verbnet and prop-bank.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics, pages 929?936.Julia Hockenmaier.
2003.
Parsing with generative mod-els of predicate-argument structure.
In Proceedings ofthe 41st Meeting of ACL, pages 359?366.Valentin Jijkoun and Maarten de Rijke.
2004.
Enrich-ing the output of a parser using memory-based learn-ing.
In Proceedings of the 42nd Meeting of the Asso-ciation for Computational Linguistics (ACL?04), MainVolume, pages 311?318, Barcelona, Spain, July.Valentin Jijkoun.
2006.
Graph Transformations for Nat-ural Language Processing.
Ph.D. thesis, University ofAmsterdam.Thorsten Joachims.
1999.
Making large-scale svmlearning practical.
In B. Scho?lkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods - Sup-port Vector Learning.
MIT-Press.Christopher R. Johnson, Miriam R. L. Petruck, Collin F.Baker, Michael Ellsworth, Josef Ruppenhofer, andCharles J. Fillmore.
2003.
FrameNet: Theory andPractice.
http://www.icsi.berkeley.edu/?framenet.Mark Johnson.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
In Proceedings of the 40th meeting of ACL,pages 136?143.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1).Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JimMar-tin, and Dan Jurafsky.
2005.
Semantic role label-ing using different syntactic views.
In Proceedings ofACL-2005.A.
Schu?rr.
1997.
Programmed graph replacement sys-tems.
In Grzegorz Rozenberg, editor, Handbook ofGraph Grammars and Computing by Graph Transfor-mation, chapter 7, pages 479?546.Kristina Toutanova, Aria Haghighi, and Chris Manning.2005.
Joint learning improves semantic role labeling.In Proceedings of the 43rd Meeting of the Associationfor Computational Linguistics (ACL).Xifeng Yan and Jiawei Han.
2002. gspan: Graph-basedsubstructure pattern mining.
In Proceedings of the2002 IEEE International Conference on Data Mining(ICDM).60
