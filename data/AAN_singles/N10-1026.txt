Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 225?228,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsImproved Extraction Assessment through Better Language ModelsArun Ahuja, Doug DowneyEECS Dept., Northwestern UniversityEvanston, IL 60208{arun.ahuja, ddowney}@eecs.northwestern.eduAbstractA variety of information extraction techniquesrely on the fact that instances of the samerelation are ?distributionally similar,?
in thatthey tend to appear in similar textual con-texts.
We demonstrate that extraction accu-racy depends heavily on the accuracy of thelanguage model utilized to estimate distribu-tional similarity.
An unsupervised model se-lection technique based on this observation isshown to reduce extraction and type-checkingerror by 26% over previous results, in experi-ments with Hidden Markov Models.
The re-sults suggest that optimizing statistical lan-guage models over unlabeled data is a promis-ing direction for improving weakly supervisedand unsupervised information extraction.1 IntroductionMany weakly supervised and unsupervised informa-tion extraction techniques assess the correctness ofextractions using the distributional hypothesis?thenotion that words with similar meanings tend to oc-cur in similar contexts (Harris, 1985).
A candidateextraction of a relation is deemed more likely to becorrect when it appears in contexts similar to thoseof ?seed?
instances of the relation, where the seedsmay be specified by hand (Pas?ca et al, 2006), takenfrom an existing, incomplete knowledge base (Snowet al, 2006; Pantel et al, 2009), or obtained in an un-supervised manner using a generic extractor (Bankoet al, 2007).
We refer to this technique as Assess-ment by Distributional Similarity (ADS).Typically, distributional similarity is computed bycomparing co-occurrence counts of extractions andseeds with various contexts found in the corpus.
Sta-tistical Language Models (SLMs) include methodsfor more accurately estimating co-occurrence proba-bilities via back-off, smoothing, and clustering tech-niques (e.g.
(Chen and Goodman, 1996; Rabiner,1989; Bell et al, 1990)).
Because SLMs can betrained from only unlabeled text, they can be appliedfor ADS even when the relations of interest are notspecified in advance (Downey et al, 2007).
Unla-beled text is abundant in large corpora like the Web,making nearly-ceaseless automated optimization ofSLMs possible.
But how fruitful is such an effortlikely to be?to what extent does optimizing a lan-guage model over a fixed corpus lead to improve-ments in assessment accuracy?In this paper, we show that an ADS techniquebased on SLMs is improved substantially whenthe language model it employs becomes more ac-curate.
In a large-scale set of experiments, wequantify how language model perplexity correlateswith ADS performance over multiple data sets andSLM techniques.
The experiments show that accu-racy over unlabeled data can be used for selectingamong SLMs?for an ADS approach utilizing Hid-den Markov Models, this results in an average errorreduction of 26% over previous results in extractionand type-checking tasks.2 Extraction Assessment with LanguageModelsWe begin by formally defining the extraction andtypechecking tasks we consider, then discuss statis-tical language models and their utilization for ex-traction assessment.225The extraction task we consider is formalized asfollows: given a corpus, a target relation R, a listof seed instances SR, and a list of candidate extrac-tions UR, the task is to order elements of UR suchthat correct instances for R are ranked above extrac-tion errors.
Let URi denote the set of the ith argu-ments of the extractions in UR, and let SRi be de-fined similarly for the seed set SR. For relations ofarity greater than one, we consider the typecheckingtask, an important sub-task of extraction (Downey etal., 2007).
The typechecking task is to rank extrac-tions with arguments that are of the proper type for arelation above type errors.
As an example, the ex-traction Founded(Bill Gates, Oracle) istype correct, but is not correct for the extraction task.2.1 Statistical Language ModelsA Statistical Language Model (SLM) is a probabil-ity distribution P (w) over word sequences w =(w1, ..., wr).
The most common SLM techniquesare n-gram models, which are Markov models inwhich the probability of a given word is dependenton only the previous n?1 words.
The accuracy of ann-gram model of a corpus depends on two key fac-tors: the choice of n, and the smoothing techniqueemployed to assign probabilities to word sequencesseen infrequently in training.
We experiment withchoices of n from 2 to 4, and two popular smoothingapproaches, Modified Kneser-Ney (Chen and Good-man, 1996) and Witten-Bell (Bell et al, 1990).Unsupervised Hidden Markov Models (HMMs)are an alternative SLM approach previously shownto offer accuracy and scalability advantages over n-gram models in ADS (Downey et al, 2007).
AnHMM models a sentence w as a sequence of obser-vations wi each generated by a hidden state variableti.
Here, hidden states take values from {1, .
.
.
, T},and each hidden state variable is itself generated bysome number k of previous hidden states.
Formally,the joint distribution of a word sequence w given acorresponding state sequence t is:P (w|t) =?iP (wi|ti)P (ti|ti?1, .
.
.
, ti?k) (1)The distributions on the right side of Equation 1 arelearned from the corpus in an unsupervised mannerusing Expectation-Maximization, such that wordsdistributed similarly in the corpus tend to be gen-erated by similar hidden states (Rabiner, 1989).2.2 Performing ADS with SLMsThe Assessment by Distributional Similarity (ADS)technique is to rank extractions in UR in decreas-ing order of distributional similarity to the seeds,as estimated from the corpus.
In our experiments,we utilize an ADS approach previously proposed forHMMs (Downey et al, 2007) and adapt it to also ap-ply to n-gram models, as detailed below.Define a context of an extraction argument ei tobe a string containing the m words preceding and mwords following an occurrence of ei in the corpus.Let Ci = {c1, c2, ..., c|Ci|} be the union of all con-texts of extraction arguments ei and seed argumentssi for a given relation R. We create a probabilis-tic context vector for each extraction ei where thej-th dimension of the vector is the probability of thecontext surrounding given the extraction, P (cj |ei),computed from the language model.
1We rank the extractions in UR according to howsimilar their arguments?
contextual distributions,P (c|ei), are to those of the seed arguments.
Specifi-cally, extractions are ranked according to:f(e) =?ei?eKL(?w?
?SRiP (c|w?
)|SRi|, P (c|ei)) (2)where KL represents KL Divergence, and the outersum is taken over arguments ei of the extraction e.For HMMs, we alternatively rank extractions us-ing the HMM state distributions P (t|ei) in place ofthe probabilistic context vectors P (c|ei).
Our exper-iments show that state distributions are much moreaccurate for ADS than are HMM context vectors.3 ExperimentsIn this section, we present experiments showing thatSLM accuracy correlates strongly with ADS perfor-mance.
We also show that SLM performance can beused for model selection, leading to an ADS tech-nique that outperforms previous results.3.1 Experimental MethodologyWe experiment with a wide range of n-gram andHMM models.
The n-gram models are trained us-ing the SRILM toolkit (Stolcke, 2002).
Evaluating a1For example, for context cj = ?I visited in July?
and ex-traction ei = ?Boston,?
P (cj |ei) is P(?I visited Boston in July?
)/ P(?Boston?
), where each string probability is computed usingthe language model.226LM Unary Binary WikipediaHMM 1-5 -.911 -.361 -.994HMM 2-5 -.856 .120 -.930HMM 3-5 -.823 -.683 .922HMM 1-10 -.916 -.967 -.905HMM 2-10 -.877 -.797 -.963HMM 3-10 -.957 -.669 -.924HMM 1-25 -.933 -.850 -.959HMM 1-50 -.942 -.942 -.947HMM 1-100 -.896 -.877 -.942N-Gram -.512 -.999 .024Table 1: Pearson Correlation value for extraction perfor-mance (in AUC) and SLM performance (in perplexity).Extraction accuracy increases as perplexity decreases,with an average correlation coefficient of -0.742.
?HMMk-T ?
denotes an HMM model of order k, with T states.variety of HMM configurations over a large corpusrequires a scalable training architecture.
We con-structed a parallel HMM codebase using the Mes-sage Passing Interface (MPI), and trained the modelson a supercomputing cluster.
All language modelswere trained on a corpus of 2.8M sentences of Webtext (about 60 million tokens).
SLM performance ismeasured using the standard perplexity metric, andassessment accuracy is measured using area underthe precision-recall curve (AUC), a standard metricfor ranked lists of extractions.
We evaluated perfor-mance on three distinct data sets.
The first two datasets evaluate ADS for unsupervised information ex-traction, and were taken from (Downey et al, 2007).The first, Unary, was an extraction task for unaryrelations (Company, Country, Language, Film) andthe second, Binary, was a type-checking task forbinary relations (Conquered, Founded, Headquar-tered, Merged).
The 10 most frequent extractionsserved as bootstrapped seeds.
The two test sets con-tained 361 and 265 extractions, respectively.
Thethird data set, Wikipedia, evaluates ADS on weakly-supervised extraction, using seeds and extractionstaken from Wikipedia ?List of?
pages (Pantel et al,2009).
Seed sets of various sizes (5, 10, 15 and20) were randomly selected from each list, and wepresent results averaged over 10 random samplings.Other members of the seed list were added to a testset as correct extractions, and elements from otherlists were added as errors.
The data set includedFigure 1: HMM 1-100 Performance.
Information Extrac-tion performance (in AUC) increases as SLM accuracyimproves (perplexity decreases).2264 extractions across 36 unary relations, includ-ing Composers and US Internet Companies.3.2 Optimizing Language Models for IEThe first question we investigate is whether opti-mizing individual language models leads to bet-ter performance in ADS.
We measured the correla-tion between SLM perplexity and ADS performanceas training proceeds in HMMs, and as n and thesmoothing technique vary in the n-gram models.
Ta-ble 1 shows that as the SLM becomes more accurate(i.e.
as perplexity decreases), ADS performance in-creases.
The correlation is strong (averaging -0.742)and is consistent across model configurations anddata sets.
The low positive correlation for the n-gram models on Wikipedia is likely due to a ?flooreffect?
; the models have low performance overallon the difficult Wikipedia data set.
The lowest-perplexity n-gram model (Mod Kneser-Ney smooth-ing with n=3, KN3) does exhibit the best IE per-formance, at 0.039 (the average performance of theHMM models is more than twice this, at 0.084).
Fig-ure 1 shows the relationship between SLM and ADSperformance in detail for the best-performing HMMconfiguration.3.3 Model SelectionDifferent language models can be configured in dif-ferent ways: for example, HMMs require choices forthe hyperparameters k and T .
Here, we show that227Figure 2: Model Selection for HMMs.
SLM perfor-mance is a good predictor of extraction performanceacross model configurations.SLM perplexity can be used to select a high-qualitymodel configuration for ADS using only unlabeleddata.
We evaluate on the Unary and Binary data sets,since they have been employed in previous workon our corpora.
Figure 2 shows that for HMMs,ADS performance increases as perplexity decreasesacross various model configurations (a similar rela-tionship holds for n-gram models).
A model selec-tion technique that picks the HMM model with low-est perplexity (HMM 1-100) results in better ADSperformance than previous results.
As shown in Ta-ble 2, HMM 1-100 reduces error over the HMM-Tmodel in (Downey et al, 2007) by 26%, on average.The experiments also reveal an important differencebetween the HMM and n-gram approaches.
WhileKN3 is more accurate in SLM than our HMM mod-els, it performs worse in ADS on average.
For exam-ple, HMM 1-25 underperforms KN3 in perpexity, at537.2 versus 227.1, but wins in ADS, 0.880 to 0.853.We hypothesize that this is because the latent statedistributions in the HMMs provide a more informa-tive distributional similarity measure.
Indeed, whenwe compute distributional similarity for HMMs us-ing probabilistic context vectors as opposed to statedistributions, ADS performance for HMM 1-25 de-creases to 5.8% below that of KN3.4 ConclusionsWe presented experiments showing that estimatingdistributional similarity with more accurate statisti-cal language models results in more accurate extrac-Relation HMM-T Best HMMCompany .966 .985Country .886 .942Languages .936 .914Film .803 .801Unary Avg .898 .911Conquered .917 .923Founded .827 .799Merged .920 .925Headquartered .734 .964Binary Average .849 .903Table 2: Extraction Performance Results in AUC for In-dividual Relations.
The lowest-perplexity HMM, 1-100,outperforms the HMM-T model from previous work.tion assessment.
We note that significantly larger,more powerful language models are possible beyondthose evaluated here, which (based on the trajectoryobserved in Figure 2) may offer significant improve-ments in assessment accuracy.ReferencesM.
Banko, M. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfrom the Web.
In Procs.
of IJCAI.T.
C. Bell, J. G. Cleary, and I. H. Witten.
1990.
TextCompression.
Prentice Hall, January.Stanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
In Proc.
of ACL.D.
Downey, S. Schoenmackers, and O. Etzioni.
2007.Sparse information extraction: Unsupervised languagemodels to the rescue.
In Proc.
of ACL.Z.
Harris.
1985.
Distributional structure.
In J. J. Katz,editor, The Philosophy of Linguistics.M.
Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.2006.
Names and similarities on the web: Fact extrac-tion in the fast lane.
In Procs.
of ACL/COLING 2006.P.
Pantel, E. Crestan, A. Borkovsky, A. M. Popescu, andV.
Vyas.
2009.
Web-scale distributional similarity andentity set expansion.
In Proc.
of EMNLP.L.
R. Rabiner.
1989.
A tutorial on Hidden MarkovModels and selected applications in speech recogni-tion.
Proceedings of the IEEE, 77(2):257?286.R.
Snow, D. Jurafsky, and A. Y. Ng.
2006.
Semantictaxonomy induction from heterogenous evidence.
InCOLING/ACL 2006.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings of ICSLP, volume 2.228
