Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 551?560,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsExploiting Syntactico-Semantic Structures for Relation ExtractionYee Seng Chan and Dan RothUniversity of Illinois at Urbana-Champaign{chanys,danr}@illinois.eduAbstractIn this paper, we observe that there exists asecond dimension to the relation extraction(RE) problem that is orthogonal to the relationtype dimension.
We show that most of thesesecond dimensional structures are relativelyconstrained and not difficult to identify.
Wepropose a novel algorithmic approach to REthat starts by first identifying these structuresand then, within these, identifying the seman-tic type of the relation.
In the real RE problemwhere relation arguments need to be identi-fied, exploiting these structures also allows re-ducing pipelined propagated errors.
We showthat this RE framework provides significantimprovement in RE performance.1 IntroductionRelation extraction (RE) has been defined as the taskof identifying a given set of semantic binary rela-tions in text.
For instance, given the span of text?.
.
.
the Seattle zoo .
.
.
?, one would like to extract therelation that ?the Seattle zoo?
is located-at ?Seattle?.RE has been frequently studied over the last fewyears as a supervised learning task, learning fromspans of text that are annotated with a set of seman-tic relations of interest.
However, most approachesto RE have assumed that the relations?
argumentsare given as input (Chan and Roth, 2010; Jiang andZhai, 2007; Jiang, 2009; Zhou et al, 2005), andtherefore offer only a partial solution to the problem.Conceptually, this is a rather simple approach asall spans of texts are treated uniformly and are be-ing mapped to one of several relation types of in-terest.
However, these approaches to RE require alarge amount of manually annotated training data toachieve good performance, making it difficult to ex-pand the set of target relations.
Moreover, as weshow, these approaches become brittle when the re-lations?
arguments are not given but rather need tobe identified in the data too.In this paper we build on the observation that thereexists a second dimension to the relation extractionproblem that is orthogonal to the relation type di-mension: all relation types are expressed in one ofseveral constrained syntactico-semantic structures.As we show, identifying where the text span is on thesyntactico-semantic structure dimension first, can beleveraged in the RE process to yield improved per-formance.
Moreover, working in the second dimen-sion provides robustness to the real RE problem, thatof identifying arguments along with the relations be-tween them.For example, in ?the Seattle zoo?, the entity men-tion ?Seattle?
modifies the noun ?zoo?.
Thus, thetwo mentions ?Seattle?
and ?the Seattle zoo?, areinvolved in what we later call a premodifier rela-tion, one of several syntactico-semantic structureswe identify in Section 3.We highlight that all relation types can be ex-pressed in one of several syntactico-semantic struc-tures ?
Premodifiers, Possessive, Preposition, For-mulaic and Verbal.
As it turns out, most of thesestructures are relatively constrained and are not dif-ficult to identify.
This suggests a novel algorith-mic approach to RE that starts by first identifyingthese structures and then, within these, identifyingthe semantic type of the relation.
Not only does thisapproach provide significantly improved RE perfor-551mance, it carries with it two additional advantages.First, leveraging the syntactico-semantic struc-ture is especially beneficial in the presence of smallamounts of data.
Second, and more important, is thefact that exploiting the syntactico-semantic dimen-sion provides several new options for dealing withthe full RE problem ?
incorporating the argumentidentification into the problem.
We explore one ofthese possibilities, making use of the constrainedstructures as a way to aid in the identification of therelations?
arguments.
We show that this already pro-vides significant gain, and discuss other possibilitiesthat can be explored.
The contributions of this paperare summarized below:?
We highlight that all relation types are ex-pressed as one of several syntactico-semanticstructures and show that most of these are rela-tively constrained and not difficult to identify.Consequently, working first in this structuraldimension can be leveraged in the RE processto improve performance.?
We show that when one does not have a largenumber of training examples, exploiting thesyntactico-semantic structures is crucial for REperformance.?
We show how to leverage these constrainedstructures to improve RE when the relations?arguments are not given.
The constrained struc-tures allow us to jointly entertain argument can-didates and relations built with them as argu-ments.
Specifically, we show that consideringargument candidates which otherwise wouldhave been discarded (provided they exist insyntactico-semantic structures), we reduce er-ror propagation along a standard pipeline REarchitecture, and that this joint inference pro-cess leads to improved RE performance.In the next section, we describe our relation ex-traction framework that leverages the syntactico-semantic structures.
We then present these struc-tures in Section 3.
We describe our mention entitytyping system in Section 4 and features for the REsystem in Section 5.
We present our RE experimentsin Section 6 and perform analysis in Section 7, be-fore concluding in Section 8.S = {premodifier, possessive, preposition, formulaic}gold mentions in training data MtrainDg = {(mi,mj) ?Mtrain ?Mtrain |mi in same sentence as mj ?
i 6= j ?
i < j}REbase = RE classifier trained on DgDs = ?for each (mi,mj) ?
Dgdop = structure inference on (mi,mj) using patternsif p ?
S ?
(mi,mj) was annotated with a S structureDs = Ds ?
(mi,mj)doneREs = RE classifier trained on DsOutput: REbase and REsFigure 1: Training a regular baseline RE classi-fier REbase and a RE classifier leveraging syntactico-semantic structures REs.2 Relation Extraction FrameworkIn Figure 1, we show the algorithm for traininga typical baseline RE classifier (REbase), and fortraining a RE classifier that leverages the syntactico-semantic structures (REs).During evaluation and when the gold mentions arealready annotated, we apply REs as follows.
Whengiven a test example mention pair (xi,xj), we per-form structure inference on it using the patterns de-scribed in Section 3.
If (xi,xj) is identified as hav-ing any of the four syntactico-semantic structures S,apply REs to predict the relation label, else applyREbase.Next, we show in Figure 2 our joint inference al-gorithmic framework that leverages the syntactico-semantic structures for RE, when mentions need tobe predicted.
Since the structures are fairly con-strained, we can use them to consider mention can-didates that are originally predicted as non men-tions.
As shown in Figure 2, we conservatively in-clude such mentions when forming mention pairs,provided their null labels are predicted with a lowprobability t1.1In this work, we arbitrary set t=0.2.
After the experiments,and in our own analysis, we observe that t=0.25 achieves betterperformance.
Besides using the probability of the 1-best predic-tion, one could also for instance, use the probability differencebetween the first and second best predictions.
However, select-ing an optimal t value is not the main focus of this work.552S = {premodifier, possessive, preposition, formulaic}candidate mentions McandLet Lm = argmaxyPMET (y|m, ?
),m ?Mcandselected mentions Msel = {m ?Mcand |Lm 6= null ?
PMET (null|m, ?)
?
t}QhasNull = {(mi,mj) ?Msel ?Msel |mi in same sentence as mj ?
i 6= j ?
i < j ?
(Lmi 6= null ?
Lmj 6= null)}Let pool of relation predictions R = ?for each (mi,mj) ?
QhasNulldop = structure inference on (mi,mj) using patternsif p ?
Sr = relation prediction for (mi,mj) using REsR = R?
relse if Lmi 6= null ?
Lmj 6= nullr = relation prediction for (mi,mj) using REbaseR = R?
rdoneOutput: RFigure 2: RE using predicted mentions and patterns.
Ab-breviations: Lm: predicted entity label for mention m us-ing the mention entity typing (MET) classifier describedin Section 4; PMET : prediction probability according tothe MET classifier; t: used for thresholding.There is a large body of work in using patternsto extract relations (Fundel et al, 2007; Greenwoodand Stevenson, 2006; Zhu et al, 2009).
However,these works operate along the first dimension, thatof using patterns to mine for relation type examples.In contrast, in our RE framework, we apply patternsto identify the syntactico-semantic structure dimen-sion first, and leverage this in the RE process.
In(Roth and Yih, 2007), the authors used entity typesto constrain the (first dimensional) relation types al-lowed among them.
In our work, although a few ofour patterns involve semantic type comparison, mostof the patterns are syntactic in nature.In this work, we performed RE evaluation on theNIST Automatic Content Extraction (ACE) corpus.Most prior RE evaluation on ACE data assumed thatmentions are already pre-annotated and given as in-put (Chan and Roth, 2010; Jiang and Zhai, 2007;Zhou et al, 2005).
An exception is the work of(Kambhatla, 2004), where the author evaluated onthe ACE-2003 corpus.
In that work, the author didnot address the pipelined errors propagated from themention identification process.3 Syntactico-Semantic StructuresIn this paper, we performed RE on the ACE-2004corpus.
In ACE-2004 when the annotators tagged apair of mentions with a relation, they also specifiedthe type of syntactico-semantic structure2.
ACE-2004 identified five types of structures: premodi-fier, possessive, preposition, formulaic, and verbal.We are unaware of any previous computational ap-proaches that recognize these structures automati-cally in text, as we do, and use it in the context ofRE (or any other problem).
In (Qian et al, 2008), theauthors reported the recall scores of their RE systemon the various syntactico-semantic structures.
Butthey do not attempt to recognize nor leverage thesestructures.In this work, we focus on detecting the first fourstructures.
These four structures cover 80% of themention pairs having valid semantic relations (wegive the detailed breakdown in Section 7) and weshow that they are relatively easy to identify usingsimple rules or patterns.
In this section, we indicatementions using square bracket pairs, and use mi andmj to represent a mention pair.
We now describe thefour structures.Premodifier relations specify the proper adjectiveor proper noun premodifier and the following nounit modifies, e.g.
: [the [Seattle] zoo]Possessive indicates that the first mention is in apossessive case, e.g.
: [[California] ?s Governor]Preposition indicates that the two mentions aresemantically related via the existence of a preposi-tion, e.g.
: [officials] in [California]Formulaic The ACE04 annotation guideline3 in-dicates the annotation of several formulaic relations,including for example address: [Medford] , [Mas-sachusetts]2ACE-2004 termed it as lexical condition.
We use the termsyntactico-semantic structure in this paper as the mention pairexists in specific syntactic structures, and we use rules or pat-terns that are syntactically and semantically motivated to detectthese structures.3http://projects.ldc.upenn.edu/ace/docs/EnglishRDCV4-3-2.PDF553Structure type PatternPremodifier Basic pattern: [u* [v+] w+] , where u, v, w represent wordsEach w is a noun or adjectiveIf u* is not empty, then u*: JJ+ ?
JJ ?and?
JJ?
?
CD JJ* ?
RB DT JJ?
?
RB CD JJ ?DT (RB|JJ|VBG|VBD|VBN|CD)?Let w1 = first word in w+.
w1 6= ??s?
and POS tag of w1 6= POSLet vl = last word in v+.
POS tag of vl 6= PRP$ nor WP$Possessive Basic pattern: [u?
[v+] w+] , where u, v, w represent wordsLet w1 = first word in w+.
If w1 = ??s?
?
POS tag of w1 = POS, accept mention pairLet vl = last word in v+.
If POS tag of vl = PRP$ or WP$, accept mention pairPreposition Basic pattern: [mi] v* [mj], where v represent wordsand number of prepositions in the text span v* between them = 0, 1, or 2If satisfy pattern: IN [mi][mj], accept mention pairIf satisfy pattern: [mi] (IN|TO) [mj], accept mention pairIf all labels in Ld start with ?prep?, accept mention pairFormulaic If satisfy pattern: [mi] / [mj] ?
Ec(mi) = PER ?
Ec(mj) = ORG, accept mention pairIf satisfy pattern: [mi][mj]If Ec(mi) = PER ?
Ec(mj) = ORG ?
GPE, accept mention pairTable 1: Rules and patterns for the four syntactico-semantic structures.
Regular expression notations: ?*?
matchesthe preceding element zero or more times; ?+?
matches the preceding element one or more times; ???
indicates thatthe preceding element is optional; ?|?
indicates or.
Abbreviations: Ec(m): coarse-grained entity type of mention m;Ld: labels in dependency path between the headword of two mentions.
We use square brackets ?[?
and ?]?
to denotemention boundaries.
The ?/?
in the Formulaic row denotes the occurrence of a lexical ?/?
in text.In this rest of this section, we present therules/patterns for detecting the above foursyntactico-semantic structure, giving an overviewof them in Table 1.
We plan to release all of therules/patterns along with associated code4.
Noticethat the patterns are intuitive and mostly syntactic innature.3.1 Premodifier Structures?
We require that one of the mentions completelyinclude the other mention.
Thus, the basic pat-tern is [u* [v+] w+].?
If u* is not empty, we require that it satisfiesany of the following POS tag sequences: JJ+ ?JJ and JJ?
?
CD JJ*, etc.
These are (optional)POS tag sequences that normally start a validnoun phrase.?
We use two patterns to differentiate betweenpremodifier relations and possessive relations,by checking for the existence of POS tagsPRP$, WP$, POS, and the word ?
?s?.4http://cogcomp.cs.illinois.edu/page/publications3.2 Possessive Structures?
The basic pattern for possessive is similar tothat for premodifier: [u?
[v+] w+]?
If the word immediately following v+ is ??s?
orits POS tag is ?POS?, we accept the mentionpair.
If the POS tag of the last word in v+ is ei-ther PRP$ or WP$, we accept the mention pair.3.3 Preposition Structures?
We first require the two mentions to be non-overlapping, and check for the existence ofpatterns such as ?IN [mi] [mj]?
and ?
[mi](IN|TO) [mj]?.?
If the only dependency labels in the depen-dency path between the head words of mi andmj are ?prep?
(prepositional modifier), acceptthe mention pair.3.4 Formulaic Structures?
The ACE-2004 annotator guidelines specifythat several relations such as reporter signingoff, addresses, etc.
are often specified in stan-dard structures.
We check for the existence ofpatterns such as ?
[mi] / [mj]?, ?
[mi] [mj]?,554Category FeatureFor every POS of wk and offset from lwword wk wk and offset from lwin POS of wk, wk, and offset from lwmention mi POS of wk, offset from lw, and lwBc(wk) and offset from lwPOS of wk, Bc(wk), and offset from lwPOS of wk, offset from lw, and Bc(lw)Contextual C?1,?1 of miC+1,+1 of miP?1,?1 of miP+1,+1 of miNE tags tag of NE, if lw of NE coincideswith lw of mi in the sentenceSyntactic parse-label of parse tree constituentparse that exactly covers miparse-labels of parse tree constituentscovering miTable 2: Features used in our mention entity typing(MET) system.
The abbreviations are as follows.
lw:last word in the mention; Bc(w): the brown cluster bitstring representing w; NE: named entityand whether they satisfy certain semantic entitytype constraints.4 Mention Extraction SystemAs part of our experiments, we perform RE usingpredicted mentions.
We first describe the features(an overview is given in Table 2) and then describehow we extract candidate mentions from sentencesduring evaluation.4.1 Mention Extraction FeaturesFeatures for every word in the mention For ev-ery word wk in a mention mi, we extract seven fea-tures.
These are a combination of wk itself, its POStag, and its integer offset from the last word (lw) inthe mention.
For instance, given the mention ?theoperation room?, the offsets for the three words inthe mention are -2, -1, and 0 respectively.
Thesefeatures are meant to capture the word and POS tagsequences in mentions.We also use word clusters which are automat-ically generated from unlabeled texts, using theBrown clustering (Bc) algorithm of (Brown et al,1992).
This algorithm outputs a binary tree wherewords are leaves in the tree.
Each word (leaf) in thetree can be represented by its unique path from theCategory FeaturePOS POS of single word between m1, m2hw of mi, mj and P?1,?1 of mi, mjhw of mi, mj and P?1,?1 of mi, mjhw of mi, mj and P+1,+1 of mi, mjhw of mi, mj and P?2,?1 of mi, mjhw of mi, mj and P?1,+1 of mi, mjhw of mi, mj and P+1,+2 of mi, mjBase chunk any base phrase chunk between mi, mjTable 3: Additional RE features.root and this path can be represented as a simple bitstring.
As part of our features, we use the cluster bitstring representation of wk and lw.Contextual We extract the word C?1,?1 immedi-ately before mi, the word C+1,+1 immediately aftermi, and their associated POS tags P .NE tags We automatically annotate the sentenceswith named entity (NE) tags using the named en-tity tagger of (Ratinov and Roth, 2009).
This taggerannotates proper nouns with the tags PER (person),ORG (organization), LOC (location), or MISC (mis-cellaneous).
If the lw of mi coincides (actual tokenoffset) with the lw of any NE annotated by the NEtagger, we extract the NE tag as a feature.Syntactic parse We parse the sentences using thesyntactic parser of (Klein and Manning, 2003).
Weextract the label of the parse tree constituent (if it ex-ists) that exactly covers the mention, and also labelsof all constituents that covers the mention.4.2 Extracting Candidate MentionsFrom a sentence, we gather the following as candi-date mentions: all nouns and possessive pronouns,all named entities annotated by the the NE tagger(Ratinov and Roth, 2009), all base noun phrase (NP)chunks, all chunks satisfying the pattern: NP (PPNP)+, all NP constituents in the syntactic parse tree,and from each of these constituents, all substringsconsisting of two or more words, provided the sub-strings do not start nor end on punctuation marks.These mention candidates are then fed to our men-tion entity typing (MET) classifier for type predic-tion (more details in Section 6.3).5555 Relation Extraction SystemWe build a supervised RE system using sentencesannotated with entity mentions and predefined targetrelations.
During evaluation, when given a pair ofmentions mi, mj , the system predicts whether anyof the predefined target relation holds between themention pair.Most of our features are based on the work of(Zhou et al, 2005; Chan and Roth, 2010).
Due tospace limitations, we refer the reader to our priorwork (Chan and Roth, 2010) for the lexical, struc-tural, mention-level, entity type, and dependencyfeatures.
Here, we only describe the features thatwere not used in that work.As part of our RE system, we need to extract thehead word (hw) of a mention (m), which we heuris-tically determine as follows: if m contains a prepo-sition and a noun preceding the preposition, we usethe noun as the hw.
If there is no preposition in m,we use the last noun in m as the hw.POS features If there is a single word between thetwo mentions, we extract its POS tag.
Given the hwof m, Pi,j refers to the sequence of POS tags in theimmediate context of hw (we exclude the POS tagof hw).
The offsets i and j denote the position (rela-tive to hw) of the first and last POS tag respectively.For instance, P?2,?1 denotes the sequence of twoPOS tags on the immediate left of hw, and P?1,+1denotes the POS tag on the immediate left of hw andthe POS tag on the immediate right of hw.Base phrase chunk We add a boolean feature todetect whether there is any base phrase chunk in thetext span between the two mentions.6 ExperimentsWe use the ACE-2004 dataset (catalogLDC2005T09 from the Linguistic Data Con-sortium) to conduct our experiments.
Followingprior work, we use the news wire (nwire) andbroadcast news (bnews) corpora of ACE-2004 forour experiments, which consists of 345 documents.To build our RE system, we use the LIBLINEAR(Fan et al, 2008) package, with its default settingsof L2-loss SVM (dual) as the solver, and we use anepsilon of 0.1.
To ensure that this baseline RE sys-tem based on the features in Section 5 is competi-tive, we compare against the state-of-the-art feature-based RE systems of (Jiang and Zhai, 2007) and(Chan and Roth, 2010).
In these works, the au-thors reported performance on undirected coarse-grained RE.
Performing 5-fold cross validation onthe nwire and bnews corpora, (Jiang and Zhai, 2007)and (Chan and Roth, 2010) reported F-measures of71.5 and 71.2, respectively.
Using the same evalua-tion setting, our baseline RE system achieves a com-petitive 71.4 F-measure.We build three RE classifiers: binary, coarse, fine.Lumping all the predefined target relations into asingle label, we build a binary classifier to predictwhether any of the predefined relations exists be-tween a given mention pair.In this work, we model the argument order of thementions when performing RE, since relations areusually asymmetric in nature.
For instance, we con-sider mi:EMP-ORG:mj and mj :EMP-ORG:mi tobe distinct relation types.
In our experiments, we ex-tracted a total of 55,520 examples or mention pairs.Out of these, 4,011 are positive relation examplesannotated with 6 coarse-grained relation types and22 fine-grained relation types5.We build a coarse-grained classifier to disam-biguate between 13 relation labels (two asymmetriclabels for each of the 6 coarse-grained relation typesand a null label).
We similarly build a fine-grainedclassifier to disambiguate between 45 relation labels.6.1 Evaluation MethodFor our experiments, we adopt the experimental set-ting in our prior work (Chan and Roth, 2010) of en-suring that all examples from a single document areeither all used for training, or all used for evaluation.In that work, we also highlight that ACE anno-tators rarely duplicate a relation link for coreferentmentions.
For instance, assume mentions mi, mj ,and mk are in the same sentence, mentions mi andmj are coreferent, and the annotators tag the men-tion pair mj , mk with a particular relation r. Theannotators will rarely duplicate the same (implicit)5We omit a single relation: Discourse (DISC).
The ACE-2004 annotation guidelines states that the DISC relation is es-tablished only for the purposes of the discourse and does notreference an official entity relevant to world knowledge.
In thiswork, we focus on semantically meaningful relations.
Further-more, the DISC relation is dropped in ACE-2005.55610 documents 5% of data 80% of dataRE model Rec% Pre% F1% Rec% Pre% F1% Rec% Pre% F1%Binary 58.0 80.3 67.4 64.4 80.6 71.6 73.2 84.0 78.2Binary+Patterns 73.1 78.5 75.7 (+8.3) 75.3 80.6 77.9 80.1 84.2 82.1Coarse 33.5 62.5 43.6 42.4 66.2 51.7 62.1 75.5 68.1Coarse+Patterns 44.2 59.6 50.8 (+7.2) 51.2 64.2 56.9 68.0 75.4 71.5Fine 18.1 47.0 26.1 26.3 51.6 34.9 51.6 68.4 58.8Fine+Patterns 24.8 43.5 31.6 (+5.5) 32.2 48.9 38.9 56.4 67.5 61.5Table 4: Micro-averaged (across the 5 folds) RE results using gold mentions.10 documents 5% of data 80% of dataRE model Rec% Pre% F1% Rec% Pre% F1% Rec% Pre% F1%Binary 32.2 46.6 38.1 35.5 48.9 41.1 40.1 52.7 45.5Binary+Patterns 46.7 45.9 46.3 (+8.2) 47.6 47.8 47.2 50.2 50.4 50.3Coarse 18.6 41.1 25.6 22.4 40.9 28.9 32.3 47.5 38.5Coarse+Patterns 26.8 34.7 30.2 (+4.6) 30.3 37.0 33.3 38.9 42.9 40.8Fine 10.7 32.2 16.1 14.6 33.4 20.3 26.9 44.3 33.5Fine+Patterns 15.7 26.3 19.7 (+3.6) 19.4 29.2 23.3 31.7 38.3 34.7Table 5: Micro-averaged (across the 5 folds) RE results using predicted mentions.relation r between mi and mk, thus leaving the goldrelation label as null.
Whether this is correct or not isdebatable.
However, to avoid being penalized whenour RE system actually correctly predicts the labelof an implicit relation, we take the following ap-proach.During evaluation, if our system correctly pre-dicts an implicit label, we simply switch its predic-tion to the null label.
Since the RE recall scoresonly take into account non-null relation labels, thisscoring method does not change the recall, but couldmarginally increase the precision scores by decreas-ing the count of RE predictions.
In our experi-ments, we observe that both the usual and our scor-ing method give very similar RE results and the ex-perimental trends remain the same.
Of course, us-ing this scoring method requires coreference infor-mation, which is available in the ACE data.6.2 RE Evaluation Using Gold MentionsTo perform our experiments, we split the 345 docu-ments into 5 equal sets.
In each of the 5 folds, 4 sets(276 documents) are reserved for drawing trainingexamples, while the remaining set (69 documents)is used as evaluation data.
In the experiments de-scribed in this section, we use the gold mentionsavailable in the data.When one only has a small amount of train-ing data, it is crucial to take advantage of externalknowledge such as the syntactico-semantic struc-tures.
To simulate this setting, in each fold, we ran-domly selected 10 documents from the fold?s avail-able training documents (about 3% of the total 345documents) as training data.
We built one binary,one coarse-grained, and one fine-grained classifierfor each fold.In Section 2, we described how we trained a base-line RE classifier (REbase) and a RE classifier usingthe syntactico-semantic patterns (REs).We first apply REbase on each test example men-tion pair (mi,mj) to obtain the RE baseline results,showing these in Table 4 under the column ?10 doc-uments?, and in the rows ?Binary?, ?Coarse?, and?Fine?.
We then applied REs on the test exam-ples as described in Section 2, showing the resultsin the rows ?Binary+Patterns?, ?Coarse+Patterns?,and ?Fine+Patterns?.
The results show that by us-ing syntactico-semantic structures, we obtain signif-icant F-measure improvements of 8.3, 7.2, and 5.5for binary, coarse-grained, and fine-grained relationpredictions respectively.6.3 RE Evaluation Using Predicted MentionsNext, we perform our experiments using predictedmentions.
ACE-2004 defines 7 coarse-grained entitytypes, each of which are then refined into 43 fine-5570123456785  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80REF1(%) ImprovementProportion (%) of data used for trainingImprovement in (gold mentions) RE by using patternsBinary+PatternCoarse+PatternFine+PatternFigure 3: Improvement in (gold mention) RE.grained entity types.
Using the ACE data annotatedwith mentions and predefined entity types, we builda fine-grained mention entity typing (MET) clas-sifier to disambiguate between 44 labels (43 fine-grained and a null label to indicate not a mention).To obtain the coarse-grained entity type predictionsfrom the classifier, we simply check which coarse-grained type the fine-grained prediction belongs to.We use the LIBLINEAR package with the same set-tings as earlier specified for the RE system.
In eachfold, we build a MET classifier using all the (276)training documents in that fold.We apply REbase on all mention pairs (mi,mj)where both mi and mj have non null entity type pre-dictions.
We show these baseline results in the Rows?Binary?, ?Coarse?, and ?Fine?
of Table 5.In Section 2, we described our algorithmic ap-proach (Figure 2) that takes advantage of the struc-tures with predicted mentions.
We show the resultsof this approach in the Rows ?Binary+Patterns?,?Coarse+Patterns?, and ?Fine+Patterns?
of Table5.
The results show that by leveraging syntactico-semantic structures, we obtain significant F-measureimprovements of 8.2, 4.6, and 3.6 for binary, coarse-grained, and fine-grained relation predictions re-spectively.7 AnalysisWe first show statistics regarding the syntactico-semantic structures.
In Section 3, we mentionedthat ACE-2004 identified five types of structures:premodifier, possessive, preposition, formulaic, and0123456785  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80REF1(%) ImprovementProportion (%) of data used for trainingImprovement in (predicted mentions) RE by using patternsBinary+PatternCoarse+PatternFine+PatternFigure 4: Improvement in (predicted mention) RE.Pattern type Rec% Pre%PreMod 86.8 79.7Poss 94.3 88.3Prep 94.6 20.0Formula 85.5 62.2Table 6: Recall and precision of the patterns.verbal.
On the 4,011 examples that we experimentedon, premodifiers are the most frequent, account-ing for 30.5% of the examples (or about 1,224 ex-amples).
The occurrence distributions of the otherstructures are 18.9% (possessive), 23.9% (preposi-tion), 7.2% (formulaic), and 19.5% (verbal).
Hence,the four syntactico-semantic structures that we fo-cused on in this paper account for a large majority(80%) of the relations.In Section 6, we note that out of 55,520 men-tion pairs, only 4,011 exhibit valid relations.
Thus,the proportion of positive relation examples is verysparse at 7.2%.
If we can effectively identify anddiscard most of the negative relation examples, itshould improve RE performance, including yieldingtraining data with a more balanced label distribution.We now analyze the utility of the patterns.
Asshown in Table 6, the patterns are effective in infer-ring the structure of mention pairs.
For instance, ap-plying the premodifier patterns on the 55,520 men-tion pairs, we correctly identified 86.8% of the 1,224premodifier occurrences as premodifiers, while in-curring a false-positive rate of only about 20%6.
We6Random selection will give a precision of about 2.2%(1,224 out of 55,520) and thus a false-positive rate of 97.8%558note that preposition structures are relatively harderto identify.
Some of the reasons are due to possi-bly multiple prepositions in between a mention pair,preposition sense ambiguity, pp-attachment ambigu-ity, etc.
However, in general, we observe that infer-ring the structures allows us to discard a large por-tion of the mention pairs which have no valid re-lation between them.
The intuition behind this isthe following: if we infer that there is a syntactico-semantic structure between a mention pair, then itis likely that the mention pair exhibits a valid rela-tion.
Conversely, if there is a valid relation betweena mention pair, then it is likely that there exists asyntactico-semantic structure between the mentions.Next, we repeat the experiments in Section 6.2and Section 6.3, while gradually increasing theamount of training data used for training the REclassifiers.
The detailed results of using 5% and 80%of all available data are shown in Table 4 and Table5.
Note that these settings are with respect to all 345documents and thus the 80% setting represents us-ing all 276 training documents in each fold.
We plotthe intermediate results in Figure 3 and Figure 4.
Wenote that leveraging the structures provides improve-ments on all experimental settings.
Also, intuitively,the binary predictions benefit the most from lever-aging the structures.
How to further exploit this is apossible future work.8 ConclusionIn this paper, we propose a novel algorithmic ap-proach to RE by exploiting syntactico-semanticstructures.
We show that this approach providesseveral advantages and improves RE performance.There are several interesting directions for futurework.
There are probably many near misses whenwe apply our structure patterns on predicted men-tions.
For instance, for both premodifier and posses-sive structures, we require that one mention com-pletely includes the other.
Relaxing this mightpotentially recover additional valid mention pairsand improve performance.
We could also try tolearn classifiers to automatically identify and disam-biguate between the different syntactico-semanticstructures.
It will also be interesting to feedback thepredictions of the structure patterns to the mentionentity typing classifier and possibly retrain to obtaina better classifier.Acknowledgements This research is supported bythe Defense Advanced Research Projects Agency(DARPA) Machine Reading Program under AirForce Research Laboratory (AFRL) prime contractno.
FA8750-09-C-0181.
Any opinions, findings,and conclusion or recommendations expressed inthis material are those of the author(s) and do notnecessarily reflect the view of the DARPA, AFRL,or the US government.We thank Ming-Wei Chang and Quang Do forbuilding the mention extraction system.ReferencesPeter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,Jenifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.Yee Seng Chan and Dan Roth.
2010.
Exploiting back-ground knowledge for relation extraction.
In Proceed-ings of the International Conference on ComputationalLinguistics (COLING), pages 152?160.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.
2007.Relex ?
Relation extraction using dependency parsetrees.
Bioinformatics, 23(3):365?371.Mark A. Greenwood and Mark Stevenson.
2006.
Im-proving semi-supervised acquisition of relation extrac-tion patterns.
In Proceedings of the COLING-ACLWorkshop on Information Extraction Beyond The Doc-ument, pages 29?35.Jing Jiang and ChengXiang Zhai.
2007.
A systematicexploration of the feature space for relation extraction.In Proceedings of Human Language Technologies -North American Chapter of the Association for Com-putational Linguistics (HLT-NAACL), pages 113?120.Jing Jiang.
2009.
Multi-task transfer learning forweakly-supervised relation extraction.
In Proceedingsof the Annual Meeting of the Association for Com-putational Linguistics and International Joint Confer-ence on Natural Language Processing (ACL-IJCNLP),pages 1012?1020.Nanda Kambhatla.
2004.
Combining lexical, syntactic,and semantic features with maximum entropy mod-els for information extraction.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 178?181.559Dan Klein and Christoper D. Manning.
2003.
Fast exactinference with a factored model for natural languageparsing.
In The Conference on Advances in NeuralInformation Processing Systems (NIPS), pages 3?10.Longhua Qian, Guodong Zhou, Qiaomin Zhu, and PeideQian.
2008.
Relation extraction using convolutiontree kernel expanded with entity features.
In PacificAsia Conference on Language, Information and Com-putation, pages 415?421.Lev Ratinov and Dan Roth.
2009.
Design challenges andmisconceptions in named entity recognition.
In Pro-ceedings of the Annual Conference on ComputationalNatural Language Learning (CoNLL), pages 147?155.Dan Roth and Wen Tau Yih.
2007.
Global inference forentity and relation identification via a linear program-ming formulation.
In Lise Getoor and Ben Taskar, ed-itors, Introduction to Statistical Relational Learning.MIT Press.Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring various knowledge in relation extrac-tion.
In Proceedings of the Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages427?434.Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen.
2009.
Statsnowball: a statistical approachto extracting entity relationships.
In The InternationalWorld Wide Web Conference, pages 101?110.560
