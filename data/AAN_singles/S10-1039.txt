Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 182?185,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSEERLAB: A System for Extracting Keyphrases from ScholarlyDocumentsPucktada Treeratpituk1Pradeep Teregowda2Jian Huang1C.
Lee Giles1,21Information Sciences and Technology2Computer Science and EngineeringPennsylvania State University, University Park, PA, USAAbstractWe describe the SEERLAB systemthat participated in the SemEval 2010?sKeyphrase Extraction Task.
SEERLAButilizes the DBLP corpus for generatinga set of candidate keyphrases from adocument.
Random Forest, a supervisedensemble classifier, is then used to selectthe top keyphrases from the candidate set.SEERLAB achieved a 0.24 F-score ingenerating the top 15 keyphrases, whichplaces it sixth among 19 participating sys-tems.
Additionally, SEERLAB performedparticularly well in generating the top 5keyphrases with an F-score that rankedthird.1 IntroductionKeyphrases are phrases that represent the impor-tant topics of a document.
There are two types ofkeyphrases associated with scholarly publications:author-assigned ones and reader-assigned ones.
Inthe Keyphrase Extraction Task (Kim et al, 2010),each system receives two set of scientific papersfrom the ACM digital library; a training set anda testing set.
The author-assigned keyphrases andreader-assigned keyphrases are given for each pa-per in the training set.
The objective is to producethe keyphrases for each article in the testing set.This paper is organized as followed.
First, Wedescribe our keyphrase extraction system, SEER-LAB.
We then discuss its performance in SemEval2010.
Lastly, we analyze the effectiveness of eachfeature used by SEERLAB, and provide a sum-mary of our findings.2 System DescriptionSEERLAB consists of three main components: asection parser, a candidate keyphrase extractor,and a keyphrase ranker.
To generate keyphrasesfor a paper, the section parser first segments thedocument into pre-defined generic section types.Secondly, the candidate keyphrase extractor gen-erates a list of candidate phrases based on the doc-ument content.
Then, the keyphrase ranker rankseach candidate according to the likelihood that itis a keyphrase.
The top candidates are selected askeyphrases of the paper.2.1 Section ParserThe goal of the section parser is to parse each doc-ument into the same set of pre-defined sections.However, segmenting a scientific article into pre-defined section types is not trivial.
While schol-arly publications generally contains similar sec-tions (such as Abstract and Conclusion), a sec-tion?s exact header description and the order inwhich it appears can vary from document to docu-ment.
For example, the ?Related Work?
section issometimes referred to as ?Previous Research?
or?Previous Work.?
Also, while the ?Related Work?section often appears right after the introduction,it could also appear near the end of a paper.
(Nguyen and Kan, 2007) had success in us-ing a maximum entropy (ME) classifier to clas-sify sections into 14 generic section types includ-ing those such as Motivation, Acknowledgement,References.
However, their approach requires an-notated training data, which is not always avail-able.
Instead, SEERLAB uses regular expres-sions to parse each document into 6 generic sec-tion types: Title, Abstract, Introduction, RelatedWork, Methodology + Experiments, and Conclu-sion + Future Work.
We decided to go with thesmaller number of section types (only 6), unlikeprevious work in (Nguyen and Kan, 2007), be-cause we believed that many sections, such as Ac-knowledgement, are irrelevant to the task.1822.2 Extracting Candidate KeyphrasesIn this section, we describe how SEERLAB de-rives a set of candidate keyphrases for a given doc-ument.
The goal of the candidate extractor is to in-clude as many actual keyphrases in the candidateset as possible, while keeping the number of can-didates small.
The performance of the candidateextractor determines the maximum achievable Re-call of the whole system.
The more correct can-didates extracted at this step, the higher the possi-ble Recall.
But a bigger candidate set potentiallycould lower Precision.
In our implementation, wedecided to ignore the Methodology + Experimentssections to limit the size of candidate sets.First, SEERLAB extracts a list of bigrams, tri-grams and quadgrams that appear at least 3 timesin titles of papers in DBLP1, ignoring those thatcontain stopwords.
Prepositions such as ?of?,?for?, ?to?
are allowed to be present in the ngrams.From 2,144,390 titles in DBLP, there are 376,577of such ngrams.
It then constructs a trie (a prefix-tree) of all ngrams so that it can later perform thelongest-prefix matching lookup efficiently.To generate candidates from a body of text, westart the cursor at the beginning of the text.
TheDBLP trie is then used to find the longest-prefixmatch.
If no match is found, the cursor is movedto the next word in the text.
If a match is found,the matched phrase is extracted and added to thecandidate set, while the cursor is moved to the endof the matched phrase.
The process is repeateduntil the cursor reaches the end of the text.However, the trie constructed as describedabove can only produce non-unigram candidatesthat appear in the DBLP corpus.
For example,it is incapable of generating candidates such as?preference elicitation problem,?
which does notappear in DBLP, and ?bet,?
which is an unigram.To remedy such limitations, for each document wealso include its top 30 most frequent unigrams,its top 30 non-unigram ngrams and the acronymsfound in the document as candidates.Our method of extracting candidate keyphrasesdiffers from most previous work.
Previous work(Kim and Kan, 2009; Nguyen and Kan, 2007) useshand-crafted regular expressions for candidate ex-tractions.
Many of these rules also require POS(part of speech) inputs.
In contrast, our methodis corpus-driven and requires no additional inputfrom the POS tagger.
Additionally, our approach1http://www.informatik.uni-trier.de/ ley/db/index.htmlallows us to effectively include phrases that appearonly once in the document as candidates, as longas they appear more than twice in the DBLP data.2.3 Ranking KeyphrasesWe train a supervised Random Forest (RF) clas-sifier to identify keyphrases from a candidate set.A Random Forest is a collection of decision trees,where its prediction is simply the aggregated votesof each tree.
Thus, for each candidate phrase, thenumber of votes that it receives is used as its fit-ness score.
Candidates with the top fitness scoresare then chosen as keyphrases.
The detail of theRandom Forest algorithm and the features used inthe model are given below.2.3.1 FeaturesWe represent each candidate as a vector of fea-tures.
There are the total of 11 features.N: The length of the keyphrase.ACRO: A binary feature indicating whether thekeyphrase appears as an acronym in the document.TFdoc: The number of times that the keyphraseappears in the document.DF: The document frequency.
This is com-puted based on the DBLP data.
For document-specific candidates (unigrams and those not foundin DBLP), their DFs are set to 1.TFIDF: The TFIDF weight of the keyphrase,computed using TFdocand DF.TFheaders: The number of occurrences that thekeyphrase appears in any section headers and sub-section headers.TFsectioni: The number of occurrences thatthe keyphrase appears in the sectioni, wheresectioni?
{Title, Abstract, Introduction, RelatedWork, Conclusion}.
These accounted for the totalof 5 features.2.3.2 Random ForestSince a random forest (RF) is an ensemble clas-sifier combining multiple decision trees (Breiman,2001), it makes predictions by aggregating votesof each of the trees.
To built a random forest, mul-tiple bootstrap samples are drawn from the origi-nal training data, and an unpruned decision tree isbuilt from each bootstrap sample.
At each nodein a tree, when selecting a feature to split, the se-lection is done not on the full feature set but on arandomly selected subset of features instead.
The183Gini index2, which measures the class dispersionwithin a node, is used to determine the best splits.RFs have been successfully applied to variousclassification problems with comparable resultsto other state-of-the-art classifiers such as SVM(Breiman, 2001; Treeratpituk and Giles, 2009).
Itachieves high accuracy by keeping a low bias ofdecision trees while reducing the variance throughthe introduction of randomness.One concern in training Random Forests foridentifying keyphrases is the data imbalancedproblem.
On average, 130 candidates are extractedper document but only 8 out of 130 are correctkeyphrases (positive examples).
Since the trainingdata is highly imbalanced, the resulting RF classi-fier tends to be biased towards the negative classexamples.
There are two methods for dealing withimbalanced data in Random Forests (Chen et al,2004).
The first approach is to incorporate classweights into the algorithm, giving higher weightsto the minority classes, so that misclassifying aminority class is penalized more.
The other ap-proach is to adjust the sampling strategy by down-sampling the majority class so that each tree isgrown on a more balanced data.
In SEERLAB,we employ the down-sampling strategy to correctthe imbalanced data problem (See Section 3).3 ResultsIn this section, we discuss the performance andthe implementation detail of our system in theKeyphrase Extraction Task.
Each model in the ex-periment is trained on the training data, containing144 documents, and is evaluated on a separate dataset of 100 documents.
The performance of eachmodel is measured using Precision (P), Recall (R)and F-measure (F) for the top 5, 10 and 15 can-didates.
A keyphrase is considered correct if andonly if it exactly matches one of the answer keys.No partial credit is given.Three baseline systems were provided by the or-ganizer: TF.IDF,NB andME.
All baselines use thesimple unigrams, bigrams and trigrams as candi-dates and TFIDF as features.
TF.IDF is an unsu-pervised method that ranks each candidate basedon TFIDF scores.
NB and ME are supervisedNaive Bayes and Maximum Entropy respectively.We use the randomForest package in R for our2For a set S of data with K classes, its Gini index is definedas: Gini(S) =PKj=1p2j, where pidenotes the probabilityof observing class i in S.tf.headerstf.conclusiontf.related_worktf.abstf.introtf.docacrotf.titletfidfdfn0.02 0.06MeanDecreaseAccuracyacrotf.titletf.related_worktf.headerstf.abstf.conclusionntf.introtf.docdftfidf0 10 25MeanDecreaseGiniFigure 1: Variable importance for each featurekeyphrase ranker (Liaw and Wiener, 2002).
AllRF models are built with the following parame-ters: the number of trees = 200 and the number offeatures considered at each split = 3.
The averagetraining and testing time are around 15s and 5s.Table 1. compares the performance of threedifferent SEERLAB models against the baselines.RF0is the basic model, where the training datais imbalanced.
For RF1:1, the negative examplesare down-sampled to make the data balanced.
ForRF1:7, the negative examples are down-sampledto where its ratio with the positive examples is 7to 1.
All three models significantly outperformthe baselines.
The RF1:7model has the high-est performance, while the RF1:1model performsslightly worse than the basic model RF0.
Thisshows that while the sampling strategy helps, over-doing it can hurt the performance.
The optimalsampling ratio (RF1:7) is chosen according to a10-fold cross-validation on the training data.
Forthe top 15 candidates, RF1:7?s F-score (C) rankssixth among the 19 participants with a 24.34% F-score approximately 1% lower than the third placeteam.
We also observed that SEERLAB performsquite well for the top 5 candidates with 39% Preci-sion (C).
Its F-scores at the top 5, 19.84% (C) and18.19% (R), place SEERLAB third and second re-spectively among other participants.Figure 1. shows two variable importance in-dicators for each feature: mean decrease accu-racy (MDA) and mean decrease Gini (MDG).Both indicators measure each feature?s contribu-tion in identifying whether a candidate phrase isa keyphrase.
The MDA of a feature is computedby randomly permuting the value of that feature inthe training data and then measuring the decreasein prediction accuracy.
If the permuted feature is184System by top 5 candidates top 10 candidates top 15 candidatesP R F P R F P R FTF.IDF R 17.80 7.39 10.44 13.90 11.54 12.61 11.60 14.45 12.87C 22.00 7.50 11.19 17.70 12.07 14.35 14.93 15.28 15.10NB R 16.80 6.98 9.86 13.30 11.05 12.07 11.40 14.20 12.65C 21.40 7.30 10.89 17.30 11.80 14.03 14.53 14.87 14.70ME R 16.80 6.98 9.86 13.30 11.05 12.07 11.40 14.20 12.65C 21.40 7.30 10.89 17.30 11.80 14.03 14.53 14.87 14.70SEERLAB (RF0) R 29.00 12.04 17.02 22.50 18.69 20.42 18.20 22.67 20.19C 36.00 12.28 18.31 28.20 19.24 22.87 22.53 23.06 22.79SEERLAB (RF1:1) R 26.00 10.80 15.26 20.80 17.28 18.88 17.40 21.68 19.31C 32.00 10.91 16.27 26.00 17.74 21.09 21.93 22.44 22.18SEERLAB (RF1:7) R 31.00 12.87 18.19 24.10 20.02 21.87 19.33 24.09 21.45C 39.00 13.30 19.84 29.70 20.26 24.09 24.07 24.62 24.34Table 1: Performance (%) comparison for the Keyphrase Extraction Task.
R (Reader) indicates that thereader-assigned keyword is used as the gold-standard and C (Combined) means that both author-assignedand reader-assigned keyword sets are used.a very good predictor, then the prediction accu-racy should decrease substantially from the orig-inal model.
The MDG of a feature implies thataverage Gini decreases for the nodes in the forestthat use that feature as the splitting criteria.TFIDF and DF are good indicators of perfor-mance according to both MDA and MDG.
Bothare very effective when used as splitting criteria,and the prediction accuracy is very sensitive tothem.
Surprisingly, the length of the phrase (N)also has high importance.
Also, TFtitleand ACROhave high MDA but low MDG.
They have highMDA because if a candidate phrase is an acronymor appears in the title, it is highly likely that itis a keyphrase.
However, most keyphrases arenot acronyms and do not appear in titles.
Thus,on average as splitting criteria, they do not de-crease Gini index by much, resulting in a lowMDG.
Also, TFrelated workand TFheadershavelower MDA and MDG than TF of other sections(TFintro, TFabs, and TFconclusion).
This mightsuggest that the occurrences in the ?Related Work?section or section headers are not strong indica-tors of being a keyphrase as the occurrences in thesections ?Introduction,?
?Abstract?
and ?Conclu-sion.
?4 ConclusionWe have described our SEERLAB system thatparticipated in the Keyphrase Extraction Task.SEERLAB combines unsupervised corpus-basedapproach with Random Forests to identifykeyphrases.
The experimental results show thatour system performs well in the Keyphrase Ex-traction Task, especially on the top 5 key phrasecandidates.
We also show that the down-samplingstrategy can be used to enhance our performance.ReferencesLeo Breiman.
2001.
Random forests.
Machine Learn-ing, Jan.Chao Chen, Andy Liaw, and Leo Breiman.
2004.
Us-ing random forest to learn imbalanced data.
Techni-cal Report, University of California, Berkeley.Su Nam Kim and Min-Yen Kan. 2009.
Re-examiningautomatic keyphrase extraction approaches in scien-tific articles.
Proceedings of the Workshop on Mul-tiword Expressions, ACL-IJCNLP, Jan.Su Nam Kim, Olena Medelyan, Min-Yen Kan, andTimothy Baldwin.
2010.
Semeval-2010 task 5: Au-tomatic keyphrase extraction from scienctific article.ACL workshop on Semantic Evaluations (SemEval2010).Andy Liaw and Matthew Wiener.
2002.
Classificationand regression by randomforest.
R News.Thuy Dung Nguyen and Min-Yen Kan. 2007.Keyphrase extraction in scientific publications.
Pro-ceedings of International Conference on Asian Dig-ital Libraries (ICADL?07), Jan.Pucktada Treeratpituk and C Lee Giles.
2009.
Dis-ambiguating authors in academic publications usingrandom forests.
In Proceedings of the Joint Confer-ence on Digital Libraries (JCDL?09), Jan.185
