Unsupervised Named Entity Classification Modelsand their EnsemblesJae-Ho Kim*, In-Ho Kang, Key-Sun Choi*Korea Advanced Institute of Science and Technology (KAIST) /Korea Terminology Research Center for Language and Knowledge Engineering* (KORTERM)373-1, Guseong-dong, Yuseong-guDaejeon, KOREA, 305-701{jjaeh@world, ihkang@csone, kschoi@world}.kaist.ac.krAbstractThis paper proposes an unsupervisedlearning model for classifying namedentities.
This model uses a training set, builtautomatically by means of a small-scalenamed entity dictionary and an unlabeledcorpus.
This enables us to classify namedentities without the cost for building a largehand-tagged training corpus or a lot of rules.Our model uses the ensemble of threedifferent learning methods and repeats thelearning with new training examplesgenerated through the ensemble learning.The ensemble of various learning methodsbrings a better result than each individuallearning method.
The experimental resultshows 73.16% in precision and 72.98% inrecall for Korean news articles.1 IntroductionNamed entity extraction is an important step forvarious applications in natural languageprocessing.
Named entity extraction involvesidentifying named entities in the text andclassifying their types such as person,organization, location, time expressions,numeric expressions, and so on (Sekine andEriguchi, 2000).One might think the named entities can beclassified easily using dictionaries because mostof named entities are proper nouns, but this iswrong opinion.
As time passes, new propernouns are created continuously.
Therefore it isimpossible to add all those proper nouns to adictionary.
Even though named entities areregistered in the dictionary it is not easy todecide their senses.
They have a semantic(sense) ambiguity that a proper noun hasdifferent senses according to the context (NinaWacholder, et al, 1997).
For example, ?UnitedStates?
refers either to a geographical area or tothe political body which governs this area.
Thesemantic ambiguity is occured frequently inKorean (Seon, et al 2001).
Let us illustrate this.Example 1 : LocationLet?s meet at KAIST.KAIST       e-seo   man-na-ja .
(PN:KAIST)  (PP:at)  (V:meet)Example 2 : OrganizationKAIST announced the list of successful candidates.KAIST      e-seo     hab-gyeok-ja(PN:KAIST)  (PP)  (N:successful candidates)myeong-dan  eul   bal-pyo-haet-da .
(N:list)      (PP)  (V:announced)PN : proper noun, N : noun, PP : postposition, V : verbIn the above examples, ?KAIST?
has differentcategories although same postposition, ?e-seo?,followed.
The classification of named entities inKorean is a little more difficult than in English.There are two main approaches to classifynamed entities.
The first approach employshand-crafted rules.
It costs too much to maintainrules because rules and dictionaries have to bechanged according to the application.
Thesecond belongs to a supervised learningapproach, which  employs a statistical method.As it is more robust and requires less humanintervention, several statistical methods based ona hidden Markov model (Bikel et al, 1997), aMaximum Entropy model (Borthwich et al,1998) and a Decision Tree model (B?chet et al2000) have been studied.
The supervisedlearning approach requires a hand-taggedtraining corpus, but it can not achieve a goodperformance without a large amount of databecause of data sparseness problem.
Forexample, Borthwich (1999) showed theperformance of 83.45% in Precision and 77.42%in F-measure for identifying and classifying the8 IREX (IREX committee, 1999) categories,with 294,000 tokens IREX training corpus.
Ittakes a lot of time and labor to build a largecorpus like this.This paper proposes an unsupervised learningmodel that uses a small-scale named entitydictionary and an unlabeled corpus forclassifiying named entities.
Collins and Singer(1999) opened the possibility of using anunlabeled corpus to classify named entities.They showed that the use of unlabeled data canreduce the requirements for supervision to just 7simple seed rules.
They used natural redundancyin the data : for many named-entity instances,both the spelling of the name and the context inwhich it appears are sufficient to determine itstype.Our model considers syntactic relations in asentence to resolve the semantic ambiguity anduses the ensemble of three different learningmethods to improve the performance.
They areMaximum Entropy Model, Memory-basedLearning and Sparse Network of Winnows(Roth, 1998).This model classifies proper nouns appearedin the documents into person, organization andlocation on the assumption that the boundariesof proper nouns were already recognized.2 The System for NE ClassificationThis section describes a system that classifiesnamed entities by using a machine learningalgorithm.
The system consists of four modulesas shown in Figure 1.First, we builds a training set, named entitytagged corpus, automatically.
This set will beused to predict the categories of named entitieswithin target documents received as the input ofthe system.The second module extracts syntacticrelations from the training set and targetdocuments.
They are encoded to the format oftraining and test examples for machine learning.In the third module, each learning forclassification is progressed independently bythree learning methods.
Three results generatedby each learner are combined into one result.Finally, the system decides the category byusing a rule for the test examples that did not belabeled yet.
And then the system outputs anamed entity tagged corpus.Extracting Syntactic RelationsBuildinga Training SetTraining SetTarget DocumentsEnsemble LearningPost-ProcessingNE Tagged CorpusInputOutputFigure 1.
System Architecture2.1 Building a Training SetThe system requires a training set which hascategories in order to get knowledge for theclassification.
We build a training setautomatically using a named entity dictionaryand a POS tagged corpus, and then use it insteadof a hand-tagged set in machine learning.We randomly extract 1500 entries per eachcategory (person, location, and organization)from a Proper Noun dictionary made byKORTERM and then reconstruct the namedentity dictionary.
The Proper Noun dictionaryhas about 51,000 proper nouns classified into 41categories (person, animal, plant and etc.).
Wedo not extract homonyms to reduce theambiguity.
In order to show that it is possible toclassify named entities with a small-scaledictionary, we limit the number of entries to be1500.We label the target word, proper noun orcapital alphabet, appeared in the POS taggedcorpus 1  by means of the NE dictionarymentioned above.
The corpus is composed of1 We used a KAIST POS tagged corpusone million eojeols2.
It is not easy to classifynamed entity correctly only with a dictionary,since named entity has the semantic ambiguity.So we have to consider the context around thetarget word.In order to consider the context, we useco-occurrence information between the category(c) of a target word (tw) and a head word (hw)appeared on the left of the target word or theright of the target word.
We modify categorieslabeled by the NE dictionary by followingprocess.1.
We extract pairs [c, hw] from the corpuslabeled by means of the dictionary.2.
If hw is occurred with several differentcategories, we suppose tw occurred withhw may have an ambiguity and then weremove the category label of tw.3.
We make rules for predicting the categoryof tw from pairs [c, hw] and apply them tothe corpus.
The rule is that tw occurredwith hw has a c.4.
We extract sentences including thelabeled target word in the corpus.In the step 3, 9 rules are made.
We label the cfor unlabeled target word occurred with hw ifthe pair [c, hw] is found more than a threshold.We set the threshold to be 10.
Sentencesincluding the 4,504 labeled target word are madeas a tringing set in this process (Table 1).Table 1.
The number of the target words in atraining setState # of target wordsCandidates in the corpus 37,831Labeled by the dictionary 3,899Removed by the ambiguity 778Added by 9 rules 1,383Total 4,5042.2 Extracting Syntactic RelationsIn order to predict the category, most of machinelearning systems usually consider two words onthe left and two ones on the right of a targetword as a context (Uchimoto and et al 2000,2 Korean linguistic units that is separated by blank orpunctuationPetasis and et al 2000).
However this methodhave some problems.If some words that are not helpful to predictthe category are near the target word, they cancause an incorrect prediction.
In the followingexample, ?Kim?
can be predicted as anorganization instead of a person because of a leftword ?Jeong-bu?
(the government).ExampleThe goverment supports KIA on the premise thatthe chairman Kim submits a resignation.Jeong-bu        neun Kim  hoi-jang     i(N:the goverment)  (PP) (PN) (N:the chairman) (PP)sa-pyo        reul  je-chul-han-da neun(N:a resignation) (PP)   (V :submit)     (PP)jeon-je       ro  KIA  reul  ji-won-han-da.
(N :the premise)(PP) (PN)   (PP)   (V :support)PN : proper noun, N : noun, PP : postposition, V : verbThe system cannot consider important wordsthat are out of the limit of the context.
In theformer example, the word ?je-chul-han-da?
(submit) is an important feature for predictingthe category of ?Kim?.
If a Korean functionalword is counted as one window, we cannot getthis information within right 4 windows.
Even ifwe do not count the functional words,sometimes it is neccessary to consider largerwindows than 2 windows like above example.We notice that words that modify the targetword or are modified by the target word aremore helpful to the prediction than any otherwords in the sentence.
So we extract thesyntactic relations like Figure 2 as the context.BLANK Kim hoi-jang(chairman)je-chul-han-da(present)imodifier targetword modifiee predicatejosaBLANK KIA BLANK ji-won-han-da(support)reulFigure 2.
Syntactic relations for the target wordThe modifier is a word modifying the targetword and the modifiee is one modified by thetarget word.
Josa3 is a postposition that followsthe target word and te predicate is a verb thatpredicates the target word.
The ?BLANK?
labelrepresents that there is no word whichcorresponds to the slot of the templet.
Thesesyntactic relations are extracted by a simpleheuristic parser.
We will show that thesesyntactic relations bring to a better resultthrough an experiment in the section 3.These syntactic relations seem to be languagespecific.
Josa represents a case for the targetword.
If case information is extracted in asentence, these syntactic relations like Figure 2are also made in other languages.As machine learner requires training and testexamples represented in a feature-vector format,syntactic relations are encoded as Figure 3.Feature-vector formatlexical morpheme (w) Modifier POS tag (t)lexical morpheme (w) Target word POS tag (t)lexical morpheme (w) Modifiee POS tag (t)Josa lexical morpheme (w)Predicate lexical morpheme (w)Category  Label tagTraining example : [w, t, w, t, w, t, w, w, person]Test example    : [w, t, w, t, w, t, w, w, Blank]Figure 3: The format of an example for learning2.3 Ensemble LearningThe ensemble of several classifiers can beimprove the performance.
Errors made by theminority can be removed through the ensembleof classifiers (Thomas G. Dietterich, 1997).
Inthe base noun phrase identification, Tjong KimSang, et al (2000) showed that the resultcombined by seven different machine learningalgorithms outperformed the best individualresult.In our module, machine learners train with thetraining examples and then classify the namedentities in the test examples.
This process isshown in Figure 4.3 Josa, attached to a nominal, is a postpositionalparticle in Korean.Ending Condition is satisfied?YesNoLabeled Test ExamplesMachine Learners learn with training examplesThe classification for the test examples is progressedby three different learners independentlyThree results are combined by combining techniquesTraining examples are modified(labeled examples in the combined result + initial training examples)LoopFigure 4.
The process of the Ensemble LearningThis ensemble learning has two characteristics.One is that the classification is progressed bythree different learners independently and thoseresults are combined into one result.
The other isthat the learning is repeated with new trainingexamples generated through the learning.
Itenables the system to receive an incrementalfeedback.Through the this learning method, we can getlarger and more precise training examples forpredicting the categories.
It is important in anunsupervisd learning model because there is nolabeled data for learning.2.3.1 Machine Learning algorithmsWe use three learning methods : Memory-basedLearning, Sparse Network of Winnows,Maximum Entropy Model.
We describe thesemethods briefly in this section.Memory-based Learning stores the trainingexamples and classifies new examples bychoosing the most frequent classification amongtraining examples which are closest to a newexample.
Examples are represented as sets offeature-value pairs.
Each feature receives aweight which is based on the amount ofinformation which it provides for computing theclassification of the examples in the training data.We use the TiMBL (Daelemans, et al, 1999), aMemory-Based Learning software package.Sparse Network of Winnows learningarchitecture is a sparse network of linear units.Nodes in the input layer of the network representsimple relations over the input example andthings being used as the input features.
Eachlinear unit is called a target node and representsclassifications which are interested in the inputexamples.
Given training examples, each inputexample is mapped into a set of features whichare active (present) in it; this representation ispresented to the input layer of SNoW andpropagated to the target nodes.
We use SnoW(Carlson, et al, 1999), Sparse Network ofWinnows software package.Maximum Entropy Model (MEM) isespecially suited for integrating evidences fromvarious information sources.
MEM allows thecomputation of p(f|h) for any f in the space ofpossible futures, F, and for every h in the spaceof possible histories, H. Futures are defined asthe possible classification and a history is all ofthe conditioning data which enable us to make adecision in the space of futures.
Thecomputation of p(f|h) is dependent on a set offeatures which are binary functions of thehistroy and future.
A feature is represented asfollowing.?????????
?=otherwise 0future  the  of  one  is  f  andcondition  some  meets  h if1),( fhgGiven a set of features and some trainingexamples, a weighing parameter i?
for everyfeature ig  is computed.
This allows us tocompute the conditional probability as follows :)()|(),(hZhfPifhgii???=?
?=f ifhgiihZ ),()( ?
?We use MEMT, Maximum Entropy ModelingToolkit (Ristad, 1998), to compute the parameterfor the features.2.3.2 Combining TechniquesWe use three different voting mechanisms tocombine results generated by three learners.The first method is a majority voting.
Eachclassification receives the same weight and themost frequent classification is chosen.
Theending condition is satisfied when there is nodifference between a result combined in thisloop and one combined in the former loop.The second method is a probability voting.MEMT and SNoW propose the probabilities forall category, but Timbl proposes only oneappropriate category for one test example.
Weset the probability for the category Timblproposes to be 0.6 and for the others to be 0.2.For each category, we multiply probabilitiesproposed by 3 learners and then choose Nexamples that have the largest probability.
In thenext learning we set N = N + 100.
When N islarger than a threshold, the ending condition issatisfied and the learning is over.
We set it to be3/4 of the number of test examples.The last method is a mixed voting.
We usetwo voting methods mentioned above one afteranother.
First, we use probability voting.
Afterthe learning is over we use majority voting.
Thethreshold of the probability voting is 1/2 of thenumber of test examples here.2.4 Post-ProcessingAfter the learning, the system modifies testexamples by using a rule, one sense perdiscourse.
One sense per discourse means thatthe sense of a target word is highly consistentwithin any given document.
David Yarowsky(1995) showed it was accurate in the word sensedisambiguation.
We label the examples that arenot labeled yet as the category of the labeledword in the discourse as following example andwe output named entity tagged corpus.Exampleafter the ensemble learning... ... KIA<type=organization> reul ji-won-han-da.KIA neon ... ...after post-processing... ... KIA<type=organization> reul ji-won-han-da.KIA<type=organization> neon ... ...3 Experimental ResultsWe used Korean news articles that consist of24,647 eojeols and contain 2,580 named entitiesas a test set.
The number of named entitieswhich belong to each category is shown in Table2.
When even a human could not classify namedentities, ?Unknown?
is labeled and it is ignoredfor the evaluation.
?Other?
is used for the wordoutside the three categories.Table 3 shows the result of the classification.The first row shows the result of theclassification using only a NE dictionary.
Therecall (14.84%) is very low because the systemuses a small-scale dictionary.
The precision(91.56%) is not 100% because of the semanticambiguity.
It means that it is necessary to refineclassifications created by a dictionary.We build a training set with a NE dictionaryand a POS tagged corpus and refine it withco-occurrence information.
The second rowshows the result of the classification using thistraining set without learning.
We can observethat the quality of the training set is improvedthanks to our refining method.A Mixed Voting shows the best results.
Itimproves the performance by taking goodcharacteristics of a majority voting andprobability voting.Table 2.
The number of named entities whichbelong to each category in the test setCategory # of NEs Category # of NEsPerson 459 Other 307Organization 814 Unknown 242Location 758 Total 2,580Table 3.
The result of the classificationMethod Precision  Recall F-measureDictionarybased 91.56% 14.84% 25.54%Training setb1ased 94.32% 20.64% 33.87%MajorityVoting 69.70% 65.74% 67.68%ProbabilityVoting 75.90% 63.45% 69.12%MixedVoting 73.16% 72.98% 73.07%We extract the syntatic relations and make 5windows (modifier, target word, modifiee, josa,predicate) as a context.
We conduct acomparative experiment using the Uchimoto?smethod, 5 windows (two words before/after thetarget word) and then we show that our methodbrings to a better result (Table 4).Table 4.
Comparison with two kinds of windowsizeWindows Precision  Recall F-measureUchimoto?s 66.86% 69.94% 68.37%Ours 73.16% 72.98% 73.07%We try to perform the co-training similar toone of Colins and Singer in the sameexperimental environment.
We extractcontextual rules from our 5 windows because wedoes not have a full parser.
The learning isstarted from 417 spelling seed rules made by theNE dictionary.
We use two independent contextand spelling rules in turn.
Table 5 shows that ourmethod improve the recall much more on thesame conditions.Table 5.
Comparison with two kinds ofunsupervised learning methodMethod Precision  Recall F-measureCo-trianing 84.62% 37.63% 52.09%Ours 73.16% 72.98% 73.07%Through the ensemble of various learningmethods, we get larger and more precise trainingexamples for the classification.
Table 6 showsthat the ensemble learning brings a better resultthan each individual learning method.Table 6.
The comparison of an ensemble learningand each individual learningLearner Precision  Recall F-measureMEMT 65.19% 61.54% 63.31%SNoW 66.93% 70.53% 68.68%Timbl 64.14% 67.59% 65.82%Ensemble 73.16% 72.98% 73.07%Three learners can use different kinds offeatures instead of same features.
We conduct acomparative experiment as following.
Asfeatures, SNoW uses a modifier and a targetword, Timbl uses a modifiee and a target word,and MEMT uses a josa, a predicate and a targetword.
Table 7 shows that the learning usingdifferent kinds of features has the lowperformance because of the lack of information.Table 7.
The comparison with the learnings usingdifferent featuresFeatures Precision  Recall F-measureSeperated  61.69% 49.85% 55.14%Same  73.16% 72.98% 73.07%The system repeats the learning with newtraining examples generated through theensemble learning.
We can see that this loopbrings to the better result as shown in Table 8.After the learning, we apply the rule, a senseper discourse.
?Post?
in Table 8 indicates theperformance after this post-processing.
It Thepost-processing improves the performance alittle.Table 8.
The improvement of the performancethrough the repeated learningMethod Loop Precision Recall F-measure1st 94.35% 20.76% 34.03%19th 76.72% 59.97% 67.32% Probability Voting Post 75.90% 63.45% 69.12%We extracted the syntactic relations by using asimple heuristic parser.
Because this parser doesnot deal with complex sentences, the failure ofparsing causes the lack of information or wronglearning.
Most of errors are actually occurred byit, therefore we need to improve the performanceof the parser.4 ConclusionWe proposed an unsupervised learning modelfor classifying the named entities.
This modelused a training set, built automatically by asmall-scale NE dictionary and an unlabeledcorpus, instead of a hand-tagged training set forlearning.
The experimental result showed73.16% in precision and 72.98% in recall forKorean news articles.
This means that it ispossible to classify named entities without thecost for building a large hand-tagged trainingcorpus or a lot of rules.The learning for classification was progressedby the ensemble of three different learningmethods.
Then the ensemble of various learningmethods brings a better result than eachindividual learning method.ReferencesB?chet, Fr?d?ric, Alexis Nasr and Franck Genet, 2000.
"Tagging Unknown Proper Names Using DecisionTrees", In proceedings of the 38th Annual Meeting of theAssociation for Computational Linguistics.Bikel, Daniel M., Scott Miller, Richard Schwartz and RalphWeischedel, 1997.
"Nymble: a High-PerformanceLearning Name-finder", In Proceedings of the FifthConference on Applied Natural Language Processing.Borthwick, Andrew, John Sterling, Eugene Agichtein andRalph Grishman, 1998.
"NYU: Description of the MENENamed Entity System as Used in MUC-7", InProceedings of the Seventh Message UnderstandingConference (MUC-7).Borthwick, 1999.
?A Japanese Named Entity RecognizerConstructed by a Non-Speaker of Japanese?, IREX.Proceedings of the IREX workshop.Carlson, Andrew J., Chad M. Cumby, Jeff L. Rosen andDan Roth, 1999.
"SNoW User Guide", University ofIllinois.
http://l2r.cs.uiuc.edu/~cogcomp/Collins, Michael and Yoram Singer.
1999.
"Unsupervisedmodels for named entity classification", In proceedingsof the Joint SIGDAT Conference on Empirical Methodsin Natural Language Processing and Very LargeCorpora.Daelemans, Walter, Jakub Zavrel, Ko van der Sloot andAntal van den Bosch, 1999.
"TiMBL: Tilburg MemoryBased Learner, version 4.0, Reference Guide", ILKTechnical Report 01-04. http://ilk.kub.nl/Dietterich, T. G., 1997.
?Machine-Learning Research: FourCurrent Dirctions?, AI Magazine 18(4): 97IREX Committee (ed.
), 1999.
Proc.
the IREX Workshop.http://cs.nyu.edu/cs/projects/proteus/irexPetasis, Georgios, Alessandro Cucchiarelli, Paola Velardi,Georgios Paliouras, Vangelis Karkaletsis andConstantine D. Spyropoulos, 2000.
"Automaticadaptation of Proper Noun Dictionaries throughcooperation of machine learning and probabilisticmethods", Proceedings of the 23rd ACM SIGIRConference on R&D in IR (SIGIR).Ristad, Eric Sven, 1998.
"Maximum Entropy ModelingToolkit".Roth, Dan, 1998.
?Learning to resolve natural languageambiguities: A unified approach?, In Proc.
NationalConference on Artificial Intelligence.Sekine, Satoshi and Yoshio Eriguchi.
2000.
"JapaneseNamed Entity Extraction Evaluation", In the proceedingsof the 18th COLING.Seon, Choong-Nyoung, Youngjoong Ko, Jeong-Seok Kimand Jungyun Seo, 2001.
?Named Entity Recognitionusing Machine Learning Methods and Pattern-SelectionRules?, Proceedings of the Sixth Natural LanguageProcessing Pacific Rim Symposium.Tjong Kim Sang, Erik F., Walter Daelemans, Herv?
D?jean,Rob Koeling, Yuval Krymolowski, Vasin Punyakanok,Dan Roth, 2000.
?Applying System Combination to BaseNoun Phrase Identification?, In the proceedings of the18th COLING.Uchimoto, Kiyotaka, Qing Ma, Masaki Murata, HiromiOzaku and Hitoshi Isahara, 2000.
?Named EntityExtraction Based on A Maximum Entropy Model andTransformation Rules", In proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics.Wacholder, Nina, Yael Ravin and Misook Choi (1997)"Disambiguation of Proper Names in Text", Proceedingsof the 5th Applied Natural Language ProcessingConference.Yarowsky, David, 1995.
"Unsupervised Word SenseDisambiguation Rivaling Supervised Methods", InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics.
