Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 169?178,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsLandmark-based Location Belief Tracking in a Spoken Dialog SystemYi MaThe Ohio State UniversityColumbus, OH 43210may@cse.ohio-state.eduAntoine Raux, Deepak Ramachandran, Rakesh GuptaHonda Research Institute, USA425 National Ave, Mountain View, CA 94043{araux,dramachandran,rgupta}@hra.comAbstractMany modern spoken dialog systems useprobabilistic graphical models to update theirbelief over the concepts under discussion, in-creasing robustness in the face of noisy input.However, such models are ill-suited to prob-abilistic reasoning about spatial relationshipsbetween entities.
In particular, a car naviga-tion system that infers users?
intended desti-nation using nearby landmarks as descriptionsmust be able to use distance measures as a fac-tor in inference.
In this paper, we describea belief tracking system for a location iden-tification task that combines a semantic belieftracker for categorical concepts based on theDPOT framework (Raux and Ma, 2011) witha kernel density estimator that incorporateslandmark evidence from multiple turns andlandmark hypotheses, into a posterior proba-bility over candidate locations.
We evaluateour approach on a corpus of destination set-ting dialogs and show that it significantly out-performs a deterministic baseline.1 IntroductionMobile devices such as smart phones and in-car in-fotainment systems have generated demand for anew generation of location-based services such aslocal business search, turn-by-turn navigation, andsocial event recommendation.
Accessing such ser-vices in a timely manner through speech is a crucialrequirement, particularly on the go when the user isunable to resort to other modalities e.g.
where safetyregulations prohibit drivers from using buttons or atouchscreeen while driving.In such systems, a Point of Interest (POI)or a destination such as a restaurant, store or apublic place is often specified.
For example, acar navigation system needs the user to input thedestination before giving directions.
Similarly, aphoto tagging application must allow its users todesignate the location where a picture was taken.While postal addresses can be used to unambigouslyidentify locations, they are often either unknownor hard for users to remember.
A more natural(though potentially ambiguous) means of speci-fying locations is to use landmarks such as ?theItalian restaurant near Red Rockcafe on Castro Street?
or ?the bakerynear that mall with a Subway anda 7 Eleven?.
A location-based dialog systemthat understands referring expressions using land-marks could lead to more succinct dialogs, higherrecognition accuracy and a greater appearance ofintelligence to the user.We present a system that performs belief track-ing over multiple turns of user speech input to inferthe most probable target location.
The user inter-acts with the system through speech in order to spec-ify a target location, and may include references toone or more landmarks.
Such a system must han-dle two sources of uncertainty.
First, ASR is notori-ously error-prone and modern ASR engines provideranked lists of possible interpretations of speech in-put rather than single hypotheses.
Second, the suit-ability of a particular landmark or its likelihood ofusage by the speaker depends on a number of factorssuch as distance, size and prominence of the land-mark, familiarity of the user and his expectation of169common ground for understanding.
These factors,or at least the resulting variability, must be taken intoaccount when making inferences about target loca-tions from landmark-based expressions.The first source of ambiguity (speech understand-ing) has been the target of research on belief tracking(Mehta et al, 2010; Raux and Ma, 2011; Thomsonand Young, 2010).
In previous work, the conceptsof interest are entities that are ontologically related(i.e.
with is-a or has-a relations), thus discrete prob-abilistic graphical models such as DBNs have gen-erally sufficed as representations.
But these mod-els are ill-suited for dense continuous spatial rela-tions like the distance between any two locations ona map.
In this paper, we introduce a kernel-basedbelief tracker as a probabilistic model for inferringtarget locations from (uncertain) landmarks.
Thekernel-based representation allows a natural way toweigh the suitability of a landmark and the speechunderstanding confidence.
The output of this trackeris combined with that of a Dynamic ProbabilisticOntology Tree (DPOT) (Raux and Ma, 2011), whichperforms ontological reasoning over other featuresof the target location, to give a posterior distribu-tion over the intended location.
We evaluate our ap-proach on a new corpus of location setting dialogsspecially collected for this work and find it to signif-icantly outperform a deterministic baseline.2 Related WorkIn the context of a location-based dialog system,Seltzer et al (2007) describes a speech understand-ing system designed to recognize street intersec-tions and map them to a database of valid intersec-tions using information retrieval techniques.
Ro-bustness is achieved by exploiting both words andphonetic information at retrieval time, allowing asoft-matching of the ASR result to the canonical in-tersection name.
Their approach is specifically tar-geted at intersections, to the exclusion of other typesof landmarks.
While intersections are frequentlyused as landmarks in North America (where theirstudy was conducted), this is not always the casein other cultures, such as Japan (Suzuki and Wak-abayashi, 2005), where points of interests such astrain stations are more commonly used.
Also, theirapproach, which is framed as speech understanding,does not exploit information from previous dialogturns to infer user intention.Landmarks have been integrated in route direc-tions (Pierre-emmanuel Michon, 2001; Tversky andLee, 1999) with significant use at origin, destinationand decision points.
Further, landmarks have beenfound to work better than street signs in wayfind-ing (Tom and Denis, 2003).
The multimodal systemdescribed in (Gruenstein and Seneff, 2007) supportsthe use of landmarks from a limited set that the userspecifies by pointing at the map and typing landmarknames.
While this allows the landmarks (and theirdesignations) to be of any kind, the burden of defin-ing them is on the user.Spatial language, including landmarks, has alsobeen the focus of research within the context ofhuman-robot interaction.
(Huang et al, 2010;MacMahon et al, 2006) describe systems that trans-late natural language directions into motion paths orphysical actions.
These works focus on understand-ing the structure of (potentially complex) spatial lan-guage and mapping it into a representation of theenvironment.
Issues such as imperfect spoken lan-guage understanding have not been investigated inthis context.
Similarly, this vein of spatial languageresearch has traditionally been conducted on smallartificial worlds with a few dozen objects and placesat most, whereas real-world location-based servicesdeal with thousands or millions of entities.3 Hybrid Semantic / Location BeliefTrackingOur belief tracking system consists of two trackersrunning in parallel: a DPOT belief tracker (Raux andMa, 2011) and a novel kernel-based location tracker.The final inference of user intentions is produced bycombining information from the two trackers.
Thegeneral idea is to rerank the user goals given spatialinformation provided by the location tracker.3.1 Semantic Belief TrackerWe perform belief tracking over non-landmark con-cepts such as business name and street using a Dy-namic Probabilistic Ontology Tree (DPOT) (Rauxand Ma, 2011).
A DPOT is a Bayesian Networkcomposed of a tree-shaped subnetwork representingthe (static) user goal (Goal Network), connected to170Figure 1: Top view heat map of spatial distribution with landmarks Subway and 7 Eleven over potential targetplaces in Mountain View, CAa series of subnetworks representing the evidencegathered from each successive dialog turn (EvidenceNetworks).
Details of the model and an efficient in-ference method for posterior probability computa-tions can be found in (Raux and Ma, 2011).In the context of this paper, the purpose of thesemantic tracker is to update a list of the mostlikely target locations using attributes of thatlocation provided by the user (see Figure 2).
Ina local business database, such attributes includeBusiness Name, Street, Category (e.g.Japanese restaurant or convenience store), etc.The structure and parameters of the Goal Networkencode probabilistic ontological relations betweenthe attributes (e.g.
a Mcdonalds would be describedas a fast-food restaurant with high probability)that can be exploited during inference.
These canbe derived from expert knowledge, learned fromdata, or as is the case in our experimental system,populated from a database of local businesses (seesection 4).
After each user utterance, the DPOToutputs a ranked list of user goal hypotheses (an ex-ample goal hypothesis is [Category=italianrestaurant,Street=castro street]).Each hypothesis is converted into a query to thebackend database, and the posterior probability ofthe hypothesis is split equally among all matchingentries.
This results in a ranked list of databaseentries corresponding to the system?s belief overpotential target locations, with potentially manyentries having the same probability.3.2 Kernel-based Location TrackerLandmark concepts extracted by the Natural Lan-guage Understanding module (NLU) are passed tothe location tracker, which maintains a distributionover coordinates of potential target locations.
Eachsuch landmark concept is treated as evidence of spa-tial proximity of the target to the landmark and thedistribution is accordingly updated.
Any location inthe database can serve as a landmark observation,including major POIs such as train stations or pub-lic facilities.
If the name of a generic chain storewith multiple locations such as Subway is used forthe landmark, then an observation corresponding toeach individual location is added to the tracker.For each observed landmark `, the locationtracker constructs a 2-dimensional Gaussian kernelwith mean equal to the longitude and latitude of thelandmark (?` = (long`, lat`)) and a fixed covari-171Figure 2: Overview of the hybrid semantic / location belief tracking approach; the database entry in shade is theunderlying true target place to which the provided landmark is closeance matrix ?` for each landmark: `(t) =12?|?|1/2 exp( 12(t  ?`)T? 1` (t  ?`))This kernel density determines the conditional prob-ability that the target is at coordinates t =(longt, latt) given the fixed landmark `.
The covari-ance matrix ?` and hence the shape of the kernelcan be adjusted for different landmarks dependingon considerations such as the familiarity, size andprominence of the landmark (a large historic monu-ment is likely to be used as a landmark for locationsmuch further away than a small corner grocery store)etc.The probability density of the location t being thetarget is then given by a weighted mixture model:Pr(t|L) =X`2Lw` `(t) (1)where L is the set of candidate landmarks returnedby the NLU (see Section 4.1) up to the current turnand w` is set to the confidence score of ` from theNLU.
Thus candidate landmarks that have higherconfidence in the NLU will contribute more stronglyto the total likelihood.
Since Pr(t|L) is a den-sity function, it is unnormalized.
In Figure 1, weshow the kernel tracker distribution for a dialog statewhere Subway and 7 Eleven are provided aslandmarks.The kernel density estimator is a simple approachto probabilistic spatial reasoning.
It is easy to imple-ment and requires only a moderate amount of tuning.It naturally models evidence from multiple speechhypotheses and multiple provided landmarks, andit benefits from accumulated evidence across dia-log turns.
It can also potentially be used to modelmore general kinds of spatial expressions by usingappropriate kernel functions.
For example, ?AlongCastro street?
can be modeled by a Gaussianwith an asymmetric covariance matrix such that theshape of the resulting distribution is elongated andconcentrated on the street.
While ?Two blocksaway from ...?
could be modeled by addingan extra ?negative?
density kernel that extends from172Figure 3: Overview of the Destination Setting Systemthe center of the landmark to a distance two blocksaway.3.3 Combining the Two TrackersAt each turn, the updated results from the Seman-tic and Location tracker are combined to give asingle ranked list of likely target locations.
InFigure 2, this process is illustrated for a dia-log turn where two possible concepts are identi-fied   a category attribute [Category:italianrestaurant] and a landmark [Landmark:redrock coffee company].
These are passed tothe DPOT tracker and the location tracker respec-tively.
The output of the DPOT is used to retrieveand score matching database entries.
The score foreach entry is reweighted by the kernel density esti-mator measured at the coordinates of the location 1:Pr(eij) = (piNi)?
?
Pr(eij |L) (2)where Ni is the number of matching database en-tries retrieved from ith goal hypothesis (having jointprobability pi) and eij is the jth such entry (j 2[1..Ni]).
The exponent ?
for the posterior term isintroduced to account for scale difference betweenthe semantic score and the kernel density.The set of candidate entries can then be rerankedaccording to Eq 2 and returned as the output of thecombined belief tracker.Figure 4: Structure of the Goal Network for the experi-mental system.4 Evaluation4.1 Experimental SystemThe architecture of our experimental system isshown in Figure 3.
The web client, shown in Figure5, runs in the participant?s web browser and displaysthe target location of the current scenario using theGoogle Map API.
The user?s goal is to convey thistarget location to the system through speech only.The system backend consists of a database of2902 businesses located in Mountain View, Cali-fornia with their name, street, street number, busi-ness category, latitude and longitude provided.
Thegrammar rules for the NLU and the probability ta-bles in the DPOT are populated from this database.The web client captures the user speech and sendsit to our server with a push-to-talk interface basedon the WAMI toolkit (Gruenstein et al, 2008).
Theserver uses a commercial cloud-based ASR servicewith generic acoustic and language models, whichwere not adapted to our task.
The n-best list of hy-potheses from the ASR is sent to our robust natural1The scores are renormalized to between 0 and1.173language understanding module for parsing.Our NLU uses a hybrid approach combininga weighted finite-state transducer (WFST) withstring matching based rescoring of the output.
TheWFST incorporates out-of-grammar word loopsthat allow skipping input words at certain pointsin the parse2.
This parser robustly maps free formutterances (e.g.
?Okay let?s go to thatItalian place near, uh..., RedRock Cafe, on Castro?)
to semantic frames(e.g.
[Category=italian restaurant,Street=castro street, Landmark=redrock coffee company]).The NLU confidence score is computed based onthe number of words skipped while parsing, andhow close the important concept words match thecanonical phrases found in the database.
For in-stance, ?Red Rock Cafe?
matches the canoni-cal name ?Red Rock Coffee Company?
withhigh confidence because rare words (Red, Rock)are identical, and differing but common words(Cafe, Coffee, Company) have a low weightin the score.
The string matching score is basedon the term-frequency/inverse document frequency(TF-IDF) metric commonly used in information re-trieval.
In our case, the weight of different terms(IDF) is estimated based on their frequency of occur-rence in different database entries (i.e.
how uniquelythey describe a matching entry).
We use the sec-ondstring open-source library (Cohen et al, 2003)for string matching.
For any ASR hypothesis, theNLU is likely to generate several parses which areall merged in a global list of candidate parses.For each candidate parse, the system generatesa set of dialog acts (one per concept in the parse)which are input to the belief tracker with their confi-dence score.
Following the approach described insection 3, dialog acts corresponding to the Land-mark concept are sent to the kernel-based locationbelief tracker, while all other concepts are sent to aDynamic Probabilistic Ontology Trees (DPOT) se-mantic belief tracker, whose structure is shown inFigure 4.
We use a two-level tree.
The value ofthe root node (Id) is never directly observed andrepresents the database entry targeted by the user.2This module is implemented using the OpenFST library(Allauzen et al, 2007)The leaf nodes correspond to the relevant attributesName, Category, and Street.
For any databaseentry e, attribute a and value of that attribute va, theconditional probability P (a = va|Id = e) is set to 1if the value of a is va for entry e in the database, andto 0 otherwise.
For attributes such as Category,which allow several possible values for each entry,the probability is split equally among valid values.After each user utterance, the network is augmentedwith a new Evidence Network capturing the possi-ble interpretations and their likelihood, as computedby the NLU.
The posterior probability distributionover user goals is computed and rescored using thekernel-based location tracker.Finally, the Response Generator takes the highestscoring target location from the belief tracker andsends it back to the web client which displays it onthe map and also indicates what are the values ofthe Name, Category, and Street concepts forthe top belief (see Figure 5).
If the top belief lo-cation does not match the goal of the scenario, theuser can speak again to refine or correct the systembelief.
After the user has spoken 5 utterances, theyalso get the choice of moving on to the next scenario(in which case the dialog is considered a failure).4.2 Data collectionTo evaluate our approach, we ran a data collectionexperiment using the Amazon Mechanical Turk on-line marketplace.
We defined 20 scenarios groupedinto 4 Human Intelligence Tasks (HITs).
Figure 5shows a screen shot of the web interface to the sys-tem.
In each scenario, the worker is given a targetlocation to describe by referring to nearby landmarkinformation.
The target locations were chosen so asto cover a variety of business categories and nearbylandmarks.
The compensation for completing eachset of 5 scenarios is 1 US dollar.
Before their firstscenario, workers are shown a video explaining thegoal of the task and how to use the interface, inwhich they are specifically encouraged to use land-marks in their descriptions.At the beginning of each scenario, the targetlocation is displayed on the map with a call-out containing a short description using either ageneric category (e.g.
Italian restaurant,Convenience store) or the name of a chainstore (e.g.
Subway, Mcdonalds).
The worker174Figure 5: Screen capture of the data collection web interface where the target location is an Italian restaurant (ingreen, underlying target place is [Ristorante Don Giovanni]) and after the first turn user input ?Italianrestaurant?
with a system belief [Frankie, Johnnie & Luigi, Too] in blue returned without any land-mark information provided so farthen interacts with the system described in section4.1 until either the system?s top belief matches thetarget location, or they decide to skip the scenario.4.3 Data StatisticsOverall, 99 workers participated in the data col-lection, providing 948 dialogs (2,869 utterances, 3turns per scenario on average), which two of theauthors manually transcribed and annotated for di-alog acts.
76% of the dialogs (46% of utterances)contained a reference to a landmark.
Other strate-gies commonly used by workers to uniquely identifya location include using a category or chain nameand a street, as well as explicitly mentioning the tar-get business name (although workers were explicitlydiscouraged form doing so).
Figure 7 in appendixprovides one example dialog from the corpus.Overall, the workers provided 203 unique land-marks, of which 143 (70%) are in the database.Workers were able to set the target destinationwithin 5 turns in 60.1% of the dialogs, which wehereafter refer to as task successes.
However, basedon the manual transcripts, 19.0% of the dialogscould not have succeeded with the current systembecause the workers used landmark or attributes thatdo not appear in the database.
Since the focus of thisstudy is robustness rather than coverage, we base ourevaluation on the remaining 768 dialogs, which wesplit between a development set of 74 dialogs anda test set of 694 dialogs.
On this test set, the livesystem has a task success rate of 70.6%.
By inspect-ing the log files, we noticed that runtime issues suchas timeouts prevented the system from getting anybelief from the belief tracker in 6.3% of the dialogs.The mean Word Error Rate (WER) per worker onthe test set is 27.5%.
There was significant variabil-ity across workers, with a standard deviation 20.7%.Besides the usual factors such as acoustic noise andnon-native accents, many of the errors came fromthe misrecognition of business names, due to the factthat ASR uses an open-ended language model that istuned neither to Mountain View, nor to businesses,nor to the kind of utterances that our set up tendsto yield, which is a realistic situation for large scalepractical applications.Concept precision of the top scoring NLU hypoth-esis is 73.0% and recall is 57.7%.
However, whenconsidering the full list of NLU hypotheses and us-ing an oracle to select the best one for each turn,precision increases to 89.3% and recall to 66.2%,underscoring the potential of using multiple inputhypotheses in the belief tracker.17542%50%69%83%0%10%20%30%40%50%60%70%80%90%W/o landmarksbaselineW/o landmarks BT W/ landmarksbaselineW/ landmarks BTTask SuccessRateFigure 6: Batch evaluation of the proposed (BT) and baseline approaches with and without landmark information.4.4 Batch ResultsTo further analyze the performance of our approach,we conducted a series of batch experiments on thedata collected with the runtime system.
We firsttuned the parameters of the belief tracker ?
and ?l(see section 3) on the development set (?
= 3 and?l corresponds to a circular Gaussian with standarddeviation 500 meters).We compare the tuned proposed belief trackingsystem (labeled BT) with three other versions.
First,we define a deterministic baseline system which, ateach turn, updates its belief by overwriting each con-cept?s value with the value found in the top NLUhypothesis.
Based on this (single) user goal hy-pothesis, we query the database to retrieve match-ing entries.
If the current goal hypothesis con-tains a Landmark concept, the baseline system se-lects the matching entry that is closest to any loca-tion matching the landmark name, by computing thepairwise distance between candidate target locationsand landmarks.We also compute the performance of both thebaseline and our proposed approach without us-ing landmark information at all.
In these versions,the belief over the attributes (Name, Street, andCategory) is updated according to either the topNLU hypothesis (baseline) or the DPOT model (BT)and the first matching database entry is returned, ig-noring any landmark information.Figure 6 shows the task success of each of the fourversions on the test set.
First, it is clear that land-mark information is critical to complete the tasks inthis corpus since both systems ignoring landmarksperform significantly worse than their counterparts.Second, the belief tracking approach significantlyoutperforms the deterministic baseline (83.0% vs69.3%, p < 0.001 using sign test for matched pairs).To further analyze the performance of the sys-tem in different input conditions, we split the di-alogs based on their measured concept accuracy (ex-pressed in terms of concept F-measure).
All dialogswith an F-measure higher than the median (70.0%)are labeled as high-accuracy, while the other half ofthe data is labeled as low-accuracy.
While both theproposed approach and the baseline perform simi-larly well for high-accuracy dialogs (task success ofresp.
96.0% and 92.8%, difference is not statisti-cally significant), the difference is much larger forlow-accuracy dialogs (70.0% vs 45.8%, p < 0.001)confirming the robustness of the landmark-based be-lief tracking approach when confronted with poorinput conditions.5 ConclusionIn this paper, we have explored the possibilities ofincorporating spatial information into belief trackingin spoken dialog systems.
We proposed a landmark-based location tracker which can be combined witha semantic belief tracker to output inferred joint usergoal.
Based on the results obtained from our batchexperiments, we conclude that integrating spatial in-formation into a location-based dialog system couldimprove the overall accuracy of belief tracking sig-nificantly.176ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-ciech Skut, and Mehryar Mohri.
2007.
Openfst: Ageneral and efficient weighted finite-state transducerlibrary.
In Proceedings of the Ninth InternationalConference on Implementation and Application of Au-tomata (CIAA) Lecture Notes in Computer Science,volume 4783, pages 11?23.
Springer.W.W.
Cohen, P. Ravikumar, and S.E.
Fienberg.
2003.A comparison of string distance metrics for name-matching tasks.
In Proceedings of the IJCAI-2003Workshop on Information Integration on the Web(IIWeb-03), pages 73?78.Alexander Gruenstein and Stephanie Seneff.
2007.
Re-leasing a multimodal dialogue system into the wild:User support mechanisms.
In In Proceedings of the8th SIGdial Workshop on Discourse and Dialogue,pages 111?119, September.A.
Gruenstein, I. McGraw, and I. Badr.
2008.
The wamitoolkit for developing, deploying, and evaluating web-accessible multimodal interfaces.
In Proceedings ofthe 10th international conference on Multimodal in-terfaces, pages 141?148.
ACM.Albert Huang, Stefanie Tellex, Abe Bachrach, ThomasKollar, Deb Roy, and Nick Roy.
2010.
Natural lan-guage command of an autonomous micro-air vehicle.In International Conference on Intelligent Robots andSystems (IROS).M.
MacMahon, B. Stankiewicz, and B. Kuipers.
2006.Walk the talk: Connecting language, knowledge, andaction in route instructions.
In Proceedings of theNational Conference on Artificial Intelligence, vol-ume 21, page 1475.
Menlo Park, CA; Cambridge, MA;London; AAAI Press; MIT Press; 1999.N.
Mehta, R. Gupta, A. Raux, D. Ramachandran, andS.
Krawczyk.
2010.
Probabilistic ontology trees forbelief tracking in dialog systems.
In Proceedings ofthe 11th Annual Meeting of the Special Interest Groupon Discourse and Dialogue, pages 37?46.
Associationfor Computational Linguistics.Michel Denis Pierre-emmanuel Michon.
2001.
Whenand why are visual landmarks used in giving direc-tions?
In D. R. Montello, editor, Spatial InformationTheory, Volume 2205 of Lecture Notes in ComputerScience, pages 292?305.
Springer, Berlin.A.
Raux and Y. Ma.
2011.
Efficient probabilistic track-ing of user goal and dialog history for spoken dialogsystems.
In Proceedings of Interspeech 2011.Michael L. Seltzer, Yun-Cheng Ju, Ivan Tashev, and AlexAcero.
2007.
Robust location understanding in spo-ken dialog systems using intersections.
In Proceed-ings of Interspeech 2007, pages 2813?2816.K.
Suzuki and Y. Wakabayashi.
2005.
Cultural dif-ferences of spatial descriptions in tourist guidebooks.Spatial Cognition IV.
Reasoning, Action, and Interac-tion, 3343:147?164.B.
Thomson and S. Young.
2010.
Bayesian update of di-alogue state: A pomdp framework for spoken dialoguesystems.
Computer Speech & Language, 24(4):562?588.Ariane Tom and Michel Denis.
2003.
Referring to land-mark or street information in route directions: Whatdifference does it make?
Spatial Information The-ory.
Foundations of Geoghraphic Information Science,Lecture Notes in Computer Science, 2825/2003:362?374.Barbara Tversky and Paul U. Lee.
1999.
Pictorial andverbal tools for conveying routes.
In Proceedings ofthe International Conference on Spatial InformationTheory: Cognitive and Computational Foundationsof Geographic Information Science(COSIT).
Springer-Verlag London.177User: &Italian&restaurant&near&ASR: %italian%restaurant%near%NLU: %Category=Italian%Restaurant%Category %Italian'Restaurant'Target !Dominos!Pizza!Category %Italian'Restaurant'Target !Dominos!Pizza!User: &Italian&restaurant&near&Kappo&Nami&Nami&ASR: %italian%restaurant%near%camp%to%numa%numa%NLU: %Category=Italian%Restaurant,%Street=Camp%Avenue%%Category=Italian%Restaurant,%Landmark=Jefunira%Camp%Category %Italian'Restaurant%%Street %Camp'Avenue'Target &No!match!Category %Italian'Restaurant'Landmark %Jefunira'Camp'Target !
!Maldonado?s%User: &Italian&restaurant&near&Tempta5ons&ASR: %italian%restaurant%near%temptaAons%NLU: %Category=Italian%Restaurant,%Landmark=TemptaAons%Category %Italian'Restaurant'Street %Camp'Avenue'Landmark %Tempta5ons'Target &No!match!Category %Italian'Restaurant'Landmark %Jefunira'Camp,'Tempta5ons'Target &Don!Giovanni!Baseline' DPOT+Kernels'Example&DialogFigure 7: Comparison between baseline and proposed method on an example dialog whose underlying true target isan Italian restaurant called Don Giovanni.178
