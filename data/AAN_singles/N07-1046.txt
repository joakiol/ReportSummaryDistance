Proceedings of NAACL HLT 2007, pages 364?371,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA Log-linear Block Transliteration Model based on Bi-Stream HMMsBing Zhao, Nguyen Bach, Ian Lane, and Stephan Vogel{bzhao, nbach, ianlane, vogel}@cs.cmu.eduLanguage Technologies InstituteSchool of Computer Science, Carnegie Mellon UniversityAbstractWe propose a novel HMM-based framework toaccurately transliterate unseen named entities.The framework leverages features in letter-alignment and letter n-gram pairs learned fromavailable bilingual dictionaries.
Letter-classes,such as vowels/non-vowels, are integrated tofurther improve transliteration accuracy.
Theproposed transliteration system is applied toout-of-vocabulary named-entities in statisticalmachine translation (SMT), and a significantimprovement over traditional transliteration ap-proach is obtained.
Furthermore, by incor-porating an automatic spell-checker based onstatistics collected from web search engines,transliteration accuracy is further improved.The proposed system is implemented withinour SMT system and applied to a real transla-tion scenario from Arabic to English.1 IntroductionCross-lingual natural language applications, such as in-formation retrieval, question answering, and machinetranslation for web-documents (e.g.
Google translation),are becoming increasingly important.
However, currentstate-of-the-art statistical machine translation (SMT) sys-tems cannot yet translate named-entities which are notseen during training.
New named-entities, such as per-son, organization, and location names are continuallyemerging on the World-Wide-Web.
To realize effectivecross-lingual natural language applications, handling out-of-vocabulary named-entities is becoming more crucial.Named entities (NEs) can be translated via transliter-ation: mapping symbols from one writing system to an-other.
Letters of the source language are typically trans-formed into the target language with similar pronunci-ation.
Transliteration between languages which sharesimilar alphabets and sound systems is usually not dif-ficult, because the majority of letters remain the same.However, the task is significantly more difficult when thelanguage pairs are considerably different, for example,English-Arabic, English-Chinese, and English-Japanese.In this paper, we focus on forward transliteration fromArabic to English.The work in (Arbabi et al, 1994), to our knowledge, isthe first work on machine transliteration of Arabic namesinto English, French, and Spanish.
The idea is to vow-elize Arabic names by adding appropriate vowels and uti-lizing a phonetic look-up table to provide the spelling inthe target language.
Their framework is strictly applica-ble within standard Arabic morphological rules.
Knightand Graehl (1997) introduced finite state transducers thatimplement back-transliteration from Japanese to English,which was then extended to Arabic-English in (Stalls andKnight, 1998).
Al-Onaizan and Knight (2002) translit-erated named entities in Arabic text to English by com-bining phonetic-based and spelling-based models, and re-ranking candidates with full-name web counts, named en-tities co-reference, and contextual web counts.
Huang(2005) proposed a specific model for Chinese-Englishname transliteration with clusterings of names?
origins,and appropriate hypotheses are generated given the ori-gins.
All of these approaches, however, are not basedon a SMT-framework.
Technologies developed for SMTare borrowed in Virga and Khudanpur (2003) and Ab-dulJaleel and Larkey (2003).
Standard SMT alignmentmodels (Brown et al, 1993) are used to align letter-pairswithin named entity pairs for transliteration.
Their ap-proach are generative models for letter-to-letter transla-tions, and the letter-alignment is augmented with heuris-tics.
Letter-level contextual information is shown to bevery helpful for transliteration.
Oh and Choi (2002)used conversion units for English-Korean Transliteration;Goto et al (2003) used conversion units, mapping En-glish letter-sequence into Japanese Katakana characterstring.
Li et al (2004) presented a framework allowingdirect orthographical mapping of transliteration units be-tween English and Chinese, and an extended model ispresented in Ekbal et al (2006).We propose a block-level transliteration framework, asshown in Figure 1, to model letter-level context infor-mation for transliteration at two levels.
First, we pro-pose a bi-stream HMM incorporating letter-clusters tobetter model the vowel and non-vowel transliterationswith position-information, i.e., initial and final, to im-prove the letter-level alignment accuracy.
Second, basedon the letter-alignment, we propose letter n-gram (letter-sequence) alignment models (block) to automaticallylearn the mappings from source letter n-grams to targetletter n-grams.
A few features specific for transliterationsare explored, and a log-linear model is used to combine364Figure 1: Transliteration System Structure.
The upper-part isthe two-directional Bi-Stream HMM for letter-alignment; thelower-part is a log-linear model for combining different featurefunctions for block-level transliteration.these features to learn block-level transliteration-pairsfrom training data.
The proposed transliteration frame-work obtained significant improvements over a strongbaseline transliteration approach similar to AbdulJaleeland Larkey (2003) and Virga and Khudanpur (2003).The remainder of this paper is organized as follows.In Section 2, we formulate the transliteration as a generaltranslation problem; in Section 4, we propose a log-linearalignment model with a local search algorithm to modelthe letter n-gram translation pairs; in Section 5, exper-iments are presented.
Conclusions and discussions aregiven in Section 6.2 Transliteration as TranslationTransliteration can be viewed as a special case of transla-tion.
In this approach, source and target NEs are split intoletter sequences, and each sequence is treated as a pseudosentence.
The appealing reason of formulating transliter-ation in this way is to utilize advanced alignment models,which share ideas applied also within phrase-based sta-tistical machine translation (Koehn, 2004).To apply this approach to transliteration, however,some unique aspects should be considered.
First, lettersshould be generated from left to right, without any re-ordering.
Thus, the transliteration models can only exe-cute forward sequential jumps.
Second, for unvowelizedlanguages such as Arabic, a single Arabic letter typicallymaps to less than four English letters.
Thus, the fertilityfor each letter should be recognized to ensure reasonablelength relevance.
Third, the position of the letter withina NE is important.
For example, in Arabic, letters suchas ?al?
at the beginning of the NE can only be translatedinto ?the?
or ?al?.
Therefore position information shouldbe considered within the alignment models.Incorporating the above considerations, transliterationcan be formulated as a noisy channel model.
Let fJ1 =f1f2...fJ denote the source NE with J letters, eI1 =e1e2...eI be an English transliteration candidate with Iletters.
According to Bayesian decision rule:e?I1=argmax{eI1}P (eI1|fJ1 )= argmax{eI1}P (fJ1 |eI1)P (eI1), (1)where P (fJ1 |eI1) is the letter translation model and P (eI1)is the English letter sequence model corresponding tothe monolingual language models in SMT.
In this noisy-channel scheme, P (fJ1 |eI1) is the key component fortransliteration, in which the transliteration between eI1and fJ1 can be modeled at either letter-to-letter level, orletter n-gram transliteration level (block-level).Our transliteration models are illustrated in Figure 1.We propose a Bi-Stream HMM of P (fJ1 |eI1) to inferletter-to-letter alignments in two directions: Arabic-to-English (F-to-E) and English-to-Arabic (E-to-F), shownin the upper-part in Figure 1; refined alignment is thenobtained.
We propose a log-linear model to extract block-level transliterations with additional informative features,as illustrated in the lower-part of Figure 1.3 Bi-Stream HMMs for TransliterationStandard IBM translation models (Brown et al, 1993)can be used to obtain letter-to-letter translations.
How-ever, these models are not directly suitable, becauseletter-alignment within NEs is strictly left-to-right.
Thissequential property is well suited to HMMs (Vogel et al,1996), in which the jumps from the current aligned posi-tion can only be forward.3.1 Bi-Stream HMMsWe propose a bi-stream HMM for letter-alignment withinNE pairs.
For the source NE fJ1 and a target NE eI1, a bi-stream HMM is defined as follows:p(fJ1 |eI1)=?aJ1J?j=1p(fj |eaj )p(cfj |ceaj )p(aj |aj?1), (2)where aj maps fj to the English letter eaj at the positionaj in the English named entity.
p(aj |aj?1) is the transi-tion probability distribution assuming first-order Markovdependency; p(fj |eaj ) is a letter-to-letter translation lex-icon; cfj is the letter cluster of fj and p(cfj |ceaj ) is acluster level translation lexicon.
As mentioned in theabove, the vowel/non-vowel linguistic features can be uti-lized to cluster the letters.
The letters from the same clus-ter tend to share the similar letter transliteration forms.p(cfj |ceaj ) enables to leverage such letter-correlation inthe transliteration process.The HMM in Eqn.
2 generates two streams of observa-tions: the letters together with the letters?
classes follow-ing the distribution of p(fj |eaj ) and p(cfj |ceaj ) at each365Figure 2: Block of letters for transliteration.
A block is definedby the left- and right- boundaries in the NE-pair.state, respectively.
To be in accordance with the mono-tone nature of the NE?s alignment mentioned before, weenforce the following constraints in Eqn.
3, so that thetransition can only jump forward or stay at the same state:aj?aj?1?0 ?j ?
[1, J ].
(3)Since the two streams are conditionally independentgiven the current state, the extended EM is straight-forward, with only small modifications of the standardforward-backward algorithm (Zhao et al, 2005), for pa-rameter estimation.3.2 Designing Letter-ClassesPronunciation is typically highly structured.
For in-stance, in English the pronunciation structure of ?cvc?
(consonant-vowel-consonant) is common.
By incorpo-rating letter classes into the proposed two-stream HMM,the models?
expressiveness and robustness can be im-proved.
In this work, we focus on transliteration of Ara-bic NEs into English.
We define six non-overlappingletter classes: vowel, consonant, initial, final, noclass,and unknown.
Initial and final classes represent semanticmarkers at the beginning or end of NEs such as ?Al?
and?wAl?
(in romanization form).
Noclass signifies letterswhich can be pronounced as both a vowel and a conso-nant depending on context, for example, the English let-ter ?y?.
The unknown class is reserved for punctuationsand letters that we do not have enough linguistic clues formapping them to phonemes.4 Transliteration BlocksTo further leverage the information from the letter-context beyond the letter-classes incorporated in our bi-stream HMM in Eqn.
2, we define letter n-grams, whichconsist of n consecutive letters, as the basic transliter-ation unit.
A block is defined as a pair of such lettern-grams which are transliterations of each other.
Dur-ing decoding of unseen NEs, transliteration is performedblock-by-block, rather than letter-by-letter.
The goal oftransliteration model is to learn high-quality translitera-tion blocks from the training data in a unsupervised fash-ion.Specifically, a block X can be represented by its leftand right boundaries in the source and target NEs shownin Figure 2:X = (f j+lj , ei+ki ), (4)where f j+lj is the source letter-ngram with (l+1) lettersin source language, and its projection of ei+ki in the En-glish NE with left boundary at the position of i, and rightboundary at (i+ k).We formulate the block extraction as a local searchproblem following the work in Zhao and Waibel (2005):given a source letter n-gram f j+lj , search for the pro-jected boundaries of candidate target letter n-gram ei+kiaccording to a weighted combination of the diverse fea-tures in a log-linear model detailed in ?4.3.
The log-linearmodel serves as a performance measure to guide the localsearch, which, in our setup, is randomized hill-climbing,to extract bilingual letter n-gram transliteration pairs.4.1 Features for Block TransliterationThree features: fertility, distortion, and lexical transla-tion are investigated for inferring transliteration blocksfrom the NE pairs.
Each feature corresponds to one as-pect of the block within the context of a given NE pair.4.1.1 Letter n-gram FertilityThe fertility P (?|e) of a target letter e specifies theprobability of generating ?
source letters for translitera-tion.
The fertilities can be easily read-off from the letter-alignment, i.e., the output from the Bi-stream HMM.Given letter fertility model P (?|ei), a target letter n-grameI1, and a source n-gram fJ1 of length J , we compute aprobability of letter n-gram length relevance: P (J |eI1)via a dynamic programming.The probability of generating J letters by the Englishletter n-gram eI1 is defined:P (J |eI1) = max{?I1,J=?Ii=1 ?i}I?i=1P (?i|ei).
(5)The recursively updated cost ?
[j, i] in dynamic program-ming is defined as follows:?
[j, i] = max????????
[j, i?
1] + logPNull(0|ei)?
[j ?
1, i?
1] + logP?(1|ei)?
[j ?
2, i?
1] + logP?(2|ei)?
[j ?
3, i?
1] + logP?
(3|ei), (6)where PNull(0|ei) is the probability of generating a Nullletter from ei; P?
(k=1|ei) is the letter-fertility model ofgenerating one source letter from ei; ?
[j, i] is the cost366so far for generating j letters from i consecutive Englishletters (letter n-gram) ei1 : e1, ?
?
?
, ei.After computing the cost of ?
[J, I], the probabilityP (J |eI1) is computed for generating the length of thesource NE fJ1 from the English NE eI1 shown in Eqn.
5.With this letter n-gram fertility model, for every block,we can compute a fertility score to estimate how relevantthe lengths of the transliteration-pairs are.4.1.2 Distortion of CentersWhen aligning blocks of letters within transliterationpairs, we expect most of them are close to the diagonaldue to the monotone alignment nature.
Thus, a simpleposition metric is proposed for each block consideringthe relative positions within NE-pairs.The center ?fj+lj of the source phrase fj+lj with alength of (l + 1) is simply a normalized relative positionin the source entity defined as follows:?fj+lj =1l + 1j?=j+l?j?=jj?l + 1 .
(7)For the center of English letter-phrase ei+ki , we firstdefine the expected corresponding relative center for ev-ery source letter fj?
using the lexicalized position scoreas follows:?ei+ki (fj?)
=1k + 1 ??
(i+k)i?=i i?
?
P (fj?
|ei?)?
(i+k)i?=i P (fj?
|ei?
), (8)where P (fj?
|ei) is the letter translation lexicon estimatedin IBM Models 1?5.
i is the position index, whichis weighted by the letter-level translation probabilities;the term of?i+ki?=i P (fj?
|ei?)
provides a normalization sothat the expected center is within the range of the targetlength.
The expected center for ei+ki is simply the aver-age of the ?ei+ki (fj?
):?ei+ki =1l + 1j+l?j?=j?ei+ki (fj?)
(9)Given the estimated centers of ?fj+lj and ?ei+ki , wecan compute how close they are via the probability ofP (?fj+lj |?ei+ki ).
In our case, because of the mono-tone alignment nature of transliteration pairs, a simplegaussian model is employed to enforce that the point(?ei+ki ,?fj+lj ) is not far away from the diagonal.4.1.3 Letter Lexical TransliterationSimilar to IBM Model-1 (Brown et al, 1993), we usea ?bag-of-letter?
generative model within a block to ap-proximate the lexical transliteration equivalence:P (f j+lj |ei+ki )=j+l?j?=ji+k?i?=iP (fj?
|ei?
)P (ei?
|ei+ki ), (10)where P (ei?
|ei+ki ) ' 1/(k+1) is approximated by a bag-of-word unigram.
Since named entities are usually rela-tively short, this approximation works reasonably well inpractice.4.2 Extended Feature FunctionsBecause of the underlying nature of the noisy-channelmodel in our proposed transliteration approach in Section2, the three base feature functions are extended to coverthe directions both from target-to-source and source-to-target.
Therefore, we have in total six feature functionsfor inferring transliteration blocks from a named entitypair.Besides the above six feature functions, we also com-pute the average letter-alignment links per block.
Wecount the number of letter-alignment links within theblock, and normalize the number by the length of thesource letter-ngram.
Note that, we can refine the letter-alignment by growing the intersections of the two di-rection letter-alignments from Bi-stream HMM via ad-ditional aligned letter-pairs seen in the union of the two.In a way, this approach is similar to those of refining theword-level alignment for SMT in (Och and Ney, 2003).This step is shown in the upper-part in Figure 1.Overall, our proposed feature functions cover rela-tively different aspects for transliteration blocks: theblock level length relevance probability in Eqn.
5, lexicaltranslation equivalence, and positions?
distortion from agaussian distribution in Eqn.
8, in both directions; andthe average number of letter-alignment links within theblock.
Also, these feature functions are positive andbounded within [0, 1].
Therefore, it is suitable to apply alog-linear model (in ?4.3) to combine the weighted indi-vidual strengths from the proposed feature functions forbetter modeling the quality of the candidate translitera-tion blocks.
This log-linear model will serve as a per-formance measure in a local-search in ?4.4 for inferringtransliteration blocks.4.3 Log-Linear Transliteration ModelWe propose a log-linear model to combine the seven fea-ture functions in ?4.1 with proper weights as in Eqn.
11:Pr(X|e, f)= exp(?Mm=1 ?m?m(X, e, f))?{X?}
exp(?Mm=1 ?m?m(X ?, e, f)),(11)where ?m(X, e, f) are the real-valued bounded featurefunctions corresponding to the seven models introducedin ?4.1.
The log-linear model?s parameters are theweights {?m} associated with each feature function.With hand-labeled data, {?m} can be learnt via gen-eralized iterative scaling algorithm (GIS) (Darroch andRatcliff, 1972) or improved iterative scaling (IIS) (Berger367et al, 1996).
However, as these algorithms are computa-tionally expensive, we apply an alternative approach us-ing a simplex down-hill algorithm to optimize the weightstoward better F-measure of block transliterations.
Eachfeature function corresponds to one dimension in the sim-plex, and the local optimum only happens at a vertex ofthe simplex.
Simplex-downhill has several advantages:it is an efficient approach for optimizing multi-variablesgiven some performance measure.
We compute the F-measure against a gold-standard block set extracted fromhand-labeled letter-alignment.To build gold-standard blocks from hand-labeledletter-alignment, we propose the block transliteration co-herence in a two-stage fashion.
First is the forward pro-jection: for each candidate source letter-ngram f j+nj ,search for its left-most el and right-most er projectedpositions in the target NE according to the given letter-alignment.
Second is the backward projection: for thetarget letter-gram erl , search for its left-most fl?
and right-most fr?
projected positions in the source NE.
Now ifl?
?j and r?
?j+n, i.e.
frl is contained within the sourceletter-ngram f j+nj , then this block X = (f j+nj , erl ) is de-fined as coherent for the aligned pairs: (f j+nj , erl ) .
Weaccept coherent X as gold-standard blocks.
This blocktransliteration coherence is generally sound for extractingthe gold-blocks mostly because of the the monotone left-to-right nature of the letter-alignment for transliteration.A related coherence assumption can be found in (Fox,2002), where their assumption on phrase-pairs for sta-tistical machine translation is shown to be somewhat re-strictive for SMT.
This is mainly because the word align-ment is often non-monotone, especially for langauge-pairs from different families such as Arabic-English andChinese-English.4.4 Aligning Letter-Blocks: a Local SearchAligning the blocks within NE pairs can be formulatedas a local search given the heuristic function defined inEqn.
11.
To be more specific: given a Arabic letter-ngramf j+lj , our algorithm searches for the best translation can-didate ei+ki in the target named entities.
In our implemen-tation, we use stochastic hill-climbing with Eqn.
11 as theperformance measure.
Down-hill moves are accepted toallow one or two left and right null letters to be attachedto ei+ki to expand the table of transliteration-blocks.To make the local search more effective, we normal-ize the letter translation lexicon p(f |e) within the parallelentity pair as in:P?
(f |e) = P (f |e)?Jj?=1 P (fj?
|e).
(12)In this way, the distribution of P?
(f |e) is sharper and morefocused in the context of an entity pair.Overall, given the parallel NE pairs, we can train theletter level translation models in both directions via theBi-stream HMM in Eqn.
2.
From the letter-alignment,we can build the letter translation lexicons and fertilitytables.
With these tables, the base feature functions arethen computed for each candidate block, and the featuresare combined in the log-linear model in Eqn.
11.
Givena named-entity pair in the training data, we rank all thetransliteration blocks by the scores using the log-linearmodel.
This step is shown in the lower-part in Figure 1.4.5 Decoding Unseen NEsThe decoding of NEs is an extension to the noisy-channelscheme in Eqn.
1.
In our configurations for NE translit-eration, the extracted transliteration blocks are used.
Ourletter ngram is a standard letter-ngram model trained us-ing the SriLM toolkit (Stolcke, 2002).
To transliterate theunseen NEs, the decoder (Hewavitharana et al, 2005) isconfigured for monotone decoding.
It loads the transliter-ation blocks and the letter-ngram LM, and it decodes theunseen Arabic named entities with block-based translit-eration from left to right.5 Experiments5.1 The DataWe have 74,887 bilingual geographic names fromLDC2005G01-NGA, 11,212 bilingual person namesfrom LDC2005G021, and about 6,000 bilingual namesextracted from the BAMA2 dictionary.
In total, there are92,099 NE pairs.
We split them into three parts: 91,459pairs as the training dataset, 100 pairs as the developmentdataset, and 540 unique NE pairs as the held-out dataset.An additional test set is collected from the TIDES 2003Arabic-English machine translation evaluation test set.The 663 sentences contain 286 unique words, which werenot covered by the available training data.
From this setof untranslated words, we manually labeled the entities ofpersons, locations and organizations, giving a total of 97unique un-translated NEs.
The BAMA toolkit was usedto romanize the Arabic words.
Some names from this testset are shown in Figure 1.These untranslated NEs make up only a very smallfraction of all words in the test set.
Therefore, havingcorrect transliterations would give only small improve-ments in terms of BLEU (Papineni et al, 2002) and NISTscores.
However, successfully translating these unknownNEs is very crucial for cross-lingual distillation tasks orquestion-answering based on the MT-output.1The corpus is provided as FOUO (for official use only) inthe DARPA-GALE project2LDC2004L02: Buckwalter Arabic Morphological Ana-lyzer version 2.0368Table 1: Test Set Examples.To evaluate the transliteration performance, we useedit-distance between the hypothesis against a referenceset.
This is to count the number of insertions, dele-tions, and substitutions required to correct the hypoth-esis to match the given reference.
An edit-distance ofzero is a perfect match.
However, NEs typically havemore than one correct variant.
For example, the Arabicname ?mHmd?
(in romanized form) can be transliteratedas Muhammad or Mohammed; both are considered ascorrect transliterations.
Ideally, we want to have all vari-ants as reference transliterations.
To enable our translit-eration evaluation to be more informative given only onereference, edit-distance of one between hypothesis andreference is considered to be an acceptable match.5.2 Comparison of Transliteration ModelsWe compare the performance of three systems within ourproposed framework in Figure.1: the baseline Block sys-tem, a system in which we use a log-linear combinationof alignment features as described in ?4.3, we call the theL-Block system, and finally a system, which also usesthe bi-stream HMM alignment model as described in ?3.This last system will be denoted LCBE system.The baseline is based on the refined letter-alignmentfrom the two directions of IBM-Model-4, trained with ascheme of 15h545 using GIZA++ (Och and Ney, 2004).The final alignment was obtained by growing the inter-sections between Arabic-to-English (AE) and English-to-Arabic (EA) alignments with additional aligned letter-pairs seen in the union.
This is to compensate for theinherent asymmetry in alignment models.
Blocks (letter-ngram pairs) were collected directly from the refinedletter-alignment, using the same algorithm as describedin ?4.3 for extracting gold-standard letter blocks.
There isno length restrictions to the letter-ngram extracted in oursystem.
All the blocks were then scored using relativefrequencies and lexical scores in both directions, similarto the scoring of phrase-pairs in SMT (Koehn, 2004).In the L-Block system additional feature functions asdefined in ?4.1 were computed on top of the letter-levelalignment obtained from the baseline system.
A log-linear model combining these features was learned withthe gold-blocks described in ?4.3.
Transliteration blockswere extracted using the local-search ?4.4.
The otherTable 2: Transliteration accuracy for different translitera-tion models.System AccuracyBaseline 39.18%L-Block 41.24%LCBE 46.39%components remained the same as in the baseline system.The LCBE system is an extension to both the baselineand the L-Block system.
The key difference in LCBEis that our proposed bi-stream HMM in Eqn.
2 was ap-plied in both directions with extended letter-classes.
Theresulting combined alignment was used together with allfeatures of the L-Block system to guide the local-searchfor extracting the blocks.
The same procedure of decod-ing was then carried out for the unseen NEs using theextracted blocks.To build the letter language model for the decodingprocess, we first split the English entities into charac-ters; additional position indicators ?
begin?
and ?
end?were added to the begin and end position of the named-entity; ?
middle?
was added between the first name andlast name.
A letter-trigram language model with SRI LMtoolkit (Stolcke, 2002) was then built using the target side(English) of NE pairs tagged with the above position in-formation.Table 2 shows that the baseline system gives an accu-racy of 39.18%, while the extended systems L-Block andLCBE give 41.24% and 46.39%, respectively.
These re-sults show that the additional features besides the letter-alignment are helpful.
The L-Block system, which usesthese features, outperforms the baseline system signifi-cantly by 2.1% absolute in accuracy.
The results alsoshow that the bi-stream HMM alignment, which uses notonly the letters but also the letter-classes, leads to signif-icant improvement.
It outperforms the L-Block system,which does not leverage the letter-classes and monotonealignment, by 4.15% absolute.5.3 Incorporation of Spell CheckingOur spelling-checker is based on the suggested word-forms from web search engines for ambiguous candi-dates.
We collected web statistics frequency for both theproposed transliteration candidates from our system, andalso the suggested candidates from web-search engines.All the candidates were re-ranked by their frequencies.Figure 3 shows the performances on the held-out set,using system LCBE augmented with a spell-checker(LCBE+Spell), with varying sizes of N-best hypotheseslists.
The held-out set contains 540 unique named entitypairs.
We show accuracy when exact match is requestedand when an edit distances of one is allowed.369Figure 3: Transliteration accuracy of LCBE and LCBE+Spellmodels for 540 named entity pairs in the held-out set.Figure 4: Transliteration accuracy of N-best hypotheses forLCBE and LCBE+Spell models it the MT-03 test set.Figure 4 shows the performances in the unseen test setof LCBE and LCBE+Spell, with varying sizes of N-besthypotheses lists.
LCBE+Spell reaches 52% accuracy in1-best hypothesis.
In the 5-best and 10-best cases, the ac-curacies of LCBE+Spell system archive the highest per-formances with 66% and 72.16% respectively.
The spell-checker increases the 1-best accuracy by 11.12% and the10-best accuracy by 7.69%.
All these improvements arestatistically significant.
These results are also comparableto other state-of-the-art statistical Arabic name transliter-ation systems such as (Al-Onaizan and Knight, 2002).5.4 Comparison with the Google Web TranslationWe finally compared our best system with thestate-of-the-art Arabic-English Google Web Translation(Google).
Table 3 shows transliteration examples fromour best system in comparison with Google (as in June20, 2006)3.
The Google system achieved 45.36% accu-racy for the 1-best hypothesis, which is comparable tothe results when using the LCBE transliteration system,while LCBE+Spell archived 52%.3http://www.google.com/translate tTable 3: Transliteration examples between LCBE+Spelland Google web translation.6 Conclusions and DiscussionsIn this paper we proposed a novel transliteration model.Viewing transliteration as a translation task we adoptalignment and decoding techniques used in a phrase-based statistical machine translation system to work onletter sequences instead of word sequences.
To improvethe performance we extended the HMM alignment modelinto a bi-stream HMM alignment by incorporating letter-classes into the alignment process.
We also showed that ablock-extraction approach, which uses a log-linear com-bination of multiple alignment features, can give signif-icant improvements in transliteration accuracy.
Finally,spell-checking based on work occurrence statistics ob-tained from the web gave an additional boost in translit-eration accuracy.The goal for this work is to improve the quality of ma-chine translation, esp.
when used in cross-lingual infor-mation retrieval and distillation tasks, by incorporatingthe proposed framework to handle unknown words.
Fig-ure 5 gives an example of the difference named entitytransliteration can make.
Shown are the original SMTsystem output, the translation when the proposed translit-eration models are used to translate the unknown named-entities, and the reference translation.
A comparison ofthe two SMT outputs indicates that integrating the pro-posed transliteration model into our machine translationsystem can significantly improve translation utility.AcknowledgmentThis work was partially supported by grants fromDARPA (GALE project) and NFS (Str-Dust project).ReferencesNasreen AbdulJaleel and Leah Larkey.
2003.
Statisticaltransliteration for English-Arabic cross language informa-tion retrieval.
In Proceedings of the 12th InternationalConference on Information and Knowledge Management,New Orleans, LA, USA, November.370Figure 5: Incorporation of the transliteration model to ourSMT System.Yaser Al-Onaizan and Kevin Knight.
2002.
Machine translit-eration of names in Arabic text.
In Proceedings of ACLWorkshop on Computational Approaches to Semitic Lan-guages, Philadelphia, PA, USA.Mansur Arbabi, Scott M. Fischthal, Vincent C. Cheng, andElizabeth Bart.
1994.
Algorithms for Arabic name translit-eration.
In IBM Journal of Research and Development,volume 38(2), pages 183?193.Adam L. Berger, Vincent Della Pietra, and Stephen A.Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
In Computational Linguistics,volume 22 of 1, pages 39?71, March.Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,and Robert L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
In Computa-tional Linguistics, volume 19(2), pages 263?331.J.N.
Darroch and D. Ratcliff.
1972.
Generalized iterativescaling for log-linear models.
In Annals of MathematicalStatistics, volume 43, pages 1470?1480.Asif Ekbal, S. Naskar, and S. Bandyopadhyay.
2006.
A modi-fied joint source channel model for machine transliteration.In Proceedings of COLING/ACL, pages 191?198, Australia.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In Proc.
of the Conference on EmpiricalMethods in Natural Language Processing, pages 304?311,Philadelphia, PA, July 6-7.Isao Goto, Naoto Kato, Noriyoshi Uratani, and TerumasaEhara.
2003.
Transliteration considering context informa-tion based on the maximum entropy method.
In Proceedingsof MT-Summit IX, New Orleans, Louisiana, USA.Sanjika Hewavitharana, Bing Zhao, Almut Silja Hildebrand,Matthias Eck, Chiori Hori, Stephan Vogel, and Alex Waibel.2005.
The CMU statistical machine translation systemfor IWSLT2005.
In The 2005 International Workshop onSpoken Language Translation.Fei Huang.
2005.
Cluster-specific name transliteration.
InProceedings of the HLT-EMNLP 2005, Vancouver, BC,Canada, October.Kevin Knight and Jonathan Graehl.
1997.
Machine transliter-ation.
In Proceedings of the Conference of the Associationfor Computational Linguistics (ACL), Madrid, Spain.Philipp Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based smt.
In Proceedings of the Conference ofthe Association for Machine Translation in the Americans(AMTA), Washington DC, USA.Haizhou Li, Min Zhang, and Jian Su.
2004.
A joint source-channel model for machine transliteration.
In Proceedingsof 42nd ACL, pages 159?166, Barcelona, Spain.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.
InComputational Linguistics, volume 1:29, pages 19?51.Franz J. Och and Hermann Ney.
2004.
The alignment templateapproach to statistical machine translation.
In Computa-tional Linguistics, volume 30, pages 417?449.Jong-Hoon Oh and Key-Sun Choi.
2002.
An English-Koreantransliteration model using pronunciation and contextualrules.
In Proceedings of COLING-2002, pages 1?7, Taipei,Taiwan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
Bleu: a method for automatic evaluation ofmachine translation.
In Proc.
of the 40th Annual Conf.
of theAssociation for Computational Linguistics (ACL 02), pages311?318, Philadelphia, PA, July.Bonnie Stalls and Kevin Knight.
1998.
Translating namesand technical terms in Arabic text.
In Proceedings of theCOLING/ACL Workshop on Computational Approaches toSemitic Languages, Montreal, Quebec, Canada.Andreas Stolcke.
2002.
SRILM ?
An extensible languagemodeling toolkit.
In Proc.
Intl.
Conf.
on Spoken LanguageProcessing, volume 2, pages 901?904, Denver.Paola Virga and Sanjeev Khudanpur.
2003.
Transliterationof proper names in cross-lingual information retrieval.
InProceedings of the ACL Workshop on Multi-lingual NamedEntity Recognition, Edmonton, Canada.Stephan.
Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM based word alignment in statistical machinetranslation.
In Proc.
The 16th Int.
Conf.
on ComputationalLingustics, (COLING-1996), pages 836?841, Copenhagen,Denmark.Bing Zhao and Alex Waibel.
2005.
Learning a log-linearmodel with bilingual phrase-pair features for statisticalmachine translation.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, Jeju Island,Korean, October.Bing Zhao, Eric P. Xing, and Alex Waibel.
2005.
Bilingualword spectral clustering for statistical machine translation.In Proceedings of the ACL Workshop on Building and UsingParallel Texts, pages 25?32, Ann Arbor, Michigan, June.371
