Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 292?295,New York City, June 2006. c?2006 Association for Computational LinguisticsAUTOMATED QUALITY MONITORING FOR CALL CENTERS USING SPEECH AND NLPTECHNOLOGIESG.
Zweig, O. Siohan, G. Saon, B. Ramabhadran, D. Povey, L. Mangu and B. KingsburyIBM T.J. Watson Research Center, Yorktown Heights, NY 10598ABSTRACTThis paper describes an automated system for assigning qual-ity scores to recorded call center conversations.
The system com-bines speech recognition, pattern matching, and maximum entropyclassification to rank calls according to their measured quality.Calls at both ends of the spectrum are flagged as ?interesting?
andmade available for further human monitoring.
In this process, theASR transcript is used to answer a set of standard quality controlquestions such as ?did the agent use courteous words and phrases,?and to generate a question-based score.
This is interpolated withthe probability of a call being ?bad,?
as determined by maximumentropy operating on a set of ASR-derived features such as ?max-imum silence length?
and the occurrence of selected n-gram wordsequences.
The system is trained on a set of calls with associatedmanual evaluation forms.
We present precision and recall resultsfrom IBM?s North American Help Desk indicating that for a givenamount of listening effort, this system triples the number of badcalls that are identified, over the current policy of randomly sam-pling calls.
The application that will be demonstrated is a researchprototype that was built in conjunction with IBM?s North Ameri-can call centers.1.
INTRODUCTIONEvery day, tens of millions of help-desk calls are recorded at callcenters around the world.
As part of a typical call center operationa random sample of these calls is normally re-played to humanmonitors who score the calls with respect to a variety of qualityrelated questions, e.g.?
Was the account successfully identified by the agent??
Did the agent request error codes/messages to help deter-mine the problem??
Was the problem resolved??
Did the agent maintain appropriate tone, pitch, volume andpace?This process suffers from a number of important problems: first,the monitoring at least doubles the cost of each call (first an opera-tor is paid to take it, then a monitor to evaluate it).
This causes thesecond problem, which is that therefore only a very small sampleof calls, e.g.
a fraction of a percent, is typically evaluated.
Thethird problem arises from the fact that most calls are ordinary anduninteresting; with random sampling, the human monitors spendmost of their time listening to uninteresting calls.This work describes an automated quality-monitoring systemthat addresses these problems.
Automatic speech recognition isused to transcribe 100% of the calls coming in to a call center,and default quality scores are assigned based on features such askey-words, key-phrases, the number and type of hesitations, andthe average silence durations.
The default score is used to rankthe calls from worst-to-best, and this sorted list is made availableto the human evaluators, who can thus spend their time listeningonly to calls for which there is some a-priori reason to expect thatthere is something interesting.The automatic quality-monitoring problem is interesting inpart because of the variability in how hard it is to answer the ques-tions.
Some questions, for example, ?Did the agent use courteouswords and phrases??
are relatively straightforward to answer bylooking for key words and phrases.
Others, however, require es-sentially human-level knowledge to answer; for example one com-pany?s monitors are asked to answer the question ?Did the agenttake ownership of the problem??
Our work focuses on calls fromIBM?s North American call centers, where there is a set of 31 ques-tions that are used to evaluate call-quality.
Because of the high de-gree of variability found in these calls, we have investigated twoapproaches:1.
Use a partial score based only on the subset of questionsthat can be reliably answered.2.
Use a maximum entropy classifier to map directly fromASR-generated features to the probability that a call is bad(defined as belonging to the bottom 20% of calls).We have found that both approaches are workable, and we presentfinal results based on an interpolation between the two scores.These results indicate that for a fixed amount of listening effort,the number of bad calls that are identified approximately tripleswith our call-ranking approach.
Surprisingly, while there has beensignificant previous scholarly research in automated call-routingand classification in the call center , e.g.
[1, 2, 3, 4, 5], there hasbeen much less in automated quality monitoring per se.2.
ASR FOR CALL CENTER TRANSCRIPTION2.1.
DataThe speech recognition systems were trained on approximately300 hours of 6kHz, mono audio data collected at one of the IBMcall centers located in Raleigh, NC.
The audio was manually tran-scribed and speaker turns were explicitly marked in the word tran-scriptions but not the corresponding times.
In order to detectspeaker changes in the training data, we did a forced-alignment ofthe data and chopped it at speaker boundaries.
The test set consistsof 50 calls with 113 speakers totaling about 3 hours of speech.2.2.
Speaker Independent SystemThe raw acoustic features used for segmentation and recognitionare perceptual linear prediction (PLP) features.
The features are292Segmentation/clustering Adaptation WERManual Off-line 30.2%Manual Incremental 31.3%Manual No Adaptation 35.9%Automatic Off-line 33.0%Automatic Incremental 35.1%Table 1.
ASR results depending on segmentation/clustering andadaptation type.Accuracy Top 20% Bottom 20%Random 20% 20%QA 41% 30%Table 2.
Accuracy for the Question Answering system.mean-normalized 40-dimensional LDA+MLLT features.
The SIacoustic model consists of 50K Gaussians trained with MPE anduses a quinphone cross-word acoustic context.
The techniques arethe same as those described in [6].2.3.
Incremental Speaker AdaptationIn the context of speaker-adaptive training, we use two formsof feature-space normalization: vocal tract length normalization(VTLN) and feature-space MLLR (fMLLR, also known as con-strained MLLR) to produce canonical acoustic models in whichsome of the non-linguistic sources of speech variability have beenreduced.
To this canonical feature space, we then apply a discrim-inatively trained transform called fMPE [7].
The speaker adaptedrecognition model is trained in this resulting feature space usingMPE.We distinguish between two forms of adaptation: off-line andincremental adaptation.
For the former, the transformations arecomputed per conversation-side using the full output of a speakerindependent system.
For the latter, the transformations are updatedincrementally using the decoded output of the speaker adapted sys-tem up to the current time.
The speaker adaptive transforms arethen applied to the future sentences.
The advantage of incrementaladaptation is that it only requires a single decoding pass (as op-posed to two passes for off-line adaptation) resulting in a decodingprocess which is twice as fast.
In Table 1, we compare the per-formance of the two approaches.
Most of the gain of full offlineadaptation is retained in the incremental version.2.3.1.
Segmentation and Speaker ClusteringWe use an HMM-based segmentation procedure for segmentingthe audio into speech and non-speech prior to decoding.
The rea-son is that we want to eliminate the non-speech segments in orderto reduce the computational load during recognition.
The speechsegments are clustered together in order to identify segments com-ing from the same speaker which is crucial for speaker adaptation.The clustering is done via k-means, each segment being modeledby a single diagonal covariance Gaussian.
The metric is given bythe symmetric K-L divergence between two Gaussians.
The im-pact of the automatic segmentation and clustering on the error rateis indicated in Table 1.Accuracy Top 20% Bottom 20%Random 20% 20%ME 49% 36%Table 3.
Accuracy for the Maximum Entropy system.Accuracy Top 20% Bottom 20%Random 20% 20%ME + QA 53% 44%Table 4.
Accuracy for the combined system.3.
CALL RANKING3.1.
Question AnsweringThis section presents automated techniques for evaluating callquality.
These techniques were developed using a train-ing/development set of 676 calls with associated manually gen-erated quality evaluations.
The test set consists of 195 calls.The quality of the service provided by the help-desk represen-tatives is commonly assessed by having human monitors listen toa random sample of the calls and then fill in evaluation forms.
Theform for IBM?s North American Help Desk contains 31 questions.A subset of the questions can be answered easily using automaticmethods, among those the ones that check that the agent followedthe guidelines e.g.?
Did the agent follow the appropriate closing script??
Did the agent identify herself to the customer?But some of the questions require human-level knowledge of theworld to answer, e.g.?
Did the agent ask pertinent questions to gain clarity of theproblem??
Were all available resources used to solve the problem?We were able to answer 21 out of the 31 questions using pat-tern matching techniques.
For example, if the question is ?Didthe agent follow the appropriate closing script?
?, we search for?THANK YOU FOR CALLING?, ?ANYTHING ELSE?
and?SERVICE REQUEST?.
Any of these is a good partial match forthe full script, ?Thank you for calling, is there anything else I canhelp you with before closing this service request??
Based on theanswer to each of the 21 questions, we compute a score for eachcall and use it to rank them.
We label a call in the test set as beingbad/good if it has been placed in the bottom/top 20% by humanevaluators.
We report the accuracy of our scoring system on thetest set by computing the number of bad calls that occur in thebottom 20% of our sorted list and the number of good calls foundin the top 20% of our list.
The accuracy numbers can be found inTable 2.3.2.
Maximum Entropy RankingAnother alternative for scoring calls is to find arbitrary features inthe speech recognition output that correlate with the outcome of acall being in the bottom 20% or not.
The goal is to estimate theprobability of a call being bad based on features extracted fromthe automatic transcription.
To achieve this we build a maximum293Fig.
1.
Display of selected calls.entropy based system which is trained on a set of calls with asso-ciated transcriptions and manual evaluations.
The following equa-tion is used to determine the score of a call C using a set of Npredefined features:P (class/C) = 1Z exp(NXi=1?ifi(class, C)) (1)where class ?
{bad, not ?
bad}, Z is a normalizing factor, fi()are indicator functions and {?i}{i=1,N} are the parameters of themodel estimated via iterative scaling [8].Due to the fact that our training set contained under 700 calls,we used a hand-guided method for defining features.
Specifi-cally, we generated a list of VIP phrases as candidate features,e.g.
?THANK YOU FOR CALLING?, and ?HELP YOU?.
Wealso created a pool of generic ASR features, e.g.
?number of hes-itations?, ?total silence duration?, and ?longest silence duration?.A decision tree was then used to select the most relevant featuresand the threshold associated with each feature.
The final set of fea-tures contained 5 generic features and 25 VIP phrases.
If we take alook at the weights learned for different features, we can see that ifa call has many hesitations and long silences then most likely thecall is bad.We use P (bad|C) as shown in Equation 1 to rank all the calls.Table 3 shows the accuracy of this system for the bottom and top20% of the test calls.At this point we have two scoring mechanisms for each call:one that relies on answering a fixed number of evaluation ques-tions and a more global one that looks across the entire call forhints.
These two scores are both between 0 and 1, and thereforecan be interpolated to generate one unique score.
After optimizingthe interpolation weights on a held-out set we obtained a slightlyhigher weight (0.6) for the maximum entropy model.
It can beseen in Table 4 that the accuracy of the combined system is greaterthat the accuracy of each individual system, suggesting the com-plementarity of the two initial systems.4.
END-TO-END SYSTEM PERFORMANCE4.1.
ApplicationThis section describes the user interface of the automated qualitymonitoring application.
As explained in Section 1, the evalua-Fig.
2.
Interface to listen to audio and update the evaluation form.tor scores calls with respect to a set of quality-related questionsafter listening to the calls.
To aid this process, the user interfaceprovides an efficient mechanism for the human evaluator to selectcalls, e.g.?
All calls from a specific agent sorted by score?
The top 20% or the bottom 20% of the calls from a specificagent ranked by score?
The top 20% or the bottom 20% of all calls from all agentsThe automated quality monitoring user interface is a J2EE webapplication that is supported by back-end databases and contentmanagement systems 1 The displayed list of calls provides a linkto the audio, the automatically filled evaluation form, the overallscore for this call, the agent?s name, server location, call id, dateand duration of the call (see Figure 1).
This interface now givesthe agent the ability to listen to interesting calls and update theanswers in the evaluation form if necessary (audio and evaluationform illustrated in 2).
In addition, this interface provides the eval-uator with the ability to view summary statistics (average score)and additional information about the quality of the calls.
The over-all system is designed to automatically download calls from mul-tiple locations on a daily-basis, transcribe and index them, therebymaking them available to the supervisors for monitoring.
Callsspanning a month are available at any given time for monitoringpurposes.4.2.
Precision and RecallThis section presents precision and recall numbers for theidentification of ?bad?
calls.
The test set consists of 195 calls thatwere manually evaluated by call center personnel.
Based on thesemanual scores, the calls were ordered by quality, and the bottom20% were deemed to be ?bad.?
To retrieve calls for monitoring,we sort the calls based on the automatically assigned quality scoreand return the worst.
In our summary figures, precision and recallare plotted as a function of the number of calls that are selectedfor monitoring.
This is important because in reality only a smallnumber of calls can receive human attention.
Precision is the ratio1In our case, the backend consists of DB2 and IBM?s Websphere Infor-mation Integrator for Content and the application is hosted on Websphere5.1.
)2940204060801000 20 40 60 80 100ObservedIdealRandomFig.
3.
Precision for the bottom 20% of the calls as a function ofthe number of calls retrieved.0204060801000 20 40 60 80 100ObservedIdealRandomFig.
4.
Recall for the bottom 20% of the calls.of bad calls retrieved to the total number of calls monitored, andrecall is the ratio of the number of bad calls retrieved to the totalnumber of bad calls in the test set.
Three curves are shown in eachplot: the actually observed performance, performance of randomselection, and oracle or ideal performance.
Oracle performanceshows what would happen if a perfect automatic ordering of thecalls was achieved.Figure 3 shows precision performance.
We see that in themonitoring regime where only a small fraction of the calls aremonitored, we achieve over 60% precision.
(Further, if 20% ofthe calls are monitored, we still attain over 40% precision.
)Figure 4 shows the recall performance.
In the regime of low-volume monitoring, the recall is midway between what could beachieved with an oracle, and the performance of random-selection.Figure 5 shows the ratio of the number of bad calls found withour automated ranking to the number found with random selection.This indicates that in the low-monitoring regime, our automatedtechnique triples efficiency.4.3.
Human vs. Computer RankingsAs a final measure of performance, in Figure 6 we present ascatterplot comparing human to computer rankings.
We do nothave calls that are scored by two humans, so we cannot present ahuman-human scatterplot for comparison.5.
CONCLUSIONThis paper has presented an automated system for quality moni-toring in the call center.
We propose a combination of maximum-entropy classification based on ASR-derived features, and questionanswering based on simple pattern-matching.
The system can ei-ther be used to replace human monitors, or to make them more11.522.533.544.550 20 40 60 80 100ObservedIdealFig.
5.
Ratio of bad calls found with QTM to Random selection asa function of the number of bad calls retrieved.0204060801001201401601802000 20 40 60 80 100 120 140 160 180 200Fig.
6.
Scatter plot of Human vs. Computer Rank.efficient.
Our results show that we can triple the efficiency of hu-man monitors in the sense of identifying three times as many badcalls for the same amount of listening effort.6.
REFERENCES[1] J. Chu-Carroll and B. Carpenter, ?Vector-based natural lan-guage call routing,?
Computational Linguistics, 1999.
[2] P. Haffner, G. Tur, and J. Wright, ?Optimizing svms for com-plex call classification,?
2003.
[3] M. Tang, B. Pellom, and K. Hacioglu, ?Call-type classifica-tion and unsupervised training for the call center domain,?
inARSU-2003, 2003.
[4] D. Hakkani-Tur, G. Tur, M. Rahim, and G. Riccardi, ?Unsu-pervised and active learning in automatic speech recognitionfor call classification,?
in ICASSP-04, 2004.
[5] C. Wu, J. Kuo, E.E.
Jan, V. Goel, and D. Lubensky, ?Improv-ing end-to-end performance of call classification through dataconfusion reduction and model tolerance enhancement,?
inInterspeech-05, 2005.
[6] H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, andG.
Zweig, ?The ibm 2004 conversational telephony systemfor rich transcription,?
in Eurospeech-2005, 2005.
[7] D. Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltau,and G. Zweig, ?fMPE: Discriminatively trained features forspeech recognition,?
in ICASSP-2005, 2004.
[8] A. Berger, S. Della Pietra, and V. Della Pietra, ?A maximumentropy approach to natural language processing,?
Computa-tional Linguistics, vol.
22, no.
1, 1996.295
