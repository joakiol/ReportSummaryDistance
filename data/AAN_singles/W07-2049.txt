Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 231?236,Prague, June 2007. c?2007 Association for Computational LinguisticsMELB-KB: Nominal Classification as Noun Compound InterpretationSu Nam Kim and Timothy BaldwinComputer Science and Software EngineeringUniversity of Melbourne, Australia{snkim,tim}@csse.unimelb.edu.auAbstractIn this paper, we outline our approach tointerpreting semantic relations in nominalpairs in SemEval-2007 task #4: Classifica-tion of Semantic Relations between Nomi-nals.
We build on two baseline approachesto interpreting noun compounds: sense col-location, and constituent similarity.
Theseare consolidated into an overall system incombination with co-training, to expand thetraining data.
Our two systems attained anaverage F-score over the test data of 58.7%and 57.8%, respectively.1 IntroductionThis paper describes two systems entered inSemEval-2007 task #4: Classification of SemanticRelations between Nominals.
A key contribution ofthis research is that we examine the compatibility ofnoun compound (NC) interpretation methods overthe extended task of nominal classification, to gainempirical insight into the relative complexity of thetwo tasks.The goal of the nominal classification task is toidentify the compatibility of a given semantic re-lation with each of a set of test nominal pairs,e.g.
between climate and forest in the fragment theclimate in the forest with respect to the CONTENT-CONTAINER relation.
Semantic relations (or SRs)in nominals represent the underlying interpretationof the nominal, in the form of the directed relationbetween the two nominals.The proposed task is a generalisation of the moreconventional task of interpreting noun compounds(NCs), in which we take a NC such as cookie jar andinterpret it according to a pre-defined inventory ofsemantic relations (Levi, 1979; Vanderwende, 1994;Barker and Szpakowicz, 1998).
Examples of seman-tic relations are MAKE,1, as exemplified in apple piewhere the pie is made from apple(s), and POSSES-SOR, as exemplified in family car where the car ispossessed by a family.In the SemEval-2007 task, SR interpretationtakes the form of a binary decision for agiven nominal pair in context and a given SR,in judging whether that nominal pair conformsto the SR.
Seven relations were used in thetask: CAUSE-EFFECT, INSTRUMENT-AGENCY,PRODUCT-PRODUCER, ORIGIN-ENTITY, THEME-TOOL, PART-WHOLE and CONTENT-CONTAINER.Our approach to the task was to: (1) naively treatall nominal pairs as NCs (e.g.
the climate in the for-est is treated as an instance of climate forest); and(2) translate the individual binary classification tasksinto a single multiclass classification task, in the in-terests of benchmarking existing SR interpretationmethods over a common dataset.
That is, we takeall positive training instances for each SR and poolthem together into a single training dataset.
For eachtest instance, we make a prediction according to oneof the seven relations in the task, which we thenmap onto a binary classification for final evaluationpurposes.
This mapping is achieved by determiningwhich binary SR classification the test instance wassourced from, and returning a positive classificationif the predicted SR coincides with the target SR, anda negative classification if not.We make three (deliberately naive) assumptionsin our approach to the nominal interpretation task.First, we assume that all the positive training in-1For direct comparability with our earlier research, seman-tic relations used in our examples are taken from (Barker andSzpakowicz, 1998), and differ slightly from those used in theSemEval-2007 task.231stances correspond uniquely to the SR in question,despite the task organisers making it plain that thereis semantic overlap between the SRs.
As a machinelearning task, this makes the task considerably moredifficult, as the performance for the standard base-lines drops considerably from that for the binarytasks.
Second, we assume that each nominal pairmaps onto a NC.
This is clearly a misconstrual of thetask, and intended to empirically validate whethersuch an approach is viable.
In line with this assump-tion, we will refer to nominal pairs as NCs for theremainder of the paper.
Third and finally, we assumethat the SR annotation of each training and test in-stance is insensitive to the original context, and useonly the constituent words in the NC to make ourprediction.
This is for direct comparability with ear-lier research, and we acknowledge that the context(and word sense) is a strong determinant of the SRin practice.Our aim in this paper is to demonstrate the effec-tiveness of general-purpose SR interpretation overthe nominal classification task, and establish a newbaseline for the task.The remainder of this paper is structured as fol-lows.
We present our methods in Section 2 and de-pict the system architectures in Section 4.
We thendescribe and discuss the performance of our meth-ods in Section 5 and conclude the paper in Section 6.2 ApproachWe used two basic NC interpretation methods.
Thefirst method uses sense collocations as proposed byMoldovan et al (2004), and the second method usesthe lexical similarity of the component words in theNC as proposed by Kim and Baldwin (2005).
Notethat neither method uses the context of usage of theNC, i.e.
the only features are the words contained inthe NC.2.1 Sense Collocation MethodMoldovan et al (2004) proposed a method called se-mantic scattering for interpreting NCs.
The intuitionbehind this method is that when the sense colloca-tion of NCs is the same, their SR is most likely thesame.
For example, the sense collocation of auto-mobile factory is the same as that of car factory, be-cause the senses of automobile and car, and factoryin the two instances, are identical.
As a result, thetwo NCs have the semantic relation MAKE.The semantic scattering model is outlined below.The probability P (r|fifj) (simplified toP (r|fij)) of a semantic relation r for wordsenses fi and fj is calculated based on simplemaximum likelihood estimation:P (r|fij) =n(r, fij)n(fij)(1)and the preferred SR r?
for the given word sensecombination is that which maximises the probabil-ity:r?
= argmaxr?RP (r|fij)= argmaxr?RP (fij |r)P (r) (2)Note that in limited cases, the same sense collo-cation can lead to multiple SRs.
However, since wedo not take context into account in our method, wemake the simplifying assumption that a given sensecollocation leads to a unique SR.2.2 Constituent Similarity MethodIn earlier work (Kim and Baldwin, 2005), we pro-posed a simplistic general-purpose method based onthe lexical similarity of unseen NCs with traininginstances.
That is, the semantic relation of a testinstance is derived from the train instance whichhas the highest similarity with the test instance, inthe form of a 1-nearest neighbour classifier.
Forexample, assuming the test instance chocolate milkand training instances apple juice and morning milk,we would calculate the similarity between modifierchocolate and each of apple and morning, and headnoun milk and each of juice and milk, and find, e.g.,the similarities .71 and .27, and .83 and 1.00 respec-tively.
We would then add these up to derive theoverall similarity for a given NC and find that applejuice is a better match.
From this, we would assignthe SR of MAKE from apple juice to chocolate milk.Formally, SA is the similarity between NCs(Ni,1, Ni,2) and (Bj,1, Bj,2):SA((Ni,1, Ni,2), (Bj,1, Bj,2)) =((?S1 + S1)?
((1?
?
)S2 + S2))2(3)where S1 is the modifier similarity (i.e.S(Ni,1, Bj1)) and S2 is head noun similarity232(i.e.
S(Ni,2, Bj2)); ?
?
[0, 1] is a weighting factor.The similarity scores are calculated using themethod of Wu and Palmer (1994) as implementedin WordNet::Similarity (Patwardhan et al,2003).
This is done for each pairing of WordNetsenses of each of the two words in question, and theoverall lexical similarity is calculated as the averageacross the pairwise sense similarities.The final classification is derived from the traininginstance which has the highest lexical similarity withthe test instance in question.3 Co-TrainingAs with many semantic annotation tasks, SR tag-ging is a time-consuming and expensive process.
Atthe same time, due to the inherent complexity of theSR interpretation task, we require large amounts oftraining data in order for our methods to performwell.
In order to generate additional training data totrain our methods over, we experiment with differentco-training methodologies for each of our two basicmethods.3.1 Co-Training for the Sense CollocationMethodFor the sense collocation method, we experimentwith a substitution method whereby we replace oneconstituent in a training NC instance by a similarword, and annotate the new instance with the sameSR as the original NC.
For example, car in car fac-tory (SR = MAKE) has similar words automobile,vehicle, truck from the synonym, hypernym and sis-ter word taxonomic relations, respectively.
Whencar is replaced by a similar word, the new nouncompound(s) (i.e.
automobile/vehicle/truck factory)share the same SR as the original car factory.
Notethat each constituent in our original example istagged for word sense, which we use both in ac-cessing sense-specific substitution candidates (viaWordNet), and sense-annotating the newly gener-ated NCs.Substitution is restricted to one constituent at atime in order to avoid extreme semantic variation.This procedure can be repeated to generate moretraining data.
However, as the procedure goes fur-ther, we introduce increasingly more noise.In our experiments, we use this co-trainingmethod with the sense collocation method to expandthe size and variation of training data, using syn-onym, hypernym and sister word relations.
For ourexperiment, we ran the expansion procedure for onlyone iteration in order to avoid generating excessiveamounts of incorrectly-tagged NCs.3.2 Co-Training for the Constituent SimilarityMethodOur experiments with the constituent similaritymethod over the trial data showed, encouragingly,that there is a strong correlation between the strengthof overall similarity with the best-matching trainingNC, and the accuracy of the prediction.
From this,we experimented with implementing the constituentsimilarity method in a cascading architecture.
Thatis, we batch evaluate all test instances on each it-eration, and tag those test instances for which thebest match with a training instance is above a pre-set threshold, which we decrease on each iteration.In subsequent iterations, all tagged test instances areincluded in the training data.
Hence, on each itera-tion, the number of training instances is increasing.As our threshold, we used a starting value of 0.85,which was decreased down to 0.65 in increments of0.05.4 ArchitecturesIn Section 4.1 and Section 4.2, we describe the ar-chitecture of our two systems.4.1 Architecture (I)Figure 1 presents the architecture of our first system,which interleaves sense collocation and constituentsimilarity, and includes co-training for each.
Thereare five steps in this system.First, we apply the basic sense collocation methodrelative to the original training data.
If the sense col-location between the test and training instances isthe same, we judge the predicted SR to be correct.Second, we apply the similarity method describedin Section 2.2 over the original training data.
How-ever, we only classify test instances where the finalsimilarity is above a threshold of 0.8.Third, we apply the sense collocation co-trainingmethod and re-run the sense collocation methodover the expanded training data from the first twosteps.
Since the sense collocations in the expanded233TESTuntagged test datauntagged test datauntagged test datauntagged test datatagged datatagged datatagged datatagged datatagged dataTRAINExtension ofTraining databy similar words?
Synonym?
Hypernym?
Sister wordExtended TRAINSense CollcationStep 1SimilarityStep 2Step 3Step 4SimilarityStep 5Sense CollcationSimilarityFigure 1: System Architecture (I)training data have been varied through the advent ofhypernyms and sister words, the number of sensecollocations in the expanded training data is muchgreater than that of the original training data (937vs.
16,676).Fourth, we apply the constituent similarity co-training method over the consolidated training data(from both sense collocation and constituent simi-larity co-training) with the threshold unchanged at0.8.Finally, we apply the constituent similaritymethod over the combined training data, without anythreshold (to guarantee a SR prediction for everytest instance).
However, since the generated train-ing instances are more likely to contain errors, wedecrement the similarity values for generated train-ing instances by 0.2, to prefer predictions based onthe original training instances.4.2 Architecture (II)Figure 2 depicts our second system, which is basedsolely on the constituent similarity method, with co-training.We perform iterative co-training as described inTRAIN#of Tagged>= 10% of testThresholdTaggedfinalize currenttags and endreduce ThresholdTESTget SimilaritySim >= TN YYNif T == 0.6 &(#of Tagged <10% of test)NYFigure 2: System Architecture (II)Section 3.2, with the slight variation that we holdoff reducing the threshold if more than 10% of thetest instances are tagged on a given iteration, givingother test instances a chance to be tagged at a higherthreshold level relative to newly generated traininginstances.
The residue of test instances on comple-tion of the final iteration (threshold = 0.6) are taggedaccording to the best-matching training instance, ir-respective of the magnitude of the similarity.5 EvaluationWe group our evaluation into two categories: (A)doesn?t use WordNet 2.1 or the query context;and (B) uses WordNet 2.1 only (again with-out the query context).
Of our two basic meth-ods the sense collocation method and co-trainingmethod are based on WordNet 2.1 only, whilethe constituent similarity method is based indirectlyon WordNet 2.1, but doesn?t preserve WordNet2.1 sense information.
Hence, our first system iscategory B while our second system is (arguably)category A.Table 1 presents the three baselines for the task,and the results for our two systems (System I andSystem II).
The performance for both systems ex-ceeded all three baselines in terms of accuracy, andall but the All True baseline (i.e.
every instance isjudged to be compatible with the given SR) in terms234Method P R F AAll True 48.5 100.0 64.8 48.5Probability 48.5 48.5 48.5 51.7Majority 81.3 42.9 30.8 57.0System I 61.7 56.8 58.7 62.5System II 61.5 55.7 57.8 62.7Table 1: System results (P = precision, R = recall, F= F-score, and A = accuracy)Team P R F A759 66.1 66.7 64.8 66.0281 60.5 69.5 63.8 63.5633 62.7 63.0 62.7 65.4220 61.5 55.7 57.8 62.7161 56.1 57.1 55.9 58.8538 48.2 40.3 43.1 49.9Table 2: Results of category A systemsof F-score and recall.Tables 2 and 3 show the performance of the teamswhich performed in the task, in categories A and B.Team 220 in Table 2 is our second system, and team220 in Table 3 is our first system.In Figures 3 and 4, we present a breakdown ofthe performance our first and second system, re-spectively, over the individual semantic relations.Our approaches performed best for the PRODUCT-PRODUCER SR, and worst for the PART-WHOLESR.
In general, our systems achieved similar perfor-mance on most SRs, with only PART-WHOLE be-ing notably worse.
The lower performance of PART-WHOLE pulls down our overall performance consid-erably.Tables 4 and 5 show the number of tagged and un-tagged instances for each step of System I and Sys-tem II, respectively.
The first system tagged morethan half of the data in the fifth (and final) step,where it weighs up predictions from the original andexpanded training data.
Hence, the performance ofthis approach relies heavily on the similarity methodand expanded training data.
Additionally, the differ-ence in quality between the original and expandedtraining data will influence the performance of theapproach appreciably.
On the other hand, the num-ber of instances tagged by the second system is welldistributed across each iteration.
However, sincewe accumulate generated training instances on eachstep, the relative noise level in the training data willTeam P R F A901 79.7 69.8 72.4 76.3777 70.9 73.4 71.8 72.9281 72.8 70.6 71.5 73.2129 69.9 64.6 66.8 71.4333 62.0 71.7 65.4 67.0538 66.7 62.8 64.3 67.2571 55.7 66.7 60.4 59.1759 66.4 58.1 60.3 63.6220 61.7 56.8 58.7 62.5371 56.8 56.3 56.1 57.7495 55.9 57.8 51.4 53.7Table 3: Results of category B systemsCE IA PP OE TT PW CCrelations(%) F?scorerecallprecision accuracy020406080100Figure 3: System I performance for each rela-tion (CC=CAUSE-EFFECT, IA=INSTRUMENT-AGENCY, PP=PRODUCT-PRODUCER,OE=ORIGIN-ENTITY, TT=THEME-TOOL,PW=PART-WHOLE, CC=CONTENT-CONTAINER)increase across iterations, impacting on the final per-formance of the system.Over the trial data, we noticed that the system pre-dictions are appreciably worse when the similarityvalue is low.
In future work, we intend to analysewhat is happening in terms of the overall systemperformance at each step.
This analysis is key toimproving the performance of our systems.Recall that we are generalising from the set ofbinary classification tasks in the original task, to amulticlass classification task.
As such, a direct com-parison with the binary classification baselines isperhaps unfair (particularly All True, which has nocorrelate in a multiclass setting), and it is if anythingremarkable that our system compares favourablycompared to the baselines.
Similarly, while weclearly lag behind other systems participating in the235(%)CE IA PP OE TT PW CCrelationsF?scorerecallprecision accuracy020406080100Figure 4: System II performance for each rela-tion (CC=CAUSE-EFFECT, IA=INSTRUMENT-AGENCY, PP=PRODUCT-PRODUCER,OE=ORIGIN-ENTITY, TT=THEME-TOOL,PW=PART-WHOLE, CC=CONTENT-CONTAINER)step method tagged accumulated untaggeds1 SC 21 3.8% 528s2 Sim 106 23.1% 422s3 extSC 0 23.1% 422s4 extSim 61 34.2% 361s5 SvsExtS 359 99.6% 2Table 4: System I: Tagged data from each step(SC= sense collocation; Sim = the similarity method;extSC = SC over the expanded training data; extSim= similarity over the expanded training data; SvsExtS= the final step over both the original and expandedtraining data)task, we believe we have demonstrated that NC in-terpretation methods can be successfully deployedover the more general task of nominal pair classifi-cation.6 ConclusionIn this paper, we presented two systems entered inthe SemEval-2007 Classification of Semantic Re-lations between Nominals task.
Both systems arebased on baseline NC interpretation methods, andthe naive assumption that the nominal classificationtask is analogous to a conventional multiclass NCinterpretation task.
Our results compare favourablywith the established baselines, and demonstrate thatNC interpretation methods are compatible with themore general task of nominal classification.I T tagged accumulated untaggedi1 .85 73 13.3% 476i2 .80 56 23.5% 420i3 .75 74 37.0% 346i4 .70 101 55.4% 245i5 .65 222 95.8% 23?
<.65 21 99.6% 2Table 5: System II: data tagged on each iteration (T= the threshold; iX = the iteration number)AcknowledgmentsThis research was carried out with support from Australian Re-search Council grant no.
DP0663879.ReferencesKen Barker and Stan Szpakowicz.
1998.
Semi-automaticrecognition of noun modifier relationships.
In Proc.
of the17th International Conference on Computational Linguis-tics, pages 96?102, Montreal, Canada.Christiane Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, USA.Timothy W. Finin.
1980.
The Semantic Interpretation of Com-pound Nominals.
Ph.D. thesis, University of Illinois.Su Nam Kim and Timothy Baldwin.
2005.
Automatic inter-pretation of Noun Compounds using WordNet similarity.
InProc.
of the 2nd International Joint Conference On NaturalLanguage Processing, pages 945?956, JeJu, Korea.Judith Levi.
1979.
The syntax and semantics of complex nom-inals.
In The Syntax and Semantics of Complex Nominals.New York:Academic Press.DanMoldovan, Adriana Badulescu, Marta Tatu, Daniel Antohe,and Roxana Girju.
2004.
Models for the semantic classifi-cation of noun phrases.
In Proc.
of the HLT-NAACL 2004Workshop on Computational Lexical Semantics, pages 60?67, Boston, USA.Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen.2003.
Using measures of semantic relatedness for wordsense disambiguation.
In Proc.
of the Fourth InternationalConference on Intelligent Text Processing and Computa-tional Linguistics, pages 241?57, Mexico City, Mexico.Lucy Vanderwende.
1994.
Algorithm for automatic interpreta-tion of noun sequences.
In Proc.
of the 15th conference onComputational linguistics, pages 782?788, Kyoto, Japan.Zhibiao Wu and Martha Palmer.
1994.
Verb semantics andlexical selection.
In Proc.
of the 32nd Annual Meeting of theAssociation for Computational Linguistics, pages 133?138,Las Cruces, USA.236
