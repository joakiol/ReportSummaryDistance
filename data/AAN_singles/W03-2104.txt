An Information-theoretic Approach for Argument InterpretationSarah George and Ingrid ZukermanSchool of Computer Science and Software EngineeringMonash UniversityClayton, VICTORIA 3800, AUSTRALIAemail: {sarahg,ingrid}@csse.monash.edu.auAbstractWe describe an information-theoreticargument-interpretation mechanism em-bedded in an interactive system.
Ourmechanism receives as input an argumententered through a web interface.
It gener-ates candidate interpretations in terms ofits underlying knowledge representation ?a Bayesian network, and applies the Mini-mum Message Length principle to selectthe best candidate.
The results of ourpreliminary evaluations are encouraging,with the system generally producing plau-sible interpretations of users?
arguments.Keywords: Minimum message length, discourse in-terpretation, Bayesian networks.1 IntroductionDiscourse interpretation is an essential componentof any dialogue system.
However, most interactivesystems developed to date afford users limited op-portunities to express their views.
The discourse in-terpretation mechanism described in this paper con-stitutes a step towards solving this problem.This research builds on our previous work onBIAS ?
a Bayesian Interactive Argumentation Sys-tem which uses Bayesian networks (BNs) (Pearl,1988) as its knowledge representation and reason-ing formalism.
BIAS is designed to be a completeargumentation system which will eventually engagein unrestricted interactions with users.
However, inthis paper, we focus on its discourse interpretationmechanism and the impact of attentional focus onthe interpretation process.The contributions of this paper are as follows.1.
We incorporate attentional focus into thediscourse-interpretation formalism described in(Zukerman and George, 2002), which uses theMinimum Message Length (MML) Principle(Wallace and Boulton, 1968) to evaluate candi-date discourse interpretations.2.
We investigate a web-based argumentation facil-ity for the detective game described in (Zuker-man, 2001).In the following section, we describe our detec-tive game, and discuss our knowledge representa-tion.
Next, we outline the argument interpretationprocess.
In Section 4, we provide an overview of ourMinimum Message Length approach to discourseinterpretation, and describe how attentional focus isincorporated into this formalism.
The results of ourevaluation are reported in Section 5.
We then discussrelated research, followed by concluding remarks.2 Detective GameAs for the system described in (Zukerman, 2001),our experimental set up takes the form of a gamewhere the user and the system are partners in solv-ing a murder mystery, and neither the user nor thesystem is omniscient.
That is, they have access onlyto information they can find out by investigating themurder.
However, our current set up differs fromthat of our previous work in that the user is a ju-nior detective, and the system is a desk-bound boss,(a) Screen shot of Mr Body?s bedroom (b) Detective Gir?s notebookFigure 1: Sample screen of the WWW interface and Detective?s Notebookwho knows only what the user tells him.
Thus,the user does all the leg-work, navigating througha virtual crime scene, making observations and in-terviewing witnesses, and reports periodically to theboss.
These reports consist of successively evolv-ing arguments for the main suspect?s guilt or inno-cence.
Further, the user has limited resources, i.e.,time and money, which are depleted as the investi-gation progresses.
To win the game, the user mustbuild a cogent argument regarding the guilt or inno-cence of the main suspect prior to exhausting his/herresources.In order to evaluate the discourse interpretationcapabilities of the system, in this paper we restrictthe users?
interaction with the system to a singleround.
That is, a user reads the initial police report,optionally explores the virtual scenario, and thenpresents an argument to his/her boss.
The systeminterprets the argument, and presents its interpreta-tion back to the user for validation.
The results ofthis validation are discussed in our evaluation (Sec-tion 5).
In the future, the boss will present counter-arguments, point out flaws in the user?s argument ormake suggestions regarding further investigations.2.1 Playing the game ?
initial interactionThe game starts with the presentation of a police re-port that describes the preliminaries of the case fora particular scenario.
The following police report ispresented for the scenario used in this paper.Yesterday, Mr Body was found dead in hisbedroom.
Fatal bullet wounds were found inMr Body?s chest.Broken glass was found inside the bedroomwindow.
A gun was found in the garden out-side the house, and fingerprints were found onthe gun.Fresh footprints were found near the house,and some peculiar indentations were ob-served in the ground.
Also, blue car paint wasscraped on the letter box.After reading the police report, the user may nav-igate through a virtual space to gather additional in-formation (Figure 1(a) shows a screen shot of thevictim?s bedroom).
The user may record informa-tion s/he considers interesting in his/her Notebook(Figure 1(b)), which is consulted by the user dur-ing the argument construction process.
Upon com-pletion of his/her investigation, the user builds anargument composed of a sequence of implicationsleading from evidence to the argument goal.
Eachimplication is composed of one or more antecedentsand consequents.
In the current implementation, theantecedents and consequents are obtained by copy-ing propositions from a drop-down menu into slotsin the argument-construction interface.1 Figure 2shows a screen-shot of the argument-constructioninterface, and an argument built by a particular user1An alternative version of our system accepts free-form Nat-ural Language (NL) input for antecedents and consequents.However, in our current version this capability has been re-placed with a web-based interface.Figure 2: Argument-construction screen and user?s argumentafter she has read the police report, seen the news-paper and spoken to the forensic experts.
Figure 3shows the interpretation generated by BIAS for theargument in Figure 2.
In it the system fills in propo-sitions and relations where the user has made infer-ential leaps, and points out its beliefs and the user?s.2.2 Domain representationThe domain propositions and the relationships be-tween them are represented by means of a Bayesiannetwork (BN) (Pearl, 1988).
Each BN in the sys-tem can support a variety of scenarios, dependingon the instantiation of the evidence nodes.
Themurder mystery used for this paper is representedby means of an 85-node BN (similar to that usedin (Zukerman, 2001)).
Figure 4 shows a portionof this BN: the evidence nodes are boxed, and thegoal node ?
GmurderedB ?
is circled.
The five evi-dence nodes mentioned in the police report are bold-faced and shaded, the two evidence nodes obtainedby the user in her investigation are boldfaced anddark shaded ([BayesianTimes Reports B Took G?s Girl-friend] and [Forensics Match G?s Fingerprints]), and theevidence nodes employed by the user in her argu-ment have white text.
The nodes corresponding tothe consequents in the user?s argument are boldfaced([G Has Means], [G Has Motive] and [G Murdered B]).3 Proposing InterpretationsBIAS generates interpretations of the user?s argu-ment in terms of its own beliefs and inferences,which may differ from those in the user?s argu-ment.
This may require adding propositions andrelations to the argument structure proposed by theuser, deleting relations from the user?s argument, orpostulating degrees of belief in the propositions inthe user?s argument which differ from those statedby the user.Our system generates candidate interpretationsfor an argument by finding different ways of con-necting the propositions in the argument ?
each vari-ant being a candidate interpretation.
This is done by(1) connecting the nodes in the argument, (2) remov-ing superfluous nodes, and (3) building connectedsub-graphs of the resultant graph.Connecting nodes.
This is done by retrieving fromthe domain BN neighbouring nodes to the nodesmentioned in the user?s argument (each propositionaccessible through the argumentation interface cor-responds to a node in the domain BN).
BIAS thenretrieves the neighbours?
neighbours, and so on fora few iterations.
These retrieved neighbours are in-ferred nodes.
This process of retrieving neighboursenables us to model ?inferential leaps?, i.e., connec-tions made by a user between two nodes that are notFigure 3: BIAS?
interpretation of the user?s argumentOn Found GunGandBarguedG Visited BargumentLast WeekLast WeekG?s ladderAtWindowOnePersonInGardenLast WeekG?sCarWasHereLast WeekNbourHeardG?s GirlfriendB SeducedBayesian Times ReportsB Took G?s GirlfriendFound on GunFingerprintsWindowBrokenFrom OutsideForensicReliableFingerprintsBulletsWithFoundGunForensicMatchReliableForensicBulletsFoundGunRegisteredToGBKilledByGunOutside WindowBKilled FromLadderAtWindowGMurderedBFoundGunIsMurderWeaponForensic SayDeathTime11FoundGunAvToGOnlyB?sBodyFoundIn BedroomBullets WoundsFound InB?sBodyTimeOfDeathGInGardenAtG?s FingerprintsForensic MatchFiredByGFoundGunFiredByGMurderWeaponGun FoundIn GardenGHasMotiveBIsDeadBelongToGFingerprints GHasOpportunityBWasMurderedGAtWindowTimeOfDeath11GHasMeansGAndBEnemiesFigure 4: Interpretation of the user?s argument and partial BNadjacent in the domain BN.
As a result of this pro-cess, mentioned nodes that are separated by a fewinferred nodes in the domain BN will now be con-nected, but mentioned nodes that are far apart willremain unconnected.
If upon completion of this pro-cess, a proposition in the user?s argument is still un-connected, the system will have failed to find an in-terpretation (in the future, the user will be asked tofill this gap).Removing superfluous nodes.
This is done bymarginalizing out nodes that are not on a path be-tween an evidence node and the goal node.Building sub-graphs.
BIAS derives all the interpre-tations of an argument by computing combinationsof paths which produce connected graphs that incor-porate the nodes mentioned by the user.The Bayesian subnets generated in this mannerare candidate interpretations of a user?s argument interms of BIAS?
domain knowledge.
However, thesesubnets alone do not always yield the beliefs statedby the user, as the user may have taken into accountimplicit assumptions that influence his/her beliefs.For instance, the argument in Figure 2 posits a beliefof A Little Likely in Mr Green?s guilt, while Bayesianpropagation from the available evidence yields a be-lief of A Little Unlikely.
This discrepancy may be at-tributed to the user?s lack of consideration of MrGreen?s opportunity to murder Mr Body (her argu-ment includes only means and motive), an erroneousassessment of Mr Green?s opportunity, or an assess-ment of the impact of opportunity on guilt which dif-fers from BIAS?.
In the future, our mechanism willconsider the first two factors for neighbouring nodesof an interpretation (the third factor involves learn-ing a user?s Conditional Probability Tables ?
a taskthat is outside the scope of this project).4 Selecting an InterpretationIn this section we present the Minimum MessageLength criterion, describe its use for argument in-terpretation, and show how attentional focus is in-corporated into our MML model.4.1 MML EncodingMML (Wallace and Boulton, 1968) is a model se-lection criterion (used to select between candidatemodels that explain observed data).
The MML cri-terion implements Occam?s Razor, which may bestated as follows: ?If you have two theories whichboth explain the observed facts, then you shoulduse the simplest until more evidence comes along?
(the same idea is embodied in Einstein?s aphorism?Make everything as simple as possible, but not sim-pler?).
This criterion balances data fit with modelcomplexity.
That is, the best model should fit thedata well and it should be simple.
In probabilisticterms, given data D, the MML criterion selects themodel M with the highest posterior probability.argmaxMPr(M |D) =Pr(D&M)Pr(D)= Pr(M) ?
Pr(D|M)Pr(D)(the constant denominator can be ignored when se-lecting the highest-probability model.
)An optimal encoding for an event E withprobability Pr(E) has message length ML(E) =?
log2 Pr(E) (in bits).
Hence, in information the-oretic terms, the MML criterion selects the modelM which yields the shortest message that transmitsthe model and the data.argminMML(D&M) = ML(M) + ML(D|M)The message for the data and the model is composedof two parts: the first part transmits the model, andthe second part transmits instructions for recover-ing the data from the model.
The model for whichML(D&M) is minimal is the model with the high-est posterior probability.4.2 Evaluating InterpretationsThe problem of selecting an interpretation for auser?s argument among candidate interpretationsmay be viewed as a model selection problem, wherethe argument is the data, and the interpretation isthe model.
Let Arg be a graphical representationof an argument (with antecedents pointing to con-sequents), and SysInt an interpretation generated byour system.
Thus, we are looking for the SysIntwhich yields the shortest message length forML(Arg&SysInt) = ML(SysInt) + ML(Arg|SysInt)The first part of the message describes the inter-pretation, and the second part describes how to re-construct the argument from the interpretation.
TheMr Green?s ladder was outside the windowMr Green was in the garden at the time of deathMr Green had the opportunityto kill Mr BodyG had Opportunity to Murder BG?s Ladder At Window G InGardenAtTimeOfDeathG AtWindowG?s Ladder At Window G InGardenAtTimeOfDeathLadder Outside WindowB Killed From Outside WindowG had Opportunity to Murder BSysIntA SysIntBFigure 5: Interpretation of a simple argumentexpectation from using the MML criterion is that infinding an interpretation that yields the shortest mes-sage for an NL argument (i.e., the interpretation withthe highest posterior probability), we will have pro-duced a plausible interpretation, which hopefully isthe intended interpretation.
This interpretation is de-termined by comparing the message length of thecandidate interpretations, which are obtained as de-scribed in Section 3.Since domain propositions (rather than NL sen-tences) are used to construct an argument, Arg canbe directly obtained from the input.2 SysInt is thenderived from Arg by using the links and nodes inthe domain BN to connect the propositions in the ar-gument (Section 3).
When the underlying represen-tation has several ways of connecting between thenodes in Arg, then more than one candidate SysInt isgenerated (each candidate has at least one inferrednode that does not appear in the other candidates).This is the case for the simple argument in Figure 5,which is composed of two antecedents and one con-sequent.
This argument has two interpretations (theportion of the domain BN from which the interpre-tations are extracted appears at the bottom of Fig-ure 4).
The inferred nodes for the two interpretationsin Figure 5 are [G At Window] for SysIntA, and [Lad-der Outside Window] and [B Killed From Outside Window]for SysIntB ; the inferred links are drawn with dashedlines (the nodes in Arg are shaded, and the links arecurved).After candidate interpretations have been postu-lated, the MML criterion is applied to select the bestinterpretation, i.e., the interpretation with the short-2This is not the case in the version of the system which takesNL input, since there may be more than one proposition that isa reasonable interpretation for a sentence in an argument.est message.
The calculation of the message lengthtakes into account the following factors: (1) the sizeof an interpretation, and (2) the structural and beliefsimilarity between the interpretation and the argu-ment.
These factors influence the components of themessage length as follows.?
ML(SysInt) represents the probability of SysInt.According to the MML principle, concise in-terpretations (in terms of number of nodes andlinks) are more probable than more verbose in-terpretations.?
ML(Arg|SysInt) represents the probability that auser uttered Arg when s/he intended SysInt.
Thisprobability depends on the structural similaritybetween SysInt and Arg (in terms of the nodesand links that appear in Arg and SysInt), and onthe similarity between the beliefs in the nodes inArg and the corresponding nodes in SysInt (thebeliefs in the nodes in SysInt are obtained by per-forming Bayesian propagation through SysInt;thus, different SysInts may yield different beliefsin the consequents of an argument).
Accordingto this component, interpretations that are moresimilar to Arg are more probable (i.e., yield ashorter message) than interpretations that are lesssimilar to Arg.Table 1 summarizes the effect of these factors onthe message length for SysIntA and SysIntB (Fig-ure 5).
SysIntA is simpler than SysIntB , thus yield-ing a shorter message length for the first componentof the message.
SysIntA is structurally more simi-lar to Arg than SysIntB , yielding a shorter messagelength for this aspect of Arg|SysIntA (SysIntA dif-fers from Arg by 1 node and 3 links, while SysIntBTable 1: Message length comparison of two interpretationsML of Factor SysIntA SysIntB Shortest MLSysInt Size 4 nodes, 3 links 5 nodes, 4 links SysIntAArg|SysInt Structural similarity 1 node, 3 links difference 2 nodes, 4 links difference SysIntABelief similarity less similar more similar SysIntBdiffers from Arg by 2 nodes and 4 links3).
In thisexample, we assume that the belief in [G had Op-portunity to Murder B] in SysIntB is stronger than thatin SysIntA, and hence closer to the asserted conse-quent of the argument in Figure 5.
This yields ashorter message length for the belief component ofML(Arg |SysIntB).
However, this is not sufficientto overcome the shorter message length of SysIntAdue to structural similarity and conciseness.
Thus,although both interpretations of the argument arereasonable, SysIntA is the preferred interpretation.As seen in this example, the MML criterionweighs possibly conflicting considerations duringdiscourse interpretation, e.g., a verbose interpreta-tion that is similar to a user?s argument is preferredto a more concise interpretation that is dissimilar.4.3 Modeling Attentional FocusThe above-presented interpretation process assumesthat all inferred propositions are equally likely to beincluded in a user?s argument.
However, this is notnecessarily the case.
We posit that a user is morelikely to imply (inside an inferential leap) proposi-tions previously seen by him/her than propositionss/he has never encountered.
In this case, the lengthof the part of the message which conveys SysIntshould not only depend on the size of the interpre-tation (in terms of number of nodes and links), butalso on the probability that the user will employ inhis/her argument the nodes in that interpretation.We have modeled the probability that a user willinclude a proposition in his/her argument as a func-tion of the proposition?s presence in the user?s focusof attention.
Attentional focus in turn was modeledby means of an activation level which depends on3There are 2 links in SysIntA that are not in Arg, and 1 linkin Arg that is not in SysIntA (total 3 links difference).
A similarcalculation is performed for SysIntB .the following factors:4?
the type of access of a proposition, e.g., whetherthe proposition was copied from the menu orseen when exploring the scenario; and?
the passage of time, i.e., the longer the timeelapsed since the last access to the proposition,the lower its level of activation.These factors were combined as follows to ex-press the probability that a proposition will be in-cluded in an argument:Pr(Prop) ?n?i=1AccessTypei(Prop) ?
[CurTime ?
TimeStmp i + 1]?bwhere n is the number of times the proposition wasaccessed, AccessType is a score that reflects the man-ner in which the proposition was accessed, b = 1 isan empirically determined exponent, CurTime is thecurrent time, and TimeStmp i is the time of the ithaccess.
According to this formula, when a propo-sition is accessed, activation is added to the currentaccumulated (and decayed) activation.
That is, thereis a spike in the level of activation of the proposition,which starts decaying from that point again.To illustrate the effect of attentional focus on theargument interpretation process, let us reconsiderthe sample argument in Figure 5, and let us assumethat [G At Window] was never seen by the user, while[Ladder Outside Window] and [B Killed From OutsideWindow] were seen recently.
In this case, a high prob-ability for these two propositions may overcome thefactors in favour of SysIntA, thereby making SysIntBthe preferred interpretation.4Other factors, such as intrinsic salience of a proposition,have not been modeled at present.5 EvaluationThe system was evaluated in two modes: (1) auto-matic and (2) user based.Our automatic evaluation, described in (Zuker-man and George, 2002), consisted of having the sys-tem interpret noisy versions of its own arguments.These arguments were generated from different sub-nets of its domain BN, and they were distorted bychanging the beliefs in the nodes, and inserting anddeleting nodes and arcs.
All these distortions wereperformed on BNs of different sizes (3, 5, 7 and 9arcs).
Our measure of performance was the edit-distance between the original BN used to generatean argument, and the BN produced as the inter-pretation of this argument.
BIAS produced an in-terpretation in 86% of the 5400 trials.
In 75% ofthe 5400 cases, the generated interpretations had anedit-distance of 3 or less from the original BN (e.g.,the interpretation differed from the original argu-ment by one node and two links), and in 50% ofthe cases, the interpretations matched perfectly theoriginal BN.A preliminary user-based evaluation was per-formed with 10 computer-literate staff and studentsfrom our University.
Our evaluation was conductedas follows.
We introduced the users to our system,and explained its aims.
We then encouraged themto explore the scenario, and when they were ready,they built an argument using the interface shown inFigure 2.
BIAS then generated an interpretation ofthe argument, presenting it as shown in Figure 3.The users were asked to assess BIAS?
interpreta-tion under two conditions: before and after seeinga diagram of the domain BN.
In the initial assess-ment, the users were asked to give BIAS?
interpre-tation a score between 1 (Very UNreasonable) and 5(Very Reasonable), and to optionally provide furthercomments.
In the second assessment, the users weregiven the complete diagram for the partial BN shownin Figure 4, and were asked to re-assess BIAS?
in-terpretation in light of this domain knowledge.
Theywere also asked to trace their preferred interpretationon the diagram (on paper).Our users found the system somewhat daunting,and indicated that the interface for entering an ar-gument was inconvenient.
We believe that this waspartly due to their lack of familiarity with the avail-able domain propositions.
That is, the users werefaced with 85 new propositions, which they had toread in order to determine which one(s) they coulduse to express what they had in mind.
Nonetheless,the users managed to construct arguments, whichranged in size from 2 propositions to 26, and gave agenerally favourable assessment of BIAS?
interpre-tations.
Overall the average score of BIAS?
inter-pretations was 4 before seeing the BN diagram and4.25 after seeing the diagram.
This indicates that un-derstanding the constraints imposed by a system?sdomain knowledge may influence users?
ability tointeract with the system.The main lessons learned from this preliminaryevaluation pertain to two aspects: (1) the interface,and (2) the use of BNs for discourse understanding.In order to improve the usability of the interface, wewill integrate it with BIAS?
NL module.
It is envis-aged that a solution combining menus and NL inputwill yield the best results.
Our evaluation also cor-roborates the insights from Section 3 regarding thedifficulties of taking into account users?
assumptionsduring the argument interpretation process.
How-ever, the results of our evaluation are encouragingwith respect to the use of the MML principle for theselection of interpretations.
In the future, we pro-pose to conduct a more rigorous evaluation with ad-ditional users to confirm these results.6 Related ResearchOur research builds on work described in (Zuker-man, 2001; Zukerman and George, 2002), which in-tegrates plan recognition for discourse understand-ing with BNs.
Zukerman (2001) used a domainmodel and user model represented as a BN, togetherwith linguistic and attentional information, to infera user?s goal from a single-proposition rejoinder toa system-generated argument.
However, the com-bination of these knowledge sources was based onheuristics.
Zukerman and George (2002) developedthe initial probabilistic model for applying the MMLprinciple to argument interpretation.
Here we extendthis model to include attentional focus, integrate itinto an interactive interpretation system, and evalu-ate it with users.The MML principle (Wallace and Boulton,1968) is a model-selection technique whichapplies information-theoretic criteria to tradedata fit against model complexity.
MML hasbeen used in a variety of applications, sev-eral of which are listed in http://www.csse.monash.edu.au/?dld/Snob.application.papers.In this paper, we demonstrate the applicability ofMML to a high-level NL task.BNs have been used in several systems that per-form plan recognition, e.g., (Charniak and Gold-man, 1993; Gertner et al, 1998; Horvitz and Paek,1999).
Charniak and Goldman?s system (Charniakand Goldman, 1993) handled complex narratives,using a BN and marker passing for plan recognition.It automatically built and incrementally extended aBN from propositions read in a story, so that theBN represented hypotheses that became plausibleas the story unfolded.
Marker passing was used torestrict the nodes included in the BN.
In contrast,we use domain knowledge to constrain our under-standing of the propositions in a user?s argument,and apply the MML principle to select a plausibleinterpretation.
Gertner et al (1998) used a BN torepresent the solution of a physics problem.
Afterobserving an action performed by a student, theirsystem (Andes) postulated candidate interpretations(like BIAS?
SysInt), each hypothesizing subsequentactions.
Unlike Andes, BIAS is presented with acomplete argument.
Hence, it must also consider thefit between all the argument propositions and the in-terpretation (Arg|SysInt).
Finally, the system devel-oped by Horvitz and Paek (1999) handled short di-alogue contributions, and used BNs at different lev-els of an abstraction hierarchy to infer a user?s goalin information-seeking interactions with a BayesianReceptionist.
In addition, they employed decision-theoretic strategies to guide the progress of the di-alogue.
We expect to use such strategies when oursystem engages in a full dialogue with users.7 ConclusionWe have offered a mechanism based on the MMLprinciple that generates interpretations of extendedarguments in the context of a BN.
The MML prin-ciple provides a theoretically sound framework forweighing possibly conflicting considerations duringdiscourse interpretation.
This framework enablesus to represent structural discrepancies between theunderlying, detailed domain representation and themore sparse arguments produced by people (whichtypically contain inferential leaps).
The results ofour formative evaluation are encouraging, support-ing the application of the MML principle for argu-ment interpretation.AcknowledgmentsThis research was supported in part by AustralianResearch Council grant A49927212.
The authorsthanks Charles Twardy on insightful comments re-garding MML.ReferencesEugene Charniak and Robert P. Goldman.
1993.
ABayesian model of plan recognition.
Artificial Intel-ligence, 64(1):50?56.Abigail Gertner, Cristina Conati, and Kurt VanLehn.1998.
Procedural help in Andes: Generating hints us-ing a Bayesian network student model.
In AAAI98 ?Proceedings of the Fifteenth National Conference onArtificial Intelligence, pages 106?111, Madison, Wis-consin.Eric Horvitz and Tim Paek.
1999.
A computational ar-chitecture for conversation.
In UM99 ?
Proceedings ofthe Seventh International Conference on User Model-ing, pages 201?210, Banff, Canada.Judea Pearl.
1988.
Probabilistic Reasoning in IntelligentSystems.
Morgan Kaufmann Publishers, San Mateo,California.C.S.
Wallace and D.M.
Boulton.
1968.
An informa-tion measure for classification.
The Computer Jour-nal, 11:185?194.Ingrid Zukerman and Sarah George.
2002.
A MinimumMessage Length approach for argument interpretation.In Proceedings of the Third SIGdial Workshop on Dis-course and Dialogue, pages 211?220, Philadelphia,Pennsylvania.Ingrid Zukerman.
2001.
An integrated approach for gen-erating arguments and rebuttals and understanding re-joinders.
In UM01 ?
Proceedings of the Eighth Inter-national Conference on User Modeling, pages 84?94,Sonthofen, Germany.
