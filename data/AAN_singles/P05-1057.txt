Proceedings of the 43rd Annual Meeting of the ACL, pages 459?466,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsLog-linear Models for Word AlignmentYang Liu , Qun Liu and Shouxun LinInstitute of Computing TechnologyChinese Academy of SciencesNo.
6 Kexueyuan South Road, Haidian DistrictP.
O.
Box 2704, Beijing, 100080, China{yliu, liuqun, sxlin}@ict.ac.cnAbstractWe present a framework for word align-ment based on log-linear models.
Allknowledge sources are treated as featurefunctions, which depend on the sourcelangauge sentence, the target languagesentence and possible additional vari-ables.
Log-linear models allow statis-tical alignment models to be easily ex-tended by incorporating syntactic infor-mation.
In this paper, we use IBM Model3 alignment probabilities, POS correspon-dence, and bilingual dictionary cover-age as features.
Our experiments showthat log-linear models significantly out-perform IBM translation models.1 IntroductionWord alignment, which can be defined as an objectfor indicating the corresponding words in a paralleltext, was first introduced as an intermediate result ofstatistical translation models (Brown et al, 1993).
Instatistical machine translation, word alignment playsa crucial role as word-aligned corpora have beenfound to be an excellent source of translation-relatedknowledge.Various methods have been proposed for findingword alignments between parallel texts.
There aregenerally two categories of alignment approaches:statistical approaches and heuristic approaches.Statistical approaches, which depend on a set ofunknown parameters that are learned from trainingdata, try to describe the relationship between a bilin-gual sentence pair (Brown et al, 1993; Vogel andNey, 1996).
Heuristic approaches obtain word align-ments by using various similarity functions betweenthe types of the two languages (Smadja et al, 1996;Ker and Chang, 1997; Melamed, 2000).
The cen-tral distinction between statistical and heuristic ap-proaches is that statistical approaches are based onwell-founded probabilistic models while heuristicones are not.
Studies reveal that statistical alignmentmodels outperform the simple Dice coefficient (Ochand Ney, 2003).Finding word alignments between parallel texts,however, is still far from a trivial work due to the di-versity of natural languages.
For example, the align-ment of words within idiomatic expressions, freetranslations, and missing content or function wordsis problematic.
When two languages widely differin word order, finding word alignments is especiallyhard.
Therefore, it is necessary to incorporate alluseful linguistic information to alleviate these prob-lems.Tiedemann (2003) introduced a word alignmentapproach based on combination of association clues.Clues combination is done by disjunction of singleclues, which are defined as probabilities of associa-tions.
The crucial assumption of clue combinationthat clues are independent of each other, however,is not always true.
Och and Ney (2003) proposedModel 6, a log-linear combination of IBM transla-tion models and HMM model.
Although Model 6yields better results than naive IBM models, it failsto include dependencies other than IBM models andHMM model.
Cherry and Lin (2003) developed a459statistical model to find word alignments, which al-low easy integration of context-specific features.Log-linear models, which are very suitable to in-corporate additional dependencies, have been suc-cessfully applied to statistical machine translation(Och and Ney, 2002).
In this paper, we present aframework for word alignment based on log-linearmodels, allowing statistical models to be easily ex-tended by incorporating additional syntactic depen-dencies.
We use IBM Model 3 alignment proba-bilities, POS correspondence, and bilingual dictio-nary coverage as features.
Our experiments showthat log-linear models significantly outperform IBMtranslation models.We begin by describing log-linear models forword alignment.
The design of feature functionsis discussed then.
Next, we present the trainingmethod and the search algorithm for log-linear mod-els.
We will follow with our experimental resultsand conclusion and close with a discussion of possi-ble future directions.2 Log-linear ModelsFormally, we use following definition for alignment.Given a source (?English?)
sentence e = eI1 = e1,.
.
.
, ei, .
.
.
, eI and a target language (?French?)
sen-tence f = fJ1 = f1, .
.
.
, fj , .
.
.
, fJ .
We define a linkl = (i, j) to exist if ei and fj are translation (or partof a translation) of one another.
We define the nulllink l = (i, 0) to exist if ei does not correspond to atranslation for any French word in f .
The null linkl = (0, j) is defined similarly.
An alignment a isdefined as a subset of the Cartesian product of theword positions:a ?
{(i, j) : i = 0, .
.
.
, I; j = 0, .
.
.
, J} (1)We define the alignment problem as finding thealignment a that maximizes Pr(a | e, f ) given e andf .We directly model the probability Pr(a | e, f ).An especially well-founded framework is maximumentropy (Berger et al, 1996).
In this framework, wehave a set of M feature functions hm(a, e, f), m =1, .
.
.
, M .
For each feature function, there existsa model parameter ?m, m = 1, .
.
.
, M .
The directalignment probability is given by:Pr(a|e, f) = exp[?Mm=1 ?mhm(a, e, f)]?a?
exp[?Mm=1 ?mhm(a?, e, f)](2)This approach has been suggested by (Papineni etal., 1997) for a natural language understanding taskand successfully applied to statistical machine trans-lation by (Och and Ney, 2002).We obtain the following decision rule:a?
= argmaxa{ M?m=1?mhm(a, e, f)}(3)Typically, the source language sentence e and thetarget sentence f are the fundamental knowledgesources for the task of finding word alignments.
Lin-guistic data, which can be used to identify associ-ations between lexical items are often ignored bytraditional word alignment approaches.
Linguistictools such as part-of-speech taggers, parsers, named-entity recognizers have become more and more ro-bust and available for many languages by now.
Itis important to make use of linguistic informationto improve alignment strategies.
Treated as featurefunctions, syntactic dependencies can be easily in-corporated into log-linear models.In order to incorporate a new dependency whichcontains extra information other than the bilingualsentence pair, we modify Eq.2 by adding a new vari-able v:Pr(a|e, f ,v) = exp[?Mm=1 ?mhm(a, e, f ,v)]?a?
exp[?Mm=1 ?mhm(a?, e, f ,v)](4)Accordingly, we get a new decision rule:a?
= argmaxa{ M?m=1?mhm(a, e, f ,v)}(5)Note that our log-linear models are different fromModel 6 proposed by Och and Ney (2003), whichdefines the alignment problem as finding the align-ment a that maximizes Pr(f , a | e) given e.3 Feature FunctionsIn this paper, we use IBM translation Model 3 as thebase feature of our log-linear models.
In addition,we also make use of syntactic information such aspart-of-speech tags and bilingual dictionaries.4603.1 IBM Translation ModelsBrown et al (1993) proposed a series of statisti-cal models of the translation process.
IBM trans-lation models try to model the translation probabil-ity Pr(fJ1 |eI1), which describes the relationship be-tween a source language sentence eI1 and a targetlanguage sentence fJ1 .
In statistical alignment mod-els Pr(fJ1 , aJ1 |eI1), a ?hidden?
alignment a = aJ1 isintroduced, which describes a mapping from a tar-get position j to a source position i = aj .
Therelationship between the translation model and thealignment model is given by:Pr(fJ1 |eI1) =?aJ1Pr(fJ1 , aJ1 |eI1) (6)Although IBM models are considered more co-herent than heuristic models, they have two draw-backs.
First, IBM models are restricted in a waysuch that each target word fj is assigned to exactlyone source word eaj .
A more general way is tomodel alignment as an arbitrary relation betweensource and target language positions.
Second, IBMmodels are typically language-independent and mayfail to tackle problems occurred due to specific lan-guages.In this paper, we use Model 3 as our base featurefunction, which is given by 1:h(a, e, f) = Pr(fJ1 , aJ1 |eI1)=(m?
?0?0)p0m?2?0p1?0l?i=1?i!n(?i|ei)?m?j=1t(fj |eaj )d(j|aj , l,m) (7)We distinguish between two translation directionsto use Model 3 as feature functions: treating Englishas source language and French as target language orvice versa.3.2 POS Tags Transition ModelThe first linguistic information we adopt other thanthe source language sentence e and the target lan-guage sentence f is part-of-speech tags.
The useof POS information for improving statistical align-ment quality of the HMM-based model is described1If there is a target word which is assigned to more than onesource words, h(a, e, f) = 0.in (Toutanova et al, 2002).
They introduce addi-tional lexicon probability for POS tags in both lan-guages.In IBM models as well as HMM models, whenone needs the model to take new information intoaccount, one must create an extended model whichcan base its parameters on the previous model.
Inlog-linear models, however, new information can beeasily incorporated.We use a POS Tags Transition Model as a fea-ture function.
This feature learns POS Tags tran-sition probabilities from held-out data (via simplecounting) and then applies the learned distributionsto the ranking of various word alignments.
Wedefine eT = eT I1 = eT1, .
.
.
, eTi, .
.
.
, eTI andfT = fT J1 = fT1, .
.
.
, fTj , .
.
.
, fTJ as POS tagsequences of the sentence pair e and f .
POS TagsTransition Model is formally described as:Pr(fT|a, eT) =?at(fTa(j)|eTa(i)) (8)where a is an element of a, a(i) is the correspondingsource position of a and a(j) is the target position.Hence, the feature function is:h(a, e, f , eT, fT) =?at(fTa(j)|eTa(i)) (9)We still distinguish between two translation direc-tions to use POS tags Transition Model as featurefunctions: treating English as source language andFrench as target language or vice versa.3.3 Bilingual DictionaryA conventional bilingual dictionary can be consid-ered an additional knowledge source.
We could usea feature that counts how many entries of a conven-tional lexicon co-occur in a given alignment betweenthe source sentence and the target sentence.
There-fore, the weight for the provided conventional dic-tionary can be learned.
The intuition is that the con-ventional dictionary is expected to be more reliablethan the automatically trained lexicon and thereforeshould get a larger weight.We define a bilingual dictionary as a set of entries:D = {(e, f, conf)}.
e is a source language word,f is a target langauge word, and conf is a positivereal-valued number (usually, conf = 1.0) assigned461by lexicographers to evaluate the validity of the en-try.
Therefore, the feature function using a bilingualdictionary is:h(a, e, f ,D) =?aoccur(ea(i), fa(j), D) (10)whereoccur(e, f,D) ={conf if (e, f) occurs in D0 else(11)4 TrainingWe use the GIS (Generalized Iterative Scaling) al-gorithm (Darroch and Ratcliff, 1972) to train themodel parameters ?M1 of the log-linear models ac-cording to Eq.
4.
By applying suitable transforma-tions, the GIS algorithm is able to handle any type ofreal-valued features.
In practice, We use YASMET2 written by Franz J. Och for performing training.The renormalization needed in Eq.
4 requires asum over a large number of possible alignments.
Ife has length l and f has length m, there are pos-sible 2lm alignments between e and f (Brown etal., 1993).
It is unrealistic to enumerate all possi-ble alignments when lm is very large.
Hence, weapproximate this sum by sampling the space of allpossible alignments by a large set of highly proba-ble alignments.
The set of considered alignments arealso called n-best list of alignments.We train model parameters on a development cor-pus, which consists of hundreds of manually-alignedbilingual sentence pairs.
Using an n-best approx-imation may result in the problem that the param-eters trained with the GIS algorithm yield worsealignments even on the development corpus.
Thiscan happen because with the modified model scalingfactors the n-best list can change significantly andcan include alignments that have not been taken intoaccount in training.
To avoid this problem, we iter-atively combine n-best lists to train model parame-ters until the resulting n-best list does not change,as suggested by Och (2002).
However, as this train-ing procedure is based on maximum likelihood cri-terion, there is only a loose relation to the final align-ment quality on unseen bilingual texts.
In practice,2Available at http://www.fjoch.com/YASMET.htmlhaving a series of model parameters when the itera-tion ends, we select the model parameters that yieldbest alignments on the development corpus.After the bilingual sentences in the develop-ment corpus are tokenized (or segmented) and POStagged, they can be used to train POS tags transitionprobabilities by counting relative frequencies:p(fT |eT ) = NA(fT, eT )N(eT )Here, NA(fT, eT ) is the frequency that the POS tagfT is aligned to POS tag eT and N(eT ) is the fre-quency of eT in the development corpus.5 SearchWe use a greedy search algorithm to search thealignment with highest probability in the space of allpossible alignments.
A state in this space is a partialalignment.
A transition is defined as the addition ofa single link to the current state.
Our start state isthe empty alignment, where all words in e and f areassigned to null.
A terminal state is a state in whichno more links can be added to increase the probabil-ity of the current alignment.
Our task is to find theterminal state with the highest probability.We can compute gain, which is a heuristic func-tion, instead of probability for efficiency.
A gain isdefined as follows:gain(a, l) = exp[?Mm=1 ?mhm(a ?
l, e, f)]exp[?Mm=1 ?mhm(a, e, f)](12)where l = (i, j) is a link added to a.The greedy search algorithm for general log-linear models is formally described as follows:Input: e, f , eT, fT, and DOutput: a1.
Start with a = ?.2.
Do for each l = (i, j) and l /?
a:Compute gain(a, l)3.
Terminate if ?l, gain(a, l) ?
1.4.
Add the link l?
with the maximal gain(a, l)to a.5.
Goto 2.462The above search algorithm, however, is not effi-cient for our log-linear models.
It is time-consumingfor each feature to figure out a probability whenadding a new link, especially when the sentencesare very long.
For our models, gain(a, l) can beobtained in a more efficient way 3:gain(a, l) =M?m=1?mlog(hm(a ?
l, e, f)hm(a, e, f))(13)Note that we restrict that h(a, e, f) ?
0 for all fea-ture functions.The original terminational condition for greedysearch algorithm is:gain(a, l) = exp[?Mm=1 ?mhm(a ?
l, e, f)]exp[?Mm=1 ?mhm(a, e, f)]?
1.0That is:M?m=1?m[hm(a ?
l, e, f)?
hm(a, e, f)] ?
0.0By introducing gain threshold t, we obtain a newterminational condition:M?m=1?mlog(hm(a ?
l, e, f)hm(a, e, f))?
twheret =M?m=1?m{log(hm(a ?
l, e, f)hm(a, e, f))?
[hm(a ?
l, e, f)?
hm(a, e, f)]}Note that we restrict h(a, e, f) ?
0 for all featurefunctions.
Gain threshold t is a real-valued number,which can be optimized on the development corpus.Therefore, we have a new search algorithm:Input: e, f , eT, fT, D and tOutput: a1.
Start with a = ?.2.
Do for each l = (i, j) and l /?
a:Compute gain(a, l)3We still call the new heuristic function gain to reduce no-tational overhead, although the gain in Eq.
13 is not equivalentto the one in Eq.
12.3.
Terminate if ?l, gain(a, l) ?
t.4.
Add the link l?
with the maximal gain(a, l)to a.5.
Goto 2.The gain threshold t depends on the added linkl.
We remove this dependency for simplicity whenusing it in search algorithm by treating it as a fixedreal-valued number.6 Experimental ResultsWe present in this section results of experiments ona parallel corpus of Chinese-English texts.
Statis-tics for the corpus are shown in Table 1.
We use atraining corpus, which is used to train IBM transla-tion models, a bilingual dictionary, a developmentcorpus, and a test corpus.Chinese EnglishTrain Sentences 108 925Words 3 784 106 3 862 637Vocabulary 49 962 55 698Dict Entries 415 753Vocabulary 206 616 203 497Dev Sentences 435Words 11 462 14 252Ave.
SentLen 26.35 32.76Test Sentences 500Words 13 891 15 291Ave.
SentLen 27.78 30.58Table 1.
Statistics of training corpus (Train), bilin-gual dictionary (Dict), development corpus (Dev),and test corpus (Test).The Chinese sentences in both the developmentand test corpus are segmented and POS tagged byICTCLAS (Zhang et al, 2003).
The English sen-tences are tokenized by a simple tokenizer of oursand POS tagged by a rule-based tagger written byEric Brill (Brill, 1995).
We manually aligned 935sentences, in which we selected 500 sentences astest corpus.
The remaining 435 sentences are usedas development corpus to train POS tags transitionprobabilities and to optimize the model parametersand gain threshold.Provided with human-annotated word-level align-ment, we use precision, recall and AER (Och and463Size of Training Corpus1K 5K 9K 39K 109KModel 3 E ?
C 0.4497 0.4081 0.4009 0.3791 0.3745Model 3 C ?
E 0.4688 0.4261 0.4221 0.3856 0.3469Intersection 0.4588 0.4106 0.4044 0.3823 0.3687Union 0.4596 0.4210 0.4157 0.3824 0.3703Refined Method 0.4154 0.3586 0.3499 0.3153 0.3068Model 3 E ?
C 0.4490 0.3987 0.3834 0.3639 0.3533+ Model 3 C ?
E 0.3970 0.3317 0.3217 0.2949 0.2850+ POS E ?
C 0.3828 0.3182 0.3082 0.2838 0.2739+ POS C ?
E 0.3795 0.3160 0.3032 0.2821 0.2726+ Dict 0.3650 0.3092 0.2982 0.2738 0.2685Table 2.
Comparison of AER for results of using IBM Model 3 (GIZA++) and log-linear models.Ney, 2003) for scoring the viterbi alignments of eachmodel against gold-standard annotated alignments:precision = |A ?
P ||A|recall = |A ?
S||S|AER = 1?
|A ?
S|+ |A ?
P ||A|+ |S|where A is the set of word pairs aligned by wordalignment systems, S is the set marked in the goldstandard as ?sure?
and P is the set marked as ?pos-sible?
(including the ?sure?
pairs).
In our Chinese-English corpus, only one type of alignment wasmarked, meaning that S = P .In the following, we present the results of log-linear models for word alignment.
We used GIZA++package (Och and Ney, 2003) to train IBM transla-tion models.
The training scheme is 15H535, whichmeans that Model 1 are trained for five iterations,HMM model for five iterations and finally Model3 for five iterations.
Except for changing the iter-ations for each model, we use default configurationof GIZA++.
After that, we used three types of meth-ods for performing a symmetrization of IBM mod-els: intersection, union, and refined methods (Ochand Ney , 2003).The base feature of our log-linear models, IBMModel 3, takes the parameters generated by GIZA++as parameters for itself.
In other words, our log-linear models share GIZA++ with the same parame-ters apart from POS transition probability table andbilingual dictionary.Table 2 compares the results of our log-linearmodels with IBM Model 3.
From row 3 to row 7are results obtained by IBM Model 3.
From row 8to row 12 are results obtained by log-linear models.As shown in Table 2, our log-linear modelsachieve better results than IBM Model 3 in all train-ing corpus sizes.
Considering Model 3 E ?
C ofGIZA++ and ours alone, greedy search algorithmdescribed in Section 5 yields surprisingly betteralignments than hillclimbing algorithm in GIZA++.Table 3 compares the results of log-linear mod-els with IBM Model 5.
The training scheme is15H5354555.
Our log-linear models still make useof the parameters generated by GIZA++.Comparing Table 3 with Table 2, we notice thatour log-linear models yield slightly better align-ments by employing parameters generated by thetraining scheme 15H5354555 rather than 15H535,which can be attributed to improvement of param-eters after further Model 4 and Model 5 training.For log-linear models, POS information and anadditional dictionary are used, which is not the casefor GIZA++/IBM models.
However, treated as amethod for performing symmetrization, log-linearcombination alone yields better results than intersec-tion, union, and refined methods.Figure 1 shows how gain threshold has an effecton precision, recall and AER with fixed model scal-ing factors.Figure 2 shows the effect of number of features464Size of Training Corpus1K 5K 9K 39K 109KModel 5 E ?
C 0.4384 0.3934 0.3853 0.3573 0.3429Model 5 C ?
E 0.4564 0.4067 0.3900 0.3423 0.3239Intersection 0.4432 0.3916 0.3798 0.3466 0.3267Union 0.4499 0.4051 0.3923 0.3516 0.3375Refined Method 0.4106 0.3446 0.3262 0.2878 0.2748Model 3 E ?
C 0.4372 0.3873 0.3724 0.3456 0.3334+ Model 3 C ?
E 0.3920 0.3269 0.3167 0.2842 0.2727+ POS E ?
C 0.3807 0.3122 0.3039 0.2732 0.2667+ POS C ?
E 0.3731 0.3091 0.3017 0.2722 0.2657+ Dict 0.3612 0.3046 0.2943 0.2658 0.2625Table 3.
Comparison of AER for results of using IBM Model 5 (GIZA++) and log-linear models.-12 -10 -8 -6 -4 -2 0 2 4 6 8 100.00.20.40.60.81.0gain thresholdPrecisionRecallAERFigure 1.
Precision, recall and AER over differentgain thresholds with the same model scaling factors.and size of training corpus on search efficiency forlog-linear models.Table 4 shows the resulting normalized modelscaling factors.
We see that adding new features alsohas an effect on the other model scaling factors.7 ConclusionWe have presented a framework for word alignmentbased on log-linear models between parallel texts.
Itallows statistical models easily extended by incor-porating syntactic information.
We take IBM Model3 as base feature and use syntactic information suchas POS tags and bilingual dictionary.
Experimental1k 5k 9k 39k 109k20040060080010001200timeconsumedforsearching(second)size of training corpusM3ECM3EC + M3CEM3EC + M3CE + POSECM3EC + M3CE + POSEC + POSCEM3EC + M3CE + POSEC + POSCE + DictFigure 2.
Effect of number of features and size oftraining corpus on search efficiency.MEC +MCE +PEC +PCE +Dict?1 1.000 0.466 0.291 0.202 0.151?2 - 0.534 0.312 0.212 0.167?3 - - 0.397 0.270 0.257?4 - - - 0.316 0.306?5 - - - - 0.119Table 4.
Resulting model scaling factors: ?1: Model3 E ?
C (MEC); ?2: Model 3 C ?
E (MCE); ?3:POS E ?
C (PEC); ?4: POS C ?
E (PCE); ?5: Dict(normalized such that ?5m=1 ?m = 1).results show that log-linear models for word align-ment significantly outperform IBM translation mod-els.
However, the search algorithm we proposed is465supervised, relying on a hand-aligned bilingual cor-pus, while the baseline approach of IBM alignmentsis unsupervised.Currently, we only employ three types of knowl-edge sources as feature functions.
Syntax-basedtranslation models, such as tree-to-string model (Ya-mada and Knight, 2001) and tree-to-tree model(Gildea, 2003), may be very suitable to be added intolog-linear models.It is promising to optimize the model parametersdirectly with respect to AER as suggested in statisti-cal machine translation (Och, 2003).AcknowledgementThis work is supported by National High Technol-ogy Research and Development Program contract?Generally Technical Research and Basic DatabaseEstablishment of Chinese Platform?
(Subject No.2004AA114010).ReferencesAdam L. Berger, Stephen A. Della Pietra, and Vincent J.DellaPietra.
1996.
A maximum entropy approach tonatural language processing.
Computational Linguis-tics, 22(1):39-72, March.Eric Brill.
1995.
Transformation-based-error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Computational Lin-guistics, 21(4), December.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert.
L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263-311.Colin Cherry and Dekang Lin.
2003.
A probabilitymodel to improve word alignment.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics (ACL), Sapporo, Japan.J.
N. Darroch and D. Ratcliff.
1972.
Generalized itera-tive scaling for log-linear models.
Annals of Mathe-matical Statistics, 43:1470-1480.Daniel Gildea.
2003.
Loosely tree-based alignment formachine translation.
In Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics (ACL), Sapporo, Japan.Sue J. Ker and Jason S. Chang.
1997.
A class-based ap-proach to word alignment.
Computational Linguistics,23(2):313-343, June.I.
Dan Melamed 2000.
Models of translational equiv-alence among words.
Computational Linguistics,26(2):221-249, June.Franz J. Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 295-302, Philadelphia, PA,July.Franz J. Och.
2002.
Statistical Machine Translation:From Single-Word Models to Alignment Templates.Ph.D.
thesis, Computer Science Department, RWTHAachen, Germany, October.Franz J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages: 160-167, Sapporo, Japan.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19-51, March.Kishore A. Papineni, Salim Roukos, and Todd Ward.1997.
Feature-based language understanding.
In Eu-ropean Conf.
on Speech Communication and Technol-ogy, pages 1435-1438, Rhodes, Greece, September.Frank Smadja, Vasileios Hatzivassiloglou, and KathleenR.
McKeown 1996.
Translating collocations for bilin-gual lexicons: A statistical approach.
ComputationalLinguistics, 22(1):1-38, March.Jo?rg Tiedemann.
2003.
Combining clues for word align-ment.
In Proceedings of the 10th Conference of Euro-pean Chapter of the ACL (EACL), Budapest, Hungary,April.Kristina Toutanova, H. Tolga Ilhan, and Christopher D.Manning.
2003.
Extensions to HMM-based statisticalword alignment models.
In Proceedings of EmpiricalMethods in Natural Langauge Processing, Philadel-phia, PA.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of the 16th Int.
Conf.
on Com-putational Linguistics, pages 836-841, Copenhagen,Denmark, August.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical machine translation model.
In Pro-ceedings of the 39th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages: 523-530,Toulouse, France, July.Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Liu.2003.
HHMM-based Chinese lexical analyzer ICT-CLAS.
In Proceedings of the second SigHan Work-shop affiliated with 41th ACL, pages: 184-187, Sap-poro, Japan.466
