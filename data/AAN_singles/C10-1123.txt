Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1092?1100,Beijing, August 2010Dependency Forest for Statistical Machine TranslationZhaopeng Tu ?
Yang Liu ?
Young-Sook Hwang ?
Qun Liu ?
Shouxun Lin ?
?Key Lab.
of Intelligent Info.
Processing ?HILab Convergence Technology CenterInstitute of Computing Technology C&I BusinessChinese Academy of Sciences SKTelecom{tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.comAbstractWe propose a structure called dependencyforest for statistical machine translation.A dependency forest compactly representsmultiple dependency trees.
We developnew algorithms for extracting string-to-dependency rules and training depen-dency language models.
Our forest-basedstring-to-dependency system obtains sig-nificant improvements ranging from 1.36to 1.46 BLEU points over the tree-basedbaseline on the NIST 2004/2005/2006Chinese-English test sets.1 IntroductionDependency grammars have become increasinglypopular in syntax-based statistical machine trans-lation (SMT).
One important advantage of depen-dency grammars is that they directly capture thedependencies between words, which are key to re-solving most parsing ambiguities.
As a result, in-corporating dependency trees proves to be effec-tive in improving statistical machine translation(Quirk et al, 2005; Ding and Palmer, 2005; Shenet al, 2008).However, most dependency-based translationsystems suffer from a major drawback: they onlyuse 1-best dependency trees for rule extraction,dependency language model training, and decod-ing, which potentially introduces translation mis-takes due to the propagation of parsing errors(Quirk and Corston-Oliver, 2006).
While thetreelet system (Quirk et al, 2005) takes a de-pendency tree as input, the string-to-dependencysystem (Shen et al, 2008) decodes on a source-language string.
However, as we will show, thestring-to-dependency system still commits to us-ing degenerate rules and dependency languagemodels learned from noisy 1-best trees.To alleviate this problem, an obvious solu-tion is to offer more alternatives.
Recent studieshave shown that SMT systems can benefit fromwidening the annotation pipeline: using packedforests instead of 1-best trees (Mi and Huang,2008), word lattices instead of 1-best segmenta-tions (Dyer et al, 2008), and weighted alignmentmatrices instead of 1-best alignments (Liu et al,2009).Along the same direction, we propose a struc-ture called dependency forest, which encodes ex-ponentially many dependency trees compactly, fordependency-based translation systems.
In this pa-per, we develop two new algorithms for extractingstring-to-dependency rules and for training depen-dency language models, respectively.
We showthat using the rules and dependency languagemodels learned from dependency forests leads toconsistent and significant improvements over thatof using 1-best trees on the NIST 2004/2005/2006Chinese-English test sets.2 BackgroundFigure 1 shows a dependency tree of an Englishsentence he saw a boy with a telescope.
Arrowspoint from the child to the parent, which is oftenreferred to as the head of the child.
For example,in Figure 1, saw is the head of he.
A dependencytree is more compact than its constituent counter-part because there is no need to build a large su-perstructure over a sentence.Shen et al (2008) propose a novel string-to-dependency translation model that features twoimportant advantages.
First, they define thata string-to-dependency rule must have a well-formed dependency structure on the target side,which makes efficient dynamic programming pos-sible and manages to retain most useful non-constituent rules.
A well-formed structure can beeither fixed or floating .
A fixed structure is a1092sawhe boy witha telescopeahe saw a boy with a telescopeta kandao yige dai wangyuanjing de nanhaiFigure 1: A training example for tree-based ruleextraction.dependency tree with all the children complete.Floating structures consist of sibling nodes of acommon head, but the head itself is unspecifiedor floating.
For example, Figure 2(a) and Figure2(b) are two fixed structures while Figure 2(c) is afloating one.Formally, for a given sentence w1:l = w1 .
.
.
wl,d1 .
.
.
dl represent the parent word IDs for eachword.
If wi is a root, we define di = 0.Definition 1.
A dependency structure di..j is fixedon head h, where h /?
[i, j], or fixed for short, ifand only if it meets the following conditions?
dh /?
[i, j]?
?k ?
[i, j] and k 6= h, dk ?
[i, j]?
?k /?
[i, j], dk = h or dk /?
[i, j]Definition 2.
A dependency structure di..j isfloating with children C, for a non-empty set C?
{i, ..., j}, or floating for short, if and only if itmeets the following conditions?
?h /?
[i, j], s.t.
?k ?
C, dk = h?
?k ?
[i, j] and k /?
C, dk ?
[i, j]?
?k /?
[i, j], dk /?
[i, j]A dependency structure is well-formed if andonly if it is either fixed or floating.2.1 Tree-based Rule ExtractionFigure 1 shows a training example consisting of anEnglish dependency tree, its Chinese translation,boya(a)withtelescopea(b)boy witha telescopea(c)Figure 2: Well-formed dependency structures cor-responding to Figure 1.
(a) and (b) are fixed and(c) is floating.and the word alignments between them.
To facil-itate identifying the correspondence between theEnglish and Chinese words, we also gives the En-glish sentence.
Extracting string-to-dependencyrules from aligned string-dependency pairs is sim-ilar to extracting SCFG (Chiang, 2007) except thatthe target side of a rule is a well-formed struc-ture.
For example, we can first extract a string-to-dependency rule that is consistent with the wordalignment (Och and Ney, 2004):with ((a) telescope) ?
dai wangyuanjing deThen a smaller rule(a) telescope ?
wangyuanjingcan be subtracted to obtain a rule with one non-terminal:with (X1) ?
dai X1 dewhere X is a non-terminal and the subscript indi-cates the correspondence between non-terminalson the source and target sides.2.2 Tree-based Dependency Language ModelAs dependency relations directly model the se-mantics structure of a sentence, Shen et al (2008)introduce dependency language model to betteraccount for the generation of target sentences.Compared with the conventional n-gram languagemodels, dependency language model excels atcapturing non-local dependencies between words(e.g., saw ... with in Figure 1).
Given a depen-dency tree, its dependency language model prob-ability is a product of three sub-models definedbetween headwords and their dependants.
For ex-ample, the probability of the tree in Figure 1 can1093saw0,7he0,1 boy2,4 with4,7a2,3 telescope5,7a5,6(a)saw0,7he0,1 boy2,7a2,3 with4,7telescope5,7a5,6(b)saw0,7he0,1 boy2,4 boy2,7with4,7e1 e2a2,3e3 e4telescope5,7e5a5,6e6(c)Figure 3: (a) the dependency tree in Figure 1, (b) another dependency tree for the same sentence, and(c) a dependency forest compactly represents the two trees.be calculated as:Prob = PT (saw)?PL(he|saw-as-head)?PR(boy|saw-as-head)?PR(with|boy, saw-as-head)?PL(a|boy-as-head)?PR(telescope|with-as-head)?PL(a|telescope-as-head)where PT (x) is the probability of word x beingthe root of a dependency tree.
PL and PR are thegenerative probabilities of left and right sides re-spectively.As the string-to-tree system relies on 1-besttrees for parameter estimation, the quality of ruletable and dependency language model might beaffected by parsing errors and therefore ultimatelyresults in translation mistakes.3 Dependency ForestWe propose to encode multiple dependency treesin a compact representation called dependencyforest, which offers an elegant solution to theproblem of parsing error propagation.Figures 3(a) and 3(b) show two dependencytrees for the example English sentence in Figure1.
The prepositional phrase with a telescope couldeither depend on saw or boy.
Figure 3(c) is adependency forest compactly represents the twotrees by sharing common nodes and edges.Each node in a dependency forest is a word.To distinguish among nodes, we attach a span toeach node.
For example, in Figure 1, the span ofthe first a is (2, 3) because it is the third word inthe sentence.
As the fourth word boy dominatesthe node a2,3, it can be referred to as boy2,4.
Notethat the position of boy itself is taken into consid-eration.
Similarly, the word boy in Figure 3(b) canbe represented as boy2,7.The nodes in a dependency forest are connectedby hyperedges.
While an edge in a dependencytree only points from a dependent to its head, ahyperedge groups all the dependants that have acommon head.
For example, in Figure 3(c), thehyperedgee1: ?
(he0,1, boy2,4,with4,7), saw0,7?denotes that he0,1, boy2,4, and with4,7 are depen-dants (from left to right) of saw0,7.More formally, a dependency forest is a pair?V,E?, where V is a set of nodes, and Eis a set of hyperedges.
For a given sentencew1:l = w1 .
.
.
wl, each node v ?
V is in theform of wi,j , which denotes that w dominatesthe substring from positions i through j (i.e.,wi+1 .
.
.
wj).
Each hyperedge e ?
E is a pair?tails(e), head(e)?, where head(e) ?
V is thehead and tails(e) ?
V are its dependants.A dependency forest has a structure of a hy-pergraph such as packed forest (Klein and Man-ning, 2001; Huang and Chiang, 2005).
However,while each hyperedge in a packed forest naturallytreats the corresponding PCFG rule probability asits weight, it is challenging to make dependencyforest to be a weighted hypergraph because depen-dency parsers usually only output a score, whichcan be either positive or negative, for each edgein a dependency tree rather than a hyperedge in a1094saw0,7he0,1 boy2,4 boy2,7with4,7e1 e2a2,3e3 e4telescope5,7e5a5,6e6he saw a boy with a telescopeta kandao yige dai wangyuanjing de nanhaiFigure 4: A training example for forest-based ruleextraction.dependency forest.
For example, in Figure 3(a),the scores for the edges he ?
saw, boy ?
saw,and with ?
saw could be 13, 22, and -12, respec-tively.To assign a probability to each hyperedge, wecan first obtain a positive number for a hyperedgeusing the scores of the corresponding edges:1c(e) = exp(?v?tails(e) s(v, head(e))|tails(e)|)(1)where c(e) is the count of a hyperedge e, head(e)is a head, tails(e) is a set of dependants of thehead, v is one dependant, and s(v, head(e)) is thescore of an edge from v to head(e).
For example,the count of the hyperedge e1 in Figure 3(c) isc(e1) = exp(13 + 22 ?
123)(2)Then, the probability of a hyperedge can be ob-tained by normalizing the count among all hyper-edges with the same head collected from a trainingcorpus:p(e) = c(e)?e?:head(e?
)=head(e) c(e?
)(3)Therefore, we obtain a weighted dependencyforest in which each hyperedge has a probability.1It is difficult to assign a probability to each hyperedge.The current method is arbitrary, and we will improve it in thefuture.Algorithm 1 Forest-based Initial Phrase Extrac-tionInput: a source sentence ?, a forest F , an alignment a,and kOutput: minimal initial phrase setR1: for each node v ?
V in a bottom-up order do2: for each hyperedge e ?
E and head(e) = v do3: W ?
?4: fixs?
EnumFixed(v,modifiers(e))5: floatings?
EnumFloating(modifiers(e))6: add structures fixs, floatings to W7: for each ?
?W do8: if ?
is consistent with a then9: generate a rule r10: R.append(r)11: keep k-best dependency structures for v4 Forest-based Rule ExtractionIn tree-based rule extraction, one just needs to firstenumerate all bilingual phrases that are consis-tent with word alignment and then check whetherthe dependency structures over the target phrasesare well-formed.
However, this algorithm fails towork in the forest scenario because there are usu-ally exponentially many well-formed structuresover a target phrase.The GHKM algorithm (Galley et al, 2004),which is originally developed for extracting tree-to-string rules from 1-best trees, has been suc-cessfully extended to packed forests recently (Miand Huang, 2008).
The algorithm distinguishesbetween minimal and composed rules.
Althoughthere are exponentially many composed rules, thenumber of minimal rules extracted from each nodeis rather limited (e.g., one or zero).
Therefore, onecan obtain promising composed rules by combin-ing minimal rules.Unfortunately, the GHKM algorithm cannot beapplied to extracting string-to-dependency rulesfrom dependency forests.
This is because theGHKM algorithm requires a complete subtree toexist in a rule while neither fixed nor floating de-pendency structures ensure that all dependants ofa head are included.
For example, the floatingstructure shown in Figure 2(c) actually containstwo trees.Alternatively, our algorithm searches for well-formed structures for each node in a bottom-upstyle.
Algorithm 1 shows the algorithm for ex-tracting initial phrases, that is, rules without non-1095terminals from dependency forests.
The algorithmmaintains k-best well-formed structures for eachnode (line 11).
The well-formed structures of ahead can be constructed from those of its depen-dants.
For example, in Figure 4, as the fixed struc-ture rooted at telescope5,7 is(a) telescopewe can obtain a fixed structure rooted for the nodewith4,7 by attaching the fixed structure of its de-pendant to the node (EnumFixed in line 4).
Figure2(b) shows the resulting fixed structure.Similarly, the floating structure for the nodesaw0,7 can be obtained by concatenating the fixedstructures of its dependants boy2,4 and with4,7(EnumFloating in line 5).
Figure 2(c) shows theresulting fixed structure.
The algorithm is similarto Wang et al (2007), which binarize each con-stituent node to create some intermediate nodesthat correspond to the floating structures.Therefore, we can find k-best fixed and float-ing structures for a node in a dependency forestby manipulating the fixed structures of its depen-dants.
Then we can extract string-to-dependencyrules if the dependency structures are consistentwith the word alignment.How to judge a well-formed structure extractedfrom a node is better than others?
We follow Miand Huang (2008) to assign a fractional count toeach well-formed structure.
Given a tree fragmentt, we use the inside-outside algorithm to computeits posterior probability:??
(t) = ?
(root(t)) ??e?tp(e)??v?leaves(t)?
(v) (4)where root(t) is the root of the tree, e is an edge,leaves(t) is a set of leaves of the tree, ?(?)
is out-side probability, and ?(?)
is inside probability.For example, the subtree rooted at boy2,7 in Fig-ure 4 has the following posterior probability:?
(boy2,7) ?
p(e4) ?
p(e5)?p(e6) ?
?
(a2,3) ?
?
(a5,6) (5)Now the fractional count of the subtree t isc(t) = ??(t)??
(TOP ) (6)where TOP denotes the root node of the forest.As a well-formed structure might be non-constituent, we approximate the fractional countby taking that of the minimal constituent tree frag-ment that contains the well-formed structure.
Fi-nally, the fractional counts of well-formed struc-tures can be used to compute the relative frequen-cies of the rules having them on the target side (Miand Huang, 2008):?
(r|lhs(r)) = c(r)?r?:lhs(r?
)=lhs(r) c(r?)(7)?
(r|rhs(r)) = c(r)?r?:rhs(r?
)=rhs(r) c(r?
)(8)Often, our approach extracts a large amount ofrules from training corpus as we usually retain ex-ponentially many well-formed structures over atarget phrase.
To maintain a reasonable rule ta-ble size, we discard any rule that has a fractionalcount lower that a threshold t.5 Forest-based Dependency LanguageModel TrainingDependency language model plays an importantrole in string-to-dependency system.
Shen etal.
(2008) show that string-to-dependency systemachieves 1.48 point improvement in BLEU alongwith dependency language model, while no im-provement without it.
However, the string-to-dependency system still commits to using depen-dency language model from noisy 1-best trees.We now turn to dependency forest for it encodesmultiple dependency trees.To train a dependency language model from adependency forest, we need to collect all headsand their dependants.
This can be easily done byenumerating all hyperedges.
Similarly, we use theinside-outside algorithm to compute the posteriorprobability of each hyperedge e,??
(e) = ?
(head(e)) ?
p(e)??v?tailes(e)?
(v) (9)For example, the posterior probability of the hy-peredge e2 in Figure 4 is calculated as??
(e2) = ?
(saw0,7) ?
p(e2)??
(he0,1) ?
?
(boy2,7) (10)1096Rule DepLM NIST 2004 NIST 2005 NIST 2006 timetree tree 33.97 30.21 30.73 19.6tree forest 34.42?
31.06?
31.37?
24.1forest tree 34.60?
31.16?
31.45?
21.7forest forest 35.33??
31.57??
32.19??
28.5Table 1: BLEU scores and average decoding time (second/sentence) on the Chinese-English test sets.The baseline system (row 2) used the rule table and dependency language model learned both from1-best dependency trees.
We use ?
*?
and ?**?
to denote a result is better than baseline significantly atp < 0.05 and p < 0.01, respectively.Then, we can obtain the fractional count of ahyperedge e,c(e) = ??(e)??
(TOP ) (11)Each n-gram (e.g., ?boy-as-head a?)
is assignedthe same fractional count of the hyperedge it be-longs to.We also tried training dependency languagemodel as in (Shen et al, 2008), which meansall hyperedges were on equal footing without re-garding probabilities.
However, the performanceis about 0.8 point lower in BLEU.
One possbilereason is that hyperedges with probabilities coulddistinguish high quality structures better.6 Experiments6.1 Results on the Chinese-English TaskWe used the FBIS corpus (6.9M Chinese words+ 8.9M English words) as our bilingual train-ing corpus.
We ran GIZA++ (Och and Ney,2000) to obtain word alignments.
We trained a4-gram language model on the Xinhua portionof GIGAWORD corpus using the SRI LanguageModeling Toolkit (Stolcke, 2002) with modi-fied Kneser-Ney smoothing (Kneser and Ney,1995).
We optimized feature weights using theminimum error rate training algorithm (Och andNey, 2002) on the NIST 2002 test set.
We evalu-ated the translation quality using case-insensitiveBLEU metric (Papineni et al, 2002) on the NIST2004/2005/2006 test sets.To obtain dependency trees and forests, weparsed the English sentences of the FBIS corpususing a shift-reduce dependency parser that en-ables beam search (Huang et al, 2009).
We onlyRules Size New Rulestree 7.2M -forest 7.6M 16.86%Table 2: Statistics of rules.
The last column showsthe ratio of rules extracted from non 1-best parsesbeing used in 1-best derivations.retained the best well-formed structure for eachnode when extracting string-to-tree rules from de-pendency forests (i.e., k = 1).
We trained two3-gram depLMs (one from trees and another fromforests) on English side of FBIS corpus plus 2Msentence pairs from other LDC corpus.After extracting rules and training depLMs, weran our replication of string-to-dependency sys-tem (Shen et al, 2008) to translate the develop-ment and test sets.Table 1 shows the BLEU scores on the testsets.
The first column ?Rule?
indicates wherethe string-to-dependency rules are learned from:1-best dependency trees or dependency forests.Similarly, the second column ?DepLM?
also dis-tinguish between the two sources for training de-pendency language models.
The baseline sys-tem used the rule table and dependency lan-guage model both learned from 1-best depen-dency trees.
We find that adding the rule table anddependency language models obtained from de-pendency forests improves string-to-dependencytranslation consistently and significantly, rangingfrom +1.3 to +1.4 BLEU points.
In addition, us-ing the rule table and dependency language modeltrained from forest only increases decoding timeinsignificantly.How many rules extracted from non 1-best1097Rule DepLM BLEUtree tree 22.31tree forest 22.73?forest tree 22.80?forest forest 23.12?
?Table 3: BLEU scores on the Korean-Chinese testset.parses are used by the decoder?
Table 2 shows thenumber of rules filtered on the test set.
We observethat the rule table size hardly increases.
One pos-sible reason is that we only keep the best depen-dency structure for each node.
The last row showsthat 16.86% of the rules used in 1-best deriva-tions are extracted from non 1-best parses in theforests, indicating that some useful rules cannotbe extracted from 1-best parses.6.2 Results on the Korean-Chinese TaskTo examine the efficacy of our approach on differ-ent language pairs, we carried out an experimenton Korean-Chinese translation.
The training cor-pus contains about 8.2M Korean words and 7.3MChinese words.
The Chinese sentences were usedto train a 5-gram language model as well as a 3-gram dependency language model.
Both the de-velopment and test sets consist of 1,006 sentenceswith single reference.
Table 3 shows the BLEUscores on the test set.
Again, our forest-based ap-proach achieves significant improvement over thebaseline (p < 0.01).6.3 Effect of K-bestWe investigated the effect of different k-beststructures for each node on translation quality(BLEU scores on the NIST 2005 set) and the ruletable size (filtered for the tuning and test sets), asshown in Figure 5.
To save time, we extractedrules just from the first 30K sentence pairs of theFBIS corpus.
We trained a language model anddepLMs on the English sentences.
We used 10different k: 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10.
Ob-viously, the higher the k is, the more rules areextracted.
When k=10, the number of rules usedon the tuning and test sets was 1,299,290 and theBLEU score was 20.88.
Generally, both the num-ber of rules and the BLEU score went up with20.420.520.620.720.820.921.021.121.221.321.421.521.621.721.80.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35BLEUscorerule table size(M)k=1,2,...,10Figure 5: Effect of k-best on rule table size andtranslation quality.20.420.520.620.720.820.921.021.121.221.321.421.521.621.721.80.98 1.00 1.02 1.04 1.06 1.08 1.10BLEUscorerule table size(M)t=1.0,0.9,...,0.1Figure 6: Effect of pruning threshold on rule tablesize and translation quality.the increase of k. However, this trend did nothold within the range [4,10].
We conjecture thatwhen retaining more dependency structures foreach node, low quality structures would be intro-duced, resulting in much rules of low quality.An interesting finding is that the rule table grewrapidly when k is in range [1,4], while graduallywithin the range [4,10].
One possible reason isthat there are limited different dependency struc-tures in the spans with a maximal length of 10,which the target side of rules cover.6.4 Effect of Pruning ThresholdFigure 6 shows the effect of pruning threshold ontranslation quality and the rule table size.
Weretained 10-best dependency structures for eachnode in dependency forests.
We used 10 different1098pruning thresholds: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8, 0.9 and 1.0.
Intuitively, the higher the prun-ing threshold is, the less rules are extracted.
Whent=0.1, the number of rules used on the tuning andtest sets was 1,081,841 and the BLEU score was20.68.Lots of rules are pruned when the pruningthreshold increases from 0.0 to 0.3 (around 20%).After pruning away these rules, we achieved 0.6point improvement in BLEU.
However, when wefiltered more rules, the BLEU score went down.Figures 5 and 6 show that using two parame-ters that have to be hand-tuned achieves a smallimprovement at the expense of an additional com-plexity.
To simplify the approach, we only keepthe best dependency structure for each node with-out pruning any rule.7 Related WorksWhile Mi and Huang (2008) and we both useforests for rule extraction, there remain two ma-jor differences.
Firstly, Mi and Huang (2008) usea packed forest, while we use a dependency forest.Packed forest is a natural weighted hypergraph(Klein and Manning, 2001; Huang and Chiang,2005), for each hyperedge treats the correspond-ing PCFG rule probability as its weight.
However,it is challenging to make dependency forest to be aweighted hypergraph because dependency parsersusually only output a score for each edge in a de-pendency tree rather than a hyperedge in a depen-dency forest.
Secondly, The GHKM algorithm(Galley et al, 2004), which is originally devel-oped for extracting tree-to-string rules from 1-besttrees, has been successfully extended to packedforests recently (Mi and Huang, 2008).
Unfor-tunately, the GHKM algorithm cannot be appliedto extracting string-to-dependency rules from de-pendency forests, because the GHKM algorithmrequires a complete subtree to exist in a rule whileneither fixed nor floating dependency structuresensure that all dependants of a head are included.8 Conclusion and Future WorkIn this paper, we have proposed to use dependencyforests instead of 1-best parses to extract string-to-dependency tree rules and train dependency lan-guage models.
Our experiments show that our ap-proach improves translation quality significantlyover a state-of-the-art string-to-dependency sys-tem on various language pairs and test sets.
Webelieve that dependency forest can also be used toimprove the dependency treelet system (Quirk etal., 2005) that takes 1-best trees as input.AcknowledgementThe authors were supported by SK Telecom C&IBusiness, and National Natural Science Founda-tion of China, Contracts 60736014 and 60903138.We thank the anonymous reviewers for their in-sightful comments.
We are also grateful to Wen-bin Jiang for his invaluable help in dependencyforest.ReferencesChiang, David.
2007.
Hierarchical phrase-basedtranslation.
Computational Linguistics, pages 201?228.Ding, Yuan and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsertion grammars.
In Proceedings of ACL.Dyer, Christopher, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice transla-tion.
In Proceedings of ACL.Galley, Michel, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of NAACL.Huang, Liang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of IWPT.Huang, Liang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proceedings of EMNLP.Klein, Dan and Christopher D. Manning.
2001.
Pars-ing and hypergraphs.
In Proceedings of IWPT.Kneser, R. and H. Ney.
1995.
Improved backing-offfor m-gram language modeling.
In Proceedings ofAcoustics, Speech, and Signal.Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu.
2009.Weighted alignment matrices for statistical machinetranslation.
In Proceedings of EMNLP.Mi, Haitao and Liang Huang.
2008.
Forest-basedtranslation rule extraction.
In Proceedings ofEMNLP.1099Och, Franz J. and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of ACL.Och, Franz J. and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisticalmachine translation.
In Proceedings of ACL.Och, Franz J. and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4):417?449.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof ACL.Quirk, Chris and Simon Corston-Oliver.
2006.
Theimpact of parsing quality on syntactically-informedstatistical machine translation.
In Proceedings ofEMNLP.Quirk, Chris, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: syntactically in-formed phrasal smt.
In Proceedings of ACL.Shen, Libin, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL.Stolcke, Andreas.
2002.
Srilm - an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP.Wang, Wei, Kevin Knight, and Daniel Marcu.
2007.Binarizing syntax trees to improve syntax-basedmachine translation accuracy.
In Proceedings ofEMNLP.1100
