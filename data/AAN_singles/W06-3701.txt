Usability Issues in an Interactive Speech-to-SpeechTranslation System for HealthcareMark Seligman Mike DillingerSpoken Translation, Inc.
Spoken Translation, Inc.Berkeley, CA, USA 94705 Berkeley, CA, USA 94705mark.seligman@spokentranslation.commike.dillinger@spokentranslation.comAbstractWe describe a highly interactive system forbidirectional, broad-coverage spoken lan-guage communication in the healthcare area.The paper briefly reviews the system's inter-active foundations, and then goes on to dis-cuss in greater depth issues of practicalusability.
We present our Translation Short-cuts facility, which minimizes the need forinteractive verification of sentences afterthey have been vetted once, considerablyspeeds throughput while maintaining accu-racy, and allows use by minimally literatepatients for whom any mode of text entrymight be difficult.
We also discuss facilitiesfor multimodal input, in which handwriting,touch screen, and keyboard interfaces are of-fered as alternatives to speech input whenappropriate.
In order to deal with issues re-lated to sheer physical awkwardness, webriefly mention facilities for hands-free oreyes-free operation of the system.
Finally,we point toward several directions for futureimprovement of the system.1 IntroductionIncreasing globalization and immigration have ledto growing demands on US institutions for health-care and government services in languages otherthan English.
These institutions are already over-whelmed:  the State of Minnesota, for example,had no Somali-speaking physicians for some12,000 Somali refugees and only six Hmong-speaking physicians to serve 50,000 Hmong resi-dents (Minnesota Interpreter Standards AdvisoryCommittee, 1998).
San Francisco General Hospi-tal, to cite another example, receives approxi-mately 3,500 requests for interpretation per month,or 42,000 per year for 35 different languages.Moreover, requests for medical interpretation ser-vices are distributed among all the wards and clin-ics, adding a logistical challenge to the problem ofa high and growing demand for interpretation ser-vices (Paras, et al, 2002).
Similar situations arefound throughout the United States.It is natural to hope that automatic real-timetranslation in general, and spoken language transla-tion (SLT) in particular, can help to meet this com-municative need.
From the viewpoint of researchand development, the high demand in healthcaremakes this area especially attractive for fieldingearly SLT systems and seeking early adopters.With this goal in view, several speech transla-tion systems have aimed at the healthcare area.
(See www.sehda.com, DARPA?s CAST program,www.phraselator.com, etc.)
However, these effortshave encountered several issues or limitations.First, they have been confined to narrow do-mains.
In general, SLT applications have been ableto achieve acceptable accuracy only by stayingwithin restricted topics, in which fixed phrasescould be used (e.g., www.phraselator.com), or inwhich grammars for automatic speech recognition(ASR) and machine translation (MT) could be op-timized.
For example, MedSLT (Bouillon et al2005) is limited to some 600 specific words persub-domain.
IBM?s MASTOR system, with 30,000words in each translation direction, has muchbroader coverage, but remains comparable in lexi-con size to commercial MT systems of the early1980s.Granted, restriction to narrow domains may of-ten be appropriate, given the large effort involvedin compiling extensive lexical resources and thetime required for deployment.
A tightly focusedapproach permits relatively quick development ofnew systems and provides a degree of flexibility toexperiment with different architectures and differ-ent languages.Our emphasis, however, is on breaking out ofnarrow domains.
We seek to maximize versatilityby providing exceptional capacity to move fromtopic to topic while maintaining adequate accu-racy.To provide a firm foundation for such versatil-ity, we ?give our systems a liberal arts education?by incorporating very broad-coverage ASR andMT technology.
Our MT lexicons, for example,contain roughly 300,000 words in each direction.But of course, as coverage increases, perplexityand the ASR and MT errors due to it increase inproportion, especially in the absence of tight inte-gration between these components.
To compen-sate, we provide a set of facilities that enable usersfrom both sides of the language barrier to interac-tively monitor and correct these errors.
Putting us-ers in the speech translation loop in this way doesin fact permit conversations to range widely(Seligman, 2000).
We believe that this highly in-teractive approach will prove applicable to thehealthcare area.We have described these interactive techniquesin (Dillinger and Seligman, 2004; Zong and Selig-man, forthcoming).
We will review them onlybriefly here, in Section 2.A second limitation of current speech transla-tion systems for healthcare is that bilingual (bidi-rectional) communication has been difficult toenable.
While speech-to-speech translation hassometimes proven practical from the English side,translation from the non-English side has beenmore difficult to achieve.
Partly, this limitationarises from human factors issues: while na?ve ob-servers might expect spoken input to be effortlessfor anyone who can talk, the reality is that usersmust learn to use most speech interfaces, and thatthis learning process can be difficult for users whoare less literate or less computer literate.
Further,many healthcare venues make speech input diffi-cult: they may be noisy, microphones may beawkward to situate or to pass from speaker tospeaker, and so on.Our group's approach to training- or venue-related difficulties for speech input is to provide anarray of alternative input modes.
In addition toproviding input through dictated speech, users ofour system can freely alternate among three otherinput modes, using handwriting, a touch screen,and standard bilingual keyboards.In this paper, we will focus on practical usabil-ity issues in the design of user interfaces for highlyinteractive approaches to SLT in healthcare appli-cations.
With respect to interactivity per se, we willdiscuss the following specific issues:?
In a highly interactive speech translationsystem, monitoring and correction of ASR and MTare vital for accuracy and confidence, but can betime consuming ?
in a field where time is always ata premium.?
Interactivity demands a minimum degreeof computer and print literacy, which some patientsmay lack.To address these issues, we have developed afacility called Translation Shortcuts?, to be ex-plained throughout Section 3.Section 4 will describe our approach to multi-modal input.
As background, however, Section 2will quickly review our approach to highly interac-tive ?
and thus uniquely broad-coverage ?
spokenlanguage translation.
Before concluding, we will inSection 5 point out planned future developments.2 Highly Interactive, Broad-coverage SLTWe now briefly summarize our group?s approachto highly interactive, broad-coverage SLT.The twin goals of accuracy and broad-coveragehave generally been in opposition: speech transla-tion systems have gained tolerable accuracy onlyby sharply restricting both the range of topics thatcan be discussed and the sets of vocabulary andstructures that can be used to discuss them.
Theessential problem is that both speech recognitionand translation technologies are still quite error-prone.
While the error rates may be tolerable wheneach technology is used separately, the errors com-bine and even compound when they are used to-gether.
The resulting translation output is generallybelow the threshold of usability ?
unless restrictionto a very narrow domain supplies sufficient con-straints to significantly lower the error rates of bothcomponents.As explained, our group?s approach has been toconcentrate on interactive monitoring and correc-tion of both technologies.First, users can monitor and correct the speaker-dependent speech recognition system to ensure thatthe text that will be passed to the machine transla-tion component is completely correct.
Voice com-mands (e.g.
Scratch That or Correct <incorrecttext>) can be used to repair speech recognitionerrors.
Thus, users of our SLT enrich the interfacebetween ASR and MT.Next, during the MT stage, users can monitor,and if necessary correct, one especially importantaspect of the translation ?
lexical disambiguation.Our system?s approach to lexical disambigua-tion is twofold: first, we supply a Back-Translation, or re-translation of the translation.Using this paraphrase of the initial input, even amonolingual user can make an initial judgmentconcerning the quality of the preliminary machinetranslation output.
(Other systems, e.g.
IBM?sMASTOR, have also employed re-translation.
Ourimplementations, however, exploit proprietarytechnologies to ensure that the lexical senses usedduring back translation accurately reflect thoseused in forward translation.
)In addition, if uncertainty remains about thecorrectness of a given word sense, we supply aproprietary set of Meaning Cues?
?
synonyms,definitions, etc.
?
which have been drawn fromvarious resources, collated in a database (calledSELECT?
), and aligned with the respective lexicaof the relevant MT systems.
With these cues asguides, the user can monitor the current, proposedmeaning and select (when necessary) a different,preferred meaning from among those available.Automatic updates of translation and back transla-tion then follow.
Future versions of the system willallow personal word-sense preferences thus speci-fied in the current session to be stored and reusedin future sessions, thus enabling a gradual tuningof word-sense preferences to individual needs.
Fa-cilities will also be provided for sharing such pref-erences across a working group.Given such interactive correction of both ASRand MT, wide-ranging, and even jocular, ex-changes become possible (Seligman, 2000).As we have said, such interactivity within aspeech translation system can enable increasedaccuracy and confidence, even for wide-rangingconversations.Accuracy of translation is, in many healthcaresettings, critical to patient safety.
When a doctor istaking a patient?s history or instructing the patientin a course of treatment, even small errors can haveclinically relevant effects.
Even so, at present,healthcare workers often examine patients and in-struct them in a course of treatment through ges-tures and sheer good will, with no translation at all,or use untrained human interpreters (friends, fam-ily, volunteers, or staff) in an error-prone attemptto solve the immediate problem (Flores, et al,2003).
As a result, low-English proficiency pa-tients are often less healthy and receive less effec-tive treatment than English speakers (Paras, et al,2002).
We hope to demonstrate that highly interac-tive real-time translation systems in general, andspeech translation systems in particular, can help tobridge the language gap in healthcare when humaninterpreters are not available.Accuracy in an automatic real-time translationsystem is necessary, but not sufficient.
If health-care workers have no means to independently as-sess the reliability of the translations obtained,practical use of the system will remain limited.Highly interactive speech translation systems canfoster the confidence on both sides of the conversa-tion, which is necessary to bring such systems intowide use.
In fact, in this respect at least, they maysometimes prove superior to human interpreters,who normally do not provide clients with themeans for judging translation accuracy.The value of enabling breadth of coverage, aswell as accuracy and confidence, should also beclear: for many purposes, the system must be ableto translate a wide range of topics outside of theimmediate healthcare domain ?
for example, whena patient tries to describe what was going on whenan accident occurred.
The ability to ask about in-terests, family matters, and other life concerns isvital for establishing rapport, managing expecta-tions and emotions, etc.3 Translation ShortcutsHaving summarized our approach to highly inter-active speech translation, we now turn to examina-tion of practical interface issues for this class ofSLT system.
This section concentrates on Transla-tion Shortcuts?.Shortcuts are designed to provide two main ad-vantages:First, re-verification of a given utterance is un-necessary.
That is, once the translation of an utter-ance has been verified interactively, it can be savedfor later reuse, simply by activating a Save asShortcut button on the translation verificationscreen.
The button gives access to a dialogue inwhich a convenient Shortcut Category for theShortcut can be selected or created.
At reuse time,no further verification will be required.
(In additionto such dynamically created Personal Shortcuts,any number of prepackaged Shared Shortcuts canbe included in the system.
)Second, access to stored Shortcuts is veryquick, with little or no need for text entry.
Severalfacilities contribute to meeting this design crite-rion.?
A Shortcut Search facility can retrieve aset of relevant Shortcuts given only keywords orthe first few characters or words of a string.
Thedesired Shortcut can then be executed with a singlegesture (mouse click or stylus tap) or voice com-mand.NOTE: If no Shortcut is found, the systemautomatically allows users access to the full powerof broad-coverage, interactive speech translation.Thus, a seamless transition is provided between theShortcuts facility and full, broad-coverage transla-tion.?
A Translation Shortcuts Browser is pro-vided, so that users can find needed Shortcuts bytraversing a tree of Shortcut categories.
Using thisinterface, users can execute Shortcuts even if theirability to input text is quite limited, e.g.
by tappingor clicking alone.Figure 1 shows the Shortcut Search and Short-cuts Browser facilities in use.
Points to notice:?
On the left, the Translation Shortcuts Panelhas slid into view and been pinned open.
It con-tains the Translation Shortcuts Browser, split intotwo main areas, Shortcuts Categories (above) andShortcuts List (below).?
The Categories section of the Panel showscurrent selection of the Conversation category,containing everyday expressions, and its Staff sub-category, containing expressions most likely to beused by healthcare staff members.
There is also aPatients subcategory, used for patient responses.Categories for Administrative topics and Pa-tient?s Current Condition are also visible; andnew ones can be freely created.?
Below the Categories section is the Short-cuts List section, containing a scrollable list of al-phabetized Shortcuts.
(Various other sortingcriteria will be available in the future, e.g.
sortingby frequency of use, recency, etc.)?
Double clicking on any visible Shortcut inthe List will execute it.
Clicking once will selectand highlight a Shortcut.
Typing Enter will exe-cute the currently highlighted Shortcut (here?Good morning?
), if any.?
It is possible to automatically relate op-tions for a patient's response to the previous staffmember?s utterance, e.g.
by automatically going tothe sibling Patient subcategory if the prompt wasgiven from the Staff subcategory.Because the Shortcuts Browser can be usedwithout text entry, simply by pointing and clicking,it enables responses by minimally literate users.
Inthe future, we plan to enable use even by com-pletely illiterate users, through two devices: wewill enable automatic pronunciation of Shortcutsand categories in the Shortcuts Browser via text-to-speech, so that these elements can in effect be readaloud to illiterate users; and we will augmentShared Shortcuts with pictorial symbols, as cluesto their meaning.A final point concerning the Shortcuts Browser:it can be operated entirely by voice commands,although this mode is more likely to be useful tostaff members than to patients.We turn our attention now to the Input Window,which does double duty for Shortcut Search andarbitrary text entry for full translation.
We willconsider the search facility first, as shown in Fig-ure 2.?
Shortcuts Search begins automatically assoon as text is entered by any means ?
voice,handwriting, touch screen, or standard keyboard ?into the Input Window.?
The Shortcuts Drop-down Menu appearsjust below the Input Window, as soon as there areresults to be shown.
The user has entered ?Good?and a space, so the search program has received itsfirst input word.
The drop-down menu shows theresults of a keyword-based search.?
Here, the results are sorted alphabetically.Various other sorting possibilities may be useful:by frequency of use, proportion of matched words,etc.?
The highest priority Shortcut according tothe specified sorting procedure can be highlightedfor instant execution.?
Other shortcuts will be highlighted differ-ently, and both kinds of highlighting are synchro-nized with that of the Shortcuts list in the ShortcutsPanel.?
Arrow keys or voice commands can beused to navigate the drop-down list.?
If the user goes on to enter the exact text ofany Shortcut, e.g.
?Good morning,?
a message willshow that this is in fact a Shortcut, so that verifica-tion will not be necessary.
However, final text notmatching a Shortcut, e.g.
?Good job,?
will bepassed to the routines for full translation with veri-fication.4 Multimodal inputAs mentioned, an unavoidable issue for speechtranslation systems in healthcare settings is thatspeech input is not appropriate for every situation.Current speech-recognition systems are unfa-miliar for many users.
Our system attempts toovercome this training issue to some extent by in-corporating standard commercial-grade dictationsystems for broad-coverage and ergonomic speechrecognition.
These products already have estab-lished user bases in the healthcare community.Even so, some training may be required: optionalgeneric Guest profiles are supplied by our systemfor male and female voices in both languages; butoptional voice enrollment, requiring five minutesor so, is helpful to achieve best results.
Such train-ing time is practical for healthcare staff, but will berealistic for patients only when they are repeat visi-tors, hospital-stay patients, etc.As mentioned, other practical usability issuesfor the use of speech input in healthcare settingsinclude problems of ambient noise (e.g.
in emer-gency rooms or ambulances) and problems of mi-crophone and computer arrangement (e.g.
toaccommodate not only desktops but counters orservice windows which may form a barrier be-tween staff and patient).To deal with these and other usability issues, wehave found it necessary to provide a range of inputmodes: in addition to dictated speech, we enablehandwritten input, the use of touch screen key-boards for text input, and the use of standard key-boards.
All of these input modes must becompletely bilingual, and language switching mustbe arranged automatically when there is a changeof active participant.
Further, it must be possible tochange input modes seamlessly within a given ut-terance: for example, users must be able to dictatethe input if they wish, but then be able to makecorrections using handwriting or one of the re-maining two modes.
Figure 3 shows such seamlessbilingual operation: the user has dictated the sen-tence ?Tengo n?useas?
in Spanish, but there was aspeech-recognition error, which is being correctedby handwriting.Of course, even this flexible range of input op-tions does not solve all problems.
As mentioned,illiterate patients pose special problems.
Again,na?ve users tend to suppose that speech is the idealinput mode for illiterates.
Unfortunately, however,the careful and relatively concise style of speechthat is required for automatic recognition is oftendifficult to elicit, so that recognition accuracy re-mains low; and the ability to read and correct theresults is obviously absent.
Just as obviously, theremaining three text input modes will be equallyineffectual for illiterates.As explained, our current approach to low liter-acy is to supply Translation Shortcuts for the mini-mally literate, and ?
in the future ?
to augmentShortcuts with text-to-speech and iconic pictures.Staff members will usually be at least mini-mally literate, but they present their own usabilityissues.Their typing skills may be low or absent.
Han-dling the computer and/or microphone may beawkward in many situations, e.g.
when examininga patient or taking notes.
(Speech translation sys-tems are expected to function in a wide range ofphysical settings: in admissions or financial aidoffices, at massage tables for physical therapy withpatients lying face down, in personal living roomsfor home therapy or interviews, and in many otherlocations.
)To help deal with the awkwardness issues, oursystem provides voice commands, which enablehands-free operation.
Both full interactive transla-tion and the Translation Shortcut facility (usingeither the Browser or Search elements) can be runhands-free.
To a limited degree, the system can beused eyes-free as well: text-to-speech can be usedto pronounce the back-translation so that prelimi-nary judgments of translation quality can be madewithout looking at the computer screen.5 Future developmentsWe have already mentioned plans to augment theTranslation Shortcuts facility with text-to-speechand iconic pictures, thus moving closer to a systemsuitable for communication with completely illiter-ate or incapacitated patients.Additional future directions follow.?
Server-based architectures:  We plan tomove toward completely or partially server-basedarrangements, in which only a very thin clientsoftware application ?
for example, a web interface?
will run on the client device.
Such architectureswill permit delivery of our system on smart phonesin the Blackberry or Treo class.
Delivery on hand-helds will considerably diminish the issues ofphysical awkwardness discussed above, and any-time/anywhere/any-device access to the systemwill considerably enlarge its range of uses.?
Pooling Translation Shortcuts:  As ex-plained above, the current system now supportsboth Personal (do-it-yourself) and Shared (pre-packaged) Translation Shortcuts.
As yet, however,there are no facilities to facilitate pooling of Per-sonal Shortcuts among users, e.g.
those in a work-ing group.
In the future, we will add facilities forexporting and importing shortcuts.?
Translation memory: Translation Short-cuts can be seen as a variant of Translation Mem-ory, a facility that remembers past successfultranslations so as to circumvent error-prone re-processing.
However, at present, we save Shortcutsonly when explicitly ordered.
If all other successfultranslations were saved, there would soon be fartoo many to navigate effectively in the TranslationShortcuts Browser.
In the future, however, wecould in fact record these translations in the back-ground, so that there would be no need to re-verifynew input that matched against them.
Messageswould advise the user that verification was beingbypassed in case of a match.?
Additional languages: The full SLT sys-tem described here is presently operational only forbidirectional translation between English andSpanish.
We expect to expand the system to Man-darin Chinese next.
Limited working prototypesnow exist for Japanese and German, though weexpect these languages to be most useful in appli-cation fields other than healthcare.?
Testing: Systematic usability testing of thefull system is under way.
We look forward to pre-senting the results at a future workshop.6 ConclusionWe have described a highly interactive system forbidirectional, broad-coverage spoken languagecommunication in the healthcare area.
The paperhas briefly reviewed the system's interactive foun-dations, and then gone on to discuss in greaterdepth issues of practical usability.We have presented our Translation Shortcutsfacility, which minimizes the need for interactiveverification of sentences after they have been vet-ted once, considerably speeds throughput whilemaintaining accuracy, and allows use by minimallyliterate patients for whom any mode of text entrymight be difficult.We have also discussed facilities for multimo-dal input, in which handwriting, touch screen, andkeyboard interfaces are offered as alternatives tospeech input when appropriate.
In order to dealwith issues related to sheer physical awkwardness,we have briefly mentioned facilities for hands-freeor eyes-free operation of the system.Finally, we have pointed toward several direc-tions for future improvement of the system.ReferencesPierrette Bouillon, Manny Rayner, et al 2005.
A Ge-neric Multi-Lingual Open Source Platform for Lim-ited-Domain Medical Speech Translation.
Presentedat EAMT 2005, Budapest, Hungary.Mike Dillinger and Mark Seligman.
2004.
A highlyinteractive speech-to-speech translation system.
Pro-ceedings of the VI Conference of the Association ofMachine Translation in the Americas.
E. Strouds-burg, PA:  American Association for Machine Trans-lation.Glenn Flores, M. Laws, S. Mays, et al 2003.
Errors inmedical interpretation and their potential clinicalconsequences in pediatric encounters.
Pediatrics,111: 6-14.Minnesota Interpreter Standards Advisory Committee.1998.
Bridging the Language Gap: How to meet theneed for interpreters in Minnesota.
Available at:http://www.cce.umn.edu/creditcourses/pti/downloads.html.Melinda Paras, O. Leyva, T. Berthold, and R. Otake.2002.
Videoconferencing Medical Interpretation:The results of clinical trials.
Oakland, CA: HeathAccess Foundation.PHRASELATOR (2006).
http://www.phraselator.com.As of April 3, 2006.S-MINDS (2006).
http://www.sehda.com/solutions.htm.As of April 3, 2006.Mark Seligman.
2000.
Nine Issues in Speech Transla-tion.
Machine Translation, 15, 149-185.Chengqing Zong and Mark Seligman.
Forthcoming.Toward Practical Spoken Language Translation.
Ma-chine Translation.Figure 1: The Input Screen, showing the Translation Shortcuts Browser and Search facilities.Figure 2: The Input Screen, showing automatic keyword search of the Translation Shortcuts.Figure 3: The Input Screen, showing correction of dictation with handwritten input.
