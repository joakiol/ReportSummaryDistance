Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 867?877,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsWikiKreator: Improving Wikipedia Stubs AutomaticallySiddhartha BanerjeeThe Pennsylvania State UniversityInformation Sciences and TechnologyUniversity Park, PA, USAsub253@ist.psu.eduPrasenjit MitraQatar Computing Research InstituteHamad Bin Khalifa UniversityDoha, Qatarpmitra@qf.org.qaAbstractStubs on Wikipedia often lack comprehen-sive information.
The huge cost of edit-ing Wikipedia and the presence of only alimited number of active contributors curbthe consistent growth of Wikipedia.
In thiswork, we present WikiKreator, a systemthat is capable of generating content au-tomatically to improve existing stubs onWikipedia.
The system has two compo-nents.
First, a text classifier built usingtopic distribution vectors is used to as-sign content from the web to various sec-tions on a Wikipedia article.
Second, wepropose a novel abstractive summariza-tion technique based on an optimizationframework that generates section-specificsummaries for Wikipedia stubs.
Experi-ments show that WikiKreator is capable ofgenerating well-formed informative con-tent.
Further, automatically generated con-tent from our system have been appendedto Wikipedia stubs and the content hasbeen retained successfully proving the ef-fectiveness of our approach.1 IntroductionWikipedia provides comprehensive informationon various topics.
However, a significant percent-age of the articles are stubs1that require exten-sive effort in terms of adding and editing contentto transform them into complete articles.
Ideally,we would like to create an automatic Wikipediacontent generator, which can generate a compre-hensive overview on any topic using available in-formation from the web and append the gener-ated content to the stubs.
Addition of automati-cally generated content can provide a useful start-1https://en.wikipedia.org/wiki/Wikipedia:Stubing point for contributors on Wikipedia, which canbe improved upon later.Several approaches to automatically generateWikipedia articles have been explored (Sauperand Barzilay, 2009; Banerjee et al, 2014; Yaoet al, 2011).
To the best of our knowledge, allthe above mentioned methods identify informa-tion sources from the web using keywords anddirectly use the most relevant excerpts in the fi-nal article.
Information from the web cannotbe directly copied into Wikipedia due to copy-right violation issues (Banerjee et al, 2014).Further, keyword search does not always sat-isfy information requirements (Baeza-Yates et al,1999).
To address the above-mentioned issues,we present WikiKreator ?
a system that can au-tomatically generate content for Wikipedia stubs.First, WikiKreator does not operate using keywordsearch.
Instead, we use a classifier trained usingtopic distribution features to identify relevant con-tent for the stub.
Topic-distribution features aremore effective than keyword search as they canidentify relevant content based on word distribu-tions (Song et al, 2010).
Second, we propose anovel abstractive summarization (Dalal and Malik,2013) technique to summarize content from mul-tiple snippets of relevant information.2Figure 1 shows a stub that we attempt to im-prove using WikiKreator.
Generally, in stubs, onlythe introductory content is available; other sec-tions (s1, ..., sr) are absent.
The stub also belongsto several categories (C1,C2, etc.
in Figure) onWikipedia.
In this work, we address the followingresearch question: Given the introductory content,the title of the stub and information on the cate-gories - how can we transform the stub into a com-2An example of our system?s output can be foundhere ?
https://en.wikipedia.org/wiki/2014_Enterovirus_D68_outbreak ?
content was added on5th Jan, 2015.
The sections on Epidemiology, Causes andPrevention have been added using content automatically gen-erated by our method.867   ???
?  						 					 	 !"								Figure 1: Overview of our word-graph based generation (left) to populate Wikipedia template (right)prehensive Wikipedia article?Our proposed approach consists of two stages.First, a text classifier assigns content retrievedfrom the web into specific sections of theWikipedia article.
We train the classifier usinga set of articles within the same category.
Cur-rently, we limit the system to learn and assigncontent into the 10 most frequent sections in anygiven category.
The training set includes con-tent from the most frequent sections as instancesand their corresponding section titles as the classlabels.
We extract topic distribution vectors us-ing Latent Dirichlet Allocation (LDA) (Blei et al,2003) and use the features to train a Random For-est (RF) Classifier (Liaw and Wiener, 2002).
Togather web content relevant to the stub, we for-mulate queries and retrieve top 20 search results(pages) from Google.
We use boilerplate detec-tion (Kohlsch?utter et al, 2010) to retain the im-portant excerpts (text elements) from the pages.The RF classifier classifies the excerpts into one ofthe most frequent classes (section titles).
Second,we develop a novel Integer Linear Programming(ILP) based abstractive summarization techniqueto generate text from the classified content.
Previ-ous work only included the most informative ex-cerpt in the article (Sauper and Barzilay, 2009); incontrast, our abstractive summarization approachminimizes loss of information that should ideallybe in an Wikipedia article by fusing content fromseveral sentences.
As shown in Figure 1, we con-struct a word-graph (Filippova, 2010) using all thesentences (WG1) assigned to a specific class (Epi-demiology) by the classifier.
Multiple paths (sen-tences) between the start and end nodes in thegraph are generated (WG2).
We represent the gen-erated paths as variables in the ILP problem.
Thecoefficients of each variable in the objective func-tion of the ILP problem is obtained by combin-ing the information score and the linguistic qualityscore of the path.
We introduce several constraintsinto our ILP model.
We limit the summary foreach section to a maximum of 5 sentences.
Fur-ther, we avoid redundant sentences in the summarythat carry similar information.
The solution to theoptimization problem decides the paths that are se-lected in the final section summary.
For example,in Figure 1, the final paths determined by the ILPsolution, ?
1 and 2 in WG2, are assigned to a sec-tion (sr), where (sr) is the section title Epidemiol-ogy.To the best of our knowledge, this work isthe first to address the issue of generating con-tent automatically to transform Wikipedia stubsinto comprehensive articles.
Further, we addressthe issue of abstractive text summarization forWikipedia content generation.
We evaluate ourapproach by generating articles in three differ-ent categories: Diseases and Disorders3, Amer-ican Mathematicians4and Software companiesof the United States5.
Our LDA-based classi-3https://en.wikipedia.org/wiki/Category:Diseases_and_disorders4https://en.wikipedia.org/wiki/Category:American_mathematicians5https://en.wikipedia.org/wiki/Category:Software_companies_of_the_United_States868fier outperforms a TFIDF-based classifier in allthe categories.
We use ROUGE (Lin, 2004) tocompare content generated by WikiKreator andthe corresponding Wikipedia articles.
The re-sults of our evaluation confirm the benefits of us-ing abstractive summarization for content genera-tion over approaches that do not use summariza-tion.
WikiKreator outperforms other comparableapproaches significantly in terms of content selec-tion.
On ROUGE-1 scores, WikiKreator outper-forms the perceptron-based baseline (Sauper andBarzilay, 2009) by?20%.
We also analyze re-viewer reactions, by appending content into sev-eral stubs on Wikipedia, most of which (?77%)have been retained by reviewers.2 Related WorkWikipedia has been used to compute semantic re-latedness (Gabrilovich and Markovitch, 2007), in-dex topics (Medelyan et al, 2008), etc.
How-ever, the problem of enhancing the content ofa Wikipedia article has not been addressed ad-equately.
Learning structures of templates fromthe Wikipedia articles have been attempted in thepast (Sauper and Barzilay, 2009; Yao et al, 2011).Both these efforts use queries to extract excerptsfrom the web and the excerpts ranked as the mostrelevant are added into the article.
However, as al-ready pointed out, current standards of Wikipediarequires rewriting of web content to avoid copy-right violation issues.To address the issue of copyright violation,multi-document abstractive summarization is re-quired.
Various abstractive approaches have beenproposed till date (Nenkova et al, 2011).
How-ever, these methods suffer from severe deficien-cies.
Template-based summarization methodswork well, but, it assumes prior domain knowl-edge (Li et al, 2013).
Writing style across ar-ticles vary widely; hence learning templates au-tomatically is difficult.
In addition, such tech-niques require handcrafted rules for sentence re-alization (Gerani et al, 2014).
Alternatively, wecan use text-to-text generation (T2T) (Ganitke-vitch et al, 2011) techniques.
WikiKreator con-structs a word-graph structure similar to (Filip-pova, 2010) using all the sentences that are as-signed to a particular section by a text classifier.Multiple paths (sentences) from the graph are gen-erated.
WikiKreator selects few sentences fromthis set of paths using an optimization problemformulation that jointly maximizes the informa-	 			 		 	!
"#"	 $"%		 				!%%&	Figure 2: WikiKreator System Architecture: Con-tent Retrieval and Content Summarizationtiveness and readability of section-specific snip-pets and generates output that is informative, well-formed and readable.3 Proposed ApproachFigure 2 shows the system architecture ofWikiKreator.
We are required to generate contentto populate sections of the stubs (S1, S2, etc.)
thatbelong to category C1.
Categories on Wikipediagroup together pages on similar subjects.
Hence,categories characterize Wikipedia articles surpris-ingly well (Zesch and Gurevych, 2007).
Naturally,we leverage knowledge existing in the categoriesto build our text classifier.
To learn category spe-cific templates, the system should learn from ar-ticles contained within the same or similar cate-gories.
WikiKreator learns category-specific tem-plates using all the articles that can be reached us-ing a top-down approach from the particular cate-gory.
For example, in addition to C1, WikiKreatoralso learns templates from articles in C2and C3(the subcategories of C1).
As shown in the Fig-ure 2, we deploy a two stage process to generatecontent for a stub:[i] Content Retrieval and[ii] Content Summarization.In the first stage, our focus is to retrieve contentthat is relevant to the stub, say, S1that belongsto C1.
We extract all the articles that belong to C1and the subcategories, namely, C2and C3.
A train-ing set is created with the contents in the sectionsof the articles as instances and the section titles asthe corresponding classes.
Topic distribution vec-tors for each section content are generated usingLDA (Blei et al, 2003).
We train a Random Forest869(RF) classifier using the topic distribution vectors.As mentioned earlier, only the top 10 most fre-quent sections are considered for the multi-classclassification task.
We retrieve relevant excerptsfrom the web by formulating queries.
The topicmodel infers the topic distribution features of eachexcerpt and the RF classifier predicts the section(s1, s2, etc.)
of the excerpt.
All web automationtasks are performed using HTMLUnit6.
In the sec-ond stage, our ILP based summarization approachsynthesizes information from multiple excerpts as-signed to a section and presents the most informa-tive and linguistically well-formed summary as thecorresponding content for each section.
A word-graph is constructed that generates several sen-tences; only a few of the sentences are retainedbased on the ILP solution.
The predicted sectionis entered in the stub article along with the finalsentences selected by the ILP solution as the cor-responding section-specific content on Wikipedia.3.1 Content RetrievalArticle Extraction: Wikipedia provides an API7to download articles in the XML format.
Given acategory, the API is capable of extracting all thearticles under it.
We recursively extract articlesby identifying all the categories in the hierarchythat can be reached by the crawler using top-downtraversal.
We use a simple python script8to ex-tract the section titles and the corresponding textcontent from the XML dump.Classification model: WikiKreator uses LatentDirichlet Allocation (LDA) to represent each doc-ument as a vector of topic distributions.
Eachtopic is further represented as a vector of proba-bilities of word distributions.
Our intuition is thatthe topic distribution vectors of the same sectionsacross different articles would be similar.
Our ob-jective is to learn these topic representations, suchthat we can accurately classify any web excerpt byinferring the topics in the text.
Say C, a categoryon Wikipedia, has k Wikipedia articles (W ).
(C) = {W1, W2, W3, W4, ..., Wk}Each article Wjhas several sections denoted assjicjiwhere sjiand cjirefer to the section title andcontent of the ith section in the jth article, respec-tively.
We concentrate on the 10 most frequent6http://htmlunit.sourceforge.net/7https://en.wikipedia.org/wiki/Special:Export8http://medialab.di.unipi.it/wiki/Wikipedia_Extractorsections in any category.
Training using contentfrom sections that are not frequent might result insub-optimal classification models.
In our experi-ments, each frequent section had enough instancesto optimally train a classifier.
Let us denote the 10most frequent sections in any category as S. Ifany sjifrom Wjexists in S, the content (cji) isincluded in the training set alng with the sectiontitle (sji) as the corresponding class label.
Thesesteps are repeated for all the articles in the cate-gory.
Each instance is then represented as:cji= {pji(t1), pji(t2), pji(t3), .
.
.
, pji(tm)}where m is the number of topics.
sjiis the cor-responding label for this training instance.
Theset of topics are t1, t2, t3,.
.
., tmwhile pji(tm)refers to the probability of topic m of content cji.Contents from the most frequent sections are eachconsidered as a document and LDA is applied togenerate document-topic distributions.
We exper-iment with several values of m and use the valuethat generates the best classification model in eachcategory.
The topic vectors and the correspond-ing labels are used to train a Random Forest (RF)classifier.
As the classes might be unbalanced, weapply resampling on the training set.Predicting sections: In this step, we search theweb for relevant content on the stub and assignthem to their respective sections.
We formulatesearch queries to retrieve web pages using a searchengine.
We extract multiple excerpts from thepages and then the RF classifier predicts the class(section label) for each excerpt.
(i) Query Generation: To search the web, weformulate queries by combining the stub title andkeyphrases extracted from the first sentence of theintroductory content of the stub.
The first sen-tence generally contains the most important key-words that represent the article.
Focused queriesincreases relevance of extraction as well as helpsin disambiguation of content.
We use the topiaterm extractor (Chatti et al, 2014) to extractkeyphrases.
For example, the query generated fora stub on Hereditary hyperbilirubinemia is Hered-itary hyperbilirubinemia bilirubin metabolic dis-order where bilirubin metabolic disorder are thekeyphrases generated from the first sentence of thestub from Wikipedia.
The query is used to identifythe top 20 URLs (search results) from Google9.
(ii) Boilerplate removal: Web content from thesearch results obtained in the previous step re-9http://www.google.com870quires cleaning to retain only the relevant informa-tion.
Removal of irrelevant content is done usingboilerplate detection (Kohlsch?utter et al, 2010).The web pages contain several excerpts (text el-ements) in between the HTML tags.
Only the ex-cerpts that are classified as relevant by the boiler-plate detection technique are retained.
(iii) Classification and assignment of excerpts:The LDA model generated earlier infers topic dis-tribution of each excerpt based on word distribu-tions.
The RF classifier predicts the class (sectiontitle) for each excerpt based on the topic distribu-tion.
However, predictions that do not have a highlevel of confidence might lead to excerpts beingappended to inappropriate sections.
Therefore, weset the minimum confidence level at 0.5.
If theprediction confidence of the RF classifier for a par-ticular excerpt is above the minimum confidencelevel, the excerpt is assigned to the class; other-wise, the excerpt is discarded.In the next step, we apply summarization on theexcerpts assigned to each section.3.2 Content SummarizationTo summarize content for Wikipedia effectively,we formulate an ILP problem to generate abstrac-tive summaries for each section with the objectiveof maximizing linguistic quality and informationcontent.Word-graph: A word-graph is constructed usingall the sentences included in the excerpts assignedto a particular section.
We used the same tech-nique to construct the word-graph as (Filippova,2010) where the nodes represent the words (alongwith parts-of-speech (POS)) and directed edgesbetween the nodes are added if the words are adja-cent in the input sentences.
Each sentence is con-nected to dummy start and end nodes to mark thebeginning and ending of the sentences.
The sen-tences from the excerpts are added to the graphin an iterative fashion.
Once the first sentenceis added, words from the following sentences aremapped onto a node in the graph provided thatthey have the exact same word form and the samePOS tag.
Inclusion of POS information preventsungrammatical mappings.
The words are added tothe graph in the following order:?
Content words are added for which there areno candidates in the existing graph;?
Content words for which multiple mappingsare possible or such words that occur morethan once in the sentence;?
Stopwords.If multiple mappings are possible, the context ofthe word is checked using word overlaps to the leftand right within a window of two words.
Even-tually, the word is mapped to that node that hasthe highest context.
We also changed Filippova?smethod by adding punctuations as nodes to thegraph.
Figure 1 shows a simple example of theword-graph generation technique.
We do not showPOS and punctuations in the figure for the sake ofclarity.
The Figure also shows that several pos-sible paths (sentences) exist between the dummystart and end nodes in the graph.
Ideally, excerptsfor any section would contain multiple commonwords as they belong to the same topic and havebeen assigned the same section.
The presence ofcommon words ensure that new sentences can begenerated from the graph by fusing original set ofsentences in the graph.
Figure 1 shows an illus-tration of our approach where the set of sentencesassigned to a particular section (WG1) are used tocreate the word-graph.
The word-graph generatesseveral possible paths between the dummy nodes;we show only three such paths (WG2).
To obtainabstractive summaries, we remove generated pathsfrom the graph that are same or very similar to anyof the original sentences.
If the cosine similarityof a generated path to any of the original sentencesis greater than 0.8, we do not retain the path.
Wecompute cosine similarity after applying stopwordremoval.
However, we do not apply stemming asour graph construction is based on words existingin the same form in multiple sentences.
Similar toFilippova?s work, we set the minimum path length(in words) to eight to avoid incomplete sentences.Paths without verbs are discarded.
The final set ofgenerated paths after discarding the ineligible onesare used in the next step of summary generation.3.2.1 ILP based Path SelectionOur goal is to select paths that maximize the in-formativeness and linguistic quality of the gener-ated summaries.
To select the best multiple pos-sible sentences, we apply an overgenerate and se-lect (Walker et al, 2001) strategy.
We formulatean optimization problem that ?selects?
a few ofthe many generated paths in between the dummynodes from the word-graph.
Let pidenote eachpath obtained from the word-graph.
We introducethree different factors to judge the relevance of871a path ?
Local informativeness (Iloc(pi)), Globalinformativeness (Iglob(pi)) and Linguistic quality(LQ(pi)).
Any sentence path should be relevantto the central topic of the article; this relevanceis tackled using Iglob(pi).
Iloc(pi) models theimportance of a sentence among several possiblesentences that are generated from the word-graph.Linguistic quality (LQ(pi)) is computed using atrigram language model (Song and Croft, 1999)that assigns a logarithmic score of probabilities ofoccurrences of three word sequences in the sen-tences.Local Informativeness: In principle, we can useany existing method that computes sentence im-portance to account for Local Informativeness.
Inour model, we use TextRank scores (Mihalcea andTarau, 2004) to generate an importance value ofeach path.
TextRank creates a graph of words fromthe sentences.
The score of each node in the graphis calculated as shown in Equation (1):S(Vi) = (1?
d) + d?
?Vj?adj(Vi)wji?Vk?adj(Vi)wjkS(Vi)(1)where Virepresents the words and adj(Vi) denotesthe adjacent nodes of Vi.
Setting d to 0.80 in ourexperiments provided the best content selection re-sults.
The computation convergences to return fi-nal word importance scores.
The informativenessscore of a path Iloc(pi) is obtained by adding theimportance scores of the individual words in thepath.Global Informativeness: To compute globalinformativeness, we compute the relevance of asentence with respect to the query to assign higherweights to sentences that explicitly mention themain title or mention certain keywords that arerelevant to the article.
We compute the cosinesimilarity using TFIDF features between eachsentence and the original query that was formu-lated during the web search stage.
We defineglobal informativeness as follows:Iglob(pi) = CosineSimilarity(Q, pi) (2)where Q denotes the formulated query.Linguistic Quality: In order to compute Linguis-tic quality, we use a language model that assignsprobabilities to sequence of words to compute lin-guistic quality.
Suppose a path contains a se-quence of q words {w1, w2, ..., wq}.
The scoreLQ(pi) assigned to each path is defined as fol-lows:LQ(pi) =11?LL(w1,w2,...,wq), (3)where LL(w1, w2, ..., wq) is defined as:LL(w1, .
.
.
, wq) =1L?
log2?qt=3P (wt|wt?1wt?2).
(4)As can be seen from Equation (4), we com-bine the conditional probability of different setsof 3-grams (trigrams) in the sentence and aver-aged the value by L ?
the number of conditionalprobabilities computed.
The LL(w1, w2, .
.
.
, wq)scores are negative; with higher magnitude imply-ing lower importance.
Therefore, in Equation (3),we take the reciprocal of the logarithmic valuewith smoothing to compute LQ(pi).
In our exper-iments, we used a 3-gram model10that is trainedon the English Gigaword corpus.
Trigram modelshave been successfully used in several text-to-textgeneration tasks (Clarke and Lapata, 2006; Filip-pova and Strube, 2008) earlier.ILP Formulation: To select the best paths, wecombine all the above mentioned factors Iloc(pi),Iglob(pi) and linguistic quality LQ(pi) in an opti-mization framework.
We maximize the followingobjective function:F (p1, .
.
.
, pK) =?Ki=11T (pi)?
Iloc(pi) ?
Iglob(pi) ?
LQ(pi) ?
pi(5)where K represents the total number of generatedpaths.
Each pirepresents a binary variable, thatcan be either 0 or 1, depending on whether the pathis selected in the final summary or not.
In addition,T (pi) ?
the number of tokens in a path, is includedin the objective function.
The term1T (pi)normal-izes the Textrank scores by the length of the sen-tences.
First, we ensure that a maximum of Smaxsentences are selected in the summary using Equa-tion (6).K?i=1pi?
Smax(6)In our experiments, we set Smaxto 5 to generateshort concise summaries in each section.
Using alength constraint enables us to only populate thesections using the most informative content.
Weintroduce Equation (7) to prevent similar informa-tion (cosine similarity?
0.5) from being conveyed10The model is available here: http://www.keithv.com/software/giga/.
We used the VP 20K vocab ver-sion.872Category Most Frequent SectionsAmerican Mathematicians Awards, Awards and honors, Biography, Books, Career, Education, Life, Publications, Selected publications, WorkDiseases and Disorders Causes, Diagnosis, Early life, Epidemiology, History, Pathophysiology, Prognosis, Signs and symptoms, Symptoms, TreatmentUS Software companies Awards, Criticism, Features, Games, History, Overview, Products, Reception, Services, TechnologyTable 1: Data characteristics of three domains on WikipediaCategory #Articles #InstancesAmerican Mathematicians ?
2100 1493Diseases and Disorders ?
7000 9098US Software companies ?
3600 2478Table 2: Dataset used for classificationby different sentences.
This constraint reduces re-dundancy.
If two sentences have a high degree ofsimilarity, only one out of the two can be selectedin the summary.
?i, i??
[1,K], i 6= i?,pi+ pi??
1 if sim(pi, pi?)
?
0.5.
(7)The ILP problem is solved using the Gurobi op-timizer (2015).
The solution to the problem de-cides the paths that should be included in the finalsummary.
We populate the sections on Wikipediausing the final summaries generated for each sec-tion along with the section title.
All the refer-ences that have been used to generate the sen-tences are appended along with the content gen-erated on Wikipedia.4 Experimental ResultsTo evaluate the effectiveness of our proposed tech-nique, we conduct several experiments.
First, weevaluate our content generation approach by gen-erating content for comprehensive articles that al-ready exist on Wikipedia.
Second, we analyze re-viewer reactions on our system generated articlesby adding content to several stubs on Wikipedia.Our experiments were designed to answer the fol-lowing questions:(i)What are the optimal number of topic distribu-tion features for each category?
What are the clas-sification accuracies in each domain?
(ii)To what extent can our technique generate thecontent for articles automatically?
(iii)What are the general reviewer reactions onWikipedia and what percentage of automaticallygenerated content on Wikipedia is retained?Dataset Construction: As mentioned earlierin Section 3.1, we crawl Wikipedia articles bytraversing the category graph.
Articles that containat least three sections were included in the trainingset; other articles having lesser number of sectionsFigure 3: Performance of Classifier in the threecategories based on the number of topics.are generally labeled as stubs and hence not usedfor training.
Table 1 shows the most frequent sec-tions in each category.
Further, Table 2 shows thetotal number of articles retrieved from Wikipediain each category.
The total number of instances arealso shown.
The number of instances denotes thetotal number of the most frequent sections in eachcategory.
As can be seen from the table, the num-ber of instances is higher than the number of arti-cles only in case of the category on diseases.
Thisimplies that there are generally more common sec-tions in the diseases category than the other cate-gories.In each category, the content from only the mostfrequent sections were used to generate a topicmodel.
The topic model is further used to in-fer topic distribution vectors from the training in-stances.
We used the MALLET toolkit (McCal-lum, 2002) for generating topic distribution vec-tors and the WEKA package (Hall et al, 2009) forthe classification tasks.Optimal number of topics: The LDA model re-quires a pre-defined number of topics.
We exper-iment with several values of the number of top-ics ranging from 10 to 100.
The topic distributionfeatures of the content of the instances are usedto train a Random Forest Classifier with the cor-responding section titles as the class labels.
Ascan be seen in the Figure 3, the classification per-formance varies across domains as well as on thenumber of topics.
The optimal number of top-ics based on the dataset are marked in blue cir-873Category LDA-RF SVM-WVAmerican Mathematicians 0.778 0.478Diseases and Disorders 0.886 0.801US Software companies 0.880 0.537Table 3: Classification: Weighted F-Scorescles (40, 50 and 20 topics for Diseases, SoftwareCompanies in US and American mathematicians,respectively) in the Figure.
We classify web ex-cerpts using the best performing classifiers trainedusing the optimal number of topic features in eachcategory.Classification performance: We use 10-foldcross validation to evaluate the accuracy of ourclassifier.
According to the F-Scores, our classifier(LDA-RF) performs similarly in the categories onDiseases and US Software companies.
However,the accuracy is lower in the American Mathemati-cians category.
We also experimented with a base-line classifier, that is trained on TFIDF features(upto trigrams).
A Support vector machine (Cortesand Vapnik, 1995) classifier obtained the best per-formance using the TFIDF features.
The base-line system is referred to as SVM-WV.
We exper-imented with several other combinations of classi-fiers; however, we show only the best performingsystems using the LDA and TFIDF features.
Ascan be seen from the Table 3, our classifier (LDA-RF) outperforms SVM-WV significantly in all thedomains.
SVM-WV performs better in the cate-gory on diseases than the other two categories andthe performance is comparable to (LDA-RF).
Thediseases category has more uniformity in terms ofthe section titles, hence specific words or phrasescharacterize the sections well.
In contrast, worddistributions (LDA) work significantly better thanTFIDF features in the other two categories.Error Analysis: We performed error analysis tounderstand the reason for misclassifications.
Ascan be seen from the Table 1, all the categorieshave several overlapping sections.
For example,Awards and honors and Awards contain similarcontent.
Authors use various section names forsimilar content in the articles within the same cat-egory.
We analyzed the confusion matrices, andfound that multiple instances in Awards were clas-sified into the class of Awards and honors.
Simi-lar observations are made on the Books and Pub-lications classes ?
which are related sections inthe context of academic biographies.
In future,we plan to use semantic measures to relate similarclasses automatically and group them in the sameCategory System ROUGE-1 ROUGE-2WikiKreator 0.522 0.311American Mathematicians Perceptron 0.431 0.193Extractive 0.471 0.254WikiKreator 0.537 0.323Diseases and Disorders Perceptron 0.411 0.197Extractive 0.473 0.232WikiKreator 0.521 0.321US Software companies Perceptron 0.421 0.228Extractive 0.484 0.257Table 4: ROUGE-1 and 2 Recall values ?
Com-paring system generated articles to model articlesclass during classification.Content Selection Evaluation: To evaluate theeffectiveness of our content generation process,we generated the content of 500 randomly se-lected articles that already exist on Wikipedia ineach of the categories.
We compare WikiKreator?soutput against the current content of those ar-ticles on Wikipedia using ROUGE (Lin, 2004).ROUGE matches N-gram sequences that existin both the system generated articles and theoriginal Wikipedia articles (gold standard).
Wealso compare WikiKreator?s output with an ex-isting Wikipedia generation system [Perceptron]of Sauper and Barzilay (2009)11that employs aperceptron learning framework to learn topic spe-cific extractors.
Queries devised using the con-junction of the document title and the section ti-tle were used to obtain excerpts from the webusing a search engine, which were used in theperceptron model.
In Perceptron, the most im-portant sections in the category was determinedusing a bisectioning algorithm to identify clus-ters of similar sections.
To understand the ef-fectiveness of our abstractive summarizer, we de-sign a system (Extractive) that uses an extrac-tive summarization module.
In Extractive, we useLexRank (Erkan and Radev, 2004) as the summa-rizer instead of our ILP based abstractive summa-rization model.
We restrict the extractive sum-maries to 5 sentences for accurate comparison ofboth the systems.
The same content was receivedas input from the classifier by the Extractive aswell as our ILP-based system.As can be seen from the Table 4, the ROUGEscores obtained by WikiKreator is higher than thatof the other comparable systems in all the cat-egories.
The higher ROUGE scores imply thatWikiKreator is generally able to retrieve usefulinformation from the web, synthesize them andpresent the important information in the article.11The system is available here: https://github.com/csauper/wikipedia874Statistics CountNumber of stubs edited 40Number of stubs retained without any changes 21Number of stubs that required minor editing 6Number of stubs where edits were modified by reviewers 4Number of stubs in which content was removed 9Average change in size of stubs 515 bytesAverage number of edits made post content-addition?3Table 5: Statistics of Wikipedia generationHowever, it may also be noted that the Extractivesystem outperforms the Perceptron framework.Summarization from multiple sources generatesmore informative summaries and is more effectivethan ?selection?
of the most informative excerpt,which is often inadequate due to potential loss ofinformation.
WikiKreator performs better than theextractive system on all the categories.
Our ILP-based abstractive summarization system fuses andselects content from multiple sentences, therebyaggregating information successfully from multi-ple sources.
In contrast, LexRank ?extracts?
thetop 5 sentences that results in some informationloss.Analysis of Wikipedia Reviews: To compare ourmethod with the other techniques, it is necessaryto generate content and append to Wikipedia stubsusing all the techniques.
However, recent work onarticle generation (Banerjee et al, 2014) has al-ready shown that content directly copied from websources cannot be used on Wikipedia.
Further,bots using copyrighted content might be bannedand real-users would have to read sub-standard ar-ticles due to the internal tests we perform.
Due tothe above mentioned reasons, we appended con-tent generated only using our abstractive summa-rization technique.We published content generated by WikiKreatoron Wikipedia and appended the content to 40 ran-domly selected stubs.
As can be seen from theTable 5, the content generated using our systemwas generally accepted by the reviewers.
Half ofthe articles did not require any further changes;while in 6 cases (15%) the reviewers asked us tofix grammatical issues.
In 9 stubs, the reliability ofthe cited references was questioned.
Informationsources on Wikipedia need to satisfy a minimumreliability standard, which our algorithm currentlycannot determine.
On an average, 3 edits weremade to the Wikipedia articles that we generated.In general, there is an average increase in the con-tent size of the stubs that we edited showing thatour method is capable of producing content thatgenerally satisfy Wikipedia criterion.Analysis of section assignment: We manually in-spected generated content of 20 articles in eachcategory.
Generated summaries are both informa-tive and precise.
However, in certain cases, thegenerated section title is not the same as the sec-tion title in the original Wikipedia article.
Forexample, we generated content for the section?Causes?
for the article on Middle East Respira-tory Syndrome (MERS)12:Milk or meat may play a role in the transmission of the virus.
People should avoid drinking raw camel milk or meat thathas not been properly cooked .
There is growing evidencethat contact with live camels or meat is causing MERS.The corresponding content on the Wikipedia is ina section labeled as ?Transmission?.
Section ti-tles at the topmost level in a category might not berelevant to all the articles.
Instead of using a top-down approach of traversing the category-graph,we can also use a bottom-up approach where welearn from all the categories that an article be-longs to.
For example, the article on MERS be-longs to two categories: Viral respiratory tract in-fection and Zoonoses.
Training using all the cat-egories will allow context-driven section identifi-cation.
Most frequent sections at a higher level inthe category graph might not always be relevant toall the articles within a category.5 Conclusions and Future WorkIn this work, we presented WikiKreator thatcan generate content automatically to improveWikipedia stubs.
Our technique employes a topic-model based text classifier that assigns web ex-cerpts into various sections on an article.
Theexcerpts are summarized using a novel abstrac-tive summarization technique that maximizes in-formativeness and linguistic quality of the gen-erated summary.
Our experiments reveal thatWikiKreator is capable of generating well-formedinformative content.
The summarization step en-sures that we avoid any copyright violation issues.The ILP based sentence generation strategy en-sures that we generate novel content by synthesiz-ing information from multiple sources and therebyimprove content selection.
In future, we plan tocluster related sections using semantic relatednessmeasures.
We also plan to estimate reliabilities ofsources to retrieve information only from reliablesources.12https://en.wikipedia.org/wiki/Middle_East_respiratory_syndrome875ReferencesRicardo Baeza-Yates, Berthier Ribeiro-Neto, et al1999.
Modern information retrieval, volume 463.ACM press New York.Siddhartha Banerjee, Cornelia Caragea, and PrasenjitMitra.
2014.
Playscript classification and auto-matic wikipedia play articles generation.
In PatternRecognition (ICPR), 2014 22nd International Con-ference on, pages 3630?3635, Aug.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Mohamed Amine Chatti, Darko Dugoija, HendrikThus, and Ulrik Schroeder.
2014.
Learner mod-eling in academic networks.
In Advanced Learn-ing Technologies (ICALT), 2014 IEEE 14th Interna-tional Conference on, pages 117?121.
IEEE.James Clarke and Mirella Lapata.
2006.
Constraint-based sentence compression an integer program-ming approach.
In Proceedings of the COL-ING/ACL on Main conference poster sessions, pages144?151.
Association for Computational Linguis-tics.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine learning, 20(3):273?297.Vipul Dalal and Latesh Malik.
2013.
A survey ofextractive and abstractive text summarization tech-niques.
In Emerging Trends in Engineering andTechnology (ICETET), 2013 6th International Con-ference on, pages 109?110.
IEEE.G?unes Erkan and Dragomir R Radev.
2004.Lexrank: Graph-based lexical centrality as saliencein text summarization.
J. Artif.
Intell.
Res.
(JAIR),22(1):457?479.Katja Filippova and Michael Strube.
2008.
Sentencefusion via dependency graph compression.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 177?185.
As-sociation for Computational Linguistics.Katja Filippova.
2010.
Multi-sentence compression:Finding shortest paths in word graphs.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, pages 322?330.
Associationfor Computational Linguistics.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In IJCAI, vol-ume 7, pages 1606?1611.Juri Ganitkevitch, Chris Callison-Burch, CourtneyNapoles, and Benjamin Van Durme.
2011.
Learn-ing sentential paraphrases from bilingual parallelcorpora for text-to-text generation.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, pages 1168?1179.
Asso-ciation for Computational Linguistics.Shima Gerani, Yashar Mehdad, Giuseppe Carenini,T.
Raymond Ng, and Bita Nejat.
2014.
Abstractivesummarization of product reviews using discoursestructure.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 1602?1613.
Association for Com-putational Linguistics.Inc.
Gurobi Optimization.
2015.
Gurobi optimizer ref-erence manual.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The weka data mining software: an update.ACM SIGKDD explorations newsletter, 11(1):10?18.Christian Kohlsch?utter, Peter Fankhauser, and Wolf-gang Nejdl.
2010.
Boilerplate detection using shal-low text features.
In Proceedings of the third ACMinternational conference on Web search and datamining, pages 441?450.
ACM.Peng Li, Yinglin Wang, and Jing Jiang.
2013.
Au-tomatically building templates for entity summaryconstruction.
Information Processing & Manage-ment, 49(1):330?340.Andy Liaw and Matthew Wiener.
2002.
Classificationand regression by randomforest.
R news, 2(3):18?22.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81.Andrew K McCallum.
2002.
{MALLET: A MachineLearning for Language Toolkit}.Olena Medelyan, Ian H Witten, and David Milne.2008.
Topic indexing with wikipedia.
In Proceed-ings of the AAAI WikiAI workshop, pages 19?24.Rada Mihalcea and Paul Tarau.
2004.
Textrank:Bringing order into texts.
Association for Compu-tational Linguistics.Ani Nenkova, Sameer Maskey, and Yang Liu.
2011.Automatic summarization.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Tutorial Abstracts of ACL2011, page 3.
Association for Computational Lin-guistics.Christina Sauper and Regina Barzilay.
2009.
Auto-matically generating wikipedia articles: A structure-aware approach.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 1-Volume1, pages 208?216.
Association for ComputationalLinguistics.876Fei Song and W Bruce Croft.
1999.
A general lan-guage model for information retrieval.
In Proceed-ings of the eighth international conference on In-formation and knowledge management, pages 316?321.
ACM.Wei Song, Yu Zhang, Ting Liu, and Sheng Li.
2010.Bridging topic modeling and personalized search.In Proceedings of the 23rd International Conferenceon Computational Linguistics: Posters, pages 1167?1175.
Association for Computational Linguistics.Marilyn A Walker, Owen Rambow, and Monica Ro-gati.
2001.
Spot: A trainable sentence planner.In Proceedings of the second meeting of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Language technologies, pages1?8.
Association for Computational Linguistics.Conglei Yao, Xu Jia, Sicong Shou, Shicong Feng, FengZhou, and HongYan Liu.
2011.
Autopedia: auto-matic domain-independent wikipedia article genera-tion.
In Proceedings of the 20th international con-ference companion on World wide web, pages 161?162.
ACM.Torsten Zesch and Iryna Gurevych.
2007.
Analy-sis of the wikipedia category graph for nlp applica-tions.
In Proceedings of the TextGraphs-2 Workshop(NAACL-HLT 2007), pages 1?8.877
