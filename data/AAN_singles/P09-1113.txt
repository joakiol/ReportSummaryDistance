Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1003?1011,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPDistant supervision for relation extraction without labeled dataMike Mintz, Steven Bills, Rion Snow, Dan JurafskyStanford University / Stanford, CA 94305{mikemintz,sbills,rion,jurafsky}@cs.stanford.eduAbstractModern models of relation extraction for tasks likeACE are based on supervised learning of relationsfrom small hand-labeled corpora.
We investigate analternative paradigm that does not require labeledcorpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corporaof any size.
Our experiments use Freebase, a largesemantic database of several thousand relations, toprovide distant supervision.
For each pair of enti-ties that appears in some Freebase relation, we findall sentences containing those entities in a large un-labeled corpus and extract textual features to traina relation classifier.
Our algorithm combines theadvantages of supervised IE (combining 400,000noisy pattern features in a probabilistic classifier)and unsupervised IE (extracting large numbers ofrelations from large corpora of any domain).
Ourmodel is able to extract 10,000 instances of 102 re-lations at a precision of 67.6%.
We also analyzefeature performance, showing that syntactic parsefeatures are particularly helpful for relations that areambiguous or lexically distant in their expression.1 IntroductionAt least three learning paradigms have been ap-plied to the task of extracting relational facts fromtext (for example, learning that a person is em-ployed by a particular organization, or that a ge-ographic entity is located in a particular region).In supervised approaches, sentences in a cor-pus are first hand-labeled for the presence of en-tities and the relations between them.
The NISTAutomatic Content Extraction (ACE) RDC 2003and 2004 corpora, for example, include over 1,000documents in which pairs of entities have been la-beled with 5 to 7 major relation types and 23 to24 subrelations, totaling 16,771 relation instances.ACE systems then extract a wide variety of lexi-cal, syntactic, and semantic features, and use su-pervised classifiers to label the relation mentionholding between a given pair of entities in a testset sentence, optionally combining relation men-tions (Zhou et al, 2005; Zhou et al, 2007; Sur-deanu and Ciaramita, 2007).Supervised relation extraction suffers from anumber of problems, however.
Labeled trainingdata is expensive to produce and thus limited inquantity.
Also, because the relations are labeledon a particular corpus, the resulting classifiers tendto be biased toward that text domain.An alternative approach, purely unsupervisedinformation extraction, extracts strings of wordsbetween entities in large amounts of text, andclusters and simplifies these word strings to pro-duce relation-strings (Shinyama and Sekine, 2006;Banko et al, 2007).
Unsupervised approaches canuse very large amounts of data and extract verylarge numbers of relations, but the resulting rela-tions may not be easy to map to relations neededfor a particular knowledge base.A third approach has been to use a very smallnumber of seed instances or patterns to do boot-strap learning (Brin, 1998; Riloff and Jones, 1999;Agichtein and Gravano, 2000; Ravichandran andHovy, 2002; Etzioni et al, 2005; Pennacchiottiand Pantel, 2006; Bunescu and Mooney, 2007;Rozenfeld and Feldman, 2008).
These seeds areused with a large corpus to extract a new set ofpatterns, which are used to extract more instances,which are used to extract more patterns, in an it-erative fashion.
The resulting patterns often sufferfrom low precision and semantic drift.We propose an alternative paradigm, distant su-pervision, that combines some of the advantagesof each of these approaches.
Distant supervisionis an extension of the paradigm used by Snow etal.
(2005) for exploiting WordNet to extract hyper-nym (is-a) relations between entities, and is simi-lar to the use of weakly labeled data in bioinfor-matics (Craven and Kumlien, 1999; Morgan et al,1003Relation name New instance/location/location/contains Paris, Montmartre/location/location/contains Ontario, Fort Erie/music/artist/origin Mighty Wagon, Cincinnati/people/deceased person/place of death Fyodor Kamensky, Clearwater/people/person/nationality Marianne Yvonne Heemskerk, Netherlands/people/person/place of birth Wavell Wayne Hinds, Kingston/book/author/works written Upton Sinclair, Lanny Budd/business/company/founders WWE, Vince McMahon/people/person/profession Thomas Mellon, judgeTable 1: Ten relation instances extracted by our system that did not appear in Freebase.2004).
Our algorithm uses Freebase (Bollacker etal., 2008), a large semantic database, to providedistant supervision for relation extraction.
Free-base contains 116 million instances of 7,300 rela-tions between 9 million entities.
The intuition ofdistant supervision is that any sentence that con-tains a pair of entities that participate in a knownFreebase relation is likely to express that relationin some way.
Since there may be many sentencescontaining a given entity pair, we can extract verylarge numbers of (potentially noisy) features thatare combined in a logistic regression classifier.Thus whereas the supervised training paradigmuses a small labeled corpus of only 17,000 rela-tion instances as training data, our algorithm canuse much larger amounts of data: more text, morerelations, and more instances.
We use 1.2 millionWikipedia articles and 1.8 million instances of 102relations connecting 940,000 entities.
In addition,combining vast numbers of features in a large clas-sifier helps obviate problems with bad features.Because our algorithm is supervised by adatabase, rather than by labeled text, it doesnot suffer from the problems of overfitting anddomain-dependence that plague supervised sys-tems.
Supervision by a database also means that,unlike in unsupervised approaches, the output ofour classifier uses canonical names for relations.Our paradigm offers a natural way of integratingdata from multiple sentences to decide if a relationholds between two entities.
Because our algorithmcan use large amounts of unlabeled data, a pair ofentities may occur multiple times in the test set.For each pair of entities, we aggregate the featuresfrom the many different sentences in which thatpair appeared into a single feature vector, allowingus to provide our classifier with more information,resulting in more accurate labels.Table 1 shows examples of relation instancesextracted by our system.
We also use this systemto investigate the value of syntactic versus lexi-cal (word sequence) features in relation extraction.While syntactic features are known to improve theperformance of supervised IE, at least using cleanhand-labeled ACE data (Zhou et al, 2007; Zhouet al, 2005), we do not know whether syntacticfeatures can improve the performance of unsuper-vised or distantly supervised IE.
Most previousresearch in bootstrapping or unsupervised IE hasused only simple lexical features, thereby avoid-ing the computational expense of parsing (Brin,1998; Agichtein and Gravano, 2000; Etzioni et al,2005), and the few systems that have used unsu-pervised IE have not compared the performanceof these two types of feature.2 Previous workExcept for the unsupervised algorithms discussedabove, previous supervised or bootstrapping ap-proaches to relation extraction have typically re-lied on relatively small datasets, or on only a smallnumber of distinct relations.
Approaches based onWordNet have often only looked at the hypernym(is-a) or meronym (part-of) relation (Girju et al,2003; Snow et al, 2005), while those based on theACE program (Doddington et al, 2004) have beenrestricted in their evaluation to a small number ofrelation instances and corpora of less than a mil-lion words.Many early algorithms for relation extractionused little or no syntactic information.
For ex-ample, the DIPRE algorithm by Brin (1998) usedstring-based regular expressions in order to rec-ognize relations such as author-book, while theSNOWBALL algorithm by Agichtein and Gravano(2000) learned similar regular expression patternsover words and named entity tags.
Hearst (1992)used a small number of regular expressions overwords and part-of-speech tags to find examples ofthe hypernym relation.
The use of these patternshas been widely replicated in successful systems,for example by Etzioni et al (2005).
Other work1004Relation name Size Example/people/person/nationality 281,107 John Dugard, South Africa/location/location/contains 253,223 Belgium, Nijlen/people/person/profession 208,888 Dusa McDuff, Mathematician/people/person/place of birth 105,799 Edwin Hubble, Marshfield/dining/restaurant/cuisine 86,213 MacAyo?s Mexican Kitchen, Mexican/business/business chain/location 66,529 Apple Inc., Apple Inc., South Park, NC/biology/organism classification rank 42,806 Scorpaeniformes, Order/film/film/genre 40,658 Where the Sidewalk Ends, Film noir/film/film/language 31,103 Enter the Phoenix, Cantonese/biology/organism higher classification 30,052 Calopteryx, Calopterygidae/film/film/country 27,217 Turtle Diary, United States/film/writer/film 23,856 Irving Shulman, Rebel Without a Cause/film/director/film 23,539 Michael Mann, Collateral/film/producer/film 22,079 Diane Eskenazi, Aladdin/people/deceased person/place of death 18,814 John W. Kern, Asheville/music/artist/origin 18,619 The Octopus Project, Austin/people/person/religion 17,582 Joseph Chartrand, Catholicism/book/author/works written 17,278 Paul Auster, Travels in the Scriptorium/soccer/football position/players 17,244 Midfielder, Chen Tao/people/deceased person/cause of death 16,709 Richard Daintree, Tuberculosis/book/book/genre 16,431 Pony Soldiers, Science fiction/film/film/music 14,070 Stavisky, Stephen Sondheim/business/company/industry 13,805 ATS Medical, Health careTable 2: The 23 largest Freebase relations we use, with their size and an instance of each relation.such as Ravichandran and Hovy (2002) and Pan-tel and Pennacchiotti (2006) use the same formal-ism of learning regular expressions over words andpart-of-speech tags to discover patterns indicatinga variety of relations.More recent approaches have used deeper syn-tactic information derived from parses of the inputsentences, including work exploiting syntactic de-pendencies by Lin and Pantel (2001) and Snow etal.
(2005), and work in the ACE paradigm suchas Zhou et al (2005) and Zhou et al (2007).Perhaps most similar to our distant supervisionalgorithm is the effective method of Wu and Weld(2007) who extract relations from a Wikipediapage by using supervision from the page?s infobox.Unlike their corpus-specific method, which is spe-cific to a (single) Wikipedia page, our algorithmallows us to extract evidence for a relation frommany different documents, and from any genre.3 FreebaseFollowing the literature, we use the term ?rela-tion?
to refer to an ordered, binary relation be-tween entities.
We refer to individual ordered pairsin this relation as ?relation instances?.
For ex-ample, the person-nationality relation holds be-tween the entities named ?John Steinbeck?
and?United States?, so it has ?John Steinbeck,United States?
as an instance.We use relations and relation instances fromFreebase, a freely available online database ofstructured semantic data.
Data in Freebase iscollected from a variety of sources.
One majorsource is text boxes and other tabular data fromWikipedia.
Data is also taken from NNDB (bio-graphical information), MusicBrainz (music), theSEC (financial and corporate data), as well as di-rect, wiki-style user editing.
After some basicprocessing of the July 2008 link export to con-vert Freebase?s data representation into binary re-lations, we have 116 million instances of 7,300relations between 9 million entities.
We next fil-ter out nameless and uninteresting entities such asuser profiles and music tracks.
Freebase also con-tains the reverses of many of its relations (book-author v. author-book), and these are merged.
Fil-tering and removing all but the largest relationsleaves us with 1.8 million instances of 102 rela-tions connecting 940,000 entities.
Examples areshown in Table 2.4 ArchitectureThe intuition of our distant supervision approachis to use Freebase to give us a training set of rela-tions and entity pairs that participate in those rela-tions.
In the training step, all entities are identified1005in sentences using a named entity tagger that la-bels persons, organizations and locations.
If a sen-tence contains two entities and those entities are aninstance of one of our Freebase relations, featuresare extracted from that sentence and are added tothe feature vector for the relation.The distant supervision assumption is that if twoentities participate in a relation, any sentence thatcontain those two entities might express that rela-tion.
Because any individual sentence may givean incorrect cue, our algorithm trains a multiclasslogistic regression classifier, learning weights foreach noisy feature.
In training, the features foridentical tuples (relation, entity1, entity2) fromdifferent sentences are combined, creating a richerfeature vector.In the testing step, entities are again identifiedusing the named entity tagger.
This time, everypair of entities appearing together in a sentence isconsidered a potential relation instance, and when-ever those entities appear together, features are ex-tracted on the sentence and added to a feature vec-tor for that entity pair.
For example, if a pair ofentities occurs in 10 sentences in the test set, andeach sentence has 3 features extracted from it, theentity pair will have 30 associated features.
Eachentity pair in each sentence in the test corpus is runthrough feature extraction, and the regression clas-sifier predicts a relation name for each entity pairbased on the features from all of the sentences inwhich it appeared.Consider the location-contains relation, imag-ining that in Freebase we had two instances ofthis relation: ?Virginia, Richmond?
and?France, Nantes?.
As we encountered sen-tences like ?Richmond, the capital of Virginia?
and?Henry?s Edict of Nantes helped the Protestants ofFrance?
we would extract features from these sen-tences.
Some features would be very useful, suchas the features from the Richmond sentence, andsome would be less useful, like those from theNantes sentence.
In testing, if we came acrossa sentence like ?Vienna, the capital of Austria?,one or more of its features would match those ofthe Richmond sentence, providing evidence that?Austria, Vienna?
belongs to the location-contains relation.Note that one of the main advantages of ourarchitecture is its ability to combine informa-tion from many different mentions of the samerelation.
Consider the entity pair ?StevenSpielberg, Saving Private Ryan?from the following two sentences, as evidence forthe film-director relation.
[Steven Spielberg]?s film [Saving PrivateRyan] is loosely based on the brothers?
story.Allison co-produced the Academy Award-winning [Saving Private Ryan], directed by[Steven Spielberg]...The first sentence, while providing evidence forfilm-director, could instead be evidence for film-writer or film-producer.
The second sentence doesnot mention that Saving Private Ryan is a film, andso could instead be evidence for the CEO relation(consider ?Robert Mueller directed the FBI?).
Inisolation, neither of these features is conclusive,but in combination, they are.5 FeaturesOur features are based on standard lexical and syn-tactic features from the literature.
Each featuredescribes how two entities are related in a sen-tence, using either syntactic or non-syntactic in-formation.5.1 Lexical featuresOur lexical features describe specific words be-tween and surrounding the two entities in the sen-tence in which they appear:?
The sequence of words between the two entities?
The part-of-speech tags of these words?
A flag indicating which entity came first in the sentence?
A window of k words to the left of Entity 1 and theirpart-of-speech tags?
A window of k words to the right of Entity 2 and theirpart-of-speech tagsEach lexical feature consists of the conjunction ofall these components.
We generate a conjunctivefeature for each k ?
{0, 1, 2}.
Thus each lexicalrow in Table 3 represents a single lexical feature.Part-of-speech tags were assigned by a max-imum entropy tagger trained on the Penn Tree-bank, and then simplified into seven categories:nouns, verbs, adverbs, adjectives, numbers, for-eign words, and everything else.In an attempt to approximate syntactic features,we also tested variations on our lexical features:(1) omitting all words that are not verbs and (2)omitting all function words.
In combination withthe other lexical features, they gave a small boostto precision, but not large enough to justify the in-creased demand on our computational resources.1006Feature type Left window NE1 Middle NE2 Right windowLexical [] PER [was/VERB born/VERB in/CLOSED] LOC []Lexical [Astronomer] PER [was/VERB born/VERB in/CLOSED] LOC [,]Lexical [#PAD#, Astronomer] PER [was/VERB born/VERB in/CLOSED] LOC [, Missouri]Syntactic [] PER [?s was ?pred born ?mod in ?pcomp?n] LOC []Syntactic [Edwin Hubble ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC []Syntactic [Astronomer ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC []Syntactic [] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?lex?mod ,]Syntactic [Edwin Hubble ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?lex?mod ,]Syntactic [Astronomer ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?lex?mod ,]Syntactic [] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?inside Missouri]Syntactic [Edwin Hubble ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?inside Missouri]Syntactic [Astronomer ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?inside Missouri]Table 3: Features for ?Astronomer Edwin Hubble was born in Marshfield, Missouri?.Astronomer Edwin Hubble was born in Marshfield , Missourilex-mod s pred mod pcomp-n lex-modinsideFigure 1: Dependency parse with dependency path from ?Edwin Hubble?
to ?Marshfield?
highlighted inboldface.5.2 Syntactic featuresIn addition to lexical features we extract a num-ber of features based on syntax.
In order to gener-ate these features we parse each sentence with thebroad-coverage dependency parser MINIPAR (Lin,1998).A dependency parse consists of a set of wordsand chunks (e.g.
?Edwin Hubble?, ?Missouri?,?born?
), linked by directional dependencies (e.g.
?pred?, ?lex-mod?
), as in Figure 1.
For eachsentence we extract a dependency path betweeneach pair of entities.
A dependency path con-sists of a series of dependencies, directions andwords/chunks representing a traversal of the parse.Part-of-speech tags are not included in the depen-dency path.Our syntactic features are similar to those usedin Snow et al (2005).
They consist of the conjunc-tion of:?
A dependency path between the two entities?
For each entity, one ?window?
node that is not part ofthe dependency pathA window node is a node connected to one of thetwo entities and not part of the dependency path.We generate one conjunctive feature for each pairof left and right window nodes, as well as featureswhich omit one or both of them.
Thus each syn-tactic row in Table 3 represents a single syntacticfeature.5.3 Named entity tag featuresEvery feature contains, in addition to the contentdescribed above, named entity tags for the two en-tities.
We perform named entity tagging using theStanford four-class named entity tagger (Finkel etal., 2005).
The tagger provides each word with alabel from {person, location, organization, miscel-laneous, none}.5.4 Feature conjunctionRather than use each of the above features in theclassifier independently, we use only conjunctivefeatures.
Each feature consists of the conjunc-tion of several attributes of the sentence, plus thenamed entity tags.
For two features to match,all of their conjuncts must match exactly.
Thisyields low-recall but high-precision features.
Witha small amount of data, this approach would beproblematic, since most features would only beseen once, rendering them useless to the classifier.Since we use large amounts of data, even complexfeatures appear multiple times, allowing our high-precision features to work as intended.
Featuresfor a sample sentence are shown in Table 3.6 Implementation6.1 TextFor unstructured text we use the FreebaseWikipedia Extraction, a dump of the full text of allWikipedia articles (not including discussion and1007Relation Feature type Left window NE1 Middle NE2 Right window/architecture/structure/architect LEXx ORG , the designer of the PERSYN designed ?s ORG ?s designed ?by?subj by ?pcn PER ?s designed/book/author/works written LEX PER s novel ORGSYN PER ?pcn by ?mod story ?pred is ?s ORG/book/book edition/author editor LEXx ORG s novel PERSYN PER ?nn series ?gen PER/business/company/founders LEX ORG co - founder PERSYN ORG ?nn owner ?person PER/business/company/place founded LEXx ORG - based LOCSYN ORG ?s founded ?mod in ?pcn LOC/film/film/country LEX PER , released in LOCSYN opened ?s ORG ?s opened ?mod in ?pcn LOC ?s opened/geography/river/mouth LEX LOC , which flows into the LOCSYN the ?det LOC ?s is ?pred tributary ?mod of ?pcn LOC ?det the/government/political party/country LEXx ORG politician of the LOCSYN candidate ?nn ORG ?nn candidate ?mod for ?pcn LOC ?nn candidate/influence/influence node/influenced LEXx PER , a student of PERSYN of ?pcn PER ?pcn of ?mod student ?appo PER ?pcn of/language/human language/region LEX LOC - speaking areas of LOCSYN LOC ?lex?mod speaking areas ?mod of ?pcn LOC/music/artist/origin LEXx ORG based band LOCSYN is ?s ORG ?s is ?pred band ?mod from ?pcn LOC ?s is/people/deceased person/place of death LEX PER died in LOCSYN hanged ?s PER ?s hanged ?mod in ?pcn LOC ?s hanged/people/person/nationality LEX PER is a citizen of LOCSYN PER ?mod from ?pcn LOC/people/person/parents LEX PER , son of PERSYN father ?gen PER ?gen father ?person PER ?gen father/people/person/place of birth LEXx PER is the birthplace of PERSYN PER ?s born ?mod in ?pcn LOC/people/person/religion LEX PER embraced LOCSYN convert ?appo PER ?appo convert ?mod to ?pcn LOC ?appo convertTable 4: Examples of high-weight features for several relations.
Key: SYN = syntactic feature; LEX =lexical feature;x = reversed; NE# = named entity tag of entity.user pages) which has been sentence-tokenized byMetaweb Technologies, the developers of Free-base (Metaweb, 2008).
This dump consists ofapproximately 1.8 million articles, with an av-erage of 14.3 sentences per article.
The totalnumber of words (counting punctuation marks) is601,600,703.
For our experiments we use abouthalf of the articles: 800,000 for training and400,000 for testing.We use Wikipedia because it is relatively up-to-date, and because its sentences tend to makeexplicit many facts that might be omitted innewswire.
Much of the information in Freebase isderived from tabular data from Wikipedia, mean-ing that Freebase relations are more likely to ap-pear in sentences in Wikipedia.6.2 Parsing and chunkingEach sentence of this unstructured text is depen-dency parsed by MINIPAR to produce a depen-dency graph.In preprocessing, consecutive words with thesame named entity tag are ?chunked?, so thatEdwin/PERSON Hubble/PERSON becomes[Edwin Hubble]/PERSON.
This chunking isrestricted by the dependency parse of the sentence,however, in that chunks must be contiguous inthe parse (i.e., no chunks across subtrees).
Thisensures that parse tree structure is preserved, sincethe parses must be updated to reflect the chunking.6.3 Training and testingFor held-out evaluation experiments (see section7.1), half of the instances of each relation are notused in training, and are later used to compareagainst newly discovered instances.
This meansthat 900,000 Freebase relation instances are usedin training, and 900,000 are held out.
These ex-periments used 800,000 Wikipedia articles in thetraining phase and 400,000 different articles in thetesting phase.For human evaluation experiments, all 1.8 mil-lion relation instances are used in training.
Again,we use 800,000 Wikipedia articles in the trainingphase and 400,000 different articles in the testingphase.For all our experiments, we only extract relationinstances that do not appear in our training data,i.e., instances that are not already in Freebase.Our system needs negative training data for thepurposes of constructing the classifier.
Towardsthis end, we build a feature vector in the train-ing phase for an ?unrelated?
relation by randomlyselecting entity pairs that do not appear in anyFreebase relation and extracting features for them.While it is possible that some of these entity pairs10080?0.1?0.2?0.3?0.4?0.5?0.6?0.7?0.8?0.9?1?0?
0.05?
0.1?
0.15?
0.2?
0.25?
0.3?
0.35?
0.4?
0.45?Precision?Oracle?recall?Both?Syntax?Surface?Figure 2: Automatic evaluation with 50% of Freebase relation data held out and 50% used in trainingon the 102 largest relations we use.
Precision for three different feature sets (lexical features, syntacticfeatures, and both) is reported at recall levels from 10 to 100,000.
At the 100,000 recall level, we classifymost of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and10% as person-nationality.are in fact related but are wrongly omitted fromthe Freebase data, we expect that on average thesefalse negatives will have a small effect on the per-formance of the classifier.
For performance rea-sons, we randomly sample 1% of such entity pairsfor use as negative training examples.
By contrast,in the actual test data, 98.7% of the entity pairs weextract do not possess any of the top 102 relationswe consider in Freebase.We use a multi-class logistic classifier opti-mized using L-BFGS with Gaussian regulariza-tion.
Our classifier takes as input an entity pairand a feature vector, and returns a relation nameand a confidence score based on the probability ofthe entity pair belonging to that relation.
Once allof the entity pairs discovered during testing havebeen classified, they can be ranked by confidencescore and used to generate a list of the n mostlikely new relation instances.Table 4 shows some high-weight featureslearned by our system.
We discuss the results inthe next section.7 EvaluationWe evaluate labels in two ways: automatically,by holding out part of the Freebase relation dataduring training, and comparing newly discoveredrelation instances against this held-out data, andmanually, having humans who look at each posi-tively labeled entity pair and mark whether the re-lation indeed holds between the participants.
Bothevaluations allow us to calculate the precision ofthe system for the best N instances.7.1 Held-out evaluationFigure 2 shows the performance of our classifieron held-out Freebase relation data.
While held-outevaluation suffers from false negatives, it gives arough measure of precision without requiring ex-pensive human evaluation, making it useful for pa-rameter setting.At most recall levels, the combination of syn-tactic and lexical features offers a substantial im-provement in precision over either of these featuresets on its own.7.2 Human evaluationHuman evaluation was performed by evaluators onAmazon?s Mechanical Turk service, shown to beeffective for natural language annotation in Snowet al (2008).
We ran three experiments: one us-ing only syntactic features; one using only lexicalfeatures; and one using both syntactic and lexicalfeatures.
For each of the 10 relations that appearedmost frequently in our test data (according to ourclassifier), we took samples from the first 100 and1000 instances of this relation generated in eachexperiment, and sent these to Mechanical Turk for1009Relation name100 instances 1000 instancesSyn Lex Both Syn Lex Both/film/director/film 0.49 0.43 0.44 0.49 0.41 0.46/film/writer/film 0.70 0.60 0.65 0.71 0.61 0.69/geography/river/basin countries 0.65 0.64 0.67 0.73 0.71 0.64/location/country/administrative divisions 0.68 0.59 0.70 0.72 0.68 0.72/location/location/contains 0.81 0.89 0.84 0.85 0.83 0.84/location/us county/county seat 0.51 0.51 0.53 0.47 0.57 0.42/music/artist/origin 0.64 0.66 0.71 0.61 0.63 0.60/people/deceased person/place of death 0.80 0.79 0.81 0.80 0.81 0.78/people/person/nationality 0.61 0.70 0.72 0.56 0.61 0.63/people/person/place of birth 0.78 0.77 0.78 0.88 0.85 0.91Average 0.67 0.66 0.69 0.68 0.67 0.67Table 5: Estimated precision on human-evaluation experiments of the highest-ranked 100 and 1000results per relation, using stratified samples.
?Average?
gives the mean precision of the 10 relations.
Key:Syn = syntactic features only.
Lex = lexical features only.
We use stratified samples because of theoverabundance of location-contains instances among our high-confidence results.human evaluation.
Our sample size was 100.Each predicted relation instance was labeled astrue or false by between 1 and 3 labelers on Me-chanical Turk.
We assigned the truth or falsehoodof each relation according to the majority vote ofthe labels; in the case of a tie (one vote each way)we assigned the relation as true or false with equalprobability.
The evaluation of the syntactic, lexi-cal, and combination of features at a recall of 100and 1000 instances is presented in Table 5.At a recall of 100 instances, the combination oflexical and syntactic features has the best perfor-mance for a majority of the relations, while at a re-call level of 1000 instances the results are mixed.No feature set strongly outperforms any of the oth-ers across all relations.8 DiscussionOur results show that the distant supervision algo-rithm is able to extract high-precision patterns fora reasonably large number of relations.The held-out results in Figure 2 suggest that thecombination of syntactic and lexical features pro-vides better performance than either feature set onits own.
In order to understand the role of syntacticfeatures, we examine Table 5, the human evalua-tion of the most frequent 10 relations.
For the top-ranking 100 instances of each relation, most of thebest results use syntactic features, either alone orin combination with lexical features.
For the top-ranking 1000 instances of each relation, the resultsare more mixed, but syntactic features still helpedin most classifications.We then examine those relations for which syn-tactic features seem to help.
For example, syn-tactic features consistently outperform lexical fea-tures for the director-film and writer-film relations.As discussed in section 4, these two relations areparticularly ambiguous, suggesting that syntacticfeatures may help tease apart difficult relations.Perhaps more telling, we noticed many exampleswith a long string of words between the directorand the film:Back Street is a 1932 film made by Univer-sal Pictures, directed by John M. Stahl, andproduced by Carl Laemmle Jr.Sentences like this have very long (and thus rare)lexical features, but relatively short dependencypaths.
Syntactic features can more easily abstractfrom the syntactic modifiers that comprise the ex-traneous parts of these strings.Our results thus suggest that syntactic featuresare indeed useful in distantly supervised informa-tion extraction, and that the benefit of syntax oc-curs in cases where the individual patterns are par-ticularly ambiguous, and where they are nearby inthe dependency structure but distant in terms ofwords.
It remains for future work to see whethersimpler, chunk-based syntactic features might beable to capture enough of this gain without theoverhead of full parsing, and whether coreferenceresolution could improve performance.AcknowledgmentsWe would like to acknowledge Sarah Spikes forher help in developing the relation extraction sys-tem, Christopher Manning and Mihai Surdeanufor their invaluable advice, and Fuliang Wengand Baoshi Yan for their guidance.
Our researchwas partially funded by the NSF via award IIS-0811974 and by Robert Bosch LLC.1010ReferencesEugene Agichtein and Luis Gravano.
2000.
Snow-ball: Extracting relations from large plain-text col-lections.
In Proceedings of the 5th ACM Interna-tional Conference on Digital Libraries.Michele Banko, Michael J. Cafarella, Stephen Soder-land, Matthew Broadhead, and Oren Etzioni.
2007.Open information extraction from the web.
InManuela M Veloso, editor, IJCAI-07, pages 2670?2676.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In SIGMOD ?08, pages 1247?1250, New York, NY.
ACM.Sergei Brin.
1998.
Extracting patterns and relationsfrom the World Wide Web.
In Proceedings WorldWide Web and Databases International Workshop,Number 1590 in LNCS, pages 172?183.
Springer.Razvan Bunescu and Raymond Mooney.
2007.
Learn-ing to extract relations from the web using minimalsupervision.
In ACL-07, pages 576?583, Prague,Czech Republic, June.Mark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informa-tion from text sources.
In Thomas Lengauer, Rein-hard Schneider, Peer Bork, Douglas L. Brutlag, Jan-ice I. Glasgow, Hans W. Mewes, and Ralf Zimmer,editors, ISMB, pages 77?86.
AAAI.George Doddington, Alexis Mitchell, Mark Przybocki,Lance Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
The Automatic Content Extrac-tion (ACE) Program?Tasks, Data, and Evaluation.LREC-04, pages 837?840.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Un-supervised named-entity extraction from the web:An experimental study.
Artificial Intelligence,165(1):91?134.Jenny R. Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In ACL-05, pages 363?370, Ann Arbor,MI.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2003.
Learning semantic constraints for the auto-matic discovery of part-whole relations.
In HLT-NAACL-03, pages 1?8, Edmonton, Canada.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In COLING-92,Nantes, France.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question-answering.
Natural Lan-guage Engineering, 7(4):343?360.Dekang Lin.
1998.
Dependency-based evaluation ofminipar.
In Workshop on the Evaluation of ParsingSystems.Metaweb.
2008.
Freebase data dumps.
http://download.freebase.com/datadumps/.Alexander A. Morgan, Lynette Hirschman, MarcColosimo, Alexander S. Yeh, and Jeff B. Colombe.2004.
Gene name identification and normalizationusing a model organism database.
J. of BiomedicalInformatics, 37(6):396?410.Patrick Pantel and Marco Pennacchiotti.
2006.Espresso: leveraging generic patterns for auto-matically harvesting semantic relations.
In COL-ING/ACL 2006, pages 113?120, Sydney, Australia.Marco Pennacchiotti and Patrick Pantel.
2006.
A boot-strapping algorithm for automatically harvesting se-mantic relations.
In in Proceedings of Inference inComputational Semantics (ICoS-06), pages 87?96.Deepak Ravichandran and Eduard H. Hovy.
2002.Learning surface text patterns for a question answer-ing system.
In ACL-02, pages 41?47, Philadelphia,PA.Ellen Riloff and Rosie Jones.
1999.
Learning dic-tionaries for information extraction by multi-levelbootstrapping.
In AAAI-99, pages 474?479.Benjamin Rozenfeld and Ronen Feldman.
2008.
Self-supervised relation extraction from the web.
Knowl-edge and Information Systems, 17(1):17?33.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted rela-tion discovery.
In HLT-NAACL-06, pages 304?311,New York, NY.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In Lawrence K. Saul, Yair Weiss, andLe?on Bottou, editors, NIPS 17, pages 1297?1304.MIT Press.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Ng.
2008.
Cheap and fast ?
but is it good?evaluating non-expert annotations for natural lan-guage tasks.
In EMNLP 2008, pages 254?263, Hon-olulu, HI.Mihai Surdeanu and Massimiliano Ciaramita.
2007.Robust information extraction with perceptrons.
InProceedings of the NIST 2007 Automatic ContentExtraction Workshop (ACE07), March.Fei Wu and Daniel S. Weld.
2007.
Autonomously se-mantifying wikipedia.
In CIKM ?07: Proceedingsof the sixteenth ACM conference on Conference oninformation and knowledge management, pages 41?50, Lisbon, Portugal.Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring various knowledge in relation ex-traction.
In ACL-05, pages 427?434, Ann Arbor,MI.Guodong Zhou, Min Zhang, Donghong Ji, and Qiaom-ing Zhu.
2007.
Tree kernel-based relation extrac-tion with context-sensitive structured parse tree in-formation.
In EMNLP/CoNLL 2007.1011
