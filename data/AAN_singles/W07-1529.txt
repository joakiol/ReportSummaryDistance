Proceedings of the Linguistic Annotation Workshop, pages 184?190,Prague, June 2007. c?2007 Association for Computational LinguisticsThe Shared Corpora Working Group ReportAdam MeyersNew YorkUniversityNew York, NYmeyersat cs.nyu.eduNancy IdeVassar CollegePoughkeepsie, NYide at cs.vassar.eduLudovic DenoyerUniversity of ParisParis, Franceludovic.denoyerat lip6.frYusuke ShinyamaNew YorkUniversityNew York, NYyusukeat cs.nyu.eduAbstractWe seek to identify a limited amount of rep-resentative corpora, suitable for annotationby the computational linguistics annotationcommunity.
Our hope is that a wide vari-ety of annotation will be undertaken on thesame corpora, which would facilitate: (1)the comparison of annotation schemes; (2)the merging of information represented byvarious annotation schemes; (3) the emer-gence of NLP systems that use informa-tion in multiple annotation schemes; and (4)the adoption of various types of best prac-tice in corpus annotation.
Such best prac-tices would include: (a) clearer demarca-tion of phenomena being annotated; (b) theuse of particular test corpora to determinewhether a particular annotation task can fea-sibly achieve good agreement scores; (c)The use of underlying models for represent-ing annotation content that facilitate merg-ing, comparison, and analysis; and (d) Tothe extent possible, the use of common an-notation categories or a mapping among cat-egories for the same phenomenon used bydifferent annotation groups.This study will focus on the problem ofidentifying such corpora as well as the suit-ability of two candidate corpora: the Openportion of the American National Corpus(Ide and Macleod, 2001; Ide and Suder-man, 2004) and the ?Controversial?
portionsof the WikipediaXML corpus (Denoyer andGallinari, 2006).1 IntroductionThis working group seeks to identify a limitedamount of representative corpora, suitable for an-notation by the computational linguistics annotationcommunity.
Our hope is that a wide variety of anno-tation will be undertaken on the same corpora, whichwould facilitate:1.
The comparison of annotation schemes2.
The merging of information represented by var-ious annotation schemes3.
The emergence of NLP systems that use infor-mation in multiple annotation schemes; and4.
The adoption of various types of best practicein corpus annotation, including:(a) Clearer demarcation of the phenomena be-ing annotated.
Thus if predicate argu-ment structure annotation adequately han-dles relative pronouns, a new project thatis annotating coreference is less likely toinclude relative pronouns in their annota-tion; and(b) The use of particular test corpora to de-termine whether a particular annotationtask can feasibly achieve good agreementscores.
(c) The use of underlying models for repre-senting annotation content that facilitatemerging, comparison, and analysis.184(d) To the extent possible, the use of commonannotation categories or a mapping amongcategories for the same phenomenon usedby different annotation groups.In selecting shared corpora, we believe that thefollowing issues must be taken into consideration:1.
The diversity of genres, lexical items and lin-guistic phenomena ?
this will ensure that thecorpora will be useful to many different typesof annotation efforts.
Furthermore, systems us-ing these corpora and annotation as data willbe capable of handling larger and more variedcorpora.2.
The availability of the same or similar corporain a wide variety of languages;3.
The availability of corpora in a standard formatthat can be easily processed ?
there should bemechanisms in place to maintain the availabil-ity of corpora in this format in the future;4.
The ease in which the corpora can be obtainedby anyone who wants to process or annotatethem ?
corpora with free licenses or that are inthe public domain are preferred5.
The degree with which the corpora is represen-tative of text to be processed ?
this criterion canbe met if the corpora is diverse (1 above) and/orif more corpora of the same kind is available forprocessing.We have selected the following corpora for con-sideration:11.
The OANC: the Open sections of the ANC cor-pus.
These are the sections of the AmericanNational Corpus subject to the opened license,allowing them to be freely distributed.
The fullOpen ANC (Version 2.0) contains about 14.5megawords of American English and covers avariety of genres as indicated by the full path-names taken from the ANC distribution (wherea final 1 or 2 indicates which DVD the directoryoriginates from):1These corpora can be downloaded from:http://nlp.cs.nyu.edu/wiki/corpuswg/SharedCorpora?
spoken/telephone/switchboard?
written 1/fiction/eggan?
written 1/journal/slate?
written 1/letters/icic?
written 2/non-fiction/OUP?
written 2/technical/biomed?
written 2/travel guides/berlitz1?
written 2/travel guides/berlitz2?
written 1/journal/verbatim?
spoken/face-to-face/charlotte?
written 2/technical/911report?
written 2/technical/plos?
written 2/technical/government2.
The Controversial-Wikipedia-Corpus, a sectionof the Wikipedia XML corpus.
WikipediaXMLis a corpus derived from Wikipedia, convert-ing Wikipedia into an XML corpus suitablefor NLP processing.
This corpus was selectedfrom:?
Those articles cited as controversialaccording to the November 28, 2006version of the following Wikipedia page:http://en.wikipedia.org/wiki/Wikipedia:List of controversial issues?
The talk pages corresponding to these ar-ticles where Wikipedia users and the com-munity debate aspects of articles.
Thesedebates may be about content or editorialconsiderations.?
Articles in Japanese that are linked tothe English pages (and the associated talkpages) are also part of our corpus.2 American National CorpusThe American National Corpus (ANC) project (Ideand Macleod, 2001; Ide and Suderman, 2004) hasreleased over 20 million words of spoken and writ-ten American English, available from the Linguis-tic Data Consortium.
The ANC 2nd release con-sists of fiction, non-fiction, newspapers, technicalreports, magazine and journal articles, a substan-tial amount of spoken data, data from blogs andother unedited web sources, travel guides, techni-cal manuals, and other genres.
All texts are an-notated for sentence boundaries; token boundaries,185lemma, and part of speech produced by two differ-ent taggers ; and noun and verb chunks.
A sub-corpus of 10 million words reflecting the genre dis-tribution of the full ANC is currently being hand-validated for word and sentence boundaries, POS,and noun and verb chunks.
For a complete descrip-tion of the ANC 2nd release and its contents, seehttp://AmericanNationalCorpus.org.Approximately 65 percent of the ANC data is dis-tributed under an open license, which allows use andre-distribution of the data without restriction.
Theremainder of the corpus is distributed under a re-stricted license that disallows re-distribution or useof the data for commercial purposes for five yearsafter its release date, unless the user is a member ofthe ANC Consortium.
After five years, the data inthe restricted portions of the corpus are covered bythe open license.ANC annotations are distributed as stand-off doc-uments representing a set of graphs over the primarydata, thus allowing for layering of annotations andinclusion of multiple annotations of the same type.Because most existing tools for corpus access andmanipulation do not handle stand-off annotations,we have developed an easy-to-use tool and user in-terface to merge the user?s choice of stand-off anno-tations with the primary data to form a single docu-ment in any of several XML and non-XML formats,which is distributed with the corpus.
The ANC ar-chitecture and format is described fully in (Ide andSuderman, 2006).2.1 The ULA SubcorpusThe Unified Linguistic Annotation (ULA) projecthas selected a 40,000 word subcorpus of the OpenANC for annotation with several different annota-tion schemes including: the Penn Treebank, Prop-Bank, NomBank, the Penn Discourse Treebank,TimeML and Opinion Annotation.2 This initial sub-corpus can be broken down as follows:?
Spoken Language?
charlotte: 5K words?
switchboard: 5K words?
letters: 10K words2Other corpora being annotated by the ULA project includesections of the Brown corpus and LDC parallel corpora.?
Slate (Journal): 5K words?
Travel guides: 5K words?
911report: 5K words?
OUP books (Kaufman): 5K wordsAs the ULA project progresses, the participantsintend to expand the corpora annotated to include alarger subsection of the OANC.
They believe that thediversity of this corpus make it a reasonable testbedfor tuning annotation schemes for diverse modali-ties.
The Travel guides and some of the slate arti-cles have already been annotated by the FrameNetproject.
Thus the inclusion of these documents fur-thered the goal of producing a multiply annotatedcorpus by one additional project.It is the recommendation of this working groupthat: (1) other groups annotate these same subcor-pora; and (2) other groups choose additional corporafrom the OANC to annotate and publicly announcewhich subsections they choose.
We would be happyto put all such subsections on our website for down-load.
The basic idea is to build up a consensus ofwhat should be mutually annotated, in part, basedon what groups choose to annotate and to try to getannotation projects to gravitate toward multiply an-notated, freely available corpora.3 The WikipediaXML Corpus3.1 Why Wikipedia?The Wikipedia corpus consists of articles in a widerange of topics written in different genres andmainly (a) main pages are encyclopedia style arti-cles; and (b) talk pages are discussions about mainpages they are linked to.
The topics of these discus-sions range from editing contents to disagreementsabout content.
Although Wikipedia texts are mostlylimited to these two genres, we believe that it is wellsuited as training data for natural language process-ing because:1. they are lexically diverse (e.g., providing a lotof lexical information for statistical systems);2. the textual information is well structured3.
Wikipedia is a large and growing corpus1864.
the articles are multilingual (cf.
section 3.4)5. and the corpus has various other properties thatmany researchers feel would be interesting toexploit.To date research in Computational Linguistics us-ing Wikipedia includes: Automatic derivation oftaxonomy information (Strube and Ponzetto, 2006;Suchanek et al, 2007; Zesch and Gurevych, 2007;Ponzetto, 2007); automatic recognition of pairs ofsimilar sentences in two languages (Adafre and deRijke, 2006); corpus mining (Ru?diger Gleim andAlexander Mehler and Matthias Dehmer, 2007),Named Entity Recognition (Toral and noz, 2007;Bunescu and Pasc?a, 2007) and relation extraction(Nguyen et al, 2007).
In addition several sharedtasks have been set up using Wikipedia as the tar-get corpus including question answering (cf.
(D.Ahn and V. Jijkoun and G. Mishne and K. Mu?llerand M. de Rijke and S. Schlobach, 2004) andhttp://ilps.science.uva.nl/WiQA/); and informationretrieval (Fuhr et al, 2006).
Some other interest-ing properties of Wikipedia that have yet to be ex-plored to our knowledge include: (1) Most main ar-ticles have talk pages which discuss them ?
perhapsthis relation can be exploited by systems which tryto detect discussions about topics, e.g., searches fordiscussions about current events topics; (2) Thereare various meta tags, many of which are not in-cluded in the WikipediaXML (see below), but nev-ertheless are retrievable from the original HTMLfiles.
Some of these may be useful for various ap-plications.
For example, the levels of disputabil-ity of the content of the main articles is annotated(cf.
http://en.wikipedia.org/wiki/Wikipedia: Tem-plate messages/Disputes ).3.2 Why WikipediaXML?WikipediaXML (Denoyer and Gallinari, 2006) is anXML version of Wikipedia data, originally designedfor Information Retrieval tasks such as INEX (Fuhret al, 2006) and the XML Document Mining Chal-lenge (Denoyer and P. Gallinari, 2006).
Wikipedi-aXML has become a standard machine readableform for Wikipedia, suitable for most Computa-tional Linguistics purposes.
It makes it easy toidentify and read in the text portions of the doc-ument, removing or altering html and wiki codethat is difficult to process in a standard way.
TheWikipediaXML standard has (so far) been used toprocess Wikipedia documents written in English,German, French, Dutch, Spanish, Chinese, Arabicand Japanese.3.3 The Controversial Wikipedia CorpusThe English Wikipedia corpus is quite large (about800K articles and growing).
Frozen versions ofthe corpus are periodically available for download.We selected a 5 million word subcorpus whichwe believed would be good for a wide varietyof annotation schemes.
In particular, we chosearticles listed as being controversial (in the En-glish speaking world) according to the November28, 2006 version of the following Wikipediapage: http://en.wikipedia.org/wiki/Wikipedia:List of controversial issues.
We believed thatcontroversial articles would be more likely thanrandomly selected articles to: (1) include interestingdiscourse phenomena and emotive language; and(2) have interesting ?talk?
pages (indeed, some ofWikipedia pages have no associated talk pages).3.4 The Multi-linguality of WikipediaOne of the main good points of Wikipedia is the factthat it is a very large multilingual resource.
Thisprovides several advantages over single-languagecorpora, perhaps the clearest such advantage beingthe availability of same-genre/same-format text formany languages.
Although, Wikipedia in languagesother than English do not approach 800K articles insize, there are currently at least 14 languages withover 100K entries.It should be clear however, that it is definitely nota parallel corpus.
Although pages are sometimestranslated in their entirety, this is the exception, notthe rule.
Pages can be partially translated or summa-rized into the target language.
Individually writtenpages can be linked after they are created if it is be-lieved that they are about the same topic.
Also, ini-tially parallel pages can be edited in both languages,causing them to diverge.
We therefore decided todo a small small pilot study to attempt to charac-terize the degree of similarity between English arti-cles in Wikipedia and articles written in other lan-guages that have been linked.
There are 476 En-glish Wikipedia articles in the Controversial corpus187Classification FrequencyTotally Different 2Same General Topic 3Overlapping Topics 11Same Topics 33Parallel 1and 384 associated ?talk?
pages.
There are approxi-mately 10,000 articles of various languages that arelinked to the English articles.
We asked some En-glish/Japanese bilingual speakers to evaluate the de-gree of similarity of as many of the the 305 Japanesearticles that were linked to English controversial ar-ticles.
As of this date, 50 articles were evaluatedwith the results summarized as table 3.4.3 Thesepreliminary results suggest the following:?
Languate-linked Wikipedia would usually beclassified as ?comparable?
corpora as 34 (68%)of the articles were classified as covering thesame topics or being parallel.?
It may be possible to extract a parallel corpusfor a given pair of languages from Wikipedia.If the above sample is representative, approxi-mately 2% of the articles are parallel.
(Whilethe existance of one parallel article does notprovide statistically significant evidence that2% of Wikipedia is parallel, the article?s ex-istance is still significant.)
Furthermore, addi-tional parallel sentences may be extracted fromsome of the other comparable articles usingtechniques along the lines of (Adafre and de Ri-jke, 2006).Obviously, a more detailed study would be neces-sary to gain a more complete understanding of howlanguage-linked articles are related in Wikipedia.4Such a study would include characterizations of alllinked articles for several languages.
This studycould lead to some practical applications, e.g., (1)the creation of parallel subcorpora for a number oflanguages; (2) the selection of an English monolin-gual subcorpus consisting of articles, each of which3According to www.wikipedia.org there are currently over350K Japanese articles.4Long Wikipedia articles may be split into multiple articles.This can result in N to 1, or even N to N, matches betweenlanguage-linked articles if a topic is split in one language, butnot in another.is parallel to some article in some other language;etc.
; (3) A compilation of parallel sentences ex-tracted from comparable articles.
While parallelsubcorpora are of maximal utility, finding parallelsentences could still be extremely useful.
(Adafreand de Rijke, 2006) reports one attempt to automat-ically select parallel Dutch/English sentences fromlanguage-linked Wikipedia articles with an accuracyof approximately 45%.
Even if higher accuracy can-not be achieved, this still suggests that it is possibleto create a parallel corpus (of isolated sentences) us-ing a combination of automatic and manual means.A human translator would have to go through pro-posed parallel sentences and eliminate about onehalf of them, but would not have to do any man-ual translation.
Selection of corpora for annotationpurposes depends on a number of factors including:the type of annotation (e.g., a corpus of isolated sen-tences would not be appropriate for discourse anno-tation); and possibly an application the annotationis tuned for (e.g., Machine Translation, InformationExtraction, etc.
)It should be noted that the corpus was chosen forthe controversialness of its articles in the English-speaking community.
It should, however, not be ex-pected that the same articles will be controversialin other languages.
More generally, the language-linked Wikipedia articles may have different culturalcontexts depending on the language they are writtenin.
This is an additional feature that we could testin a wider study.
Furthermore, English pages aresomewhat special because they?re considered as thecommon platform and expected to be neutral to anycountry.
But other lanauages somewhat reflects theview of each country where the language is spoken.Indeed, some EN articles are labeled as USA-centric(cf.
http://en.wikipedia.org/wiki/Category:USA-centric).Finally, our choice of a corpus based on contro-versy may have not been the most efficient choiceif our goal had been specifically to find parallel cor-pora.
Just as choosing corpora of articles that arecontroversial (in the English-speaking world) mayhave helped finding articles interesting to annotateit is possible that some other choice, e.g., techni-cal articles, may have helped select articles likely188to be translated in full5 Thus further study may berequired to choose the right Wikipedia balance for aset of priorities agreed upon by the annotation com-munity.4 Legal IssuesThe American National Corpus has taken great painsto establish that the open subset of the corpus isfreely usable by the community.
The open license6makes it clear that these corpora can be used for anyreason and are freely distributable.In contrast, some aspects of the licensing agree-ment of corpora derived from Wikipedia are unclear.Wikipedia is governed by the GNU Free DocumentLicense which includes a provision that ?derivedworks?
are subject to this license as well.
Whilemost academic researchers would be uneffected bythis provision, the effect of this provision is unclearwith respect to commercial products.Under one view, a machine translation system thatuses a statistical model trained on Wikipedia corporais not derived from these corpora.
However, on an-other view it is derived.
We contacted Wikipediastaff by letter asking for clarification on this issueand received the following response from MichelleKinney on behalf of Wikipedia information team:Wikipedia does not offer legal advice,and therefore cannot help you decide howthe GNU Free Documentation License(GFDL) or any other free license appliesto your particular situation.
Please con-tact a local bar association, law society orsimilar association of jurists in your legaljurisdiction to obtain a referral to a com-petent legal professional.You may also wish to review the full textof the GFDL yourself:http://en.wikipedia.org/wiki/Wikipedia:Text of the GNU Free Documentation License5Informally, we observe that linked Japanese/English pairsof articles about abstract topics (e.g., Adultery, Agnosticsism,Antisemitism, Capitalism, Censorship, Catholicism) are lesslikely to contain parallel sentences than articles about specificevents or people (e.g., Adolf Hitler, Barbara Streisand, The LosAngeles Riots, etc.
)6http://projects.ldc.upenn.edu/ANC/ANC SecondReleaseEndUserLicense Open.htmWhile some candidate corpora are completely inthe public domain, e.g., political speeches and veryold documents, many candidate corpora are underthe GFDL or similar ?copyleft?
licenses.
These in-clude other licenses by the GNU organization andseveral Creative Commons licenses.
It is simply un-clear how copyleft licenses should be applied to cor-pora used as data in computational linguistics andwe believe that this is an important legal questionfor the Computational Linguistics community.
Inaddition to Wikipedia, this issue effects a wide vari-ety of corpora (e.g., other wiki corpora, some of thecorpora being developed by the American NationalCorpus, etc.
).However, getting such legal opinions is expensiveand has to be done carefully.
Hypothetically, sup-pose NYU?s legal department wrote an opinion let-ter stating that products that were not corpora them-selves were not to be considered derived works forpurposes of some list of copyleft licensing agree-ments.
Furthermore, let?s suppose that several anno-tation projects relied on this opinion and producedmillions of dollars worth of annotation for one suchcorpus.
Large corporations still might not use thesecorpora unless their own legal departments agreedwith NYU?s opinion.
For the annotation community,this could mean that certain annotation would onlybe used by academics and not by industry, and mostannotation researchers would not be happy with thisoutcome.
It therefore may be worth some efforton the part of whole NLP community to seek someclear determinations on this issue.5 Concluding RemarksThe working group selected two freely distributablecorpora for purposes of annotation.
Our goal was tochoose texts for annotation by multiple annotationresearch groups and describe the process and the pit-falls involved in selecting those texts.
We, further-more, aimed to establish a protocol for sharing texts,so that the same texts are annotated with multipleannotation schemes.
This protocol cannot be setupcarte blanche by this group of researchers.
Rather,we believe that our report in combination with thediscussion at the upcoming meeting of the Lingus-tic Annotation Workshop will provide the jumpstartnecessary for such a protocol to be put in place.189ReferencesSisay Fissaha Adafre and Maarten de Rijke.
2006.
Find-ing Similar Sentences across Multiple Languages inWikipedia.
In EACL 2006 Workshop: Wikis and blogsand other dynamic text source, Trento, Italy.Razvan Bunescu and Marius Pasc?a.
2007.
Using En-cyclopedic Knowledge for Named Entity Disambigua-tion.
In Proc.
of NAACL/HLT 2007.D.
Ahn and V. Jijkoun and G. Mishne and K. Mu?ller andM.
de Rijke and S. Schlobach.
2004.
Using Wikipediaat the TREC QA Track.
In Proc.
TREC 2004.Ludovic Denoyer and Patrick Gallinari.
2006.
TheWikipedia XML Corpus.
SIGIR Forum.L.
Denoyer and A. Vercoustre P. Gallinari.
2006.
Reporton the XML Mining Track at INEX 2005 and INEX2006 : Categorization and Clustering of XML Docu-ments.
In Advances in XML Information Retrieval andEvaluation: Fifthth Workshop of the INitiative for theEvaluation of XML Retrieval (INEX?06).N.
Fuhr, M. Lalmas, and S. Malik.
2006.
Advances inXML Information Retrieval and Evaluation.
In 5th In-ternational Workshop of the Initiative for the Evalua-tion of XML Retrieval, INEX 2006.N.
Ide and C. Macleod.
2001.
The american nationalcorpus: A standardized resource of american english.In Proceedings of Corpus Linguistics 2001, Lancaster,UK.N.
Ide and K. Suderman.
2004.
The american nationalcorpus first release.
In Proceedings of LREC 2004,pages 1681?1684, Lisbon, Portugal.N.
Ide and K. Suderman.
2006.
Integrating linguistic re-sources: The american national corpus model.
In Pro-ceedings of the 6th International Conference on Lan-guage Resources and Evaluation, Genoa, Italy.D.
P.T.
Nguyen, Y. Matsuo, and M. Ishizuka.
2007.
Sub-tree Mining for Relation Extraction from Wikipedia.In Proc.
of NAACL/HLT 2007.Simone Paolo Ponzetto.
2007.
Creating a KnowledgeBase From a Collaboratively Generated Encyclopedia.In Proc.
of NAACL/HLT 2007.Ru?diger Gleim and Alexander Mehler and MatthiasDehmer.
2007.
Web Corpus Mining by instance ofWikipedia.
In Proc.
2nd Web as Corpus Workshop atEACL 2006.M.
Strube and S. P. Ponzetto.
2006.
WikiRelate!
Com-puting semantic relatedness using Wikipedia.
In Proc.of AAAI-06, pages 1419?1424.F.
M. Suchanek, G. Kasneci, and G.Weikum.
2007.YAGO: A core of semantic knowledge.
In Proc.
ofWWW-07.Antonio Toral and Rafael Mu noz.
2007.
A proposal toautomatically build and maintain gazetteers for NamedEntity Recognition by using Wikipedia.
In Proc.
ofNAACL/HLT 2007.Torsten Zesch and Iryna Gurevych.
2007.
Analysis ofthe Wikipedia Category Graph for NLP Applications.In Proc of NAACL-HLT 2007 Workshop: TextGraphs-2.190
