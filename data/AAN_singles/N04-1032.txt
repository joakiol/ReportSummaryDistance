Shallow Semantic Parsing of ChineseHonglin Sun1Center for Spoken Language ResearchUniversity of Colorado at BoulderDaniel Jurafsky2Center for Spoken Language ResearchUniversity of Colorado at BoulderAbstractIn this paper we address the question of assigningsemantic roles to sentences in Chinese.
We showthat good semantic parsing results for Chinese canbe achieved with a small 1100-sentence training set.In order to extract features from Chinese, wedescribe porting the Collins parser to Chinese,resulting in the best performance currently reportedon Chinese syntactic parsing; we include our head-rules in the appendix.
Finally, we compare Englishand Chinese semantic-parsing performance.
Whileslight differences in argument labeling make aperfect comparison impossible, our resultsnonetheless suggest significantly betterperformance for Chinese.
We show that much ofthis difference is due to grammatical differencesbetween English and Chinese, such as theprevalence of passive in English, and the strictword order constraints on adjuncts in Chinese.1  IntroductionThematic roles (AGENT, THEME,  LOCATION, etc)provide a natural level of shallow semanticrepresentation for a sentence.
A number of algorithmshave been proposed for automatically assigning suchshallow semantic structure to English sentences.
Butlittle is understood about how these algorithms mayperform in other languages, and in general the role oflanguage-specific idiosyncracies in the extraction ofsemantic content and how to train these algorithmswhen large hand-labeled training sets are not available.In this paper we address the question of assigningsemantic roles to sentences in Chinese.
Our  work  is1 Currently at Department of Computer Science, QueensCollege, City University of New York.
Email: sunh@qc.edu.2 Currently at Department of Linguistics, Stanford University.Email: jurafsky@stanford.edu.based on the SVM-based algorithm proposed forEnglish by Pradhan et al(2003).
We first describe ourcreation of a small 1100-sentence Chinese corpuslabeled according to principles from the English and(in-progress) Chinese PropBanks.
We then introducethe features used by our SVM classifier, and show theirperformance on semantic parsing for both seen andunseen verbs, given hand-corrected (Chinese TreeBank)syntactic parses.
We then describe our port of theCollins (1999) parser to Chinese.
Finally, we apply ourSVM semantic parser to a matching English corpus,and discuss the differences between English andChinese that lead to significantly better performance onChinese.2  Semantic Annotation and the CorpusWork on semantic parsing in English has generallyrelated on the PropBank, a portion of the PennTreeBank in which the arguments of each verb areannotated with semantic roles.
Although a project toproduce a Chinese PropBank is underway (Xue andPalmer 2003), this data is not expected to be availablefor another year.
For these experiments, we thereforehand-labeled a small corpus following the PennChinese Propbank labeling guidelines (Xue, 2002).
Inthis section, we first describe the semantic roles weused in the annotation and then introduce the data forour experiments.2.1  Semantic rolesSemantic roles in the English (Kingsbury et al2002)and Chinese (Xue 2002) PropBanks are grouped intotwo major types:(1) arguments, which represent central participants inan event.
A verb may require one, two or morearguments and they are represented with a contiguoussequence of numbers prefixed by arg, as arg0, arg1.
(2) adjuncts, which are optional for an event but supplymore information about an event, such as time, location,reason, condition, etc.
An adjunct role is representedwith argM plus a tag.
For example, argM-TMP standsfor temporal, argM-LOC for location.In our corpus three argument roles and 15 adjunctroles appear.
The whole set of roles is given at Table 1.Table1  The list of semantic rolesRole FreqtrainFreqTestNotearg0  556 63arg1 872 91arg2  23 5argM-ADV 191 32 adverbialargM-BFY 26 2 beneficiary(e.g.
givesupport [to the plan])argM-CMP 35 3 object to be comparedargM-CND 14 1 conditionargM-CPN 7 3 companion (e.g.
talk[with you])argM-DGR 53 4 degreeargM-FRQ 3 0 frequencyargM-LOC 207 31 locationargM-MNR 10 1 mannerargM-PRP 11 0 purpose or reasonargM-RNG  7 2 range(e.g.
help you [inthis aspect])argM-RST    15 1 result(e.g.
increase [to$100])argM-SRC11 1 source(e.g.
increase[from $50] to $100)argM-TMP 376 45 temporalargM-TPC 12 2 topic2.2 The training and test setsWe created our training and test corpora by choosing10 Chinese verbs, and then selecting all sentencescontaining these 10 verbs from the 250K-word PennChinese Treebank 2.0.
We chose the 10 verbs byconsidering frequency, syntactic diversity, and wordsense.
We chose words that were frequent enough toprovide sufficient training data.
The frequencies of the10 verbs range from 41 to 230, with an average of 114.We chose verbs that were representative of the varietyof verbal syntactic behavior in Chinese, including verbswith one, two, and three arguments, and verbs withvarious patterns of argument linking.
Finally, we choseverbs that varied in their number of word senses.In total, we selected 1138 sentences.
The first authorthen labeled each verbal argument/adjunct in eachsentence with a role label.
We created our training andtest sets by splitting the data for each verb into twoparts: 90% for training and 10% for test.
Thus there are1025 sentences in the training set and 113 sentences inthe test set, and each test set verb has been seen in thetraining set.
The list of verbs chosen and their numberof senses, argument numbers and frequencies are givenin Table 2.Table 2    List of verbs for experimentsVerb # ofsensesArgnumberFreq?
?/set up 1 2 106?
?/emerge 1 1 80?
?/publish 1 2 113?
?/give 2 3/2 41?
?/build into 2 2/3 113?
?/enter 1 2 123?
?/take place 1 2 230?
?/pass 3 2 75?
?/hope 1 2 90?
?/increase 1 2 1673  Semantic Parsing3.1 Architecture and ClassifierFollowing the architecture of earlier semantic parserslike Gildea and Jurafsky (2002), we treat the semanticparsing task as a 1-of-N classification problem.
Foreach (non-aux/non-copula) verb in each sentence, ourclassifier examines each node in the syntactic parse treefor the sentence and assigns it a semantic role label.Most constituents are not arguments of the verb, and sothe most common label is NULL.
Our architecture isbased on a Support Vector Machine classifier,following Pradhan et al (2003).
Since SVMs are binaryclassifiers, we represent this 1-of-19 classificationproblem (18 roles plus NULL) by training 19 binaryone-versus-all classifiers.Following Pradhan et al (2003), we used tinySVMalong with YamCha (Kudo and Matsumoto 2000, 2001)as the SVM training and test software.
The systemuses a polynominal kernel with degree 2; the cost perunit violation of the margin, C=1; tolerance of thetermination criterion e=0.001.3.2 FeaturesThe literature on semantic parsing in English relies ona number of features extracted from the input sentenceand its parse.
These include the constituent?s syntacticphrase type, head word, and governing category, thesyntactic path in the parse tree connecting it to the verb,whether the constitutent is before or after the verb,  thesubcategorization bias of the verb, and the voice(active/passive) of the verb.
We investigated each ofthese features in Chinese; some acted quite similarly toEnglish, while others showed interesting differences.Features that acted similarly to English include thetarget verb, the  phrase type,  the syntactic category ofthe constituent.
(NP, PP, etc), and the subcategorizationof the target verb.
The sub-categorization featurerepresents the phrase structure rule for the verb phrasecontaining the target verb (e.g., VP -> VB NP, etc).Five features (path, position, governing category,headword, and voice) showed interesting patterns thatare discussed below.3.2.1 Path in the syntactic parse tree.
The pathfeature represents the path from a constituent to thetarget verb in the syntactic parse tree, using "^" forascending a parse tree, and "!"
for descending.
Thisfeature manifests the syntactic relationship between theconstituent and the target verb.
For example the path?NP^IP!VP!VP!VV?
indicates that the constituent is an?NP?
which is the subject of the predicate verb.
Ingeneral, we found the path feature to be sparse.
In ourtest set, 60% of path types and 39% of path tokens areunseen in the training.
The distributions of paths arevery uneven.
In the whole corpus, paths for roles havean average frequency of 14.5 while paths for non-roleshave an average of 2.7.
Within the role paths, a smallnumber of paths account for majority of the totaloccurrences; among the 188 role path types, the top 20paths account for 86% of the tokens.
Thus, althoughthe path feature is sparse, its sparsity may not be amajor problem in role recognition.
Of the 291 roletokens in our test set, only 9 have unseen paths, i.e.,most of the unseen paths are due to non-roles.Table 3   The positional distribution of rolesRoleBeforeverbAfterverbTotalarg0arg1arg2argM-ADVargM-BFYargM-CMPargM-CNDargM-CPNargM-DGRargM-FRQargM-LOCargM-MNRargM-PRPargM-RNGargM-RSTargM-SRCargM-TMPargM-TPC54731922328381510233111191240814726442857351613619963282232838151057323811119161242114Total187883827163.2.2 Position before or after the verb.
The positionfeature indicates that a constituent is before or after thetarget verb.
In our corpus, 69% of the roles are beforethe verb while 31% are after the verb.
As in English,the position is a useful cue for role identity.
Forexample, 88% of arg0s are before the verb, 67% ofarg1s are after the verb and all the arg2s are after theverb.
Adjuncts have even a stronger bias.
Ten of theadjunct types can only occur before the verb, whilethree are always after the verb.
The two most commonadjunct roles, argM-LOC and argM-TMP are almostalways before the verb, a sharp difference from English.The details are shown seen in Table 3.3.2.3 Governing Category.
The governing categoryfeature is only applicable for NPs.
In the originalformulation for English in Gildea and Jurafsky (2002),it answers the question: Is the NP governed by IP orVP?
An NP governed by an IP is likely to be a subject,while an NP governed by a VP is more likely to be anobject.
For Chinese, we added a third option in whichthe governing category of an NP is neither IP nor VP,but an NP.
This is caused by the ?DE?
construction, inwhich a clause is used as a modifier of an NP.
Forinstance, in the example indicated in Figure 1, for thelast NP, ??????????
(?international Olympicconference?)
the parent node is NP, from where it goesdown to the target verb ????
(?taking place?
).NPCPVP                                                        NPDEC?????
?
???????
?in Paris take place          DE      intl Olympic conf.
?the international Olympic Conference held in Paris?Figure 1  Example of  DE constructionSince the governing category information is encoded inthe path feature, it may be redundant; indeed thisredundancy might explain why the governing categoryfeature was used in Gildea & Jurafsky(2002) but not inGildea and Palmer(2002).
Since the ?DE?
constructioncaused us to modify the feature for Chinese, weconducted several experiments to test whether thegoverning category feature is useful or whether it isredundant with the path and position features.
Usingthe paradigm to be described in section 3.4, we found asmall improvement using governing category, and sowe include it in our model.3.2.4 Head word and its part of speech.
The headword is a useful but sparse feature.
In our corpus, of the2716 roles, 1016 head words (type) are used, in which646 are used only once.
The top 20 words are given inTable 4.Table 4   Top 20 head words for rolesWord Freq Word Freq?/in  214 ?
?/China 25?
?/meeting 43 ?/for 23?
?/today 41 ?
?/statement 19?/at 40 ?
?/speech 18?/already 38 ?
?/stage 17?
?/enterprise 35 ?
?/government 16?
?/company 32 ?
?/present 16?/than  31 ?
?/bank 15?/will  30 ?
?/recently 14??/ceremony28?
?/base 14In the top 20 words, 4 are prepositions (??/in??/at??/than??/for?)
and 3 are temporal nouns(???
/today???
/present???
/recently?)
and 2 areadverbs(??
/already, ?
/will?).
These closed classwords are highly correlated with specific semanticroles.
For example,"?/for" occurs 195 times as thehead of a constituent, of which 172 are non-roles, 19are argM-BFYs, 3 are arg1s and 1 is an argM-TPC.
"?/in" occurs 644 times as a head, of which 430 are non-roles, 174 are argM-LOCs, 24 are argM-TMPs, 9 areargM-RNGs, and 7 are argM-CND.
"?
/already"occurs 135 times as a head, of which 97 are non-rolesand 38 are argM-ADVs.
"?
?/today" occurs 69 timesas a head, of which 41 are argM-TMPs and 28 are non-roles.Within the open class words, some are closelycorrelated to the target verb.
For example, "?
?/meeting; conference" occurs 43 times as a head forroles, of which 24 are for the target "?
?/take place"and 19 for "??/pass".
"?
?/ceremony" occurs 28times and all are arguments of "??
"(take place)."?
?/statement" occurs 19 times, 18 for "?
?/release;publish" and one for "?
?/hope".These statistics emphasize the key role of thelexicalized head word feature in capturing thecollocation between verbs and their arguments.
Due tothe sparsity of the head word feature, we also use thepart-of-speech of the head word, following Surdeanu etal (2003).
For example, ?7?
26?/July 26?
may notbe seen in the training,  but its POS, NT(temporalnoun) , is a good indicator that it is a temporal.3.2.5  Voice.
The passive construction in English givesinformation about surface location of arguments.
InChinese the marked passive voice is indicated by theuse of the preposition "?/by" (POS tag LB in PennChinese Treebank).
This passive, however, is seldomused in Chinese text.
In our entire 1138-sentencecorpus, only 13 occurrences of "LB" occur, and onlyone (in the training set) is related to the target verb.Thus we do not use the voice feature in our system.3.3 Experimental Results for Seen VerbsWe now test the performance of our classifier, trainedon the 1025-sentence training set and tested on the 113-sentence test set introduced in Section 2.2.
Recall thatin this ?stratified?
test set, each verb has been seen inthe training data.
The last row in Table 5 shows thecurrent best performance of our system on this test set.The preceding rows show various subsets of the featureset, beginning with the path feature.Table 5  Semantic parsing results on seen verbsfeature set                             P              R           F(%)         (%)        (%)path                                     71.8        59.4       65.0path + pt                              72.9        62.9       67.5path + position                    72.5  60.8  66.2path + head POS                 77.6  63.3        69.7path + sub-cat                      80.8       63.6        71.2path + head word                 85.0       66.0  74.3path + target verb                85.8  68.4  76.1path + pt + gov + position+ subcat + target+ head word+ head POS                  91.7       76.0        83.1As Table 5 shows, the most important feature is path,followed by target verb and head word.
In general, thelexicalized features are more important than the otherfeatures.
The combined feature set outperforms anyother feature sets with less features and it has an F-score of 83.1.
The performance is better for thearguments (i.e., only ARG0-2), 86.7 for arg0 and 89.4for arg1.3.4 Experimental Results for Unseen VerbsTo test the performance of the semantic parser onunseen verbs, we used cross-validation, selecting oneverb as test and the other 9 as training, and iteratingwith each verb as test.
All the results are given in Table6.
The results for some verbs are almost equal to theperformance on seen verbs.
For example for ???
?and ???
?, the F-scores are over 80.
However, forsome verbs, the results are much worse.
The worst caseis the verb ???
?, which has an F-score of 11.
This isdue to the special syntactic characteristics of this verb.This verb can only have one argument and thisargument most often follows the verb, in objectposition.
In the surface structure, there is often an NPbefore the verb working as its subject, but semanticallythis subject cannot be analyzed as arg0.
For example:(1)?
?/China ?/not ?/will ?
?/emerge ??/food??/crisis.
(A food crisis won't emerge in China.)(2)?
?/Finland ?
?/economy ?
?/emerge  ?/AUX??
/post-war ?
/most ??
/serious ?
/AUX ??/depression.
(The most severe post-war depressionemerged  in the Finland economy.
)The subjects, ???/China?
in (1) and ???/Finland?
?/economy?, are locatives, i.e.
argM-LOC, and theobjects, ??
?/food ??/crisis?
in (1) and ??
?/post-war ?/most ?
?/serious ?/AUX ??/depression?
in(2), are analyzed as arg0.
But the parser classified thesubjects as arg0 and the objects as arg1.
These arecorrect for most common verbs but wrong for thisparticular verb.
It is difficult to know how common thisproblem would be in a larger, test set.
The fact that weconsidered diversity of syntactic behavior whenselecting verbs certainly helps make this test set reflectthe difficult cases.If most verbs prove not to be as idiosyncratic as ??
?/emerge?, the real performance of the parser on unseenverbs may be better than the average given here.Table 6   Experimental Results for Unseen Verbstarget               P(%) R(%) F(%)?
?/publish 90.7 72.9 80.8?
?/increase 49.6 34.3 40.5?
?/take place 90.1 63.3 74.4?
?/build into 65.2 55.5 60.0?
?/give 65.7 37.9 48.1?
?/pass 85.9 77.0 81.2?
?/emerge 12.6 10.2 11.3?
?/enter 81.9 58.8 68.4?
?/set up 79.0 61.1 68.9?
?/hope 77.7 35.9 49.1Average          69.8 50.7 58.3Another important difficulty in processing unseenverbs is the fact that roles in PropBank are defined in averb-dependent way.
This may be easiest to see with anEnglish example.
The roles arg2, arg3, arg4 havedifferent meaning for different verbs; underlined in thefollowing are some examples of arg2:(a) The state gave  CenTrust 30 days to sell the Rubens.
(b) Revenue increased 11 to 2.73 billion from 2.46billion.
(c) One of Ronald Reagan 's attributes as President wasthat he rarely gave his blessing to the claptrap thatpasses for consensus in various internationalinstitutions.In (a), arg2 represents the goal of ?give?, in (b), itrepresents the amount of increase, and in (c) itrepresents yet another role.
These complete differentsemantic relations are given the same semantic label.For unseen verbs, this makes it difficult for thesemantic parser to know what would count as an arg2.4 Using Automatic ParsesThe results in the last section are based on the use ofperfect (hand-corrected) parses drawn from the PennChinese Treebank.
In practical use, of course,automatic parses will not be as accurate.
In this sectionwe describe experiments on semantic parsing whengiven automatic parses produced by an automaticparser, the Collins (1999) parser, ported to Chinese.We first describe how we ported the Collins parser toChinese and then present the results of the semanticparser with features drawn from the automatic parses.4.1 The Collins parser for ChineseThe Collins parser is a state-of-the-art statistical parserthat has high performance on English (Collins, 1999)and Czech(Collins et al 1999).
There have beenattempts in applying other algorithms in Chineseparsing (Bikel and Chiang, 2000; Chiang and Bikel2002; Levy and Manning 2003), but there has been noreport on applying the Collins parser on Chinese.The Collins parser is a lexicalized statistical parserbased on a head-driven extended PCFG model; thus thechoice of head node is crucial to the success of theparser.
We analyzed the Penn Chinese Treebank dataand worked out head rules for the Chinese Treebankgrammar (we were unable to find any published headrules for Chinese in the literature).
There are two majordifferences in the head rules between English andChinese.
First, NP heads in Chinese are rigidlyrightmost, that is to say, no modifiers of an NP canfollow the head.
In contrast, in English a modifier mayfollow the head.
Second, just as with NPs in Chinese,the head of ADJP is rigidly rightmost.
In English, bycontrast, the head of an ADJP is mainly the leftmostconstituent.
Our head rules for the Chinese Treebankgrammar are given in the Appendix.In addition to the head rules, we modified the POS tagsfor all punctuation.
This is because all cases ofpunctuation in the Penn Chinese Treebank are assignedthe same POS tag ?PU?.
The Collins parser, on theother hand, expects the punctuation tags in the EnglishTreeBank format, where the tag for a punctuation markis the punctuation mark itself.
We therefore replacedthe POS tags for all punctuation marks in the Chinesedata to conform to the conventions in English.Finally, we made one further augmentation also relatedto punctuation.
Chinese has one punctuation mark thatdoes not exist in English.
This commonly used mark,?semi-stop?, is used in Chinese to link coordinateswithin a sentence (for example between elements of alist).
This function is represented in English by acomma.
But the comma in English is ambiguous; inaddition to its use in coordination and lists, it can alsorepresent the end of a clause.
In Chinese, by contrastthe semi-stop has only the conjunction/list function.Chinese thus uses the regular comma only forrepresenting clause boundaries.
We investigated twoways to model the use of the Chinese semi-stop: (1)just converting the semi-stop to the comma, thusconflating the two functions as in English; and (2) bygiving the semi-stop the POS tag ?CC?, a conjunction.We compared parsing results with these two methods;the latter (conjunction) method gained 0.5% netimprovement in F-score over the former one.
Wetherefore include it in our Collins parser port.We trained the Collins parser on the Penn ChineseTreebank(CTB) Release 2 with 250K words, firstremoving from the training set any sentences that occurin the test set for the semantic parsing experiments.
Wethen tested on the test set used in the semantic parsingwhich includes 113 sentences(TEST1).
The results ofthe syntactic parsing on the test set are shown in Table7.Table 7     Results for syntactic parsing, trained onCTB Release 2, tested on test set in semantic parsingLP(%) LR(%) F1(%)overall            81.6 82.1 81.0len<=40          86.1 85.5 86.7To compare the performance of the Collins parser onChinese with those of other parsers, we conducted anexperiment in which we used the same training and testdata (Penn Chinese Treebank Release 1, with 100Kwords) as used in those reports.
In this experiment, weused articles 1-270 for training and 271-300 astest(TEST2).
Table 8 shows the results and thecomparison with other parsers.Table 8 only shows the performance on sentences ?
40words.
Our performance on all the sentences TEST2 isP/R/F=82.2/83.3/82.7.
It may seem surprising that theoverall F-score on TEST2 (82.7) is higher than theoverall F-score on TEST1 (81.0) despite the fact thatour TEST1 system had more than twice as muchtraining as our TEST2 system.
The reason lies in themakeup of the two test sets; TEST1 consists ofrandomly selected long sentences; TEST2 consists ofsequential text, including many short sentences.
Theaverage sentence length in TEST1 is 35.2 words, vs.22.1 in TEST2.
TEST1 has 32% long sentences (>40words) while TEST2 has only 13%.Table 8      Comparison with other parsers: TEST2?
40 wordsLP(%) LR(%) F1(%)Bikel & Chiang 2000      77.2 76.2 76.7Chiang & Bikel 2002      81.1 78.8 79.9Levy & Manning 2003   78.4 79.2 78.8Collins parser                86.4 85.5 85.94.2 Semantic parsing using Collins parsesIn the test set of 113 sentences, there are 3 sentences inwhich target verbs are given the wrong POS tags, sothey can not be used for semantic parsing.
For theremaining 100 sentences, we used the feature setcontaining eight features (path, pt, gov,  position,subcat, target, head word and head POS) , the same asthat used in the experiment on perfect parses.
Theresults are shown in Table 9.Table 9  Result for semantic parsing using automaticsyntactic parsesP(%) R(%) F(%)110 sentences 86.0 70.8 77.6113 sentences 86.0 69.2 76.7Compared to the F-score using hand-correctedsyntactic parses from the TreeBank, using automaticparses decreases the F-score by 6.4.5  Comparison with EnglishRecent research on English semantic parsing hasachieved quite good results by relying on the largeamounts of training data available in the Propbank andFramenet (Baker et al 1998) databases.
But inextending the semantic parsing approach to otherlanguages, we are unlikely to always have large datasets available.
Thus it is crucial to understand howsmall amounts of data affect semantic parsing.
At thesame time, there have been no comparisons betweenEnglish and other languages with respect to semanticparsing.
It is thus not clear what language-specificissues may arise in general with the automatic mappingof syntactic structures to semantic relations.
In thissection, we compare English and Chinese by using thesame semantic parser, similar verbs and similaramounts of data.
Our goals are two-folds: (1) tocompare the performance of the parser on English andChinese; and (2) to understand differences betweenEnglish and Chinese that affect automatic mappingbetween syntax and semantics.
At first, we introducethe data used in the experiments and then we  presentthe results and give analysis.5.1 The English dataIn order to create an English corpus which matched oursmall Chinese corpus, we selected 10 English verbswhich corresponded to our 10 Chinese verbs inmeaning and frequency; exact translations of theChinese when possible, or the closest possible wordwhen an extract translation did not exist.
The Englishverbs and their Chinese correspondents are given inTable 10.Table 10   English verbs chosen for experimentsEnglish   Freq  Chinese English Freq Chinesebuild 46 ??
hold 120 ?
?emerge 30 ??
hope 63 ?
?enter 108 ??
increase 231 ?
?found 248 ??
pass 143 ?
?give 124 ??
publish 77 ?
?Table 12       The comparison between adjuncts in English and ChineseEnglish ChineseRole BeforeverbAfterverbFreq intestP     R     F(%)BeforeverbAfterverbFreq intestP      R       F(%)argM-ADV 22 43 5 0      0      0 223 0 37 91.3    56.8   70argM-LOC 25 82 11 80   36.4   50 233 5 31 90.0    87.1  88.5argM-MNR 22 75 14 0      0      0 11 0 1 0        0        0argM-TMP 119 164 37 66.7   27    38.5 408 13 44 96.7   65.9   78.4After the verbs were chosen, we extracted everysentence containing these verbs from section 02 tosection 21 of the Wall Street Journal data from thePenn English Propbank.
The number of sentences foreach verb is given in Table 10.5.2 Experimental ResultsAs in our Chinese experiments, we used our SVM-based classifier, using N one-versus-all classifiers.Table 11 shows the performance on our English test set(with Chinese for comparison), beginning with the pathfeature, and incrementally adding features until in thelast row we combine all 8 features together.Table 11       Experimental results of EnglishChinese Englishfeature set R/F/P P/R/Fpath 71.8/59.4/65.0 78.2/48.3/59.7path + pt 72.9/62.9/67.5 77.4/51.2/61.6path + position 72.5/60.8/66.2 75.7/50.9/60.8path + hd POS 77.6/63.3/69.7 79.1/49.7/61.0path + sub-cat 80.8/63.6/71.2 79.9/45.3/57.8path + hd word 85.0/66.0/74.3 84.0/47.7/60.8path + target 85.8/68.4/76.1 85.7/49.1/62.5COMBINED 91.7/76.0/83.1 84.1/62.2/71.5It is immediately clear from Table 11 that using similarverbs, the same amount of data, the same classifier, thesame number of roles, and the same features, theresults from English are much worse than those forChinese.
While some part of the difference is probablydue to idiosyncracies of particular sentences in theEnglish and Chinese data, other aspects of thedifference might be accounted for systematically, as wediscuss in the next section.5.3 Discussion: English/Chinese differencesWe first investigated whether the differences betweenEnglish and Chinese could be attributed to particularsemantic roles.
We found that this was indeed the case.The great bulk of the error rate difference betweenEnglish and Chinese was caused by the 4 adjunctclasses argM-ADV, argM-LOC, argM-MNR, andargM-TMP, which together account for 19.6% of therole tokens in our English corpus.
The average F-scorein English for the four roles is 36.7, while in Chinesethe F-score for the four roles is 78.6.
Why should theseroles be so much more difficult to identify in Englishthan Chinese?
We believe the answer lies in theanalysis of the position feature in section 3.2.2.
This isrepeated, with error rate information in Table 12.
Wesee there that adjuncts in English have no strongpreference for occurring before or after the verb.Chinese adjuncts, by contrast, are well-known to havean extremely strong preference to be preverbal, asTable 12 shows.
The relatively fixed word order ofadjuncts makes it much easier in Chinese to map theseroles from surface syntactic constituents than inEnglish.If the average F-score of the four adjuncts in English israised to the level of that in Chinese, the overall F-score on English would be raised from 71.5 to 79.7,accounting for 8.2 of the 11.6 difference in F-scoresbetween the two languages.We next investigated the one feature from our originalEnglish-specific feature set that we had dropped in ourChinese system: passive.
Recall that we dropped thisfeature because marked passives are extremely rare inChinese.
When we added this feature back into ourEnglish system, the performance rose fromP/R/F=84.1/62.2/71.5 to 86.4/65.1/74.3.
As might beexpected, this effect of voice is mainly reflected in animprovement on arg0 and arg1, as Table 13 showsbelow:Table 13.
Improvement in English semantic parsingwith the addition of the voice feature-voice +voiceP      R      F P      R        Farg0 88.9  75.3  81.5 94.4   80     86.6arg1 86.5  82.8  84.6 88.5  86.2   87.3A third source of English-Chinese differences is thedistribution of roles; the Chinese data hasproportionally more adjuncts (ARGMs), while theEnglish data has proportionally more obliquearguments (ARG2, ARG3, ARG4).
Oblique argumentsare more difficult to process than other arguments, aswas discussed in section 3.4.
This difference is mostlikely to be caused by labeling factors rather than bytrue structural differences between English in Chinese.In summary, the higher performance in our Chinesesystem is due to 3 factors: the importance of passive inEnglish; the strict word-order constraints of Chineseadverbials, and minor labeling differences.6  ConclusionsWe can draw a number of conclusions from ourinvestigation of semantic parsing in Chinese.
First,reasonably good performance can be achieved with avery small (1100 sentences) training set.
Second, thefeatures that we extracted for English semantic parsingworked well when applied to Chinese.
Many of thesefeatures required creating an automatic parse; in doingso we showed that the Collins (1999) parser whenported to Chinese achieved the best reportedperformance on Chinese syntactic parsing.
Finally, weshowed that semantic parsing is significantly easier inChinese than in English.
We show that thiscounterintuitive result seems to be due to the strictconstraints on adjunct ordering in Chinese, makingadjuncts easier to find and label.AcknowledgementsThis work was partially supported by the NationalScience Foundation via a KDD Supplement to NSFCISE/IRI/Interactive Systems Award  IIS-9978025.Many thanks to Ying Chen for her help on the Collinsparser port, and to Nianwen Xue and Sameer Pradhanfor providing the data.
Thanks to Kadri Hacioglu,Wayne Ward, James Martin, Martha Palmer, and threeanonymous reviewers for helpful  advice.Appendix: Head rules for ChineseParent     Direction          Priority ListADJP      Right        ADJP  JJ  ADADVP     Right        ADVP AD CS JJ NP PP P VA VVCLP         Right       CLP  M  NN  NPCP           Right        CP  IP  VPDNP        Right        DEG   DNP  DEC   QPDP           Left          M(r)   DP  DT  ODDVP        Right        DEV  AD  VPIP            Right        VP  IP  NPLCP        Right        LCP  LCLST        Right        CD  NP  QPNP          Right        NP  NN  IP  NR  NTPP           Left          P   PPPRN        Left          PUQP           Right       QP  CLP  CDUCP        Left          IP  NP  VPVCD       Left          VV  VA  VEVP          Left          VE VC VV VNV VPT VRDVSB VCD VPVPT         Left         VA  VVVRD        Left         VVl VAVSB         Right      VV  VEReferencesBaker, Collin F., Charles J. Fillmore, and John B.Lowe.
1998.
The Berkekey FrameNet Project.
InProceeding of COLING/ACL.Bikel, Daniel and David Chiang.
2000.
Two StatisticalParsing models Applied to the Chinese Treebank.
InProceedings of the Second Chinese LanguageProcessing Workshop, pp.
1-6.Chiang, David and Daniel Bikel.
2002.
RecoveringLatent Information in Treebanks.
In Proceedings ofCOLING-2002, pp.183-189.Collins, Michael.
1999.
Head-driven Statistical Modelsfor Natural Language Parsing.
Ph.D. dissertation,University of Pennsylvannia.Collins, Michael, Jan Hajic, Lance Ramshaw andChristoph Tillmann.
1999.
A Statistical Parser forCzech.
In Proceedings of the 37th Meeting of the ACL,pp.
505-512.Gildea, Daniel and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
ComputationalLinguistics, 28(3):245-288.Gildea, Daniel and Martha Palmer.
2002.
TheNecessity of Parsing for Predicate ArgumentRecognition, In Proceedings of the 40th Meeting of theACL, pp.
239-246.Kingsbury, Paul, Martha Palmer, and Mitch Marcus.2002.
Adding semantic annotation to the PennTreebank.
In Proceedings of HLT-02.Kudo, Taku and Yuji Matsumoto.
2000.
Use of supportvector learning for chunk Identification.
InProceedings of the 4th Conference on CoNLL, pp.142-144.Kudo, Taku and Yuji Matsumoto.
2001 Chunking withSupport Vector Machines.
In Proceeding of the 2ndMeeting of the NAACL.
pp.192-199.Levy, Roger and Christopher Manning.
2003.
Is itharder to parse Chinese, or the Chinese Treebank?ACL 2003, pp.
439-446.Pradhan, Sameer, Kadri Hacioglu,.
Wayne Ward,James Martin, and Daniel Jurafsky.
2003.
?SemanticRole Parsing: Adding Semantic Structure toUnstructured Text?.
In the Proceedings of theInternational Conference on Data Mining (ICDM-2003), Melbourne, FL, 2003Surdeanu, Mihai, Sanda Harabagiu, John Williams andPaul Aarseth.
2003.
Using Predicate-ArgumentStructures for Information Extraction, In Proceedingsof ACL.Xue, Nianwen.
2002.
Guidelines for the Penn ChineseProposition Bank (1st Draft), UPenn.Xue, Nianwen, Fu-Dong Chiou and Martha Palmer.2002.
Building a large-scale annotated Chinese corpus.In Proceedings of COLING-2002.Xue, Nianwen, Martha Palmer.
2003.
Annotating thepropositions in the Penn Chinese Treebank.
InProceedings of the 2nd SIGHAN Workshop on ChineseLanguage Processing.
