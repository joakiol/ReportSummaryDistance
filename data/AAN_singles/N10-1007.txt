Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 55?63,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsQme!
: A Speech-based Question-Answering system on Mobile DevicesTaniya MishraAT&T Labs-Research180 Park AveFlorham Park, NJtaniya@research.att.comSrinivas BangaloreAT&T Labs-Research180 Park AveFlorham Park, NJsrini@research.att.comAbstractMobile devices are becoming the dominantmode of information access despite beingcumbersome to input text using small key-boards and browsing web pages on smallscreens.
We present Qme!, a speech-basedquestion-answering system that allows forspoken queries and retrieves answers to thequestions instead of web pages.
We presentbootstrap methods to distinguish dynamicquestions from static questions and we showthe benefits of tight coupling of speech recog-nition and retrieval components of the system.1 IntroductionAccess to information has moved from desktop andlaptop computers in office and home environmentsto be an any place, any time activity due to mo-bile devices.
Although mobile devices have smallkeyboards that make typing text input cumbersomecompared to conventional desktop and laptops, theability to access unlimited amount of information,almost everywhere, through the Internet, using thesedevices have made them pervasive.Even so, information access using text input onmobile devices with small screens and soft/smallkeyboards is tedious and unnatural.
In addition, bythe mobile nature of these devices, users often liketo use them in hands-busy environments, ruling outthe possibility of typing text.
We address this issueby allowing the user to query an information repos-itory using speech.
We expect that spoken languagequeries to be a more natural and less cumbersomeway of information access using mobile devices.A second issue we address is related to directlyand precisely answering the user?s query beyondserving web pages.
This is in contrast to the currentapproach where a user types in a query using key-words to a search engine, browses the returned re-sults on the small screen to select a potentially rele-vant document, suitably magnifies the screen to viewthe document and searches for the answer to herquestion in the document.
By providing a methodfor the user to pose her query in natural language andpresenting the relevant answer(s) to her question, weexpect the user?s information need to be fulfilled ina shorter period of time.We present a speech-driven question answeringsystem, Qme!, as a solution toward addressing thesetwo issues.
The system provides a natural inputmodality ?
spoken language input ?
for the usersto pose their information need and presents a col-lection of answers that potentially address the infor-mation need directly.
For a subclass of questionsthat we term static questions, the system retrievesthe answers from an archive of human generated an-swers to questions.
This ensures higher accuracyfor the answers retrieved (if found in the archive)and also allows us to retrieve related questions onthe user?s topic of interest.
For a second subclass ofquestions that we term dynamic questions, the sys-tem retrieves the answer from information databasesaccessible over the Internet using web forms.The layout of the paper is as follows.
In Section 2,we review the related literature.
In Section 3, weillustrate the system for speech-driven question an-swering.
We present the retrieval methods we usedto implement the system in Section 4.
In Section 5,we discuss and evaluate our approach to tight cou-pling of speech recognition and search components.In Section 6, we present bootstrap techniques to dis-tinguish dynamic questions from static questions,and evaluate the efficacy of these techniques on atest corpus.
We conclude in Section 7.2 Related WorkEarly question-answering (QA) systems, such asBaseball (Green et al, 1961) and Lunar (Woods,1973) were carefully hand-crafted to answer ques-tions in a limited domain, similar to the QAcomponents of ELIZA (Weizenbaum, 1966) andSHRDLU (Winograd, 1972).
However, there hasbeen a resurgence of QA systems following theTREC conferences with an emphasis on answeringfactoid questions.
This work on text-based question-answering which is comprehensively summarized55in (Maybury, 2004), range widely in terms of lin-guistic sophistication.
At one end of the spectrum,There are linguistically motivated systems (Katz,1997; Waldinger et al, 2004) that analyze the user?squestion and attempt to synthesize a coherent an-swer by aggregating the relevant facts.
At the otherend of the spectrum, there are data intensive sys-tems (Dumais et al, 2002) that attempt to use theredundancy of the web to arrive at an answer forfactoid style questions.
There are also variants ofsuch QA techniques that involve an interaction anduse context to resolve ambiguity (Yang et al, 2006).In contrast to these approaches, our method matchesthe user?s query against the questions in a large cor-pus of question-answer pairs and retrieves the asso-ciated answer.In the information retrieval community, QA sys-tems attempt to retrieve precise segments of a doc-ument instead of the entire document.
In (To-muro and Lytinen, 2004), the authors match theuser?s query against a frequently-asked-questions(FAQ) database and select the answer whose ques-tion matches most closely to the user?s question.An extension of this idea is explored in (Xue et al,2008; Jeon et al, 2005), where the authors match theuser?s query to a community collected QA archivesuch as (Yahoo!, 2009; MSN-QnA, 2009).
Our ap-proach is similar to both these lines of work in spirit,although the user?s query for our system originatesas a spoken query, in contrast to the text queries inprevious work.
We also address the issue of noisyspeech recognition and assess the value of tight in-tegration of speech recognition and search in termsof improving the overall performance of the system.A novelty in this paper is our method to address dy-namic questions as a seamless extension to answer-ing static questions.Also related is the literature on voice-search ap-plications (Microsoft, 2009; Google, 2009; Yellow-Pages, 2009; vlingo.com, 2009) that provide a spo-ken language interface to business directories andreturn phone numbers, addresses and web sites ofbusinesses.
User input is typically not a free flowingnatural language query and is limited to expressionswith a business name and a location.
In our system,users can avail of the full range of natural languageexpressions to express their information need.And finally, our method of retrieving answers todynamic questions has relevance to the database andmeta search community.
There is growing interestin this community to mine the ?hidden?
web ?
infor-mation repositories that are behind web forms ?
andprovide a unified meta-interface to such informa-tion sources, for example, web sites related travel,or car dealerships.
Dynamic questions can be seenas providing a natural language interface (NLI) tosuch web forms, similar to early work on NLI todatabases (Androutsopoulos, 1995).3 Speech-driven Question RetrievalSystemWe describe the speech-driven query retrieval appli-cation in this section.
The user of this applicationprovides a spoken language query to a mobile deviceintending to find an answer to the question.
Someexample users?
inputs are1 what is the fastest ani-mal in water, how do I fix a leaky dishwasher, whyis the sky blue.
The result of the speech recognizeris used to search a large corpus of question-answerpairs to retrieve the answers pertinent to the user?sstatic questions.
For the dynamic questions, the an-swers are retrieved by querying a web form fromthe appropriate web site (e.g www.fandango.com formovie information).
The result from the speech rec-ognizer can be a single-best string or a weightedword lattice.2 The retrieved results are ranked usingdifferent metrics discussed in the next section.
InFigure 2, we illustrate the answers that Qme!returnsfor static and dynamic quesitons.Lattice1?bestQ&A corpusASRSpeechDynamicClassifyfrom WebRetrieveRankSearchRanked ResultsMatchFigure 1: The architecture of the speech-driven question-answering system4 Methods of RetrievalWe formulate the problem of answering staticquestions as follows.
Given a question-answerarchive QA = {(q1, a1), (q2, a2), .
.
.
, (qN , aN )}1The query is not constrained to be of any specific questiontype (for example, what, where, when, how).2For this paper, the ASR used to recognize these utterancesincorporates an acoustic model adapted to speech collectedfrom mobile devices and a four-gram language model that isbuilt from the corpus of questions.56Figure 2: Retrieval results for static and dynamic ques-tions using Qme!of N question-answer pairs, and a user?s ques-tion qu, the task is to retrieve a subset QAr ={(qr1, ar1), (qr2, ar2), .
.
.
, (qrM , arM )} M << N us-ing a selection function Select and rank the mem-bers of QAr using a scoring function Score suchthat Score(qu, (qri , ari )) > Score(qu, (qri+1, ari+1)).Here, we assumeScore(qu, (qri , ari )) = Score(qu, qri ).The Select function is intended to select thematching questions that have high ?semantic?
simi-larity to the user?s question.
However, given there isno objective function that measures semantic simi-larity, we approximate it using different metrics dis-cussed below.Ranking of the members of the retrieved set canbe based on the scores computed during the selec-tion step or can be independently computed basedon other criteria such as popularity of the question,credibility of the source, temporal recency of the an-swer, geographical proximity to the answer origin.4.1 Question Retrieval MetricsWe retrieve QA pairs from the data repository basedon the similarity of match between the user?s queryand each of the set of questions (d) in the repos-itory.
To measure the similarity, we have experi-mented with the following metrics.1.
TF-IDF metric: The user input query and thedocument (in our case, questions in the repos-itory) are represented as bag-of-n-grams (akaterms).
The term weights are computed using acombination of term frequency (tf ) and inversedocument frequency (idf ) (Robertson, 2004).If Q = q1, q2, .
.
.
, qn is a user query, then theaggregated score for a document d using a un-igram model of the query and the document isgiven as in Equation 1.
For a given query, thedocuments with the highest total term weightare presented as retrieved results.
Terms canalso be defined as n-gram sequences of a queryand a document.
In our experiments, we haveused up to 4-grams as terms to retrieve and rankdocuments.Score(d) =?w?Qtfw,d ?
idfw (1)2.
String Comparison Metrics: Since the lengthof the user query and the query to be retrievedare similar in length, we use string compar-ison methods such as Levenshtein edit dis-tance (Levenshtein, 1966) and n-gram overlap(BLEU-score) (Papineni et al, 2002) as simi-larity metrics.We compare the search effectiveness of these sim-ilarity metrics in Section 5.3.5 Tightly coupling ASR and SearchMost of the speech-driven search systems use the1-best output from the ASR as the query for thesearch component.
Given that ASR 1-best outputis likely to be erroneous, this serialization of theASR and search components might result in sub-optimal search accuracy.
A lattice representationof the ASR output, in particular, a word-confusionnetwork (WCN) transformation of the lattice, com-pactly encodes the n-best hypothesis with the flexi-bility of pruning alternatives at each word position.An example of a WCN is shown in Figure 3.
Theweights on the arcs are to be interpreted as costs andthe best path in the WCN is the lowest cost pathfrom the start state (0) to the final state (4).
Notethat the 1-best path is how old is mama, while theinput speech was how old is obama which also is inthe WCN, but at a higher cost.0 1how/0.001who/6.2922old/0.006does/12.63late/14.14was/14.43_epsilon/5.0103is/0.000a/12.60_epsilon/8.3694/1obama/7.796lil/7.796obamas/13.35mama/0.000bottle/12.60Figure 3: A sample word confusion network with arccosts as negative logarithm of the posterior probabilities.570how:qa25/c1old:qa25/c2is:qa25/c3obama:qa25/c4old:qa150/c5how:qa12/c6obama:qa450/c7is:qa1450/c8Figure 4: Example of an FST representing the search in-dex.5.1 Representing Search Index as an FSTLucene (Hatcher and Gospodnetic., 2004) is an off-the-shelf search engine that implements the TF-IDFmetric.
But, we have implemented our own searchengine using finite-state transducers (FST) for thisreason.
The oracle word/phrase accuracy using n-best hypotheses of an ASR is usually far greater thanthe 1-best output.
However, using each of the n-best(n > 1) hypothesis as a separate query to the searchcomponent is computationally sub-optimal since thestrings in the n-best hypotheses usually share largesubsequences with each other.
The FST representa-tion of the search index allows us to efficiently con-sider lattices/WCNs as input queries.The FST search index is built as follows.
We in-dex each question-answer (QA) pair from our repos-itory ((qi, ai), qai for short) using the words (wqi) inquestion qi.
This index is represented as a weightedfinite-state transducer (SearchFST) as shown in Fig-ure 4.
Here a word wqi (e.g old) is the input symbolfor a set of arcs whose output symbol is the indexof the QA pairs where old appears in the question.The weight of the arc c(wqi ,qi) is one of the simi-larity based weights discussed in Section 4.1.
Ascan be seen from Figure 4, the words how, old, isand obama contribute a score to the question-answerpair qa25; while other pairs, qa150, qa12, qa450 arescored by only one of these words.5.2 Search Process using FSTsA user?s speech query, after speech recognition, isrepresented as an FSA (either 1-best or WCN), aQueryFSA.
The QueryFSA (denoted as q) is thentransformed into another FSA (NgramFSA(q)) thatrepresents the set of n-grams of the QueryFSA.Due to the arc costs from WCNs, the NgramFSAfor a WCN is a weighted FSA.
The NgramFSA iscomposed with the SearchFST and we obtain allthe arcs (wq, qawq , c(wq ,qawq )) where wq is a queryterm, qawq is a QA index with the query term and,c(wq ,qawq ) is the weight associated with that pair.
Us-ing this information, we aggregate the weight for aQA pair (qaq) across all query words and rank theretrieved QAs in the descending order of this aggre-gated weight.
We select the top N QA pairs fromthis ranked list.
The query composition, QA weightaggregation and selection of top N QA pairs arecomputed with finite-state transducer operations asshown in Equations 2 to 5.3D1 = pi2(NgramFSA(q) ?
SearchFST ) (2)R1 = fsmbestpath(D1, 1) (3)D2 = pi2(NgramFSA(R1) ?
SearchFST ) (4)TopN = fsmbestpath(fsmdeterminize(D2), N)(5)The process of retrieving documents using theLevenshtein-based string similarity metric can alsobe encoded as a composition of FSTs.5.3 Experiments and ResultsWe have a fairly large data set consisting of over amillion question-answer pairs collected by harvest-ing the web.
In order to evaluate the retrieval meth-ods discussed earlier, we use two test sets of QApairs: a Seen set of 450 QA pairs and an Unseen setof 645 QA pairs.
The queries in the Seen set havean exact match with some question in the database,while the queries in the Unseen set may not matchany question in the database exactly.
4 The questionsin theUnseen set, however, like those in the Seen set,also have a human generated answer that is used inour evaluations.For each query, we retrieve the twenty most rel-evant QA pairs, ranked in descending order of thevalue of the particular metric under consideration.However, depending on whether the user query is aseen or an unseen query, the evaluation of the rele-vance of the retrieved question-answer pairs is dif-ferent as discussed below.53We have dropped the need to convert the weights into thereal semiring for aggregation, to simplify the discussion.4There may however be semantically matching questions.5The reason it is not a recall and precision curve is that, forthe ?seen?
query set, the retrieval for the questions is a zero/oneboolean accuracy.
For the ?unseen?
query set there is no perfectmatch with the input question in the query database, and so wedetermine the closeness of the questions based on the closenessof the answers.
Coherence attempts to capture the homogen-ity of the questions retrieved, with the assumption that the usermight want to see similar questions as the returned results.585.3.1 Evaluation MetricsFor the set of Seen queries, we evaluate the rele-vance of the retrieved top-20 question-answer pairsin two ways:1.
Retrieval Accuracy of Top-N results: We eval-uate whether the question that matches the userquery exactly is located in the top-1, top-5,top-10, top-20 or not in top-20 of the retrievedquestions.2.
Coherence metric: We compute the coherenceof the retrieved set as the mean of the BLEU-score between the input query and the set oftop-5 retrieved questions.
The intuition is thatwe do not want the top-5 retrieved QA pairsto distract the user by not being relevant to theuser?s query.For the set of Unseen queries, since there are noquestions in the database that exactly match the in-put query, we evaluate the relevance of the top-20 re-trieved question-answer pairs in the following way.For each of the 645 Unseen queries, we know thehuman-generated answer.
We manually annotatedeach unseen query with the Best-Matched QA pairwhose answer was the closest semantic match to thehuman-generated answer for that unseen query.
Weevaluate the position of the Best-Matched QA in thelist of top twenty retrieved QA pairs for each re-trieval method.5.3.2 ResultsOn the Seen set of queries, as expected the re-trieval accuracy scores for the various retrieval tech-niques performed exceedingly well.
The unigrambased tf.idf method retrieved 93% of the user?s queryin the first position, 97% in one of top-5 positionsand 100% in one of top-10 positions.
All the otherretrieval methods retrieved the user?s query in thefirst position for all the Seen queries (100% accu-racy).In Table 1, we tabulate the results of the Coher-ence scores for the top-5 questions retrieved usingthe different retrieval techniques for the Seen set ofqueries.
Here, the higher the n-gram the more co-herent is the set of the results to the user?s query.
Itis interesting to note that the BLEU-score and Lev-enshtein similarity driven retrieval methods do notdiffer significantly in their scores from the n-gramtf.idf based metrics.Method Coherence Metricfor top-5 resultsTF-IDF unigram 61.58bigram 66.23trigram 66.234-gram 69.74BLEU-score 66.29Levenshtein 67.36Table 1: Coherence metric results for top-5 queries re-trieved using different retrieval techniques for the seenset.In Table 2, we present the retrieval results usingdifferent methods on the Unseen queries.
For 240 ofthe 645 unseen queries, the human expert found thatthat there was no answer in the data repository thatcould be considered semantically equivalent to thehuman-generated response to that query.
So, these240 queries cannot be answered using the currentdatabase.
For the remaining 405 unseen queries,over 60% have their Best-Matched question-answerpair retrieved in the top-1 position.
We expect thecoverage to improve considerably by increasing thesize of the QA archive.Method Top-1 Top-20TFIDF Unigram 69.13 75.81Bigram 62.46 67.41Trigram 61.97 65.934-gram 56.54 58.77WCN 70.12 78.52Levenshtein 67.9 77.29BLEU-score 72.0 75.31Table 2: Retrieval results for the Unseen queries5.3.3 Speech-driven query retrievalIn Equation 6, we show the tight integration ofWCNs and SearchFST using the FST compositionoperation (?).
?
is used to scale the weights6 fromthe acoustic/language models on the WCNs againstthe weights on the SearchFST.
As before, we useEquation 3 to retrieve the top N QA pairs.
The tightintegration is expected to improve both the ASR andSearch accuracies by co-constraining both compo-nents.D = pi2(Unigrams(WCN)?
?SearchFST ) (6)For this experiment, we use the speech utterancescorresponding to the Unseen set as the test set.
Weuse a different set of 250 speech queries as the6fixed using the development set59development set.
In Table 3, we show the Wordand Sentence Accuracy measures for the best pathin the WCN before and after the composition ofSearchFST with the WCN on the development andtest sets.
We note that by integrating the constraintsfrom the search index, the ASR accuracies can beimproved by about 1% absolute.Set # of Word Sentenceutterances Accuracy AccuracyDev Set 250 77.1(78.2) 54(54)Test Set 645 70.8(72.1) 36.7(37.1)Table 3: ASR accuracies of the best path before and after(in parenthesis) the composition of SearchFSTSince we have the speech utterances of the Un-seen set, we were also able to compute the searchresults obtained by integrating the ASR WCNs withthe SearchFST, as shown in line 5 of Table 2.
Theseresults show that the the integration of the ASRWCNs with the SearchFST produces higher searchaccuracy compared to ASR 1-best.6 Dynamic and Static QuestionsStoring previously answered questions and their an-swers allows Qme!to retrieve the answers to a sub-class of questions quickly and accurately.
We termthis subclass as static questions since the answersto these questions remain the same irrespective ofwhen and where the questions are asked.
Examplesof such questions are What is the speed of light?,When is George Washington?s birthday?.
In con-trast, there is a subclass of questions, which we termdynamic questions, for which the answers dependon when and where they are asked.
For such ques-tions the above method results in less than satisfac-tory and sometimes inaccurate answers.
Examplesof such questions are What is the stock price of Gen-eral Motors?, Who won the game last night?, Whatis playing at the theaters near me?.We define dynamic questions as questions whoseanswers change more frequently than once a year.In dynamic questions, there may be no explicit ref-erence to time, unlike the questions in the TERQAScorpus (Radev and Sundheim., 2002) which explic-itly refer to the temporal properties of the entitiesbeing questioned or the relative ordering of past andfuture events.
The time-dependency of a dynamicquestion lies in the temporal nature of its answer.For example, consider the dynamic question, ?Whatis the address of the theater ?White Christmas?
isplaying at in New York??.
White Christmas is a sea-sonal play that plays in New York every year for afew weeks in December and January, but it does notnecessarily at the same theater every year.
So, de-pending when this question is asked, the answer willbe different.Interest in temporal analysis for question-answering has been growing since the late 1990?s.Early work on temporal expressions identifica-tion using a tagger led to the development ofTimeML (Pustejovsky et al, 2001), a markuplanguage for annotating temporal expressions andevents in text.
Other examples include QA-by-Dossier with Constraints (Prager et al, 2004), amethod of improving QA accuracy by asking auxil-iary questions related to the original question in or-der to temporally verify and restrict the original an-swer.
(Moldovan et al, 2005) detect and representtemporally related events in natural language usinglogical form representation.
(Saquete et al, 2009)use the temporal relations in a question to decom-pose it into simpler questions, the answers of whichare recomposed to produce the answers to the origi-nal question.6.1 Dynamic/Static ClassificationWe automatically classify questions as dynamic andstatic questions.
Answers to static questions can beretrieved from the QA archive.
To answer dynamicquestions, we query the database(s) associated withthe topic of the question through web forms on theInternet.
We use a topic classifier to detect the topicof a question followed by a dynamic/static classifiertrained on questions related to a topic, as shown infigure 5.
Given the question what movies are play-ing around me?, we detect it is a movie related dy-namic question and query a movie information website (e.g.
www.fandango.com) to retrieve the resultsbased on the user?s GPS information.Figure 5: Chaining two classifiersWe used supervised learning to train the topic60classifier, since our entire dataset is annotated by hu-man experts with topic labels.
In contrast, to train adynamic/static classifier, we experimented with thefollowing three different techniques.Baseline: We treat questions as dynamic if theycontain temporal indexicals, e.g.
today, now, thisweek, two summers ago, currently, recently, whichwere based on the TimeML corpus.
We also in-cluded spatial indexicals such as here, and other sub-strings such as cost of and how much is.
A questionis considered static if it does not contain any suchwords/phrases.Self-training with bagging: The general self-training with bagging algorithm (Banko and Brill,2001) is presented in Table 6 and illustrated in Fig-ure 7(a).
The benefit of self-training is that we canbuild a better classifier than that built from the smallseed corpus by simply adding in the large unlabeledcorpus without requiring hand-labeling.1.
Create k bags of data, each of size |L|, by samplingwith replacement from labeled set L.2.
Train k classifiers; one classifier on each of k bags.3.
Each classifier predicts labels of the unlabeled set.4.
The N labeled instances that j of k classifiers agreeon with the highest average confidence is added to thelabeled set L, to produce a new labeled set L?.5.
Repeat all 5 steps until stopping criteria is reached.Figure 6: Self-training with bagging(a) (b)Figure 7: (a) Self-training with bagging (b) Committee-based active-learningIn order to prevent a bias towards the majorityclass, in step 4, we ensure that the distribution ofthe static and dynamic questions remains the sameas in the annotated seed corpus.
The benefit of bag-ging (Breiman, 1996) is to present different views ofthe same training set, and thus have a way to assessthe certainty with which a potential training instancecan be labeled.Active-learning: This is another popular method fortraining classifiers when not much annotated data isavailable.
The key idea in active learning is to anno-tate only those instances of the dataset that are mostdifficult for the classifier to learn to classify.
It isexpected that training classifiers using this methodshows better performance than if samples were cho-sen randomly for the same human annotation effort.Figure 7(b) illustrates the algorithm and Figure 8describes the algorithm, also known as committee-based active-learning (Banko and Brill, 2001).1.
Create k bags of data, each of size |L|, by samplingwith replacement from the labeled set L.2.
Train k classifiers, one on each bag of the k bags.3.
Each classifier predicts the labels of the unlabeled set.4.
Choose N instances from the unlabeled set for humanlabeling.
N/2 of the instances are those whose labels thecommittee of classifiers have highest vote entropy (un-certainity).
The other N/2 of the instances are selectedrandomly from the unlabeled set.5.
Repeat all 5 steps until stopping criteria is reached.Figure 8: Active Learning algorithmWe used the maximum entropy classifier inLlama (Haffner, 2006) for all of the above classi-fication tasks.6.2 Experiments and Results6.2.1 Topic ClassificationThe topic classifier was trained using a trainingset consisted of over one million questions down-loaded from the web which were manually labeledby human experts as part of answering the questions.The test set consisted of 15,000 randomly selectedquestions.
Word trigrams of the question are usedas features for a MaxEnt classifier which outputs ascore distribution on all of the 104 possible topiclabels.
The error rate results for models selectingthe top topic and the top two topics according to thescore distribution are shown in Table 4.
As can beseen these error rates are far lower than the baselinemodel of selecting the most frequent topic.Model Error RateBaseline 98.79%Top topic 23.9%Top-two topics 12.23%Table 4: Results of topic classification61Figure 9: Change in classification results6.2.2 Dynamic/static ClassificationAs mentioned before, we experimented withthree different approaches to bootstrapping a dy-namic/static question classifier.
We evaluate thesemethods on a 250 question test set drawn from thebroad topic of Movies.
For the baseline model, weused the words/phrases discussed earlier based ontemporal and spatial indexicals.
For the ?super-vised?
model, we use the baseline model to tag 500Kexamples and use the machine-annotated corpus totrain a MaxEnt binary classifier with word trigramsas features.
The error rate in Table 5 shows that itperforms better than the baseline model mostly dueto better lexical coverage contributed by the 500Kexamples.Training approach Lowest Error rateBaseline 27.70%?Supervised?
learning 22.09%Self-training 8.84%Active-learning 4.02%Table 5: Best Results of dynamic/static classificationIn the self-training approach, we start with a smallseed corpus of 250 hand-labeled examples from theMovies topic annotated with dynamic or static tags.We used the same set of 500K unlabeled examplesas before and word trigrams from the question wereused as the features for a MaxEnt classifier.
We used11 bags in the bagging phase of this approach andrequired that all 11 classifiers agree unanimouslyabout the label of a new instance.
Of all such in-stances, we randomly selected N instances to beadded to the training set of the next iteration, whilemaintaining the distribution of the static and dy-namic questions to be the same as that in the seedcorpus.
We experimented with various values of N ,the number of newly labeled instances added at eachiteration.
The error rate at initialization is 10.4%compared to 22.1% of the ?supervised?
approachwhich can be directly attributed to the 250 hand-labeled questions.
The lowest error rate of the self-training approach, obtained at N=100, is 8.84%, asshown in Table 5.
In Figure 9, we show the changein error rate for N=40 (line S1 in the graph) andN=100 (line S2 in the graph).For the active learning approach, we used thesame set of 250 questions as the seed corpus, thesame set of 500K unlabeled examples, the same testset, and the same set of word trigrams features as inthe self-training approach.
We used 11 bags for thebagging phase and selected top 20 new unlabeled in-stances on which the 11 classifiers had the greatestvote entropy to be presented to the human labeler forannotation.
We also randomly selected 20 instancesfrom the rest of the unlabeled set to be presented forannotation.
The best error rate of this classifier onthe test set is 4.02%, as shown in Table 5.
The errorrate over successive iterations is shown by line A1in Figure 9.In order to illustrate the benefits of selecting theexamples actively, we repeated the experiment de-scribed above but with all 40 unlabeled instances se-lected randomly for annotation.
The error rate oversuccessive iterations is shown by line R1 in Fig-ure 9.
Comparing A1 to R1, we see that the error de-creases faster when we select some of the unlabeledinstances for annotation actively at each iteration.7 ConclusionIn this paper, we have presented a system Qme!,a speech-driven question-answering system for mo-bile devices.
We have proposed a query retrievalmodel for question-answering and demonstrated themutual benefits of tightly coupling the ASR andSearch components of the system.
We have pre-sented a novel concept of distinguishing questionsthat need dynamic information to be answered fromthose questions whose answers can be retrieved froman archive.
We have shown results on bootstrap-ping such a classifier using semi-supervised learningtechniques.62ReferencesL.
Androutsopoulos.
1995.
Natural language interfacesto databases - an introduction.
Journal of Natural Lan-guage Engineering, 1:29?81.M.
Banko and E. Brill.
2001.
Scaling to very very largecorpora for natural language disambiguation.
In Pro-ceedings of the 39th annual meeting of the associationfor computational linguistics: ACL 2001, pages 26?33.L.
Breiman.
1996.
Bagging predictors.
Machine Learn-ing, 24(2):123?140.S.
Dumais, M. Banko, E. Brill, J. Lin, and A. Ng.
2002.Web question answering: is more always better?
InSIGIR ?02: Proceedings of the 25th annual interna-tional ACM SIGIR conference on Research and devel-opment in information retrieval, pages 291?298, NewYork, NY, USA.
ACM.Google, 2009. http://www.google.com/mobile.B.F.
Green, A.K.
Wolf, C. Chomsky, and K. Laughery.1961.
Baseball, an automatic question answerer.
InProceedings of the Western Joint Computer Confer-ence, pages 219?224.P.
Haffner.
2006.
Scaling large margin classifiers for spo-ken language understanding.
Speech Communication,48(iv):239?261.E.
Hatcher and O. Gospodnetic.
2004.
Lucene in Action(In Action series).
Manning Publications Co., Green-wich, CT, USA.J.
Jeon, W. B. Croft, and J. H. Lee.
2005.
Finding sim-ilar questions in large question and answer archives.In CIKM ?05: Proceedings of the 14th ACM interna-tional conference on Information and knowledge man-agement, pages 84?90, New York, NY, USA.
ACM.B.
Katz.
1997.
Annotating the world wide web usingnatural language.
In Proceedings of RIAO.V.I.
Levenshtein.
1966.
Binary codes capable of correct-ing deletions, insertion and reversals.
Soviet PhysicsDoklady, 10:707?710.M.
T.Maybury, editor.
2004.
NewDirections in QuestionAnswering.
AAAI Press.Microsoft, 2009. http://www.live.com.D.
Moldovan, C. Clark, and S. Harabagiu.
2005.
Tem-poral context representation and reasoning.
In Pro-ceedings of the 19th International Joint Conference onArtificial Intelligence, pages 1009?1104.MSN-QnA, 2009. http://qna.live.com/.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.Bleu: A method for automatic evaluation of machinetranslation.
In Proceedings of 40th Annual Meetingof the Association of Computational Linguistics, pages313?318, Philadelphia, PA, July.J.
Prager, J. Chu-Carroll, and K. Czuba.
2004.
Ques-tion answering using constraint satisfaction: Qa-by-dossier-with-contraints.
In Proceedings of the 42ndannual meeting of the association for computationallinguistics: ACL 2004, pages 574?581.J.
Pustejovsky, R. Ingria, R.
Saur?
?, J. Casta no, J. Littman,and R.
Gaizauskas., 2001.
The language of time: Areader, chapter The specification languae ?
TimeML.Oxford University Press.D.
Radev and B. Sundheim.
2002.
Using timeml in ques-tion answering.
Technical report, Brandies University.S.
Robertson.
2004.
Understanding inverse documentfrequency: On theoretical arguments for idf.
Journalof Documentation, 60.E.
Saquete, J. L. Vicedo, P.
Mart?
?nez-Barco, R. Munoz, and H. Llorens.
2009.
Enhancing qa sys-tems with complex temporal question processing ca-pabilities.
Journal of Artificial Intelligence Research,35:775?811.N.
Tomuro and S. L. Lytinen.
2004.
Retrieval modelsand Q and A learning with FAQ files.
In New Direc-tions in Question Answering, pages 183?202.vlingo.com, 2009. http://www.vlingomobile.com/downloads.html.R.
J. Waldinger, D. E. Appelt, J. L. Dungan, J. Fry, J. R.Hobbs, D. J. Israel, P. Jarvis, D. L. Martin, S. Riehe-mann, M. E. Stickel, and M. Tyson.
2004.
Deductivequestion answering from multiple resources.
In NewDirections in Question Answering, pages 253?262.J.
Weizenbaum.
1966.
ELIZA - a computer programfor the study of natural language communication be-tween man and machine.
Communications of theACM, 1:36?45.T.
Winograd.
1972.
Understanding Natural Language.Academic Press.W.
A.
Woods.
1973.
Progress in natural language un-derstanding - an application to lunar geology.
In Pro-ceedings of American Federation of Information Pro-cessing Societies (AFIPS) Conference.X.
Xue, J. Jeon, and W. B. Croft.
2008.
Retrieval modelsfor question and answer archives.
In SIGIR ?08: Pro-ceedings of the 31st annual international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 475?482, New York, NY, USA.ACM.Yahoo!, 2009. http://answers.yahoo.com/.F.
Yang, J. Feng, and G. DiFabbrizio.
2006.
A datadriven approach to relevancy recognition for contex-tual question answering.
In HLT-NAACL 2006 Work-shop on Interactive Question Answering, New York,USA, June 8-9.YellowPages, 2009. http://www.speak4it.com.63
