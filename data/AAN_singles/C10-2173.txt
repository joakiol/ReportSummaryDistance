Coling 2010: Poster Volume, pages 1515?1523,Beijing, August 2010Active Deep Networks for Semi-SupervisedSentiment ClassificationShusen Zhou, Qingcai Chen and Xiaolong WangShenzhen Graduate School, Harbin Institute of Technologyzhoushusen@hitsz.edu.cn, qincai.chen@hitsz.edu.cn,wangxl@insun.hit.edu.cnAbstractThis paper presents a novel semi-supervised learning algorithm called Ac-tive Deep Networks (ADN), to addressthe semi-supervised sentiment classifica-tion problem with active learning.
First,we propose the semi-supervised learningmethod of ADN.
ADN is constructed byRestricted Boltzmann Machines (RBM)with unsupervised learning using labeleddata and abundant of unlabeled data.Then the constructed structure is fine-tuned by gradient-descent based super-vised learning with an exponential lossfunction.
Second, we apply active learn-ing in the semi-supervised learningframework to identify reviews thatshould be labeled as training data.
ThenADN architecture is trained by the se-lected labeled data and all unlabeled data.Experiments on five sentiment classifica-tion datasets show that ADN outper-forms the semi-supervised learning algo-rithm and deep learning techniques ap-plied for sentiment classification.1 IntroductionIn recent years, sentiment analysis has receivedconsiderable attentions in Natural LanguageProcessing (NLP) community (Blitzer et al,2007; Dasgupta and Ng, 2009; Pang et al, 2002).Polarity classification, which determine whetherthe sentiment expressed in a document is posi-tive or negative, is one of the most popular tasksof sentiment analysis (Dasgupta and Ng, 2009).Sentiment classification is a special type of textcategorization, where the criterion of classifica-tion is the attitude expressed in the text, ratherthan the subject or topic.
Labeling the reviewswith their sentiment would provide succinctsummaries to readers, which makes it possible tofocus the text mining on areas in need of im-provement or on areas of success (Gamon, 2004)and is helpful in business intelligence applica-tions, recommender systems, and message filter-ing (Pang, et al, 2002).While topics are often identifiable by key-words alone, sentiment classification appears tobe a more challenge task (Pang, et al, 2002).First, sentiment is often conveyed with subtlelinguistic mechanisms such as the use of sar-casm and highly domain-specific contextualcues (Li et al, 2009).
For example, although thesentence ?The thief tries to protect his excellentreputation?
contains the word ?excellent?, it tellsus nothing about the author?s opinion and in factcould be well embedded in a negative review.Second, sentiment classification systems are typ-ically domain-specific, which makes the expen-sive process of annotating a large amount of datafor each domain and is a bottleneck in buildinghigh quality systems (Dasgupta and Ng, 2009).This motivates the task of learning robust senti-ment models from minimal supervision (Li, etal., 2009).Recently, semi-supervised learning, whichuses large amount of unlabeled data togetherwith labeled data to build better learners (Rainaet al, 2007; Zhu, 2007), has drawn more atten-tion in sentiment analysis (Dasgupta and Ng,2009; Li, et al, 2009).
As argued by several re-searchers (Bengio, 2007; Salakhutdinov andHinton, 2007), deep architecture, composed ofmultiple levels of non-linear operations (Hintonet al, 2006), is expected to perform well insemi-supervised learning because of its capabili-ty of modeling hard artificial intelligent tasks.Deep Belief Networks (DBN) is a representative1515deep learning algorithm achieving notable suc-cess for semi-supervised learning (Hinton, et al,2006).
Ranzato and Szummer (2008) propose analgorithm to learn text document representationsbased on semi-supervised auto-encoders that arecombined to form a deep network.Active learning is another way that can mi-nimize the number of required labeled datawhile getting competitive result.
Usually, thetraining set is chosen randomly.
However, activelearning choose the training data actively, whichreduce the needs of labeled data (Tong andKoller, 2002).
Recently, active learning hadbeen applied in sentiment classification(Dasgupta and Ng, 2009).Inspired by the study of semi-supervisedlearning, active learning and deep architecture,this paper proposes a novel semi-supervised po-larity classification algorithm called ActiveDeep Networks (ADN) that is based on a repre-sentative deep learning algorithm Deep BeliefNetworks (DBN) (Hinton, et al, 2006) and ac-tive learning (Tong and Koller, 2002).
First, wepropose the ADN architecture, which utilizes anew deep architecture for classification, and anexponential loss function aiming to maximizethe separability of the classifier.
Second, wepropose the ADN algorithm.
It firstly identifies asmall number of manually labeled reviews by anactive learner, and then trains the ADN classifierwith the identified labeled data and all of theunlabeled data.Our paper makes several important contribu-tions: First, this paper proposes a novel ADNarchitecture that integrates the abstraction abilityof deep belief nets and the classification abilityof backpropagation strategy.
It improves the ge-neralization capability by using abundant unla-beled data, and directly optimizes the classifica-tion results in training dataset using back propa-gation strategy, which makes it possible toachieve attractive classification performancewith few labeled data.
Second, this paper pro-poses an effective active learning method thatintegrates the labeled data selection ability ofactive learning and classification ability of ADNarchitecture.
Moreover, the active learning isalso based on the ADN architecture, so the la-beled data selector and the classifier are basedon the same architecture, which provides an uni-fied framework for semi-supervised classifica-tion task.
Third, this paper applies semi-supervised learning and active learning to senti-ment classification successfully and gets com-petitive performance.
Our experimental resultson five sentiment classification datasets showthat ADN outperforms previous sentiment clas-sification methods and deep learning methods.The rest of the paper is organized as follows.Section 2 gives an overview of sentiment classi-fication.
The proposed semi-supervised learningmethod ADN is described in Section 3.
Section4 shows the empirical validation of ADN bycomparing its classification performance withprevious sentiment classifiers and deep learningmethods on sentiment datasets.
The paper isclosed with conclusion.2 Sentiment ClassificationSentiment classification can be performed onwords, sentences or documents, and is generallycategorized into lexicon-based and corpus-basedclassification method (Wan, 2009).
The detailedsurvey about techniques and approaches ofsentiment classification can be seen in the book(Pang and Lee, 2008).
In this paper we focus oncorpus-based classification method.Corpus-based methods use a labeled corpus totrain a sentiment classifier (Wan, 2009).
Pang etal.
(2002) apply machine learning approach tocorpus-based sentiment classification firstly.They found that standard machine learning tech-niques outperform human-produced baselines.Pang and Lee (2004) apply text-categorizationtechniques to the subjective portions of the sen-timent document.
These portions are extractedby efficient techniques for finding minimum cutsin graphs.
Gamon (2004) demonstrate that usinglarge feature vectors in combination with featurereduction, high accuracy can be achieved in thevery noisy domain of customer feedback data.Xia et al (2008) propose the sentiment vectorspace model to represent song lyric document,assign the sentiment labels such as light-heartedand heavy-hearted.Supervised sentiment classification systemsare domain-specific and annotating a large scalecorpus for each domain is very expensive(Dasgupta and Ng, 2009).
There are several so-lutions for this corpus annotation bottleneck.The first type of solution is using old domainlabeled examples to new domain sentiment clas-1516sification.
Blitzer et al (2007) investigate do-main adaptation for sentiment classifiers, whichcould be used to select a small set of domains toannotate and their trained classifiers wouldtransfer well to many other domains.
Li andZong (2008) study multi-domain sentiment clas-sification, which aims to improve performancethrough fusing training data from multiple do-mains.The second type of solution is semi-supervised sentiment classification.
Sindhwaniand Melville (2008) propose a semi-supervisedsentiment classification algorithm that utilizeslexical prior knowledge in conjunction with un-labeled data.
Dasgupta and Ng (2009) firstlymine the unambiguous reviews using spectraltechniques, and then exploit them to classify theambiguous reviews via a novel combination ofactive learning, transductive learning, and en-semble learning.The third type of solution is unsupervised sen-timent classification.
Zagibalov and Carroll(2008) describe an automatic seed word selec-tion for unsupervised sentiment classification ofproduct reviews in Chinese.However, unsupervised learning of sentimentis difficult, partially because of the prevalence ofsentimentally ambiguous reviews (Dasgupta andNg, 2009).
Using multi-domain sentiment cor-pus to sentiment classification is also hard toapply, because each domain has a very limitedamount of training data, due to annotating alarge corpus is difficult and time-consuming (Liand Zong, 2008).
So in this paper we focus onsemi-supervised approach to sentiment classifi-cation.3 Active Deep NetworksIn this part, we propose a semi-supervisedlearning algorithm, Active Deep Networks(ADN), to address the sentiment classificationproblem with active learning.
Section 3.1formulates the ADN problem.
Section 3.2proposes the semi-supervised learning of ADNwithout active learning.
Section 3.3 proposes theactive learning method of ADN.
Section 3.4gives the ADN procedure.3.1 Problem FormulationThere are many review documents in the dataset.We preprocess these reviews to be classified,which is similar with Dasgupta and Ng (2009).Each review is represented as a vector of uni-grams, using binary weight equal to 1 for termspresent in a vector.
Moreover, the punctuations,numbers, and words of length one are removedfrom the vector.
Finally, we sort the vocabularyby document frequency and remove the top1.5%.
It is because that many of these high doc-ument frequency words are stopwords or domainspecific general-purpose words.After preprocess, every review can berepresented by a vector.
Then the dataset can berepresented as a matrix:?
?1 21 1 11 21 2 2 2 21 2, , ,, , ,, , , 1, , ,, , ,R TR TR TR TD D Dx x xx x xx x x?????
??
??
??
??
??
?
?
??
??
??
?X x x x????
?
?
?
?where R is the number of training samples, T isthe number of test samples, D is the number offeature words in the dataset.
Every column of Xcorresponds to a sample x, which is a representa-tion of a review.
A sample that has all features isviewed as a vector in D, where the ith coordi-nate corresponds to the ith feature.The L labeled samples are chosen randomlyfrom R training samples, or chosen actively byactive learning, which can be seen as:?
?
?
?1 2, [ , ,..., ] 1 2L R L is s s s R?
?
?
?X X S Swhere S is the index of selected training reviewsto be labeled manually.Let Y be a set of labels corresponds to L la-beled training samples and is denoted as:?
?1 21 1 11 21 2 2 2 21 2, , ,, , ,, , , 3, , ,, , ,LLL LLC C Cy y yy y yy y y?
??
??
??
??
??
?
?
??
??
??
?Y y y y????
?
?
?
?where C is the number of classes.
Every columnof Y is a vector in C, where the jth coordinatecorresponds to the jth class.?
?thth1    if  class 4-1  if  classiij ijy j?
???
?
??
?xxFor example, if a review x is positive, y=[1, -1]?
; else, y = [-1, 1]?.We intend to seek the mapping functionL L?X Y  using the L labeled data and R+T-Lunlabeled data.
After training, we can determiney by the trained ADN while a new sample x isfed.15173.2 Semi-Supervised LearningTo address the problem formulated in section 3.1,we propose a novel deep architecture for ADNmethod, as show in Figure 1.
The deep architec-ture is a fully interconnected directed belief netswith one input layer h0, N hidden layers h1,h2, ?, hN, and one label layer at the top.
Theinput layer h0 has D units, equal to the number offeatures of sample data x.
The label layer has Cunits, equal to number of classes of label vectory.
The numbers of units for hidden layers, cur-rently, are pre-defined according to the expe-rience or intuition.
The seeking of the mappingfunction L L?X Y , here, is transformed to theproblem of finding the parameter space W={w1,w2,?,wN} for the deep architecture.The semi-supervised learning method basedon ADN architecture can be divided into twostages: First, AND architecture is constructed bygreedy layer-wise unsupervised learning usingRBMs as building blocks.
All the unlabeled datatogether with L labeled data are utilized to findthe parameter space W with N layers.
Second,ADN architecture is trained according to the ex-ponential loss function using gradient descentmethod.
The parameter space W is retrained byan exponential loss function using L labeled data.x1x2xD?
?
?
?
??
?
?
?RBMh0h1w1?
?
?h2RBMw2????
?hNf(hN(x), y)?
?y1y2yClabelsMinimizeLossFigure 1.
Architecture of Active Deep NetworksFor unsupervised learning, we define theenergy of the state (hk-1, hk) as:?
??
?1111 11 11 1, ;5k kk kD Dk 1 k k k kst s ts tD Dk k k ks s t ts tE w h hb h c h????
??
??
??
??
??
????
?h hwhere ?
?cbw ,,??
are the model parameters:kstw is the symmetric interaction term betweenunit s in the layer hk-1 and unit t in the layer hk, k=1,?, N-1.1ksb ?
is the sth bias of layer hk-1 andktcis the tth bias of layer hk.
Dk is the number of unitin the kth layer.The probability that the model assigns to hk-1is:?
?
?
?
?
??
?
?
?1 11; exp , ; 6kk k kP EZ?
???
??
?
?hh h h?
?
?
??
?
?
?1 1exp , ; 7k k k kZ E?
??
??
??
?h h h hwhere ?
?
?Z  denotes the normalizing constant.The conditional distributions over hk and hk-1 are:?
?
?
?
?
?1 1| | 8k k k kttp p h?
??
?h h h?
?
?
?
?
?1 1| | 9k k k kssp p h?
??
?h h hthe probability of turning on unit t is a logisticfunction of the states of hk-1 andkstw :?
?
?
?1 11| sigm 10k k k k kt t st ssp h c w h?
??
??
?
??
??
?
?hthe probability of turning on unit s is a logisticfunction of the states of hk andkstw :?
?
?
?1 11| sigm 11k k k k ks s st ttp h b w h?
??
??
?
??
??
?
?hwhere the logistic function is:?
?
?
?sigm 1 1 12e ??
??
?The derivative of the log-likelihood with re-spect to the model parameter wk can be obtainedby the CD method (Hinton, 2002):?
?01 1 1log ( ) 13Mk k k k ks t s tP Pstp h h h hw?
?
??
?
?
?hwhere0P?denotes an expectation with respect tothe data distribution andMP?denotes a distribu-tion of samples from running the Gibbs samplerinitialized at the data, for M full steps.The above discussion is based on the trainingof the parameters between two hidden layerswith one sample data x.
For unsupervised learn-ing, we construct the deep architecture using alllabeled data with unlabeled data by inputtingthem one by one from layer h0, train the parame-ter between h0 and h1.
Then h1 is constructed, we1518can use it to construct the up one layer h2.
Thedeep architecture is constructed layer by layerfrom bottom to top, and in each time, the para-meter space wk is trained by the calculated datain the k-1th layer.According to the wk calculated above, thelayer hk can be got as below when a sample x isfed from layer h0:?
?111( ) sigm( )    1, ,1, , 1 14kDk k k kt t st s ksh c w h t Dk N????
?
??
?
?x x ???
?The parameter space wN is initialized random-ly, just as backpropagation algorithm.
ThenADN architecture is constructed.
The top hiddenlayer is formulated as:?
?1 11( ) 1, , 15NDN N N Nt t st s Nsh c w h t D?
???
?
?
?x x ??
?For supervised learning, the ADN architectureis trained by L labeled data.
The optimizationproblem is formulized as:?
??
?
?
?harg min f h , 16N N L LX Ywhere?
??
?
?
??
?
?
?1 1f h , T h 17L CN L L N i ij ji j y?
???
?X Y xand the loss function is defined as?
?T( ) exp( ) 18r r?
?In the supervised learning stage, the stochasticactivities are replaced by deterministic, real va-lued probabilities.
We use gradient-descentthrough the whole deep architecture to retrainthe weights for optimal classification.3.3 Active LearningSemi-supervised learning allows us to classifyreviews with few labeled data.
However, anno-tating the reviews manually is expensive, so wewant to get higher performance with fewer la-beled data.
Active learning can help to choosethose reviews that should be labeled manually inorder to achieving higher classification perfor-mance with the same number of labeled data.For such purpose, we incorporate pool-basedactive learning with the ADN method, whichaccesses to a pool of unlabeled instances andrequests the labels for some number of them(Tong and Koller, 2002).Given an unlabeled pool XR and a initial la-beled data set XL (one positive, one negative),the ADN architecture hN  will decide which in-stance in XR to query next.
Then the parametersof hN are adjusted after new reviews are labeledand inserted into the labeled data set.
The mainissue for an active learner is the choosing of nextunlabeled instance to query.
In this paper, wechoose the reviews whose labels are most uncer-tain for the classifier.
Following previous workon active learning for SVMs (Dasgupta and Ng,2009; Tong and Koller, 2002), we define theuncertainty of a review as its distance from theseparating hyperplane.
In other words, reviewsthat are near the separating hyperplane are cho-sen as the labeled training data.After semi-supervised learning, the parame-ters of ADN are adjusted.
Given an unlabeledpool XR, the next unlabeled instance to be que-ried are chosen according to the location ofhN(XR).
The distance of a point hN(xi) and theclasses separation line1 2N Nh h?
is:?
?
?
?
?
?1 2 2 19i N i N ih h?
?d x xThe selected training reviews to be labeledmanually are given by:?
??
?
?
?
: min 20js j?
?d dWe can select a group of most uncertainty re-views to label at each time.The experimental setting is similar withDasgupta & Ng (2009).
We perform activelearning for five iterations and select twenty ofthe most uncertainty reviews to be queried eachtime.
Then the ADN is re-trained on all of la-beled and unlabeled reviews so far with semi-supervised learning.
At last, we can decide thelabel of reviews x according to the output hN(x)of the ADN architecture as below:?
?
?
??
??
?
?
??
?
?
?1    if max21-1  if maxN Njj N Njhyh?
???
?
??
?x h xx h xAs shown by Tong and Koller (2002), the Ba-lanceRandom method, which randomly samplean equal number of positive and negative in-stances from the pool, has much better perfor-mance than the regular random method.
So weincorporate this ?Balance?
idea with ADN me-thod.
However, to choose equal number of posi-tive and negative instances without labeling theentire pool of instances in advance may not bepracticable.
So we present a simple way to ap-proximate the balance of positive and negativereviews.
At first, count the number of positiveand negative labeled data respectively.
Second,1519for each iteration, classify the unlabeled reviewsin the pool and choose the appropriate number ofpositive and negative reviews to let them equally.3.4 ADN ProcedureThe procedure of ADN is shown in Figure 2.
Forthe training of ADN architecture, the parametersare random initialized with normal distribution.All the training data and test data are used totrain the ADN with unsupervised learning.
Thetraining set XR can be seen as an unlabeled pool.We randomly select one positive and one nega-tive review in the pool to input as the initial la-beled training set that are used for supervisedlearning.
The number of units in hidden layerD1?DN and the number of epochs Q are set ma-nually based on the dimension of the input dataand the size of training dataset.
The iterationtimes I and the number G of active choosing da-ta for each iteration can be set manually basedon the number of labeled data in the experiment.For each iteration, the ADN architecture istrained by all the unlabeled data and labeled datain existence with unsupervised learning and su-pervised learning firstly.
Then we choose G re-views from the unlabeled pool based on the dis-tance of these data from the separating line.
Atlast, label these data manually and add them tothe labeled data set.
For the next iteration, theADN architecture can be trained on the new la-beled data set.
At last, ADN architecture is re-trained by all the unlabeled data and existinglabeled data.
After training, the ADN architec-ture is tested based on Equation (21).The proposed ADN method can active choosethe labeled data set and classify the data with thesame architecture, which avoid the barrier be-tween choosing and training with different archi-tecture.
More importantly, the parameters ofADN are trained iteratively on the label data se-lection process, which improve the performanceof ADN.
For the ADN training process: in unsu-pervised learning stage, the reviews can be ab-stracted; in supervised learning stage, ADN istrained to map the samples belong to differentclasses into different regions.
We combine theunsupervised and supervised learning, and trainparameter space of ADN iteratively.
The properdata that should be labeled are chosen in eachiteration, which improves the classification per-formance of ADN.Figure 2.
Active Deep Networks Procedure.4 Experiments4.1 Experimental SetupWe evaluate the performance of the proposedADN method using five sentiment classificationdatasets.
The first dataset is MOV (Pang, et al,2002), which is a widely-used movie review da-taset.
The other four dataset contain reviews offour different types of products, including books(BOO), DVDs (DVD), electronics (ELE), andkitchen appliances (KIT) (Blitzer, et al, 2007;Dasgupta and Ng, 2009).
Each dataset includes1,000 positive and 1,000 negative reviews.Similar with Dasgupta and Ng (2009), we di-vide the 2,000 reviews into ten equal-sized foldsrandomly and test all the algorithms with cross-validation.
In each folds, 100 reviews are ran-dom selected as training data and the remaining100 data are used for test.
Only the reviews inthe training data set are used for the selection oflabeled data by active learning.The ADN architecture has different number ofhidden units for each hidden layer.
For greedyActive Deep Networks ProcedureInput:  data Xnumber of units in every hidden layer D1?DNnumber of epochs Qnumber of training data Rnumber of test data Tnumber of iterations Inumber of active choose data for every iteration GInitialize: W = normally distributed random numbersXL = one positive and one negative reviewsfor i = 1 to IStep 1.
Greedy layer-wise training hidden layers using RBMfor  n = 1 to N-1for  q = 1 to Qfor k = 1 to R+TCalculate the non-linear positive and negative phaseaccording to (10) and (11).Update the weights and biases by (13).end forend forend forStep 2.
Supervised learning the ADN with gradient descentMinimize f(hN(X),Y) on labeled data set XL, update theparameter space W according to (16).Step 3.
Choose instances for labeled data setChoose G instances which near the separating line by (20)Add  G instances into the labeled data set XLendTrain ADN with Step 1 and Step 2.Output: ADN  hN(x)1520layer-wise unsupervised learning, we train theweights of each layer independently with thefixed number of epochs equal to 30 and thelearning rate is set to 0.1.
The initial momentumis 0.5 and after 5 epochs, the momentum is set to0.9.
For supervised learning, we run 10 epochs,three times of linear searches are performed ineach epoch.We compare the classification performance ofADN with five representative classifiers, i.e.,Semi-supervised spectral learning (Spectral)(Kamvar et al, 2003), Transductive SVM(TSVM), Active learning (Active) (Tong andKoller, 2002), Mine the Easy Classify the Hard(MECH) (Dasgupta and Ng, 2009), and DeepBelief Networks (DBN) (Hinton, et al, 2006).Spectral learning, TSVM, and Active learningmethod are three baseline methods for sentimentclassification.
MECH is a new semi-supervisedmethod for sentiment classification (Dasguptaand Ng, 2009).
DBN (Hinton, et al, 2006) is theclassical deep learning method proposed recent-ly.4.2 ADN PerformanceFor MOV dataset, the ADN structure used inthis experiment is 100-100-200-2, whichrepresents the number of units in output layer is2, in 3 hidden layers are 100, 100, and 200 re-spectively.
For the other four data sets, the ADNstructure is 50-50-200-2.
The number of unit ininput layer is the same as the dimensions of eachdatasets.
All theses parameters are set based onthe dimension of the input data and the scale ofthe dataset.
Because that the number of vocabu-lary in MOV dataset is more than other four da-tasets, so the number of units in previous twohidden layers for MOV dataset are more thanother four datasets.
We perform active learningfor 5 iterations.
In each iteration, we select andlabel 20 of the most uncertain points, and thenre-train the ADN on all of the unlabeled dataand labeled data annotated so far.
After 5 itera-tions, 100 labeled data are used for training.The classification accuracies on test data incross validation for five datasets and six me-thods are shown in Table 1.
The results of pre-vious four methods are reported by Dasguptaand Ng (2009).
For ADN method, the initial twolabeled data are selected randomly, so we repeatthirty times for each fold and the results are av-eraged.
For the randomness involved in thechoice of labeled data, all the results of otherfive methods are achieved by repeating ten timesfor each fold and then taking average on results.Through Table 1, we can see that the perfor-mance of DBN is competitive with MECH.Since MECH is the combination of spectral clus-tering, TSVM and Active learning, DBN is just aclassification method based on deep neural net-work, this result proves the good learning abilityof deep architecture.
ADN is a combination ofsemi-supervised learning and active learningbased on deep architecture, the performance ofADN is better than all other five methods on fivedatasets.
This could be contributed by: First,ADN uses a new architecture to guide the outputvector of samples belonged to different regionsof new Euclidean space, which can abstract theuseful information that are not accessible to oth-er learners; Second, ADN use an exponentialloss function to maximize the separability oflabeled data in global refinement for better dis-criminability; Third, ADN fully exploits the em-bedding information from the large amount ofunlabeled data to improve the robustness of theclassifier; Fourth, ADN can choose the usefultraining data actively, which also improve theclassification performance.Type MOV KIT ELE BOO DVDSpectral 67.3 63.7 57.7 55.8 56.2TSVM 68.7 65.5 62.9 58.7 57.3Active 68.9 68.1 63.3 58.6 58.0MECH 76.2 74.1 70.6 62.1 62.7DBN 71.3 72.6 73.6 64.3 66.7ADN 76.3 77.5 76.8 69.0 71.6Table 1.
Test Accuracy with 100 Labeled Datafor Five Datasets and Six Methods.4.3 Effect of Active LearningTo test the performance of our proposed activelearning method, we conduct following addi-tional experiments.Passive learning: We random select 100 re-views from the training fold and use them aslabeled data.
Then the proposed semi-supervised1521learning method of ADN is used to train and testthe performance.
Because of randomness, werepeat 30 times for each fold and take averageon results.
The test accuracies of passive learn-ing for five datasets are shown in Table 2.
Incomparison with ADN method in Table 1, wecan see that the proposed active learning methodyields significantly better results than randomlychosen points, which proves effectiveness ofproposed active learning method.Fully supervised learning: We train a fullysupervised classifier using all 1,000 training re-views based on the ADN architecture, results arealso shown in Table 2.
Comparing with theADN method in Table 1, we can see that em-ploying only 100 active learning points enablesus to almost reach fully-supervised performancefor three datasets.Type MOV KIT ELE BOO DVDPassive 72.2 75.0 75.0 66.0 67.9Supervised 77.2 79.4 79.1 69.3 72.1Table 2.
Test Accuracy of ADN with differentexperiment setting for Five Datasets.4.4 Semi-Supervised Learning with Va-riance of Labeled DataTo verify the performance of semi-supervisedlearning with different number of labeled data,we conduct another series of experiments on fivedatasets and show the results on Figure 3.
Werun ten-fold cross validation for each dataset.Each fold is repeated ten times and the resultsare averaged.We can see that ADN can also get a relativehigh accuracy even by using just 20 labeled re-views for training.
For most of the sentimentdatasets, the test accuracy is increasing slowlywhile the number of labeled review is growing.This proves that ADN reaches good performanceeven with few labeled reviews.5 ConclusionsThis paper proposes a novel semi-supervisedlearning algorithm ADN to address the senti-ment classification problem with a small numberof  labeled  data.
ADN  can  choose  the  proper20 30 40 50 60 70 80 90 1006062646668707274767880Number of labeled reviewTestaccuracy(%)MOVKITELEBOODVDFigure 3.
Test Accuracy of ADN with DifferentNumber of Labeled Reviews for Five Datasets.training data to be labeled manually, and fullyexploits the embedding information from thelarge amount of unlabeled data to improve therobustness of the classifier.
We propose a newarchitecture to guide the output vector of sam-ples belong to different regions of new Eucli-dean space, and use an exponential loss functionto maximize the separability of labeled data inglobal refinement for better discriminability.Moreover, ADN can make the right decisionabout which training data should be labeledbased on existing unlabeled and labeled data.
Byusing unsupervised and supervised learning ite-ratively, ADN can choose the proper trainingdata to be labeled and train the deep architectureat the same time.
Finally, the deep architecture isre-trained using the chosen labeled data and allthe unlabeled data.
We also conduct experimentsto verify the effectiveness of ADN method withdifferent number of labeled data, and demon-strate that ADN can reach very competitive clas-sification performance just by using few labeleddata.
This results show that the proposed ADNmethod, which only need fewer manual labeledreviews to reach a relatively higher accuracy,can be used to train a high performance senti-ment classification system.AcknowledgementThis work is supported in part by the NationalNatural Science Foundation of China (No.60703015 and No.
60973076).1522ReferencesBengio, Yoshua.
2007.
Learning deep architecturesfor AI.
Montreal: IRO, Universite de Montreal.Blitzer, John, Mark Dredze, and Fernando Pereira.2007.
Biographies, Bollywood, Boom-boxes andBlenders: Domain Adaptation for SentimentClassification.
In 45th Annual Meeting of theAssociation of Computational Linguistics.Dasgupta, Sajib, and Vincent Ng.
2009.
Mine theEasy, Classify the Hard: A Semi-SupervisedApproach to Automatic Sentiment Classification.In Joint Conference of the 47th Annual Meeting ofthe Association for Computational Linguistics and4th International Joint Conference on NaturalLanguage Processing of the Asian Federation ofNatural Language Processing.Gamon, Michael.
2004.
Sentiment classification oncustomer feedback data: noisy data, large featurevectors, and the role of linguistic analysis.
InInternational Conference on ComputationalLinguistics.Hinton, Geoffrey E. .
2002.
Training products ofexperts by minimizing contrastive divergence.Neural Computation, 14(8): 1771-1800.Hinton, Geoffrey E. , Simon Osindero, and Yee-Whye Teh.
2006.
A Fast Learning Algorithm forDeep Belief Nets.
Neural Computation, 18: 1527-1554.Kamvar, Sepandar, Dan Klein, and ChristopherManning.
2003.
Spectral Learning.
InInternational Joint Conferences on ArtificialIntelligence.Li, Shoushan, and Chengqing Zong.
2008.
Multi-domain Sentiment Classification.
In 46th AnnualMeeting of the Association of ComputationalLinguistics.Li, Tao, Yi Zhang, and Vikas Sindhwani.
2009.
ANon-negative Matrix Tri-factorization Approach toSentiment Classification with Lexical PriorKnowledge.
In Joint Conference of the 47thAnnual Meeting of the Association forComputational Linguistics and 4th InternationalJoint Conference on Natural Language Processingof the Asian Federation of Natural LanguageProcessing.Pang, Bo, and Lillian Lee.
2004.
A SentimentalEducation: Sentiment Analysis Using SubjectivitySummarization Based on Minimum Cuts.
In 42thAnnual Meeting of the Association ofComputational Linguistics.Pang, Bo, and Lillian Lee.
2008.
Opinion mining andsentiment analysis (Vol.
2).Pang, Bo, Lillian Lee, and ShivakumarVaithyanathan.
2002.
Thumbs up?
SentimentClassification using Machine Learning Techniques.In Conference on Empirical Methods in NaturalLanguage Processing.Raina, Rajat, Alexis Battle, Honglak Lee, BenjaminPacker, and Andrew Y. Ng.
2007.
Self-taughtlearning: transfer learning from unlabeled data.
InInternational conference on Machine learning.Ranzato, Marc'Aurelio, and Martin Szummer.
2008.Semi-supervised learning of compact documentrepresentations with deep networks.
InInternational Conference on Machine learning.Salakhutdinov, Ruslan, and Geoffrey E. Hinton.
2007.Learning a Nonlinear Embedding by PreservingClass Neighbourhood Structure.
In Proceedings ofEleventh International Conference on ArtificialIntelligence and Statistics.Sindhwani, Vikas, and Prem Melville.
2008.Document-Word Co-regularization for Semi-supervised Sentiment Analysis.
In InternationalConference on Data Mining.Tong, Simon, and Daphne Koller.
2002.
Supportvector machine active learning with applications totext classification.
Journal of Machine LearningResearch, 2: 45-66.Wan, Xiaojun.
2009.
Co-Training for Cross-LingualSentiment Classification.
In Joint Conference ofthe 47th Annual Meeting of the Association forComputational Linguistics and 4th InternationalJoint Conference on Natural Language Processingof the Asian Federation of Natural LanguageProcessing.Xia, Yunqing, Linlin Wang, Kam-Fai Wong, andMingxing Xu.
2008.
Lyric-based Song SentimentClassification with Sentiment Vector Space Model.In 46th Annual Meeting of the Association ofComputational Linguistics.Zagibalov, Taras, and John Carroll.
2008.
AutomaticSeed Word Selection for Unsupervised SentimentClassification of Chinese Text.
In InternationalConference on Computational Linguistics.Zhu, Xiaojin.
2007.
Semi-supervised learningliterature survey.
University of WisconsinMadison.1523
