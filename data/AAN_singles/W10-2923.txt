Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 192?202,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsTagging and Linking Web Forum PostsSu Nam Kim, Li Wang and Timothy BaldwinDept of Computer Science and Software EngineeringUniversity of Melbourne, Australiasunamkim@gmail.com, li.wang.d@gmail.com, tb@ldwin.netAbstractWe propose a method for annotating post-to-post discourse structure in online userforum data, in the hopes of improvingtroubleshooting-oriented information ac-cess.
We introduce the tasks of: (1) postclassification, based on a novel dialogueact tag set; and (2) link classification.
Wealso introduce three feature sets (structuralfeatures, post context features and seman-tic features) and experiment with three dis-criminative learners (maximum entropy,SVM-HMM and CRF).We achieve above-baseline results for both dialogue act andlink classification, with interesting diver-gences in which feature sets perform wellover the two sub-tasks, and go on to per-form preliminary investigation of the inter-action between post tagging and linking.1 IntroductionWith the advent of Web 2.0, there has been an ex-plosion of web authorship from individuals of allwalks of life.
Notably, social networks, blogs andweb user forums have entered the mainstream ofmodern-day society, creating both new opportuni-ties and challenges for organisations seeking to en-gage with clients or users of any description.
Onearea of particular interest is web-based user sup-port, e.g.
to aid a user in purchasing a gift for afriend, or advising a customer on how to config-ure a newly-acquired wireless router.
While suchinteractions traditionally took place on an indi-vidual basis, leading to considerable redundancyfor frequently-arising requests or problems, userforums support near-real-time user interaction inthe form of a targeted thread made up of individ-ual user posts.
Additionally, they have the poten-tial for perpetual logging to allow other users tobenefit from them.
This in turn facilitates ?sup-port sharing??i.e.
the ability for users to lookover the logs of past support interactions to deter-mine whether there is a documented, immediately-applicable solution to their current problem?on ascale previously unimaginable.
This research istargeted at this task of enhanced support sharing,in the form of text mining over troubleshooting-oriented web user forum data (Baldwin et al, toappear).One facet of our proposed strategy for enhanc-ing information access to troubleshooting-orientedweb user forum data is to preprocess threads touncover the ?content structure?
of the thread, inthe form of its post-to-post discourse structure.Specifically, we identify which earlier post(s) agiven post responds to (linking) and in what man-ner (tagging), in an amalgam of dialogue act tag-ging (Stolcke et al, 2000) and coherence-baseddiscourse analysis (Carlson et al, 2001; Wolf andGibson, 2005).
The reason we do this is gaugethe relative role/import of individual posts, to in-dex and weight component terms accordingly, ul-timately in an attempt to enhance information ac-cess.
Evidence to suggest that this structure canenhance information retrieval effectiveness comesfrom Xi et al (2004) and Seo et al (2009) (seeSection 2).To illustrate the task, consider the thread fromthe CNET forum shown in Figure 1, made up of5 posts (Post 1, ..., Post 5) with 4 distinct partici-pants (A, B, C, D).
In the first post, A initiates thethread by requesting assistance in creating a webform.
In response, B proposes a Javascript-basedsolution (i.e.
responds to the first post with a pro-posed solution), and C proposes an independentsolution based on .NET (i.e.
also responds to thefirst post with a proposed solution).
Next, A re-sponds to C?s post asking for details of how to in-clude this in a web page (i.e.
responds to the thirdpost asking for clarification), and in the final post,D proposes a different solution again (i.e.
respondsto the first post with a different solution again).192HTML Input Code - CNET Coding & scripting ForumsUser A HTML Input CodePost 1 .
.
.
Please can someone tell me how to create aninput box that asks the user to enter their ID,and then allows them to press go.
It will thenredirect to the page .
.
.User B Re: html input codePost 2 Part 1: create a form with a text field.
See.
.
.
Part 2: give it a Javascript action .
.
.User C asp.net c# videoPost 3 Ive prepared for you video.link click .
.
.User A Thank You!Post 4 Thanks a lot for that .
.
.
I have Microsoft Vi-sual Studio 6, what program should I do thisin?
Lastly, how do I actually include this in mysite?.
.
.User D A little more helpPost 5 .
.
.
You would simply do it this way: .
.
.
Youcould also just .
.
.
An example of this is:.
.
.Figure 1: Snippeted posts in a CNET threadA	ABCDAEBFEBFEBFFigure 2: Post links and dialogue act labels for theexample thread in Figure 1In this, we therefore end up with a tree-based de-pendency link structure, with each post (other thanthe initial post) relating back to a unique preced-ing post via a range of link types, as indicated inFigure 2.
Note, however, that more generally, itis possible for a post to link to multiple preced-ing posts (e.g.
refuting one proposed solution, andproposing a different solution to the problem in theinitial post).Our primary contributions in this paper are: (1)a novel post label set for post structure in webforum data, and associated dataset; and (2) a se-ries of results for post dependency linking and la-belling, which achieve strong results for the re-spective tasks.2 Related WorkRelated work exists in the broad fields of dialogueprocessing, discourse analysis and information re-trieval, and can be broken down into the followingtasks: (1) dialogue act tagging; (2) discourse ?dis-entanglement?
; (3) community question answer-ing; and (4) newsgroup/user forum search.Dialogue act (DA) tagging is a means of cap-turing the function of a given utterance relativeto an encompassing discourse, and has been pro-posed variously as a means of enhancing dialoguesummarisation (Murray et al, 2006), and track-ing commitments and promises in email (Cohenet al, 2004; Lampert et al, 2008), as well as be-ing shown to improve speech recognition accu-racy (Stolcke et al, 2000).
A wide range of DAtag sets have been proposed, usually customisedto a particular medium such as speech dialogue(Stolcke et al, 2000; Shriberg et al, 2004), task-focused email (Cohen et al, 2004; Wang et al,2007; Lampert et al, 2008) or instant messag-ing (Ivanovic, 2008).
The most immediately rel-evant DA-based work we are aware of is that ofXi et al (2004), who proposed a 5-way classifi-cation for newsgroup data (including QUESTIONand AGREEMENT/AMMENDMENT), but did notpresent any results based on the tagset.A range of supervised models have been appliedto DA classification, including graphical mod-els (Ji and Bilmes, 2005), kernel methods (Wanget al, 2007), dependency networks (Carvalhoand Cohen, 2005), transformation-based learning(Samuel et al, 1998), maxent models (Ang etal., 2005) and HMMs (Ivanovic, 2008).
There issome contention about the import of context in DAclassification, with the prevailing view being thatcontext aids classification (Carvalho and Cohen,2005; Ang et al, 2005; Ji and Bilmes, 2005), butalso evidence to suggest that strictly local mod-elling is superior (Ries, 1999; Serafin and Di Eu-genio, 2004).In this work, we draw on existing work (esp.Xi et al (2004)) in proposing a novel DA tagset customised to the analysis of troubleshooting-oriented web user forums (Section 3), and com-pare a range of text classification and structuredclassification methods for post-level DA classifi-cation.Discourse disentanglement is the process ofautomatically identifying coherent sub-discoursesin a single thread (in the context of user fo-rums/mailing lists), chat session (in the context ofIRC chat data: Elsner and Charniak (2008)), sys-tem interaction (in the context of HCI: Lemon etal.
(2002)) or document (Wolf and Gibson, 2005).The exact definition of what constitutes a sub-discourse varies across domains, but for our pur-poses, entails an attempt to resolve the informa-193tion need of the initiator by a particular approach;if there are competing approaches proposed in asingle thread, multiple sub-discourses will neces-sarily arise.
The data structure used to representthe disentangled discourse varies from a simpleconnected sub-graph (Elsner and Charniak, 2008),to a stack/tree (Grosz and Sidner, 1986; Lemonet al, 2002; Seo et al, 2009), to a full directedacyclic graph (DAG: Rose?
et al (1995), Wolf andGibson (2005), Schuth et al (2007)).
Disentan-glement has been carried out via analysis of di-rect citation/user name references (Schuth et al,2007; Seo et al, 2009), topic modelling (Lin et al,2009), and clustering over content-based featuresfor pairs of posts, optionally incorporating variousconstraints on post recency (Elsner and Charniak,2008; Wang et al, 2008; Seo et al, 2009).In this work, we follow Rose?
et al (1995) andWolf and Gibson (2005) in adopting a DAG repre-sentation of discourse structure, and draw on thewide set of features used in discourse entangle-ment to model coherence.Community question answering (cQA) is thetask of identifying question?answer pairs in agiven thread, e.g.
for the purposes of thread sum-marisation (Shrestha and McKeown, 2004) or au-tomated compilation of resources akin to Yahoo!Answers.
cQA has been applied to both mail-ing list and user forum threads, conventionallybased on question classification, followed by rank-ing of candidate answers relative to each question(Shrestha and McKeown, 2004; Ding et al, 2008;Cong et al, 2008; Cao et al, 2009).
The task issomewhat peripheral to our work, but relevant inthat it involves the implicit tagging of certain postsas containing questions/answers, as well as link-ing the posts together.
Once again, we draw on thefeatures used in cQA in this research.There has been a spike of recent interest innewsgroup/user forum search.
Xi et al (2004)proposed a structured information retrieval (IR)model for newsgroup search, based on author fea-tures, thread structure (based on the tree defined bythe reply-to structure), thread ?topology?
featuresand content-based features, and used a supervisedranking method to improve over a baseline IR sys-tem.
Elsas and Carbonell (2009) ?
building onearlier work on blog search (Elsas et al, 2008) ?proposed a probabilistic IR approach which ranksuser forum threads relative to selected posts in theoverall thread, and again demonstrated the superi-ority of this method over a model which ignoresthread structure.
Finally, Seo et al (2009) auto-matically derived thread structure from user forumthreads, and demonstrated that the IR effectivenessover the ?threaded?
structure was superior to thatusing a monolithic document representation.The observations and results of Xi et al (2004)and Seo et al (2009) that threading information(or in our case ?disentangled?
DAG structure) en-hances IR effectiveness is a core motivator for thisresearch.3 Post Label SetOur post label set contains 12 categories, intendedto capture the typical interactions that take place introubleshooting-oriented threads on technical fo-rums.
There are 2 super-categories (QUESTION,ANSWER) and 3 singleton classes (RESOLUTION,REPRODUCTION, and OTHER).
QUESTION, inturn, contains 4 sub-classes (QUESTION, ADD,CONFIRMATION, CORRECTION), while ANSWERcontains 5 sub-classes (ANSWER, ADD, CONFIR-MATION, CORRECTION, and OBJECTION), par-tially mirroring the sub-structure of QUESTION.We represent the amalgam of a super- and sub-class as QUESTION-ADD, for example.All tags other than QUESTION-QUESTION andOTHER are relational, i.e.
relate a given post to aunique earlier post.
A given post can potentiallybe labelled with multiple tags (e.g.
confirm detailsof a proposed solution, in addition to providing ex-tra details of the problem), although, based on thestrictly chronological ordering of posts in threads,a post can only link to posts earlier in the thread(and can also not cross thread boundaries).
Addi-tionally, the link structure is assumed to be tran-sitive, in that if post A links to post B and post Bto post C, post A is implicitly linked to post C. Assuch, an explicit link from post A to post C shouldexist only in the case that the link between them isnot inferrable transitively.Detailed definitions of each post tag are givenbelow.
Note that initiator refers to the user whostarted the thread with the first post.QUESTION-QUESTION (Q-Q): the post con-tains a new question, independent of thethread context that precedes it.
In general,QUESTION-QUESTION is reserved for thefirst post in a given thread.QUESTION-ADD (Q-ADD): the post supple-194ments a question by providing additionalinformation, or asking a follow-up question.QUESTION-CONFIRMATION (Q-CONF): thepost points out error(s) in a question withoutcorrecting them, or confirms details of thequestion.QUESTION-CORRECTION (Q-CORR): the postcorrects error(s) in a question.ANSWER-ANSWER (A-A): the post proposes ananswer to a question.ANSWER-ADD (A-ADD): the post supplementsan answer by providing additional informa-tion.ANSWER-CONFIRMATION (A-CONF): thepost points out error(s) in an answer withoutcorrecting them, or confirms details of theanswer.ANSWER-CORRECTION (A-CORR): the postcorrects error(s) in an answer.ANSWER-OBJECTION (A-OBJ): the post ob-jects to an answer on experiential or theoreti-cal grounds (e.g.
It won?t work.
).RESOLUTION (RES): the post confirms that ananswer works, on the basis of implementingit.REPRODUCTION (REP): the post either: (1)confirms that the same problem is being ex-perienced (by a non-initiator, e.g.
I?m seeingthe same thing.
); or (2) confirms that the an-swer should work.OTHER (OTHER): the post does not belong toany of the above classes.4 Feature DescriptionIn this section, we describe our post feature repre-sentation, in the form of four feature types.4.1 Lexical featuresAs our first feature type, we use simple lexical fea-tures, in the form of unigram and bigram tokenscontained within a given post (without stopping).We also POS tagged and lemmatised the posts,postfixing the lemmatised token with its POS tag(using Lingua::EN::Tagger and morpha (Min-nen et al, 2001)).
Finally, we bin together thecounts for each token, and represent it via its rawfrequency.4.2 Structural featuresThe identity of the post author, and position of thepost within the thread, can be indicators of thepost/link structure of a given post.
We representthe post author as a simple binary feature indicat-ing whether s/he is the thread initiator, and the postposition via its relative position in the thread (as aratio, relative to the total number of posts).4.3 Post context featuresAs mentioned in Section 2, post context has gen-erally (but not always) been shown to enhance theclassification accuracy of DA tagging tasks, in theform of Markov features providing predicted postlabels for previous posts, or more simply, post-to-post similarity.
We experiment with a range ofpost context features, all of which are compatiblewith features both from the same label set as thatbeing classified (e.g.
link features for link classifi-cation), as well as features from a second label set(e.g.
DA label features for link classification).Previous Post: There is a strong prior for poststo link to their immediately preceding post (as ob-served for 79.9% of the data in our dataset), andalso strong sequentiality in our post label set (e.g.a post following a Q-Q is most likely to be an A-A).
As such, we represent the predicted post labelof the immediately preceding post, as a first-orderMarkov feature, as well as a binary feature to in-dicate whether the author of the previous post alsoauthored the current post.Previous Post from Same Author: A givenuser tends to author posts of the same basic type(e.g.
QUESTION or ANSWER) in a given thread,and pairings such as A-A and A-CONF from agiven author are very rare.
To capture this obser-vation, we look to see if the author of the currentpost has posted earlier in the thread, and if so, in-clude the label and relative location (in posts) oftheir most recent previous post.Full History: As a final option, we include thepredictions for all posts P1, ..., Pi?1 preceding thecurrent post Pi.4.4 Semantic featuresWe tested four semantic features based on postcontent and title.195Title Similarity: For forums such as CNETwhich include titles for individual posts (as rep-resented in Figure 1), a post having the same orsimilar title as a previous post is often a strongindicator that it responds to that post.
This bothprovides a strong indicator of which post a givenpost responds (links) to, and can aid in DA tag-ging.
We use simple cosine similarity to find thepost with the most-similar title, and represent itsrelative location to the current post.Post Similarity: Posts of the same general typetend to have similar content and be linked.
Forexample, A-A and A-ADD posts tend to sharecontent.
We capture this by identifying the postwith most-similar content based on cosine similar-ity, and represent its relative location to the currentpost.Post Characteristics: We separately representthe number of question marks, exclamation marksand URLs in the current post.
In general, ques-tion marks occur in QUESTION and CONFIRMA-TION posts, exclamation marks occur in RES andOBJECTION posts, and URLs occur in A-A andA-ADD posts.User Profile: Some authors tend to answer ques-tions more, while others tend to ask more ques-tions.
We capture the class priors for the author ofthe current post by the distribution of post labelsin their posts in the training data.5 Experimental SetupAs our dataset, we collected 320 threads contain-ing a total of 1,332 posts from the Operating Sys-tem, Software, Hardware, and Web Developmentsub-forums of CNET.1The annotation of post labels and links was car-ried by two annotators in a custom-built web inter-face which supported multiple labels and links fora given post.
For posts with multilabels, we useda modified version of Cohen?s Kappa, which re-turned ?
values of 0.59 and 0.78 for the post labeland link annotations, respectively.
Any disagree-ments in labelling were resolved through adjudi-cation.Of the 1332 posts, 65 posts have multiple labels(which possibly link to a common post) and 22posts link to two different links.
The majority postlabel in the dataset is A-A (40.30%).1http://forums.cnet.com/?tag=TOCleftColumn.0We built machine learners using a conven-tional Maximum Entropy (ME) learner,2 as well astwo structural learners, namely: (1) SVM-HMMs(Joachims et al, 2009), as implemented in SVM-struct3, with a linear kernel; and (2) conditionalrandom fields (CRFs) using CRF++.4 SVM-HMMs and CRFs have been successfully appliedto a range of sequential tagging tasks such assyllabification (Bartlett et al, 2009), chunk pars-ing (Sha and Pereira, 2003) and word segmen-tation (Zhao et al, 2006).
Both are discrimina-tive models which capture structural dependen-cies, which is highly desirable in terms of mod-elling sequential preferences between post labels(e.g.
A-CONF typically following a A-A).
SVM-HMM has the additional advantage of scaling tolarge numbers of features (namely the lexical fea-tures).
As such, we only experiment with lexicalfeatures for SVM-HMM and ME.All of our evaluation is based on stratified 10-fold cross-validation, stratifying at the thread levelto ensure that if a given post is contained in thetest data for a given iteration, all other posts inthat same thread are also in the test data (or morepertinently, not in the training data).
We evalu-ate using micro-averaged precision, recall and F-score (?
= 1).
We test the statistical significanceof all above-baseline results using randomised es-timation (p < 0.05; Yeh (2000)), and present allsuch results in bold in our results tables.In our experiments, we first look at the postclassification task in isolation (i.e.
we predictwhich labels to associate with each post, under-specifying which posts those labels relate to).
Wethen move on to look at the link classification task,again in isolation (i.e.
we predict which previousposts each post links to, underspecifying the na-ture of the link).
Finally, we perform preliminaryinvestigation of the joint task of DA and link clas-sification, by incorporating DA class features intothe link classification task.6 DA Classification ResultsOur first experiment is based on post-level dia-logue act (DA) classification, ignoring link struc-ture in the first instance.
That is, we predict thelabels on edges emanating from each post in theDAG representation of the post structure, without2http://maxent.sourceforge.net/3http://www.cs.cornell.edu/People/tj/svm_light/svm_hmm.html4http://crfpp.sourceforge.net/196Features CRF SVM-HMM MELexical ?
.566 .410Structural .742 .638 .723Table 1: DA classification F-score with lexical andstructural features (above-baseline results in bold)specifying the edge destination.
Returning to ourexample in Figure 2, e.g., the gold-standard clas-sification for Post 1 would be Q-Q, Post 2 wouldbe A-A, etc.As a baseline for DA classification, simple ma-jority voting attains an F-score of 0.403, based onthe A-A class.
A more realistic baseline, how-ever, is a position-conditioned variant, where thefirst post is always classified as Q-Q, and all sub-sequent posts are classified as A-A, achieving anF-score of 0.641.6.1 Lexical and structural featuresFirst, we experiment with lexical and structuralfeatures (recalling that we are unable to scale theCRF model to full lexical features).
Lexical fea-tures produce below-baseline performance, whilesimple structural features immediately lead to animprovement over the baseline for CRF and ME.The reason for the poor performance with lex-ical features is that our dataset contains onlyaround 1300 posts, each of which is less than 100words in length on average.
The models are sim-ply unable to generalise over this small amount ofdata, and in the case of SVM-HMM, the presenceof lexical features, if anything, appears to obscurethe structured nature of the labelling task (i.e.
theclassifier is unable to learn the simple heuristicused by the modified majority class baseline).The success of the structural features, on theother hand, points to the presence of predictablesequences of post labels in the data.
That SVM-HMM is unable to achieve baseline performancewith structural features is slightly troubling.6.2 Post context featuresNext, we test the two post context features: Previ-ous Post (P) and Previous Post from Same Author(A).
Given the success of structural features, weretain these in our experiments.
Note that the la-bels used in the post context are those which areinteractively learned by that model for the previ-ous posts.Table 2 presents the results for structural fea-Features CRF SVM-HMM MEStruct+R .740 .640 .632Struct+A .742 .676 .693Struct+F .744 .641 .577Struct+RA .397 .636 .665Struct+AF .405 .642 .586Table 2: DA classification F-score with structuraland DA-based post context features (R = ?Previ-ous Post?, A = ?Previous Post from Same Author?,and F = ?Full History?
; above-baseline results inbold)tures combined with DA-based post context; wedo not present any combinations of Previous Postand Full History, as Full History includes the Pre-vious Post.Comparing back to the original results usingonly the structural results, we can observe that Pre-vious Post from Same Author and Full History (Aand F, resp., in the table) lead to a slight incre-ment in F-score for both CRF and SVM-HMM,but degrade the performance of ME.
Previous Postleads to either a marginal improvement, or a dropin results, most noticeably for ME.
It is slightlysurprising that the CRF should benefit from con-text features at all, given that it is optimising overthe full tag sequence, but the impact is relativelylocalised, and when all sets of context featuresare used, the combined weight of noisy featuresappears to swamp the learner, leading to a sharpdegradation in F-score.6.3 Semantic featuresWe next investigate the relative impact of the se-mantic features, once again including structuralfeatures in all experiments.
Table 3 presents the F-score using the different combinations of semanticfeatures.Similarly to the post context features, the se-mantic features produced slight increments overthe structural features in isolation, especially forCRF and ME.
For the first time, SVM-HMMachieved above-baseline results, when incorporat-ing title similarity and post characteristics.
Of theindividual semantic features, title and post simi-larity appear to be the best performers.
Slightlydisappointingly, the combination of semantic fea-tures generally led to a degradation in F-score, al-most certainly due to data sparseness.
The bestoverall result was achieved with CRF, incorporat-197Features CRF SVM-HMM MEStruct+T .751 .636 .660Struct+P .747 .636 .662Struct+C .738 .587 .630Struct+U .722 .564 .620Struct+TP .740 .627 .720Struct+TC .744 .646 .589Struct+TU .738 .600 .609Struct+PC .745 .630 .583Struct+PU .736 .626 .605Struct+CU .730 .599 .619Struct+TPC .739 .622 .580Struct+TPU .729 .613 .6120Struct+TCU .750 .611 .6120Struct+PCU .738 .616 .614Struct+TPCU .737 .619 .605Table 3: DA classification F-score with semanticfeatures (T = ?Title Similarity?, P = ?Post Simi-larity?, C = ?Post Characteristics?, and U = ?UserProfile?
; above-baseline results in bold)ing structural features and title similarity, at an F-score of 0.751.To further explore the interaction between postcontext and semantic features, we built CRF clas-sifiers for different combinations of post contextand semantic features, and present the results inTable 4.5 We achieved moderate gains in F-score,with all post context features, in combination withstructural features, post similarity and post char-acteristics achieving an F-score of 0.753, slightlyhigher than the best result achieved for just struc-tural and post context features.It is important to refer back to the results forlexical features (comparable to what would havebeen achieved with a standard text categorisationapproach to the task), and observe that we haveachieved far higher F-scores using features cus-tomised to user forum data.
It is also importantto reflect that post context (in terms of the featuresand the structured classification results of CRF)appears to markedly improve our results, contrast-ing with the results of Ries (1999) and Serafin andDi Eugenio (2004).5We omit the results for Full History post context for rea-sons of space, but there is relatively little deviation from thenumbers presented.Features R A RAStruct+T .649 .649 .649Struct+P .737 .736 .742Struct+C .741 .741 .742Struct+U .745 .742 .737Struct+TP .645 .656 .658Struct+TC .383 .402 .408Struct+TU .650 .652 .652Struct+PC .730 .743 .753Struct+PU .232 .232 .286Struct+CU .719 .471 .710Struct+TPC .498 .469 .579Struct+TPU .248 .232 .248Struct+TCU .388 .377 .380Struct+PCU .231 .231 .261Struct+TPCU .231 .231 .231Table 4: DA classification F-score for CRF withdifferent combinations of post context features andsemantic features (R = ?Previous Post?, and A= ?Previous Post from Same Author?
; T = ?Ti-tle Similarity?, P = ?Post Similarity?, C = ?PostCharacteristics?, and U = ?User Profile?
; above-baseline results in bold)7 Link Classification ResultsOur second experiment is based on link classifi-cation in isolation.
Here, we predict unlabellededges, e.g.
in Figure 2, the gold-standard classifi-cation for Post 1 would be NULL, Post 2 would bePost 1, Post 3 would be Post 1, etc.Note that the initial post cannot link to any otherpost, and also that the second post always linksto the first post.
As this is a hard constraint onthe data, and these posts simply act to inflate theoverall numbers, we exclude all first and secondposts from our evaluation of link classification.We experimented with a range of baselines aspresented in Table 5, but found that the best per-former by far was the simple heuristic of linkingeach post (except for the initial post) to its imme-diately preceding post.
This leads to an F-score of0.631, comparable to that for the post classifica-tion task.7.1 Lexical and structural featuresOnce again, we started by exploring the effective-ness of lexical and structural features using thethree learners, as detailed in Table 6.Similarly to the results for post classification,198Baseline Prec Rec F-scorePrevious post .641 .622 .631First post .278 .269 .274Title similarity .311 .301 .306Post similarity .255 .247 .251Table 5: Baselines for link classificationFeatures CRF SVM-HMM MELexical ?
.154 .274Structural .446 .220 .478Table 6: Link classification F-score with lexicaland structural features (above-baseline results inbold)structural features are more effective than lexicalfeatures for link classification, but this time, nei-ther feature set approaches the baseline F-scorefor any of the learners.
Once again, the results forSVM-HMM are well below those for the other twolearners.7.2 Post context featuresNext, we experiment with link-based post con-text features, in combination with the structuralfeatures, as the results were found to be consis-tently better when combined with the structuralfeatures (despite the below-baseline performanceof the structural features in this case).
The link-based post context features in all cases are gener-ated using the CRF with structural features fromTable 6.
As before, we do not present any combi-nations of Previous Post and Full History, as FullHistory includes the Previous PostAs seen in Table 9, here, for the first time, weachieve an above-baseline result for link classifi-cation, for SVM and ME based on Previous Postfrom Same Author in isolation, and also some-times in combination with the other feature sets.The results for CRF also improve, but not to alevel of statistical significance over the baseline.Similarly to the results for DA classification, theresults for CRF drop appreciably when we com-bine feature sets.7.3 Semantic featuresFinally, we experiment with semantic features,once again in combination with structural features.The results are presented in Table 8.The results for semantic features largely mir-Features CRF SVM-HMM MEStruct+R .234 .605 .618Struct+A .365 .665 .665Struct+F .624 .648 .615Struct+RA .230 .615 .661Struct+AF .359 .663 .621Table 7: Link classification F-score with structuraland link-based post context features (R = ?Previ-ous Post?, A = ?Previous Post from Same Author?,and F = ?Full History?
; above-baseline results inbold)Features CRF SVM-HMM MEStruct+T .464 .223 .477Struct+P .433 .198 .453Struct+C .438 .213 .419Struct+U .407 .160 .376Struct+TP .459 .194 .491Struct+TC .449 .229 .404Struct+TU .456 .174 .353Struct+PC .422 .152 .387Struct+PU .439 .166 .349Struct+CU .397 .178 .366Struct+TPC .449 .185 .418Struct+TPU .449 .160 .365Struct+TCU .459 .185 .358Struct+PCU .439 .161 .358Struct+TPCU .443 .163 .365Table 8: Link classification F-score with semanticfeatures (T = ?Title Similarity?, P = ?Post Simi-larity?, C = ?Post Characteristics?, and U = ?UserProfile?
; above-baseline results in bold)ror those for post classification: small improve-ments are observed for title similarity with CRF,but otherwise, the results degrade across the board,and the combination of different feature sets com-pounds this effect.The best overall result achieved for link classifi-cation is thus the 0.743 for CRF with the structuraland post context features.We additionally experimented with combina-tions of features as for post classification, but wereunable to improve on this result.7.4 Link Classification using DA FeaturesUltimately, we require both DA and link classifica-tion of each post, which is possible by combiningthe outputs of the component classifiers described199Features CRF SVM-HMM MEStruct+R .586 .352 .430Struct+A .591 .278 .568Struct+F .704 .477 .546Struct+RA .637 .384 .551Struct+AF .743 .527 .603Table 9: Link classification F-score with structuraland post-based post context features (R = ?Previ-ous Post?, A = ?Previous Post from Same Author?,and F = ?Full History?
; above-baseline results inbold)above, by rolling the two tasks into a single clas-sification task, or alternatively by looking to jointmodelling methods.
As a preliminary step in thisdirection, and means of exploring the interactionbetween the two tasks, we repeat the experimentbased on post context features from above (seeSection 7.2), but rather than using link-based postcontext, we use DA-based post context.As can be seen in Table 9, the results for SVM-HMM and ME drop appreciably as compared tothe results using link-based post context in Table 9,while the results for CRF jump to the highest levelachieved for the task for all three learners.
Theeffect can be ascribed to the ability of CRF tonatively model the (bidirectional) link classifica-tion history in the process of performing structuredlearning, and the newly-introduced post featurescomplementing the link classification task.8 Discussion and Future WorkUltimately, we require both DA and link classifica-tion of each post, which is possible in (at least) thefollowing three ways: (1) by combining the out-puts of the component classifiers described above;(2) by rolling the two tasks into a single classifi-cation task; or (3) by looking to joint modellingmethods.
Our results in Section 7.4 are suggestiveof the empirical potential of performing the twotasks jointly, which we hope to explore in futurework.One puzzling effect observed in our experi-ments was the generally poor results for SVM.
Er-ror analysis indicates that the classifier was heav-ily biased towards the high-frequency classes, e.g.classifying all posts as either Q-Q or A-A for DAclassification.
The classifications for the other twolearners were much more evenly spread across thedifferent classes.CRF was limited in that it was unable to cap-ture lexical features, but ultimately, lexical fea-tures were found to be considerably less effec-tive than structural and post context features forboth tasks, and the ability of the CRF to opti-mise the post labelling over the full sequence ofposts in a thread more than compensated for thisshortcoming.
Having said this, there is more workto be done exploring synergies between the dif-ferent feature sets, especially for DA classifica-tion where all feature sets were found to produceabove-baseline results.Another possible direction for future research isto explore the impact of inter-post time on linkstructure, based on the observation that follow-up posts from the initiator tend to be tempo-rally adjacent to posts they respond to with rela-tively short time intervals, while posts from non-initiators which are well spaced out tend not to re-spond to one another.
Combining this with pro-filing of the cross-thread behaviour of individualforum participants (Weimer et al, 2007; Lui andBaldwin, 2009), and formal modelling of ?forumbehaviour?
is also a promising line of research,taking the lead from the work of Go?tz et al (2009),inter alia.9 ConclusionIn this work, we have proposed a method foranalysing post-to-post discourse structure in on-line user forum data, in the form of post link-ing and dialogue act tagging.
We introducedthree feature sets: structural features, post con-text features and semantic features.
We exper-imented with three learners (maximum entropy,SVM-HMM and CRF), and established that CRFis the superior approach to the task, achievingabove-baseline results for both post and link clas-sification.
We also demonstrated the complemen-tarity of the proposed feature sets, especially forthe post classification task, and carried out a pre-liminary exploration of the interaction between thelinking and dialogue act tagging tasks.AcknowledgementsThis research was supported in part by fundingfrom Microsoft Research Asia.ReferencesJeremy Ang, Yang Liu, and Elizabeth Shriberg.
2005.Automatic dialog act segmentation and classifica-200tion in multiparty meetings.
In Proceedings ofthe 2005 IEEE International Conference on Acous-tics, Speech, and Signal Processing (ICASSP 2005),pages 1061?1064, Philadelphia, USA.Timothy Baldwin, David Martinez, Richard Penman,Su Nam Kim, Marco Lui, Li Wang, and AndrewMacKinlay.
to appear.
Intelligent Linux informa-tion access by data mining: the ILIAD project.
InProceedings of the NAACL 2010 Workshop on Com-putational Linguistics in a World of Social Media:#SocialMedia, Los Angeles, USA.Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.2009.
On the syllabification of phonemes.
In Pro-ceedings of the North American Chapter of the As-sociation for Computational Linguistics ?
HumanLanguage Technologies 2009 (NAACL HLT 2009),pages 308?316, Boulder, USA.Xin Cao, Gao Cong, Bin Cui, Christian S. Jensen, andCe Zhang.
2009.
The use of categorization infor-mation in language models for question retrieval.
InProceedings of the 18th ACM Conference on Infor-mation and Knowledge Management (CIKM 2009),pages 265?274, Hong Kong, China.Lynn Carlson, Daniel Marcu, and Mary EllenOkurowski.
2001.
Building a discourse-taggedcorpus in the framework of rhetorical structure the-ory.
In Proceedings of the Second SIGdial Work-shop on Discourse and Dialogue, pages 1?10, Aal-borg, Denmark.
Association for Computational Lin-guistics Morristown, NJ, USA.Vitor R. Carvalho and William W. Cohen.
2005.
Onthe collective classification of email ?speech acts?.In Proceedings of 28th International ACM-SIGIRConference on Research and Development in Infor-mation Retrieval (SIGIR 2005), pages 345?352.William W. Cohen, Vitor R. Carvalho, and Tom M.Mitchell.
2004.
Learning to classify email into?speech acts?.
In Proceedings of the 2004 Con-ference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2004), pages 309?316,Barcelona, Spain.Gao Cong, Long Wang, Chin-Yew Lin, Young-InSong, and Yueheng Sun.
2008.
Finding question-answer pairs from online forums.
In Proceedings of31st International ACM-SIGIR Conference on Re-search and Development in Information Retrieval(SIGIR?08), pages 467?474, Singapore.Shilin Ding, Gao Cong, Chin-Yew Lin, and XiaoyanZhu.
2008.
Using conditional random fields to ex-tract context and answers of questions from onlineforums.
In Proceedings of the 46th Annual Meet-ing of the ACL: HLT (ACL 2008), pages 710?718,Columbus, USA.Jonathan L. Elsas and Jaime G. Carbonell.
2009.
Itpays to be picky: An evaluation of thread retrievalin online forums.
In Proceedings of 32nd Inter-national ACM-SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR?09),pages 714?715, Boston, USA.Jonathan L. Elsas, Jaime Arguello, Jamie Callan, andJaime G. Carbonell.
2008.
Retrieval and feed-back models for blog feed search.
In Proceedings of31st International ACM-SIGIR Conference on Re-search and Development in Information Retrieval(SIGIR?08), pages 347?354, Singapore.Micha Elsner and Eugene Charniak.
2008.
You talk-ing to me?
a corpus and algorithm for conversationdisentanglement.
In Proceedings of the 46th AnnualMeeting of the ACL: HLT (ACL 2008), pages 834?842, Columbus, USA.Michaela Go?tz, Jure Leskovec, Mary McGlohon, andChristos Faloutsos.
2009.
Modeling blog dynamics.In Proceedings of the Third International Confer-ence on Weblogs and Social Media (ICWSM 2009),pages 26?33, San Jose, USA.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intention and the structure of discourse.
Com-putational Linguistics, 12(3):175?204.Edward Ivanovic.
2008.
Automatic instant messagingdialogue using statistical models and dialogue acts.Master?s thesis, University of Melbourne.Gang Ji and Jeff Bilmes.
2005.
Dialog act tag-ging using graphical models.
In Proceedings ofthe 2005 IEEE International Conference on Acous-tics, Speech, and Signal Processing (ICASSP 2005),pages 33?36, Philadelphia, USA.Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu.
2009.
Cutting-plane training ofstructural SVMs.
Machine Learning, 77(1):27?59.Andrew Lampert, Robert Dale, and Ce?cile Paris.2008.
The nature of requests and commitments inemail messages.
In Proceedings of the AAAI 2008Workshop on Enhanced Messaging, pages 42?47,Chicago, USA.Oliver Lemon, Alex Gruenstein, and Stanley Pe-ters.
2002.
Collaborative activities and multi-tasking in dialogue systems.
Traitement Automa-tique des Langues (TAL), Special Issue on Dialogue,43(2):131?154.Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,Wei Wang, and Lei Zhang.
2009.
Modeling se-mantics and structure of discussion threads.
In Pro-ceedings of the 18th International Conference on theWorld Wide Web (WWW 2009), pages 1103?1104,Madrid, Spain.Marco Lui and Timothy Baldwin.
2009.
You are whatyou post: User-level features in threaded discourse.In Proceedings of the Fourteenth Australasian Doc-ument Computing Symposium (ADCS 2009), Syd-ney, Australia.201Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?223.Gabriel Murray, Steve Renals, Jean Carletta, and Jo-hanna Moore.
2006.
Incorporating speaker and dis-course features into speech summarization.
In Pro-ceedings of the Main Conference on Human Lan-guage Technology Conference of the North Amer-ican Chapter of the Association of ComputationalLinguistics, pages 367?374.Klaus Ries.
1999.
HMM and neural networkbased speech act detection.
In Proceedings of the1999 IEEE International Conference on Acoustics,Speech, and Signal Processing (ICASSP-99), pages497?500, Phoenix, USA.Carolyn Penstein Rose?, Barbara Di Eugenio, Lori S.Levin, and Carol Van Ess-Dykema.
1995.Discourse processing of dialogues with multiplethreads.
In Proceedings of the 33rd Annual Meet-ing of the Association for Computational Linguis-tics, pages 31?38, Cambridge, USA.Ken Samuel, Carbeery Sandra Carberry, and K. Vijay-Shanker.
1998.
Dialogue act tagging withtransformation-based learning.
In Proceedings ofthe 36th Annual Meeting of the ACL and 17th In-ternational Conference on Computational Linguis-tics (COLING/ACL-98), pages 1150?1156, Mon-treal, Canada.Anne Schuth, Maarten Marx, and Maarten de Rijke.2007.
Extracting the discussion structure in com-ments on news-articles.
In Proceedings of the 9thAnnual ACM International Workshop on Web Infor-mation and Data Management, pages 97?104, Lis-boa, Portugal.Jangwon Seo, W. Bruce Croft, and David A. Smith.2009.
Online community search using thread struc-ture.
In Proceedings of the 18th ACM Conferenceon Information and Knowledge Management (CIKM2009), pages 1907?1910, Hong Kong, China.Riccardo Serafin and Barbara Di Eugenio.
2004.FLSA: Extending latent semantic analysis with fea-tures for dialogue act classification.
In Proceedingsof the 42nd Annual Meeting of the Association forComputational Linguistics (ACL 2004), pages 692?699, Barcelona, Spain.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Proceedingsof the 3rd International Conference on Human Lan-guage Technology Research and 4th Annual Meetingof the NAACL (HLT-NAACL 2003), pages 213?220,Edmonton, Canada.Lokesh Shrestha and Kathleen McKeown.
2004.
De-tection of question-answer pairs in email conver-sations.
In Proceedings of the 20th InternationalConference on Computational Linguistics (COLING2004), pages 889?895, Geneva, Switzerland.Elinzabeth Shriberg, Raj Dhillon, Sonali Bhagat,Jeremy Ang, and Hannah Carvey.
2004.
The ICSImeeting recorder dialog act (MRDA) corpus.
InProceedings of the 5th SIGdial Workshop on Dis-course and Dialogue, pages 97?100, Cambridge,USA.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-abeth Shriberg, Rebecca Bates, Daniel Jurafsky,Pail Taylor, Rachel Martin, Carol Van Ess-Dykema,and Marie Meteer.
2000.
Dialogue Act Mod-eling for Automatic Tagging and Recognition ofConversational Speech.
Computational Linguistics,26(3):339?373.Yi-ChiaWang, Mahesh Joshi, and Carolyn Rose?.
2007.A feature based approach to leveraging context forclassifying newsgroup style discussion segments.
InProceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics CompanionVolume Proceedings of the Demo and Poster Ses-sions (ACL 2007), pages 73?76, Prague, Czech Re-public.Yi-Chia Wang, Mahesh Joshi, William W. Cohen, andCarolyn Rose?.
2008.
Recovering implicit threadstructure in newsgroup style conversations.
In Pro-ceedings of the Second International Conference onWeblogs and Social Media (ICWSM 2008), pages152?160, Seattle, USA.Markus Weimer, Iryna Gurevych, and MaxMu?hlha?user.
2007.
Automatically assessingthe post quality in online discussions on software.In Proceedings of the 45th Annual Meeting ofthe ACL: Interactive Poster and DemonstrationSessions, pages 125?128, Prague, Czech Republic.Florian Wolf and Edward Gibson.
2005.
Representingdiscourse coherence: A corpus-based study.
Com-putational Linguistics, 31(2):249?287.Wensi Xi, Jesper Lind, and Eric Brill.
2004.
Learningeffective ranking functions for newsgroup search.In Proceedings of 27th International ACM-SIGIRConference on Research and Development in In-formation Retrieval (SIGIR 2004), pages 394?401.Sheffield, UK.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Pro-ceedings of the 18th International Conference onComputational Linguistics (COLING 2000), pages947?953, Saarbru?cken, Germany.Hai Zhao, Chang-Ning Huang, and Mu Li.
2006.
Animproved Chinese word segmentation system withconditional random field.
In Proceedings of the FifthSIGHAN Workshop on Chinese Language Process-ing, pages 162?165.
Sydney, Australia.202
