Proceedings of NAACL HLT 2007, pages 276?283,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsEstimating the Reliability of MDP Policies: A Confidence Interval ApproachJoel R. TetreaultUniversity of PittsburghLRDCPittsburgh PA, 15260, USAtetreaul@pitt.eduDan BohusCarnegie Mellon UniversityDept.
of Computer SciencePittsburgh, PA, 15213, USAdbohus@cs.cmu.eduDiane J. LitmanUniversity of PittsburghDept.
of Computer ScienceLRDCPittsburgh PA, 15260, USAlitman@cs.pitt.eduAbstractPast approaches for using reinforcementlearning to derive dialog control policieshave assumed that there was enough col-lected data to derive a reliable policy.
Inthis paper we present a methodology fornumerically constructing confidence inter-vals for the expected cumulative rewardfor a learned policy.
These intervals areused to (1) better assess the reliabilityof the expected cumulative reward, and(2) perform a refined comparison betweenpolicies derived from different MarkovDecision Processes (MDP) models.
Weapplied this methodology to a prior ex-periment where the goal was to select thebest features to include in the MDP state-space.
Our results show that while someof the policies developed in the prior workexhibited very large confidence intervals,the policy developed from the best featureset had a much smaller confidence intervaland thus showed very high reliability.1 IntroductionNLP researchers frequently have to deal with issuesof data sparsity.
Whether the task is machine transla-tion or named-entity recognition, the amount of dataone has to train or test with can greatly impact the re-liability and robustness of one?s models, results andconclusions.One research area that is particularly sensitive tothe data sparsity issue is machine learning, specifi-cally in using Reinforcement Learning (RL) to learnthe optimal action for a dialogue system to makegiven any user state.
Typically this involves learn-ing from previously collected data or interacting inreal-time with real users or user simulators.
One ofthe biggest advantages to this machine learning ap-proach is that it can be used to generate optimal poli-cies for every possible state.
However, this methodrequires a thorough exploration of the state-space tomake reliable conclusions on what the best actionsare.
States that are infrequently visited in the train-ing set could be assigned sub-optimal actions, andtherefore the resulting dialogue manager may notprovide the best interaction for the user.In this work, we present an approach for esti-mating the reliability of a policy derived from col-lected training data.
The key idea is to take into ac-count the uncertainty in the model parameters (MDPtransition probabilities), and use that information tonumerically construct a confidence interval for theexpected cumulative reward for the learned policy.This confidence interval approach allows us to: (1)better assess the reliability of the expected cumula-tive reward for a given policy, and (2) perform a re-fined comparison between policies derived from dif-ferent MDP models.We apply the proposed approach to our previouswork (Tetreault and Litman, 2006) in using RL toimprove a spoken dialogue tutoring system.
In thatwork, a dataset of 100 dialogues was used to de-velop a methodology for selecting which user statefeatures should be included in the MDP state-space.But are 100 dialogues enough to generate reliablepolicies?
In this paper we apply our confidence in-276terval approach to the same dataset in an effort to in-vestigate how reliable our previous conclusions are,given the amount of available training data.In the following section, we discuss the priorwork and its data sparsity issue.
In section 3, wedescribe in detail our confidence interval methodol-ogy.
In section 4, we show how this methodologyworks by applying it to the prior work.
In sections 5and 6, we present our conclusions and future work.2 Previous WorkPast research into using RL to improve spoken di-alogue systems has commonly used Markov Deci-sion Processes (MDP?s) (Sutton and Barto, 1998)to model a dialogue (such as (Levin and Pieraccini,1997) and (Singh et al, 1999)).A MDP is defined by a set of states {si}i=1..n,a set of actions {ak}k=1..p, and a set of transitionprobabilities which reflect the dynamics of the en-vironment {p(si|sj, ak)}k=1..pi,j=1..n: if the model is attime t in state sj and takes action ak, then it willtransition to state si with probability p(si|sj , ak).Additionally, an expected reward r(si, sj , ak) is de-fined for each transition.
Once these model parame-ters are known, a simple dynamic programming ap-proach can be used to learn the optimal control pol-icy pi?, i.e.
the set of actions the model should takeat each state, to maximize its expected cumulativereward.The dialog control problem can be naturally castin this formalism: the states {si}i=1..n in the MDPcorrespond to the dialog states (or an abstractionthereof), the actions {ak}k=1..p correspond to theparticular actions the dialog manager might take,and the rewards r(si, sj , ak) are defined to reflecta particular dialog performance metric.
Once theMDP structure has been defined, the model param-eters {p(si|sj, ak)}k=1..pi,j=1..n are estimated from a cor-pus of dialogs (either real or simulated), and, basedon them, the policy which maximizes the expectedcumulative reward is computed.While most work in this area has focused on de-veloping the best policy (such as (Walker, 2000),(Henderson et al, 2005)), there has been relativelylittle work done with respect to selecting the bestfeatures to include in the MDP state-space.
For in-stance, Singh et al (1999) showed that dialoguelength was a useful state feature and Frampton andLemon (2005) showed that the user?s last dialogueact was also useful.
In our previous work, we com-pare the worth of several features.
In addition, Paekand Chickering?s (2005) work showed how a state-space can be reduced by only selecting features thatare relevant to maximizing the reward function.The motivation for this line of research is that ifone can properly select the most informative fea-tures, one develops better policies, and thus a bet-ter dialogue system.
In the following sections wesummarize our past data, approach, results, and is-sue with policy reliability.2.1 MDP StructureFor this study, we used an annotated corpus ofhuman-computer spoken dialogue tutoring sessions.The fixed-policy corpus contains data collected from20 students interacting with the system for five prob-lems (for a total of 100 dialogues of roughly 50 turnseach).
The corpus was annotated with 5 state fea-tures (Table 1).
It should be noted that two of thefeatures, Certainty and Frustration, were manuallyannotated while the other three were done automat-ically.
All features are binary except for Certaintywhich has three values.State ValuesCorrectness Student is correct or incorrectin the current turnCertainty Student is certain, neutralor uncertain in the current turnConcept Repetition A particular concept is either newor repeatedFrustration Student is frustrated or notin the current turnPercent Correct Student answers over 66% ofquestions correctly in dialogueso far, or lessTable 1: State Features in Tutoring CorpusFor the action set {ak}k=1..p, we looked at whattype of question the system could ask the studentgiven the previous state.
There are a total of fourpossible actions: ask a short answer question (onethat requires a simple one word response), a com-plex answer question (one that requires a longer,deeper response), ask both a simple and complexquestion in the same turn, or do not ask a questionat all (give a hint).
The reward function r was the277learning gain of each student based on a pair of testsbefore and after the entire session of 5 dialogues.The 20 students were split into two groups (highand low learners) based on their learning gain, so10 students and their respective five dialogues weregiven a positive reward of +100, while the remain-der were assigned a negative reward of -100.
Therewards were assigned in the final dialogue state, acommon approach when applying RL in spoken di-alogue systems.2.2 Approach and ResultsTo investigate the usefulness of different features,we took the following approach.
We started withtwo baseline MDPs.
The first model (Baseline 1)used only the Correctness feature in the state-space.The second model (Baseline 2) included both theCorrectness and Certainty features.
Next we con-structed 3 new models by adding each of the remain-ing three features (Frustration, Percent Correct andConcept Repetition) to the Baseline 2 model.We defined three metrics to compare the policiesderived from these MDPs: (1) Diff?s: the number ofstates whose policy differs from the Baseline 2 pol-icy, (2) Percent Policy change (P.C.
): the weightedamount of change between the two policies (100%indicates total change), and (3) Expected Cumula-tive Reward (or ECR) which is the average rewardone would expect in that MDP when in the state-space.The intuition is that if a new feature were rele-vant, the corresponding model would lead to a dif-ferent policy and a better expected cumulative re-ward (when compared to the baseline models).
Con-versely, if the features were not useful, one wouldexpect that the new policies would look similar(specifically, the Diff?s count and % Policy Changewould be low) or produce similar expected cumula-tive rewards to the original baseline policy.The results of this analysis are shown in Table 2 1The Diff?s and Policy Change metrics are undefinedfor the two baselines since we only use these twometrics to compare the other three features to Base-1Please note that to due to refinements in code, there is aslight difference between the ECR?s reported in this work andthe ECR?s reported in the previous work, for the three featuresadded to Baseline 2.
These changes did not alter the rankingsof these models, or the conclusions of the previous work.line 2.
All three metrics show that the best featureto add to the Baseline 2 model is Concept Repetitionsince it results in the most change over the Baseline2 policy, and also the expected reward is the highestas well.
For the remainder of this paper, when werefer to Concept Repetition, Frustration, or PercentCorrectness, we are referring to the model that in-cludes that feature as well as the Baseline 2 featuresCorrectness and Certainty.State Feature # Diff?s % P.C.
ECRBaseline 1 N/A N/A 6.15Baseline 2 N/A N/A 31.92B2 + Concept Repetition 10 80.2% 42.56B2 + Frustration 8 66.4% 32.99B2 + Percent Correctness 4 44.3% 28.50Table 2: Feature Comparison Results2.3 Problem with ReliabilityHowever, the approach discussed above assumesthat given the size of the data set, the ECR and poli-cies are reliable.
If the MDP model were very frag-ile, that is the policy and expected cumulative rewardwere very sensitive to the quality of the transitionprobability estimates, then the metrics could revealquite different rankings.
Previously, we used a qual-itative approach of tracking how the worth of eachstate (V-value) changed over time.
The V-valuesindicate how much reward one would expect fromstarting in that state to get to a final state.
We hy-pothesized that if the V-values stabilized as data in-creased, then the learned policy would be more reli-able.So is this V-value methodology adequate for as-sessing if there is enough data to determine a sta-ble policy, and also for assessing if one model isbetter than another?
Since our approach for state-space selection is based on comparing a new pol-icy with a baseline policy, having a stable policy isextremely important since instability could lead todifferent conclusions.
For example, in one compar-ison, a new policy could differ with the baseline in8 out of 10 states.
But if the MDP were unstable,adding just a little more data could result in a differ-ence of only 4 out of 10 states.
Is there an approachthat can categorize whether given a certain data size,278that the expected cumulative reward (and thus thepolicy) is reliable?
In the next section we present anew methodology for numerically constructing con-fidence intervals for these value function estimates.Then, in the following section, we reevaluate ourprior work with this methodology and discuss theresults.3 Confidence Interval Methodology3.1 Policy Evaluation with ConfidenceIntervalsThe starting point for the proposed methodologyis the observation that for each state sj and ac-tion ak in the MDP, the set of transition probabili-ties {p(si|sj, ak)}i=1..n are modeled as multinomialdistributions that are estimated from the transitioncounts in the training data:p?
(si|sj, ak) =c(si, sj, ak)?ni=1 c(si, sj , ak)(1)where n is the number of states in the model, andc(si, sj , ak) is the number of times the system wasin state sj , took action ak, and transitioned to statesi in the training data.It is important to note that these parameters arejust estimates.
The reliability of these estimatesclearly depends on the amount of training data, morespecifically on the transition counts c(si, sj, ak).
Forinstance, consider a model with 3 states and 2 ac-tions.
Say the model was in state s1 and took actiona1 ten times.
Out of these, three times the modeltransitioned back to state s1, two times it transi-tioned to state s2, and five times to state s3.
Thenwe have:p?
(si|s1, a1) = ?0.3; 0.2; 0.5?
= ?
310 ;210 ;510 ?
(2)Additionally, let?s say the same model was in states2 and took action a2 1000 times.
Following that ac-tion, it transitioned 300 times to state s1, 200 timesto state s2, and 500 times to state s3.p?
(si|s2, a2) = ?0.3; 0.2; 0.5?
= ?
3001000 ;2001000 ;5001000 ?
(3)While both sets of transition parameters have thesame value, the second set of estimates is more reli-able.
The central idea of the proposed approach is tomodel this uncertainty in the system parameters, anduse it to numerically construct confidence intervalsfor the value of the optimal policy.Formally, each set of transition probabilities{p(si|sj , ak)}i=1..n is modeled as a multinomial dis-tribution, estimated from data2.
The uncertainty ofmultinomial estimates are commonly modeled bymeans of a Dirichlet distribution.
The Dirichlet dis-tribution is characterized by a set of parameters ?1,?2, ..., ?n, which in this case correspond to thecounts {c(si, sj , ak)}i=1..n. For any given j, thelikelihood of the set of multinomial transition pa-rameters {p(si|sj, ak)}i=1..n is then given by:P ({p(si|sj , ak)}i=1..n|D) == 1Z(D)?ni=1 p(si|sj , ak)?i?1 (4)where Z(D) =?ni=1 ?(?i)?
(?ni=1 ?i)and ?i = c(si, sj , ak).Note that the maximum likelihood estimates for theformula above correspond to the frequency countformula we have already described:p?ML(si|sj, ak) =?i?ni=1 ?i= c(si, sj, ak)?ni=1 c(si, sj , ak)(5)To capture the uncertainty in the model parame-ters, we therefore simply need to store the countsof the observed transitions c(si, sj , ak).
Based onthis model of uncertainty, we can numerically con-struct a confidence interval for the value of the opti-mal policy pi?.
Instead of computing the value of thepolicy based on the maximum likelihood transitionestimates T?ML = {p?ML(si|sj , ak)}k=1..pi,j=1..n, we gen-erate a large number of transition matrices T?1, T?1,... T?m by sampling from the Dirichlet distributionscorresponding to the counts observed in the train-ing data (in the experiments reported in this paper,we used m = 1000).
We then compute the valueof the optimal policy pi?
in each of these models{Vpi?(T?i)}i=1..m.
Finally, we numerically constructthe 95% confidence interval for the value functionbased on the resulting value estimates: the boundsfor the confidence interval are set at the lowest andhighest 2.5 percentile of the resulting distribution ofthe values for the optimal policy {Vpi?
(T?i)}i=1..m.The algorithm is outlined below:2By p we will denote the true model parameters; by p?
wewill denote data-driven estimates for these parameters2791.
compute transition counts from the training set:C = {c(si, sj, ak)}k=1..pi,j=1..n (6)2. compute maximum likelihood estimates fortransition probability matrix:T?ML = {p?ML(si|sj , ak)}k=1..pi,j=1..n (7)3. use dynamic programming to compute the op-timal policy pi?
for model T?ML4.
sample m transition matrices {T?k}k=1..m, us-ing the Dirichlet distribution for each row:{p?i(si|sj, ak)}i=1..n == Dir({c(si, sj , ak)}i=1..n) (8)5. evaluate the optimal policy pi?
in each of thesem models, and obtain Vpi?(T?i)6.
numerically build the 95% confidence intervalfor Vpi?
from these estimates.To summarize, the central idea is to take into ac-count the reliability of the transition probability esti-mates and construct a confidence interval for the ex-pected cumulative reward for the learned policy.
Inthe standard approach, we would compute an esti-mate for the expected cumulative reward, by simplyusing the transition probabilities derived from thetraining set.
Note that these transition probabilitiesare simply estimates which are more or less accu-rate, depending on how much data is available.
Theproposed methodology does not fully trust these es-timates, and asks the question: given that the realworld (i.e.
real transition probabilities) might actu-ally be a bit different than we think it is, how wellcan we expect the learned policy to perform?
Notethat the confidence interval we construct, and there-fore the conclusions we draw, are with respect to thepolicy learned from the current estimates, i.e.
fromthe current training set.
If more data becomes avail-able, a different optimal policy might emerge, aboutwhich we cannot say much.3.2 Related WorkGiven the stochastic nature of the models, confi-dence intervals are often used to estimate the reli-ability of results in machine learning experiments,e.g.
(Rivals and Personnaz, 2002), (Schapire, 2002)and (Dumais et al, 1998).
In this work we use aconfidence interval methodology in the context ofMDPs.
The idea of modeling the uncertainty ofthe transition probability estimates using Dirichletmodels also appears in (Jaulmes et al, 2005).
Inthat work, the authors used the uncertainty in modelparameters to develop active learning strategies forpartially observable MDPs, a topic not previouslyaddressed in the literature.
In our work we rely onthe same model of uncertainty for the transition ma-trix, but use it to derive confidence intervals for theexpected cumulative reward for the learned optimalpolicy, in an effort to assess the reliability of thispolicy.4 ResultsOur previous results indicated that Concept Repe-tition was the best feature to add to the Baseline 2state-space model, but also that Percent Correctnessand Frustration (when added to Baseline 2) offeredan improvement over the Baseline MDP?s.
How-ever, these conclusions were based on a very quali-tative approach for determining if a policy is reliableor not.
In the following subsection, we apply our ap-proach of confidence intervals to empirically deter-mine if given this data set of 100 dialogues, whetherthe estimates of the ECR are reliable, and whetherthe original rankings and conclusions hold up underthis refined analysis.
In subsection 4.2, we providea methodology for pinpointing when one model isbetter than another.4.1 Quantitative Analysis of ECR ReliabilityFor our first investigation, we look at the confidenceintervals of each MDP?s ECR over the entire data setof 20 students (later in this section we show plots forthe confidence intervals as data increases).
Table 3shows the upper and lower bounds for the ECR orig-inally reported in Table 2.
The first column showsthe original, estimated ECR of the MDP and the lastcolumn is the width of the bound (the difference be-tween the upper and lower bound).So what conclusions can we make about the reli-ability of the ECR, and hence of the learned policiesfor the different MDP?s, given this amount of train-ing data?
The confidence interval for the ECR for280State Feature ECR Lower Bound Upper Bound WidthBaseline 1 6.15 0.21 23.73 23.52Baseline 2 (B2) 31.92 -5.31 60.48 65.79B2 + Concept Repetition 42.56 28.37 59.29 30.92B2 + Frustration 32.99 -4.12 61.30 65.42B2 + Percent Correctness 28.50 -5.89 57.82 63.71Table 3: Confidence Intervals with complete datasetthe Baseline 1 model ranges from 0.21 to 23.73.
Re-call that the final states are capped at +100 and -100,and are thus the maximum and minimum boundsthat one can see in this experiment.
These boundstell us that, if we take into account the uncertaintyin the model estimates (given the small training setsize), with probability 0.95 the actual true ECR forthis policy will be greater than 0.21 and smaller than23.73.
The width of this confidence interval is 23.52.For the Baseline 2 model, the bounds are muchwider: from -5.31 to 60.48, for a total width of65.79.
While the ECR estimate is 31.92 (whichis seemingly larger than 6.15 for the Baseline 1model), the wide confidence interval tells us that thisestimate is not very reliable.
It is possible that thepolicy derived from this model with this amount ofdata could perform poorly, and even get a negativereward.
From the dialogue system designer?s stand-point, a model like this is best avoided.Of the remaining three models ?
Concept Repeti-tion, Frustration, and Percent Correctness, the firstone exhibits a tighter confidence interval, indicat-ing that the estimated expected cumulative reward(42.56) is fairly reliable: with 95% probability ofbeing between 28.37 and 59.29.
The ECR for theother two models (Frustration and Percent Correct-ness) again shows a wide confidence interval oncewe take into account the uncertainty in the modelparameters.These results shed more light on the shortcom-ings of the ECR metric used to evaluate the modelsin prior work.
This estimate does not take into ac-count the uncertainty of the model parameters.
Forexample, a model can have an optimal policy witha very high ECR value, but have very wide confi-dence bounds reaching even into negative rewards.On the other hand, another model can have a rela-tively lower ECR but if its bounds are tighter (andthe lower bound is not negative), one can know thatthat policy is less affected by poor parameter esti-mates stemming from data sparsity issues.
Using theconfidence intervals associated with the ECR gives amuch more refined, quantitative estimate of the reli-ability of the reward, and hence of the policy derivedfrom that data.An extension of this result is that confidence in-tervals can also allow us to make refined judgmentsabout the comparative utility of different features,the original motivation of our prior study.
Basi-cally, a model (M1) is better than another (M2) ifM1?s lower bound is greater than the upper bound ofM2.
That is, one knows that 95% of the time, theworst case situation of M1 (the lower bound) willalways yield a higher reward than the best case ofM2.
In our data, this happens only once, with Con-cept Repetition being empirically better than Base-line 1, since the lower bound of Concept Repetitionis 28.37 and the upper bound of Baseline 1 is 23.73.Given this situation, Concept Repetition is a usefulfeature which, when included in the model, leads toa better policy than simply using Correctness.
Wecannot draw any conclusions about the other fea-tures, since their bounds are generally quite wide.Given this amount of training data, we cannot saywhether Percent Correctness and Frustration are bet-ter features than the Baseline MDP?s.
Although theirECR?s are higher, there is too much uncertainty todefinitely conclude they are better.4.2 Pinpointing Model Cross-overThe previous analysis focused on a quantitativemethod of (1) determining the reliability of the MDPECR estimate and policy, as well as (2) assessingwhether one model is better than another.
In thissection, we present an extension to the second con-tribution by answering the question: given that onemodel is more reliable than another, is it possibleto determine at which point one model?s estimatesbecome more reliable than another model?s?
In our2810 2 4 6 8 10 12 14 16 18 20?100?80?60?40?20020406080100Baseline 1# of studentsECRConfidence BoundsCalculated ECR0 2 4 6 8 10 12 14 16 18 20?100?80?60?40?20020406080100Baseline 2 +Concept Repetition# of studentsECRConfidence BoundsCalculated ECRFigure 1: Confidence Interval Plotscase, we want to know at what point Concept Rep-etition becomes more reliable than Baseline 1.
Todo this, we investigate how the confidence intervalchanges as the amount of training data increases in-stead of looking at the reliability estimate at only oneparticular data size.We incrementally increase the amount of train-ing data (adding the data from one new student at atime), and calculate the corresponding optimal pol-icy and confidence interval for the expected cumula-tive reward for that policy.
Figure 1 shows the con-fidence interval plots as data is added to the MDPfor the Baseline 1 and Concept Repetition MDP?s.For reference, Baseline 2, Percent Correctness andFrustration plots did not exhibit the same converg-ing behavior as these two, which is not surprisinggiven how wide the final bounds are.
For each plot,the bold lines represent the upper and lower bounds,and the dotted line represents the calculated ECR.Analyzing the two MDP?s, we find that the confi-dence intervals for Baseline 1 and Concept Repeti-tion converge as more data is added, which is an ex-pected trend.
One useful result from observing thechange in confidence intervals is that one can de-termine the point in one which one model becomesempirically better than another.
Superimposing theupper and lower bounds (Figure 2) reveals that afterwe include the data from the first 13 students, thelower bound of Concept Repetition crosses over theupper bound of Baseline 1.Observing this behavior is especially useful forperforming model switching.
In automatic modelswitching, a dialogue manager runs in real time andas it collects data, it can switch from using a sim-ple dialogue model to a complex model.
Confidenceintervals can be used to determine when to switchfrom one model to the next by checking if a complexmodel?s bounds cross over the bounds of the currentmodel.
Basically, the dialogue manager switcheswhen it can be sure that the more complex model?sECR is not only higher, but statistically significantlyso.0 2 4 6 8 10 12 14 16 18 20?50050100# of studentsECRBaseline 1 and Concept Repetition SuperimposedBaseline 1B2 + Concept RepetitionFigure 2: Baseline 1 and Concept Repetition Bounds5 ConclusionsPast work in using MDP?s to improve spoken dia-logue systems have usually glossed over the issue ofwhether or not there was enough training data to de-velop reliable policies.
In this work, we present anumerical method for building confidence intervalsfor the expected cumulative reward for a learned pol-icy.
The proposed approach allows one to (1) better282assess the reliability of the expected cumulative re-ward for a given policy, and (2) perform a refinedcomparison between policies derived from differentMDP models.We applied this methodology to a prior experi-ment where the objective was to select the best fea-tures to include in the MDP state-space.
Our resultsshow that policies constructed from the Baseline 1and Concept Repetition models are more reliable,given the amount of data available for training.
TheConcept Repetition model (which is composed ofthe Concept Repetition, Certainty and Correctnessfeatures) was especially useful, as it led to a policythat outperformed the Baseline 1 model, even whenwe take into account the uncertainty in the modelestimates caused by data sparsity.
In contrast, forthe Baseline 2, Percent Correctness, and Frustrationmodels, the estimates for the expected cumulativereward are much less reliable, and no conclusion canbe reliably drawn about the usefulness of these fea-tures.
In addition, we showed that our confidenceinterval approach has applications in another MDPproblem: model switching.6 Future WorkAs an extension of this work, we are currently inves-tigating in more detail what makes some MDP?s reli-able or unreliable for a certain data size (such as thecase where Baseline 2 does not converge but a morecomplicated model does, such as Concept Repeti-tion).
Our initial findings indicate that, as more databecomes available the bounds tighten for most pa-rameters in the transition matrix.
However, for someof the parameters the bounds can remain wide, andthat is enough to keep the confidence interval for theexpected cumulative reward from converging.AcknowledgmentsWe would like to thank Jeff Schneider, Drew Bag-nell, Pam Jordan, as well as the ITSPOKE and PittNLP groups, and the Dialog on Dialogs group fortheir help and comments.
Finally, we would like tothank the four anonymous reviewers for their com-ments on the initial version of this paper.
Support forthis research was provided by NSF grants #0325054and #0328431.ReferencesS.
Dumais, J. Platt, D. Heckerman, and M. Sahami.
1998.Inductive learning algorithms and representations fortext categorization.
In Conference on Information andKnowledge Management.M.
Frampton and O.
Lemon.
2005.
Reinforcement learn-ing of dialogue strategies using the user?s last dialogueact.
In IJCAI Wkshp.
on K&R in Practical DialogueSystems.J.
Henderson, O.
Lemon, and K. Georgila.
2005.
Hybridreinforcement/supervised learning for dialogue poli-cies from communicator data.
In IJCAI Wkshp.
onK&R in Practical Dialogue Systems.R.
Jaulmes, J. Pineau, and D. Precup.
2005.
Active learn-ing in partially observable markov decision processes.In European Conference on Machine Learning.E.
Levin and R. Pieraccini.
1997.
A stochastic model ofcomputer-human interaction for learning dialogues.
InProc.
of EUROSPEECH ?97.T.
Paek and D. Chickering.
2005.
The markov assump-tion in spoken dialogue management.
In 6th SIGDialWorkshop on Discourse and Dialogue.I.
Rivals and L. Personnaz.
2002.
Construction of con-fidence intervals for neural networks based on leastsquares estimation.
In Neural Networks.R.
Schapire.
2002.
The boosting approach to machinelearning: An overview.
In MSRI Workshop on Nonlin-ear Estimation and Classification.S.
Singh, M. Kearns, D. Litman, and M. Walker.
1999.Reinforcement learning for spoken dialogue systems.In Proc.
NIPS ?99.R.
Sutton and A. Barto.
1998.
Reinforcement Learning.The MIT Press.J.
Tetreault and D. Litman.
2006.
Comparing the utilityof state features in spoken dialogue using reinforce-ment learning.
In NAACL.M.
Walker.
2000.
An application of reinforcement learn-ing to dialogue strategy selection in a spoken dialoguesystem for email.
JAIR, 12.283
