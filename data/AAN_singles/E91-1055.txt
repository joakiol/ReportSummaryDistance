A COMMON FRAMEWORK FOR ANALYS IS  AND GENERATIONAllan t~ amsayDepartment of Computer Science,University College Dublin,Belfield, DUBLIN 4, IrelandABSTRACTIt seems highly desirable to use a single representa-tion of linguistic knowledge for both analysis andgeneration.
We argue that the only part of theaverage NL system's knowledge that we can haveany faith in is its vocabulary and, to a lesser ex-tent, its syntactic rules, and we investigate theconsequences of this for generation.1 ANALYS ISConsider a typical NLU system.
You give it a pieceof text, say:(1) The house I live in is damp.It grinds away, trying out syntactic rules untilit has an analysis of the structure of the text.The syntactic rules incorporate a semantic ele-ment, which automatically builds up a representa-tion of the meaning of the text in some appropri-ate formal language - -  something like the follow-ing: presupp(.\[3!
B (hon,e(B) & p~e,uw( .
\ [3  !
C(speaker(C))\]), 3 n (,~ate(n, live) ~ ~gen~(n,C) & 3 E : l interval(E)} (contains(E, now) &during(E, D)) & in(n, B))\]), 3 e (condition(F,damp) & object(F, B) b.
3 G : (interval(G)}(contains(G, now) during(G, F)))Exactly what formal language you choose forthe representation of meaning will depend on anumber of things, notably on the intended appli-cation (if any) of the system, on the availabilityof automatic inference systems for the languagein question, and on the perceived need for ex-pressive power.
For the system that lies behindthe discussion in this paper we chose a version ofTurner's \[1987\] property theory.
The details ofproperty theory do not really matter very muchhere.
What matters is that any attempt o give acomplete formal paraphrase of (1) must include atleast as much information as we have given above.In particular, the logical structure of our para-phrases contains essential information (about, forinstance, the differences between objects which areintroduced in the utterance and ones whose exis-tence is presupposed), even if there is still consid-erable debate about the best way of representingthis information.2 GENERATIONSuppose we have the formula given above as a for-mal paraphrase of (1), and we want to generatean English sentence which corresponds to it.
Wemight hope to use our syntactic/semantic rules"backwards", looking for something which wouldgenerate a sentence and whose semantic compo-nent could be made to match the given sequence.The final rule we actually used in our analysis of(1) is an elaboration of the standard S ---, NP VPrule which contains a description of how the mean-ings of the NP and the VP should be combinedto obtain the meaning of the NP.
Space does notpermit inclusion of this rule.
The important pointfor our present purposes is that the representa-tion of the meaning of the S is built up from thediscourse representations of the subject and thepredicate.
The subject and predicate each pro-vide some background constraints, and then theirmeanings get combined (along with a complex ab-straction to the effect that there is some object gwhich satisfies two properties PO and Pl) to pro-duce a further constraint.
The question we wantto investigate here is: can we use rules of this kindto generate (1) from the above semantic represen-tation?The problem is that rules of this kind explainhow to combine the meanings of constituents onceyou have identified them.
Given an expression ofproperty theory like the one above, it is very dif-ficult to see how to decompose into parts corre-sponding to an NP and a VP.
So difficult, in fact,that without a great deal of extra guidance it mustbe regarded as impossible.The final semantic representation reflects ourbeliefs about the best formal paraphrase of theEnglish text, whereas the semantic representationsof the components reflect the way we think thatthis paraphrase might be obtained.
Somebody elsemight decide that they liked our final analysis, butthat they preferred some other way of deriving it.In view of the number of different ways of obtain-ing a given expression E as the result of simplifyingsome complex expression (t E *\[z, P\]), it is sim-ply unreasonable to hope to find the right decom-position of a given semantic representation u lessyou already know a great deal about the way thelinguistic theory being used builds up its repre-sentations.
Indeed, unless you already have thisknowledge it is unlikely that you will even be ableto tell whether some semantic representation hasa realisation as a natural anguage text at all.If we look again at the knowledge available toour "average NL system", we see that it will in-clude a vocabulary of lexical items, a set of syntac-- 309 -tic rules, and a set of semantic interpretations ofthose rules.
It is worth reflecting briefly on the ev-idence that lies behind particular choices oflexicalentry, grammatical rule and semantics interpreta-tion.The evidence that leads to a particular choiceof words to go in the vocabulary is fairly concrete.We can, for instance, take a corpus of written En-glish and collect all the contiguous equences ofletters separated by spaces.
We can be fairly con-fident that nearly every such sequence is a word,and that those things that are not words will befairly easily detected.
We would in fact proba-bly want to do a bit better than simply collectingall such letter sequences, ince we would want torecognise the connection between eat and eaten,and between die and dying, but at least the ob-jects that we are interested in are available forinspection.The evidence that leads to a particular choiceof syntactic theory is less directly available.
Oncewe have a vocabulary derived from some corpus,we can start to build up word classes on the basisof looking for words that can be exchanged with-out turning a meaningful sentence into a mean-ingless one - -  to spot that almost any meaning-ful sentence containing the word rJalk could beturned into a meaningful sentence containing theword run, for instance.
We can then start lookingfor phrase types and for relations between phrasetypes.
We can perhaps be reasonably confidentabout our basic classification into word classes,though we may find some surprises, but the ev-idence for specific phrase types is often in the eyeof the beholder, and the evidence for subtler rela-tionships can be remarkably intangible.
Nonethe-less, there is some concrete evidence, and it has ledto some degree of consensus about the basic ele-ments of syntactic theory.
You will, for instance,find very few NL  systems that do not utilise thenotion of an NP, or that do not r~cognise the phe-nomena of agreement and unbounded dependency.The evidence for specific semantic theories, bycontrast, is almost entirely circumstantial.
We canusually tell whether two sentences mean the samething; we can usually tell whether a sentence isambiguous; and we can sometimes tell whetherone sentence entails another, or whether one con-tradicts another.
To get from here to a decisionthat one representation scheme is more appropri-ate than another, and to a particular translationof some piece of NL  into the chosen scheme, re-quires quite a bit of faith.
In order to build a sys-tem for translating NL  input into some computer-amenable representation we have no choice butto make that act of faith.
We have to choosea representation scheme, and we have to decidehow to translate specific fragments of NL  into itand how to combine such translated fragments tobuild translations of larger fragments.
Examplesabound.
The system that constructed the trans-lation of (1) into the given sequence of proposi-tions in PT is described and defended at lengthin \[Ramsay 1990\], and we will not recapitulate ithere.
We note, however, that the rules we usefor translating from English into this representa-tion scheme wilt not generate arbitrary such se-quences.
Only sequences which correspond to theoutput of the rules we are using applied to thetranslations we have allocated to the lexical itemsin our vocabulary will be generated.
Tibia is true ofall NL s!/stems that translate from a natural lan-guage into some formal representation language.For any such system, only a fraction of the pos-sible sentences of the representation language willcorrespond to direct translations of NL sentences,and the only way of telling which they are is tolook for the corresponding NL sentence.Suppose we wanted to develop a system whichused our linguistic knowledge base to generatetexts corresponding to the output of some appli-cation system.
It would be absurd to expect theapplication program to generate sentences of ourchosen representation language, and to try to workfrom these via our syntactic/semantic rules to anNL realisation.
We have no convincing evidencethat our representation language is correct; wehave no easy way of specifying which sentencesof the representation language correspond via ourrules to NL sentences; and even if we did havea sentence in the representation language whichcorresponded to an NL sentence, we would havea great deal of difficulty in breaking it into ap-propriate components, particularly if this involvedreplacing a single formula by the instantiation ofsome abstraction with an appropriate term.We suggest instead that the best way to get anNL system to generate text to satisfy the require-ments of some application program is for it to of-fer suggestions about how it is going to build thetext, along with explanations of why it is going tobuild it that way.
We therefore supplement ourdescriptions of linguistic structures with a compo-nent describing their functional structure.For the rule for S, for instance, we add an ele-ment describing what the SUBJECT and PRED arefor.
We could say that the SUBJECT is the themeand the PRED is the theme, using terms from func-tional grammar \[Halliday 1985\] for the purpose.
Alanguage generation system using the above rulecan now ask the application program whether itis prepared to describe a theme and a theme.
Ad-mittedly this still presumes that the applicationprogram knows enough about the linguistic theoryto know about themes and themes, but at least itdoes not need to know how they are organised intosentences, how they can be realised, or how theirsemantic representations are combined to form asentence in the representation language.
Further-more, if the application program is to make fulluse of the expressive power of NL then it must beable to make sensible choices about such matters,since any hearer will be sensitive to them.
If thecombination of application program and N L gener-- 310 .
,ation system cannot make rational decisions aboutwhether to say, for instance, John ate it or It waseaten by John then they must expect to be mis-understood by native English speakers who are,albeit unconsciously, aware that these two carrydifferent messages.Once the application program has agreed to de-scribe a theme and a rheme, the NL system canthen elicit these descriptions.
Since the rule beingused specifies that the theme must be an NP thenit can move on to rules and lexieal entries thatcan be used for constructing NPs and start askingquestions about these.3 COMPARISONSWe are concerned here almost entirely with whathas come to be known as the "tactical" componentof language generation - -  with how to realise somechosen message as NL text, rather than with howto decide what message we want realised.
Thetwo are not entirely separable, but we have lit-tle to say about "strategic" tasks such as decidingwhat properties hould be used for characterisingan item being referred to by an NP, which we ex-pect the application program to deal with.
Theresponsibility for deciding whether to pronomi-nalise something, for instance, would be handedover to the application program by the NL sys-tem bluntly asking whether a description with theproperty qua l i f  i e r  :pronoun was acceptable.
Wethus completely side-step the issues addressed bysystems which plan what to say to produce spe-cific effects in a hearer \[Appelt 1985\], which workout how organise multi-sentence t xts in order toconvey complex messages without disorienting theheater \[McKeown 1985\], or which invent effectivedescriptions for use in referring expressions /Dale1988\].
These are all important asks, but they arenot what we are concerned with here.The most direct comparison is with \[Shieberet al 1990\], where an approach to generat-ing text from a given logical form is described.The algorithm described by Shieber and his col-leagues takes a realisable A-calculus expressionand uses their syntactic/semantic rules "back-wards" to generate appropriate text.
Their em-phasis is on controlling the way these rules are ap-plied, with rules satisfying certain rather stringentcriteria being applied top-down and al\] other rulesbeing used bottom-up.
The algorithm looks effec-tive, so long as (a) it is reasonable to assume thatan application program can be relied on to pro-duce realisable expressions in the representationlanguage and (b) there are any rules which satisfytheir criteria.
We argued at some length abovethat the first of these conditions is unlikely to holdunless the application program knows a great dealabout the syntactic/semantic rules which are go-ing to be used.
We also suspect hat the way theycontrol the top-down application of rules imposesunacceptable constraints on the way that seman-tic representations of wholes are composed out ofsemantic representations of parts.
Certainly noneof the rules we used in the system described in\[Ramsay 1990\] satisfy their criteria.
We there-fore believe that our approach, where the appli-cation decides whether the fragments of text pro-posed by the NL system are acceptable as theyare proposed, is more flexible than any approachwhich depends on getting a reaiisable expressionof the representation language from the applica-tion program and systematically translating it intoa natural anguage using syntactic/semantic ruleswhich were primarily designed for translating inthe other direction.REFERENCESAppelt D. (1085): Planning English Sentences:Cambridge University Press, Cambridge.Dale R. (1988): Generating Referring Ezpres-sions in a Domain of Objects and Processes,Ph.D.
thesis, Centre for Cognitive Science,University of Edinburgh.Halliday M.A.K.
(1985): An Introduction toFunctional Grammar: Arnold, London.Kamp H. (1984): A Theory of Truth and Seman-tic Representation, i  Formal Method8 in LheStudy of Language (eds.
J. Groenend~jk, J.Janssen & M. Stokhof): Foris Publications,Dordtecht: 277-322.McKeown K. (1985): Generating English Tezt:Cambridge University Press, Cambridge.Ramsay A.M. (1990): The Logical Structure ofEnglish: Computing Semantic Content: Pit-man, London.Shieber S.M, van Noord G., Pereira F.C.N.
&Moore R.C.
(1090): Semantic-Head-DrivenGeneration, Computational Linguistics 16(1):30-42.Turner R. (1087): A Theory of Properties, Jour-nal of Symbolic Logic 52(2): 455-472.-311  -
