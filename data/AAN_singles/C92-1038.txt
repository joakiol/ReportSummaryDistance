A Fast Algorithmfor the Generation of Referring ExpressionsAbst rac tWe simplify previous work in the development ofalgorithms for the generation of referring expre~sions while at the same time taking account of psy-cholinguistic findings and transcript data.
The resultis a straightforward algorithm that is computation-ally tractable, sensitive to the preferences of humanusers, and reasonably domain-independent.
We pro-vide a specification of the resources a host systemmust provide in order to make use of the algorithm,and describe an implementation used in the IDAS sys-tem.In t roduct ionIn previous work \[Da189,DH91,Rei90a,Rei90b\] wehave proposed algorithms for determining the con-tent of referring expressions.
Scrutiny of the psy-cholinguistics literature and transcripts of human di-alogues hows that in a number of respects the be-haviour of these algorithms does not correspond towhat people do.
In particular, as compared to thesealgorithms, human speakers pay far less attention toreducing the length of a referring expression, and farmore attention to making sure they use attributesand values that human hearers can easily process; inthe terms introduced in \[Da188,Da189\], hearers aremore concerned with the principle of sensitivity thanwith the principle of efficiency.
We have designed anew referring expression generation algorithm thatis based on the~ observations, and believe that thenew algorithm is more practical for real-world natu-ral language generation systems than the algorithmswe have previously proposed.
In particular, the al-gorithm is:?
fast: its run-time is linear in the number of distrac-tors, and independent of the number of possiblemodifiers;?
sensitive to human preferences: it attempts touse easily perceivable attributes and basic-level\[Ros78\] attribute values; and?
Supported by SERC grant GR/F/36750.
E-mail ad-dress is E.Reiter@ed.
ac.uk.tAiso of the Centre for Cognitive Science at the Univer-sity of Edinburgh.
E-mail address i  R. DaleQed.
ac .uk.Ehud Re i te r*and  Rober t  Da le fDepar tment  of Art i f ic ia l  Inte l l igenceUn ivers i ty  of Ed inburghEd inburgh  EH1 1tlNScot land?
domain-independent: he core algorithm shouldwork in any domain, once an appropriate knowl-edge base and user model has been set up.A version of the algorithm has been implementedwithin the IDAS natural-language neration system\[RML92\], and it is performing satisfactorily.The algorithm presented in this paper only gener-ates definite noun phrases that identify an objectthat is in the current focus of attention.
Algorithmsand models that can be used to generate pronominaland one-anaphoric referring expressions have beenpresented elsewhere, .g., \[Sid81,GJW83,Da189\].
Wehave recently begun to look at the problem of gen-erating referring expressions for objects that are notin the current focus of attention; this is discussed inthe section on Future Work.BackgroundD is t ingu ish ing  Descr ip t ionsThe term 'referring expression' has been used by dif-ferent people to mean different things.
In this paper,we define a referring expression i  intentional terms:a noun phrase is considered to be a referring expres-sion if and only if its only communicative purpose isto identify an object to the hearer, in Kronfeld's ter-minology \[Kro86\], we only use the modal aspect ofDonefian's distinction between attributive and ref-erential descriptions \[Don66\]; we consider a nounphrase to be referential if it is intended to identifythe object it describes to the hearer, and attributiveif it is intended to communicate information aboutthat object to the hearer.
This usage is similar tothat adopted by Reiter \[Rei90b\] and Dale and Had-dock \[DH91\], but differs from the terminology usedby Appelt \[App85\], who allowed 'referring expres-sions' to satisfy any communicative goal that couldbe stated in the underlying logical framework.We here follow Dale and Haddock \[DH91\] in assum-ing that a referring expression satisfies the referentialcommunicative goal if it is a d is t ingu ish ing  de-acript ion, i.e., if it is an accurate description of theentity being referred to, but not of any other objectin the current context  set.
We define the contextset to be the set of entities that the hearer is cur-rently assumed to be attending to; this is similarACRES DE COLING-92, NAtCrES, 23-28 AO't~q" 1992 2 3 2 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992to the notion of a discour~ focus space \[GS86\].
Wealso define the cont ras t  set  to be all elements of thecontext set except he intended referent.
The role oftile conlponcnts of a referring expression can thenbe regarded as 'ruling out'  members of the contrastset.
For example, if the speaker wished to identify asmall black dog in a situation wlmre tile contrast setconsisted of a large white dog and a small black cat,she might choose the adjective black in order to ruleout the white dog and the heart noun dog in orderto rule out the eat; this results in the referring ex-pression the black dog, which matches the intendedreferent but no other object in the current context.The small dog would also be a succ~csful referringexpre~-~ion i this context, under the distinguishingdescription model.Unnecessary  Mod i f ie rsA referring expression must communicate enough in-fornlation to be able to uniquely identify the in-tended referent in the current discourse context(i.e., it must adhere to the principle of adequacy\[Da188,Da189\]).
But.this is not the only constrainta good referring expression must obey; it is clearthat many referring expressions that meet this con-straint are inappropriate because they couvey incor-rect and unwanted conversat iona l  imp l ieatures\[Gri75,Rei90a\] to a human hearer.One source of such false implicatures can he the pre~ence of redundant or otherwise unnecessary modifiersin a referring expression.
For example, consider twopossible referring expressions that  a speaker mightuse to request ilat a hearer sit by a talfle:(l) a.
Sit by the table.h.
Sit by the brown wooden table.If the context was such that only one table was vis-ible, and this table was brown raid made of wood,utterances (In) and (lb) would both be distinguish-ing descriptions that unranbiguously identified theintended referent o the hearer; a hearer who heardeither utterance would know where he was supposedto sit.
However, a hearer who heard utterance (lb)in such a context might make the additional infer-enee that  it was important to the disc~mrse that thetM)le was brown and made of wood; for, tile hearermight reason, why else would the speaker include in-formation about the table's colour and material thatwas not necessary for the reference task?
This infer-enc~ is an example of a conversational implicaturecaused by a violation of Grice's maxim of Quantity\[Gri75\].Inappropr ia te  Mod i f ie rsUnwanted conversational implicatures can alse becaused by the use of overly specific or otherwise un-expected modifiers.
One example is ms follows:(2) IL l,ook at the doq.b.
Look at the pil bull.In a context where there is only one dog present,tile hearer would nommlly expect utterance (2a) tobe used, since dog is a basic- level  class \[Ros78\] formost native speakers of English.
Hence the use of ut-terance (2b) might implicate to the hearer that  thespeaker thought it was relevant that  the animal was apit bull and not some other kind of dog \[Cru77\], per-haps because the speaker wished to warn the hearerthat the animal might be dangerous; if the speakerhad no such intention, site should avoid using utter-ance (2b), despite the fact that it fulfills the referen-tial communicative goal.P rev ious  WorkIn previous work \[Dalgg,D}191,Rei90a,Rei90b\] wehave noted that  the presence of extra informationin a referring expression can lead the hearer to makefalse implicaturcs, and therefore concluded that areferring-expression generation system should taakea strong attempt to ensure that  generated refer-ring expressions do not include unnecessary infor-mation, either as superfluous NP modifiers or asoverly-specific head nouns or attr ibute values.
Dale\[DaI88,DaI89,DH91\] }ins suggested oing this by re-quiring the generation system to produce mtn imaldistinguishing descriptions, i.e., distinguishing de-scriptions that  include as few attributes of the in-tended referent as possible.
Keiter \[Keig0a, Rei90b\]ha.s pointed out that  this task is in fact NP-Hard,and has proposed instead that referring expressionsshould obey three rules:No  Unnecessary  Components :  all componentsof a referring expremion must be necessary to ful-fill the referential goal.
For example, the smallblack dog is not acceptable if the black dog is a dis-tinguishing description, since this means small isan unnecessary component.Loca l  Brev i ty :  it should not be po~ible to pro-duce a shorter eferring expression by replacing aset of existing modifiers by a single new modifier.For exanlple, the sleeping female dog should nothe treed if the small dog is a distinguishing descrip-tion, since the two modifiers sleeping and femalecan bc replaced by the single modifier small.Lexical  P re ference :  this is an extension of theba.sicAcvel preference proposed by Cruse \[Cru77\];more details are given in \[Reigl\].A referring expression that mt.~ts l\[eiter's con-straints cart be foumt in polynomial time if the lexicalpreference r lation meets certain conditions \[Rei90a\];such a referring expression can not, imwever, alwaysbe found in linear time.Psycho log ica l  and  Transcr ip t  DataPsycho log ica l  Ev idenceSubsequent to performing the above research, wehave looked in some detail at the psychological lit-erature on human generation of referring expres-sions.
This research (e.g., \[FO75,Whi76,Son85,A(X'ES DE COLING-92, NANrES, 23-28 AOl'rl' 1992 2 3 3 I'SOC.
hi: COLING-92, NANTES, AUG. 23-28, 1992Pec891; \[Lev89, pages 129-134\] is a useful summaryof much of this work) clearly shows that in manycases human speakem do include unnecessary modi-tiers in referring expressions; this presumably impliesthat in many cases human hearers do not make impli-catures from the presence of unnecessary modifiers.For example, if human subjects are shown a pictureof a white bird, a black cup, and a white cup, andare asked to identify the white bird, they frequentlysay the white bird, even though just the bird wouldhave been sufficient in this ease.A partial explanation for this use of redundancy maybe that human speakers generate referring expres-sions incrementally \[Pee89\].
An incremental gener-ation algorithm cannot always detect unnecessarymodifiers; in the above example, for instance, onecould imagine the algorithm choosing the adjectivewhite to rule out the black cup, and then the nounbird in order to rule out the white cup, without thenerasing white because the black cup is also ruled outby bird.Another explanation of redundancy might involvethe speaker's desire to make it easier for the hearerto identify the object; the speaker might believe, forexample, that it is easier for the hearer to identifya white bird than a bird, since colour may be moreimmediately perceptible than shape.
1.Both of the above explanations primarily justify ad-jectives that have some discriminatory power even ifthey are redundant in this particular context.
In theabove example, for instance, white possesses somediscriminatory power since it rules out the black cup,even though it does happen to be redundant in theexpression the white bird.
It would be harder foreither of the above factors to explain the use of amodifier with no discriminatory power, e.g., the useof white if all objects in the contrast set were white.2qaere is some psychological research (e.g., \[FO75\])that suggests that human speakers do not use mod-ifiers that have no discriminatory power, but thisresearch is probably not conclusive.Thc argument can be made that  psychological real-ism is not the most important constraint for gener-ation algorithms; the goal of such algorithms houldbe to produce referring expressions that human hear-ers will understand, rather than referring expressionsthat human speakers would utter.
The fact that hu-man speakers include redundant modifiers in refer-ring expressions does not mean that NL generationsystems are also required to include such modifiers;there is nothing in principle wrong with building gen-eration systems that perform more optimizatious oftheir output than human speakers.
On the otherhand, if such beyond-human-speaker optimizations1Another possible explanation is that speakers may insome cases use precompiled 'reference scripts' instead ofcomputing a referring expression from scratch; such refer-enoe scripts pecify a set of attributes that are included asa group in a referring expression, even if some membersof the group have no discriminatory power in the currentcontextare computationally expensive and require complexalgorithms, they may not be worth performing; theyare clearly unnecessary in some sense, after all, sincehuman speakers do not perform them.Transcr ip t  Ana lys i sIn addition to the l~ychological l iterature review, wehave also examined a transcript of a dialogue be-tween two humans performing an assembly task.
~We were particularly interested in questions of mod-ifier choice; if a discriminating description can beformed by adding any one of several modifiers to ahead noun, which modifier should be used?
In par-ticular,1.
Which attribute should be used?
E.g., is it betterto generate the small dog, the black dog, or thefemale dog, if these are discriminating descriptionsbut jnst the dog is not?2.
Is it preferable to add a modifier or to use a morespecific head noun?
E.g., is it better to say thesmall dog or the chihuahua?3.
Should relative or absolute adjectives be used?E.g., is it better to say the small dog or the onefoot high dog?In our analysis, we observed several phenomenawhich we believe may generalise to other situationsinvolving spoken, face-tooface language:1.
Human speakers prefer to use adjectives that com-municate size, shape, or colour in referring expres?sions.
In tile above examples, for instance, a hu-man speaker would probably prefer the black dogand the small dog over the female dog.2.
Human hearers ometimes have trouble determin-ing if an object belongs to a specialized class.
Inthe above example, for instance, the chihuahuashould only be used if the speaker is certainthe hearer is capable of distinguishing chihuahuasfrom other types of dogs.
If there is any doubtabout the heater's ability to do this, adding anexplicit modifier (e.g., the small dog) is a betterstrategy than using a specialized head noun.3.
Human speakers eem to prefer to use relative ad-jectives, and human hearers eem to have less trou-ble understanding them.
However, human-writteninstructional texts sometimes use absolute adjec-tives instead of relative ones; this may be a con-sequence of the fact that writers cannot predictthe context heir text will be read in, and hencehow readers will interpret relative adjectives.
Inthe above example, therefore, a speaker would beexpected to use the small dog, but a writer mightuse the one foot high dog.ZThe transcript was made by Phil Agre and JohnBatali, from a videotape taken by Candy Sidser.
We arevery grateful to them for allowing us to use it.ACTF~ DE COLING-92, NANqT~, 23-28 Aotrr 1992 2 3 4 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992The A lgor i thmBased on the above considerations, we have createda new algorithm for generating referring expressions.This algorithm is simpler and faster than the algo-rithms proposed in \[Dai89,Rei90a\] because i t  per-forms much less length-oriented optimization of itsoutputi we now believe that  the level of optimiza-tion suggested in \[Da189,ReigOa\] was unnecessaryand psycholinguistically implausible.
The algorithmhas been implemented as part  of a larger natural-language generation system, and we are pleased withits performance to date.Assumpt ions  about  the  Knowledge BaseOur algorithm is intended to be reasonably domain-independent.
We.
do, however, make some assump-tions about the structure of the host system's un-derlying knowledge base, and require that certaininterface functions be provided.in particular, we assume that:?
Every entity is characterised in terms of a col-lection of a t t r ibutes  and their values.
Anattrii)ute-value pair is what is sometimes thoughtof as a property; an example is (colour, red).Every entity has as one of its attributes ometype .
This is a special attr ibute that  correspondsto the kinds of properties that  are typically real-iT, ed by head nouns; an example is (type, dog).?
The knowledge base may organize some attr ibutevalues in a subsumption taxonomy (e.g., as is donein KI:ONE \[BS85\] and related KR systems).
Sucha taxonomy might record, for example, that  an-imM subsumes dog, and that  red subsumes car-let.
For such taxonomically-organized values,the knowledge-base or an associated user-modelshould specify which level of the taxonomy isbasic-level for the current user.We require that the following interface functions beprovided:value(object,attribute) returns the value (if any) thatan attribute has for a particular object.
Valueshould return the most specific possible value forthis attribute, e.g., chihuahua instead of dog, andscarlet instead of red.taxonomy-children(value) returns the immediate chil-dren of a value in the taxonomy.
For example,taxonomy-children(animal) might be the set {dog,cat, horse, .
.
.
}.basle-level-value(object.attribute) returns the basic~level value of an attribute of an object.
FOr exam-ple, basic-level-value(Garfield, type) might be cat.The knowledge-representation system should inprinciple allow different basic-level classes to bespecified for different users \[Ros78,Rei91\].user-knows(object, a tribute-value-pair) returns trueif the user knows or can easily determine (e.g., bydirect visual perception) that  the attribute-valucpair applies to the object; false if the user knows orcan easily determine that  the attr ibute-value pairdoes not apply to the object; and unknown other-wise.
FOr exmnple, if object x had the attr ibute-value pair (type, chihuahua), and the user was ca-pable of distinguishing dogs from eats, then user-knows(x, (type, dog)) would be true, while user-knows(x, (type, cat)) would be false.
If the userwas not, however, capable of distinguishing differ-ent breeds of dogs, and had no prior knowledgeof x's breed, then user-knows(x, (type, chihuahua))and user~knows(x, (type, poodle)) would both re-turn unknown, since the user would not know orbe able to easily determine whether x was a chi-huahua, poodle, or some other breed of dog.Finally, we a~ume that the global variable*p~eferred-attributes* lists the attributes that  humanspeakers and hearers prefer (e.g., type, size, shape,and colour in the ~.,~embly task transcript mentionedabove).
These attributes hould be listed in order ofpreference, with the most preferable attr ibute flint.The elements of this list and their order will varywith the domain, a~ld should be determined by em-pirical inv~tigation.Inputs  to  the  A lgor i thmIn order to construct a reference to a particular emtity, tile host system must provide:- a symbol corresponding to the intended referent;and?
a list of symbols correspondiug to the members ofthe contrast set (i.e., the other entities in focus,besides the intended referent).The algorithm returus a list of attr ibute-value pairsthat  correspond to tim romantic ontent of the refer-ring expression to be realized.
This list can then beconverted into an SPL \[K&~9\] term, as is done in theII)AS implementation; it can also be converted into arecoverab le  semant ic  s t ructure  of the kind usedin Daie's EPICOltE system \[Da188,Dai89\].The  A lgor i thmIn general terms, the algorithm iterates throughthe attributes in *preferred-attributes*.
For each at-tribute, it checks if specifying a value for it wouldrule out at least one member of the contrast set thathas not already becu ruled out; if so, this attribute isadded to the referring ~t ,  with a value that  is knownto the User, rules out as many contrast set mem-bers as possible, and, subject to these constraints,is as cl(~e as possible to the basic-level value.
Theprocess of adding attr ibut~value pairs continues mt-til a referring expression has been formed that  rulesout every member of the contrast set.
There is nobacktracking; once an attr ibute-value pair has beenadded to the referring expression, it is not removedeven if the addition of subsequent attribute-valuepairs make it unnecessary.
A head noun (i.e., a valuefor tim type attribute) is always included, even if itAcres DE COLING-92, NANTES, 23-28 AO~n' 1992 2 3 5 PROC.
OV COTING-92, NANTES, AUG. 23-28, 1992l make-referring-expression(r, C,P) IL*-- {}D, -Cfor each member A~ of list P doV = flnd-best-value(A~, baslc-level-value(r, A~))I fV ~ nil A rules-out((A~, V)) ~ nilthen L ~ L U {(AI, V)}D ~ D - rules-out((At, V))endlfIf D = {} thenif (type, X) (: L for some Xthen re turn  Lelse return L U {(type, basic-level-value(r, type))}endifendifnextreturn failureI find-best-value(A, initial-valse) lff user-knows(r, (A. initial-value)) = truethen value ~-- initial-valueelse value ~ nilvndlffor v~ E taxonomy-children(initial-value)lfv~ subsumes value(r, A) A(new-value ~ find-best-value(A, vi)) ~ nil A(value = nll YIrules-out((A, new-value)) I > Irules-out((a, valse) l)then value ~ new-valueendifnextreturn value\[ ,ul;s-out(<A, v>)\[return {x : x E D A user-knows(x, (A, V)) = false}Figure 1: The Algorithmhas no discriminatory power (in which ease the basiclevel value is used); other attribute values are onlyincluded if, at the time they were under considera-tion, they had some discriminatory power.More precisely, the algorithm is as shown in Figure 1.Here, r is the intended referent, C is the contrast set,P is the list of preferred attributes, D is the set ofdistractom (contrast set members) that have not yetbeen ruled out, and L is the list of attr ibute-valuepairs returned, amake-referring-expression is the top level function.This returns a list of attr ibute-value pairs thatspecify a referring expression for the intended ref-a For simplicity of expo6ition, the algorithm as describedhere returns failure if it is not pesaible to rule out all themernbem of the contrast set.
A more robust algorithmmight attempt o pur~m other strategies here, e.g, gen-erating a referring expression of the form one of the Xs,or modifying the contrast set by adding navigation i for-mation (navigation is discussed in the section on FutureWork).erent.
Note that the attributes are tried in the or-der specified in the *preferred-attributes* li t, andthat a value for type is always included, even iftype has no discriminatory power.find-best-value takes an attribute and an initialvalue; it returns a value for that attribute that issubsumed by the initial value, accurately describesthe intended referent (i.e., subsumes the value theintended referent possesses for the attribute), rulesout as many distractors as possible, and, subjectto these constraints, is as close as possible in thetaxonomy to the initial value.rules-out akes an attribut~.~value pair and returnsthe elements of the set of remaining distractomthat are ruled out by this attr ibute-value pair.An  ExampleAssume the task is to create a referring expressionfor Objectl in a context that  also includes Object2and Object3:?
Object1: (type, chihuahua), (size, small), (calour,black)?
Object2: (type, chihuahua), (size, large), (colour,white)* Object3: (type, siamese-cat), (size, small), (colour,black)In other words, r = 0bject l  and (7 = {Objest2,Object3}.
Assume that P = {type, colour, size, .
.
.
}.When make-referring-expression i  called in this con-text, it initializes L to the empty set and D to C, i.e.,to {Object2, Object3}.
Find-best-value is then caUedwith A = type, and initial-value set to the basic-leveltype of Object1, which, let us assume, is dog.Assume user-knows(Object1, (type, dog)) is true, i.e.,the user knows or can easily perceive that  Objectlis a dog.
Find-best-value then sets value to dog,and examines the taxonomic descendants of dog tosee if any of them are accurate descriptions of Ob-ject1 (this is the subsumption test) and rule outmore distractors than dog does.
In this case, theonly accurate child of dog is chihuahua, but (type,chihuahua) does not have more discriminatory powerthan (type, dog) (both rule out {Object3}), so find-best-value returns dog as the best value for the typeattribute.
Make-referring-expression the  verifies that(type.
dog) rules out at least one distraetor, andtherefore adds this attribute-value pair to L, whileremoving rules-out((type, dog)) = {Object3} from D.This means that  the only remaining distraetor inD is Object2.
Make-referring-expression (after cheek-ing that  D is not empty) calls find-best-value againwith A = colour (the second member of P).
Find-best-value returns Objectl~s basic-level colour value,which is black, since no more specific colour termhas more discriminatory power.
Make-referring-expression then adds (colour, black) to L and removesrules-out((colour, black)) = {Object2} from D. Dis then empty, so the generation task is completed,ACTES DE COLING-92, NANTES, 23-28 AO~r 1992 2 3 6 PROC.
OF COLING-92, NArCrEs.
AUG. 23-28.
1992and make-referring-expression returns {(type, dog),(celour, black)} , i.e., a specification for the refer-ring expression the black day.
Note that if P hadbeen {type, size, colour, .
.
.  }
instead of {type, cnlour,size, .
.
.}
,  make-referring-expeession would have rc~turned {(type, dog), (size, small)} instead, i.e., thesraall do#.Imp lementat ionThe algorithm is currently being used within then)AS system \[RML92\].
ll)hS is a natural m~guagegeneration system that generates on-line documen-tation and help texts from a domain arid linguisticknowledge base, lining user expertise models, usertask models, and discourse models.IDAS uses a KL-ONE type knowledge repr~entationsystem, with roles corresponding to attributes andlillem to values.
The type attribute is implicit inthe position of an object in the taxonomy, and isnot explicitly represented.
The value and taxonomy-children functions are defined in terms of standardknowledge-base access functions.A knowledg~base author can specify explicit basic-level attr ibute values in IDAS user models, but IDAS isalso capable of using heuristics to guess which valueis basic-level.
The heuristics are fairly simple (e.g.,"nse the most general value that is not in the upper-model \[BKMW90\] and has a one-word realization"),but they seem (so far) to be at least somewhat effec-tive.
A *preferred-attributes* li t has been crcatedfor IOAS's domain (complex electronic machinery)by visual inspection of the equipment being docu-mented; its first members are type, colour, and la-bel.
The user-knows function simply returns true ifthe attributc~value pair is accurate and false other-wise; this essentially assumes that the user can visu-ally perceive the value of any attribute in *preferred-attributes*, which may not tie true in general.The referring expression generation model seems rea-sonably successful in IDAS.
In parLieular, the algo-rithm lure proven to be useful because:1.
It is fast.
The algorithm runs in linear time inthe number of distractors, which is probably im-possible for any algorithm that includes an ex-plicit brevity requirement (e.g., the algorithms of\[Da189,Rei90a\]).
Of equal importance, its run-time is independent of the number of potentialattributes that could be used in the referring ex-preszion.
This ks a consequence of the fact thatthe algorithm does not attempt to find the at-tribute with the highest discriminatory power, butrather simply takes attributes from the *preferred-attributes* list until it has built a successful refer-ring expression.2.
It allows human preferences and capabilities tobe taken into consideration.
The *preferred-attributes* list, the preference for basic-level val-ues, and the user~knows function are all ways ofbiasing the algorithm towards generating referringexpressions that use attributes and values that hu-taan hearers, with all their perceptual limitations,lind easy to process.Almost all referring expressions generated by IDAScontain a head noun and zero, one, or perhaps atmost two modifiers; longer referring expressions arerare.
The most important task of the algorithm istherefore to quickly generate asy-to-understand re-ferring expre~mions in such simple cases; optimal han-dling of more complex referring expressions i leesimportant, although the algorithm should be robuatenough to generate something plausible if a long re-ferring expression is needed.Future  WorkNav igat ionAs mentioned in the introduction, the algorithm pre-sented here assumes that  the intended referent is inthe context set.
An important question we need toaddress is what action should be taken if this is notthe c~.se, i.e., if the intended referent is not in thecurrent focus of attention.Unfortunately, we have very little data available ouwhich to bose a model of the generation of such refer-ring expressions.
Psyclmlinguistic researchers seemto have paid relatively little attention to such eases,and the transcripts we have (to date) examined havecontained relatively few instances where the intendedreferent was not already salient.ltowever, we take the view that, in the general case,a referring expression contains two kinds of informa-tion: nav igat ion  and d isc r iminat ion .
Each de~scriptor used in a referring expression plays one ofthese two roles.?
Navigational, or a t tent ion -d i rec t ing  informa-tion, is intended to bring the intended referent intothe hearer's focus of attention.?
Discrimination information is intended to distin-guish the intended referent from other objects inthe hearer's focus of attention; such informationhas been the subject of this paper.Navigational information is not needed if the in-tended referent is already in the focus of attention.If it is needed, it frequently (although not always)takes the form of loeational information.
The IDASsystem, for example, can generate referring expres-sions such as tl~e black power supply in the equipmentrack.
In this case, in the equipment rack is navigationinformation that is intended to bring the equipmentrack and its components into the hearer's focus ofattention, while black power supply is discriminationinformation that is intended Ix) distinguish the in-tended referent from other members of the context~t  (e.g., the white power supply that is also presentin the equipment rack).The navigation model currently implemented in If)ASis simplistic and not theoretically well-justified.
Wehope to do further research on building a better-ju~stified model of navigation.AcrEs DE COLING-92, NAbrlns.
23-28 ^ o~r 1992 2 3 7 PRoc, OF COLING-92, NANTES, AtJcl.
23-28, 1992Relat ive  At t r ibute  Va luesAs mentioned previously, the transcript analysisshows that human speakers and hearers often pre-fer relative instead of absolute attr ibute values, e.g.,small instead of one inch.
Knowledge bases some-times explicitly encode relative attribute values (e.g.,(size.
small)), but this can cause difficulties when re-ferring expressions need to he generated in differentcontexts; a one-inch screw, for example, might beconsidered to be small in a context where the otherscrews were all two-inch screws, but large in a contextwhere the other screws were all half-inch screws.A better solution is for the knowledge base to recordabsolute attr ibute values, and then for the genera-tion algorithm to automatically convert absolute val-ues to relative values, depending on the values thatother members of the context set pussc~ for this at-tribute.
Thus, the knowledge base might record thata particular screw had (size.
one-inch), and the gen-eration system would choose to call this screw smallor/arye depending on the size of the other screws inthe context set.
We hope to do further research ondetermining how exactly this process hould work.Conc lus ionsWe have presented an algorithm for the generationof referring expressions that  is substantially simplerand faster than the algorithms we have proposedin previous work \[Da189,Rei90a\], largely because itperforms much less length-oriented optimization ofits output.
We have been guided in this simplifica-tion effort by psycholinguistic findings and transcriptanalyses, and believe that the resulting algorithm isa more practical one for natural  anguage generationsystems than the ones we proposed previously.Re ferenceslapp85\] Douglas E Appelt.
Planning English Sentences.Cambridge Univemity Press, New York, 1985.\[BKMWgO\] John Bateman, Robert T Kasper, JohannaD Moore and Richard A Whitney.
A GeneralOrganization ofKnowledge for Natural LanguageProcessing:.
The Penman Upper Model.
Unpub-lished technical report, Information Sciences Insti-tute/University ofSouthern California, 1990.\[BS85\] Ronald Brachman and James Schmolze.
Anoverview of the KL-ONE knowledge r presentationsystem.
Cognitive Science 9:171-216, 1985.\[Cru77\] D. Cruse.
The pragmatics of lexieal specificity.Journal of Linguistics , 13:153-164~ 1977.\[Da188\] Robert Dale.
Generating Referring Expressions ina Domain of Objects and Processes.
PhD Thesis,Centre for Cognitive Science, University of Edin-burgh, 1988.IDol89\] Robert Dale.
Cooking up referring expre~ens.In Proceedings of the $7th Annual Meeting of theAssociation for Computational Linguistics, pages68-75.
1989.\[DH91\] Robert Dale and Nicholas Haddock.
Content de-termination i the generation of referring expres-sions.
Computational Intelligence, 7(4), 1991.\[Don66\] Kelth Donnellan.
Reference and definite descrip-tion.
Philosophical Review, 75:281-304, 1986.\[17075\] William Ford and David Olsea The elaborationof the noun phrase in children's de~riptton of ob-jects.
Journal of Ezpemmentol Child Psychology,19:371-382, 1975.\[GJW83\] Barbara Gresz, Aravind Jeshi, and Scott Wein-stein.
Providing a unified account of definite nounphrasesin discourse.
In Proceedings ofthe ?1st An-nual Meeting of the A ssoeiation for ComputafionalLinguistics, pages 44-50.
1983.\[Gri75\] H. Paul Grice.
Logic and conversation.
In P. Coleand J. Morgun~ editors, Syntax and Semantics: Vat3, Speech Acts, pages 43-58.
Academic Prese, NewYork, 1975.\[GS86\] Barbara Grenz and Candaen Siduer.
Attentlon~intention, and the structure of discourse.
Compu-tatioual Linguistic, 12:175-208, 1986.\[Kns89\] Robert Kasper.
A flexible interface for linkingapplications toPenman's sentence generator.
Pro-ceedings of the 1989 DARPA Speech and NaturalLanguage Workshop~ pages 153-158.\[Kro86\] Amichai Kronfeid.
Donnsllan's distinction and acomputational model of reference.
In Proceedingsof the ~.~th Annual Meeting of the Association forComputational Linguistics, pages 185-191.
1986.\[Lev89\] Willera Levelt.
Speaking: bq~om Intention to Ar-ticulation.
MIT Pre~s, 1989.\[Pec89\] Thomas Pechmann.
Incremental speech produc-tion and referential overspeeificatlon.
Linguistics,27:89-110, 1989.\[Rdg0a\] Ehud Reiter.
The computational complexity ofavoiding conversational implicatures.
In Pea~.2A-ings of the ~8th Annual Meeting of the Associationfor Computational Linguistics, pages 97-104.
1990.\[Relg0b\] Ehud Reiter.
Generating Appropriate NaturalLanguage Object Descriptions.
Phi3 thesis, AikenComputation Lab, Harvard Univeraity, 1990.
Alsoavailable as Aiken Computation Lab technical re-port TK-10-go.\[Rei91\] Ehud Reiter.
A new model of lexical choice fornouns.
Computational Intelligence, 7(4), 1991.\[RML92\] Ehod Relter, Chris Mellish, and John Levine.Automatic generation ofon-line documentation Intbe IDAS project.
In Proceedings of the Third Cor~ferenee on Applied Natural Language Pn~.p_.~sing,pages 64-71.
1992.IRes78\] Eleanor Rcech.
Principles of categorization.
InE.
Resc~ and B. Lloydj editors, Cognition and Cat-egorization, pages 27-48.
Lawrence Erlbaum, Hills-dale, NJ, 1978.\[Sid81\] Canda~e Sidner.
Focusing in the comprehension ofdefinite anaphora.
In M. Brady and R. Berwick,editors, Computational Models of Discourse, pages267-330.
MIT Press, Cambridge, Mass, 1981.\[Son85\] Susan Sonnenschein.
The development of rder-ential communication skills: Some situations inwhich speakers give redundant messages.
Journalof Paycholingnistic Research, 14:489-508, 1985.\[Wh176\] Graver Whitehurst.
The development of commu-nicatiea: Changes with age and modeling.
ChildDevelopment, 47:473-482, 1876.ACIds DE COLING-92, NANI~S, 23-28 AOUT 1992 2 3 8 PROC.
OF COL1NG-92, NANI'ES, AUG. 23-28, 1992
