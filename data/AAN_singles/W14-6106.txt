First Joint Workshop on Statistical Parsing of Morphologically Rich Languagesand Syntactic Analysis of Non-Canonical Languages, pages 66?73 Dublin, Ireland, August 23-29 2014.Self-Training for Parsing Learner TextAoife Cahill, Binod Gyawali and James V. BrunoEducational Testing Service,660 Rosedale Road,Princeton, NJ 08541,USA{acahill, bgyawali, jbruno}@ets.orgAbstractWe apply the well-known parsing technique of self-training to a new type of text: language-learner text.
This type of text often contains grammatical and other errors which can causeproblems for traditional treebank-based parsers.
Evaluation on a small test set of student datashows improvement over the baseline, both by training on native or non-native text.
The maincontribution of this paper adds additional support for the claim that the new self-trained parserhas improved over the baseline by carrying out a qualitative linguistic analysis of the kinds ofdifferences between two parsers on non-native text.
We show that for a number of linguisticallyinteresting cases, the self-trained parser is able to provide better analyses, despite the sometimesungrammatical nature of the text.1 IntroductionThe vast majority of treebank-based parsing research assumes that the text to be parsed is well-formed.In this paper, we are concerned with parsing text written by non-native speakers of English into phrasestructure trees, as a precursor for applications in automated scoring and error detection.
Non-native textoften contains grammatical errors ranging in severity from minor collocational differences to extremelygarbled strings that are difficult to interpret.
These kinds of errors are known to cause difficulty forautomated analyses (De Felice and Pulman, 2007; Lee and Knutsson, 2008).We explore a previously documented technique for adapting a state-of-the-art parser to be able to bet-ter parse learner text: domain adaptation using self-training.
Self-training is a semi-supervised learningtechnique that relies on some labeled data to train an initial model, and then uses large amounts of unla-beled data to iteratively improve that model.
Self-training was first successfully applied in the newspaperparsing domain by McClosky et al.
(2006) who used the Penn Treebank WSJ as their labeled data and un-labeled data from the North American News Text corpus.
Previous attempts (Charniak, 1997; Steedmanet al., 2003) had not shown encouraging results, and McClosky et al.
(2006) hypothesize that the gainthey saw was due to the two-phase nature of the BLLIP parser used in their experiments.
In a follow-upstudy (McClosky et al., 2008) they find that one major factor leading to successful self-training is whenthe process sees known words in new combinations.2 Related WorkFoster et al.
(2011) compare edited newspaper text and unedited forum posts in a self-training parsingexperiment, evaluating on a treebank of informal discussion forum entries about football.
They find thatboth data sources perform about equally well on their small test set overall, but that the underlying gram-mars learned from the two sources were different.
Ott and Ziai (2010) apply an out-of-the-box Germandependency parser to learner text and analyze the impact on down-stream semantic interpretation.
Theyfind that core functions such as subject and object can generally be reliably detected, but that when thereare key elements (e.g.
main verbs) missing from the sentence that the parses are less reliable.
TheyThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/66also found that less-severe grammatical errors such as agreement did not tend to cause problems for theparser.An alternative approach to parsing learner text is to modify the underlying dependency scheme used inparsing to account for any grammatical errors.
This can be useful because it is not always clear what thesyntactic analysis of ungrammatical text should be, given some scheme designed for native text.
Dick-inson and Ragheb (2009) present such a modified scheme for English, designed for annotating syntacticdependencies over a modified POS tagset.
Dickinson and Lee (2009) retrain a Korean dependency parser,but rather than adding additional unlabeled data as we do, they modify the original annotated trainingdata.
The modifications are specifically targeted to be able to detect errors relating to Korean postpo-sitional particles.
They show that the modified parser can be useful in detecting those kinds of particleerrors and in their conclusion suggest self-training as an alternative approach to parsing of learner text.A similar alternative approach is to directly integrate error detection into the parsing process (Menzeland Schro?der, 1999; Vandeventer Faltin, 2003).3 Self-training a new parserWe first describe the data that we use for both training and evaluating our parsers, and then we describeour experiments and results.We take the standard portion of the Penn Treebank (sections 02?21) as our seed labeled data.
Wethen compare two different unlabeled training data sets.
The first data set consists of 480,000 sentencesof newspaper text extracted from the LA Times portion of the North American News Corpus (NANC).The second is a corpus of non-native written English text randomly sampled from a large dataset ofstudent essays.
It consists of 480,900 sentences from 33,637 essays written as part of a test of Englishproficiency, usually administered to non-native college-level students.
The essays have been written to422 different prompts (topics) and so cover a wide range of vocabulary and usage.
Each essay has beenassigned a proficiency level (high, medium, low) by a trained human grader.
17.5% of the sentences werefrom low proficiency essays, 42% from medium proficiency and 40.5% from high proficiency essays.In order to determine the optimal number of self-training iterations and carry out our final evaluationswe use a small corpus of manually treebanked sentences.
The corpus consists of 1,731 sentences writtenby secondary level students which we randomly split into a development set (865 sentences) and a test set(866 sentences).
The native language of the students is unknown, but it is likely that many spoke Englishas their first language.
In addition, this corpus had originally been developed for another purpose andtherefore contains modifications that are not ideal for our experiments.
The main changes are that spellingand punctuation errors were corrected before the trees were annotated (and we do not have access to theoriginal text).
Although the treebanked corpus does not align perfectly with our requirements, we believethat it is a more useful evaluation data set than any other existing treebanked corpus.We used the Charniak and Johnson (2005) (BLLIP) parser1 to perform the self training experiments.Our experiment is setup as follows: first we train a baseline model on the Penn Treebank WSJ data(sections 02-21).
Then, iteratively, sentences are selected from the unlabeled data sets, parsed by theparser, and combined with the previously annotated data to retrain the parser.
The parser also requiresdevelopment data, for which we use section 22 of the WSJ data.
After each iteration we evaluate theparser using our 865-sentence development set.
Parser evaluation was done using the EVALB2 tool andwe report the performance in terms of F1 score.There are two main parameters in our self-training setup: the size of the unlabeled data set added ateach iteration and the weight given to the original labeled data.3 In preliminary experiments, we foundthat a block size of 40,000 sentences per each iteration and a weight of 5 on the original labeled dataperformed best.
Given our training data, and a block size of 40K, this results in 12 iterations.
In eachiteration, the training data consists of the PTB data repeated 5 times, plus the parsed output of previousblocks of unlabeled data.1https://github.com/BLLIP/bllip-parser2http://nlp.cs.nyu.edu/evalb/3Note that this approach differs to that outlined in McClosky et al.
(2006) who only perform one self-training iteration.
It ismore similar to the approach described in Reichart and Rappoport (2007).67The results of our experiments are as shown in Figure 1.
Iteration 0 corresponds to the baseline parserwhile iterations 1?12 are the self trained parsers.
We see that the F1 score of the baseline parser is80.9%.4 The self trained parsers have higher accuracies compared to the baseline parser starting at thefirst iteration.
The highest score training on non-native text (82.3%) was achieved on the 11th iteration,and the highest score training on newspaper text (81.8%) was achieved on the 8th iteration.
Both of theseresults are statistically significantly better than the baseline parser only trained on WSJ text.5 The graphalso shows that the non-native training results in slightly higher overall f-scores than the parser trainedon the native data after iteration 5, however these differences are not statistically significant.81.581.758282.2582.5F 1 S c o rF1?score?of?each?iterationNon?Native?EnglishNANC80.758181.250123456789101112r e ( % )Iteration?numberFigure 1: Performance of parsers after each iteration.
Parsers used WSJ Section 22 as development dataand were evaluated on the student response development data.The final evaluation was carried out by evaluating on the student test corpus of 866 sentences, usingthe parsing model that performed best on the student dev corpus.
The parser trained on native textachieved an f-score of 82.4% and the parser trained on the non-native text achieved an f-score of 82.6%.This difference is not statistically significant and is a similar finding to Foster et al.
(2011).
In anotherexperiment, we found that if the development data used during self-training is similar to the test data, wesee even smaller differences between the two different kinds of training data.64 AnalysisWe carry out a qualitative analysis of the differences in parses between the original parser and one of thebest-performing self-trained ones, trained on non-native text.
We randomly sample 5 essays written bynon-native speakers (but not overlapping with the data used to self-train the parser).
Table 1 shows thenumber of sentences and the number of parse trees that differ, according to each proficiency level.Proficiency # Essays # Sentences # Words # Differing Parses % Differing ParsesHigh 2 30 694 12 40Mid 1 22 389 12 54Low 2 17 374 8 47Totals 5 69 1457 32 46Table 1: Descriptive Statistics for Essays in the Qualitative Sample4Note that these overall f-scores are considerably lower than current state-of-the-art for newspaper text, indicating that thisset of student texts are considerably different.5Significance testing was carried out using Dan Bikel?s Randomized Parsing Evaluation Comparator script for comparingevalb output files.
We performed 1000 random shuffles and tested for p-values < 0.01.6These data sets were all quite small, however, so further investigation is required to fully assess this finding.68Figure 2 reports the number of differences by proficiency level.
It is important to note that thesedifferences only included ones that were considered to be independent (e.g.
a change in POS tag thatnecessitated a change in constituent label was only counted once).
We note a trend in which the self-trained parser produces better parses than the baseline more often; however, at the highest proficiencylevel the baseline parser produces better parses more often.
In some applications it might be possible totake the proficiency level into account before running the parser.
However for many applications this willpresent a challenge since the parser output plays a role in predicting the proficiency level.
A possiblealternative would be to approximate proficiency using frequencies of spelling and other grammaticalerrors that can be automatically detected without relying on parser output and use this information todecide which version of the parser to use.10 14 75 434 3 910%20%30%40%50%60%70%80%90%100%Independent Differences0% Low(Total = 19) Mid(Total = 21) High(Total = 19)IProficiency LevelBetter Self-Trained Better Unclear Better OriginalFigure 2: Unrelated Differences by Proficiency Level.We systematically examine each of the 32 pairs of differing parse trees in the sample and manuallycategorize the differences.
Figure 3 shows the 5 most frequent types of differences, their breakdownby proficiency level, as well as the results of a subjective evaluation on which parse was better.
Thesejudgements were made by one of the authors of this paper who is a trained linguist.3 3 5 4 2 4 5 42221 1 11251231 1 2 102468101214Low Mid High Low Mid High Low Mid High Low Mid High Low Mid HighAttachment Site(Total = 22) POS Tag(Total = 15) Sentential Components(Total = 10)POS of Misspelled Terminal(Total = 6)Headedness(Total = 5)Better Self-Trained Better Unclear Better OriginalFigure 3: Parse Tree Differences by Proficiency Level.The differences in Figure 3 are defined as follows.
Attachment Site: the same constituent is attachedto different nodes in each parse; POS Tag: the same terminal bears a different POS tag in each parse,where the terminal exists in our dictionary of known English7 words; Sentential Components: One parsegroups a set of constituents exhaustively into an S-node, while the other does not; POS of misspelledterminal: the same terminal bears a different POS tag in each parse, where the terminal has been flaggedas a misspelling; Headedness: a terminal heads a maximal projection of a different syntactic category inone parse but not the other, (e.g.
a VP headed by a nominal).7We use the python package enchant with US spelling dictionaries to carry out spelling error detection.69We characterized the differences according to whether the better output was produced by the originalparser, the self trained parser, or if it was not clear that either parse was better than the other.
AttachmentSite differences were evaluated according to whether or not they were attached to the constituent theymodified; POS Tag differences were evaluated according to the Penn Treebank Guidelines (Santorini,1995); Sentential Components differences were evaluated according to whether or not the terminalsshould indeed form a clausal constituent, infinitive, or gerund; POS of Misspelled Terminal differenceswere evaluated according to the evaluator?s perception of the writer?s intended target.
We note thatthe most abundant differences are in Attachment Site, that the biggest improvements resulting from self-training are in the recognition of Sentential Components and in the identification of the POS of MisspelledTerminals, and that the biggest degradation is in Headedness.4.1 General Difference PatternsUsing the categories defined during the manual analysis of the 5 essays, we develop rules to automaticallydetect these kinds of differences in a large dataset.
We expect that the automatic rules will identify moredifferences than the linguist, however we hope to see the same general patterns.
We apply our rules to anadditional set of data consisting of roughly 10,000 sentences written by non-native speakers of English.Table 2 shows the number of sentences for which the parsers found different parses at each proficiencylevel, and Table 3 gives the totals for each of the five difference categories described above.Proficiency # Essays # Sentences # Words # Differing Parses % Differing ParsesHigh 256 4178 266543 2214 53Mid 285 4168 263685 2364 57Low 149 1657 93466 971 59Totals 690 10003 623694 5549 55Table 2: Descriptive Statistics for Essays in the Larger SampleDifference Total Low Medium HighAttachment Site 7805 1331 3474 3000POS Tag 6827 1205 3238 2384Sentential Components 4103 778 1786 1539POS of Misspelled Terminal 2040 346 894 800Headedness 1357 353 568 436Table 3: Total number of differences detected automatically by proficiency levelWe see that the proportion of sentences with different parses is similar to the 5-essay sample and alsothat the relative ordering of the five difference categories is identical.
This at least indicates that the5-essay sample does not differ largely in its general properties from a larger set.4.2 Illustrative ObservationsWe highlight some of the most interesting differences between the baseline parser and the self-trainedparser, using examples from our 5-essay sample described above.Ambiguity of subordinating conjunctions: Figure 4 shows an example from a lower proficiencyessay that contains multiple interacting differences, primarily stemming from the fact that the POS tagfor a subordinating conjunction is the same as the POS tag for a regular preposition according to thePenn Treebank guidelines (Santorini, 1995).
The original parser (4a) treats it as a preposition: it isdominated by PP and takes NP as a complement.
The self-trained parser (4b) correctly treats becauseas a subordinating conjunction: it is dominated by SBAR and takes S as a complement.
In addition,the original parser identified suffer as the main verb in the sentence.
The self-trained parser correctlyanalyzes this as part of the dependent clause, however this results in no main verb being identified andan overall FRAGMENT analysis.
Since it is unclear what the original intention of the writer was, thisfragment analysis could be more useful for identifying grammatical errors and giving feedback.70S.
.
.
PPINbecauseNPDTtheNNworldVPVBPsuffer.
.
.
(a) Original ParserFRAG.
.
.
SBARINbecauseSNPDTtheNNworldVPVBPsuffer.
.
.
(b) Self-Trained ParserFigure 4: Parses for Especaily, in this time, because the world suffer, the economy empress.Ambiguity of to: Figure 5 exemplifies a difference related to the analysis of infinitives.
Here we cansee that the original parser analyzed the to phrase as a PP (c.f.
afraid of) whereas the self-trained parseranalyzes it as an infinitival.
We believe that the infinitival interpretation is slightly more likely (with amissing verb do), though of course it is difficult to say for sure what the intended meaning is.
Here thereare two interacting difference types: Sentential Components and Headedness.
In the self-trained parse,anything is an NN that heads a VP, whereas it is an NN that appropriately heads an NP in the originalparse.
However, it is important to note that the self-trained parse treats to anything as an infinitive: a TOdominated by a VP, which is dominated by a unary-branching S. The original parse treats to anything asa regular PP.
The fact that the self-trained parse contains a set of terminals exhaustively dominated byan S-node that does not exist in the original parse constitutes a Sentential Components difference.
Webelieve that it is more useful to correctly identify infinitives and gerunds as sentential constituents, evenat the cost of an XP that is apparently headed by an inappropriate terminal (VP headed by NN).S.
.
.
AdjPJJafraidPPTOtoNPNNanythingPPduring your life(a) Original ParserS.
.
.
AdjPJJafraidSVPTOtoVPNNanythingPPduring your life(b) Self-Trained ParserFigure 5: Parses for If you have this experience, you will do not afraid to anything during your life.Attachment ambiguity: We turn now to Figure 6.
The main difference has to do with the attachmentof the phrase that you think it worth: the SBAR is attached to the VP in the original parse (as a clausalcomplement) and to the NP in the self-trained parse (as a relative clause).
This example also shows thata change in POS-tag can have a significant impact on the final parse tree.5 Future Work and ConclusionsWe have shown that it is possible to apply self-training techniques in order to adapt a state-of-the-artparser to be able to better parse English language learner text.
We experimented with training the parseron native text as well as non-native text.
In an evaluation on student data (not necessarily language-71VPVBNusedPPINinNPDTtheNNthingSBARINthatSyou think it worth(a) Original ParserVPVBNusedPPINinNPNPDTtheNNthingSBARWHNPWDTthatSyou think it worth(b) Self-Trained ParserFigure 6: Parses for So I support that the money should be used in the thing that you think it worth.learner data) we found that both training sets performed at about the same level, but that both significantlyout-performed the baseline parser trained only on WSJ text.We carry out an in-depth study on a small data set of 5 learner essays and define a set of differencecategories in order to describe the parse-tree differences from a linguistic perspective.
We implementrules to automatically detect these parse-tree differences and show that the general proportions of errorsfound in the small data set are similar to that of a larger data set.
We highlight some of the most interestingimprovements of the parser, and we show that despite various grammatical errors present in sentences,the self-trained parser is, in general, able to assign better analyses than the baseline parser.Of course, the self-trained parser does sometimes choose a parse that is less appropriate than thebaseline one.
In particular, we noticed that this happened most frequently for the highest proficiencyessays.
Further investigation is required to be able to better understand the reasons for this.
In futurework, the most informative evaluation of the self-trained parser would be in a task-based setting.
Weplan to investigate whether the self-trained parser improves the overall performance of tasks such asautomated essay scoring or automated error detection, which internally rely on parser output.ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL?05), pages173?180, Ann Arbor, Michigan, June.
Association for Computational Linguistics.Eugene Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In Proceedings of theFourteenth National Conference on Artificial Intelligence, pages 598?603, Menlo Park, CA.
AAAI Press/MITPress.Rachele De Felice and Stephen Pulman.
2007.
Automatically Acquiring Models of Preposition Use.
In Pro-ceedings of the Fourth ACL-SIGSEM Workshop on Prepositions, pages 45?50, Prague, Czech Republic, June.Association for Computational Linguistics.Markus Dickinson and Chong Min Lee.
2009.
Modifying corpus annotation to support the analysis of learnerlanguage.
CALICO Journal, 26(3):545?561.Markus Dickinson and Marwa Ragheb.
2009.
Dependency Annotation for Learner Corpora.
In Proceedings ofthe Eighth Workshop on Treebanks and Linguistic Theories (TLT-8), pages 59?70, Milan, Italy.Jennifer Foster, O?zlem C?etinog?lu, Joachim Wagner, and Josef van Genabith.
2011.
Comparing the Use of Editedand Unedited Text in Parser Self-Training.
In Proceedings of the 12th International Conference on ParsingTechnologies, pages 215?219, Dublin, Ireland.
Association for Computational Linguistics.72John Lee and Ola Knutsson.
2008.
The Role of PP Attachment in Preposition Generation.
In Proceedingsof CICLing 2008, 9th International Conference on Intelligent Text Processing and Computational Linguistics,pages 643?654, Haifa, Israel.David McClosky, Eugene Charniak, and Mark Johnson.
2006.
Effective Self-Training for Parsing.
In Proceedingsof the Human Language Technology Conference of the NAACL, Main Conference, pages 152?159, New YorkCity, USA, June.
Association for Computational Linguistics.David McClosky, Eugene Charniak, and Mark Johnson.
2008.
When is Self-Training Effective for Parsing?
InProceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 561?568,Manchester, UK, August.
Coling 2008 Organizing Committee.Wolfgang Menzel and Ingo Schro?der.
1999.
Error diagnosis for language learning systems.
ReCALL, 11:20?30.Niels Ott and Ramon Ziai.
2010.
Evaluating dependency parsing performance on German learner language.
InProceedings of the Ninth Workshop on Treebanks and Linguistic Theories (TLT-9), pages 175?186.Roi Reichart and Ari Rappoport.
2007.
Self-Training for Enhancement and Domain Adaptation of StatisticalParsers Trained on Small Datasets.
In Proceedings of the 45th Annual Meeting of the Association of Computa-tional Linguistics, pages 616?623, Prague, Czech Republic, June.
Association for Computational Linguistics.Beatrice Santorini.
1995.
Part-of-speech tagging guidelines for the Penn Treebank Project (3rd revision).
Techni-cal Report, Department of Computer and Information Science, University of Pennsylvania.Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim.
2003.
Bootstrapping statistical parsers from small datasets.
In Proceedingsof EACL 03, pages 331?228.Anne Vandeventer Faltin.
2003.
Syntactic Error Diagnosis in the context of Computer Assisted Language Learn-ing.
Ph.D. thesis, Universite?
de Gene`ve.73
