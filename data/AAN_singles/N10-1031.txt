Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 250?253,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsExtending the METEOR Machine Translation Evaluation Metric to thePhrase LevelMichael Denkowski and Alon LavieLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15232, USA{mdenkows,alavie}@cs.cmu.eduAbstractThis paper presents METEOR-NEXT, an ex-tended version of the METEOR metric de-signed to have high correlation with post-editing measures of machine translation qual-ity.
We describe changes made to the met-ric?s sentence aligner and scoring scheme aswell as a method for tuning the metric?s pa-rameters to optimize correlation with human-targeted Translation Edit Rate (HTER).
Wethen show that METEOR-NEXT improves cor-relation with HTER over baseline metrics, in-cluding earlier versions of METEOR, and ap-proaches the correlation level of a state-of-the-art metric, TER-plus (TERp).1 IntroductionRecent focus on the need for accurate automaticmetrics for evaluating the quality of machine trans-lation output has spurred much development in thefield of MT.
Workshops such as WMT09 (Callison-Burch et al, 2009) and the MetricsMATR08 chal-lenge (Przybocki et al, 2008) encourage the devel-opment of new MT metrics and reliable human judg-ment tasks.This paper describes our work extending the ME-TEOR metric to improve correlation with human-targeted Translation Edit Rate (HTER) (Snover etal., 2006), a semi-automatic post-editing based met-ric which measures the distance between MT out-put and a targeted reference.
We identify severallimitations of the original METEOR metric and de-scribe our modifications to improve performance onthis task.
Our extended metric, METEOR-NEXT, isthen tuned to maximize segment-level correlationwith HTER scores and tested against several base-line metrics.
We show that METEOR-NEXT outper-forms earlier versions of METEOR when tuned to thesame HTER data and approaches the performance ofa state-of-the-art TER-based metric, TER-plus.2 The METEOR-NEXT Metric2.1 Traditional METEOR ScoringGiven a machine translation hypothesis and a refer-ence translation, the traditional METEOR metric cal-culates a lexical similarity score based on a word-to-word alignment between the two strings (Baner-jee and Lavie, 2005).
When multiple references areavailable, the hypothesis is scored against each andthe reference producing the highest score is used.Alignments are built incrementally in a series ofstages using the following METEOR matchers:Exact: Words are matched if and only if their sur-face forms are identical.Stem: Words are stemmed using a language-appropriate Snowball Stemmer (Porter, 2001) andmatched if the stems are identical.Synonym: Words are matched if they are bothmembers of a synonym set according to the Word-Net (Miller and Fellbaum, 2007) database.
Thismatcher is limited to translations into English.At each stage, one of the above matchers iden-tifies all possible word matches between the twotranslations using words not aligned in previousstages.
An alignment is then identified as the largestsubset of these matches in which every word in eachsentence aligns to zero or one words in the other sen-250tence.
If multiple such alignments exist, the align-ment is chosen that best preserves word order byhaving the fewest crossing alignment links.
At theend of each stage, matched words are fixed so thatthey are not considered in future stages.
The finalalignment is defined as the union of all stage align-ments.Once an alignment has been constructed, the to-tal number of unigram matches (m), the number ofwords in the hypothesis (t), and the number of wordsin the reference (r) are used to calculate precision(P = m/t) and recall (R = m/r).
The parame-terized harmonic mean of P and R (van Rijsbergen,1979) is then calculated:Fmean =P ?R?
?
P + (1 ?
?)
?RTo account for differences in word order, the min-imum number of ?chunks?
(ch) is calculated where achunk is defined as a series of matched unigrams thatis contiguous and identically ordered in both sen-tences.
The fragmentation (frag = ch/m) is thenused to calculate a fragmentation penalty:Pen = ?
?
frag?The final METEOR score is then calculated:Score = (1 ?
Pen) ?
FmeanThe free parameters ?, ?, and ?
can be tuned tomaximize correlation with various types of humanjudgments (Lavie and Agarwal, 2007).2.2 Extending the METEOR AlignerTraditional METEOR is limited to unigram matches,making it strictly a word-level metric.
By focus-ing on only one match type per stage, the alignermisses a significant part of the possible alignmentspace.
Further, selecting partial alignments basedonly on the fewest number of per-stage crossingalignment links can in practice lead to missing fullalignments with the same number of matches infewer chunks.
Our extended aligner addresses theselimitations by introducing support for multiple-wordphrase matches and considering all possible matchesin a single alignment stage.We introduce an additional paraphrase matcherwhich matches phrases (one or more successivewords) if one phrase is considered a paraphrase ofthe other by a paraphrase database.
For English, weuse the paraphrase database developed by Snover etal.
(2009), using techniques presented by Bannardand Callison-Burch (2005).The extended aligner first constructs a searchspace by applying all matchers in sequence to iden-tify all possible matches between the hypothesis andreference.
To reduce redundant matches, stem andsynonym matches between pairs of words whichhave already been identified as exact matches are notconsidered.
Matches have start positions and lengthsin both sentences; a word occurring less than lengthpositions after a match start is said to be covered bythe match.
As exact, stem, and synonym matcheswill always have length one in both sentences, theycan be considered phrase matches of length one.Since other matches can cover phrases of differentlengths in the two sentences, matches are now saidto be one-to-one at the phrase level rather than theword level.Once all possible matches have been identified,the aligner identifies the final alignment as thelargest subset of these matches meeting the follow-ing criteria in order of importance:1.
Each word in each sentence is covered by zeroor one matches2.
Largest number of covered words across bothsentences3.
Smallest number of chunks, where a chunk isnow defined as a series of matched phrases thatis contiguous and identically ordered in bothsentences4.
Smallest sum of absolute distances betweenmatch start positions in the two sentences (pre-fer to align words and phrases that occur at sim-ilar positions in both sentences)The resulting alignment is selected from the fullspace of possible alignments and directly optimizesthe statistics on which the the final score will be cal-culated.2.3 Extended METEOR ScoringOnce an alignment has been chosen, the METEOR-NEXT score is calculated using extended versions of251the traditional METEOR statistics.
We also introducea tunable weight vector used to dictate the relativecontribution of each match type.
The extended ME-TEOR score is calculated as follows.The number of words in the hypothesis (t) andreference (r) are counted.
For each of the match-ers (mi), count the number of words covered bymatches of this type in the hypothesis (mi(t)) andreference (mi(r)) and apply the appropriate moduleweight (wi).
The weighted Precision and Recall arethen calculated:P =?iwi ?mi(t)tR =?iwi ?mi(r)rThe minimum number of chunks (ch) is then cal-culated using the new chunk definition.
Once P , R,and ch are calculated, the remaining statistics andfinal score can be calculated as in Section 2.1.3 Tuning for Post-Editing Measures ofQualityHuman-targeted Translation Edit Rate (HTER)(Snover et al, 2006), is a semi-automatic assessmentof machine translation quality based on the numberof edits required to correct translation hypotheses.
Ahuman annotator edits each MT hypothesis so that itis meaning-equivalent with a reference translation,with an emphasis on making the minimum possiblenumber of edits.
The Translation Edit Rate (TER)is then calculated using the human-edited transla-tion as a targeted reference for the MT hypothe-sis.
The resulting scores are shown to correlate wellwith other types of human judgments (Snover et al,2006).3.1 Tuning Toward HTERThe GALE (Olive, 2005) Phase 2 unsequestereddata includes HTER scores for multiple Arabic-to-English and Chinese-to-English MT systems.
Weused HTER scores for 10838 segments from 1045documents from this data set to tune both the orig-inal METEOR and METEOR-NEXT.
Both were ex-haustively tuned to maximize the length-weightedsegment-level Pearson?s correlation with the HTERscores.
This produced globally optimal ?, ?, and ?values for METEOR and optimal ?, ?, ?
values plusstem, synonym, and paraphrase match weights forTask ?
?
?Adequacy & Fluency 0.81 0.83 0.28Ranking 0.95 0.50 0.50HTER 0.70 1.95 0.50HTER (extended) 0.65 1.95 0.45Stem Syn Par0 0.4 0.9Table 1: Parameter values for various METEOR tasks fortranslations into English.METEOR-NEXT (with the weight of exact matchesfixed at 1).
Table 1 compares the new HTER pa-rameters to those tuned for other tasks including ad-equacy and fluency (Lavie and Agarwal, 2007) andranking (Agarwal and Lavie, 2008).As observed by Snover et al (2009), HTERprefers metrics which are more balanced betweenprecision and recall: this results in the lowest valuesof ?
for any task.
Additionally, non-exact matchesreceive lower weights, with stem matches receivingzero weight.
This reflects a weakness in HTER scor-ing where words with matching stems are treated ascompletely dissimilar, requiring full word substitu-tions (Snover et al, 2006).4 ExperimentsThe GALE (Olive, 2005) Phase 3 unsequestereddata includes HTER scores for Arabic-to-EnglishMT output.
We created a test set from HTER scoresof 2245 segments from 195 documents in this dataset.
Our evaluation metric (METEOR-NEXT-hter)was tested against the following established metrics:BLEU (Papineni et al, 2002) with a maximum N -gram length of 4, TER (Snover et al, 2006), versionsof METEOR based on release 0.7 tuned for adequacyand fluency (METEOR-0.7-af) (Lavie and Agarwal,2007), ranking (METEOR-0.7-rank) (Agarwal andLavie, 2008), and HTER (METEOR-0.7-hter).
Alsoincluded is the HTER-tuned version of TER-plus(TERp-hter), a metric with state-of-the-art perfor-mance in recent evaluations (Snover et al, 2009).Length-weighted Pearson?s and Spearman?s correla-tion are shown for all metrics at both the segment(Table 2) and document level (Table 3).
System levelcorrelations are not shown as the Phase 3 data onlycontained the output of 2 systems.252Metric Pearson?s r Spearman?s ?BLEU-4 -0.496 -0.510TER 0.539 0.510METEOR-0.7-af -0.573 -0.561METEOR-0.7-rank -0.561 -0.556METEOR-0.7-hter -0.574 -0.562METEOR-NEXT-hter -0.600 -0.581TERp-hter 0.627 0.610Table 2: Segment level correlation with HTER.Metric Pearson?s r Spearman?s ?BLEU-4 -0.689 -0.686TER 0.675 0.679METEOR-0.7-af -0.696 -0.699METEOR-0.7-rank -0.691 -0.693METEOR-0.7-hter -0.704 -0.705METEOR-NEXT-hter -0.719 -0.713TERp-hter 0.738 0.747Table 3: Document level correlation with HTER.METEOR-NEXT-hter outperforms all baselinemetrics at both the segment and document level.Bootstrap sampling indicates that the segment-levelcorrelation improvements of 0.026 in Pearson?s rand 0.019 in Spearman?s ?
over METEOR-0.7-hterare statistically significant at the 95% level.
TERp?scorrelation with HTER is still significantly higheracross all categories.
Our metric does run signifi-cantly faster than TERp, scoring approximately 120segments per second to TERp?s 3.8.5 ConclusionsWe have presented an extended METEOR metricwhich shows higher correlation with HTER thanbaseline metrics, including traditional METEORtuned on the same data.
Our extensions are notspecific to HTER tasks; improved alignments andadditional features should improve performance onany task having sufficient tuning data.
Although ourmetric does not outperform TERp, it should be notedthat HTER incorporates TER alignments, providingTER-based metrics a natural advantage.
Our metricalso scores segments relatively quickly, making it aviable choice for tuning MT systems.AcknowledgementsThis work was funded in part by NSF grants IIS-0534932 and IIS-0915327.ReferencesAbhaya Agarwal and Alon Lavie.
2008.
Meteor, m-bleuand m-ter: Evaluation Metrics for High-Correlationwith Human Rankings of Machine Translation Output.In Proc.
of WMT08, pages 115?118.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Proc.of the ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization, pages 65?72.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proc.
ofACL05, pages 597?604.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
In Proc.of WMT09, pages 1?28.Alon Lavie and Abhaya Agarwal.
2007.
METEOR: AnAutomatic Metric for MT Evaluation with High Lev-els of Correlation with Human Judgments.
In Proc.
ofWMT07, pages 228?231.George Miller and Christiane Fellbaum.
2007.
WordNet.http://wordnet.princeton.edu/.Joseph Olive.
2005.
Global Autonomous Language Ex-ploitation (GALE).
DARPA/IPTO Proposer Informa-tion Pamphlet.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In Proc.
of ACL02,pages 311?318.Martin Porter.
2001.
Snowball: A language for stem-ming algorithms.
http://snowball.tartarus.org/texts/.M.
Przybocki, K. Peterson, and S Bronsart.
2008.Official results of the NIST 2008 "Metrics forMAchine TRanslation" Challenge (MetricsMATR08).http://nist.gov/speech/tests/metricsmatr/2008/results/.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Proc.
of AMTA-2006, pages 223?231.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, Adequacy, orHTER?
Exploring Different Human Judgments with aTunable MT Metric.
In Proc.
of WMT09, pages 259?268.C.
van Rijsbergen, 1979.
Information Retrieval, chap-ter 7.
2nd edition.253
