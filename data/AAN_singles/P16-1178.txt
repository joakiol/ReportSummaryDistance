Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1893?1902,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLinguistic Benchmarks of Online News Article QualityIoannis Arapakis?EurecatBarcelona, Spainarapakis.ioannis@gmail.comFilipa Peleja?VodafoneLisbon, Portugalfilipapeleja@gmail.comB.
Barla Cambazoglu?Independent Researcherbarla@berkantbarlacambazoglu.comJoao MagalhaesNOVA-LINCS, DI, FCTUniversidade NOVA Lisboa, Portugaljmag@fct.unl.ptAbstractOnline news editors ask themselves thesame question many times: what is miss-ing in this news article to go online?
Thisis not an easy question to be answered bycomputational linguistic methods.
In thiswork, we address this important questionand characterise the constituents of newsarticle editorial quality.
More specifically,we identify 14 aspects related to the con-tent of news articles.
Through a correla-tion analysis, we quantify their indepen-dence and relation to assessing an article?seditorial quality.
We also demonstrate thatthe identified aspects, when combined to-gether, can be used effectively in qualitycontrol methods for online news.1 IntroductionA recent study1found that online news is nowa-days the main source of news for the populationin the 18-29 age group (71%), and as popular asTV in the 30-39 age group (63%).
The readersappetite for high-quality online news result in anoffer of thousands of articles published every dayin the whole of the Web.
For instance, it is not un-common to find the same facts reported by manydifferent online news articles.
However, only afew of them actually grab the attention of the read-ers.
Journalists and editors follow standardiseddiscourse rules and techniques aiming at engagingthe reader in the article?s narrative of article (Louisand Nenkova, 2013).Analysing the discourse of such articles is cen-tral to properly assessing the quality of online?This work was done while the authors were at Ya-hoo!Research Barcelona.1http://www.people-press.org/2013/08/08/amid-criticism-support-for-medias-watchdog-role-stands-outnews (van Dijk and Kintsch, 1983).
Definingthe variables that computational linguistics shouldquantify is a challenging task.
Several questionsarise from this exercise.
For example, what doesthe quality refer to?
What makes a new article per-ceived as high quality by the editors/users?
Whataspects of an article correlate better with its per-ceived quality?
Can we predict the quality of anarticle using linguistic features extracted from itscontent?
These are the kind of questions we ad-dress in this paper.To this end, we propose a linguistic resourceand assessment methodology to quantify the ed-itorial quality of online news discourse.
We ar-gue that quality is too complex to be representedby a single number and should be instead decom-posed into a set of simpler variables that capturethe different linguistic and narrative aspects of on-line news.
Thus, we depart from current literatureand propose a multidimensional representation ofquality.
The first contribution of this paper is ataxonomy of 14 different content aspects that areassociated with the editor-perceived quality of on-line news articles.
The proposed 14 aspects are theresult of an editorial study involving professionaleditors, journalists, and computational linguists.The second contribution of this paper is anexpert-annotated corpus of online news articlesobtained from a major news portal.
This corpusis curated by the editors and journalists who an-notated the articles with respect to the 14 aspectsand to the general editorial quality.
To confirmthe independence and relevance of the proposedaspects, we perform a correlation analysis on thisground-truth to determine the strength of the asso-ciations between different aspects and article edi-torial quality.
Our analysis shows that the editor-perceived quality of an article exhibits a strongpositive correlation with certain aspects, such as1893fluency and completeness, while it is weakly cor-related with other aspects like subjectivity and po-larity.As a baseline benchmark, we investigate thefeasibility of predicting the quality aspects of anarticle using features extracted from the articleonly.
Our findings indicate that article editorialquality prediction is a challenging task and that ar-ticle quality can be predicted to a varying degree,depending on the feature space.
The proposed as-pects can be used to control the editorial qualitywith a Root Mean Squared Error (RMSE) of 0.398on a 5-point Likert-scale.The rest of the paper is organised as follows.Next, we discuss existing literature in discourseanalysis and text quality metrics.
In Section 3,we present the aspects that we identified as po-tential indicators of article quality.
Section 4 pro-vides the details of our online news corpus target-ing the aspects of editorial quality control.
Theresults of the correlation analysis conducted be-tween the identified aspects and article quality arepresented in Section 4.
In Section 5, we present abaseline benchmark to automatically infer individ-ual aspects and editorial quality from online news.2 Related WorkA very recent work related to ours is (Gao et al,2014), where the authors try to predict the inter-estingness of a news article for a user who is cur-rently reading another news article.
In our work,however, we try to predict the perceived quality ofan article without using any context informationother than the content of the article itself.
More-over, while the authors of (Gao et al, 2014) takea quite pragmatic approach to handle the prob-lem, we follow a more principled approach andmodel the quality of a news article according tofive orthogonal dimensions: readability, informa-tiveness, style, topic, and sentiment.
Work hasbeen done in each one of these dimensions, butnone has tackled the problem of modelling overallarticle quality in a comprehensive and articulatedmanner as we do.
Below, we provide a survey ofthe previous work on these dimensions.The readability of a piece of text can be de-fined as the ease that the text can be processedand understood by a human reader (Richards andSchmidt, 2013; Zamanian and Heydari, 2012).The readability is usually associated with fluencyand writing quality (Nenkova et al, 2010; Pitlerand Nenkova, 2008).
Even though there is a sig-nificant amount of research that targets readabil-ity, most work (Redish, 2000; Yan et al, 2006)were originally designed to measure the readabil-ity of school books and do not suit well to morecomplex reading materials, such as news articles,which form the focus of our work.The informativeness of a news article has beentackled from several different angles.
In (Tang etal., 2003), news information quality was charac-terised by a set of nine aspects that were shownto have a good correlation with textual features.Catchy titles were shown to often lead to frustra-tion, as the reader does not get the content thatshe expects (Louis and Nenkova, 2011).
The taskof assessing a news title?s descriptiveness is re-lated to semantic text similarity and has been re-searched by the SemEval initiative (Agirre et al,2013).
Moreover, the completeness of a news ar-ticle is an aspect that has been considered in thepast by (Louis and Nenkova, 2014), which showedthat reporting the news with adequate detail is keyto provide the reader with enough information tograsp the entire story.
The freshness of news in-formation also sets the tone of the discourse: in-formation can be novel to the average reader or itcan be already known and be presented as a ref-erence to the reader.
The novelty of an article isessentially accomplished by either analysing pre-vious articles (Gamon, 2006) or by relying on real-time data from social-media services (Phelan etal., 2009).The characterisation of the style of text compo-sitions has been an active topic of research in com-munication sciences and humanities.
An excellentexample of the research done in this area is the in-fluential work in (McNamara et al, 2009), wherethe authors found the best predictors of writingquality to be the syntactic complexity (numberof words before the main verb), the diversity ofwords used by the author, and some other shallowfeatures.
In NLP, the writing style has been inves-tigated in several contexts.
A problem relevant tothe one we addressed is the characterisation of anauthor?s writing style to predict the success of nov-els (Ashok et al, 2013).
The authors investigated awide range of complex linguistic features, rangingfrom simple unigrams to distribution of word cate-gories, grammar rules, distribution of constituents,sentiment, and connotation.
The comparison ofnovels and news articles revealed a great similar-1894ity in the writing style of novels and informativearticles.The broadness of a news topic has an impacton the reader?s perceived quality of the article.
Atechnical article is usually targeting niche groupsof users and a popular article targets the masses.One of the few corpus (Louis and Nenkova, 2013)addressing quality was limited to the domain ofscientific journalism, thus more technical articles.This corpus only considered news from the NewYork Times, thus contained already very goodquality news.
Two recent work investigated thefeasibility of predicting news articles?
feature pop-ularity in social media at cold start (Bandari et al,2012; Arapakis et al, 2014a).
In (Bandari et al,2012), features extracted from the article?s contentas well as additional meta-data was used to pre-dict the number of times an article will be sharedin Twitter after it went online.
In (Arapakis et al,2014a), a similar study was repeated to predict thepopularity of a news article in social media usingadditional features obtained from external sources.Sentiment analysis concerns the subjectivityand the strength and sign of the opinions expressedin a given piece of text.
In (Arapakis et al, 2014b),it was demonstrated that news articles exhibit con-siderable variation in terms of the sentimentalityand polarity of their content.
The work in (Phelanet al, 2009) has provided evidence that sentiment-related aspects are important to profile and assessthe quality of news articles.
Sentiment analysishas been applied to news articles in other contextsas well (Godbole et al, 2007; Balahur et al, 2010).3 Modeling News Article QualityThe editorial control of news articles is an un-solved task that involves addressing a number ofissues, such as identifying the characteristics of aneffective text, determining what methods producereliable and valid judgments for text quality, aswell as selecting appropriate aspects of text evalu-ation that can be automated using machine learn-ing methods.
Underlying these tasks is a maintheme: can we identify benchmarks for character-ising news article quality?
Therefore, there is aneed for empirical work to identify the global andlocal textual features which will help us make anoptimal evaluation of news articles.By doing so, we achieve two goals.
On onehand, we can offer valuable insights with respectto what constitutes an engaging, good quality newsarticle.
On the other hand, we can identify bench-marks for characterising news article quality in anautomatic and scalable way and, thus, predict poorwriting before a news article is even published.This can help reduce greatly the burden of manualevaluation which is currently performed by profes-sional editors.3.1 MethodologyThe methodology described here provides aframework for characterising and modelling newsarticle editorial quality.
In our work, we followa bottom-up approach and identify 14 differentcontent aspects that are good predictors (as wedemonstrate in Section 6.1) of news article qual-ity.
The aspects we identified are informed byinput from news editors, journalists and compu-tational linguists, and previous research in NLPand, particularly, the efforts in text summarisa-tion (Bouayad-Agha et al, 2012), document un-derstanding (Dang, 2005; Seki et al, 2006) andquestion answering (Surdeanu et al, 2008; Shtoket al, 2012).After discussing the editorial quality controlwith professionals, we gathered a set of heuristicsand examined the literature for ways of design-ing quantitative measures to achieve our goal.
Wegroup the aspects under five headings: readability,informativeness, style, topic, and sentiment (seeFig.
1).
Below, we provide a brief description ofeach aspect.3.2 ReadabilityHigh quality articles are written in a way thatmakes them easier to read.
In our model, we in-clude two different aspects related to readability(Pitler and Nenkova, 2008): fluency and concise-ness.Fluency: Fluent articles are built from sentenceto sentence, forming a coherent body of informa-tion.
Consecutive sentences are meaningfully con-nected.
Similarly, paragraphs are written in a log-ical sequence.Conciseness: Concise articles have a focus.
Sen-tences contain information that is related to themain theme of the article.
The same or similarinformation is tried to be not repeated.3.3 InformativenessAs a main reason for reading online news is toremain well-informed (Tang et al, 2003), infor-mativeness of articles have an effect on their per-1895QualityReadabilityStyleTopicSentimentInformativenessFluencyConcisenessDescriptivenessNoveltyCompletenessReferencingFormalityRichnessAttractivenessTechnicalityPopularitySubjectivitySentimentalityPolarityFigure 1: A taxonomy of the identified aspects.ceived quality.
In our model, we consider four dif-ferent aspects related to informativeness: descrip-tiveness, novelty, completeness, and referencing.Descriptiveness: Descriptiveness indicates howwell the title of an article reflects its main bodycontent.
Titles with low descriptiveness are oftenclick baits (e.g., ?You won?t believe what you willsee?).
Such titles may lead to dissatisfaction, asthe provided news content usually does not meetthe raised user expectation.Novelty: Novel articles provide new and valuableinformation to the readers.
The provided informa-tion is unlikely to be known to an average reader.Completeness: Complete articles cover the topicin an adequate level of detail (Louis and Nenkova,2014; Bouayad-Agha et al, 2012).
A reader cansatisfy her information need after reading such anarticle.Referencing: Referencing is about the degree towhich the article references external sources (in-cluding other people?s opinions and related arti-cles).
Providing references allows the reader toaccess related information sources easily, (Gamon,2006; van Dijk and Kintsch, 1983).3.4 StyleThe language and aesthetics is also related to thearticle quality (McNamara et al, 2009; Ashok etal., 2013; Pavlick and Tetreault, 2016; Peterson etal., 2011).
We consider three style-related aspects:formality, richness, and attractiveness.Formality: Formal articles are written by follow-ing certain writing guidelines.
They are morelikely to contain formal words and obey punctu-ation/grammar rules(Peterson et al, 2011).Richness: The vocabulary of rich articles is per-ceived as diverse and interesting by the readers.Rich articles are not written in a plain and straight-forward manner.Attractiveness: Attractiveness measures the de-gree to which the title of an article raises curios-ity in its readers.
Attractive titles entice people tocontinue reading the main content of the article.3.5 TopicEditors consider the nature of the article with re-spect to its target audience, i.e., according to thetarget audience (technical or popular) the other as-pects may play a different role.
We investigate twotopic-related aspects: technicality and popularity.Technicality: Technical articles (Louis andNenkova, 2013) usually require some effort to un-derstand as well as previous knowledge on thetopic.
Examples of usually technical news topicsinclude science and finance.Popularity: The popularity refers to the size ofthe audience who would be interested in the topicof the article (Bandari et al, 2012; Arapakis et al,2014b).
For example, while many readers are in-terested in reading about celebrities, few readersare interested in articles about anthropology.3.6 SentimentFinally, we consider the sentiments expressed inan article.
Besides opinion articles (which are sub-jective by nature), many news may also convey aparticular emotion.
We evaluate three sentiment-related aspects: subjectivity, sentimentality, andpolarity.Subjectivity: Subjective articles tend to containopinions, preferences, or possibilities.
There arerelatively few factual statements.Sentimentality: Sentimentality is a measure ofthe total magnitude of positive or negative state-ments made in the article regarding an object oran event.
Highly sentimental articles include rela-tively few neutral statements.Polarity: Polarity indicates the overall sign of thesentiments expressed in the article (Arapakis et al,2014a).
Articles with positive (negative) polarityinclude relatively more statements with positive(negative) sentiment.4 Corpus: Editorial Quality ControlOur goal is to identify proxies of news article qual-ity that can be learned and predicted in an auto-matic and scalable manner.
To identify these prox-ies, we rely on the domain knowledge and humanintuition of expert judges, whom we employ in arigorous, crowdsourcing-based evaluation for gen-1896erating a ground-truth dataset.
Through an edito-rial study we create an in-domain, annotated newscorpus that allows us to learn predictive modelswhich can estimate accurately the perceived qual-ity of news articles.4.1 Online News ArticlesOur analysis was conducted on a dataset consist-ing of 13, 319 news articles taken from a majornews portal2.
We opted for a single news por-tal to be able to extract features that are consis-tent across all news articles.
The dataset was con-structed by crawling news articles over a period oftwo weeks.
During the crawling period, we con-nected to the RSS news feed of the portal every15 minutes and fetched newly published articleswritten in English.
The content of the discoveredarticles was then downloaded from the portal.Each article is identified by its unique URI andstored in a database, along with some meta-data,such as article?s genre, its publication date, and itsHTML content.
We applied further filtering on theinitial set of 13,319 news articles.
The word countdistribution of the articles followed a bimodal pat-tern, with the bulk of the articles located around amean value of 447.5.
Using this value as a refer-ence point, we removed articles that contain lessthan 150 or more than 800 words.
We then sam-pled a smaller set of articles such that each of themost frequent 15 genres have at least 65 articlesin the sample.
This left us with 1,043 new arti-cles, out of which a randomly selected set of 561articles were used in the editorial study.The selected news articles were preprocessedbefore the editorial study.
The preprocessingwas performed in two steps.
First, we removedthe boilerplate of HTML pages and extracted themain body text of news articles, using Boiler-pipe (Kohlsch?utter et al, 2010).
Second, we seg-mented the body text into sentences and para-graphs.
For sentence segmentation, we used theStanford CoreNLP library, which includes a prob-abilistic parser (Klein and Manning, 2003; Mihal-cea and Csomai, 2007).
For each news article wegenerated a body- and sentence- level annotationform (see example in the supplementary notes).4.2 Annotations of Editorial Quality AspectsFor our editorial study, we employed ten expertjudges (male = 4, female = 6) who had a back-2Yahoo!
News at http://www.yahoo.com/news.62.1% 31.1% 3.2%3.4%0% 20% 40% 60% 80% 100%FluencyConcisenessDescriptivenessNoveltyCompletenessReferencingFormalityRichnessAttractivenessTechnicalityPopularitySubjectivityPositivityNegativityTotalFull agreement 1-level disag.
2-level disag.
3-level disag.Figure 2: Annotators agreement.ground in computational linguistics, journalism,or were media monitoring experts.
The expertjudges were either native English speakers or wereproficient with the English language.
The expertjudges assessed a total of 561 news articles on 15measures (14 aspects and the main quality mea-sure), using a 5-point Likert scale, where low andhigh scores suggest weak or strong presence of theassessed measure, respectively.The annotation took place remotely, and eachexpert judge could annotate up to ten news arti-cles per day (this threshold was set to ensure ahigh quality of annotation), and each article wasannotated by one expert judge and by one of theauthors of this paper.
Prior to that, there was a pi-lot session were each expert judge was asked tobecome familiar with the quality criteria and an-notate three trial news articles.
Next, a meeting(physical or online) was arranged and the authorsdiscussed with the expert judge the rationale be-hind assigning the scores, and appropriate correc-tions and recommendations were made.
This stepensured that we had disambiguated any questionsprior to the editorial study and also assured that ex-pert judges followed the same scoring procedure.The compensation for annotating was 10eper ar-ticle.
The annotated corpus is publicly available.3Fig.
2 illustrates the details of the overall an-notations agreement.
We can see that annotationsagree on 62.1% of the articles, on 65.5% they vary3http://novasearch.org/datasets/.1897only 1-point and in 96.6% they vary 2 points inthe 5-point Likert-scale.
These results are quitesatisfying and show a good level of agreement andconsistency across all aspects.4.3 Corpus StatisticsTable 1 shows the mean (M) and standard devi-ation (SD) values for five different distributions(number of characters, words, unique words, en-tities, and sentences) and four different subsets ofthe corpus.
The subsets contain all articles, high-quality articles (labels 4 and 5), medium-qualityarticles (label 3), or low-quality articles (labels 1and 2).
The last three subsets contain 84, 298,and 179 news articles, respectively.
According tothese numbers, the article quality follows an un-balanced distribution: about half of the articles arelabeled as medium quality, and there are about twotimes more low-quality articles than high-qualityarticles.
According to Table 1, there is a cleardifference between distributions for the high- andlow- quality articles.
In general, we observe thathigher-quality articles are relatively longer (e.g.,more words or sentences), on average.5 Aspects Correlation AnalysisTo identify which aspects of a news article arebetter discriminants of its quality, we perform acorrelation analysis.
Given that we are lookingat ordinal data that violates parametric assump-tions, we compute the Spearman?s rank correlationcoefficients (rs) between the aspects?
scores andthe news article quality that we acquired from ourground truth.
The motivation behind this analysisis to get a first intuition into the aspects?
effective-ness to act as quality predictors, by understandinghow they are associated to news article quality.In Table 2, we report several statistically sig-nificant correlations between the different aspects.Given that our correlation analysis involves multi-ple pairwise comparisons, we need to correct thelevel of significance for each test such that theoverall Type I error rate (?)
across all comparisonsremains at .05.
Given that the Bonferroni correc-tion is too conservative in the Type I error rate, weopt for the more liberal criterion proposed by Ben-jamini and Hochberg (Benjamini and Hochberg,1995; Benjamini and Hochberg, 2000) and com-pute the critical p-value for every pairwise com-parisons aspcrit=jk?, (1)where j is the index of all pairwise comparison p-values, listed in an ascending order, and k is thenumber of comparisons.
If we consider Cohen?sconventions for the interpretation of effect size,we observe that most of the correlation coefficientsshown in Table 2 represent sizeable effects, whichrange from small (?.1) to large (?.5).
For exam-ple, completeness is highly correlated with quality(rs= .70) while polarity is the least correlatedwith quality (rs= .05).
In addition, Table 2 doesnot provide any evidence of multicollinearity sincenone of the aspects (with the exception of quality)are significantly highly correlated (rs> .80).6 Predicting Editorial Quality6.1 Predicting EQ with the AspectsIn this section, we demonstrate the predictive char-acteristics of the proposed aspects (Section 3) withrespect to news article quality.
We formulate theprediction problem as a regression problem, andconduct a 10-fold cross validation to estimate theregression model.
For our regression task we usea Generalised Linear Model (GLM) via penalizedmaximum likelihood (Friedman et al, 2010).
Theregularisation path is computed for the lasso orelasticnet penalty at a grid of values for the reg-ularisation parameter lambda.
The GLM solves thefollowing problemmin?0,?1NN?i=1wil(yi, ?0+?Txi)+?[(1??)???222+???
?1, ],(2)over a grid of values of ?
covering the entire range.Here l(y, ?)
is the negative log-likelihood contri-bution for observation i.
The elastic-net penalty iscontrolled by ?, and bridges the gap between lasso(?
= 1, the default) and ridge (?
= 0).
The tun-ing parameter ?
controls the overall strength of thepenalty.
It is known that the ridge penalty shrinksthe coefficients of correlated predictors towardseach other while the lasso tends to pick one ofthem and discard the others, which makes it morerobust against predictor collinearity and overfit-ting.
We used the values that minimise RMSE,i.e., ?
= 0.95 and ?
= 0.01.In Table 3, we see the coefficients of the fi-nal GLM model which are to be interpreted in thesame manner as a Cox model.
A positive regres-sion coefficient for an explanatory variable meansthat the variable is associated with a higher risk ofan event.
In our case, all coefficients are positive,being completeness, fluency and richness the ones1898Table 1: Statistics for the annotated news corpus (M ?
SD values)All High quality Medium quality Low qualityCharacters 2490.28 ?
1900.95 4321.92 ?
2258.51 2698.94 ?
1641.85 1290.07 ?
1166.65Words 413.03 ?
318.63 717.87 ?
386.08 447.46 ?
274.88 213.76 ?
193.12Unique words 167.29 ?
110.25 269.08 ?
122.14 180.27 ?
95.72 98.29 ?
77.09Entities 18.45 ?
14.43 23.85 ?
15.09 19.43 ?
11.03 14.29 ?
17.59Sentences 20.67 ?
17.89 35.27 ?
24.92 21.76 ?
14.02 12.03 ?
14.42Table 2: Correlations between different aspects in the ground-truth dataConc.
Desc.
Nov. Comp.
Ref.
Form.
Rich.
Attr.
Tech.
Pop.
Subj.
Sent.
Pol.
Qual.Fluency .61??.38??.34??.57??.37??.40??.53??.41??.15??.27??.12??.11?
?.03 .66?
?Conciseness .33??.32??.38??.28??.41??.39??.30??.24??.25??
?.00 .08 .01 .47?
?Descriptiveness .18??.32??.23??.23??.19??.13??.13??.17?
?.00?.09 .00 .37?
?Novelty .39??.40??.44??.35??.37??.16??.32?
?.05 .25??
?.05 .41?
?Completeness .48??.38??.51??.39??.30??.26??.18??.20?
?.03 .70?
?References .51??.35??.33??.30??.29??.27??.44?
?.00 .52?
?Formality .43??.30??.46??.25?
?.01 .31??
?.08 .47?
?Richness .50??.25??.35??.24??.15?
?.04 .63?
?Attractiveness .15??.55??.28??.23??
?.02 .52?
?Technicality .22??.16??.22??.11?.30?
?Popularity .28??.24?
?.02 .41?
?Subjectiveness .42??.11?.23?
?Sentimemtality ?.13?.27?
?Polarity .05Significance levels (two-tailed) are as follows:?
:< .01;??
:< .001.Table 3: The coefficients of the final GLM model.The intercept value is 2.9103.Group Aspects CoefficientsReadability Fluency .1730Conciseness .0372Informativeness Completeness .2062Descriptiveness .0723Referencing .0343Novelty -Style Richness .1192Formality .0602Attractiveness .0515Topic Popularity .0578Technicality .0047Sentiment Subjectivity -Polarity -Sentimentality -showing a higher relation to the overall editorialquality.Next, we replicate our regression experimentsfor the GLM regression model, but this time weapply a leave-one-aspect-out method, to examinethe relative importance of each aspect in explain-ing our predicted variable, i.e., the news articlequality.
To this end, we evaluate the 14 regres-sion models, each one with out one of the aspects.The goal is to verify how prediction is affected byeach individual quality aspect.Table 4: Average performance across all ten foldsfor the GL model and for different feature sets.Group Aspects RMSE RRSEAll groups All aspects .3984 -Readability w/o Fluency .4158 -4.36%w/o Conciseness .3984 .00%Informative.
w/o Completeness .4233 -6.25%w/o Referencing .4000 -.40%w/o Descriptiveness .3999 -.37%w/o Novelty .3981 -.07%Style w/o Richness .4081 -2.43%w/o Attractiveness .4009 -.62%w/o Formality .3990 -.15%Topic w/o Popularity .4003 -.47%w/o Technicality .3976 .20%Sentiment w/o Subjectivity .3974 .25%w/o Polarity .3984 -.10%w/o Sentimentality .3983 .02%To compare the performance of our GLM re-gression model against the baseline method (withall quality aspects), we compute the Root MeanSquared Error (RMSE), given byRMSE =??Ni=1(y?
?
yi)2N(3)where y?
is the sample mean and yiis the i-th es-timate.
However, while regression results give anidea of the prediction quality of the models they do1899not quantify the size of the difference of their per-formance.
We, therefore, also compute the RootRelative Squared Error (RRSE) metric as it pro-vides a good indication of any relative improve-ment over the baseline methods, given byRRSE = 1?RMSEGLMRMSEBaseline.
(4)Table 4 shows the RMSE and RRSE, with respectto the GLM regression model trained on all the fea-tures.
These results show that completeness, flu-ency and richness are the aspects that most affectRMSE when they are missing from the full model.6.2 Automatic Prediction of EQWe examined a baseline model (BaselineM)that always predicts the mean value and a base-line GLM model (BaselineShallow) trainedon shallow features, to automatically predictthe editorial quality.
Shallow or lexical fea-tures are commonly used in traditional read-ability metrics, which are based on the analy-sis of superficial text properties.
Flesh-KincaidGrade Level (Flesch, 1979; Franc?ois and Fairon,2012), SMOG (McLaughlin, 1969), and Gun-ning Fog (Gunning, 1952) are some examples ofreadability metrics.
The simplicity of these fea-tures makes them an attractive solution comparedto computationally more expensive features, suchas syntactic (Feng et al, 2010).
However, asShriver (Schriver, 1989) points out, the readabil-ity metrics can be useful when used as gross indexof readability.
For our baseline, we consider theFlesh Kincaid, Coleman Liau, ARI, RIX, GunningFog, SMOG, LIX features.In Table 5, we report the average performanceof the GLM regression model, BaselineM, andBaselineShallow across all folds.
We notethat our GLM regression model improves theRMSE by at least 40%, compared to both base-lines.Finally, as a reference for future research withthe proposed corpus, we trained GLM regressionmodels to predict each aspect individually.
Table 6presents the RMSE for each aspect, for two differ-ent sets of feature: a standard BoW and the shal-low features described previously, as well as theBaselineM.
Despite the simplicity of the fea-tures, we can see that the aspects can be inferredfrom the articles.
In particular, the model trainedon the BoW features achieves an RMSE that isvery close to that of the BaselineM, whereas theTable 5: Average performance across allten folds for the GLM, BaselineM andBaselineShallow.Method RMSE RRSEBaselineM 0.7048 43.47%BaselineShallow 0.8937 55.41%GLM 0.3984 -Table 6: Average performance across all ten foldsfor the GL model and for different feature sets.Aspects BoW Shallow BaselineMFluency 1.1571 1.1181 1.1462Conciseness 1.2622 1.1968 1.2456Completeness .8408 .7945 .8130Referencing .7047 .6613 .7048Descriptiveness .9260 .8730 .9073Novelty .7994 .7607 .7797Richness .9866 .9454 .9568Attractiveness .7048 .6702 .6907Formality .7025 .6691 .6920Popularity .8329 .7825 .8250Technicality .7923 .7409 .7907Subjectivity .8750 .8283 .9094Polarity .8109 .7780 .8009Sentimentality .8170 .7668 .8046model trained on the shallow features outperformsall other models.7 ConclusionsIn this paper, we proposed an annotated corpusfor controlling the editorial quality of online newsthrough 14 aspects related to editors perceivedquality of news articles.
To this end, we performedan editorial study with expert judges either in com-putational linguistics, journalism, or media mon-itoring experts.
The judges assessed a total of561 news articles with respect to 14 aspects.
Thestudy produced valuable insights.
One importantfinding was that high quality articles share a sig-nificant amount of variability with several of theproposed aspects, which supports the claim thatthe proposed aspects may characterise news arti-cle quality in an automatic and scalable way.
An-other finding was that fluency, completeness andrichness are the aspects that best correlate withquality, while technicality, subjectivity and polar-ity aspects show a poor correlation with quality.This shows that the text comprehension and writ-ing style are aspects that are more relevant thansentiment.
Later, we showed that using the entire1900set of 14 aspects we could predict the text qualitywith an RMSE of only 0.400 in a 5-point Likert-scale.
This renders a very effective decomposi-tion of news article quality into the 14 aspects.
Asfuture work, we plan to investigate other linguis-tic representations that can improve the automatedextraction of the proposed aspects to better predictthe article?s perceived quality.AcknowledgmentsThe first three authors were supported by MUL-TISENSOR project, partially funded by the Eu-ropean Commission, under the contract numberFP7-610411.
The fourth author was partially sup-ported by FCT/MCTES through the project NOVALINCS Ref.
UID/CEC/04516/2013.ReferencesEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-agirre, and Weiwei Guo.
2013. sem 2013 sharedtask: Semantic textual similarity, including a piloton typed-similarity.
In In *SEM 2013: The SecondJoint Conference on Lexical and Computational Se-mantics.
Association for Computational Linguistics.Ioannis Arapakis, B.Barla Cambazoglu, and MouniaLalmas.
2014a.
On the feasibility of predictingnews popularity at cold start.
In LucaMaria Aielloand Daniel McFarland, editors, Social Informatics,volume 8851 of Lecture Notes in Computer Science,pages 290?299.
Springer International Publishing.Ioannis Arapakis, Mounia Lalmas, Berkant Barla Cam-bazoglu, Mari-Carmen Marcos, and Joemon M.Jose.
2014b.
User engagement in online news: Un-der the scope of sentiment, interest, affect, and gaze.JASIST, 65(10):1988?2005.Vikas Ganjigunte Ashok, Song Feng, and Yejin Choi.2013.
Success with style: Using writing style topredict the success of novels.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing.Alexandra Balahur, Ralf Steinberger, Mijail Kabad-jov, Vanni Zavarella, Erik van der Goot, MatinaHalkia, Bruno Pouliquen, and Jenya Belyaeva.2010.
Sentiment analysis in the news.
In Nico-letta Calzolari (Conference Chair), Khalid Choukri,Bente Maegaard, Joseph Mariani, Jan Odijk, SteliosPiperidis, Mike Rosner, and Daniel Tapias, editors,Proceedings of the Seventh International Confer-ence on Language Resources and Evaluation, Val-letta, Malta, may.
European Language ResourcesAssociation (ELRA).Roja Bandari, Sitaram Asur, and Bernardo A Huber-man.
2012.
The pulse of news in social media:Forecasting popularity.
In ICWSM, pages 26?33.Y.
Benjamini and Y. Hochberg.
1995.
Controlling thefalse discovery rate - a practical and powerful ap-proach to multiple testing.
Journal of the Royal Sta-tistical Society, Series B, 57(1):289?300.Y.
Benjamini and Y. Hochberg.
2000.
On the adaptivecontrol of the false discovery fate in multiple testingwith independent statistics.
Journal of Educationaland Behavioral Statistics, 25(1):60?83.Nadjet Bouayad-Agha, Gerard Casamayor, SimonMille, and Leo Wanner.
2012.
Perspective-orientedgeneration of football match summaries: Old tasks,new challenges.
ACM Transactions on Speech andLanguage Processing (TSLP), 9(2):3.Hoa Trang Dang.
2005.
Overview of duc 2005.
InProceedings of the document understanding confer-ence, volume 2005, pages 1?12.Lijun Feng, Martin Jansche, Matt Huenerfauth, andNo?emie Elhadad.
2010.
A Comparison of Featuresfor Automatic Readability Assessment.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics: Posters (COLING), COL-ING ?10, pages 276?284, Stroudsburg, PA, USA.Association for Computational Linguistics.Rudolf Franz Flesch.
1979.
How to write plain En-glish: A book for lawyers and consumers.
Harper-collins.Thomas Franc?ois and C?edrick Fairon.
2012.
AnAI readability formula for French as a foreign lan-guage.
In Proceedings of the Conference on Empir-ical Methods in Natural Language (EMNLP), pages466?477.
Association for Computational Linguis-tics.Jerome Friedman, Trevor Hastie, and Rob Tibshirani.2010.
Regularization paths for generalized linearmodels via coordinate descent.
Journal of statisti-cal software, 33(1):1.Michael Gamon.
2006.
Graph-based text represen-tation for novelty detection.
In Proceedings of theFirst Workshop on Graph Based Methods for Natu-ral Language Processing, pages 17?24.
Associationfor Computational Linguistics.Jianfeng Gao, Patrick Pantel, Michael Gamon, Xi-aodong He, Li Deng, and Yelong Shen.
2014.
Mod-eling interestingness with deep neural networks.
InProceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing.Namrata Godbole, Manjunath Srinivasaiah, and StevenSkiena.
2007.
Large-scale sentiment analysis fornews and blogs.
In Proceedings of the InternationalConference on Weblogs and Social Media.Robert Gunning.
1952.
The Technique of Clear Writ-ing.
McGraw-Hill.1901Dan Klein and Christopher D. Manning.
2003.
Fastexact inference with a factored model for naturallanguage parsing.
In Advances in Neural Informa-tion Processing Systems, volume 15.
MIT Press.Christian Kohlsch?utter, Peter Fankhauser, and Wolf-gang Nejdl.
2010.
Boilerplate detection using shal-low text features.
In Proceedings of the Third ACMInternational Conference on Web Search and DataMining, WSDM ?10, pages 441?450, New York,NY, USA.
ACM.Annie Louis and Ani Nenkova.
2011.
Text specificityand impact on quality of news summaries.
In Pro-ceedings of the Workshop on Monolingual Text-To-Text Generation, pages 34?42.
Association for Com-putational Linguistics.Annie Louis and Ani Nenkova.
2013.
A corpus of sci-ence journalism for analyzing writing quality.
Dia-logue & Discourse, 4(2):87?117.Annie Louis and Ani Nenkova.
2014.
Verbose, laconicor just right: A simple computational model of con-tent appropriateness under length constraints.
pages636?644.G Harry McLaughlin.
1969.
SMOG grading: A newreadability formula.
Journal of reading, JSTOR,12(8):639?646.Danielle S McNamara, Scott A Crossley, and Philip MMcCarthy.
2009.
Linguistic features of writingquality.
Written Communication.Rada Mihalcea and Andras Csomai.
2007.
Wikify!
:Linking documents to encyclopedic knowledge.
InProceedings of the Sixteenth ACM Conference onConference on Information and Knowledge Man-agement, CIKM ?07, pages 233?242, New York,NY, USA.
ACM.Ani Nenkova, Jieun Chae, Annie Louis, and EmilyPitler.
2010.
Structural Features for Predicting theLinguistic Quality of Text.
Proceddings of the Em-pirical Methods in Natural Language Generation(EMNLP), 5790:222?241.Ellie Pavlick and Joel Tetreault.
2016.
An empiri-cal analysis of formality in online communication.Transactions of the Association for ComputationalLinguistics, 4:61?74.Kelly Peterson, Matt Hohensee, and Fei Xia.
2011.Email formality in the workplace: A case study onthe enron corpus.
In Proceedings of the Workshopon Languages in Social Media, pages 86?95.
Asso-ciation for Computational Linguistics.Owen Phelan, Kevin McCarthy, and Barry Smyth.2009.
Using twitter to recommend real-time topicalnews.
In Proceedings of the third ACM conferenceon Recommender systems, pages 385?388.
ACM.Emily Pitler and Ani Nenkova.
2008.
Revisitingreadability: A unified framework for predicting textquality.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?08.Janice Redish.
2000.
Readability formulas have evenmore limitations than Klare discusses.
ACM Journalof Computer Documentation (JCD), 24(3):132?137.Jack C Richards and Richard W Schmidt.
2013.
Long-man dictionary of language teaching and appliedlinguistics, volume 78.
Routledge.Karen A Schriver.
1989.
Evaluating text qual-ity: The continuum from text-focused to reader-focused methods.
Professional Communication,IEEE Transactions on, 32(4):238?255.Yohei Seki, Koji Eguchi, Noriko Kando, and MasakiAono.
2006.
Opinion-focused summarization andits analysis at duc 2006.
In Proceedings of the Docu-ment Understanding Conference (DUC), pages 122?130.Anna Shtok, Gideon Dror, Yoelle Maarek, and IdanSzpektor.
2012.
Learning from the past: answeringnew questions with past answers.
In Proceedingsof the 21st international conference on World WideWeb, pages 759?768.
ACM.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2008.
Learning to rank answers on largeonline qa collections.
In ACL, volume 8, pages 719?727.Rong Tang, Kwong Bor Ng, Tomek Strzalkowski, andPaul B Kantor.
2003.
Automatically predicting in-formation quality in news documents.
In Proceed-ings of the 2003 Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics on Human Language Technology: com-panion volume of the Proceedings of HLT-NAACL2003?short papers-Volume 2, pages 97?99.
Associ-ation for Computational Linguistics.Teun A. van Dijk and W. Kintsch.
1983.
Strategiesof discourse comprehension.
New York: AcademicPress.Xin Yan, Dawei Song, and Xue Li.
2006.
Concept-based document readability in domain specific in-formation retrieval.
In Proceedings of the 15th ACMConference on Information and Knowledge Man-agement (CIKM), pages 540?549.
ACM.Mostafa Zamanian and Pooneh Heydari.
2012.
Read-ability of texts: State of the art.
Theory and Practicein Language Studies, 2(1):43?53.1902
