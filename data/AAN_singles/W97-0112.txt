Knowledge Acqulsit ion=Classif ication of Terms in a thesaurus from~a CorpusSTA Jean-DavidEDF, Direction des Etudes et des Recherches1, av.
de G~n4ral de Gaulle92140 Clamart - France(33) 1 47.65.49.58 E-mail : jd.sta@der.edfgdf.frAbstract!Faced with growing volume and accessibi l i ty of electronic textualinformation, information retrieval, and, in general, automaticdocumentat ion require updated terminological resources that are evermore voluminous.
A current problem is the automated construction ofthese resources (e.g., terminologies, thesauri, glossaries, etd~ ~) froma corpus.
Various linguistic and statistical methods to handle thisproblem are coming to light.
One problem that has been less studied isthat of updating these resources, in particular, of classifying a termextracted from a corpus in a subject field, discipl ine or branch of anexist ing thesaurus.
This is the first step in posit ioning a termextracted from a corpus in the structure of a thesaurus (genericrelations, synonymy relations ..).
This is an important problem incertain disciplines in which knowledge, and, in particular, vocabularyis not very stable over time, especial ly because of neologisms.This experiment compares different models for representing a termfrom a corpus for its automatic classif ication in the subject fieldsof a thesaurus.
The classif ication method used is lineardiscr iminatory analysis, based on a learning sample.
The modelsevaluated here are: a term/document model where each term is a vectorin document vector space, two term/term models where each term is avector in term space, and the coordinates are either the co-occurrence, or the mutual information between terms.
The mosteffective model is the one based on mutual information between terms,which typifies the fact that two terms often appear together in thecorpus, but rarely apar t .
.I.
IntroductionIn documentation, terminologies, thesauri and otherterminological lists are reference systems which can be used formanual or automatic indexing.
Indexing consists of recognising theterms in a text that belong to a reference system; this is calledcontrol led indexing.
The quality of the result of the indexing processdepends in large part on the quality of the terminology (completeness,consistence ..).
Thus, applications downstream from the indexing dependon these terminological resources.101,IThe most thoroughly studied application is the informationretr ieval (IR).
Here, the term provides a means for accessinginformation through its standardising effect on the query and on thetext to be found.
The term can also be a variable that is used instatist ical c lassif icat ion or clustering processes of documents (\[BLO92\] and \[STA 95a\]), or in selective dissemination of information, inwhich it is used to bring together a document to be disseminated andits target \[STA 93\].Textual information is becoming more and more accessible inelectronic form.
This accessibi l i ty is certainly one of theprerequis i tes for the massive use of natural language process ing (NLP)techniques.
These techniques applied on part icular domains, often useterminological resources that supplement the lexical resources.
Thelexical resources (general language dictionaries) are fairly stable,whereas terminologies evolve dynamical ly with the fields theydescribe.
In particular, the discipl ines of information processing(computers, etc.)
and biology or genetics are characterised today byan extraordinary terminological activity.Unfortunately, the abundance of electronic corpora and therelat ive matur i ty  of natural language processing techniques haveinduced a shortage of updated terminological data.
The various effortsin automatic acquis i t ion of terminologies from a corpus stem from thisobservation, and try to answer the following question: "How cancandidate terms be extracted from a corpus?
"Another inKoortant question is how to posit ion a term in anexist ing thesaurus.
That question can itself be subdivided intoseveral questions that concern the role of the standard relationshipsin a thesaurus: synonymy, hyperonymy, etc.
The question studied inthis experiment concerns the posit ioning or classif icat ion of a termin a subject f ield or semantic field of a thesaurus.
This is the firststep in a precise posit ioning using the standard relat ionships of athesaurus.
This prob lem is very diff icult for a human being to resolvewhen he is not an expert in the field to which the term belongs andone can hope that an automated classif ication process would be ofgreat help.To classify a term in a subject field can be considered similarto word sense disambiguation (WSD) which consists in classifying aword in a conceptual class (one of its senses).
The difference isthat, in a corpus, a term is general ly monosemous and a word ispolysemous.
Word sense disambiguation uses a single context (generallya window of a few words around the word to be disambiguated) as inputto predict  its sense among a few possible senses (generally less thanten).
Term subject field discr imination uses a representat ion of theterm calculated on the whole corpus in order to classify it into about330 subject fields in this experiment.The experiment described here was used to evaluate differentmethods for c lassi fying terms from a corpus in the subject fields of athesaurus.
After a brief description of the corpus and the thesaurus,automatic indexing and terminology extraction are described.Linguist ic and statistical techniques are used to extract a candidateterm from a corpus or to recognise a term in a document.
Thispreparatory processing allows the document to be represented as a set102of terms (candidate terms and key words A c lass i f i cat ion method isthen implemented to c lass i fy  a subset of 1,000 terms in the 49 themesand 330 semantic fields that make up the thesaurus.
The 1,000 termsthus c lass i f ied comprise the test sample that is used to evaluatethree models  for represent ing terms.IX.
Data PreparationIX.i.
Descript ion of the CorpusThe corpus studied is a set of I0,000 scient i f ic  and technicaldoc~unents in French (4,150,000 words).
Each document consists of oneor two pages of text.
This corpus descr ibes research carr ied out bythe research d iv is ion of EDF, the French e lectr ic i ty  company.
Manyd iverse subjects are dealt with: nuclear  energy, thermal energy, homeautomation, sociology, art i f ic ia l  intel l igence, etc.
Each documentdescr ibes the object ives and stages of a research pro ject  on apar t icu lar  subject.
These documents are used to p lan EDF researchactivity.Thus, the vocabulary  used is either very technical, w i th  subjectf ie ld terms and candidate terms, or very general, w i th  sty l ist icexpressions, etc.|Obj ectif  : .... ,IObj ecfifConst ruct ion  de thesaurus~Etatd 'avancement  ~- -~ genera/.
.
.
.
, _ ~ ~'-qphase d~ndus~/a/isafion expressions ~an a avancemen~ : - 2~-~La phase d' industr ia l isat ion /... ~ Iconstrucfion de the~z , .
.
. "
\[ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
auru_L indexat lon automat lque .
.
.
.
.
.
.
.-imqxexauon automauouektermsA document with terms and general expressionsII.2.
Descript ion of the ThesaurusThe EDF thesaurus consists of 20,000 terms ( including 6,000synonyms) that cover a wide var ie ty  of fields (statistics, nuclearpower  plants, informat ion retrieval, etc.).
This reference system wascreated manual ly  from corporate documents, and was va l idated w i th  thehelp of many experts.
Currently, updates are handled by a group ofdocumental is ts  who regular ly  examine and insert new terms.
One of thesources of new terms is the corpora.
A l inguist ic  and stat ist icalextractor  proposes candidate terms for va l idat ion  by thedocumental ists .
A f ter  val idation, the documental ists  must pos i t ion  these lected terms in the thesaurus.
It's a di f f icult  exerc ise because ofthe wide var ie ty  of fields.103The thesaurus is composed of 330 semantic (or subject) fieldsincluded in 49 themes such as mathematics, sociology, etc.'
ith6ode des erreurs \[I analyse discriminante I \]statistique \[ , s .ttmation Il analyse statistique\[:modUle statistique lineaire !l analyse de la variance ~ Generic Relationshipi'statistique n'on param6trique \]See AlsoRelationshipExtract from the "statistics" sm-~t ic  f ield from the EDF thesaurusThis example gives an overview of the various relations betweenterms.
Each term belongs to a single semantic field.
Each term isl inked to other terms through a generic relat ion (arrow) or aneighbourhood relation (line).
Other relations (e.g., synonym,translated by, etc.)
exist, but are not shown in this example.II?3.
Document IndexingAs a first step, the set of documents in the corpus is indexed.This consists of producing two types of indexes: candidate terms, anddescriptors.
The candidate terms are expressions that may becometerms, and are submitted to an expert for validation.
Descriptors areterms from the EDF thesaurus that are automatical ly recognised in thedocuments.II.3.1.
Terminological  Fi l ter ingIn this experiment, terminological f i l tering is used for eachdocument to produce terms that do not belong to the thesaurus, butwhich nonetheless might be useful to describe the documents.
Moreover,these expressions are candidate terms that are submitted to experts ordocumental ists for validation.L inguist ic and statistical terminological f i ltering are used.
Themethod chosen for this experiment combines an initial l inguist icextraction with statistical f i ltering \[STA 95b\].104Linguistic ExtractionGeneral ly,  it appears that the syntact ical  s t ructure  of a term inF rench  language is the noun phrase.
For example, in the EDF thesaurus,the syntact ic  structures of terms are d is t r ibuted as fol lows:syntact ic  structureNoun Ad~ect iveNoun Prepos i t ion  NounNounProper  nounNoun Prepos i t ion  Art ic leNounNoun Prepos i t ion  NounAd ject iveNoun Part ic ipeNoun Nounexample~rosion f luvia leanalyse de contenud~centra l i sat ionChinonassurance de laqual i t4unit4 de bandemagn4t iquepuissance absorb4eacc~s m4moire%2S .i24.418.16.83.2Distriknation of the syntact ic structures of terms2.82.22.1Thus, term extract ion is in i t ia l ly  syntact ical .
It cons ists  ofapp ly ing  seven recurs ive syntact ic patterns to the corpus \[OGO 94\].NP <- ADJECTIVE NPNP<-  NPADJECT IVENP <- NP ~ NPNP <- NP de NPNP <- NP en NPNP <- NP pour  NPNP <- NPNPThe seven syntact ic  patterns for terminology extract ionStatistical FilteringL inguist ic  extraction, however, is not enough.
In fact, manyexpress ions  wi th  a noun phrase structure are not terms.
This includesgenera l  expressions,  styl ist ic effects, etc.
Stat ist ica l  methods  canthus be used, in a second step, to d iscr iminate terms from non-termino log ica l  expressions.
Three indicators are used  here:Frequency:  This is based on the fact that the more of ten anexpress ion  is found in the corpus, the more l ike ly  it is to be aterm.
This statement must be kept in proportion, however.
Indeed, itseems that a small  number of words (usually, very  general  uniterms)are very  frequent, but are not terms.- Var iance:  This is based on the idea that the more the occurrences ina document  of an expression are scattered, the more l ike ly  it is tobe a term.
This is the most ef fect ive indicator.
Its d rawback  isthat it also h ighl ights  large noun phrases in wh ich  the terms areincluded.Local  dens i ty  \[STA 95b\]: This is based on the idea that the c losertogether  the documents  are that contain the expression, the morel i ke ly  it is to be a term.
The local densi ty  of an express ion  is the105mean of the cosines between documents which contain the givenexpression.
A document is a vector in the Document Vector  Spacewhere a dimension is a term.
This indicator highlights a certainnumber of terms that are not transverse to the corpus, but ratherconcentrated in documents that are close to each other.
Nonetheless,this is not a very effective indicator for terms that are transverseto  the corpus.
For example, terms from computer science, which arefound in a lot of documents, are not highl ighted by this indicator.Results of the Tez~inological ExtractionDuring this experiment, the terminological extraction u l t imatelyproduced 3,000 new terms that did not belong to the thesaurus.
Thesenew' terms are used in the various representat ion models descr ibedbelow.
The initial l inguistic extracting produced about 50,000expressions.II.3.2.
Controlled IndexingA supplementary way of characterising a document's contents is byrecognising control led terms in the document that belong to athesaurus.
To do this, an NLP technique is used \[BLO 92\].
Eachsentence is processed on three levels: morphologically, syntactically,and semantically.
These steps use a grammar and a general languagedictionary.The method consists of breaking down the text fragment beingprocessed by a series of successive transformations that may besyntactical (nominalisation, de-coordination, etc.
), semantic (e.g.,nuclear and atomic), or pragmatic (the thesaurus" synonymrelationships are scanned to transform a synonym by its main form).
Atthe end of these transformations, the decomposed text is compared tothe list of documented terms of the thesaurus in order to supply thedescriptors.Results of the Controlled Iz~exingControl led indexing of the corpus supplied 4,000 terms (of 20,000in the thesaurus).
Each document was indexed by 20 to 30 terms.
Thesedocumented terms, like the candidate terms, are used in therepresentat ion models described below.
The quality of the indexingprocess is estimated at 70 percents (number of right terms div ided bynumber of terms).
The wrong terms are essential ly due to problems ofpolysemy.
Indeed some terms (generally uniterms) have mult ip le senses(for example "BASE") and produce a great part of the noise.106III.
Term Subject Field DiscriminationXXI.l.
Word Sense D isa~higuat ionand Term Subject Field Discr iminationThe d iscr iminat ion  of word  senses is a wel l  known prob lem incomputat iona l  l inguist ics \[YNG 55\].
The prob lem of WSD is to bu i ldind icators  descr ib ing  the di f ferent senses of a word.
Given a contextof a word, these indicators are used to predict  its sense.
Face to thed i f f i cu l ty  of manua l ly  bu i ld ing these indicators, researchers  haveturned to resources such as machine- readable  d ict ionar ies  \[VER 90\] andcorpora  \[YAR 92\].WSD and term subject f ie ld d iscr iminat ion f rom corpora can becons idered  s imi lar  in the way that they are both  a prob lem ofc lass i f i ca t ion  into a class (a sense for a word  and a subject  f ie ldfor a term).
Nevertheless,  the prob lem is s tat i s t ica l ly  di f ferent.
Inone case, a word  is represented by a few var iables (its context) andis c lass i f ied  into one class chosen among a few classes.
In the othercase, a term is represented by hundred of var iables (one of the modelsdescr ibed  in chapter  IV) and is c lass i f ied  into a class chosen amonghundred  of classes.ZII.2.
L inear Discr~.mlnatozyAnalysisThe prob lem of d iscr iminat ion can be descr ibed as follows: Arandom var iab le  X is d is t r ibuted in a p -d imens iona l  space, xrepresents  the observed values of var iab le  X.
The prob lem is todetermine the d is t r ibut ion of X among q d ist r ibut ions (the classes),based  on the observed values x.
The method implemented here is l ineard i sc r iminatory  analysis.Us ing a sample that has a l ready been classif ied, d iscr iminatoryanalys is  can construct  c lass i f icat ion functions wh ich  take intoaccount  the var iab les  that descr ibe the elements to be classi f ied.Each e lement x to be c lass i f ied is descr ibed by a b inary  vector  x=(xl, x2 .
.
.
.
.
xi .
.
.
.
.
xp) where xi=l or xi=0.xi=l  means the var iab le  xi descr ibes the term x.x i=0 means the vara ib le  xi does not descr ibe the term x.The probab i l i ty  that an element x is in a class c is written:P( C=c I X=x ) where C is a random var iab le  and X is a random vector.Us ing Bayes formula, it may be deduced that:P( C=c \[ X=x ) = ( P( C = c ) P( X = x I C = c ) ) / P( X = x )There are three probabi l i t ies  to estimate:- Est imate  P( C = c)107P( C = c ) is est imated by nc / n where:nc is the number  of elements of the class cn is the number  of elements in the sample- Es t imate  P( X = x)This est imate  is s impl i f ied by normal is ing  the probabi l i t ies  to I.- Es t imate  P( X = x I C = c )For this estimate, we assume that the random var iab les  Xl, X2 .
.
.
.
Xmare independent  for a given class c. This leads to:P( X=x I C=c)  =HP(  xi =x i  I C =c  ) andP( Xi = 1 I C = c ) is est imated by nc, i  / ncP( Xi = 0 I C = c ) is est imated by 1 - nc, i  / ncwhere  nc, i  is the number of elements in the sample which are in classc, and for which  xi =i, and nc is the number  of e lements of the samplein c lass c.Once all the probabi l i t ies  are est imated, the c lass i f i ca t ionfunct ion  for an element x consists of choosing the class that has theh ighest  probabi l i ty .
This funct ion min imises  the r i sk  ofc lass i f i ca t ion  error  \[RAO 65\].IV.
Description of the ExperimentThe purpose  of this exper iment is to determine the best  way toc lass i fy  candidate terms from a corpus in semant ic  fields.
The generalp r inc ip le  is, firstly, to represent  the candidate terms to bec lassi f ied,  then, to c lass i fy  them, and finally, to eva luate  thequa l i ty  of the c lassi f icat ion.
The c lass i f i ca t ion  method is based  onlearn ing process, which requires a set of p rev ious ly -c lass i f ied  terms(the learn ing sample manual ly  c lassi f ied).
The  eva luat ion  alsorequi res  a test sample, a set of p rev ious ly -c lass i f ied  terms whichhave to be automat ica l ly  classif ied.
The eva luat ion  then cons ists  ofcompar ing  the results  of the c lass i f i cat ion  process  to the prev iousmanual  c lass i f i ca t ion  of the test sample.IV.I.
Learning and Test SampleThe thesaurus terms found in the corpus were separated into twosets: a subset of about 3,000 terms which composed the learn ingsample, and a subset of 1,000 terms which  composed the test sample.Al l  these terms had already been manua l ly  c lass i f ied  by theme andsemant ic  f ie ld in the thesaurus.108Rate of We l l  C lass i f ied  TermsThe eva luat ion  cr i ter ia  is the rate of wel l  c lass i f ied  termsca lcu la ted  among the 1,000 terms of the test sample.Rate of wel l  c lass i f ied terms = number  of wel l  c lass i f ied  termsd iv ided  by the number  of c lass i f ied  terms.ZV.2 .
TezmRepresentat ion  ModelsThe representat ion  of the terms to be c lass i f ied is the mainparameter  that determines the qual i ty of the classi f icat ion.
Indeed,this exper iment  showed that, for a s ingle representat ion model,  thereis no s ign i f icant  d i f ference between the results of the var iousc lass i f i ca t ion  methods.
By example, the nearest neighbours method(KNN) \[DAS 90\] was tested without  any s igni f icant difference.
The onlyparameter  that t ru ly  inf luences the result  is the way of represent ingthe terms to be classif ied.
Three models  were evaluated.
The first isbased  on a term/document approach, and the two others by a term/termapproach.IV2 .
i .
The TezmlDocument  Mode lThe term/document  model  uses the transposi t ion of the standarddocument / te rm matrix.
Each l ine represents a term, and each column adocument.
At the intersect ion of a term and a document, there is 0 ifthe term is not in the document in question, and 1 if it is present.The s tandard  document / term matr ix  showed its worth  in the Saltonvector  model  \[SAL 88\].
It can therefore be hoped that the documentsthat conta in  a term provide a good representat ion of this term for itsc lass i f i ca t ion  in a field.ZV.2 .2 .
The Tezm/TezmMode lsThe term/term model uses a matr ix  where each line represents  aterm to be classif ied, and each column represents a thesaurus termrecogn ised  in the corpus, or a candidate term extracted f rom thecorpus.
At the intersect ion of a line and a column, two indicatorshave been studied.Co-occurrences  matr ix:  The indicator  is the co-occurrence between twoterms.
Co-occurrence ref lects the fact that two terms are foundtogether  in documents.Mutual  in format ion  matrix: The indicator is the mutual  in format ionbetween two terms.
Mutual informat ion (\[CHU 89\] and \[FEA 61\]) ref lectsthe fact that two terms are often found together in documents, butrare ly  alone.
MI(x,y) is the mutual  information between terms x and y,and is written:109MI(x,y) = log2(P(xy)  /P(x) P(y ))where P(x,y) the probabil ity of observing x and y together and P(x)the probabi l i ty  of observing x, P(y) the probabi l i ty  of observing y.In the two cases, the matrix has to be transformed into a binarymatrix.
The solution is to choice a threshold under which the value isput to 0 and above which the value is put to i.
Lots of values hadbeen tested.
The best classif ication for the co-occurrence matr ix  isobtained for a threshold of three.
The best c lass i f icat ion for themutual  information matrix is obtained for a threshold of 0.05.Resu l tsThe main results concern three term representat ion models and twoclassif ications: the first in 49 themes, and the second in 330semantic fields.
The criterion chosen for the evaluation is the wellc lassi f ied rate.Method=Term Document modelTerm term model with co-occurrenceTerm term model  wi th  tmatualin format ionThemesclassi f icat ion42.931 .589.8Semanticfieldsclassif icat ion27.319.865.2Rate of Wel l  C lass i f ied  TermsThere is a significant difference between the term/term modelwi th  mutual information and the other two models.
The good rates (89.8and 65.2) can be improved if the system proposes more than one class.In the case of 3 proposed classes (sorted by descendingprobabil it ies),  the probabil ity that the r ight class is in theseclasses is est imated respectively by 97.1 and 91.2 for the themes andthe semantic fields.Discuss ionWithout a doubt, the term/term model with mutual information hasthe best performance.
Nonetheless, these good results must bequalified.A detai led examination of the results shows that there is a widedispersion of the rate of well c lassif ied terms depending on the f ield(the 49 themes or the 320 semantic fields).
The explanation is thatthe documents in the corpus are essential ly thematic.
Thus, thevocabulary for certain fields in the thesaurus is essential lyconcentrated in a few documents.
Classi f icat ion based on mutualinformation is then efficient.
On the other hand, certain fields aretransverse (e.g., computer science, etc.
), and are found in manydocuments that have few points in co~ton (and l itt le common technicalvocabulary).
Terms in these fields are diff icult to classify.110Another problem with the method is connected to therepresentat iveness of the learning sample.
Commonly, for a givenfield, a certain number of terms are available (for example 20,000terms in the EDF thesaurus).
It is more rare for all these terms to befound in the corpus under study (4,000 terms found in thisexperiment).
Thus, if a class (a theme or semantic field) is not wellrepresented in the corpus, the method is unable to classify candidateterms in this class because the learning sample for this class is notenough.Through this experiment, an automatic classif ication of 300candidate terms in 330 semantic fields was proposed to the group thatval idates new thesaurus terms.
This classif ication was used by thedocumental ists to update the EDF thesaurus.
Each term was proposed inthree semantic fields (among 330) sorted from the highest probabi l i ty(to be the right semantic field of the term) to the lowest.111References\[BLO 92\] Blosseville M.J., Hebrail G., Monteil M.G., Penot N.,"Automatic Document Classification: Natural Language Processing,Statistical Data Analysis, and Expert System Techniques used together", ACM-SIGIR'92 proceedings, 51-58,1992.\[CHU 89\] Church K., "Word Association Norms, Mutual information, andLexicography ", ACL 27 proceedings, Vancouver, 76-83, 1989.\[DAS 90\] Dasarathy B.V., "Nearest Neighbor (NN) Norms: NN PatternClassification Techniques", IEEE Computer Society Press, 1990.\[FEA 61\] Fano R., "Transmission of Information", MIT Press, Cambridge,Massachusetts, 1961.lOGO 94\] Ogonowski A., Herviou M.L.,extracting and structuring knowledgeproceedings, 1049-1053, 1994.Monteil M.G., "Tools forfrom text", Coling'94\[RAO 65\] Rao C.R., "Linear Statistical Inference and its applications", 2nd edition, Wiley, 1965.\[SAL 88\] Salton G., ~ Automatic Text Processing : the Transformation,Analysis, and Retrieval of Information by Computer ~, Addison-Wesley,1988.\[STA 93\] Sta J.D., "Information filtering : a tool for communicationbetween researches", INTERCHI'93 proceedings, Amsterdam, 177-178,1993.\[STA 95a\] Sta J.D., "Document expansion applied to classification :weighting of additional terms", ACM-SIGIR'95 proceedings, Seattle,177-178, 1995.\[STA 95b\] Sta J.D., "Comportement statistique des termes etacquisition terminologique & partir de corpus", T.A.L., Vol.
36, Num.1-2, 119-132, 1995.\[VER 90\] Veronis J., Ide N., "Word Sens Disambiguation with VeryLarge Neural Networks Etracted from Machine Radable Dictionnaries"COLING'90 proceedings, 389-394, 1990.\[YAR 92\] Yarowsky D., "Word-Sense Disambiguation Using StatisticalModels of Roger's Categories Trained on Large Corpora", COLING'92proceedings, 454-460, 1992.\[YNG 55\] Yngve V., "Syntax and the Problem of Multiple Meaning" inMachine Translation of Languages, Will iam Lock and Donald Booth eds.,Wiley, New York, 1955.112
