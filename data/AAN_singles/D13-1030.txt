Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 300?310,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsInterpreting Anaphoric Shell Nouns using Antecedents ofCataphoric Shell Nouns as Training DataVarada KolhatkarDepartment of Computer ScienceUniversity of Torontovarada@cs.toronto.eduHeike ZinsmeisterInstitut fu?r GermanistikUniversita?t Hamburgheike.zinsmeister@uni-hamburg.deGraeme HirstDepartment of Computer ScienceUniversity of Torontogh@cs.toronto.eduAbstractInterpreting anaphoric shell nouns(ASNs) such as this issue and this factis essential to understanding virtuallyany substantial natural language text.One obstacle in developing methodsfor automatically interpreting ASNs isthe lack of annotated data.
We tacklethis challenge by exploiting cataphoricshell nouns (CSNs) whose constructionmakes them particularly easy to interpret(e.g., the fact that X).
We propose an ap-proach that uses automatically extractedantecedents of CSNs as training data tointerpret ASNs.
We achieve precisionsin the range of 0.35 (baseline = 0.21) to0.72 (baseline = 0.44), depending uponthe shell noun.1 IntroductionAnaphors such as this fact and this issue encapsulatecomplex abstract entities such as propositions, facts,and events.
An example is shown below.
(1) Here is another bit of advice: EnvironmentalDefense, a national advocacy group, notes that?Mowing the lawn with a gas mower producesas much pollution in half an hour as driving acar 172 miles.?
This fact may help to explainthe recent surge in the sales of the good old-fashioned push mowers or the battery-poweredmowers.Here, the anaphor this fact is interpreted with thehelp of the clausal antecedent marked in bold.
Theantecedent here is complex because it involves anumber of entities and events (e.g., mowing thelawn, a gas mower) and relationships between them,and is abstract because the antecedent itself is not apurely physical entity.The distinguishing property of these anaphors isthat they contain semantically rich abstract nouns(e.g., fact in (1)) which characterize and label theircorresponding antecedents.
Linguists and philoso-phers have studied such abstract nouns for decades(Vendler, 1968; Halliday and Hasan, 1976; Francis,1986; Ivanic, 1991; Asher, 1993).
Our work is in-spired by one such study, namely that of Schmid(2000).
Following Schmid, we refer to these abstractnouns as shell nouns, as they serve as conceptualshells for complex chunks of information.
Accord-ingly, we refer to the anaphoric occurrences of shellnouns (e.g., this fact in (1)) as anaphoric shell nouns(ASNs).An important reason for studying ASNs is theirubiquity in all kinds of text.
Schmid (2000) ob-served that shell nouns such as fact, idea, point, andproblem were among the 100 most frequently oc-curring nouns in a corpus of 225 million words ofBritish English.
Moreover, ASNs can play severalroles in organizing a discourse such as encapsulationof complex information, cohesion, and topic bound-ary marking.
So correct interpretation of ASNs canbe an important step for correct interpretation of adiscourse, and in a number of NLP applications suchas text summarization, information extraction, andnon-factoid question answering.Despite their importance, ASNs have not receivedmuch attention in Computational Linguistics, andresearch in this field remains in its earliest stages.
At300present, the major obstacle is that there is very littleannotated data available that could be used to train asupervised machine learning system for robustly in-terpreting these anaphors, and manual annotation isan expensive and time-consuming task.We tackle this challenge by exploiting a categoryof examples, as shown in (2), whose construction isparticularly easy to interpret.
(2) Congress has focused almost solely on the factthat special education is expensive ?
and thatit takes away money from regular education.Here, in contrast with (1), the fact is not anaphoricin the traditional sense, but is an easy case of aforward-looking anaphor ?
a cataphor.
While theresolution process of this fact in (1) is quite chal-lenging as it requires the use of semantics and worldknowledge, it is fairly easy to interpret the fact in(2) based on the syntactic structure alone.
We referto these easy-to-interpret cataphoric occurrences ofshell nouns as cataphoric shell nouns (CSNs).
Theinterpretation of both ASNs and CSNs will be re-ferred to as antecedent.1 The antecedent of the factin (2) is given in the post-nominal that clause.
Weuse the term shell concept to refer to the general no-tion of a shell noun, i.e., the semantic type of theantecedent.
For example, the notion of an issue is animportant problem which requires a solution.In this work, we propose an approach to interpretASNs that exploits unlabelled but easy-to-interpretCSN examples to extract characteristic features as-sociated with the antecedent of different shell con-cepts.
We evaluate our approach using crowdsourc-ing.
Our results show that these unlabelled CSN ex-amples provide useful linguistic properties that helpin interpreting ASNs.2 Related workThe resolution of anaphors to non-nominal an-tecedents has been well analyzed taking discoursestructure and semantic types into account (Web-ber, 1991; Passonneau, 1989; Asher, 1993).
Mostwork in machine anaphora resolution, however,is restricted to anaphora that involve nominal an-tecedents only (Poesio et al 2011).1Sadly, the more-logical term for the interpretation of aCSN, succedent, does not actually exist.There are some notable exceptions which havetackled the challenge of interpreting non-nominalantecedents (Eckert and Strube, 2000; Strube andMu?ller, 2003; Byron, 2004; Mu?ller, 2008).
Theseapproaches are limited as they either rely heavilyon domain-specific syntactic and semantic annota-tion or prepossessing, or mark only verbal proxiesfor non-nominal antecedents.Recently, Kolhatkar and Hirst (2012) presenteda machine-learning based resolution system for thisissue anaphora, identifying full syntactic phrases asantecedents.
Although they achieved promising re-sults, their approach was limited in two respects.First, it focused on only one type of shell nounanaphora (issue anaphora).
Second, their trainingdata was restricted to MEDLINE abstracts in whichthis issue is used in a rather systematic way.
Further-more, their work is based on manually labelled ASNantecedents, whereas we use automatically identi-fied CSN antecedents, which we interpret as explic-itly expressed antecedents in comparison to the moreimplicitly expressed ASN antecedents.Using explicitly expressed structure in the textto identify implicit structure is not new.
The sameidea has been applied before in computational lin-guistics.
Marcu and Echihabi (2002) identified im-plicit discourse relations using explicit ones.
Mark-ert and Nissim (2005) used Hearst?s (1992) explicitpatterns to learn lexical semantic relations for NP-coreference and other-anaphora resolution from theweb.
Although our work focuses on a different topic,the methodology is in the same vein.3 Hypothesis of this workThe hypothesis of this work is that CSN antecedentsand ASN antecedents share some linguistic proper-ties and hence linguistic knowledge encoded in CSNantecedents will help in interpreting ASNs.
Accord-ingly, we examine which features present in CSNantecedents are relevant in interpreting ASNs.The motivation and intuition behind this hypoth-esis is as follows.
The antecedents of both ASNsand CSNs represent the corresponding shell con-cept.
So are there any characteristic features asso-ciated with this shell concept?
Do speakers of En-glish follow certain patterns of syntactic shape orwords, for instance, when they state facts, decisions,301The CSN corpus  (211,722 instances) The ASN corpus  (2,323 instances)TrainingTraining dataCSN antecedent modelsCSN antecedent extractor Ranked ASN antecedent candidatesCrowdsourcing evaluationTestingFigure 1: Overview of our approachor issues?
There is an abundance of data for CSNantecedents and if we are able to capture particu-lar linguistic characteristic features associated witha shell concept using this data, we can use this in-formation to interpret ASNs.
For instance, exam-ple (2) demonstrates characteristic properties of an-tecedents of the shell noun fact including that (a)they are propositions and are generally expressedwith clauses or sentences rather than noun phrases,and (b) they are generally expressed in the presenttense.
Observe that these properties also hold forthe antecedent of this fact in example (1).We test our hypothesis by building machine learn-ing models that are trained on automatically ex-tracted CSN antecedents and then applying thesemodels to recover ASN antecedents.
Figure 1 showsan overview of our methodology.4 BackgroundFormal definition Shell-nounhood is a functionalnotion; it is defined by the use of an abstract nounrather than the inherent properties of the noun itself(Schmid, 2000).
An abstract noun is a shell nounwhen the speaker decides to use it as a shell noun.Shell noun categorization Schmid (2000) givesa list of 670 English nouns which are frequentlyused as shell nouns.
He divides them into sixbroad semantic classes: factual, linguistic, mental,modal, eventive, and circumstantial.
Table 1 showsthis classification, along with example shell nounsfor each category.
For this work, we selected sixfrequently occurring shell nouns covering four ofSchmid?s six classes: fact and reason from factual,issue and decision from mental, question from lin-guistic, and possibility from modal.
These shellnouns tend to have antecedents that lie within a sin-gle sentence.
We excluded eventive and circumstan-Class Description Examplesfactual states of affairs fact, reasonlinguistic linguistic acts questionmental ideas issue, decisionmodal judgements possibilityeventive events act, reactioncircumstantial situations situation, wayTable 1: Schmid?s classification of shell nouns.
Thenouns given in the Example column tend to occur fre-quently with the respective class.
The shell nouns used inthis work are shown in boldface.Pattern ExampleN-to Several people at the group said the deci-sion to write the letters was not controver-sial internally.N-be-to The principal reason is to create a repre-sentative government rather than to selectthe most talented person.N-that Mr. Shoval left open the possibility thatIsrael would move into other West Bankcities.N-be-that The simple and reassuring fact is that afuture generation of leaders is seeking newchallenges during challenging times.N-wh There is now some question whether thecountry was ever really in a recession.N-be-wh Of course, the central, and probably in-soluble, issue is whether animal testing iscruel.Table 2: Easy-to-interpret CSN patterns given by Schmid(2000).
In the Example column, the patterns are markedin boldface and the antecedents are marked in italics.tial classes because the shell nouns in these classestend to have rather unclear and long antecedents.2Shell noun patterns Schmid (2000) also providesa number of lexico-grammatical patterns for shellnouns.
In Section 1, we noted two such patterns:this-N (this fact in example (1)) and N-that (fact thatin example (2)).
We also noted that CSNs with pat-tern N-that are fairly easy to interpret compared tothe ASN pattern this-N. Table 2 shows some othereasy-to-interpret CSN patterns given by Schmid.Generally, for all these patterns, the antecedent is2These observations are based on an exploratory pilot anno-tation we carried out on sample data of 150 ASN instances.302quite easy to extract with a few predefined rules.Shell antecedent properties Antecedents ofCSNs and ASNs share some properties while theyare distinguished by others.
The distinguishingproperty is that CSNs, by their construction, havetheir antecedents in the same sentence, as shownin example (2).
On the other hand, ASNs canhave long-distance as well as short-distance an-tecedents.3 The common properties are as follows.First, antecedents of both ASNs and CSNs representthe corresponding shell concept, e.g., the notionof a fact or an issue.
Second, in both cases, theantecedents are complex abstract entities, whichinvolve a number of entities and relationshipsbetween them.
Finally, in both cases, there is noone-to-one correspondence between the syntactictype of an antecedent and semantic type of itsreferent (Webber, 1991).
For instance, a semantictype such as fact can be expressed with differentsyntactic shapes such as a clause, a verb phrase, ora complex sentence.
Conversely, a syntactic shape,such as a clause, can function as several semantictypes, including fact, proposition, and event.5 Training phaseAs shown in Figure 1, the goal of the training phaseis to build training data from CSNs and their an-tecedents and train models which can be used forresolving ASNs.5.1 The CSN corpusWe automatically constructed a corpus, a subset ofthe New York times (NYT) corpus4, which contains211,722 sentences following CSN patterns from Ta-ble 2.
We considered part-of-speech information5while looking for the patterns.
For instance, in-stead of the pattern N-that, we actually looked for{shell noun NN that IN}.3In our annotated sample data, we observed ASN an-tecedents as close as the same sentence and as far as 7 sentencesaway from the anaphor.4http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2008T195http://nlp.stanford.edu/software/tagger.shtml5.2 Antecedent extractor for CSNsThe goal of the antecedent extractor is to create auto-matically labelled CSN antecedent data.
Recall thatantecedents of CSNs can be extracted using simplepredefined rules that are based on the syntactic struc-ture alone.
For instance, the antecedent extractionrule for example (2) would be: if the example fol-lows the pattern fact-that, extract the post-nominalthat clause as the antecedent.
To come up with a listof such extraction rules, we systematically analyzeda sample of examples (about 20 examples) of eachpattern for each shell noun.
Table 3 summarizes theresulting antecedent extraction rules.The actual antecedent extraction works as fol-lows.
First, we parsed the examples from the CSNcorpus using the Stanford parser.6 Then for each ex-ample, we applied rules from Table 3 depending onthe shell noun and the pattern it follows to extractan appropriate syntactic constituent as the CSN an-tecedent.
For instance, for the noun fact followingthe N-that pattern, as in example (2), we first lookedfor the NP constituent containing the shell noun fact,and then extracted the sentential constituent follow-ing the NP constituent as the CSN antecedent.
Al-though, in most of the cases, the antecedent is givenin the post-nominal wh, that, or infinitive clauses,sometimes it is not present in the immediately fol-lowing clause but is given only as a predicate, asshown in (3).
(3) The primary reason that the archdiocese cannotpay teachers more is that its students cannot af-ford higher tuition.In such cases, we looked for the pattern (VP (VBbe verb) X) in the right sibling of the NP contain-ing the pattern shell noun-that and extracted X asthe CSN antecedent.Two contradictory goals need to be achievedwhile extracting antecedents of CSNs.
The first re-quires only considering CSNs with high-confidencepatterns, whereas the second requires considering asmany patterns as possible to allow a wide variety ofantecedent examples with different linguistic prop-erties (e.g., syntactic shape).
Our antecedent extrac-tor tries to find a balance between the two goals.6http://nlp.stanford.edu/software/lex-parser.shtml303fact reason issue decision question possibilityN-to ?
?
?
inf clause predicate inf clauseN-be-to ?
inf clause inf clause inf clause inf /wh clause inf clauseN-that that clause predicate predicate ?
predicate that clauseN-be-that that clause that clause that clause that clause that clause that clauseN-wh ?
predicate wh clause wh clause wh clause ?N-be-wh wh clause wh clause wh clause wh clause wh clause wh clauseTable 3: Content extraction patterns for CSNs.
Patterns in boldface are the prominent patterns for the respective shellnoun.
inf clause = infinitive clause.
Discarded patterns are denoted by ?.To address the first goal, we filter examples fol-lowing noisy patterns, i.e., the patterns that do notunambiguously encode antecedents of that CSN.
Forinstance, the pattern N-to is a highly preferred pat-tern for decision, as shown in (4).
The antecedentextraction rule here is relatively simple: if the exam-ple follows the pattern decision-to, extract the post-nominal infinitive clause as the correct antecedent.
(4) President Jacques Chirac?s arrogant decision todefy the world and go ahead with two nuclearbomb tests in Polynesia deserves contempt.But the same pattern is noisy for reason.
In (5), forexample, the actual reason is not given anywhere inthe sentence.
So we discard the examples followingthe pattern N-to for reason.
(5) Investors have had reason to worry about stocks.We also discard examples with negative determiners,as in (6), because in such cases, the extraction rulesdo not precisely give the antecedent of the givenCSN.
(6) He was careful to repeat anew that he had madeno decision to go to war.For the N-wh pattern, we exclude certain whwords for certain nouns.
For example, we excludethe wh word which for question as the Penn Tree-bank tagset7 does not distinguish between which as arelative pronoun and as a question.
We are interestedin the latter but not the former.
Other discarded whwords include which and when for fact; all wh wordsexcept when and why for reason, all wh words excepthow and whether for issue; which, whom, when, andwhy for decision; which and when for question; andall wh words for possibility.7The Stanford tagger we employ uses the Penn Treebanktagset (Marcus et al 1993).To address the second goal of allowing a widevariety of antecedent examples, we try to includeas many patterns as possible for each shell noun,as shown in Table 3.
For instance, the patternsquestion-to and question-be-to will have infinitiveclauses as antecedents (marked as VP or S+VPby the parser), whereas for the examples follow-ing patterns question-wh and question-be-wh theantecedent will be in the wh clauses (marked asSBAR).
For the pattern question-that, the antecedentwill be in the predicate (similar to example (3)),which can be a prepositional phrase, a noun phraseor a clause.5.3 Models for CSN antecedentsThe antecedent extractor gives labels for each in-stance in the CSN corpus.
Using this labelled data,we train machine learning ranking models for dif-ferent shell concepts that capture the characteristicfeatures associated with that shell concept.
The fol-lowing sections describe each step of our rankingmodels in detail.5.3.1 Candidate extractionThe first step is to extract the set of eligible an-tecedent candidates C = {C1,C2, ...,Ck} for the CSNinstance ai.
To train a machine learning model weneed positive and negative examples.
We alreadyhave positive examples for antecedent candidates ?the true antecedents given by the antecedent extrac-tor.
But we also need negative examples of an-tecedent candidates.
By their construction, CSNshave their antecedents in the same sentence.
Sowe extract all syntactic constituents of this sentence,given by the Stanford parser.
All the syntactic con-stituents, except the true antecedent, are consideredas negative examples.
With this candidate extractionmethod, we end up with many more negative exam-304ples than positive examples, but that is exactly whatwe expect with ASN antecedent candidates, i.e., thetest data on which we will be applying our models.5.3.2 FeaturesAlthough our problem is similar to anaphora res-olution, we cannot make use of the usual anaphoraor coreference resolution features such as agreementor string matching (Soon et al 2001) because of thenature of ASN and CSN antecedents.
We came upwith a set of features based on the properties thatwere common in both ASN and CSN antecedents,according to our judgement.Syntactic type of the candidate (S) We observedthat each shell noun prefers specific CSN patternsand each pattern involves a particular syntactic type.For instance, decision prefers the pattern N-to andconsequently realizes as its antecedents more verbphrases than, for example, noun phrases.
We employtwo versions of syntactic type: fine-grained syntac-tic type given by the Stanford parser (e.g., NP-TMP,RRC) and coarse-grained syntactic type (e.g., NP,VP, S, PP) in which we consider ten basic syntacticcategories and map all fine-grained syntactic typesto these categories.Context features (C) Context features allow ourmodels to learn about the contextual clues that signalthe antecedent.
This class contains two features: (a)coarse-grained syntactic type of left and right sib-lings of the candidate, and (b) part-of-speech tag ofthe preceding and following words of the candidate.Embedding level features (E) These features(Mu?ller, 2008) encode the embedding level of thecandidate within its sentence.
We consider two em-bedding level features: top embedding level and im-mediate embedding level.
Top embedding level isthe level of embedding of the given candidate withrespect to its top clause (the root node), and immedi-ate embedding level is the level of embedding withrespect to its immediate clause (the closest ancestorof type S or SBAR).
The intuition behind this fea-ture is that if the candidate is deep in the parse tree,it is possibly not salient enough to be an antecedent.As we consider all syntactic constituents as poten-tial candidates, there are many that clearly cannot beantecedents.
This feature will allow us to get rid ofthis noise.Subordinating conjunctions (SC) As we can seein Table 2, subordinating conjunctions are commonwith CSN and ASN antecedents.
Vendler (1968)points out that the shell noun fact prefers a that-clause, and question and issue prefer a wh-questionclause.
We observed that the pattern because X iscommon with reason.
The subordinating conjunc-tion feature encodes these preferences for differentshell nouns.
The feature checks whether the candi-date follows the pattern SBAR ?
(IN sconj) (S ...),where sconj is a subordinating conjunction.Verb features (V) A prominent property of CSNand ASN antecedents is that they tend to containverbs.
All examples from Table 2, for example, con-tain verbs.
Moreover, certain shell nouns have tenseand aspect preferences.
For instance, for shell nounfact, lexical verbs in past and present tenses predom-inate (Schmid, 2000), whereas modal forms are ex-tremely common for possibility.
We use three verbfeatures that capture this idea: (a) presence of verbsin general, (b) whether the main verb is finite or non-finite, and (c) presence of modals.Length features (L) The intuition behind thesefeatures is that CSN and ASN antecedents tend to belong, especially for nouns such as fact.
We considertwo length features: (a) length of the candidate inwords, and (b) relative length of the candidate withrespect to the sentence containing the antecedent.Lexical features (LX) Our extractor gives us alarge number of antecedent examples for each shellnoun.
A natural question is whether certain wordstend to occur more frequently in the antecedent thannon-antecedent parts of the sentence.
To deal withthis question, we extracted all antecedent unigrams(i.e., unigrams occurring in antecedent part of thesentence) and non-antecedent unigrams (i.e., uni-grams occurring in non-antecedent parts of the sen-tence) for each shell noun.
Then for all antecedentunigrams for a particular shell noun, we computedterm goodness in terms of information gain (Yangand Pedersen, 1997) and considered the first 50highly ranked unigrams as the lexical features forthat noun.
Note that, in contrast with the other fea-tures, these lexical features are tailored for each shellnoun and are extracted a priori.3055.3.3 Candidate ranking modelsNow that we have the set of candidate antecedentsand a set of features, we are ready to train CSN an-tecedent models.
We follow the candidate-rankingmodels proposed by Denis and Baldridge (2008) be-cause they allow us to evaluate how good an an-tecedent candidate is relative to all other candidates.For every shell noun, we gather automatically ex-tracted antecedent data given by the extractor for allinstances of that shell noun.
Then for each instancein this data, we extract the set C as explained inSection 5.3.1.
For each candidate Ci ?
C, we ex-tract a feature vector to create a corresponding set offeature vectors, C f = {C f 1,C f 2, ...,C f k}.
For everyCSN ai and a set of feature vectors correspondingto its eligible candidates C f = {C f 1,C f 2, ...,C f k},we create training examples (ai,C f i,rank),?C f i ?C f .
The rank is 1 if Ci is same as the true an-tecedent, i.e., the automatically extracted antecedentfor that CSN, otherwise the rank is 2.
We use thesvm rank learn call of SVMrank (Joachims, 2002)for training the candidate-ranking models.6 Testing phaseIn this phase, we use the learned candidate rankingmodels to identify the antecedents of ASNs.6.1 The ASN corpusWe started with about 450 instances for each of thesix selected shell nouns (2,700 total instances), con-taining the pattern {this shell noun}.
The instanceswere extracted from the NYT.
Each instance con-tains three paragraphs from the corresponding NYTarticle: the paragraph containing the ASN and twopreceding paragraphs as context.
After automati-cally removing duplicates and ASNs with a non-abstract sense (e.g., this issue with a publication-related sense), we were left with 2,323 instances.6.2 Antecedent identificationCandidate extraction The search space of ASNantecedents is quite large for two reasons: ASNstend to have long-distance as well as short-distanceantecedents, and there is no clear restriction on thesyntactic type of the antecedents.
In the ASN cor-pus, each sentence on average had 49.5 distinct syn-tactic constituents given by the Stanford parser.
Ifwe consider n preceding sentences, the sentencecontaining the anaphor, and one following sentence8as sources for antecedents, then the average num-ber of antecedent candidates will be 49.5?
(n+ 2).This is large compared to the search space of ordi-nary nominal anaphora.
In our previous work (Kol-hatkar et al 2013), we have developed methods thatidentify the sentence containing the antecedent ofthe ASN before identifying the precise antecedent.In brief, given a set of a fixed number of sentencesaround the sentence containing an ASN, these meth-ods reliably identify the sentence containing the an-tecedent.
In this paper, we treat these methods as ablack box.Given the sentence containing the antecedent,we extract all syntactic constituents given by theStanford parser from that sentence as potential an-tecedent candidates as for the training phase.
Inthe training phase, the antecedent is always con-tained in the set of syntactic constituents given bythe Stanford parser because the extractor obtains theappropriate antecedent using the syntactic informa-tion.
But in the testing phase, we cannot guaranteethat the true antecedent occurs in the extracted syn-tactic constituents due to the parser?s errors.
So forrobust candidate extraction, we extract all distinctconstituents from the 30-best parses instead of onlyconsidering the best parse, which increases the aver-age number of candidates from 49.5 to 55.2.Feature extraction and candidate rankingGiven the antecedent candidates, feature extractionand candidate ranking are essentially the same asfor the training phase, except of course we do notknow the true antecedent.
Once we have the featurevectors for each antecedent candidate, the appro-priate trained model, i.e., the model trained for thecorresponding shell noun, is invoked and the can-didates are ranked using the svm rank classifycall of SVMrank.7 EvaluationWe evaluate the ranked candidates of ASN instancesusing crowdsourcing.8The ASN corpus contains a few cataphoric examples thatdo not follow the standard patterns of the CSNs shown in Table2, but actually refer to an antecedent in the following sentence(e.g., Mr. Dukakis put this question to him: X).306Interface We chose to use CrowdFlower9 as ourcrowdsourcing interface because of its integratedquality-control mechanism.
For instance, it throwsgold questions randomly at the workers and theworkers who do not answer them correctly are notallowed to continue.We presented to the crowd evaluators the ASNinstances from the ASN corpus.
Recall that eachASN instance is made up of the paragraph contain-ing the ASN and two preceding paragraphs as con-text.
We displayed the first 10 highly-ranked candi-dates (ordered randomly) given by our testing phaseand asked the evaluators to choose the best answerthat represents the ASN antecedent.
We encouragedthe evaluators to select None when they did not agreewith any of the displayed answers.
We also askedthem how satisfied they were with the displayed an-swers.
We provided them with three options: unsat-isfied, satisfied, and partially satisfied.Our job contained 2,323 evaluation units.
Weasked for 8 judgements per instance and paid 6cents per evaluation unit.
As we were interestedin the verdict of native speakers of English, welimited the allowed demographic region to English-speaking countries.Results Among the 2,323 ASN instances, 96% ofthem were labelled as satisfied, 3% as partially satis-fied and 1% as unsatisfied.
Only 2% of the instanceswere labelled as None.
As expected, evaluators wereunsatisfied or partially satisfied with the options ofthese instances.
These results suggest that our res-olution models trained on automatically extractedantecedents of CSNs bring the relevant candidatesof ASN antecedents to the top, i.e., within first 10highly-ranked candidates.
This itself is a positive re-sult given the large search space of ASN antecedentcandidates (more than 55 candidates on average).Among the evaluation units, more than half of theevaluators agreed on an answer for 1,810 units.
Weused these instances for further analysis.To examine which CSN antecedent features arerelevant in identifying ASN antecedents, we carriedout ablation experiments with all feature class com-binations.
We compared the rankings given by ourranker to the crowd?s answer using precision at n9http://crowdflower.com/(P@n).10 More specifically, we count the number ofinstances where the crowd?s answers occur withinour ranker?s first n choices.
P@n then is this countdivided by the total number of instances.
Note thatP@1 is equivalent to the standard precision.We compared our results against two baselines:preceding sentence and chance.
The preceding sen-tence baseline chooses the previous sentence as thecorrect antecedent.
The chance baseline chooses acandidate from a uniform random distribution overthe set of 10 top-ranked candidates.The results are shown in Table 4.
Although dif-ferent feature combinations gave the best results fordifferent shell nouns, the features that occur fre-quently in many best-performing combinations wereembedding level (E), lexical (LX), and subordinat-ing conjunction (SC) features.
The SC features wereparticularly effective for issue and question, wherewe expected patterns such as whether X.Surprisingly, the syntactic type features (S) didnot show up very often in the best-performing fea-ture combinations, suggesting that the ASN an-tecedents had a greater variety of syntactic typesthan what was available in our CSN training data.The context features (C) did not appear in any ofthe best-performing feature combinations.
In fact,they resulted in a sharp decline in the precision.
Forinstance, for question, adding the context featuresto the best-performing combination {E,SC,V,L,LX}resulted in a drop of 16 percentage points.
Thisresult was not surprising because although the an-tecedents of ASNs and CSNs share similar proper-ties such as common words, we know that their con-text is generally different.We did not observe specific features associatedwith Schmid?s semantic categories.
An exceptionwas the E features which were particularly effectivefor the factual nouns fact and reason: the resultswith them alone gave high precision (0.68 for factand 0.72 for reason).
That said, the E features werepresent in most of the best-performing combinationseven for the shell nouns in other semantic categories.10CrowdFlower gives us a unique answer for each instance,which we take to be the crowd?s answer.
During annotation, ev-ery annotator is presented with a few gold questions randomlyand each annotator is assigned a trust score based on her per-formance on these gold questions.
The unique answer for aninstance is the answer with the highest sum of trusts.307fact (43,000 train and 472 test instances)Features P@1 P@2 P@3 P@4{E,L,LX} .70?
.85 .91 .94{E,V,L,LX} .68?
.86 .92 .95{E,SC,L,LX} .66?
.83 .92 .95PSbaseline .40 ?
?
?reason (4,520 train and 443 test instances)Features P@1 P@2 P@3 P@4{E,V,L} .72?
.86 .90 .93{E,V} .72?
.85 .90 .92{E,SC,LX} .69?
.84 .90 .94PSbaseline .44 ?
?
?issue (3,000 train and 303 test instances)Features P@1 P@2 P@3 P@4{SC,L} .47?
.59 .71 .78{SC,L,LX} .46?
.60 .70 .81{S,E,SC,L,LX}.40?
.61 .72 .81PSbaseline .30 ?
?
?decision (42,332 train and 390 test instances)Features P@1 P@2 P@3 P@4{E,LX} .35?
.53 .67 .76{E,SC,LX} .30?
.48 .65 .75{E,SC,V,L,LX}.27 .44 .57 .69PSbaseline .21 ?
?
?question (9,336 train and 440 test instances)Features P@1 P@2 P@3 P@4{E,SC,V,L,LX} .70?
.82 .87 .90{E,SC,LX} .68?
.83 .88 .91{E,SC,V,LX} .69?
.80 .87 .91PSbaseline .25 ?
?
?possibility (11,735 train and 278 test instances)Features P@1 P@2 P@3 P@4{SC,L,LX} .56?
.75 .87 .92{E,SC} .56?
.76 .87 .91{E,L,LX} .54?
.76 .86 .91PSbaseline .34 ?
?
?Table 4: Evaluation of our ranker for antecedents of six ASNs.
For each noun we show the three best-performingfeature combinations.
P@n is the precision at rank n (P@1 = standard precision).
Boldface indicates the best in thecolumn.
PSbaseline = preceding sentence baseline.
The P@1 results significantly higher than PSbaseline are markedwith ?
(two-sample ?2 test: p < 0.05).
The chance baseline results were 0.1, 0.2, 0.3, and 0.4 for P@1, P@2, P@3,and P@4, respectively.The only previous work with which our resultscould be compared is that of Kolhatkar and Hirst(2012).
The work reports precision in the rangeof 0.41 to 0.61 in resolving this issue anaphora inthe Medline domain.
In our case, for this issue in-stances from the NYT corpus, we achieved precisionin the range of 0.40 to 0.47.
Furthermore, we ap-plied our models to resolve this issue instances fromKolhatkar and Hirst?s (2012) work.11 Even withmodels trained on automatically labelled CSN an-tecedents, we achieved similar results to Kolhatkarand Hirst?s results: P@1 of 0.45, P@2 of 0.59, P@3of 0.65, and P@4 of 0.67.
These results show thedomain robustness of our methods with respect tothe shell noun issue.
Recall that Kolhatkar and Hirst(2012) looked at only very specific cases of this is-sue and used manually annotated data (Section 2),as opposed to the automatically extracted CSN an-tecedent data we use.11We thank an anonymous reviewer for suggesting this to us.8 Discussion and conclusionThe goal of this paper was to examine to what ex-tent CSNs help in interpreting ASNs.
Based on theevaluators?
satisfaction level and very few None re-sponses, we conclude that our models trained onCSN antecedents were able to bring the relevantASN antecedent candidates into the top 10 candi-dates.When we applied the models trained on CSN an-tecedents to interpret ASNs, we achieved precisionin the range of 0.35 to 0.72.
The precision results ashigh as 0.72 for reason and 0.70 for fact and ques-tion support our hypothesis that the linguistic knowl-edge provided by CSN antecedents helps in identify-ing the antecedents of ASNs.
We observed differentbehaviour for different nouns.
The mental nouns is-sue and decision in general were harder to interpretthan other shell nouns.
The models trained on CSNsachieved precisions of 0.35 for decision and 0.47 forissue.
So there is still much room for improvement.That said, for the same nouns, the antecedents werein the first four ranks about 76% to 81% of the times,308suggesting that in future research, our models can beused as base models to reduce the large search spaceof ASN antecedent candidates.We observed a wide range of performance for dif-ferent shell nouns.
One reason is that the size of thetraining data was different for different shell nouns.After excluding the noisy examples (Section 5.2),there were about 43,000 training examples for fact,but only about 3,000 for issue.
In addition, a par-ticular shell concept itself can be difficult, e.g., thevery idea of what counts as an issue is more fuzzythan what counts as a fact.One limitation of our approach is that it onlylearns the properties that are present in CSN an-tecedents.
However, ASN antecedents have addi-tional properties which are not always captured byCSN antecedents.
For instance, for the shell noundecision, most of the training examples were infini-tive phrases of the form to X.
But antecedents of theASN decision were mostly court decisions and wereexpressed with full sentences.Moreover, although the models trained on CSNantecedents are able to encode characteristic fea-tures associated with the general shell concept, theyare unable to distinguish between two competingcandidates both containing the characteristic fea-tures of that shell concept.
For instance, our ap-proach will not be able to handle the constructed ex-amples in (7).
(7) The teacher erased the solutions before John hadtime to copy them out, as he had momentarilybeen distracted by a band playing outside.a) This fact infuriated him, as the teacher al-ways erased the board quickly and John sus-pected it was just to punish anyone who waslost in thought, even for a moment.b) This fact infuriated the teacher, who had al-ready told John several times to focus onclass work.Here, both propositions possess properties of theshell concept fact.
Understanding the context of theanaphor itself is crucial in correctly identifying thefact in each case, which cannot be learnt from CSNantecedents due to their specific context patterns.A number of extensions are planned for this work.First, we plan to use both kinds of data, CSN andASN antecedent data, which will give us a basisfor developing a better performing ASN resolver.We also plan to incorporate contextual features (e.g.,right-frontier rule (Webber, 1991) and context rank-ing (Eckert and Strube, 2000)).
Finally, we will ex-amine whether a model trained for one shell nouncan be generalized to other shell nouns from thesame semantic category.AcknowledgementsWe thank the anonymous reviewers for theirconstructive comments.
This material is basedupon work supported by the United States AirForce and the Defense Advanced Research ProjectsAgency under Contract No.
FA8650-09-C-0179,Ontario/Baden-Wu?rttemberg Faculty Research Ex-change, and the University of Toronto.ReferencesNicholas Asher.
1993.
Reference to Abstract Objects inDiscourse.
Kluwer Academic Publishers, Dordrecht,Netherlands.Donna K. Byron.
2004.
Resolving pronominal refer-ence to abstract entities.
Ph.D. thesis, Rochester, NewYork: University of Rochester.Pascal Denis and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 660?669,Honolulu, Hawaii, October.
Association for Computa-tional Linguistics.Miriam Eckert and Michael Strube.
2000.
Dialogue acts,synchronizing units, and anaphora resolution.
Journalof Semantics, 17:51?89.Gill Francis.
1986.
Anaphoric Nouns.
Discourse Anal-ysis Monographs 11, Birmingham: English LanguageResearch, University of Birmingham.M.
A. K. Halliday and Ruqaiya Hasan.
1976.
Cohesionin English.
Longman Pub Group.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In In Proceedings ofthe 14th International Conference on ComputationalLinguistics, pages 539?545, Nantes, France.
Associa-tion for Computational Linguistics.Roz Ivanic.
1991.
Nouns in search of a context: A studyof nouns with both open- and closed-system character-istics.
International Review of Applied Linguistics inLanguage Teaching, 29:93?114.Thorsten Joachims.
2002.
Optimizing search engines us-ing clickthrough data.
In ACM SIGKDD Conference309on Knowledge Discovery and Data Mining (KDD),pages 133?142.Varada Kolhatkar and Graeme Hirst.
2012.
Resolv-ing ?this-issue?
anaphora.
In Proceedings of the2012 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning, pages 1255?1265, Jeju Island,Korea, July.
Association for Computational Linguis-tics.Varada Kolhatkar, Heike Zinsmeister, and Graeme Hirst.2013.
Annotating anaphoric shell nouns with their an-tecedents.
In Proceedings of the 7th Linguistic Anno-tation Workshop and Interoperability with Discourse,pages 112?121, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse re-lations.
In Proceedings of the 40th Annual Meet-ing on Association for Computational Linguistics,pages 368?375, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313?330.Christoph Mu?ller.
2008.
Fully Automatic Resolution ofIt, This and That in Unrestricted Multi-Party Dialog.Ph.D.
thesis, Universita?t Tu?bingen.Rebecca J. Passonneau.
1989.
Getting at discourse refer-ents.
In Proceedings of the 27th Annual Meeting of theAssociation for Computational Linguistics, pages 51?59, Vancouver, British Columbia, Canada.
Associationfor Computational Linguistics.Massimo Poesio, Simone Ponzetto, and Yannick Versley.2011.
Computational models of anaphora resolution:A survey.
Unpublished.Hans-Jo?rg Schmid.
2000.
English Abstract Nouns AsConceptual Shells: From Corpus to Cognition.
Topicsin English Linguistics 34.
Mouton de Gruyter, Berlin.Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.2001.
A machine learning approach to coreferenceresolution of noun phrases.
Computational Linguis-tics, 27(4):521?544.Michael Strube and Christoph Mu?ller.
2003.
A machinelearning approach to pronoun resolution in spoken di-alogue.
In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics, pages168?175, Sapporo, Japan, July.
Association for Com-putational Linguistics.Zeno Vendler.
1968.
Adjectives and Nominalizations.Mouton de Gruyter, The Hague.Bonnie Lynn Webber.
1991.
Structure and ostensionin the interpretation of discourse deixis.
In Languageand Cognitive Processes, pages 107?135.Yiming Yang and Jan O. Pedersen.
1997.
A comparativestudy on feature selection in text categorization.
InProceedings of the 14th International Conference onMachine Learning, pages 412?420, Nashville, TN.310
