Proceedings of the ACL Interactive Poster and Demonstration Sessions,pages 21?24, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsDescriptive Question Answering in EncyclopediaHyo-Jung Oh, Chung-Hee Lee, Hyeon-Jin Kim, Myung-Gil JangKnowledge Mining Research TeamElectronics and Telecommunications Research Institute (ETRI)Daejeon, Korea{ohj, forever, jini, mgjang} @ etri.re.krAbstractRecently there is a need for a QA system toanswer not only factoid questions but alsodescriptive questions.
Descriptive questionsare questions which need answers thatcontain definitional information about thesearch term or describe some special events.We have proposed a new descriptive QAmodel and presented the result of a systemwhich we have built to answer descriptivequestions.
We defined 10 DescriptiveAnswer Type(DAT)s as answer types fordescriptive questions.
We discussed howour proposed model was applied to thedescriptive question with some experiments.1 IntroductionMuch of effort in Question Answering has focusedon the ?short answers?
or factoid questions, whichanswer questions for which the correct response isa single word or short phrase from the answersentence.
However, there are many questionswhich are better answer with a longer descriptionor explanation in logs of web searchengines(Voorhees, 2003).
In this paper, weintroduce a new descriptive QA model and presentthe result of a system which we have built toanswer such questions.Descriptive question are questions such as ?Whois Columbus?
?, ?What is tsunami?
?, or ?Why isblood red?
?, which need answer that contain thedefinitional information about the search term,explain some special phenomenon.(i.e.
chemicalreaction) or describe some particular events.At the recent works, definitional QA, namelyquestions of the form ?What is X?
?, is adeveloping research area related with a subclass ofdescriptive questions.
Especially in TREC-12conference(Voorhees, 2003), they had produced 50definitional questions in QA track for thecompetition.
The systems in TREC-12(Blair et al2003; Katz et al 2004) applied complicatedtechnique which was integrated manuallyconstructed definition patterns with statisticalranking component.Some experiments(Cui et al 2004) tried to useexternal resources such as WordNet and WebDictionary associated with a syntactic pattern.Further recent work tried to use online knowledgebases on web.
Domain-specific definitional QAsystems in the same context of our works havebeen developed.
Shiffman et al2001) applied onbiographical summaries for people with data-driven method.In contrast to former research, we focus on theother descriptive question, such as ?why,?
?how,?and ?what kind of?.
We also present ourdescriptive QA model and its experimental results.2 Descriptive QA2.1 Descriptive Answer TypeOur QA system is a domain specific system forencyclopedia 1 .
One of the characteristics ofencyclopedia is that it has many descriptivesentences.
Because encyclopedia contains factsabout many different subjects or about oneparticular subject explained for reference, there are1 Our QA system can answer both factoid questions and descriptive questions.
Inthis paper, we present only sub system for descriptive QA21many sentences which present definition such as?X is Y.?
On the other hand, some sentencesdescribe process of some special event(i.e.
the 1stWorld War) so that it forms particular sentencestructures like news article which reveal reasons ormotives of the event.We defined Descriptive Answer Type (DAT) asanswer types for descriptive questions with twopoints of view: what kind of descriptive questionsare in the use?s frequently asked questions?
andwhat kind of descriptive answers can bepatternized in the our corpus?
On the view ofquestion, most of user?s frequently asked questionsare not only factoid questions but also definitionalquestions.
Furthermore, the result of analyzing thelogs of our web site shows that there are manyquestions about ?why?, ?how?, and so on.
On theother side, descriptive answer sentences in corpusshow particular syntactic patterns such asappositive clauses, parallel clauses, and adverbclauses of cause and effect.
In this paper, wedefined 10 types of DAT to reflect these features ofsentences in encyclopedia.Table 1 shows example sentences with patternfor each DAT.
For instance, ?A tsunami is a largewave, often caused by an earthquake.?
is anexample for ?Definition?
DAT with pattern of [X isY].
It also can be an example for ?Reason?
DATbecause of matching pattern of [X is caused by Y].Table 1: Descriptive Answer TypeDAT Example/PatternDEFINITION A tsunami is a large wave, often caused by an earthquake.
[X is Y]FUCTIONAir bladder is an air-filled structure in manyfishes that functions to maintain buoyancy or toaid in respiration.
[ X that function to Y]KIND The coins in States are 1 cent, 5 cents, 25 cents, and 100cents.
[X are Y1, Y2,.. and Yn]METHOD The method that prevents a cold is washing often your hand.
[The method that/of X is Y]CHARCTERSea horse, characteristically swimming in anupright position and having a prehensile tail.
[ Xis characteristically Y]OBJECTIVE An automobile used for land transports.
[ X used for Y]REASON A tsunami is a large wave, often caused by an earthquake.
[X is caused by Y]COMPONENTAn automobile usually is composed of 4 wheels,an engine, and a steering wheel.
[X is composedof Y1, Y2,.. and Yn]PRINCIPLEOsmosis is the principle, transfer of a liquidsolvent through a semipermeable membrane thatdoes not allow dissolved solids to pass.
[X is theprinciple, Y]ORIGINThe Achilles tendon is the name from themythical Greek hero Achilles.
[X is the namefrom Y]2.2 Descriptive Answer IndexingDescriptive Answer indexing process consists oftwo parts: pattern extraction from pre-taggedcorpus and extraction of DIU(Descriptive IndexingUnix) using a pattern matching technique.Descriptive answer sentences generally have aparticular syntactic structure.
For instance,definitional sentences has patterns such as ?X isY,?
?X is called Y,?
and ?X means Y.?
In case ofsentence which classifies something into sub-kinds,i.e.
?Our coin are 50 won, 100 won and 500 won.
?it forms parallel structure like ?X are Y1, Y2,.. andYn?.To extract these descriptive patterns, we firstbuild initial patterns.
We constructed pre-taggedcorpus with 10 DAT tags, then performed sentencealignment by the surface tag boundary.
The taggedsentences are then processed through part-of-speech(POS) tagging in the first step.
In this stage,we can get descriptive clue terms and structures,such as ?X is caused by Y?
for ?Reason?, ?X wasmade for Y?
for ?Function?, and so on.In the second step, we used linguistic analysisincluding chunking and parsing to extend initialpatterns automatically.
Initial patterns are too rigidbecause we look up only surface of sentences in thefirst step.
If some clue terms appear with longdistance in a sentence, it can fail to be recognizedas a pattern.
To solve this problem, we addedsentence structure patterns on each DAT patterns,such as appositive clause patterns for ?Definition?,parallel clause patterns for ?Kind?, and so on.Finally, we generalized patterns to conductflexible pattern matching.
We need to grouppatterns to adapt to various variations of termswhich appear in un-training sentences.
Severalsimilar patterns under the same DAT tag wereintegrated into regular-expression union which is tobe formulated automata.
For example, ?Definition?patterns are represented by [X<NP> becalled/named/known as Y<NP>].We defined DIU as indexing unit for descriptiveanswer candidate.
In DIU indexing stageperformed pattern matching, extracting DIU, andstoring our storage.
We built a pattern matchingsystem based on Finite State Automata(FSA).
Afterpattern matching, we need to filtering over-generated candidates because descriptive patternsare naive in a sense.
In case of ?Definition?, ?X isY?
is matched so many times, that we restrict the22pattern when ?X?
and ?Y?
under the same meaningon our ETRI-LCN for Noun ontology 2 .
Forexample, ?Customs duties are taxes that people payfor importing and exporting goods[X is Y]?
areaccepted because ?custom duty?
is under the ?tax?node so they have same meaning.DIU consists of Title, DAT tag, Value, V_title,Pattern_ID, Determin_word, and Clue_word.
Titleand Value means X and Y in result of patternmatching, respectively.
Determin_word andClue_word are used to restrict X and Y in theretrieval stage, respectively.
V_title isdistinguished from Title by whether X is an entryin the encyclopedia or not.
Figure 1 illustratedresult of extracting DIU.Title: Cold?The method that prevents a cold is washing often your hand.
?1623: METHOD:[The method that/of X is Y]The method that [X:prevents a cold] is [Y:washing often your hand]z Title: Coldz DAT tag: METHODz Value: washing often your handz V_title: NONEz Pattern_ID: 1623z Determin_Word: preventz Clue_Word: wash handFigure 1: Result of DIU extracting2.3 Descriptive Answer RetrievalDescriptive answer retrieval performs finding DIUcandidates which are appropriate to user questionsthrough query processing.
The important role ofquery processing is to catch out <QTitle, DAT>pair in the user question.
QTitle means the keysearch word in a question.
We used LSP pattern3for question analysis.
Another function of queryprocessing is to extract Determin_word orClue_Terms in question in terms of determiningwhat user questioned.
Figure 2 illustrates the resultof QDIU(Question DIU).
?How can we prevent a cold?z QTitle: Coldz DAT tag: METHODz Determin_Word: preventFigure 2: Result of Question Analysis2 LCN: Lexical Concept Network.
ETRI-LCN for Noun consists of 120,000nouns and 224,000 named entities.3 LSP pattern: Lexico-Syntactic Pattern.
We built 774 LSP patterns.3 Experiments3.1 Evaluation of DIU IndexingTo extract descriptive patterns, we built 1,853 pre-tagged sentences within 2,000 entries.
About40%(760 sentences) of all are tagged with?Definition, while only 9 sentences were assignedto ?Principle?.
Table 2 shows the result of extracteddescriptive patterns using tagged corpus.
408patterns are generated for ?Definition?
from 760tagged sentences, while 938 patterns for ?Function?from 352 examples.
That means the sentences ofdescribing something?s function formed verydiverse expressions.Table 2: Result of Descriptive Pattern ExtractionDAT # of Patterns DAT # of PatternsDEFINITION 408(22) OBJECTIVE 166(22)FUCTION 938(26) REASON 38(15)KIND 617(71) COMPONENT 122(19)METHOD 104(29) PRINCIPLE 3(3)CHARCTER 367(20) ORIGIN 491(52)Total 3,254(279)* The figure in ( ) means # of groups of patternsTable 3: Result of DIU IndexingDAT # of DIUs DAT # of DIUsDEFINITION 164,327(55%) OBJECTIVE 9,381(3%)FUCTION 25,105(8%) REASON 17,647(6%)KIND 45,801(15%) COMPONENT 12,123(4%)METHOD 4,903(2%) PRINCIPLE 64(0%)CHARCTER 10,397(3%) ORIGIN 10,504(3%)Total 300,252Table 3 shows the result of DIU indexing.
Weextracted 300,252 DIUs from the wholeencyclopedia 4  using our Descriptive AnswerIndexing process.
As expected, most DIUs(about55%, 164,327 DIUs) are ?Definition?.
We assumedthat the entries belonging to the ?History?
categoryhave many sentences about ?Reason?
becausehistory usually describes some events.
However,we obtained only 25,110 DIUs(8%) of ?Reason?because patterns of ?Reason?
have lack ofexpressing syntactic structure of adverb clauses ofcause and effect.
?Principle?
also has same problemof lack of patterns so we only 64 DIUs.3.2 Evaluation of DIU RetrievalTo evaluate our descriptive question answeringmethod, we used 152 descriptive questions fromour ETRI QA Test Set 2.05, judged by 4 assessors.4 Our encyclopedia consists of 163,535 entries and 13 main categories in Korean.5 ETRI QA Test Set 2.0 consists of 1,047 <question, answer> pairs includingboth factoid and descriptive questions for all categories in encyclopedia23For performance comparisons, we used Top 1 andTop 5 precision, recall and F-score.
Top 5 precisionis a measure to consider whether there is a correctanswer in top 5 ranking or not.
Top 1 measuredonly one best ranked answer.For our experimental evaluations we constructedan operational system in the Web, named?AnyQuestion 2.0.?
To demonstrate howeffectively our model works, we compared to asentence retrieval system.
Our sentence retrievalsystem used vector space model for query retrievaland 2-poisson model for keyword weighting.Table 4 shows that the scores using our proposedmethod are higher than that of traditional sentenceretrieval system.
As expected, we obtained betterresult(0.608) than sentence retrieval system(0.508).We gain 79.3% (0.290 to 0.520) increase on Top1than sentence retrieval and 19.6%(0.508 to 0.608)on Top5.
The fact that the accuracy on Top1 hasdramatically increased is remarkable, in thatquestion answering wants exactly only one relevantanswer.Whereas even the recall of sentence retrievalsystem(0.507) is higher than descriptive QAresult(0.500) on Top5, the F-score(0.508) is lowerthan that(0.608).
It comes from the fact thatsentence retrieval system tends to produce morenumber of candidates retrieved.
While sentenceretrieval system retrieved 151 candidates, ourdescriptive QA method retrieved 98 DIUs underthe same condition that the number of correctedanswers of sentence retrieval is 77 and ours is 76.Table 4: Result of Descriptive QASentence Retrieval Descriptive QATop l Top 5 Top 1 Top 5Retrieved 151 151 98 98Corrected 44 77 65 76Precision 0.291 0.510 0.663 0.776Recall 0.289 0.507 0.428 0.500F-score 0.290 0.508 0.520(+79.3%)0.608(+19.6%)We further realized that our system has a fewweek points.
Our system is poor for invertedretrieval which should answer to the quiz stylequestions, such as ?What is a large wave, oftencaused by an earthquake??
Moreover, our systemdepends on initial patterns.
For the details,?Principle?
has few initial patterns, so that it hasfew descriptive patterns.
This problem hasinfluence on retrieval results, too.4 ConclusionWe have proposed a new descriptive QA modeland presented the result of a system which we havebuilt to answer descriptive questions.
To reflectcharacteristics of descriptive sentences inencyclopedia, we defined 10 types of DAT asanswer types for descriptive questions.
Weexplained how our system constructed descriptivepatterns and how these patterns are worked on ourindexing process.
Finally we presented howdescriptive answer retrieval performed andretrieved DIU candidates.
We have shown that ourproposed model outperformed the traditionalsentence retrieval system with some experiments.We obtained F-score of 0.520 on Top1 and 0.680on Top5.
It showed better results when comparedwith sentence retrieval system on both Top1 andTop5.Our Further works will concentrate on reducinghuman efforts for building descriptive patterns.
Toachieve automatic pattern generation, we will try toapply machine learning technique like the boostingalgorithm.
More urgently, we have to build aninverted retrieval method.
Finally, we will comparewith other systems which participated in TREC bytranslating definitional questions of TREC inKorean.ReferencesS.
Blair-Goldensohn, K. R. McKeown, and A, H,Schlaikjer.
2003.
A Hybrid Approach for QA TrackDefinitional Questions, Proceedings of the twelveText REtreival Conference(TREC-12), pp.
336-342.H.
Cui, M-Y.
Kan, T-S. Chua, and J. Xian.
2004.
AComparative Study on Sentence Retrieval forDefinitional Question Answering, Proceedings ofSIGIR 2004 workshop on Information Retrieval 4Question Answering(IR4QA).B.
Katz, M. Bilotti, S. Felshin, et.
al.
2004.
AnsweringMultiple Questions on a Topic from HeterogeneousResources, Proceedings of the thirteenth TextREtreival Conference(TREC-13).B.
Shiffman, I. Mani, and K.Concepcion.
2001.Producing Biographical Summaries: CombiningLinguistic Resources and Corpus Statistics,Proceedings of the European Association forComputational Linguistics (ACL-EACL 01).Ellen M. Voorhees.
2003.
Overview of TREC 2003Question Answering Track, Proceedings of thetwelfth Text REtreival Conference(TREC-12).24
