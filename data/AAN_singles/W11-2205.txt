Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 35?42,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsEvaluating unsupervised learning for natural language processing tasksAndreas VlachosDepartment of Biostatistics and Medical InformaticsUniversity of Wisconsin-Madisonvlachos@biostat.wisc.eduAbstractThe development of unsupervised learningmethods for natural language processing taskshas become an important and popular areaof research.
The primary advantage of thesemethods is that they do not require annotateddata to learn a model.
However, this advan-tage makes them difficult to evaluate againsta manually labeled gold standard.
Using un-supervised part-of-speech tagging as our casestudy, we discuss the reasons that render thisevaluation paradigm unsuitable for the evalu-ation of unsupervised learning methods.
In-stead, we argue that the rarely used in-contextevaluation is more appropriate and more infor-mative, as it takes into account the way thesemethods are likely to be applied.
Finally, bear-ing the issue of evaluation in mind, we pro-pose directions for future work in unsuper-vised natural language processing.1 IntroductionThe development of unsupervised learning methodsfor natural language processing (NLP) tasks has be-come an important and popular area of research.
Themain attraction of these methods is that they canlearn a model using only unlabeled data.
This isan important advantage, as unlabeled text in digi-tal form is in abundance, while labeled datasets areusually expensive to construct.
While methods suchas crowdsourcing (Snow et al, 2008) can help re-duce this cost, in tasks for which specialist knowl-edge is required, such as part-of-speech (PoS) tag-ging or syntactic parsing, labeling datasets in thisfashion can be substantially harder.Nevertheless, the advantage of requiring only un-labeled data to learn a model renders the evaluationof unsupervised learning methods to be more chal-lenging than that of their supervised counterparts.This is primarily because the output of unsupervisedmethods does not contain labels that would be foundin a manually constructed gold standard.
Simplisti-cally expressed, no labels for model learning meansthat there are no labels in the output.
As a result, thestandard evaluation paradigm of comparing againsta gold standard using a performance measure suchas accuracy or F-score cannot be used, at least notin the way it would be used in evaluating supervisedmethods.
Since methods are proposed or rejectedby researchers, and papers describing these methodsare assessed by their peers partly on the basis of suchresults, the issue of evaluation is an important one.Before we proceed, it is important to character-ize the unsupervised learning methods we are con-sidering, as the term unsupervised is used in mul-tiple ways in the literature.
In this work we focuson methods that use only unlabeled data to learn amodel and do not involve any form of supervision atany stage.
Thus we exclude methods that use seedssuch as the dictionaries of PoS tags used by Ravi andKnight (2009) and rules for producing labeled out-put, e.g.
those proposed by Teichert and Daume?
III(2009).
We also exclude methods for which the dataused to learn a model does not contain any of thelabels we are learning to predict, but it does containother information that we use in the learning pro-cess.
For example, the multilingual PoS inductionapproach of Das and Petrov (2011) assumes no su-pervision for the language whose PoS tags are being35induced, but it assumes access to a labeled dataset ofa different language.We begin by surveying recent work on unsuper-vised PoS tagging, focusing on the issue of eval-uation (Section 2).
While PoS tagging is not theonly task for which unsupervised learning methodsare popular, its relative simplicity and the varietyof evaluation paradigms employed make it a use-ful case study.
Based on this survey, we show thatevaluation against a PoS tagging gold standard isnot only difficult, but it can be misleading as well.The reason for this is that the unsupervised learn-ing methods used, while they produce output thatcorrelates with PoS tags, perform a different task,namely clustering-based word representation induc-tion (Turian et al, 2010).
Instead, we argue thatin-context evaluation is more appropriate and moreinformative, as it takes into account the applicationcontext in which these methods are intended to beused (Section 3).
Finally, bearing the issue of evalu-ation in mind, we propose some directions for futurework in unsupervised learning for NLP (Section 4).2 The case of unsupervised part-of-speechtaggingPoS tagging is the task of assigning lexical cate-gories such as noun or verb to tokens in a sentence.It is commonly used either as an end-goal or as inter-mediate processing stage for a downstream task suchas syntactic parsing.
For languages with substan-tial amounts of labeled data available such as En-glish, the performance of supervised approaches hasreached very high levels.1 Thus, the research focushas shifted to semisupervised and unsupervised ap-proaches which would allow the processing of lan-guages which do not have similar resources avail-able.At an abstract level, the unsupervised learningmethods applied to PoS tagging take as input tok-enized unlabeled sentences, from which they learna model.
These models are either hidden Markovmodels (HMMs) (Clark, 2003; Goldwater and Grif-fiths, 2007) or clustering models (Biemann, 2006;Abend et al, 2010).
During model learning, stateidentifiers are assigned to the tokens (Figure 1a).
In-1According to the ACL wiki, state-of-the-art performance inEnglish is more than 97% per token accuracy.dependently of the learning method and the model,these identifiers are semantically void, i.e.
they haveno linguistic meaning.
Nevertheless, all the studiesconclude that there is a strong correlation betweenthe state identifiers assigned and the PoS tags in alabeled gold standard (Figure 1b).The most common way of assessing the level ofcorrelation achieved is the use clustering evaluationmeasures.
The latter operate on a confusion matrix(Figure 1c), which is constructed by assuming thateach cluster consists of all the tokens assigned thesame state identifier.
Intuitively, all clustering eval-uation measures provide definitions for the two de-sirable properties that a good clustering should pos-sess with respect to a gold standard, homogeneityand completeness.
Homogeneity represents the de-gree to which each cluster contains instances from asingle gold standard class, while completeness thedegree to which each gold standard class is con-tained in a single cluster.
Note that there tends tobe a trade-off between these two properties since,increasing the number of clusters is likely to im-prove homogeneity but worsen completeness andvice-versa.
Therefore, clustering evaluation mea-sures need to balance appropriately between them.Some authors proposed clustering evaluationtechniques that first induce the mapping fromstate identifiers to gold standard tags automaticallyand then use supervised measures to compare themapped output to the gold standard.
For example,Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags fromone half of the corpus and evaluate on the secondhalf, which is referred to as cross-validation accu-racy.
However, such techniques evaluate the clus-tering together with the induced mapping, thus thequality of the latter influences the results obtained.This can be misleading as unsupervised learningmethods for PoS tagging induce the clustering, butnot the mapping on which they are eventually eval-uated.In order to avoid the mapping induction step,the use of information theoretic measures was pro-posed instead.
These include Variation of Informa-tion (VI) (Meila?, 2007), V-measure (Rosenberg andHirschberg, 2007), and their respective variants NVI(Reichart and Rappoport, 2009) and V-beta (Vla-chos et al, 2009).
Each of these measures exhibits36There are 70 children there .1 2 3 4 1 5(a) Unsupervised PoS tagger outputThere are 70 children there .EX VBP CD NNS RB .
(b) Gold standard1 2 3 4 5EX 1 0 0 0 0VBP 0 1 0 0 0CD 0 0 0 1 0NNS 0 0 1 0 0RB 1 0 0 0 0.
0 0 0 0 1(c) Confusion matrixFigure 1: Unsupervised PoS tagging evaluation pipeline.some kind of bias towards certain solutions though,e.g.
V-measure favors clusterings with large numberof clusters, while VI exhibits the opposite behavior.While these biases might follow some reasonable in-tuitions, unsurprisingly none is universally acceptedas the most appropriate.In order to avoid these problems, Biemann et al(2007) proposed to evaluate unsupervised PoS tag-ging as a source of features for supervised learn-ing approaches to NLP tasks, such as named entityrecognition and shallow parsing.
The intuition be-hind this extrinsic evaluation is that if a task relieson discriminating between PoS labels rather than thePoS labels semantics themselves, then the state iden-tifiers obtained by an unsupervised method can beused in the same way as PoS tags obtained froma gold standard or a supervised system.
In theirexperiments they showed that the features obtainedfrom the unsupervised PoS tagger improve the per-formance in all tasks, and in particular when littletraining data is available.Van Gael et al (2009) evaluated the output of dif-ferent configurations of their unsupervised PoS tag-ging approach both by comparing it against a goldstandard via clustering evaluation measures and byusing it as a source of features for shallow pars-ing.
Table 1 summarizes the results of their exper-iments.
In agreement with Biemann et al (2007),they found that the features provided by the unsu-pervised PoS tagger improved shallow parsing per-formance.
However, they observed that the cluster-ing evaluation scores did not correlate with the re-sults of this extrinsic evaluation.
In other words,better clustering evaluation scores did not alwaysresult in better features for shallow parsing.
VanGael et al noted that homogeneity correlated bet-ter with shallow parsing performance, hypothesizingit is probably worse to assign the same state identi-fier to tokens that belong to different PoS tags, e.g.verb and adverbs, rather than to generate more thanone state identifier for the same PoS.
In the samespirit, Christodoulopoulos et al (2010) used the out-put of a number of unsupervised PoS tagging meth-ods to extract seeds for the prototype-driven modelof Haghighi and Klein (2006).
Like Van Gael etal., they also found that better clustering evaluationscores did not result in better seeds.Given these results, as well as remembering thatunsupervised learning methods do not use any la-bel information in model learning, one is entitledto question whether it is reasonable to expect theiroutput to match a particular labeled gold standard.Why not assume that the state identifiers obtainedcorrelate with named entity recognition tags or cat-egorial grammar tags instead of PoS tags, tasks forwhich sequential models are very common?
Evenif the state identifiers induced correlate better withPoS tags than with other kinds of annotation, eval-uating them using a PoS tagging gold standard andeven naming the task unsupervised PoS tagging orinduction is probably misleading.
We argue thatthe task performed by the unsupervised PoS tag-ging methods proposed is more accurately describedas clustering-based word representation induction37homogeneity completeness VI V-measure V-beta F-score accuracyDP-learned 69.39 51.21 4.19 58.93 55.37 90.98 94.48DP-fixed 51.80 54.84 3.94 53.27 52.88 89.99 93.89PY-fixed 62.02 56.25 3.74 59.00 58.79 90.31 94.15no PoS - - - - - 93.81 96.07supervised PoS - - - - - 88.58 93.25Table 1: Summary the results reported for the three configurations (DP-learned, DP-fixed, PY-fixed) of theunsupervised PoS tagger of Van Gael et al (2009) and the two baselines (no PoS tags, supervised PoS tags).Except for VI, higher scores mean better performance.
The clustering evaluation scores (VI, V-measure, V-beta) are obtained by comparing against a PoS gold standard, while F-score and accuracy scores are obtainedby extrinsinc evaluation using shallow parsing.
(Turian et al, 2010), and that this should be takeninto account in the evaluation.
As further evidenceof the relation between the two tasks, note that someof the unsupervised PoS tagging methods applied byChristodoulopoulos et al (2010) were also used byTurian et al (2010) for clustering-based word repre-sentation induction.3 In-context evaluationAll the papers on unsupervised PoS tagging men-tioned in the previous section agree on the fact thatits evaluation, at least using clustering evaluationmeasures, is difficult.
This is an important problemfor other NLP tasks (e.g.
anaphora resolution, wordsense induction) in which systems produce clustersthat need to be mapped to gold standard classes.
Intheir recent position paper, Guyon et al (2009) arguethat the problem lies in ignoring the context in whichclustering is performed.
They distinguish betweentwo such contexts.
The first one is the use of cluster-ing as a pre-processing step for a downstream task,in which the evaluation of the latter is used to eval-uate the former.
The second context is that of dataexploration in order to assist a human to analyze alarge dataset.
In this case, performance might not beas straightforward to assess, since it relies on manyexternal factors among which the human computerinteraction interface used is likely to be crucial.
Wecumulatively refer to these evaluation paradigms asin-context evaluation.Returning to unsupervised PoS tagging and NLP,the extrinsic evaluation of Biemann et al (2007) andVan Gael et al (2009) falls under the pre-processingparadigm.
The approach of Christodoulopoulos etal.
(2010) falls between pre-processing and data ex-ploration, as the clusters of tokens produced aresemi-automatically processed in order to produceseeds which were then used by the prototype-drivenmodel of Haghighi and Klein (2006).2 In-contextevaluation can be used to assess the performance ofunsupervised learning methods for tasks other thanclustering-based word representation approaches.For example, topic modeling (Blei et al, 2003) hasrecently been used and evaluated in approaches tolearning models of selectional preferences (Ritter etal., 2010; O?
Se?aghdha, 2010).The issues affecting the evaluation of unsuper-vised learning methods are not restricted to PoS tag-ging.
Schwartz et al (2011) discussed similar issuesin the context of unsupervised dependency parsing.Note that some of them arise due to the fact unsu-pervised dependency parsing produces unlabeled di-rected edges which are interpreted as denoting head-dependent relations.
However, there are linguisticphenomena where unless the edges are labeled witha specific interpretation, both directions could beconsidered correct, e.g.
the relation between modalverb and main verb.
Even though evaluation againsta syntactic parsing gold standard is useful, we arguethat in-context evaluation of the output of unsuper-vised dependency parsers is likely to be more infor-mative and more appropriate.Despite the criticism against clustering evaluationmeasures as well as other methods for comparing the2Note that while evaluating in-context, these authors still re-fer to the task performed as PoS tagging or induction and someof their conclusions are drawn via comparisons against a PoStagging gold standard.38output of unsupervised learning methods against agold standard, we argue that they are still useful.
Thevarious measures proposed, along with their inher-ent biases and definitions of clustering quality, pro-vide quantitative analysis of the behavior of unsu-pervised learning methods by assessing correlationsbetween their output and a gold standard.
This canbe very useful when developing such methods, astheir use is admittedly simpler than the in-contextevaluation paradigms discussed.
However, they arenot as informative as in-context evaluation and theyshould not be used to draw strong conclusions aboutthe usefulness of a method.Acknowledging that the evaluation of unsuper-vised learning for NLP is better performed in-context instead of against a labeled gold standardleads to the use of more appropriate experimen-tal setups.
Sometimes unsupervised learning meth-ods are restricted to learning models using the un-labeled gold standard against which they are evalu-ated subsequently.
Thus, they neither take full ad-vantage of nor they demonstrate their main strength,which is that they can use as much data as possi-ble.
Using the pre-processing paradigm, clustering-based word representations induced from a largeunlabeled dataset would be evaluated according towhether they improve the performance of the down-stream task they are evaluated with, whose evalua-tion is likely to be on a different dataset.
This use ofclustering-based word representation is sometimesreferred to as semi-supervised learning and has beenshown to be effective in a variety of tasks, includingnamed entity recognition, shallow parsing and syn-tactic dependency parsing (Koo et al, 2008; Turianet al, 2010).The use of large datasets would also help as-sess the scalability of the unsupervised methods pro-posed, as the amount of data that can be handled ef-ficiently by an unsupervised method can be as im-portant as the range of linguistic intuitions it cancapture.
To examine this trade-off, it would be in-formative to show performance curves with differ-ent amounts of data, which should be straightfor-ward to produce under the pre-processing evalua-tion paradigm.
An added benefit is that, as discussedby Ben-Hur et al (2002), assessing clustering stabil-ity using multiple runs and sub-samples of a datasetcan help establish whether a particular combinationof clustering algorithm and user-defined parameters(including the number of clusters to be discovered)is able to discover an appropriate clustering of thedataset considered.Avoiding comparisons against a labeled gold stan-dard would also remove the temptation of adaptingit to the output of the unsupervised learning method.For example, in unsupervised PoS tagging authorssometimes simplify the gold standard by collapsingthe original 45 PoS tags of the Penn treebank to 17,e.g.
by removing the distinctions between differentnoun tags.
While such simplifications are linguisti-cally plausible, they substitute one problem for an-other, as methods are no longer penalized for miss-ing some of the finer distinctions, but they are pe-nalized for making them.
Perhaps more importantly,they result in fitting the gold standard to the outputof the method being evaluated, which is unlikely tobe informative.Another related issue is that since unsupervisedlearning methods do not need labeled data, it is atempting and common practice to learn a model andreport results on the same dataset, which usuallyconsists of all the labeled data available and whichis used to tune the parameters of the method evalu-ated.
This is equivalent to reporting results for su-pervised learning methods on the development set,while it is generally accepted that results on a sepa-rate test set on which no parameter tuning is allowedprovide better performance estimates.
The use ofthe pre-processing evaluation paradigm with a su-pervised learning approach for the downstream taskis likely to result in use the standard distinction be-tween training, development and test set for the eval-uation of unsupervised learning methods.4 Directions for future workWhile the previous sections have focused on whyunsupervised learning for NLP tasks is hard to eval-uate, our intention is not to discourage further re-search, but to encourage it.
Unsupervised learningcan help exploit the large amounts of unlabeled textthat are available.
For this purpose though we needappropriate evaluation, and we argue that in-contextevaluation is likely to be more informative than theevaluation against a gold standard.A potential problem is that in-context evaluation39adds an extra layer in the experimental setup, eitherin the form of a downstream task or of a human-computer interaction study.
This can make compar-isons between methods harder as there are more ex-perimental conditions to control for and discourageresearchers from adopting it.
Therefore, it wouldbe useful to have a shared task that would providean experimental setup that can be re-used.
Sharedtasks have been beneficial in cases where the exis-tence of multiple datasets and task definitions hin-dered progress and we would expect them to have asimilar effect on unsupervised learning methods.As different application contexts are likely to ben-efit from different solutions, this naturally leads tothe development of modeling approaches that areadaptable, preferably in ways that enable experts toincorporate their knowledge.
This research directionhas already been pursued in clustering (Wagstaff andCardie, 2000; Basu et al, 2006) and more recentlyin topic modeling (Blei and Mcauliffe, 2008; An-drzejewski et al, 2011).
We argue though that thewider adoption of in-context evaluation will help as-sess their performance and merits in a more infor-mative way.
An alternative approach to accommo-date for the needs of different application contextsis to induce multiple clusterings simultaneously forthe same dataset as proposed by Dasgupta and Ng(2010) in the context of text classification.
Such con-siderations are particularly relevant to NLP applica-tions as language exhibits ambiguity and polysemy,which are rather difficult to capture in a context-independent labeled gold standard.If in-context evaluation must be avoided, it is ad-visable to focus on tasks for which most applica-tion contexts would agree on the clustering or latentstructure that must be discovered, such as the WebPeople Search (Artiles et al, 2010) task on clus-tering webpages about persons who share the samename.
Even in this case though, in-context evalua-tion as pre-processing for an information extractionsystem or as a visualization component in an inter-face for exploring web pages is still likely to be in-formative.Finally, in this paper we considered methodswhose output consists of state identifiers which aresemantically void.
However, obtaining meaningfullabels such as those found in a gold standard is auseful and important goal in many NLP tasks.
How-ever, this purpose is better served by injecting ap-propriate supervision to the model, instead of tryingto achieve it as an afterthought.
Such approaches in-clude the use of PoS dictionaries by sequential tag-ging models (Haghighi and Klein, 2006; Ravi andKnight, 2009), the use of labeled data from differ-ent languages (Snyder et al, 2008; Das and Petrov,2011) or the (possibly indirect) assignment of labelsto topics (Ramage et al, 2009; Zhu et al, 2009).
Re-search in unsupervised learning methods is likely tobenefit these partially supervised ones, as they bothseek to take advantage of unlabeled data.
As the out-put of such methods uses the same labels as thosefound in the gold standard, they can be evaluatedagainst a labeled gold standard.5 ConclusionsIn this position paper, we discussed the issue of eval-uation of unsupervised learning methods for NLPtasks.
Using PoS tagging as our case study, we ex-amined recent attempts of evaluating unsupervisedapproaches and showed that a lot of confusion iscaused due to evaluating their output against a la-beled gold standard.
Instead, we argue that it ismore appropriate to evaluate unsupervised meth-ods in context, either as a pre-processing step fora downstream task or as a tool for data exploration.Following this, we proposed that future work shouldfocus on adapting to and evaluating unsupervisedlearning methods in the context in which they areintended to be used and that a shared task would fa-cilitate research in this direction.
Finally, we hopethat the adoption of in-context evaluation will resultin the development of improved unsupervised learn-ing methods for NLP tasks, so that researchers andpractitioners can exploit the large amounts of textualdata available.AcknowledgmentsThe author would like thank Mark Craven and Diar-muid O?
Se?aghdha for helpful comments and discus-sions.
The author was funded by NIH/NLM grantR01 / LM07050.ReferencesOmri Abend, Roi Reichart, and Ari Rappoport.
2010.Improved unsupervised POS induction through pro-40totype discovery.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1298?1307.David Andrzejewski, Xiaojin Zhu, Mark Craven, andBenjamin Recht.
2011.
A framework for incorporat-ing general domain knowledge into latent Dirichlet allocation using first-order logic.
In Proceedings of the22nd International Joint Conferences on Artificial In-telligence.Javier Artiles, Andrew Borthwick, Julio Gonzalo, SatoshiSekine, and Enrique Amigo?.
2010.
WePS-3 evalu-ation campaign: Overview of the web people searchclustering and attribute extraction tasks.
In Proceed-ings of the Conference on Multilingual and Multi-modal Information Access Evaluation.Sugato Basu, Mikhail Bilenko, Arindam Banerjee, andRaymond J. Mooney.
2006.
Probabilistic semi-supervised clustering with constraints.
In O. Chapelle,B.
Schoelkopf, and A. Zien, editors, Semi-SupervisedLearning, pages 73?102.
MIT Press.Asa Ben-Hur, Andre?
Elisseeff, and Isabelle Guyon.2002.
A stability based method for discovering struc-ture in clustered data.
In Proceedings of the PacificSymposium on Biocomputing, pages 6?17.Chris Biemann, Claudio Giuliano, and Alfio Gliozzo.2007.
Unsupervised part-of-speech tagging support-ing supervised methods.
In Proceedings of the In-ternational Conference in Recent Advances in NaturalLanguage Processing.Chris Biemann.
2006.
Unsupervised part-of-speech tag-ging employing efficient graph clustering.
In Proceed-ings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics: Student Re-search Workshop, pages 7?12.David Blei and Jon Mcauliffe.
2008.
Supervised topicmodels.
In Proceedings of the 22nd Annual Confer-ence on Neural Information Processing Systems.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022, January.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsupervisedPOS induction: how far have we come?
In Proceed-ings of the 2010 Conference on Empirical Methods inNatural Language Processing, pages 575?584.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of the 10th Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, pages 59?66.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies.Sajib Dasgupta and Vincent Ng.
2010.
Mining clusteringdimensions.
In Proceedings of the 27th InternationalConference on Machine Learning, pages 263?270.Jianfeng Gao and Mark Johnson.
2008.
A comparison ofBayesian estimators for unsupervised hidden Markovmodel POS taggers.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 344?352.Sharon Goldwater and Tom Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 744?751.Isabelle Guyon, Ulrike Von Luxburg, and Robert C.Williamson.
2009.
Clustering: Science or art.
InNIPS 2009 Workshop on Clustering Theory.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings of Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association of Computa-tional Linguistics, pages 320?327.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of the 46th Annual Meeting of the Associa-tion of Computational Linguistics: Human LanguageTechnologies, pages 595?603.Marina Meila?.
2007.
Comparing clusterings?an infor-mation based distance.
Journal of Multivariate Analy-sis, 98(5):873?895.Diarmuid O?
Se?aghdha.
2010.
Latent variable modelsof selectional preference.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 435?444.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled LDA: a su-pervised topic model for credit attribution in multi-labeled corpora.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 248?256.Sujith Ravi and Kevin Knight.
2009.
Minimized modelsfor unsupervised part-of-speech tagging.
In Proceed-ings of the Joint Conference of the 47th Annual Meet-ing of the ACL and the 4th International Joint Confer-ence on Natural Language Processing of the AFNLP,pages 504?512.Roi Reichart and Ari Rappoport.
2009.
The NVI clus-tering evaluation measure.
In Proceedings of the13th Conference on Computational Natural LanguageLearning, pages 165?173.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A la-tent Dirichlet alocation method for selectional prefer-ences.
In Proceedings of the 48th Annual Meeting of41the Association for Computational Linguistics, pages424?434.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clusterevaluation measure.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 410?420.Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-poport.
2011.
Neutralizing linguistically problematicannotations in unsupervised dependency parsing eval-uation.
In Proceedings of the 49th Annual Meeting ofthe Association of Computational Linguistics.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and Fast ?
But is it Good?Evaluating Non-Expert Annotations for Natural Lan-guage Tasks.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural Language Process-ing, pages 254?263.Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, andRegina Barzilay.
2008.
Unsupervised multilinguallearning for pos tagging.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 1041?1050.Adam R. Teichert and Hal Daume?
III.
2009.
Unsuper-vised part of speech tagging without a lexicon.
InNIPS Workshop on Grammar Induction, Representa-tion of Language and Language Learning.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 384?394.Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahra-mani.
2009.
The infinite HMM for unsupervisedPoS tagging.
In Proceedings of 2009 Conference onEmpirical Methods in Natural Language Processing,pages 678?687.Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-mani.
2009.
Unsupervised and Constrained DirichletProcess Mixture Models for Verb Clustering.
In Pro-ceedings of the EACL workshop on GEometrical Mod-els of Natural Language Semantics.Kiri Wagstaff and Claire Cardie.
2000.
Clustering withinstance-level constraints.
In Proceedings of the 17thInternational Conference on Machine Learning, pages1103?1110.Jun Zhu, Amr Ahmed, and Eric P. Xing.
2009.MedLDA: maximum margin supervised topic modelsfor regression and classification.
In Proceedings ofthe 26th Annual International Conference on MachineLearning, pages 1257?1264.42
