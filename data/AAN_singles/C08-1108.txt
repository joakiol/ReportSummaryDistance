Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 857?864Manchester, August 2008Experiments with Reasoning for Temporal Relations between EventsMarta Tatu and Munirathnam SrikanthLymba CorporationRichardson, Texas, United Statesmarta,srikanth@lymba.comAbstractFew attempts have been made to inves-tigate the utility of temporal reasoningwithin machine learning frameworks fortemporal relation classification betweenevents in news articles.
This paper presentsthree settings where temporal reasoningaids machine learned classifiers of tempo-ral relations: (1) expansion of the datasetused for learning; (2) detection of inconsis-tencies among the automatically identifiedrelations; and (3) selection among multipletemporal relations.
Feature engineering isanother effort in our work to improve clas-sification accuracy.1 IntroductionIn recent years, there has been a growing inter-est in temporal information extraction, as moreand more operational natural language processing(NLP) systems demand dealing with time-relatedissues in natural language texts.
Machine learning-based temporal relation identification has been ex-plored by only a few researchers, including Bogu-raev and Ando (2005), Mani et al (2006), Cham-bers et al (2007), and the TempEval 2007 partici-pants (Verhagen et al, 2007).For a given ordered pair of elements (x1, x2),where x1and x2are events or times, temporal re-lation resolution is the task of automatic identifi-cation of the relation ri?
TempRel that tem-porally links x1and x2.
For example, giventhe statement Mr. Antar was chargede137lastmontht237in a civil suite138filede140in federalc?
Lymba Corporation 2008.
Licensed under the Cre-ative Commons Attribution-Noncommercial-Share Alike 3.0Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.court in Newark by the Securities and ExchangeCommission (wsj 07781) and the pairs (e137, t137),(e137, e138), and (e138, e140), the task is to automat-ically label the given pairs with the is included,is included, and simultaneous relations, respec-tively.
We note that the granularity of the temporalrelations (TempRel) varies from TimeML?s 14 re-lations to TempEval?s three coarse-grain relations.While machine learning approaches attempt toimprove classification accuracy through featureengineering, Mani et al (2006) introduced a tem-poral reasoning component to greatly expand thetraining data.
By computing the temporal closureof the training data relations, they increased thetraining set by a factor of 10.
They reported en-couraging accuracy of classification on event-eventand event-time relations.
According to their ex-periments, the event-event relation accuracy goesfrom 62.5% to 94.95% and the event-time rela-tion accuracy ranges from 73.68% to 90.16%.
Re-cently, extensions of Mani et al (2006)?s researchis briefly described in (Mani et al, 2007).
Thistechnical report addresses two problems foundin (Mani et al, 2006): (1) feature vector dupli-cation caused by the data normalization process(once fixed, the accuracy drops to 76.56% and83.23%) and (2) a somewhat unrealistic evaluationscheme (we describe Mani et al (2007)?s results inSection 4.1).TempEval 2007 is the first standard evaluationarena that consists of three temporal relation clas-sification tasks (Verhagen et al, 2007).
The par-ticipants reported F-measure scores ranging from42% to 55% for event-event relations, and 73% to80% for event-time relations.Because of their different experimental settings,1All examples shown here are taken from TimeBank 1.2.857the results reported in (Mani et al, 2007) cannotbe directly compared with those of TempEval 2007participants.
Among others, the three major differ-ences are:1. significantly different training and testingdata.
Although the datasets used Time-Bank 1.2 (Pustejovsky et al, 2003), Maniet al (2007) added the AQUAINT Corpus(www.timeml.org) to their experimental data;2. different sets of temporal relations.
Mani etal.
(2006; 2007) target six normalized rela-tions (before, immediately before (ibefore),includes, ends, begins and simultaneous).
InTempEval 2007, a set of three coarse-graintemporal relations was used (before, after,and overlap).3. different relation scope.
In (Mani et al, 2006;Mani et al, 2007), event-event temporal re-lations are discourse-wide, i.e.
any pair ofevents can be temporally linked.
For Tem-pEval 2007, the event-event relations are re-stricted to events within two consecutive sen-tences.These two modeling frameworks for solving theproblem of temporal relation classification pro-duce highly dissimilar results.
With this in mind,we are interested in two issues in this paper: (1)How might temporal reasoning assist in tempo-ral relation identification?
(2) What other featuresmight be used to improve the performance of clas-sification?
As a byproduct of our exploration tothese two questions, we hope to find some insightson why the same problem explored under differentenvironment produces highly divergent results.In this paper, we investigate several interactionsbetween temporal reasoning and a machine learn-ing approach for temporal ordering of events innatural language texts.
We continue by describingthe data used for our experiments.
In Section 3, webriefly describe the set of features we currently useto build Support Vector Machine (SVM) (Changand Lin, 2001) and Maximum Entropy (ME) mod-els for temporal relation resolution.
The three in-teractions we envision between temporal reasoningand the learned models are presented in Section 4.In conclusion, we present a discussion of our ex-perimental results and future research directions.2 Data Preparation and Analysis2.1 TimeBank 1.2In this paper, we use the TimeBank 1.2 data (Puste-jovsky et al, 2003).
This is the first attempt tocreate a corpus with human annotated temporal re-lations.
It contains 183 news documents collectedfrom several news agencies.2.2 Data normalizationSimilar to (Mani et al, 2006; Mani et al, 2007),we use a normalized version of the 14 tempo-ral relations annotated in TimeBank where theinverse relations are removed and simultaneousand identity are collapsed as well as during andis included.
The distribution of the normalizedevent-event temporal relations annotated in thedata we used for training our temporal resolutionmodels is shown in Table 2.2.3 Experimental dataFor the experiments described in this paper, weused a random 80-20 percent split of the TimeBankdata to train and test the learned classifiers (36 ran-domly selected documents for testing and the re-maining 147 for training the models) and 5-fold-cross-validation of the training data for parame-ter tuning.
We note that our experimental setupis closer to the one used in (Mani et al, 2006;Mani et al, 2007).
Noting that we do not use theAQUAINT Corpus in our experiments, our resultscan be compared with theirs, but not with the Tem-pEval system performances.3 Feature EngineeringAs reported by participants in TempEval2007 (Verhagen et al, 2007), (Boguraev andAndo, 2005), (Chambers et al, 2007), and (Maniet al, 2007), most of the features used for learningare syntactic attributes extracted for single eventterms.
In our work, we have experimented withsemantic features and attributes which take theevent?s linguistic context into consideration (Minet al, 2007).
Our experiments show that onlyfew features are critical and impact the classifier?saccuracy (Table 1).
These include the basicfeatures available in TimeBank, e.g.
event-class,tense, aspect, polarity, modality, stem, and part ofspeech of event terms (Baseline row in Table 1).Additional features that we explored include:858Feature set Accuracy (%)Baseline 46.3Baseline with sameActor +0.4Baseline with eventCoref +0.2Baseline with oneSent +4.0Baseline with relToDocDate +0.2Baseline with tensebigram +0.8Baseline with tensetrigram +0.6Baseline with all 57.4Table 1: New features impact1.
sameActor.
This binary feature indicateswhether the two events share the same se-mantic role AGENT.
The motivation behindthis feature is that two event terms, especially,verbs, which have the same agent, have acloser semantic relationship and, accordingly,are temporally related.2.
eventCoref.
This binary attribute captures theevent co-reference information.
If two eventsco-refer, even though they have different sur-face forms, they must take place simultane-ously.
For instance, the offer and deal events,mentioned in the following sentences, refer tothe same transaction and, therefore, must belinked by a simultaneous relation.a) Sony Corp. completed its tender offer for ColumbiaPictures Entertainment Inc., with Columbia sharehold-ers tendering 99.3% of all common shares outstandingby the Tuesday deadline.b) Sony Columbia Acquisition Corp., formed for theColumbia deal, will formally take ownership of themovie studio later this month, a spokesman said.3.
oneSent.
This binary feature is true if the twoevents are part of the same sentence.
Cham-bers et al(2007) introduced this feature intheir experiments, and our analysis shows thatthis attribute has a relatively larger contribu-tion to the overall performance.
The intu-ition behind this feature is that the closer twoevents get, the closer their temporal relation-ship is.4.
relToDocDate.
This feature encodes the tem-poral relation between each event and theDocument Date.
This was one of the sub-tasks of TempEval 2007 and we used this re-lationship as a feature.
Our motivation is thatwe might be able to infer the relationship be-tween two events e1and e2from the tem-poral relations they have with the DocumentDate.
For example, if before(e1, DocDate)and after(e2, DocDate) are true, thenbefore(e1, e2).
There may be two reasons forthe low impact of this feature: (1) an accu-rate computation of the temporal relation be-tween an event and the Document Date is noteasy, as demonstrated in TempEval 2007 and(2) if two events have the same relation withthe Document Date, there is no way to deter-mine the event-event relation.5.
tenseBigram and tenseTrigram.
Going be-yond using the tense attribute for single eventterms, we extract bigrams and trigrams withthe tense values of the current event and im-mediately preceding and following events.This feature is intended to reflect the tenseshifts of sequential events as part of a largercontext of the current event.All these features have a positive impact on theperformance of the learned classifiers (Table 1).Further improvement is desired and we use tem-poral reasoning in three different settings in an at-tempt to obtain more accurate temporal relations.4 Temporal ReasoningFollowing our feature set improvements for ma-chine learned models of temporal relations, weturned to temporal reasoning and explored differ-ent ways in which it can aid the resolution of tem-poral relations.
We experimented with three dif-ferent interactions between our temporal reasoningand temporal relation resolution modules.Our natural language reasoning engine (Tatuand Moldovan, 2005; Tatu and Moldovan, 2007)makes use of (1) a first-order logical representationof the input document which captures the conceptsmentioned in the text, their attributes includingnamed entity class values, event class or normal-ized values (for times) and the syntactic as well asthe semantic dependencies between concepts2; (2)a rich set of axioms which encode the knowledgeneeded to derive meaningful information from adocument; (3) a logic prover which operates in aproof by contradiction manner (a hypothesis H isentailed by a text T assumed to be true, denotedby T ?
H , if and only if (T ?
?H) ?
?, where ?is false).
Given the logical transformation of a textT , the prover uses the knowledge encoded in the2These dependencies include the temporal relations iden-tified either by human annotators or by the models presentedin Section 3.859axioms (Bk) to derive new information (T ?)
aboutT3 and scores the best mapping of the hypothesisH to T ?.For the temporal relation resolution experimentspresented in this paper, we are interested in de-riving additional temporal information from an in-put document without checking the entailment be-tween this document and a hypothesis.
There-fore, for the following tasks, the text T is aTimeBank 1.2 document and the set of axiomsBk used by the prover contains 94 temporalaxioms which link each temporal relation withits inverse (R?1(x, y) ?
R(y, x), for examplebefore(x, y) ?
after(y, x)) and define the tem-poral relation resulting from the combination oftwo relations (R1(x, y) ?
R2(y, z) ?
R3(x, z),for example, before(x, y) ?
before(y, z) ?before(x, z)).
These axioms were derived fromAllen?s interval algebra (Allen, 1983).We note that the prover computes and uses tem-poral relations between any two temporal expres-sions mentioned in an input TimeBank document4(e.g.
now [19891101] and last year [1988] arelinked by a before temporal relation in wsj 0324).Within this reasoning setup, the information de-rived by the prover (T ?)
will include the temporalclosure of the input text?s relations.
We note thatthe temporal closure includes event-event, event-time and time-time temporal relations.
We alsonote that the temporal axioms are considered 100%accurate (if the temporal relations given as inputare correct, then the temporal relations derived us-ing the axioms are also correct).4.1 Training data expansionOur first effort to create more accurate temporal re-lation resolution classifiers given our temporal rea-soning engine is to augment the gold training datawith new relations from the temporal closure of therelations identified by human annotators.
There-fore, given the 3,527 temporal relations annotatedin the TimeBank data used to train our initial tem-poral resolution models, we derived 12,270 newrelations (an increase of 3.47 times).
We show inTable 2 statistics of the normalized event-event re-lations for both the original and the closed train-ing data.
We note that the temporal inconsisten-cies identified in the original training data (by the3T?
contains all the information the prover can derivefrom T given the axioms Bk.4We restricted the time-time relations to only before, si-multaneous, and after.Relation Original data Closed (?)
dataFreq.
% Freq.
%ibefore 51 2.06 137 1.59begins 52 2.10 119 1.38ends 61 2.47 125 1.45includes 434 17.59 1,161 13.47before 885 35.88 3,165 36.73simultaneous 983 39.86 3,909 45.36Total 2,466 100.00 8,616 100.00Table 2: Normalized training data (event-event re-lations)procedure described in Section 4.2) were resolvedmanually by one of the authors of this paper.We built SVM and ME models from the totalof 8,616 normalized temporal relations using theset of 15 features described in Section 3.
Table 3shows the performance of the learned models onthe test data (original data as well as closed testdata).
Unlike (Mani et al, 2006), the accuracy ofAccuracy for event-event relationsTraining data Original test Closed test Train(845) (4,189)ME modelsOriginal (2,466) 50.4 46.1 83.3Closed (8,616) 47.0 41.0 76.1SVM modelsOriginal (2,466) 56.9 45.8 74.2Closed (8,616) 52.4 52.0 77.5Table 3: Event-event temporal resolutionthe learned classifiers drops when they are trainedon the closed training dataset.
By analyzing the re-sults from Table 3, one cannot help but notice thehigh accuracy on the data used for training and thesignificant difference between the performance onthe training and testing datasets.
This may suggestthat (1) the machine learners overfit the models onthe training data and they are not able to gener-alize and resolve the relations in the test data5 or(2) the two datasets are very different (in termsof feature values) and the data split happened tocreate a training data which is not (fully) charac-teristic to the problem we are trying to solve (thetwo datasets have different distributions).
There-fore, we measured the accuracy of ME models forevent-event relation resolution using 5-fold-cross-validation of the entire TimeBank data (Table 4).For these experiments, each TimeBank document(with all its temporal relations) was used as part5The accuracy of the SVM models is lower on the trainingdata when compared with the ME models while their perfor-mance on the test dataset is better.860Test data Train data1/5 of the data remaining 4/5s5-fold-cross split at the document levelOriginal (3,311) 57.4 89.5Closed (11,530) 58.2 85.55-fold-cross split at the relation levelOriginal (3,311) 58.3 90.0Closed (11,530) 73.4 85.3Table 4: Average ME accuracy for event-event re-lations using 5-fold-cross-validation on the entireTimeBank dataof either the training or the testing dataset.
Our re-sults for the random 5-fold-cross split of the data atthe document level are similar to the ones obtainedfor the models learned on the pre-established train-ing data (top two rows in Table 4).
Thus, our ini-tial split of the data was not an ?unfortunate?
divi-sion.
The same significant difference between theperformance on the unseen data and the trainingset can be seen.
This suggests that some overfit-ting occurs.
Features, such as the event term stem,with a large number of possible values mislead themachine learning algorithms and the models theycreate are not able to correctly classify event pairswith unseen values for these high-valued features.For instance, showdown is part of a single Time-Bank document (AP900816-0139) and the mod-els learned using other documents will misclassifyshowdown?s temporal relations.
We note that, byexpanding the training data using its temporal clo-sure, no new events are added to the training set,only new temporal relations between the same setof events are added.
Long-term solutions include(1) the expansion of the annotated data or (2) thereduction in the number of values for certain fea-tures (for example, by generalizing the event termstem to its WordNet hypernym).
In an attempt tohomogenize the feature values for the training andthe testing datasets, we split the set of normalizedevent-event temporal relations annotated in Time-Bank into training and testing without consideringthe document boundaries.
The performance of thelearned classifiers increases by 1% when trainedon unclosed data and by more than 15% when theclosed data is used (Table 4).In their most recent technical paper, Mani etal.
(2007) revise their evaluation method and reportperformance values for classifiers learned by par-titioning the data at the document level (accuracydrops from 59.68% to 51.14% when closed train-ing data is used).
These results are consistent withour findings.
In the near future, we shall experi-ment with the second solution we propose above.4.2 Testing data validationGiven that almost half of the temporal relationsautomatically identified for the testing data areincorrect when compared to the gold annotation,we decided, as our second experiment, to usetemporal reasoning to find temporal inconsisten-cies and replace some of the relations contribut-ing to the inconsistency by the immediate lowerconfidence relation returned by the learned classi-fiers.
For this purpose, we use an additional setof 77 temporal axioms which encode the irreflex-ivity of temporal relations (?R(x, x), for exam-ple, ?before(x, x)) and their empty intersections(R1(x, y) ?
?R2(x, y) when R16= R2, for ex-ample before(x, y) ?
?simultaneous(x, y)).Our process of testing data validation is itera-tive.
Once a temporal inconsistency is identifiedin the test data, it is resolved and the procedurewhich computes the temporal closure is re-started.A temporal inconsistency in a TimeBank docu-ment (T ) is detected every time ?
?
T ?.
Theautomatically identified temporal relations (part ofthe text T ) which contributed to the derivation of?
become candidates for the resolution of the in-consistency6.
These candidates are sorted basedon the confidence assigned by the machine learn-ing algorithm7 and the lowest confidence relationis replaced either by the temporal relation foundby the prover which directly contradicted the auto-matically identified relation8 (Figure 1(a)) or, forthe cases where such a relation does not exist, bythe immediate lower confidence relation identifiedby the learned models (Figure 1(b)).If, for example, for the statement The US isbolstering its military presence in the gulf, asPresident Clinton discussede1the Iraq crisis withthe one ally who has backede2his threate3offorce, British prime minister Tony Blair, the MEclassifier built in Section 3 identifies the tempo-ral relations before(e2, e1) (confidence: 0.53),before(e3, e2) (0.47) and includes(e3, e1) (0.42),the prover identifies the temporal inconsistency6The temporal closure axioms are accurate and do not ?in-troduce?
incorrect temporal relations.7For all experiments which exploit the confidence as-signed by the machine learning algorithm, we use the learnedME models (SVM models do not provide a confidence fortheir decision).8The confidence of a relation derived by the prover is theaverage of the its ?parent?s confidence values.861event1event3event4event2 c2cminc3c1 R34R23R12 R?13R?14R14(a) R14replaced by R?14event1event3event4event2 cminc4c3c1 R34R23R12 R?13R?14R14(b) R23replaced by the nextbest relation identified for(event2, event3)Figure 1: Temporal inconsistency resolutiongenerated by these three relations and replaces thelowest confidence relation (includes(e3, e1)) withthe relation it derives from the closure of the othertwo relations (before(e3, e1), confidence: 0.50).We note that, during the inconsistency checkingprocess, all types of temporal relations are used(event-event, event-time and time-time).
For thisinconsistency resolution process, we make the as-sumption that only one of the temporal relationswhich generated the inconsistency is incorrect andshould be replaced.For the testing dataset described in Section 2.3,the validation algorithm found inconsistencies inonly 25% of the test documents.
This is not veryencouraging, given that the accuracy of the tem-poral relations identified in the other 75% of thedocuments is 50.4%.
The documents marked asinconsistent include, on average, 3.66 temporal in-consistencies (with a maximum of 8 in a singledocument).
For each pair of events, we consid-ered only the top three temporal relations (in termsof confidence) identified by the learned classifiers.When the third relation identified for a given pairof events had to be removed by the inconsistencyresolution algorithm, no other temporal relationwas added to replace it.
Table 5 shows the impactof the validation step on the unclosed test data.Precision Recall F-measureBaseline 50.4 50.4 50.4With test validation 50.1 49.7 49.9Table 5: Performance change after the testing datavalidation step.
The baseline is the ME modellearned on the original (unclosed) training data.Our error analysis shows that, for each discov-ered temporal inconsistency, more than one incor-rect relation lead to an inconsistent temporal clo-sure.
Frequently enough, replacing the lowest con-fidence relation does not resolve the inconsistencyand the temporal relations used to replace it are,in turn, replaced in the next iterations (the confi-dence of the replacing relation is lower than theconfidence of the replaced relation).
The ME clas-sifier?s numerous errors and the low applicabilityof this process make its contribution to the overalltemporal relation resolution process negative.
Ourfuture work will focus on (1) experimenting withless erroneous data for which our one-incorrect-relation-per-inconsistency assumption holds ((per-haps) the models learned from closed training datausing a data split at the relation level) and (2) test-ing the existence of a consistent temporal closurein the absence of the lowest confidence relation.
Ifnone of the six temporal relations that we use to la-bel an event-event relation can replace the lowestconfidence relation and lead to a consistent tempo-ral closure, then our candidate incorrect relation isamong the other higher confidence relations.
Wealso note that we rely heavily on the confidencesautomatically assigned by the ME classifiers.Mani et al (2007) briefly describe a Greedymethod for ensuring global consistency of auto-matically labeled testing data.
No evaluation re-sults are reported.
As far as we can tell, Mani atel.
(2007) use this algorithm to decide whether ornot to assign the top 1 relation automatically iden-tified by ME classifiers to a given pair of events.No attempts are made to replace this relation.
Ourvalidation algorithm uses lower confidence rela-tions found by the learned models for the same pairof events to replace the lowest confidence relationthat leads to a temporal inconsistency.4.3 Temporal relation alignmentIn the previous section, we used temporal reason-ing to replace certain relations automatically iden-tified by the learned temporal relation resolutionmodels with the next best relation (in terms ofthe confidence) found for the same pair of events.For our third experiment, we use the top n re-lations automatically identified for a single pairof events.
Across a document, these relation-ships can be grouped to form different tempo-ral orderings of the events mentioned in the doc-ument.
For instance, for four pairs of events,81 different temporal settings can be created us-ing the top 3 temporal relations.
Figure 2 showstwo of these 81 facets ({R12, R?23, R?34, R41} and{R?12, R23, R?34, R?41}) for events e1, .
.
.
, e4.For these event temporal orderings, we pro-862R12,c1(event1,event2) (event2,event3) (event3,event4) (event4,event1)R?12,c?1R?12,c?1R23,c2R?23,c?2R?23,c?2R34,c3R?34,c?3R?34,c?3R41,c4R?41,c?4R?41,c?4Figure 2: Two possible relation alignmentspose to use our temporal reasoning module to de-rive, score, and rank their temporal closures.
Wemake the assumption that the correct document-level event ordering, the document?s temporal co-hesion can be identified by measuring the closureof the document?s temporal relations and that or-derings that use incorrect relations do not gener-ate good closures.
Thus, the relations that generatethe best temporal closure will be considered finaland will be used to label the test document?s event-event and event-time pairs.For the example shown in Figure 2, 81 differenttemporal closures are generated depending on theset of relations used to derive them from.
In orderto find the final four temporal relations betweenevents e1, .
.
.
, e4, we score and order the derivedtemporal closures.
The best closure decides, foreach pair of events, which relation among the top3 should be selected as final.Our first step is to identify the best value forn.
Table 6 shows the maximum gain in perfor-mance when multiple relations are considered forthe same pair of events (an instance is consideredcorrect if the gold annotation is among the top nrelations returned by the system).Top n relations Accuracy (%)1 57.402 80.663 94.494 97.385 99.126 100.00Table 6: Top n oracle performance using 5-fold-cross-validation on the TimeBank dataBecause there is substantial improvement in thetop 3 relation set, we use for our experiments thefirst three relations identified by the ME classi-fiers.
But, if we consider the top 3 relations foreach pair of events in a document, we end up with3N possible alignments, where N is the number ofevent-event and event-time pairs9 and the scoring9For each time-time pair, there is a single temporal relationwith confidence equal to 1.AccuracyBaseline - top 1 50.4Oracle (upper bound) - top 3 92.4With test alignment 47.5Table 7: Test dataset performance change after thetesting data alignment step.
The top 1 and top3 baselines were generated using the ME modellearned on the original (unclosed) training data.and ranking of all 3N temporal closures becomeshardly possible.
Therefore, we use a more Greedyapproach.
Iteratively, we score and rank temporalclosures derived from a small set of top 3 relationsbetween N ?
event-event pairs (N ?
< N ) and anyfinal temporal relations.
The best closure is usedto decide on N ?
temporal relations which will beadded to the best partial alignment and will be usedto compute all the following temporal closures.Secondly, we must identify the temporal clo-sure scoring function.
For our experiments, thisfunction takes into account the size of the tempo-ral closure (|T ?|) as well as the confidence val-ues of the relations identified by the ME classi-fiers in the test set (not derived by the tempo-ral closure algorithm) (only {c12, c?23, c?34, c41} and{c?12, c23, c?34, c?41} for the example shown inFigure 2).
The correlation between these param-eters and the scoring function is not straightfor-ward.
A preference for the confidence values fa-vors closures which use only the top relations (interms of confidence).
However, weighing the sizeof the temporal closure leads to a result dominatedby relations that close very well10, such as simulta-neous or before (which are also very frequent in thedataset).
In the settings which produced the resultsshown in Table 7, we used the score1function: forT = {(R1, c1), .
.
.
, (Rk, ck)},score1(T?)
= lg(|T?|)?k?i=1ci.The temporal relation accuracy drops by 3% afterthe relation selection among the top 3 best tem-poral relations for the testing documents.
Posibileexplanation: score1does not promote the close-to-gold temporal closures.
The difinition of a goodscoring function is not an easy process.
Machinelearning approaches might give us better coefi-cients for the parameters we consider.
Alternativ-elly, our main assumption might prove incorrect:10When present, these relations will quickly generate manyothers in the temporal closure.863temporal closure is not a good indicator of a docu-ment?s event ordering.
The information conveyedby a document need not disclose a rich total order-ing of its events.5 ConclusionIn this paper, we briefly described our feature en-gineering efforts for temporal relation resolutionand we analyzed three methods that exploit tem-poral reasoning and, more specifically, the closureof temporal relations, for the purpose of improvingthe performance of machine learned classifiers oftemporal relations between events in text.Based on our experiments, we find that featureengineering helps improve the classification prob-lem, when compared with several baseline perfor-mances.
However, given our current NLP capabil-ities, it is clear that we are faced with the perfor-mance bottleneck problem (accuracy below 60%).Any attempt to derive more advanced features de-mands more sophisticated methodologies of mod-eling temporal expressions, events and their re-lationships as well as advanced discourse under-standing capabilities.
For instance, the temporalduration or the start/end time points of events arehighly useful for learning temporal relations.
But,this introduces an even more challenging problem.In terms of the utility of temporal reasoningin classifying temporal relation, the idea of usingtemporal reasoning to boost training data is cer-tainly sound.
But in order for the boosted train-ing data to really take effect, more advanced fea-tures need to be investigated.
Certainly, the pro-cess of dividing the data into training and testinghas its impact on the system?s performance and weare faced with the data sparseness problem.
Tem-poral inconsistencies in our automatically labeledtest dataset occurred in just a few test documentsand the resolution process did not impact the sys-tem?s performance.
Improvements are needed inthe process of selection of the to-be-replaced re-lations.
Temporal data alignment largely dependson the function used to score the temporal closuresand we plan to analyze the temporal closure ofthe training data and to explore other scoring func-tions.ReferencesAllen, J. F. 1983.
Maintaining Knowledge aboutTemporal Intervals.
Communications of the ACM,26(11):832?843.Boguraev, B. and R. K. Ando.
2005.
TimeML-Compliant Text Analysis for Temporal Reasoning.In Proceedings of Nineteenth International JointConference on Artificial Intelligence (IJCAI-05),pages 997?1003, Edinburgh, Scotland, August.Chambers, N., S. Wang, and D. Jurafsky.
2007.
Clas-sifying Temporal Relations Between Events.
In Pro-ceedings of the ACL 2007 Demo and Poster Sessions,pages 173?176, Prague, Czech Republic, June.Chang, C. and C. Lin, 2001.
LIBSVM: a libraryfor support vector machines.
Software available atwww.csie.ntu.edu.tw/?cjlin/libsvm.Mani, I., M. Verhagen, B. Wellner, C. Min Lee, andJ.
Pustejovsky.
2006.
Machine Learning of Tem-poral Relations.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the ACL, pages 753?760,Sydney, Australia, July.Mani, I., B. Wellner, M. Verhagen, and J. Pustejovsky.2007.
Three Approaches to Learning TLINKs inTimeML.
Technical Report CS-07-268, ComputerScience Department, Brandeis University, Waltham,USA.Min, Congmin, Munirathnam Srikanth, and AbrahamFowler.
2007.
LCC-TE: A Hybrid Approach toTemporal Relation Identification in News Text.
InProceedings of the Fourth International Workshopon Semantic Evaluations (SemEval-2007), pages219?222, Prague, Czech Republic, June.Pustejovsky, J., P. Hanks, R. Sauri, A. See,R.
Gaizauskas, A. Setzer, D. Radev, B. Sundheim,D.
Day, L. Ferro, and M. Lazo.
2003.
The TIME-BANK Corpus.
In Proceedings of Corpus Lin-guistics, pages 647?656, Lancaster University (UK),March.Tatu, Marta and Dan Moldovan.
2005.
A Semantic Ap-proach to Recognizing Textual Entailment.
In Pro-ceedings of the HTL-EMNLP 2005, pages 371?378,Vancouver, BC Canada, October.Tatu, Marta and Dan Moldovan.
2007.
COGEX atRTE3.
In Proceedings of the ACL-PASCAL Work-shop on Textual Entailment and Paraphrasing, pages22?27, Prague, Czech Republic, June.Verhagen, M., R. Gaizauskas, F. Schilder, M. Hep-ple, G. Katz, and J. Pustejovsky.
2007.
SemEval-2007 Task 15: TempEval Temporal Relation Identi-fication.
In Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations (SemEval-2007),pages 75?80, Prague, Czech Republic, June.864
