Developing a Nonsymbolic PhoneticNotation for Speech SynthesisAndrew CohewUniversity of ReadingThe goal of the research presented here is to apply unsupervised neural network learning methodsto some of the lower-level problems in speech synthesis currently performed by rule-based systems.The latter tend to be strongly influenced by notations developed by linguists (see figure 1 in Klatt(1987)), which were primarily devised to deal with written rather than spoken language.
Ingeneral terms, what is needed in phonetics is a notation that captures information about ratiosrather than absolute values, as is typically seen in biological systems.
The notations derived hereare based on an ordered pattern space that can be dealt with more easily by neural networks,and by systems involving a neural and symbolic omponent.
Hence, the approach described heremight also be useful in the design of a hybrid neural~symbolic system to operate in the speechsynthesis domain.1.
Background and phonetic motivationPhonological and phonetic notations have been developed by linguists primarily asdescriptive tools, using rewrite-rules operating on highly abstracted basic units de-rived from articulatory phonetics.
Even some connectionist work has followed thistradition (Touretzky et al 1990).
The primary aim of these notations is explanationand understanding, and there are difficulties in incorporating them into systems witha practical aim such as deriving speech from text, which tend to be data-driven.
Onerecent study claimed that introduction of linguistic knowledge degrades performancein grapheme-phoneme conversion (van den Bosch and Daelemans 1993).
However,typical purely data-driven systems are opaque from a phonetic or phonological pointof view.
In order to handle many of the very hard problems remaining in speech syn-thesis, there is a need to develop a basic underlying notation (or method of derivinga notation) that can be parameterized for different speakers.
This notation could bebased on articulatory phonetics (where a higher-level task, such as grapheme-phonemeconversion, is being performed) or on a spectral/perceptual measure of similarity, formore low-level tasks such as duration adjustment.
This notation would ideally be rep-resented in a low-dimensional, topological space so as to be both perspicuous andflexible enough to use in further nonsymbolic modules.Existing synthesis-by-rule (SBR) systems (Allen et al 1987) have been concernedwith text-to-speech onversion, and have made use of a segmental approach derivedfrom traditional phonology.
Among the simplifying assumptions remaining from thisapproach are that transitions into and out of a consonant are identical, and that thesame transition may be used in each CV combination, regardless of the larger phoneticenvironment.
These assumptions need to be modified in a principled manner ratherthan by tables of exceptions.
* Department of Cybernetics, University of Reading.
E-mail: cybadc@cyber.reading.ac.uk(~) 1995 Association for Computational LinguisticsComputational Linguistics Volume 21, Number 4It has been argued by phoneticians that articulatory models cannot account forall the variability found in natural speech (Bladon and A1-Bamerni 1976; Kelly andLocal 1986).
Therefore, there is a need to find ways of incorporating other sources ofvariability into synthetic speech, including, for example, the feedback a talker receivesfrom the perception of their own voice.
Evidence that such feedback affects speech isthe degradation seen in the speech of persons with acquired deafness.
One possibleway to introduce this kind of variability is through the development of representationsthat encode (in a reduced imensionality) a range of examples of the phenomenon tobe accounted for.
Formant data can be used to introduce a perceptual measure ofsimilarity (see section 3 below).This report describes the theoretical motivations of an experimental system thathas been implemented as a set of shell scripts and 'C' programs; not all of the technicaldetails of this system have been finalized, and it has not been formally tested.
Whileformants have been made use of as training data (as well as acoustic tube data), as yetno use has been made of a formant synthesizer for creating the output speech, due tothe need for handcrafting of values.
At present, waveform segment concatenation isbeing used to explore a parametric duration model based on the kind of proximity-based notations described here.2.
Application of the SOM to phoneme dataIn outline, the Self-Organizing Map (SOM, Kohonen 1988) approximates to the prob-ability density function of the input pattern space, by representing the N-dimensionalpattern vectors on a 2D array of reference vectors in such a way that the resulting clus-terings conform to an elastic surface, where neighboring units share similar referencevectors.
This algorithm and Learning Vector Quantization (LVQ) are described in Ko-honen (1990), which has practical advice for implementation, and in more theoreticaldetail in Kohonen (1989).It has been widely noted that 2D representations of speech are useful where thereis a need to transmit information to humans at a phonetic level--for example, in tactilelistening systems (Ellis and Robinson 1993).
If a speech synthesis ystem has a phoneticinterface or level of operation, it is then possible to introduce learning techniques forsubsequent modules (e.g., those which calculate durations or an intonation contour)and to have an idea of what is happening, in phonetic terms, when things go wrong,and therefore how the training program or learning method may be adjusted.
Thereis a long tradition of two-dimensional representations of formant data in attemptsto classify vowels, going back at least to the study of Peterson and Barney (1952).Another type of advantage lies in the flexibility given by the very large dimensionalityreductions achievable by Kohonen's technique.
These reductions are possible evenwhere the input pattern space may be only sparsely populated, yielding a flexibleencoding with not too many degrees of freedom.
It is possible for Kohonen's techniqueto work in 3D (3D maps have been produced by the author, but are more difficult towork with and are still undergoing evaluation).
In 4D or above, interpretation becomesmuch more difficult.
Refinements such as the Growing Cells technique (Fritzke 1993)might be preferable to a move to higher dimensionality, so as to retain transparencyof the notation and a possible link to symbol-based stages of operation.Figure 1 shows a map resulting from applying the SOM algorithm to phonemefeature data.
The following nine binary articulatory features were used: continuant,voiced, nasal, strident, grave, compact, vowel height(I), vowel height(2), and round.The features hl and h2 are used for height simply because there are three possibilities:568Cohen Nonsymbolic Phonetic Notation for Speech Synthesisf sV Zr 1sh chdhmu i dthnkbnchgo e t p@ aFigure 1Clustering of phoneme data (8 x 12).kopen, mid and closed, which cannot be encoded by a binary bit} In this case, the pointis not to do feature xtraction (since the features are already known), but to providea statistical clustering in 2D that can indicate whether the features chosen providea good basis for analysis.
Figure 1 suggests that phoneticians have 'got it right' inthat the features do result in a clustering of similar sounds such as stops, fricativesand nasals, as well as the more obvious separation between vowels and consonants.It is worth pointing out that neither the SOM nor the LVQ algorithm handles rawdata (such as waveform values or image intensity values), but each operates on datasuch as spectral components or LPC coefficients that are themselves the output of asignificant processing stage, and can justifiably be called features.The phoneme map is produced by a single Kohonen layer that self-organizes u ingthe standard algorithm (Kohonen, 1990), taking as input nine articulatory featurescommonly used by phoneticians to describe the possible speech sounds.
The featureswere designed so that any phoneme (or syllable) may be uniquely specified as a clusterof features, without reference to specific units (segments such as phones, syllables,etc.)
--any feature may run across unit boundaries.
Figure I shows a 12 x 8 map created(as are all the following maps) with hexagonal connections in the lattice indicatingwhich units are neighbors.
A monotonically shrinking 'bubble' neighborhood wasused in all the maps shown here.
Kohonen refers to this type of kernel as a bubblebecause it relates to certain kinds of activity bubbles in laterally connected networks(see Kohonen 1989).1 Thanks to John Local for providing the basis for the data.569Cohen Nonsymbolic Phonetic Notation for Speech Synthesisaawaazhaaraayaagaataash .aaz  aadaab aach aavFigure 2aaajaalaahaaf aang .Clustering of diphone data for aa-C.aath .as.maapaakaasaan aadhrelevant information is captured in the formant trajectories.
Maps based on acoustictube data computed from the LPC coefficients have also been created, with much thesame kind of results as seen in the formant maps.
That the results should be similar isto be expected as this data is essentially spectral, and bears little resemblance to realvocal tract data.
Experiments are currently being carried out to determine whetherthese maps or those based on formants will work better as part of h prototype speechsynthesis ystem.To factor out the influence of the initial configuration of the network (the referencevectors are initialized to small random values), twenty trials were run on each dataset, and the map with the lowest quantization error (QE) was selected as the best.
TheQE is simply the mean error over the N pattern vectors in the set,Y~zt=l IIx(t) - me(011 QE Nwhere x(t) is the input vector and mc the best matching reference vector for x(t).In order to compare QEs, the topology (form of lateral connections) and adaptationfunctions must be the same, since the amount of lateral interaction determines theself-organizing power of the network.
In the simplest case of competitive l arning theneighborhood contains only one unit, so a minimal QE may be achieved, but in thiscase there is no self-organizing effect.Schematically, then, resynthesis would take place on the basis of a trajectory acrossa diphone map.
The trajectory could be stored simply as a vector of co-ordinates thatare 'lit up' on the map.
These vectors would occupy little storage space, and mightbe passed as input to a further SOM layer to try to cluster similar sounding words.The time-varying, sequential properties of speech, which are difficult for neural netsto handle, can thus be modeled as a spatial pattern in an accessible and straightfor-ward manner.
Vectors of addresses would be completely different (e.g., the endpoints571Cohen Nonsymbolic Phonetic Notation for Speech Synthesisairp.oapdip oorparppoiieperppeepadahp aap epoipawppuoopuuppopoapeer .pair .per poopoor poupaw pai puupie parFigure 2cClustering of data for transitions into and out of 'p'.50000400003000020000i00000-I0000-20000-30000-40000-50000-60000~ee~u~o-40000-dip%~orp  ~ap"iep~d%p Rrp ~Dup Rip"awp~upREDiDo o ~e~uh ~aa"p ~_-20000.rp~erp 'ap ~P-ip~op ~p ~p~oa ~oi~e~ai r  ~oor~er~uu ~ail ~ml  I !0 20000 40000opipeeppepipuhKey:Sym Example"air" /air/"all" /bard/"e" /bed/"oa" /oak/"ie" /pie/"oi" /oil/"oo" /good/"ai" /pain/"oor" /poor/"er" /bird/"aw" /board/"i" /bid/"uu" /brood/"ee" /bead/"u" /bud/"o" /body/"uh" /above/"ou" /out/"eer" /ear/"dr" /art/p---p" D_.
sam"~aw~iei60000 80000Figure 2dClustering of data with Sammon's mapping.573Computational Linguistics Volume 21, Number 4concatenation procedure, on which various enhancements based on the SOM are beingtried, which will be more fully described in future reports.
Using the examples givenon a record supplied with Klatt's (1987) review article, informal comparison shows ahigh degree of variability in quality of the sentences generated: the best are comparablewith the diphone concatenation methods (which have better transitions than DECtalk,even if the prosody is in some cases not as well developed), while the worst are highlyunnatural, but usually intelligible.4.
Conclusion and further workThe outline of a conventional SBR system has a series of symbolic stages, assuming amodularity of data at each level, before the final low-level stage ('synthesis routines')calculates the synthesizer parameters.
The essential feature is the 'abstract linguisticdescription', which must be derived before any attempt ismade to calculate parametervalues.
In the proposed system, this middle stage is replaced by the SOM stage, whichintroduces a learned notation based on acoustic data.
Generation of an intonationcontour, though this has been implemented with neural nets, is probably best handledwith rules as it is almost purely a prosodic (i.e., sentence l vel) matter.The SOM coding replaces the linguistic description, and leads to direct accessof waveform values for a given diphone, which then become default values for thenext stage to operate on.
In conclusion, arguments have been presented for the useof nonsymbolic odings as the central stage of a text to-speech system.
These cod-ings are both closer to the acoustic domain and capable of greater flexibility thanthe standard phonetic notations.
Additional sources of variability, such as stress andemotional quality, could also be accounted for with this kind of trajectory in a low-dimensional space, rather than attempting to derive a speaker-independent symbolicnotation.
These maps are also capable of being operated on by a neural network infurther processing stages, opening the way to a different ype of phonetics based ona multitude of soft constraints rather than the rigid phoneme and rewrite rule.Further work is needed to investigate he usefulness of the SOMs in speech synthe-sis, and how they may be integrated in a hybrid system that uses rule-based prosody.Other data sets need to be explored to introduce other kinds of variability.
It wouldalso be important to determine whether the distance measure provided by the diphonemaps correlates better with subjective perception of the mismatch between successivediphones than more standard measures of spectral distance, such as various distancemeasures between frames of cepstral coefficients.AcknowledgmentsThanks to Stephen Isard of Edinburgh CSTRand Linda Shockey of the LinguisticsDepartment, Reading University for helpwith diphones and related matters.
Theauthor is grateful to John Local forenthusiasm and help with phonetics.
Anyremaining mistakes are the author'sresponsibility.
Thanks to the Laboratory ofComputer and Information Science,Helsinki University of Technology formaking available their 'SOM3~AK ' software,used here with minor modifications.ReferencesAllen, J., Hunnicutt, M. S., and Klatt, D. H.(1987).
From text o speech: The MITalksystem.
Cambridge University Press.Bladon, A., and A1-Bamerni, A.
(1976).
"Coarticulation resistance in English.
"Journal of Phonetics, 4, 137-150.van den Bosch, A., and Daelemans, W.(1993).
"Data-Oriented Methods forGrapheme-to-Phoneme Conversion.
"Proceedings, 6th Conference ofthe EuropeanChapter of the ACL, Utrecht, April 1993.Durand, J.
(1990).
Generative and Non-LinearPhonology.
Longman, London.574Cohen Nonsymbolic Phonetic Notation for Speech SynthesisEllis, E. M., and Robinson, A. J.
(1993).
"APhonetic Tactile Speech ListeningSystem."
Cambridge UniversityEngineering Department TechnicalReport, CUED/F-INFENG/TR122, May.Fritzke, B.
(1993).
"Growing CellStructures--A Self-organizing Networkfor Unsupervised and supervisedLearning."
International ComputerScience Institute Technical ReportTR-93-026, May.Goldsmith, J.
(1990).
Autosegmental andMetrical Phonology.
Blackwell, Oxford.Huang, C. B.
(1990).
"Modelling HumanVowel Identification Using Aspects ofFormant Trajectory and Context."
InSpeech Perception, Production and LinguisticStructure, edited by Y. Tohkura,E.
Vatikiotis-Bateson, and Y. Sagisaka.Proceedings, ATR workshop, Kyoto, Japan,November 1990, IOS press, Oxford, UK.Kelly, J., and Local, J.
(1986).
"Long-domainresonance patterns in English."
InProceedings IEEE Speech Input~OutputConference, Pub.
No.
258, 77-82.Klatt, D. H. (1982a).
"Speech processingstrategies based on auditory models."
InThe Representation f Speech in the PeripheralAuditory System, edited by R. Carlson andB.
Granstrom, Elsevier, Amsterdam.Klatt, D. H. (1982b).
"Prediction ofperceived phonetic distance from criticalband spectra: a first step."
In ProceedingsIEEE ICASSP-82, 1278-1281.Klatt, D. H. (1987).
"Review oftext-to-speech onversion for English.
"JASA 82(3), 737-793.Kohonen, T. (1988).
"The 'neural' phonetictypewriter."
IEEE Computer 21, 11-22.Kohonen, T. (1989).
Self-Organization a dAssociative Memory.
Springer Verlag, 3rded.Kohonen, T. (1990).
"The Self-OrganizingMap."
IEEE Proceedings 78(9), 1464-1480.Peterson, G., and Barney, H.
(1952).
"Control methods used in a study of thevowels."
\]ASA 24, 175-184.Sammon, J. W. (1969).
"A nonlinearmapping for data structure analysis.
"IEEE Trans.
Computers, C-18, 401-409.Touretzky, D. S., Wheeler, D. W., andElvgren III, G. (1990).
"Rules and MapsIII: Further Progress in ConnectionistPhonology," School Of Computer Science,Carnegie Mellon, Technical ReportCMU-CS-90-138575
