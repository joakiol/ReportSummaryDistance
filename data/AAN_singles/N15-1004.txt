Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 32?41,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsA Compositional and Interpretable Semantic SpaceAlona Fyshe,1Leila Wehbe,1Partha Talukdar,2Brian Murphy,3and Tom Mitchell11Machine Learning Department, Carnegie Mellon University, Pittsburgh, USA2Indian Institute of Science, Bangalore, India3Queen?s University Belfast, Belfast, Northern Irelandafyshe@cs.cmu.edu, lwehbe@cs.cmu.edu, ppt@serc.iisc.in,brian.murphy@qub.ac.uk, tom.mitchell@cs.cmu.eduAbstractVector Space Models (VSMs) of Semanticsare useful tools for exploring the semantics ofsingle words, and the composition of wordsto make phrasal meaning.
While many meth-ods can estimate the meaning (i.e.
vector) ofa phrase, few do so in an interpretable way.We introduce a new method (CNNSE) that al-lows word and phrase vectors to adapt to thenotion of composition.
Our method learns aVSM that is both tailored to support a chosensemantic composition operation, and whoseresulting features have an intuitive interpreta-tion.
Interpretability allows for the explorationof phrasal semantics, which we leverage to an-alyze performance on a behavioral task.1 IntroductionVector Space Models (VSMs) are models of wordsemantics typically built with word usage statisticsderived from corpora.
VSMs have been shown toclosely match human judgements of semantics (foran overview see Sahlgren (2006), Chapter 5), andcan be used to study semantic composition (Mitchelland Lapata, 2010; Baroni and Zamparelli, 2010;Socher et al, 2012; Turney, 2012).Composition has been explored with differenttypes of composition functions (Mitchell and La-pata, 2010; Mikolov et al, 2013; Dinu et al,2013) including higher order functions (such as ma-trices) (Baroni and Zamparelli, 2010), and somehave considered which corpus-derived informationis most useful for semantic composition (Turney,2012; Fyshe et al, 2013).
Still, many VSMs actlike a black box - it is unclear what VSM dimen-sions represent (save for broad classes of corpusstatistic types) and what the application of a com-position function to those dimensions entails.
Neu-ral network (NN) models are becoming increas-ingly popular (Socher et al, 2012; Hashimoto et al,2014; Mikolov et al, 2013; Pennington et al, 2014),and some model introspection has been attempted:Levy and Goldberg (2014) examined connectionsbetween layers, Mikolov et al (2013) and Penning-ton et al (2014) explored how shifts in VSM spaceencodes semantic relationships.
Still, interpretingNN VSM dimensions, or factors, remains elusive.This paper introduces a new method, Composi-tional Non-negative Sparse Embedding (CNNSE).In contrast to many other VSMs, our method learnsan interpretable VSM that is tailored to suit the se-mantic composition function.
Such interpretabilityallows for deeper exploration of semantic composi-tion than previously possible.
We will begin with anoverview of the CNNSE algorithm, and follow withempirical results which show that CNNSE produces:1. more interpretable dimensions than the typicalVSM,2.
composed representations that outperform pre-vious methods on a phrase similarity task.Compared to methods that do not consider composi-tion when learning embeddings, CNNSE produces:1. better approximations of phrasal semantics,2.
phrasal representations with dimensions thatmore closely match phrase meaning.322 MethodTypically, word usage statistics used to create aVSM form a sparse matrix with many columns, toounwieldy to be practical.
Thus, most models usesome form of dimensionality reduction to compressthe full matrix.
For example, Latent Semantic Anal-ysis (LSA) (Deerwester et al, 1990) uses SingularValue Decomposition (SVD) to create a compactVSM.
SVD often produces matrices where, for thevast majority of the dimensions, it is difficult to in-terpret what a high or low score entails for the se-mantics of a given word.
In addition, the SVD fac-torization does not take into account the phrasal re-lationships between the input words.2.1 Non-negative Sparse EmbeddingsOur method is inspired by Non-negative Sparse Em-beddings (NNSEs) (Murphy et al, 2012).
NNSEpromotes interpretability by including sparsity andnon-negativity constraints into a matrix factoriza-tion algorithm.
The result is a VSM with extremelycoherent dimensions, as quantified by a behavioraltask (Murphy et al, 2012).
The output of NNSEis a matrix with rows corresponding to words andcolumns corresponding to latent dimensions.To interpret a particular latent dimension, we canexamine the words with the highest numerical val-ues in that dimension (i.e.
identify rows with thehighest values for a particular column).
Though therepresentations in Table 1 were created with our newmethod, CNNSE, we will use them to illustrate theinterpretability of both NNSE and CNNSE, as theform of the learned representations is similar.
Oneof the dimensions in Table 1 has top scoring wordsguidance, advice and assistance - words related tohelp and support.
We will refer to these word listsummaries as the dimension?s interpretable sum-marization.
To interpret the meaning of a particu-lar word, we can select its highest scoring dimen-sions (i.e.
choose columns with maximum valuesfor a particular row).
For example, the interpretablesummarizations for the top scoring dimensions ofthe word military include both positions in the mil-itary (e.g.
commandos), and military groups (e.g.paramilitary).
More examples in SupplementaryMaterial (http://www.cs.cmu.edu/?fmri/papers/naacl2015/).NNSE is an algorithm which seeks a lower di-mensional representation for w words using the c-dimensional corpus statistics in a matrixX ?
Rw?c.The solution is two matrices: A ?
Rw?`that issparse, non-negative, and represents word semanticsin an `-dimensional latent space, and D ?
R`?c:the encoding of corpus statistics in the latent space.NNSE minimizes the following objective:argminA,D12w?i=1??Xi,:?Ai,:?D?
?2+ ?1??Ai,:?
?1(1)st: Di,:DTi,:?
1,?
1 ?
i ?
` (2)Ai,j?
0, 1 ?
i ?
w, 1 ?
j ?
` (3)where Ai,jindicates the entry at the ith row and jthcolumn of matrix A, and Ai,:indicates the ith rowof the matrix.
The L1constraint encourages sparsityin A; ?1is a hyperparameter.
Equation 2 constrainsD to eliminate solutions where the elements of Aare made arbitrarily small by making the norm of Darbitrarily large.
Equation 3 ensures that A is non-negative.
Together, A and D factor the original cor-pus statistics matrix X to minimize reconstructionerror.
One may tune ` and ?1to vary the sparsity ofthe final solution.Murphy et al (2012) solved this system of con-straints using the Online Dictionary Learning algo-rithm described in Mairal et al (2010).
ThoughEquations 1-3 represent a non-convex system, whensolving for A with D fixed (and vice versa) the lossfunction is convex.
Mairal et al break the prob-lem into two alternating optimization steps (solv-ing for A and D) and find the system convergesto a stationary solution.
The solution for A isfound with a LARS implementation for lasso regres-sion (Efron et al, 2004); D is found via gradient de-scent.
Though the final solution may not be globallyoptimal, this method is capable of handling largeamounts of data and has been shown to produce use-ful solutions in practice (Mairal et al, 2010; Murphyet al, 2012).2.2 Compositional NNSEWe add an additional constraint to the NNSE lossfunction that allows us to learn a latent representa-tion that respects the notion of semantic composi-tion.
As we will see, this change to the loss functionhas a huge effect on the learned latent space.
Just as233Table 1: CNNSE interpretable summarizations for the top 3 dimensions of an adjective, noun and adjective-noun phrase.military aid military aid (observed)servicemen, commandos, guidance, advice, assistance servicemen, commandos,military intelligence military intelligenceguerrilla, paramilitary, anti-terrorist mentoring, tutoring, internships guidance, advice, assistanceconglomerate, giants, conglomerates award, awards, honors compliments, congratulations, repliesthe L1regularizer can have a large impact on spar-sity, our composition constraint represents a consid-erable change in composition compatibility.Consider a phrase p made up of words i and j. Inthe most general setting, the following compositionconstraint could be applied to the rows of matrix Acorresponding to p, i and j:A(p,:)= f(A(i,:), A(j,:)) (4)where f is some composition function.
The com-position function constrains the space of learned la-tent representations A ?
Rw?`to be those solutionsthat are compatible with the composition functiondefined by f .
Incorporating f into Equation 1 wehave:argminA,D,?w?i=112??Xi,:?Ai,:?D?
?2+ ?1??Ai,:?
?1+?c2?phrase p,p=(i,j)(A(p,:)?
f(A(i,:), A(j,:)))2(5)Where each phrase p is comprised of words (i, j)and ?
represents all parameters of f to be optimized.We have added a squared loss term for composition,and a new regularization parameter ?cto weightthe importance of respecting composition.
We callthis new formulation Compositional Non-NegativeSparse Embeddings (CNNSE).
Some examples ofthe interpretable representations learned by CNNSEfor adjectives, nouns and phrases appear in Table 1.There are many choices for f : addition, multi-plication, dilation, etc.
(Mitchell and Lapata, 2010).Here we choose f to be weighted addition because ithas has been shown to work well for adjective nouncomposition (Mitchell and Lapata, 2010; Dinu et al,2013; Hashimoto et al, 2014), and because it lendsitself well to optimization.
Weighted addition is:f(A(i,:), A(j,:)) = ?A(i,:)+ ?A(j,:)(6)This choice of f requires that we simultaneously op-timize forA,D,?
and ?.
However, ?
and ?
are sim-ply constant scaling factors for the vectors in A cor-responding to adjectives and nouns.
For adjective-noun composition, the optimization of ?
and ?
canbe absorbed by the optimization of A.
For modelsthat include noun-noun composition, if ?
and ?
areassumed to be absorbed by the optimization of A,this is equivalent to setting ?
= ?.We can further simplify the loss function by con-structing a matrix B that imposes the compositionby addition constraint.
B is constructed so that foreach phrase p = (i, j): B(p,p)= 1, B(p,i)= ?
?,and B(p,j)= ??.
For our models, we use ?
= ?
=0.5, which serves to average the single word repre-sentations.
The matrix B allows us to reformulatethe loss function from Eq 5:argminA,D12?
?X ?AD?
?2F+ ?1??A??1+?c2??BA?
?2F(7)where F indicates the Frobenius norm.
B acts as aselector matrix, subtracting from the latent represen-tation of the phrase the average latent representationof the phrase?s constituent words.We now have a loss function that is the sum ofseveral convex functions of A: squared reconstruc-tion loss for A, L1regularization and the composi-tion constraint.
This sum of sub-functions is the for-mat required for the alternating direction method ofmultipliers (ADMM) (Boyd, 2010).
ADMM substi-tutes a dummy variable z for A in the sub-functions:argminA,D12?
?X ?AD?
?2F+ ?1??z1??1+?c2??Bzc?
?2F(8)and, in addition to constraints in Eq 2 and 3, incor-porates constraints A = z1and A = zcto ensuredummy variables match A. ADMM uses an aug-334mented Lagrangian to incorporate and relax thesenew constraints.
We optimize for A, z1and zcsep-arately, update the dual variables and repeat untilconvergence (see Supplementary material for La-grangian form, solutions and updates).
We modi-fied code for ADMM, which is available online1.ADMM is used when solving for A in the OnlineDictionary Learning algorithm, solving for D re-mains unchanged from the NNSE implementation(see Algorithms 1 and 2 in Supplementary Material).We use the weighted addition composition func-tion because it performed well for adjective-nouncomposition in previous work (Mitchell and Lap-ata, 2010; Dinu et al, 2013; Hashimoto et al, 2014),maintains the convexity of the loss function, and iseasy to optimize.
In contrast, an element-wise mul-tiplication, dilation or higher-order matrix compo-sition function will lead to a non-convex optimiza-tion problem which cannot be solved using ADMM.Though not explored here, we hypothesize that Acould be molded to respect many different compo-sition functions.
However, if the chosen composi-tion function does not maintain convexity, finding asuitable solution for A may prove challenging.
Wealso hypothesize that even if the chosen composi-tion function is not the ?true?
composition function(whatever that may be), the fact that A can changeto suit the composition function may compensate forthis mismatch.
This has the flavor of variational in-ference for Bayesian methods: an approximation inplace of an intractable problem often yields betterresults with limited data, in less time.3 Data and ExperimentsWe use the semantic vectors made available byFyshe et al (2013), which were compiled from a 16billion word subset of ClueWeb09 (Callan and Hoy,2009).
We used the 1000 dependency SVD dimen-sions, which were shown to perform well for compo-sition tasks.
Dependency features are tuples consist-ing of two POS tagged words and their dependencyrelationship in a sentence; the feature value is thepointwise positive mutual information (PPMI) forthe tuple.
The dataset is comprised of 54,454 wordsand phrases.
We randomly split the approximately14,000 adjective noun phrases into a train (2/3) and1http://www.stanford.edu/?boyd/papers/admm/Table 2: Median rank, mean reciprocal rank (MRR)and percentage of test phrases ranked perfectly (i.e.first in a sorted list of approx.
4,600 test phrases)for four methods of estimating the test phrase vec-tors.
w.addSVDis weighted addition of SVD vectors,w.addNNSEis weighted addition of NNSE vectors.Model Med.
Rank MRR Perfectw.addSVD99.89 35.26 20%w.addNNSE99.80 28.17 16%Lexfunc 99.65 28.96 20%CNNSE 99.91 40.65 26%test (1/3) set.
From the test set we removed 200 ran-domly selected phrases as a development set for pa-rameter tuning.
We did not lexically split the trainand test sets, so many words appearing in trainingphrases also appear in test phrases.
For this reasonwe cannot make specific claims about the generaliz-ability of our methods to unseen words.NNSE has one parameter to tune (?1); CNNSEhas two: ?1and ?c.
In general, these methods arenot overly sensitive to parameter tuning, and search-ing over orders of magnitude will suffice.
We foundthe optimal settings for NNSE were ?1= 0.05, andfor CNNSE ?1= 0.05, ?c= 0.5.
Too large ?1leads to overly sparse solutions, too small reducesinterpretability.
We set ` = 1000 for both NNSEand CNNSE and altered sparsity by tuning only ?1.3.1 Phrase Vector EstimationTo test the ability of each model to estimate phrasesemantics we trained models on the training set, andused the learned model and the composition functionto estimate vectors of held out phrases.
We sort thevectors for the test phrases, Xtest, by their cosinedistance to the predicted phrase vector?X(p,:).We report two measures of accuracy.
The first ismedian rank accuracy.
Rank accuracy is: 100?
(1?rP), where r is the position of the correct phrasein the sorted list of test phrases, and P = |Xtest|(the number of test phrases).
The second measureis mean reciprocal rank (MRR), which is often usedto evaluate information retrieval tasks (Kantor andVoorhees, 2000).
MRR is100?
(1PP?i=1(1r)).
(9)435For both rank accuracy and MRR, a perfect score is100.
However, MRR places more emphasis on rank-ing items close to the top of the list, and less on dif-ferences in ranking lower in the list.
For example,if the correct phrase is always ranked 2, 50 or 100out of list of 4600, median rank accuracy would be99.95, 98.91 or 97.83.
In contrast, MRR would be50, 2 or 1.
Note that rank accuracy and reciprocalrank produce identical orderings of methods.
Thatis, whatever method performs best in terms of rankaccuracy will also perform best in terms of recip-rocal rank.
MRR simply allows us to discriminatebetween very accurate models.
As we will see, therank accuracy of all models is very high (> 99%),approaching the rank accuracy ceiling.3.1.1 Estimation MethodsWe will compare to two other previouslystudied composition methods: weighted addition(w.addSVD), and lexfunc (Baroni and Zamparelli,2010).
Weighted addition finds ?, ?
to optimize(X(p,:)?
(?X(i,:)+ ?X(j,:)))2Note that this optimization is performed over theSVD matrix X , rather than on A.
To estimate Xfor a new phrase p = (i, j) we compute?X(p,:)= ?X(i,:)+ ?X(j,:)Lexfunc finds an adjective-specific matrix MithatsolvesX(p,:)= MiX(j,:)for all phrases p = (i, j) for adjective i.
We solvedeach adjective-specific problem with Matlab?s par-tial least squares implementation, which uses theSIMPLS algorithm (Dejong, 1993).
To estimate Xfor a new phrase p = (i, j) we compute?X(p,:)= MiX(j,:)We also optimized the weighted addition compo-sition function over NNSE vectors, which we callw.addNNSE.
After optimizing ?
and ?
using thetraining set, we compose the latent word vectors toestimate the held out phrase:?A(p,:)= ?A(i,:)+ ?A(j,:)For CNNSE, as in the loss function, ?
= ?
= 0.5so that the average of the word vectors approximatesthe phrase.
?A(p,:)= 0.5?
(A(i,:)+A(j,:))Crucially, w.addNNSEestimates ?, ?
after learningthe latent space A, whereas CNNSE simultaneouslylearns the latent space A, while taking the compo-sition function into account.
Once we have an esti-mate?A(p,:)we can use the NNSE and CNNSE solu-tions for D to estimate the corpus statistics X.?X(p,:)=?A(p,:)DResults for the four methods appear in Table 2.Median rank accuracies were all within half a per-centage point of each other.
However, MRR showsa striking difference in performance.
CNNSE hasMRR of 40.64, more than 5 points higher than thesecond highest MRR score belonging to w.addSVD(35.26).
CNNSE ranks the correct phrase in thefirst position for 26% of phrases, compared to 20%for w.addSVD.
Lexfunc ranks the correct phrasefirst for 20% of the test phrases, w.addNNSE16%.So, while all models perform quite well in termsof rank accuracy, when we use the more discrim-inative MRR, CNNSE is the clear winner.
Notethat the performance of w.addNNSEis much lowerthan CNNSE.
Incorporating a composition con-straint into the learning algorithm has produced a la-tent space that surpasses all methods tested for thistask.We were surprised to find that lexfunc performedrelatively poorly in our experiments.
Dinu et al(2013) used simple unregularized regression to es-timate M .
We also replicated that formulation, andfound phrase ranking to be worse when comparedto the Partial Least Squares method described in Ba-roni and Zamparelli (2010).
In addition, Baroni andZamparelli use 300 SVD dimensions to estimateM .We found that, for our dataset, using all 1000 dimen-sions performed slightly better.We hypothesize that our difference in perfor-mance could be due to the difference in input cor-pus statistics (in particular the thresholding of infre-quent words and phrases), or due to the fact that wedid not specifically create the training and tests setsto evenly distribute the phrases for each adjective.If an adjective i appears only in phrases in the testset, lexfunc cannot estimate Miusing training data(a hindrance not present for other methods, which536require only that the adjective appear in the train-ing data).
To compensate for this possibly unfairtrain/test split, the results in Table 2 are calculatedover only those adjectives which could be estimatedusing the training set.Though the results reported here are not as highas previously reported, lexfunc was found to beonly slightly better than w.addSVDfor adjective nouncomposition (Dinu et al, 2013).
CNNSE outper-forms w.addSVDby a large margin, so even if Lex-func could be tuned to perform at previous levels onthis dataset, CNNSE would likely still dominate.3.1.2 Phrase Estimation ErrorsNone of the models explored here are perfect.Even the top scoring model, CNNSE, only identi-fies the correct phrase for 26% of the test phrases.When a model makes a ?mistake?, it is possible thatthe top-ranked phrase is a synonym of, or closelyrelated to, the actual phrase.
To evaluate mistakes,we chose test phrases for which all 4 models are in-correct and produce a different top ranked phrase(likely these are the most difficult phrases to es-timate).
We then asked Mechanical Turk (Mturkhttp://mturk.com) users to evaluate the mis-takes.
We presented the 4 mistakenly top-rankedphrases to Mturk users, who were asked to choosethe one phrase most related to the actual test phrase.We randomly selected 200 such phrases and asked5 Mturk users to evaluate each, paying $0.01 per an-swer.
We report here the results for questions wherea majority (3) of users chose the same answer (82%of questions).
For all Mturk experiments describedin this paper, a screen shot of the question appears inthe Supplementary Material.Table 3 shows the Mturk evaluation of model mis-takes.
CNNSE and lexfunc make the most reason-able mistakes, having their top-ranked phrase cho-sen as the most related phrase 35.4% and 31.7% ofthe time, respectively.
This makes us slightly morecomfortable with our phrase estimation results (Ta-ble 2); though lexfunc does not reliably predict thecorrect phrase, it often chooses a close approxima-tion.
The mistakes from CNNSE are chosen slightlymore often than lexfunc, indicating that CNNSEalso has the ability to reliably predict the correctphrase, or a phrase deemed more related than thosechosen by other methods.Table 3: A comparison of mistakes in phrase rank-ing across 4 composition methods.
To evaluate mis-takes, we chose phrases for which all 4 models ranka different (incorrect) phrase first.
Mturk users wereasked to identify the phrase that was semanticallyclosest to the target phrase.Predicted phrase deemedModel closest match to actual phrasew.addSVD21.3%w.addNNSE11.6%Lexfunc 31.7%CNNSE 35.4%3.2 InterpretabilityThough our improvement in MRR for phrase vec-tor estimation is compelling, we seek to explore themeaning encoded in the word space features.
Weturn now to the interpretation of phrasal semanticsand semantic composition.3.2.1 Interpretability of Latent DimensionsDue to the sparsity and non-negativity constraints,NNSE produces dimensions with very coherent se-mantic groupings (Murphy et al, 2012).
Murphyet al used an intruder task to quantify the inter-pretability of semantic dimensions.
The intrudertask presents a human user with a list of words, andthey are to choose the one word that does not belongin the list (Chang et al, 2009).
For example, fromthe list (red, green, desk, pink, purple, blue), it isclear to see that the word ?desk?
does not belong inthe list of colors.To create questions for the intruder task, we se-lected the top 5 scoring words in a particular di-mension, as well as a low scoring word from thatsame dimension such that the low scoring word isalso in the top 10th percentile of another dimen-sion.
Like the word ?desk?
in the example above,this low scoring word is called the intruder, and thehuman subject?s task is to select the intruder from ashuffled list of 6 words.
Five Mturk users answeredeach question, each paid $0.01 per answer.
If Mturkusers identify a high percentage of intruders, this in-dicates that the latent representation groups words ina human-interpretable way.
We chose 100 questionsfor each of the NNSE, CNNSE and SVD represen-tations.
Because the output of lexfunc is the SVD637Table 4: Quantifying the interpretability of learnedsemantic representations via the intruder task.
In-truders detected: % of questions for which the ma-jority response was the intruder.
Mturk agreement:the % of questions for which a majority of userschose the same response.Method Intruders Detected Mturk AgreementSVD 17.6% 74%NNSE 86.2% 94%CNNSE 88.9% 90%representationX , SVD interpretability is a proxy forlexfunc interpretability.Results for the intruder task appear in Table 4.Consistent with previous studies, NNSE provides amuch more interpretable latent representation thanSVD.
We find that the additional composition con-straint used in CNNSE has maintained the inter-pretability of the learned latent space.
Because in-truders detected is higher for CNNSE, but agreementamongst Mturk users is higher for NNSE, we con-sider the interpretability results for the two methodsto be equivalent.
Note that SVD interpretability isclose to chance (1/6 = 16.7%).3.2.2 Coherence of Phrase RepresentationsThe dimensions of NNSE and CNNSE are com-parably interpretable.
But, has the composition con-straint in CNNSE resulted in better phrasal repre-sentations?
To test this, we randomly selected 200phrases, and then identified the top scoring dimen-sion for each phrase in both the NNSE and CNNSEmodels.
We presented Mturk users with the inter-pretable summarizations for these top scoring di-mensions.
Users were asked to select the list ofwords (interpretable summarization) most closelyrelated to the target phrase.
Mturk users couldalso select that neither list was related, or that thelists were equally related to the target phrase.
Wepaid $0.01 per answer and had 5 users answer eachquestion.
In Table 5 we report results for phraseswhere the majority of users selected the same an-swer (78% questions).
CNNSE phrasal represen-tations are found to be much more consistent, re-ceiving a positive evaluation almost twice as oftenas NNSE.Together, these results show that CNNSE repre-sentations maintain the interpretability of NNSE di-Table 5: Comparing the coherence of phrase rep-resentations from CNNSE and NNSE.
Mturk userswere shown the interpretable summarization for thetop scoring dimension of target phrases.
Represen-tations from CNNSE and NNSE were shown side byside and users were asked to choose the list (summa-rization) most related to the phrase, or that the listswere equally good or bad.Model representation deemedModel most consistent with phraseCNNSE 54.5%NNSE 29.5%Both 4.5%Neither 11.5%mensions, while improving the coherence of phraserepresentations.3.3 Evaluation on Behavioral DataWe now compare the performance of various com-position methods on an adjective-noun phrase sim-ilarity dataset (Mitchell and Lapata, 2010).
Thisdataset is comprised of 108 adjective-noun phrasepairs split into high, medium and low similaritygroups.
Similarity scores from 18 human subjectsare averaged to create one similarity score per phrasepair.
We then compute the cosine similarity betweenthe composed phrasal representations of each phrasepair under each compositional model.
As in Mitchelland Lapata (2010), we report the correlation of thecosine similarity measures to the behavioral scores.We withheld 12 of the 108 questions for parame-ter tuning, four randomly selected from each of thehigh, medium and low similarity groups.Table 6 shows the correlation of each model?ssimilarity scores to behavioral similarity scores.Again, Lexfunc performs poorly.
This is proba-bly attributable to the fact that there are, on aver-age, only 39 phrases available for training each ad-jective in the dataset, whereas the original Lexfuncstudy had at least 50 per adjective (Baroni and Zam-parelli, 2010).
CNNSE is the top performer, fol-lowed closely by weighted addition.
Interestingly,weighted NNSE correlation is lower than CNNSEby nearly 0.15, which shows the value of allowingthe learned latent space to conform to the desiredcomposition function.7383.3.1 Interpretability and Phrase SimilarityCNNSE has the additional advantage of inter-pretability.
To illustrate, we created a web pageto explore the dataset under the CNNSE model.The page http://www.cs.cmu.edu/?fmri/papers/naacl2015/cnnse_mitchell_lapata_all.html displays phrase pairs sortedby average similarity score.
For each phrasein the pair we show a summary of the CNNSEcomposed phrase meaning.
The scores of the 10top dimensions are displayed in descending order.Each dimension is described by its interpretablesummarization.
As one scrolls down the page, thesimilarity scores increase, and the number of dimen-sions shared between the phrase pairs (highlightedin red) increases.
Some phrase pairs with highsimilarity scores share no top scoring dimensions.Because we can interpret the dimensions, we canbegin to understand how the CNNSE model isfailing, and how it might be improved.For example, the phrase pair judged most similarby the human subjects, but that shares none of thetop 10 dimensions in common, is ?large number?and ?great majority?
(behavioral similarity score5.61/7).
Upon exploration of CNNSE phrasal repre-sentations, we see that the representation for ?greatmajority?
suffers from the multiple word senses ofmajority.
Majority is often used in political settingsto describe the party or group with larger member-ship.
We see that the top scoring dimension for?great majority?
has top scoring words ?candidacy,candidate, caucus?, a politically-themed dimension.Though the CNNSE representation is not incorrectfor the word, the common theme between the twotest phrases is not political.The second highest scoring dimension for ?largenumber?
is ?First name, address, complete address?.Here we see another case of the collision of multipleword senses, as this dimension is related to identify-ing numbers, rather than the quantity-related senseof number.
While it is satisfying that the word sensesfor majority and number have been separated outinto different dimensions for each word, it is clearthat both the composition and similarity functionsused for this task are not gracefully handling multi-ple word senses.
To address this issue, we could par-tition the dimensions of A into sense-related groupsTable 6: Correlation of phrase similarity judgements(Mitchell and Lapata, 2010) to pairwise distances inseveral adjective-noun composition models.Correlation toModel behavioral dataw.addSVD0.5377w.addNNSE0.4469Lexfunc 0.1347CNNSE 0.5923and use the maximally correlated groups to scorephrase pairs.
CNNSE interpretability allows us toperform these analyses, and will also allow us to it-erate and improve future compositional models.4 ConclusionWe explored a new method to create an interpretableVSMs that respects the notion of semantic compo-sition.
We found that our technique for incorporat-ing phrasal relationship constraints produced a VSMthat is more consistent with observed phrasal repre-sentations and with behavioral data.We found that, compared to NNSE, human eval-uators judged CNNSE phrasal representations to bea better match to phrase meaning.
We leveraged thisimproved interpretability to explore composition inthe context of a previously published compositionaltask.
We note that the collision of word senses of-ten hinders performance on the behavioral data fromMitchell and Lapata (2010).More generally, we have shown that incorporat-ing constraints to represent the task of interest canimprove a model?s performance on that task.
Ad-ditionally, incorporating such constraints into an in-terpretable model allows for a deeper exploration ofperformance in the context of evaluation tasks.AcknowledgmentsThis work was supported in part by a gift fromGoogle, NIH award 5R01HD075328, IARPA awardFA865013C7360, DARPA award FA8750-13-2-0005, and by a fellowship to Alona Fyshe from theMultimodal Neuroimaging Training Program (NIHawards T90DA022761 and R90DA023420).ReferencesMarco Baroni and Roberto Zamparelli.
Nouns arevectors, adjectives are matrices: Representing839adjective-noun constructions in semantic space.In Proceedings of the 2010 Conference on Em-pirical Methods in Natural Language Processing,pages 1183?1193.
Association for ComputationalLinguistics, 2010.Stephen Boyd.
Distributed Optimization and Sta-tistical Learning via the Alternating DirectionMethod of Multipliers.
Foundations and Trendsin Machine Learning, 3(1):1?122, 2010.
ISSN1935-8237.
doi: 10.1561/2200000016.Jamie Callan and Mark Hoy.
The ClueWeb09Dataset, 2009.
URL http://boston.lti.cs.cmu.edu/Data/clueweb09/.Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,Chong Wang, and David M Blei.
Reading TeaLeaves : How Humans Interpret Topic Models.
InAdvances in Neural Information Processing Sys-tems, pages 1?9, 2009.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harsh-man.
Indexing by Latent Semantic Analysis.Journal of the American Society for InformationScience, 41(6):391?407, 1990.S Dejong.
SIMPLS - An alternative approach topartial least squares regression.
Chemometricsand Intelligent Laboratory Systems, 18(3):251?263, 1993.
ISSN 01697439. doi: 10.1016/0169-7439(93)85002-x.Georgiana Dinu, Nghia The Pham, and Marco Ba-roni.
General estimation and evaluation of com-positional distributional semantic models.
InWorkshop on Continuous Vector Space Modelsand their Compositionality, Sofia, Bulgaria, 2013.Bradley Efron, Trevor Hastie, Iain Johnstone, andRobert Tibshirani.
Least angle regression.
Annalsof Statistics, 32(2):407?499, 2004.Alona Fyshe, Partha Talukdar, Brian Murphy, andTom Mitchell.
Documents and Dependencies : anExploration of Vector Space Models for Seman-tic Composition.
In Computational Natural Lan-guage Learning, Sofia, Bulgaria, 2013.Kazuma Hashimoto, Pontus Stenetorp, MakotoMiwa, and Yoshimasa Tsuruoka.
Jointly learn-ing word representations and composition func-tions using predicate-argument structures.
Pro-ceedings of the Conference on Empirical Methodson Natural Language Processing, pages 1544?1555, 2014.Paul B. Kantor and Ellen M. Voorhees.
The TREC-5Confusion Track: Comparing Retrieval Methodsfor Scanned Text.
Information Retrieval, 2:165?176, 2000.
ISSN 1386-4564, 1573-7659. doi:10.1023/A:1009902609570.Omer Levy and Yoav Goldberg.
Neural Word Em-bedding as Implicit Matrix Factorization.
In Ad-vances in Neural Information Processing Sys-tems, pages 1?9, 2014.Julien Mairal, Francis Bach, J Ponce, and GuillermoSapiro.
Online learning for matrix factoriza-tion and sparse coding.
The Journal of MachineLearning Research, 11:19?60, 2010.Tomas Mikolov, Ilya Sutskever, Kai Chen, GregCorrado, and Jeff Dean.
Distributed representa-tions of words and phrases and their composition-ality.
In Proceedings of Neural Information Pro-cessing Systems, pages 1?9, 2013.Jeff Mitchell and Mirella Lapata.
Compositionin distributional models of semantics.
Cogni-tive science, 34(8):1388?429, November 2010.ISSN 1551-6709. doi: 10.1111/j.1551-6709.2010.01106.x.Brian Murphy, Partha Talukdar, and Tom Mitchell.Learning Effective and Interpretable SemanticModels using Non-Negative Sparse Embedding.In Proceedings of Conference on ComputationalLinguistics (COLING), 2012.Jeffrey Pennington, Richard Socher, and Christo-pher D Manning.
GloVe : Global Vectors forWord Representation.
In Conference on Empir-ical Methods in Natural Language Processing,Doha, Qatar, 2014.Magnus Sahlgren.
The Word-Space Model Usingdistributional analysis to represent syntagmaticand paradigmatic relations between words.
Doc-tor of philosophy, Stockholm University, 2006.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
Semantic Composition-ality through Recursive Matrix-Vector Spaces.In Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, 2012.940Peter D Turney.
Domain and Function : A Dual-Space Model of Semantic Relations and Com-positions.
Journal of Artificial Intelligence Re-search, 44:533?585, 2012.1041
