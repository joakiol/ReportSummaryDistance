Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163?171,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPEfficient Minimum Error Rate Training andMinimum Bayes-Risk Decoding forTranslation Hypergraphs and LatticesShankar Kumar1 and Wolfgang Macherey1 and Chris Dyer2 and Franz Och11Google Inc.1600 Amphitheatre Pkwy.Mountain View, CA 94043, USA{shankarkumar,wmach,och}@google.com2Department of LinguisticsUniversity of MarylandCollege Park, MD 20742, USAredpony@umd.eduAbstractMinimum Error Rate Training (MERT)and Minimum Bayes-Risk (MBR) decod-ing are used in most current state-of-the-art Statistical Machine Translation (SMT)systems.
The algorithms were originallydeveloped to work with N -best lists oftranslations, and recently extended to lat-tices that encode many more hypothesesthan typical N -best lists.
We here extendlattice-based MERT and MBR algorithmsto work with hypergraphs that encode avast number of translations produced byMT systems based on Synchronous Con-text Free Grammars.
These algorithmsare more efficient than the lattice-basedversions presented earlier.
We show howMERT can be employed to optimize pa-rameters for MBR decoding.
Our exper-iments show speedups from MERT andMBR as well as performance improve-ments from MBR decoding on several lan-guage pairs.1 IntroductionStatistical Machine Translation (SMT) systemshave improved considerably by directly using theerror criterion in both training and decoding.
Bydoing so, the system can be optimized for thetranslation task instead of a criterion such as like-lihood that is unrelated to the evaluation met-ric.
Two popular techniques that incorporate theerror criterion are Minimum Error Rate Train-ing (MERT) (Och, 2003) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004).These two techniques were originally developedfor N -best lists of translation hypotheses and re-cently extended to translation lattices (Machereyet al, 2008; Tromble et al, 2008) generated by aphrase-based SMT system (Och and Ney, 2004).Translation lattices contain a significantly highernumber of translation alternatives relative to N -best lists.
The extension to lattices reduces theruntimes for both MERT and MBR, and gives per-formance improvements from MBR decoding.SMT systems based on synchronous contextfree grammars (SCFG) (Chiang, 2007; Zollmannand Venugopal, 2006; Galley et al, 2006) haverecently been shown to give competitive perfor-mance relative to phrase-based SMT.
For thesesystems, a hypergraph or packed forest provides acompact representation for encoding a huge num-ber of translation hypotheses (Huang, 2008).In this paper, we extend MERT and MBRdecoding to work on hypergraphs produced bySCFG-based MT systems.
We present algorithmsthat are more efficient relative to the lattice al-gorithms presented in Macherey et al (2008;Tromble et al (2008).
Lattice MBR decoding usesa linear approximation to the BLEU score (Pap-ineni et al, 2001); the weights in this linear lossare set heuristically by assuming that n-gram pre-cisions decay exponentially with n. However, thismay not be optimal in practice.
We employ MERTto select these weights by optimizing BLEU scoreon a development set.A related MBR-inspired approach for hyper-graphs was developed by Zhang and Gildea(2008).
In this work, hypergraphs were rescored tomaximize the expected count of synchronous con-stituents in the translation.
In contrast, our MBRalgorithm directly selects the hypothesis in thehypergraph with the maximum expected approx-imate corpus BLEU score (Tromble et al, 2008).will soon announceX1  X2X1  X2X1  X2X1  X2X1  X2X1 its future in theX1 its future in theSuzukisoonits future inX1 announcesRally World ChampionshipFigure 1: An example hypergraph.1632 Translation HypergraphsA translation lattice compactly encodes a largenumber of hypotheses produced by a phrase-basedSMT system.
The corresponding representationfor an SMT system based on SCFGs (e.g.
Chi-ang (2007), Zollmann and Venugopal (2006), Miet al (2008)) is a directed hypergraph or a packedforest (Huang, 2008).Formally, a hypergraph is a pair H = ?V, E?consisting of a vertex set V and a set of hyperedgesE ?
V?
?
V .
Each hyperedge e ?
E connects ahead vertex h(e) with a sequence of tail verticesT (e) = {v1, ..., vn}.
The number of tail verticesis called the arity (|e|) of the hyperedge.
If the ar-ity of a hyperedge is zero, h(e) is called a sourcevertex.
The arity of a hypergraph is the maximumarity of its hyperedges.
A hyperedge of arity 1 is aregular edge, and a hypergraph of arity 1 is a regu-lar graph (lattice).
Each hyperedge is labeled witha rule re from the SCFG.
The number of nontermi-nals on the right-hand side of re corresponds withthe arity of e. An example without scores is shownin Figure 1.
A path in a translation hypergraph in-duces a translation hypothesis E along with its se-quence of SCFG rules D = r1, r2, ..., rK which,if applied to the start symbol, derives E. The se-quence of SCFG rules induced by a path is alsocalled a derivation tree for E.3 Minimum Error Rate TrainingGiven a set of source sentences FS1 with corre-sponding reference translations RS1 , the objectiveof MERT is to find a parameter set ?
?M1 which min-imizes an automated evaluation criterion under alinear model:?
?M1 = argmin?M1?
SXs=1Err`Rs, E?
(Fs; ?M1 )?ffE?
(Fs; ?M1 ) = argmaxE?
SXs=1?mhm(E, Fs)ff.In the context of statistical machine translation,the optimization procedure was first described inOch (2003) for N -best lists and later extended tophrase-lattices in Macherey et al (2008).
The al-gorithm is based on the insight that, under a log-linear model, the cost function of any candidatetranslation can be represented as a line in the planeif the initial parameter set ?M1 is shifted along adirection dM1 .
Let C = {E1, ..., EK} denote a setof candidate translations, then computing the bestscoring translation hypothesis E?
out of C results inthe following optimization problem:E?
(F ; ?)
= argmaxE?Cn(?M1 + ?
?
dM1 )> ?
hM1 (E,F )o= argmaxE?C?Xm?mhm(E,F )| {z }=a(E,F )+ ?
?Xmdmhm(E,F )| {z }=b(E,F )ff= argmaxE?C?a(E,F ) + ?
?
b(E,F )| {z }(?
)?Hence, the total score (?)
for each candidate trans-lation E ?
C can be described as a line with?
as the independent variable.
For any particu-lar choice of ?, the decoder seeks that translationwhich yields the largest score and therefore corre-sponds to the topmost line segment.
If ?
is shiftedfrom ??
to +?, other translation hypothesesmay at some point constitute the topmost line seg-ments and thus change the decision made by thedecoder.
The entire sequence of topmost line seg-ments is called upper envelope and provides an ex-haustive representation of all possible outcomesthat the decoder may yield if ?
is shifted alongthe chosen direction.
Both the translations andtheir corresponding line segments can efficientlybe computed without incorporating any error crite-rion.
Once the envelope has been determined, thetranslation candidates of its constituent line seg-ments are projected onto their corresponding errorcounts, thus yielding the exact and unsmoothed er-ror surface for all candidate translations encodedin C. The error surface can now easily be traversedin order to find that ??
under which the new param-eter set ?M1 + ??
?
dM1 minimizes the global error.In this section, we present an extension of thealgorithm described in Macherey et al (2008)that allows us to efficiently compute and repre-sent upper envelopes over all candidate transla-tions encoded in hypergraphs.
Conceptually, thealgorithm works by propagating (initially empty)envelopes from the hypergraph?s source nodesbottom-up to its unique root node, thereby ex-panding the envelopes by applying SCFG rules tothe partial candidate translations that are associ-ated with the envelope?s constituent line segments.To recombine envelopes, we need two operators:the sum and the maximum over convex polygons.To illustrate which operator is applied when, wetransform H = ?V, E?
into a regular graph withtyped nodes by (1) marking all vertices v ?
V withthe symbol ?
and (2) replacing each hyperedgee ?
E , |e| > 1, with a small subgraph consistingof a new vertex v?
(e) whose incoming and out-going edges connect the same head and tail nodes164Algorithm 1 ?-operation (Sum)input: associative map a: V ?
Env(V), hyperarc eoutput: Minkowski sum of envelopes over T (e)for (i = 0; i < |T (e)|; ++i) {v = Ti(e);pq.enqueue(?
v, i, 0?
);}L = ?
;D = ?
e, ?1 ?
?
?
?|e|?while (!pq.empty()) {?
v, i, j?
= pq.dequeue();` = A[v][j];D[i+1] = `.D;if (L.empty() ?
L.back().x < `.x) {if (0 < j) {`.y += L.back().y - A[v][j-1].y;`.m += L.back().m - A[v][j-1].m;}L.push_back(`);L.back().D = D;} else {L.back().y += `.y;L.back().m += `.m;L.back().D[i+1] = `.D;if (0 < j) {L.back().y -= A[v][j-1].y;L.back().m -= A[v][j-1].m;}}if (++j < A[v].size())pq.enqueue(?
v, i, j?
);}return L;in the transformed graph as were connected by ein the original graph.
The unique outgoing edgeof v?
(e) is associated with the rule re; incomingedges are not linked to any rule.
Figure 2 illus-trates the transformation for a hyperedge with ar-ity 3.
The graph transformation is isomorphic.The rules associated with every hyperedge spec-ify how line segments in the envelopes of a hyper-edge?s tail nodes can be combined.
Suppose wehave a hyperedge e with rule re : X ?
aX1bX2cand T (e) = {v1, v2}.
Then we substitute X1 andX2 in the rule with candidate translations associ-ated with line segments in envelopes Env(v1) andEnv(v2) respectively.To derive the algorithm, we consider the gen-eral case of a hyperedge e with rule re : X ?w1X1w2...wnXnwn+1.
Because the right-handside of re has n nonterminals, the arity of e is|e| = n. Let T (e) = {v1, ..., vn} denote thetail nodes of e. We now assume that each tailnode vi ?
T (e) is associated with the upper en-velope over all candidate translations that are in-duced by derivations of the corresponding nonter-minal symbol Xi.
These envelopes shall be de-Algorithm 2 ?-operation (Max)input: array L[0..K-1] containing line objectsoutput: upper envelope of LSort(L:m);j = 0; K = size(L);for (i = 0; i < K; ++i) {` = L[i];`.x = -?
;if (0 < j) {if (L[j-1].m == `.m) {if (`.y <= L[j-1].y) continue;--j;}while (0 < j) {`.x = (`.y - L[j-1].y)/(L[j-1].m - `.m);if (L[j-1].x < `.x) break;--j;}if (0 == j) `.x = -?
;L[j++] = `;} else L[j++] = `;}L.resize(j);return L;noted by Env(vi).
To decompose the problem ofcomputing and propagating the tail envelopes overthe hyperedge e to its head node, we now definetwo operations, one for either node type, to spec-ify how envelopes associated with the tail verticesare propagated to the head vertex.Nodes of Type ???
: For a type ?
node, theresulting envelope is the Minkowski sum overthe envelopes of the incoming edges (Berg etal., 2008).
Since the envelopes of the incomingedges are convex hulls, the Minkowski sum pro-vides an upper bound to the number of line seg-ments that constitute the resulting envelope: thebound is the sum over the number of line seg-ments in the envelopes of the incoming edges, i.e.:??Env(v?(e))??
??v?
?T (e)??Env(v?)?
?.Algorithm 1 shows the pseudo code for comput-ing the Minkowski sum over multiple envelopes.The line objects ` used in this algorithm areencoded as 4-tuples, each consisting of the x-intercept with `?s left-adjacent line stored as `.x,the slope `.m, the y-intercept `.y, and the (partial)derivation tree `.D.
At the beginning, the leftmostline segment of each envelope is inserted into apriority queue pq.
The priority is defined in termsof a line?s x-intercept such that lower values implyhigher priority.
Hence, the priority queue enumer-ates all line segments from left to right in ascend-ing order of their x-intercepts, which is the orderneeded to compute the Minkowski sum.Nodes of Type ???
: The operation performed165=?= maxFigure 2: Transformation of a hypergraph intoa factor graph and bottom-up propagation of en-velopes.at nodes of type ???
computes the convex hullover the union of the envelopes propagated overthe incoming edges.
This operation is a ?max?operation and it is identical to the algorithm de-scribed in (Macherey et al, 2008) for phrase lat-tices.
Algorithm 2 contains the pseudo code.The complete algorithm then works as follows:Traversing all nodes in H bottom-up in topolog-ical order, we proceed for each node v ?
V overits incoming hyperedges and combine in each suchhyperedge e the envelopes associated with the tailnodes T (e) by computing their sum according toAlgorithm 1 (?-operation).
For each incominghyperedge e, the resulting envelope is then ex-panded by applying the rule re to its constituentline segments.
The envelopes associated with dif-ferent incoming hyperedges of node v are thencombined and reduced according to Algorithm 2(?-operation).
By construction, the envelope atthe root node is the convex hull over the line seg-ments of all candidate translations that can be de-rived from the hypergraph.The suggested algorithm has similar propertiesas the algorithm presented in (Macherey et al,2008).
In particular, it has the same upper boundon the number of line segments that constitute theenvelope at the root node, i.e, the size of this enve-lope is guaranteed to be no larger than the numberof edges in the transformed hypergraph.4 Minimum Bayes-Risk DecodingWe first review Minimum Bayes-Risk (MBR) de-coding for statistical MT.
An MBR decoder seeksthe hypothesis with the least expected loss under aprobability model (Bickel and Doksum, 1977).
Ifwe think of statistical MT as a classifier that mapsa source sentence F to a target sentence E, theMBR decoder can be expressed as follows:E?
= argminE??G?E?GL(E,E?
)P (E|F ), (1)where L(E,E?)
is the loss between any two hy-potheses E and E?, P (E|F ) is the probabilitymodel, and G is the space of translations (N -bestlist, lattice, or a hypergraph).MBR decoding for translation can be performedby reranking an N -best list of hypotheses gener-ated by an MT system (Kumar and Byrne, 2004).This reranking can be done for any sentence-level loss function such as BLEU (Papineni et al,2001), Word Error Rate, or Position-independentError Rate.Recently, Tromble et al (2008) extendedMBR decoding to translation lattices under anapproximate BLEU score.
They approximatedlog(BLEU) score by a linear function of n-grammatches and candidate length.
If E and E?
are thereference and the candidate translations respec-tively, this linear function is given by:G(E,E?)
= ?0|E?|+?w?|w|#w(E?
)?w(E), (2)where w is an n-gram present in either E or E?,and ?0, ?1, ..., ?N are weights which are deter-mined empirically, where N is the maximum n-gram order.Under such a linear decomposition, the MBRdecoder (Equation 1) can be written asE?
= argmaxE??G?0|E?|+?w?|w|#w(E?
)p(w|G), (3)where the posterior probability of an n-gram in thelattice is given byp(w|G) =?E?G1w(E)P (E|F ).
(4)Tromble et al (2008) implement the MBRdecoder using Weighted Finite State Automata(WFSA) operations.
First, the set of n-gramsis extracted from the lattice.
Next, the posteriorprobability of each n-gram is computed.
A newautomaton is then created by intersecting each n-gram with weight (from Equation 2) to an un-weighted lattice.
Finally, the MBR hypothesis isextracted as the best path in the automaton.
Wewill refer to this procedure as FSAMBR.The above steps are carried out one n-gram ata time.
For a moderately large lattice, there canbe several thousands of n-grams and the proce-dure becomes expensive.
We now present an alter-nate approximate procedure which can avoid this166enumeration making the resulting algorithm muchfaster than FSAMBR.4.1 Efficient MBR for latticesThe key idea behind this new algorithm is torewrite the n-gram posterior probability (Equa-tion 4) as follows:p(w|G) =?E?G?e?Ef(e, w,E)P (E|F ) (5)where f(e, w,E) is a score assigned to edge e onpath E containing n-gram w:f(e, w,E) =??
?1 w ?
e, p(e|G) > p(e?|G),e?
precedes e on E0 otherwise(6)In other words, for each pathE, we count the edgethat contributes n-gramw and has the highest edgeposterior probability relative to its predecessors onthe path E; there is exactly one such edge on eachlattice path E.We note that f(e, w,E) relies on the full pathE which means that it cannot be computed basedon local statistics.
We therefore approximate thequantity f(e, w,E) with f?
(e, w,G) that countsthe edge e with n-gram w that has the highest arcposterior probability relative to predecessors in theentire lattice G.
f?
(e, w,G) can be computed lo-cally, and the n-gram posterior probability basedon f?
can be determined as follows:p(w|G) =XE?GXe?Ef?
(e, w,G)P (E|F ) (7)=Xe?E1w?ef?
(e, w,G)XE?G1E(e)P (E|F )=Xe?E1w?ef?
(e, w,G)P (e|G),where P (e|G) is the posterior probability of a lat-tice edge.
The algorithm to perform Lattice MBRis given in Algorithm 3.
For each node t in the lat-tice, we maintain a quantity Score(w, t) for eachn-gram w that lies on a path from the source nodeto t. Score(w, t) is the highest posterior probabil-ity among all edges on the paths that terminate on tand contain n-gram w. The forward pass requirescomputing the n-grams introduced by each edge;to do this, we propagate n-grams (up to maximumorder ?1) terminating on each node.4.2 Extension to HypergraphsWe next extend the Lattice MBR decoding algo-rithm (Algorithm 3) to rescore hypergraphs pro-duced by a SCFG based MT system.
Algorithm 4is an extension to the MBR decoder on latticesAlgorithm 3 MBR Decoding on Lattices1: Sort the lattice nodes topologically.2: Compute backward probabilities of each node.3: Compute posterior prob.
of each n-gram:4: for each edge e do5: Compute edge posterior probability P (e|G).6: Compute n-gram posterior probs.
P (w|G):7: for each n-gram w introduced by e do8: Propagate n?
1 gram suffix to he.9: if p(e|G) > Score(w, T (e)) then10: Update posterior probs.
and scores:p(w|G) += p(e|G) ?
Score(w, T (e)).Score(w, he) = p(e|G).11: else12: Score(w, he) = Score(w, T (e)).13: end if14: end for15: end for16: Assign scores to edges (given by Equation 3).17: Find best path in the lattice (Equation 3).
(Algorithm 3).
However, there are important dif-ferences when computing the n-gram posteriorprobabilities (Step 3).
In this inside pass, we nowmaintain both n-gram prefixes and suffixes (up tothe maximum order?1) on each hypergraph node.This is necessary because unlike a lattice, new n-grams may be created at subsequent nodes by con-catenating words both to the left and the right sideof the n-gram.
When the arity of the edge is 2,a rule has the general form aX1bX2c, where X1and X2 are sequences from tail nodes.
As a result,we need to consider all new sequences which canbe created by the cross-product of the n-grams onthe two tail nodes.
E.g.
if X1 = {c, cd, d} andX2 = {f, g}, then a total of six sequences willresult.
In practice, such a cross-product is not pro-Algorithm 4 MBR Decoding on Hypergraphs1: Sort the hypergraph nodes topologically.2: Compute inside probabilities of each node.3: Compute posterior prob.
of each hyperedge P (e|G).4: Compute posterior prob.
of each n-gram:5: for each hyperedge e do6: Merge the n-grams on the tail nodes T (e).
If thesame n-gram is present on multiple tail nodes, keepthe highest score.7: Apply the rule on e to the n-grams on T (e).8: Propagate n?
1 gram prefixes/suffixes to he.9: for each n-gram w introduced by this hyperedge do10: if p(e|G) > Score(w, T (e)) then11: p(w|G) += p(e|G) ?
Score(w, T (e))Score(w, he) = p(e|G)12: else13: Score(w, he) = Score(w, T (e))14: end if15: end for16: end for17: Assign scores to hyperedges (Equation 3).18: Find best path in the hypergraph (Equation 3).167hibitive when the maximum n-gram order in MBRdoes not exceed the order of the n-gram languagemodel used in creating the hypergraph.
In the lat-ter case, we will have a small set of unique prefixesand suffixes on the tail nodes.5 MERT for MBR ParameterOptimizationLattice MBR Decoding (Equation 3) assumes alinear form for the gain function (Equation 2).This linear function contains n + 1 parameters?0, ?1, ..., ?N , where N is the maximum order ofthe n-grams involved.
Tromble et al (2008) ob-tained these factors as a function of n-gram preci-sions derived from multiple training runs.
How-ever, this does not guarantee that the resultinglinear score (Equation 2) is close to the corpusBLEU.
We now describe how MERT can be usedto estimate these factors to achieve a better ap-proximation to the corpus BLEU.We recall that MERT selects weights in a lin-ear model to optimize an error criterion (e.g.
cor-pus BLEU) on a training set.
The lattice MBRdecoder (Equation 3) can be written as a lin-ear model: E?
= argmaxE?
?G?Ni=0 ?igi(E?, F ),where g0(E?, F ) = |E?| and gi(E?, F ) =?w:|w|=i #w(E?
)p(w|G).The linear approximation to BLEU may nothold in practice for unseen test sets or language-pairs.
Therefore, we would like to allow the de-coder to backoff to the MAP translation in suchcases.
To do that, we introduce an additional fea-ture function gN+1(E,F ) equal to the original de-coder cost for this sentence.
A weight assignmentof 1.0 for this feature function and zeros for theother feature functions would imply that the MAPtranslation is chosen.
We now have a total ofN+2feature functions which we optimize using MERTto obtain highest BLEU score on a training set.6 ExperimentsWe now describe our experiments to evaluateMERT and MBR on lattices and hypergraphs, andshow how MERT can be used to tune MBR pa-rameters.6.1 Translation TasksWe report results on two tasks.
The first one isthe constrained data track of the NIST Arabic-to-English (aren) and Chinese-to-English (zhen)translation task1.
On this task, the parallel and the1http://www.nist.gov/speech/tests/mtDataset # of sentencesaren zhendev 1797 1664nist02 1043 878nist03 663 919Table 1: Statistics over the NIST dev/test sets.monolingual data included all the allowed train-ing sets for the constrained track.
Table 1 reportsstatistics computed over these data sets.
Our de-velopment set (dev) consists of the NIST 2005 evalset; we use this set for optimizing MBR parame-ters.
We report results on NIST 2002 and NIST2003 evaluation sets.The second task consists of systems for 39language-pairs with English as the target languageand trained on at most 300M word tokens minedfrom the web and other published sources.
The de-velopment and test sets for this task are randomlyselected sentences from the web, and contain 5000and 1000 sentences respectively.6.2 MT System DescriptionOur phrase-based statistical MT system is simi-lar to the alignment template system described in(Och and Ney, 2004; Tromble et al, 2008).
Trans-lation is performed using a standard dynamic pro-gramming beam-search decoder (Och and Ney,2004) using two decoding passes.
The first de-coder pass generates either a lattice or an N -bestlist.
MBR decoding is performed in the secondpass.We also train two SCFG-based MT systems:a hierarchical phrase-based SMT (Chiang, 2007)system and a syntax augmented machine transla-tion (SAMT) system using the approach describedin Zollmann and Venugopal (2006).
Both systemsare built on top of our phrase-based systems.
Inthese systems, the decoder generates an initial hy-pergraph or anN -best list, which are then rescoredusing MBR decoding.6.3 MERT ResultsTable 2 shows runtime experiments for the hyper-graph MERT implementation in comparison withthe phrase-lattice implementation on both the arenand the zhen system.
The first two columns showthe average amount of time in msecs that eitheralgorithm requires to compute the upper envelopewhen applied to phrase lattices.
Compared to thealgorithm described in (Macherey et al, 2008)which is optimized for phrase lattices, the hyper-graph implementation causes a small increase in168Avg.
Runtime/sent [msec](Macherey 2008) Suggested Alg.aren zhen aren zhenphrase lattice 8.57 7.91 10.30 8.65hypergraph ?
?
8.19 8.11Table 2: Average time for computing envelopes.running time.
This increase is mainly due to therepresentation of line segments; while the phrase-lattice implementation stores a single backpointer,the hypergraph version stores a vector of back-pointers.The last two columns show the average amountof time that is required to compute the upper en-velope on hypergraphs.
For comparison, we prunehypergraphs to the same density (# of edges peredge on the best path) and achieve identical run-ning times for computing the error surface.6.4 MBR ResultsWe first compare the new lattice MBR (Algo-rithm 3) with MBR decoding on 1000-best listsand FSAMBR (Tromble et al, 2008) on latticesgenerated by the phrase-based systems; evaluationis done using both BLEU and average run-time persentence (Table 3).
Note that N -best MBR usesa sentence BLEU loss function.
The new latticeMBR algorithm gives about the same performanceas FSAMBR while yielding a 20X speedup.We next report the performance of MBR on hy-pergraphs generated by Hiero/SAMT systems.
Ta-ble 4 compares Hypergraph MBR (HGMBR) withMAP and MBR decoding on 1000 best lists.
Onsome systems such as the Arabic-English SAMT,the gains from Hypergraph MBR over 1000-bestMBR are significant.
In other cases, HypergraphMBR performs at least as well as N -best MBR.In all cases, we observe a 7X speedup in run-time.
This shows the usefulness of HypergraphMBR decoding as an efficient alternative to N -best MBR.6.5 MBR Parameter Tuning with MERTWe now describe the results by tuning MBR n-gram parameters (Equation 2) using MERT.
Wefirst compute N + 1 MBR feature functions oneach edge of the lattice/hypergraph.
We also in-clude the total decoder cost on the edge as as addi-tional feature function.
MERT is then performedto optimize the BLEU score on a development set;For MERT, we use 40 random initial parameters aswell as parameters computed using corpus basedstatistics (Tromble et al, 2008).BLEU (%) Avg.aren zhen timenist03 nist02 nist03 nist02 (ms.)MAP 54.2 64.2 40.1 39.0 -N -best MBR 54.3 64.5 40.2 39.2 3.7Lattice MBRFSAMBR 54.9 65.2 40.6 39.5 3.7LatMBR 54.8 65.2 40.7 39.4 0.2Table 3: Lattice MBR for a phrase-based system.BLEU (%) Avg.aren zhen timenist03 nist02 nist03 nist02 (ms.)HieroMAP 52.8 62.9 41.0 39.8 -N -best MBR 53.2 63.0 41.0 40.1 3.7HGMBR 53.3 63.1 41.0 40.2 0.5SAMTMAP 53.4 63.9 41.3 40.3 -N -best MBR 53.8 64.3 41.7 41.1 3.7HGMBR 54.0 64.6 41.8 41.1 0.5Table 4: Hypergraph MBR for Hiero/SAMT systems.Table 5 shows results for NIST systems.
Wereport results on nist03 set and present three sys-tems for each language pair: phrase-based (pb),hierarchical (hier), and SAMT; Lattice MBR isdone for the phrase-based system while HGMBRis used for the other two.
We select the MBRscaling factor (Tromble et al, 2008) based on thedevelopment set; it is set to 0.1, 0.01, 0.5, 0.2, 0.5and 1.0 for the aren-phrase, aren-hier, aren-samt,zhen-phrase zhen-hier and zhen-samt systems re-spectively.
For the multi-language case, we trainphrase-based systems and perform lattice MBRfor all language pairs.
We use a scaling factor of0.7 for all pairs.
Additional gains can be obtainedby tuning this factor; however, we do not explorethat dimension in this paper.
In all cases, we prunethe lattices/hypergraphs to a density of 30 usingforward-backward pruning (Sixtus and Ortmanns,1999).We consider a BLEU score difference to be a)gain if is at least 0.2 points, b) drop if it is at most-0.2 points, and c) no change otherwise.
The re-sults are shown in Table 6.
In both tables, the fol-lowing results are reported: Lattice/HGMBR withdefault parameters (?5, 1.5, 2, 3, 4) computed us-ing corpus statistics (Tromble et al, 2008),Lattice/HGMBR with parameters derived fromMERT both without/with the baseline model costfeature (mert?b, mert+b).
For multi-languagesystems, we only show the # of language-pairswith gains/no-changes/drops for each MBR vari-ant with respect to the MAP translation.169We observed in the NIST systems that MERTresulted in short translations relative to MAP onthe unseen test set.
To prevent this behavior,we modify the MERT error criterion to includea sentence-level brevity scorer with parameter ?:BLEU+brevity(?).
This brevity scorer penalizeseach candidate translation that is shorter than theaverage length over its reference translations, us-ing a penalty term which is linear in the differencebetween either length.
We tune ?
on the develop-ment set so that the brevity score of MBR transla-tion is close to that of the MAP translation.In the NIST systems, MERT yields small im-provements on top of MBR with default param-eters.
This is the case for Arabic-English Hi-ero/SAMT.
In all other cases, we see no changeor even a slight degradation due to MERT.We hypothesize that the default MBR parame-ters (Tromble et al, 2008) are well tuned.
There-fore there is little gain by additional tuning usingMERT.In the multi-language systems, the results showa different trend.
We observe that MBR with de-fault parameters results in gains on 18 pairs, nodifferences on 9 pairs, and losses on 12 pairs.When we optimize MBR features with MERT, thenumber of language pairs with gains/no changes/-drops is 22/5/12.
Thus, MERT has a bigger impacthere than in the NIST systems.
We hypothesizethat the default MBR parameters are sub-optimalfor some language pairs and that MERT helps tofind better parameter settings.
In particular, MERTavoids the need for manually tuning these param-eters by language pair.Finally, when baseline model costs are addedas an extra feature (mert+b), the number of pairswith gains/no changes/drops is 26/8/5.
This showsthat this feature can allow MBR decoding to back-off to the MAP translation.
When MBR does notproduce a higher BLEU score relative to MAPon the development set, MERT assigns a higherweight to this feature function.
We see such aneffect for 4 systems.7 DiscussionWe have presented efficient algorithmswhich extend previous work on lattice-basedMERT (Macherey et al, 2008) and MBR de-coding (Tromble et al, 2008) to work withhypergraphs.
Our new MERT algorithm can workwith both lattices and hypergraphs.
On lattices, itachieves similar run-times as the implementationSystem BLEU (%)MAP MBRdefault mert-b mert+baren.pb 54.2 54.8 54.8 54.9aren.hier 52.8 53.3 53.5 53.7aren.samt 53.4 54.0 54.4 54.0zhen.pb 40.1 40.7 40.7 40.9zhen.hier 41.0 41.0 41.0 41.0zhen.samt 41.3 41.8 41.6 41.7Table 5: MBR Parameter Tuning on NIST systemsMBR wrt.
MAP default mert-b mert+b# of gains 18 22 26# of no-changes 9 5 8# of drops 12 12 5Table 6: MBR on Multi-language systems.described in Macherey et al (2008).
The newLattice MBR decoder achieves a 20X speeduprelative to either FSAMBR implementationdescribed in Tromble et al (2008) or MBR on1000-best lists.
The algorithm gives comparableresults relative to FSAMBR.
On hypergraphsproduced by Hierarchical and Syntax AugmentedMT systems, our MBR algorithm gives a 7Xspeedup relative to 1000-best MBR while givingcomparable or even better performance.Lattice MBR decoding is obtained under a lin-ear approximation to BLEU, where the weightsare obtained using n-gram precisions derived fromdevelopment data.
This may not be optimal inpractice for unseen test sets and language pairs,and the resulting linear loss may be quite differ-ent from the corpus level BLEU.
In this paper, wehave described how MERT can be employed toestimate the weights for the linear loss functionto maximize BLEU on a development set.
On anexperiment with 40 language pairs, we obtain im-provements on 26 pairs, no difference on 8 pairsand drops on 5 pairs.
This was achieved with-out any need for manual tuning for each languagepair.
The baseline model cost feature helps the al-gorithm effectively back off to the MAP transla-tion in language pairs where MBR features alonewould not have helped.MERT and MBR decoding are popular tech-niques for incorporating the final evaluation met-ric into the development of SMT systems.
We be-lieve that our efficient algorithms will make themmore widely applicable in both SCFG-based andphrase-based MT systems.170ReferencesM.
Berg, O. Cheong, M. Krefeld, and M. Overmars,2008.
Computational Geometry: Algorithms andApplications, chapter 13, pages 290?296.
Springer-Verlag, 3rd edition.P.
J. Bickel and K. A. Doksum.
1977.
MathematicalStatistics: Basic Ideas and Selected topics.
Holden-Day Inc., Oakland, CA, USA.D.
Chiang.
2007.
Hierarchical phrase based transla-tion .
Computational Linguistics, 33(2):201 ?
228.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,W.
Wang, and I. Thayer.
2006.
Scalable Inferenceand Training of Context-Rich Syntactic TranslationModels.
.
In COLING/ACL, Sydney, Australia.L.
Huang.
2008.
Advanced Dynamic Programmingin Semiring and Hypergraph Frameworks.
In COL-ING, Manchester, UK.S.
Kumar and W. Byrne.
2004.
Minimum Bayes-Risk Decoding for Statistical Machine Translation.In HLT-NAACL, Boston, MA, USA.W.
Macherey, F. Och, I. Thayer, and J. Uszkoreit.2008.
Lattice-based Minimum Error Rate Train-ing for Statistical Machine Translation.
In EMNLP,Honolulu, Hawaii, USA.H.
Mi, L. Huang, and Q. Liu.
2008.
Forest-BasedTranslation.
In ACL, Columbus, OH, USA.F.
Och and H. Ney.
2004.
The Alignment TemplateApproach to Statistical Machine Translation.
Com-putational Linguistics, 30(4):417 ?
449.F.
Och.
2003.
Minimum Error Rate Training in Statis-tical Machine Translation.
In ACL, Sapporo, Japan.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2001.Bleu: a Method for Automatic Evaluation of Ma-chine Translation.
Technical Report RC22176(W0109-022), IBM Research Division.A.
Sixtus and S. Ortmanns.
1999.
High QualityWord Graphs Using Forward-Backward Pruning.
InICASSP, Phoenix, AZ, USA.R.
Tromble, S. Kumar, F. Och, andW.Macherey.
2008.Lattice Minimum Bayes-Risk Decoding for Statis-tical Machine Translation.
In EMNLP, Honolulu,Hawaii.H.
Zhang and D. Gildea.
2008.
Efficient Multi-passDecoding for Synchronous Context Free Grammars.In ACL, Columbus, OH, USA.A.
Zollmann and A. Venugopal.
2006.
Syntax Aug-mented Machine Translation via Chart Parsing.
InHLT-NAACL, New York, NY, USA.171
