Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 52?60,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsSemantic Role Labeling for Open Information ExtractionJanara Christensen, Mausam, Stephen Soderland and Oren EtzioniUniversity of Washington, SeattleAbstractOpen Information Extraction is a recentparadigm for machine reading from arbitrarytext.
In contrast to existing techniques, whichhave used only shallow syntactic features, weinvestigate the use of semantic features (se-mantic roles) for the task of Open IE.
We com-pare TEXTRUNNER (Banko et al, 2007), astate of the art open extractor, with our novelextractor SRL-IE, which is based on UIUC?sSRL system (Punyakanok et al, 2008).
Wefind that SRL-IE is robust to noisy heteroge-neous Web data and outperforms TEXTRUN-NER on extraction quality.
On the otherhand, TEXTRUNNER performs over 2 ordersof magnitude faster and achieves good pre-cision in high locality and high redundancyextractions.
These observations enable theconstruction of hybrid extractors that outputhigher quality results than TEXTRUNNER andsimilar quality as SRL-IE in much less time.1 IntroductionThe grand challenge of Machine Reading (Etzioniet al, 2006) requires, as a key step, a scalablesystem for extracting information from large, het-erogeneous, unstructured text.
The traditional ap-proaches to information extraction (e.g., (Soderland,1999; Agichtein and Gravano, 2000)) do not oper-ate at these scales, since they focus attention on awell-defined small set of relations and require largeamounts of training data for each relation.
The re-cent Open Information Extraction paradigm (Bankoet al, 2007) attempts to overcome the knowledgeacquisition bottleneck with its relation-independentnature and no manually annotated training data.We are interested in the best possible techniquefor Open IE.
The TEXTRUNNER Open IE system(Banko and Etzioni, 2008) employs only shallowsyntactic features in the extraction process.
Avoid-ing the expensive processing of deep syntactic anal-ysis allowed TEXTRUNNER to process at Web scale.In this paper, we explore the benefits of semanticfeatures and in particular, evaluate the application ofsemantic role labeling (SRL) to Open IE.SRL is a popular NLP task that has seen sig-nificant progress over the last few years.
The ad-vent of hand-constructed semantic resources such asPropbank and Framenet (Martha and Palmer, 2002;Baker et al, 1998) have resulted in semantic role la-belers achieving high in-domain precisions.Our first observation is that semantically labeledarguments in a sentence almost always correspondto the arguments in Open IE extractions.
Similarly,the verbs often match up with Open IE relations.These observations lead us to construct a new OpenIE extractor based on SRL.
We use UIUC?s publiclyavailable SRL system (Punyakanok et al, 2008) thatis known to be competitive with the state of the artand construct a novel Open IE extractor based on itcalled SRL-IE.We first need to evaluate SRL-IE?s effectivenessin the context of large scale and heterogeneous inputdata as found on the Web: because SRL uses deeperanalysis we expect SRL-IE to be much slower.
Sec-ond, SRL is trained on news corpora using a re-source like Propbank, and so may face recall lossdue to out of vocabulary verbs and precision loss dueto different writing styles found on the Web.In this paper we address several empirical ques-52tions.
Can SRL-IE, our SRL based extractor,achieve adequate precision/recall on the heteroge-neous Web text?
What factors influence the relativeperformance of SRL-IE vs. that of TEXTRUNNER(e.g., n-ary vs. binary extractions, redundancy, local-ity, sentence length, out of vocabulary verbs, etc.
)?In terms of performance, what are the relative trade-offs between the two?
Finally, is it possible to designa hybrid between the two systems to get the best ofboth the worlds?
Our results show that:1.
SRL-IE is surprisingly robust to noisy hetero-geneous data and achieves high precision andrecall on the Open IE task on Web text.2.
SRL-IE outperforms TEXTRUNNER along di-mensions such as recall and precision on com-plex extractions (e.g., n-ary relations).3.
TEXTRUNNER is over 2 orders of magnitudefaster, and achieves good precision for extrac-tions with high system confidence or high lo-cality or when the same fact is extracted frommultiple sentences.4.
Hybrid extractors that use a combination ofSRL-IE and TEXTRUNNER get the best ofboth worlds.
Our hybrid extractors make effec-tive use of available time and achieve a supe-rior balance of precision-recall, better precisioncompared to TEXTRUNNER, and better recallcompared to both TEXTRUNNER and SRL-IE.2 BackgroundOpen Information Extraction: The recently pop-ular Open IE (Banko et al, 2007) is an extractionparadigm where the system makes a single data-driven pass over its corpus and extracts a largeset of relational tuples without requiring any hu-man input.
These tuples attempt to capture thesalient relationships expressed in each sentence.
Forinstance, for the sentence, ?McCain fought hardagainst Obama, but finally lost the election?
anOpen IE system would extract two tuples <McCain,fought (hard) against, Obama>, and <McCain, lost,the election>.
These tuples can be binary or n-ary,where the relationship is expressed between morethan 2 entities such as <Gates Foundation, invested(arg) in, 1 billion dollars, high schools>.TEXTRUNNER is a state-of-the-art Open IE sys-tem that performs extraction in three key steps.
(1)A self-supervised learner that outputs a CRF basedclassifier (that uses unlexicalized features) for ex-tracting relationships.
The self-supervised nature al-leviates the need for hand-labeled training data andunlexicalized features help scale to the multitudes ofrelations found on the Web.
(2) A single pass extrac-tor that uses shallow syntactic techniques like part ofspeech tagging, noun phrase chunking and then ap-plies the CRF extractor to extract relationships ex-pressed in natural language sentences.
The use ofshallow features makes TEXTRUNNER highly effi-cient.
(3) A redundancy based assessor that re-ranksthese extractions based on a probabilistic model ofredundancy in text (Downey et al, 2005).
This ex-ploits the redundancy of information in Web text andassigns higher confidence to extractions occurringmultiple times.
All these components enable TEX-TRUNNER to be a high performance, general, andhigh quality extractor for heterogeneous Web text.Semantic Role Labeling: SRL is a common NLPtask that consists of detecting semantic argumentsassociated with a verb in a sentence and their classi-fication into different roles (such as Agent, Patient,Instrument, etc.).
Given the sentence ?The pearlsI left to my son are fake?
an SRL system wouldconclude that for the verb ?leave?, ?I?
is the agent,?pearls?
is the patient and ?son?
is the benefactor.Because not all roles feature in each verb the rolesare commonly divided into meta-roles (A0-A7) andadditional common classes such as location, time,etc.
Each Ai can represent a different role basedon the verb, though A0 and A1 most often refer toagents and patients respectively.
Availability of lexi-cal resources such as Propbank (Martha and Palmer,2002), which annotates text with meta-roles for eachargument, has enabled significant progress in SRLsystems over the last few years.Recently, there have been many advances in SRL(Toutanova et al, 2008; Johansson and Nugues,2008; Coppola et al, 2009; Moschitti et al, 2008).We use UIUC-SRL as our base SRL system (Pun-yakanok et al, 2008).
Our choice of the system isguided by the fact that its code is freely available andit is competitive with state of the art (it achieved thehighest F1 score on the CoNLL-2005 shared task).UIUC-SRL operates in four key steps: pruning,argument identification, argument classification and53inference.
Pruning involves using a full parse treeand heuristic rules to eliminate constituents that areunlikely to be arguments.
Argument identificationuses a classifier to identify constituents that are po-tential arguments.
In argument classification, an-other classifier is used, this time to assign role labelsto the candidates identified in the previous stage.
Ar-gument information is not incorporated across argu-ments until the inference stage, which uses an inte-ger linear program to make global role predictions.3 SRL-IEOur key insight is that semantically labeled argu-ments in a sentence almost always correspond to thearguments in Open IE extractions.
Thus, we canconvert the output of UIUC-SRL into an Open IEextraction.
We illustrate this conversion process viaan example.Given the sentence, ?Eli Whitney created the cot-ton gin in 1793,?
TEXTRUNNER extracts two tuples,one binary and one n-ary, as follows:binary tuple:arg0 Eli Whitneyrel createdarg1 the cotton ginn-ary tuple:arg0 Eli Whitneyrel created (arg) inarg1 the cotton ginarg2 1793UIUC-SRL labels constituents of a sentence withthe role they play in regards to the verb in the sen-tence.
UIUC-SRL will extract:A0 Eli Whitneyverb createdA1 the cotton gintemporal in 1793To convert UIUC-SRL output to Open IE format,SRL-IE treats the verb (along with its modifiers andnegation, if present) as the relation.
Moreover, itassumes SRL?s role-labeled arguments as the OpenIE arguments related to the relation.
The argumentshere consist of all entities labeled Ai, as well as anyentities that are marked Direction, Location, or Tem-poral.
We order the arguments in the same order asthey are in the sentence and with regard to the re-lation (except for direction, location and temporal,which cannot be arg0 of an Open IE extraction andare placed at the end of argument list).
As we areinterested in relations, we consider only extractionsthat have at least two arguments.In doing this conversion, we naturally ignore partof the semantic information (such as distinctions be-tween various Ai?s) that UIUC-SRL provides.
Inthis conversion process an SRL extraction that wascorrect in the original format will never be changedto an incorrect Open IE extraction.
However, an in-correctly labeled SRL extraction could still convertto a correct Open IE extraction, if the argumentswere correctly identified but incorrectly labeled.Because of the methodology that TEXTRUNNERuses to extract relations, for n-ary extractions of theform <arg0, rel, arg1, ..., argN>, TEXTRUNNERoften extracts sub-parts <arg0, rel, arg1>, <arg0,rel, arg1, arg2>, ..., <arg0, rel, arg1, ..., argN-1>.UIUC-SRL, however, extracts at most only one re-lation for each verb in the sentence.
For a fair com-parison, we create additional subpart extractions foreach UIUC-SRL extraction using a similar policy.4 Qualitative Comparison of ExtractorsIn order to understand SRL-IE better, we first com-pare with TEXTRUNNER in a variety of scenarios,such as sentences with lists, complex sentences, sen-tences with out of vocabulary verbs, etc.Argument boundaries: SRL-IE is lenient in de-ciding what constitutes an argument and tends toerr on the side of including too much rather thantoo little; TEXTRUNNER is much more conservative,sometimes to the extent of omitting crucial informa-tion, particularly post-modifying clauses and PPs.For example, TEXTRUNNER extracts <Bunsen, in-vented, a device> from the sentence ?Bunsen in-vented a device called the Spectroscope?.
SRL-IEincludes the entire phrase ?a device called the Spec-troscope?
as the second argument.
Generally, thelonger arguments in SRL-IE are more informativethan TEXTRUNNER?s succinct ones.
On the otherhand, TEXTRUNNER?s arguments normalize betterleading to an effective use of redundancy in ranking.Lists: In sentences with a comma-separated lists ofnouns, SRL-IE creates one extraction and treats theentire list as the argument, whereas TEXTRUNNERseparates them into several relations, one for eachitem in the list.Out of vocabulary verbs: While we expected54TEXTRUNNER to handle unknown verbs with lit-tle difficulty due to its unlexicalized nature, SRL-IE could have had severe trouble leading to a lim-ited applicability in the context of Web text.
How-ever, contrary to our expectations, UIUC-SRL hasa graceful policy to handle new verbs by attempt-ing to identify A0 (the agent) and A1 (the patient)and leaving out the higher numbered ones.
In prac-tice, this is very effective ?
SRL-IE recognizes theverb and its two arguments correctly in ?Larry Pagegoogled his name and launched a new revolution.
?Part-of-speech ambiguity: Both SRL-IE andTEXTRUNNER have difficulty when noun phraseshave an identical spelling with a verb.
For example,the word ?write?
when used as a noun causes troublefor both systems.
In the sentence, ?Be sure the filehas write permission.?
SRL-IE and TEXTRUNNERboth extract <the file, write, permission>.Complex sentences: Because TEXTRUNNER onlyuses shallow syntactic features it has a harder timeon sentences with complex structure.
SRL-IE,because of its deeper processing, can better handlecomplex syntax and long-range dependencies, al-though occasionally complex sentences will createparsing errors causing difficulties for SRL-IE.N-ary relations: Both extractors suffer significantquality loss in n-ary extractions compared to binary.A key problem is prepositional phrase attachment,deciding whether the phrase associates with arg1 orwith the verb.5 Experimental ResultsIn our quantitative evaluation we attempt to answertwo key questions: (1) what is the relative differencein performance of SRL-IE and TEXTRUNNER onprecision, recall and computation time?
And, (2)what factors influence the relative performance ofthe two systems?
We explore the first question inSection 5.2 and the second in Section 5.3.5.1 DatasetOur goal is to explore the behavior of TEXTRUN-NER and SRL-IE on a large scale dataset containingredundant information, since redundancy has beenshown to immensely benefit Web-based Open IE ex-tractors.
At the same time, the test set must be amanageable size, due to SRL-IE?s relatively slowprocessing time.
We constructed a test set that ap-proximates Web-scale distribution of extractions forfive target relations ?
invent, graduate, study, write,and develop.We created our test set as follows.
We queried acorpus of 500M Web documents for a sample of sen-tences with these verbs (or their inflected forms, e.g.,invents, invented, etc.).
We then ran TEXTRUNNERand SRL-IE on those sentences to find 200 distinctvalues of arg0 for each target relation, 100 from eachsystem.
We searched for at most 100 sentences thatcontain both the verb-form and arg0.
This resultedin a test set of an average of 6,000 sentences per re-lation, for a total of 29,842 sentences.
We use thistest set for all experiments in this paper.In order to compute precision and recall on thisdataset, we tagged extractions by TEXTRUNNERand by SRL-IE as correct or errors.
A tuple is cor-rect if the arguments have correct boundaries andthe relation accurately expresses the relationship be-tween all of the arguments.
Our definition of cor-rect boundaries does not favor either system overthe other.
For instance, while TEXTRUNNER ex-tracts <Bunsen, invented, a device> from the sen-tence ?Bunsen invented a device called the Spectro-scope?, and SRL-IE includes the entire phrase ?adevice called the Spectroscope?
as the second argu-ment, both extractions would be marked as correct.Determining the absolute recall in these experi-ments is precluded by the amount of hand labelingnecessary and the ambiguity of such a task.
Instead,we compute pseudo-recall by taking the union ofcorrect tuples from both methods as denominator.15.2 Relative PerformanceTable 1 shows the performance of TEXTRUNNERand SRL-IE on this dataset.
Since TEXTRUNNERcan output different points on the precision-recallcurve based on the confidence of the CRF we choosethe point that maximizes F1.SRL-IE achieved much higher recall at substan-tially higher precision.
This was, however, at thecost of a much larger processing time.
For ourdataset, TEXTRUNNER took 6.3 minutes and SRL-1Tuples from the two systems are considered equivalent iffor the relation and each argument, the extracted phrases areequal or if one phrase is contained within the phrase extractedby the other system.55TEXTRUNNER SRL-IEP R F1 P R F1Binary 51.9 27.2 35.7 64.4 85.9 73.7N-ary 39.3 28.2 32.9 54.4 62.7 58.3All 47.9 27.5 34.9 62.1 79.9 69.9Time 6.3 minutes 52.1 hoursTable 1: SRL-IE outperforms TEXTRUNNER in both re-call and precision, but has over 2.5 orders of magnitudelonger run time.IE took 52.1 hours ?
roughly 2.5 orders of magni-tude longer.
We ran our experiments on quad-core2.8GHz processors with 4GB of memory.It is important to note that our results for TEX-TRUNNER are different from prior results (Banko,2009).
This is primarily due to a few operationalcriteria (such as focusing on proper nouns, filteringrelatively infrequent extractions) identified in priorwork that resulted in much higher precision, proba-bly at significant cost of recall.5.3 Comparison under Different ConditionsAlthough SRL-IE has higher overall precision,there are some conditions under which TEXTRUN-NER has superior precision.
We analyze the perfor-mance of these two systems along three key dimen-sions: system confidence, redundancy, and locality.System Confidence: TEXTRUNNER?s CRF-basedextractor outputs a confidence score which can bevaried to explore different points in the precision-recall space.
Figure 1(a) and Figure 2(a) report theresults from ranking extractions by this confidencevalue.
For both binary and n-ary extractions the con-fidence value improves TEXTRUNNER?s precisionand for binary the high precision end has approxi-mately the same precision as SRL-IE.
Because ofits use of an integer linear program, SRL-IE doesnot associate confidence values with extractions andis shown as a point in these figures.Redundancy: In this experiment we use the re-dundancy of extractions as a measure of confidence.Here redundancy is the number of times a relationhas been extracted from unique sentences.
We com-pute redundancy over normalized extractions, ignor-ing noun modifiers, adverbs, and verb inflection.Figure 1(b) and Figure 2(b) display the results forbinary and n-ary extractions, ranked by redundancy.We use a log scale on the x-axis since high redun-dancy extractions account for less than 1% of therecall.
For binary extractions, redundancy improvedTEXTRUNNER?s precision significantly, but at a dra-matic loss in recall.
TEXTRUNNER achieved 0.8precision with 0.001 recall at redundancy of 10 andhigher.
For highly redundant information (commonconcepts, etc.)
TEXTRUNNER has higher precisionthan SRL-IE and would be the algorithm of choice.In n-ary relations for TEXTRUNNER and in binaryrelations for SRL-IE, redundancy actually hurtsprecision.
These extractions tend to be so specificthat genuine redundancy is rare, and the highest fre-quency extractions are often systematic errors.
Forexample, the most frequent SRL-IE extraction was<nothing, write, home>.Locality: Our experiments with TEXTRUNNER ledus to discover a new validation scheme for the ex-tractions ?
locality.
We observed that TEXTRUN-NER?s shallow features can identify relations morereliably when the arguments are closer to each otherin the sentence.
Figure 1(c) and Figure 2(c) reportthe results from ranking extractions by the numberof tokens that separate the first and last arguments.We find a clear correlation between locality andprecision of TEXTRUNNER, with precision 0.77 atrecall 0.18 for TEXTRUNNER where the distance is4 tokens or less for binary extractions.
For n-ary re-lations, TEXTRUNNER can match SRL-IE?s preci-sion of 0.54 at recall 0.13.
SRL-IE remains largelyunaffected by locality, probably due to the parsingused in SRL.6 A TEXTRUNNER SRL-IE HybridWe now present two hybrid systems that combinethe strengths of TEXTRUNNER (fast processing timeand high precision on a subset of sentences) with thestrengths of SRL-IE (higher recall and better han-dling of long-range dependencies).
This is set in ascenario where we have a limited budget on com-putational time and we need a high performance ex-tractor that utilizes the available time efficiently.Our approach is to run TEXTRUNNER on all sen-tences, and then determine the order in which to pro-cess sentences with SRL-IE.
We can increase preci-sion by filtering out TEXTRUNNER extractions thatare expected to have low precision.560.0 0.2 0.4 0.6 0.8 1.00.00.40.8RecallPrecisionTextRunnerSRL?IE1e?04 1e?03 1e?02 1e?01 1e+000.00.40.8RecallPrecisionTextRunnerSRL?IE0.0 0.2 0.4 0.6 0.8 1.00.00.40.8RecallPrecisionTextRunnerSRL?IEFigure 1: Ranking mechanisms for binary relations.
(a) The confidence specified by the CRF improves TEXTRUN-NER?s precision.
(b) For extractions with highest redundancy, TEXTRUNNER has higher precision than SRL-IE.
Notethe log scale for the x-axis.
(c) Ranking by the distance between arguments gives a large boost to TEXTRUNNER?sprecision.0.0 0.2 0.4 0.6 0.8 1.00.00.40.8RecallPrecisionTextRunnerSRL?IE1e?04 1e?03 1e?02 1e?01 1e+000.00.40.8RecallPrecisionTextRunnerSRL?IE0.0 0.2 0.4 0.6 0.8 1.00.00.40.8RecallPrecisionTextRunnerSRL?IEFigure 2: Ranking mechanisms for n-ary relations.
(a) Ranking by confidence gives a slight boost to TEXTRUNNER?sprecision.
(b) Redundancy helps SRL-IE, but not TEXTRUNNER.
Note the log scale for the x-axis.
(c) Ranking bydistance between arguments raises precision for TEXTRUNNER and SRL-IE.A naive hybrid will run TEXTRUNNER over allthe sentences and use the remaining time to runSRL-IE on a random subset of the sentences andtake the union of all extractions.
We refer to thisversion as RECALLHYBRID, since this does not loseany extractions, achieving highest possible recall.A second hybrid, which we call PRECHYBRID,focuses on increasing the precision and uses the fil-ter policy and an intelligent order of sentences forextraction as described below.Filter Policy for TEXTRUNNER Extractions: Theresults from Figure 1 and Figure 2 show that TEX-TRUNNER?s precision is low when the CRF confi-dence in the extraction is low, when the redundancyof the extraction is low, and when the arguments arefar apart.
Thus, system confidence, redundancy, andlocality form the key factors for our filter policy: ifthe confidence is less than 0.5 and the redundancyis less than 2 or the distance between the argumentsin the sentence is greater than 5 (if the relation isbinary) or 8 (if the relation is n-ary) discard this tu-ple.
These thresholds were determined by a param-eter search over a small dataset.Order of Sentences for Extraction: An optimalordering policy would apply SRL-IE first to the sen-tences where TEXTRUNNER has low precision andleave the sentences that seem malformed (e.g., in-complete sentences, two sentences spliced together)for last.
As we have seen, the distance between thefirst and last argument is a good indicator for TEX-TRUNNER precision.
Moreover, a confidence valueof 0.0 by TEXTRUNNER?s CRF classifier is good ev-idence that the sentence may be malformed and isunlikely to contain a valid relation.We rank sentences S in the following way, withSRL-IE processing sentences from highest rankingto lowest: if CRF.confidence = 0.0 then S.rank = 0,else S.rank = average distance between pairs of ar-guments for all tuples extracted by TEXTRUNNERfrom S.While this ranking system orders sentences ac-cording to which sentence is likely to yield maxi-mum new information, it misses the cost of compu-tation.
To account for computation time, we alsoestimate the amount of time SRL-IE will take toprocess each sentence using a linear model trainedon the sentence length.
We then choose the sentence57that maximizes information gain divided by compu-tation time.6.1 Properties of Hybrid ExtractorsThe choice between the two hybrid systems is atrade-off between recall and precision: RECALLHY-BRID guarantees the best recall, since it does not loseany extractions, while PRECHYBRID is designed tomaximize the early boost in precision.
The evalua-tion in the next section bears out these expectations.6.2 Evaluation of Hybrid ExtractorsFigure 3(a) and Figure 4(a) report the precision ofeach system for binary and n-ary extractions mea-sured against available computation time.
PRECHY-BRID starts at slightly higher precision due to ourfiltering of potentially low quality extractions fromTEXTRUNNER.
For binary this precision is evenbetter than SRL-IE?s.
It gradually loses precisionuntil it reaches SRL-IE?s level.
RECALLHYBRIDimproves on TEXTRUNNER?s precision, albeit at amuch slower rate and remains worse than SRL-IEand PRECHYBRID throughout.The recall for binary and n-ary extractions areshown in Figure 3(b) and Figure 4(b), again mea-sured against available time.
While PRECHYBRIDsignificantly improves on TEXTRUNNER?s recall, itdoes lose recall compared to RECALLHYBRID, es-pecially for n-ary extractions.
PRECHYBRID alsoshows a large initial drop in recall due to filtering.Lastly, the gains in precision from PRECHYBRIDare offset by loss in recall that leaves the F1 mea-sure essentially identical to that of RECALLHYBRID(Figures 3(c),4(c)).
However, for a fixed time bud-get both hybrid F-measures are significantly bet-ter than TEXTRUNNER and SRL-IE F-measuresdemonstrating the power of the hybrid extractors.Both methods reach a much higher F1 than TEX-TRUNNER: a gain of over 0.15 in half SRL-IE?sprocessing time and over 0.3 after the full process-ing time.
Both hybrids perform better than SRL-IEgiven equal processing time.We believe that most often constructing a higherquality database of facts with a relatively lowerrecall is more useful than vice-versa, makingPRECHYBRID to be of wider applicability than RE-CALLHYBRID.
Still the choice of the actual hybridextractor could change based on the task.7 Related WorkOpen information extraction is a relatively recentparadigm and hence, has been studied by only asmall number of researchers.
The most salient isTEXTRUNNER, which also introduced the model(Banko et al, 2007; Banko and Etzioni, 2008).A version of KNEXT uses heuristic rules and syn-tactic parses to convert a sentence into an unscopedlogical form (Van Durme and Schubert, 2008).
Thiswork is more suitable for extracting common senseknowledge as opposed to factual information.Another Open IE system, Kylin (Weld et al,2008), suggests automatically building an extractorfor each relation using self-supervised training, withtraining data generated using Wikipedia infoboxes.This work has the limitation that it can only extractrelations expressed in Wikipedia infoboxes.A paradigm related to Open IE is Preemptive IE(Shinyama and Sekine, 2006).
While one goal ofPreemptive IE is to avoid relation-specificity, Pre-emptive IE does not emphasize Web scalability,which is essential to Open IE.
(Carlson et al, 2009) presents a semi-supervisedapproach to information extraction on the Web.
Itlearns classifiers for different relations and couplesthe training of those classifiers with ontology defin-ing constraints.
While we attempt to learn unknownrelations, it learns a pre-defined set of relations.Another related system is WANDERLUST (Akbikand Bro?, 2009).
The authors of this system anno-tated 10,000 sentences parsed with LinkGrammar,resulting in 46 general linkpaths as patterns for rela-tion extraction.
With these patterns WANDERLUSTextracts binary relations from link grammar link-ages.
In contrast to our approaches, this requires alarge set of hand-labeled examples.USP (Poon and Domingos, 2009) is based onMarkov Logic Networks and attempts to create afull semantic parse in an unsupervised fashion.
Theyevaluate their work on biomedical text, so its appli-cability to general Web text is not yet clear.8 Discussion and Future WorkThe Heavy Tail: It is well accepted that informa-tion on the Web is distributed according to Zipf?s580 10 20 30 40 500.00.20.40.6Time (hours)PrecisionTextRunnerSRL?IERecallHybridPrecHybrid0 10 20 30 40 500.00.40.8Time (hours)RecallTextRunnerSRL?IERecallHybridPrecHybrid0 10 20 30 40 500.00.40.8Time (hours)F?measureTextRunnerSRL?IERecallHybridPrecHybridFigure 3: (a) Precision for binary extractions for PRECHYBRID starts higher than the precision of SRL-IE.
(b) Recallfor binary extractions rises over time for both hybrid systems, with PRECHYBRID starting lower.
(c) Hybrid extractorsobtain the best F-measure given a limited budget of computation time.0 10 20 30 40 500.00.20.40.6Time (hours)PrecisionTextRunnerSRL?IERecallHybridPrecHybrid0 10 20 30 40 500.00.40.8Time (hours)RecallTextRunnerSRL?IERecallHybridPrecHybrid0 10 20 30 40 500.00.40.8Time (hours)F?measureTextRunnerSRL?IERecallHybridPrecHybridFigure 4: (a) PRECHYBRID also gives a strong boost to precision for n-ary extractions.
(b) Recall for n-ary extractionsfor RECALLHYBRID starts substantially higher than PRECHYBRID and finally reaches much higher recall than SRL-IE alone.
(c) F-measure for n-ary extractions.
The hybrid extractors outperform others.Law (Downey et al, 2005), implying that there is aheavy tail of facts that are mentioned only once ortwice.
The prior work on Open IE ascribes primeimportance to redundancy based validation, which,as our results show (Figures 1(b), 2(b)), captures avery tiny fraction of the available information.
Webelieve that deeper processing of text is essential togather information from this heavy tail.
Our SRL-IE extractor is a viable algorithm for this task.Understanding SRL Components: UIUC-SRLas well as other SRL algorithms have different sub-components ?
parsing, argument classification, jointinference, etc.
We plan to study the effective con-tribution of each of these components.
Our hope isto identify the most important subset, which yieldsa similar quality at a much reduced computationalcost.
Another alternative is to add the best perform-ing component within TEXTRUNNER.9 ConclusionsThis paper investigates the use of semantic features,in particular, semantic role labeling for the task ofopen information extraction.
We describe SRL-IE,the first SRL based Open IE system.
We empiricallycompare the performance of SRL-IE with TEX-TRUNNER, a state-of-the-art Open IE system andfind that on average SRL-IE has much higher re-call and precision, however, TEXTRUNNER outper-forms in precision for the case of highly redundantor high locality extractions.
Moreover, TEXTRUN-NER is over 2 orders of magnitude faster.These complimentary strengths help us design hy-brid extractors that achieve better performance thaneither system given a limited budget of computationtime.
Overall, we provide evidence that, contrary tobelief in the Open IE literature (Banko and Etzioni,2008), semantic approaches have a lot to offer forthe task of Open IE and the vision of machine read-ing.10 AcknowledgementsThis research was supported in part by NSF grantIIS-0803481, ONR grant N00014-08-1-0431, andDARPA contract FA8750-09-C-0179, and carriedout at the University of Washington?s Turing Cen-ter.59ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In Proceedings of the Fifth ACM International Con-ference on Digital Libraries.Alan Akbik and Ju?gen Bro?.
2009.
Wanderlust: Extract-ing semantic relations from natural language text us-ing dependency grammar patterns.
In Proceedings ofthe Workshop on Semantic Search (SemSearch 2009)at the 18th International World Wide Web Conference(WWW 2009).Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceedingsof the 17th international conference on Computationallinguistics, pages 86?90.Michele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InProceedings of ACL-08: HLT, pages 28?36.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open infor-mation extraction from the web.
In IJCAI?07: Pro-ceedings of the 20th international joint conference onArtifical intelligence, pages 2670?2676.Michele Banko.
2009.
Open Information Extraction forthe Web.
Ph.D. thesis, University of Washington.Andrew Carlson, Justin Betteridge, Estevam R.
HruschkaJr., and Tom M. Mitchell.
2009.
Coupling semi-supervised learning of categories and relations.
InProceedings of the NAACL HLT 2009 Workskop onSemi-supervised Learning for Natural Language Pro-cessing.Bonaventura Coppola, Alessandro Moschitti, andGiuseppe Riccardi.
2009.
Shallow semantic parsingfor spoken language understanding.
In NAACL ?09:Proceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,Companion Volume: Short Papers, pages 85?88.Doug Downey, Oren Etzioni, and Stephen Soderland.2005.
A probabilistic model of redundancy in infor-mation extraction.
In IJCAI ?05: Proceedings of the20th international joint conference on Artifical intelli-gence, pages 1034?1041.Oren Etzioni, Michele Banko, and Michael J. Cafarella.2006.
Machine reading.
In AAAI?06: proceedings ofthe 21st national conference on Artificial intelligence,pages 1517?1519.Richard Johansson and Pierre Nugues.
2008.
The ef-fect of syntactic representation on semantic role label-ing.
In Proceedings of the 22nd International Con-ference on Computational Linguistics (Coling 2008),pages 393?400.Paul Kingsbury Martha and Martha Palmer.
2002.
Fromtreebank to propbank.
In In Proceedings of LREC-2002.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role labeling.Computational Linguistics, 34(2):193?224.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In EMNLP ?09: Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 1?10.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The impor-tance of syntactic parsing and inference in semanticrole labeling.
Computational Linguistics, 34(2).Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted relationdiscovery.
In Proceedings of the main conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Com-putational Linguistics, pages 304?311.Stephen Soderland.
1999.
Learning information extrac-tion rules for semi-structured and free text.
MachineLearning, 34(1-3):233?272.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for semanticrole labeling.
Computational Linguistics, 34(2):161?191.Benjamin Van Durme and Lenhart Schubert.
2008.
Openknowledge extraction through compositional languageprocessing.
In STEP ?08: Proceedings of the 2008Conference on Semantics in Text Processing, pages239?254.Daniel S. Weld, Raphael Hoffmann, and Fei Wu.
2008.Using wikipedia to bootstrap open information extrac-tion.
SIGMOD Rec., 37(4):62?68.60
