USING PLAUSIBLE INFERENCE RULES INDESCRIPTION PLANNINGAlison Cawsey*Computer  Laboratory ,  Univers i ty  of  Cambr idgeNew Museum~ Site, Pembroke  St, Cambr idge ,  Eng land .ABSTRACTCurrent approaches to generating multi-sentence t xtfail to consider what the user may infer from the dif-ferent statements in a description.
This paper presentsa system which contains an explicit model of the infer-ences that people may make from different statementtypes, and uses this model, together with assumptionsabout the user's prior knowledge, to pick the most ap-propriate sequence of utterances for achieving a givencommunicative goal.INTRODUCTIONExamples, analogies and class identification areused in many explanations and descriptions.
Yetcurrent ext generation techniques all fail to tacklethe problem of when an example, analogy or classis appropriate, what example, analogy or class isbest, and exactly what the user may infer froma given example, analogy or class.
McKeown, forexample, in her identification schema (given in fig-ure 1) includes the 'rhetorical predicates' identi-fication (as an instance of some class), analogy,particular.illustration and attributive (McKeown,1985).
From each of these, different informationcould be inferred by the user.
In a human expla-nation they might be used to efficiently convey agreat deal of information about the object, or toreinforce some information about an object so itmay be better ecalled.
Yet in McKeown's chemabased approach the only mechanism for selectingbetween these different explanation options is the*This work was carried out while the author was at thedepartment of Artificial Intelligence, University of Edin-burgh, funded by a post doctoral fellowship from the Scienceand Engineering Research Council.
Thanks to Ehud Re-iter, Paul Brna and to the anonymous reviewers for helpfulcomments .Identification (class &: attribute/function)(Analogy/Constituence/At tributive/Renaming/Amplification}*Particular-Illustration/Evidence+{ Amplification/Analogy/At tributive){Particular-Illustration/Evidence)Note: '()' indicates optionality, '/ '  alternatives, '+'that item may appear 1-n times, '*' 0-n times.Figure 1: McKeown's identification schema\[McKeown 851initial pool of knowledge available to be conveyed,and focus rules, which just enforce some local co-herence on the discourse.
A particular example oranalogy could perhaps be selected using the func-tions interfacing the rhetorical predicates to the do-main knowledge base, but this is not discussed inthe theory.More recently, Moore has included examples,analogies etc.
in her text planner (Moore,1990).
She includes planning operators to deseribe-by-superclass, describe-by-abstraction, describe-by-ezample, describe-by-analogy and describe-by.parts-and.use.
Two of these are illustrated in figure 2.But again there are no principled ways of selectingwhich strategy to use (beyond, for example, possi-bly selecting an analogy if the analogous conceptis known), and the effect of each strategy is th~same - that the relevant concept is 'known'.
In re-ality, of course, the detailed effects of the differentstrategies on the hearer'e knowledge will be verydifferent, and will depend on their prior knowl-119 -(def ine-text-p lan-operator:NAME describe-by-example:EFFECT (BEL ?hearer (CONCEPT ?concept)):CONSTRAINTS (AND (ISA ?concept OBJECT)(IMMEDIATE-SUBCLASS?example ?concept)):NUCLEUS ((FORALL ?example(ELABORATE-C0NCEPT-EXA~,~LE?concept ?example))):SATELLITES n i l )( def ins - text  -plan- operat or: NAME descrlbe-by-analogy:EFFECT (BEL ?hearer CCONCEPT ?concept)): CONSTRAINTS(AND (ISA ?concept OBJECT)(ANALOGOUS-CONCEPT?analogy-concept ?concept)(BEL ?hearer (CONCEPT?analogy-concept) ):NUCLEUS (INFORM ?speaker ?hearer(SIMILAR ?concept?analogy- concept) ):SATELLITES ((CONTRAST ?concept?analogy-concept))))Figure 2: Moore's example and analogy text plan-ning operatorsedge.
Failing to take this into account results inpossible incoherent dialogues which don't addressthe speaker's real communicative goals.The rest of this paper will present an approach tothe problem of selecting between different state-ment  types in a description, based on a set of in-' ference rules for guessing what the hearer couldinfer given a particular statement.
These guessesare used to guide the choice of examples, analo-gies, class identification and attributes given par-ticular goals, and influence how the user model isupdated after these kinds of statements are used.The paper first describes the overall framework forexplanation generation.
This is followed by a briefdiscussion of the inference rules and knowledge rep-resentation used, and a number of examples wherethe system is used to generate leading descriptionsof bicycles.
The approach is intended to be comple-mentary to existing approaches which emphasisethe coherence of the text, and could reasonable becombined with these.OUTL INE OF'PLANNER'EXPLANATIONThe system described below 1 aims to show howplausible inference rules may be used to guide ex-planation planning given different communicativegoals.
The basic approach is to find some set ofpossible utterances, and select the one which - as-suming that the user makes certain plausible in-ferences - contributes most to the stated commu-nicative goal.
This process is repeated until someterminating condition is met, such as the commu-nicative goal being satisfied.This explanation 'planning' strategy is a kind ofheuristic search, using a modified best-first searchstrategy.
The search space consists of the space ofall possible utterance sequences, and the heuris-tic scoring function assesses how far each utter-ance would contribute to the communicative goal.Because this gives a potentially very large searchspace, only certain utterances are considered ateach point.
Currently these are constrained to bethose which appear to make some contribution tothe communicative goal - for example, the systemmight consider describing an object as an instanceof some class if that class had some attributeswhich contributed to the target state.
These pos-sible utterances are then scored by using the plau-sible inference rules to predict what might reason-ably be inferred by the user from this statement,given his current knowledge, and comparing thatwith the communicative goal.For example, if the communicative goal is for theuser to have a positive impression of the object, andthe system knows of some feature which the userbelieves is desirable in an object, then the systemmay select utterances which allow the user to plau-sibly infer this feature given their current assumedknowledge about this and other objects.The search space is defined by the range of possi-ble utterance types.
Currently the following types(and associated plausible inference procedures) areallowed, where there may be many possible state-ments about a given object of each type:IReferred to from now on as the GIBBER system - Gen-erating Inference-Based Biased Explanatory Responses.120 -.
TheIdentification, as an instance (or sub-class) ofsome class.Similarity, given some related object withmany shared attributes 2.Examples, of instances or sub-classes,Attributes of that object.selection of possible utterances, and their scor-ing \[given the probable inferences which might bemade) depends on the communicative goal set.
Inthe current system, given some object to describe,two different ypes of communicative goal may beset.
The system may either be given an explicitset of attribute values which should be inferrablefrom the generated escription, or it can be givena 'property' that the inferrable attributes houldhave.
This property can be, for example, that theuser believes the attribute value to be a !desirableone, where an 'evaluation form' similar to Jame-son's (1983) is used to rate different values.
Wherea set of attribute values are given these Can be ei-ther specific values, or value ranges.This approach uses a set of rules which may be usedto propose a possible move/statement (given thetarget/communicative goal), a set of rules whichmay be used to guess what would be inferred orlearned from that statement, given the assumedcurrent state of the user's knowledge, and a scor-ing function which assesses how far the 'guessed at'inferences would contribute to the target.
State-ments are generated one at a time, with currently 3the only relation between the utterances being en-forced by the common overall communicative goaland by the fact that the statements are selected toincrementally update the user's model of the objectdescribed.Using plausible inference rules in this way is un-doubtedly error-prone, as assumptions about theuser may be wrong and not all hearers will makethe expected inferences.
However, it is certainlybetter than ignoring these inferences entirely.
Solong as the user can ask follow-up questions in anexplanatory dialogue (e.g., Cawsey, 1989; Moore,1990) any such errors are not crucial.~Note that full analogies, where a complex mapping isrequired between two conceptually distinct objects, are cur-rently not possible in the system.SAdding further coherences relations and global strate-gies may be the subject of further work.INFERENCE RULES ANDKNOWLEDGEREPRESENTAT IONFor this approach to text planning to be effective,the rules used for guessing what the reader mightinfer should correspond as far as possible to humanplausible inference rules.
There are a relativelysmall number of AI systems which attempt tomodel human plausible inferences {compared withthose attempting tomodel efficient learning strate-gies in artificial situations).
Zuckerman (1990) usessome simple plausible inference rules in her expla-nation system, in order to attempt o block in-correct plausible inferences, while a more compre-hensive model of human plausible reasoning is pro-vided by Collins and Michalski (1989).
This lattertheory is concerned with how people make plausibleinferences given generalisation, specia|isation, sim-ilarity and dissimilarity relations between objects,using a large number of certainty parameters to in-fluence the inferences.
The theory assumes a repre-sentation of human memory based on dynamic hi-erarchies, where, for example, given the statementcolour(eyes( John)) fb lue then colour,  eyes,John and blue would all be objects in some hierar-chy.
The theory is used to account for the plausibleinferences made when people guess the answer toquestions given uncertain knowledge.The GIBBER system uses inference rules some-what differently to Collins' and Michalski's.Whereas they are concerned with the competinginferences which may be made from existing knowl-edge to answer a single question, the GIBBER sys-tem is concerned with mutually supporting infer-ences from multiple given relationships in orderto build up a picture of an object.
So, althoughthe basic knowledge representation a d relation-ship types (apart from dissimilarity) are borrowedfrom their work, the actual inference rules used areslightly different.It should be possible to use the inference rules toincrementally update a representation f what iscurrently known about an attribute, where gener-alisation, similarity and specialisation relationshipsmay all contribute to the final 'conclusion'.
In or-der to allow such incremental updates, the repre-sentation used in Mitchell's version space learn-ing algorithm is adopted (1977), where each at-tribute has a pointer to the most specific valuethat attribute could take, and to the most gen-121 -eral value, given current evidence.
Positive ex-amples (or Oeneralisation relationships) are usedto generallse the specific value (as in Mitchell'salgorithm) 4 while class identification (specialisa-tion) is used to update the general value usingthe inherited attributes.
Similarity transforms aredone by first finding a common context for thetransform (a common parent object), and thentransferring those attributes which belong to that?
context which are not ruled out by current evi-dence.
Explicit statement of attribute values fixthe attribute value, but further evidence may beused to increase the certainty of any value.The system also allows for other kinds of domainspecific inference rules to be defined - for exam-ple, if a user has just been told that a bike hasderailleur gears, a rule may be used to show thatthe user could probably guess that the bike hadbetween 5 and 21 gears.
The different kinds of in-ference rules are used to incrementally update therepresentation f the user's assumed knowledge ofthe object and the scoring function, discussed inthe previous section, will compare that assumedknowledge of the object with the target.The knowledge representation is based on a framehierarchy describing the objects in the domain,where the slot values may point to other objects,also in some hierarchy.
In figure 4 a small sectionof a knowledge base of different kinds of bicycleis illustrated, along with some simple hierarchiesof attribute values.
In the GIBBER system sep-arate hierarchies are defined for the system's andfor the user's assumed knowledge, where the latteris initialised from a user stereotype and updatedfollowing each query and explanation.Of course, the knowledge representation a d infer-ence rules described in this section are by no meansdefinitive - there is no implied claim that people re-ally use these rules rather than others in learningfrom descriptions.
They simply provide a start-ing point for exploring how explanation generationmay take into account possible learning and infer-ence rules, and thus better select statements in adescription given knowledge of the domain and ofthe user's knowledge.Partial ConceptHierarchyAttribute Hierarchiestype(gears)Bicycle d ~ ~ u bno-of(gears)=l-21 no-of(wheels) = 2 shitnano-indexO ~  \] ~n?
'?i~gears)1-3mno-of(gears)=18-21 ~\ [  5-12 18-21weight --medium \type(gears) =deraiUeur sportstype~saddle) =anatomic weight=quite-light no-of(gears) = 5-12type\[tires) =knobby type(gears) =derailleursize(tires) =wide type(saddle) =narrowCascade Trek-S00no-of(gears)=18 no-of(gears)=21type(gears) =shhnano-index type(gears)=shhnano-inde:weight=311b weight=311b7Alison's bikeextras= \[mudguard,rack\]colour=blackFigure 3: Partial Bicycle HierarchiesEXAMPLE DESCRIPT IONSThis section will give two examples of how descrip-tions of bicycles may be generated using this ap-proach.
We will assume that the system's knowl-edge includes the hierarchy given in figure 4, and(for simplification) the user's knowledge includesall the items except he 'Cascade', but includes thefact that Alison's bike has shimano indexed gears.The first example will show how the system willselect utterances to economically convey informa-tion given some target attribute values, while thesecond will show how biased descriptions may begenerated given a specification of the desired prop-erty of inferrable attributes.Suppose the user requests a description of the Cas-cade and that the communicative goal set by thesystem (by some other process) is to convey thefollowing attributes:4Note that Collins' and Michalski's theory does not ap-pear to allow multiple xamples tobe used by generalisingthe inferred values.type_of(saddle) = anatomictype_of(tires) ffiknobbyweight ~ 311bnumber_of(gears) ffi 18type_of(gears) ffi shimano_index- 122 -There are many possible statements which couldbe made about the Cascade.
The user knows Ali-son's bike, so this example could be mentioned.
Itcould be described as an instance of a mountainbike, or just as a bicycle; a comparison could bemade with the Trek-800; or any one of the bikesattributes could be mentioned.
In this case if it isidentified as an instance of a mountain bike the sys-tem guesses that the user could infer the first twoattributes, which gives the highest score given thetarget s. A comparison with the Trek-800 also givestwo possible inferrable attributes, {though one in-correct value, which is currently allowed}, and thisis the next choice.
Finally the system informs theuser of the number of gears, blocking the incorrectinference in the previous utterance.
The resultingshort description is the followingS:aThe Cascade is a kind of mountain bike.It is a bit like the Trek-800.It has 18 gears.
"If the scoring function is changed so that it isbiased further towards highly certain inferences,rather than efficient presentation of information,then given the same communicative goal the de-scription may end up as an explicit list of all theattributes of the bike, or in a less extreme case,a class identification and three explicit attributes.This scoring function therefore allows for furthervariation in descriptions, given a communicativegoal, and different scoring functions should be useddepending on the type of description required.Suppose now that the same bike is to be described,but the communicative goal is that the user hasa positive impression of the Cascade.
If the userregards it to be good for a bike to be black with 21?
shimano index gears then the following descriptionwill be generated.5The scoring function compares the plausibly inferredinformation with the target, preferring more certain infer-ences, and inferences bring the knowledge of the objectcloser to the target (given the attribute value hierarchy}.For example, an inference that the bike had 18-21 gears~ oran uncertain inference that it had 18, would be given a lowerscore than a certain inference that it had 18 gears.
The to-tal score is the sum of the scores of each possibly inferredvalue.eOf course this description would be more coherent if ahigher level cornpare-contra~t relation was used to generatethe last two inferences, with resulting text: Ult is a bit likethe Trek-800 but has 18 gears.".
Allowing these higer levelstrategies within an inference-based approach is the subjectof further work.aThe Cascade is a bit like the Trek-800.Alison's bike is a Cascade.The Cascade has Shimano Index Gears.
~Here the system evaluates each statement by com-paring the plausible inferences against an evalua-tion form {Jameson, 1983).
The evaluation formdescribes how far different attribute values are ap-preciated by different classes of user.
Instead ofcomparing inferred values with some target at-tribute values the scoring function will score eachagainst the evaluation form.
For example, the firstutterance (comparison with the Trek-800) is se-lected because the attributes which might be plau-sibly inferred from this statement by this user arerated highly on the evaluation form for that classof user.
In this case the system assumes that thistype of user will prefer a bike with a large numberof indexed gears.
Of course, one of the plausible in-ferences which can be made will be incorrect (thefact the Cascade has 21 gears).
The system is notrequired to block such false inferences if they con-tribute to its goals {though the ethics of generatingsuch leading descriptions might be doubted!
).It should be clear f rom this that  the descr ipt ionsgenerated by the system are very sensit ive to theassumpt ions  about  the user's pr ior  knowledge, andthe inference rules and the scoring funct ion used,as well  as to the communicat ive  goal set.
Thereis much poss ib i l i ty  for error (and further  researchrequired) in each of these.
However, the approachst i l l  seems to provide the potent ia l  for generat ingimproveddescr ip t ions ,  and provides a new princi-pled way of mak ing  choices in a descr ipt ion whichis absent, in schema-based (and RST-based) ap-proaches.
It gives a simple example of how, givena model of how people update their beliefs, ut-terances may be strategically generated to changethose beliefs.CONCLUSIONThis paper  has discussed how, by ant ic ipat ing theuser 's  inferences, bet ter  exp lanat ions  may be gen-erated and assumpt ions  about  the user 's  knowledgeupdated  in a more pr inc ip led way.
A l though thereare problems with the approach  - par t i cu lar ly  thediff iculty of re l iably pred ict ing the user 's  inferences- it seems to provide a more pr inc ip led way of se-lect ing certain utterance types than existing multi-sentence 'text generation systems.
Other question- 1 2 3  -answering systems have attempted to simulate theuser's inferences in order to block false inferences(Joshi etal .
,  1984; Zuckerman, 1990), and par-ticular inferences have been considered in lexicalchoice (Reiter, 1990) and in generating narrativesummaries (Cook et al, 1984).
However, it hasnot been used previously as a general technique forselecting between different options in an descrip-tion.Considering what is implicitly conveyed in differenttypes of description may also begin to explain someof the empirically derived results used in other sys-tems.
For example, the GIBBER system generallychooses to begin a description with class identifi-cation or with a comparison, as most informationmay be inferred from these (compared with men-tioning specific attributes).
This may be One of theprinciples influencing the organisation of the dis-course strategies developed by McKeown (1985).The general approach would also suggest hat ex-perts might prefer structural descriptions to pro-cess descriptions (Paris, 1988) because they can al-ready infer the process description from the struc-tural, the former therefore conveying more implicitinformation.By looking at possible plausible inferences whenplanning descriptions we attempt give a better so-lution to the problem of determining what to saygiven a particular communicative goal.
The ap-proach has potential for generating more memo-rable descriptions, where different ypes of state-ment are used to re-inforce some information, aswell showing us how to economically convey a greatdeal of information, where some of this informationmay be implicit.
It does not provide a solution tothe problem of determining how to structure thiscommunicative content (considered in much otherresearch), though we may find that by: consider-ing further how people incrementally learn fromdescriptions we may obtain better structured text.The prototype system has been fully implemented,but much further research is needed.
The inferencerules, user modelling and scoring functions need tobe further developed, and other influences on textstructure (such as focus and higher level rhetoricalrelations) incorporated into the overall model.REFERENCESCawsey, Alison (1989), Generating ExplanatoryDiscourse: A Plan-Based, Interactive Ap-proach, Unpublished PhD thesis, Departmentof Artificial Intelligence, University of Edin-burgh.Collins, Allan & Michalski, Ryssard (1989) Thelogic of plausible reasoning: A core theory.Cognitive Science, 14:1-49.Cook, Malcolm, E., Lehnert, Wendy, G. and Mc-Donald, David, D. (1984) Conveying ImplicitContent in Narrative Summaries.
In Proceed-ings of COLING-84, pages 5-7.Jameson, Anthony (1983), Impression monitoringin evaluation-oriented dialogue.
In Proceed-ings of the 8th International Conference onArtificial Intelligence, pages 616-620.Joshi, Aravind, Webber, Bonnie and Weiscedel,Ralph, M. (1984) Living up to expectations:computing expert responses.
In Proceedingsof the 7th National Conference on ArtificialIntelligence, pages 169-175.McKeown, Kathleen, R. (1985), Test Genera-tion : Using discourse strategies and focusconstraints to generate natural anguage test.Cambridge University Press.Mitchell, Tom, M. (1977), Version spa~es: A can-didate elimination approaA:h to rule learn-ing.
In Proceedings of 5th International Con-ference on Artificial Intelligence, pages 305-310.Moore, Johanna, D. (1990), A Reactive Approachto Ezplanation in Expert and Advice-GivingSystems.
PhD thesis, Information SciencesInstitute, University of Southern California(published as ISI-SR-90-251).Paris, Cecile.
(1988), Tailoring Object Descrip-tions to a User's Level of Expertise.
In Com-putational Linguistics (Special Issue on UserModelling), vol 14.Reiter, Ehud (1990), Generating descriptions thatexploit a user's domain knowledge.
In R.Dale, C. Mellish, a~d M. Zock, editors, Cur-rent Research in Natural Language Genera-tion, Academic Press.Zuckerman, Ingrid (1990), A Predictive Approachfor the Generation of Rhetorical Devices.
InComputational Intelligence, vol 6, issue 1.124 -
