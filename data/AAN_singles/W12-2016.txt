The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 136?146,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsGenerating Diagnostic Multiple Choice Comprehension Cloze QuestionsJack Mostow and Hyeju JangProject LISTEN (www.cs.cmu.edu/~listen)Carnegie Mellon UniversityRI-NSH 4103, 5000 Forbes AvenuePittsburgh, PA 15213-3890, USAmostow@cs.cmu.edu, hyejuj@cs.cmu.eduAbstractThis paper describes and evaluates DQGen,which automatically generates multiple choicecloze questions to test a child?s comprehen-sion while reading a given text.
Unlike previ-ous methods, it generates different types ofdistracters designed to diagnose differenttypes of comprehension failure, and testscomprehension not only of an individual sen-tence but of the context that precedes it.
Weevaluate the quality of the overall questionsand the individual distracters, according to 8human judges blind to the correct answers andintended distracter types.
The results, errors,and judges?
comments reveal limitations andsuggest how to address some of them.1 IntroductionThis paper presents an automated method to checka reader?s comprehension of a given text whilereading it, and to diagnose comprehension failures.In contrast to testing reading comprehension skill,for which there are published tests with well-established psychometric properties (e.g.,Wiederholt & Bryant, 1992; Woodcock, 1998),testing comprehension during reading of a giventext requires generating a test for that specific text.A widely used solution is to replace some of thewords with blanks for the student to fill, typicallyby selecting from multiple candidates.
Such mul-tiple choice fill-in-the-blank questions are calledcloze questions.
They are trivial to score becausethe correct answer is simply the original text word.Cloze questions test the ability to decide whichword is consistent with the surrounding context.Thus it taps the comprehension processes thatjudge various types of consistency, such as syntac-tic, semantic, and inter-sentential.In a nutshell, these processes successively en-code sentences, integrate them into an overall rep-resentation of meaning, notice gaps andinconsistencies, and repair them (see, e.g., Kintsch,1993, 2005; van den Broek, Everson, Virtue, Sung,& Tzeng, 2002).
The reader?s resulting situationmodel represents ?the content or microworld thatthe text is about?
(Graesser & Bertus, 1998).In this paper, we introduce DQGen (DiagnosticQuestion Generator), a system that uses naturallanguage processing to generate diagnostic clozequestions that check the comprehension of some-one reading a given text.
DQGen differs from pre-vious methods for generating cloze questions inthat it is designed to minimize disruption to thereading process, and to diagnose different types ofcomprehension failure.The intended application context that motivatedthe development of DQGen is an automated read-ing tutor that listens to children read aloud andhelps them build their oral reading fluency(Mostow, 2008).
Periodic comprehension checksshould deter children from reading as fast as theycan and ignoring what the text means.
When thechild answers incorrectly, the wrong answersshould provide clues to why they are wrong.136The rest of this article is organized as follows.Section 2 describes the generated questions.
Sec-tion 3 describes how DQGen generates distracters.Section 4 reports a pilot evaluation of it.
Section 5analyzes errors.
Section 6 relates DQGen to priorwork.
Section 7 concludes.2 Form of Generated Cloze QuestionsGenerating cloze questions requires deciding:1.
Which sentences to make cloze questions?2.
Which words to delete from them?3.
How many distracters to provide for them?4.
What types of distracters?To illustrate the results of DQGen?s decisions,Figure 1 shows one of the better questions it gen-erated:Figure 1.
An example of a generated questionThe four decisions enumerated above involvetradeoffs among preserving the flow of reading,encouraging comprehension, and assessing it accu-rately.
As this example illustrates, DQGen insertscloze questions as comprehension checks at theend of paragraphs, where there are natural breaks,in order to minimize disruption to the flow of read-ing.
If the last sentence is shorter than four wordsor DQGen fails to find an acceptable distracter ofeach type, it simply leaves the last sentence un-changed rather than turn it into a bad cloze ques-tion.DQGen deletes the last word of the sentence, inorder to allow normal reading up till that point andthereby minimize disruption to the flow of reading.Deleting a word earlier in the sentence would forcethe reader to skip the deleted word and read aheadto answer the cloze question.
Indeed, a review ofof comprehension assessments (Pearson & Hamm,2005) indicates that end-of-sentence multiplechoice cloze questions are widely used:  ?Deletewords at the end of sentences and provide a set ofchoices from which examinees are to pick the bestanswer (this tack is employed in several standard-ized tests, including the Stanford Diagnostic Read-ing Test and the Degrees of Reading Power).
?The number of distracters involves a tradeoff.On the one hand, the more distracters, the lesschance of lucky guesses, and the more types ofdistracters possible.
On the other hand, offeringmore distracters lengthens the disruption to theflow of reading and raises the cognitive load on thereader to remember the paragraph when readingthe distracters.
As a compromise, DQGen addsthree distracters, for a total of four choices to pre-sent in randomized order ?
typical for multiplechoice questions on educational tests for children.DQGen uses three types of distracters.
Eachtype of distracter indicates a different type of com-prehension failure when chosen incorrectly by thereader as the answer.
By aggregating children?sperformance over questions with these same threetypes of distracters, we hope not only to test theircomprehension, but to profile the difficulties en-countered by a given child or posed by a given text.2.1 Ungrammatical distractersThe first and presumably easiest type of distracterrenders the completed sentence ungrammatical.Syntactic processing is part of comprehension butnot necessarily well-developed in children.
Analy-sis of children's responses to 69,000 multiple clozequestions automatically generated, presented, andscored by the Reading Tutor (Mostow et al, 2004)found that children?s performance decreased as thenumber of distracters with the same part of speechas the correct answer increased.
However, thiseffect was weaker for lower-level readers, indicat-ing less sensitivity to syntax (Hensler & Beck,2006).
Choosing an ungrammatical distracter indi-cates failure to detect a syntactic inconsistency.The ungrammatical distracter, e.g., are in Figure 1,has a different part of speech (POS) than the cor-rect answer germs.2.2 Nonsensical distractersThe second type of distracter makes the completedsentence grammatical but nonsensical.
Choosing anonsensical distracter indicates failure to detect alocal semantic inconsistency with the rest of thesentence.
The nonsensical distracter has the sameSome of those cells patrol your body.
They arehungry, and they eat germs!
Some stop thetrouble germs make.
Others make antibodies.They stick to germs.
That helps your body findand kill _____  .a) areb) intestinesc) terroristsd) germs137Ungrammatical Nonsensical PlausibleSource of candidatesOther words inparagraphList of words atgrade level up to 4Matching Google N-gramsSame as correct answer?
No No No (94.96%)Related to words earlier in paragraph?
?
?
No (lowest score)Related to words earlier in sentence?
?
?
Yes (55.77%)Contains a space?
?No No (100%) ?NoFrequent enough for children to know?
?Yes ?Yes Yes (96.15%)Passes grammar checker?
No (65.48%) Yes (52.62%) Yes (92.31%)*Same POS as answer?
?No Yes (26.67%) ?Matches a Google N-gram?
No (95.83%) No (91.67%) ?YesTable 1.
Sources and constraints for each distracter type, in order tested (with % satisfied in pilot data).Constraints guaranteed to be satisfied or violated without explicit testing are marked ?Yes or ?No.
* We added this test after the pilot evaluation because Google N-grams aren?t always grammatical.POS as the correct answer, but plugging it into thesentence forms a context not found in the GoogleN-grams corpus.
For example, the nonsensical dis-tracter in Figure 1 is intestines.2.3 Plausible distractersThe third and hardest type of distracter makes thecompleted sentence meaningful in isolation butinconsistent with the preceding global context.This type of distracter is essential in testing inter-sentential processing, i.e.
?understanding thatreaches across sentences in a passage,?
becauseotherwise ?an individual's ability to fill in clozeblanks does not depend on passage context?
?
afrequent criticism of cloze questions (Pearson &Hamm, 2005).
A plausible distracter has the samePOS as the correct answer, like a nonsensical dis-tracter, but the sentence it forms when plugged intothe blank sounds reasonable ?
in isolation.
That is,it ends with an N-gram that occurs in the GoogleN-grams corpus.
However, it doesn?t make sensein the context of the preceding sentences, becausethe distracter is unrelated to the words in the pre-ceding sentences.
For example, terrorists in Fig-ure 1 is a plausible distracter.3 Generating and Filtering DistractersDQGen uses generate-and-test to construct eachtype of distracter:  it chooses randomly from asource of candidates and backtracks if the chosencandidate violates a constraint on that type of dis-tracter.
If none of the candidates that satisfy a con-straint survive subsequent tests, DQGen drops theconstraint and considers candidates that violate it.The source and constraints vary by distracter type(ungrammatical, nonsensical, plausible).
Table 1summarizes the tests and the order they are applied.Sections 3.1-3.3 discuss them in further detail.3.1 Lexical constraints on distractersThree constraints apply at the word level.No spaces: We constrain all three types of dis-tracters to be words rather than phrases.
This con-straint is guaranteed for paragraph words andGoogle N-grams, DQGen?s respective sources ofungrammatical and plausible distracters.
However,our source of nonsensical distracters is a table(Biemiller, 2009) that specifies the grade level notonly of words but also of some phrases, such asbarbeque sauce, which DQGen therefore filters out.Table 2 shows an excerpt from the table used.Word Meaning Level ?barbecue sauce flavored sauce for meat 2intestines guts  4intimate close, friendly 10intimate a close friend 10Table 2.
Excerpt from Biemiller's (2009) tableDistinct: DQGen explicitly excludes the correctanswer as a distracter.
Other constraints on differ-ent types of distracters are mutually exclusive with138each other.
Consequently, no answer choice canappear twice.Familiar: Distracters must be familiar to chil-dren.
DQGen satisfies this constraint for ungram-matical and nonsensical distracters by choosingthem from the paragraph and a grade-leveled wordlist (Biemiller, 2009), respectively.
These sourcessuffice to provide candidates, but they are notcomprehensive enough to test candidates from an-other source, such as the Google N-grams used togenerate plausible distracters.
To exclude wordslikely to be unfamiliar to children, DQGen filtersout candidates whose unigram frequency falls be-low 5,000,000.
We tuned this threshold by infor-mal trial and error; higher thresholds proved toostringent to allow any distracters from the limitedsource of candidate plausible distracters.3.2 Constraints on completed sentencesThree constraints pertain to making completed sen-tences sensible or not.Grammatical: As Table 1 shows, all three typesof distracters involve grammaticality constraints.Ungrammatical distracters must make the complet-ed sentence ungrammatical, e.g., That helps yourbody find and kill are.
In contrast, nonsensical andplausible distracters must make the completed sen-tence grammatical, e.g., That helps your body findand kill terrorists.To check grammaticality of a completed sen-tence, we use the Link Grammar Parser (Sleator &Temperley, 1993), a syntactic dependency parser,as a grammar checker.
As a grammar checker, theLink Grammar Parser usually accepts grammaticalsentences and rejects ungrammatical ones, perhapsbecause sentences in children?s text tend to beshort.
However, it sometimes fails to accept agrammatical sentence, as the last row of Table 3illustrates.sentence grammatically parserThe germs hide in food orpeoplecorrect acceptedThe germs hide in food orworldincorrect rejectedSo keep dirty hands awayfrom cuts and your face.correct rejectedTable 3.
Examples of grammar checking by parserPart of speech: More than one POS may makea distracter grammatical.
DQGen uses the Stan-ford POS Tagger (Toutanova, Klein, Manning, &Singer, 2003) to tag the correct answer and a can-didate nonsensical distracter when used to com-plete the sentence, and requires them to have thesame POS.
This test is superfluous for ungram-matical distracters and unnecessary for plausibledistracters.Google N-gram: As a heuristic test of whether acompleted sentence is plausible, we check whetherits ending occurs in the Google N-grams corpus(Brants & Franz, 2006), which means that it ap-pears at least 40 times on the Web.
For ungram-matical and nonsensical distracters, the last 4words of the completed sentence must not occur inthis corpus.
For plausible distracters, the last 4words followed by ?.?
must occur.
To enforce thisconstraint, DQGen?s source of candidate plausibledistracters consists of Google 5-grams of the formW X Y __ .
Here W, X, and Y are the words pre-ceding the correct answer in the original sentence,e.g., find and kill.
If there are fewer than 5 such 5-grams, DQGen allows 4-grams of the form X Y __ .,e.g.
and kill __.3.3 Relevance to contextTwo constraints on distracters concern context.Irrelevant to words earlier in paragraph: Aplausible distracter should not be too plausible, soDQGen tries to ensure that it is unrelated to theearlier sentences and hence unlikely to make sensein context.
We measure the relatedness of a dis-tracter to words in the earlier sentences by howoften it co-occurs with them when used as in thelast sentence.
DQGen therefore first pairs the can-didate distracter, e.g.
terrorists, with the last con-tent word preceding the blank, e.g., kill in Thathelps your body find and kill ____.
It then esti-mates the probability of these two words (kill andterrorists) co-occurring with the words in the earli-er sentences of the paragraph, using a Na?ve Bayesformula to score their relevance to that context:1Pr( , | ) Pr( , ) Pr( | , )niic k w c k w c k=?
?
?The formula omits Pr( )w?because it?s the same forall candidate plausible distracters for a given ques-tion.
Here c is a candidate distracter (e.g., terror-ists), k is the last content word before the blank(e.g., kill), w?is a vector of the n content wordsearlier in the paragraph, and wi is the ith such word.139Figure 2.
Prompt for the pilot user testDQGen scores Pr( | , )iw c k  based on how oftenword wi co-occurs with words c and k in the same30-word window in the British National Corpus(BNC).The purpose of a plausible distracter is to detectfailures of intersentential comprehension processesthat monitor global consistency.
As a heuristic toviolate global consistency, DQGen picks distract-ers with the lowest relevance scores.Relevant to words earlier in sentence: A plau-sible distracter should be relevant to the words ear-lier in the sentence.
To score local relevance,DQGen uses a Na?ve Bayes formula similar to itsformula for global relevance:1Pr( | ) Pr( ) Pr( | )niic w c w c=?
?
?Here, c is a candidate distracter, w?is a vector ofthe n content words earlier in the sentence, and wiis the ith such word.
DQGen estimates Pr( | )iw c  inthe same way as before, but omits k because n is somuch smaller for the sentence than for the para-graph context preceding it.
DQGen averages theselocal coherence scores over the candidates, andallows only candidates whose local coherencescores are above the mean.4 Pilot StudyHow good are the generated questions?
To evalu-ate DQGen, we asked human judges to score them.Section 4.1 explains how we evaluated questions,Section 4.2 reports inter-rater reliability, and Sec-tion 4.3 presents results.4.1 MethodologyFor the evaluation, we used DQGen to insert sam-ple questions in an informational text for children,The Germs, which explains the concept of germsand their danger.
Of the 18 paragraphs in this text,we rejected one because it was only two sentenceslong, and DQGen rejected another because the lastsentence failed the grammar checker.
For each ofthe other 16 paragraphs, DQGen generated a clozequestion with ungrammatical and nonsensical dis-tracters, but it found plausible distracters for only13 of the questions, which we evaluated as follows.We recruited eight human judges, members ofour research team but unfamiliar with DQGen.
Weasked them to evaluate each question at two levels,using the form illustrated in Figure 2.At the high level, we evaluated the overall quali-ty of each question by asking judges to rate it as140Good, OK, or Bad.
We report the percentage ofgenerated questions rated by human judges as ac-ceptable, defined as Good or OK.  We used a 3-point scale rather than a finer-grained scale both toget higher inter-rater reliability, and because wewere interested more in how many of the questionswere acceptable than in precise ratings of quality.At the low level, we evaluated how oftenDQGen generated the intended type of distracter.We asked the judges to categorize each of the mul-tiple choices (correct answer plus 3 distracters) asUngrammatical, Nonsensical but grammatical,Meaningful but incorrect given the preceding text,or Correct.
To avoid biasing their responses, wedid not tell them that each question was supposedto have one choice in each category.To elicit additional feedback, the form invitedjudges to comment on the questions and distracters.4.2 Inter-rater reliabilityIt is important to measure inter-rater reliabilityamong human judges, especially on experimenter-designed measures such as the form we used.The overall quality ratings involved ranked datafrom more than two judges, so to measure theirinter-rater reliability we used Kendall?s Coefficientof Concordance (Kendall & Smith, 1939).
KCCfor overall quality was .40 on a scale from 0 to 1.This low value reflects the considerable variationbetween the judges, whose average ratings of over-all quality ranged from 1.3 to 2.6.Categorization of each answer choice involvedunranked data from more than two judges, so weused Fleiss?
Kappa (Shrout & Fleiss, 1979) tomeasure its inter-rater reliability.
Kappa was .58;a value of .4-.6 is considered moderate, .6-.8 sub-stantial, and .8-1 outstanding (Landis & Koch,1977).
Figure 3 shows the Kappa values for eachlabel by the judges.Figure 3.
Fleiss's Kappa for inter-rater reliabilityof each type of choiceThe low values of inter-rater reliability measuresrevealed the raters?
lack of consensus, presumablydue to differing interpretations of the instructions.For instance, one judge commented that instructionfor rating the overall quality did not indicatewhether a good question requires reading the pre-ceding text.
Another issue was missing and multi-ple categorical responses.Evidently we need to specify our rating criteriamore clearly, both for overall quality and for indi-vidual components, especially nonsensical andplausible distracters.
A worked-out example mighthelp judges understand each type better, but mustavoid phrasing biased toward how DQGen works.4.3 ResultsWe computed average ratings of overall qualityand agreement with the intended category of eachanswer choice.We averaged all the ratings of overall quality af-ter converting Bad, OK, and Good ratings into 1, 2,and 3, respectively.
Overall quality ratings aver-aged 2.04, which corresponds to OK.  For agree-ment of judges with the intended category of eachanswer choice, Cohen?s Kappa was .60.
Note thatin contrast to Section 4.2, where we used Kappa tomeasure inter-rater reliability, i.e., how well thejudges agreed with each other on overall questionquality, here we use Kappa to measure distracterquality, i.e., how well the judges agreed withDQGen on the intended type of answer choices.Individual judges ranged from 63% to 79%agreement with the intended answer (Cohen'sKappa .51 to .72).
As Figure 4 shows, agreementwas stronger for correct answers and ungrammati-cal distracters than for nonsensical and plausibledistracters.
On average, judges rated 94% of thecorrect answers as correct and agreed withDQGen?s intended distracter type for 91% of theungrammatical distracters, 63% of the nonsensicaldistracters, and only 32% of the plausible distract-ers.
Apparently correct answers are obviouslyright and ungrammatical answers are obviouslywrong, but nonsensical and plausible distractersare harder to classify.5 Analysis of errorsWe now discuss issues revealed by errors andjudges?
comments, and how to address them.141Figure 4.
Cohen?s Kappa for agreement with theintended type of each choice5.1 Dependence on preceding textThe judges?
most frequent comment about thequality of a question was that answering it did notrequire reading the preceding text.
The judges rat-ed only 32% of the intended plausible distracters asplausible.
Evidently we need to identify furtherconstraints on plausible distracters.
We may alsoneed to identify constraints on sentences whereplausible distracters exist for the correct answer.5.2 IdiomsAnswer choices, whether correct answers or dis-tracters, are problematic when they form idiomssuch as twisted in knots or make do.
For instance,one pilot cloze question ended with twisted in ____,where the correct answer was knots.
Anotherquestion ended with get your body to make ____,with do as a supposedly ungrammatical distracter.Idioms pose multiple problems, although wefound only two cases in our small pilot study.
First,we want to test comprehension of the paragraph,not just knowledge of specific idioms.
Second, theword that completes an idiom can be far likelierthan any other choice, making it too easy to guessbased solely on local context, whether correct ornot.
Third, because idioms have non-componentialsemantics, the missing word is liable to be seman-tically unrelated to other sentence words, causingDQGen to badly underestimate its local relevance.Detecting idioms automatically is a researchproblem in its own right (Li, Roth, & Sporleder,2010; Li & Sporleder, 2009).
We might be able torecognize idioms by using the fact that its N-gramfrequency is much higher than expected based onthe frequency of its individual words.
A simplerapproach is to consult a dictionary of commonphrases.
Either approach would require extensionto handle parameterized idioms such as a chip on[someone?s] shoulder, or non-contiguous formssuch as Actions do in fact speak louder than words.5.3 Lexical issues for distractersThe pilot study exposed a number of issues affect-ing the suitability of words as distracters.Same-root wordsDQGen ensures that answer choices are distinct.However, one question included two forms of thesame word as choices, namely throats as the cor-rect answer and throat as a plausible distracter.We need to ensure that answer choices are not onlydistinct but dissimilar, unless we want questionsthat focus on minor differences between them.Common verbs and modal verbsOne judge commented that we might want to avoidcommon verbs as distracters, such as any form ofbe, do, have, and get, and modal verbs, such as can,cannot, and will, lest children notice that they areseldom the correct answer, and therefore eliminatethem without considering them.
Accordingly, weplan to filter out common verbs and modal verbs.Word difficultyThe same judge considered some words too diffi-cult for children, such as gauge and roast.
Actual-ly, Biemiller (2009) rates noun senses of thesewords at grade 2, but the verb sense of gauge asestimate at grade 10.
These examples illustrate alimitation of DQGen?s methods to pick familiarwords as distracters.
It picks ungrammatical dis-tracters from the words in the paragraph, nonsensi-cal distracters from Biemiller?s word list, andplausible distracters from Google N-grams, filteredby unigram frequency to avoid rare words.
In allthree cases, DQGen constrains words rather thanword senses.A more sophisticated approach would determinea distracter?s word sense, or at least POS, whenused to complete the sentence, and rate the famili-arity of its specific sense or POS.
Tagging the dis-tracter POS is easier than determining its wordsense(s) when inserted in the sentence.
Rating thefamiliarity of different word senses would requireeither a grade-leveled list of them like Biemiller?s(2009), or a resource with information about thefrequency of different word senses or POS.1426 Relation to Prior WorkHow does this research relate to previous work?There has been considerable research on automaticgeneration of multiple choice cloze questions totest vocabulary, grammar, and comprehension.Although these types of questions differ in purpose,they have much in common when it comes to gen-erating them automatically.6.1 Vocabulary and grammar cloze questionsA multiple choice cloze question to test vocabularyand grammar is constructed from a sentence se-lected from a corpus by deleting part of it (typical-ly the target vocabulary word) and selectingdistracters for it.Selecting distracters with the same POS and ap-proximate frequency as the answer word is a com-mon strategy (Brown, Frishkoff, & Eskenazi,2005; Coniam, 1997; Liu, Wang, & Gao, 2005).Besides matching the correct answer?s POS andfrequency, Liu et al (2005) added a culture-dependent strategy for generating distracters:choose English words with semantically similartranslations in the learner?s native language to thetranslation of the answer word.Correia et al (2010) generated vocabulary ques-tions for Portuguese with three types of distracters.One type of distracter had the same POS and wordlevel as the target word, based on its unigram fre-quency in Portuguese textbooks used in differentgrades.
A second type had the lowest Levenshteindistance to the target out of all words with its POS.A third type was misspellings of the target wordusing a table of common spelling mistakes.
Al-dabe et al (2007) also included students?
commonmistakes as candidate distracters.Some work also used semantic similaritybetween a distracter and the answer word to choosedistracters.
Pino et al (2008) selected distractersthat made the completed sentence grammatical andtended to co-occur with the words in the sentence,but were semantically distant from the target wordas measured by WordNet.
In constrast, Smith et al(2008) looked for distracters semantically similarto the answer word based on distributional simi-larity.
In addition, Sumita et al (2005) used a the-saurus for the same purpose, and then consultedthe web to filter out plausible distracters.Aldabe et al (2009) considered context in aquestion sentence when choosing distracters.
Theyused an n-gram language model to predict theprobability of occurrence of a distracter with itspreceding words.Gates et al (2011) generated phrase-type dis-tracters, unlike other work.
They generated ques-tions from a dictionary definition of the targetvocabulary word.
Rather than delete the targetword, they parsed the definition, deleted a phrasefrom it, and chose distracters with the same syntac-tic phrase type from definitions of other words,filtered to exclude synonyms of the target word.6.2 Comprehension cloze questionsIn contrast to vocabulary and grammar questionsconstructed from isolated sentences, DQGen?scomprehension questions are for (and inserted into)connected text.The most closely related work was by Mostowet al (2004).
Their Reading Tutor dynamicallygenerated multiple choice cloze questions to testchildren?s comprehension of randomly chosen sen-tences while reading a story.
It randomly chose anapproximate level of difficulty (?sight?, ?easy?,?hard?, and ?defined?)
for which word to deletefrom the sentence, and which words to choose ran-domly from the same story as distracters.Goto et al (2010) also generated questions fromtexts.
They used a training corpus of existing clozequestions to learn how to select sentences to turninto cloze questions, words to delete, and types ofdistracters distinguished by their relation to theanswer word:  inflectional (e.g., ask ?
asked); der-ivational (e.g., work ?
worker); orthographic (e.g.,circulation ?
circumcision); and semantic (e.g.,synonyms and antonyms).Aldabe et al (2010) generated questions forlearners?
assessment in the science domain.
Togenerate distracters, they measured semantic simi-larity by using Latent Semantic Analysis (LSA)and additional information such as semantic rela-tionships between words.
Experts discarded dis-tracters that could form a correct answer.DQGen differs from prior work on generatingcloze questions for vocabulary and comprehensionin two key respects.
First, each question it gener-ates has multiple types of distracters designed todetect different types of comprehension failure.Second, to generate plausible distracters it consid-ers their relation not only to the clozed sentencebut to the entire paragraph that contains it.1437 ConclusionWe conclude by summarizing contributions, limi-tations, and future work.7.1 ContributionsThis paper describes a method for generating mul-tiple choice cloze questions to test students?
com-prehension while reading.
Unlike previousmethods, some of which also generate multipletypes of distracters, DQGen?s distracter types arediagnostic.
It generates ungrammatical, nonsensi-cal, and plausible distracters in order to detect fail-ures of syntactic, semantic, and intersententialprocessing, respectively.
Unlike prior methods,which test comprehension only of individual sen-tences, DQGen?s plausible distracters take theirpreceding context into account.We observed that candidate plausible distracterswith high relevance scores tend to be surprisinglysensible answers ?
even though the formuladoesn?t ?know?
the correct answer or even the un-grammatical and nonsensical distracters.
That is,grammaticality, N-grams, and a simple relevancemeasure often suffice to produce intelligent an-swers to a cloze question despite their shallow rep-resentation of the meaning of the paragraph ?
thatis, without really understanding it.
This finding issurprising insofar as one would expect good per-formance on such questions to require a deep rep-resentation such as the situation model constructedby human readers.7.2 LimitationsBesides describing DQGen?s design and imple-mentation, we report on an evaluation of 13 gener-ated questions by eight human judges blind tocorrect answer and intended distracter type.
Onaverage they rated overall question quality OK, butwith a wide range from the least to most favorablejudge.
They agreed well with DGQen in classify-ing answers as ungrammatical or correct, but not asnonsensical or plausible.
They criticized manyquestions as answerable without reading the text.7.3 Future workOur analysis of errors and judges?
comments re-vealed several limitations and suggested ways toaddress some of them.
In addition to identifyingfurther constraints on plausible distracters, we needto identify constraints on good sentences to turninto end-of-paragraph cloze questions, beyond justthe ability to generate a distracter of each type.One criterion is reliability:  how well does perfor-mance on a question correlate with performance onother questions about the same text?
Another cri-terion is informativeness:  what do wrong answersreveal about comprehension?Besides improving DQGen, we need to test it onmore stories (both narrative fiction and informa-tional text) and readers (especially children, ourtarget population) to expose additional problemsand avoid overfitting their solutions.One possible use of DQGen is machine-assistedgeneration of comprehension questions, or moreprecisely, human-assisted machine generation, forexample with the human vetting or selectingamong candidate questions generated automatical-ly, thereby reducing the amount of human effortcurrently required to compose comprehensionquestions, and producing them more systematically.Success in getting DQGen to produce clozequestions on a large scale would have useful appli-cations.
Periodic comprehension checks shoulddeter children from reading as fast as they can andignoring what the text means.
Diagnostic feedbackbased on incorrect answers should shed light on thenature of their comprehension failures and may bevaluable as feedback to teachers or as guidance tothe reading tutor.Another use for large numbers of automaticallygenerated cloze questions is to develop methods tomonitor reading comprehension unobtrusively.Student responses to cloze questions could provideautomated labels for data collected while they readthe preceding text.
Such data could include oralreading (Zhang, Mostow, & Beck, 2007) or evenEEG (Mostow, Chang, & Nelson, 2011).
Modelstrained and tested on the labeled data could esti-mate reading comprehension based on unlabeleddata ?
that is, without interrupting to ask questions.AcknowledgementsThe research reported here was supported by the Insti-tute of Education Sciences, U.S. Department of Educa-tion, through Grant R305A080157.
The opinionsexpressed are those of the authors and do not necessari-ly represent the views of the Institute or the U.S. De-partment of Education.
We thank our colleagues whojudged the generated questions, and the reviewers fortheir helpful comments.144ReferencesAldabe, I., & Maritxalar, M. (2010).
AutomaticDistractor Generation for Domain Specific TextsAdvances in Natural Language Processing.
Paperpresented at the The 7th International Conference onNLP, Reykjavk, Iceland.Aldabe, I., Maritxalar, M., & Martinez, E. (2007).Evaluating and Improving Distractor-GeneratingHeuristics.
Paper presented at the The Workshop onNLP for Educational Resources.
In conjuction withRANLP07.Aldabe, I., Maritxalar, M., & Mitkov, R. (2009).
AStudy on the Automatic Selection of CandidateSentences and Distractors.
Paper presented at theProceedings of the 14th International Conference onArtificial Intelligence in Education (AIED2009),Brighton, UK.Biemiller, A.
(2009).
Words Worth Teaching:  Closingthe Vocabulary Gap.
Columbus, OH: SRA/McGraw-Hill.Brants, T., & Franz, A.
(2006).
Web IT 5-gram Version1.
Philadelpha: Linguistic Data Consortium.Brown, J. C., Frishkoff, G. A., & Eskenazi, M. (2005).Automatic Question Generation for VocabularyAssessment.
Paper presented at the Proceedings ofHuman Language Technology Conference andConference on Empirical Methods in NaturalLanguage Processing (HLT/EMNLP), Vancouver.Coniam, D. (1997).
A preliminary inquiry into usingcorpus word frequency data in the automaticgeneration of English language cloze tests.
CALICOJournal, 14(2-4), 15-33.Correia, R., Baptista, J., Mamede, N., Trancoso, I., &Eskenazi, M. (2010, September 22-24).
Automaticgeneration of cloze question distractors.
Paperpresented at the Proceedings of the Interspeech 2010Satellite Workshop on Second Language Studies:Acquisition, Learning, Education and Technology,Waseda University, Tokyo, Japan.Gates, D., Aist, G., Mostow, J., Mckeown, M., & Bey, J.
(2011, November 4-6).
How to Generate ClozeQuestions from Definitions: a Syntactic Approach.Paper presented at the Proceedings of the AAAISymposium on Question Generation, Arlington, VA.Goto, T., Kojiri, T., Watanabe, T., Iwata, T., & Yamada,T.
(2010).
Automatic Generation System of Multiple-Choice Cloze Questions and its Evaluation.Knowledge Management & E-Learning: AnInternational Journal (KM&EL), 2(3).Graesser, A. C., & Bertus, E. L. (1998).
TheConstruction of Causal Inferences While ReadingExpository Texts on Science and Technology.Scientific Studies of Reading, 2(3), 247-269.Hensler, B. S., & Beck, J.
(2006, June 26-30).
Betterstudent assessing by finding difficulty factors in afully automated comprehension measure [Best Papernominee].
Paper presented at the Proceedings of the8th International Conference on Intelligent TutoringSystems, Jhongli, Taiwan.Kendall, M. G., & Smith, B.
B.
(1939).
The Problem ofm Rankings.
The Annals of Mathematical Statistics,10(3), 275-287.Kintsch, W. (1993).
Information Accretion andReduction in Text Processing: Inferences.
DiscourseProcesses, 16(1-2), 193-202.Kintsch, W. (2005).
An Overview of Top-Down andBottom-Up Effects in Comprehension: The CIPerspective.
Discourse Processes A MultidisciplinaryJournal, 39(2&3), 125-128.Landis, J. R., & Koch, G. G. (1977).
The measurementof observer agreement for categorical data.Biometrics, 33(1), 159-174.Li, L., Roth, B., & Sporleder, C. (2010).
Topic modelsfor word sense disambiguation and token-basedidiom detection.
Paper presented at the Proceedingsof the 48th Annual Meeting of the Association forComputational Linguistics, Uppsala, Sweden.Li, L., & Sporleder, C. (2009).
Classifier combinationfor contextual idiom detection without labelled data.Paper presented at the Proceedings of the 2009Conference on Empirical Methods in NaturalLanguage Processing, Singapore.Liu, C.-L., Wang, C.-H., & Gao, Z.-M. (2005).
UsingLexical Constraints to Enhance the Quality ofComputer-Generated Multiple-Choice Cloze Items.Computational Linguistics and Chinese LanguageProcessing, 10(3), 303-328.Liu, C.-L., Wang, C.-H., Gao, Z.-M., & Huang, S.-M.(2005).
Applications of lexical information foralgorithmically composing multiple-choice clozeitems.
Paper presented at the Proceedings of thesecond workshop on Building EducationalApplications Using NLP, Ann Arbor, Michigan.Mostow, J.
(2008).
Experience from a Reading Tutorthat listens:  Evaluation purposes, excuses, andmethods.
In C. K. Kinzer & L. Verhoeven (Eds.
),Interactive literacy education:  facilitating literacyenvironments through technology (pp.
117-148).
NewYork: Lawrence Erlbaum Associates, Taylor &Francis Group.Mostow, J., Beck, J. E., Bey, J., Cuneo, A., Sison, J.,Tobin, B., & Valeri, J.
(2004).
Using automatedquestions to assess reading comprehension,vocabulary, and effects of tutorial interventions.Technology, Instruction, Cognition and Learning,2(1-2), 97-134.Mostow, J., Chang, K.-m., & Nelson, J.
(2011, June 28 -July 2).
Toward Exploiting EEG Input in a ReadingTutor [Best Paper Nominee].
Paper presented at theProceedings of the 15th International Conference onArtificial Intelligence in Education, Auckland, NZ.145Pearson, P. D., & Hamm, D. N. (2005).
The history ofreading comprehension assessment.
S. G. Paris & S.A. Stahl (Eds.
), Children's reading comprehensionand assessment, 13-69.Pino, J., Heilman, M., & Eskenazi, M. (2008).
Aselection strategy to improve cloze question quality.Paper presented at the Proceedings of the Workshopon Intelligent Tutoring Systems for Ill-DefinedDomains.
9th International Conference on IntelligentTutoring Systems, Montreal, Canada.Shrout, P. E., & Fleiss, J. L. (1979).
Intraclasscorrelations: Uses in assessing rater reliability.Psychological Bulletin, 86(2), 420-428.Sleator, D. D. K., & Temperley, D. (1993, August 10-13).
Parsing English with a link grammar.
Paperpresented at the Third International Workshop onParsing Technologies, Tilburg, NL, and Durbuy,Belgium.Smith, S., Sommers, S., & Kilgarriff, A.
(2008).Learning words right with the Sketch Engine andWebBootCat: Automatic cloze generation fromcorpora and the web.
Paper presented at theProceedings of the 25th International Conference ofEnglish Teaching and Learning & 2008 InternationalConference on English Instruction  and Assessment,Lisbon, Portugal.Sumita, E., Sugaya, F., & Yamamoto, S. (2005).Measuring non-native speakers' proficiency ofEnglish by using a test with automatically-generatedfill-in-the-blank questions.
Paper presented at theProceedings of the second workshop on BuildingEducational Applications Using NLP, Ann Arbor,Michigan.Toutanova, K., Klein, D., Manning, C., & Singer, Y.(2003).
Feature-rich part-ofspeech tagging with acyclic dependency network.
Paper presented at theHLT-NAACL, Edmonton, Canada.van den Broek, P., Everson, M., Virtue, S., Sung, Y., &Tzeng, Y.
(2002).
Comprehension and memory ofscience texts: Inferential processes and theconstruction of a mental representation.
In J. L. J.Otero, & A. C. Graesser (Ed.
), The psychology ofscience text comprehension.
Mahwah, NJ: Erlbaum.Wiederholt, J. L., & Bryant, B. R. (1992).
Gray OralReading Tests (3rd ed.).
Austin, TX: Pro-Ed.Woodcock, R. W. (1998).
Woodcock Reading MasteryTests - Revised (WRMT-R/NU).
Circle Pines,Minnesota: American Guidance Service.Zhang, X., Mostow, J., & Beck, J. E. (2007, July 9-13).Can a computer listen for fluctuations in readingcomprehension?
Paper presented at the Proceedingsof the 13th International Conference on ArtificialIntelligence in Education, Marina del Rey, CA.146
