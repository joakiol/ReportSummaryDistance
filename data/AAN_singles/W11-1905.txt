Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 45?50,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsExploring Lexicalized Features for Coreference ResolutionAnders Bjo?rkelundLund University / LTHLund / SwedenAnders.Bjorkelund@cs.lth.sePierre NuguesLund University / LTHLund / SwedenPierre.Nugues@cs.lth.seAbstractIn this paper, we describe a coreference solverbased on the extensive use of lexical fea-tures and features extracted from dependencygraphs of the sentences.
The solver uses Soonet al (2001)?s classical resolution algorithmbased on a pairwise classification of the men-tions.We applied this solver to the closed track ofthe CoNLL 2011 shared task (Pradhan et al,2011).
We carried out a systematic optimiza-tion of the feature set using cross-validationthat led us to retain 24 features.
Using this set,we reached a MUC score of 58.61 on the testset of the shared task.
We analyzed the impactof the features on the development set and weshow the importance of lexicalization as wellas of properties related to dependency links incoreference resolution.1 IntroductionIn this paper, we present our contribution to theclosed track of the 2011 CoNLL shared task (Prad-han et al, 2011).
We started from a baseline systemthat uses Soon et al (2001)?s architecture and fea-tures.
Mentions are identified by selecting all nounphrases and possessive pronouns.
Then, the reso-lution algorithm relies on a pairwise classifier thatdetermines whether two mentions corefer or not.Lexicalization has proved effective in numeroustasks of natural language processing such as part-of-speech tagging or parsing.
However, lexicalizedmodels require a good deal of annotated data toavoid overfit.
The data set used in the CoNLL 2011shared task has a considerable size compared to cor-pora traditionally used in coreference resolution ?the training set comprises 2,374 documents.
SeePradhan et al (2007) for a previous work using anearlier version of this dataset.
Leveraging this size,we investigated the potential of lexicalized features.Besides lexical features, we created features thatuse part-of-speech tags and semantic roles.
We alsoconstructed features using dependency tree pathsand labels by converting the constituent trees pro-vided in the shared task into dependency graphs.The final feature set was selected through an au-tomated feature selection procedure using cross-validation.2 System ArchitectureDuring both training and decoding, we employedthe same mention detection and preprocessing steps.We considered all the noun phrases (NP) and posses-sive pronouns (PRP$) as mentions.
In order to ex-tract head words from the NP constituents, we con-verted the constituent trees provided in the data setsto dependency graphs using the Penn treebank con-verter of Johansson and Nugues (2007).
Using thedependency tree, we extracted the head word of allthe NPs by taking the word that dominates the sub-tree constructed from the NP.The dependency tree is also used later to ex-tract features of mentions based on dependency treepaths, which is further described in Sec.
3.In the preprocessing step, we assigned a numberand a gender to each mention.
For the pronominalmentions, we used a manually compiled lists of pro-nouns, where we marked the number and gender.45For nonpronominal mentions, we used the numberand gender data (Bergsma and Lin, 2006) providedby the task organizers and queried it for the headword of the mention.
In cases of ambiguity (e.g.
thepronoun you), or missing entries in the data for non-pronominals, we assigned an unknown value.2.1 Generation of training examplesTo create a set of training examples, we used pairsof mentions following the method outlined by Soonet al (2001).
For each anaphoric mention mj andits closest preceding antecedent mi, we built a pos-itive example: P = {(mi,mj)}.
We constructedthe negative examples with noncoreferring pairs ofmentions, where the first term is a mention occur-ring between mi and mj and the second one is mj :N = {(mk,mj)|i < k < j)}.The training examples collected from the CoNLL2011 training set consist of about 5.5% of positiveexamples and 94.5% of negative ones.2.2 Learning methodWe evaluated two types of classifiers: decision treesand logistic regression.
We used the decision treesand the C4.5 algorithm from the Weka distribution(Hall et al, 2009) for our baseline system.
We thenopted for linear logistic regression as it scaled betterwith the number of features and feature values.Logistic regression is faster to train and allowedus to carry out an automated feature selection, whichis further described in Sec.
3.4.
In addition, the lo-gistic classifiers enabled us to interpret their resultsin terms of probabilities, which we used for the de-coding step.
We trained the logistic regression clas-sifiers using the LIBLINEAR package (Fan et al,2008).2.3 DecodingThe decoding algorithm devised by Soon et al(2001) selects the closest preceding mention deemedto be coreferent by the classifier.
This clusteringalgorithm is commonly referred to as closest-firstclustering.
Ng and Cardie (2002) suggested a dif-ferent clustering procedure, commonly referred toas best-first clustering.
This algorithm selects themost likely antecedent classified as coreferent withthe anaphoric mention.
During early experiments,we found that while the best-first method increasesthe performance on nonpronominal anaphoric ex-pressions, it has the opposite effect on pronominalanaphoric expressions.
Consequently, we settled onusing the closest-first clustering method for pronom-inal mentions, and the best-first clustering methodotherwise.
For the best-first clustering, we used theprobability output from our logistic classifiers and athreshold of 0.5.After clustering mentions in a document, we dis-card all remaining singleton mentions, as they wereexcluded from the annotation in the CoNLL 2011shared task.2.4 PostprocessingThe initial detection of mentions is a direct mappingfrom two categories of constituents: NP and PRP$.In the postprocessing step, we reclaim some of thementions that we missed in the initial step.The automatically generated constituent trees pro-vided in the data set contain errors and this causesthe loss of many mentions.
Another source of lossis the bracketing of complex NPs, where the in-ternal structure uses the tag NML.
In a few cases,these nested nodes participate in coreference chains.However, when we tried to include this tag in themention detection, we got worse results overall.This is possibly due to an even more skewed dis-tribution of positive and negative training examples.In the postprocessing step, we therefore searcheach document for sequences of one or more propernoun tokens, i.e.
tokens with the part-of-speechtags NNP or NNPS.
If their common ancestor, i.e.the parse tree node that encloses all the tokens, isnot already in a mention, we try to match this se-quence to any existing chain using the binary fea-tures: STRINGMATCH and ALIAS (cf.
Sec.
3).
Ifeither of them evaluates to true, we add this span ofproper nouns to the matched chain.3 FeaturesFor our baseline system, we started with the featureset described in Soon et al (2001).
Due to spacelimitations, we omit the description of these featuresand refer the reader to their paper.We also defined a large number of feature tem-plates based on the syntactic dependency tree, aswell as features based on semantic roles.
In the fol-46lowing sections, we describe these features as wellas the naming conventions we use.
The final featureset we used is given in Sec.
4.3.1 Mention-based featuresOn the mention level, we considered the head word(HD) of the mention, and following the edges in thedependency tree, we considered the left-most andright-most children of the head word (HDLMC andHDRMC), the left and right siblings of the head word(HDLS and HDRS), as well as the governor1 of thehead word (HDGOV).For each of the above mentioned tokens, we ex-tracted the surface form (FORM), the part-of-speechtag (POS), and the grammatical function of the token(FUN), i.e.
the label of the dependency edge of thetoken to its parent.
For head words that do not haveany leftmost or rightmost children, or left or rightsiblings, we used a null-value placeholder.In each training pair, we extracted these valuesfrom both mentions in the pair, i.e.
both the anaphorand the tentative antecedent.
Table 3 shows the fea-tures we used in our system.
We used a namingnomenclature consisting of the role in the anaphora,where I stands for antecedent and J for anaphor; thetoken we selected from the dependency graph, e.g.HD or HDLMC; and the value extracted from thetoken, e.g.
POS or FUN.
For instance, the part-of-speech tag of the governor of the head word of theanaphor is denoted: J-HDGOVPOS.The baseline features taken from Soon et al(2001) include features such as I-PRONOUN and J-DEMONSTRATIVE that are computed using a wordlist and by looking at the first word in the mention,respectively.
Our assumption is that these traits canbe captured by our new features by considering thepart-of-speech tag of the head word and the surfaceform of the left-most child of the head word, respec-tively.3.2 Path-based featuresBetween pairs of potentially coreferring mentions,we also considered the path from the head word ofthe anaphor to the head word of the antecedent inthe syntactic dependency tree.
If the mentions arenot in the same sentence, this is the path from the1We use the term governor in order not to confuse it withhead word of an NP.anaphor to the root of its sentence, followed by thepath from the root to the antecedent in its sentence.We differentiate between the features depending onwhether they are in the same sentence or in differentsentences.
The names of these features are prefixedwith SS and DS, respectively.Following the path in the dependency tree, weconcatenated either the surface form, the part-of-speech tag, or the grammatical function label withthe direction of the edge to the next token, i.e.
up ordown.
This way, we built six feature templates.
Forinstance, DSPATHFORM is the concatenation of thesurface forms of the tokens along the path betweenmentions in different sentences.Bergsma and Lin (2006) built a statistical modelfrom paths that include the lemma of the intermedi-ate tokens, but replace the end nodes with noun, pro-noun, or pronoun-self for nouns, pronouns, and re-flexive pronouns, respectively.
They used this modelto define a measure of coreference likelihood to re-solve pronouns within the same sentence.
Ratherthan building an explicit model, we simply includedthese paths as features in our set.
We refer to thisfeature template as BERGSMALINPATH in Table 3.3.3 Semantic role featuresWe tried to exploit the semantic roles that were in-cluded in the CoNLL 2011 data set.
Ponzetto andStrube (2006) suggested using the concatenation ofthe predicate and the role label for a mention thathas a semantic role in a predicate.
They introducedtwo new features, I SEMROLE and J SEMROLE, thatcorrespond to the semantic roles filled by each of thementions in a pair.
We included these features in ourpool of feature templates, but we could not see anycontribution from them during the feature selection.We also introduced a number of feature templatesthat only applied to pairs of mentions that occur inthe same semantic role proposition.
These templatesincluded the concatenation of the two labels of thearguments and the predicate sense label, and vari-ations of these that also included the head wordsof either the antecedent or anaphor, or both.
Theonly feature that was selected during our feature se-lection procedure corresponds to the concatenationof the argument labels, the predicate sense, and thehead word of the anaphor: SEMROLEPROPJHD inTable 3.
In the sentence A lone protestor parked47herself outside the UN, the predicate park has thearguments A lone protestor, labeled ARG0, and her-self, labeled ARG1.
The corresponding value of thisfeature would be ARG0-park.01-ARG1-herself.3.4 Feature selectionStarting from Soon et al (2001)?s feature set, weperformed a greedy forward selection.
The fea-ture selection used a 5-fold cross-validation over thetraining set, where we evaluated the features usingthe arithmetic mean of MUC, BCUB, and CEAFE.After reaching a maximal score using forward se-lection, we reversed the process using a backwardelimination, leaving out each feature and removingthe one that had the worst impact on performance.This backwards procedure was carried out until thescore no longer increased.
We repeated this forward-backward procedure until there was no increase inperformance.
Table 3 shows the final feature set.Feature bigrams are often used to increase theseparability of linear classifiers.
Ideally, we wouldhave generated a complete bigram set from our fea-tures.
However, as this set is quadratic in natureand due to time constraints, we included only a sub-set of it in the selection procedure.
Some of them,most notably the bigram of mention head words (I-HDFORM+J-HDFORM) were selected in the proce-dure and appear in Table 3.4 EvaluationTable 1 shows some baseline figures using the binaryfeatures STRINGMATCH and ALIAS as sole corefer-ence properties, as well as our baseline system usingSoon et al (2001)?s features.MD MUC BCUBSTRINGMATCH 59.91 44.43 63.65ALIAS 19.25 16.77 48.07Soon baseline/LR 60.79 47.50 63.97Soon baseline/C4.5 58.96 47.02 65.36Table 1: Baseline figures using string match and aliasproperties, and our Soon baseline using decision treeswith the C4.5 induction program and logistic regression(LR).
MD stands for mention detection.4.1 Contribution of postprocessingThe postprocessing step described in Sec.
2.4 provedeffective, contributing from 0.21 to up to 1 point tothe final score across the metrics.
Table 2 shows thedetailed impacts on the development set.MD MUC BCUB CEAFENo postproc.
66.56 54.61 65.93 40.46With postproc.
67.21 55.62 66.29 40.67Increase 0.65 1.01 0.36 0.21Table 2: Impact of the postprocessing step on the devel-opment set.4.2 Contribution of featuresThe lack of time prevented us from running a com-plete selection from scratch and describing the con-tribution of each feature on a clean slate.
Nonethe-less, we computed the scores when one feature isremoved from the final feature set.
Table 3 showsthe performance degradation observed on the devel-opment set, which gives an indication of the impor-tance of each feature.
In these runs, no postprocess-ing was not used.Toward the end of the table, some features showa negative contribution to the score on the devel-opment set.
This is explained by the fact that ourfeature selection was carried out in a cross-validatedmanner over the training set.4.3 Results on the test setTable 4 shows the results we obtained on the test set.The figures are consistent with the performance onthe development set across the three official metrics,with an increase of the MUC score and a decreaseof both BCUB and CEAFE.
The official score in theshared task is computed as the mean of these threemetrics.The shared task organizers also provided a test setwith given mention boundaries.
The given bound-aries included nonanaphoric and singleton mentionsas well.
Using this test set, we replaced our mentionextraction step and used the given mention bound-aries instead.
Table 4 shows the results with thissetup.
As mention boundaries were given, we turnedoff our postprocessing module for this run.48Metric\Corpus Development set Test set Test set with gold mentionsR P F1 R P F1 R P F1Mention detection 65.68 68.82 67.21 69.87 68.08 68.96 74.18 70.74 72.42MUC 55.26 55.98 55.62 60.20 57.10 58.61 64.33 60.05 62.12BCUB 65.07 67.56 66.29 66.74 64.23 65.46 68.26 65.17 66.68CEAFM 52.51 52.51 52.51 51.45 51.45 51.45 53.84 53.84 53.84CEAFE 41.02 40.33 40.67 38.09 41.06 39.52 39.86 44.23 41.93BLANC 69.6 70.41 70 71.99 70.31 71.11 72.53 71.04 71.75Official CoNLL score 53.78 54.62 54.19 55.01 54.13 54.53 57.38 56.48 56.91Table 4: Scores on development set, on the test set, and on the test set with given mention boundaries: recall (R),precision (P), and harmonic mean (F1).
The official CoNLL score is computed as the mean of MUC, BCUB, andCEAFE.MD MUC BCUBAll features 66.56 54.61 65.93I-HDFORM+J-HDFORM -1.35 -2.66 -1.82STRINGMATCH?
-1.12 -1.32 -1.55DISTANCE?
-0.16 -0.62 -0.59J-HDGOVPOS -0.51 -0.49 -0.13I-HDRMCFUN -0.27 -0.39 -0.2ALIAS?
-0.47 -0.36 -0.06I-HDFORM -0.42 -0.18 0.04I-GENDER+J-GENDER -0.3 -0.15 0.05NUMBERAGREEMENT?
0.01 -0.14 -0.41I-HDPOS -0.32 -0.14 0.05J-PRONOUN?
-0.25 -0.08 -0.09I-HDLMCFORM+J-HDLMCFORM -0.41 -0.04 0.08I-HDLSFORM -0.01 0.01 0SSBERGSMALINPATH -0.04 0.02 -0.13I-HDGOVFUN -0.09 0.09 0.01J-HDFUN -0.01 0.13 -0.04I-HDLMCPOS -0.08 0.13 -0.09DSPATHFORM -0.03 0.16 -0.02J-HDGOVFUN -0.04 0.16 -0.05J-DEMONSTRATIVE?
-0.03 0.18 0.03GENDERAGREEMENT?
0 0.18 -0.01SEMROLEPROPJHD 0.01 0.2 0.01I-PRONOUN?
0.01 0.22 0.04I-HDFUN 0.05 0.22 -0.06Table 3: The final feature set and, for each feature, thedegradation in performance when leaving out this featurefrom the set.
All evaluations were carried out on the de-velopment set.
The features marked with a dagger ?
orig-inate from the Soon et al (2001) baseline feature set.5 ConclusionsThe main conclusions and contributions of our workto the CoNLL 2011 shared task concern the detec-tion of mention boundaries, feature lexicalization,and dependency features.The mention boundaries are relatively difficult toidentify.
Although far from perfect, we applied a di-rect mapping from constituents to extract the men-tions used in the resolution procedure.
We then re-claimed some mentions involving proper nouns in apostprocessing step.
Using the gold-standard men-tion boundaries in the test set, we saw an increase inall metrics with up to 3.51 for the MUC score.The lexicalization of the feature set brings a sig-nificant improvement to the scores.
By order of per-formance loss in Table 3, the first feature of ourmodel is a lexical one.
This property does not seemto have been systematically explored before, possi-bly because of a tradition of using corpora of modestsizes in coreference resolution.Grammatical dependencies seem to play an im-portant role in the anaphoric expressions.
Results inTable 3 also show this, although in a less pronouncedmanner than lexicalization.
Features extracted fromdependencies are implicit in many systems, but arenot explicitly mentioned as such.
We hope our workhelped clarified this point through a more systematicexploration of this class of features.AcknowledgementsThis research was supported by Vetenskapsra?det, theSwedish research council, under grant 621-2010-4800.ReferencesShane Bergsma and Dekang Lin.
2006.
Bootstrappingpath-based pronoun resolution.
In Proceedings of the4921st International Conference on Computational Lin-guistics and 44th Annual Meeting of the ACL, pages33?40, July.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Explorations, 11(1):10?18, July.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InJoakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek,and Mare Koit, editors, NODALIDA 2007 ConferenceProceedings, pages 105?112, Tartu, May 25-26.Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics, pages 104?111.Simone Paolo Ponzetto and Michael Strube.
2006.
Se-mantic role labeling for coreference resolution.
InProceedings of the 11th Conference of EACL: Postersand Demonstrations, pages 143?146, April.Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007.
Unre-stricted coreference: Identifying entities and events inOntoNotes.
In Proceedings of the IEEE InternationalConference on Semantic Computing (ICSC), Irvine,CA, September 17-19.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 shared task: Modeling unre-stricted coreference in OntoNotes.
In Proceedings ofthe Fifteenth Conference on Computational NaturalLanguage Learning (CoNLL 2011), Portland, Oregon,June.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.50
