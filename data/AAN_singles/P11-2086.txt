Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 491?495,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAutomatic Assessment of Coverage Quality in IntelligenceReportsSamuel BrodySchool of Communicationand InformationRutgers Universitysdbrody@gmail.comPaul KantorSchool of Communicationand InformationRutgers Universitypaul.kantor@rutgers.eduAbstractCommon approaches to assessing docu-ment quality look at shallow aspects, suchas grammar and vocabulary.
For manyreal-world applications, deeper notions ofquality are needed.
This work representsa first step in a project aimed at devel-oping computational methods for deep as-sessment of quality in the domain of intel-ligence reports.
We present an automatedsystem for ranking intelligence reportswith regard to coverage of relevant mate-rial.
The system employs methodologiesfrom the field of automatic summarization,and achieves performance on a par withhuman judges, even in the absence of theunderlying information sources.1 IntroductionDistinguishing between high- and low-qualitydocuments is an important skill for humans, anda challenging task for machines.
The majority ofprevious research on the subject has focused onlow-level measures of quality, such as spelling,vocabulary and grammar.
However, in manyreal-world situations, it is necessary to employdeeper criteria, which look at the content of thedocument and the structure of argumentation.One example where such criteria are essentialis decision-making in the intelligence commu-nity.
This is also a domain where computationalmethods can play an important role.
In a typi-cal situation, an intelligence officer faced with animportant decision receives reports from a teamof analysts on a specific topic of interest.
Eachdecision may involve several areas of interest,resulting in several collections of reports.
Addi-tionally, the officer may be engaged in many de-cision processes within a small window of time.Given the nature of the task, it is vital thatthe limited time be used effectively, i.e., thatthe highest-quality information be handled first.Our project aims to provide a system that willassist intelligence officers in the decision makingprocess by quickly and accurately ranking re-ports according to the most important criteriafor the task.In this paper, as a first step in the project,we focus on content-related criteria.
In particu-lar, we chose to start with the aspect of ?cover-age?.
Coverage is perhaps the most importantelement in a time-sensitive scenario, where anintelligence officer may need to choose amongseveral reports while ensuring no relevant andimportant topics are overlooked.2 Related WorkMuch of the work on automatic assessment ofdocument quality has focused on student essays(e.g., Larkey 1998; Shermis and Burstein 2002;Burstein et al 2004), for the purpose of grad-ing or assisting the writers (e.g., ESL students).This research looks primarily at issues of gram-mar, lexical selection, etc.
For the purpose ofjudging the quality of intelligence reports, theseaspects are relatively peripheral, and relevantmostly through their effect on the overall read-ability of the document.
The criteria judgedmost important for determining the quality ofan intelligence report (see Sec.
2.1) are morecomplex and deal with a deeper level of repre-sentation.In this work, we chose to start with crite-ria related to content choice.
For this task,491we propose that the most closely related priorresearch is that on automatic summarization,specifically multi-document extractive summa-rization.
Extractive summarization works alongthe following lines (Goldstein et al, 2000): (1)analyze the input document(s) for importantthemes; (2) select the best sentences to includein the summary, taking into account the sum-marization aspects (coverage, relevance, redun-dancy) and generation aspects (grammaticality,sentence flow, etc.).
Since we are interested incontent choice, we focus on the summarizationaspects, starting with coverage.
Effective waysof representing content and ensuring coverageare the subject of ongoing research in the field(e.g., Gillick et al 2009, Haghighi and Vander-wende 2009).
In our work, we draw on ele-ments from this research.
However, they mustbe adapted to our task of quality assessment andmust take into account the specific characteris-tics of our domain of intelligence reports.
Moredetail is provided in Sec.
3.1.2.1 The ARDA Challenge WorkshopGiven the nature of our domain, real-world dataand gold standard evaluations are difficult to ob-tain.
We were fortunate to gain access to thereports and evaluations from the ARDA work-shop (Morse et al, 2004), which was conductedby NIST in 2004.
The workshop was designed todemonstrate the feasibility of assessing the effec-tiveness of information retrieval systems.
Dur-ing the workshop, seven intelligence analystswere each asked to use one of several IR sys-tems to obtain information about eight differentscenarios and write a report about each.
Thisresulted in 56 individual reports.The same seven analysts were then asked tojudge each of the 56 reports (including theirown) on several criteria on a scale of 0 (worst)to 5 (best).
These criteria, listed in Table 1,were chosen by the researchers as desirable ina ?high-quality?
intelligence report.
From anNLP perspective they can be divided into threebroad categories: content selection, structure,and readability.
The written reports, along withtheir associated human quality judgments, formthe dataset used in our experiments.
As men-tioned, this work focuses on coverage.
When as-ContentCOVER covers the material relevant to the queryNO-IRR avoids irrelevant materialNO-RED avoids redundancyStructureORG organized presentation of materialReadabilityCLEAR clear and easy to read and understandTable 1: Quality criteria used in the ARDA work-shop, divided into broad categories.sessing coverage, it is only meaningful to com-pare reports on the same scenario.
Therefore,we regard our dataset as 8 collections (ScenarioA to Scenario H), each containing 7 reports.3 Experiments3.1 MethodologyIn the ARDA workshop, the analysts weretasked to extract and present the informationwhich was relevant to the query subject.
Thiscan be viewed as a summarization task.
In fact,a high quality report shares many of the charac-teristics of a good document summary.
In par-ticular, it seeks to cover as much of the impor-tant information as possible, while avoiding re-dundancy and irrelevant information.When seeking to assess these qualities, wecan treat the analysts?
reports as output from(human) summarization systems, and employmethods from automatic summarization to eval-uate how well they did.One challenge to our analysis is that we donot have access to the information sources usedby the analysts.
This limitation is inherent tothe domain, and will necessarily impact the as-sessment of coverage, since we have no means ofdetermining whether an analyst has included allthe relevant information to which she, in partic-ular, had access.
We can only assess coveragewith respect to what was included in the otheranalysts?
reports.
For our task, however, thisis sufficient, since our purpose is to identify, forthe person who must choose among them, thereport which is most comprehensive in its cover-age, or indicate a subset of reports which coverall topics discussed in the collection as a whole1.1The absence of the sources also means the systemis only able to compare reports on the same subject, asopposed to humans, who might rank the coverage quality492As a first step in modeling relevant conceptswe employ a word-gram representation, and usefrequency as a measure of relevance.
Exam-ination of high-quality human summaries hasshown that frequency is an important factor(Nenkova et al, 2006), and word-gram repre-sentations are employed in many summariza-tion systems (e.g., Radev et al 2004, Gillick andFavre 2009).
Following Gillick and Favre (2009),we use a bigram representation of concepts2.
Foreach document collection D, we calculate the av-erage prevalence of every bigram concept in thecollection:prevD(c) =1|D|?r?DCountr(c) (1)Where r labels a report in the collection, andCountr(c) is the number of times the concept cappears in report r.This scoring function gives higher weight toconcepts which many reports mentioned manytimes.
These are, presumably, the terms consid-ered important to the subject of interest.
Weignore concepts (bigrams) composed entirely ofstop words.
To model the coverage of a report,we calculate a weighted sum of the concepts itmentions (multiple mentions do not increase thisscore), using the prevalence score as the weight,as shown in Equation 2.CoverScore(r ?
D) =?c?Concepts(r)prevD(c)(2)Here, Concepts(r) is the set of concepts ap-pearing at least once in report r. The systemproduces a ranking of the reports in order oftheir coverage score (where highest is consideredbest).3.2 EvaluationAs a gold standard, we use the average ofthe scores given to each report by the humanof two reports on completely different subjects, based onexternal knowledge.
For our usage scenario, this is notan issue.2We also experimented with unigram and trigram rep-resentations, which did not do as well as the bigram rep-resentation (as suggested by Gillick and Favre 2009).judges3.
Since we are interested in ranking re-ports by coverage, we convert the scores fromthe original numerical scale to a ranked list.We evaluate the performance of the algorithms(and of the individual judges) using Kendall?sTau to measure concordance with the gold stan-dard.
Kendall?s Tau coefficient (?k) is com-monly used (e.g., Jijkoun and Hofmann 2009)to compare rankings, and looks at the numberof pairs of ranked items that agree or disagreewith the ordering in the gold standard.
LetT = {(ai, aj) : ai ?g aj} denote the set of pairsordered in the gold standard (ai precedes aj).Let R = {(al, am) : al ?r am} denote the set ofpairs ordered by a ranking algorithm.
C = T?Ris the set of concordant pairs, i.e., pairs orderedthe same way in the gold standard and in theranking, and D = T ?R is the set of discordantpairs.
Kendall?s rank correlation coefficient ?k isdefined as follows:?k =|C| ?
|D||T |(3)The value of ?k ranges from -1 (reversed rank-ing) to 1 (perfect agreement), with 0 beingequivalent to a random ranking (50% agree-ment).
As a simple baseline system, we rank thereports according to their length in words, whichasserts that a longer document has ?more cov-erage?.
For comparison, we also examine agree-ment between individual human judges and thegold standard.
In each scenario, we calculatethe average agreement (Tau value) between anindividual judge and the gold standard, and alsolook at the highest and lowest Tau value fromamong the individual judges.3.3 ResultsFigure 1 presents the results of our ranking ex-periments on each of the eight scenarios.Human Performance There is a relativelywide range of performance among the human3Since the judges in the NIST experiment were alsothe writers of the documents, and the workshop report(Morse et al, 2004) identified a bias of the individualjudges when evaluating their own reports, we did notinclude the score given by the report?s author in thisaverage.
I.e, the gold standard score was the average ofthe scores given by the 6 judges who were not the author.493-0.200.20.40.60.81HGFEDCBAAgreementScenarioNum.
WordsJudgesConceptsFigure 1: Agreement scores (Kendall?s Tau) for the word-count baseline (Num.
Words), the concept-basedalgorithm (Concepts).
Scores for the individual human judges (Judges) are given as a range from lowest tohighest individual agreement score, with ?x?
indicating the average.judges.
This is indicative of the cognitive com-plexity of the notion of coverage.
We can seethat some human judges are better than oth-ers at assessing this quality (as represented bythe gold standard).
It is interesting to note thatthere was not a single individual judge who wasworst or best across all cases.
A system that out-performs some individual human judge on thistask can be considered successful, and one thatsurpasses the average individual agreement evenmore so.Baseline The experiments bear out the intu-ition that led to our choice of baseline.
The num-ber of words in a document is significantly corre-lated with its gold-standard coverage rank.
Thissimple baseline is surprisingly effective, outper-forming the worst human judge in seven out ofeight scenarios, and doing better than the aver-age individual in two of them.System Performance Our concept-basedranking system exhibits very strong perfor-mance4.
It is as good or better than thebaseline in all scenarios.
It outperforms theworst individual human judge in seven of theeight cases, and does better than the averageindividual agreement in four.
This is in spite ofthe fact that the system had no access to the4Our conclusions are based on the observed differencesin performance, although statistical significance is diffi-cult to assess, due to the small sample size.sources of information available to the writers(and judges) of the reports.When calculating the overall agreement withthe gold-standard over all the scenarios, ourconcept-based system came in second, outper-forming all but one of the human judges.
Theword-count baseline was in the last place, closebehind a human judge.
A unigram-based sys-tem (which was our first attempt at modelingconcepts) tied for third place with two humanjudges.3.4 Discussion and Future WorkWe have presented a system for assessing therelative quality of intelligence reports with re-gard to their coverage.
Our method makes useof ideas from the summarization literature de-signed to capture the notion of content units andrelevance.
Our system is as accurate as individ-ual human judges for this concept.The bigram representation we employ is onlya rough approximation of actual concepts orthemes.
We are in the process of obtaining moredocuments in the domain, which will allow theuse of more complex models and more sophis-ticated representations.
In particular, we areconsidering clusters of terms and probabilistictopic models such as LDA (Blei et al, 2003).However, the limitations of our domain, primar-494ily the small amount of relatively short docu-ments, may restrict their applicability, and ad-vocate instead the use of semantic knowledgeand resources.This work represents a first step in the com-plex task of assessing the quality of intelligencereports.
In this paper we focused on coverage -perhaps the most important aspect in determin-ing which single report to read among several.There are many other important factors in as-sessing quality, as described in Section 2.1.
Wewill address these in future stages of the qualityassessment project.4 ACKNOWLEDGMENTSThe authors were funded by an IC PostdocGrant (HM 1582-09-01-0022).
The secondauthor also acknowledges the support of theAQUAINT program, and the KDD program un-der NSF Grants SES 05-18543 and CCR 00-87022.
We would like to thank Dr. EmileMorse of NIST for her generosity in providingthe documents and set of judgments from theARDA Challenge Workshop project, and Prof.Dragomir Radev for his assistance and advice.We would also like to thank the anonymous re-viewers for their helpful comments.ReferencesBlei, David M., Andrew Y. Ng, and Michael I.Jordan.
2003.
Latent dirichlet alocation.Journal of Machine Learning Research 3:993?1022.Burstein, Jill, Martin Chodorow, and ClaudiaLeacock.
2004.
Automated essay evaluation:the criterion online writing service.
AI Mag.25:27?36.Gillick, Dan and Benoit Favre.
2009.
A scal-able global model for summarization.
In Proc.of the Workshop on Integer Linear Program-ming for Natural Language Processing .
ACL,Stroudsburg, PA, USA, ILP ?09, pages 10?18.Gillick, Daniel, Benoit Favre, Dilek Hakkani-Tur, Berndt Bohnet, Yang Liu, and ShashaXie.
2009.
The ICSI/UTD SummarizationSystem at TAC 2009.
In Proc.
of the TextAnalysis Conference workshop, Gaithersburg,MD (USA).Goldstein, Jade, Vibhu Mittal, Jaime Carbonell,and Mark Kantrowitz.
2000.
Multi-documentsummarization by sentence extraction.
InProc.
of the 2000 NAACL-ANLP Work-shop on Automatic summarization - Volume4 .
Association for Computational Linguis-tics, Stroudsburg, PA, USA, NAACL-ANLP-AutoSum ?00, pages 40?48.Haghighi, Aria and Lucy Vanderwende.
2009.Exploring content models for multi-documentsummarization.
In Proc.
of Human LanguageTechnologies: The 2009 Annual Conferenceof the North American Chapter of the Asso-ciation for Computational Linguistics.
ACL,Boulder, Colorado, pages 362?370.Jijkoun, Valentin and Katja Hofmann.
2009.Generating a non-english subjectivity lexicon:Relations that matter.
In Proc.
of the 12thConference of the European Chapter of theACL (EACL 2009).
ACL, Athens, Greece,pages 398?405.Larkey, Leah S. 1998.
Automatic essay grad-ing using text categorization techniques.
InSIGIR ?98: Proceedings of the 21st annualinternational ACM SIGIR conference on Re-search and development in information re-trieval .
ACM, New York, NY, USA, pages 90?95.Morse, Emile L., Jean Scholtz, Paul Kantor, Di-ane Kelly, and Ying Sun.
2004.
An investi-gation of evaluation metrics for analytic ques-tion answering.
Available by request from thefirst author.Nenkova, Ani, Lucy Vanderwende, and Kath-leen McKeown.
2006.
A compositional contextsensitive multi-document summarizer: ex-ploring the factors that influence summariza-tion.
In SIGIR.
ACM, pages 573?580.Radev, Dragomir R., Hongyan Jing, Ma lgorzataStys?, and Daniel Tam.
2004.
Centroid-basedsummarization of multiple documents.
Inf.Process.
Manage.
40:919?938.Shermis, Mark D. and Jill C. Burstein, editors.2002.
Automated Essay Scoring: A Cross-disciplinary Perspective.
Routledge, 1 edition.495
