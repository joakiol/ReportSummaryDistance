Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326?1336,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsCharacter-Level Chinese Dependency ParsingMeishan Zhang?, Yue Zhang?, Wanxiang Che?, Ting Liu??
?Research Center for Social Computing and Information RetrievalHarbin Institute of Technology, China{mszhang, car, tliu}@ir.hit.edu.cn?Singapore University of Technology and Designyue zhang@sutd.edu.sgAbstractRecent work on Chinese analysis has ledto large-scale annotations of the internalstructures of words, enabling character-level analysis of Chinese syntactic struc-tures.
In this paper, we investigate theproblem of character-level Chinese depen-dency parsing, building dependency treesover characters.
Character-level infor-mation can benefit downstream applica-tions by offering flexible granularities forword segmentation while improving word-level dependency parsing accuracies.
Wepresent novel adaptations of two ma-jor shift-reduce dependency parsing algo-rithms to character-level parsing.
Exper-imental results on the Chinese Treebankdemonstrate improved performances overword-based parsing methods.1 IntroductionAs a light-weight formalism offering syntacticinformation to downstream applications such asSMT, the dependency grammar has received in-creasing interest in the syntax parsing commu-nity (McDonald et al, 2005; Nivre and Nilsson,2005; Carreras et al, 2006; Duan et al, 2007; Kooand Collins, 2010; Zhang and Clark, 2008; Nivre,2008; Bohnet, 2010; Zhang and Nivre, 2011; Choiand McCallum, 2013).
Chinese dependency treeswere conventionally defined over words (Chang etal., 2009; Li et al, 2012), requiring word segmen-tation and POS-tagging as pre-processing steps.Recent work on Chinese analysis has embarkedon investigating the syntactic roles of characters,leading to large-scale annotations of word internalstructures (Li, 2011; Zhang et al, 2013).
Such an-notations enable dependency parsing on the char-acter level, building dependency trees over Chi-nese characters.
Figure 1(c) shows an example of?Corresponding author.???
???
?
?
?
?forestry administration deputy director meeting in make a speech(a) a word-based dependency tree?
?
?
?
?
?
?
?
?
?woods industry office deputy office manager meeting in make speech(b) a character-level dependency tree by Zhao (2009) withreal intra-word and pseudo inter-word dependencies?
?
?
?
?
?
?
?
?
?woods industry office deputy office manager meeting in make speech(c) a character-level dependency tree investigated in this pa-per with both real intra- and inter-word dependenciesFigure 1: An example character-level dependencytree.
?????????????
(The deputydirector of forestry administration make a speechin the meeting)?.a character-level dependency tree, where the leafnodes are Chinese characters.Character-level dependency parsing is interest-ing in at least two aspects.
First, character-leveltrees circumvent the issue that no universal stan-dard exists for Chinese word segmentation.
In thewell-known Chinese word segmentation bakeofftasks, for example, different segmentation stan-dards have been used by different data sets (Emer-son, 2005).
On the other hand, most disagreementon segmentation standards boils down to disagree-ment on segmentation granularity.
As demon-strated by Zhao (2009), one can extract both fine-grained and coarse-grained words from character-level dependency trees, and hence can adapt toflexible segmentation standards using this formal-ism.
In Figure 1(c), for example, ????
(deputy1326director)?
can be segmented as both ??
(deputy)| ??
(director)?
and ????
(deputy direc-tor)?, but not ??
(deputy) ?
(office) | ?
(man-ager)?, by dependency coherence.
Chinese lan-guage processing tasks, such as machine transla-tion, can benefit from flexible segmentation stan-dards (Zhang et al, 2008; Chang et al, 2008).Second, word internal structures can also beuseful for syntactic parsing.
Zhang et al (2013)have shown the usefulness of word structures inChinese constituent parsing.
Their results on theChinese Treebank (CTB) showed that character-level constituent parsing can bring increased per-formances even with the pseudo word structures.They further showed that better performances canbe achieved when manually annotated word struc-tures are used instead of pseudo structures.In this paper, we make an investigation ofcharacter-level Chinese dependency parsing usingZhang et al (2013)?s annotations and based ona transition-based parsing framework (Zhang andClark, 2011).
There are two dominant transition-based dependency parsing systems, namely thearc-standard and the arc-eager parsers (Nivre,2008).
We study both algorithms for character-level dependency parsing in order to make a com-prehensive investigation.
For direct comparisonwith word-based parsers, we incorporate the tra-ditional word segmentation, POS-tagging and de-pendency parsing stages in our joint parsing mod-els.
We make changes to the original transitionsystems, and arrive at two novel transition-basedcharacter-level parsers.We conduct experiments on three data sets, in-cluding CTB 5.0, CTB 6.0 and CTB 7.0.
Exper-imental results show that the character-level de-pendency parsing models outperform the word-based methods on all the data sets.
Moreover,manually annotated intra-word dependencies cangive improved word-level dependency accuraciesthan pseudo intra-word dependencies.
These re-sults confirm the usefulness of character-levelsyntax for Chinese analysis.
The source codesare freely available at http://sourceforge.net/projects/zpar/, version 0.7.2 Character-Level Dependency TreeCharacter-level dependencies were first proposedby Zhao (2009).
They show that by annotat-ing character dependencies within words, one canadapt to different segmentation standards.
Thedependencies they study are restricted to intra-word characters, as illustrated in Figure 1(b).
Forinter-word dependencies, they use a pseudo right-headed representation.In this study, we integrate inter-word syntacticdependencies and intra-word dependencies usinglarge-scale annotations of word internal structuresby Zhang et al (2013), and study their interac-tions.
We extract unlabeled dependencies frombracketed word structures according to Zhang etal.
?s head annotations.
In Figure 1(c), the depen-dencies shown by dashed arcs are intra-word de-pendencies, which reflect the internal word struc-tures, while the dependencies with solid arcs areinter-word dependencies, which reflect the syntac-tic structures between words.In this formulation, a character-level depen-dency tree satisfies the same constraints as thetraditional word-based dependency tree for Chi-nese, including projectivity.
We differentiate intra-word dependencies and inter-word dependenciesby the arc type, so that our work can be com-pared with conventional word segmentation, POS-tagging and dependency parsing pipelines under acanonical segmentation standard.The character-level dependency trees hold to aspecific word segmentation standard, but are notlimited to it.
We can extract finer-grained wordsof different granulities from a coarse-grained wordby taking projective subtrees of different sizes.
Forexample, taking all the intra-word modifier nodesof ??
(manager)?
in Figure 1(c) results in theword ????
(deputy director)?, while taking thefirst modifier node of ??
(manager)?
results in theword ???
(director)?.
Note that ???
(deputyoffice)?
cannot be a word because it does not forma projective span without ??
(manager)?.Inner-word dependencies can also bring bene-fits to parsing word-level dependencies.
The headcharacter can be a less sparse feature comparedto a word.
As intra-word dependencies lead tofine-grained subwords, we can also use these sub-words for better parsing.
In this work, we usethe innermost left/right subwords as atomic fea-tures.
To extract the subwords, we find the inner-most left/right modifiers of the head character, re-spectively, and then conjoin them with all their de-scendant characters to form the smallest left/rightsubwords.
Figure 2 shows an example, where thesmallest left subword of ????
(chief lawyer)?is ???
(lawyer)?, and the smallest right subword1327?
?
?big law officer(a) smallest left subword?
?
?agree with law ize(b) smallest right subwordFigure 2: An example to illustrate the innermostleft/right subwords.of ????
(legalize)?
is ???
(legal)?.3 Character-Level Dependency ParsingA transition-based framework with global learn-ing and beam search decoding (Zhang and Clark,2011) has been applied to a number of natural lan-guage processing tasks, including word segmen-tation, POS-tagging and syntactic parsing (Zhangand Clark, 2010; Huang and Sagae, 2010; Bohnetand Nivre, 2012; Zhang et al, 2013).
It modelsa task incrementally from a start state to an endstate, where each intermediate state during decod-ing can be regarded as a partial output.
A num-ber of actions are defined so that the state ad-vances step by step.
To learn the model param-eters, it usually uses the online perceptron algo-rithm with early-update under the inexact decod-ing condition (Collins, 2002; Collins and Roark,2004).
Transition-based dependency parsing canbe modeled under this framework, where the stateconsists of a stack and a queue, and the set of ac-tions can be either the arc-eager (Zhang and Clark,2008) or the arc-standard (Huang et al, 2009)transition systems.When the internal structures of words are an-notated, character-level dependency parsing canbe treated as a special case of word-level depen-dency parsing, with ?words?
being ?characters?.A big weakness of this approach is that full wordsand POS-tags cannot be used for feature engineer-ing.
Both are crucial to well-established featuresfor word segmentation, POS-tagging and syntacticparsing.
In this section, we introduce novel exten-sions to the arc-standard and the arc-eager tran-sition systems, so that word-based and character-based features can be used simultaneously forcharacter-level dependency parsing.3.1 The Arc-Standard ModelThe arc-standard model has been applied to jointsegmentation, POS-tagging and dependency pars-ing (Hatori et al, 2012), but with pseudo wordstructures.
For unified processing of annotatedword structures and fair comparison betweencharacter-level arc-eager and arc-standard sys-tems, we define a different arc-standard transitionsystem, consistent with our character-level arc-eager system.In the word-based arc-standard model, the tran-sition state includes a stack and a queue, wherethe stack contains a sequence of partially-parseddependency trees, and the queue consists of un-processed input words.
Four actions are definedfor state transition, including arc-left (AL, whichcreates a left arc between the top element s0andthe second top element s1on the stack), arc-right(AR, which creates a right arc between s0and s1),pop-root (PR, which defines the root node of a de-pendency tree when there is only one element onthe stack and no element in the queue), and the lastshift (SH, which shifts the first element q0of thequeue onto the stack).For character-level dependency parsing, thereare two types of dependencies: inter-word depen-dencies and intra-word dependencies.
To parsethem with both character and word features, weextend the original transition actions into two cat-egories, for inter-word dependencies and intra-word dependencies, respectively.
The actions forinter-word dependencies include inter-word arc-left (ALw), inter-word arc-right (ARw), pop-root(PR) and inter-word shift (SHw).
Their definitionsare the same as the word-based model, with oneexception that the inter-word shift operation hasa parameter denoting the POS-tag of the incomingword, so that POS disambiguation is performed bythe SHwaction.The actions for intra-word dependencies in-clude intra-word arc-left (ALc), intra-word arc-right (ARc), pop-word (PW) and inter-word shift(SHc).
The definitions of ALc, ARcand SHcarethe same as the word-based arc-standard model,while PW changes the top element on the stackinto a full-word node, which can only take inter-word dependencies.
One thing to note is that, dueto variable word sizes in character-level parsing,the number of actions can vary between differ-ent sequences of actions corresponding to differ-ent analyses.
We use the padding method (Zhuet al, 2013), adding an IDLE action to finishedtransition action sequences, for better alignmentsbetween states in the beam.In the character-level arc-standard transition1328step action stack queue dependencies0 - ?
?
?
?
?
?
?1 SHw(NR) ?/NR ?
?
?
?
?
?2 SHc?/NR ?/NR ?
?
?
?
?
?3 ALc?/NR ?
?
?
?
?
A1= {?x?
}4 SHc?/NR ?/NR ?
?
?
?
?
A15 ALc?/NR ?
?
?
?
?
A2= A1?{?x?
}6 PW ??
?/NR ?
?
?
?
?
A27 SHw(NN) ??
?/NR ?/NN ?
?
?
?
?
A2?
?
?
?
?
?
?
?
?
?
?
?
?
?
?12 PW ??
?/NR ??
?/NN ?
?
?
?
?
Ai13 ALw??
?/NN ?
?
?
?
?
Ai+1= Ai?{???/NRx???/NN}?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(a) character-level dependency parsing using the arc-standard algorithmstep action stack deque queue dependencies0 - ?
?
?
?
?
?1 SHc(NR) ?
?/NR ?
?
?
?
?
?2 ALc?
?
?/NR ?
?
?
?
A1= {?x?
}3 SHc?
?/NR ?
?
?
?
?
A14 ALc?
?
?/NR ?
?
?
?
A2= A1?{?x?
}5 SHc?
?/NR ?
?
?
?
?
A26 PW ?
??
?/NR ?
?
?
?
?
A27 SHw??
?/NR ?
?
?
?
?
?
A2?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?13 PW ??
?/NR ??
?/NN ?
?
?
?
?
Ai14 ALw?
??
?/NN ?
?
?
?
?
Ai+1= Ai?{???/NRx???/NN}?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(b) character-level dependency parsing using the arc-eager algorithm, t = 1Figure 3: Character-level dependency parsing of the sentence in Figure 1(c).system, each word is initialized by the action SHwwith a POS tag, before being incrementally mod-ified by a sequence of intra-word actions, and fi-nally being completed by the action PW.
The inter-word actions can be applied when all the elementson the stack are full-word nodes, while the intra-word actions can be applied when at least the topelement on the stack is a partial-word node.
Forthe actions ALcand ARcto be valid, the top twoelements on the stack are both partial-word nodes.For the action PW to be valid, only the top ele-ment on the stack is a partial-word node.
Figure3(a) gives an example action sequence.There are three types of features.
The first twotypes are traditionally established features for thedependency parsing and joint word segmentationand POS-tagging tasks.
We use the features pro-posed by Hatori et al (2012).
The word-leveldependency parsing features are added when theinter-word actions are applied, and the featuresfor joint word segmentation and POS-tagging areadded when the actions PW, SHwand SHcare ap-plied.
Following the work of Hatori et al (2012),we have a parameter ?
to adjust the weights forjoint word segmentation and POS-tagging fea-tures.
We apply word-based dependency pars-ing features to intra-word dependency parsing aswell, by using subwords (the conjunction of char-acters spanning the head node) to replace words inword features.
The third type of features is word-structure features.
We extract the head charac-ter and the smallest subwords containing the headcharacter from the intra-word dependencies (Sec-tion 2).
Table 1 summarizes the features.3.2 The Arc-Eager ModelSimilar to the arc-standard case, the state of aword-based arc-eager model consists of a stackand a queue, where the stack contains a sequenceof partial dependency trees, and the queue con-sists of unprocessed input words.
Unlike the arc-standard model, which builds dependencies on thetop two elements on the stack, the arc-eager modelbuilds dependencies between the top element ofthe stack and the first element of the queue.
Fiveactions are defined for state transformation: arc-left (AL, which creates a left arc between the topelement of the stack s0and the first element inthe queue q0, while popping s0off the stack),arc-right (AR, which creates a right arc between1329Feature templatesLc, Lct, Rc, Rct, Llc1c, Lrc1c, Rlc1c,Lc ?Rc, Llc1ct, Lrc1ct, Rlc1ct,Lc ?Rw, Lw ?Rc, Lct ?Rw,Lwt ?Rc, Lw ?Rct, Lc ?Rwt,Lc ?Rc ?
Llc1c, Lc ?Rc ?
Lrc1c,Lc ?Rc ?
Llc2c, Lc ?Rc ?
Lrc2c,Lc ?Rc ?Rlc1c, Lc ?Rc ?Rlc2c,Llsw, Lrsw, Rlsw, Rrsw, Llswt,Lrswt, Rlswt, Rrswt, Llsw ?Rw,Lrsw ?Rw, Lw ?Rlsw, Lw ?RrswTable 1: Feature templates encoding intra-worddependencies.
L and R denote the two elementsover which the dependencies are built; the sub-scripts lc1 and rc1 denote the left-most and right-most children, respectively; the subscripts lc2 andrc2 denote the second left-most and second right-most children, respectively; w denotes the word;t denotes the POS tag; c denotes the head charac-ter; lsw and rsw denote the smallest left and rightsubwords respectively, as shown in Figure 2.s0and q0, while shifting q0from the queue ontothe stack), pop-root (PR, which defines the ROOTnode of the dependency tree when there is onlyone element on the stack and no element in thequeue), reduce (RD, which pops s0off the stack),and shift (SH, which shifts q0onto the stack).There is no previous work that exploits thearc-eager algorithm for jointly performing POS-tagging and dependency parsing.
Since the firstelement of the queue can be shifted onto the stackby either SH or AR, it is more difficult to assigna POS tag to each word by using a single action.In this work, we make a change to the configu-ration state, adding a deque between the stack andthe queue to save partial words with intra-word de-pendencies.
We divide the transition actions intotwo categories, one for inter-word dependencies(ARw, ALw, SHw, RDwand PR) and the otherfor intra-word dependencies (ARc, ALc, SHc, RDcand PW), requiring that the intra-word actions beoperated between the deque and the queue, whilethe inter-word actions be operated between thestack and the deque.For character-level arc-eager dependency pars-ing, the inter-word actions are the same as theword-based methods.
The actions ALcand ARcare the same as ALwand ARw, except that theyoperate on characters, but the SHcoperation has aparameter to denote the POS tag of a word.
ThePW action recognizes a full-word.
We also havean IDLE action, for the same reason as the arc-standard model.In the character-level arc-eager transition sys-tem, a word is formed in a similar way with thatof character-level arc-standard algorithm.
Eachword is initialized by the action SHcwith a POStag, and then incrementally changed a sequence ofintra-word actions, before being finalized by theaction PW.
All these actions operate between thequeue and deque.
For the action PW, only thefirst element in the deque (close to the queue) isa partial-word node.
For the actions ARcand ALcto be valid, the first element in the deque must bea partial-word node.
The action SHchave a POStag when shifting the first character of a word,butdoes not have such a parameter when shifting thenext characters of a word.
For the action SHcwitha POS tag to be valid, the first element in the dequemust be a full-word node.
Different from the arc-standard model, at any stage we can choose eitherthe action SHcwith a POS tag to initialize a newword on the deque, or the inter-word actions onthe stack.
In order to eliminate the ambiguity, wedefine a new parameter t to limit the max size ofthe deque.
If the deque is full with t words, inter-word actions are performed; otherwise intra-wordactions are performed.
All the inter-word actionsmust be applied on full-word nodes between thestack an the deque.
Figure 3(b) gives an exampleaction sequence.Similar to the arc-standard case, there are threetypes of features, with the first two types beingtraditionally established features for dependencyparsing and joint word segmentation and POS-tagging.
The dependency parsing features aretaken from the work of Zhang and Nivre (2011),and the features for joint word segmentation andPOS-tagging are taken from Zhang and Clark(2010)1.
The word-level dependency parsing fea-tures are triggered when the inter-word actions areapplied, while the features of joint word segmenta-tion and POS-tagging are added when the actionsSHc, ARcand PW are applied.
Again we use a pa-rameter ?
to adjust the weights for joint word seg-mentation and POS-tagging features.
The word-level features for dependency parsing are appliedto intra-word dependency parsing as well, by us-ing subwords to replace words.
The third type offeatures is word-structure features, which are the1Since Hatori et al (2012) also use Zhang and Clark(2010)?s features, the arc-standard and arc-eager character-level dependency parsing models have the same features forjoint word segmentation and POS-tagging.1330CTB50 CTB60 CTB70Training#sent 18k 23k 31k#word 494k 641k 718kDevelopment#sent 350 2.1k 10k#word 6.8k 60k 237k#oov 553 3.3k 13kTest#sent 348 2.8k 10k#word 8.0k 82k 245k#oov 278 4.6k 13kTable 2: Statistics of datasets.same as those of the character-level arc-standardmodel, shown in Table 1.4 Experiments4.1 Experimental SettingsWe use the Chinese Penn Treebank 5.0, 6.0 and 7.0to conduct the experiments, splitting the corporainto training, development and test sets accordingto previous work.
Three different splitting meth-ods are used, namely CTB50 by Zhang and Clark(2010), CTB60 by the official documentation ofCTB 6.0, and CTB70 by Wang et al (2011).
Thedataset statistics are shown in Table 2.
We usethe head rules of Zhang and Clark (2008) to con-vert phrase structures into dependency structures.The intra-word dependencies are extracted fromthe annotations of Zhang et al (2013)2.The standard measures of word-level precision,recall and F1 score are used to evaluate word seg-mentation, POS-tagging and dependency parsing,following Hatori et al (2012).
In addition, we usethe same measures to evaluate intra-word depen-dencies, which indicate the performance of pre-dicting word structures.
A word?s structure is cor-rect only if all the intra-word dependencies are allcorrectly recognized.4.2 Baseline and Proposed ModelsFor the baseline, we have two different pipelinemodels.
The first consists of a joint segmentationand POS-tagging model (Zhang and Clark, 2010)and a word-based dependency parsing model us-ing the arc-standard algorithm (Huang et al,2009).
We name this model STD (pipe).
Thesecond consists of the same joint segmentationand POS-tagging model and a word-based depen-dency parsing model using the arc-eager algorithm2https://github.com/zhangmeishan/wordstructures; their annotation was conductedon CTB 5.0, while we made annotations of the remainder ofthe CTB 7.0 words.
We also make the annotations publiclyavailable at the same site.
(Zhang and Nivre, 2011).
We name this modelEAG (pipe).
For the pipeline models, we use abeam of size 16 for joint segmentation and POS-tagging, and a beam of size 64 for dependencyparsing, according to previous work.We study the following character-level depen-dency parsing models:?
STD (real, pseudo): the arc-standard modelwith annotated intra-word dependencies andpseudo inter-word dependencies;?
STD (pseudo, real): the arc-standard modelwith pseudo intra-word dependencies andreal inter-word dependencies;?
STD (real, real): the arc-standard model withannotated intra-word dependencies and realinter-word dependencies;?
EAG (real, pseudo): the arc-eager modelwith annotated intra-word dependencies andpseudo inter-word dependencies;?
EAG (pseudo, real): the arc-eager modelwith pseudo intra-word dependencies andreal inter-word dependencies;?
EAG (real, real): the arc-eager model withannotated intra-word dependencies and realinter-word dependencies.The annotated intra-word dependencies refer tothe dependencies extracted from annotated wordstructures, while the pseudo intra-word depen-dencies used in the above models are similarto those of Hatori et al (2012).
For a givenword w = c1c2?
?
?
cm, the intra-word depen-dency structure is cx1cx2?
?
?xcm3.
The real inter-word dependencies refer to the syntactic word-level dependencies by head-finding rules fromCTB, while the pseudo inter-word dependenciesrefer to the word-level dependencies used by Zhao(2009) (wx1wx2?
?
?xwn).
The character-levelmodels with annotated intra-word dependenciesand pseudo inter-word dependencies are comparedwith the pipelines on word segmentation and POS-tagging accuracies, and are compared with thecharacter-level models with annotated intra-worddependencies and real inter-word dependencieson word segmentation, POS-tagging and word-structure predicating accuracies.
All the proposed3We also tried similar structures with right arcs, whichgave lower accuracies.1331STD (real, real) SEG POS DEP WS?
= 1 95.85 91.60 76.96 95.14?
= 2 96.09 91.89 77.28 95.29?
= 3 96.02 91.84 77.22 95.23?
= 4 96.10 91.96 77.49 95.29?
= 5 96.07 91.90 77.31 95.21Table 3: Development test results of the character-level arc-standard model on CTB60.EAG (real, real) SEG POS DEP WS?
= 1t = 1 96.00 91.66 74.63 95.49t = 2 95.93 91.75 76.60 95.37t = 3 95.93 91.74 76.94 95.36t = 4 95.91 91.71 76.82 95.33t = 5 95.95 91.73 76.84 95.40t = 3?
= 1 95.93 91.74 76.94 95.36?
= 2 96.11 91.99 77.17 95.56?
= 3 96.16 92.01 77.48 95.62?
= 4 96.11 91.93 77.40 95.53?
= 5 96.00 91.84 77.10 95.43Table 4: Development test results of the character-level arc-eager model on CTB60.models use a beam of size 64 after consideringboth speeds and accuracies.4.3 Development ResultsOur development tests are designed for two pur-poses: adjusting the parameters for the two pro-posed character-level models and testing the effec-tiveness of the novel word-structure features.
Tun-ing is conducted by maximizing word-level depen-dency accuracies.
All the tests are conducted onthe CTB60 data set.4.3.1 Parameter TuningFor the arc-standard model, there is only one pa-rameter ?
that needs tuning.
It adjusts the weightsof segmentation and POS-tagging features, be-cause the number of feature templates is much lessfor the two tasks than for parsing.
We set the valueof ?
to 1 ?
?
?
5, respectively.
Table 3 shows theaccuracies on the CTB60 development set.
Ac-cording to the results, we use ?
= 4 for our finalcharacter-level arc-standard model.For the arc-eager model, there are two parame-ters t and ?.
t denotes the deque size of the arc-eager model, while ?
shares the same meaning asthe arc-standard model.
We take two steps for pa-rameter tuning, first adjusting the more crucial pa-rameter t and then adjusting ?
on the best t. Bothparameters are assigned the values of 1 to 5.
Ta-SEG POS DEP WSSTD (real, real) 96.10 91.96 77.49 95.29STD (real, real)/wo 95.99 91.79 77.19 95.35?
-0.11 -0.17 -0.30 +0.06EAG (real, real) 96.16 92.01 77.48 95.62EAG (real, real)/wo 96.09 91.82 77.12 95.56?
-0.07 -0.19 -0.36 -0.06Table 5: Feature ablation tests for the novel word-structure features, where ?/wo?
denotes the corre-sponding models without the novel intra-word de-pendency features.ble 4 shows the results.
According to results, weset t = 3 and ?
= 3 for the final character-levelarc-eager model, respectively.4.3.2 Effectiveness of Word-StructureFeaturesTo test the effectiveness of our novel word-structure features, we conduct feature ablation ex-periments on the CTB60 development data set forthe proposed arc-standard and arc-eager models,respectively.
Table 5 shows the results.
We cansee that both the two models achieve better accu-racies on word-level dependencies with the novelword-structure features, while the features do notaffect word-structure predication significantly.4.4 Final ResultsTable 6 shows the final results on the CTB50,CTB60 and CTB70 data sets, respectively.
Theresults demonstrate that the character-level depen-dency parsing models are significantly better thanthe corresponding word-based pipeline models,for both the arc-standard and arc-eager systems.Similar to the findings of Zhang et al (2013), wefind that the annotated word structures can givebetter accuracies than pseudo word structures.
An-other interesting finding is that, although the arc-eager algorithm achieves lower accuracies in theword-based pipeline models, it obtains compara-tive accuracies in the character-level models.We also compare our results to those of Hatoriet al (2012), which is comparable to STD (pseudo,real) since similar arc-standard algorithms andfeatures are used.
The major difference is theset of transition actions.
We rerun their systemon the three datasets4.
As shown in Table 6, ourarc-standard system with pseudo word structures4http://triplet.cc/.
We use a differentconstituent-to-dependency conversion scheme in com-parison with Hatori et al (2012)?s work.1332ModelCTB50 CTB60 CTB70SEG POS DEP WS SEG POS DEP WS SEG POS DEP WSThe arc-standard modelsSTD (pipe) 97.53 93.28 79.72 ?
95.32 90.65 75.35 ?
95.23 89.92 73.93 ?STD (real, pseudo) 97.78 93.74 ?
97.40 95.77?91.24??
95.08 95.59?90.49??
94.97STD (pseudo, real) 97.67 94.28?81.63??
95.63?91.40?76.75??
95.53?90.75?75.63?
?STD (real, real) 97.84 94.62?82.14?97.30 95.56?91.39?77.09?94.80 95.51?90.76?75.70?94.78Hatori+ ?12 97.75 94.33 81.56 ?
95.26 91.06 75.93 ?
95.27 90.53 74.73 ?The arc-eager modelsEAG (pipe) 97.53 93.28 79.59 ?
95.32 90.65 74.98 ?
95.23 89.92 73.46 ?EAG (real, pseudo) 97.75 93.88 ?
97.45 95.63?91.07??
95.06 95.50?90.36??
95.00EAG (pseudo, real) 97.76 94.36?81.70??
95.63?91.34?76.87??
95.39?90.56?75.56?
?EAG (real, real) 97.84 94.36?82.07?97.49 95.71?91.51?76.99?95.16 95.47?90.72?75.76?94.94Table 6: Main results, where the results marked with ?
denote that the p-value is less than 0.001 comparedwith the pipeline word-based models using pairwise t-test.brings consistent better accuracies than their workon all the three data sets.Both the pipelines and character-level mod-els with pseudo inter-word dependencies performword segmentation and POS-tagging jointly, with-out using real word-level syntactic information.
Acomparison between them (STD/EAG (pipe) vs.STD/EAG (real, pseudo)) reflects the effectivenessof annotated intra-word dependencies on segmen-tation and POS-tagging.
We can see that both thearc-standard and arc-eager models with annotatedintra-word dependencies can improve the segmen-tation accuracies by 0.3% and the POS-tagging ac-curacies by 0.5% on average on the three datasets.Similarly, a comparison between the character-level models with pseudo inter-word dependen-cies and the character-level models with real inter-word dependencies (STD/EAG (real, pseudo) vs.STD/EAG (real, real)) can reflect the effectivenessof annotated inter-word structures on morphologyanalysis.
We can see that improved POS-taggingaccuracies are achieved using the real inter-worddependencies when jointly performing inner- andinter-word dependencies.
However, we find thatthe inter-word dependencies do not help the word-structure accuracies.4.5 AnalysisTo better understand the character-level parsingmodels, we conduct error analysis in this section.All the experiments are conducted on the CTB60test data sets.
The new advantage of the character-level models is that one can parse the internalword structures of intra-word dependencies.
Thuswe are interested in their capabilities of predict-ing word structures.
We study the word-structureaccuracies in two aspects, including OOV, wordlength, POS tags and the parsing model.4.5.1 OOVThe word-structure accuracy of OOV words re-flects a model?s ability of handling unknownwords.
The overall recalls of OOV word structuresare 67.98% by STD (real, real) and 69.01% byEAG (real, real), respectively.
We find that mosterrors are caused by failures of word segmenta-tion.
We further investigate the accuracies whenwords are correctly segmented, where the accura-cies of OOV word structures are 87.64% by STD(real, real) and 89.07% by EAG (real, real).
Theresults demonstrate that the structures of Chinesewords are not difficult to predict, and confirm thefact that Chinese word structures have some com-mon syntactic patterns.4.5.2 Parsing ModelFrom the above analysis in terms of OOV, wordlengths and POS tags, we can see that the EAG(real, real) model and the STD (real, real) mod-els behave similarly on word-structure accuracies.Here we study the two models more carefully,comparing their word accuracies sentence by sen-tence.
Figure 4 shows the results, where eachpoint denotes a sentential comparison betweenSTD (real, real) and EAG (real, real), the x-axisdenotes the sentential word-structure accuracy ofSTD (real, real), and the y-axis denotes that ofEAG (real, real).
The points at the diagonal showthe same accuracies by the two models, while oth-ers show that the two models perform differentlyon the corresponding sentences.
We can see thatmost points are beyond the diagonal line, indicat-13330.6 0.7 0.8 0.9 10.60.70.80.91STD (real, real)EAG(real,real)Figure 4: Sentential word-structure accuracies ofSTD (real, real) and EAG (real, real).ing that the two parsing models can be comple-mentary in parsing intra-word dependencies.5 Related WorkZhao (2009) was the first to study character-leveldependencies; they argue that since no consistentword boundaries exist over Chinese word segmen-tation, dependency-based representations of wordstructures serve as a good alternative for Chineseword segmentation.
Thus their main concern isto parse intra-word dependencies.
In this work,we extend their formulation, making use of large-scale annotations of Zhang et al (2013), so that thesyntactic word-level dependencies can be parsedtogether with intra-word dependencies.Hatori et al (2012) proposed a joint modelfor Chinese word segmentation, POS-tagging anddependency parsing, studying the influence ofjoint model and character features for parsing,Their model is extended from the arc-standardtransition-based model, and can be regarded asan alternative to the arc-standard model of ourwork when pseudo intra-word dependencies areused.
Similar work is done by Li and Zhou (2012).Our proposed arc-standard model is more concisewhile obtaining better performance than Hatori etal.
(2012)?s work.
With respect to word structures,real intra-word dependencies are often more com-plicated, while pseudo word structures cannot beused to correctly guide segmentation.Zhao (2009), Hatori et al (2012) and ourwork all study character-level dependency pars-ing.
While Zhao (2009) focus on word internalstructures using pseudo inter-word dependencies,Hatori et al (2012) investigate a joint model usingpseudo intra-word dependencies.
We use manualdependencies for both inner- and inter-word struc-tures, studying their influences on each other.Zhang et al (2013) was the first to perform Chi-nese syntactic parsing over characters.
They ex-tended word-level constituent trees by annotatedword structures, and proposed a transition-basedapproach to parse intra-word structures and word-level constituent structures jointly.
For Hebrew,Tsarfaty and Goldberg (2008) investigated jointsegmentation and parsing over characters using agraph-based method.
Our work is similar in ex-ploiting character-level syntax.
We study the de-pendency grammar, another popular syntactic rep-resentation, and propose two novel transition sys-tems for character-level dependency parsing.Nivre (2008) gave a systematic description ofthe arc-standard and arc-eager algorithms, cur-rently two popular transition-based parsing meth-ods for word-level dependency parsing.
We extendboth algorithms to character-level joint word seg-mentation, POS-tagging and dependency parsing.To our knowledge, we are the first to apply the arc-eager system to joint models and achieve compar-ative performances to the arc-standard model.6 ConclusionsWe studied the character-level Chinese depen-dency parsing, by making novel extensions totwo commonly-used transition-based dependencyparsing algorithms for word-based dependencyparsing.
With both pseudo and annotated wordstructures, our character-level models obtainedbetter accuracies than previous work on seg-mentation, POS-tagging and word-level depen-dency parsing.
We further analyzed some im-portant factors for intra-word dependencies, andfound that two proposed character-level pars-ing models are complementary in parsing intra-word dependencies.
We make the source codepublicly available at http://sourceforge.net/projects/zpar/, version 0.7.AcknowledgmentsWe thank the anonymous reviewers for theirconstructive comments, and gratefully acknowl-edge the support of the National Basic Re-search Program (973 Program) of China via Grant2014CB340503, the National Natural ScienceFoundation of China (NSFC) via Grant 61133012and 61370164, the Singapore Ministry of Educa-tion (MOE) AcRF Tier 2 grant T2MOE201301and SRG ISTD 2012 038 from Singapore Univer-sity of Technology and Design.1334ReferencesBernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Pro-ceedings of the EMNLP-CONLL, pages 1455?1465,Jeju Island, Korea, July.Bernd Bohnet.
2010.
Very high accuracy and fast de-pendency parsing is not a contradiction.
In Proceed-ings of the 23rd COLING, number August, pages89?97.Xavier Carreras, Mihai Surdeanu, and Llu?
?s M`arquez.2006.
Projective dependency parsing with per-ceptron.
In Proceedings of the Tenth Confer-ence on Computational Natural Language Learning(CoNLL-X), pages 181?185, New York City, June.Pi-Chuan Chang, Michel Galley, and Chris Manning.2008.
Optimizing chinese word segmentation formachine translation performance.
In ACL Workshopon Statistical Machine Translation.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, , andChristopher D. Manning.
2009.
Discriminativereordering with chinese grammatical relations fea-tures.
In Proceedings of the Third Workshop on Syn-tax and Structure in Statistical Translation.Jinho D. Choi and Andrew McCallum.
2013.Transition-based dependency parsing with selec-tional branching.
In Proceedings of ACL, pages1052?1062, August.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Volume,pages 111?118, Barcelona, Spain, July.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the 7th EMNLP.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Proba-bilistic models for action-based chinese dependencyparsing.
In Proceedings of ECML/ECPPKDD, vol-ume 4701 of Lecture Notes in Computer Science,pages 559?566.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedingsof the Second SIGHAN Workshop on Chinese Lan-guage Processing, pages 123?133.Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2012.
Incremental joint approachto word segmentation, pos tagging, and dependencyparsing in chinese.
In Proceedings of the 50th ACL,pages 1045?1053, Jeju Island, Korea, July.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of the 48th ACL, pages 1077?1086, Up-psala, Sweden, July.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing: Volume 3-Volume 3, pages 1222?1231.
Asso-ciation for Computational Linguistics.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the48th Annual Meeting of the ACL, pages 1?11.Zhongguo Li and Guodong Zhou.
2012.
Unified de-pendency parsing of chinese morphological and syn-tactic structures.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 1445?1454, Jeju Island, Ko-rea, July.Zhenghua Li, Ting Liu, and Wanxiang Che.
2012.
Ex-ploiting multiple treebanks for parsing with quasi-synchronous grammars.
In Proceedings of the 50thACL, pages 675?684, Jeju Island, Korea, July.Zhongguo Li.
2011.
Parsing the internal structure ofwords: A new paradigm for chinese word segmenta-tion.
In Proceedings of the 49th ACL, pages 1405?1414, Portland, Oregon, USA, June.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL, numberJune, pages 91?98, Morristown, NJ, USA.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proceedings ofACL.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34(4):513?553.Reut Tsarfaty and Yoav Goldberg.
2008.
Word-basedor morpheme-based?
annotation strategies for mod-ern hebrew clitics.
In LREC.
European LanguageResources Association.Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang, and Kentaro Tori-sawa.
2011.
Improving chinese word segmenta-tion and pos tagging with semi-supervised methodsusing large auto-analyzed data.
In Proceedings of5th IJCNLP, pages 309?317, Chiang Mai, Thailand,November.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: Investigating and combining graph-based and transition-based dependency parsing.
InProceedings of EMNLP, pages 562?571, Honolulu,Hawaii, October.Yue Zhang and Stephen Clark.
2010.
A fast decoderfor joint word segmentation and POS-tagging usinga single discriminative model.
In Proceedings of theEMNLP, pages 843?852, Cambridge, MA, October.1335Yue Zhang and Stephen Clark.
2011.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational Linguistics, 37(1):105?151.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th ACL, pages 188?193, Port-land, Oregon, USA, June.Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.2008.
Chinese word segmentation and statisticalmachine translation.
IEEE Transactions on SignalProcessing, 5(2).Meishan Zhang, Yue Zhang, Wanxiang Che, and TingLiu.
2013.
Chinese parsing exploiting characters.In Proceedings of the 51st ACL, pages 125?134,Sofia, Bulgaria, August.Hai Zhao.
2009.
Character-level dependencies in chi-nese: Usefulness and learning.
In Proceedings ofthe EACL, pages 879?887, Athens, Greece, March.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,and Jingbo Zhu.
2013.
Fast and accurate shift-reduce constituent parsing.
In Proceedings of the51st ACL, pages 434?443, Sofia, Bulgaria, August.1336
