Coling 2010: Poster Volume, pages 1194?1202,Beijing, August 2010Semi-supervised Semantic Pattern Discovery with Guidancefrom Unsupervised Pattern ClustersAng SunComputer Science DepartmentNew York Universityasun@cs.nyu.eduRalph GrishmanComputer Science DepartmentNew York Universitygrishman@cs.nyu.eduAbstractWe present a simple algorithm forclustering semantic patterns based ondistributional similarity and use clustermemberships to guide semi-supervisedpattern discovery.
We apply thisapproach to the task of relationextraction.
The evaluation resultsdemonstrate that our novelbootstrapping procedure significantlyoutperforms a standard bootstrapping.Most importantly, our algorithm caneffectively prevent semantic drift andprovide semi-supervised learning with anatural stopping criterion.1 IntroductionThe Natural Language Processing (NLP)community faces new tasks and new domainsall the time.
Without enough labeled data of anew task or a new domain to conduct supervisedlearning, semi-supervised learning (SSL) isparticularly attractive to NLP researchers sinceit only requires a handful of labeled examples,known as seeds.
SSL starts with these seeds totrain an initial model; it then applies this modelto a large volume of unlabeled data to get morelabeled examples and adds the most confidentones as new seeds to re-train the model.
Thisiterative procedure has been successfullyapplied to a variety of NLP tasks, such ashypernym/hyponym extraction (Hearst, 1992),word sense disambiguation (Yarowsky, 1995),question answering (Ravichandran and Hovy,2002), and information extraction (Brin, 1998;Collins and Singer, 1999; Riloff and Jones,1999; Agichtein and Gravano, 2000; Yangarberet al, 2000; Chen and Ji, 2009).While SSL can give good performance formany tasks, it is a procedure born with twodefects.
One is semantic drift.
When SSL isunder-constrained, the semantics of newlypromoted examples might stray away from theoriginal meaning of seed examples as discussedin (Brin, 1998; Curran et al, 2007; Carlson etal., 2010).
For example, a SSL procedure tolearn semantic patterns for the LocatedInrelation (PERSON in LOCATION/GPE1) mightaccept patterns for the Employment relation(employee of GPE / ORGANIZATION)because many unlabeled pairs of names areconnected by patterns belonging to multiplerelations.
Patterns connecting <Bill Clinton,Arkansas> include LocatedIn patterns such as?visit?, ?arrive in?
and ?fly to?, but also patternsindicating other relations such as ?governor of?,?born in?, and ?campaign in?.
Similar analysescan be applied to many other examples such as<Bush, Texas> and <Schwarzenegger,California>.
Without careful design, SSLprocedures usually accept bogus examplesduring certain iterations and hence the learningquality degrades.The other shortcoming of SSL is its lack ofnatural stopping criteria.
Most SSL algorithmseither run a fixed number of iterations(Agichtein and Gravano, 2000) or run against aseparate labeled test set to find the best stoppingcriterion (Abney, 2008).
The former solutionneeds a human to keep eyeballing the learningquality of different iterations and set ad-hocthresholds accordingly.
The latter requires a1 These are the types of relations and names used inthe NIST-sponsored ACE evaluation.http://www.itl.nist.gov/iad/mig//tests/ace/.
GPErepresents a Geo-Political Entity ?
an entity withland and a government.1194separate labeled test set for each new task ordomain.
They make SSL less appealing than itcould be since the intention of using SSL is tominimize supervision.In this paper, we propose a novel learningframework which can automatically monitor thesemantic drift and find a natural stoppingcriterion for SSL.
Central to our idea is thatinstead of using unlabeled data directly in SSL,we first cluster the seeds and unlabeled data inan unsupervised way before conducting SSL.The semantics of unsupervised clusters areusually unknown.
However, the cluster to whichthe seeds belong can serve as the target cluster.Then we guide the SSL procedure using thetarget cluster.
Under such learning settings,semantic drift can be automatically detected anda stopping criterion can be found:  stopping theSSL procedure when it tends to accept examplesbelonging to clusters other than the targetcluster.We demonstrate in this paper the abovegeneral idea by considering a bootstrappingprocedure to discover semantic patterns forextracting relations between named entities(NE).
Standard bootstrapping usually starts withsome high-precision and high frequency seedpatterns for a specific relation to match namedentities, then it uses newly promoted entities tosearch for additional confident patternsconnecting them.
It is a procedure driven by theduality between patterns and entities: a goodpattern can connect more than one pair ofnamed entities and a pair of named entities isusually connected by more than one goodpattern.We present a new bootstrapping procedure inwhich we first cluster the seed and otherpatterns in a large corpus based on distributionalsimilarity.
We then guide the bootstrappingusing the target cluster.The next section describes our unsupervisedpattern clusters.
Section 3 presents the details ofour novel bootstrapping procedure withguidance from pattern clusters.
We evaluate ouralgorithms in Section 4 and present related workin Section 5.
We draw conclusions and point tofuture work in Section 6.2 Pattern Clusters2.1 Distributional HypothesisThe Distributional Hypothesis (Harris, 1954)states that words that tend to occur in similarcontexts tend to have similar meanings.
Lin andPantel (2001) extended this hypothesis to coverpatterns (dependency paths in their case).
Theidea of the extension is that if two patterns tendto occur in similar contexts then the meaningsof the patterns tend to be similar.
For example,in ?X solves Y?
and ?X finds a solution to Y?,?solves?
and ?finds a solution to?
share manycommon Xs and Ys and hence are similar toeach other.
This extended distributionalhypothesis serves as the basis on which wecompute similarities for each pair of patterns.2.2 Pattern Representation ?
ShortestDependency PathWe adopt a shortest dependency path (SDP)representation of relation patterns.
SDP hasdemonstrated its power in kernel methods forrelation extraction (Bunescu and Mooney, 2005).Its capability in capturing most of theinformation of interest is also evidenced by asystematic comparison of effectiveness ofdifferent information extraction (IE) patterns in(Stevenson and Greenwood, 2006) 2 .
Forexample, ?nsubj ?
met ?
prep_in?
is able torepresent LocatedIn between ?Gates?
and?Seattle?
while a token-based pattern would bemuch less general because it would have tospecify all the intervening tokens.Figure 1.
Stanford dependency tree for sentence?Gates, Microsoft?s chairman, met with PresidentClinton in Seattle?.2 SDP is equivalent to the linked chains described inStevenson and Greenwood (2006) when thedependency of a sentence is represented as a tree nota graph.11952.3 Pre-processingWe tag and parse each sentence in our corpuswith the NYU named entity tagger 3  and theStanford dependency parser.
Then for each pairof names in the dependency tree, we extract theSDP connecting them.
Names in the path arereplaced by their types.
We require SDP tocontain at least one verb or noun.
We use thebase form of words in SDP.
We also require thelength of the path (defined as the number ofdependency relations and words in it) to bebetween 3 and 7.
Short paths are more likely tobe generic patterns such as ?of?
and can behandled separately as in (Pantel andPennacchiotti, 2006).
Very long paths are morelikely to be non-relation patterns and too sparseto be useful even if they are relation patterns.2.4 Clustering AlgorithmThe basic idea of our clustering algorithm is togroup all the paths (including the seed pathsused later for SSL) in our corpus into differentclusters based on distributional similarities.
Wefirst extract a variety of features from the namedentities X and Y connected by a path P as shownin Table 1.
We then compute an analogue of tf-idf for each feature f of P as follows: tf as thenumber of corpus instances of P having feature fdivided by the number of instances of P; idf asthe total number of paths in the corpus dividedby the number of paths with at least oneinstance with feature f. Then we adopt a vectorspace model, i.e., we construct a tf-idf featurevector for each P.  Now we compute thesimilarity between two vectors/paths usingCosine similarity and cluster all the paths usingComplete Linkage.Some technical details deserve more attentionhere.Feature extraction: We extract more typesof features than the DIRT paraphrase discoveryprocedure used in (Lin and Pantel, 2001).
Linand Pantel (2001) considered X and Y separatelywhile we also use the conjunction of X and Y.We also extract named entity types as featuressince we are interested in discovering relationsamong different types of names.
Some namesare ambiguous such as Jordan.
We hope3 Please refer to Grishman et al (2005) andhttp://cs.nyu.edu/grishman/jet/license.htmlcoupling the type with the string of the namemay alleviate the ambiguity.Table 1.
Sample features for ?X visited Y?
as in ?Jordanvisited China?Feature Type ExampleName Type of X LEFT_PERSONName Type of Y RIGHT_GPECombination ofTypes of X and YPERSON_GPEConjunction of Stringand Type of XLEFT_Jordan_PERSONConjunction of Stringand Type of YRIGHT_China_GPEConjunction ofStrings and Types ofX and YJordan_PERSON_China_GPESimilarity measure and clustering method:There are many ways to compute thesimilarity/distance between two feature vectors,such as Cosine, Euclidean, Hamming, andJaccard coefficient.
There are also manystandard clustering algorithms.
A systematiccomparison of the performance of differentdistance measures and clustering algorithms isbeyond the scope of this paper.3 Semi-supervised Relation PatternDiscoveryWe first present a standard bootstrappingalgorithm coupled with analyses of some of itsshortcomings.
Then we describe our newbootstrapping procedure which is guided bypattern clusters.3.1 Bootstrapping without GuidanceThe procedure associates a precision between 0and 1 with each pattern, and a confidencebetween 0 and 1 with each name pair.
Initiallythe seed patterns for a specific relation R haveprecision 1 and all other patterns 0.
It consists ofthe following steps:Step1: Use seed patterns to match new NEpairs and evaluate NE pairs.Intuitively, for a newly matched NE pair iN ,if many of the k patterns connecting the twonames are high-precision patterns then the namepair has a high confidence.
The confidence iscomputed by the following formula.1( ) 1 (1 Pr ( ))ki jjConf N ec p== ?
??
(1)1196Problem: While the intuition is correct, inpractice this will over-rank NE pairs which arenot only matched by patterns belonging to thetarget relation R but are also connected bypatterns of many other relations.
This is becauseof the initial settings used in many SSL systems:seeds are assigned high confidence.
Thus all NEpairs matched by initial seed patterns will havevery high confidence.Suppose the target relation is LocatedIn, and?visited?
is a seed pattern; then the <Clinton,Arkansas> example will be over-rated becausewe cannot take into account that it would alsomatch patterns of other relations such asPersonGovernorOfLocation andPersonBornInLocation in a real corpus.
Thiswill cause a vicious circle, i.e., bogus NE pairsextract more bogus patterns which furtherextract more bogus NE pairs.
We believe thisflaw of the initial settings partially results in thesemantic drift problem.One can imagine that this is not a problemthat can be solved by using a different formulato replace the one presented here.
A possiblesolution is to study the structure of unlabeleddata (NE pairs in our case) and integrate thisstructure information into the initial settings.Indeed, this is where pattern clusters come intoplay.
We will demonstrate this in Section 3.2.Step 2: Use NE pairs to search for newpatterns and rank patterns.Similar to the intuition in Step 1, for a patternp, if many of the NE pairs it matches are veryconfident then p has many supporters andshould have a high ranking.
We can use formula(2) to estimate the confidence of patterns andrank them.
( )( ) log ( )| |Sup pConf p Sup pH= ?
(2)Here |H| is the number of unique NE pairsmatched by p and Sup(p) is the sum of thesupport it can get from the |H| pairs:| |1( ) ( )HjjSup p Conf N== ?
(3)The precision of p is given by the averageconfidence of the NE pairs matched by p.( )Pr ( )| |Sup pec pH=     (4)Formula (4) normalizes the precision to rangefrom 0 to 1.
As a result the confidence of eachNE pair is also normalized to between 0 and 1.Step 3: Accept patternsMost systems accept the K top rankedpatterns in Step 2 as new seeds, subject to somerestrictions such as requiring the differences ofconfidence of the K patterns to be within a smallrange.Step 4: Loop or stopThe procedure now decides whether to repeatfrom Step 1 or to terminate.Most systems simply do not know when tostop.
They either run a fixed number ofiterations or use some held-out data to find onecriterion that works the best for the held-outdata.3.2 Bootstrapping Guided by ClustersRecall that our clustering algorithm in Section 2provides us with K clusters, each of whichcontains n (n differs in different clusters)patterns.
Every pattern in our corpus now has acluster membership (the seed patterns have thesame membership).The most important benefit from our patternclusters is that now we can measure howstrongly a NE pair iN  is associated with ourtarget cluster tC  (the one to which the seedpatterns belong).
( , )Pr ( ) tip Ci tfreq N pob N Cm??
=?
(5)Here ( , )ifreq N p  is the number of times pmatches iN  and m is the total number of patterninstances matching iN .We integrate this prior cluster distribution ofeach NE pair into the initial settings of our newbootstrapping procedure.Step1: Use seed patterns to match new NEpairs and evaluate NE pairs.Assumption: A good NE pair must bestrongly associated with the target cluster andcan be matched by multiple high-precisionpatterns.So we evaluate a NE pair by the harmonicmean of two confidence scores, namely theconfidence as its association with the targetcluster and the confidence given by the patternsmatching it.1197_ ( ) _ ( )( ) 2_ ( ) _ ( )i iii iSemi Conf N Cluster Conf NConf NSemi Conf N Cluster Conf N?= ?
+(6)1_ ( ) 1 (1 ( ))ki jjSemi Conf N Prec p== ?
??
(7)_ ( ) Pr ( )i i tCluster Conf N ob N C= ?
(8)Under such settings, <Clinton, Arkansas>will be assigned a lower confidence score forthe LocatedIn relation than it is in the standardbootstrapping.
Even if we assign high precisionto our seed patterns such as ?visited?
andconsequently the Semi_Conf is very high, it canstill be discounted by the Cluster_Conf4.Step 2: Use NE pairs to search for newpatterns and rank patterns.All the measurement functions are the sameas those used in the standard bootstrapping.However, with better ranking of NE pairs inStep 1, the patterns are also ranked better thanthey are in the standard bootstrapping.Step 3: Accept patternsWe also accept the K top ranked patterns.Step 4: Loop or stopSince each pattern in our corpus has a clustermembership, we can monitor the semantic drifteasily and naturally stop: it drifts when theprocedure tries to accept patterns which do notbelong to the target cluster; we can stop whenthe procedure tends to accept more patternsoutside of the target cluster.If our clustering algorithm can give us perfectpattern clusters, we can stop bootstrappingimmediately after it accepts the first pattern notbelonging to the target cluster.
Then thebootstrapping becomes redundant since all itdoes is to consume the patterns of the targetcluster.Facing the reality of the behavior of manyclustering algorithms, we allow the procedure tooccasionally accept patterns outside of the targetcluster but we are not tolerant when it tries toaccept more patterns outside of the target clusterthan patterns in it.
Note that when such patternsare accepted they will be moved to the targetcluster and invoke the recomputation ofCluster_Conf of NE pairs connected by thesepatterns.
The ranking functions in step 1 and 24 The Cluster_Conf of <Clinton, Arkansas> relatedto the LocatedIn relation is indeed very low (lessthan 0.1) in our experiments.insure that the procedure will only acceptpatterns which can gain strong support from NEpairs that are strongly associated with the targetcluster and are connected by many confidentpatterns.4 Experiments4.1 CorpusOur corpora contain 37 years of news articles:TDT5, NYT(94-00), APW(98-00),XINHUA(96-00), WSJ(94-96), LATWP(94-97),REUFF(94-96), REUTE(94-96), andWSJSF(87-94).
It contains roughly 65 millionsentences and 1.3 billion tokens.4.2 SeedsSeeds of the 3 relations we are going to test aregiven in table 2.
LocatedIn detects relationbetween PERSON and LOCATION/GPE;Social (SOC) detects social relations (eitherbusiness or family) between PERSON andPERSON; Employment (EMP) detectsemployment relations between PERSON andORGANIZATION.Table 2.
Seed PatternsRelation SeedsLocated-innsubj' visit dobjnsubj' travel prep_toposs' trip prep_toSOC appos friend/lawyer possappos son/spokesman prep_of/prep_fornsubj' fire dobjnsubjpass' fire agentEMP5 appos chairman/executive/founder prep_ofappos editor prep_ofappos director/head/officer/analyst prep_atappos manager prep_with(nsubj, dobj, prep, appos, poss, nsubjpass, agentstand for subject, direct object, preposition,apposition, possessive, passive nominal subjectand complement of passive verb.
The quotemarks in Table 2 and Table 3 denote inversedependencies in the dependency path.
)We work on these three relations mainlybecause of the availability of benchmarkevaluation data.
These are the most frequentrelations in our evaluation data.5 We provide more seeds (executives and staff) forEMP because it has been pointed out in (Sun, 2009)that EMP contains a lot of job titles.11984.3 Unsupervised ExperimentsWe run the clustering algorithm described inSection 2 using all the 37 years?
data.
Werequire that a pattern match at least 7 distinctNE pairs and that an NE pair must be connectedby at least 7 unique patterns.
As a result, thereare 635,128 patterns (22,225 unique ones) usedin experiments.
We use 0.005 as the cutoffthreshold of complete linkage.
The threshold isdecided by trying a series of thresholds andsearching for the maximal6 one that is capableof placing the seed patterns for each relationinto a single cluster.
Table 3 shows the top 15patterns (ranked by their corpus frequency) ofthe cluster into which our LocatedIn seeds fall.Table 3.
Top 15 patterns in the LocatedIn ClusterIndex Pattern Frequency1 nsubj' said prep_in 22032 nsubj' visit dobj 18313 poss' visit prep_to 15224 nsubj' return prep_to 13945 nsubj' tell prep_in 13636 nsubj' be prep_in 12837 nsubj' arrive prep_in 11138 nsubj' leave dobj 11069 nsubj' go prep_to 92610 nsubj' fly prep_to 70011 nsubj' come prep_to 65812 appos leader poss 45413 poss' trip prep_to 44214 rcmod be prep_in 41915 nsubj' make prep_in 4184.4 Semi-supervised ExperimentsTo provide strong statistical evidence, we divideour data into 10 folds (combinations of newsarticles from different years and different newsresources).
We then run both the standard andour new bootstrapping on the 10 folds.
For bothprocedures, we accept n patterns in a singleiteration (n is initialized to 2 and set to n + 1after each iteration).
We run 50 iterations in thestandard bootstrapping and 1,325 patterns areaccepted for each fold and each relation.
Ournew bootstrapping procedure stops when thereare two consecutive iterations in which morethan half of the newly accepted patterns do notbelong to the target cluster.
Thus the number of6  We choose the maximal value because manyclusters will be merged to a single one when thethreshold is close to 0, making the clusters toogeneral to be useful.patterns accepted for each fold and each relationdiffers as the last iteration differs.4.5 EvaluationThe output of our bootstrapping procedures is60 sets of patterns (3 relations ?
2 methods ?10 folds).
We need a data set and evaluationmethod which can compare their effectivenessequally and consistently.Evaluation data: ACE 2004 training data.ACE does not provide relation annotationbetween each pair of names.
For example, in?US President Clinton said that the UnitedStates ??
ACE annotates an EMP relationbetween the name ?US?
and nominal?President?.
There is no annotation between?US?
and ?Clinton?.
However, it provides entityco-reference information which connects?President?
to ?Clinton?.
So we take advantageof this entity co-reference information toautomatically re-annotate the relations wherepossible to link a pair of names within a singlesentence.
The re-annotation yields an EMPrelation between ?US?
and ?Clinton?.
The re-annotation is reviewed by hand to avoid addinga relation linking ?Clinton?
and the more distantco-referent ?United States?, even though ?US?and ?the United States?
refer to the same entity.This data set provides us with 412/3492positive/negative relation instances betweennames.
Among the 412 positive instances, thereare 188/117/35 instances forEMP/LocatedIn/SOC relations.Evaluation method: We adopt a directevaluation method, i.e., use our sets of patternsto extract relations between names on ACE data.Applying patterns to a benchmark data set canprovide us with better precision/recall analyses.We use a strict pattern match strategy.
We cancertainly take advantage of loose match or addpatterns as additional features to feature-basedrelation extraction systems to boost ourperformance but we do not want these tocomplicate the comparison of the standard andour new bootstrapping procedures.4.6 Results and AnalysesWe average our results on the 10 folds.
We plotprecision against recall and semantic drift rateagainst iterations (Drift).
We compute thesemantic drift rate as the percentage of false1199Figure 2.
Performance for EMP/LocatedIn/SOCEMP  Precision vs. RecallRecall0.0 .1 .2 .3 .4 .5 .6Precision.5.6.7.8.91.0LocatedIn  Precision vs. RecallRecall0.00 .05 .10 .15 .20 .25 .30 .35Precision.3.4.5.6.7.8.91.01.1SOC  Precision vs. RecallRecall.05 .10 .15 .20 .25 .30 .35 .40 .45Precision0.0.2.4.6.81.01.2positive instances belonging to ACE relationsother than the target relation.
Take EMP forexample, we compute how many of the falsepositive instances belonging to other relationssuch as LocatedIn, SOC and other ACErelations.
In all plots, red solid lines representbootstrapping with guidance from clusters andblue dotted lines standard bootstrapping.There are a number of conclusions that can beFigure 3.
Drift for EMP/LocatedIn/SOCEMP  DriftIteration0 10 20 30 40 50 60Drift0.00.05.10.15.20.25.30LocatedIn  DriftIteration0 10 20 30 40 50 60Drift0.00.05.10.15.20.25.30.35SOC  DriftIteration0 10 20 30 40 50 60Drift0.00.01.02.03.04.05.06.07drawn from these results.
We are particularlyinterested in the following two questions: Towhat extent did we prevent semantic drift by theguidance of pattern clusters?
Did we stop at theright point, i.e., can we keep high precisionwhile maintaining near maximal recall?1) It is obvious from the drift curves that ourbootstrapping effectively prevents semantic drift.Indeed, there is no drift at all when LocatedIn1200and SOC learners terminate.
Although driftindeed occurs in the EMP relation, its curve ismuch lower than that of the standardbootstrapping.2) Our new procedure terminates when theprecision is still high while maintaining areasonable recall.
Our bootstrapping forEMP/SOC/LocatedIn terminates at F-measuresof 60/37/28 (in percentage).
We conducted theWilcoxon Matched-Pairs Signed-Ranks Test onthe 10 folds, comparing the F-measures of thelast iteration of our bootstrapping guided byclusters and the iteration which provides thebest average F-measure over the 3 relations ofthe standard bootstrapping.
The results showthat the improvement of using clusters to guidebootstrapping is significant at a 97% confidencelevel.We hypothesize that when working ondozens or hundreds of relations the gain of ourprocedure will be even bigger since we caneffectively prevent inter-class errors.5 Related WorkRecent research starts exploring unlabeled datafor discriminative learning.
Miller et al, (2004)augmented name tagging training data withhierarchical word clusters and encoded clustermembership in features for improving nametagging.
Lin and Wu (2009) further explored atwo-stage cluster-based approach: firstclustering phrases and then relying on asupervised learner to identify useful clusters andassign proper weights to cluster features.
Othersimilar work includes (Wong and Ng, 2007) forname tagging, and (Koo et.
al., 2008) fordependency parsing.While similar in spirit, our supervision isminimal, i.e., we only use a few seeds while theabove approaches rely on a large amount oflabeled data.
To the best of our knowledge, thetheme explored in this paper is the first study ofusing pattern clusters for preventing semanticdrift in semi-supervised pattern discovery.Recent research also explored the idea ofdriving SSL with explicit constraintsconstructed by hand such as identifying mutualexclusion of different categories (i.e., peopleand sport are mutually exclusive).
This istermed constraint-driven learning in (Chang etal., 2007), coupled learning in (Carlson et al,2010) and counter-training in (Yangarber, 2003).The learning quality largely depends on thecompleteness of explicit constraints.
While weshare the same goal, i.e., to prevent semanticdrift, we rely on unsupervised clusters todiscover implicit constraints for us instead ofgenerating constraints by hand.Our research is also close to semi-supervisedIE pattern learners including (Riloff and Jones,1999), (Agichtein and Gravano, 2000),(Yangarber et al, 2000), and many others.While they conduct bootstrapping on unlabeleddata directly, we first cluster unlabeled data andthen bootstrap with help from clusters.There are also clear connections to work onunsupervised relation discovery (Hasegawa etal., 2004; Zhang et al, 2005; Rosenfeld andFeldman, 2007).
They group pairs of names intorelation clusters based on the contexts betweennames while we group the contexts/patterns intoclusters based on features extracted from names.6 Conclusions and Future WorkWe presented a simple algorithm for clusteringpatterns and used pattern clusters to guide semi-supervised semantic pattern discovery.
Thenovel bootstrapping procedure can achieve thebest F-1 score while maintaining a good trade-off between precision and recall.
We alsodemonstrated that it can effectively preventsemantic drift and naturally terminate.We plan to extend this idea to improverelation extraction performance with a richermodel as used in (Zhang et al, 2004; Zhou et al,2008) than a simple pattern learner.
The featurespace will be much larger than the one adoptedin this paper.
We will investigate how toovercome the memory bottleneck when weapply rich models to millions of instances.7 AcknowledgementsWe would like to thank Prof. Satoshi Sekine forhis useful suggestions.ReferencesSteven Abney.
2008.
Semisupervised Learning forComputational Linguistics, Chapman and Hall.Eugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text1201collections.
In Proc.
of the Fifth ACMInternational Conference on Digital Libraries.Sergey Brin.
Extracting patterns and relations fromthe World-Wide Web.
1998.
In Proc.
of the 1998Intl.
Workshop on the Web and Databases.Razvan C. Bunescu and Raymond J. Mooney.
2005.A Shortest Path Dependency Kernel for RelationExtraction.
In Proc.
of HLT/EMNLP.Andrew Carlson, Justin Betteridge, Richard C. Wang,Estevam Rafael Hruschka Junior and Tom M.Mitchell.
2010.
Coupled Semi-SupervisedLearning for Information Extraction.
In WSDM.Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007.Guiding semisupervision with constraint-drivenlearning.
In Proc.
of ACL-2007, Prague.Zheng Chen and Heng Ji.
2009.
Can One LanguageBootstrap the Other: A Case Study on EventExtraction.
In NAACL HLT Workshop on Semi-supervised Learning for NLP.Michael Collins and Yoram Singer.
1999.Unsupervised models for named entityclassication.
In Proc.
of EMNLP-99.James R. Curran, Tara Murphy, and Bernhard Scholz.2007.
Minimising semantic drift with MutualExclusion Bootstrapping.
In Proc.
of PACLING.Ralph Grishman, David Westbrook and AdamMeyers.
2005.
NYU?s English ACE 2005 SystemDescription.
ACE 2005 Evaluation Workshop.Zellig S. Harris.
1954.
Distributional Structure.
Word.Vol 10,1954, 146-162.Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman.2004.
Discovering Relations among NamedEntities from Large Corpora.
In Proc.
of ACL-04.Marti Hearst.
1992.
Automatic acquisition ofhyponyms from large text corpora.
In Proc.
of the14th Intl.
Conf.
on Computational Linguistics.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple Semi-supervised DependencyParsing.
In Proceedings of ACL-08: HLT.Dekang Lin and Patrick Pantel.
2001.
Discovery ofinference rules for question-answering.
NaturalLanguage Engineering, 7(4):343?360.Dekang Lin and Xiaoyun Wu.
2009.
PhraseClustering for Discriminative Learning.
InProceedings of the ACL and IJCNLP 2009.Marie-Catherine de Marneffe and Christopher D.Manning.
2008.
The Stanford typed dependenciesrepresentation.
In COLING Workshop on Cross-framework and Cross-domain Parser Evaluation.Scott Miller, Jethran Guinness and Alex Zamanian.2004.
Name Tagging with Word Clusters andDiscriminative Training.
In Proc.
of HLT-NAACL.Patrick Pantel and Marco Pennacchiotti.
2006.Espresso: Leveraging Generic Patterns forAutomatically Harvesting Semantic Relations.
InProc.
of COLING-06 and ACL-06.Deepak Ravichandran and Eduard Hovy.
2002.Learning Surface Text Patterns for a QuestionAnswering System.
In Proc.
of ACL-2002.Ellen Riloff and Rosie Jones.
1999.
Learningdictionaries for information extraction by multi-level bootstrapping.
In Proc.
of AAAI-99.Benjamin Rosenfeld, Ronen Feldman.
2007.Clustering for Unsupervised RelationIdentification.
In Proc.
of CIKM ?07.Mark Stevenson and Mark A. Greenwood.
2006.Comparing Information Extraction PatternModels.
In Proceedings of the Workshop onInformation Extraction Beyond The Document.Mark Stevenson and Mark A. Greenwood.
2005.
ASemantic Approach to IE Pattern Induction.
InProc.
of the 43rd Annual Meeting of the ACL.Ang Sun.
2009.
A Two-stage BootstrappingAlgorithm for Relation Extraction.
In RANLP-09.Yingchuan Wong and Hwee Tou Ng.
2007.
OneClass per Named Entity: Exploiting UnlabeledText for Named Entity Recognition.
In Proc.
ofIJCAI-07.Roman Yangarber.
2003.
Counter-training in thediscovery of semantic patterns.
In Proc.
of ACL.Roman Yangarber, Ralph Grishman, PasiTapanainen and Silja Huttunen.
2000.
Automaticacquisition of domain knowledge for informationextraction.
In Proc.
of COLING-2000.David Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProc.
of ACL-95.Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,and Chew Lim Tan.
2005.
Discovering RelationsBetween Named Entities from a Large RawCorpus Using Tree Similarity-Based Clustering.In IJCNLP 2005, LNAI 3651, pp.
378 ?
389.Zhu Zhang.
(2004).
Weakly supervised relationclassification for information extraction.
In Proc.of CIKM?2004.GuoDong Zhou, JunHui Li, LongHua Qian andQiaoMing Zhu.
2008.
Semi-supervised learningfor relation extraction.
IJCNLP?2008:32-39.1202
