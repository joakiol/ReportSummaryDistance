Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 486?492,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMachine Comprehension using Rich Semantic RepresentationsMrinmaya Sachan Eric P. XingSchool of Computer ScienceCarnegie Mellon University{mrinmays, epxing}@cs.cmu.eduAbstractMachine comprehension tests the sys-tem?s ability to understand a piece of textthrough a reading comprehension task.For this task, we propose an approach us-ing the Abstract Meaning Representation(AMR) formalism.
We construct mean-ing representation graphs for the giventext and for each question-answer pair bymerging the AMRs of comprising sen-tences using cross-sentential phenomenasuch as coreference and rhetorical struc-tures.
Then, we reduce machine compre-hension to a graph containment problem.We posit that there is a latent mapping ofthe question-answer meaning representa-tion graph onto the text meaning represen-tation graph that explains the answer.
Wepresent a unified max-margin frameworkthat learns to find this mapping (given acorpus of texts and question-answer pairs),and uses what it learns to answer questionson novel texts.
We show that this approachleads to state of the art results on the task.1 IntroductionLearning to efficiently represent and reason withnatural language is a fundamental yet long-standing goal in NLP.
This has led to a series ofefforts in broad-coverage semantic representation(or ?sembanking?).
Recently, AMR, a new seman-tic representation in standard neo-Davidsonian(Davidson, 1969; Parsons, 1990) framework hasbeen proposed.
AMRs are rooted, labeled graphswhich incorporate PropBank style semantic roles,within-sentence coreference, named entities andthe notion of types, modality, negation, quantifi-cation, etc.
in one framework.In this paper, we describe an approach to useElaborationText:Snippet Graph:Alignments:Hypothesis Graph:buthaveKatiearg0 dogarg1alsomodop1like -polarityBowsarg1op2 be nameSammydomainhepossarg1buthaveKatiearg0 dogarg1alsomodop1like -polarityBowsarg1op2 be nameSammydomainarg1be nameSammydomaindog Katiepossprep-ofarg1arg0arg0possText: ... Katie also has a dog, but he does not like Bows.
... His name is Sammy.
...Hypothesis: Sammy is the name of Katie?s dog.Question: What is the name of Katie?s dog.
Answer: SammyFigure 1: Example latent answer-entailing structure from the MCTestdataset.
The question and answer candidate are combined to generate a hy-pothesis.
This hypothesis is AMR parsed to construct a hypothesis meaningrepresentation graph after some post-processing (?
2.1).
Similar processingis done for each sentence in the passage as well.
Then, a subset (not neces-sarily contiguous) of these sentence meaning representation graphs is found.These representation subgraphs are further merged using coreference informa-tion, resulting into a structure called the relevant text snippet graph.
Finally, thehypothesis meaning representation graph is aligned to the snippet graph.
Thedashed red lines show node alignments, solid red lines show edge alignments,and thick solid black arrow shows the rhetorical structure label (elaboration).AMR for the task of machine comprehension.
Ma-chine comprehension (Richardson et al, 2013)evaluates a machine?s understanding by posing aseries of multiple choice reading comprehensiontests.
The tests are unique as the answer to eachquestion can be found only in its associated texts,requiring us to go beyond simple lexical solutions.Our approach models machine comprehension asan extension to textual entailment, learning to out-put an answer that is best entailed by the pas-sage.
It works in two stages.
First, we constructa meaning representation graph for the entire pas-sage (?
2.1) from the AMR graphs of compris-ing sentences.
To do this, we account for cross-sentence linguistic phenomena such as entity and486be nameperson name Sammyop1namedomaindog person name Katyop1namepossprep-ofarg1Figure 2: The AMR parse for the hypothesis in Figure 1.
The person nodesare merged to achieve the hypothesis meaning representation graph.event coreference, and rhetorical structures.
Asimilar meaning representation graph is also con-structed for each question-answer pair.
Once wehave these graphs, the comprehension task hence-forth can be reduced to a graph containment prob-lem.
We posit that there is a latent subgraph ofthe text meaning representation graph (called snip-pet graph) and a latent alignment of the question-answer graph onto this snippet graph that entailsthe answer (see Figure 1 for an example).
Then,we propose a unified max-margin approach (?
2.2)that jointly learns the latent structure (subgraphselection and alignment) and the QA model.
Weevaluate our approach on the MCTest dataset andachieve competitive or better results than a numberof previous proposals for this task.2 The Approach2.1 The Meaning Representation GraphsWe construct the meaning representation graph us-ing individual sentences AMR graphs and mergingidentical concepts (using entity and event corefer-ence).
First, for each sentence AMR, we mergenodes corresponding to multi-word expressionsand nodes headed by a date entity (?date-entity?
),or a named entity (?name?)
or a person entity(?person?).
For example, the hypothesis meaningrepresentation graph in Figure 1 was achieved bymerging the AMR parse shown in Figure 2.Next, we select the subset of sentence AMRscorresponding to sentences needed to answer thequestion.
This step uses cross-sentential phe-nomena such as rhetorical structures1and en-tities/event coreference.
The coreferent enti-ties/event mentions are further merged into onenode resulting in a graph called the relevant textsnippet graph.
A similar process is also per-1Rhetorical structure theory (Mann and Thompson, 1988)tells us that sentences with discourse relations are related toeach other.
Previous works in QA (Jansen et al, 2014) haveshown that these relations can help us answer certain kinds ofquestions.
As an example, the ?cause?
relation between sen-tences in the text can often give cues that can help us answer?why?
or ?how?
questions.
Hence, the passage meaning rep-resentation also remembers RST relations between sentences.formed with the hypothesis sentences (generatedby combining the question and answer candidate)as shown in Figure 1.2.2 Max-Margin SolutionFor each question qi?
Q, let tibe the corre-sponding passage text and Ai= {ai1, .
.
.
, aim}be the set of candidate answers to the question.Our solution casts the machine comprehensiontask as a textual entailment task by convertingeach question-answer candidate pair (qi, aij) intoa hypothesis statement hij.
We use the questionmatching/rewriting rules described in Cucerzanand Agichtein (2005) to get the hypothesis state-ments.
For each question qi, the machine com-prehension task reduces to picking the hypothe-sis?hithat has the highest likelihood of being en-tailed by the text tiamong the set of hypotheseshi= {hi1, .
.
.
, him} generated for the questionqi.
Let h?i?
hibe the hypothesis correspondingto the correct answer.As described, we use subgraph matching to helpus model the inference.
We assume that the se-lection of sentences to generate the relevant textsnippet graph and the mapping of the hypothe-sis meaning representation graph onto the passagemeaning representation graph is latent and inferit jointly along with the answer.
We treat it as astructured prediction problem of ranking the hy-pothesis set hisuch that the correct hypothesis h?iis at the top of this ranking.
We learn a scoringfunction Sw(t, h, z) with parameter w such thatthe score of the correct hypothesis h?iand corre-sponding best latent structure z?iis higher than thescore of the other hypotheses and correspondingbest latent structures.
In a max-margin fashion, wewant that Sw(ti, h?i, z?i) > S(ti, hij, zij) + 1?
?ifor all hj?
h \ h?for some slack ?i.
Writing therelaxed max margin formulation:min||w||12||w||22+ C?imaxzij,hij?hi\h?iSw(ti, hij, zij) + ?
(h?i, hij)?C?iSw(ti, h?i, z?i) (1)We use 0-1 cost, i.e.
?
(h?i, hij) = 1(h?i6=hij).
If the scoring function is convex then thisobjective is in concave-convex form and hencecan be solved by the concave-convex program-ming procedure (CCCP) (Yuille and Rangara-jan, 2003).
We assume the scoring function tobe linear:Sw(t, h, z) = wT?
(t, h, z).
Here,487?
(t, h, z) is a feature map discussed later.
TheCCCP algorithm essentially alternates betweensolving for z?i, zij?j s.t.
hij?
hi\ h?iand wto achieve a local minima.
In the absence of in-formation regarding the latent structure z we pickthe structure that gives the best score for a givenhypothesis i.e.
arg maxzSw(t, h, z).2.3 Scoring Function and InferenceNow, we define the scoring function Sw(t, h, z).Let the hypothesis meaning representation graphbe G?= (V?, E?).
Our latent structure z decom-poses into the selection (zs) of relevant sentencesthat lead to the text snippet graph G, and the map-ping (zm) of every node and edge in G?onto G.We define the score such that it factorizes overthe nodes and edges in G?.
The weight vector walso has three components ws, wvand wecorre-sponding to the relevant sentences selection, nodematches and edge matches respectively.
An edgein the graph is represented as a triple (v1, r, v2)consisting of the enpoint vertices and relation r.Sw(t, h, z) = wTsf(G?, G, t, h, zs)+?v?
?V?wTvf(v?, zm(v?))
+?e?
?E?wTef(e?, zm(e?
))Here, t is the text corresponding to the hypoth-esis h, and f are parts of the feature map ?
to bedescribed later.
z(v?)
maps a node v??
V?to anode in V .
Similarly, z(e?)
maps an edge e??
E?to an edge in E.Next, we describe the inference procedure i.e.how to select the structure that gives the best scorefor a given hypothesis.
The inference is per-formed in two steps: The first step selects therelevant sentences from the text.
This is doneby simply maximizing the first part of the score:zs= arg maxzswTsf(G?, G, t, h, zs).
Here, weonly consider subsets of 1, 2 and 3 sentences asmost questions can be answered by 3 sentencesin the passage.
The second step is formulated asan integer linear program by rewriting the scoringfunction.
The ILP objective is:?v??V?
?v?Vzv?,vwTvf(v?, v) +?e??E?
?e?Eze?,ewTef(e?, e)Here, with some abuse of notation, zv?,vandze?,eare binary integers such that zv?,v= 1 iff zmaps v?onto v else zv?,v= 0.
Similarly, ze?,e= 1iff z maps e?onto e else ze?,e= 0.
Additionally,we have the following constrains to our ILP:?
Each node v??
V?
(or each edge e??
E?)
ismapped to exactly one node v ?
V (or oneedge e ?
E).
Hence:?v?Vzv?,v= 1 ?v?and?e?Eze?,e= 1 ?e??
If an edge e??
E?is mapped to an edgee ?
E, then vertices (v1e?, v2e?)
that form theend points of e?must also be aligned to ver-tices (v1e, v2e) that form the end points of e.Here, we note that AMR parses also have in-verse relations such as ?arg0-of?.
Hence, weresolve this with a slight modification.
If nei-ther or both relations (corresponding to edgese?and e) are inverse relations (case 1), we en-force that v1e?align with v1eand v2e?align withv2e.
If exactly one of the relations is an in-verse relation (case 2), we enforce that v1e?align with v2eand v2e?align with v1e.
Hence,we introduce the following constraints:ze?e?
zv1e?v1eand ze?e?
zv2e?v2e?e?.e in case 1ze?e?
zv1e?v2eand ze?e?
zv2e?v1e?e?.e in case 22.4 FeaturesOur feature function ?
(t, h, z) decomposes intothree parts, each corresponding to a part of the la-tent structure.The first part corresponds to relevant sentenceselection.
Here, we include features for match-ing local neighborhoods in the sentence subset andthe hypothesis: features for matching bigrams, tri-grams, dependencies, semantic roles, predicate-argument structure as well as the global syntac-tic structure: a graph kernel for matching AMRgraphs of entire sentences (Srivastava and Hovy,2013).
Before computing the graph kernel, we re-verse all inverse relation edges in the AMR graph.Note that if a sentence subset contains the answerto the question, it should intuitively be similar tothe question as well as to the answer.
Hence,we add features that are the element-wise prod-uct of features for the subset-question match andsubset-answer match.
In addition to features forthe exact word/phrase match of the snippet and thehypothesis, we also add features using two para-phrase databases: ParaPara (Chan et al, 2011) andDIRT (Lin and Pantel, 2001).
These databasescontain paraphrase rules of the form string1?string2.
ParaPara rules were extracted throughbilingual pivoting and DIRT rules were extractedusing the distributional hypothesis.
Whenever we488have a substring in the text snippet that can betransformed into another using any of these twodatabases, we keep match features for the sub-string with a higher score (according to the cur-rent w) and ignore the other substring.
Finally,we also have features corresponding to the RST(Mann and Thompson, 1988) links to enable infer-ence across sentences.
RST tells us that sentenceswith discourse relations are related to each otherand can help us answer certain kinds of questions(Jansen et al, 2014).
For example, the ?cause?relation between sentences in the text can oftengive cues that can help us answer ?why?
or ?how?questions.
Hence, we have additional features -conjunction of the rhetorical structure label from aRST parser and the question word as well.The second part corresponds to node matches.Here, we have features for (a) Surface-form match(Edit-distance), and (b) Semantic word match(cosine similarity using SENNA word vectors(Collobert et al, 2011) and ?Antonymy?
?Class-Inclusion?
or ?Is-A?
relations using Wordnet).The third part corresponds to edge matches.
Letthe edges be e = (v1, r, v2) and e?= (v?1, r?, v?2)for notational convenience.
Here, we introducetwo features based on the relations - indicator thatthe two relations are the same or inverse of eachother, indicator that the two relations are in thesame relation category ?
categories as describedin Banarescu et al (2013).
Then, we introducea number of features based on distributional rep-resentation of the node pairs.
We compute threevertex vector compositions (sum, difference andproduct) of the nodes for each edge proposed inrecent representation learning literature in NLP(Mitchell and Lapata, 2008; Mikolov et al, 2013)i.e.
v1v2and v?1v?2for  = {+,?,?
}.Then, we compute the cosine similarities of theresulting compositions producing three features.Finally we introduce features based on the struc-tured distributional semantic representation (Erkand Pad?o, 2008; Baroni and Lenci, 2010; Goyalet al, 2013) which takes the relations into accountwhile performing the composition.
Here, we use alarge text corpora (in our experiments, the EnglishWikipedia) and construct a representation matrixM(r)?
V ?
V for every relation r (V is thevocabulary) where, the ijthelement M(r)ijhas thevalue log(1+x) where x is the frequency for the ithand jthvocabulary items being in relation r in thecorpora.
This allows us to compose the node andrelation representations and compare them.
Herewe compute the cosine similarity of the compo-sitions (v1)TM(r)and (v?1)TM(r?
), the compo-sitions M(r)v2and M(r?
)v?2and their repectivesums (v1)TM(r)+ M(r)v2and (v?1)TM(r?)+M(r?
)v?2to get three more features.2.5 Negation and Multi-task LearningNext, we borrow two ideas from Sachan et al(2015) namely, negation and multi-task learning,treating different question types in the machinecomprehension setup as different tasks.Handling negation is important for our modelas facts align well with their negated versions.We use a simple heuristic.
During training, if wedetect negation (using a set of simple rules thattest for presence of negation words (?not?, ?n?t?,etc.
)), we flip the corresponding constraint, nowrequiring that the correct hypothesis to be rankedbelow all the incorrect ones.
During test phase ifwe detect negation, we predict the answer corre-sponding to the hypothesis with the lowest score.QA systems often include a question classifica-tion component that divides the questions into se-mantic categories based on the type of the ques-tion or answers expected.
This allows the modelto learn question type specific parameters whenneeded.
We experiment with three task classifi-cations proposed by Sachan et al (2015).
Firstis QClassification, which classifies the question,based on the question word (what, why, what,etc.).
Next is the QAClassification scheme, whichclassifies questions into different semantic classesbased on the possible semantic types of the an-swers sought.
The third scheme, TaskClassifica-tion classifies the questions into one of 20 subtasksfor Machine Comprehension proposed in Westonet al (2015).
We point the reader to Sachan et al(2015) for details on the multi-task model.3 ExperimentsDatasets: We use MCTest-500 dataset (Richard-son et al, 2013), a freely available set of 500 sto-ries (300 train, 50 dev and 150 test) and associatedquestions to evaluate our model.
Each story inMCTest has four multiple-choice questions, eachwith four answer choices.
Each question has ex-actly one correct answer.
Each question is alsoannotated as ?single?
or ?multiple?.
The questionsannotated ?single?
require just one sentence in thepassage to answer them.
For ?multiple?
questions489it should not be possible to find the answer to thequestion with just one sentence of the passage.
In asense, ?multiple?
questions are harder than ?single?questions as they require more complex inference.We will present the results breakdown for ?single?or ?multiple?
category questions as well.Baselines: We compare our approach to the fol-lowing baselines: (1-3) The first three baselinesare taken from Richardson et al (2013).
SW andSW+D use a sliding window and match a bag ofwords constructed from the question and the can-didate answer to the text.
RTE uses textual en-tailment by selecting the hypothesis that has thehighest likelihood of being entailed by the pas-sage.
(4) LEX++, taken from Smith et al (2015)is another lexical matching method that takesinto account multiple context windows, questiontypes and coreference.
(5) JACANA uses an offthe shelf aligner and aligns the hypothesis state-ment with the passage.
(6-7) LSTM and QANTA,taken from Sachan et al (2015), use neural net-works (LTSMs and Recursive NNs, respectively).
(8) ATTENTION, taken from Yin et al (2016),uses an attention-based convolutional neural net-work.
(9) DISCOURSE, taken from Narasimhanand Barzilay (2015), proposes a discourse basedmodel.
(10-14) LSSVM, LSSVM+Negation,LSSVM+Negation (MultiTask), taken from Sachanet al (2015) are all discourse aware latent struc-tural svm models.
LSSVM+Negation accountsfor negation.
LSSVM+Negation+MTL further in-coporates multi-task learning based on questiontypes.
Here, we have three variants of multitasklearners based on the three question classificationstrategies.
(15) Finally, SYN+FRM+SEM, takenfrom Wang et al (2015) proposes a frameworkwith features based on syntax, frame semantics,coreference and word embeddings.Results: We compare our AMR subgraph contain-ment approach2where we consider our modifica-tions for negation and multi-task learning as wellin Table 1.
We can observe that our models havea comparable performance to all the baselines in-cluding the neural network approaches and all pre-vious approaches proposed for this task.
Further,when we incorporate multi-task learning, our ap-proach achieves the state of the art.
Also, our ap-proaches have a considerable improvement overthe baselines for ?multiple?
questions.
This shows2We tune the SVM parameter C on the dev set.
We useStanford CoreNLP, HILDA parser (Feng and Hirst, 2014) andJAMR (Flanigan et al, 2014) for preprocessing.Single Multiple AllAMRSubgraph 67.28 65.24 66.16Subgraph+Negation 69.48 66.46 67.83+MTLQClassification70.59 67.99 69.17QAClassification71.32 68.29 69.67TaskClassification72.05 68.90 70.33BaselinesSW 54.56 54.04 54.28SW+D 62.99 58.00 60.26RTE 69.85 42.71 55.01LEX++ 69.12 63.34 65.96JACANA Aligner 58.82 54.88 56.67LSTM 62.13 58.84 60.33QANTA 63.23 59.45 61.00ATTENTION 54.20 51.70 52.90DISCOURSE 68.38 59.90 63.75LSSVM 61.12 66.67 64.15LSSVM+Negation 63.24 66.15 64.83+MTLQClassification64.34 66.46 65.50QAClassification66.18 67.37 66.83TaskClassification67.65 67.99 67.83SYN+FRM+SEM 72.05 67.94 69.94Table 1: Comparison of variations of our method against several baselines onthe MCTest-500 dataset.
The table shows accuracy on the test set of MCTest-500.
All differences between the baselines (except SYN+FRM+SEM) and ourapproaches, and the improvements due to negation and multi-task learning aresignificant (p < 0.05) using the two-tailed paired T-test.the benefit of our latent structure that allows us tocombine evidence from multiple sentences.
Thenegation heuristic helps significantly, especiallyfor ?single?
questions (majority of negation casesin the MCTest dataset are for the ?single?
ques-tions).
The multi-task method which performs aclassification based on the subtasks for machinecomprehension defined in Weston et al (2015)does better than QAClassification that learns thequestion answer classification.
QAClassificationin turn performs better than QClassification thatlearns the question classification only.These results, together, provide validation forour approach of subgraph matching over mean-ing representation graphs, and the incorporation ofnegation and multi-task learning.4 ConclusionWe proposed a solution for reading comprehen-sion tests using AMR.
Our solution builds inter-mediate meaning representations for passage andquestion-answers.
Then it poses the comprehen-sion task as a subgraph matching task by learn-ing latent alignments from one meaning represen-tation to another.
Our approach achieves compet-itive or better performance than other approachesproposed for this task.
Incorporation of negationand multi-task learning leads to further improve-ments establishing it as the new state-of-the-art.490References[Banarescu et al2013] Laura Banarescu, Claire Bonial,Shu Cai, Madalina Georgescu, Kira Griffitt, UlfHermjakob, Kevin Knight, Philipp Koehn, MarthaPalmer, and Nathan Schneider.
2013.
Abstractmeaning representation for sembanking.
In Pro-ceedings of the 7th Linguistic Annotation Workshopand Interoperability with Discourse, pages 178?186,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.
[Baroni and Lenci2010] Marco Baroni and AlessandroLenci.
2010.
Distributional memory: A generalframework for corpus-based semantics.
Computa-tional Linguistics, 36(4):673?721.
[Chan et al2011] Tsz Ping Chan, Chris Callison-Burch, and Benjamin Van Durme.
2011.
Rerank-ing bilingually extracted paraphrases using mono-lingual distributional similarity.
In Proceedings ofthe GEMS 2011 Workshop on GEometrical Modelsof Natural Language Semantics, pages 33?42.
[Collobert et al2011] Ronan Collobert, Jason Weston,L?eon Bottou, Michael Karlen, Koray Kavukcuoglu,and Pavel Kuksa.
2011.
Natural language process-ing (almost) from scratch.
The Journal of MachineLearning Research, 12:2493?2537.
[Cucerzan and Agichtein2005] S. Cucerzan andE.
Agichtein.
2005.
Factoid question answeringover unstructured and structured content on the web.In Proceedings of TREC 2005.
[Davidson1969] Donald Davidson.
1969.
The individ-uation of events.
Springer.
[Erk and Pad?o2008] Katrin Erk and Sebastian Pad?o.2008.
A structured vector space model for wordmeaning in context.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?08, pages 897?906, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.
[Feng and Hirst2014] Vanessa Wei Feng and GraemeHirst.
2014.
A linear-time bottom-up discourseparser with constraints and post-editing.
In Proceed-ings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 511?521.
[Flanigan et al2014] Jeffrey Flanigan, Sam Thomson,Jaime G. Carbonell, Chris Dyer, and Noah A. Smith.2014.
A discriminative graph-based parser for theabstract meaning representation.
In Proceedingsof the 52nd Annual Meeting of the Association forComputational Linguistics, ACL 2014, June 22-27,2014, Baltimore, MD, USA, Volume 1: Long Papers,pages 1426?1436.
[Goyal et al2013] Kartik Goyal, Sujay Kumar Jauhar,Huiying Li, Mrinmaya Sachan, Shashank Srivas-tava, and Eduard H. Hovy.
2013.
A structured dis-tributional semantic model for event co-reference.In Proceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics, ACL 2013,4-9 August 2013, Sofia, Bulgaria, Volume 2: ShortPapers, pages 467?473.
[Jansen et al2014] Peter Jansen, Mihai Surdeanu, andPeter Clark.
2014.
Discourse complements lexicalsemantics for non-factoid answer reranking.
In Pro-ceedings of the 52nd Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 977?986.
[Lin and Pantel2001] Dekang Lin and Patrick Pantel.2001.
Dirt@ sbt@ discovery of inference rules fromtext.
In Proceedings of the seventh ACM SIGKDDinternational conference on Knowledge discoveryand data mining, pages 323?328.
[Mann and Thompson1988] William C Mann and San-dra A Thompson.
1988.
{Rhetorical Struc-ture Theory: Toward a functional theory of textorganisation}.
Text, 3(8):234?281.
[Mikolov et al2013] Tomas Mikolov, Wen-tau Yih, andGeoffrey Zweig.
2013.
Linguistic regularities incontinuous space word representations.
In Proceed-ings of the 2013 Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages746?751, Atlanta, Georgia, June.
Association forComputational Linguistics.
[Mitchell and Lapata2008] Jeff Mitchell and MirellaLapata.
2008.
Vector-based models of seman-tic composition.
In ACL 2008, Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics, June 15-20, 2008, Columbus,Ohio, USA, pages 236?244.
[Narasimhan and Barzilay2015] Karthik Narasimhanand Regina Barzilay.
2015.
Machine comprehen-sion with discourse relations.
In Proceedings ofthe 53rd Annual Meeting of the Association forComputational Linguistics and the 7th InternationalJoint Conference on Natural Language Processingof the Asian Federation of Natural LanguageProcessing, ACL 2015, July 26-31, 2015, Beijing,China, Volume 1: Long Papers, pages 1253?1262.
[Parsons1990] Terence Parsons.
1990.
Events in theSemantics of English, volume 5.
In MIT Press.
[Richardson et al2013] Matthew Richardson, Christo-pher JC Burges, and Erin Renshaw.
2013.
Mctest:A challenge dataset for the open-domain machinecomprehension of text.
In Proceedings of Em-pirical Methods in Natural Language Processing(EMNLP).
[Sachan et al2015] Mrinmaya Sachan, Avinava Dubey,Eric P Xing, and Matthew Richardson.
2015.Learning answer-entailing structures for machinecomprehension.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguis-tics.491[Smith et al2015] Ellery Smith, Nicola Greco, MatkoBosnjak, and Andreas Vlachos.
2015.
A strong lex-ical matching method for the machine comprehen-sion test.
In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 1693?1698.
[Srivastava and Hovy2013] Shashank Srivastava andDirk Hovy.
2013.
A walk-based semantically en-riched tree kernel over distributed word representa-tions.
In Proceedings of Empirical Methods in Nat-ural Language Processing, pages 1411?1416.
[Wang et al2015] Hai Wang, Mohit Bansal, KevinGimpel, and David A. McAllester.
2015.
Machinecomprehension with syntax, frames, and semantics.In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing of the Asian Federation of NaturalLanguage Processing, ACL 2015, July 26-31, 2015,Beijing, China, Volume 2: Short Papers, pages 700?706.
[Weston et al2015] Jason Weston, Antoine Bordes,Sumit Chopra, and Tomas Mikolov.
2015.
Towardsai-complete question answering: A set of prerequi-site toy tasks.
arXiv preprint arXiv:1502.05698.
[Yin et al2016] Wenpeng Yin, Sebastian Ebert, andHinrich Schtze.
2016.
Attention-based convolu-tional neural network for machine comprehension.arXiv preprint arXiv:1602.04341.
[Yuille and Rangarajan2003] A. L. Yuille and AnandRangarajan.
2003.
The concave-convex procedure.Neural Comput.492
