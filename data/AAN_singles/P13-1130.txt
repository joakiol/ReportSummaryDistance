Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1321?1330,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLanguage Acquisition and Probabilistic Models: keeping it simpleAline Villavicencio?, Marco Idiart?Robert Berwick?, Igor Malioutov?
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)?Institute of Physics, Federal University of Rio Grande do Sul (Brazil)?LIDS, Dept.
of EECS, Massachusetts Institute of Technology (USA)?
CSAIL, Dept.
of EECS, Massachusetts Institute of Technology (USA)avillavicencio@inf.ufrgs.br, marco.idiart@if.ufrgs.brberwick@csail.mit.edu, igorm@mit.eduAbstractHierarchical Bayesian Models (HBMs)have been used with some successto capture empirically observed pat-terns of under- and overgeneralizationin child language acquisition.
How-ever, as is well known, HBMs are?ideal?
learning systems, assuming ac-cess to unlimited computational re-sources that may not be availableto child language learners.
Conse-quently, it remains crucial to carefullyassess the use of HBMs along with al-ternative, possibly simpler, candidatemodels.
This paper presents suchan evaluation for a language acquisi-tion domain where explicit HBMs havebeen proposed: the acquisition of En-glish dative constructions.
In particu-lar, we present a detailed, empirically-grounded model-selection compari-son of HBMs vs. a simpler alternativebased on clustering along with max-imum likelihood estimation that wecall linear competition learning (LCL).Our results demonstrate that LCL canmatch HBM model performance with-out incurring on the high computa-tional costs associated with HBMs.1 IntroductionIn recent years, with advances in probabilityand estimation theory, there has been muchinterest in Bayesian models (BMs) (Chater,Tenenbaum, and Yuille, 2006; Jones andLove, 2011) and their application to child lan-guage acquisition with its challenging com-bination of structured information and in-complete knowledge, (Perfors, Tenenbaum,and Wonnacott, 2010; Hsu and Chater, 2010;Parisien, Fazly, and Stevenson, 2008; Parisienand Stevenson, 2010) as they offer several ad-vantages in this domain.
They can readilyhandle the evident noise and ambiguity of ac-quisition input, while at the same time pro-viding efficiency via priors that mirror knownpre-existing language biases.
Further, hierar-chical Bayesian Models (HBMs) can combinedistinct abstraction levels of linguistic knowl-edge, from variation at the level of individ-ual lexical items, to cross-item variation, usinghyper-parameters to capture observed pat-terns of both under- and over-generalizationas in the acquisition of e.g.
dative alterna-tions in English (Hsu and Chater, 2010; Per-fors, Tenenbaum, and Wonnacott, 2010), andverb frames in a controlled artificial language(Wonnacott, Newport, and Tanenhaus, 2008).HBMs can thus be viewed as providing a?rational?
upper bound on language learn-ability, yielding optimal models that accountfor observed data while minimizing any re-quired prior information.
In addition, theclustering implicit in HBM modeling intro-duces additional parameters that can be tunedto specific data patterns.
However, this comesat a well-known price: HBMs generally arealso ideal learning systems, known to becomputationally infeasible (Kwisthout, Ware-ham, and van Rooij, 2011).
Approximationsproposed to ensure computational tractabil-ity, like reducing the number of classes thatneed to be learned may also be linguisti-cally and cognitively implausible.
For in-stance, in terms of verb learning, this could1321take the form of reducing the number of sub-categorization frames to the relevant subset,as in (Perfors, Tenenbaum, and Wonnacott,2010), where only 2 frames are considered for?take?, when in fact it is listed in 6 framesby Levin (1993).
Finally, comparison of vari-ous Bayesian models of the same task is rare(Jones and Love, 2011) and Bayesian infer-ence generally can be demonstrated as sim-ply one class of regularization or smooth-ing techniques among many others; given theproblem at hand, there may well be other,equally compelling regularization methodsfor dealing with the bias-variance dilemma(e.g., SVMs (Shalizi, 2009)).
Consequently, therelevance of HBMs for cognitively accurate ac-counts of human learning remains uncertainand needs to be carefully assessed.Here we argue that the strengths of HBMsfor a given task must be evaluated in light oftheir computational and cognitive costs, andcompared to other viable alternatives.
The fo-cus should be on finding the simplest statis-tical models consistent with a given behav-ior, particularly one that aligns with knowncognitive limitations.
In the case of manylanguage acquisition tasks this behavior oftentakes the form of overgeneralization, but witheventual convergence to some target languagegiven exposure to more data.In particular, in this paper we consider howchildren acquire English dative verb construc-tions, comparing HBMs to a simpler alterna-tive, a linear competition learning (LCL) al-gorithm that models the behavior of a givenverb as the linear competition between the ev-idence for that verb, and the average behav-ior of verbs belonging to its same class.
Theresults show that combining simple cluster-ing methods along with ordinary maximumlikelihood estimation yields a result compara-ble to HBM performance, providing an alter-native account of the same facts, without thecomputational costs incurred by HBM modelsthat must rely, for example, on Markov ChainMonte Carlo (MCMC) methods for numeri-cally integrating complex likelihood integrals,or on Chinese Restaurant Process (CRP) forproducing partitions.In terms of Marr?s hierarchy (Marr, 1982)learning verb alternations is an abstract com-putational problem (Marr?s type I), solvableby many type II methods combining repre-sentations (models, viz.
HBMs or LCLs) withparticular algorithms.
The HBM conventionof adopting ideal learning amounts to invok-ing unbounded algorithmic resources, solv-ability in principle, even though in practicesuch methods, even approximate ones, areprovably NP-hard (cf.
(Kwisthout, Wareham,and van Rooij, 2011)).
Assuming cognitiveplausibility as a desideratum, we therefore ex-amine whether HBMs can also be approxi-mated by another type II method (LCLs) thatdoes not demand such intensive computa-tion.
Any algorithm that approximates anHBM can be viewed as implementing a some-what different underlying model; if it repli-cates HBM prediction performance but is sim-pler and less computationally complex thenwe assume it is preferable.This paper is organized as follows: we startwith a discussion of formalizations of lan-guage acquisition tasks, ?2.
We present ourexperimental framework for the dative acqui-sition task, formalizing a range of learningmodels from simple MLE methods to HBMtechniques, ?3, and a computational evalua-tion of each model, ?4.
We finish with conclu-sions and possibilities for future work, ?5.2 Evidence in Language AcquisitionA familiar problem for language acquisition ishow children learn which verbs participate inso-called dative alternations, exemplified bythe child-produced sentences 1 to 3, from theBrown (1973) corpus in CHILDES (MacWhin-ney, 1995).1. you took me three scrambled eggs (a direct object da-tive (DOD) from Adam at age 3;6)2.
Mommy can you fix dis for me ?
(a prepositional da-tive (PD) from Adam at age 4;7)3.
*Mommy, fix me my tiger (from Adam at age 5;2)Examples like these show that children gen-eralize their use of verbs.
For example, in sen-tence (1), the child Adam uses take as a DODbefore any recorded occurrence of a similaruse of take in adult speech to Adam.
Suchverbs alternate because they can also occurwith a prepositional form, as in sentence (2).However, sometimes a child?s use of verbs like1322these amounts to an overgeneralization ?
thatis, their productive use of a verb in a patternthat does not occur in the adult grammar, as insentence (3), above.
Faced with these two verbframes the task for the learner is to decide for aparticular verb if it is a non-alternating DODonly verb, a PD only verb, or an alternatingverb that allows both forms.This ambiguity raises an important learn-ability question, conventionally known asBaker?s paradox (Baker, 1979).
On the as-sumption that children only receive positiveexamples of verb forms, then it is not clearhow they might recover from the overgener-alization exhibited in sentence (3) above, be-cause they will never receive positive sen-tences from adults like (3), using fix in a DODform.
As has long been noted, if negative ex-amples were systematically available to learn-ers, then this problem would be solved, sincethe child would be given evidence that theDOD form is not possible in the adult gram-mar.
However, although parental correctioncould be considered to be a source of negativeevidence, it is neither systematic nor generallyavailable to all children (Marcus, 1993).
Evenwhen it does occur, all careful studies have in-dicated that it seems mostly concerned withsemantic appropriateness rather than syntax.In the cases where it is related to syntax, itis often difficult to determine what the cor-rection refers to in the utterance and besideschildren seem to be oblivious to the correction(Brown and Hanlon, 1970; Ingram, 1989).One alternative solution to Baker?s paradoxthat has been widely discussed at least sinceChomsky (1981) is the use of indirect negativeevidence.
On the indirect negative evidencemodel, if a verb is not found where it wouldbe expected to occur, the learner may con-clude it is not part of the adult grammar.
Cru-cially, the indirect evidence model is inher-ently statistical.
Different formalizations of in-direct negative evidence have been incorpo-rated in several computational learning mod-els for learning e.g.
grammars (Briscoe, 1997;Villavicencio, 2002; Kwiatkowski et al, 2010);dative verbs (Perfors, Tenenbaum, and Won-nacott, 2010; Hsu and Chater, 2010); and mul-tiword verbs (Nematzadeh, Fazly, and Steven-son, 2013).
Since a number of closely relatedmodels can all implement the indirect nega-tive evidence approach, the decision of whichone to choose for a given task may not be en-tirely clear.
In this paper we compare a rangeof statistical models consistent with a certainbehavior: early overgeneralization, with even-tual convergence to the correct target on thebasis of exposure to more data.3 Materials and Methods3.1 Dative CorporaTo emulate a child language acquisition en-vironment we use naturalistic longitudinalchild-directed data, from the Brown corpus inCHILDES, for one child (Adam) for a subsetof 19 verbs in the DOD and PD verb frames,figure 1.
This dataset was originally reportedin Perfors, Tenenbaum, and Wonnacott (2010),and longitudinal and incremental aspects toacquisition are approximated by dividing thedata available into 5 incremental epochs (E1 toE5 in the figures), where at the final epoch thelearner has seen the full corpus.Model comparison requires a gold standarddatabase for acquisition, reporting whichframes have been learned for which verbs ateach stage, and how likely a child is of mak-ing creative uses of a particular verb in a newframe.
An independent gold standard withdevelopmental information (e.g.
Gropen etal.
(1989)) would clearly be ideal.
Absentthis, a first step is demonstrating that sim-pler alternative models can replicate HBMperformance on their own terms.
Therefore,the gold standard we use for evaluation isthe classification predicted by Perfors, Tenen-baum, and Wonnacott (2010).
The evaluationsreported in our analysis take into account in-trinsic characteristics of each model in rela-tion to the likelihoods of the verbs, to deter-mine the extent to which the models go be-yond the data they were exposed to, discussedin section 2.
Further, since it has been ar-gued that very low frequency verbs may notyet be firmly placed in a child?s lexicon (Yang,2010; Gropen et al, 1989), at each epoch wealso impose a low-frequency threshold of 5occurrences, considering only verbs that thelearner has seen at least 5 times.
This use of alow-frequency threshold for learning has ex-tensive support in the literature for learning1323of all kinds in both human and non-humananimals, e.g.
(Gallistel, 2002).
A cut-off fre-quency in this range has also commonly beenused in NLP tasks like POS tagging (Ratna-parkhi, 1999).3.2 The learnersWe selected a set of representative statisticalmodels that are capable in principle of solv-ing this classification task, ranging from whatis perhaps the simplest possible, a simple bi-nomial, all the way to multi-level hierarchicalBayesian approaches.A Binomial distribution serves as the sim-plest model for capturing the behavior of averb occurring in either DOD or PD frame.Representing the probability of DOD as ?, af-ter n occurrences of the verb the probabilitythat y of them are DOD is:p( y| ?,n) =(ny)?y (1 ?
?
)n?y (1)Considering that p(y| ?,n) is the likelihoodin a Bayesian framework, the simplest and themost intuitive estimator of ?, given y in n verboccurrences, is the Maximum Likelihood Esti-mator (MLE):?MLE =yn (2)?MLE is viable as a learning model in the sensethat its accuracy increases as the amount of ev-idence for a verb grows (n ?
?
), reflectingthe incremental, on-line character of languagelearning.
However, one well known limita-tion of MLE is that it assigns zero probabilitymass to unseen events.
Ruling out events onthe grounds that they did not occur in a finitedata set early in learning may be too strong ?though it should be noted that this is simplyone (overly strong) version of the indirect neg-ative evidence position.Again as is familiar, to overcome zerocount problem, models adopt one or anothermethod of smoothing to assign a small prob-ability mass to unseen events.
In a Bayesianformulation, this amounts to assigning non-zero probability mass to some set of priors;smoothing also captures the notion of gener-alization, making predictions about data thathas never been seen by the learner.
In thecontext of verb learning smoothing could bebased on several principles:?
an (innate) expectation as to how verbs ingeneral should behave;?
an acquired class-based expectation ofthe behavior of a verb, based on its associ-ation to similar but more frequent verbs.The former can be readily implementedin terms of prior probability estimates.
Aswe discuss below, class-based estimates arisefrom one or another clustering method, andcan produce more accurate estimates for lessfrequent verbs based on patterns alreadylearned for more frequent verbs in the sameclass; see (Perfors, Tenenbaum, and Wonna-cott, 2010).
In this case, smoothing is a side-effect of the behavior of a class as a whole.When learning begins, the prior probabilityis the only source of information for a learnerand, as such, dominates the value of the poste-rior probability.
However, in the large samplelimit, it is the likelihood that dominates theposterior distribution regardless of the prior.In Hierarchical Bayesian Models both effectsare naturally incorporated.
The prior distri-bution is structured as a chain of distributionsof parameters and hyper-parameters, and thedata may be divided into classes that sharesome of the hyper-parameters, as defined be-low for the case of a three levels model:?
?
Exponential(1)?
?
Exponential(1)?k ?
Exponential(?
)?k ?
Beta(?, ?
)?ik ?
Beta(?k?k, ?k(1 ?
?k))yi|ni ?
Binomial(?ik)The indices refer to the possible hierarchiesamong the hyper-parameters.
?
and ?
are inthe top, and they are shared by all verbs.
Thenthere are classes of different ?k, ?k, and theprobabilities for the DOD frame for the dif-ferent verbs (?ik) are drawn according to theclasses k assigned to them.
An estimate for(?ik) for a given configuration of clusters isgiven by1324Figure 1: Verb tokens per epoch (E1 to E5)Figure 2: Verb tokens ?
5 per epoch (E1 to E5)where P(Y) is the evidence of the data,the unnormalized posterior for the hyper-parameters isand the likelihood for ?
and ?
isThe Hierarchical Bayesian Model predictionfor?i is the average of the estimate?ikHBM overall possible partitions of the verbs in the task.To simplify the notation we can write?HBM = E[ y + ?
?n + ?
](3)where in the expression E[.
.
. ]
are includedthe integrals described above and the averageof all possible class partitions.
Due to thiscomplexity, in practice even small data sets re-quire the use of MCMC methods, and statisti-cal models for partitions, like CRP (Gelman etal., 2003; Perfors, Tenenbaum, and Wonnacott,2010).
This complexity also calls into questionthe cognitive fidelity of such approaches.Eq.3 is particularly interesting because byfixing?
and ?
(instead of averaging over them)it is possible to deduce simpler (and classical)models: MLE corresponds to ?
= 0; the socalled ?add-one?
smoothing (referred in thispaper as L1) corresponds to ?
= 2 and ?
= 1/2.From Eq.3 it is also clear that if ?
and ?
(ortheir distributions) are unchanged, as the evi-dence of a verb grows (n??
), the HBM esti-mate approaches MLE?s, (?HBM ?
?MLE).
Onthe other hand, when ?
>> n, ?HBM ?
?, sothat ?
can be interpreted as a prior value for ?in the low frequency limit.Following this reasoning, we propose analternative approach, a linear competitionlearner (LCL), that explicitly models the be-havior of a given verb as the linear competi-tion between the evidence for the verb, andthe average behavior of verbs of the sameclass.
As clustering is defined independentlyfrom parameter estimation, the advantages ofthe proposed approach are twofold.
First, itis computationally much simpler, not requir-ing approximations by Monte Carlo meth-ods.
Second, differently from HBMs wherethe same attributes are used for clustering andparameter estimation (in this case the DODand PD counts for each verb), in LCL cluster-1325ing may be done using more general contextsthat employ a variety of linguistic and envi-ronmental attributes.For LCL the prior and class-based informa-tion are incorporated as:?LCL =yi + ?C?Cni + ?C (4)where ?C and ?C are defined via justifiableheuristic expressions dependent solely on thestatistics of the class attributed to each verb i.The strength of the prior (?C) is a mono-tonic function of the number of elements (mC)in the class C, excluding the target verb vi.To approximate the gold standard behavior ofthe HBM for this task (Perfors, Tenenbaum,and Wonnacott, 2010) we chose the followingfunction for ?C:?C = mC3/2(1 ?mC?1/5) + 0.1 (5)with the strength of the prior for the LCLmodel depending on the number of verbs inthe class, not on their frequency.
Eq.5 waschosen as a good fit to HBMs, without incur-ring their complexity.
The powers are simplefractions, not arbitrary numbers.
A best fitwas not attempted due to the lack of assess-ment of how accurate HBMs are on real data.The prior value (?C) is a smoothed estima-tion of the probability of DOD in a given class,combining the evidence for all verbs in thatclass:?C =YC + 1/2NC + 1 (6)in this case YC is the number of DOD occur-rences in the class, and NC the total numberof verb occurrences in the class, in both casesexcluding the target verb vi.The interpretation of these parameters isas follows: ?C is the estimate of ?
in the ab-sence of any data for a verb; and ?C controlsthe crossover between this estimate and MLE,with a large ?C requiring a larger sample (ni)to overcome the bias given by ?C.For comparative purposes, in this paper weexamine alternative models for (a) probabilityestimation and (b) clustering.
The models arethe following:?
two models without clusters: MLE andL1;?
two models where clusters are performedindependently: LCL and MLE??
; and?
the full HBM described before.MLE??
corresponds to replacing ?, ?
in eq.3by their maximal likelihood values calculatedfrom P({yi,ni}i?k|?, ?)
described before.For models without clustering, estimationis based solely on the observed behavior ofverbs.
With clustering, same-cluster verbsshare some parameters, influencing one an-other.
HBMs place distributions over pos-sible clusters, with estimation derived fromaverages over distributions.
In HBMs, clus-tering and probability estimation are calcu-lated jointly.
In the other models these twoestimates are calculated separately, permit-ting ?plug-and-play?
use of external cluster-ing methods, like X-means (Pelleg and Moore,2000)1.
However, to further assess the impactof cluster assignment on alternative modelperformance, we also used the clusters thatmaximize the evidence of the HBM for theDOD and PD counts of the target verbs, andwe refer to these as Maximum Evidence (ME)clusters.
In MWE clusters, verbs are separatedinto 3 classes: one if they have counts for bothframes; another for only the DOD frame; anda final for only the PD frame.4 EvaluationThe learning task consists of estimating theprobability that a given verb occurs in a partic-ular frame, using previous occurrences as thebasis for this estimation.
In this context, over-generalization can be viewed as the model?spredictions that a given verb seen only in oneframe (say, a PD) can also occur in the other(say, a DOD) as well, and it decreases as thelearner receives more data.
In one extremewe have MLE, which does not overgeneralize,and in the other the L1 model, which assignsuniform probability for all unseen cases.
Theother 3 models fall somewhere in between,overgeneralizing beyond the observed data,using the prior and class-based smoothing toassign some (low) probability mass to an un-seen verb-frame pair.
The relevant models?1Other clustering algorithms were also used; herewe report X-means results as representative of thesemodels.
X-means is available from http://www.cs.waikato.ac.nz/ml/weka/1326predictions for each of the target verbs in theDOD frame, given the full corpus, are in fig-ure 3.
In either end of the figure are the verbsthat were attested in only one of the frames(PD only at the left-hand end, and DOD onlyat the right-hand end).
For these verbs, LCLand HBM exhibit similar behavior.
When thelow-frequency threshold is applied, MLE?
?,HBM and LCL work equally well, figure 4.Figure 4: Probability of verbs in DOD frame,Low Frequency Threshold.To examine how overgeneralization pro-gresses during the course of learning as themodels were exposed to increasing amountsof data, we used the corpus divided by cumu-lative epochs, as described in ?3.1.
For eachepoch, verbs seen in only one of the frameswere divided in 5 frequency bins, and themodels were assessed as to how much over-generalization they displayed for each of theseverbs.
Following Perfors, Tenenbaum, andWonnacott (2010) overgeneralization is calcu-lated as the absolute difference between themodels predicted ?
and ?MLE, for each of theepochs, figure 5, and for comparative pur-poses their alternating/non-alternating clas-sification is also adopted.
For non-alternatingverbs, overgeneralization reflects the degreeof smoothing of each model.
As expected, themore frequent a verb is, the more confidentthe model is in the indirect negative evidenceit has for that verb, and the less it overgeneral-izes, shown in the lighter bars in all epochs.
Inaddition, the overall effect of larger amountsof data are indicated by a reduction in over-generalization epoch by epoch.
The effects ofclass-based smoothing can be assessed com-paring L1, a model without clustering whichdisplays a constant degree of overgeneraliza-tion regardless of the epoch, while HBM usesa distribution over clusters and the other mod-els X-means.
If a low-frequency threshold isapplied, the differences between the modelsdecrease significantly and so does the degreeof overgeneralization in the models?
predic-tions, as shown in the 3 lighter bars in the fig-ure.Figure 5: Overgeneralization, per epoch, perfrequency bin, where 0.5 corresponds to themaximum overgeneralization.While the models differ somewhat in theirpredictions, the quantitative differences needto be assessed more carefully.
To comparethe models and provide an overall differencemeasure, we use the predictions of the morecomplex model, HBM, as a baseline and thencalculate the difference between its predic-tions and those of the other models.
Weused three different measures for comparingmodels, one for their standard difference; onethat prioritizes agreement for high frequencyverbs; and one that focuses more on low fre-quency verbs.The first measure, denoted Difference, cap-tures a direct comparison between two mod-els, M1 and M2 as the average prediction dif-ference among the verbs, and is defined as:This measure treats all differences uniformly,regardless of whether they relate to high orlow frequency verbs in the learning sample(e.g.
for bring with 150 counts and serve withonly 1 have the same weight).
To focus on highfrequency verbs, we also define the WeightedDifference between two models as:Here we expect Dn < D since models tend to1327Figure 3: Probability of verbs in DOD frame.agree as the amount of evidence for each verbincreases.
Conversely, our third measure, de-noted Inverted, prioritizes the agreement be-tween two models on low frequency verbs, de-fined as follows:D1/n captures the degree of similarity in over-generalization between two models.
The re-sults of applying these three difference mea-sures are shown in figure 6 for the relevantmodels, where grey is for D(M1,M2), blackfor Dn(M1,M2) and white for D1/n(M1,M2).Given the probabilistic nature of Monte Carlomethods, there is also a variation between dif-ferent runs of the HBM model (HBM to HBM-2), and this indicates that models that per-form within these bounds can be consideredto be equivalent (e.g.
HBMs and ME-MLE?
?for Weighted Difference, and the HBMs andX-MLE??
for the Inverted Difference).Comparing the prediction agreement, thestrong influence of clustering is clear: themodels that have compatible clusters havesimilar performances.
For instance, all themodels that adopt the ME clusters for thedata perform closest to HBMs.
Moreover, theweighted differences tend to be smaller than0.01 and around 0.02 for the inverted differ-ences.
The results for these measures becomeeven closer in most cases when the low fre-quency threshold is adopted, figure 7, as theFigure 6: Model Comparisons.Figure 7: Model Comparison - Low FrequencyThreshold.0 5 10 15 20 25 30 35 40 45 500.50.60.70.80.91number of examplesDOD probabilityMLEL1HBMLCLMLEL1HBMLCLFigure 8: DOD probability evolution for mod-els with increase in evidenceevidence reduces the influence of the prior.To examine the decay of overgeneralizationwith the increase in evidence for these mod-els, two simulated scenarios are defined for asingle generic verb: one where the evidencefor DOD amounts to 75% of the data (dashedlines) and in the other to 100% (solid lines),figures 9 and 8.
Unsurprisingly, the perfor-mance of the models is dependent on theamount of evidence available.
This is a con-sequence of the decrease in the influence ofthe priors as the sample size increases in a rateof 1/N, as shown in figure 9 for the decreasein overgeneralization.
Ultimately it is the ev-1328100 101 10210?410?310?210?1100number of examplesovergeneralizationL1HBMLCLL1HBMLCLFigure 9: Overgeneralization reduction withincrease in evidenceidence that dominates the posterior probabil-ity.
Although the Bayesian model exhibits fastconvergence, after 10 examples, the simplermodel L1 is only approximately 3% below theBayesian model in performance for scenario 1and is still 90% accurate in scenario 2, figure 8.These results suggest that while these mod-els all differ slightly in the degree of overgen-eralization for low frequency data and noise,these differences are small, and as evidencereaches approximately 10 examples per verb,the overall performance for all models ap-proaches that of MLE.5 Conclusions and Future WorkHBMs have been successfully used for anumber of language acquisition tasks captur-ing both patterns of under- and overgeneral-ization found in child language acquisition.Their (hyper)parameters provide robustnessfor dealing with low frequency events, noise,and uncertainty and a good fit to the data,but this fidelity comes at the cost of complexcomputation.
Here we have examined HBMsagainst computationally simpler approachesto dative alternation acquisition, which imple-ment the indirect negative approach.
We alsoadvanced several measures for model com-parison in order to quantify their agreementto assist in the task of model selection.
The re-sults show that the proposed LCL model, inparticular, that combines class-based smooth-ing with maximum likelihood estimation, ob-tains results comparable to those of HBMs,in a much simpler framework.
Moreover,when a cognitively-viable frequency thresh-old is adopted, differences in the performanceof all models decrease, and quite rapidly ap-proach the performance of MLE.In this paper we used standard clusteringtechniques grounded solely on verb counts toenable comparison with previous work.
How-ever, a variety of additional linguistic and dis-tributional features could be used for cluster-ing verbs into more semantically motivatedclasses, using a larger number of frames andverbs.
This will be examined in future work.We also plan to investigate the use of cluster-ing methods more targeted to language tasks(Sun and Korhonen, 2009).AcknowledgementsWe would like to thank the support ofprojects CAPES/COFECUB 707/11, CNPq482520/2012-4, 478222/2011-4, 312184/2012-3, 551964/2011-1 and 312077/2012-2.
We alsowant to thank Amy Perfors for kindly sharingthe input data.ReferencesBaker, Carl L. 1979.
Syntactic Theory and the Pro-jection Problem.
Linguistic Inquiry, 10(4):533?581.Briscoe, Ted.
1997.
Co-evolution of language andthe language acquisition device.
In Proceedingsof the 35th Annual Meeting of the Association forComputational Linguistics (ACL), pages 418?427.Morgan Kaufmann.Brown, Roger.
1973.
A first language: Ehe earlystages.
Harvard University Press, Cambridge,Massachusetts.Brown, Roger and Camille Hanlon.
1970.
Deriva-tional complexity and the order of acquisition ofchild?s speech.
In J. Hays, editor, Cognition andthe Development of Language.
NY: John Wiley.Chater, Nick, Joshua B. Tenenbaum, and AlanYuille.
2006.
Probabilistic models of cogni-tion: where next?
Trends in Cognitive Sciences,10(7):292 ?
293.Chomsky, Noam.
1981.
Lectures on government andbinding.
Mouton de Gruyter.1329Gallistel, Charles R. 2002.
Frequency, contin-gency, and the information processing theory ofconditioning.
In P.Sedlmeier and T. Betsch, ed-itors, Frequency processing and cognition.
OxfordUniversity Press, pages 153?171.Gelman, Andrew, John B. Carlin, Hal S. Stern, andDonald B. Rubin.
2003.
Bayesian Data Analy-sis, Second Edition (Chapman & Hall/CRC Texts inStatistical Science).
Chapman and Hall/CRC, 2edition.Gropen, Jess, Steve Pinker, Michael Hollander,Richard Goldberg, and Ronald Wilson.
1989.The learnability and acquisition of the dative al-ternation in English.
Language, 65(2):203?257.Hsu, Anne S. and Nick Chater.
2010.
The logi-cal problem of language acquisition: A proba-bilistic perspective.
Cognitive Science, 34(6):972?1016.Ingram, David.
1989.
First Language Acquisition:Method, Description and Explanation.
CambridgeUniversity Press.Jones, Matt and Bradley C. Love.
2011.
BayesianFundamentalism or Enlightenment?
On the ex-planatory status and theoretical contributionsof Bayesian models of cognition.
Behavioral andBrain Sciences, 34(04):169?188.Kwiatkowski, Tom, Luke Zettlemoyer, SharonGoldwater, and Mark Steedman.
2010.
Induc-ing probabilistic CCG grammars from logicalform with higher-order unification.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, pages 1223?1233.Kwisthout, Johan, Todd Wareham, and Iris vanRooij.
2011.
Bayesian intractability is not anailment that approximation can cure.
CognitiveScience, 35(5):779?1007.Levin, B.
1993.
English Verb Classes and Alterna-tions: A Preliminary Investigation.
University ofChicago Press, Chicago, IL.MacWhinney, Brian.
1995.
The CHILDES project:tools for analyzing talk.
Hillsdale, NJ: LawrenceErlbaum Associates, second edition.Marcus, Gary F. 1993.
Negative evidence in lan-guage acquisition.
Cognition, 46:53?85.Marr, D. 1982.
Vision.
San Francisco, CA: W. H.Freeman.Nematzadeh, Aida, Afsaneh Fazly, and SuzanneStevenson.
2013.
Child acquisition of multi-word verbs: A computational investigation.
InA.
Villavicencio, T. Poibeau, A. Korhonen, andA.
Alishahi, editors, Cognitive Aspects of Com-putational Language Acquisition.
Springer, pages235?256.Parisien, Christopher, Afsaneh Fazly, and SuzanneStevenson.
2008.
An incremental bayesianmodel for learning syntactic categories.
In Pro-ceedings of the Twelfth Conference on ComputationalNatural Language Learning, CoNLL ?08, pages89?96, Stroudsburg, PA, USA.
Association forComputational Linguistics.Parisien, Christopher and Suzanne Stevenson.2010.
Learning verb alternations in a usage-based bayesian model.
In Proceedings of the 32ndAnnual Conference of the Cognitive Science Society.Pelleg, Dan and Andrew Moore.
2000.
X-means:Extending k-means with efficient estimation ofthe number of clusters.
In Proceedings of theSeventeenth International Conference on MachineLearning, pages 727?734, San Francisco.
MorganKaufmann.Perfors, Amy, Joshua B. Tenenbaum, and Eliz-abeth Wonnacott.
2010.
Variability, nega-tive evidence, and the acquisition of verb argu-ment constructions.
Journal of Child Language,(37):607?642.Ratnaparkhi, Adwait.
1999.
Learning to parse nat-ural language with maximum entropy models.Machine Learning, pages 151?175.Shalizi, Cosma R. 2009.
Dynamics of bayesianupdating with dependent data and misspeci-fied models.
ElectroCosmanic Journal of Statistics,3:1039?1074.Sun, Lin and Anna Korhonen.
2009.
Improvingverb clustering with automatically acquired se-lectional preferences.
In EMNLP, pages 638?647.Villavicencio, Aline.
2002.
The Acquisition of aUnification-Based Generalised Categorial Grammar.Ph.D.
thesis, Computer Laboratory, Universityof Cambridge.Wonnacott, Elizabeth, Elissa L. Newport, andMichael K. Tanenhaus.
2008.
Acquiring andprocessing verb argument structure: Distribu-tional learning in a miniature language.
Cogni-tive Psychology, 56:165?209.Yang, Charles.
2010.
Three factors in languagevariation.
Lingua, 120:1160?1177.1330
