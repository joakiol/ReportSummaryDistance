Proceedings of the Third Workshop on Statistical Machine Translation, pages 155?158,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsKernel Regression Framework for Machine Translation: UCL SystemDescription for WMT 2008 Shared Translation TaskZhuoran WangUniversity College LondonDept.
of Computer ScienceGower Street, London, WC1E 6BTUnited Kingdomz.wang@cs.ucl.ac.ukJohn Shawe-TaylorUniversity College LondonDept.
of Computer ScienceGower Street, London, WC1E 6BTUnited Kingdomjst@cs.ucl.ac.ukAbstractThe novel kernel regression model for SMTonly demonstrated encouraging results onsmall-scale toy data sets in previous works dueto the complexities of kernel methods.
It isthe first time results based on the real-worlddata from the shared translation task will bereported at ACL 2008 Workshop on Statisti-cal Machine Translation.
This paper presentsthe key modules of our system, including thekernel ridge regression model, retrieval-basedsparse approximation, the decoding algorithm,as well as language modeling issues under thisframework.1 IntroductionThis paper follows the work in (Wang et al, 2007;Wang and Shawe-Taylor, 2008) which applied thekernel regression method with high-dimensionaloutputs proposed originally in (Cortes et al, 2005)to statistical machine translation (SMT) tasks.
In ourapproach, the machine translation problem is viewedas a string-to-string mapping, where both the sourceand the target strings are embedded into their re-spective kernel induced feature spaces.
Then ker-nel ridge regression is employed to learn the map-ping from the input feature space to the output one.As a kernel method, this model offers the potentialadvantages of capturing very high-dimensional cor-respondences among the features of the source andtarget languages as well as easy integration of ad-ditional linguistic knowledge via selecting particu-lar kernels.
However, unlike the sequence labelingtasks such as optical character recognition in (Corteset al, 2005), the complexity of the SMT problem it-self together with the computational complexities ofkernel methods significantly complicate the imple-mentation of the regression technique in this field.Our system is actually designed as a hybrid ofthe classic phrase-based SMT model (Koehn et al,2003) and the kernel regression model as follows:First, for each source sentence a small relevant set ofsentence pairs are retrieved from the large-scale par-allel corpus.
Then, the regression model is trainedon this small relevant set only as a sparse approx-imation of the regression hyperplane trained on theentire training set, as proposed in (Wang and Shawe-Taylor, 2008).
Finally, a beam search algorithm isutilized to decode the target sentence from the verynoisy output feature vector we predicted, with thesupport of a pre-trained phrase table to generate pos-sible hypotheses (candidate translations).
In addi-tion, a language model trained on a monolingual cor-pus can be integrated either directly into the regres-sion model or during the decoding procedure as anextra scoring function.Before describing each key component of our sys-tem in detail, we give a block diagram overview inFigure 1.2 Problem FormulationConcretely, the machine translation problem in ourmethod is formulated as follows.
If we define a fea-ture space Hx of our source language X , and definethe mapping ?
: X ?
Hx, then a sentence x ?
Xcan be expressed by its feature vector ?
(x) ?
Hx.The definition of the feature space Hy of our targetlanguage Y can be made in a similar way, with cor-155AlignmentParallelCorpusRetrieverPhrase TablePhraseExtractionKernelRegression DecoderMonolingualCorpusLanguageModelingN-gramModelTarget TextRelevant SetSource TextFigure 1: System overview.
The processes in gray blocksare pre-performed for the whole system, while the whiteblocks are online processes for each input sentence.
Thetwo dash-line arrows represent two possible ways of lan-guage model integration in our system described in Sec-tion 6.responding mapping ?
: Y ?
Hy.
Now in the ma-chine translation task, we are trying to seek a matrixrepresented linear operator W, such that:?
(y) = W?
(x) (1)to predict the translation y for an arbitrary sourcesentence x.3 Kernel Ridge RegressionBased on a set of training samples, i.e.
bilingualsentence pairs S = {(xi,yi) : xi ?
X ,yi ?
Y, i =1, .
.
.
,m.}, we use ridge regression to learn the Win Equation (1), as:min ?WM?
?
M?
?2F + ?
?W?2F (2)where M?
= [?
(x1), ...,?
(xm)], M?
=[?
(y1), ...,?
(ym)], ?
?
?F denotes the Frobeniusnorm that is a matrix norm defined as the square rootof the sum of the absolute squares of the elements inthat matrix, and ?
is a regularization coefficient.Differentiating the expression and setting it tozero gives the explicit solution of the ridge regres-sion problem:W = M?(K?
+ ?I)?1M??
(3)where I is the identity matrix, and K?
=M??M?
= (??(xi,xj)1?i,j?m).
Note here, we usethe kernel function:??
(xi,xj) = ??(xi),?(xj)?
= ?(xi)??
(xj) (4)to denote the inner product between two feature vec-tors.
If the feature spaces are properly defined, the?kernel trick?
will allow us to avoid dealing withthe very high-dimensional feature vectors explicitly(Shawe-Taylor and Cristianini, 2004).Inserting Equation (3) into Equation (1), we ob-tain our prediction as:?
(y) = M?(K?
+ ?I)?1k?
(x) (5)where k?
(x) = (??
(x,xi)1?i?m) is an m ?
1 col-umn matrix.
Note here, we will use the exact matrixinversion instead of iterative approximations.3.1 N -gram String KernelIn the practical learning and prediction processes,only the inner products of feature vectors are re-quired, which can be computed with the kernel func-tion implicitly without evaluating the explicit coor-dinates of points in the feature spaces.
Here, we de-fine our features of a sentence as its word n-gramcounts, so that a blended n-gram string kernel canbe used.
That is, if we denote by xi:j a substringof sentence x starting with the ith word and endingwith the jth, then for two sentences x and z, theblended n-gram string kernel is computed as:?
(x, z) =n?p=1|x|?p+1?i=1|z|?p+1?j=1[[xi:i+p?1 = zj:j+p?1]](6)Here, | ?
| denotes the length of the sentence, and[[?]]
is the indicator function for the predicate.
In oursystem, the blended tri-gram kernel is used, whichmeans we count the n-grams of length up to 3.4 Retrieval-based Sparse ApproximationFor SMT, we are not able to use the entire trainingset that contains millions of sentences to train ourregression model.
Fortunately, it is not necessary ei-ther.
Wang and Shawe-Taylor (2008) suggested thata small set of sentences whose source is relevant tothe input can be retrieved, and the regression modelcan be trained on this small-scale relevant set only.156Src n?
y a-t-il pas ici deux poids , deux mesuresRlv pourquoi y a-t-il deux poids , deux mesurespourquoi deux poids et deux mesurespeut-e?tre n?
y a-t-il pas d?
e?pide?mie nonpluspourquoi n?
y a-t-il pas urgencecette directive doit exister d?
ici deux moisTable 1: A sample input (Src) and some of the retrievedrelevant examples (Rlv).In our system, we take each sentence as a docu-ment and use the tf-idf metric that is frequently usedin information retrieval tasks to retrieve the relevantset.
Preliminary experiments show that the size ofthe relevant set should be properly controlled, as ifmany sentences that are not very close to the sourcetext are involved, they will correspond to addingnoise.
Hence, we use a threshold of the tf-idf scoreto filter the relevant set.
On average, around 1500sentence pairs are extracted for each source sen-tence.
Table 1 shows a sample input and some ofits top relevant sentences retrieved.5 DecodingAfter the regression, we have a prediction of thetarget feature vector as in Equation (1).
To ob-tain the target sentence, a decoding algorithm is stillrequired to solve the pre-image problem.
This isachieved in our system by seeking the sentence y?whose feature vector has the minimum Euclideandistance to the prediction, as:y?
= arg miny?Y(x)?W?
(x) ?
?(y)?
(7)where Y(x) ?
Y denotes a finite set covering allpotential translations for the given source sentencex.
To obtain a smaller search space and more re-liable translations, Y(x) is generated with the sup-port of a phrase table extracted from the whole train-ing set.
Then a modified beam search algorithmis employed, in which we restricted the distortionof the phrases by only allowing adjacent phrases toexchange their positions, and rank the search statesin the beams according to Equation (7) but applieddirectly to the partial translations and their corre-sponding source parts.
A more detailed explanationof the decoding algorithm can be found in (Wanget al, 2007).
In addition, Wang and Shawe-Taylor(2008) further showed that the search error rate ofthis algorithm is acceptable.6 Language Model IntegrationIn previous works (Wang et al, 2007; Wang andShawe-Taylor, 2008), there was no language modelutilized in the regression framework for SMT, assimilar function can be achieved by the correspon-dences among the n-gram features.
It was demon-strated to work well on small-scale toy data, how-ever, real-world data are much more sparse andnoisy, where a language model will help signifi-cantly.There are two ways to integrate a language modelin our framework.
First, the most straightforward so-lution is to add a weight to adjust the strength of theregression based translation scores and the languagemodel score during the decoding procedure.
Alter-natively, as language model is n-gram-based whichmatches the definition of our feature space, we canadd a langauge model loss to the objective functionof our regression model as follows.
We define ourlanguage score for a target sentence y as:LM(y) = V??
(y) (8)where V is a vector whose components Vy?
?y?y willtypically be log-probabilities logP (y|y??y?
), and y,y?
and y??
are arbitrary words.
Note here, in or-der to match our blended tri-gram induced featurespace, we can make V of the same dimension as?
(y), while zero the components corresponding touni-grams and bi-grams.
Then the regression prob-lem can be defined as:min ?WM??M?
?2F +?1?W?2F ?
?2V?WM?1(9)where ?2 is a coefficient balancing between the pre-diction being close to the target feature vector andbeing a fluent target sentence, and 1 denotes a vec-tor with components 1.
By differentiating the ex-pression with respect to W and setting the result tozero, we can obtain the explicit solution as:W = (M?
+ ?2V1?)(K?
+ ?1I)?1M??
(10)7 Experimental ResultsPreliminary experiments are carried out on theFrench-English portion of the Europarl corpus.
We157System BLEU (%) NIST METEOR (%) TER (%) WER (%) PER (%)Kernel Regression 26.59 7.00 52.63 55.98 60.52 43.20Moses 31.15 7.48 56.80 55.14 59.85 42.79Table 3: Evaluations based on different metrics with comparison to Moses.train our regression model on the training set, andtest the effects of different language models on thedevelopment set (test2007).
The results evaluatedby BLEU score (Papineni et al, 2002) is shown inTable 2.It can be found that integrating the languagemodel into the regression framework works slightlybetter than just using it as an additional score com-ponent during decoding.
But language models ofhigher-order than the n-gram kernel cannot be for-mulated to the regression problem, which would bea drawback of our system.
Furthermore, the BLEUscore performance suggests that our model is notvery powerful, but some interesting hints can befound in Table 3 when we compare our method witha 5-gram language model to a state-of-the-art systemMoses (Koehn and Hoang, 2007) based on variousevaluation metrics, including BLEU score, NISTscore (Doddington, 2002), METEOR (Banerjee andLavie, 2005), TER (Snover et al, 2006), WER andPER.
It is shown that our system?s TER, WER andPER scores are very close to Moses, though thegaps in BLEU, NIST and METEOR are significant,which suggests that we would be able to produce ac-curate translations but might not be good at makingfluent sentences.8 ConclusionThis work is a novel attempt to apply the advancedkernel method to SMT tasks.
The contribution at thisstage is still preliminary.
When applied to real-worlddata, this approach is not as powerful as the state-of-the-art phrase-based log-linear model.
However, in-teresting prospects can be expected from the sharedtranslation task.AcknowledgementsThis work is supported by the European Commis-sion under the IST Project SMART (FP6-033917).no-LM LM13gram LM23gram LM15gramBLEU 23.27 25.19 25.66 26.59Table 2: BLEU score performance of different languagemodels.
LM1 denotes adding the language model dur-ing decoding process, while LM2 represents integratingthe language model into the regression framework as de-scribed in Problem (9).ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization, pages 65?72.Corinna Cortes, Mehryar Mohri, and Jason Weston.2005.
A general regression technique for learningtransductions.
In Proc.
of ICML?05.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proc.
of HLT?02, pages 138?145.Philipp Koehn and Hieu Hoang.
2007.
Factored transla-tion models.
In Proc.
of EMNLP-CoNLL?07.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HAACL-HLT?03, pages 48?54.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic evalu-ation of machine translation.
In Proc.
of ACL?02.John Shawe-Taylor and Nello Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge UniversityPress.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
of AMTA?06.Zhuoran Wang and John Shawe-Taylor.
2008.
Kernel-based machine translation.
In Cyril Goutte, NicolaCancedda, Marc Dymetman, and George Foster, edi-tors, Learning Machine Translation.
MIT Press, to ap-pear.Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-mak.
2007.
Kernel regression based machine transla-tion.
In Proc.
of NAACL-HLT?07, Short Paper Volume,pages 185?188.158
