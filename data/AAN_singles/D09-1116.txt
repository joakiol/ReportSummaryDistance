Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1114?1123,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPA Joint Language Model With Fine-grain Syntactic TagsDenis Filimonov11Laboratory for Computational Linguisticsand Information ProcessingInstitute for Advanced Computer StudiesUniversity of Maryland, College Parkden@cs.umd.eduMary Harper1,22Human Language TechnologyCenter of ExcellenceJohns Hopkins Universitymharper@umiacs.umd.eduAbstractWe present a scalable joint languagemodel designed to utilize fine-grain syn-tactic tags.
We discuss challenges sucha design faces and describe our solutionsthat scale well to large tagsets and cor-pora.
We advocate the use of relativelysimple tags that do not require deep lin-guistic knowledge of the language but pro-vide more structural information than POStags and can be derived from automati-cally generated parse trees ?
a combina-tion of properties that allows easy adop-tion of this model for new languages.
Wepropose two fine-grain tagsets and evalu-ate our model using these tags, as well asPOS tags and SuperARV tags in a speechrecognition task and discuss future direc-tions.1 IntroductionIn a number of language processing tasks, particu-larly automatic speech recognition (ASR) and ma-chine translation (MT), there is the problem of se-lecting the best sequence of words from multiplehypotheses.
This problem stems from the noisychannel approach to these applications.
The noisychannel model states that the observed data, e.g.,the acoustic signal, is the result of some inputtranslated by some unknown stochastic process.Then the problem of finding the best sequence ofwords given the acoustic input, not approachabledirectly, is transformed into two separate models:argmaxwn1p(wn1|A) = argmaxwn1p(A|wn1) ?
p(wn1)(1)where A is the acoustic signal and wn1is a se-quence of n words.
p(A|wn1) is called an acousticmodel and p(wn1) is the language model1.Typically, these applications use language mod-els that compute the probability of a sequence in agenerative way:p(wn1) =n?i=1p(wi|wi?11)Approximation is required to keep the parameterspace tractable.
Most commonly the context is re-duced to just a few immediately preceding words.This type of model is called an ngram model:p(wi|wi?11) ?
p(wi|wi?1i?n+1)Even with limited context, the parameter space canbe quite sparse and requires sophisticated tech-niques for reliable probability estimation (Chenand Goodman, 1996).
While the ngram modelsperform fairly well, they are only capable of cap-turing very shallow knowledge of the language.There is extensive literature on a variety ofmethods that have been used to imbue modelswith syntactic and semantic information in differ-ent ways.
These methods can be broadly catego-rized into two types:?
The first method uses surface words withinits context, sometimes organizing them intodeterministic classes.
Models of this type in-clude: (Brown et al, 1992; Zitouni, 2007),which use semantic word clustering, and(Bahl et al, 1990), which uses variable-length context.?
The other method adds stochastic variablesto express the ambiguous nature of surfacewords2.
To obtain the probability of the next1Real applications use argmaxwn1p(A|wn1)?p(wn1)?
?n?instead of Eq.
1, where ?
and ?
are set to optimize a heldoutset.2These variables have to be predicted by the model.1114word we need to sum over all assignments ofthe stochastic variables, as in Eq.
2.p(wi|wi?11) =?t1...tip(witi|wi?11ti?11) (2)=?t1...tip(witi|wi?11ti?11)p(wi?11ti?11)?t1...ti?1p(wi?11ti?11)Models of this type, which we call jointmodels since they essentially predict jointevents of words and some random vari-able(s), include (Chelba and Jelinek, 2000)which used POS tags in combination with?parser instructions?
for constructing a fullparse tree in a left-to-right manner; (Wanget al, 2003) used SuperARVs (complex tu-ples of dependency information) without re-solving the dependencies, thus called almostparsing; (Niesler and Woodland, 1996; Hee-man, 1999) utilize part of speech (POS) tags.Note that some models reduce the context bymaking the following approximation:p(witi|wi?11ti?11) ?
p(wi|ti)?p(ti|ti?11) (3)thus, transforming the problem into a stan-dard HMM application.
However, thesemodels perform poorly and have only beenable to improve over the ngram model wheninterpolated with it (Niesler and Woodland,1996).Although joint models have the potentialto better express variability in word usagethrough the introduction of additional latentvariables, they do not necessarily performbetter because the increased dimensionalityof the context substantially increases the al-ready complex problem of parameter estima-tion.
The complexity of the space also makescomputation of the probability a challengebecause of space and time constraints.
Thismakes the choice of the random variables amatter of utmost importance.The model presented in this paper has some el-ements borrowed from prior work, notably (Hee-man, 1999; Xu and Jelinek, 2004), while othersare novel.1.1 Paper OutlineThe message we aim to deliver in this paper canbe summarized in two theses:?
Use fine-grain syntactic tags in a joint LM.We propose a joint language model that canbe used with a variety of tagsets.
In Section2, we describe those that we used in our ex-periments.
Rather than tailoring our model tothese tagsets, we aim for flexibility and pro-pose an information theoretic framework forquick evaluation for tagsets, thus simplifyingthe creation of new tagsets.
We show thatour model with fine-grain tagsets outperformthe coarser POS model, as well as the ngrambaseline, in Section 5.?
Address the challenges that arise in a jointlanguage model with fine-grain tags.
Whilethe idea of using joint language modeling isnot novel (Chelba and Jelinek, 2000; Hee-man, 1999), nor is the idea of using fine-graintags (Bangalore, 1996; Wang et al, 2003),none of prior papers focus on the issues thatarise from the combination of joint languagemodeling with fine-grain tags, both in termsof reliable parameter estimation and scalabil-ity in the face of the increased computationalcomplexity.
We dedicate Sections 3 and 4 tothis problem.In Section 6, we summarize conclusions and layout directions for future work.2 Structural InformationAs we have mentioned, the selection of the ran-dom variable in Eq.
2 is extremely important forthe performance of the model.
On one hand, wewould like for this variable to provide maximuminformation.
On the other hand, as the number ofparameters grow, we must address reliable param-eter estimation in the face of sparsity, as well asincreased computational complexity.
In the fol-lowing section we will compare the use of Super-ARVs, POS tags, and other structural tags derivedfrom parse trees.2.1 POS TagsPart-of-speech tags can be easily obtained forunannotated data using off-the-shelf POS taggersor PCFG parsers.
However, the amount of infor-mation these tags typically provide is very limited,1115Figure 1: A parse tree examplee.g., while it is helpful to know whether fly is averb or a noun, knowing that you is a personal pro-noun does not carry the information whether it isa subject or an object (given the Penn Tree Banktagset), which would certainly help to predict thefollowing word.2.2 SuperARVThe SuperARV essentially organizes informationconcerning one consistent set of dependency linksfor a word that can be directly derived from itssyntactic parse.
SuperARVs encode lexical in-formation as well as syntactic and semantic con-straints in a uniform representation that is muchmore fine-grained than POS.
It is a four-tuple(C;F ;R+;D), where C is the lexical categoryof the word, F is a vector of lexical features forthe word, R+ is a set of governor and need labelsthat indicate the function of the word in the sen-tence and the types of words it needs, and D rep-resents the relative position of the word and its de-pendents.
We refer the reader to the literature forfurther details on SuperARVs (Wang and Harper,2002; Wang et al, 2003).SuperARVs can be produced from parse treesby applying deterministic rules.
In this work weuse SuperARVs as individual tags and do not clus-ter them based of their structure.
While Super-ARVs are very attractive for language modeling,developing such a rich set of annotations for a newlanguage would require a large amount of humaneffort.We propose two other types of tags which havenot been applied to this task, although similar in-formation has been used in parsing.2.3 Modifee TagThis tag is a combination of the word?s POStag and the POS tag of its governor role.
Wedesigned it to resemble dependency parse struc-ture.
For example, the sentence in Figure 1 wouldbe tagged: the/DT-NN black/JJ-NN cat/NN-VBDsat/VBD-root.
Henceforth, we will refer to thiskind of tag as head.2.4 Parent ConstituentThis tag is a combination of the word?s POS tagwith its immediate parent in the parse tree, alongwith the POS tag?s relative position among its sib-lings.
We refer to this type of tags as parent.
Theexample in Figure 1 will be tagged: the/DT-NP-start black/JJ-NP-mid cat/NN-NP-end sat/VB-VP-single.
This tagset is designed to represent con-stituency information.Note that the head and parent tagsets are morelanguage-independent (all they require is a tree-bank) than the SuperARVs which, not only uti-lized the treebank, but were explicitly designed bya linguist for English only.2.5 Information Theoretic Comparison ofTagsAs we have mentioned in Section 1, the choice ofthe tagset is very important to the performance ofthe model.
There are two conflicting intuitions fortags: on one hand they should be specific enoughto be helpful in the language model?s task; on theother hand, they should be easy for the LM to pre-dict.Of course, in order to argue which tags are moresuitable, we need some quantifiable metrics.
Wepropose an information theoretic approach:?
To quantify how hard it is to predict a tag, wecompute the conditional entropy:Hp(ti|wi) = Hp(tiwi)?Hp(wi)=?witip(tiwi) log p(ti|wi)?
To measure how helpful a tagset is in the LMtask, we compute the reduction of the condi-tional cross entropy:Hp?,q(wi|wi?1ti?1) ?Hp?,q(wi|wi?1) =??wii?1ti?1p?
(wii?1ti?1) log q(wi|wi?1ti?1)+?wii?1p?
(wii?1) log q(wi|wi?1)= ??wii?1ti?1p?
(wii?1ti?1) logq(wi|wi?1ti?1)q(wi|wi?1)1116Note that in this case we use conditionalcross entropy because conditional entropyhas the tendency to overfit the data as we se-lect more and more fine-grain tags.
Indeed,Hp(wi|wi?1ti?1) can be reduced to zero ifthe tags are specific enough, which wouldnever happen in reality.
This is not a prob-lem for the former metric because the con-text there, wi, is fixed.
For this metric, weuse a smoothed distribution p?
computed onthe training set3 and the test distribution q.Bits00.511.522.53TagsPOS SuperARV parent headFigure 2: Changes in entropy for different tagsetsThe results of these measurements are presentedin Figure 2.
POS tags, albeit easy to predict, pro-vide very little additional information about thefollowing word, and therefore we would not ex-pect them to perform very well.
The parent tagsetseems to perform somewhat better than Super-ARVs ?
it provides 0.13 bits more informationwhile being only 0.09 bits harder to predict basedon the word.
The head tagset is interesting: it pro-vides 0.2 bits more information about the follow-ing word (which would correspond to 15% per-plexity reduction if we had perfect tags), but onthe other hand the model is less likely to predictthese tags accurately.This approach is only a crude estimate (it usesonly unigram and bigram context) but it is veryuseful for designing tagsets, e.g., for a new lan-guage, because it allows us to assess relative per-formance of tagsets without having to train a fullmodel.3We used one-count smoothing (Chen and Goodman,1996).3 Language Model StructureThe size and sparsity of the parameter space of thejoint model necessitate the use of dimensionalityreduction measures in order to make the modelcomputationally tractable and to allow for accu-rate estimation of the model?s parameters.
We alsowant the model to be able to easily accommodateadditional sources of information such as morpho-logical features, prosody, etc.
In the rest of thissection, we discuss avenues we have taken to ad-dress these problems.3.1 Decision Tree ClusteringBinary decision tree clustering has been shown tobe effective for reducing the parameter space inlanguage modeling (Bahl et al, 1990; Heeman,1999) and other language processing applications,e.g., (Magerman, 1994).
Like any clustering algo-rithm, it can be represented by a function H thatmaps the space of histories to a set of equivalenceclasses.p(witi|wi?1i?n+1ti?1i?n+1) ?
p(witi|H(wi?1i?n+1ti?1i?n+1))(4)While the tree construction algorithm is fairlystandard ?
to recursively select binary questionsabout the history optimizing some function ?
thereare important decisions to make in terms of whichquestions to ask and which function to optimize.In the remainder of this section, we discuss the de-cisions we made regarding these issues.3.2 FactorsThe Factored Language Model (FLM) (Bilmesand Kirchhoff, 2003) offers a convenient view ofthe input data: it represents every word in a sen-tence as a tuple of factors.
This allows us to extendthe language model with additional parameters.
Inan FLM, however, all factors have to be determin-istically computed in a joint model; whereas, weneed to distinguish between the factors that aregiven or computed and the factors that the modelmust predict stochastically.
We call these typesof factors overt and hidden, respectively.
Exam-ples of overt factors include surface words, mor-phological features such as suffixes, case informa-tion when available, etc., and the hidden factorsare POS, SuperARVs, or other tags.Henceforth, we will use word to represent theset of overt factors and tag to represent the set ofhidden factors.11173.3 Hidden Factors TreeSimilarly to (Heeman, 1999), we construct a bi-nary tree where each tag is a leaf; we will referto this tree as the Hidden Factors Tree (HFT).
Weuse Minimum Discriminative Information (MDI)algorithm (Zitouni, 2007) to build the tree.
TheHFT represents a hierarchical clustering of the tagspace.
One of the reasons for doing this is to allowquestions about subsets of tags rather than individ-ual tags alone4.Unlike (Heeman, 1999), where the tree of tagswas only used to create questions, this representa-tion of the tag space is, in addition, a key featureof our decoding optimizations, which we discussin Section 4.3.4 QuestionsThe context space is partitioned by means of bi-nary questions.
We use different types of ques-tions for hidden and overt factors.?
Questions about surface words are con-structed using the Exchange algorithm (Mar-tin et al, 1998).
This algorithm takes the setof words that appear at a certain position inthe training data associated with the currentnode in the history tree and divides the setinto two complementary subsets greedily op-timizing some target function (we use the av-erage entropy of the marginalized word dis-tribution, the same as for question selection).Note that since the algorithm only operateson the words that appear in the training data,we need to do something more to account forthe unseen words.
Thus, to represent this typeof question, we create the history tree struc-ture depicted in Fig.
4.For other overt factors with smaller vocabu-laries, such as suffixes, we use equality ques-tions.?
As we mentioned in Section 3.3, we use theHidden Factors Tree to create questions abouthidden factors.
Note that every node in a bi-nary tree can be represented by a binary pathfrom the root with all nodes under an innernode sharing the same prefix.
Thus, a ques-tion about whether a tag belongs to a subset4Trying all possible subsets of tags is not feasible sincethere are 2|T | of them.
The tree allows us to reduce the num-ber to O(T ) of the most meaningful (as per the clusteringalgorithm) subsets.Figure 3: Recursive smoothing: p?n= ?npn+(1?
?n)p?n?of tags dominated by a node can be expressedas whether the tag?s path matches the binaryprefix.3.5 Optimization Criterion and StoppingRuleTo select questions we use the average entropy ofthe marginalized word distribution.
We found thatthis criterion significantly outperforms the entropyof the distribution of joint events.
This is proba-bly due to the increased sparsity of the joint distri-bution and the fact that our ultimate metrics, i.e.,WER and word perplexity, involve only words.3.6 Distribution RepresentationIn a cluster Hx, we factor the joint distribution asfollows:p(witi|Hx) = p(wi|Hx) ?
p(ti|wi, Hx)where p(ti|wi, Hx) is represented in the form ofan HFT, in which each leaf has the probability of atag and each internal node contains the sum of theprobabilities of the tags it dominates.
This repre-sentation is designed to assist the decoding processdescribed in Section 4.3.7 SmoothingIn order to estimate probability distributions at theleaves of the history tree, we use the following re-cursive formula:p?n(witi) = ?npn(witi) + (1?
?n)p?n?
(witi) (5)where n?
is the n-th node?s parent, pn(witi) isthe distribution at node n (see Figure 3).
The1118root of the tree is interpolated with the distribu-tion punif(witi) =1|V |pML(ti|wi)5.
To estimateinterpolation parameters ?n, we use the EM algo-rithm described in (Magerman, 1994); however,rather than setting aside a separate developmentset of optimizing ?n, we use 4-fold cross valida-tion and take the geometric mean of the resultingcoefficients6.
We chose this approach because asmall development set often does not overlap withthe training set for low-count nodes, leading theEM algorithm to set ?n= 0 for those nodes.Let us consider one leaf of the history tree inisolation.
Its context can be represented by thepath to the root, i.e., the sequence of questions andanswers q1, .
.
.
q(n?)?qn?
(with q1being the answerto the topmost question):p?n(witi) = p?(witi|q1.
.
.
q(n?)?qn?
)Represented this way, Eq.
5 is a variant of Jelinek-Mercer smoothing:p?(witi|q1.
.
.
qn?)
= ?np(witi|q1.
.
.
qn?)
+(1?
?n)p?(witi|q1.
.
.
q(n?)?
)For backoff nodes (see Fig.
4), we use a lowerorder model7 interpolated with the distribution atthe backoff node?s grandparent (see node A in Fig.4):p?B(witi|wi?1i?n+1ti?1i?n+1) =?Ap?bo(witi|wi?1i?n+2ti?1i?n+2) + (1 ?
?A)p?A(witi)How to compute ?Ais an open question.
For thisstudy, we use a simple heuristic based on obser-vation that the further node A is from the rootthe more reliable the distribution p?A(witi) is, andhence ?Ais lower.
The formula we use is as fol-lows:?A=1?1 + distanceToRoot(A)5We use this distribution rather than uniform joint distri-bution 1|V ||T |because we do not want to allow word-tag pairsthat have never been observed.
The idea is similar to (Thedeand Harper, 1999).6To avoid a large number of zeros due to the product, weset a minimum for ?
to be 10?7.7The lower order model is constructed by the same algo-rithm, although with smaller context.
Note that the lower or-der model can back off on words or tags, or both.
In this paperwe backoff both on words and tags, i.e., p(witi|wi?1i?2ti?1i?2)backs off to p(witi|wi?1ti?1), which in turn backs off to theunigram p(witi).Figure 4: A fragment of the decision tree with abackoff node.
S ?
?S is the set of words observedin the training data at the node A.
To account forunseen words, we add the backoff node B.4 DecodingAs in HMM decoding, in order to compute prob-abilities for i-th step, we need to sum over |T |n?1possible combinations of tags in the history, whereT is the set of tags and n is the order of themodel.
With |T | predictions for the i-th step, wehave O(|T |n) computational complexity per word.Straightforward computation of these probabili-ties is problematic even for a trigram model withPOS tags, i.e., n = 3, |T | ?
40.
A standard ap-proach to limit computational requirements is touse beam search where only N most likely pathsare retained.
However, with fine-grain tags where|T | ?
1, 500, a tractable beam size would onlycover a small fraction of the whole space, leadingto search errors such as pruning good paths.Note that we have a history clustering function(Eq.
4) represented by the decision tree, and weshould be able to exploit this clustering to elimi-nate unnecessary computations involving equiva-lent histories.
Note that words in the history areknown exactly, thus we can create a projection ofthe clustering function H in Eq.
4 to the planewi?1i?n+1= const, i.e., where words in the contextare fixed to be whatever is observed in the history:H(wi?1i?n+1ti?1i?n+1) ?
?Hwi?1i?n+1=const(ti?1i?n+1)(6)The number of distinct clusters in the projection?H depends on the decision tree configuration andcan vary greatly for different words wi?1i?n+1in thehistory, but generally it is relatively small:|?Hwi?1i?n+1=const(ti?1i?n+1)| ?
|Tn?1| (7)1119Figure 5: Questions about hidden factors splitstates (see Figure 6) in the decoding lattice rep-resented by HFTs.thus, the number of probabilities that we need tocompute is | ?Hwi?1i?n+1=const| ?
|T |.Our decoding algorithm works similarly toHMM decoding with the exception that the set ofhidden states is not predetermined.
Let us illus-trate how it works in the case of a bigram model.Recall that the set of tags T is represented as abinary tree (HFT) and the only type of questionsabout tags is about matching a binary prefix in theHFT.
Such a question dissects the HFT into twoparts as depicted in Figure 5.
The cost of this op-eration is O(log |T |).We represent states in the decoding lattice asshown in the Figure 6, where pSinis the probabilityof reaching the state S:pSin=?S??INS??pS?inp(wi?2|HS?)?t?TS?p(t|wi?2HS?)?
?where INSis the set of incoming links to thestate S from the previous time index, and TS?
isthe set of tags generated from the state S?
repre-sented as a fragment of the HFT.
Note, that sincewe maintain the property that the probability as-signed to an inner node of the HFT is the sumof probabilities of the tags it dominates, the sum?t?TS?p(t|wi?2HS?)
is located at the root of TS?
,and therefore this is an O(1) operation.Now given the state S at time i ?
1, in order togenerate tag predictions for i-th word, we applyquestions from the history clustering tree, start-ing from the top.
Questions about overt factorsFigure 6: A state S in the decoding lattice.
pSinisthe probability of reaching the state S through theset of links INS.
The probabilities of generatingthe tags p(ti?1|wi?1, Hs), (ti?1?
TS) are repre-sented in the form of the HFT.always follow either a true or false branch, implic-itly computing the projection in Eq.
6.
Questionsabout hidden factors, can split the state S into twostates Strueand Sfalse, each retaining a part of TSas shown in the Figure 5.The process continues until each fragment ofeach state at the time i ?
1 reaches the bottom ofthe history tree, at which point new states for timei are generated from the clusters associated withleaves.
The states at i?
1 that generate the clusterH?Sbecome the incoming links to the state ?S.Higher order models work similarly, except thatat each time we consider a state S at time i ?
1along with one of its incoming links (to somedepth according to the size of the context).5 Experimental SetupTo evaluate the impact of fine-grain tags on lan-guage modeling, we trained our model with fivesettings: In the first model, questions were re-stricted to be about overt factors only, thus makingit a tree-based word model.
In the second model,we used POS tags.
To evaluate the effect of fine-grain tags, we train two models: head and parentdescribed in Section 2.3 and Section 2.4 respec-tively.
Since our joint model can be used withany kind of tags, we also trained it with Super-ARV tags (Wang et al, 2003).
The SuperARVswere created from the same parse trees that wereused to produce POS and fine-grain tags.
All ourmodels, including SuperARV, use trigram context.We include standard trigram, four-gram, and five-1120gram models for reference.
The ngram modelswere trained using SRILM toolkit with interpo-lated modified Kneser-Ney smoothing.We evaluate our model with an nbest rescoringtask using 100-best lists from the DARPA WSJ?93and WSJ?92 20k open vocabulary data sets.
Thedetails on the acoustic model used to produce thenbest lists can be found in (Wang and Harper,2002).
Since the data sets are small, we com-bined the 93et and 93dt sets for evaluation andused 92et for the optimization8.
We transformedthe nbest lists to match PTB tokenization, namelyseparating possessives from nouns, n?t from auxil-iary verbs in contractions, as well as contractionsfrom personal pronouns.All language models were trained on the NYT1994-1995 section of the English Gigaword cor-pus (approximately 70M words).
Since the NewYork Times covers a wider range of topics thanthe Wall Street Journal, we eliminated the most ir-relevant stories based on their trigram coverage bysections 00-22 of WSJ.
We also eliminated sen-tences over 120 words, because the parser?s per-formance drops significantly on long sentences.After parsing the corpus, we deleted sentences thatwere assigned a very low probability by the parser.Overall we removed only a few percent of the data;however, we believe that such a rigorous approachto data cleaning is important for building discrim-inating models.Parse trees were produced by an extended ver-sion of the Berkeley parser (Huang and Harper,2009).
We trained the parser on a combination ofthe BN and WSJ treebanks, preprocessed to makethem more consistent with each other.
We alsomodified the trees for the speech recognition taskby replacing numbers and abbreviations with theirverbalized forms.
We pre-processed the NYT cor-pus in the same way, and parsed it.
After that, weremoved punctuation and downcased words.
Forthe ngram model, we used text processed in thesame way.In head and parent models, tag vocabulariescontain approximately 1,500 tags each, while theSuperARV model has approximately 1,400 dis-tinct SuperARVs, most of which represent verbs(1,200).In these experiments we did not use overt fac-tors other than the surface word because they split8We optimized the LM weight and computed WER withscripts in the SRILM and NIST SCTK toolkits.Models WERtrigram (baseline) 17.5four-gram 17.7five-gram 17.8Word Tree 17.3POS Tags 17.0Head Tags 16.8Parent Tags 16.7SuperARV 16.9Table 1: WER results, optimized on 92et set, eval-uated on combined 93et and 93dt set.
The OracleWER is 9.5%.<unk>, effectively changing the vocabulary thusmaking perplexity incomparable to models with-out these factors, without improving WER notice-ably.
However, we do plan to use more overtfactors in Machine Translation experiments wherea language model faces a wider range of OOVphenomena, such as abbreviations, foreign words,numbers, dates, time, etc.Table 1 summarizes performance of the LMs onthe rescoring task.
The parent tags model outper-forms the trigram baseline model by 0.8% WER.Note that four- and five-gram models fail to out-perform the trigram baseline.
We believe this isdue to the sparsity as well as relatively short sen-tences in the test set (16 words on average).Interestingly, whereas the improvement of thePOS model over the baseline is not statisticallysignificant (p < 0.10)9, the fine-grain models out-perform the baseline much more reliably: p <0.03 (SuperARV) and p < 0.007 (parent).We present perplexity evaluations in Table 2.The perplexity was computed on Section 23 ofWSJ PTB, preprocessed as the rest of the data weused.
The head model has the lowest perplexityoutperforming the baseline by 9%.
Note, it evenoutperforms the five-gram model, although by asmall 2% margin.Although the improvements by the fine-graintagsets over POS are not significant (due to thesmall size of the test set), the reductions in per-plexity suggest that the improvements are not ran-dom.9For statistical significance, we used SCTK implementa-tion of the mapsswe test.1121Models PPLtrigram (baseline) 162four-gram 152five-gram 150Word Tree 160POS Tags 154Head Tags 147Parent Tags 150SuperARV 150Table 2: Perplexity results on Section 23 WSJPTB6 Conclusion and Future WorkIn this paper, we presented a joint language mod-eling framework.
Unlike any prior work knownto us, it was not tailored for any specific tag set,rather it was designed to accommodate any setof tags, especially large sets (?
1, 000), whichpresent challenges one does not encounter withsmaller tag sets, such at POS tags.
We discussedthese challenges and our solutions to them.
Someof the solutions proposed are novel, particularlythe decoding algorithm.We also proposed two simple fine-grain tagsets,which, when applied in language modeling, per-form comparably to highly sophisticated tag sets(SuperARV).
We would like to stress that, whileour fine-grain tags did not significantly outperformSuperARVs, the former use much less linguisticknowledge and can be automatically induced forany language with a treebank.Because a joint language model inherently pre-dicts hidden events (tags), it can also be used togenerate the best sequence of those events, i.e.,tagging.
We evaluated our model in the POS tag-ging task and observed similar results: the fine-grain models outperform the POS model, whileboth outperform the state-of-the-art HMM POStaggers.
We refer to (Filimonov and Harper, 2009)for details on these experiments.We plan to investigate how parser accuracy anddata selection strategies, e.g., based on parser con-fidence scores, impact the performance of ourmodel.
We also plan on evaluating the model?sperformance on other genres of speech, as well asin other tasks such as Machine Translation.
Weare also working on scaling our model further toaccommodate amounts of data typical for mod-ern large-scale ngram models.
Finally, we plan toapply the technique to other languages with tree-banks, such as Chinese and Arabic.We intend to release the source code of ourmodel within several months of this publication.7 AcknowledgmentsThis material is based upon work supported inpart by the Defense Advanced Research ProjectsAgency (DARPA) under Contract No.
HR0011-06-C-0023 and NSF IIS-0703859.
Any opinions,findings and/or recommendations expressed in thispaper are those of the authors and do not necessar-ily reflect the views of the funding agencies or theinstitutions where the work was completed.ReferencesLalit R. Bahl, Peter F. Brown, Peter V. de Souza, andRobert L. Mercer.
1990.
A tree-based statisticallanguage model for natural language speech recog-nition.
Readings in speech recognition, pages 507?514.Srinivas Bangalore.
1996.
?Almost parsing?
techniquefor language modeling.
In Proceedings of the Inter-national Conference on Spoken Language Process-ing, volume 2, pages 1173?1176.Jeff A. Bilmes and Katrin Kirchhoff.
2003.
Factoredlanguage models and generalized parallel backoff.In Proceedings of HLT/NACCL, 2003, pages 4?6.Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-ouza, Jennifer C. Lai, and Robert L. Mercer.
1992.Class-based n-gram models of natural language.Computational Linguistics, 18(4):467?479.Ciprian Chelba and Frederick Jelinek.
2000.
Struc-tured language modeling for speech recognition.CoRR.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th annual meet-ing on Association for Computational Linguistics,pages 310?318, Morristown, NJ, USA.
Associationfor Computational Linguistics.Denis Filimonov and Mary Harper.
2009.
Measuringtagging performance of a joint language model.
InProceedings of the Interspeech 2009.Peter A. Heeman.
1999.
POS tags and decision treesfor language modeling.
In In Proceedings of theJoint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Cor-pora, pages 129?137.Zhongqiang Huang and Mary Harper.
2009.
Self-Training PCFG grammars with latent annotationsacross languages.
In Proceedings of the EMNLP2009.1122David M. Magerman.
1994.
Natural language pars-ing as statistical pattern recognition.
Ph.D. thesis,Stanford, CA, USA.Sven Martin, Jorg Liermann, and Hermann Ney.
1998.Algorithms for bigram and trigram word clustering.In Speech Communication, pages 1253?1256.Thomas R. Niesler and Phil C. Woodland.
1996.A variable-length category-based n-gram languagemodel.
Proceedings of the IEEE International Con-ference on Acoustics, Speech, and Signal Process-ing, 1:164?167 vol.
1, May.Scott M. Thede and Mary P. Harper.
1999.
A second-order hidden markov model for part-of-speech tag-ging.
In Proceedings of the 37th Annual Meeting ofthe ACL, pages 175?182.Wen Wang and Mary P. Harper.
2002.
The SuperARVlanguage model: investigating the effectiveness oftightly integrating multiple knowledge sources.
InEMNLP ?02: Proceedings of the ACL-02 conferenceon Empirical methods in natural language process-ing, pages 238?247, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Wen Wang, Mary P. Harper, and Andreas Stolcke.2003.
The robustness of an almost-parsing languagemodel given errorful training data.
In Proceedingsof the IEEE International Conference on Acoustics,Speech, and Signal Processing.Peng Xu and Frederick Jelinek.
2004.
Random forestsin language modeling.
In in Proceedings of the 2004Conference on Empirical Methods in Natural Lan-guage Processing.Imed Zitouni.
2007.
Backoff hierarchical class n-gram language models: effectiveness to model un-seen events in speech recognition.
Computer Speech& Language, 21(1):88?104.1123
