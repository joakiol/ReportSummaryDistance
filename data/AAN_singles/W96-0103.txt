Hierarchical Clustering of Words and Application to NLP TasksAkira Ushioda*Fujitsu Laborator ies L td .Kawasaki,  Japanemail: ush ioda@f lab ,  fu j  ??su.
co.  jpAbstractThis paper describes a data-driven method for hierarchical clustering of words andclustering of multiword compounds.
A large vocabulary of English words (70,000 words) isclustered bottom-up, with respect o corpora ranging in size from 5 million to 50 millionwords, using mutual information as an objective function.
The resulting hierarchical clustersof words are then naturally transformed to a bit-string representation f (i.e.
word bits for)all the words in the vocabulary.
Evaluation of the word bits is carried out through themeasurement of the error rate of the ATR Decision-Tree Part-Of-Speech Tagger.
The sameclustering technique is then applied to the classification of multiword compounds.
In orderto avoid the explosion of the number of compounds to be handled, compounds in a smallsubclass are bundled and treated as a single compound.
Another merit of this approach isthat we can avoid the data sparseness problem which is ubiquitous in corpus statistics.
Thequality of one of the obtained compound classes is examined and compared to a conventionalapproach.1 Introduct ionOne of the fundamental issues concerning corpus-based NLP is that we can never expect toknow from the training data all the necessary quantitative information for the words that mightoccur in the test data if the vocabulary is large enough to cope with a real world domain.
Inview of the effectiveness of class-based n-gram language models against the data sparsenessproblem (Kneser and Ney 1993, Ueberla 1995), it is expected that classes of words are alsouseful for NLP tasks in such a way that statistics on classes are used whenever statistics onindividual words are unavailable or unreliable.
An ideal type of clusters for NLP is the onewhich guarantees mutual substitutability, in terms of both syntactic and semantic soundness,among words in the same class (Harris 1951, Brill and Marcus 1992).
Take, for example, thefollowing sentences.
(a) He went to the house by car.
(b) He went to the apartment by bus.
(c) He went to the ?
by ?
.
(d) He went to the house by the sea.Suppose that we want to parse sentences using a statistical parser and that sentences (a) and(b) appeared in the training and test data, respectively.
Since (a) is in the training data,we know that the prepositional phrase by car is attached to the main verb went, not to thenoun phrase the house.
Sentence (b) is quite similar to (a) in meaning, and identical to (a) insentence structure.
Now if the words apartment and bus are unknown to the parsing system*A part of this work is done when the author was at ATR Interpreting Telecommunications Research Labo-ratories, Kyoto, Japan.28(i.e.
never occurred in the training data), then sentence (b) must look to the system verymuch like (c), and it will be very hard for the parsing system to tell the difference in sentencestructure between (c) and (d).
However, if the system has access to a predefined set of classesof words, and if car and bus are in the same class, and house and apartme.nt are in anotherclass, it will not be hard for the system to detect he similarity between (a) and (b) and assignthe correct sentence structure to (b) without confusing it with (d).
The same argument holdsfor an example-based machine translation system.
In that case, an appropriate translation of(b) is expected to be derived with an example translation of (a) if the system has an accessto the classes of words.
Therefore, it is desirable that we build clustering of the vocabulary interms of mutual substitutability.Furthermore, clustering is much more useful if the clusters are of variable granularity.
Sup-pose, for example, that we have two sets of clusters, one is finer than the other, and that word-1and word-2 are in different finer classes.
With finer clusters alone, the amount of informationon the association of the two words that the system can obtain from the clusters is minimal.However, if the system has a capability of falling back and checking if they belong to the samecoarser class, and if that is the case, then the system can take advantage of the class informa-tion for the two words.
When we extend this notion of two-level word clustering to many levels,we will have a tree representation f all the words in the vocabulary in which the root noderepresents the whole vocabulary and a leaf node represents a word in the vocabulary.
Also,any set of nodes in the tree constitutes a partition (or clustering) of the vocabulary if thereexists one and only one node in the set alng the path from the root node to each leaf node.In the following sections, we will first describe a method of creating a binary tree repre-sentation of the vocabulary and present results of evaluating and comparing the quality ofthe clusters obtained from texts of very different sizes.
Then we will extend the paradigm ofclustering from word-based clustering to compound-based clustering.
In the above exampleswe looked only at the mutual substitutability of words; however, a lot of information can alsobe gained if we look at the substitutability of word compounds for either other word com-pounds or single words.
We will introduce the notion of compound-classes, propose a methodfor constructing them, and present results of our approach.2 Hierarchical Clustering of WordsSeveral algorithms have been proposed for automatically clustering words based on a largecorpus (Jardino and Adda 91, Brown et al 1992, Kneser and Ney 1993, Martin et al 1995,Ueberla 1995).
They are classified into two types.
One type is based on shuffling words fromclass to class starting from some initial set of classes.
The other type repeats merging classesstarting from a set of singleton classes (which contain only one word).
Both types are driven bysome objective function, in most cases by perplexity or average mutual information.
The meritof the second type for the purpose of constructing hierarchical clustering is that we can easilyconvert he history of the merging process to a tree-structured representation f the vocabulary.On the other hand, the second type is prone to being trapped by a local minimum.
The firsttype is more robust to the local minimum problem, but the quality of classes greatly dependson the initial set of classes, and finding an initial set of good quality is itself a very difficultproblem.
Moreover, the first approach only provides a means of partitioning the vocabularyand it doesn't provide a way of constructing a hierarchical clustering of words.
In this paperwe adopt the merging approach and propose an improved method of constructing hierarchicalclustering.
An attempt is also made to combine the two types of clustering and some resultswill be shown.
The combination is realized by the construction of clusters using the mergingmethod followed by the reshuffling of words from class to class.Our word bits construction algorithm (Ushioda 1996) is a modification and an extension29of the mutual information (MI) clustering algorithm proposed by Brown et al (1992).
Thereader is referred to (Ushioda 1996) and (Brown et al 1992) for details of MI clustering, butwe will first briefly summarize the MI clustering and then describe our hierarchical clusteringalgorithm.2.1 Mutua l  In fo rmat ion  C lus ter ing  A lgor i thmSuppose we have a text of T words, a vocabulary of V words, and a partition 7r of the vocabularywhich is a function from the vocabulary V to the set C of classes of words in the vocabulary.Brown et al showed that the likelihood L(Tr) of a bigram class model generating the text isgiven by the following formula.L(r) -- -H  -4- I (1)Here H is the entropy of the 1-gram word distribution, and I is the average mutual information(AMI) of adjacent classes in the text and is given by equation 2.F (elc2)I= ~ Pr(clc2)log Pr(cl)Pr(c2) (2)Cl ~C2Since H is independent of r, the partition that maximizes the AMI also maximizes the likelihoodL(r) of the text.
Therefore, we can use the AMI as an objective function for the constructionof classes of words.The mutual information clustering method employs abottum-up merging procedure.
In theinitial stage, each word is assigned to its own distinct class.
We then merge two classes if themerging of them induces minimum AMI reduction among all pairs of classes, and we repeatthe merging step until the number of the classes i reduced to the predefined number C. Timecomplexity of this basic algorithm is O(V s) when implemented straightforwardly, but it canbe reduced to O(V 3) by storing the result of all the trial merges at the previous merging step.Even with the O(V 3) algorithm, however, the calculation is not practical for a large vo-cabulary of order 104 or higher.
Brown et al proposed the following method, which we alsoadopted.
We first make V singleton classes out of the V words in the vocabulary and arrangethe classes in the descending order of frequency, then define the merging region as the firstC + 1 positions in the sequence of classes.
So initially the C + 1 most frequent words are inthe merging region.
Then do the following.I.
Merge the pair of classes in the merging region merging of which induces minimum AMIreduction among all the pairs in the merging region.2.
Put the class in the (C + 2) nd position into the merging region and shift each class afterthe (C + 2) nd position to its left.3.
Repeat I. and 2. until C classes remain.With this algorithm, the time complexity becomes O(C2V) which is practical for a workstationwith V in the order of 100,00O and C up to 1,000.2.2 Word  B i t s  Const ruct ion  A lgor i thmThe simplest way to construct a tree-structured representation f words is to construct adendrogram as a byproduct of the merging process, that is, to keep track of the order of mergingand make a binary tree out of the record.
A simple example with a five word vocabulary isshown in Figure 1.
If we apply this method to the above O(C2V) algorithm straightforwardly,however, we obtain for each class an extremely unbalanced, almost left branching subtree.
The30Merging History:Merge(A, B -> A)Merge(C, D -> C)Merge(C, E -> C)Merge(A, C -> A)Merge(X,Y->Z) reads"merge X and Y and namethe new class as Z"DendrogramfFFigure 1: Dendrogram Constructionreason is that after classes in the merging region are grown to a certain size, it is much lessexpensive, in terms of AMI, to merge a singleton class with lower frequency into a higherfrequency class than merging two higher frequency classes with substantial sizes.A new approach we adopted incorporates the following steps.1.
MI-clustering: Make C classes using the mutual information clustering algorithm withthe merging region constraint mentioned in (2.1).2.
Outer-clustering: Replace all words in the text with their class token 1 and execute binarymerging without he merging region constraint until all the classes are merged into a singleclass.
Make a dendrogram out of this process.
This dendrogram, Droot, constitutes theupper part of the final tree.3.
Inner-clustering: Let {C(1), C(2), ..., C(C)} be the set of the classes obtained at step 1.For each i (1 < i < C) do the following.
(a) Replace all words in the text except those in C(i) with their class token.
De-fine a new vocabulary V' = V1 U V2, where V1 = {all the words in C(i)}, V2 ={C1,C2, ...,Ci-l,Ci+l,Cc}, and Cj is a token for C(j) for 1 < j _< C. Assign eachelement in V' to its own class and execute binary merging with a merging constraintsuch that only those classes which only contain elements of V1 can be merged.
Thiscan be done by ordering elements of V' with elements of V1 in the first Ivll positionsand keep merging with a merging region whose width is \]Vll initially and decreasesby one with each merging step.
(b) Repeat merging until all the elements in V1 are put in a single class.Make a dendrogram Dsub out of the merging process for each class.
This dendrogramconstitutes a subtree for each class with a leaf node representing each word in the class.4 Combine the dendrograms by substituting each leaf node of Droot with the correspondingD sub .This algorithm produces a balanced binary tree representation of words in which thosewords which are close in meaning or syntactic feature come close in position.
Figure 2 showsan example of Dsu b for one class out of 500 classes constructed using this algorithm with avocabulary of the 70,000 most frequently occurring words in the Wall Street Journal Corpus.Finally, by tracing the path from the root node to a leaf node and assigning a bit to each branchwith zero or one representing a left or right branch, respectively, we can assign a bit-string (wordbits) to each word in the vocabulary.1In the actual implementation, weonly have to work on the bigram table instead of the whole text.31I I IFigure 2: Sample Subtree for One Class3 Word  C lus ter ing  Exper imentsWe performed experiments u ing plain texts from six years of the Wall Street Journal Corpusto create clusters and word bits.
The sizes of the texts are 5 million words (MW), 10MW,20MW, and 50MW.
The vocabulary is selected as the 70,000 most frequently occurring wordsin the entire corpus.
We set the number C of classes to 500.
The obtained hierarchical c ustersare evaluated via the error rate of the ATR Decision-Tree Part-Of-Speech Tagger.Then as an attempt o combine the two types of clustering methods discussed in Section2, we performed an experiment for incorporating a word-reshuffling process into the word bitsconstruction process.3.1 Dec is ion -Tree  Par t -Of -Speech  Tagg ingThe ATR Decision-Tree Part-Of-Speech Tagger is an integrated module of the ATR Decision-Tree Parser which is based on SPATTER (Magerman 1994).
The tagger employs a set of441 syntactic tags, which is one order of magnitude larger than that of the University ofPennsylvania Treebank Project.
Training texts, test texts, and held-out exts are all sequencesof word-tag pairs.
In the training phase, a set of events are extracted from the training texts.An event is a set of feature-value pairs or question-answer pairs.
A feature can be any attributeof the context in which the current word word(O) appears; it is conveniently expressed as aquestion.
Tagging is performed left to right.
Figure 3 shows an example of an event witha current word like.
The last pair in the event is a special item which shows the answer,i.e., the correct ag of the current word.
The first two lines show questions about identity ofwords around the current word and tags for previous words.
These questions are called basicquestions.
The second type of questions, word bits questions, are on clusters and word bits suchas is the current word in Class 295?
or what is the 29th bit of the previous word's word bits?.The third type of questions are called linguist's questions and these are compiled by an expertgrammarian.
Such questions could concern membership relations of words or sets of words, ormorphological features of words.Out of the set of events, a decision tree is constructed.
The root node of the decisiontree represents the set of all the events with each event containing the correct tag for thecorresponding word.
Probability distribution of tags for the root node can be obtained bycalculating relative frequencies of tags in the set.
By asking a value of a specific feature oneach event in the set, the set can be split into N subsets where N is the number of possiblevalues for the feature.
We can then calculate conditional probability distribution of tags for32Event- 128:{(word(0), "like" ) (word(-1), "flies" ) (word(-2), "time" } (word(l), "a~" ) (word(2), "arrow" )(tag(-1), "Verb-3rd-Sg-type3" ) (tag(-2), "Noun-Sg-typel4" ).
.
.
.
.
.
.
.
.
.
.
(Basic Questions)(Inclass?
(word(0), Class295), "yes" ) (WordBits(Word(-1), 29), "1" )(\]sMember?
(word(-2), Set("and", or" ,"nor" )), "no" )(Tag, "Prep-typeS" )}(WordBits Questions)(IsPrefix?
(Word(0), "anti"), "no" )(Linguist's Questions)Figure 3: Example of an eventtlL.
t-img,\[-q262422201816140\[\] WSJ  Text?
ATR Corpust i60Clustering Text Size (Million Words)Figure 4: Tagging Error Rateeach subset, conditioned on the feature value.
After computing for each feature the entropyreduction incurred by splitting the set, we choose the best feature which yields maximumentropy reduction.
By repeating this step and dividing the sets into their subsets we canconstruct a decision tree whose leaf nodes contain conditional probability distributions of tags.The obtained probability distributions are then smoothed using the held-out data.
The readeris referred to (Magerman 1994) for the details of smoothing.
In the test phase the system looksup conditional probability distributions of tags for each word in the test text and chooses themost probable tag sequences using beam search.We used WSJ texts and the ATR corpus for the tagging experiment.
The WSJ texts arere-tagged manually using the ATR syntactic tag set.
The ATR corpus is a comprehensivesampling of Written American English, displaying language use in a very wide range of stylesand settings, and compiled from many different domains (Black et al 1996).
Since the ATRcorpus is still in the process of development, he size of the texts we have at hand for thisexperiment is rather minimal considering the large size of the tag set.
Table 1 shows thesizes of texts used for the experiment.
Figure 4 shows the tagging error rates plotted againstvarious clustering text sizes.
Out of the three types of questions, basic questions and word bits33Text Size (words) Training Test Held-OutWSJ Text 75,139 5,831 6,534ATR Text 76,132 23,163 6,680Table 1: Texts for Tagging ExperimentsoiN282d242~2C181614WSJ Text?
Word,Bit@ LingQuest & Word\]3itsm II I12 .
.
.
.
.0 10 20 30 40 50 60Cluster ing Text Size (Mi l l ion Words)Figure 5: Comparison of WordBits with LingQuest & WordBitsquestions are used in this experiment.
To see the effect of introducing word bits informationinto the tagger, we performed a separate xperiment in which a randomly generated bit-stringis assigned to each word 2 and basic questions and word bits questions are used.
The results areplotted at zero clustering text size.
For both WSJ texts and ATR corpus, the tagging error ratedropped by more than 30% when using word bits information extracted from the 5MW text,and increasing the clustering text size further decreases the error rate.
At 50MW, the errorrate drops by 43%.
This shows the improvement of the quality of clusters with increasing sizeof the clustering text.
Overall high error rates are attributed to the very large tag set and thesmall training set.
One notable point in this result is that introducing word bits constructedfrom WSJ texts is as effective for tagging ATtt texts as it is for tagging WS3 texts even thoughthese texts are from very different domains.
To that extent, the obtained hierarchical c ustersare considered to be portable across domains.Figure 5 contrasts the tagging results using only word bits against he results with both wordbits and linguistic questions 3 for the WS3 text.
The zero clustering text size again corresponds~Since a distinctive bit-string is assigned to each word, the tagger also uses the bit-string as an ID numberfor each word in the process.
In this control experiment bit-strings are assigned in a random way, but no twowords are assigned the same word bits.
Random word bits are expected to give no class information to thetagger except for the identity of words.3The linguistic questions we used here are still in the initial stage of development and are by no means34WSJ Text22q2016 I14i12100 60l \[\] ?
Word.Bits@ ?
LingQuest &WordBits,i i i I i I J r i i10 20 30 40 50Clustering Text Size (Million Words)Figure 6: Effects of Reshuffling for Taggingto a randomly generated bit-string.
Introduction of linguistic questions is shown to significantlyreduce the error rates for the WSJ corpus.
Note that the dependency of the error rates on theclustering text size is quite similar in the two cases.
This indicates the effectiveness of combiningautomatically created word bits and hand-crafted linguistic questions in the same platform, i.e.,as features.
In Figure 5 the tagging error rates seem to be approaching saturation after theclustering text size of 50MW.
However, whether no further improvement can be obtained byusing texts of greater size is still an unsolved question.3.2 Reshuf f l ingOne way to improve the quality of word bits is to introduce a reshuffling process just after step1 (MI-clustering) of the word bits construction process (cf.
?
2.2).
The reshuffling process weadopted is quite simple.1.
Pick a word from the vocabulary.
Move the word from its current class to another classif that movement increases the AMI most among all the possible movements.2.
Repeat step 1 starting from the most frequent word through the least frequent word.This constitutes one round of reshuffling.
After several rounds of reshuffling, the word bitsconstruction process is resumed from step 2 (Outer-clustering).Figure 6 shows the tagging error rates with word bits obtained by zero, two and five roundsof reshuffling 4 with a 23MW text.
Tagging results presented in Figure 5 are also shown as areference.
Although the vocabulary used in this experiment is slightly different from the othercomprehensive.4The vocabulary used for the reshuffling experiment shown in Figure 6 is the one used for a preliminaryexperiment and its size is 63850.35experiments, we can clearly see the effect of reshuffling for both the word-bits-only case andthe case with word bits and linguistic questions.
After five rounds of reshuffling, the taggingerror rates become much smaller than the error rates using the 50MW clustering text withno reshuffling.
It is yet to be determined if the effect of reshuffling increases with increasingamount of clustering text.4 From Word Clustering to Compound ClusteringWe showed in section 3 that the clusters we obtained are useful for Part-Of-Speech tagging.However, the clusters we have worked on so far have all been clusters of words, and the Part-Of-Speech tagging task has been limited to individual words.
For many other NLP tasks,however, similarities among phrases or multiword compounds are more important than thoseamong individual words.
Let's turn back to the motivation of the clustering work discussed inthe Introduction.
Consider the following sentences.
(e) The music sent Mary to sleep.
(f) The music sent Professor Frederic K. Thompson to sleep.Suppose that we want to translate sentence (f) to some language by an example-based machinetranslation system with example data including sentence (e) and its translation.
In this case,what the system has to detect is that both "Mary" and "Professor Frederic K. Thompson"represent a human.
The similarity between "Mary" and "Frederic" as being first names doesn'thelp in this case.
Similarly, the detection of a correspondence between "CBS Inc." and "Ameri-can Telephone & Telegraph Co." might be necessary in another case.
This observation leads usto construct classes o.f compounds rather than classes of just words.
Individual words can alsobe in the same class as multiword compounds, but we will generically call such a class a class ofcompounds in this paper.
While several methods have been proposed to automatically extractcompounds (Smadja 1993, Suet al 1994), we know of no successful attempt to automaticallymake classes of compounds.The obvious problem we face when we construct classes of compounds i that the possiblenumber of compounds i too large if we try to handle them individually.
However, if we repre-sent compounds by a series of word-classes 5 instead of a series of words, we can constrain theexplosion of the number of compounds.
One way of looking at this approach is to bundle quitesimilar compounds in a small subclass and treat them as a single compound.
For example, inthe experiment described in Section 3, it was found that some word class, say WC129, con-tains almost exclusively first names, and another class, say WC246, contains almost exclusivelyfamily names.
Then the chain of classes "WC129_WC246" represents one pattern of humannames, or one group of two-word compounds representing human names.
There are of coursemany other patterns, or class chains, of different lengths which represent human names.
There-fore, our aim is to collect all the different class chains which are syntactically and semanticallysimilar and put them in one compound-class.In the following subsection, we describe one approach to this goal which is completelyautomatic.4.1 Compound Cluster ing  MethodOur compound clustering method consists of the following three steps.1.
Identification of Class ChainsFirst, we replace each word in a large text with its word-class.
We then use mutualinformation as a measure of "stickiness" of two classes, and identify which class pair5We use the term word-class for a class of words  to make a clear dist inct ion from a compound.class.36..should be chained.
Let MI(C1,C2) be mutual information of adjacent classes C1 and C2in the text.
Then we form chain "C1_C2" ifPr(ClC2) > *TH * (3)MI(C1,  C2) = log Pr(cl)Pr(c2) -for some threshold *TH*.If it is found, in the series of three classes "C1 C2 C3" in the text, that (C1,C2) formsa chain and (C2,C3) also forms a chain, then we simply form one large chain C1_C2_C3.In a similar way we form a chain of maximum length for any series of classes in the text.Construction of Reduced Text and New VocabularyEach class chain identified is then replaced in the text with a token which represents hechain.
We call such a token a class chain token.
After the scan through the text with thisreplacement operation of a class chain with its token, the text is represented by a seriesof word-classes and class chain tokens.
The word classes remaining in the text are theones which don't form a chain in their context.
Those word classes are then convertedback to their corresponding words in the text 6The resulting text is the same as the original text except hat a multiword compoundwhich matches one of the extracted class chains is represented bya class chain token.
Wecall this text the reduced text.
Out of the reduced text, a new vocabulary is created asa set of words and tokens whose frequency in the reduced text is more than or equal tosome threshold.Compound ClusteringWe conduct MI-clustering (step 1 of the word bits construction process) using the reducedtext and the new vocabulary.
The classes we obtained, which we call compound-classes,contain words and class chain tokens.
Each class chain token in a compound-class i  thenexpanded.
This means that all the multiword compounds that are represented bythe classchain token in the text are put into the compound-class.
After expanding all the tokens,the tokens are removed from the compound-classes.
This results in compound-classescontaining words and multiword compounds.
It is also possible to construct hierarchicalclustering of compounds if we follow all the steps in the word bits construction processafter this step.4.2 Compound Clustering ExperimentWe used plain texts from two years (1987 and 1988) of Wall Street Journal Corpus to createcompound clusters.
The total volume amounts to about 40 MW of text.
The word-classes usedin this experiment are taken from the result of MI clustering with the 50MW text followed byfive rounds of reshuffling.
The quality of the compound clusters depends on the threshold *TH*in equation 3.
We used *TH*=3 following "a very rough rule of thumb" used for word-basedmutual information i  (Church and Hanks, 1990).Out of the 40MW text, 7621 distinct class chains and 287,656 distinct multiword compoundsare extracted.
To construct a new vocabulary, we selected the words and tokens whose fre-quency in the reduced text is more than four.
The size of the new vocabulary is 60589 and itcontains 4685 class chain tokens.
Some of the compound-classes that were obtained are shownin Figure 7.
The compounds are listed in descending order of frequency in each class, and thelists are truncated at an arbitrary point.6The conversion of a word-class to a word is not a one-to-one mapping, but with the context in the text theconversion is unique.
In the actual implementation, the text is represented by a series of (word, word-class)pairs and no conversion is actually carried out.3"7Figure 7: Examples of Compound ClassesCOMPOUND CLASS 171:President_Reagan Mr._Reagan Mr._Bush Mr._Dukakis Judge_Bork Ronald_ReaganGeorge_Bush Michael_Dukakis Treasury_Secretary_J ames.B aker Mr._HolmesVice_President_George_Bush Gov._Dukakis Gen._Noriega Mrs._Thatcher some-one_who Mrs._Aquino Mr._Roh Gen._Secord Mr._Lawson Adm._Poindexteranyone_who Mr..Dole Lt._Col._North Jimmy_Carter Sen._Dole Mr._MulroneyMr._Quayle Sen._Bentsen Mr._Chirac Mr._Gephardt Mr._Marcos Vice_President_BushSen._Quayle Mr._Carter Mr._Chun Prime_Minister_Margaret_Thatcher Judge_GreeneMr..Brady President_Carter President_Chun Judge_Kennedy Sen._ProxmireRobert_Bork Rep._Rostenkowski Mr._Kohl Robert.
Holmes Judge_Pollack Mr..KempPrime_Minister_Yasuhiro_Nakasone Mr._Kennedy President_AquinoCOMPOUND CLASS 179:General_Motors_Corp.
Drexel_Burnham_Lambert..Inc. Ford.Motor_Co.
Interna-tional..Business_Machines_Corp.
General_Electric_Co.
Shearsoniehman_Brothers_Inc.Chrysler_Corp.
First.Boston_Corp.
Merrill_Lynch_&_Co.
Morgan_Stanley_&_Co.
Shear-son_Lehman_Hut ton..Inc. News_Corp.
American_Telephone_&_Telegraph_Co.
PaineWeb-bet_Inc.
Prudential- B ache_SecuritiesAnc.
TexacoAnc.
McDonnell_Douglas_Corp.Dean_Witter_ReynoldsAnc.
Time..Inc. AMR_Corp.
CB SAnc.
Ameri-can.
Express_Co.
Campeau_Corp.
BankAmerica_Corp.
Du_Pont_Co.
Allegis_Corp.General.Dynamics_Corp.
Digital_Equipment_Corp.
Kohlb erg_Kravis_Roberts_&_Co.Exxon_Corp.
Chase_Manhattan_Corp.
USX_Corp.
Nikko_Securities_Co.
Lock-heed_Corp.COMPOUND CLASS 221:common_stock preferred_stock cash_flow bank_debt long-term_debt foreign_debtsubordinated_debt senior_debt balance_sheet short-term_debt balance_sheetscost_overruns corporate_debt debt_load convertible_preferred_stock international_debtdebt_outstanding Class_B_stock debt_ratings cumulative_preferred_stock corporateAOUscurrent_delivery preference_stock ozoneAayer buffer_stock unsecured_debt convert-ible_preferred external_debt debt_offering current_contract blood_clots Class_B_commoncumulative_convertible_preferred_stock corporate_governance Class_B_common_stock un-sold_balance secured_debt debt.issue cumulative_preferred municipal_debt convert-ible_exchangeable_preferred_stock cash.hoard debt.rating 65-day_supply cash_balancesenior_subordinated_debt senior_secured_debtCOMPOUND CLASS 256:Fed SEC Reagan_administration IRS Pentagon Justice.
Department Navy Com-merce.Department FDA Army FCC FDIC Federal.Reserve_Board State_DepartmentBundesbank EPA FAA IMF Labor_Department Agriculture_Department FBINASD Defense.
Department Federal_Home.
Loan_Bank_Board British..government NRCFinance.Ministry Japanese.government FTC UAW Kremlin PRI Transporta-tion._Department PLO Federal_Trade_Commission CFTC Canadian_government NSCGAO Teamsters Carter_Hawley INS GSA Environmental_Protection..Agency ANCLabor_Party AFL-CIO FASB NFL Federal_Aviation_Administration ACLU38Compound-class-171 consists of names with title many of which are politicians' names.Compound-class-179 contains multiword company names.
Compound-class-221 consists ofmultiword compound nouns from several specific semantic domains including money, surgeryand natural environment, but most of the frequent compounds are money-related terms.Compound-class-256 is worth special attention in the sense that although single words andmultiword compounds are mixed almost evenly, most of the single words are abbreviations oforganizations, mostly public organizations, and the multiword compounds also ahnost exclu-sively represent public organizations.
Another point to note here is that the pattern of caseis not uniform in this list.
Although both "Defense Department" and "British government"represent political organizations, the former consists of only capitalized words and the latterdoesn't.In order to measure the performance of this compound clustering method, a consistencycheck is performed for one class.
The objective is to check what proportion of the identi-fied members of the class actually deserves to be included in the class.
Because this kind ofjudgement is very difficult in general, we must choose a class whose membership s quite clearto identify.
By this criterion we chose compound-class-179 because it is quite easy to decideif some compound is a correct company name or not.
From the 40MW text, we randomlychose 3000 occurrences of multiword compounds which are members of compound-class-179.By manual anMysis, it was found that 133 identified compounds were wrong.
The precision istherefore 95.6 %.
Most of the errors are due to the truncation of correct company names.
Forexample, from the string "North American Philips Corp.", only "Philips Corp." was extracted.Although "Philips Corp." is itself a correct company name, we treated this instance as an errorbecause our judgement was occurrence-based.
There was only one instance where a compoundirrelevant to company names was extracted (a person name).
For a control experiment whichwe will describe shortly, all the incorrect compounds are corrected by hand and a standard fileis created which contains all the correct 2999 occurrences of company names.One merit of the current approach is that the identification of a compound-class i  carriedout in time linear with the text size.
Therefore, by associating a word with its word-class as afeature in the lexicon, and by storing class chain patterns and their membership to compoundclasses, we can carry out a real time identification of the compound-classes without actuallystoring the compounds in the lexicon.As a control experiment to the above experiment, we conducted word-based compoundextraction and compared the result with the above result.
Instead of mutual information ofadjacent classes, mutual information of adjacent words are calculated for all the bigrams in thetext.
Then using various MI threshold values, words are chained in a similar way as describedin Section 4.1, and compounds are identified.
We then evaluated how many of the occurrencesof company names in the standard file are identified in the word-based compound extractionexperiment.
We varied the MI threshold values from 1.0 to 6.0 with a step of 0.5, but theprecision of the word-based approach with respect o the standard file was always below 50 %.The main reason of the superiority of the class-based approach against he word-based oneis associated with the data sparseness problem.
Most of the previously proposed methods toextract compounds or to measure word association using mutual information (MI) either ignoreor penalize items with low co-occurrence ounts (Church and Hanks 1990, Su, Wu and Chang1994), because MI becomes unstable when the co-occurrence ounts are very small.
Take forexample a class chain "WC129_WC246" discussed above.
Figure 8 shows some examples ofcompounds matching the pattern "WC129_WC246" in the 40MW text.
Each column shows,from left to right, word-based MI for the word bigram (WORD-1,WORD-2), co-occurrencefrequency of the word bigram, the first word, the second word, class-based MI for the classbigram (CLASS-I, CLASS-2), co-occurrence frequency of the class bigram, the word-class ofWORD-l, and the word-class of WORD-2.
Note that the numbers for class-based entries are39Figure 8: Examples of Compounds for NamesWORD-MI BI-COUNT WORD-1 WORD-2 CLASS-MI CL-BI-COUNT CLASS-I CLASS-216.104915 1 Takako Doi 3.941235 52087 129 24615.881772 3 Mandy Patinkin 3.941235 52087 129 24614.783159 3 Hideo Sakamaki 3.941235 52087 129 24612.280086 10 Curt Bradbury 3.941235 52087 129 24611.048669 3 Matthew Kennelly 3.941235 52087 129 2469.358209 1 Marsha Gardner 3.941235 52087 129 2467.994606 7 Ralph Whitehead 3.941235 52087 129 2465.073718 1 George Hartman 3.941235 52087 129 2464.328457 1 Daniel Owen 3.941235 52087 129 2463.914939 3 Charles Walker 3.941235 52087 129 2463.319351 1 Robert Fischer 3.941235 52087 129 2462.939145 1 Robert Lucas 3.941235 52087 129 2462.236354 2 Edward Baker 3.941235 52087 129 2461.119861 1 Robert Shultz 3.941235 52087 129 2461.072005 1 Robert Hall 3.941235 52087 129 2461.069133 1 George Jackson 3.941235 52087 129 2460.771154 1 Richard Baker 3.941235 52087 129 2460.218531 1 John Jackson 3.941235 52087 129 246the same for all the compounds because we collected compounds with the same class chain.Although all the compounds are compounds of a first name and a family name, the word-based MI varies considerably.
This is because frequencies of first names and family names varyconsiderably while frequencies of pairs of first names and family names in the list are verysmall.
For example, "John" and "Jackson" are very common first and second names, but thename "John Jackson" appeared only once in the text.
Therefore the word-based MI becomesvery small.
On the other hand, because "Takako" and "Doi" were very rare names in WSJnews articles in 1987 and 1988, the MI becomes very high even though "Takako Doi" appearedonly once in the text.
In contrast, the class-based MI is very stable because the co-occurrencefrequency of the two classes is as high as 52087.
When we examined frequencies of all thecompounds in the text that match "WC129_WC246", it turned out that more than 80 % ofthe compounds appeared less than five times in the text.
This shows how the data sparsenessproblem is critical for the purpose of compound extraction.5 ConclusionWe presented an algorithm for hierarchical clustering of words, and conducted a clusteringexperiment using large texts ranging in size from 5MW to 50MW.
High quality of the obtainedclusters is confirmed by the effect of introducing word bits into the ATR Decision-Tree Part-Of-Speech Tagger.
The hierarchical c usters obtained from WSJ texts are also shown to be usefulfor tagging ATR texts which are from quite different domains than WSJ texts.
The word-classes thus obtained are then used to identify and cluster multiword compounds.
It is shownthat by using statistics on classes instead of on words, the data sparseness problem is avoidedand the reliability of mutual information is increased.
As a result, class-based compoundsidentification and extraction becomes more reliable than word-based methods.
This approach40also provides a way of automatically clustering compounds, which has rarely been attempted.AcknowledgementsWe thank John Lafferty and Christopher Manning for their helpful comments, uggestions anddiscussion with us.
Special thanks are to Eric Visser for reviewing a draft of this paper.ReferencesBrill, E. and Marcus, M. (1992) "Automatically Acquiring Phrase Structure Using Distribu-tional Analysis."
Darpa Workshop on Speech and Natural Language, Harriman, N.Y.Black, E., Eubank, S., Kashioka, H., Magerman, D., Garside, R., and Leech, G. (1996) "Be-yond Skeleton Parsing: Producing a Comprehensive Large-Scale General-English TreebankWith Full Grammatical Analysis".
Proceedings of the 16th International Conference onComputational Linguistics.Brown, P., Della Pietra, V., deSouza, P., Lai, J., Mercer, R. (1992) "Class-Based n-gramModels of Natural Language".
Computational Linguistics, Vol.
18, No 4, pp.
467-479.Church, K. and Hanks, P. (1990) "Word Association Norms, Mutual Information, And Lexi-cography".
Computational Linguistics, Vol.
16, No 1, pp.
22-29.Harris, Z.
(1951) Structural Linguistics.
Chicago, University of Chicago Press.Jardino, M. and Adda, G. (1991) "Automatic word classification using simulated annealing",ICASSP 91.Martin, M., Liermann, J. and Ney, H. (1995) "Algorithms for Bigram and Trigram WordClustering", Proceedings of European Conference on Speech Communication a d Technology.Kneser, R. and Ney, H. (1993) "Improved Clustering Techniques for Class-Based StatisticalLanguage Modelling".
Proceedings of European Conference on Speech Communication a dTechnology.Magerman, D. (1994) Natural Language Parsing as Statistical Pattern Recognition.
Doctoraldissertation.
Stanford University, Stanford, California.Smadja, D. (1993) "Retrieving Collocations From Text: Xtract".
Computational Linguistics,Vol.
19, No 1, pp.
143-177.Su K.-Y., Wu M.-W. and Chan J.-S. (1994) "A Corpus-based Approach to Automatic Com-pound Extraction".
Proceedings of the 32nd Annual Meeting of the Association for Com-putational Linguistics.Ueberla, J.
(1995) "More Efficient Clustering of N-Grams for Statistical Language Modeling".Proceedings of European Conference on Speech Communication a d Technology.Ushioda, A.
(1996) "Hierarchical Clustering of Words".
Proceedings of the 16th InternationalConference on Computational Linguistics.41
