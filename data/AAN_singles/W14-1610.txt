Proceedings of the Eighteenth Conference on Computational Language Learning, pages 87?97,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsFocused Entailment Graphs for Open IE PropositionsOmer Levy?Ido Dagan?Jacob Goldberger??
Computer Science Department ?
Faculty of EngineeringBar-Ilan UniversityRamat-Gan, Israel{omerlevy,dagan,goldbej}@{cs,cs,eng}.biu.ac.ilAbstractOpen IE methods extract structured propo-sitions from text.
However, these propo-sitions are neither consolidated nor gen-eralized, and querying them may leadto insufficient or redundant information.This work suggests an approach to or-ganize open IE propositions using entail-ment graphs.
The entailment relation uni-fies equivalent propositions and induces aspecific-to-general structure.
We create alarge dataset of gold-standard propositionentailment graphs, and provide a novelalgorithm for automatically constructingthem.
Our analysis shows that predicateentailment is extremely context-sensitive,and that current lexical-semantic resourcesdo not capture many of the lexical infer-ences induced by proposition entailment.1 IntroductionOpen information extraction (open IE) extractsnatural language propositions from text withoutpre-defined schemas as in supervised relation ex-traction (Etzioni et al., 2008).
These proposi-tions represent predicate-argument structures astuples of natural language strings.
Open IE en-ables knowledge search by aggregating billions ofpropositions from the web1.
It may also be per-ceived as capturing an unsupervised knowledgerepresentation schema, complementing supervisedknowledge bases such as Freebase (Bollacker etal., 2008), as suggested by Riedel et al (2013).However, language variability obstructs open IEfrom becoming a viable knowledge representationframework.
As it does not consolidate natural lan-guage expressions, querying a database of open IEpropositions may lead to either insufficient or re-dundant information.
As an illustrative example,1See demo: openie.cs.washington.eduquerying the demo (footnote 1) for the generallyequivalent relieves headache or treats headachereturns two different lists of entities; out of the topfew results, the only answers these queries seemto agree on are caffeine and sex.
This is a majordrawback relative to supervised knowledge rep-resentations, which map natural language expres-sions to structured formal representations, such astreatments in Freebase.In this work, we investigate an approach for or-ganizing and consolidating open IE propositionsusing the novel notion of proposition entailmentgraphs (see Figure 1) ?
graphs in which eachnode represents a proposition and each directededge reflects an entailment relation, in the spiritof textual entailment (Dagan et al., 2013).
En-tailment provides an effective structure for ag-gregating natural-language based information; itmerges semantically equivalent propositions intocliques, and induces specification-generalizationedges between them.
For example, (aspirin, elim-inate, headache) entails, and is more specific than,(headache, respond to, painkiller).We thus propose the task of constructing anentailment graph over a set of open IE proposi-tions (Section 3), which is closely related to Be-rant et al?s work (2012) who introduced predicateentailment graphs.
In contrast, our work explorespropositions, which are essentially predicates in-stantiated with arguments, and thus semanticallyricher.
We provide a dataset of 30 such graphs,which represent 1.5 million pairwise entailmentdecisions between propositions (Section 4).To approach this task, we extend the state-of-the-art method for building entailment graphs (Be-rant et al., 2012) from predicates to completepropositions.
Both Snow et al (2006) and Berant etal used WordNet as distant supervision when train-ing a local pairwise model of lexical entailment.However, analyzing our data revealed that the lex-ical inferences captured in WordNet are quite dif-87Figure 1: An excerpt from a proposition entailment graph focused on the topic headache.
The dashed boundaries in the figuredenote cliques, meaning that all propositions within them are equivalent.ferent from the real lexical inferences induced byproposition entailment, making WordNet a mis-leading form of supervision.
We therefore employdirect proposition-level supervision, and design aprobabilistic model that captures the underlyinglexical-component inferences (Section 5).
We ex-plore a variety of natural extensions to prior art asbaselines (Section 6) and show that our model out-performs them (Section 7).While our model increases performance on thistask, there is still much room for improvement.
Adeeper analysis (Section 8) shows that commonlexical-semantic resources, on which we rely aswell, are either too noisy or provide inadequate re-call regarding lexical entailment.
In particular, wefind that predicate inference within propositionsoften goes beyond inference between the predi-cates?
linguistic meanings.
While pneumonia re-quires antibiotics and pneumonia is treated by an-tibiotics mean the same, the inherent meanings ofrequire and treat are different.
These inferencespertain to specific world knowledge, and warrantfuture research.Our work also contributes to textual entailmentresearch.
First, we extend entailment graphs tocomplete propositions.
Secondly, we investigatean intermediate problem of recognizing entail-ment between language-based predicate-argumenttuples.
Though this problem is simpler thansentence-level entailment, it does capture entail-ment of complete statements, which proves to bequite challenging indeed.2 BackgroundOur work builds upon two major research threads:open IE, and entailment graphs.2.1 Open Information ExtractionResearch in open IE (Etzioni et al., 2008) has fo-cused on transforming text to predicate-argumenttuples (propositions).
The general approach is tolearn proposition extraction patterns, and use themto create tuples while denoting extraction confi-dence.
Various methods differ in the type of pat-terns they acquire.
For instance, (Banko et al.,2007) and (Fader et al., 2011) used surface pat-terns, while (Mausam et al., 2012) and (Xu et al.,2013) used syntactic dependencies.Yates and Etzioni (2009) tried to mitigate theissue of language variability (as exemplified inthe introduction) by clustering synonymous predi-cates and arguments.
While these clusters do con-tain semantically related items, they do not neces-sarily reflect equivalence or implication.
For ex-ample, coffee, tea, and caffeine may all appearin one cluster, but coffee does not imply tea; onthe other hand, separating any element from thiscluster removes a valid implication.
Entailment,however, can capture the fact that both beveragesimply caffeine, but not one another.
Also related,Riedel et al (2013) try to generalize over open IEextractions by combining knowledge from Free-base and globally predicting which unobservedpropositions are true.
In contrast, our work identi-fies inference relations between concrete pairs ofobserved propositions.2.2 Entailment Graphs of Words and PhrasesPrevious work focused on entailment graphs orsimilar structures at the sub-propositional level.In these graphs, each node represents a natu-ral language word or phrase, and each directededge an entailment (or generalization) relation.Snow et al (2006) created a taxonomy of sense-88disambiguated nouns and their hyponymy rela-tions.
Berant et al (2012) constructed entailmentgraphs of predicate templates.
Recently, Mehdadet al (2013) built an entailment graph of nounphrases and partial sentences for topic labeling.The notion of proposition entailment graphs, how-ever, is novel.
This distinction is critical, be-cause apparently, entailment in the context of spe-cific propositions does not behave like context-oblivious lexical entailment (see Section 8).Berant et al?s work was implemented in Adleret al?s (2012) text exploration demo, which instan-tiated manually-annotated predicate entailmentgraphs with arguments, and used an additionallexical resource to determine argument entail-ment.
The combined graphs of predicate and argu-ment entailments induced a proposition entailmentgraph, which could then be explored in a faceted-search scheme.
Our work goes beyond, and at-tempts to build entailment graphs of propositionsautomatically.2.2.1 Berant et al?s Algorithm for PredicateEntailment Graph ConstructionWe present Berant et al?s algorithm in detail, as werely on it later on.
Given a set of predicates {i}1..nas input (constituting the graph nodes), it returnsa set of entailment decisions (i, j), which becomethe directed edges of the entailment graph.
Themethod works in two phases: (1) local estimation,and (2) global optimization.The local estimation model considers every po-tential edge (i, j) and estimates the probability pijthat this edge indeed exists, i.e.
that i entails j.Each predicate pair is represented with distribu-tional similarity features, providing some indica-tion of whether i entails j.
The estimator then useslogistic regression (or a linear SVM) over thosefeatures to predict the probability of entailment.
Itis trained with distant supervision from WordNet,employing synonyms, hypernyms, and (WordNet)entailments as positive examples, and antonyms,hyponyms, and cohyponyms as negative.The global optimization phase then searchesfor the most probable transitive entailment graph,given the local probability estimations.
It does sowith an integer linear program (ILP), where eachpair of predicates is represented by a binary vari-able xij, denoting whether there is an entailmentedge from i to j.
The objective function corre-sponds to the log likelihood of the assignment:?i 6=jxij(log(pij1?pij)+ log(pi1?pi)).
The priorterm pi is the probability of a random pair of pred-icates to be in an entailment relation, and can beestimated in advance.
The ILP solver searchesfor the optimal assignment that maximizes the ob-jective function under transitivity constraints, ex-pressed as linear constraints ?i,j,kxij+ xjk?xik?
1.3 Task DefinitionA proposition entailment graph is a directed graphwhere each node is a proposition si(s for sen-tence) and each edge (si, sj) represents an en-tailment relation from sito sj.
A proposi-tion siis a predicate-argument structure si=(pi, a1i, a2i, ..., amii)with one predicate piand itsarguments.
A proposition-level entailment (si, sj)holds if the verbalization of siimplies sj, accord-ing to the definition of textual entailment (Daganet al., 2013); i.e.
if humans reading siwould typi-cally infer that sjis most likely true.
Given a set ofpropositions (graph nodes), the task of construct-ing a proposition entailment graph is to recognizeall the entailments among the propositions, i.e.deciding which directional edges connect whichpairs of nodes.In this paper, we consider the narrower taskof constructing focused proposition entailmentgraphs, following Berant et al?s methodologyin creating focused predicate entailment graphs.First, all predicates are binary (have two argu-ments) and are denoted si=(a1i, pi, a2i).
Sec-ondly, we assume that the propositions were re-trieved by querying for a particular concept; outof the two arguments, one argument t (topic) iscommon to all the propositions in a single graph.We denote the non-topic argument as ai.
Figure 1presents an example of an informative entailmentgraph focused on the topic headache.Though confined, this setting still challengesthe state-of-the-art in textual entailment (see Sec-tion 7).
Moreover, these restrictions facilitatepiece-wise investigation of the entailment problem(see Section 8).4 DatasetTo construct our dataset of open IE extractions, wefound Google?s syntactic ngrams (Goldberg andOrwant, 2013) as a useful source of high-qualitypropositions.
Based on a corpus of 3.5 million En-glish books, it aggregates every syntactic ngram89?
subtree of a dependency parse ?
with at most4 dependency arcs.
The resource contains onlytree fragments that appeared at least 10 times inthe corpus, filtering out many low-quality syntac-tic ngrams.We extracted the syntactic ngrams that reflectpropositions, i.e.
subject-verb-object fragmentswhere object modifies the verb with either dobjor pobj.
Prepositions in pobj were concatenatedto the verb (e.g.
use with).
In addition, both sub-ject and object must each be a noun phrase con-taining two tokens at most, which are either nounsor adjectives.
Each token in the extracted frag-ments was then lemmatized using WordNet.
Afterlemmatization, we grouped all identical proposi-tions and aggregated their counts.
Approximately68 million propositions were collected.We chose 30 topics from the healthcare domain(such as influenza, hiv, and penicillin).
For eachtopic, we collected the set of propositions con-taining it, and manually filtered noisy extractions.This yielded 30 high-quality sets of 5,714 propo-sitions in total, where each set becomes the set ofnodes in a separate focused entailment graph.
Thegraphs range from 55 propositions (scurvy) to 562(headache), with an average of over 190 proposi-tions per graph.
Summing the number of propo-sition pairs within each graph amounts to a totalof 1.5 million potential entailment edges, whichmakes it by far the largest annotated textual entail-ment dataset to date.We used a semi-automatic annotation process,which dramatically narrows down the number ofmanual decisions, and hence, the required anno-tation time.
In short, the annotators are given aseries of small clustering tasks before annotatingentailment between those clusters.2The annotation process was carried out by twonative English speakers, with the aid of encyclope-dic knowledge for unfamiliar medical terms.
Theagreement on a subset of five randomly sampledgraphs was ?
= 0.77.
Annotating a single graphtook about an hour and a half on average.Positive entailment judgements constituted only8.4% of potential edges, and were found to be100% transitive.
We observe that in nearly all ofthose cases, a natural alignment between entail-ing components occurs: predicates align with eachother, the topic is shared, and the remaining non-2The annotated dataset is publicly available on the firstauthor?s website.topic argument aligns with its counterpart.
Con-sider the topic arthritis and the entailing proposi-tion pair (arthritis, cause, pain)?
(symptom, as-sociate with, arthritis); cause?associate with,while pain?symptom.
Rarely, some mis-alignments do occur; for instance (vaccine,protects, body)?
(vaccine, provides, protection).However, it is almost always the case that proposi-tions entail if and only if their aligned lexical com-ponents entail as well.5 AlgorithmIn this section, we extend Berant et al?s algorithm(2012) to construct entailment graphs of proposi-tions.
As described in Section 2.2.1, their methodfirst performs local estimation of predicate entail-ment and then global optimization.
We modify thelocal estimation phase to estimate proposition en-tailment instead, and then apply the same globaloptimization in the second phase.In Section 4, we observed the alignment-basedrelationship between proposition and lexical en-tailment.
We leverage this observation to predictproposition entailment with lexical entailment fea-tures (as Berant et al), using the Component En-tailment Conjunction (CEC) model in Section 5.1.Following Snow et al (2006) and Berant etal, we could train CEC using distant supervisionfrom WordNet.
In fact, we did try this approach(presented as baseline methods, Section 6) andfound that it performed poorly.
Furthermore, ouranalysis (Section 8) suggests that WordNet rela-tions do not adequately capture the lexical infer-ences induced by proposition-level entailment.
In-stead, we use a more realistic signal to train CEC ?direct supervision from the annotated dataset.
Sec-tion 5.2 describes how we propagate proposition-level entailment annotations to the latent lexicalcomponents.5.1 Component Entailment ConjunctionCEC assumes that proposition-level entailmentis the result of entailment within each pair ofaligned components, i.e.
a pair of propositionsentail if and only if both their predicate and ar-gument pairs entail.
This assumption stems fromour observation of alignment in Section 4.
Fur-thermore, CEC leverages this interdependence tolearn separate predicate-entailment and argument-entailment features through proposition-level su-pervision.90Formally, for every ordered pair of propositions(i, j) we denote proposition entailment as a binaryrandom variable xsijand predicate and argumententailments as xpijand xaij, respectively.
In oursetting, proposition entailment (xsij) is observed,but component entailments (xpij, xaij) are hidden.We use logistic regression, with features ?pijandparameter wp, as a probabilistic model of predi-cate entailment (and so for arguments with ?aijandwa):pij= P(xpij= 1???
?pij;wp)= ?(?pij?
wp)aij= P(xaij= 1???
?aij;wa)= ?(?aij?
wa)(1)where ?
is the sigmoid ?
(z) =11+e?z.
We thendefine proposition entailment as the conjunction ofits binary components: xsij= xpij?xaij.
Therefore,the probability of proposition entailment given thecomponent features is:sij= P(xsij= 1???
?pij, ?aij;wp, wa)= P(xpij= 1, xaij= 1???
?pij, ?aij;wp, wa)= P(xpij= 1????pij;wp)?
P(xaij= 1???
?aij;wa)= pij?
aijThe proposition entailment probability is thus theproduct of component entailment probabilities.Given the proposition-level information{xsij},the log-likelihood is:` (wp, wa)=?i 6=jlogP(xsij???
?pij, ?aij;wp, wa)=?i 6=j(xsijlog (pijaij) +(1?
xsij)log (1?
pijaij))5.2 Learning Component ModelsWe wish to learn the model?s parameters (wp, wa).Our approach uses direct proposition-level super-vision from our annotated dataset to train the com-ponent logistic regression models.
Since compo-nent entailment (xpij, xaij) is not observed in thedata, we apply the iterative EM algorithm (Demp-ster et al., 1977).
In the E-step we estimate theirprobabilities from proposition-level labels (xsij),and in the M-step we use those estimates as ?soft?labels to learn the component-level model param-eters (wp, wa).E-Step During the E-step in iteration t + 1,we compute the probability of component entail-ments given the proposition entailment informa-tion, based on the parameters at iteration t (wpt,wat).
The predicate probabilities are given by:cpij= P(xpij= 1??
?xsij, ?pij, ?aij;wpt, wat)(2)and are computed with Bayes?
law:cpij=??
?1 if xsij= 1ptij(1?atij)1?ptijatijif xsij= 0(3)where ptijis computed as in Equations 1, with theparameters at iteration t (wpt).
Argument entail-ment probabilities (caij) are computed analogously.M-Step In the M-step, we compute new valuesfor the parameters (wpt+1, wat+1).
In our case, thereis no closed-form formula for updating the param-eters.
Instead, at each iteration, we solve a sepa-rate logistic regression for each component.
Whilewe have each component model?s features (?pij,assuming predicates for notation), we do not ob-serve the component-level entailment labels (xpij);instead, we obtain their probabilities (cpij) from theexpectation step.To learn the parameters (wpt+1, wat+1) from thecomponent entailment probabilities (cpij), we em-ploy a weighted variant of logistic regression, thatcan utilize ?soft?
class labels (i.e.
a probabilitydistribution over {0, 1}).
To solve such a logisticregression (e.g.
for wpt+1), we maximize the log-likelihood:`(wpt+1)=?ij(cpijlog(P(xpij= 1????pij;wpt+1))+(1?
cpij)log(P(xpij= 0???
?pij;wpt+1)))For optimization, we calculate the derivative, anduse gradient ascent to update wpt+1:?wpt+1=?`(wpt+1)?wpt+1=?ij(cpij?
P(xpij= 1???
?pij;wpt+1))?pijThis optimization is concave, and therefore theunique global maximum can be efficiently ob-tained.5.3 FeaturesSimilar to Berant et al, we used three types of fea-tures to describe both predicate pairs (?pij) and ar-gument pairs (?aij): distributional similarities, lex-ical resources, and string distances.91We used the entire database of 68 million ex-tracted propositions (see Section 4) to create aword-context matrix; context was defined as otherwords that appeared in the same proposition, andeach word was represented as (string, role), rolebeing the location within the proposition, eithera1, p, or a2.
The matrix was then normalized withpointwise mutual information (Church and Hanks,1990).
We used various metrics to measure dif-ferent types of similarities between each compo-nent pair, including: cosine similarity, Lin?s sim-ilarity (1998), inclusion (Weeds and Weir, 2003),average precision, and balanced average precision(Kotlerman et al., 2010).
Weed?s and Kotlerman?smetrics are directional (asymmetric) and indicatethe direction of a potential entailment relation.These features were used for both predicates andarguments.
In addition, we used Melamud et al?s(2013) method to learn a context-sensitive modelof predicate entailment, which estimates predicatesimilarity in the context of the given arguments.We leveraged the Unified Medical LanguageSystem (UMLS) to check argument entailment,using the parent and synonym relations.
A singlefeature indicated whether such a connection ex-ists.
We also used WordNet relations as features,specifically: synonyms, hypernyms, entailments,hyponyms, cohyponyms, antonyms.
Each Word-Net relation constituted a different feature for bothpredicates and arguments.Finally, we added a string equality feature and aLevenshtein distance feature (Levenshtein, 1966)for different spellings of the same word to bothpredicate and argument feature vectors.6 Baseline MethodsWe consider four algorithms that naturally ex-tend the state-of-the-art to propositions, while us-ing distant supervision (from WordNet).
SinceCEC uses direct supervision, we also examinedanother (simpler) directly-supervised algorithm.As a naive unsupervised baseline, we use Argu-ment Equality, which returns ?entailing?
if the ar-gument pair is identical.
Predicate Equality is de-fined similarly for predicates.Component-Level Distant Supervision Thefollowing methods use distant supervision fromWordNet (as in Berant et al?s work, Section 2.2.1)to explicitly train component-level entailment esti-mators.
Specifically, we train a logistic regressionmodel for each component as specified in Equa-tions 1 in Section 5.1.
We present four methods,which differ in the way they obtain global graph-level entailment decisions for propositions, basedon the local component entailment estimates (pij,aijin Section 5.1).The first method, Opt(Arg ?
Pred), uses theproduct of both component models to estimate lo-cal proposition-level entailment: sij= pij?
aij.The global set of proposition entailments is thendetermined using Berant et al?s global optimiza-tion, according to the proposition-level scores sij.Note that this method is identical to CEC dur-ing inference, but differs in the way the local es-timators are learned (with component-level super-vision from WordNet).An alternative is Opt(Arg) ?
Opt(Pred).
Itfirst obtains local probabilities (pij, aij) for eachcomponent as in Opt(Arg ?
Pred), but then em-ploys component-level global optimization (tran-sitivity enforcement), yielding two sets of entail-ment decisions, xpijand xaij.
Proposition entail-ment is then determined by the conjunction xsij=xpij?
xaij, as in (Adler et al., 2012).Finally, Opt(Arg) ignores the predicate com-ponent.
Instead, it uses only the argument en-tailment graph (as produced by Opt(Arg) ?Opt(Pred)) to decide on proposition entailment;i.e.
a pair of propositions entail if and only if theirarguments entail.
Opt(Pred) is defined analo-gously.Proposition-Level Direct Supervision A sim-pler alternative to CEC that also employsproposition-level supervision is Joint Features,which concatenates the component level featuresinto a unified feature vector: ?sij= ?pij?
?aij.
Wethen couple them with the gold-standard annota-tions xsijto create a training set for a single logisticregression.
We use the trained logistic regressionto estimate the local probability of proposition en-tailment, and then perform global optimization toconstruct the entailment graph.7 Empirical EvaluationWe evaluate the models in Sections 5 & 6 on the30 annotated entailment graphs presented in Sec-tion 4.
During testing, each graph was evaluatedseparately.
The results presented in this sectionare all micro-averages, though macro-averageswere also computed and found to reflect the sametrends.
Models trained with distant supervisionwere evaluated on all graphs.
For directly super-92vised methods, we used 2 ?
6-fold cross valida-tion (25 training graphs per fold).
In this scenario,each graph induced a set of labeled examples ?its edges being positive examples, and the miss-ing potential edges being negative ones ?
and theunion of these sets was used as the training set ofthat cross-validation fold.7.1 ResultsTable 1 compares the performance of CEC withthat of the baseline methods.While Joint Features and CEC share exactly thesame features, CEC exploits the inherent conjunc-tion between predicate and argument entailments(as observed in Section 4 and modeled in Sec-tion 5.1), and forces both components to decide onentailment separately.
This differs from the sim-pler log-linear model (Joint Features) where, forexample, a very strong predicate entailment fea-ture might override the overall proposition-leveldecision, even if there was no strong indicationof argument entailment.
As a result, CEC dom-inates Joint Features in both precision and recall.The F1difference between these methods is sta-tistically significant with McNemar?s test (1947)with p  0.01.
Specifically, CEC corrected JointFeatures 7621 times, while the opposite occurredonly 4048 times.CEC also yields relatively high precisionand recall.
While it has 2% less recall thanOpt(Arg) (the highest-recall baseline), it sur-passes Opt(Arg)?s precision by 14%.
Along witha similar comparison to Argument Equality (thehighest precision baseline), CEC notably outper-forms all baselines.It is also evident that both directly super-vised methods outperform the distantly super-vised methods.
Our analysis (Section 8.1) showsthat WordNet lacks significant coverage, and maytherefore be a problematic source of supervision.Perhaps the most surprising result is the com-plete failure of WordNet-supervised methods thatconsider predicate information.
A deeper analy-sis (Section 8.2) shows that predicate inference ishighly context-sensitive, and deviates beyond thelexical inferences provided by WordNet.7.2 Learning CurveWe measure the supervision needed to train the di-rectly supervised models by their learning curves(Figure 2).
Each point is the average F1scoreSupervision Method Prec.
Rec.
F1NoneArgument81.6% 42.2% 55.6%EqualityPredicate9.3% 1.5% 2.6%EqualityComponent(WordNet)Opt(Arg73.8% 3.8% 7.2%?
Pred)Opt(Arg) ?72.3% 3.2% 6.0%Opt(Pred)Opt(Arg) 64.6% 55.4% 59.7%Opt(Pred) 11.0% 6.2% 8.0%Proposition(Annotated)Joint76.3% 51.7% 61.6%FeaturesCEC 78.7% 53.5% 63.7%Table 1: Performance on gold-standard (micro averaged).Figure 2: Learning curve of directly supervised methods.across 12 cross-validation folds; e.g.
for 10 train-ing graphs, we used 4 ?
3-fold cross validation.Even 5 training graphs (a day?s worth of annota-tion) are enough for CEC to perform on-par withthe best distantly supervised method, and with 15training graphs it outperforms every baseline, in-cluding Joint Features trained with 25 graphs.7.3 Effects of Global OptimizationWe evaluate the effects of enforcing transitivity byconsidering CEC with and without the global op-timization phase.
Table 2 shows how many entail-ment edges were added (and removed) by enforc-ing transitivity, and measures how many of thosemodifications were correct.
Apparently, transi-tivity?s greatest effect is the removal of incorrectentailment edges.
The same phenomenon wasalso observed in the work on predicate entailmentgraphs (Berant et al., 2012).
Overall, transitivitymade 4,848 correct modifications out of 6,734 intotal.
A ?2test reveals that the positive contribu-tion of enforcing transitivity is indeed statisticallysignificant (p 0.01).93Gold Global Opt Global OptStandard Added Edge Removed EdgeEdge Exists 1150 482No Edge 1404 3698Table 2: The modifications made by enforcing transitivityw.r.t.
the gold standard.
55% of the edges added by enforcingtransitivity are incorrect, but it removed even more incorrectedges, improving the overall performance.8 Analysis of Lexical InferenceAlthough CEC had a statistically-significant im-provement upon the baselines, its absolute perfor-mance leaves much room for improvement.
Wehypothesize that the lexical entailment features weused, following state-of-the-art lexical entailmentmodeling, do not capture many of the actual lexi-cal inferences induced by proposition entailment.We demonstrate that this is indeed the case.8.1 Argument EntailmentTo isolate the effect of different features on pre-dicting argument entailment, we collected allproposition pairs that shared exactly the samepredicate and topic, and thus differed in only their?free?
argument.
This yielded 20,336 aligned ar-gument pairs, whose entailment annotations areequal to the corresponding proposition-entailmentannotation in the dataset.Using WordNet synonyms and hypernyms topredict entailment yielded a precision of about88%, at 40% recall.
Though relatively precise,WordNet?s coverage is limited, and misses manyinferences.
We describe three typical types of in-ferences that were absent from WordNet.The first type constitutes of widely-used paraphrases such as people?persons,woman?female, and pain?ache.
These may beseen as weaker types of synonyms, which mayhave nuances, but are typically interchangeable.Another type is metonymy, in which a conceptis not referred to by its own name, but by that ofan associated concept.
This is very common inour healthcare dataset, where a disease is often re-ferred to by its underlying pathogen and vice-versa(e.g.
pneumonia?pneumococcus).The third type of missing inferences is causal-ity.
Many instances of metonymy (such as thedisease-pathogen example) may be seen as causal-ity as well.
Other examples can be drug and ef-fect (laxative?diarrhea) or condition and symp-tom (influenza?fever).WordNet?s lack of such common-sense infer-ences, which are abundant in our proposition en-tailment dataset, might make WordNet a problem-atic source of distant supervision.
The fact that60% of the entailing examples in our dataset arelabeled by WordNet as non-entailing, means thatfor each truly positive training example, there is ahigher chance that it will have a negative label.Distributional similarity is commonly used tocapture such missing inferences and complementWordNet-like resources.
On this dataset, how-ever, it failed to do so.
One of the more in-dicative similarity measures, inclusion (Weeds andWeir, 2003), yielded only 27% precision at 40%recall when tuning a threshold to optimize F1.
In-creasing precision caused a dramatic drop in re-call: 50% precision limited recall to 3.2%.
Othersimilarity measures performed similarly or worse.It seems that current methods of distributionalword similarity also capture relations quite differ-ent from inference, such as cohyponyms and do-main relatedness, and might be less suitable formodeling lexical entailment on their own.8.2 Context-Sensitive Predicate EntailmentThe proposition-level entailment annotation in-duces an entailment relation between the predi-cates, which holds in the particular context of theproposition pair.
We wish to understand the na-ture of this predicate-level entailment, and how itcompares to classic lexical inference as portrayedin the lexical semantics literature.
To that end, wecollected all the entailing proposition pairs withequal arguments, and extracted the correspondingpredicate pairs (which, assuming alignment, arenecessarily entailing in that context).
This list con-tains 52,560 predicate pairs.In our first analysis, we explored which Word-Net relations correlate with predicate entailment,by checking how well each relation covers the setof entailed predicate pairs.
Synonyms and hyper-nyms, which are considered positive entailmentindicators, covered only about 8% each.
Sur-prisingly, the hyponym and cohyponym relations(which are considered negative entailment indica-tors) covered over 9% and 14%, respectively.
Ta-ble 3 shows the exact details.It seems that WordNet relations are hardly cor-related with the context-sensitive predicate-levelentailments in our dataset, and that the classic in-terpretation of WordNet relations with respect toentailment does not hold in practice, where en-94Interpretation WordNet Relation CoveragePositiveSynonyms 7.85%Direct Hypernyms 5.62%Indirect Hypernyms 3.14%Entailment 0.33%NegativeAntonyms 0.31%Direct Hyponyms 5.74%Indirect Hyponyms 3.51%Cohyponyms 14.30%Table 3: The portion of positive predicate entailments cov-ered by each WordNet relation.
WordNet relations are di-vided according to their common interpretations with respectto lexical entailment.tailments are judged in the context of concretepropositions.
In fact, negative indicators in Word-Net seem to cover more predicate entailmentsthan positive ones.
This explains the failure ofWordNet-supervised methods with predicate en-tailment features (Section 7.1).Since we do not expect WordNet to cover allshades of entailment, we conducted a manual anal-ysis as well.
100 entailing predicate pairs wererandomly sampled, and manually annotated forlexical-level entailment, without seeing their argu-ments.
To compensate for the lack of context, weguided the annotators to assume a general health-care scenario, and use a more lenient interpretationof textual entailment (biased towards positive en-tailment decisions).
Nevertheless, only 56% of thepredicate pairs were labeled as entailing, indicat-ing that the context-sensitive predicate inferencescaptured in our dataset can be quite different fromgeneric predicate inferences.We suggest that this phenomenon goes one stepbeyond what the current literature considers ascontext-sensitive entailment, and that it is morespecific than determining an appropriate lexicalsense.
To demonstrate, we present four suchpredicate-entailment phenomena.First, there are cases in which an appropriatelexical sense could exist in principle, but it is toospecific to be practically covered by a manual re-source.
For example, cures cancer?kills cancer,but the appropriate sense for kill (cause to ceaseexisting) does not exist, and in turn, neither doesthe hypernymy relation from cure to kill.
It is hardto expect these kinds of obscure senses or relation-ships to comprehensively appear in a manually-constructed resource.In many cases, such a specific sense does notexist.
For example, (pneumonia, require, antibi-otic)?
(pneumonia, treated by, antibiotics), but re-quire does not have a general sense which meanstreat by.
The inference in this example does notstem from the linguistic meaning of each predi-cate, but rather from the real-world situation theirencapsulating propositions describe.Another aspect of predicate entailment thatmay change when considering propositional con-text is the direction of inference.
For instance,cause9trigger.
While it may be the case that trig-ger entails cause, the converse is not necessarilytrue since cause is far more general.
However,when considering (caffeine, cause, headache) and(caffeine, trigger, headache), both propositions de-scribe the same real-world situation, and thus bothpropositions are mutually entailing.
In this con-text, cause does indeed entail trigger as well.Finally, figures of speech (such as metaphors)are abundant and diverse.
Though it may not beso common to read about a drug that ?banishes?headaches, most readers would understand the un-derlying meaning.
These phenomena exceed thecurrent scope of lexical-semantic resources suchas WordNet, and require world knowledge.9 ConclusionThis paper proposes a novel approach, based onentailment graphs, for consolidating informationextracted from large corpora.
We define the prob-lem of building proposition entailment graphs, andprovide a large annotated dataset.
We also presentthe CEC model, which models the connection be-tween proposition entailment and lexical entail-ment.
Although it outperforms the state-of-the-art, its performance is not ideal because it relies oninadequate lexical-semantic resources that do notcapture the common-sense and context-sensitiveinferences which are inherent in proposition en-tailment.
In future work, we intend to further in-vestigate lexical entailment as induced by proposi-tion entailment, and hope to develop richer meth-ods of lexical inference that address the phenom-ena exhibited in this setting.AcknowledgementsThis work has been supported by the Israeli Min-istry of Science and Technology grant 3-8705, theIsrael Science Foundation grant 880/12, and theEuropean Communitys Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreementno.
287923 (EXCITEMENT).
We would like tothank our reviewers for their insightful comments.95ReferencesMeni Adler, Jonathan Berant, and Ido Dagan.
2012.Entailment-based text exploration with applicationto the health-care domain.
In Proceedings of theSystem Demonstrations of the 50th Annual Meet-ing of the Association for Computational Linguistics(ACL 2012), pages 79?84.Michele Banko, Michael J. Cafarella, Stephen Soder-land, Matthew Broadhead, and Oren Etzioni.
2007.Open information extraction from the web.
In IJ-CAI, volume 7, pages 2670?2676.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2012.
Learning entailment relations by global graphstructure optimization.
Computational Linguistics,38(1):73?111.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In Proceedings of the 2008 ACMSIGMOD international conference on Managementof data, pages 1247?1250.
ACM.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational Linguistics, 16(1):22?29.Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-simo Zanzotto.
2013.
Recognizing textual entail-ment: Models and applications.
Synthesis Lectureson Human Language Technologies, 6(4):1?220.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incompletedata via the em algorithm.
Journal of the Royal Sta-tistical Society.
Series B (Methodological), pages 1?38.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S Weld.
2008.
Open information extrac-tion from the Web.
Communications of the ACM,51(12):68?74.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1535?1545, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Yoav Goldberg and Jon Orwant.
2013.
A dataset ofsyntactic-ngrams over time from a very large cor-pus of english books.
In Second Joint Conferenceon Lexical and Computational Semantics (*SEM),Volume 1: Proceedings of the Main Conference andthe Shared Task: Semantic Textual Similarity, pages241?247, Atlanta, Georgia, USA, June.
Associationfor Computational Linguistics.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distribu-tional similarity for lexical inference.
Natural Lan-guage Engineering, 16(4):359?389.Vladimir I. Levenshtein.
1966.
Binary codes capableof correcting deletions, insertions and reversals.
InSoviet Physics Doklady, volume 10, page 707.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and 17th International Conference on Com-putational Linguistics, Volume 2, pages 768?774,Montreal, Quebec, Canada, August.
Association forComputational Linguistics.Mausam, Michael Schmitz, Stephen Soderland, RobertBart, and Oren Etzioni.
2012.
Open language learn-ing for information extraction.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 523?534, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Quinn McNemar.
1947.
Note on the sampling errorof the difference between correlated proportions orpercentages.
Psychometrika, 12(2):153?157.Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng,and Shafiq Joty.
2013.
Towards topic labelingwith phrase entailment and aggregation.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages179?189, Atlanta, Georgia, June.
Association forComputational Linguistics.Oren Melamud, Jonathan Berant, Ido Dagan, JacobGoldberger, and Idan Szpektor.
2013.
A two levelmodel for context sensitive inference rules.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 1331?1340, Sofia, Bulgaria, August.Association for Computational Linguistics.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M. Marlin.
2013.
Relation extractionwith matrix factorization and universal schemas.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 74?84, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Rion Snow, Dan Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenousevidence.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th Annual Meeting of the Association for Com-putational Linguistics (ACL-COLING 2006), pages801?808.Julie Weeds and David Weir.
2003.
A generalframework for distributional similarity.
In MichaelCollins and Mark Steedman, editors, Proceedings ofthe 2003 Conference on Empirical Methods in Nat-ural Language Processing, pages 81?88.96Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,and Denilson Barbosa.
2013.
Open informationextraction with tree kernels.
In Proceedings of the2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 868?877, At-lanta, Georgia, June.
Association for ComputationalLinguistics.Alexander Yates and Oren Etzioni.
2009.
Unsuper-vised methods for determining object and relationsynonyms on the web.
Journal of Artificial Intelli-gence Research, 34(1):255.97
