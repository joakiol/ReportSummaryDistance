Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 34?42,Baltimore, Maryland, 26-27 July 2014.c?2014 Association for Computational LinguisticsThe Illinois-Columbia System in the CoNLL-2014 Shared TaskAlla Rozovskaya1Kai-Wei Chang2Mark Sammons2Dan Roth2Nizar Habash11Center for Computational Learning Systems, Columbia University{alla,habash}@ccls.columbia.edu2Cognitive Computation Group, University of Illinois at Urbana-Champaign{kchang10,mssammon,danr}@illinois.eduAbstractThe CoNLL-2014 shared task is an ex-tension of last year?s shared task and fo-cuses on correcting grammatical errors inessays written by non-native learners ofEnglish.
In this paper, we describe theIllinois-Columbia system that participatedin the shared task.
Our system ranked sec-ond on the original annotations and first onthe revised annotations.The core of the system is based on theUniversity of Illinois model that placedfirst in the CoNLL-2013 shared task.
Thisbaseline model has been improved and ex-panded for this year?s competition in sev-eral respects.
We describe our underly-ing approach, which relates to our previ-ous work, and describe the novel aspectsof the system in more detail.1 IntroductionThe topic of text correction has seen a lot of inter-est in the past several years, with a focus on cor-recting grammatical errors made by English as aSecond Language (ESL) learners.
ESL error cor-rection is an important problem since most writersof English are not native English speakers.
The in-creased interest in this topic can be seen not onlyfrom the number of papers published on the topicbut also from the three competitions devoted togrammatical error correction for non-native writ-ers that have recently taken place: HOO-2011(Dale and Kilgarriff, 2011), HOO-2012 (Dale etal., 2012), and the CoNLL-2013 shared task (Nget al., 2013).In all three shared tasks, the participating sys-tems performed at a level that is considered ex-tremely low compared to performance obtained inother areas of NLP: even the best systems attainedF1 scores in the range of 20-30 points.The key reason that text correction is a diffi-cult task is that even for non-native English speak-ers, writing accuracy is very high, as errors arevery sparse.
Even for some of the most com-mon types of errors, such as article and preposi-tion usage, the majority of the words in these cate-gories (over 90%) are used correctly.
For instance,in the CoNLL training data, only 2% of preposi-tions are incorrectly used.
Because errors are sosparse, it is more difficult for a system to identify amistake accurately and without introducing manyfalse alarms.The CoNLL-2014 shared task (Ng et al., 2014)is an extension of the CoNLL-2013 shared task(Ng et al., 2013).
Both competitions make useof essays written by ESL learners at the NationalUniversity of Singapore.
However, while the firstone focused on five kinds of mistakes that are com-monly made by ESL writers ?
article, preposition,noun number, verb agreement, and verb form ?this year?s competition covers all errors occurringin the data.
Errors outside the target group werepresent in the task corpora last year as well, butwere not evaluated.Our system extends the one developed by theUniversity of Illinois (Rozovskaya et al., 2013)that placed first in the CoNLL-2013 competition.For this year?s shared task, the system has beenextended and improved in several respects: we ex-tended the set of errors addressed by the system,developed a general approach for improving theerror-specific models, and added a joint inferencecomponent to address interaction among errors.See Rozovskaya and Roth (2013) for more detail.We briefly discuss the task (Section 2) and givean overview of the baseline Illinois system (Sec-tion 3).
Section 4 presents the novel aspects of thesystem.
In Section 5, we evaluate the completesystem on the development data and show the re-sults obtained on test.
We offer error analysis and abrief discussion in Section 6.
Section 7 concludes.34Error type Rel.
freq.
ExamplesArticle (ArtOrDet) 14.98% *?/The government should help encourage *the/?breakthroughs as well as *a/?
complete medicationsystem .Wrong collocation (Wci) 11.94% Some people started to *think/wonder if electronicproducts can replace human beings for better perfor-mances .Local redundancy (Rloc-) 10.52% Some solutions *{as examples}/?
would be to designplants/fertilizers that give higher yield ...Noun number (Nn) 8.49% There are many reports around the internet and onnewspaper stating that some users ?
*iPhone/iPhonesexploded .Verb tense (Vt) 7.21% Through the thousands of years , most Chinese scholars*are/{have been} greatly affected by Confucianism .Orthography/punctuation (Mec) 6.88% Even British Prime Minister , Gordon Brown *?/, hasurged that all cars in *britain/Britain to be green by2020 .Preposition (Prep) 5.43% I do not agree *on/with this argument that surveillancetechnology should not be used to track people .Word form (Wform) 4.87% On the other hand , the application of surveillance tech-nology serves as a warning to the *murders/murderersand they might not commit more murder .Subject-verb agreement (SVA) 3.44% However , tracking people *are/is difficult and differentfrom tracking goods .Verb form (Vform) 3.25% Travelers survive in desert thanks to GPS*guide/guiding them .Tone (Wtone) 1.29% Hence , as technology especially in the medical fieldcontinues to get developed and updated , people {don?t}/{do not} risk their lives anymore .Table 1: Example errors.
In the parentheses, the error codes used in the shared task are shown.
Notethat only the errors exemplifying the relevant phenomena are marked in the table; the sentences maycontain other mistakes.
Errors marked as verb form include multiple grammatical phenomena that maycharacterize verbs.
Our system addresses all of the error types except ?Wrong Collocation?
and ?LocalRedundancy?.2 Task DescriptionBoth the training and the test data of the CoNLL-2014 shared task consist of essays written by stu-dents at the National University of Singapore.
Thetraining data contains 1.2 million words from theNUCLE corpus (Dahlmeier et al., 2013) correctedby English teachers, and an additional set of about30,000 words that was released last year as a testset for the CoNLL-2013 shared task.
We use lastyear?s test data as a development set; the results inthe subsequent sections are reported on this subset.The CoNLL corpus error tagset includes 28 er-ror categories.
Table 1 illustrates the most com-mon error categories in the training data; errors aremarked with an asterisk, and ?
denotes a missingword.
Our system targets all of these, with the ex-ception of collocation and local redundancy errors.Among the less commonly occurring error types,our system addresses tone (style) errors; these areillustrated in the table.It should be noted that the proportion of erro-neous instances is several times higher in the de-velopment data than in the training data for all ofthe error categories.
For example, while only 2.4%of noun phrases in the training data have deter-miner errors, in the development data 10% of nounphrases have determiner errors.35?Hence, the environmental *factor/factors also*contributes/contribute to various difficulties,*included/including problems in nuclear tech-nology.
?Error type Confusion setNoun number {factor, factors}Verb Agreement {contribute, contributes}Verb Form{included, including,includes, include}Table 2: Sample confusion sets for noun num-ber, verb agreement, and verb form.3 The Baseline SystemIn this section, we briefly describe the Univer-sity of Illinois system (henceforth Illinois; in theoverview paper of the shared task the system is re-ferred to as UI) that achieved the best result in theCoNLL-2013 shared task and which we use as ourbaseline model.
For a complete description, werefer the reader to Rozovskaya et al.
(2013).The Illinois system implements fiveindependently-trained machine-learning clas-sifiers that follow the popular approach to ESLerror correction borrowed from the context-sensitive spelling correction task (Golding andRoth, 1999; Carlson et al., 2001).
A confusionset is defined as a list of confusable words.Each occurrence of a confusable word in text isrepresented as a vector of features derived from acontext window around the target.
The problemis cast as a multi-class classification task and aclassifier is trained on native or learner data.
Atprediction time, the model selects the most likelycandidate from the confusion set.The confusion set for prepositions includes thetop 12 most frequent English prepositions (thisyear, we extend the confusion set and also targetextraneous preposition usage).
The article confu-sion set is as follows: {a, the, ?
}.1The confu-sion sets for noun, agreement, and form modulesdepend on the target word and include its morpho-logical variants.
Table 2 shows sample confusionsets for noun, agreement, and form errors.Each classifier takes as input the corpus doc-uments preprocessed with a part-of-speech tag-1?
denotes noun-phrase-initial contexts where an articleis likely to have been omitted.
The variants ?a?
and ?an?
areconflated and are restored later.ger2and shallow parser3(Punyakanok and Roth,2001).
The other system components use the pre-processing tools only as part of candidate genera-tion (e.g., to identify all nouns in the data for thenoun classifier).The choice of learning algorithm for each clas-sifier is motivated by earlier findings showingthat discriminative classifiers outperform othermachine-learning methods on error correctiontasks (Rozovskaya and Roth, 2011).
Thus, theclassifiers trained on the learner data make use ofa discriminative model.
Because the Google cor-pus does not contain complete sentences but onlyn-gram counts of length up to five, training a dis-criminative model is not desirable, and we thus useNB (details in Rozovskaya and Roth (2011)).The article classifier is a discriminative modelthat draws on the state-of-the-art approach de-scribed in Rozovskaya et al.
(2012).
The modelmakes use of the Averaged Perceptron (AP) algo-rithm (Freund and Schapire, 1996) and is trainedon the training data of the shared task with richfeatures.
The article module uses the POS andchunker output to generate some of its features andcandidates (likely contexts for missing articles).The original word choice (the source article)used by the writer is also used as a feature.
Sincethe errors are sparse, this feature causes the modelto abstain from flagging mistakes, resulting in lowrecall.
To avoid this problem, we adopt the ap-proach proposed in Rozovskaya et al.
(2012), theerror inflation method, and add artificial article er-rors to the training data based on the error distribu-tion on the training set.
This method prevents thesource feature from dominating the context fea-tures, and improves the recall of the system.The other classifiers in the baseline system ?noun number, verb agreement, verb form, andpreposition ?
are trained on native English data,the Google Web 1T 5-gram corpus (henceforth,Google, (Brants and Franz, 2006)) with the Na?
?veBayes (NB) algorithm.
All models use word n-gram features derived from the 4-word windowaround the target word.
In the preposition model,priors for preposition preferences are learned fromthe shared task training data (Rozovskaya andRoth, 2011).The modules targeting verb agreement and2http://cogcomp.cs.illinois.edu/page/software view/POS3http://cogcomp.cs.illinois.edu/page/software view/Chunker36verb form mistakes draw on the linguistically-motivated approach to correcting verb errors pro-posed in Rozovskaya et.
al (2014).4 The CoNLL-2014 SystemThe system in the CoNLL-2014 shared task is im-proved in three ways: 1) Additional error-specificclassifiers: word form, orthography/punctuation,and style; 2) Model combination; and 3) Joint in-ference to address interacting errors.
Table 3 sum-marizes the Illinois and the Illinois-Columbia sys-tems.4.1 Targeting Additional ErrorsThe Illinois-Columbia system implements severalnew classifiers to address word form, orthographyand punctuation, and style errors (Table 1).4.1.1 Word Form ErrorsWord form (Wform) errors are grammatical er-rors that involve confusing words that share abase form but differ in derivational morphology,e.g.
?use?
and ?usage?
(see also Table 1).
Con-fusion sets for word form errors thus should in-clude words that differ derivationally but share thesame base form.
In contrast to verb form errorswhere confusion sets specify all possible inflec-tional forms for a given verb, here, the associatedparts-of-speech may vary more widely.
An ex-ample of a confusion set is {technique, technical,technology, technological}.Because word form errors encompass a widerange of misuse, one approach is to consider ev-ery word as an error candidate.
We follow a moreconservative method and only attempt to correctthose words that occurred in the training data andwere tagged as word form errors (we cleaned upthat list by removing noisy annotations).A further challenge in addressing word form er-rors is generating confusion sets.
We found thatabout 45% of corrections for word form errors inthe development data are covered by the confusionsets from the training data for the same word.
Wethus derive the confusion sets using the trainingdata.
Specifically, for every source word that istagged as a word form error in the training data,the confusion set includes all labels to which thatword is mapped in the training data.
In addition,plural and singular forms are added for all wordstagged as nouns, and inflectional forms are addedfor words tagged as verbs.
For more detail oncorrecting verb errors, we refer the reader to Ro-zovskaya et al.
(2014).4.1.2 Orthography and Punctuation ErrorsThe Mec error category includes errors inspelling, context-sensitive spelling, capitalization,and punctuation.
Our system addresses punctua-tion errors and capitalization errors.To correct capitalization errors, we collectedwords that are always capitalized in the train-ing and development data when not occurringsentence-initially.The punctuation classifier includes two mod-ules: a learned component targets missing andextraneous comma usage and is an AP classifiertrained on the learner data with error inflation.A second, pattern-based component, complementsthe AP model: it inserts missing commas by usinga set of patterns that overwhelmingly prefer the us-age of a comma, e.g.
when a sentence starts withthe word ?hence?.
The patterns are learned auto-matically over the training data: specifically, us-ing a sliding window of three words on each side,we compiled a list of word n-gram contexts thatare strongly associated with the usage of a comma.This list is then used to insert missing commas inthe test data.4.1.3 Style ErrorsThe style (Wtone) errors marked in the corpus arediverse, and the annotations are often not consis-tent.
We constructed a pattern-based system todeal with two types of style errors that are com-monly annotated.
The first type of style edit avoidsusing contractions of negated auxiliary verbs.
Forexample, it changes ?do n?t?
to ?do not?.
We use apattern-based classifier to identify such errors andreplace the contractions.
The second type of styleedit encourages the use of a semi-colon to jointwo independent clauses when a conjunctive ad-verb is used.
For example, it edits ?
[clause], how-ever, [clause]?
to ?
[clause]; however, [clause]?.
Toidentify such errors, we use a part-of-speech tag-ger to recognize conjunctive adverbs signifying in-dependent clauses: if two clauses are joined by thepattern ?, [conjunctive adverb],?, we will replace itwith ?
; [conjunctive adverb],?.4.2 Modules not Included in the Final SystemIn addition to the modules described above, we at-tempted to address two other common error cate-gories: spelling errors and collocation errors.
We37IllinoisClassifiers Training data AlgorithmArticle Learner AP with inflationPreposition Native NB-priorsNoun number Native NBVerb agreement Native NBVerb form Native NBIllinois-ColumbiaClassifiers Training data AlgorithmArticle Learner and native AP with infl.
(learner) and NB-priors (native)Preposition Learner and native AP with infl.
(learner) and NB-priors (native)Noun number Learner and native AP with infl.
(learner) and NB (native)Verb agreement Native AP with infl.
(learner) and NB (native)Verb form Native NB-priorsWord form Native NB-priorsOrthography/punctuation Learner AP and pattern-basedStyle Learner Pattern-basedModel combination Section 4.3Global inference Section 4.4Table 3: The baseline (Illinois) system vs. the Illinois-Columbia system.
AP stands for AveragedPerceptron, and NB stands for the Na?
?ve Bayes algorithm.describe these below even though they were notincluded in the final system.Regular spelling errors are noticeable but notvery frequent, and a number are not marked inthe corpus (for example, the word ?dictronary?
in-stead of ?dictionary?
is not tagged as an error).
Weused an open source package ?
?Jazzy?4?
to at-tempt to automatically correct these errors to im-prove context signals for other modules.
However,there are often multiple similar words that can beproposed as corrections, and Jazzy uses phoneticguidelines that sometimes lead to unintuitive pro-posals (such as ?doctrinaire?
for ?dictronary?).
Itwould be possible to extend the system with a filteron candidate answers that uses n-grams or someother context model to choose better candidates,but the relatively small number of such errors lim-its the potential impact of such a system.Collocation errors are the second most commonerror category accounting for 11.94% of all errorsin the training data (Table 1).
We tried using theIllinois context-sensitive spelling system5to de-tect these errors, but this system requires prede-fined confusion sets to detect possible errors andto propose valid corrections.
The coverage of thepre-existing confusion sets was poor ?
the system4http://jazzy.sourceforge.net5http://cogcomp.cs.illinois.edu/cssc/could potentially correct only 2.5% of collocationerrors ?
and it is difficult to generate new con-fusion sets that generalize well, which requires agreat deal of annotated training data.
The sys-tem performance was relatively poor because itproposed many spurious corrections: we believethis is due to the relatively limited context it uses,which makes it particularly susceptible to makingmistakes when there are multiple errors in closeproximity.4.3 Model CombinationModel combination is another key extension of theIllinois system.In the Illinois-Columbia system, article, prepo-sition, noun, and verb agreement errors are eachaddressed via a model that combines error predic-tions made by a classifier trained on the learnerdata with the AP algorithm and those made bythe NB model trained on the Google corpus.
TheAP classifiers all make use of richer sets of fea-tures than the native-trained classifiers: the article,noun number, and preposition classifiers employfeatures that use POS information, while the verbagreement classifier also makes use of dependencyfeatures extracted using a parser (de Marneffe etal., 2008).
For more detail on the features usedin the agreement module, we refer the reader to38Rozovskaya et al.
(2014).
Finally, all of the APmodels use the source word of the author as a fea-ture and, similar to the article AP classifier (Sec-tion 3), implement the error inflation method.
Thecombined model generates a union of correctionsproduced by the components.We found that for every error type, the com-bined model is superior to each of the single classi-fiers, as it combines the advantages of both of theclassifiers so that they complement one another.In particular, while each of the learner and nativecomponents have similar precision, since the pre-dictions made differ, the recall of the combinedmodel improves.4.4 Joint InferenceOne of the mistakes typical for Illinois systemwere inconsistent predictions.
Inconsistent predic-tions occur when the classifiers address grammat-ical phenomena that interact at the sentence level,e.g.
noun number and verb agreement.
To ad-dress this problem, the Illinois-Columbia systemmakes use of global inference via an Integer Lin-ear Programming formulation (Rozovskaya andRoth, 2013).
Note that Rozovskaya and Roth(2013) also describe a joint learning model thatperforms better than the joint inference approach.However, the joint learning model is based ontraining a joint model on the Google corpus, andis not as strong as the individually-trained classi-fiers of the Illinois-Columbia system that combinepredictions from two components ?
NB classifierstrained on the native data from the Google corpusand AP models trained on the learner data (Sec-tion 4.3).5 Experimental ResultsIn Sections 3 and 4, we described the individualsystem components that address different types oferrors.
In this section, we show how the systemimproves when each component is added into thesystem.
In this year?s competition, systems arecompared using F0.5 measure instead of F1.
Thisis because in error correction good precision ismore important than having a high recall, and theF0.5 reflects that by weighing precision twice asmuch as recall.
System output is scored with theM2 scorer (Dahlmeier and Ng, 2012).Table 4 reports performance results of each in-dividual classifier.
In the final system, the arti-cle, preposition, noun number, and verb agree-Model P R F0.5Articles (AP) 38.97 8.85 23.19Articles (NB-priors) 47.34 6.01 19.93Articles (Comb.)
38.73 10.93 25.67Prep.
(AP) 34.00 0.5 2.35Prep.
(NB-priors) 33.33 0.79 3.61Prep.
(Comb.)
30.06 1.17 5.13Noun number (NB) 44.74 5.48 18.39Noun number (AP) 82.35 0.41 2.01Noun number (Comb.)
45.02 5.57 18.63Verb agr.
(AP) 38.56 1.23 5.46Verb agr.
(NB) 63.41 0.76 3.64Verb agr.
(Comb.)
41.09 1.55 6.75Verb form (NB-priors) 59.26 1.41 6.42Word form (NB-priors) 57.54 3.02 12.48Mec (AP; patterns) 48.48 0.47 2.26Style (patterns) 84.62 0.64 3.13Table 4: Performance of classifiers targetingspecific errors.Model P R F0.5The baseline (Illinois) systemArticles 38.97 8.85 23.19+Prepositions 39.24 9.35 23.93+Noun number 42.13 14.83 30.79+Subject-verb agr.
42.25 16.06 31.86+Verb form 43.19 17.20 33.17Model Combination+Model combination 42.72 20.19 34.92Additional Classifiers+Word form 43.39 21.54 36.07+Mec 43.70 22.04 36.52+Style 44.22 21.54 37.09Joint Inference+Joint Inference 44.28 22.57 37.13Table 5: Results on the development data.
Thetop part of the table shows the performance of thebaseline (Illinois) system from last year.P R F0.5Scores based on the original annotations41.78 24.88 36.79Scores based on the revised annotations52.44 29.89 45.57Table 6: Results on Test.39ment classifiers use combined models, each con-sisting of a classifier trained on the learner dataand a classifier trained on native data.
We reportperformance of each such component separatelyand when they are combined.
The results showthat combining models boosts the performance ofeach classifier: for example, the performance ofthe article classifier improves by more than 2 F0.5points.
It should be noted that results are com-puted with respect to all errors present in the data.For this reason, recall is low.Next, in Table 5, we show the contribution ofthe novel components over the baseline system onthe development set.
As described in Section 3,the baseline Illinois system consists of five indi-vidual components; their performance is shown inthe top part of the table.
Note that although for thedevelopment set we make use of last year?s testset, these results are not comparable to the perfor-mance results reported in last year?s competitionthat used the F1 measure.
Overall, the baselinesystem achieves an F0.5 score of 33.17 on the de-velopment set.Then, by applying the model combination tech-nique introduced in Section 4.3, the performanceis improved to 34.92.
By adding modules to tar-get three additional error types, the overall perfor-mance becomes 37.09.
Finally, the joint inferencetechnique (see Section 4.4) slightly improves theperformance further.
The final system achieves anF0.5 score of 37.13.Table 6 shows the results on the test set providedby the organizers.
As was done previously, theorganizers also offered another set of annotationsbased on the combination of revised official anno-tations and accepted alternative annotations pro-posed by participants.
Performance results on thisset are also shown in Table 6.6 Discussion and Error AnalysisHere, we present some interesting errors that oursystem makes on the development set and discussour observations on the competition.
We analyzeboth the false positive errors and those cases thatare missed by our system.6.1 Error AnalysisStylistic preference Surveillance technologysuch as RFID (radio-frequency identification) isone type of examples that has currently been im-plemented.Here, our system proposes a change to pluralfor the noun ?technology?.
The gold standardsolution instead proposes a large number of cor-rections throughout that work with the choice ofthe singular ?technology?.
However, using theplural ?technologies?
as proposed by the Illinois-Columbia system is quite acceptable, and a com-parable number of corrections would make the restof the sentence compatible.
Note also that thegold standard proposes the use of commas aroundthe phrase ?such as RFID (radio-frequency iden-tification)?, which could also be omitted based onstylistic considerations alone.Word choice The high accuracy in utiliz-ing surveillance technology eliminates the*amount/number of disagreements among people.The use of ?amount?
versus ?number?
dependson the noun to which the term attaches.
This couldconceivably be achieved by using a rule and wordlist, but many such rules would be needed and eachwould have relatively low coverage.
Our systemdoes not detect this error.Presence of multiple errors Not only the detailsof location will be provided, but also may lead tofind out the root of this kind of children tradingagency and it helps to prevent more this kind oftragedy to happen on any family.The writer has made numerous errors in thissentence.
To determine the correct preposition inthe marked location requires at least the preced-ing verb phrase to be corrected to ?from happen-ing?
; the extraneous ?more?
after ?prevent?
in turnmakes the verb phrase correction more unlikely asit perturbs the contextual clues that a system mightlearn to make that correction.
Our system pro-poses a different preposition ?
?in?
?
that is betterthan the original in the local context, but which isnot correct in the wider context.Locally coherent, globally incorrect People?slives become from increasingly convenient to al-most luxury, thanks to the implementation of in-creasingly technology available for the Man?s life.In this example, the system proposes to deletethe preposition ?from?.
This correctiom improvesthe local coherency of the sentence.
However, theresulting construction is not consistent with ?to al-most luxury?, suggesting a more complex correc-tion (changing the word ?become?
to ?are going?
).40Cascading NLP errors In this, I mean that wecan input this device implant into an animal orbirds species, for us to track their movements andactions relating to our human research that canbring us to a new regime.The word ?implant?
in the example sentencehas been identified as a verb by the system andnot a noun due to the unusual use as part of thephrase ?device implant?.
As a result, the systemincorrectly proposes the verb form correction ?im-planted?.6.2 DiscussionThe error analysis suggests that there are three sig-nificant challenges to developing a better gram-mar correction system for the CoNLL-2014 sharedtask: identifying candidate errors; modeling thecontext of possible errors widely enough to cap-ture long-distance cues where necessary; andmodeling stylistic preferences involving wordchoice, selection of plural or singular, standardsfor punctuation, use of a definite or indefinite arti-cle (or no article at all), and so on.
For ESL writ-ers, the tendency for multiple errors to be made inclose proximity means that global decisions mustbe made about sets of possible mistakes, and a sys-tem must therefore have a quite sophisticated ab-stract model to generate the basis for consistentsets of corrections to be proposed.7 ConclusionWe have described our system that participated inthe shared task on grammatical error correction.The system builds on the elements of the Illinoissystem that participated in last year?s shared task.We extended and improved the Illinois system inthree key dimensions, which we presented andevaluated in this paper.
We have also presentederror analysis of the system output and discussedpossible directions for future work.AcknowledgmentsThis material is based on research sponsored by DARPA un-der agreement number FA8750-13-2-0008.
The U.S. Gov-ernment is authorized to reproduce and distribute reprints forGovernmental purposes notwithstanding any copyright nota-tion thereon.
The views and conclusions contained herein arethose of the authors and should not be interpreted as necessar-ily representing the official policies or endorsements, eitherexpressed or implied, of DARPA or the U.S. Government.This research is also supported by a grant from the U.S. De-partment of Education and by the DARPA Machine ReadingProgram under Air Force Research Laboratory (AFRL) primecontract no.
FA8750-09-C-018.
The first and last authorswere partially funded by grant NPRP-4-1058-1-168 from theQatar National Research Fund (a member of the Qatar Foun-dation).
The statements made herein are solely the responsi-bility of the authors.ReferencesT.
Brants and A. Franz.
2006.
Web 1T 5-gram Version1.
Linguistic Data Consortium, Philadelphia, PA.A.
J. Carlson, J. Rosen, and D. Roth.
2001.
Scaling upcontext sensitive text correction.
In IAAI.D.
Dahlmeier and H.T.
Ng.
2012.
Better evaluationfor grammatical error correction.
In NAACL, pages568?572, Montr?eal, Canada, June.
Association forComputational Linguistics.D.
Dahlmeier, H.T.
Ng, and S.M.
Wu.
2013.
Buildinga large annotated corpus of learner english: The nuscorpus of learner english.
In Proc.
of the NAACLHLT 2013 Eighth Workshop on Innovative Use ofNLP for Building Educational Applications, Atlanta,Georgia, June.
Association for Computational Lin-guistics.R.
Dale and A. Kilgarriff.
2011.
Helping Our Own:The HOO 2011 pilot shared task.
In Proceedings ofthe 13th European Workshop on Natural LanguageGeneration.R.
Dale, I. Anisimoff, and G. Narroway.
2012.
Areport on the preposition and determiner error cor-rection shared task.
In Proc.
of the NAACL HLT2012 Seventh Workshop on Innovative Use of NLPfor Building Educational Applications, Montreal,Canada, June.
Association for Computational Lin-guistics.Marie-Catherine de Marneffe, Anna N. Rafferty, andChristopher D. Manning.
2008.
Finding contradic-tions in text.
In ACL.Yoav Freund and Robert E. Schapire.
1996.
Experi-ments with a new boosting algorithm.
In Proc.
13thInternational Conference on Machine Learning.A.
R. Golding and D. Roth.
1999.
A Winnowbased approach to context-sensitive spelling correc-tion.
Machine Learning.H.T.
Ng, S.M.
Wu, Y. Wu, C. Hadiwinoto, andJ.
Tetreault.
2013.
The conll-2013 shared taskon grammatical error correction.
In Proceedings ofthe Seventeenth Conference on Computational Nat-ural Language Learning: Shared Task, pages 1?12,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.41H.
T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.Susanto, and C. Bryant.
2014.
The CoNLL-2014shared task on grammatical error correction.
In Pro-ceedings of the Eighteenth Conference on Compu-tational Natural Language Learning: Shared Task,Baltimore, Maryland, USA, June.
Association forComputational Linguistics.V.
Punyakanok and D. Roth.
2001.
The use of classi-fiers in sequential inference.
In NIPS.A.
Rozovskaya and D. Roth.
2011.
Algorithm selec-tion and model adaptation for esl correction tasks.In ACL.A.
Rozovskaya and D. Roth.
2013.
Joint learningand inference for grammatical error correction.
InEMNLP, 10.A.
Rozovskaya, M. Sammons, and D. Roth.
2012.The UI system in the HOO 2012 shared task on er-ror correction.
In Proc.
of the Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics (NAACL) Workshopon Innovative Use of NLP for Building EducationalApplications.A.
Rozovskaya, K.-W. Chang, M. Sammons, andD.
Roth.
2013.
The University of Illinois systemin the CoNLL-2013 shared task.
In CoNLL SharedTask.A.
Rozovskaya, D. Roth, and V. Srikumar.
2014.
Cor-recting grammatical verb errors.
In EACL.42
