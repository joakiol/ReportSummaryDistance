Proceedings of NAACL-HLT 2013, pages 507?517,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsRobust Systems for Preposition Error Correction Using Wikipedia RevisionsAoife Cahill?, Nitin Madnani?, Joel Tetreault?
and Diane Napolitano??
Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA{acahill, nmadnani, dnapolitano}@ets.org?
Nuance Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085, USAJoel.Tetreault@nuance.comAbstractWe show that existing methods for trainingpreposition error correction systems, whetherusing well-edited text or error-annotated cor-pora, do not generalize across very differ-ent test sets.
We present a new, large error-annotated corpus and use it to train systemsthat generalize across three different test sets,each from a different domain and with differ-ent error characteristics.
This new corpus isautomatically extracted from Wikipedia revi-sions and contains over one million instancesof preposition corrections.1 IntroductionOne of the main themes that has defined the field ofautomatic grammatical error correction has been theavailability of error-annotated learner data to trainand test a system.
Some errors, such as determiner-noun number agreement, are easily corrected us-ing rules and regular expressions (Leacock et al2010).
On the other hand, errors involving the usageof prepositions and articles are influenced by sev-eral factors including the local context, the prior dis-course and semantics.
These errors are better han-dled by statistical models which potentially requiremillions of training examples.Most statistical approaches to grammatical errorcorrection have used one of the following trainingparadigms: 1) training solely on examples of cor-rect usage (Han et al 2006); 2) training on exam-ples of correct usage and artificially generated er-rors (Rozovskaya and Roth, 2010); and 3) trainingon examples of correct usage and real learner er-rors (Dahlmeier and Ng, 2011; Dale et al 2012).The latter two methods require annotated corpora oferrors, and while they have shown great promise,manually annotating grammatical errors in a largeenough corpus of learner writing is often a costlyand time-consuming endeavor.In order to efficiently and automatically acquire avery large corpus of annotated learner errors, we in-vestigate the use of error corrections extracted fromWikipedia revision history.
While Wikipedia re-vision history has shown promise for other NLPtasks including paraphrase generation (Max andWisniewski, 2010; Nelken and Yamangil, 2008) andspelling correction (Zesch, 2012), this resource hasnot been used for the task of grammatical error cor-rection.To evaluate the usefulness of Wikipedia revisionhistory for grammatical error correction, we addressthe task of correcting errors in preposition selection(i.e., where the context licenses the use of a prepo-sition, but the writer selects the wrong one).
Wefirst train a model directly on instances of correctand incorrect preposition usage extracted from theWikipedia revision data.
We also generate artificialerrors using the confusion distributions derived fromthis data.
We compare both of these approaches tomodels trained on well-edited text and evaluate eachon three test sets with a range of different character-istics.
Each training paradigm is applied to multipledata sources for comparison.
With these multipleevaluations, we address the following research ques-tions:1.
Across multiple test sets, which data source507is more useful for correcting preposition er-rors: a large amount of well-edited text, a largeamount of potentially noisy error-annotateddata (either artificially generated or automati-cally extracted) or a smaller amount of higherquality error-annotated data?2.
Given error-annotated data, is it better to trainon the corrections directly or to use the con-fusion distributions derived from these correc-tions for generating artificial errors in well-edited text?3.
What is the impact of having a mismatch in theerror distributions of the training and test sets?2 Related WorkIn this section, we only review work in preposi-tion error correction in terms of the three trainingparadigms and refer the reader to Leacock et al(2010) for a more comprehensive review of the field.2.1 Training on Well-Edited TextEarly approaches to error detection and correctiondid not have access to large amounts of error-annotated data to train statistical models and thus,systems were trained on millions of well-edited ex-amples from news text instead (Gamon et al 2008;Tetreault and Chodorow, 2008; De Felice and Pul-man, 2009).
Feature sets usually consisted of n-grams around the preposition, POS sequences, syn-tactic features and semantic information.
Since themodel only had knowledge of correct usage, an errorwas flagged if the system?s prediction for a particu-lar preposition context differed from the prepositionthe writer used.2.2 Artificial ErrorsThe issue with training solely on correct usage wasthat the systems had no knowledge of typical learnererrors.
Ideally, a system would be trained on ex-amples of correct and incorrect usage, however, formany years, such error-annotated corpora were notavailable.
Instead, several researchers generated ar-tificial errors based on the error distributions derivedfrom the error-annotated learner corpora available atthe time.
Izumi et al(2003) was the first to evaluatea model trained on incorrect usage as well as artifi-cial errors for the task of correcting several differenterror types, including prepositions.
However, withlimited training data, system performance was quitepoor.
Rozovskaya and Roth (2010) evaluated dif-ferent ways of generating artificial errors and foundthat a system trained on artificial errors could outper-form the more traditional training paradigm of usingonly well-edited texts.
Most recently, Imamura et al(2012) showed that performance could be improvedby training a model on artificial errors and address-ing domain adaptation for the task of Japanese par-ticle correction.2.3 Error-Annotated Learner CorporaRecently, error-annotated learner data has becomemore readily and publicly available allowing modelsto be trained on both examples of correct usage aswell typical learner errors.
Han et al(2010) showedthat a preposition error detection and correction sys-tem trained on 100,000 annotated preposition errorsfrom the Chungdahm Corpus of Korean Learner En-glish (in addition to 1 million examples of correctusage) outperformed a model trained only on 5 mil-lion examples of correct usage.
Gamon (2010) andDahlmeier and Ng (2011) showed that combiningmodels trained separately on examples of correctand incorrect usage could also improve the perfor-mance of a preposition error correction system.3 Mining Wikipedia Revisions forGrammatical Error Corrections3.1 Related WorkMany NLP researchers have taken advantage of thewealth of information available in Wikipedia revi-sions.
Dutrey et al(2011) define a typology of mod-ifications found in the French Wikipedia (WiCo-PaCo).
They show that the kinds of edits made rangefrom specific lexical changes to more general rewriteedits.
Similar types of edits are found in the En-glish Wikipedia.
The data extracted from Wikipediarevisions has been used for a wide variety of tasksincluding spelling correction (Max and Wisniewski,2010; Zesch, 2012), lexical error detection (Nelkenand Yamangil, 2008), sentence compression (Ya-mangil and Nelken, 2008), paraphrase generation(Max and Wisniewski, 2010; Nelken and Yamangil,2008), lexical simplification (Yatskar et al 2010)and entailment (Zanzotto and Pennacchiotti, 2010;508(1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at?
in) the refrigerator.
(2) [Wiki clean] Also none of the witnesses present (of?
on) those dates supports Ranneft?s claims.
(3) [Wiki dirty] .
.
.
cirque has a permanent production (to?
at) the Mirage, love.
(4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in hisperformances for French tourists (in?
to) Petersburg.Figure 1: Example sentences with preposition errors extracted from Wikipedia revisions.
The second preposition isassumed to be the correction.Cabrio et al 2012).
To our knowledge, no one haspreviously extracted data for training a grammaticalerror detection system from Wikipedia revisions.3.2 Extracting Preposition Correction Datafrom Wikipedia RevisionsAs the source of our Wikipedia revisions, we used anXML snapshot of Wikipedia generated in July 2011containing 8,735,890 articles and 288,583,063 revi-sions.1 We then used the following process to ex-tract preposition errors and their corresponding cor-rections from this snapshot:Step 1: Extract the plain text versions of all revi-sions of all articles using the Java WikipediaLibrary (Ferschke et al 2011).Step 2: For each Wikipedia article, compare eachrevision with the revision immediately preced-ing it using an efficient diff algorithm.2Step 3: Compute all 1-word edit chains for the arti-cle, i.e., sequences of related edits derived fromall revisions of the same article.
For example,say revision 10 of an article inserts the preposi-tion of into a sentence and revision 12 changesthat preposition to on.
Assuming that no otherrevisions change this sentence, the correspond-ing edit chain would contain the following 3 el-ements: ?of?on.
The extracted chains con-tain the full context on either side of the 1-wordedit, up to the automatically detected sentenceboundaries.Step 4: (a) Ignore any circular chains, i.e., wherethe first element in the edit chain is the same asthe last element.
(b) Collapse all non-circular1http://dumps.wikimedia.org/enwiki/2http://code.google.com/p/google-diff-match-patch/chains, i.e., only retain the first and the last ele-ments in a chain.
Both these decisions are mo-tivated by the assumption that the intermediatelinks in the chain are unreliable for training anerror correction system since a Wikipedia con-tributor modified them.Step 5 : From all remaining 2-element chains, findthose where a preposition is replaced with an-other preposition.
If the preposition edit is theonly edit in the sentence, we convert the chaininto a sentence pair and label it clean.
If thereare other 1-word edits but not within 5 words ofthe preposition edit on either side, we label thesentence somewhat clean.
Otherwise, we labelit dirty.
The motivation is that the presence ofother nearby edits make the preposition correc-tion less reliable when used in isolation, due tothe possible dependencies between corrections.All extracted sentences were part-of-speech taggedusing the Stanford Tagger (Toutanova et al 2003).Using the above process, we are able to extract ap-proximately 2 million sentences containing preposi-tions errors and their corrections.
Some examplesof the sentences we extracted are given in Figure 1.Example (4) shows an example of a bad correction.4 CorporaWe use several corpora for training and testing ourpreposition error correction system.
The proper-ties of each are outlined in Table 1, organized byparadigm.
For each corpus we report the total num-ber of prepositions used for training, as well as thenumber and percentage of preposition corrections.4.1 Well-edited TextWe train our system on two well-edited corpora.The first is the same corpus used by Tetreault and509Corpus Total # Preps # Corrected PrepsWell-edited TextWikipedia Snapshot (10m sents) 26,069,860 0 (0%)Lexile/SJM 6,719,077 0 (0%)Artificially GeneratedErrorsWikipedia Snapshot 26,127,464 2,844,227 (10.9%)Lexile/SJM 6,723,206 792,195 (11.8%)Naturally OccurringErrorsWikipedia Revisions All 7,125,317 1,027,643 (20.6%)Wikipedia Revisions ?Clean 3,001,900 381,644 (12.7%)Wikipedia Revisions Clean 1,978,802 266,275 (14.4%)Lang-8 129,987 53,493 (41.2%)NUCLE Train 72,741 922 (1.3%)Test CorporaNUCLE Test 9,366 125 (1.3%)FCE 33,243 2,900 (8.7%)HOO 2011 Test 1,703 81 (4.8%)Table 1: Corpora characteristicsChodorow (2008), comprising roughly 1.8 millionsentences from the San Jose Mercury News Corpus3and roughly 1.8 million sentences from grades 11and 12 of the MetaMetrics Lexile Corpus.
Our sec-ond corpus is a random sample of 10 million sen-tences containing at least one preposition from theJune 2012 snapshot of English Wikipedia Articles.44.2 Artificially Generated ErrorsSimilar to Foster and Andersen (2009) and Ro-zovskaya and Roth (2010), we artificially introducepreposition errors into well-edited corpora (the twodescribed above).
We do this based on a distribu-tion of possible confusions and train a model thatis aware of the corrections.
The two sets of con-fusion distributions we used were derived based onthe errors extracted from Wikipedia revisions andLang-8 respectively (discussed in Section 4.3).
Foreach corrected preposition pi in the revision data,we calculated P (pi|pj), where pj is each of the pos-sible original prepositions that were confused withpi.
Then, for each sentence in the well-edited text,all prepositions are extracted.
A preposition is ran-domly selected (without replacement) and changedbased on the distribution of possible confusions(note that the original preposition is also includedin the distribution, usually with a high probabil-3The San Jose Mercury News is available from the Linguis-tic Data Consortium (catalog number LDC93T3A).4We used a newer version of the Wikipedia text for the well-edited text, since we assume that more recent versions of thetext will be most grammatical, and therefore closer to well-edited.ity, meaning that there is a strong preference not tochange the preposition).
If a preposition is changedto something other than the original preposition, allremaining prepositions in the sentence are left un-changed.4.3 Naturally Occurring ErrorsWe have a number of corpora that contain annotatedpreposition errors.
Note that we are only consideringincorrectly selected prepositions, we do not considermissing or extraneous.NUCLE The NUS Corpus of Learner English (NU-CLE)5 contains one million words of learneressay text, manually annotated with error tagsand corrections.
We use the same training, devand test splits as Dahlmeier and Ng (2011).FCE The CLC FCE Dataset6 is a collection of1,244 exam scripts written by learners of En-glish as part of the Cambridge ESOL First Cer-tificate in English (Yannakoudakis et al 2011).It includes demographic metadata about thecandidate, a grade for each essay and manually-annotated error corrections.Wikipedia We use three versions of the preposi-tion errors extracted from the Wikipedia revi-sions as described in Section 3.2.
The first in-cludes corrections where the preposition wasthe only word corrected in the entire sentence5http://bit.ly/nuclecorpus6http://ilexir.co.uk/applications/clc-fce-dataset/510(clean).
The second contains all clean cor-rections, as well as all corrections where therewere no other edits within a five-word span oneither side of the preposition (?clean).
Thethird contains all corrections regardless of anyother changes in the surrounding context (all).Lang-8 The Lang-8 website contains journals writ-ten by language learners, where native speakershighlight and correct errors on a sentence-by-sentence basis.
As a result, it contains typicalgrammatical mistakes made by language learn-ers, which can be easily downloaded.
We auto-matically extract 75,622 sentences with prepo-sition errors and corrections from the first mil-lion journal entries.7HOO 2011 We take the test set from the HOO 2011shared task (Dale and Kilgarriff, 2011) and ex-tract all examples of preposition selection er-rors.
The texts are fragments of ACL papersthat have been manually annotated for gram-matical errors.8It is important to note that the three test sets we useare from entirely different domains: exam scriptsfrom non-native English speakers (FCE), essays byhighly proficient college students in Singapore (NU-CLE) and ACL papers (HOO).
In addition, they havea different number of total prepositions as well as er-roneous prepositions.5 Preposition Error CorrectionExperimentsWe use the preposition error correction model de-scribed in Tetreault and Chodorow (2008)9 to eval-uate the many ways of using Wikipedia error cor-rections as described in the Section 4.
We use thissystem since it has been recreated for other work(Dahlmeier and Ng, 2011; Tetreault et al 2010) andis similar in methodology to Gamon et al(2008)7Tajiri et al(2012) extract a corpus of English verb phrasescorrected for tense/aspect errors from Lang-8.
They kindly pro-vided us with their scripts to carry out the scraping of Lang-8.8The results of the HOO 2011 shared task were not reportedat level of preposition selection error, therefore it is not possibleto compare the results presented in this paper with those results.9Note that in that work, the model was evaluated in terms ofpreposition error detection rather than correction, however themodel itself does not change.and De Felice and Pulman (2009).
In short, themethod models the problem of preposition error cor-rection (for replacement errors) as a 36-way classifi-cation problem using a multinomial logistic regres-sion model.10 The system uses 25 lexical, syntac-tic and n-gram features derived from the contexts ofeach preposition training instance.We modified the training paradigm of Tetreaultand Chodorow (2008) so that a model could betrained on examples of correct usage as well as ac-tual errors.
We did this by adding a new featurespecifying the writer?s original preposition (as inHan et al(2010) and Dahlmeier and Ng (2011)).5.1 ResultsWe train a preposition correction system using eachof the three data paradigms and test on the FCE,NUCLE and HOO 2011 test corpora.
For eachpreposition in the test corpus, we record whetherthe system predicted that it should be changed,and if so, what it should be changed to.
We thencompare the prediction to the annotation in the testcorpus.
We report results in terms of f-score, whereprecision and recall are calculated as follows:11Precision = Number of correct preposition correctionsTotal number of corrections suggestedRecall = Number of correct preposition correctionsTotal number of corrections in test setNote that due to the high volume of unchangedprepositions in the test corpus, we obtain very highaccuracies, which are not indicative of true perfor-mance, and are not included in our results.The results of our experiments are presented inTable 2.12 The first part of the table shows the f-scores of preposition error correction systems that10We use liblinear (Fan et al 2008) with the L1-regularizedlogistic regression solver and default parameters.11As Chodorow et al(2012) note, it is not clear how to han-dle cases where the system predicts a preposition that is neitherthe same as the writer preposition nor the correct preposition.We count these cases as false positives.12No thresholds were used in the systems that were trainedon well-edited text.
Traditionally, thresholds are applied so asto only predict a correction when the system is highly confident.This has the effect of increasing precision at the cost of recall,and sometimes leads to an overall improved f-score.
Here wetake the prediction of the system, regardless of the confidence,reflecting a lower-bound of this method.511Data Source Paradigm CLC-FCE NUCLE HOO2011N=33,243 N=9,366 N=1,703WithoutWikipediaRevisions(nonWikiRev)Wikipedia Snapshot Well-edited Text 24.43?
5.02?
12.36?Lexile/SJM Well-edited Text 24.73?
4.29?
9.73?Wikipedia Snapshot Artificial Errors (Lang-8) 42.15?
19.91?
28.75Lexile/SJM Artificial Errors (Lang-8) 45.36 18.00?
25.15Lang-8 Error-annotated Text 38.22?
8.18?
24.00NUCLE train Error-annotated Text 5.38?
20.14 4.82?WithWikipediaRevisions(WikiRev)Wikipedia Snapshot Artificial Errors (Wiki) 31.17?
24.52 28.30Lexile/SJM Artificial Errors (Wiki) 34.35?
23.38 32.76Wikipedia Revisions All Error-annotated Text 33.59?
26.39 36.84Wikipedia Revisions ?Clean Error-annotated Text 29.68?
22.13 36.04Wikipedia Revisions Clean Error-annotated Text 28.09?
21.74 28.30Table 2: Preposition selection error correction results (f-score).
The systems with scores in bold are statisticallysignificantly better than all systems marked with an asterisk (p < 0.01).
Confidence intervals were obtained usingbootstrap resampling with 50,000 replicates.one might be able to train with publicly availabledata excluding the Wikipedia revisions that we haveextracted.
We refer to these systems as nonWikiRevsystems.
The second part of the table shows the f-scores of systems trained on the Wikipedia revisionsdata ?
either directly on the annotated errors or onthe artificial errors produced using the confusion dis-tributions derived from these annotated errors.
Werefer to this second set of systems as WikiRev sys-tems.
The nonWikiRev systems perform inconsis-tently, heavily dependent on the characteristics ofthe test set in question.
On the other hand, it isobvious that the WikiRev systems ?
while not al-ways outperforming the best nonWikiRev systems?
generalize much better across the three test sets.In fact, for the NUCLE test set, the best WikiRevsystem performs as well as the nonWikiRev systemtrained on data from the same domain and with iden-tical error characteristics as the test set.
The distri-butions of errors in the three test sets are not sim-ilar, and therefore, the stability in performance ofthe WikiRev systems cannot be attributed to the hy-pothesis that the WikiRev training data error distri-butions are more similar to the test data than any ofthe other training corpora.
Therefore, we claim thatif a preposition error correction system is to be de-ployed on data for which the error characteristics arenot known in advance, i.e.
most real-world scenar-ios, training the system using Wikipedia revisions islikely to be the most robust option.6 DiscussionWe examine the results of our experiments in lightof the research questions we posed in Section 1.6.1 Which Data Source is More Useful?We wanted to know whether it was better to havea smaller corpus of carefully annotated corrections,or a much larger (but automatically generated, andtherefore noisier) error-annotated corpus.
We alsowanted to compare this scenario to training on largeamounts of well-edited text.
From our experiments,it is clear that the composition of the test set playsa major role in answering this question.
On a testset with few corrections (NUCLE), training on well-edited text (and without using thresholds) performsparticularly poorly.
On the other hand, when eval-uating on the FCE test set which contains far moreerrors, training on well-edited text performs reason-ably well (though statistically significantly worsethan training on all of the Wikipedia errors).
Sim-ilarly, training on the smaller, high-quality NU-CLE corpus and evaluating on the NUCLE test setachieves good results, however training on NUCLEand testing on FCE achieves the lowest f-score of allour systems on that test set.Figure 2 shows the learning curves obtained byincreasing the size of the training data for twoof the test sets.13 Although one might assume13For space reasons, the graph for HOO2011 is omitted.
Alsonote that the results in Table 2 may not appear in the graph,512Wiki (All)Wiki (Clean)Lang-8NUCLELexile (artificial via Wiki)Lexile (artificial via Lang-8)F-score01020304050log(training data size in thousands of instances)1 2 3 4Wiki (All)Wiki (Clean)Lang-8NUCLELexile (artificial via Wiki)Lexile (artificial via Lang-8)F-score05101520251 2 3 4(a) NUCLE(b) FCEFigure 2: The effect of varying the size of the training corpusthat Wikipedia-clean would be more reliable thanWikipedia-all, the cleanness of the Wikipedia dataseems to make very little difference, probably be-cause the data extracted in the dirty contexts is notas noisy as we expected.
Interestingly, it also seemsthat additional data would lead to further improve-ments for models trained on artificial errors in Lexiledata and for those trained on all of the automaticallyextracted Wikipedia errors.Another interesting aspect of Figure 2 is thatsince we were sampling at specific data points which did notcorrespond exactly to the total sizes of the training corpora.training on the Lang-8 data shows a very steep risingtrend.
This suggests that automatically-scraped datathat is highly targeted towards language learners isvery useful in correcting preposition errors in textswhere they are reasonably frequent.6.2 Natural or Artificially Generated Errors?Table 2 shows that training on artificially generatederrors via Wikipedia revisions performs fairly con-sistently across test corpora.
While using Lang-8for artificial error generation is also quite promis-ing for FCE, it does not generalize across test sets.513Wiki (All)Wiki (Clean)Lang-8Lexile (artificial via Wiki)Lexile (artificial via Lang-8)F-score01020304050Percentage of Errors in Training Data0 5 10 15 20 25 30 35 40 45 50 55Wiki (All)Wiki (Clean)Lang-8Lexile (artificial via Wiki)Lexile (artificial via Lang-8)F-score510152025300 5 10 15 20 25 30 35 40 45 50 55(a) NUCLE(b) FCEFigure 3: The effect of varying the percentage of errors in the training corpusOn FCE it achieves the highest results, on NUCLEit performs statistically significantly worse than thebest system, and on HOO 2011 it achieves a lower(though not statistically significant) result than thebest system.
This highlights that extracting errorsfrom Wikipedia is useful in two ways: (1) training asystem on the errors alone works well and (2) gener-ating artificial errors in well-edited corpora of differ-ent domains and training a system on that also workswell.
It also indicates that if the system were to beapplied to a specific domain, applying the confusiondistributions to a domain specific corpus ?
if avail-able ?
would likely yield the best results.6.3 Mismatching DistributionsThe proportion of errors in the training and test dataplays an important role in the performance of anypreposition error correction system.
This is clearlyevident by comparing system performances acrossthe three test sets which have fairly different compo-sitions.
FCE contains a much higher proportion oferrors than NUCLE, and HOO falls somewhere inbetween.
Interestingly, the system trained on Lang-8 data (which contains the highest proportion of er-514rors among all training corpora) performs best onthe FCE data.
On the other hand, the same sys-tem performs poorly on NUCLE test which containsfar fewer errors.
In this instance, the system learnsto predict an incorrect preposition too often.
Wesee a similar pattern with the system trained on theNUCLE training data.
It performs poorly on FCEwhich contains many errors, but well on NUCLEtest which contains a similar proportion of errors.In order to better understand the relationship be-tween the percentage of errors in the training dataand system performance, we vary the percentage oferrors in each training corpus from 1-50% and teston the unchanged FCE and NUCLE test corpora.For each training corpus, we reduce the size to betwice the size of the total number of errors.14 Keep-ing this size constant, we then artificially change thepercentage of errors.
Note that because the total sizeof the corpus has changed, the results in Table 2 maynot appear in the graph.
Figure 3 shows the effect onf-score when the data composition is changed.
Forboth test sets, there is a peak after which increas-ing the proportion of errors in the training corpus isdetrimental.
For NUCLE test with its low numberof preposition errors, this peak is very pronounced.For FCE, it is more of a gentle degradation in per-formance, but the pattern is clear.
Also noteworthyis the fact that the degradation for models trained onartificial errors is less steep suggesting that they maybe more stable across test sets.In general, these results indicate that whenbuilding a preposition error detection using error-annotated data, the characteristics of the data towhich the system will be applied should play a vitalrole in how the system is to be trained.
Our resultsshow that the WikiRev systems are robust acrosstest sets, however if the exact distribution of errorsin the data is known in advance, other models mayperform better.7 ConclusionAlthough previous approaches to preposition er-ror correction using either well-edited text or smallhand-annotated corrections performed well on somespecific test set, they did not generalize well across14We omit the NUCLE train corpus from this comparison,because it contains too few errors to obtain a meaningful result.very different test sets.
In this paper, we presentwork that automatically extracts preposition errorcorrections from Wikipedia Revisions and uses itto build robust error correction systems.
We showthat this data is useful for two purposes.
Firstly, amodel trained directly on the corrections performswell across test sets.
Secondly, models trained on ar-tificial errors generated from the distribution of con-fusions in the Wikipedia data perform equally well.The distribution of confusions can also be applied toother well-edited corpora in different domains, pro-viding a very powerful method of automatically gen-erating error corpora.
The results of our experimentsalso highlight the importance of the distribution ofexpected errors in the test set.
Models that performwell on one kind of distribution may not necessar-ily work on a completely different one, as evidentin the performances of the systems trained on eitherLang-8 or NUCLE.
In general, the WikiRev mod-els perform well across distributions.
We also con-ducted some preliminary system combination exper-iments and found that while they yielded promisingresults, further investigation is necessary.
We havealso made the Wikipedia preposition correction cor-pus available for download.15In future work, we will examine whether theresults we obtain for English generalize to otherWikipedia languages.
We also plan to extract multi-word corrections for other types of errors and to ex-amine the usefulness of including error contexts inour confusion distributions (e.g., preposition confu-sions following verbs versus those following nouns).AcknowledgmentsThe authors would like to thank Daniel Dahlmeier,Torsten Zesch, Mamoru Komachi, Tajiri Toshikazu,Tomoya Mizumoto and Yuji Matsumoto for provid-ing scripts and data that enabled us to carry outthis research.
We would also like to thank MartinChodorow and the anonymous reviewers for theirhelpful suggestions and comments.ReferencesElena Cabrio, Bernardo Magnini, and Angelina Ivanova.2012.
Extracting Context-Rich Entailment Rules from15http://bit.ly/etsprepdata515Wikipedia Revision History.
In Proceedings of the 3rdWorkshop on the People?s Web Meets NLP: Collabora-tively Constructed Semantic Resources and their Ap-plications to NLP, pages 34?43, Jeju, Republic of Ko-rea, July.
Association for Computational Linguistics.Martin Chodorow, Markus Dickinson, Ross Israel, andJoel Tetreault.
2012.
Problems in Evaluating Gram-matical Error Detection Systems.
In Proceedings ofCOLING 2012, pages 611?628, Mumbai, India, De-cember.
The COLING 2012 Organizing Committee.Daniel Dahlmeier and Hwee Tou Ng.
2011.
Grammat-ical Error Correction with Alternating Structure Op-timization.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 915?923, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Robert Dale and Adam Kilgarriff.
2011.
Helping OurOwn: The HOO 2011 Pilot Shared Task.
In Pro-ceedings of the Generation Challenges Session at the13th European Workshop on Natural Language Gener-ation, pages 242?249, Nancy, France, September.
As-sociation for Computational Linguistics.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A Report on the Prepositionand Determiner Error Correction Shared Task.
InProceedings of the Seventh Workshop on BuildingEducational Applications Using NLP, pages 54?62,Montre?al, Canada, June.
Association for Computa-tional Linguistics.Rachele De Felice and Stephen G. Pulman.
2009.
Auto-matic detection of preposition errors in learner writing.CALICO Journal, 26(3):512?528.Camille Dutrey, Houda Bouamor, Delphine Bernhard,and Aure?lien Max.
2011.
Local modifications andparaphrases in Wikipedias revision history.
SEPLNjournal(Revista de Procesamiento del Lenguaje Nat-ural), 46:51?58.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.2011.
Wikipedia Revision Toolkit: Efficiently Access-ing Wikipedia?s Edit History.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies.System Demonstrations.Jennifer Foster and Oistein Andersen.
2009.
Gen-ERRate: Generating Errors for Use in GrammaticalError Detection.
In Proceedings of the Fourth Work-shop on Innovative Use of NLP for Building Educa-tional Applications, pages 82?90, Boulder, Colorado,June.
Association for Computational Linguistics.Michael Gamon, Jianfeng Gao, Chris Brockett, AlexKlementiev, William B. Dolan, Dmitriy Belenko, andLucy Vanderwende.
2008.
Using Contextual SpellerTechniques and Language Modeling for ESL ErrorCorrection.
In Proceedings of the International JointConference on Natural Language Processing (IJC-NLP), pages 449?456, Hyderabad, India.Michael Gamon.
2010.
Using Mostly Native Data toCorrect Errors in Learners?
Writing.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 163?171, Los An-geles, California, June.
Association for ComputationalLinguistics.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineering,12(2):115?129.Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-Young Ha.
2010.
Using Error-Annotated ESL Datato Develop an ESL Error Correction System.
In Pro-ceedings of the Seventh International Conference onLanguage Resources and Evaluation (LREC), Malta.Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, andHitoshi Nishikawa.
2012.
Grammar Error Correc-tion Using Pseudo-Error Sentences and Domain Adap-tation.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 388?392, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, ThepchaiSupnithi, and Hitoshi Isahara.
2003.
Automatic ErrorDetection in the Japanese Learners?
English SpokenData.
In The Companion Volume to the Proceedingsof 41st Annual Meeting of the Association for Compu-tational Linguistics, pages 145?148, Sapporo, Japan,July.
Association for Computational Linguistics.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated GrammaticalError Detection for Language Learners.
SynthesisLectures on Human Language Technologies.
MorganClaypool.Aure?lien Max and Guillaume Wisniewski.
2010.
MiningNaturally-occurring Corrections and Paraphrases fromWikipedia?s Revision History.
In Nicoletta Calzo-lari (Conference Chair), Khalid Choukri, Bente Mae-gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,Mike Rosner, and Daniel Tapias, editors, Proceed-ings of the Seventh conference on International Lan-guage Resources and Evaluation (LREC?10), Valletta,Malta, may.
European Language Resources Associa-tion (ELRA).Rami Nelken and Elif Yamangil.
2008.
MiningWikipedias Article Revision History for Training516Computational Linguistics Algorithms.
In Proceed-ings of the 1st AAAI Workshop on Wikipedia and Arti-ficial Intelligence, pages 31?36, Chicago, IL.Alla Rozovskaya and Dan Roth.
2010.
Generating Con-fusion Sets for Context-Sensitive Error Correction.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 961?970, Cambridge, MA, October.
Association for Com-putational Linguistics.Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-sumoto.
2012.
Tense and Aspect Error Correctionfor ESL Learners Using Global Context.
In Proceed-ings of the 50th Annual Meeting of the Association forComputational Linguistics (ACL), Short Papers, pages198?202, Jeju Island, Korea.Joel R. Tetreault and Martin Chodorow.
2008.
TheUps and Downs of Preposition Error Detection inESL Writing.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics (Col-ing 2008), pages 865?872, Manchester, UK.Joel Tetreault, Jennifer Foster, and Martin Chodorow.2010.
Using Parse Features for Preposition Selectionand Error Detection.
In Proceedings of the ACL 2010Conference Short Papers, pages 353?358, Uppsala,Sweden, July.
Association for Computational Linguis-tics.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich Part-of-speechTagging with a Cyclic Dependency Network.
In Pro-ceedings of NAACL, pages 173?180.Elif Yamangil and Rani Nelken.
2008.
MiningWikipedia Revision Histories for Improving SentenceCompression.
In Proceedings of ACL-08: HLT, ShortPapers, pages 137?140, Columbus, Ohio, June.
Asso-ciation for Computational Linguistics.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A New Dataset and Method for Automati-cally Grading ESOL Texts.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 180?189, Portland, Oregon, USA, June.
Associ-ation for Computational Linguistics.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of simplic-ity: Unsupervised extraction of lexical simplificationsfrom Wikipedia.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, pages 365?368, Los Angeles, California, June.Association for Computational Linguistics.Fabio Massimo Zanzotto and Marco Pennacchiotti.2010.
Expanding textual entailment corpora fromWikipedia using co-training.
In Proceedings of the2nd Workshop on The People?s Web Meets NLP: Col-laboratively Constructed Semantic Resources, pages28?36, Beijing, China, August.
Coling 2010 Organiz-ing Committee.Torsten Zesch.
2012.
Measuring Contextual Fitness Us-ing Error Contexts Extracted from the Wikipedia Revi-sion History.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 529?538, Avignon, France,April.
Association for Computational Linguistics.517
