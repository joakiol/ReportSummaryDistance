Stochastic Modelling: From Pattern Classificationto Language TranslationHermann NeyLehrstuhl fu?r Informatik VI, Computer Science DepartmentRWTH Aachen ?
University of TechnologyD-52056 Aachen, Germanyney@informatik.rwth-aachen.deAbstractThis paper gives an overview ofthe stochastic modelling approach tomachine translation.
Starting withthe Bayes decision rule as in patternclassification and speech recognition,we show how the resulting systemarchitecture can be structured into threeparts: the language model probability,the string translation model probabilityand the search procedure that gener-ates the word sequence in the targetlanguage.
We discuss the propertiesof the system components and reportresults on the translation of spokendialogues in the VERBMOBIL project.The experience obtained in the VERB-MOBIL project, in particular a large-scale end-to-end evaluation, showedthat the stochastic modelling approachresulted in significantly lower errorrates than three competing translationapproaches: the sentence error rate was29% in comparison with 52% to 62%for the other translation approaches.1 IntroductionThe use of statistics in computational linguisticshas been extremely controversial for more thanthree decades.
The controversy is very wellsummarized by the statement of Chomsky in 1969(Chomsky 1969):?It must be recognized that the notion ofa ?probability of a sentence?
is an entirelyuseless one, under any interpretation of thisterm?.This statement was considered to be correct bythe majority of experts from artificial intelligenceand computational linguistics, and the conceptof statistics was banned from computationallinguistics for many years.What is overlooked in this statement is thefact that, in an automatic system for speechrecognition or language translation, we are facedwith the problem of taking decisions.
It isexactly here where statistical decision theorycomes in.
In automatic speech recognition (ASR),the success of the statistical approach is based onthe equation:ASR = Acoustic?Linguistic Modelling+ Statistical Decision TheorySimilarly, for machine translation (MT), thestatistical approach is expressed by the equation:MT = Linguistic Modelling+ Statistical Decision TheoryFor the ?low-level?
description of speech andimage signals, it is widely accepted that thestochastic framework allows an efficient couplingbetween the observations and the models, whichis often described by the buzz word ?subsymbolicprocessing?.
But there is another advantage inusing probability distributions in that they offer anexplicit formalism for expressing and combininghypothesis scores:?
The probabilities are directly used as scores:These scores are normalized, which is adesirable property: when increasing thescore for a certain element in the set of allhypotheses, there must be one or severalother elements whose scores are reduced atthe same time.?
It is evident how to combine scores:depending on the task, the probabilities areeither multiplied or added.?
Weak and vague dependencies can bemodelled easily.
Especially in spoken andwritten natural language, there are nuancesand shades that require ?grey levels?
between0 and 1.Even if we think we can manage withoutstatistics, we will need models which alwayshave some free parameters.
Then the question ishow to train these free parameters.
The obviousapproach is to adjust these parameters in sucha way that we get optimal results in terms oferror rates or similar criteria on a representativesample.
So we have made a complete cycle andhave reached the starting point of the stochasticmodelling approach again!When building an automatic system for speechor language, we should try to use as muchprior knowledge as possible about the taskunder consideration.
This knowledge is usedto guide the modelling process and to enableimproved generalization with respect to unseendata.
Therefore in a good stochastic modellingapproach, we try to identify the common patternsunderlying the observations, i.e.
to capturedependencies between the data in order to avoidthe pure ?black box?
concept.2 Language Translation as PatternClassification2.1 Bayes Decision RuleKnowing that language translation is a difficulttask, we want to keep the number of wrongtranslations as small as possible.
The corre-sponding formalism is provided by the so-calledstatistical decision theory.
The resulting decisonrule is referred to as Bayes decision rule and isthe starting point for many techniques in patternclassification (Duda et al 2001).
To classifyan observation vector y into one out of severalclasses c, the Bayes decision rule is:c?
= argmaxc {Pr(c|y)}= argmaxc {Pr(c) ?
Pr(y|c)} .For language translation, the starting point is theobserved sequence of source symbols y = fJ1 =f1...fJ , i.e.
the sequence of source words, forwhich the target word sequence c = eI1 = e1...eIhas to be determined.
In order to minimize thenumber of decision errors at the sentence level,we have to choose the sequence of target wordse?I1 according to the equation (Brown et al 1993):e?I1 = argmaxeI1{Pr(eI1|fJ1 )}= argmaxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)}.Here, the posterior probability Pr(eI1|fJ1 ) isdecomposed into the language model probabilityPr(eJ1 ) and the string translation probabilityPr(fJ1 |eI1).
Due to this factorization, we havetwo separate probability distributions which canbe modelled and trained independently of eachother.Fig.1 shows the architecture that results fromthe Bayes decision theory.
Here we have alreadytaken into account that, in order to implementthe string translation model, we will decomposeit into a so-called alignment model and a lexiconmodel.
As also shown in this figure, we explicitlyallow for optional transformations to make thetranslation task simpler for the algorithm.In total, we have the following crucialconstituents of the stochastic modelling approachto language translation:?
There are two separate probability distribu-tions or stochastic knowledge sources:?
the language model distributionPr(eI1), which is assigned to eachpossible target word sequence eI1 andwhich ultimately captures all syntactic,semantic and pragmatic constraintsof the target language domain underconsideration;?
the string translation probability dis-tribution Pr(fJ1 |eI1) which assigns ascore as to how well the source stringfJ1 matches the hypothesized targetsequence eI1.?
In addition to these two knowledge sources,we need another system component which isreferred to as a search or decision process.According to the Bayes decision rule, thissearch has to carry out the maximizationof the product of the two probabilitydistributions and thus ensures an optimalinteraction of the two knowledge sources.Source Language TextTransformationLexicon ModelLanguage ModelGlobal Search:Target Language TextoverPr(f1  J  |e1I )Pr(   e1I )Pr(f1  J  |e1I )   Pr(   e1I )e1If1 Jmaximize  Alignment ModelTransformationFigure 1: Bayes architecture for languagetranslation.Note that there is a guarantee of the minimizationof decision errors if we know the true probabilitydistributions Pr(eI1) and Pr(fJ1 |eI1) and if wecarry out a full search over all target wordsequences eI1.
In addition, it should be notedthat both the sequence of source words fJ1 andthe sequence of unknown target words eI1 aremodelled as a whole.
The advantage then isthat context dependencies can be fully taken intoaccount and the syntactic analysis of both sourceand target sequences (at least in principle) can beintegrated into the translation process.2.2 Implementation of Stochastic ModellingTo build a real operational system for languagetranslation, we are faced with the following threeproblems:?
Search problem:In principle, the innocent looking maximiza-tion requires the evaluation of 20 00010 =1043 possible target word sequences, whenwe assume a vocabulary of 20 000 targetwords and a sentence length of I =10 words.
This is the price we haveto pay for a full interaction between thelanguage model Pr(eI1) and and the stringtranslation model Pr(fJ1 |eI1).
In such away, however, it is guaranteed that there isno better way to take the decisions aboutthe words in the target language (for thegiven probability distributions Pr(eI1) andPr(fJ1 |eI1)).
In a practical system, weof course use suboptimal search strategieswhich require much less effort than a fullsearch, but nevertheless should find theglobal optimum in virtually all cases.?
Modelling problem:The two probability distributions Pr(eI1)and Pr(fJ1 |eI1) are too general to be usedin a table look-up approach, because thereis a huge number of possible values fJ1and eI1.
Therefore we have to introducesuitable structures into the distributionssuch that the number of free parameters isdrastically reduced by taking suitable datadependencies into account.A key issue in modelling the string transla-tion probability Pr(fJ1 |eI1) is the questionof how we define the correspondencebetween the words of the target sentenceand the words of the source sentence.In typical cases, we can assume a sortof pairwise dependence by considering allword pairs (fj , ei) for a given sentencepair (fJ1 ; eI1).
Typically, the dependence isfurther constrained by assigning each sourceword to exactly one target word.
Modelsdescribing these types of dependencies arereferred to as alignment mappings (Brown etal.
1993):alignment mapping: j ?
i = aj ,which assigns a source word fj in position jto a target word ei in position i = aj .
As aresult, the string translation probability canbe decomposed into a lexicon probabilityand an alignment probability (Brown et al1993).?
Training problem:After choosing suitable models for the twodistributions Pr(eI1) and Pr(fJ1 |eI1), thereremain free parameters that have to belearned from a set of training observa-tions, which in the statistical terminologyis referred to as parameter estimation.For several reasons, especially for theinterdependence of the parameters, thislearning task typically results in a com-plex mathematical optimization problem thedetails of which depend on the chosenmodel and on the chosen training criterion(such as maximum likelihood, squared errorcriterion, discriminative criterion, minimumnumber of recognition errors, ...).In conclusion, stochastic modelling as suchdoes not solve the problems of automaticlanguage translation, but defines a basis on whichwe can find the solutions to the problems.
Incontradiction to a widely held belief, a stochasticapproach may very well require a specific model,and statistics helps us to make the best of agiven model.
Since undoubtedly we have to takedecisions in the context of automatic languageprocessing (and speech recognition), it can onlybe a rhetoric question of whether we should usestatistical decision theory at all.
To make acomparison with another field: in constructinga power plant, it would be foolish to ignore theprinciples of thermodynamics!As to the search problem, the most successfulstrategies are based on either stack decodingor A?
search and dynamic programming beamsearch.
For comparison, in speech recognition,over the last few years, there has been a lotof progress in structuring the search process togenerate a compact word lattice or word graph.To make this point crystal clear: Thecharacteristic property of the stochastic modellingapproach to language translation is not the useof hidden Markov models or hidden alignments.These methods are only the time-honouredmethods and successful methods of today.
Thecharacteristic property lies in the systematic useof a probabilistic framework for the constructionof models, in the statistical training of the freeparameters of these models and in the explicituse of a global scoring criterion for the decisionmaking process.3 Experimental ResultsWhereas stochastic modelling is widely used inspeech recognition, there are so far only a fewresearch groups that apply stochastic modelling tolanguage translation (Berger et al 1994; Brown etal.
1993; Knight 1999).
The presentation here isbased on work carried out in the framework of theEUTRANS project (Casacuberta et al 2001) andthe VERBMOBIL project (Wahlster 2000).We will consider the experimental resultsobtained in the VERBMOBIL project.
The goalof the VERBMOBIL project is the translation ofspoken dialogues in the domains of appointmentscheduling and travel planning.
The languagesare German and English.
Whereas duringthe progress of the project many offline testswere carried out for the optimization and tuningof the statistical approach, the most importantevaluation was the final evaluation of theVERBMOBIL prototype in spring 2000.
This end-to-end evaluation of the VERBMOBIL system wasperformed at the University of Hamburg (Tessioreet al 2000).
In each session of this evaluation,two native speakers conducted a dialogue.
Thespeakers did not have any direct contact and couldonly interact by speaking and listening to theVERBMOBIL system.In addition to the statistical approach, threeother translation approaches had been integratedinto the VERBMOBIL prototype system (Wahlster2000):?
a classical transfer approach,which is based on a manually designedanalysis grammar, a set of transfer rules, anda generation grammar,?
a dialogue act based approach,which amounts to a sort of slot filling byclassifying each sentence into one out of asmall number of possible sentence patternsand filling in the slot values,?
an example based approach,where a sort of nearest neighbour conceptis applied to the set of bilingual trainingsentence pairs after suitable preprocessing.In the final end-to-end evaluation, humanevaluators judged the translation quality for eachof the four translation results using the followingcriterion: Is the sentence approximatively correct:yes/no?
The evaluators were asked to payparticular attention to the semantic information(e.g.
date and place of meeting, participants etc.
)contained in the translation.
A missing translationas it may happen for the transfer approach or otherapproaches was counted as wrong translation.The evaluation was based on 5069 dialogue turnsfor the translation from German to English andon 4136 dialogue turns for the translation fromTable 1: Error rates of spoken sentence translationin the VERBMOBIL end-to-end evaluation.Translation Method Error [%]Semantic Transfer 62Dialogue Act Based 60Example Based 52Statistical 29English to German.
The speech recognizersused had a word error rate of about 25%.
Theoverall sentence error rates, i.e.
resulting fromrecognition and translation, are summarized inTable 1.
As we can see, the error rates for thestatistical approach are smaller by a factor ofabout 2 in comparison with the other approaches.In agreement with other evaluation experi-ments, these experiments show that the statisticalmodelling approach may be comparable to orbetter than the conventional rule-based approach.In particular, the statistical approach seems tohave the advantage if robustness is important, e.g.when the input string is not grammatically corrector when it is corrupted by recognition errors.4 ConclusionIn summary, in the comparative evaluations, bothtext and speech input were translated with goodquality on the average by the statistical approach.Nevertheless, there are examples where thesyntactic structure of the produced target sentenceis not correct.
Some of these syntactic errorsare related to long range dependencies andsyntactic structures that are not captured by them-gram language model used.
To cope withthese problems, morpho-syntactic analysis andgrammar-based language models are currentlybeing studied.AcknowledgmentThis paper is based on work supported partlyby the VERBMOBIL project (contract number01 IV 701 T4) by the German Federal Ministryof Education, Science, Research and Technologyand as part of the EUTRANS project (ESPRITproject number 30268) by the European Commu-nity.ReferencesA.
L. Berger, P. F. Brown, J. Cocke, S. A. DellaPietra, V. J. Della Pietra, J. R. Gillett, J. D. Lafferty,R.
L. Mercer, H. Printz, L. Ures: ?The CandideSystem for Machine Translation?, ARPA HumanLanguage Technology Workshop, Plainsboro, NJ,Morgan Kaufmann Pub., San Mateo, CA, pp.
152-157, March 1994.P.
F. Brown, S. A. Della Pietra, V. J. DellaPietra, R. L. Mercer: ?Mathematics of StatisticalMachine Translation: Parameter Estimation?,Computational Linguistics, Vol.
19.2, pp.
263-311,June 1993.N.
Chomsky: ?Quine?s Empirical Assumptions?,in D. Davidson, J. Hintikka (eds.
): Words andobjections.
Essays on the work of W. V. Quine,Reidel, Dordrecht, The Netherlands, 1969.F.
Casacuberta, D. Llorenz, C. Martinez, S. Molau,F.
Nevado, H. Ney, M. Pastor, D. Pico, A. Sanchis,E.
Vidal, J. Vilar: ?Speech-To-Speech TranslationBased on Finite-State Transducers?, IEEE Int.
Conf.on Acoustics, Speech and Signal Processing, SaltLake City, UT, May 2001.R.
O. Duda, P. E. Hart, D. G. Stork: PatternClassification, 2nd ed., John Wiley & Sons, NewYork, NY, 2001.K.
Knight: ?Decoding Complexity in Word-Replacement Translation Models?, ComputationalLinguistics, No.
4, Vol.
25, pp.
607-615, 1999.H.
Ney, F. J. Och, S. Vogel: ?The RWTH Systemfor Statistical Translation of Spoken Dialogues?,Human Language Technology Conference, SanDiego, CA, Proceedings in press, March 2001.F.
J. Och, C. Tillmann, H. Ney: ?Improved AlignmentModels for Statistical Machine Translation?, JointSIGDAT Conf.
on Empirical Methods in NaturalLanguage Processing and Very Large Corpora,pp.
20?28, University of Maryland, College Park,MD, June 1999.L.
Tessiore, W. v. Hahn: ?Functional Validationof a Machine Interpretation System: Verbmobil?,pp.
611?631, in (Wahlster 2000).W.
Wahlster (Ed.
): Verbmobil: Foundations of Speech-to-Speech Translation.
Springer-Verlag, Berlin,Germany, 2000.
