Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 216?224,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsContextual Information Improves OOV Detection in SpeechCarolina Parada, Mark DredzeHLTCOEJohns Hopkins University3400 North Charles Street,Baltimore MD 21210, USAcarolinap@jhu.edumdredze@cs.jhu.eduDenis FilimonovHLTCOEUniversity of Maryland,College Park, MD 20742 USAden@cs.umd.eduFrederick JelinekHLTCOEJohns Hopkins University3400 North Charles Street,Baltimore MD 21210, USAjelinek@jhu.eduAbstractOut-of-vocabulary (OOV) words represent animportant source of error in large vocabularycontinuous speech recognition (LVCSR) sys-tems.
These words cause recognition failures,which propagate through pipeline systems im-pacting the performance of downstream ap-plications.
The detection of OOV regions inthe output of a LVCSR system is typically ad-dressed as a binary classification task, whereeach region is independently classified usinglocal information.
In this paper, we show thatjointly predicting OOV regions, and includ-ing contextual information from each region,leads to substantial improvement in OOV de-tection.
Compared to the state-of-the-art, wereduce the missed OOV rate from 42.6% to28.4% at 10% false alarm rate.1 IntroductionEven with a vocabulary of one hundred thou-sand words, a large vocabulary continuous speechrecognition (LVCSR) system encounters out-of-vocabulary (OOV) words, especially in new do-mains or genres.
New words often include namedentities, foreign words, rare and invented words.Since these words were not seen during training, theLVCSR system has no way to recognize them.OOV words are an important source of error inLVCSR systems for three reasons.
First, OOVs cannever be recognized by the LVCSR system, even ifrepeated.
Second, OOV words contribute to recog-nition errors in surrounding words, which propagateinto to later processing stages (translation, under-standing, document retrieval, etc.).
Third, OOVsare often information-rich nouns ?
mis-recognizedOOVs can have a greater impact on the understand-ing of the transcript than other words.One solution is to simply increase the LVCSRsystem?s vocabulary, but there are always newwords.
Additionally, increasing the vocabulary sizewithout limit can sometimes produce higher worderror rates (WER), leading to a tradeoff betweenrecognition accuracy of frequent and rare words.A more effective solution is to detect the presenceof OOVs directly.
Once identified, OOVs can beflagged for annotation and addition to the system?svocabulary, or OOV segments can be transcribedwith a phone recognizer, creating an open vocabu-lary LVCSR system.
Identified OOVs prevent errorpropagation in the application pipeline.In the literature, there are two basic approachesto OOV detection: 1) filler models, which explicitlyrepresent OOVs using a filler, sub-word, or genericword model (Bazzi, 2002; Schaaf, 2001; Bisani andNey, 2005; Klakow et al, 1999; Wang, 2009); and2) confidence estimation models, which use differ-ent confidence scores to find unreliable regions andlabel them as OOV (Lin et al, 2007; Burget et al,2008; Sun et al, 2001; Wessel et al, 2001).Recently, Rastrow et al (2009a) presented an ap-proach that combined confidence estimation modelsand filler models to improve state-of-the-art resultsfor OOV detection.
This approach and other confi-dence based systems (Hazen and Bazzi, 2001; Linet al, 2007), treat OOV detection as a binary clas-sification task; each region is independently classi-fied using local information as IV or OOV.
Thiswork moves beyond this independence assumption216that considers regions independently for OOV de-tection.
We treat OOV detection as a sequence la-beling problem and add features based on the locallexical context of each region as well as global fea-tures from a language model using the entire utter-ance.
Our results show that such information im-proves OOV detection and we obtain large reduc-tions in error compared to the best previously re-ported results.
Furthermore, our approach can becombined with any confidence based system.We begin by reviewing the current state-of-the-artresults for OOV detection.
After describing our ex-perimental setup, we generalize the framework to asequence labeling problem, which includes featuresfrom the local context, lexical context, and entire ut-terance.
Each stage yields additional improvementsover the baseline system.
We conclude with a reviewof related work.2 Maximum Entropy OOV DetectionOur baseline system is the Maximum Entropy modelwith features from filler and confidence estimationmodels proposed by Rastrow et al (2009a).
Basedon filler models, this approach models OOVs byconstructing a hybrid system which combines wordsand sub-word units.
Sub-word units, or fragments,are variable length phone sequences selected usingstatistical methods (Siohan and Bacchiani, 2005).The vocabulary contains a word and a fragment lex-icon; fragments are used to represent OOVs in thelanguage model text.
Language model training textis obtained by replacing low frequency words (as-sumed OOVs) by their fragment representation.
Pro-nunciations for OOVs are obtained using graphemeto phoneme models (Chen, 2003).This approach also includes properties from con-fidence estimation systems.
Using a hybrid LVCSRsystem, they obtain confusion networks (Mangu etal., 1999), compact representations of the recog-nizer?s most likely hypotheses.
For an utterance,the confusion network is composed of a sequenceof confused regions, indicating the set of most likelyword/sub-word hypotheses uttered and their poste-rior probabilities1 in a specific time interval.1P (wi|A): posterior probability of word i given the acous-tics, which includes the language model and acoustic modelscores, as described in (Mangu et al, 1999).Figure 1 depicts a confusion network decoded bythe hybrid system for a section of an utterance in ourtest-set.
Below the network we present the referencetranscription.
In this example, two OOVs were ut-tered: ?slobodan?
and ?milosevic?
and decoded asfour and three in-vocabulary words, respectively.
Aconfused region (also called ?bin?)
corresponds toa set of competing hypothesis between two nodes.The goal is to correctly label each of the ?bins?
asOOV or IV.
Note the presence of both fragments(e.g.
s l ow, l aa s) and words in some of thehypothesis bins.For any bin of the confusion network, Rastrow etal.
combine features from that region using a binaryMaximum Entropy classifier (White et al, 2007).Their most effective features were:Fragment-Posterior =?f?tjp(f |tj)Word-Entropy = ?
?w?tjp(w|tj) log p(w|tj)tj is the current bin in the confusion network and fis a fragment in the hybrid dictionary.We obtained confusion networks for a standardword based system and the hybrid system describedabove.
We re-implemented the above features, ob-taining nearly identical results to Rastrow et al us-ing Mallet?s MaxEnt classifier (McCallum, 2002).
2All real-valued features were normalized and quan-tized using the uniform-occupancy partitioning de-scribed in White et al (2007).3 The MaxEnt modelis regularized using a Gaussian prior (?2 = 100),but we found results generally insensitive to ?.3 Experimental SetupBefore we introduce and evaluate our context ap-proach, we establish an experimental setup.
We usedthe dataset constructed by Can et al (2009) to eval-uate Spoken Term Detection (STD) of OOVs; werefer to this corpus as OOVCORP.
The corpus con-tains 100 hours of transcribed Broadcast News En-glish speech emphasizing OOVs.
There are 1290unique OOVs in the corpus, which were selectedwith a minimum of 5 acoustic instances per word.2Small differences are due to a change in MaxEnt library.3All experiments use 50 partitions with a minimum of 100training values per partition.217Figure 1: Example confusion network from the hybrid system with OOV regions and BIO encoding.
Hypothesis areordered by decreasing value of posterior probability.
Best hypothesis is the concatenation of the top word/fragmentsin each bin.
We omit posterior probabilities due to spacing.Common English words were filtered out to ob-tain meaningful OOVs: e.g.
NATALIE, PUTIN,QAEDA, HOLLOWAY.
Since the corpus was de-signed for STD, short OOVs (less than 4 phones)were explicitly excluded.
This resulted in roughly24K (2%) OOV tokens.For a LVCSR system we used the IBM SpeechRecognition Toolkit (Soltau et al, 2005)4 withacoustic models trained on 300 hours of HUB4 data(Fiscus et al, 1998) and excluded utterances con-taining OOV words as marked in OOVCORP.
The lan-guage model was trained on 400M words from var-ious text sources with a 83K word vocabulary.
TheLVCSR system?s WER on the standard RT04 BNtest set was 19.4%.
Excluded utterances were di-vided into 5 hours of training and 95 hours of testdata for the OOV detector.
Both train and test setshave a 2% OOV rate.
We used this split for all exper-iments.
Note that the OOV training set is differentfrom the LVCSR training set.In addition to a word-based LVCSR system, weuse a hybrid LVCSR system, combining word andsub-word (fragments) units.
Combined word/sub-word systems have improved OOV Spoken TermDetection performance (Mamou et al, 2007; Paradaet al, 2009), better phone error rates, especially inOOV regions (Rastrow et al, 2009b), and state-of-the-art performance for OOV detection.
Our hybridsystem?s lexicon has 83K words and 20K fragmentsderived using Rastrow et al (2009a).
The 1290 ex-cluded words are OOVs to both the word and hybrid4We use the IBM system with speaker adaptive trainingbased on maximum likelihood with no discriminative training.systems.Note that our experiments use a different datasetthan Rastrow et.
al., but we have a larger vocabu-lary (83K vs 20K), which is closer to most modernLVCSR system vocabularies; the resulting OOVsare more challenging but more realistic.3.1 EvaluationConfusion networks are obtained from both theword and hybrid LVCSR systems.
In order to eval-uate the performance of the OOV detector, we alignthe reference transcript to the audio.
The LVCSRtranscript is compared to the reference transcript atthe confused region level, so each confused regionis tagged as either OOV or IV.
The OOV detectorassigns a score/probability for IV/OOV to each ofthese regions.Previous research reported OOV detection accu-racy on all test data.
However, once an OOV wordhas been observed in the training data for the OOVdetector, even if it never appeared in the LVCSRtraining data, it is no longer truly OOV.
The fea-tures used in previous approaches did not necessar-ily provide an advantage on observed versus unob-served OOVs, but our features do yield an advan-tage.
Therefore, in the sections that follow we re-port unobserved OOV accuracy: OOV words thatdo not appear in either the OOV detector?s or theLVCSR?s training data.
While this penalizes our re-sults, it is a more informative metric of true systemperformance.We present results using standard detection errortradeoff (DET) curves (Martin et al, 1997).
DET218curves measure tradeoffs between misses and falsealarms and can be used to determine the optimal op-erating point of a system.
The x-axis varies the falsealarm rate (false positive) and the y-axis varies themiss (false negative) rate; lower curves are better.4 From MaxEnt to CRFsAs a classification algorithm, Maximum Entropy as-signs a label to each region independently.
However,OOV words tend to be recognized as two or more IVwords, hence OOV regions tend to co-occur.
In theexample of Figure 1, the OOV word ?slobodan?
wasrecognized as four IV words: ?slow vote i mean?.This suggests that sequence models, which jointlyassign all labels in a sequence, may be more appro-priate.
Therefore, we begin incorporating context bymoving from classification to sequence models.MaxEnt classification models the target label asp(yi|xi), where yi is a discrete variable representingthe ith label (?IV?
or ?OOV?)
and xi is a featurevector representing information for position i. Theconditional distribution for yi takes the formp(yi|xi) =1Z(xi)exp(K?k=1?kfk(yi,xi)) ,Z(xi) is a normalization term and f(yi,xi) is a vec-tor ofK features, such as those defined in Section 2.The model is trained discriminatively: parameters ?are chosen to maximize conditional data likelihood.Conditional Random Fields (CRF) (Lafferty etal., 2001) generalize MaxEnt models to sequencetasks.
While having the same model structure asHidden Markov Models (HMMs), CRFs are traineddiscriminatively and can use large numbers of corre-lated features.
Their primary advantage over Max-Ent models is their ability to find an optimal labelingfor the entire sequence rather than greedy local deci-sions.
CRFs have been used successfully used in nu-merous text processing tasks and while less popularin speech, still applied successfully, such as sentenceboundary detection (Liu et al, 2005).A CRF models the entire label sequence y as:p(y|x) =1Z(x)exp(?F (y,x)) ,where F (y,x) is a global feature vector for inputsequence x and label sequence y and Z(x) is a nor-malization term.55 Context for OOV DetectionWe begin by including a minimal amount of localcontext in making OOV decisions: the predicted la-bels for adjacent confused regions (bins).
This infor-mation helps when OOV bins occur in close proxim-ity, such as successive OOV bins.
This is indeed thecase: in the OOV detector training data only 48% ofOOV sequences contained a single bin; sequenceswere of length 2 (40%), 3 (9%) and 4 (2%).
Wefound similar results in the test data.
Therefore, weexpect that even a minimal amount of context basedon the labels of adjacent bins will help.A natural way of incorporating contextual infor-mation is through a CRF, which introduces depen-dencies between each label and its neighbors.
If aneighboring bin is likely an OOV, it increases thechance that the current bin is OOV.In sequence models, another technique for cap-turing contextual dependence is the label encodingscheme.
In information extraction, where sequencesof adjacent tokens are likely to receive the sametag, the beginning of each sequence receives a dif-ferent tag from words that continue the sequence.For example, the first token in a person name islabeled B-PER and all subsequent tokens are la-beled I-PER.
This is commonly referred to as BIOencoding (beginning, inside, outside).
We appliedthis encoding technique to our task, labeling binsas either IV (in vocabulary), B-OOV (begin OOV)and I-OOV (inside OOV), as illustrated in Figure 1.This encoding allows the algorithm to identify fea-tures which might be more indicative of the begin-ning of an OOV sequence.
We found that this en-coding achieved a superior performance to a simpleIV/OOV encoding.
We therefore utilize the BIO en-coding in all CRF experiments.Another means of introducing context is throughthe order of the CRF model.
A first order model(n = 1) adds dependencies only between neighbor-ing labels, whereas an n order model creates depen-dencies between labels up to a distance of n posi-tions.
Higher order models capture length of label5CRF experiments used the CRF++ packagehttp://crfpp.sourceforge.net/219regions (up to length n).
We experiment with botha first order and a second order CRF.
Higher ordermodels did not provide any improvements.In order to establish a comparative baseline, wefirst present results using the same features fromthe system described in Section 2 (Word-Entropyand Fragment-Posterior).
All real-valued featureswere normalized and quantized using the uniform-occupancy partitioning described in White et al(2007).6 Quantization of real valued features is stan-dard for log-linear models as it allows the model totake advantage of non-linear characteristics of fea-ture values and is better handled by the regulariza-tion term.
As in White et.
al.
we found it improvedperformance.Figure 2 depicts DET curves for OOV detectionfor the MaxEnt baseline and first and second orderCRFs with BIO encoding on unobserved OOVs inthe test data.
We generated predictions at differentfalse alarm rates by varying a probability threshold.For MaxEnt we used the predicted label probabilityand for CRFs the marginal probability of each bin?slabel.
While the first order CRF achieves nearlyidentical performance to the MaxEnt baseline, thesecond order CRF shows a clear improvement.
Thesecond order model has a 5% absolute improvementat 10% false alarm rate, despite using the identi-cal features as the MaxEnt baseline.
Even a smallamount of context as expressed through local label-ing decisions improves OOV detection.The quantization of the features yields quan-tized prediction scores, resulting in the non-smoothcurves for the MaxEnt and 1st order CRF results.However, when using a second order CRF the OOVscore varies more smoothly since more features(context labels) are considered in the prediction ofthe current label.6 Local Lexical ContextA popular approach in sequence tagging, such as in-formation extraction or part of speech tagging, is toinclude features based on local lexical content andcontext.
In detecting a name, both the lexical form?John?
and the preceding lexical context ?Mr.?
pro-vide clues that ?John?
is a name.
While we do not6All experiments use 50 partitions with a minimum of 100training values per partition.0 2 4 6 8 10 12 14P(FA)102030405060P(Miss)MaxEnt (Baseline)CRF (First Order)CRF (Second Order)Figure 2: DET curves for OOV detection using a Max-imum Entropy (MaxEnt) classifier and contextual infor-mation using a 1st order and 2nd order CRF.
All modelsuse the same baseline features (Section 2).know the actual lexical items in the speech sequence,the speech recognizer output can be used as a bestguess.
In the example of Figure 1, the words ?for-mer president?
are good indicators that the followingword is either the word ?of?
or a name, and hence apotential OOV.
Combining this lexical context withhypothesized words can help label the subsequentregions as OOVs (note that none of the hypothesizedwords in the third bin are ?of?, names, or nouns).Words from the LVCSR decoding of the sentenceare used in the CRF OOV detector.
For each bin inthe confusion network, we select the word with thehighest probability (best hypothesis).
We then addthe best hypothesis word as a feature of the form:current word=X.
These features capture how theLVCSR system incorrectly recognizes OOV words.However, since detection is measured on unobservedOOVs, these features alone may not help.Instead, we turn to lexical context, which includescorrectly recognized IV words.
We evaluate the fol-lowing sets of features derived from lexical context:?
Current bin?s best hypothesis.
(Current-Word)?
Unigrams and bigrams from the best hypoth-esis in a window of 5 words around currentbin.
This feature ignores the best hypothesis inthe current bin, i.e., word[-2],word[-1]is included, but word[-1],word[0] is not.
(Context-Bigrams)2200 2 4 6 8 10 12 14P(FA)102030405060P(Miss)CRF (Second Order)+Current-Word+Context-Bigrams+Current-Trigrams+All-Words+All-Words-StemmedFigure 3: A second order CRF (Section 5) and additionalfeatures including including word identities from currentand neighboring bins (Section 6).?
Unigrams, bigrams, and trigrams in a windowof 5 words around and including current bin.(Current-Trigrams)?
All of the above features.
(All-Words)?
All above features and their stems.7 (All-Words-Stemmed)We added these features to the second order CRFwith BIO encoding and baseline features (Figure 3).As expected, the current words did not improve per-formance on unobserved OOVs.
When the currentwords are combined with the lexical context andtheir lemmas, they give a significant boost in perfor-mance: a 4.2% absolute improvement at 10% falsealarm rate over the previous CRF system, and 9.3%over the MaxEnt baseline.
Interestingly, only com-bining context and current word gives a substantialgain.
This indicates that OOVs tend to occur withcertain distributional characteristics that are inde-pendent of the OOV word uttered (since we consideronly unobserved OOVs), perhaps because OOVstend to be named entities, foreign words, or rarenouns.
The importance of distributional features iswell known for named entity recognition and partof speech tagging (Pereira et al, 1993).
Other fea-tures such as sub-strings or baseline features (Word-7To obtain stemmed words, we use the CPAN package:http://search.cpan.org/~snowhare/Lingua-Stem-0.83.Entropy, Fragment-Posterior) from neighboring binsdid not provide further improvement.7 Global Utterance ContextWe now include features that incorporate informa-tion from the entire utterance.
The probability of anutterance as computed by a language model is of-ten used as a measure of fluency of the utterance.We also observe that OOV words tend to take veryspecific syntactic roles (more than half of them areproper nouns), which means the surrounding contextwill have predictive lexical and syntactic properties.Therefore, we use a syntactic language model.7.1 Language ModelsWe evaluated both a standard trigram languagemodel and a syntactic language model (Filimonovand Harper, 2009a).
The syntactic model estimatesthe joint probability of the word and its syntactic tagbased on the preceding words and tags.
The proba-bility of an utterance wn1 of length n is computed bysumming over all latent syntactic tag assignments:p(utt) = p(wn1 ) =?t1...tnn?i?1p(wi, ti|wi?11 , ti?11 )(1)where wi and ti are the word and tag at posi-tion i, and wi?11 and ti?11 are sequences of wordsand tags of length i ?
1 starting a position 1.The model is restricted to a trigram context, i.e.,p(wi, ti|wi?1i?2, ti?1i?2); experiments that increased theorder yielded no improvement.We trained the language model on 130 millionwords from Hub4 CSR 1996 (Garofolo et al, 1996).The corpus was parsed using a modified Berkeleyparser (Huang and Harper, 2009) and tags extractedfrom parse trees incorporated the word?s POS, thelabel of its immediate parent, and the relative posi-tion of the word among its siblings.
8 The parserrequired separated contractions and possessives, butwe recombined those words after parsing to matchthe LVCSR tokenization, merging their tags.
Sincewe are considering OOV detection, the languagemodel was restricted to LVCSR system?s vocabu-lary.8The parent tagset of Filimonov and Harper (2009a).2210 2 4 6 8 10 12 14P(FA)102030405060P(Miss)All-Words-Lemmas+3gram-LM+Syntactic-LM+Syntactic-LM+TagsFigure 4: Features from a language model added to thebest CRF from Section 6 (All-Words-Stemmed).We also used the standard trigram LM for refer-ence.
It was trained on the same data and with thesame vocabulary using the SRILM toolkit.
We usedinterpolated modified KN discounting.7.2 Language Model FeaturesWe designed features based on the entire utteranceusing the language model to measure how the utter-ance is effected by the current token: whether theutterance is more likely given the recognized wordor some OOV word.Likelihood-ratio = logp(utt)p(utt|wi = unknown)Norm-LM-score =log p(utt)length(utt)where p(utt) represents the probability of the ut-terance using the best path hypothesis word of theLVCSR system, and p(utt|wi = unknown) is theprobability of the entire utterance with the currentword in the LVCSR output replaced by the token<unk>, used to represent OOVs.
Intuitively, whenan OOV word is recognized as an IV word, the flu-ency of the utterance is disrupted, especially if theIV is a function word.
The Likelihood-ratio is de-signed to show whether the utterance is more fluent(more likely) if the current word is a misrecognizedOOV.
9 The second feature (Norm-LM-score) is the9Note that in the standard n-gram LM the feature reduces tologQi+n?1k=i p(wk|wk?1k?n+1)Qi+n?1k=i p(wk|wk?1k?n+1,wi=unknown), i.e., only n n-grams actu-0 5 10 15 20 25 30 35 40P(FA)01020304050607080P(Miss)MaxEnt (Baseline)CRF All FeaturesCRF All Features (Unobserved)CRF All Features (Observed)Figure 5: A CRF with all context features compared tothe state-of-the-art MaxEnt baseline.
Results for the CRFare shown for unobserved, observed and both OOVs.normalized likelihood of the utterance.
An unlikelyutterance biases the system to predicting OOVs.We evaluated a CRF with these features andall lexical context features (Section 6) using boththe trigram model and the joint syntactic languagemodel (Figure 4).
Each model improved perfor-mance, but the syntactic model provided the largestimprovement.
At 10% false alarm rate it yields a4% absolute improvement with respect to the pre-vious best result (All-Words-Stemmed) and 13.3%over the MaxEnt baseline.
Higher order languagemodels did not improve.7.3 Additional Syntactic FeaturesWe explored other syntactic features; the most ef-fective was the 5-tag window of POS tags of thebest hypothesis.10 The additive improvement of thisfeature is depicted in Figure 4 labeled ?+Syntactic-LM+Tags.?
With this feature, we achieve a small ad-ditional gain.
We tried other syntactic features with-out added benefit, such as the most likely POS tagfor <unk>in the utterance.ally contribute.
However, in the syntactic LM, the entire utter-ance is affected by the change of one word through the latentstates (tags) (Eq.
1), thus making it a truly global feature.10The POS tags were generated by the same syntactic LM(see Section 7.1) as described in (Filimonov and Harper,2009b).
In this case, POS tags include merged tags, i.e., the vo-cabulary word fred?s may be tagged as NNP-POS or NNP-VBZ.2228 Final SystemFigure 5 summarizes all of the context features in asingle second order BIO encoded CRF.
Results areshown for state-of-the-art MaxEnt (Rastrow et al,2009a) as well as for the CRF on unobserved, ob-served and combined OOVs.
For unobserved OOVsour final system achieves a 14.2% absolute improve-ment at 10% FA rate.
The absolute improvementon all OOVs was 23.7%.
This result includes ob-served OOVs: words that are OOV for the LVCSRbut are encountered in the OOV detector?s trainingdata.
MaxEnt achieved similar performance for ob-served and unobserved OOVs so we only include asingle combined result.Note that the MaxEnt curve flattens at 26% falsealarms, while the CRF continues to decrease.
Theelbow in the MaxEnt curve corresponds to the prob-ability threshold at which no other labeled OOV re-gion has a non-zero OOV score (regions with zeroentropy and no fragments).
In this case, the CRFmodel can still rely on the context to predict a non-zero OOV score.
This helps applications wheremisses are more heavily penalized than false alarms.9 Related WorkMost approaches to OOV detection in speech canbe categorized as filler models or confidence esti-mation models.
Filler models vary in three dimen-sions: 1) The type of filler units used: variable-length phoneme units (as the baseline system) vsjoint letter sound sub-words; 2) Method used to de-rive units: data-driven (Bazzi and Glass, 2001) orlinguistically motivated (Choueiter, 2009); 3) Themethod for incorporating the LVCSR system: hi-erarchical (Bazzi, 2002) or flat models (Bisani andNey, 2005).
Our approach can be integrated withany of these systems.We have shown that combining the presence ofsub-word units with other measures of confidencecan provided significant improvements, and otherproposed local confidence measures could be in-cluded in our system as well.
Lin et al (2007)uses joint word/phone lattice alignments and clas-sifies high local miss-alignment regions as OOVs.Hazen and Bazzi (2001) combines filler models withword confidence scores, such as the minimum nor-malized log-likelihood acoustic model score for aword and, the fraction of the N-best utterance hy-potheses in which a hypothesized word appears.Limited contextual information has been pre-viously exploited (although maintaining indepen-dence assumptions on the labels).
Burget et al(2008) used a neural-network (NN) phone-posteriorestimator as a feature for OOV detection.
Thenetwork is fed with posterior probabilities fromweakly-constrained (phonetic-based) and strongly-constrained (word-based) recognizers.
Their sys-tem estimates frame-based scores, and interestingly,they report large improvements when using tempo-ral context in the NN input.
This context is quite lim-ited; it refers to posterior scores from one frame oneach side.
Other features are considered and com-bined using a MaxEnt model.
They attribute thisgain to sampling from neighboring phonemes.
Sunet al (2001) combines a filler-based model with aconfidence approach by using several acoustic fea-tures along with context based features, such aswhether the next word is a filler, acoustic confidencefeatures for next word, number of fillers, etc.None of these approaches consider OOV detec-tion as a sequence labeling problem.
The work ofLiu et al (2005) is most similar to the approach pre-sented here, but applies a CRF to sentence boundarydetection.10 Conclusion and Future WorkWe have presented a novel and effective approach toimprove OOV detection in the output confusion net-works of a LVCSR system.
Local and global con-textual information is integrated with sub-word pos-terior probabilities obtained from a hybrid LVCSRsystem in a CRF to detect OOV regions effectively.At a 10% FA rate, we reduce the missed OOV ratefrom 42.6% to 28.4%, a 33.3% relative error reduc-tion.
Our future work will focus on additional fea-tures from the recognizer aside from the single best-hypothesis, as well as other applications of contex-tual sequence prediction to speech tasks.AcknowledgmentsThe authors thank Ariya Rastrow for providing thebaseline system code, Abhinav Sethy and BhuvanaRamabhadran for providing the data used in the ex-periments and for many insightful discussions.223ReferencesIssam Bazzi and James Glass.
2001.
Learning unitsfor domain-independent out-of-vocabulary word mod-elling.
In Eurospeech.Issam Bazzi.
2002.
Modelling out-of-vocabulary wordsfor robust speech recognition.
Ph.D. thesis, Mas-sachusetts Institute of Technology.M.
Bisani and H. Ney.
2005.
Open vocabulary speechrecognition with flag hybrid models.
In INTER-SPEECH.L.
Burget, P. Schwarz, P. Matejka, M. Hannemann,A.
Rastrow, C. White, S. Khudanpur, H. Hermansky,and J. Cernocky.
2008.
Combination of strongly andweakly constrained recognizers for reliable detectionof OOVS.
In ICASSP.Dogan Can, Erica Cooper, Abhinav Sethy, Chris White,Bhuvana Ramabhadran, and Murat Saraclar.
2009.Effect of pronounciations on OOV queries in spokenterm detection.
ICASSP.Stanley F. Chen.
2003.
Conditional and joint models forgrapheme-to-phoneme conversion.
In Eurospeech.G.
Choueiter.
2009.
Linguistically-motivated sub-word modeling with applications to speech recogni-tion.
Ph.D. thesis, Massachusetts Institute of Technol-ogy.Denis Filimonov and Mary Harper.
2009a.
A jointlanguage model with fine-grain syntactic tags.
InEMNLP.Denis Filimonov and Mary Harper.
2009b.
Measuringtagging performance of a joint language model.
InProceedings of the Interspeech 2009.Jonathan Fiscus, John Garofolo, Mark Przybocki,William Fisher, and David Pallett, 1998.
1997 En-glish Broadcast News Speech (HUB4).
LinguisticData Consortium, Philadelphia.John Garofolo, Jonathan Fiscus, William Fisher, andDavid Pallett, 1996.
CSR-IV HUB4.
Linguistic DataConsortium, Philadelphia.Timothy J. Hazen and Issam Bazzi.
2001.
A comparisonand combination of methods for OOV word detectionand word confidence scoring.
In Proceedings of theInternational Conference on Acoustics.Zhongqiang Huang and Mary Harper.
2009.
Self-Training PCFG grammars with latent annotationsacross languages.
In EMNLP.Dietrich Klakow, Georg Rose, and Xavier Aubert.
1999.OOV-detection in large vocabulary system using au-tomatically defined word-fragments as fillers.
In Eu-rospeech.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Interna-tional Conference on Machine Learning (ICML).Hui Lin, J. Bilmes, D. Vergyri, and K. Kirchhoff.
2007.OOV detection by joint word/phone lattice alignment.In ASRU, pages 478?483, Dec.Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and MaryHarper.
2005.
Using conditional random fields forsentence boundary detection in speech.
In ACL.Jonathan Mamou, Bhuvana Ramabhadran, and OlivierSiohan.
2007.
Vocabulary independent spoken termdetection.
In SIGIR.L.
Mangu, E. Brill, and A. Stolcke.
1999.
Finding con-sensus among words.
In Eurospeech.A.
Martin, G. Doddington, T. Kamm, M. Ordowski, andM.
Przybocky.
1997.
The DET curve in assessment ofdetection task performance.
In Eurospeech.Andrew McCallum.
2002.
MALLET: A machine learn-ing for language toolkit.
http://mallet.cs.umass.edu.Carolina Parada, Abhinav Sethy, and Bhuvana Ramab-hadran.
2009.
Query-by-example spoken term detec-tion for OOV terms.
In ASRU.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional clustering of english words.
In ACL.Ariya Rastrow, Abhinav Sethy, and Bhuvana Ramabhad-ran.
2009a.
A new method for OOV detection usinghybrid word/fragment system.
ICASSP.Ariya Rastrow, Abhinav Sethy, Bhuvana Ramabhadran,and Fred Jelinek.
2009b.
Towards using hybrid,word, and fragment units for vocabulary independentLVCSR systems.
INTERSPEECH.T.
Schaaf.
2001.
Detection of OOV words using gen-eralized word models and a semantic class languagemodel.
In Eurospeech.O.
Siohan and M. Bacchiani.
2005.
Fast vocabulary-independent audio search using path-based graph in-dexing.
In INTERSPEECH.H.
Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,and G. Zweig.
2005.
The IBM 2004 conversationaltelephony system for rich transcription.
In ICASSP.H.
Sun, G. Zhang, f. Zheng, and M. Xu.
2001.
Usingword confidence measure for OOV words detection ina spontaneous spoken dialog system.
In Eurospeech.Stanley Wang.
2009.
Using graphone models in au-tomatic speech recognition.
Master?s thesis, Mas-sachusetts Institute of Technology.F.
Wessel, R. Schluter, K. Macherey, and H. Ney.
2001.Confidence measures for large vocabulary continuousspeech recognition.
IEEE Transactions on Speech andAudio Processing, 9(3).Christopher White, Jasha Droppo, Alex Acero, and Ju-lian Odell.
2007.
Maximum entropy confidence esti-mation for speech recognition.
In ICASSP.224
