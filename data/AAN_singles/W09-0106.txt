Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 26?32,Athens, Greece, 30 March, 2009. c?2009 Association for Computational LinguisticsLinguistically Na?
?ve != Language Independent:Why NLP Needs Linguistic TypologyEmily M. BenderUniversity of WashingtonSeattle, WA, USAebender@u.washington.eduAbstractIn this position paper, I argue that in orderto create truly language-independent NLPsystems, we need to incorporate linguis-tic knowledge.
The linguistic knowledgein question is not intricate rule systems,but generalizations from linguistic typol-ogy about the range of variation in linguis-tic structures across languages.1 IntroductionLanguage independence is commonly presentedas one of the advantages of modern, machine-learning approaches to NLP.
Once an algorithm isdeveloped, the argument goes, it can trivially beextended to another language; ?all?
that is neededis a suitably large amount of training data for thenew language.1 This is indeed a virtue.
How-ever, the typical approach to developing language-independent systems is to eschew using any lin-guistic knowledge in their production.
In this po-sition paper, I argue that, on the contrary, the pro-duction of language-independent NLP technologyrequires linguistic knowledge, and that the rele-vant kind of linguistic knowledge is in fact rela-tively inexpensive.The rest of this paper is structured as follows:In Section 2, I discuss how linguistically na?
?vesystems can end up tuned to the languages theywere originally developed for.
In Section 3, Isurvey the long papers from ACL2008:HLT togive a snapshot of how linguistic diversity is cur-rently handled in our field.
In Section 4, I give1This of course abstracts away from the production ofsuch data, which may require both significant pre-processingand annotation work.
For the purposes of the present argu-ment, however, we can assume that all language-independentNLP systems are unicode-enabled, assume a definition of?word?
that is cross-linguistically applicable, and require thetype of annotations that are likely to have already been de-ployed for another purpose.a brief overview of Linguistic Typology, and sug-gest how knowledge derived from this field can beprofitably incorporated into language-independentNLP systems.2 Hidden Language DependenceA simple example of subtle language dependenceis the way in which n-gram models work better forlanguages that share important typological proper-ties with English.
On the face of it, n-gram mod-els code in no linguistic knowledge.
They treatnatural language text as simple sequences of sym-bols and automatically reflect the ?hidden?
struc-ture through the way it affects the distributionsof words in various (flat, unstructured) contexts.However, the effectiveness of n-gram models inEnglish (and similar languages) is partially pred-icated on two properties of those languages: rel-atively low levels of inflectional morphology, andrelatively fixed word order.As is well-known by now, languages withmore elaborate morphology (more morphemes perword, more distinctions within the same numberof morphological slots, and/or fewer uninflectedwords) present greater data sparsity problems forlanguage models.
This data sparsity limits theability of n-gram models to capture the depen-dencies between open-class morphemes, but alsoclosed class morphemes.
The information ex-pressed by short function words in English is typ-ically expressed by the inflectional morphology inlanguages with more elaborate morphological sys-tems.
Word-based n-gram models have no wayof representing the function morphemes in sucha language.
In addition, for n-gram models tocapture inter-word dependencies, both words haveto appear in the n-gram window.
This will hap-pen more consistently in languages with relativelyfixed word order, as compared to languages withrelatively free word order.Thus even though n-grams models can be built26without any hand-coding of linguistic knowledge,they are not truly language independent.
Rather,their success depends on typological propertiesof the languages they were first developed for.A more linguistically-informed (and thus morelanguage independent) approach to n-gram mod-els is the factored language model approach ofBilmes and Kirchhoff (2003).
Factored languagemodels address the problems of data-sparsity inmorphologically complex languages by represent-ing words as bundles of features, thus capturingdependencies between subword parts of adjacentwords.A second example of subtle language depen-dence comes from Dasgupta and Ng (2007), whopresent an unsupervised morphological segmenta-tion algorithm meant to be language-independent.Indeed, this work goes much further towards lan-guage independence than is the norm (see Section3).
It is tested against data from English, Bengali,Finnish and Turkish, a particularly good selectionof languages in that it includes diversity along akey dimension (degree of morphological complex-ity), as well as representatives of three languagefamilies (Indo-European, Uralic, and Altaic).
Fur-thermore, the algorithm is designed to detect morethan one prefix or suffix per word, which is impor-tant for analyzing morphologically complex lan-guages.
However, it seems unrealistic to expect aone-size-fits-all approach to be achieve uniformlyhigh performance across varied languages, and,in fact, it doesn?t.
Though the system presentedin (Dasgupta and Ng, 2007) outperforms the bestsystems in the 2006 PASCAL challenge for Turk-ish and Finnish, it still does significantly worse onthese languages than English (F-scores of 66.2 and66.5, compared to 79.4).This seems to be due to an interesting interac-tion of at least two properties of the languagesin question.
First, the initial algorithm for dis-covering candidate roots and affixes relies on thepresence of bare, uninflected roots in the train-ing vocabulary, extracting a string as a candidateaffix (or sequence of affixes) when it appears atthe end (or beginning) of another string that alsoappears independently.
In Turkish and Finnish,verbs appear as bare roots in many fewer con-texts than in English.2 This is also true in Ben-2In Finnish, depending on the verb class, the bareroot may appear in negated present tense sentences, insecond-person singular imperatives, and third-person singu-lar present tense, or not at all (Karlsson and Chesterman,gali, and the authors note that their technique fordetecting allomorphs is critical to finding ?out-of-vocabulary?
roots (those unattested as stand-alonewords) in that language.
However, the techniquefor finding allomorphs assumes that ?roots exhibitthe character changes during attachment, not suf-fixes?
(p.160), and this is where another propertyof Finnish and Turkish becomes relevant: Both ofthese languages exhibit vowel harmony, where thevowels in many suffixes vary depending on thevowels of the root, even if consonants intervene.Thus I speculate that at least some of the reducedperformance in Turkish and Finnish is due to thesystem not being able to recognize variants of thesame suffixes as the same, and, in addition, not be-ing able to isolate all of the roots.Of course, in some cases, one language mayrepresent, in some objective sense, a harder prob-lem than another.
A clear example of this is En-glish letter-to-phoneme conversion, which, as a re-sult of the lack of transparency in English orthog-raphy, is a harder problem that letter-to-phonemeconversion in other languages.
Not surprisingly,the letter-to-phoneme systems described in e.g.
(Jiampojamarn et al, 2008) and (Bartlett et al,2008) do worse on the English test data than theydo on German, Dutch, or French.
On the otherhand, just because one language may present aharder problem than the other doesn?t mean thatsystem developers can assume that any perfor-mance differences can be explained in such a way.If one aims to create a language-independent sys-tem, then one must explore the possibility thatthe system includes assumptions about linguis-tic structure which do not hold up across all lan-guages.The conclusions I would like to draw fromthese examples are as follows: A truly language-independent system works equally well across lan-guages.
When a system that is meant to be lan-guage independent does not in fact work equallywell across languages, it is likely because some-thing about the system design is making implicitassumptions about language structure.
These as-sumptions are typically the result of ?overfitting?to the original development language(s).3 In Sec-1999).
In Turkish, the bare root can function as a familiarimperative, but other forms are inflected (Lewis, 1967; Un-derhill, 1976).3Here I use the term ?overfitting?
metaphorically, to callout the way in which, as the developers of NLP methodol-ogy, we rely on our intuitions about the structure of the lan-guage(s) we?re working with and the feedback we get by test-27tion 4, I will argue that the best way to achieve lan-guage independence is by including, rather thaneschewing, linguistic knowledge.3 Language Independence and LanguageRepresentation at ACLThis section reports on a survey of the 119 longpapers from ACL2008:HLT.
Of these 119 papers,18 explicitly claimed (16) or suggested (2) thatthe methods described could be applied to otherlanguages.
Another 13 could be read as implic-itly claiming that.
Still others present the kindof methodology that often is claimed to be cross-linguistically applicable, such as statistical ma-chine translation.
Of the 16 explicitly claiminglanguage independence, 7 evaluated their systemson multiple languages.
Since many of the tech-niques are meant to be cross-linguistically appli-cable, I collected information about the languagesstudied in all 119 papers.
Table 1 groups the pa-pers by how many languages (or language pairs)they study.
The three papers studying zero lan-guages involved abstract, formal proofs regarding,e.g., grammar formalisms.
95 of the papers stud-ied just one language or language pair.Languages or language Number of paperspairs considered0 31 952 133 34 25 112 113 1Total 119Table 1: Number of languages/language pairs con-sideredThe two papers looking at the widest variety oflanguages were (Ganchev et al, 2008) and (Nivreand McDonald, 2008).
Ganchev et al (2008)explore whether better alignments lead to bettertranslations, across 6 language pairs, in each di-rection (12 MT systems), collecting data from avariety of sources.
Nivre and McDonald (2008)present an approach to dependency parsing whichintegrates graph-based and transition-based meth-ods, and evaluate the result against the 13 datasetsing our ideas against particular languages.provided in the CoNLL-X shared task (Nivre et al,2007).It is encouraging to see such use of multilingualdatasets; the field as a whole will be in a betterposition to test (and improve) the cross-linguisticapplicability of various methods to the extent thatmore such datasets are produced.
It is worth not-ing, however, that the sheer number of languagestested is not the only important factor: Becauserelated languages tend to share typological prop-erties, it is also important to sample across theknown language families.Tables 2 and 3 list the languages and languagepairs studied in the papers in the survey.
Table2 presents the data on methodologies that involveproducing results for one language at a time, andgroups the languages by genus and family (accord-ing to the classification used by the World Atlas ofLanguage Structures Online4).
Table 3 presentsthe data on methodologies that involve symmetri-cal (e.g., bilingual lexicon extraction) or asymmet-rical (e.g., MT) language pairs.5The first thing to note in these tables is the con-centration of work on English: 63% of the single-language studies involved English, and all of thelanguage pairs studied included English as onemember.
In many cases, the authors did not ex-plicitly state which language they were workingon.
That it was in fact English could be inferredfrom the data sources cited, in some cases, or fromthe examples used, in others.
The common prac-tice of not explicitly stating the language when it isEnglish would seem to follow from a general sensethat the methods should be crosslinguistically ap-plicable.The next thing to note about these tables is thatmany of the languages included are close relativesof each other.
Ethnologue6 lists 94 language fami-lies; ACL2008:HLT papers studied six.
Of course,the distribution of languages (and perhaps moreto the point, speakers) is not uniform across lan-4http://wals.info (Haspelmath et al, 2008); Note thatJapanese is treated as a language isolate and Chinese is thename for the genus including (among others) Mandarin andCantonese.5The very interesting study by Snyder and Barzilay (2008)on multilingual approaches to morphological segmentationwas difficult to classify.
Their methodology involved jointlyanalyzing two languages at a time in order to produce mor-phological segmenters for each.
Since the resulting systemswere monolingual, the data from these studies are included inTable 2.6http://www.ethnologue.com/ethno docs/distribution.asp,accessed on 6 February 2009.28Language Studies Genus Studies Family StudiesN % N % N %English 81 63.28 Germanic 91 71.09 Indo-European 109 85.16German 5 3.91Dutch 3 2.34Danish 1 0.78Swedish 1 0.78Czech 3 2.34 Slavic 8 6.25Russian 2 1.56Bulgarian 1 0.78Slovene 1 0.78Ukranian 1 0.78Portuguese 3 2.34 Romance 8 6.25Spanish 3 2.34French 2 1.56Hindi 2 1.56 Indic 2 1.56Arabic 4 3.13 Semitic 9 7.03 Afro-Asiatic 9 7.03Hebrew 4 3.13Aramaic 1 0.78Chinese 5 3.91 Chinese 5 3.91 Sino-Tibetan 5 3.91Japanese 3 2.34 Japanese 3 3.24 Japanese 3 3.24Turkish 1 0.78 Turkic 1 0.78 Altaic 1 0.78Wambaya 1 0.78 West Barkly 1 0.78 Australian 1 0.78Total 128 100.00 128 100.00 128 100.00Table 2: Languages studied in ACL 2008 papers, by language genus and familySource Target N Source Target N Symmetrical pair NChinese English 9 English Chinese 2 English, Chinese 3Arabic English 5 English Arabic 2 English, Arabic 1French English 2 English French 2 English, French 1Czech English 1 English Czech 2 English, Spanish 1Finnish English 1 English Finnish 1German English 1 English German 1Italian English 1 English Italian 1Spanish English 1 English Spanish 1English Greek 1English Russian 1Table 3: Language pairs studied in ACL 2008 papers29Language family Living Examples % pop.lgs.Indo-European 430 Welsh 44.78PashtoBengaliSino-Tibetan 399 Mandarin 22.28SherpaBurmeseNiger-Congo 1,495 Swahili 6.26WolofBissaAfro-Asiatic 353 Arabic 5.93CopticSomaliAustronesian 1,246 Bali 5.45TagalogMalayTotal 3,923 84.7Table 4: Six most populous language families,from Ethnologueguage families.
Table 4 gives the five most pop-ulous language families, again from Ethnologue.7These language families together account for al-most 85% of the world?s population.Of course, language independence is not theonly motivation for machine-learning approachesto NLP.
Others include scaling to different genreswithin a language, robustness in the face of noisyinput, the argument (in some cases) that creatingor obtaining training data is cheaper than creatinga rule-based system, and the difficulty in certaintasks of creating rule-based systems.
Nonetheless,to the extent that language independence is an im-portant goal, the field needs to improve both itstesting of language independence and its samplingof languages to test against.4 Linguistic KnowledgeTypically, when we think of linguistic knowledge-based NLP systems, what comes to mind are com-plicated, intricate sets of language-specific rules.While I would be the last to deny that such sys-tems can be both linguistically interesting and thebest approach to certain tasks, my purpose here is7Ibid.
Example languages are included to give the readera sense of where these language families are spoken, and aredeliberately chosen to represent the breadth of each languagefamily while still being relatively recognizable to the EACLaudience.to point out that there are other kinds of linguis-tic knowledge that can be fruitfully incorporatedinto NLP systems.
In particular, the results of lan-guage typology represent a rich source of knowl-edge that, by virtue of being already produced bythe typologists, can be relatively inexpensively in-corporated into NLP systems.Linguistic typology is an approach to the sci-entific study of language which was pioneered inits modern form by Joseph Greenberg in the 1950sand 1960s (see e.g.
Greenberg, 1963).8 In the in-tervening decades, it has evolved from a searchfor language universals and the limits of languagevariation to what Bickel (2007) characterizes asthe study of ?what?s where why?.
That is, typol-ogists are interested in how variations on particu-lar linguistic phenomena are distributed through-out the world?s languages, both in terms of lan-guage families and geography, and how those dis-tributions came to be the way they are.For the purposes of improving language-independent NLP systems, we are primarily con-cerned with ?what?
and ?where?
: Knowing?what?
(how languages can vary) allows us to bothbroaden and parameterize our systems.
Know-ing ?where?
also helps with parameterizing, aswell as with selecting appropriate samples of lan-guages to test the systems against.
We can broadenthem by studying what typologists have to sayabout our initial development languages, and iden-tifying those characteristics we might be implic-itly relying on.
This is effectively what Bilmesand Kirchhoff (2003) did in generalizing n-gramlanguage models to factored language models.We can parameterize our systems by identifyingand specifically accommodating relevant languagetypes (?what?)
and then using databases producedby typologists to map specific input languages totypes (?where?
).The practical point of language independence isnot to be able to handle in principle any possi-ble language in the universe (human or extrater-restrial!
), but to improve the scalability of NLPtechnology across the existing set of human lan-guages.
There are approximately 7,000 languagesspoken today, of which 347 have more than 1 mil-lion speakers.9 An NLP system that uses differ-ent parameters or algorithms for each one of a set8See (Ramat, to appear) for discussion of much earlierapproaches.9http://wwww.ethnologue.com/ethno docs/distribution.asp;accessed 6 February 200930of known languages is not language independent.One that uses different parameters or even algo-rithms for different language types, and includes asa first step the classification of the input language,either automatically or with reference to some ex-ternal typological database, is language indepen-dent, at least on the relevant, practical sense.The preeminent typological database amongthose which are currently publicly available isWALS: The World Atlas of Linguistic StructuresOnline (Haspelmath et al, 2008).
WALS currentlyincludes studies of 142 chapters studying linguis-tic features, each of which defines a dimension ofclassification, describes values along that dimen-sion, and then classifies a large sample of lan-guages.
It is also possible to view the data on alanguage-by-language basis.
These chapters rep-resent concise summaries, as well as providingpointers into the relevant literature for more infor-mation.To give a sense of how this information mightbe of relevance to NLP or speech systems, here isa brief overview of three chapters:Maddieson (2008) studies tone, or the use ofpitch to differentiate words or inflectional cate-gories.
He classifies languages into those with notone systems, those with simple tone systems (abinary contrast between high and low tone), andthose with more complex tone systems (more thantwo tone types).
Nearly half of the languages inthe sample have some tone, and Maddieson pointsout that the sample in fact underestimates the num-ber of languages with tone.Dryer (2008b) investigates prefixing and suffix-ing in inflectional morphology, looking at 10 com-mon types of affixes (from case affixes on nouns toadverbial subordinator affixes on verbs), and us-ing them to classify languages in terms of tenden-cies towards prefixing or suffixing.10 His result-ing categories are: little affixation, strongly suf-fixing, weakly suffixing, equal prefixing and suf-fixing, weakly prefixing, and strongly prefixing.The most common category (382/894 languages)is predominantly suffixing.Dryer (2008a) investigates the expression ofclausal negation.
One finding of note is that alllanguages studied use dedicated morphemes to ex-press negation.
This contrasts with the expressionof yes-no questions which can be handled with10For the purposes of this study, he sets aside less com-mon inflectional strategies such as infixing, tone changes, andstem changes.word order changes, intonation, or no overt markat all.
The types of expression of clausal negationthat Dryer identifies are: negative affix, negativeauxiliary verb, and negative particle.
In addition,some languages are classified as using a negativeword that may be a verb or may be a particle, ashaving variation between negative affixes and neg-ative words, and as having double (or two-part)negation, where each negative clause requires twomarkers, one before the verb, and one after it.These examples illustrate several useful aspectsof the knowledge systematized by linguistic typol-ogy: First, languages show variation beyond thatwhich one might imagine looking only at a fewfamiliar (and possibly closely related) languages.Second, however, that variation is still bounded:Though typologists are always interested in find-ing new categories that stretch the current classifi-cation, for the purposes of computational linguis-tics, we can get very far by assuming the knowntypes exhaust the possibilities.
Finally, because ofthe work done by field linguists and typologists,this knowledge is available as high-level gener-alizations about languages, of the sort that caninform the design of linguistically-sophisticated,language-independent NLP systems.5 ConclusionThis paper has briefly argued that the best wayto create language-independent systems is to in-clude linguistic knowledge, specifically knowl-edge about the ways in which languages vary intheir structure.
Only by doing so can we ensurethat our systems are not overfitted to the devel-opment languages.
Furthermore, this knowledgeis relatively inexpensive to incorporate, as it doesnot require building or maintaining intricate rulesystems.
Finally, if the field as a whole valueslanguage independence as a property of NLP sys-tems, then we should ensure that the languages weselect to use in evaluations are representative ofboth the language types and language families weare interested in.AcknowledgmentsI am grateful to Stephan Oepen and Timothy Bald-win for helpful discussion.
Any remaining in-felicities are my own.
This material is based inpart upon work supported by the National ScienceFoundation under Grant No.
0644097.
Any opin-ions, findings, and conclusions or recommenda-31tions expressed in this material are those of the au-thor and do not necessarily reflect the views of theNational Science Foundation.ReferencesSusan Bartlett, Grzegorz Kondrak, and Colin Cherry.2008.
Automatic syllabification with structuredSVMs for letter-to-phoneme conversion.
In Pro-ceedings of ACL-08: HLT, pages 568?576, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Balthasar Bickel.
2007.
Typology in the 21st centure:Major current developments.
Linguistic Typology,pages 239?251.Jeff A. Bilmes and Katrin Kirchhoff.
2003.
Factoredlanguage models and generalized parallel backoff.In in Proceedings of HLT/NACCL, 2003, pages 4?6.Sajib Dasgupta and Vincent Ng.
2007.
High-performance, language-independent morphologicalsegmentation.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;Proceedings of the Main Conference, pages 155?163, Rochester, New York, April.
Association forComputational Linguistics.Matthew S. Dryer.
2008a.
Negative morphemes.In Martin Haspelmath, Matthew S. Dryer, DavidGil, and Bernard Comrie, editors, The World At-las of Language Structures Online.
Max PlanckDigital Library, Munich.
Available online athttp://wals.info/feature/112.
Accessed on 2009-02-07.Matthew S. Dryer.
2008b.
Prefixing vs. suffixingin inflectional morphology.
In Martin Haspelmath,Matthew S. Dryer, David Gil, and Bernard Comrie,editors, The World Atlas of Language Structures On-line.
Max Planck Digital Library, Munich.
Avail-able online at http://wals.info/feature/26.
Accessedon 2009-02-07.Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar.2008.
Better alignments = better translations?In Proceedings of ACL-08: HLT, pages 986?993,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Joseph Greenberg.
1963.
Some universals of grammarwith particular reference to the order of meaningfulelements.
In Univerals of Language, pages 73?113.MIT Press, Cambridge.Martin Haspelmath, Matthew S. Dryer, David Gil, andBernard Comrie, editors.
2008.
The World Atlasof Language Structures Online.
Max Planck DigitalLibrary, Munich.
http://wals.info.Sittichai Jiampojamarn, Colin Cherry, and GrzegorzKondrak.
2008.
Joint processing and discriminativetraining for letter-to-phoneme conversion.
In Pro-ceedings of ACL-08: HLT, pages 905?913, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Fred Karlsson and Andrew Chesterman.
1999.Finnish: An Essential Grammar.
Routledge, Lon-don.Geoffrey Lewis.
1967.
Turkish Grammar.
ClarendonPress, Oxford.Ian Maddieson.
2008.
Tone.
In Martin Haspelmath,Matthew S. Dryer, David Gil, and Bernard Comrie,editors, The World Atlas of Language Structures On-line.
Max Planck Digital Library, Munich.
Avail-able online at http://wals.info/feature/13.
Accessedon 2009-02-07.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of ACL-08: HLT, pages950?958, Columbus, Ohio, June.
Association forComputational Linguistics.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages915?932, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Paolo Ramat.
to appear.
The (early) history of linguis-tic typology.
In The Oxford Handbook of LinguisticTypology.
Oxford University Press, Oxford.Benjamin Snyder and Regina Barzilay.
2008.
Un-supervised multilingual learning for morphologicalsegmentation.
In Proceedings of ACL-08: HLT,pages 737?745, Columbus, Ohio, June.
Associationfor Computational Linguistics.Robert Underhill.
1976.
Turkish Grammar.
MITPress, Cambridge, MA.32
