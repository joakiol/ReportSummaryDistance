Exper iments  in Automated  Lex icon  Bu i ld ing  for Text  Search ingBarry Schiffman and Kathleen R. McKeownDepartment of Computer ScienceColumbia UniversityNew York, NY 10027, USA{ bschiff,kathy} @)cs.columbia.eduAbstractThis paper describes experiment's in the automat'icconstruction of lexicons that would be useflfl insearching large document collect'ions tot text frag~ments tinct address a specific inibrmation eed, suchas an answer to a quest'ion.1 Introduct ionIn develot)ing a syst'em to find answers in text touser questions, we mmovered a major obstacle: Doe-mnent sentences t'hat contained answers dkl not of_ten use the same expressions as the question.
Whilean:;wers in documents and questiolts llse terms that'are relat'e(l to each other, a system that sear(:hes foranswers based on the quesl:ion wording will oftenfail.
3.b address t'his probleln, we develol)ed tech-niques to al,tomatically build a lexicon of associatedterms t'hat can be used to hell) lind al)lIrol/riate bext'seglllent,s.The mismatch })et'ween (tuestion an(l doctlttlentwording was I)rought home to us in an analysis of atestbed of question/answer l/airs.
\~Ze had a collec-tion of newswire articles about the Clinton impeach-ment t'() use as a small-scale corl)uS fin' developmentof ;_t system.
V~Ze asked several )eol)le to 1)ose ques-tions about this well-known t'opic, but we (lid notmake the corpus availal)le to our cont'ril)utors.
\~Zewanted to avoid quest'ions that tracked t'he terminol-ogy in t'he corlms too (:losely to s innl late quest'ionst'o a real-world syst'em.
The result was a set of ques-tions that  used language that' rarely nmtched t'hephrasing in the.
corl)us.
\,Ve had expected t'hat' wewould be able to make most of these lexical connec-tions with the hel l) of V~rordnet (Miller, 1990).For example, consider a simple quest'ion al)out tes-timony: "Did Secret Service agents give testimonyabout' Bill Clinton?"
There is no reason t'o expectthat' the answer would appear 1)aldly st'ated as "Se-cret Service.
agents dkl testi(y ..." What  we needto know is what' testimony is about', where it: occurs,who gives it.
The answer would lie likely to be foundin a passage ment'ioning juries, or 1)roseeut'ors, likethese tbund in our Clinton corl)uS:Starr immediately brought Secret Serviceemployees before tim grand jury for ques-tioning.Prosecutors repeat'edly asked Secret Ser-viee 1)ersonnel to rel)eat' gossil) they mayhave heard.Yet, tile V~ordnet synsets fbr "testinlony" offer:"evidence, assertion, averment alia asseveration,"not a very hell)tiff selection here.
-Wordnet hyper-nyms become general quickly: "declarat'ion," indi-cat'ion" and " infor lnat ion" are only one st, eli u 1) int'lle hierarehy.
Following these does not lead us intoa courtroom.We asked our cont'ril)ut'ors for a second round ofquestions, but this time made the corpus availableto them, exl)laining t'hat we wanted to be sure theanswers were contained in t'he collection of articles.
'J'he result was a set of questions that' mueh moreclosely matched t'he wording in the corpus.
This was~in t'aet, what' the 1999 DARPA question-answering(:oml)et'ition did in order t'o ensure that their ques-tions couhl be answered (Singhal, 1!199).
The sec-trod quest ion-answer ing  conference adopted a newapproach to gathering questions and verifying sepa-rately that' they a.re answerable.Our intuition is t'hat if we can lind the tyl)icallexical neighborhoods of concept's, we can efficientlylocate a concept described in a query or a questionwithout needing to know the precise way the answeris phrased and without relying on a cost'ly, hand-built concept' hierarchy.The example above illustrat'es the 1)oint.
Tes-t imony is given 1) 3, wit'nesses, defendant's, eyewit-nesses.
It is solicited by 1)rosecutors, counsels,lawyers.
It is heard by judges, juries at trials, hear-ings, and recorded in depositions and transcripts.What' we wanted was a complete description of t'heworld of testimony - the who, what, when andwhere of the word.
Or, in other words, the "meta-aboutness" of terms.To this end, we exl)erimented /tSitlg shallow lin-guist.k: techniques t'o gat'her and analyze word co-occurrence data in various configurat'ions.
Unlikeprevious collocation research, we were int'erestedin an expansive set' of relationships between words719rather than a specific relationship.
More important,we felt that the information we needed could be de-rived from an analysis that crossed clause and sen-tence boundaries.
We hyl)othesized that news ar-ticles would be coherent so that the sequences ofsentences and clauses would be linked conceptually.We exanfined the nouns in a number of configura-tions - paragraphs, entences, clauses and sequencesof clauses - and obtained tile strongest results fromconfigurations that count co-occurrences across thesurface subjects of sequences of two to six clauses.Exl)eriments with multi-clause configurations weregenerally more accurate in a variety of experiments.In the next section, we briefly review related re-search.
In section 3 we describe our experiments.In section 4, we discuss the problem of evaluation,and look ahead to future directions in the concludingsections.2 Re la ted  WorkThere has been a large body of work ill the collec-tion of co-occurrence data from a broad spectrum ofperspectives, fi'om information retrieval to the devel-opnlent of statistical methods for investigating wordsimilarity and classification.
Our efforts fall some-where in tile middle.Compared with document retrieval tasks, we aremore closely focused on the words themselves andon specific concepts than on document "aboutness.
"Jing and Croft (1994) exanfined words and phrasesin paragraph units, and found that the associationdata improves retrieval performance.
Callan (1994)compared paragraph units and fixed windows of textin examining passage-level retrieval.In the question-answering context, Morton (1999)collected document co-occurrence statistics to un-cover 1)art-whole and synonymy relationships to usein a question-answering system.
The key differ-ence here was that co-occurrence was considered ona whole-docmnent basis.
Harabagiu and Maiorano(1999) argued that indexing in question answeringshould be based on 1)aragraphs.One recent al)proach to automatic lexicon build-ing has used seed words to lmild up larger sets ofsemmltically similar words in one or nlore categories(Riloff and Shepherd, 1997).
In addition, Strza-lkowski and Wang (1996) used a bootstrapping tech-nique to identify types of references, and Riloff andJones (1999) adapted bootstrapping techniques tolexicon building targeted to information extraction.In the same vein, researchers at Brown Univer-sity (Caraballo and Charniak, 1999)~ (Berland andCharniak, 1999), (Caraballo, 1999) and (Roark andCharniak, 1998) focused on target constructions, inparticular complex noun t)hrases, and searched forinformation ot only on identifying classes of nouns,lint also hypernyms, noun specificity and meronymy.We have a diflbrent perspective than these lines ofinquiry.
They were specifying various semantic rela-tionships and seeking ways to collect similar pairs.We.
have a less restrictive focus and are relying onsurface syntactic information about clauses.For more than a decade, a variety of statisticaltechniques have been developed and refilled.
Tilefocus of much of this work was to develop themethods themselves.
Church and Hanks (1989) ex-plored tile use of mutual information statistics inranking co-occurrences within five-word windows.Smadja (1992) gathered co-occurrences within five-word windows to find collocations, particularly inspecific domains.
Hindle (1990) classified nounson the basis of co-occurring patterns of subject-verb and verb-object pairs.
Hatzivassiloglou andMeKeown (1993) clustered adjectives into semanticclasses, and Pereira et al (1993) clustered nouns ontheir appearance ill verb-object pairs.
We are try-ing to be less restrictive in learning multiple salientrelationshil)s between words rather than seeldng aparticular elationship.Ill a way, our idea is the mirror image of Barzilayand Elhadad (1997), who used Wordnet to identifylexical chains that would coincide with cohesive textsegments.
We assunmd that documents are cohesiveand that co-occurrence l)atterns call uncover wordrelationships.3 ExperimentsTile focus of onr experiment was on units of text inwhich the constituents must fit together in order forthe discourse to be coherent.
We made the assump-tion that the documents in our corpus were coherentand reasoned that if we had enough text, coveringa broad range of topics, we could pick out domain-independent associations.
For example, testimonycan be about virtually anything, since anything canwind up in a court dispute.
But over a large enoughcollection of text, the terms that directly relate totile "who," "what" and "where" of testimony perse should appear in segments with testimony morefrequently than chance.These associations do not necessarily appear in adictionary or thesaurus.
When huntans explain allunfamiliar word, they often use scenarios and analo-gies.We divided the experiments in two groups: onegroup that looks at co-occurrences within a singleunit, and another that looks at a sequence of units.In the first group of experinmnts, we consideredparagraphs, sentences and clauses, each with andwithout prepositional phrases.?
Single paragraphs with/without PP?
Single sentences with/without PP?
Single clauses with/without PP720\]in the second group, we considered two clausesand sequences of subject 110un phrases from two tosix chmses.
Ill this group, we had:,, Two clauses with/without Pl),, A sequence of subject NPs fl'onl 2 clausesA sequence of subject NPs Dora 3 clauses,, A sequence of subject NPs from 4 clauses?
A sequence of subject NPs fi'om 5 clauses,, A sequence of subject NPs from 6 clausesThe intuition for the second groul) is that a topicflows from one granmm.tical unit to another so thatthe salient nouns, l)articularly the surface subjects,in successive clauses should reveal the associationswe are seeldng.
'\[lo illustrate the method, consider the three-clauseconfiguration: Say that ~vordi apl)ears in clausc,~.We maintain a table of all word pairs and incrementthe entries for O,,o,'(h , ',,,o,'d~ ), where , ,0 ,% is a sub-ject noun in cla'usc,~, clauscn+~, or ell'use,+2.
Noeffort was made to resolve pronomial references, andthese were skipped.We used nollnS Olfly' because l)reliminary testsshowed that pairings between ouns seemed to standout.
V~Te included tokens that were tagged as 1)ropernall leS when they also have have con ln lon  n lean ings .For example, consider the Linguistic Data Consor-l;ium at the University of Pennsylvania.
Data, Con-sortium and University wouM be on tile list used tobuild the table of nmtchul)s with other nouns, \])litl)emlsylvania would not.
V~To also collected nounmodifiers as well as head nouns as they can carrymore information than the surface heads, such as"business group", '".science class" or "crinm scene.
"The corpus consisted of all tile general-interest ar-ticles from the New York Tinms newswire in 1996in the North American News Corlms , and (lid notinclude either st)orts or l)usiness news.
We tirst re-moved dul)licate articles.
The data fl'om 1996 wastoo slmrse for the sequence-of-subjects ontigura-lions.
'\]'o l)alance the expcrinmnts better, we addedanother year's worth of newswire articles, from 1995,tbr the sequence-of subject configurations sothat wehad more than one million matchups for each con-figuration (Table 1).The I)roeess is flflly automatic, requiring no su-1)ervision or training examples.
The corpus wastagged with a decision-tree tagger (Schmid, 1994)and parsed with a finite-state parser (Abney, 1996)using a specially written context-fi'ee-grannnar thatfocused on locating clause boundaries.
The gram-mar also identified extended noun l)hrases in tile sub-ject position, verb l)hrases and other noun l)hrasesand prepositional 1)hrases.
The nouns in the tagged,parsed corl)uS were reduced to their syntactic roots(removing l)lurals from nouns) with a lookup tablecreated t'rom Wordnet (Miller, 1990) and CELEX(1995).
We.
performed this last step mainly to ad-dress the sparse data problem.
There were a sub-stantial nunfl)er of paMngs that occurred only once.We elinfinated from considerat;ion all such single-tons, although it did not al)peal to have much etfecton the overall outcome.Confi.q MatchupsPara +pp 6.5 millionSent 1.7 millionSent +pp 4 million1 Clause 1.1 million1 Clause +pp 2.8 million2 Clause 1.9 million2 Clause +I)P 5 nfillionSubj 2 Clause 1.1 million*Subj 3 Clause 1.6 million*Subj 4 Clause 2.1 million*Subj 5 Clause 2 .6m~Subj 6 Clause 3.1 million*'lhble 1: Nmnl)er of matchut)s ibund; tile "*" de-notes the inclusion of 1995 dataThere were about 1.2 million paragraphs, 2.2 mil-lion sentences and 3.4 million clauses in the selectedportions of the 1996 COl'pus.
The total number ofwords was 57 million.
Table 2 shows the nmnl)er ofdistinct nouns.I I All ExtractedNo l)ps 74,500W/pps 91,700Subjs 51,000Counts > 1.44,40053,90030,800Td)le 2: Distinct Nouns, 1996 DataTo score the nmtchups in our initial exlmriments ,we used the Dice Coeliicient, which l)roduces valuesi'ronl 0 to 1, to measure the association between pairsof words and then produced an ordered associationlist fl'om the co-occurrence table, ranked accordingto the scores of the entries.2 ?
f, '~q(wo,.,h n ,oo ,%)score,, = frcq(wordi) + frcq(wordj)One 1)roblem was immediately al)parent: Thequality of tile association lists wxried greatly.
Tilescoring was doing an acceptable job in ranking thewords within each list, but tile scores varied greatlyfrom one list to another.
Our initial strategy wasto choose a cutoff, which we set at 21 tbr each list,and we tried several alternatives to weed out weakassociations.721In one method, we filtered the association listsby cross-referencing, removing from the associationlist for wordi any wordj that failed to reciprocateand to give a high rank to wordi on its associationlist.
Another similar approach was to try to con>bine evidence fl'om different experiments by takingthe results fl'om two configurations into considera-tion.
A third strategy was to calculate the mutualinformation between the target word and the otherwords on its association list.scorc,,i = p(xy) * log \p(z)p(y) ( (xy) )Using the mutual information computation pro-vided an way of using a single measure that was ableto compare matchups across lists.
We set a thresholdof lxl.0 -6 for all matchups.
Thus these associationlists vary in length, depending on the distributionsfor the words, allowing them to grow up to 40, whilesome ended up with only one or two words.4 EvaluationThe evaluation of a system like ours is problematic.The judgments we made to determine correctnesswere not only highly subjective but time-consunfing.We had 12 large lexicons fl'om the different config-urations.
We had chosen a random sample of 10percent of the 2,700 words that occurred at least100 times in tile corpus, and manually constructedan answer key, which ended up with ahnost 30,000entries.From the resulting 270 words, we discarded 15 ofthose that coincided with common names of peo-ple, such as "carter," which could refer to the for-mer American president, Chris Carter (creator oftile television show "X-Files"), among others.
Wethought it better to delay making decisions on howto handle such cases, especially since it would requiredistinguishing one Carter fl'om another.
Such wordspresented several difficulties.
Unless the individualsinvolved were well-known, it was often impossible todistinguish whether the system was making errorsor whether the resulting descriptive terms were in-tbrmative.Tables 3 and 4 show an example from the answerkey tbr the word "faculty.
"The overall results from the first stage of the pro-cess, before the cross-referencing filter are shown inTable 5, ranging from 73% to 80% correct.
The con-figurations that included prepositional phrases andthose that used sequences of subject noun phrasesoutperformed the configurations that relied on suh-jects and objects in a single grammatical unit.
Thesedifferences were statistically significant, with p <0.01 in all eases.The overall results after cross-referencing, in Ta-ble 6, showed improvements of 5 to 10 percentageenrollment hiring adnfinistratorjournalism alumnus studentschool union mathengineering curriculum trusteegroup seminar thesistenure stair departmentmathematician educator memberivy arts collegechancellor report senateactivism university el,airmanprofessor teaching lawregent doctorate mtministrationacademic committee semesterboard camI)us undergraduatesalary council researchpresident adviser mathematicscourse advisor sociologydean study scienceteacher cannon provostvoteTable 3: Answer Key for Faculty: OKload tratllcway unrestarchitecture diversity hurdleshield minority revisiondisburse percent womanclementTable 4: Answer Key ff)r Faculty: Wrongpoints, while the effect of the number of matchupswas diminished.
Here, the subject-sequence onfig-urations showed a distinct advantage.
While morenoise might be expected when a large segment of text;is considered, these results support the notion thatthe nnderlying coherence of a discourse can be recov-ered with the prol)er selection of linguistic features.The improvements in each configuration over thecorresponding configuration in the first stage wereall statistically significant, with p < 0.01.
Likewise,the edge the sequence-of subjects configurations hadover tile other configurations, was also statisticallysignificant.The results fl'om combining the evidence from dif-ferent configurations, in Table 7, showed a muchhigher accnrae> but a sharp drop in the total nnm-ber of associated words found.
The most fl'uitfulpairs of experiments were those that combined dis-tinct approaches, for example, tile five-subject con-figuration with either fifll paragraphs or with sen-tences with prepositional phrases.
It will remainunclear until we conduct a task-based evaluationwhether the smaller number of associations will beharnfful.The final experiment, computing the mutual in-formation statistic tbr the matchul)s of a key wordwith co-occurring words was perhaps the most ill-teresting because it gave us the ability to apply a722(Jontig OK Wrong l)ct OKPara +l/ l)  3832 1054 78,qent 3773 1270 75Sent +Pl) 3973 1070 79\] Clause 3652 1371 73\] Clauses q-l)l) 3935 1108 78"!
Clauses 3695 1328 74"!
Clauses -t-l)l) 3983 1018 80Subj 2 CI 3877 1139 77Sul)j 3 CI 3899 1117 78Subj 4 CI 3!
)(/5 :1082 78Sul)j 5 C1 390d 1076 78Sul)j 6 CI 3909 1066 7!
)Table 5: Results 13efore Cross I loferencingContig ()K Wrong Pet ()KPara q-Pl) 3651} 73/1 83Sent 3328 742 82Sent -bpp 3751 8:18 82:1 Clause 3067 748 801 Clauses +1)I / 3659 826 822 Cbmses 3048 55d 852 Clauses +pp 3232 60d 8dSubj 2 CI 2910 450 87t-;ul~j 3 CI 3020 4d() 87Subj 4 CI 3050 d28 88l~tll).j 5 (J\] ;1:12t3 dd2 88Subj 6 C1 3237 dd9 88' lhble 6: l{esults After Cross Referencingsingle threshold across different key words, savingthe effort of performing the cross-retbrencing calm>lations and providing a deeper assorl:ment in SOllleC~lSeS.
lilt lnost of the configurations, lltlltllPl illfOr-mat.ion gave 118 lllore \Vol'ds, and greater ln'ecisionat; the sanle time, but nlost of all, gave us a reason-able threshold to apply throughout  he exlicrinlent.Whi le the accuracies in most of the configurationswere close to one another,  those that  used only sin-g\]e units tended to be weaker than the mult i -c lauseunits.
Note that  the paragraI)h contiguration wastested with far more data  than any of the others.Our system maD~s no eth)rt to aeCOllnt for lexi-cal aml)iguil;y.
The uses we intend for our lexiconshould provide some insulat ion from the ett'ects ofpolysemy, since searches will be conducted on a nun>l)er of terms, which should converge to one meaning.It is clear that  in lists for key words with mult i -ple senses, the donfinant sense where there is one,al)pears much lnore frequently, such as "faculty ,"where the meaning of "teacher" is more t:'re(tuentthan the meaning of "al)ility."
F igure \] shows thetop 21 words in the sequence-otCsix subjects,  beibrethe cross-referencing ii lter was applied.
Twenty ofthe 21. entries were scored aeceptal)le.After the cross-referencing is applied, doctorate,,education and revision were elinfinated.Contig OK \?rong Pcl; OKl~ara 2003 183 92Sent 1962 222 90Sent-t- 2033 213 911 Clause 1791 218 891 C lause+ 2004 198 912 Clause 2028 277 882 C lause+ 2:129 24,1 90Tal)le 7: Results of coml)ining evidence; all configu-rat ions were combined with the sequence of six sub-jectsConlig OK "Wrong Pet OKPara +pp 4923 807 86Sent 5193 990 84Sent +Pl) 4876 775 861 Claus(; 52!
)!/ 1233 811 Clauses-t-l)l) 5047 878 852 Clauses 5025 928 842 Clauses -I-Pl) d668 728 87Subj 2 C1 5229 939 85Subj 3 C1 5187 860 85Subj ~1 C1 5119 808 86Subj 5 C1 500"{ 76d 87Subj 6 CI 4!
)80 736 87Table 8: l lesults with mutual  informationThe results from the single clause configuration(Figure 2) were almost as strong, with three erroFs,and a fair amount of overlap between the two.The word "admiral" was more difficult %r the ex-\])erilllellt ilSilig the l)ice coefficient.
The.
list showssome of l.he confusion arising from our strate.~y Ollprot)er nouns.
Admiral  would be expected to oc-cur with many proper ll~tnles, i l lcluding some thataxe st)elled liD; common 11o1111.q, bi l l  the list h)r thesingle clause q pp conf iguratkm presented a lmzzlinglist (F igure 3).The sparseness of the data  is also al)lmrent, but itwas the dog reDxenees that  al)peared quite strangeat a ghulce: Inspection of the.
articles showed thatthey callle froln all a.rticle on the pets of famouspeople.
Note that the dogs did not al)l)ear in topranks of the sequence of subjects  configuration inthe Dice exper iment (Figure 4), nor were they in theresults t'rom the experiments with cross-referencing,combining evidence and mutua l  information.After cross-reR;rencing, the much-shorter list forthe Sub j-6 configuration had "aviator",  "break-up",' ;commander",  "decoration",  "equal-ot)portunity","tleet", "merino", "navf ' ,  "pearl",  "promotion","rear" ~ alia "short".'
l 'he combined-evidence list contained only eightwords: "navy", "short", "aviator",  "merino", "dis-honor",  "decoration",  "sul)" and "break-ul)".Using the mutual  intbr lnat ion scoring, the listin the Subj-6 configuration tbr admiral  had only723faculty trustee(51) 0.053; carat)us(d1) (/.045;college(ll3) 0.034; member(369) 0.028; profes-sor(102) 0.028; university(203) 0.027; student(206)0.025; regent(19) 0.025; tenure(15) 0.025; ctmncel-lor(28) 0.023; administrator(34) 0.023; provost(12)0.023; dean(27) 0.021; ahmmus(13) 0.021; math(12)0.017; revision(8) 0.013; salary(13) 0.013; so-ciology(7) 0.013; educator(l l) 0.012; doctorate(6)0.011.; teaching(9) 0.011;Figure 1: Tile top-ranked matchups for "fac-ulty" from the Subj-6-Clause configuration be-fore cross-referencing.
The nmnbers in paren-theses are the number of matchups and the realumnbers following are the scores.
Errors are inboldfaculty trustee(31) 0.033; meml)er(266) 0.025; ad-nfinistrator(31) 0.023; college(42) 0.012; dean(15)0.012; tenure(8) 0.011; ivy(6) 0.011; staff(a3) 0.01;semester(6) 0.01; regent(7) 0.01; salary(12) 0.01;math(7) 0.008; professor(a1) 0.008; load(6) 0.007;curricuhun(5) 0.006; revision(4) 0.006; minor-ity( l l )  0.006;Figure 2: The top-ranked matchups for "fac-ulty" under the single clause confignration.
Er-rors are in bold.nine words: "navy", "general", "commander","vice", "promotion", "officer", "fleet", "military"and "smith.
"Finally, the even-sparser mutual information listfor the paragraph configuration lists only "navy"and "suicide.
"5 Conc lus ionOur results are encouraging.
We were able to deci-pher a broad type of word association, and showedthat our method of searching sequences of subjectsoutperformed the snore traditional approaches infinding collocations.
We believe we can use tiffs tech-nique to build a large-scale l xicon to help in diffi-cult information retrieval and information extractiontasks like question answering.The most interesting aspect of" this work lies inthe system's ability to look across several clausesand strengthen tile connections between associatedwords.
We are able to deal with input that con-tains numerous errors from the tagging and shallowparsing processes.
Local context has been studiedextensively in recent years with sophisticated statis-tical tools and the availability of enormous amountsof text in digital form.
Perhaps we can expand thisperspective to look at a window of perhaps everalsentences by extracting the correct linguistic units inorder to explore a large range of language processingproblems.admiral- navy(all) 0.027; ayMon(d) 0.024; cheat-ing(5) 0.02; gallantry(3) 0.016; chow(4) 0.015; ser-vice,nan(d) 0.013; short(3) 0.013; wardroom(2)0.012; american(2) 0.012; enos(2) 0.012; self-assessment(2) 0.
(/11; merino(2) 0.011; ocelot(2)0.011; wolfhound(2)0.011; igloo(2)0.011; pa-prika(2) 0.011; spaniel(2) 0.01; medal(8) 0.01;awe(a) 0.01; pedigree(2) 0.009; te,'rier(2) 0.009;Figure 3: Top-ranked matchups for "adnfiral"under the clause +pp configuration.admiral - navy(88) 0.071; short(7) 0.03; promo-tion(ll) 0.027; hal)l)iness(8) 0.026; fleet(ll) 0.024;aviator(5) 0.022; mnbition(8) 0.019; merino(3)0.019; dishonor(3)0.018; rear(4)0.018; deco-ration(4) 0.015; sub(a) 0.013; airman(3) 0.013;graveses(2) 0.012; submariner(2) 0.012; equal-opportunity(2) 0.012; break-up(2) 0.012; comman-der(18) 0.012; pearl(7) 0.012; l)rophccy(d) 0.01.2;torturer(2) 0.012;Figure 4: The list for admiral fi'om the Sub j-6contiguration.6 Future  Work?
We will have the scoring key itself evaluated bypeople who are not involved in tile research.?
~re are planning to conduct ask-based evalua-tion in question answering.?
We are considering deploying a named entitymodule to provide sonic classification of whichproper nouns should be counted and whichshould not.?
We 1)lan to experiment with ways to incorpo-rate using examining verbs and making use ofsurface objects in the configurations with se-quences of clauses, as well as strengthen the fi-nite state grammar.?
We will explore using tile system to extract bi-ographic information.AcknowledgmentsThis material is based upon work supported by tileNational Science Foundation under grants Nos.
IIS-96-19124 and IRI-96-18797, and work jointly sup-ported by the National Science Foundation and theNational Library of Medicine under grant No.
IIS-98-17434.
Any opinions, findings, and conclusionsor recmmnendations expressed in this material arethose of tile authors and do not necessarily reflectthe views of the National Science Foundation.724ReferencesSteven Abney.
1996.
Partial parsing via finite-statecascades.
In Proceedin9s of th, e ESSLLI '95 RobustParsin9 Workshop.Regina Barzilay and Michael Elhadad.
1997.
Usinglexical chains tbr text smmnarization.
In Pwcced-ings of the Ntelligent Scalable Text b'ummariza-tion Workshop.
ACL.Matthew Berland and Eugene Charniak.
1999.Finding parts in very large corpora.
'l.bchnical Re-port TR CS99-02, Brown University.James P. Callan.
1994.
Passage-level vidence indocument retrieval.
In Proceedin9s of the Seven-teenth Annual Intcunational A CM SIGIR Confer-ence, Dublin, Ireland.
ACM.Sharon Caraballo and Eugene Charniak.
1999.
De-termining the speciticity of nouns from text.
InP~vceedinfls of Co~@rcnce on E,mpi~eal Methodsin Nat'u'ral Langua9e Processing.Sharon Caraballo.
1999.
Automatic acquisition ofa hylmrnym-labeled noun hierarchy from text.
InPwceedings of th, e 37th Annual Meeting of the As-sociation for Comp'utational Linguistics, June.CELEX, 19!)5.
Tit(; CELEX lezical databaseDutch,, English, Ge.rntan.
Centr for Lexical hffor-mation, Max Planck Institute for Psycholinguis-ties, Nijmegen.Kenneth W. Church and Patrk:k Itanks.
1989.
Wordassociation orms, mutual infornmtion and lexi-cography.
In Proceedings of th.e 27th.
nteetin9 ofthe ACL.S&nda ~/\[.
Ilara,bagiu anti S|;even J. Maiorano.
1999.Finding answers in large collectkms of texts: Para-graph indexiltg -t- adductive inference.
In Q'aes-tion Answering Systema'.
AAAI, November.Vasileios Hatziw~ssiloglou and Kathleen R. McKe-own.
1993.
'lbwards the automatic identificationof adjectival scales: Clustering adjectives accord-ing to meaning.
In P~vceedin9 s of th, c 31st AnnualMeeting of th, e A CL.Donald Hindle.
1990.
Noun classitication ti'ompredicate-argument structures.
In PTvceedin9s ofthe 28th Annual Meeting of the A CL.Yufcng Jing and W. Bruce Croft.
1994.
An associa-tion thesaurus for information retrieval, tech.
rep.no 94-17.
2bchnical report, Amherst: Universityof Massachusetts, Center for Intelligent hfforma-tion Retrieval.G.
Millet'.
1990.
Wordnet: An on-line lexicaldatabase.
International .
ournal of Lezicoqraphy.Thomas S. Morton.
1999.
Using coreibrence torquestion answering.
In P~vccedings of thc Work-shop on Coreference and Its Applications, l)ages85-89, College Park, Maryland, June.
Associa-tion for Computational Linguisties, Associationfor Computation Linguistics.Fernando Pereira, Naffali Tishby, and Lillian Lee.1993.
Distributional clustering of english words.In Pwcecdings of the 31st Annual Meeting of theACL.Ellen Rilotf and Pmsie Jones.
1999.
Learning die-tionarics for intbrmation extraction by multi-level bootstral)ping.
In Proceedings of the Six-teenth Na, tional Co~@rencc on Artificial Intelli-gence.
AAAI.Ellen I{ilotf and Jessica Shepherd.
1997.
A corpus-based approach for building semantic lexicons.
InProceedings of the Second Conference on Empir~i -cal Meth, ods in Natural Langua9 c Processing.Brian lloark and Eugene Charniak.
1998.
Noun-phrasae co-occurrence statistics for semi-automatic semantk: lexicon construction.
InP~vcccdings of thc 36th Annual Meetin9 of theAssociation for Computational Linguistics andthe 17th htternational Conference on Computa-tion Linguistics.Hehnut Schmid.
1994.
Probabilistic part-of speechtagging using decision trees.
In Proceedings of theInternational Cor@rence on New Methods in Lan-9ua.qe Proecssin9.Amit Singhal.
1999.
Question and answer trackhome page.
WWW.Frank Smadja.
1992.
Retrieving collocations fi'omtext: Xtract.
Comp'll, tational Linguistics, SpecialIssue.Tomek Strzalkowski and Jin V~rang.
1996.
A self-learning universal concept spotter.
In lhvceedinflsof th, e International 6'm@renee on ComputationalLinfluisties (Colin 9 199@.725
