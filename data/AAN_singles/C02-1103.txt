Automatic Text Categorization using the Importance of SentencesYoungjoong Ko, Jinwoo Park, and Jungyun SeoDepartment of Computer Science,Sogang University1 Sinsu-dong, Mapo-guSeoul, 121-742, Korea{kyj,jwpark}@nlpzodiac.sogang.ac.kr, seojy@ccs.sogang.ac.krAbstractAutomatic text categorization is a problemof automatically assigning text documents topredefined categories.
In order to classifytext documents, we must extract goodfeatures from them.
In previous research, atext document is commonly represented bythe term frequency and the inverteddocument frequency of each feature.
Sincethere is a difference between importantsentences and unimportant sentences in adocument, the features from more importantsentences should be considered more thanother features.
In this paper, we measure theimportance of sentences using textsummarization techniques.
Then a documentis represented as a vector of features withdifferent weights according to theimportance of each sentence.
To verify ournew method, we conducted experiments ontwo language newsgroup data sets: onewritten by English and the other written byKorean.
Four kinds of classifiers were usedin our experiments: Na?ve Bayes, Rocchio,k-NN, and SVM.
We observed that our newmethod made a significant improvement inall classifiers and both data sets.IntroductionThe goal of text categorization is to classifydocuments into a certain number of pre-definedcategories.
Text categorization is an activeresearch area in information retrieval andmachine learning.
A wide range of supervisedlearning algorithms has been applied to thisproblem using a training data set of categorizeddocuments.
For examples, there are the Na?veBayes (McCallum et al, 1998; Ko et al, 2000),Rocchio (Lewis et al, 1996), Nearest Neighbor(Yang et al, 2002), and Support VectorMachines (Joachims, 1998).A text categorization task consists of atraining phase and a text classification phase.The former includes the feature extractionprocess and the indexing process.
The vectorspace model has been used as the conventionalmethod for text representation (Salton et al,1983).
This model represents a document as avector of features using Term Frequency (TF)and Inverted Document Frequency (IDF).
Thismodel simply counts TF without consideringwhere the term occurs.
But each sentence in adocument has different importance foridentifying the content of the document.
Thus,by assigning a different weight according to theimportance of the sentence to each term, we canachieve better results.
For this problem, severaltechniques have been studied.
First, termweights were differently weighted by thelocation of a term, so that the structuralinformation of a document was applied to termweights (Murata et al, 2000).
But this methodsupposes that only several sentences, which arelocated at the front or the rear of a document,have the important meaning.
Hence it can beapplied to only documents with fixed form suchas articles.
The next technique used the title of adocument in order to choose the important terms(Mock et al, 1996).
The terms in the title werehandled importantly.
But a drawback of thismethod is that some titles, which do not containwell the meaning of the document, can ratherincrease the ambiguity of the meaning.
This caseoften comes out in documents with a informalstyle such as Newsgroup and Email.
Toovercome these problems, we have studied textsummarization techniques with great interest.Among text summarization techniques, there arestatistical methods and linguistic methods(Radev et al, 2000; Marcu et al, 1999).
Sincethe former methods are simpler and faster thanthe latter methods, we use the former methods tobe applied to text categorization.
Therefore, weemploy two kinds of text summarizationtechniques; one measures the importance ofsentences by the similarity between the title andeach sentence in a document, and the other bythe importance of terms in each sentence.In this paper, we use two kinds of textsummarization techniques for classifyingimportant sentences and unimportant sentences.The importance of each sentence is measured bythese techniques.
Then term weights in eachsentence are modified in proportion to thecalculated sentence importance.
To test ourproposed method, we used two differentnewsgroup data sets; one is a well known dataset, the Newsgroup data set by Ken Lang, andthe other was gathered from Korean UseNetdiscussion group.
As a result, our proposedmethod showed the better performance thanbasis system in both data sets.The rest of this paper is organized as follows.Section 1 explains the proposed textcategorization system in detail.
In section 2, wediscuss the empirical results in our experiments.Section 3 is devoted to the analysis of ourmethod.
The final section presents conclusionsand future works.1.
The Proposed Text CategorizationSystemThe proposed system consists of two modules asshown in Figure 1: one module for trainingphase and the other module for textclassification phase.
The each process of Figure1 is explained in the following sections.? 	 	 			? Figure 1.
Overview of the proposed system1.1 PreprocessingA document from newsgroup data is composedof subject, author, data, group, server, messageID, and body.
In our system, we use only thecontents of subject and body.The contents of documents are segmented intosentences.
Then we extract content words fromeach sentence and represent each sentence as avector of content words.
To extract contentwords, we use two kinds of POS taggers: Brilltagger for English and Sogang POS tagger forKorean.
We employ TF values as term weightsof content words in each sentence.1.2 Measuring the importance ofSentencesThe importance of each sentence is measured bytwo methods.
First, the sentences, which aremore similar to the title, have higher weights.
Inthe next method, we first measure theimportance of terms by TF, IDF, and ?2 statisticvalues.
Then we assign the higher importance tothe sentence with more important terms.
Finally,the importance of a sentence is calculated bycombination of two methods.1.2.1 The importance of sentences by the titleGenerally, we believe that a title summarizes theimportant content of a document (Endres-Niggemeyer et al, 1998).
By Mock (1996),terms occurred in the title have higher weights.But the effectiveness of this method depends onthe quality of the title.
In many cases, the titlesof documents from Newsgroup or Email do notrepresent the contents of these documents well.Hence we use the similarity between eachsentence and the title instead of directly usingterms in the title.
The similar sentences to thetitle contain important terms generally.
Forexample, ?I have a question.?
This title does notcontain any meaning about the contents of adocument.
Nevertheless, sentences with the term,?question?, must be handled importantly becausethey can have key terms about the question.We measure the similarity between the titleand each sentence, and then we assign the higherimportance to the sentences with the highersimilarity.
The title and each sentence of adocument are represented as the vectors ofcontent words.
The similarity value of them iscalculated by the inner product and thecalculated values are normalized into valuesbetween 0 and 1 by a maximum value.
Thesimilarity value between title T and sentence Siin a document d is calculated by the followingformula:)(max),(TSTSTSSimidSiiirrrr??=?
(1)where Trdenotes a vector of the title, and iSrdenotes a vector of a sentence.1.2.2 The importance of sentences by theimportance of termsSince the method by the title still depends on thequality of the title, it can be useless in thedocument with a meaningless title or no title.Besides, the sentences, which do not containimportant terms, need not be handledimportantly although they are similar to the title.On the contrary, sentences with important termsmust be handled importantly although they aredissimilar to the title.In consideration to these points, we firstmeasure the importance values of terms by TF,IDF, and ?2 statistic value, and then the sum ofthe importance values of terms in each sentenceis assigned to the importance value of thesentence.
Here, since the ?2 statistic value of aterm presents information of the term fordocument classification, it is added to ourmethod unlike the conventional TF-IDF.
In thismethod, the importance value of a sentence iS ina document d is calculated as follows:??????????????=?????iiiStdSStittidfttfttidfttfSCen)()()(max)()()()(22??
(2)where tf(t) denotes term frequency of term t,idf(t) denotes inverted document frequency, and?(t)denotes ?2 statistic value.1.2.3 The combination of two sentenceimportance valuesTwo kinds of sentence importance are simplycombined by the following formula:)(),(0.1)( 21 iii SCenkTSSimkSScore ?+?+= (3)In formula (3), k1 and k2 are constant weights,which control the rates of reflecting twoimportance values.1.2.4 The indexing processThe importance value of a sentence by formula(3) is used for modifying TF value of a term.That is, since a TF value of a term in a documentis calculated by the sum of the TF values ofterms in each sentence, the modified TF value(WTF(d,t)) of the term t in the document discalculated by formula (4).??
?=dSiiiSScoretStftdWTF )(),(),( (4)where tf(Si,t) denotes TF of the term t insentence Si.By formula (4), the terms, which occurr in asentence with the higher importance value, havehigher weights than the original TF value.
In ourproposed method, we compute the weightvectors for each document using the WTF andthe conventional TF-IDF scheme (Salton et al,1988).
The weight of a term t in a document d iscalculated as follows:?= ?????????????????????????
?=Ti titinNtdWTFnNtdWTFtdw12log),(log),(),( (5)where N is the number of documents in thetraining set, T is the number of features limitedby feature selection, and nt is the number oftraining documents in which t occurs.The weight by formula (5) is used in k-NN,Rocchio, and SVM.
But Na?ve Bayes classifieruses only WTF value.2.
Empirical Evaluation2.1 Data Sets and Experimental SettingsTo test our proposed system, we used twonewsgroup data sets written by two differentlanguages: English and Korean.The Newsgroups data set, collected by KenLang, contains about 20,000 articles evenlydivided among 20 UseNet discussion groups(McCallum et al, 1998).
4,000 documents(20%) were used for test data and the remaining16,000 documents (80%) for training data.
Then4,000 documents from training data wereselected for a validation set.
After removingwords that occur only once or on a stop word list,the vocabulary from training data has 51,018words (with no stemming).The second data set was gathered from theKorean UseNet group.
This data set contains atotal of 10,331 documents and consists of 15categories.
3,107 documents (30%) are used fortest data and the remaining 7,224 documents(70%) for training data.
The resultingvocabulary from training data has 69,793 words.This data set is uneven data set as shown inTable 1.Table 1 The constitution of Korean newsgroup data setCategory Trainingdata Test data Totalhan.arts.music 315 136 451han.comp.databas 198 86 284han.comp.devtools 404 174 578han.comp.lang 1,387 595 1,982han.comp.os.linux 1,175 504 1,679han.comp.os.window 517 222 739han.comp.sys 304 131 435han.politics 1,469 630 2,099han.rec.cars 291 126 417han.rec.games 261 112 373han.rec.movie 202 88 290han.rec.sports 130 56 186han.rec.travel 102 45 147han.sci 333 143 476han.soc.religion 136 59 195Total 7,224 3,107 10,331We used ?2 statistics for statistical featureselection (Yang et al, 1997).
To evaluate ourmethod, we implemented Na?ve Bayes, k-NN,Rocchio, and SVM classifier.
The k in k-NN wasset to 30 and ?=16 and ?=4 were used in ourRocchio classifier.
This choice was based on ourprevious parameter optimization learned byvalidation set.
For SVM, we used the linearmodels offered by SVMlight.As performance measures, we followed thestandard definition of recall, precision, and F1measure.
For evaluation performance averageacross categories, we used the micro-averagingmethod and macro-averaging method.2.2 Experimental ResultsWe tested our system through the followingsteps.
First, using the validation set ofNewsgroup data set, we set the number offeature and the constant weights (k1 and k2) inthe combination of two importance values in thesection 1.2.3.
Then, using the resulting values,we conducted experiments and compared oursystem with a basis system; the basis systemused the conventional TF and our system usedWTF by formula (4).2.2.1 Setting the number of featuresFirst of all, we set the number of features in eachclassifier using validation set of training data.The number of features in this experiment waslimited from 1,000 to 20,000 by featureselection.
Figure 2 displays the performancecurves for the proposed system and the basissystem using SVM.
We simply set both constantweights (k1 and k2) to 1.0 in this experiment. Figure 2.
Comparison of proposed system and basissystem using SVMAs shown in Figure 2, the proposed systemachieved the better performance than the basissystem over all intervals.
We set the number offeatures for SVM to 7,000 with regard to theconvergence of the performance curve andrunning time.
By the similar method, the numberof features in other classifiers was set: 7,000 forNa?ve Bayes, 10,000 for Rocchio, and 9,000 fork-NN.
Note that, over all intervals and allclassifiers, the performance of the proposedsystem was better than that of the basis system.2.2.2 Setting the constant weights k1 and k2In advance of the experiment for setting theconstant weights, we evaluated two importancemeasure methods and their combination methodindividually; we used simply the same value fork1 and k2 (k1=k2) in the combination method(formula (3)).
We observed the results in eachinterval when constant weights were changedfrom 0.0 to 3.0.
In Figure 3, Sim(S,T) denotesthe method using the title, Cen(S) the methodusing the importance of terms, andSim(S,T)+Cen(S) the combination method.   	          	! ""#$" ""#%$"Figure 3.
Comparison of importance measuremethods in different constant weights (k1 and k2)In this experiment, we used SVM as a classifierand set feature number to 7,000.
We mostlyobtained a best performance in the combinationmethod.In order to set the constant weights k1 and k2in each classifier, we carried out the total 900trials on the validation set because each methodhad 30 intervals from 0.0 to 3.0 (interval size:0.1).
As a result, we obtained the bestperformance at 1.5 (k1) and 0.4 (k2) for SVM:1.9 and 3.0 for Na?ve Bayes, 2.0 and 0.0 forRocchio, and 0.8 and 2.8 for k-NN.
Theseconstant weights of each classifier were used inthe following experiments.2.2.3 Results in two newsgroup data setsIn this section, we reported results in twonewsgroup data sets using parametersdetermined above experiments.Table 2.
Results in English newsgroup data setNa?ve Bayes Rocchiobasissystemproposedsystembasissystemproposedsystemmacro-avgF183.2 84.4 79.8 80.5micro-avgF182.9 84.3 79.4 80.3k-NN SVMbasissystemproposedsystembasissystemproposedsystemmacro-avgF181.3 82.7 85.8 86.4micro-avgF181.1 82.5 85.8 86.3Table 3.
Results in Korean newsgroup data setNa?ve Bayes Rocchiobasissystemproposedsystembasissystemproposedsystemmacro-avgF178.4 80.8 77.8 79.2micro-avgF179.1 81.3 78.7 80.1k-NN SVMbasissystemproposedsystembasissystemproposedsystemmacro-avgF178.6 80.6 84.8 85.5micro-avgF179.9 81.3 86.0 86.5In both data sets, the proposed system producedthe better performance in all classifiers.
As aresult, our proposed system can be useful for allclassifiers and both two different languages.3.
DiscussionsSalton stated that a collection of small tightlyclustered documents with wide separationbetween individual clusters should produce thebest performance (Salton et al, 1975).
Hence weemployed the method used by Salton et al(1975) to verify our method.
Then we conductedexperiments in English newsgroup data set(Newsgroup data set) and observed the resultingvalues.We define the cohesion within a category andthe cohesion between categories.
The cohesionwithin a category is a measure for similarityvalues between documents in the same category.The cohesion between categories is a measurefor similarities between categories.
The formeris calculated by formula (6) and the latter byformula (7):1,11??
?= ??
?==Kk IdkwithiinIdkkkkCdDCodICrrrrrr(6)( )||1,||111??==?
?=?=KkkglobkbetweenKkkkglobCCIDCoCIDCrrrr(7)where D denotes the total training document set,Ik denotes training document set in k-th category,kCrdenotes a centroid vector of k-th category,and globCrdenotes a centroid vector of the totaltraining documents.An indexing method with a high cohesionwithin a category and a low cohesion betweencategories should produce the betterperformance in text categorization.
First, wemeasured the cohesion within a category in eachindexing method: a basis method by theconventional TF value, a method using the title(Sim(S,T)), a method using the importance ofterms (Cen(S)), and a combination method(Sim(S,T)+Cen(S)).
Figure 4 shows the resultingcurves in each different constant weight; weused simply the same values for k1 and k2 in thecombination method.      	 	   Figure 4.
The cohesion within a categoryAs shown in Figure 4, Cen(S) shows the highestcohesion value, but Sim(S,T) does not have aneffect on the cohesion in comparison with themethod by conventional TF value.Figure 5 displays the resulting curves of thecohesion between categories as the same mannerin Figure 4.      	 	 Figure 5.
The cohesion between categoriesWe obtained the lowest cohesion value inSim(S,T).
Using Cen(S), the resulting cohesionvalues are slightly higher than those of themethod by conventional TF value.
In bothFigure 4 and Figure 5, the cohesion values of thecombination method show middle valuesbetween Sim(S,T) and Cen(S).By the results in Figure 4 and Figure 5, wecan observe that our proposed indexing methodreforms the vector space for the betterperformance: high cohesion within a categoryand low cohesion between categories.
Using theproposed indexing method, the documentvectors in a category are located more closelyand individual categories are separated morewidely.
These effects were also observed in ourexperiments.
According to properties of eachclassifier, k-NN has an advantage in a vectorspace with the high cohesion within a categoryand Rocchio has an advantage in a vector spacewith the low cohesion between categories.
Weachieved the similar results in our experiments.That is, k-NN produced a better performance byusing Cen(S) and Rocchio produced a betterperformance by using Sim(S,T).
Table 4 showsthe summarized results in each individualmethod of k-NN and Rocchio.Table 4.
Top performance by two different methodsk-NN Rocchiosim(S,T) Cen(S) Sim(S,T) Cen(S)macro-avgF180.6 82.4 79.7 78.8ConclusionsIn this paper, we have presented a new indexingmethod for text categorization using two kindsof text summarization techniques; one uses thetitle and the other uses the importance of terms.For our experiments, we used two differentlanguage newsgroup data sets and four kinds ofclassifiers.
We achieved the better performancethan the basis system in all classifiers and bothtwo languages.
Then we verified the effect of theproposed indexing method by measuring the twokinds of cohesion.
We confirm that the proposedindexing method can reform the documentvector space for the better performance in textcategorization.
As a future work, we need theadditional research for applying the morestructural information of document to textcategorization techniques and testing theproposed method on other types of texts such asnewspapers with fixed form.ReferencesEndres-Niggemeyer B. et al (1998) SummarizingInformation.
Springer-Verlag Berlin Heidelberg, pp.307-338.Joachims T. (1998) Text Categorization with SupportVector Machines: Learning with Many RelevantFeatures.
In European Conference on Machine Learning(ECML), pp.
137-142.Ko Y. and Seo J.
(2000) Automatic Text Categorization byUnsupervised Learning.
In Proceedings of the 18thInternational Conference on Computational Linguistics(COLING?2000), pp.
453-459.Lewis D.D., Schapire R.E., Callan J.P., and Papka R.(1996) Training Algorithms for Linear Text Classifiers.In Proceedings of the 19th International Conference onResearch and Development in Information Retrieval(SIGIR?96), pp.289-297.Marcu D., (1999) Discourse trees are good indicators ofimportance in text.
Advances in Automatic TextSummarization.
pp.123-136, The MIT Press.McCallum A. and Nigam K. (1998) A Comparison ofEvent Models for Na?ve Bayes Text Classification.AAAI ?98 workshop on Learning for Text Categorization.pp.
41-48.Mock K.J., (1996) Hybrid hill-climbing andknowledge-based techniques for intelligent newsfiltering.
In Proceedings of the National Conference onArtificial Intelligence (AAAI?96).Murata M., Ma Q., Uchimoto K., Ozaku H., Isahara H., andUtiyama M. (2000) Information retrieval using locationand category information.
Journal of the Association forNatural Language Processing, 7(2).Radev D.R., Jing H., and Stys-Budzikowska M. (2000)Summarization of multiple documents: clustering,sentence extraction, and evaluation.
In Proceedings ofANLP-NAACL Workshop on AutomaticSummarization.Salton G., Yang C., and Wang A.
(1975) A vector spacemodel for automatic indexing.
Communications of theACM, Vol.
18, No.
11, pp.
613-620.Salton G., Fox E.A., and Wu H. (1983) Extended Booleaninformation retrieval.
Communications of the ACM 26(12), pp.
1022-1036.Salton G. and Buckley C. (1988) Term weightingapproaches in automatic text retrieval.
InformationProcessing and Management, 24:513-523.Yang Y. and Pedersen J.P. (1997) Feature selection instatistical learning of text categorization.
In TheFourteenth International Conference on MachineLearning, pages 412-420.Yang Y., Slattery S., and Ghani R. (2002) A study ofapproaches to hypertext categorization, Journal ofIntelligent Information Systems, Volume 18, Number 2.
