Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 501?509,Beijing, August 2010FactRank: Random Walks on a Web of FactsAlpa JainYahoo!
Labsalpa@yahoo-inc.comPatrick PantelMicrosoft Researchppantel@microsoft.comAbstractFact collections are mostly built usingsemi-supervised relation extraction tech-niques and wisdom of the crowds meth-ods, rendering them inherently noisy.
Inthis paper, we propose to validate the re-sulting facts by leveraging global con-straints inherent in large fact collections,observing that correct facts will tend tomatch their arguments with other factsmore often than with incorrect ones.
Wemodel this intuition as a graph-rankingproblem over a fact graph and explorenovel random walk algorithms.
Wepresent an empirical study, over a large setof facts extracted from a 500 million doc-ument webcrawl, validating the model andshowing that it improves fact quality overstate-of-the-art methods.1 IntroductionFact bases, such as those contained in Freebase,DBpedia, KnowItAll, and TextRunner, are increas-ingly burgeoning on the Internet, in government,in high tech companies and in academic laborato-ries.
Bar the accurate manual curation typified byCyc (Lenat, 1995), most fact bases are built us-ing either semi-supervised techniques or wisdomof the crowds techniques, rendering them inher-ently noisy.
This paper describes algorithms tovalidate and re-rank fact bases leveraging globalconstraints imposed by the semantic argumentspredicated by the relations.Facts are defined as instances of n-ary typed re-lations such as acted-in?movie, actor?, director-of?movie, director?, born-in?person, date?, andbuy?person, product, person?.
In all but verysmall fact bases, relations share an argumenttype, such as movie for the relations acted-in anddirector-of in the above example.
The hypothesisexplored in this paper is that when two fact in-stances from two relations share the same valuefor a shared argument type, then the validity ofboth facts should be increased.
Conversely, wealso hypothesize that an incorrect fact instancewill tend to match a shared argument with otherfacts far less frequently.
For example, considerthe following four facts from the relations acted-in, director-of, and is-actor:t1: acted-in?Psycho, Anthony Perkins?t2: *acted-in?Walt Disney Pictures, Johnny Depp?t3: director-of?Psycho, Alfred Hitchcock?t4: is-actor?Anthony Perkins?Our confidence in the validity of t1 increaseswith the knowledge of t3 and t4 since the argu-ment movie is shared with t3 and actor with t4.Similarly, t1 increases our confidence in the va-lidity of t3 and t4.
For t2, we expect to find fewfacts that will match a movie argument with WaltDisney Pictures.
Facts that share the actor argu-ment Johnny Depp with t2 will increase its valid-ity, but the lack of matches on its movie argumentwill decrease its validity.In this paper, we present FactRank, which for-malizes the above intuitions by constructing a factgraph and running various random walk graph-ranking algorithms over it to re-rank and validatethe facts.
A collection of facts is modeled in theform of a graph where nodes are fact instances andedges connect nodes that have the same value for ashared argument type (e.g., t1 would be linked byan edge to both t3 and t4.)
Given a graph represen-tation of facts, we explore various random walkalgorithms to propagate our confidence in individ-ual facts through the web of facts.
We explorealgorithms such as PageRank (Page et al, 1999)as well as propose novel algorithms that lever-age several unique characteristics of fact graphs.Finally, we present an empirical analysis, over alarge collection of facts extracted from a 500 mil-501lion document webcrawl, supporting our modeland confirming that global constraints in a factbase can be leveraged to improve the quality ofthe facts.
Our proposed algorithms are agnostic tothe sources of a fact base, however our reportedexperiments were carried over a state-of-the-artsemi-supervised extraction system.
In summary,the main contributions of this paper are:?
We formalize the notion of ranking facts ina holistic manner by applying graph-basedranking algorithms (Section 2).?
We propose novel ranking algorithms usingrandom walk models on facts (Section 3).?
We establish the effectiveness of our ap-proach through an extensive experimentalevaluation over a real-life dataset and showimprovements over state-of-the-art rankingmethods (Section 4).2 Fact Validation RevisitedWe denote an n-ary relation r with typed argu-ments t1, t2, ?
?
?
, tn as r?t1, t2, ?
?
?
tn?.
In this pa-per, we limit our focus to unary and binary re-lations.
A fact is an instance of a relation.
Forexample, acted-in?Psycho, Anthony Perkins?
is afact from the acted-in?movie, actor?
relation.Definition 2.1 [Fact base]: A fact base is a col-lection of facts from several relations.
Textrunnerand Freebase are example fact bases (note thatthey also contain knowledge beyond facts such asentity lists and ontologies.)
2Definition 2.2 [Fact farm]: A fact farm is a sub-set of interconnected relations in a fact base thatshare arguments among them.
2For example, consider a fact base consisting offacts for relations involving movies, organiza-tions, products, etc., of which the relations acted-in and director-of could form a MOVIES fact farm.Real-world fact bases are built in many ways.Semi-supervised relation extraction methods in-clude KnowItAll (Etzioni et al, 2005), TextRun-ner (Banko and Etzioni, 2008), and many otherssuch as (Riloff and Jones, 1999; Pantel and Pen-nacchiotti, 2006; Pas?ca et al, 2006; Mintz et al,2009).
Wisdom of the crowds methods includeDBpedia (Auer et al, 2008) and Freebase whichextracts facts from various open knowledge basesand allow users to add or edit its content.Most semi-supervised relation extraction meth-ods follow (Hearst, 1992).
Starting with a rela-tively small set of seed facts, these extractors it-eratively learn patterns that can be instantiated toidentify new facts.
To reflect their confidence inan extracted fact, extractors assign an extractionscore with each fact.
Methods differ widely inhow they define the extraction score.
Similarly,many extractors assign a pattern score to eachdiscovered pattern.
In each iteration, the high-est scoring patterns and facts are saved, which areused to seed the next iteration.
After a fixed num-ber of iterations or when a termination conditionis met, the instantiated facts are ranked by theirextraction score.Several methods have been proposed to gen-erate such ranked lists (e.g., (Riloff and Jones,1999; Banko and Etzioni, 2008; Matuszek et al,2005; Pantel and Pennacchiotti, 2006; Pas?ca et al,2006).
In this paper, we re-implement the large-scale state-of-the-art method proposed by Pas?ca etal.
(2006).
This pattern learning method generatesbinary facts and computes the extraction scores ofa fact based on (a) the scores of the patterns thatgenerated it, and (b) the distributional similarityscore between the fact and the seed facts.
Wecomputed the distributional similarity between ar-guments using (Pantel et al, 2009) over a largecrawl of the Web (described in Section 4.1).
Otherimplementation details follow (Pas?ca et al, 2006).In our experiments, we observed some interest-ing ranking problems as illustrated by the follow-ing example facts for the acted-in relation:id: Facts (#Rank)t1: acted-in?Psycho, Anthony Perkins?
(#26)t2: *acted-in?Walt Disney Pictures, Johnny Depp?
(#9)Both t1 and t2 share similar contexts in documents(e.g., ?movie?
film starring ?actor?
and ?movie?starring ?actor?
), and this, in turn, boosts thepattern-based component of the extraction scoresfor t1.
Furthermore, due to the ambiguity of theterm psycho, the distributional similarity-basedcomponent of the scores for fact t2 is also lowerthan that for t1.502Relations id : Factsacted-in t1 : ?Psycho, Anthony Perkins?t2 : *?Walt Disney Pictures, Johnny Depp?director-of t3 : ?Psycho, Alfred Hitchcock?producer-of t4 : ?Psycho, Hilton Green?is-actor t5 : ?Anthony Perkins?t6 : ?Johnny Depp?is-director t7 : ?Alfred Hitchcock?is-movie t8 : ?Psycho?Table 1: Facts share arguments across relationswhich can be exploited for validation.Our work in this paper is motivated by thefollowing observation: the ranked list generatedby an individual extractor does not leverage anyglobal information that may be available whenconsidering a fact farm in concert.
To under-stand the information available in a fact farm,consider a MOVIES fact farm consisting of rela-tions, such as, acted-in, director-of, producer-of,is-actor, is-movie, and is-director.
Table 1 listssample facts that were generated in our experi-ments for these relations1.
In this example, weobserve that for t1 there exist facts in foreign re-lations, namely, director-of and producer-of thatshare the same value for the Movie argument, andintuitively, facts t3 and t4 add to the validity of t1.Furthermore, t1 shares the same value for the Ac-tor argument with t5.
Also, t3, which is expectedto boost the validity of t1, itself shares values forits arguments with facts t4 and t7, which again in-tuitively adds to the validity of t1.
In contrast tothis web of facts generated for t1, the fact t2 sharesonly one of its argument value with one other fact,i.e., t6.The above example underscores an importantobservation: How does the web of facts gener-ated by a fact farm impact the overall validity ofa fact?
To address this question, we hypothesizethat facts that share arguments with many facts aremore reliable than those that share arguments withfew facts.
To capture this hypothesis, we model aweb of facts for a farm using a graph-based repre-sentation.
Then, using graph analysis algorithms,we propagate reliability to a fact using the scoresof other facts that recursively connect to it.Starting with a fact farm, to validate the facts ineach consisting relation, we:1The is-actor?actor?, is-director?director?, and is-movie?movie?
rela-tions are equivalent to the relation is-a?c-instance, class?
where class ?
{actor, director,movie}.
(1) Identify arguments common to relations in the farm.
(2) Run extraction methods to generate each relation.
(3) Construct a graph-based representation of the extractedfacts using common arguments identified in Step (1)(see Section 3.1 for details on constructing this graph.
)(4) Perform link analysis using random walk algorithmsover the generated graph, propagating scores to eachfact through the interconnections (see Section 3.2 fordetails on various proposed random walk algorithms).
(5) Rank facts in each relation using the scores generatedin Step (4) or by combining them with the original ex-traction scores.For the rest of the paper, we focus on generatingbetter ranked lists than the original rankings pro-posed by a state-of-the-art extractor.3 FactRank: Random Walk on FactsOur approach considers a fact farm holistically,leveraging the global constraints imposed by thesemantic arguments of the facts in the farm.
Wemodel this idea by constructing a graph represen-tation of the facts in the farm (Section 3.1) overwhich we run graph-based ranking algorithms.We give a brief overview of one such ranking al-gorithm (Section 3.2) and present variations of itfor fact re-ranking (Section 3.3).
Finally, we in-corporate the original ranking from the extractorinto the ranking produced by our random walkmodels (Section 3.4).3.1 Graph Representation of FactsDefinition 3.1 We define a fact graph FG(V, E),with V nodes and E edges, for a fact farm, as agraph containing facts as nodes and a set of edgesbetween these nodes.
An edge between nodes viand vj indicates that the facts share the samevalue for an argument that is common to the re-lations that vi and vj belong to.
2Figure 1 shows the fact graph for the examplein Table 1 centered around the fact t1.Note on the representation: The above graphrepresentation is just one of many possible op-tions.
For instance, instead of representing factsby nodes, nodes could represent the arguments offacts (e.g., Psycho) and nodes could be connectedby edges if they occur together in a fact.
The taskof studying a ?best?
representation remains a fu-ture work direction.
However, we believe that ourproposed methods can be easily adapted to othersuch graph representations.503<Psycho, Anthony Perkins><Psycho, movie><Psycho, Hilton Green><AlfredHitchchock, director><Anthony Perkins,actor><Psycho, Alfred Hitchcock>Figure 1: Fact graph centered around t1 in Table 1.3.2 The FactRank HypothesisWe hypothesize that connected facts increase ourconfidence in those facts.
We model this ideaby propagating extraction scores through the factgraph similarly to how authority is propagatedthrough a hyperlink graph of the Web (used to es-timate the importance of a webpage).
Several linkstructure analysis algorithms have been proposedfor this goal, of which we explore a particular ex-ample, namely, PageRank (Page et al, 1999).
Thepremise behind PageRank is that given the hyper-link structure of the Web, when a page v generatesa link to page u, it confers some of its importanceto u.
Therefore, the importance of a webpage udepends on the number of pages that link to u andfurthermore, on the importance of the pages thatlink to u.
More formally, given a directed graphG = (V,E) with V vertices and E edges, let I(u)be the set of nodes that link to a node u and O(v)be the set of nodes linked by v. Then, the impor-tance of a node u is defined as:p(u) =Xv?I(u)p(v)|O(v)| (1)The PageRank algorithm iteratively updates thescores for each node in G and terminates when aconvergence threshold is met.
To guarantee the al-gorithm?s convergence, G must be irreducible andaperiodic (i.e., a connected graph).
The first con-straint can be easily met by converting the adja-cency matrix for G into a stochastic matrix (i.e.,all rows sum up to 1.)
To address the issue of peri-odicity, Page et al (1999) suggested the followingmodification to Equation 1:p(u) = 1 ?
d|V | + d ?Xv?I(u)p(v)|O(v)| (2)where d is a damping factor between 0 and 1,which is commonly set to 0.85.
Intuitively, Page-Rank can be viewed as modeling a ?randomwalker?
on the nodes inG and the score of a node,i.e., PageRank, determines the probability of thewalker arriving at this node.While our method makes use of the PageRankalgorithm, we can also use other graph analysisalgorithms (e.g., HITS (Kleinberg, 1999)).
A par-ticularly important property of the PageRank al-gorithm is that the stationary scores can be com-puted for undirected graphs in the same mannerdescribed above, after replacing each undirectededge by a bi-directed edge.
Recall that the edgesin a fact graph are bi-directional (see Figure 1).3.3 Random Walk ModelsBelow, we explore various random walk modelsto assign scores to each node in a fact graph FG.3.3.1 Model ImplementationsPln: Our first method applies the traditional Page-Rank model to FG and computes the score of anode u using Equation 2.Traditional PageRank, as is, does not make useof the strength of the links or the nodes connectedby an edge.
Based on this observation, researchershave proposed several variations of the PageRankalgorithm in order to solve their problems.
Forinstance, variations of random walk algorithmshave been applied to the task of extracting impor-tant words from a document (Hassan et al, 2007),for summarizing documents (Erkan and Radev,2004), and for ordering user preferences (Liu andYang, 2008).
Following the same idea, we buildupon the discussion in Section 3.2 and presentrandom walk models that incorporate the strengthof an edge.Dst: One improvement over Pln is to distinguishbetween nodes in FG using the extraction scoresof the facts associated with them: extraction meth-ods such as our reimplementation of (Pas?ca et al,2006) assign scores to each output fact to reflectits confidence in it (see Section 3.2).
Intuitively, ahigher scoring node that connects to u should in-crease the importance of umore than a connectionfrom a lower scoring node.
Let I(u) be the set ofnodes that link to u and O(v) be the set of nodes504linked by v. Then, if w(u) is the extraction scorefor the fact represented by node u, the score fornode u is defined:p(u) = 1 ?
d|V | + d ?Xv?I(u)w(v) ?
p(v)|O(v)| (3)where w(v) is the confidence score for the factrepresented by v. Naturally, other (externally de-rived) extraction scores can also be substituted forw(v).Avg: We can further extend the idea of deter-mining the strength of an edge by combining theextraction scores of both nodes connected by anedge.
Specifically,p(u) = 1 ?
d|V | + d ?Xv?I(u)avg(u, v) ?
p(v)|O(v)| (4)where avg(u, v) is the average of the extractionscores assigned to the facts associated with nodesu and v.Nde: In addition to using extraction scores, wecan also derive the strength of a node dependingon the number of distinct relations it connects to.For instance, in Figure 1, t1 is linked to four dis-tinct relations, namely, director-of, producer-of,is-actor, is-movie, whereas, t2 is linked to one re-lation, namely, is-actor.
We compute p(u) as:p(u)=1 ?
d|V | +d ?Xv?I(u)(?
?
w(v)+(1 ?
?)
?
r(v)) ?
p(v)|O(v)| (5)where w(v) is the confidence score for node v andr(v) is the fraction of total number of relations inthe farm that contain facts with edges to v.3.3.2 Dangling nodesIn traditional hyperlink graphs for the Web,dangling nodes (i.e., nodes with no associatededges) are considered to be of low importancewhich is appropriately represented by the scorescomputed by the PageRank algorithm.
How-ever, an important distinction from this setting isthat fact graphs are sparse causing them to havevalid facts with no counterpart matching argu-ments in other relation, thus rendering them dan-gling.
This may be due to several reasons, e.g.,extractors often suffer from less than perfect recalland they may miss valid facts.
In our experiments,about 10% and 40% of nodes from acted-in anddirector-of, respectively, were dangling nodes.Handling dangling nodes in our extraction-based scenario is a particularly challenging issue:while demoting the validity of dangling nodescould critically hurt the quality of the facts, lackof global information prevents us from systemati-cally introducing them into the re-ranked lists.
Weaddress this issue by maintaining the original rankpositions when re-ranking dangling nodes.3.4 Incorporating Extractor RanksOur proposed random walk ranking methods ig-nore the ranking information made available bythe original relation extractor (e.g., (Pas?ca et al,2006) in our implementation).
Below, we pro-pose two ways of combining the ranks suggestedby the original ranked list O and the re-ranked listG, generated using the algorithms in Section 3.3.R-Avg: The first combination method computesthe average of the ranks obtained from the twolists.
Formally, if O(i) is the original rank for facti and G(i) is the rank for i in the re-ranked list,the combined rank M(i) is computed as:M(i) = O(i) +G(i)2 (6)R-Wgt: The second method uses a weighted aver-age of the ranks from the individual lists:M(i) = wo ?
O(i) + (1 ?
wo) ?
G(i)2 (7)In practice, this linear combination can be learned;in our experiments, we set them towo = 0.4 basedon our observations over an independent trainingset.
Several other combination functions couldalso be applied to this task.
For instance, we ex-plored the min and max functions but observed lit-tle improvements.4 Experimental Evaluation4.1 Experimental SetupExtraction method: For our extraction method,we reimplemented the method described in (Pas?caet al, 2006) and further added a validation layeron top of it based on Wikipedia (we boosted thescores of a fact if there exists a Wikipedia pagefor either of the fact?s arguments, which mentionsthe other argument.)
This state-of-the-art methodforms a strong baseline in our experiments.Corpus and farms: We ran our extractor over alarge Web crawl consisting of 500 million English50525000 2000025000 150002000025000f nodes10000150002000025000mber o0500010000150002000025000Nu05000100001500020000250001248163264128256Nodedegree05000100001500020000250001248163264128256NodedegreeFigure 2: Degree distribution for MOVIES.webpages crawled by the Yahoo!
search engine.We removed paragraphs containing fewer than 50tokens and then removed all duplicate sentences.The resulting corpus consists of over 5 millionsentences.
We defined a farm, MOVIES, with rela-tions, acted-in, director-of, is-movie, is-actor, andis-director.Evaluation methodology: Using our extractionmethod over the Web corpus, we generate over100,000 facts for the above relations.
However, tokeep our evaluation manageable, we draw a ran-dom sample from these facts.
Specifically, wefirst generate a ranked list using the extractionscores output by our extractor.
We will refer tothis method as Org (original).
We then generatea fact graph over which we will run our methodsfrom Section 3.3 (each of which will re-rank thefacts).
Figure 2 shows the degree, i.e., numberof edges, distribution of the fact graph generatedfor MOVIES.
We ran Avg, Dst, Nde, R-Avg, andR-Wgt on this fact graph and using the scores were-rank the facts for each of the relations.
In Sec-tion 4.2, we will discuss our results for the acted-in and director-of relations.Fact Verification: To verify whether a fact isvalid or not, we recruit human annotators usingthe paid service Mechanical Turk.
For each fact,two annotations were requested (keeping the totalcost under $100).
The annotators were instructedto mark incorrect facts as well as disallow any val-ues that were not ?well-behaved.?
For instance,acted-in?Godfather, Pacino?
is correct, but acted-in?The, Al Pacino?
is incorrect.
We manually ad-judicated 32% of the facts where the judges dis-agreed.Evaluation metrics: Using the annotated facts,we construct a goldset S of facts and compute theprecision of a list L as: |L?S||S| .
To compare theeffectiveness of the ranked lists, we use averageprecision, a standard measure in information re-trieval for evaluating ranking algorithms, definedMethod Average precision30% 50% 100%Org 0.51 0.39 0.38Pln 0.44 0.35 0.32Avg 0.55 0.44 0.42Dst 0.54 0.44 0.41Nde 0.53 0.40 0.41R-Avg 0.58 0.46 0.45R-Wgt 0.60 0.56 0.44Table 2: Average precision for acted-in for vary-ing proportion of fact graph of MOVIES.Method Average precision30% 50% 100%Org 0.64 0.69 0.66Pln 0.69 0.67 0.59Avg 0.69 0.70 0.64Dst 0.67 0.69 0.64Nde 0.69 0.69 0.64R-Avg 0.70 0.70 0.64R-Wgt 0.71 0.71 0.69Table 3: Average precision for director-of forvarying proportion of fact graph of MOVIES.as: Ap(L) =P|L|i=1 P (i)?isrel(i)P|L|i=1 isrel(i), where P (i) is theprecision of L at rank i, and isrel(i) is 1 if the factat rank i is in S, and 0 otherwise.
We also studythe precision values at varying ranks in the list.For robustness, we report the results using 10-foldcross validation.4.2 Experimental ResultsEffectiveness of graph-based ranking: Ourfirst experiment studies the overall quality of theranked lists generated by each method.
Table 2compares the average precision for acted-in, withthe maximum scores highlighted for each column.We list results for varying proportions of the orig-inal fact graph (30%, 50%, and 100%).
Due toour small goldset sizes, these results are not sta-tistically significant over Org, however we con-sistently observed a positive trend similar to thosereported in Table 2 over a variety of evaluationsets generated by randomly building 10-folds ofall the facts.Overall, the Avg method offers a competitivealternative to the original ranked list generatedby the extractor Org: not only are the averageprecision values for Avg higher than Org, butas we will see later, the rankings generated byour graph-based methods exhibits some positiveunique characteristics.
These experiments also506R Org Pln Avg Dst Nde R-Avg R-Wgt5 0.44 0.40 0.52 0.48 0.40 0.52 0.5610 0.36 0.36 0.42 0.38 0.36 0.36 0.3615 0.287 0.24 0.30 0.28 0.26 0.30 0.3020 0.26 0.26 0.26 0.26 0.26 0.27 0.2721 0.27 0.27 0.27 0.27 0.27 0.27 0.27Table 4: Precision at varying ranks for the acted-in relation (R stands for Ranks).R Org Pln Avg Dst Nde R-Avg R-Wgt5 0.58 0.68 0.70 0.68 0.64 0.66 0.7010 0.60 0.57 0.59 0.58 0.59 0.6 0.6915 0.57 0.53 0.58 0.56 0.56 0.56 0.6020 0.57 0.57 0.58 0.58 0.58 0.58 0.6025 0.60 0.54 0.56 0.57 0.56 0.57 0.5730 0.57 0.57 0.57 0.57 0.57 0.58 0.5933 0.56 0.56 0.56 0.56 0.56 0.56 0.56Table 5: Precision at varying ranks for thedirector-of relation (R stands for Ranks).confirm our initial observations: using traditionalPageRank (Pln) is not desirable for the task of re-ranking facts (see Section 3.3).
Our modificationsto the PageRank algorithm (e.g., Avg, Dst, Nde)consistently outperform the traditional PageRankalgorithm (Pln).
The results also underscore thebenefit of combining the original extractor rankswith those generated by our graph-based rank-ing algorithms with R-Wgt consistently leading tohighest or close to the highest average precisionscores.In Table 3, we show the average precision val-ues for director-of.
In this case, the summarystatistic, average precision, does not show manydifferences between the methods.
To take a finerlook into the quality of these rankings, we investi-gated the precision scores at varying ranks acrossthe methods.
Table 4 and Table 5 show the preci-sion at varying ranks for acted-in and director-ofrespectively.
The maximum precision values foreach rank are highlighted.For acted-in again we see that Avg, R-Avg, R-Wgt outperform Org and Pln at all ranks, andDst outperforms Org at two ranks.
While themethod Nde outperforms Org for a few cases, weexpected it to perform better.
Error analysis re-vealed that the sparsity of our fact graph was theproblem.
In our MOVIES fact graph, we observedvery few nodes that are linked to all possible re-lation types, and the scores used by Nde rely onbeing able to identify nodes that link to numer-ous relation types.
This problem can be alleviated#Relation Avg Dst Nde2 0.35 0.34 0.333 0.35 0.35 0.344 0.37 0.36 0.355 0.38 0.38 0.376 0.42 0.41 0.41Table 6: Average precision for acted-in for vary-ing number of relations in the MOVIES fact farm.by reducing the sparsity of the fact graphs (e.g.,by allowing edges between nodes that are ?simi-lar enough?
), which we plan to explore as futurework.
For director-of, Table 5 now shows that forsmall ranks (less than 15), a small (but consistentin our 10-folds) improvement is observed whencomparing our random walk algorithms over Org.While our proposed algorithms show a con-sistent improvement for acted-in, the case ofdirector-of needs further discussion.
For both av-erage precision and precision vs. rank values, Avg,R-Avg, and R-Wgt are similar or slightly betterthan Org.
We observed that the graph-based algo-rithms tend to bring together ?clusters?
of noisyfacts that may be spread out in the original rankedlist of facts.
To illustrate this point, we show theten lowest scoring facts for the director-of rela-tion.
Table 7 shows these ten facts for Org as wellas Avg.
These examples highlight the ability ofour graph-based algorithms to demote noisy facts.Effect of number of relations: To understandthe effect of the number of relations in a farm(and hence connectivity in a fact graph), we veri-fied the re-ranking quality of our proposed meth-ods on various subsets of the MOVIES fact farm.We generated five different subsets, one with 2 re-lations, another with 3 relations, and three morewith four, five, and six relations (note that al-though we have 5 relations in the farm, is-moviecan be used in combination with both acted-inand director-of, thus yielding six relations to ab-late.)
Table 6 shows the results for acted-in.
Over-all, performance improves as we introduce morerelations (i.e., more connectivity).
Once again,we observe that the performance deteriorates forsparse graphs: using very few relations results indegenerating the average precision of the originalranked list.
The issue of identifying the ?right?characteristics of the fact graph (e.g., number ofrelations, degree distribution, etc.)
remains futurework.507Org Avg?david mamet, bob rafelson?
?
drama, nicholas ray?
?cinderella, wayne sleep?
?
drama, mitch teplitsky official?
?mozartdie zauberflte, julie taymor?
?
hollywood, marta bautis?
?matthew gross, julie taymor?
?
hollywood, marek stacharski?
?steel magnolias, theater project?
?
drama, kirk shannon-butts?
?rosie o?donnell, john badham?
?
drama, john pietrowski?
?my brotherkeeper, john badham?
?
drama, john madden starring?
?goldie hawn, john badham?
?
drama, jan svankmajer?
?miramaxbad santa, terry zwigoff?
?
drama, frankie sooknanan?
?premonition, alan rudolph?
?
drama, dalia hager?Table 7: Sample facts for director-of at the bot-tom of the ranked list generated by (a) Org and(b) Avg.Evaluation conclusion: We demonstrated the ef-fectiveness of our graph-based algorithms for re-ranking facts.
In general, Avg outperforms Organd Pln, and we can further improve the perfor-mance by using a combination-based ranking al-gorithm such as R-Wgt.
We also studied the im-pact of the size of the fact graphs on the qualityof the ranked lists and showed that increasing thedensity of the fact farms improves the ranking us-ing our methods.5 Related WorkInformation extraction from text has received sig-nificant attention in the recent years (Cohen andMcCallum, 2003).
Earlier approaches reliedon hand-crafted extraction rules such as (Hearst,1992), but recent efforts have developed su-pervised and semi-supervised extraction tech-niques (Riloff and Jones, 1999; Agichtein andGravano, 2000; Matuszek et al, 2005; Pan-tel and Pennacchiotti, 2006; Pas?ca et al, 2006;Yan et al, 2009) as well as unsupervised tech-niques (Davidov and Rappoport, 2008; Mintzet al, 2009).
Most common methods todayuse semi-supervised pattern-based learning ap-proaches that follow (Hearst, 1992), as dis-cussed in Section 2.
Recent work has also ex-plored extraction-related issues such as, scal-ability (Pas?ca et al, 2006; Ravichandran andHovy, 2002; Pantel et al, 2004; Etzioni et al,2004), learning extraction schemas (Cafarella etal., 2007a; Banko et al, 2007), and organizing ex-tracted facts (Cafarella et al, 2007b).
There isalso a lot of work on deriving extraction scoresfor facts (Agichtein and Gravano, 2000; Downeyet al, 2005; Etzioni et al, 2004; Pantel and Pen-nacchiotti, 2006).These extraction methods are complementaryto our general task of fact re-ranking.
Since ourproposd re-ranking algorithms are agnostic to themethods of generating the initial facts and sincethey do not rely on having available corpus statis-tics, we can use any of the available extractors incombination with any of the scoring methods.
Inthis paper, we used Pas?ca et al?s (2006) state-of-the-art extractor to learn a large set of ranked facts.Graph-based ranking algorithms have been ex-plored for a variety of text-centric tasks.
Randomwalk models have been built for document sum-marization (Erkan and Radev, 2004), keyword ex-traction (Hassan et al, 2007), and collaborativefiltering (Liu and Yang, 2008).
Closest to ourwork is that of Talukdar et al (2008) who pro-posed random walk algorithms for learning in-stances of semantic classes from unstructured andstructured text.
The focus of our work is on ran-dom walk models over fact graphs in order to re-rank collections of facts.6 ConclusionIn this paper, we show how information avail-able in a farm of facts can be exploited for re-ranking facts.
As a key contribution of the pa-per, we modeled fact ranking as a graph rankingproblem.
We proposed random walk models thatdetermine the validity of a fact based on (a) thenumber of facts that ?vote?
for it, (b) the validityof the voting facts, and (c) the extractor?s confi-dence in these voting facts.
Our experimental re-sults demonstrated the effectiveness of our algo-rithms, thus establishing a stepping stone towardsexploring graph-based frameworks for fact vali-dation.
While this paper forms the basis of em-ploying random walk models for fact re-ranking,it also suggests several interesting directions forfuture work.
We use and build upon PageRank,however, several alternative algorithms from thelink analysis literature could be adapted for rank-ing facts.
Similarly, we employ a single (simple)graph-based representation that treats all edges thesame and exploring richer graphs that distinguishbetween edges supporting different arguments ofa fact remains future work.508References[Agichtein and Gravano2000] Agichtein, Eugene and LuisGravano.
2000.
Snowball: Extracting relations fromlarge plain-text collections.
In DL-00.
[Auer et al2008] Auer, S., C. Bizer, G. Kobilarov,J.
Lehmann, R. Cyganiak, and Z. Ives.
2008.
Dbpedia: Anucleus for a web of open data.
In ISWC+ASWC 2007.
[Banko and Etzioni2008] Banko, Michele and Oren Etzioni.2008.
The tradeoffs between open and traditional relationextraction.
In ACL-08.
[Banko et al2007] Banko, Michele, Michael J. Cafarella,Stephen Soderland, Matthew Broadhead, and Oren Et-zioni.
2007.
Open information extraction from the web.In Proceedings of IJCAI-07.
[Cafarella et al2007a] Cafarella, Michael, Dan Suciu, andOren Etzioni.
2007a.
Navigating extracted data withschema discovery.
In Proceedings of WWW-07.
[Cafarella et al2007b] Cafarella, Michael J., Christopher Re,Dan Suciu, Oren Etzioni, and Michele Banko.
2007b.Structured querying of web text: A technical challenge.In Proceedings of CIDR-07.
[Cohen and McCallum2003] Cohen, William and AndrewMcCallum.
2003.
Information extraction from the WorldWide Web (tutorial).
In KDD.
[Davidov and Rappoport2008] Davidov, Dmitry and AriRappoport.
2008.
Unsupervised discovery of generic re-lationships using pattern clusters and its evaluation by au-tomatically generated sat analogy questions.
In ACL-08.
[Downey et al2005] Downey, Doug, Oren Etzioni, andStephen Soderland.
2005.
A probabilistic model of re-dundancy in information extraction.
In Proceedings ofIJCAI-05.
[Erkan and Radev2004] Erkan, Gu?nes?
and Dragomir R.Radev.
2004.
Lexrank: Graph-based lexical centralityas salience in text summarization.
JAIR, 22:457?479.
[Etzioni et al2004] Etzioni, Oren, Michael J. Cafarella, DougDowney, Stanley Kok, Ana-Maria Popescu, Tal Shaked,Stephen Soderland, Daniel S. Weld, and Alexander Yates.2004.
Web-scale information extraction in KnowItAll.
InProceedings of WWW-04.
[Etzioni et al2005] Etzioni, Oren, Michael Cafarella, DougDowney, Ana-Maria Popescu, Tal Shaked, StephenSoderland, Daniel S. Weld, and Alexander Yates.
2005.Unsupervised named-entity extraction from the web: anexperimental study.
Artif.
Intell., 165:91?134.
[Hassan et al2007] Hassan, Samer, Rada Mihalcea, and Car-men Banea.
2007.
Random-walk term weighting for im-proved text classification.
ICSC.
[Hearst1992] Hearst, Marti A.
1992.
Automatic acquisitionof hyponyms from large text corpora.
In Proceedings ofCOLING-92.
[Kleinberg1999] Kleinberg, Jon Michael.
1999.
Authorita-tive sources in a hyperlinked environment.
Journal of theACM, 46(5):604?632.
[Lenat1995] Lenat, Douglas B.
1995.
Cyc: a large-scale in-vestment in knowledge infrastructure.
Commun.
ACM,38(11).
[Liu and Yang2008] Liu, Nathan and Qiang Yang.
2008.Eigenrank: a ranking-oriented approach to collaborativefiltering.
In SIGIR 2008.
[Matuszek et al2005] Matuszek, Cynthia, Michael Witbrock,Robert C. Kahlert, John Cabral, Dave Schneider, PurveshShah, and Doug Lenat.
2005.
Searching for commonsense: Populating cyc from the web.
In AAAI-05.
[Mintz et al2009] Mintz, Mike, Steven Bills, Rion Snow, andDaniel Jurafsky.
2009.
Distant supervision for relationextraction without labeled data.
In ACL-09.
[Pas?ca et al2006] Pas?ca, Marius, Dekang Lin, JeffreyBigham, Andrei Lifchits, and Alpa Jain.
2006.
Organiz-ing and searching the world wide web of facts - step one:The one-million fact extraction challenge.
In Proceedingsof AAAI-06.
[Page et al1999] Page, Lawrence, Sergey Brin, Rajeev Mot-wani, and Terry Winograd.
1999.
The PageRank citationranking: Bringing order to the Web.
Technical Report1999/66, Stanford University, Computer Science Depart-ment.
[Pantel and Pennacchiotti2006] Pantel, Patrick and MarcoPennacchiotti.
2006.
Espresso: leveraging generic pat-terns for automatically harvesting semantic relations.
InACL/COLING-06.
[Pantel et al2004] Pantel, Patrick, Deepak Ravichandran,and Eduard Hovy.
2004.
Towards terascale knowledgeacquisition.
In COLING-04.
[Pantel et al2009] Pantel, Patrick, Eric Crestan, ArkadyBorkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.Web-scale distributional similarity and entity set expan-sion.
In EMNLP-09.
[Ravichandran and Hovy2002] Ravichandran, Deepak andEduard Hovy.
2002.
Learning surface text patterns fora question answering system.
In Proceedings of ACL-08,pages 41?47.
Association for Computational Linguistics.
[Riloff and Jones1999] Riloff, Ellen and Rosie Jones.
1999.Learning dictionaries for information extraction by multi-level bootstrapping.
In Proceedings of AAAI-99.
[Talukdar et al2008] Talukdar, Partha Pratim, JosephReisinger, Marius Pasca, Deepak Ravichandran, RahulBhagat, and Fernando Pereira.
2008.
Weakly-supervisedacquisition of labeled class instances using graph randomwalks.
In Proceedings of EMNLP-08.
[Yan et al2009] Yan, Yulan, Yutaka Matsuo, Zhenglu Yang,and Mitsuru Ishizuka.
2009.
Unsupervised relation ex-traction by mining wikipedia texts with support from webcorpus.
In ACL-09.509
