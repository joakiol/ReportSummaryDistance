Unsupervised Construction of Large Paraphrase Corpora:Exploiting Massively Parallel News SourcesBill DOLAN, Chris QUIRK, and Chris BROCKETTNatural Language Processing Group, Microsoft ResearchOne Microsoft WayRedmond, WA 90852, USA{billdol,chrisq,chrisbkt}@microsoft.comAbstractWe investigate unsupervised techniques foracquiring monolingual sentence-levelparaphrases from a corpus of temporally andtopically clustered news articles collected fromthousands of web-based news sources.
Twotechniques are employed: (1) simple string editdistance, and (2) a heuristic strategy that pairsinitial (presumably summary) sentences fromdifferent news stories in the same cluster.
Weevaluate both datasets using a word alignmentalgorithm and a metric borrowed from machinetranslation.
Results show that edit distance datais cleaner and more easily-aligned than theheuristic data, with an overall alignment errorrate (AER) of 11.58% on a similarly-extractedtest set.
On test data extracted by the heuristicstrategy, however, performance of the twotraining sets is similar, with AERs of 13.2%and 14.7% respectively.
Analysis of 100 pairsof sentences from each set reveals that the editdistance data lacks many of the complex lexicaland syntactic alternations that characterizemonolingual paraphrase.
The summarysentences, while less readily alignable, retainmore of the non-trivial alternations that are ofgreatest interest learning paraphraserelationships.1 IntroductionThe importance of learning to manipulatemonolingual paraphrase relationships forapplications like summarization, search, and dialoghas been highlighted by a number of recent efforts(Barzilay & McKeown 2001; Shinyama et al2002; Lee & Barzilay 2003; Lin & Pantel 2001).While several different learning methods havebeen applied to this problem, all share a need forlarge amounts of data in the form of pairs or sets ofstrings that are likely to exhibit lexical and/orstructural paraphrase alternations.
One approach11An alternative approach involves identifying anchorpoints--pairs of words linked in a known way--andcollecting the strings that intervene.
(Shinyama, et al2002; Lin & Pantel 2001).
Since our interest is inthat has been successfully used is edit distance, ameasure of similarity between strings.
Theassumption is that strings separated by a small editdistance will tend to be similar in meaning:The leading indicators measure the economy?The leading index measures the economy?.Lee & Barzilay (2003), for example, use Multi-Sequence Alignment (MSA) to build a corpus ofparaphrases involving terrorist acts.
Their goal isto extract sentential templates that can be used inhigh-precision generation of paraphrase alter-nations within a limited domain.Our goal here is rather different: our interest liesin constructing a monolingual broad-domaincorpus of pairwise aligned sentences.
Such datawould be amenable to conventional statisticalmachine translation (SMT) techniques (e.g., thosediscussed in Och & Ney 2003).2 In what followswe compare two strategies for unsupervisedconstruction of such a corpus, one employingstring similarity and the other associating sentencesthat may overlap very little at the string level.
Wemeasure the relative utility of the two derivedmonolingual corpora in the context of wordalignment techniques developed originally forbilingual text.We show that although the edit distance corpus iswell-suited as training data for the alignmentalgorithms currently used in SMT, it is anincomplete source of information about paraphraserelations, which exhibit many of the characteristicsof comparable bilingual corpora or freetranslations.
Many of the more complexalternations that characterize monolingualparaphrase, such as large-scale lexical alternationsand constituent reorderings, are not readilylearning sentence level paraphrases, including majorconstituent reorganizations, we do not address thisapproach here.2Barzilay & McKeown (2001) consider thepossibility of using SMT machinery, but reject theidea because of the noisy, comparable nature of theirdataset.captured by edit distance techniques, whichconflate semantic similarity with formal similarity.We conclude that paraphrase research wouldbenefit by identifying richer data sources anddeveloping appropriate learning techniques.2 Data/MethodologyOur two paraphrase datasets are distilled from acorpus of news articles gathered from thousands ofnews sources over an extended period.
While theidea of exploiting multiple news reports forparaphrase acquisition is not new, previous efforts(for example, Shinyama et al 2002; Barzilay andLee 2003) have been restricted to at most two newssources.
Our work represents what we believe tobe the first attempt to exploit the explosion of newscoverage on the Web, where a single event cangenerate scores or hundreds of different articleswithin a brief period of time.
Some of these articlesrepresent minor rewrites of an original AP orReuters story, while others represent truly distinctdescriptions of the same basic facts.
The massiveredundancy of information conveyed with widelyvarying surface strings is a resource begging to beexploited.Figure 1 shows the flow of our data collectionprocess.
We begin with sets of pre-clustered URLswhich point to news articles on the Web,representing thousands of different news sources.The clustering algorithm takes into account the fulltext of each news article, in addition to temporalcues, to produce a set of topically and temporallyrelated articles.
Our method is believed to beindependent of the specific clustering technologyused.
The story text is isolated from a sea ofadvertisements and other miscellaneous textthrough use of a supervised HMM.Altogether we collected 11,162 clusters in an 8-month period, assembling 177,095 articles with anaverage of 15.8 articles per cluster.
The clustersare generally coherent in topic and focus.
Discreteevents like disasters, business announcements, anddeaths tend to yield tightly focused clusters, whileongoing stories like the SARS crisis tend toproduce less focused clusters.
While exactduplicate articles are filtered out of the clusters,many slightly-rewritten variants remain.2.1 Extracting Sentential ParaphrasesTwo separate techniques were employed toextract likely pairs of sentential paraphrases fromthese clusters.
The first used string edit distance,counting the number of lexical deletions andinsertions needed to transform one string intoanother.
The second relied on a discourse-basedheuristic, specific to the news genre, to identifylikely paraphrase pairs even when they have littlesuperficial similarity.3 Levenshtein DistanceA simple edit distance metric (Levenshtein1966) was used to identify pairs of sentenceswithin a cluster that are similar at the string level.First, each sentence was normalized to lower caseand paired with every other sentence in the cluster.Pairings that were identical or differing only bypunctuation were rejected, as were those where theshorter sentence in the pair was less than two thirdsthe length of the longer, this latter constraint ineffect placing an upper bound on edit distancerelative to the length of the sentence.
Pairs that hadbeen seen before in either order were also rejected.Filtered in this way, our dataset yields 139K non-identical sentence pairs at a Levenshtein distanceof n ?
12.
3  Mean Levenshtein distance was 5.17,and mean sentence length was 18.6 words.
We willrefer to this dataset as L12.3.1.1 First sentencesThe second extraction technique wasspecifically intended to capture paraphrases whichmight contain very different sets of content words,word order, and so on.
Such pairs are typicallyused to illustrate the phenomenon of paraphrase,but precisely because their surface dissimilarityrenders automatic discovery difficult, they havegenerally not been the focus of previouscomputational approaches.In order to automatically identify sentence pairsof this type, we have attempted to take advantageof some of the unique characteristics of the dataset.The topical clustering is sufficiently precise toensure that, in general, articles in the same clusteroverlap significantly in overall semantic content.Even so, any arbitrary pair of sentences fromdifferent articles within a cluster is unlikely toexhibit a paraphrase relationship:The Phi-X174 genome is short and compact.This is a robust new step that allows us to make muchlarger pieces.To isolate just those sentence pairs that representlikely paraphrases without requiring significantstring similarity, we exploited a commonjournalistic convention: the first sentence or two of3A maximum Levenshtein distance of 12 was selectedfor the purposes of this paper on the basis ofexperiments with corpora extracted at various editdistances.a newspaper article typically summarize itscontent.
One might reasonably expect, therefore,that initial sentences from one article in a clusterwill be paraphrases of the initial sentences in otherarticles in that cluster.
This heuristic turns out to bea powerful one, often correctly associatingsentences that are very different at the string level:In only 14 days, US researchers have created anartificial bacteria-eating virus from syntheticgenes.An artificial bacteria-eating virus has been made fromsynthetic genes in the record time of just two weeks.Also consider the following example, in whichrelated words are obscured by different parts ofspeech:Chosun Ilbo, one of South Korea's leading newspapers,said North Korea had finished developing a newballistic missile last year and was planning todeploy it.The Chosun Ilbo said development of the new missile,with a range of up to %%number%% kilometres(%%number%% miles), had been completed anddeployment was imminent.A corpus was produced by extracting the firsttwo sentences of each article, then pairing theseacross documents within each cluster.
We willrefer to this collection as the F2 corpus.
Thecombination of the first-two sentences heuristicplus topical article clusters allows us to takeadvantage of meta-information implicit in ourcorpus, since clustering exploits lexicalinformation from the entire document, not just thefew sentences that are our focus.
The assumptionthat two first sentences are semantically related isthus based in part on linguistic information that isexternal to the sentences themselves.Sometimes, however, the strategy of pairingsentences based on their cluster and position goesastray.
This would lead us to posit a paraphraserelationship where there is none:Terence Hope should have spent most of yesterday inhospital performing brain surgery.A leading brain surgeon has been suspended from workfollowing a dispute over a bowl of soup.To prevent too high an incidence of unrelatedsentences, one string-based heuristic filter wasfound useful: a pair is discarded if the sentences donot share at least 3 words of 4+ characters.
Thisconstraint succeeds in filtering out many unrelatedpairs, although it can sometimes be too restrictive,excluding completely legitimate paraphrases:There was no chance it would endanger our planet,astronomers said.NASA emphasized that there was never danger of acollision.An additional filter ensured that the word countof the shorter sentence is at least one-half that ofthe longer sentence.
Given the relatively longsentences in our corpus (average length 18.6words), these filters allowed us to maintain adegree of semantic relatedness between sentences.Accordingly, the dataset encompasses manyparaphrases that would have been excluded under amore stringent edit-distance threshold, forexample, the following non-paraphrase pair thatcontain an element of paraphrase:A staggering %%number%% million Americans havebeen victims of identity theft in the last five years ,according to federal trade commission survey outthis week.In the last year alone, %%number%% million peoplehave had their identity purloined.Nevertheless, even after filtering in these ways,a significant amount of unfiltered noise remains inthe F2 corpus, which consisted of 214K sentencepairs.
Out of a sample of 448 held-out sentencepairs, 118 (26.3%) were rated by two independenthuman evaluators as sentence-level paraphrases,while 151 (33.7%) were rated as partialparaphrases.
The remaining ~40% were assessed asNews article clusters: URLsDownload URLs,Isolate content (HMM),Sentence separateTextual content of articlesSelect and filterfirst sentence pairsApproximately parallelmonolingual corpusFigure 1.
Data collectionunrelated.
4   Thus, although the F2 data set isnominally larger than the L12 data set, when thenoise factor is taken into account, the actualnumber of full paraphrase sentences in this data setis estimated to be in the region of 56K sentences,with a further estimated 72K sentences containingsome paraphrase material that might be a potentialsource of alignment.Some of these relations captured in this data canbe complex.
The following pair, for example,would be unlikely to pass muster on edit distancegrounds, but nonetheless contains an inversion ofdeep semantic roles, employing different lexicalitems.The Hartford Courant reported %%day%% that TonyBryant said two friends were the killers.A lawyer for Skakel says there is a claim that themurder was carried out by two friends of one ofSkakel's school classmates, Tony Bryan.The F2 data also retains pairs like the followingthat involve both high-level semantic alternationsand long distance dependencies:Two men who robbed a jeweller's shop to raise fundsfor the Bali bombings were each jailed for%%number%% years by Indonesian courts today.An Indonesian court today sentenced two men to%%number%% years in prison for helpingfinance last year's terrorist bombings in Bali byrobbing a jewelry store.These examples do not by any means exhaustthe inventory of complex paraphrase types that arecommonly encountered in the F2 data.
Weencounter, among other things, polarityalternations, including those involving long-distance dependencies, and a variety of distributedparaphrases, with alignments spanning widelyseparated elements.3.2 Word Error Alignment RateAn objective scoring function was needed tocompare the relative success of the two datacollection strategies sketched in 2.1.1 and 2.1.2.Which technique produces more data?
Are thetypes of data significantly different in character orutility?
In order to address such questions, we usedword Alignment Error Rate (AER), a metricborrowed from the field of statistical machinetranslation (Och & Ney 2003).
AER measures howaccurately an automatic algorithm can align wordsin corpus of parallel sentence pairs, with a human-4This contrasts with 16.7% pairs assessed asunrelated in a 10,000 pair sampling of the L12 data.tagged corpus of alignments serving as the goldstandard.
Paraphrase data is of course monolingual,but otherwise the task is very similar to the MTalignment problem, posing the same issues withone-to-many, many-to-many, and one/many-to-null word mappings.
Our a priori assumption wasthat the lower the AER for a corpus, the morelikely it would be to yield learnable informationabout paraphrase alternations.We closely followed the evaluation standardsestablished in Melamed (2001) and Och & Ney(2000, 2003).
Following Och & Ney?smethodology, two annotators each created aninitial annotation for each dataset, subcategorizingalignments as either SURE (necessary) or POSSIBLE(allowed, but not required).
Differences were thenhighlighted and the annotators were asked toreview these cases.
Finally we combined the twoannotations into a single gold standard in thefollowing manner: if both annotators agreed that analignment should be SURE, then the alignment wasmarked as sure in the gold-standard; otherwise thealignment was marked as POSSIBLE.To compute Precision, Recall, and AlignmentError Rate (AER) for the twin datasets, we usedexactly the formulae listed in Och & Ney (2003).Let A be the set of alignments in the comparison, Sbe the set of SURE alignments in the gold standard,and P be the union of the SURE and POSSIBLEalignments in the gold standard.
Then we have:||||precisionAPA ?=||||recallSSA ?=||||AERSASAPA+?+?=We held out a set of news clusters from ourtraining data and randomly extracted two sets ofsentence pairs for blind evaluation.
The first is aset of 250 sentence pairs extracted on the basis ofan edit distance of 5 ?
n ?
20, arbitrarily chosen toallow a range of reasonably divergent candidatepairs.
These sentence pairs were checked by anindependent human evaluator to ensure that theycontained paraphrases before they were tagged foralignments.
The second set comprised 116sentence pairs randomly selected from the set offirst-two sentence pairs.
These were likewise hand-vetted by independent human evaluators.
After aninitial training pass and refinement of the linkingspecification, interrater agreement measured interms of AER5 was 93.1% for the edit distance testset versus 83.7% for the F2 test set, suggestive ofthe greater variability in the latter data set.3.3 Data AlignmentEach corpus was used as input to the wordalignment algorithms available in Giza++ (Och &Ney 2000).
Giza++ is a freely availableimplementation of IBM Models 1-5 (Brown et al1993) and the HMM alignment (Vogel et al 1996),along with various improvements andmodifications motivated by experimentation byOch & Ney (2000).
Giza++ accepts as input acorpus of sentence pairs and produces as output aViterbi alignment of that corpus as well as theparameters for the model that produced thosealignments.While these models have proven effective at theword alignment task (Mihalcea & Pedersen 2003),there are significant practical limitations in theiroutput.
Most fundamentally, all alignments haveeither zero or one connection to each target word.Hence they are unable to produce the many-to-many alignments required to identifycorrespondences with idioms and other phrasalchunks.To mitigate this limitation on final mappings,we follow the approach of Och (2000): we alignonce in the forward direction and again in thebackward direction.
These alignments cansubsequently be recombined in a variety of ways,5The formula for AER given here and in Och & Ney(2003) is intended to compare an automatic alignmentagainst a gold standard alignment.
However, whencomparing one human against another, both comparisonand reference distinguish between SURE and POSSIBLElinks.
Because the AER is asymmetric (though eachdirection differs by less than 5%), we have presented theaverage of the directional AERs.such as union to maximize recall or intersection tomaximize precision.
Och also documents a methodfor heuristically recombining the unidirectionalalignments intended to balance precision andrecall.
In our experience, many alignment errorsare present in one side but not the other, hence thisrecombination also serves to filter noise from theprocess.4 EvaluationTable 1 shows the results of training translationmodels on data extracted by both methods and thentested on the blind data.
The best overallperformance, irrespective of test data type, isachieved by the L12 training set, with an 11.58%overall AER on the 250 sentence pair edit distancetest set (20.88% AER for non-identical words).The F2 training data is probably too sparse and,with 40% unrelated sentence pairs, too noisy toachieve equally good results; nevertheless the gapbetween the results for the two training data typesis dramatically narrower on the F2 test data.
Thenearly comparable numbers for the two trainingdata sets, at 13.2% and 14.7% respectively, suggestthat the L12 training corpus provides nosubstantive advantage over the F2 data when testedon the more complex test data.
This is particularlystriking given the noise inherent in the F2 trainingdata.5 Analysis/DiscussionTo explore some of the differences between thetraining sets, we hand-examined a random sampleof sentence pairs from each corpus type.
The mostcommon paraphrase alternations that we observedfell into the following broad categories:?
Elaboration: Sentence pairs can differ in totalinformation content, with an added word,phrase or clause in one sentence that has noTraining Data Type: L12 F2 L12 F2Test Data Type: 250 Edit Dist 250 Edit Dist 116 F2 Heuristic 116 F2 HeuristicPrecision   87.46% 86.44% 85.07% 84.16%Recall      89.52% 82.64% 88.70% 86.55%AER         11.58% 15.41% 13.24% 14.71%Identical word precision   89.36% 88.79% 92.92% 93.41%Identical word recall      89.50% 83.10% 93.49% 92.47%Identical word AER         10.57% 14.14% 6.80% 7.06%Non-Identical word precision   76.99% 71.86% 60.54% 53.69%Non-Identical word recall      90.22% 69.57% 59.50% 50.41%Non-Identical word AER         20.88% 28.57% 39.81% 47.46%Table 1.
Precision, recall, and alignment error rates (AER) for F2 and L12counterpart in the other (e.g.
the NASDAQ /the tech-heavy NASDAQ).?
Phrasal: An entire group of words in onesentence alternates with one word or a phrasein the other.
Some are non-compositionalidioms (has pulled the plug on / is droppingplans for); others involve different phrasing(electronically / in electronic form, more thana million people / a massive crowd).?
Spelling: British/American sources system-atically differ in spellings of common words(colour / color); other variants also appear(email / e-mail).?
Synonymy:  Sentence pairs differ only in oneor two words (e.g.
charges / accusations),suggesting an editor?s hand in modifying asingle source sentence.?
Anaphora: A full NP in one sentencecorresponds to an anaphor in the other (PrimeMinister Blair / He).
Cases of NP anaphora(ISS / the Atlanta-based security company) arealso common in the data, but in quantifyingparaphrase types we restricted our attention tothe simpler case of pronominal anaphora.?
Reordering: Words, phrases, or entireconstituents occur in different order in tworelated sentences, either because of majorsyntactic differences (e.g.
topicalization, voicealternations) or more local pragmatic choices(e.g.
adverb or prepositional phrase placement).These categories do not cover all possiblealternations between pairs of paraphrasedsentences; moreover, categories often overlap inthe same sequence of words.
It is common, forexample, to find instances of clausal Reorderingcombined with Synonymy.Figure 2 shows a hand-aligned paraphrase pairtaken from the F2 data.
This pair displays oneSpelling alternation (defence / defense), oneReordering (position of the ?since?
phrase), andone example of Elaboration (terror attacks occursin only one sentence).To quantify the differences between L12 and F2,we randomly chose 100 sentence pairs from eachdataset and counted the number of times eachphenomenon was encountered.
A given sentencepair might exhibit multiple instances of a singlephenomenon, such as two phrasal paraphrasechanges or two synonym replacements.
In thiscase all instances were counted.
Lower-frequencychanges that fell outside of the above categorieswere not tallied: for example, the presence orabsence of a definite article (had authority / hadthe authority) in Figure 2 was ignored.
Aftersumming all alternations in each sentence pair, wecalculated the average number of occurrences ofeach paraphrase type in each data set.
The resultsare shown in Table 2.Several major differences stand out between thetwo data sets.
First, the F2 data is less parallel, asevidenced by the higher percentage of Elaborationsfound in those sentence pairs.
Loss of parallelism,however, is offset by greater diversity ofparaphrase types encountered in the F2 data.Phrasal alternations are more than 4x morecommon, and Reorderings occur over 20x morefrequently.
Thus while string difference methodsmay produce relatively clean training data, this isachieved at the cost of filtering out common (andinteresting) paraphrase relationships.6 Conclusions and Future WorkEdit distance identifies sentence pairs thatexhibit lexical and short phrasal alternations thatcan be aligned with considerable success.
Given alarge dataset and a well-motivated clustering ofdocuments, useful datasets can be gleaned evenwithout resorting to more sophisticated techniquesFigure 2.
Sample human-aligned paraphraseL12 F2Elaboration 0.83 1.3Phrasal 0.14 0.69Spelling 0.12 0.01Synonym 0.18 0.25Anaphora 0.1 0.13Reordering 0.02 0.41Table 2.
Mean number of instances ofparaphrase phenomena per sentence(such as Multiple Sequence Alignment, asemployed by Barzilay & Lee 2003).However, there is a disparity between the kindsof paraphrase alternations that we need to be ableto align and those that we can already align wellusing current SMT techniques.
Based solely on thecriterion of word AER, the L12 data would seem tobe superior to the F2 data as a source of paraphraseknowledge.
Hand evaluation, though, indicatesthat many of the phenomena that we are interestedin learning may be absent from this L12 data.String edit distance extraction techniques involveassumptions about the data that are inadequate, butachieve high precision.
Techniques like our F2extraction strategies appear to extract a morediverse variety of data, but yield more noise.
Webelieve that an approach with the strengths of bothmethods would lead to significant improvement inparaphrase identification and generation.In the near term, however, the relatively similarperformances of F2 and L12-trained models on theF2 test data suggest that with further refinements,this more complex type of data can achieve goodresults.
More data will surely help.One focus of future work is to build a classifierto predict whether two sentences are relatedthrough paraphrase.
Features might include editdistance, temporal/topical clustering information,information about cross-document discoursestructure, relative sentence length, and synonymyinformation.
We believe that this work haspotential impact on the fields of summarization,information retrieval, and question answering.Our ultimate goal is to apply current SMTtechniques to the problems of paraphraserecognition and generation.
We feel that this is anatural extension of the body of recentdevelopments in SMT; perhaps explorations inmonolingual data may have a reciprocal impact.The field of SMT, long focused on closely aligneddata, is only now beginning to address the   kindsof problems immediately encountered inmonolingual paraphrase (including phrasaltranslations and large scale reorderings).Algorithms to address these phenomena will beequally applicable to both fields.
Of course abroad-domain SMT-influenced paraphrase solutionwill require very large corpora of sententialparaphrases.
In this paper we have described justone example of a class of data extractiontechniques that we hope will scale to this task.AcknowledgementsWe are grateful to the Mo Corston-Oliver, JeffStevenson and Amy Muia of the Butler Hill Groupfor their work in annotating the data used in theexperiments.
We have also benefited fromdiscussions with Ken Church, Mark Johnson,Daniel Marcu and Franz Och.
We remain,however, responsible for all content.ReferencesR.
Barzilay and K. R. McKeown.
2001.
ExtractingParaphrases from a parallel corpus.
In Proceedings ofthe ACL/EACL.R.
Barzilay and  L. Lee.
2003.
Learning to Paraphrase:an unsupervised approach using multiple-sequencealignment.
In Proceedings of HLT/NAACL.P.
Brown, S. A. Della Pietra, V.J.
Della Pietra and R. L.Mercer.
1993.
The Mathematics of StatisticalMachine Translation.
Computational Linguistics,19(2): 263-311.V.
Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
SovietPhysice-Doklady, 10:707-710.D.
Lin and P. Pantel.
2001.
DIRT - Discovery ofInference Rules from Text.
In Proceedings of ACMSIGKDD Conference on Knowledge Discovery andData Mining.I.
D. Melamed.
2001.
Empirical Methods for ExploitingParallel Texts.
MIT Press.R.
Mihalcea and T. Pedersen.
2003 An EvaluationExercise for Word Alignment.
In Proceedings of theWorkshop on Building and Using Parallel Texts:Data Driven Machine Translation and Beyond.
May31, 2003.
Edmonton, Canada.F.
Och and H. Ney.
2000.
Improved StatisticalAlignment Models.
In Proceedings of the 38thAnnual Meeting of the ACL, Hong Kong, China.F.
Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.Computational Linguistics, 29(1):19-52.Y.
Shinyama, S. Sekine and K. Sudo.
2002.
AutomaticParaphrase Acquisition from News Articles.
InProceedings of NAACL-HLT.S.
Vogel, H. Ney and C. Tillmann.
1996.
HMM-BasedWord Alignment in Statistical Translation.
InProceedings of the Annual Meeting of the ACL,Copenhagen, Denmark.
