Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1617?1628,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsA Theme-Rewriting Approach for Generating Algebra Word ProblemsRik Koncel-KedziorskiIoannis KonstasLuke ZettlemoyerHannaneh HajishirziUniversity of Washingtonkedzior@uw.edu, {ikonstas, lsz, hannaneh}@cs.washington.eduAbstractTexts present coherent stories that have a par-ticular theme or overall setting, for examplescience fiction or western.
In this paper, wepresent a text generation method called rewrit-ing that edits existing human-authored narra-tives to change their theme without changingthe underlying story.
We apply the approachto math word problems, where it might helpstudents stay more engaged by quickly trans-forming all of their homework assignmentsto the theme of their favorite movie with-out changing the math concepts that are be-ing taught.
Our rewriting method uses a two-stage decoding process, which proposes newwords from the target theme and scores the re-sulting stories according to a number of fac-tors defining aspects of syntactic, semantic,and thematic coherence.
Experiments demon-strate that the final stories typically representthe new theme well while still testing the orig-inal math concepts, outperforming a numberof baselines.
We also release a new dataset ofhuman-authored rewrites of math word prob-lems in several themes.1 IntroductionStorytelling is the complex activity of expressing aplot, its events and participants in words meaning-ful to an audience.
Automatic storytelling systemscan be used for customized sport commentaries, en-riching video games with personalized or dynamicplot-lines (Barros and Musse, 2007), or providingcustomized learning materials which meet each indi-vidual student?s needs and interests (Bartlett, 2004).In this paper, we focus on generating narrative-styleJim walked 0.2 of a mile from school to David?shouse and 0.7 of a mile from David?s house to hisown house.
How many miles did Jim walk in all?Star WarsUncle Owen walked 0.2 of a mile from hangar to LukeSkywalker?s room and 0.7 of a mile from Luke Sky-walker?s room to his own room.
How many miles didUncle Owen walk in all?CartoonFinn squished 0.2 of a mile from cupboard toMelissa?s dock and 0.7 of a mile from Melissa?s dockto his own dock.
How many miles did Finn squish inall?WesternDuane strolled 0.2 of a mile from barn to Madeline?scamp and 0.7 of a mile from Madeline?s camp to hisown camp.
How many miles did Duane stroll in all?Figure 1: An example story and rewrites in 3 themes.math word problems (Figure 1) and demonstrate thatit is possible to design an algorithm that can auto-matically change the overall theme of a text withoutchanging its underlying story, for example to createmore engaging homework that is in the theme of astudent?s favorite movie.A math word problem is a coherent story thatprovides the student with good clues to the cor-rect mathematical operations between the numericalquantities described therein.
However, the particulartheme of a problem, whether it be about collectingapples or traveling distances through space, can varysignificantly so long as the correlation between thestory and underlying equation is maintained.
Stu-dents?
success at solving a word problem is tiedto their interest in the problem?s theme (Renninger1617et al, 2002), and personalizing word problems in-creases student understanding, engagement, and per-formance in the problem solving process (Hart,1996; Davis-Dorsey et al, 1991).Motivated by this need for thematically diverse,highly coherent stories, we address the problemof story rewriting, or transforming human-authoredstories into novel, coherent stories in a new theme.Rather than synthesizing first a story plot (McIntyreand Lapata, 2009; McIntyre and Lapata, 2010) orscript (Chambers and Jurafsky, 2009; Pichotta andMooney, 2016; Granroth-Wilding and Clark, 2016)from scratch, we instead begin from an existingstory and iteratively edit it towards a thematicallynovel but ?most crucially?
semantically compatiblestory.
This approach allows us to reuse much, butnot all, of the syntactic and semantic structure of theoriginal text, resulting in the creation of more coher-ent and solvable math word problems.We define a theme to be a collection of refer-ence texts, such as a movie script or series of books.Given a theme, the rewrite algorithm constructs newtexts by substituting thematically appropriate wordsand phrases, as measured with automatic metricsover the theme text collection, for parts of the orig-inal texts.
This process optimizes for a number ofmetrics of overall text quality, including syntactic,semantics, and discourse scores.
It uses no handcrafted templates and requires no theme-specifictuning data, making it easy to apply for new themesin practice.
Tables 4?6 show example stories gener-ated from the rewrite system.To evaluate performance, we collected a corpusof 450 rewrites of math word problems in Star Warsand Children?s Cartoon themes via crowdsourcing.1Experiments with automated metrics and humanevaluations demonstrate that the approach describedhere outperforms a number of baselines and can pro-duce solvable problems in multiple different themes,even with no in-domain tuning.2 Related WorkOur approach is related to the previous work in storygeneration (e.g., McIntyre and Lapata (2010)) andsentence rewriting (e.g., text simplification (Xu et1Data and code available at https://gitlab.cs.washington.edu/kedzior/Rewriter/.al., 2016)), as reviewed in this section.
It has threemajor differences from all these approaches: First,we focus on multi-sentence stories where preservingthe coherence, discourse relations, and solvability isessential.
Previous work mainly focuses on rewrit-ing single sentences.
Second, we build a themefrom a text corpus and show how the stories can beadapted to new themes.
Third, our method leveragesthe human-authored story to capture the semanticskeleton and the plot of the current story, rather thansynthesizing the story plot.
To our knowledge, weare the first to introduce a text rewriting formulationfor story generation.Story generation has been of long interest to AIresearchers (Meehan, 1976; Lebowitz, 1987; Turner,1993; Liu and Singh, 2002; Mostafazadeh et al,2016).
Recent methods in story generation first syn-thesize candidate plots for a story and then compilethose plots into text.
Li et al (2013) use crowd-sourcing to build plot graphs.
McIntyre and Lap-ata (2009; 2010) address story generation throughthe automatic deduction and reassembly of scripts(Schank and Abelson, 1977), or structured represen-tations of events and their participants, and causalrelationships involved.
Leveraging the automaticscript learning methods of Chambers and Jurafsky(2009), McIntyre and Lapata (2010) learn candidateentity-centered plot graphs, or possible events in-volving the entity and an ordering between theseevents, with the use of a genetic algorithm.
Thenplots are compiled into stories through the use ofa rule-based text surface realizer (Lavoie and Ram-bow, 1997) and reranked using a language model.Polozov et al (2015) automatically generate mathword problems tailored to a student?s interest us-ing Answer Set Programming to satisfy a collec-tion of pedagogical and narrative requirements.
Thismethod naturally produces highly coherent, person-alized story problems that meet pedagogical require-ments, at the expense of building the thematic on-tologies and discourse constraints by hand.2Additionally, there is related work in text simpli-fication (Wubben et al, 2012; Kauchak, 2013; Zhuet al, 2010; Vanderwende et al, 2007; Woodsendand Lapata, 2011b; Hwang et al, 2015), sentence2According to Polozov et al (2015) building small thematicontologies of types, relations, and discourse tropes (100-200 en-tries) for each of only 3 literary settings took 1-2 person months.1618compression (Filippova and Strube, 2008; Rush etal., 2015), and paraphrasing (Ganitkevitch et al,2013; Chen and Dolan, 2011; Ganitkevitch et al,2011).
All these tasks are focused on rewriting sen-tences under a predefined set of constraints, such assimplicity.
Different rule-based and data-driven ap-proaches are introduced by Petersen and Ostendorf(2007), Vickrey and Koller (2008), and Siddharthan(2004).
Most data-driven approaches take advantageof machine translation techniques, use source-targetsentence pairs, and learn rewrite operations (Yatskaret al, 2010; Woodsend and Lapata, 2011a), or useadditional external paraphrasing resources (Xu et al,2016).Finally, this work is related to those on auto-matically solving math word problems.
Specifictopics include number word problems (Shi et al,2015), logic puzzle problems (Mitra and Baral,2015), arithmetic word problems (Hosseini et al,2014; Roy and Roth, 2015), algebra word prob-lems (Kushman et al, 2014; Zhou et al, 2015;Koncel-Kedziorski et al, 2015a; Roy et al, 2016),and geometry word problems (Seo et al, 2015; Seoet al, 2014).
Several datasets of word problems areavailable (Koncel-Kedziorski et al, 2016; Huang etal., 2016), though none address the need for thematictext.3 Problem FormulationOur system takes as input a story s and a theme t,and outputs the best rewrite s?
from generated can-didates S.A theme t is defined as a textual corpus that de-scribes a topic or a domain.
This is an intentionallybroad definition that allows a variety of textual re-sources to serve as themes.
For example, the collec-tion of all Science Fiction stories from the ProjectGutenberg can be a theme, or the script of a singlemovie, or a sampling of fan fiction from the Inter-net.
This flexibility adds to the utility of our work,as varying amounts of thematic text may be avail-able.The generated candidate s?
is the most themati-cally fit problem that is syntactically and semanti-cally coherent given the original problem s and thenew theme t. We represent a story in terms of thewords it contains, so that s = {w1, w2, .
.
.
wn} andSam had 2 dogs.
Each had 3 puppies.Syntactic relationsCandidates (s?
)Semantic relationsLuke Skywalker had 2 ships.
Each had 3 droids.Original (s)Syn(s?|s)SemPair(s?|s)Th(s?|t)SemLex(s?|s)Figure 2: An overview of our method for scoring a candi-date story s?
given a human-authored story s and a themet.
Syn(s?|s): compatibility of syntactic relations (purplearrows), Sempair(s?|s): coherence of semantic relations(blue arrows), SemLex(s?|s): semantic mapping of indi-vidual words, and Th(s?|s, t): thematicity.|s| = n. The new story s?
is defined as:s?
={f(w1), f(w2), .
.
.
f(wn)}where the function f(w) : Vo ?
VKt ?
?, rewrites aword from the vocabulary of the original problem Voto either a word, a trivial noun compound of lengthK (e.g., multi-word named entity) from the vocab-ulary of the the thematic vocabulary Vt, or reducesto the empty symbol, i.e., omits the input word en-tirely; hence the length of s?
can differ from that ofthe original problem.Formally, our goal is to select the candidates?
?
S by maximizing a scoring functionR over the-matic, syntactic and semantic constraints, subject toa set of parameters ?:s?
= argmaxs?
?SR(s?|s, t; ?)
(1)In order to find the best story s?, our problemreduces to generating candidate stories s?
from thespace of possible rewrites of the human-authoredstory s in a new theme t (Section 5).
Since thereare exponentially many rewrites, we follow a two-stage decoding approach: first we identify only thecontent words wi in the input problem, and providefor each a list of the top-k most salient thematic can-didate words and trivial noun compounds.
We thensearch the space by progressively introducing morerewrites in the beam, and scoring them according toR (Section 4).
Figure 2 shows the overview of thescoring function for a candidate sentence s?.16194 Scoring StoriesThe scoring functionR decomposes into three com-ponents, capturing aspects of syntactic compatibil-ity, semantic coherence, and thematicity:R(s?|s, t; ?)
=??
Sem(s?|s)+ ?
?
Syn(s?|s) (2)+ ?
?
Th(s?|s, t)The syntactic (Syn) and semantic (Sem) coherencecomponents measure the coherence of the words inthe new story s?, as well as their compatibility to thesyntactic and semantic relations in the original storys.
On the other hand, thematicity (Th) scores therelevance and importance of words in the new storywith respect to theme t.We describe each of these components and the de-coding process in the following sections.4.1 ThematicityRecall that a theme t is defined as a collectionof documents that share a common topic, such asbooks in the science fiction genre, or scripts of hor-ror movies.
We define thematicity of a word w?
asthe measure of salience, or how discriminative thatword is to a given theme.3 For example, robot andspaceship are expected to be highly thematic withrespect to Star Wars.
In our setting we extend thisdefinition to a candidate problem s?
given s and t as:Th(s?|s, t) =|s|?iSal(w?i, t) (3)where w?i is a word from the candidate problem, andSal is its salience score with respect to the theme.In the context of this work we argue that the the-matic adaptation of the content words, i.e., nouns,verbs, named entities, and adjectives, plays the mostimportant role in forming a new thematic problem.Therefore, we define their salience (except namedentities) based on their tf-idf score over the theme t,and set it to zero for function words.
Since namedentities have relatively low frequencies in the themecorpus we set their salience to 1?
1c(w?i) , where c(w?i)3We will be interchangeably referring to w?
as either theword or the head of the multi-word noun compound thatrewrites the equivalent word w in the original problem.is the number of times w?i occurs in the theme.
Inthe example story in Figure 2 the thematicity scoreis derived as Sal(Luke Skywalker) + Sal(ships) +Sal(droids).4.2 Syntactic compatibilityThis work offers a new method for syntactic anddiscourse coherence based on preserving human-authored syntactic structure in generated text (henceour use of the term rewriting).
The syntactic con-structs in a document play a distinctive role in main-taining cohesion across sentences.
We consider thehuman-authored syntax of the original story s asgold standard, and use it to score a candidate prob-lem s?
by considering how well the syntactic rela-tions of s apply to s?.Formally, given a dependency triple (wi, wj , l)from a parse of a sentence in s, we compute thelikelihood for the corresponding triple (w?i, w?j , l) forw?i, w?j in s?.
We define the syntactic score for allsentences in s?
as:Syn(s?|s) =?i,j,l|(wi,wj ,l)?Dep(s)LDep(w?i, w?j , l) (4)where Dep(s) are the dependency parse trees for allsentences in s; LDep is a 3-gram language modelover dependency triples which gives the likelihoodof an arc label l being used between a pair of words(w?i, w?j).
For example in Figure 2, the syntacticcompatibility score includes dependency likelihoodsof LDep(ship, 2, num), LDep(had, ship, dobj).Therefore, the Syn function prefers stories s?
that(a) have similar dependency structure to the origi-nal story s and (b) make use of a common syntacticconfiguration.4.3 Semantic CoherenceThe semantic coherence component expresses howwell a candidate s?
rewrites individual words andrealizes the semantic relationships that exist in thehuman-authored story s. Ideally, we would like topreserve enough of the semantics of s in order toproduce a coherent story s?, yet we are populating s?with words taken from an unrelated theme.
There-fore, we model the semantics of a story s?
in termsof the lexical semantics contributed by individualwords as well as semantic relationships that exist be-tween its elements.
Note that the relationships can1620cross the sentence boundaries, promoting discoursecoherence.We decompose semantic relations in a story intoa set of local, lexical relationships between pairs ofwords.
Specifically, we consider semantic relationsfor noun-noun and verb-verb pairs as provided byWordNet (Miller, 1995).
Since some relations arenot directly outlined in these resources (e.g., the se-lectional preferences of nouns with regard to theiradjectival modifiers), we also consider the word-embedding similarity between words.
For examplein Figure 2 the semantic relationships are denotedwith blue arrows between pairs of content words inthe story (e.g., {Sam, dogs}, {dogs, puppies}, etc).More formally, we define the semantic coherenceof s?
with respect to s as:Sem(s?|s) =|s?|?iSemLex(wi, w?i) (5)+?i,j?CWSemPair({wi, wj}, {w?i, w?j})where CW is the set of pairs of indices of contentwords (nouns, verbs, adjectives, and named entities)from s. We focus on the content words of the orig-inal problem, as they carry most of the semantic in-formation.
SemLex and SemPair functions are se-mantic adaptation scores for individual words andsemantic relations respectively, described below.Semantic Compatibility between words (SemLex)is defined as:SemLex(wi, w?i) = cos(wi, w?i) +Resnik(wi, w?i)(6)where cos(wi, w?i) denotes the cosine similarity be-tween the vector space embeddings of two words wiandw?i4, andResnik(wi, w?i) expresses the informa-tion content of the lowest subsumer of {wi, w?i} inWordNet.
For example in Figure 2, the semanticcompatibility score incorporates lexical similaritiesSemLex(dog, ship), etc.Compatibility score between semantic relations(SemPair) is defined by adding two components:PairSim and Analogy that compute how seman-tic relations between pairs of words are preserved in4For the ease of notation, we represent the embedding of thewords with wi as well.the new story:PairSim =cos(wi, wj) ?
cos(w?i, w?j) (7)Analogy =cos(w?i + wj ?
wi, w?j) (8)PairSim preserves the similarity between pairs ofwords {wi, wj} in s and the corresponding pair{w?i, w?j} in the new story s?.
Intuitively, ifwi andwjare semantically close to each other, we would likethe corresponding words to be close in the new storyas well.
For example in Figure 2, ?dog?
and ?puppy?are similar in the original story, we expect the cor-responding words ?ship?
and ?droid?
to be similar inthe new story.
The Analogy function, inspired byMikolov et al (2013), computes the analogy of w?jfromw?i given the relationship that holds betweenwiand wj in the vector space.
For example in Figure 2,the relation between ?Sam?
and ?dog?
is similar tothe relation between ?Luke Skywalker?
and ?ship?.5 DecodingOur decoding process begins by first identifyingthe content words wi (nouns, verbs, adjectives andnamed entities) in the original problem s that will beconsidered as initial points for rewriting.
For eachof these lexical classes we extract the top-k mostthematic words and trivial noun compounds fromthe theme t. For example, in Figure 2, candidatenouns are: ?ships?, ?robots?, ?droids?, etc., and forverbs: ?blast?, ?soar?, ?command?, etc.
Recall thatthe space of candidate rewrites is large, prohibitingan exhaustive enumeration.
We therefore do approx-imate search with a beam by considering simultane-ously all possible paths that start at the different ini-tial points.
At each step the decoder considers anadditional rewrite from the list of candidates, adds itto the existing hypothesis path, and scores it accord-ing to functionR (Equation 2).All the counterpart scores are locally optimal,as they factor over each new word w?i or pair of{w?i, w?j}, where w?j is a rewrite already existing inthe hypothesis path.
At any given step we may re-combine hypotheses that share the same prefix hy-pothesis path, and keep the top scoring one.
Theprocess terminates when there are no more rewritesleft.
We also experimented decoding with a varietyof orderings of the text in the original problem s,including left-to-right, and head-first following the1621dependency tree of each sentence and then concate-nating these linearizations; we observed that consid-ering multiple paths achieves the best performance.6 Data CollectionFor the set of human-authored stories {s}, we use acorpus of math word problems described in Koncel-Kedziorski et al (2016).
We select a subset of150 problems targeting 5th and 6th grade levels, allof which involve a single equation in one variable.These problems have 2.7 sentences and 29.4 wordson average, 12.6 of which are considered contentwords by our system.
In order to tune and evaluateour model, we collect a corpus of human-authoredrewrites produced by workers from Amazon Me-chanical Turk based on two themes: Star Wars, andAdventure Time (a children?s cartoon).We experimented with different ways of helpingto define the theme for the workers, including of-fering automatically generated word clouds or en-forcing that a response includes one of several key-words.
In practice, we have found that using specificcultural elements as themes (such as famous movieor cartoon franchises) attracts workers who alreadyhave a strong knowledge of the theme, resulting inhigher quality work.To help explain the rewriting process, we showworkers three examples of thematic rewrites withvarying degrees of correlation to the original prob-lems.
We then show workers a random problemfrom the original set {s} and a corresponding equa-tion for that problem.
We instruct the workers to?rewrite?
the problem according to the theme, en-suring that their rewritten problem can be solved bythe provided equation.
The final dataset collectioncomprises of 450 human-authored rewrites.
We col-lect 3 rewrites for 100 of the original problems forthe Star Wars theme (based on the popular Star Warssequel movies), and 3 rewrites for the rest of the 50original problems, for the Children Cartoons Theme(CARTOON), based on the Adventure Time TV show.We keep 150 examples from the Star Wars theme fordevelopment (STARdev), and the rest 150 for testing(STARtest).We collected the STARdev and CARTOON databased on workers with the ?master?
designationand at least 95% approval rating.
Then we pro-ceeded collecting STARtest by a subset of the authorsof STARdev who self-identify as theme experts andwhose quality of work is manually confirmed.7 Experiments7.1 SetupImplementation Details We pre-process thethemes using the Stanford CoreNLP tools (Man-ning et al, 2014) for tokenization, Named EntityRecognition (Finkel et al, 2005), and dependencyparsing (Chen and Manning, 2014).
For calculatingsalience scores, we use the ScriptBase dataset ofmovie scripts (Gorinski and Lapata, 2015).
TheStar Wars theme is constructed from the availablescript, roughly 7300 words.
The Cartoon theme isconstructed from fan-authored scripts of the first 10episodes of the show (Springfield, 2016) totaling1370 words.
Since our thematic options are takenfrom arbitrary text, we use the lists of offensiveterms published by The Racial Slur database(Database, 2016) and FrontGate Media (Media,2016) to filter out offensive content.
To prohibitovergeneration, we forbid the transformation of stopwords or math-specific words (Survivors, 2013;Koncel-Kedziorski et al, 2015b).For syntactic compatibility score Syn (Equa-tion 4) we use the English Fiction subset of theGoogle Syntactic N-grams corpus (Goldberg andOrwant, 2013) and train a 3-gram language modelusing KenLM (Heafield, 2011).
For SemLex,PairSim and Analogy (Equations 6-8) we usethe pretrained word embeddings of Levy and Gold-berg (2014).
These embeddings are trained usingdependency contexts rather than windows of ad-jacent words, allowing them to capture functionalword similarity.
Finally, we tune the parametersof our model (Equation 2) on the development setSTARdev and pick those values5 that maximize ME-TEOR score (Denkowski and Lavie, 2014) against 3human references.Evaluation We compare two ablated configura-tions of our method against our full model (FULL):-SYN that only uses semantic and thematicity com-ponents and does not incorporate the syntactic com-patibility score, -SEM replaces the semantic coher-5We set ?
= 0.1, ?
= 0.1 and ?
= 11622Model STARdev STARtest CARTOONFULL 31.82 29.16 32.08-SEM 28.72 25.55 27.55-SYN 31.92 29.14 32.04Table 1: METEOR results for different configu-ration of our model on STARdev , STARtest andCARTOON datasets.ence score with the simpler cos(wi, w?i), effectivelyrewriting only single words, and not pairs.
We re-frained from ablating the thematicity score as it isthe core part of our model that drives the rewritingprocess into a new theme.We evaluate our method using an automatic met-ric, and via eliciting human judgments on Ama-zon Mechanical Turk.
For automatic evaluation, wecompute the METEOR score, comparing the out-put of each model for a given problem and themeto the 3 human rewrites we collected, on STARdev,STARtest and CARTOON.
METEOR is a recall-oriented metric, widely used in the MT community;the additional stemming, synonym and paraphrasematching modules make it more applicable for ouruse, given the nature of our rewriting task.6For human evaluation, we conduct pairwise com-parison tests, pairing FULL against a human rewrite(HUMAN), FULL against -SYN, and FULL against-SEM.
Participants were given a short descriptionof the theme, and the output of each system.
Foreach test we asked 40 subjects to select which prob-lem they preferred over 5 pairs of outputs; we ob-tained a total of 200 (5x40) responses for STARtestand CARTOON.In order to better understand the strengths andweaknesses of the generated stories, we conducted amore detailed human evaluation.
8 participants werepresented with the output of the three automatic sys-tems, human rewrites (HUMAN), and a theme.
Theparticipants were asked to rate the stories acrossthree dimensions: coherence (how coherent is thetext of the problem?
), solvability (can elementaryschool students solve it?
), and thematicity (how welldoes the problem express them?)
on a scale from1 to 5.
We collected ratings over 16 outputs from6The average METEOR score comparing 1 annotatoragainst the other 2 is 0.26, indicating that there are diverse cor-rect strategies for solving the rewriting problem.Model STARtest CARTOONFULL 65.0 57.9-SYN 35.0 42.1FULL 68.8 69.4-SEM 31.2 30.6FULL 17.9 10.0HUMAN 82.1 90.0Table 2: Human evaluation results on pairwise compar-isons between FULL and -SYN, and FULL and HUMAN,on STARtest and CARTOON datasets.Model Thematicity Coherence SolvabilityHUMAN 3.7 3.175 4.025FULL 3.7 3.025 3.9-SYN 3.375 3.075 3.825-SEM 3.325 2.65 3.7Table 3: Human evaluation results for FULL, -SYN,-SEMand HUMAN on thematicity, coherence and solv-ability on STARtest.STARtest, resulting in 128 responses.7.2 ResultsTable 1 reports METEOR; we notice that removingthe semantic coherence scores in -SEM hurts the per-formance compared to FULL; this confirms our claimthat semantic compatibility is crucial for buildingcoherent stories.
On the other hand, -SYN performssimilarly to FULL.
Closer inspection of the -SYN sys-tem?s output reveals a greater diversity in thematicelements as a result of the relaxed syntactic compat-ibility constraints.
Hence it is more likely to havegreater overlap with any of the reference rewrites,resulting in higher METEOR scores.However, a pairwise comparison betweenFULL and -SYN (Table 2) reveals that human sub-jects consistently prefer the output of FULL insteadof -SYN both for STARtest and CARTOON.
Table 2also reports that HUMAN outperforms the outputof the FULL model, and a pairwise comparison ofFULL and -SEM which yields a result in line with theMETEOR scores.Table 3 shows the results of the detailed com-parison of Thematicity, Coherence, and Solvability.This table clearly shows the strong contribution ofthe semantic component of our system.
The specificcontribution of the syntactic component is to pro-1623Star Warss1.
Wendy bought 4 new chairs and 4 new tables forher house.
If she spent 6 minutes on each piece fur-niture putting it together, how may minutes did it takeher to finish?s?1.
Leia bought 4 new ships and 4 new guns forher room.
If she spent 6 minutes on each wastelandweapon putting it together, how many minutes did ittake her to terminate?s2.
My car gets 20 miles per gallon of gas.
How manymiles can I drive on 5 gallons of gas?s?2.
My cruiser gets 20 miles per gallon of light.
Howmany miles can I drive on 5 gallons of light?s3.Tyler had 15 dogs.
Each dog had 5 puppies.
Howmany puppies does Tyler now have?s?3.
Biggs had 15 creatures.
Each creature had 5 crea-tures.
How many creatures does Biggs now have?Table 4: Examples of the original stories si and rewrittenmath word problems s?i in Star War theme.duce overall more solvable and thematically satisfy-ing problems, although it can slightly affect coher-ence especially when automatic parses fail.
Finally,the overall high ratings for human-authored storiesacross all three dimensions, confirm the high qualityof the crowd-sourced stories.7.3 Qualitative ExamplesTable 4?6 shows some problems generated by ourmethod.
Recall that since our system needs no an-notated thematic training data, we can easily gen-erate from any theme where thematic text is avail-able.
To demonstrate this fact, we include gener-ated examples in a Western theme from novels fromthe Project Gutenberg corpus.
Many of the resultsof our system are very legible, with only minoragreement errors.
Coherent, thematic semantic re-lations are evident in problems such as s?1, whereships, guns, and weapons combine to effect the StarWars theme; this is also evident in s?5, where peoplewith western sounding names like Kurt and Made-line trade in cigarettes, an old-fashioned pre-cursorto e-cigarettes.In some cases, semantic inconsistencies result inweird sounding problems, such as in s?6 where themain character receives ?wheat of grub?.
But be-cause of the syntactic compatibility component, ourmodel scores this candidate higher because of theCartoons7.
Dave was helping the cafeteria workers pick uplunch trays, but he could only carry 9 trays at a time.If he had to pick up 17 trays from one table and 55trays from another, how many trips will he make?s?7.
Finn was helping the cupboard men pick up candybottles, but he could only carry 9 bottles at a time.If he had to pick up 17 bottles from one ring and 55bottles from another, how many swords will he make?s8.
If books came from all the 4 continents that Bryanhad been into and he collected 122 books per conti-nent, how many books does he have from all 4 conti-nents combined?s?8.
If dances came from all the 4 mountains that Finnhad been into and he collected 122 dances per moun-tain, how many dances does he have from all 4 moun-tains combined?s9.
A bucket contains 3 gallons of water.
If Derekadds 6.8 gallons more, how many gallons will therebe in all?s?9.
A bottle makes 3 gallons of serum.
If Finn adds6.8 gallons more, how many gallons will there be inall?Table 5: Examples of the original stories si and rewrittenmath word problems s?i in Cartoon theme.connection between ?wheat?
and ?graze?.Semantic incoherence is less of a problem in thecartoon theme, where absurd interactions betweencharacters are expected.
However, a difficulty forour system is demonstrated in s?7, where the physicalentity ?swords?
is substituted for the nominalizationof an event ?trips?.
Improvements to the semanticcoherence component could resolve such issues.Table 7 shows some instances where the rewritealgorithm produces unusable results.
An exampleof under-generation is s?10.
Here, too many wordsare left untouched, resulting in both ungrammatical-ity and semantic incoherence.
In s?11, we witnesssome limitations of using word vectors.
The rareword ?Ferris?
is not close to anything in the StarWars theme, and is thus mapped almost arbitrarilyto ?int?
(movie script shorthand for an interior shot).Better treatment of noun compounds and the use ofphrase vectors would reduce such errors.8 ConclusionWe formalized the problem of story rewriting as au-tomatically changing the theme of a text without1624Westerns4.
Christians father and the senior ranger gatheredfirewood as they walked towards the lake in the park...s?4.
Christian ?s partner and the lone sheriff harvestedbarley as they strolled towards the hip in the orchard...s5.
Sally had 27 cards.
Dan gave her 41 new cards.Sally bought 20 cards.
How many cards does Sallyhave now?s?5.
Madeline had 27 cigarettes.
Kurt gave her 41 newcigarettes.
Madeline bought 20 cigarettes.
How manycigarettes does madeline have now?s6.
For Halloween Megan received 11 pieces of candyfrom neighbors and 5 pieces from her older sister.
Ifshe only ate 8 pieces a day, how long would the candylast her?s?6.
For Halloween Madeline received 11 wheat ofgrub from proprietors and 5 wheat from her namelesspartner.
If she only grazed 8 wheat a day, how longwould the grub last her?Table 6: Examples of the original stories si and rewrittenmath word problems s?i in Western theme.altering the underlying story and developed an ap-proach for rewriting algebra word problems wherethe rewriting model optimized for a number of mea-sures of overall text coherence.
Experiments on anewly gathered dataset demonstrated our model canproduce themed texts that are usually solvable.Future work could improve the thematicity andsolvability components by incorporating domain-specific and commonsense knowledge, leveraginginformation extraction.
Additionally, neural net-work architectures (e.g., LSTMs, seq2seq) can betrained to rewrite coherently with less reliance onbrittle syntactic parses.
Additionally, we plan tostudy rewriting in other domains such as children?sshort stories and extend the model to generate mathword problems directly from equations.
Finally, weintend to incorporate the generated problems in ed-ucational technology and tutoring systems.AcknowledgmentsThis research was supported by the NSF (IIS1616112), Allen Institute for AI (66-9175),Allen Distinguished Investigator Award, DARPA(FA8750-13-2-0008) and a Google research facultyPoor Rewritess10.
It rained 0.9 inches on Monday.
On Tuesday, itrained 0.7 inches less than on Monday.
How much didit rain on Tuesday?s?10.
It blasted 0.9 inches on Monday.
On Tuesday, itblasted 0.7 inches less than on Monday.
How muchdid it light on Tuesday?s11.
The Ferris wheel in Paradise Park has 14 seats.Each seat can hold 6 people.
How many people canride the Ferris wheel at the same time?s?11.
The int grab in chewbacca mesa has 14 areas.Each area can hold 6 troops.
How many troops canride the int grab at the same time?Table 7: Examples of the original stories si and poorerrewrites s?i in the Star Wars theme.award.
We thank the anonymous reviewers for theirhelpful comments.ReferencesLeandro Motta Barros and Soraia Raupp Musse.
2007.Planning algorithms for interactive storytelling.
Com-puters in Entertainment (CIE), 5(1):4.Lora Bartlett.
2004.
Expanding teacher work roles: a re-source for retention or a recipe for overwork?
Journalof Education Policy, 19(5):565?582.Nathanael Chambers and Dan Jurafsky.
2009.
Unsuper-vised Learning of Narrative Schemas and Their Par-ticipants.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the AFNLP: Volume 2-Volume 2, pages 602?610.
Association for Computational Linguistics.David Chen and William B. Dolan.
2011.
Collectinghighly parallel data for paraphrase evaluation.
In Pro-ceedings of the Association for Computational Lin-guistics (ACL).Danqi Chen and Christopher Manning.
2014.
A fast andaccurate dependency parser using neural networks.In Proceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 740?750, Doha, Qatar, October.
Association forComputational Linguistics.The Racial Slur Database.
2016.
The racial slurdatabase.Judy Davis-Dorsey, Steven M Ross, and Gary R Morri-son.
1991.
The role of rewording and context per-sonalization in the solving of mathematical word prob-lems.
Journal of Educational Psychology, 83(1):61.Michael Denkowski and Alon Lavie.
2014.
Meteor Uni-versal: Language Specific Translation Evaluation for1625Any Target Language.
In Proceedings of the EACL2014 Workshop on Statistical Machine Translation.K.
Filippova and M. Strube.
2008.
Dependencytree based sentence compression.
In Proceedings ofthe Fifth International Natural Language GenerationConference (INLG)).Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 363?370, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.J.
Ganitkevitch, C. Callison-Burch, C. Napoles, andB.
Van Durme.
2011.
Learning sentential paraphrasesfrom bilingual parallel corpora for text-to-text genera-tion.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP).Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of the Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies (NAACL HLT), pages 758?764, Atlanta, Georgia,June.
Association for Computational Linguistics.Yoav Goldberg and Jon Orwant.
2013.
A Dataset ofsyntactic-Ngrams over Time from a Very Large Cor-pus of English Books.
In Second Joint Conference onLexical and Computational Semantics (* SEM), vol-ume 1, pages 241?247.Philip John Gorinski and Mirella Lapata.
2015.
Moviescript summarization as graph-based scene extraction.In Human Language Technologies: The 2015 AnnualConference of the North American Chapter of the ACL.Mark Granroth-Wilding and Stephen Clark.
2016.
Whathappens next?
event prediction using a compositionalneural network model.
In Proceedings of the 30thAAAI Conference on Artificial Intelligence (AAAI-16),Phoenix, Arizona.Janis M Hart.
1996.
The Effect of Personalized WordProblems.
Teaching Children Mathematics, 2(8):504?505.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, WMT?11, pages 187?197, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Mohammad Javad Hosseini, Hannaneh Hajishirzi, OrenEtzioni, and Nate Kushman.
2014.
Learning to SolveArithmetic Word Problems with Verb Categorization.In EMNLP, pages 523?533.Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin,and Wei-Ying Ma.
2016.
How well do computerssolve math word probl ems?
large-scale dataset con-struction and evaluation.
In Proceedings of the 2016North American Chapter of the ACL (NAACL HLT).William Hwang, Hannaneh Hajishirzi, Mari Ostendorf,and Wei Wu.
2015.
Aligning Sentences from StandardWikipedia to Simple Wikipedia.
In Proceedings of the2015 Conference of the North American Chapter of theAssociation for Computational Linguistics (NAACL).David Kauchak.
2013.
Improving text simplification lan-guage modeling using unsimplified text data.
In Pro-ceedings of the Conference of the Association for Com-putational Linguistics (ACL).Rik Koncel-Kedziorski, Hannaneh Hajishirzi, AshishSabharwal, Oren Etzioni, and Siena Ang.
2015a.Parsing Algebraic Word Problems into Equations.TACL, 3.Rik Koncel-Kedziorski, Hannaneh Hajishirzi, AshishSabharwal, Oren Etzioni, and Siena Ang.
2015b.Parsing algebraic word problems into equations.Transactions of the Association for ComputationalLinguistics, 3:585?597.Rik Koncel-Kedziorski, Subhro Roy, Aida Aimini, NateKushman, and Hannaneh Hajishirzi.
2016.
MAWPS:A Math Word Problem Repository.
In Proceedingsof the Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies (NAACL HLT).Nate Kushman, Yoav Artzi, Luke Zettlemoyer, andRegina Barzilay.
2014.
Learning to AutomaticallySolve Algebra Word Problems.
In ACL, pages 271?281.Benoit Lavoie and Owen Rambow.
1997.
A Fast andPortable Realizer for Text Generation Systems.
InProceedings of the fifth conference on Applied natu-ral language processing, pages 265?268.
Associationfor Computational Linguistics.Michael Lebowitz.
1987.
Planning Stories.
In Proceed-ings of the cognitive science society, Hillsdale, pages234?242.Omer Levy and Yoav Goldberg.
2014.
Dependency-Based Word Embeddings.
In ACL, pages 302?308.Boyang Li, Stephen Lee-Urban, George Johnston, andMark O. Riedl.
2013.
Story generation with crowd-sourced plot graphs.
In Proceedings of AAAI Confer-ece on Artificial Intelligence (AAAI).Hugo Liu and Push Singh.
2002.
MAKEBELIEVE: Us-ing Commonsense Knowledge to Generate Stories.
InAAAI/IAAI, pages 957?958.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Proceedings of the Conference ofthe Association for Computational Linguistics: SystemDemonstrations (ACL), pages 55?60.1626Neil McIntyre and Mirella Lapata.
2009.
Learning toTell Tales: A Data-driven Approach to Story Gener-ation.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the AFNLP: Volume 1-Volume 1, pages 217?225.
Association for Computational Linguistics.Neil McIntyre and Mirella Lapata.
2010.
Plot Inductionand Evolutionary Search for Story Generation.
In Pro-ceedings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 1562?1572.
As-sociation for Computational Linguistics.FrontGate Media.
2016.
Terms to block.James Richard Meehan.
1976.
The Metanovel: WritingStories by Computer.
Technical report, DTIC Docu-ment.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient Estimation of WordRepresentations in Vector Space.
arXiv preprintarXiv:1301.3781.George A Miller.
1995.
WordNet: A Lexical Databasefor English.
Communications of the ACM, 38(11):39?41.Arindam Mitra and Chitta Baral.
2015.
Learning to au-tomatically solve logic grid puzzles.
In EMNLP.Nasrin Mostafazadeh, Nathanael Chambers, XiaodongHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,Pushmeet Kohli, and James Allen.
2016.
A corpusand evaluation framework for deeper understandingof commonsense stories.
In Proceedings of the 2016North American Chapter of the ACL (NAACL HLT).Sarah Petersen and Mari Ostendorf.
2007.
Text simpli-fication for langauge learners: A corpus analysis.
InProceedings of the Speech and Language Technologyin Education Workshop (SLaTE).Karl Pichotta and Raymond J. Mooney.
2016.
Learn-ing statistical scripts with LSTM recurrent neural net-works.
In Proceedings of the 30th AAAI Conferenceon Artificial Intelligence (AAAI-16), Phoenix, Arizona.Oleksandr Polozov, Eleanor ORourke, Adam M Smith,Luke Zettlemoyer, Sumit Gulwani, and ZoranPopovic.
2015.
Personalized Mathematical WordProblem Generation.
In Proceedings of the 24th In-ternational Joint Conference on Artificial Intelligence(IJCAI 2015).
To appear.KA Renninger, L Ewen, and AK Lasher.
2002.
Indi-vidual interest as context in expository text and math-ematical word problems.
Learning and Instruction,12(4):467?490.Subhro Roy and Dan Roth.
2015.
Solving General Arith-metic Word Problems.
In EMNLP.Subhro Roy, Shyam Upadhyay, and Dan Roth.
2016.Equation parsing : Mapping sentences to groundedequations.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP).Alexander M Rush, Sumit Chopra, and Jason Weston.2015.
A neural attention model for abstractive sen-tence summarization.
Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP).Roger C Schank and Robert P Abelson.
1977.
Scripts,Plans, Goals, and Understanding: An Inquiry into Hu-man Knowledge Structures.
Hillsdale, NJ: LawrenceErlbaum.Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, andOren Etzioni.
2014.
Diagram Understanding in Ge-ometry Questions.
In AAAI.Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Et-zioni, and Clint Malcolm.
2015.
Solving GeometryProblems: Combining Text and Diagram Interpreta-tion.
In EMNLP.Shuming Shi, Yuehui Wang, Chin-Yew Lin, XiaojiangLiu, and Yong Rui.
2015.
Automatically Solv-ing Number Word Problems by Semantic Parsing andReasoning.
In EMNLP.Advaith Siddharthan.
2004.
Syntactic simplification andtext cohesion.
Research on Language and Computa-tion, 4(1):77?109.Springfield.
2016.
Adventure time with finn & jakeepisode scripts.Ladder Survivors.
2013.
Key words for math problems.Scott R Turner.
1993.
Minstrel: A Computer Model ofCreativity and Storytelling.Lucy Vanderwende, Hisami Suzuki, Chris Brockett, andAni Nenkova.
2007.
Beyond sumbasic: Task-focusedsummarization with sentence simplification and lexi-cal expansion.
Information Processing and Manage-ment.David Vickrey and Daphne Koller.
2008.
Sentence sim-plification for semantic role labeling.
In Proceedingsof the Conference of the Association for Computa-tional Linguistics (ACL), pages 344?352.Kristian Woodsend and Mirella Lapata.
2011a.
Learningto simplify sentences with quasi-synchronous gram-mar and integer programming.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP).Kristian Woodsend and Mirella Lapata.
2011b.
Wikisim-ple: Automatic simplification of wikipedia articles.
InProceedings of the Association for Advancement ofArtificial Intelligence Conference on Artificial Intelli-gence (AAAI), pages 927?932, San Francisco, CA.Sander Wubben, Antal Van Den Bosch, and Emiel Krah-mer.
2012.
Sentence simplification by monolingual1627machine translation.
In Proceedings of the Confer-ence of the Association for Computational Linguistics(ACL), pages 1015?1024.Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen,and Chris Callison-Burch.
2016.
Optimizing statisti-cal machine translation for text simplification.
Trans-actions of Association of Computational Linguistics.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of sim-plicity: Unsupervised extraction of lexical simplifica-tions from wikipedia.
In Proceedings of the Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies (NAACL HLT).Lipu Zhou, Shuaixiang Dai, and Liwei Chen.
2015.Learn to Solve Algebra Word Problems UsingQuadratic Programming.
In EMNLP.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of the In-ternational Conference on Computational Linguistics(COLING).1628
