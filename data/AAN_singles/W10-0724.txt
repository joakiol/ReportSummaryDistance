Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 159?162,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsEvaluation of Commonsense Knowledge with Mechanical TurkJonathan GordonDept.
of Computer ScienceUniversity of RochesterRochester, NY, USAjgordon@cs.rochester.eduBenjamin Van DurmeHLTCOEJohns Hopkins UniversityBaltimore, MD, USAvandurme@cs.jhu.eduLenhart K. SchubertDept.
of Computer ScienceUniversity of RochesterRochester, NY, USAschubert@cs.rochester.eduAbstractEfforts to automatically acquire world knowl-edge from text suffer from the lack of an easymeans of evaluating the resulting knowledge.We describe initial experiments using Mechan-ical Turk to crowdsource evaluation to non-experts for little cost, resulting in a collectionof factoids with associated quality judgements.We describe the method of acquiring usablejudgements from the public and the impactof such large-scale evaluation on the task ofknowledge acquisition.1 IntroductionThe creation of intelligent artifacts that can achievehuman-level performance at problems like question-answering ultimately depends on the availability ofconsiderable knowledge.
Specifically, what is neededis commonsense knowledge about the world in a formsuitable for reasoning.
Open knowledge extraction(Van Durme and Schubert, 2008) is the task of miningtext corpora to create useful, high-quality collectionsof such knowledge.Efforts to encode knowledge by hand, such as Cyc(Lenat, 1995), require expensive man-hours of laborby experts.
Indeed, results from Project Halo (Fried-land et al, 2004) suggest that properly encoding the(domain-specific) knowledge from just one page of atextbook can cost $10,000.
OKE, on the other hand,creates logical formulas automatically from existingstores of human knowledge, such as books, newspa-pers, and the Web.
And while crowdsourced efforts togather knowledge, such as Open Mind (Singh, 2002),learn factoids people come up with off the tops oftheir heads to contribute, OKE learns from what peo-ple normally write about and thus consider important.Open knowledge extraction differs from open infor-mation extraction (Banko et al, 2007) in the focuson everyday, commonsense knowledge rather thanspecific facts, and on the logical interpretability ofthe outputs.
While an OIE system might learn thatTolstoy wrote using a dip pen, an OKE system wouldprefer to learn that an author may write using a pen.An example of an OKE effort is the KNEXT sys-tem1 (Schubert, 2002), which uses compositional se-mantic interpretation rules to produce logical formu-las from the knowledge implicit in parsed text.
Theseformulas are then automatically expressed as English-like ?factoids?, such as ?A PHILOSOPHER MAY HAVEA CONVICTION?
or ?NEGOTIATIONS CAN BE LIKELYTO GO ON FOR SOME HOURS?.While it is expected that eventually sufficientlyclean knowledge bases will be produced for infer-ences to be made about everyday things and events,currently the average quality of automatically ac-quired knowledge is not good enough to be used intraditional reasoning systems.
An obstacle for knowl-edge extraction is the lack of an easy method forevaluating ?
and thus improving ?
the quality of re-sults.
Evaluation in acquisition systems is typicallydone by human judging of random samples of output,usually by the reporting authors themselves (e.g., Linand Pantel, 2002; Schubert and Tong, 2003; Banko etal., 2007).
This is time-consuming, and it has the po-tential for bias: it would be preferable to have peopleother than AI researchers label whether an output iscommonsense knowledge or not.
We explore the useof Amazon?s Mechanical Turk service, an online la-bor market, as a means of acquiring many non-expertjudgements for little cost.2 Related WorkWhile Open Mind Commons (Speer, 2007) asks usersto vote for or against commonsense statements con-tributed by others users in order to come to a consen-sus, we seek to evaluate an automatic system.
Snowet al (2008) compared the quality of labels producedby non-expert Turkers against those made by expertsfor a variety of NLP tasks and found that they re-quired only four responses per item to emulate expertannotations.
Kittur et al (2008) describe the use and1Public release of the basic KNEXT engine is forthcoming.159The statement above is a reasonably clear, entirelyplausible, generic claim and seems neither too spe-cific nor too general or vague to be useful:?
I agree.?
I lean towards agreement.?
I?m not sure.?
I lean towards disagreement.?
I disagree.Figure 1: Rating instructions and answers.necessity of verifiable questions in acquiring accurateratings of Wikipedia articles from Mechanical Turkusers.
These results contribute to our methods below.3 ExperimentsPrevious evaluations of KNEXT output have tried tojudge the relative quality of knowledge learned fromdifferent sources and by different techniques.
Herethe goal is simply to see whether the means of evalu-ation can be made to work reasonably, including atwhat scale it can be done for limited cost.
For theseexperiments, we relied on $100 in credit provided byAmazon as part of the workshop shared task.
Thisamount was used for several small experiments in or-der to empirically estimate what $100 could achieve,given a tuned method of presentation and evaluation.We took a random selection of factoids generatedfrom the British National Corpus (BNC Consortium,2001), split into sets of 20, and removed those mosteasily filtered out as probably being of low qualityor malformed.
We skipped the more stringent filters(originally created for dealing with noisy Web text),leaving more variety in the quality of the factoidsTurkers were asked to rate.The first evaluation followed the format of previ-ous, offline ratings.
For each factoid, Turkers weregiven the instructions and choices in Fig.
1, wherethe options correspond in our analysis to the num-bers 1?5, with 1 being agreement.
To help Turkersmake such judgements, they were given a brief back-ground statement: ?We?re gathering the sort of every-day, commonsense knowledge an intelligent computersystem should know.
You?re asked to rate several pos-sible statements based on how well you think theymeet this goal.?
Mason and Watts (2009) suggest thatwhile money may increase the number and speed ofresponses, other motivations such as wanting to helpwith something worthwhile or interesting are morelikely to lead to high-quality responses.Participants were then shown the examples andexplanations in Fig.
2.
Note that while they are toldsome categories that bad factoids can fall into, theTurkers are not asked to make such classificationsExamples of good statements:?
A SONG CAN BE POPULAR?
A PERSON MAY HAVE A HEAD?
MANEUVERS MAY BE HOLD -ED IN SECRETIt?s fine if verb conjugations are not attached or are a bitunnatural, e.g.
?hold -ed?
instead of ?held?.Examples of bad statements:?
A THING MAY SEEK A WAYThis is too vague.
What sort of thing?
A way for/towhat??
A COCKTAIL PARTY CAN BE ATSCOTCH_PLAINS_COUNTRY_CLUBThis is too specific.
We want to know that a cocktailparty can be at a country club, not at this particular one.The underscores are not a problem.?
A PIG MAY FLYThis is not literally true even though it happens to be anexpression.?
A WORD MAY MEANThis is missing information.
What might a word mean?Figure 2: The provided examples of good and bad factoids.themselves, as this is a task where even experts havelow agreement (Van Durme and Schubert, 2008).An initial experiment (Round 1) only requiredTurkers to have a high (90%) approval rate.
Underthese conditions, out of 100 HITs2, 60 were com-pleted by participants whose IP addresses indicatedthey were in India, 38 from the United States, and2 from Australia.
The average Pearson correlationbetween the ratings of different Indian Turkers an-swering the same questions was a very weak 0.065,and between the Indian responders and those fromthe US and Australia was 0.132.
On the other hand,the average correlation among non-Indian Turkerswas 0.508, which is close to the 0.6?0.8 range seenbetween the authors in the past, and which can betaken as an upper bound on agreement for the task.Given the sometimes subtle judgements of mean-ing required, being a native English speaker has pre-viously been assumed to be a prerequisite.
This differ-ence in raters?
agreements may thus be due to levelsof language understanding, or perhaps to differentlevels of attentiveness to the task.
However, it doesnot seem to be the case that the Indian respondentsrushed: They took a median time of 201.5 seconds(249.18 avg.
with a high standard deviation of 256.3s?
some took more than a minute per factoid).
The non-Indian responders took a median time of just 115.5 s(124.5 avg., 49.2 std dev.
).Regardless of the cause, given these results, we re-stricted the availability of all following experimentsto Turkers in the US.Ideally we would include otherEnglish-speaking countries, but there is no straight-2Human Intelligence Tasks ?
Mechanical Turk assignments.In this case, each HIT was a set of twenty factoids to be rated.160All High Corr.
(> 0.3)Round Avg.
Std.
Dev.
Avg.
Std.
Dev.1 (BNC) 2.59 1.55 2.71 1.643 (BNC) 2.80 1.66 2.83 1.684 (BNC) 2.61 1.64 2.62 1.645 (BNC) 2.76 1.61 2.89 1.686 (Weblogs) 2.83 1.67 2.85 1.677 (Wikipedia) 2.75 1.64 2.75 1.64Table 1: Average ratings for all responses and for highlycorrelated responses.
to other responses.
Lower numbersare more positive.
Round 2 was withdrawn without beingcompleted.forward way to set multiple allowable countries onMechanical Turk.When Round 2 was posted witha larger set of factoids to be rated and the locationrequirement, responses fell off sharply, leading usto abort and repost with a higher payrate (7?
for 20factoids vs 5?
originally) in Round 3.To avoid inaccurate ratings, we rejected submis-sions that were improbably quick or were stronglyuncorrelated with other Turkers?
responses.
We col-lected five Turkers?
ratings for each set of factoids,and for each persons?
response to a HIT computedthe average of their three highest correlations withothers?
responses.
We then rejected if the correla-tions were so low as to indicate random responses.The scores serve a second purpose of identifying amore trustworthy subset of the responses.
(A cut-offscore of 0.3 was chosen based on hand-examination.
)In Table 1, we can see that these more strongly corre-lated responses rate factoids as slightly worse overall,possibly because those who either casual or uncertainare more likely to judge favorably on the assumptionthat this is what the task authors would prefer, or theyare simply more likely to select the top-most option,which was ?I agree?.An example of a factoid that was labeled incor-rectly by one of the filtered out users is ?A PER-SON MAY LOOK AT SOME THING-REFERRED-TO OFPRESS RELEASES?, for which a Turker from Madrasin Round 1 selected ?I agree?.
Factoids containingthe vague ?THING-REFERRED-TO?
are often filteredout of our results automatically, but leaving them ingave us some obviously bad inputs for checking Turk-ers?
responses.
Another (US) Turker chose ?I agree?when told ?TES MAY HAVE 1991ES?
but ?I disagree?when shown ?A TRIP CAN BE TO A SUPERMARKET?.We are interested not only in whether there is a gen-eral consensus to be found among the Turkers but alsohow that consensus correlates with the judgementsof AI researchers.
To this end, one of the authorsrated five sets (100 factoids) presented in Round 3,050010001500200025000 1 2 3 4 5FrequencyRatingFigure 3: Frequency of ratings in the high-corr.
results ofRound 3.which yielded an average correlation between all theTurkers and the author of 0.507, which rises slightlyto 0.532 if we only count those Turkers considered?highly correlated?
as described above.As another test of agreement, for ten of the sets inRound 3, two factoids were designated as fixpoints ?the single best and worst factoid in the set, assignedratings 1 and 5 respectively.
From the Turkers whorated these factoids, 65 of the 100 ratings matchedthe researchers?
designations and 77 were within onepoint of the chosen rating.3A few of the Turkers who participated had fairlystrong negative correlations to the other Turkers, sug-gesting that they may have misunderstood the taskand were rating backwards.4 Furthermore, one Turkercommented that she was unsure whether the state-ment she was being asked to agree with (Fig.
1) ?wasa positive or negative?.
To see how it would affect theresults, we ran (as Round 4) twenty sets of factoids,asking simplified question ?Do you agree this is agood statement of general knowledge??
The choiceswere also reversed in order, running from ?I disagree?to ?I agree?
and color-coded, with agree being greenand disagree red.
This corresponded to the coloringof the good and bad examples at the top of the page,which the Turkers were told to reread when they werehalfway through the HIT.
The average correlation forresponses in Round 4 was 0.47, which is an improve-ment over the 0.34 avg.
correlation of Round 3.Using the same format as Round 4, we ran factoidsfrom two other corpora.
Round 6 consisted of 300 ran-dom factoids taken from running KNEXT on weblogdata (Gordon et al, 2009) and Round 7 300 randomfactoids taken from running KNEXT on Wikipedia.3If we only look at the highly correlated responses, this in-creases slightly to 68% exact match, 82% within one point.4This was true for one Turker who completed many HITs, aproblem that might be prevented by accepting/rejecting HITs assoon as all scores for that set of factoids were available ratherthan waiting for the entire experiment to finish.161The average ratings for factoids from these sourcesare lower than for the BNC, reflecting the noisy na-ture of much writing on weblogs and the many overlyspecific or esoteric factoids learned from Wikipedia.The results achieved can be quite sensitive to thedisplay of the task.
For instance, the frequency ofratings in Fig.
3 shows that Turkers tended towardthe extremes: ?I agree?
and ?I disagree?
but rarely?I?m not sure?.
This option might have a negativeconnotation (?Waffling is undesirable?)
that anotherphrasing would not.
As an alternative presentation ofthe task (Round 5), for 300 factoids, we asked Turk-ers to first decide whether a factoid was ?incoher-ent (not understandable)?
and, otherwise, whether itwas ?bad?, ?not very good?, ?so-so?, ?not so bad?, or?good?
commonsense knowledge.
Turkers indicatedfactoids were incoherent 14% of the time, with a cor-responding reduction in the number rated as ?bad?,but no real increase in middle ratings.
The averageratings for the ?coherent?
factoids are in Table 1.4 Uses of ResultsBeyond exploring the potential of Mechanical Turkas a mechanism for evaluating the output of KNEXTand other open knowledge extraction systems, theseexperiments have two useful outcomes:First, they give us a large collection of almost 3000factoids that have associated average ratings and al-low for the release of the subset of those factoidsthat are believed to probably be good (rated 1?2).This data set is being publicly released at http://www.cs.rochester.edu/research/knext, andit includes a wide range of factoids, such as ?A REP-RESENTATION MAY SHOW REALITY?
and ?DEMON-STRATIONS MAY MARK AN ANNIVERSARY OF ANUPRISING?.Second, the factoids rated from Round 2 onwardwere associated with the KNEXT extraction rules usedto generate them: The factoids generated by differentrules have average ratings from 1.6 to 4.8.
We hope infuture to use this data to improve KNEXT?s extractionmethods, improving or eliminating rules that oftenproduce factoids judged to be bad.
Inexpensive, fastevaluation of output on Mechanical Turk could be away to measure incremental improvements in outputquality coming from the same source.5 ConclusionsThese initial experiments have shown that untrainedTurkers evaluating the natural-language verbaliza-tions of an open knowledge extraction system willgenerally give ratings that correlate strongly withthose of AI researchers.
Some simple methods weredescribed to find those responses that are likely tobe accurate.
This work shows promise for cheap andquick means of measuring the quality of automati-cally constructed knowledge bases and thus improv-ing the tools that create them.AcknowledgementsThis work was supported by NSF grants IIS-0535105and IIS-0916599.ReferencesMichele Banko, Michael J. Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open infor-mation extraction from the Web.
In Proc.
of IJCAI-07.BNC Consortium.
2001.
The British National Corpus,v.2.
Dist.
by Oxford University Computing Services.Noah S. Friedland et al.
2004.
Project Halo: Towards adigital Aristotle.
AI Magazine, 25(4).Jonathan Gordon, Benjamin Van Durme, and Lenhart K.Schubert.
2009.
Weblogs as a source for extractinggeneral world knowledge.
In Proc.
of K-CAP-09.Aniket Kittur, Ed H. Chi, and Bongwon Suh.
2008.Crowdsourcing user studies with Mechanical Turk.
InProc.
of CHI ?08.Douglas B. Lenat.
1995.
Cyc: A Large-scale Investmentin Knowledge Infrastructure.
Communications of theACM, 38(11):33?48.Dekang Lin and Patrick Pantel.
2002.
Concept discoveryfrom text.
In Proc.
of COLING-02.Winter Mason and Duncan J. Watts.
2009.
Financialincentives and the ?performance of crowds?.
In Proc.of HCOMP ?09.Lenhart K. Schubert and Matthew H. Tong.
2003.
Extract-ing and evaluating general world knowledge from theBrown corpus.
In Proc.
of the HLT-NAACL Workshopon Text Meaning.Lenhart K. Schubert.
2002.
Can we derive general worldknowledge from texts?
In Proc.
of HLT-02.Push Singh.
2002.
The public acquisition of common-sense knowledge.
In Proc.
of AAAI Spring Sympo-sium on Acquiring (and Using) Linguistic (and World)Knowledge for Information Access.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast ?
but is it good?
InProc.
of EMNLP-08.Robert Speer.
2007.
Open mind commons: An inquisitiveapproach to learning common sense.
In Workshop onCommon Sense and Intelligent User Interfaces.Benjamin Van Durme and Lenhart K. Schubert.
2008.Open knowledge extraction through compositional lan-guage processing.
In Proc.
of STEP 2008.162
