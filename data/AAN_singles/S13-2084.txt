Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 508?512, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguisticssenti.ue-en: an approach for informally written short textsin SemEval-2013 Sentiment Analysis taskJose?
SaiasDI - ECT - Universidade de E?voraRua Roma?o Ramalho, 597000-671 E?vora, Portugaljsaias@uevora.ptHila?rio FernandesCortex IntelligenceRua Sebastia?o Mendes Bolas, 2 K7005-872 E?vora, Portugalhilario.fernandes@cortex-intelligence.comAbstractThis article describes a Sentiment Analysis(SA) system named senti.ue-en, builtfor participation in SemEval-2013 Task 2, aTwitter SA challenge.
In both challenge sub-tasks we used the same supervised machinelearning approach, including two classifiers inpipeline, with 22 semantic oriented features,such as polarized term presence and index,and negation presence.
Our system achieveda better score on Task A (0.7413) than in theTask B (0.4785).
In the first subtask, there isa better result for SMS than the obtained forthe more trained type of data, the tweets.1 IntroductionThis paper describes the participation of a groupled by Universidade de E?vora?s Computer ScienceDepartment in SemEval-2013 Task 2 (Wilson etal., 2013), using senti.ue-en system.
Havingprevious experience in NLP tasks, such as ques-tion answering (Saias, 2010; Saias and Quaresma,2012), this was the authors first attempt to imple-ment a system for Sentiment Analysis (SA) in En-glish language.
We have a recent work (Fernandes,2013) involving SA but it is geared towards Por-tuguese language, and thought for regular text.
Itwas based on rules on the outcome of linguistic anal-ysis, which did not work well for tweets, because themorphosyntactic analyzer misses much, due to theabundance of writing errors, symbols and abbrevia-tions.
Moreover, in that work we began by detectingnamed entities and afterwards classify the sentimentbeing expressed about them.
For SemEval the goalis different, being target-independent.
In both A andB subtasks, systems must work on sentiment polar-ity, in a certain context or full message, but the targetentity (or the opinion topic) will not appear in theoutput.
Thus, we have decided that senti.ue-ensystem would be implemented from scratch, for En-glish language and according to the objectives of thischallenge, in particular the Task B.2 Related WorkMicroblogging and social networks are platformswhere people express opinions.
In recent yearsmany papers have been published on social me-dia content SA.
Pang et al(2002) applied machinelearning based classifiers for sentiment classificationon movie reviews.
Their experimental results usingNaive Bayes, Maximum Entropy, and Support Vec-tor Machines (SVM) algorithms achieved best re-sults with SVM and unigram presence as features.Some target-dependent approaches are sensitive tothe entity that is receiving each sentiment.
A sen-tence can have a positive sentiment about an entityand a negative for another.
Such classification canbe performed with rules on the occurrence of nouns,verbs and adjectives, as done in (Nasukawa and Yi,2003).
It is common to use parsers and part-of-speech tagging.
Barbosa and Feng (2010) exploretweet writting details and meta-information in fea-ture selection.
Instead of using many unigrams asfeatures, the authors propose the use of 20 features(related to POS tags, emoticons, upper case usage,word polarity and negation), achieving faster train-ing and test times.
A two-phase approach first clas-508sifies messages as subjective and objective, and thenthe polarity is classified as positive or negative fortweets having subjectivity.
Groot (2012) builds afeature vector with polarized words and frequentlyoccurring words being taken as predictive for Twit-ter messages.
Supervised learning algorithms asSVM and Naive Bayes are then used to create a pre-diction model.
The work (Gebremeskel, 2011) is fo-cused on tweets about news.
Authors report an ac-curacy of 87.78% for a three-classed sentiment clas-sification using unigram+bigram presence featuresand Multinomial Naive Bayes classifier.
In Jiang etal.
(2011) work, Twitter SA starts with a query, iden-tifying a target, and classifies sentiment in the queryresult tweets, related to that target.
Instead of con-sidering only the text of a tweet, their context-awareapproach also considers related tweets and target-dependent features.
With precise criteria for the con-text of a tweet, authors seek to reduce ambiguity andreport performance gains.3 MethodologyAs in most systems described in the literature, inthis area, our senti.ue-en system is based on su-pervised machine learning.
To handle the data for-mat, in the input and on the outcome of the system,we chose to use Python and the Natural LanguageToolkit (NLTK), a platform with resources and pro-gramming libraries suitable for linguistic processing(Bird, 2006).
Task A asks us to classify the senti-ment in a word or phrase in the context of the mes-sage to which it belongs.
For Task B, we had toclassify the overall sentiment expressed in each mes-sage.
Since tweets are short messages, we early havechosen to apply the same system for both tasks, ad-mitting some possible difference in training or pa-rameterization.
As the fine control of the correspon-dence between each sentiment expression and its tar-get entity is not sought, Task A is treated as a spe-cial case of Task B, and our system does not con-sider the text around the expression to classify.
Theorganization prepared a message corpus for trainingand another to be used as a development-time eval-uation dataset.
We merged the training corpus withthe development corpus, and our development testset was dynamically formed by random selectionof instances for each class (positive, negative andneutral).
Some tweets were not downloaded prop-erly.
For message polarity classification, we endedup with 9191 labeled messages, which we split intotraining and test sets.Text processing started with tokenization, that waswhite space or punctuation based.
Some experi-ments also included lemmatization, done with theNLTK WordNet Lemmatizer.
In the first approachto Task B, we applied the Naive Bayes classificationalgorithm using term presence features.
The test setwas formed by random selection of 200 instancesof each class.
After several experiments with thissystem configuration, the average accuracy for the 3classes was close to 45%.
Looking for better results,instead of the bag-of-words approach, we chose asmaller set of semantic oriented features:?
presence of polarized term?
overall value of sentiment in text?
negation presence?
negation before polarized expression?
presence of polarized task A n-grams?
overall value of polarized task A n-grams?
overall and presence of similar to Task A n-grams?
first and last index of polarized termsChecking for the presence of positive and negativepolarized terms produces two features for each ofthe three sentiment lexicons used by our system.AFINN (Nielsen, 2011) is a sentiment lexicon con-taining a list of English words rated between minusfive (negative) and plus five (positive).
The wordshave been manually labeled by Finn A?rup Nielsen,from 2009 to 2011.
SentiWordNet (Baccianella etal., 2010) is a lexical resource for opinion miningthat assigns sentiment scores to each synset ofWord-Net (Princeton University, 2010).
After some exper-imentation with this resource, we decided to apply athreshold, disregarding terms whose score absolutevalue is less than 0.3.
Another sentiment lexicon,from Liu et al(2005), derived from a work on onlinecustomer reviews of products.
The overall text sen-timent value is calculated by adding the sentimentvalue in each word.
This is the way chosen to handlemore than one sentiment in a single tweet.
Our sys-tem creates a separated overall sentiment value fea-ture for AFINN, SentiWordNet and Liu?s lexicons,because each resource uses a different range of val-ues.
Each of these features is calculated by summingthe sentiment value in each word of the text clas-sify.
Detection of denial in the text also gave rise toa feature.
Thinking in cases like ?This meal was not509good?, we created features for the presence of denialbefore positive and negative expressions, where theadjective?s sentiment value is inverted by negation.In these two features, an expression is polarized ifit is included in any of the sentiment lexicons.
Thetraining corpus for Task A included words or phrasesmarked as positive or negative.
We created twomorefeatures to signal the presence of polarized wordsor n-grams in the texts to be classified.
To comple-ment, another feature accounts for the overall TaskA polarized n-grams value, adding 1 for each posi-tive occurrence and subtracting 1 every negative oc-currence in the tweet.
Because a term can arise ininflected form, we added another three features toassess the same on Task A data, but accepting varia-tions in words or expressions.
Using lemmatizationand synonyms, we seek more flexibility in n-gramverification.
The last four features identify the texttoken index for the first and the last occurrence, foreach sentiment flavor, positive and negative, accord-ing to any used sentiment lexicon.
Emoticons arepresent in sentiment lexicons, so it was not created aspecific feature for them.Using these 22 features with Naive Bayes, the aver-age overall accuracy was 60%.
When analyzed byclass, the lower accuracy happens on neutral class,near 50%.
Accuracy por positive class was 68%,and for negative it was 63%.
For the next iteration,the NLTK classifier was set up for Decision Tree al-gorithm.
After several runs, we noticed that whilethe overall accuracy remained identical, the poorestresults came now for the negative class, having 54%accuracy.
The run average accuracy for classes pos-itive and neutral, was respectively 59% and 64%.
Inthe latest evolution the system applies two classifiersin sequence.
Each tweet is first classified with NaiveBayes.
This creates a new feature for the secondclassifier, which is considered along with the previ-ous ones by the Decision Tree algorithm.
This con-figuration led us to the best overall accuracy in thedevelopment stage, with 62%, and was the versionapplied to Task B in constrained mode.The unconstrained mode allowed systems to use ad-ditional data for training.
The IMDB dataset (Maaset al 2011) contains movie reviews with their asso-ciated binary sentiment polarity labels.
We chose asubset of this corpus consisting of 500 positive and500 negative reviews with less than 350 characters.T Data Mode Positive Negative NeutralAsmsC 0.8079 0.8985 0.1130U 0.8695 0.9206 0.1348twitterC 0.9190 0.8162 0.0588U 0.9412 0.8411 0.0705BsmsC 0.4676 0.4356 0.7168U 0.4625 0.4161 0.7293twitterC 0.6264 0.3996 0.5538U 0.6036 0.3589 0.5621Table 1: senti.ue-en precision in Tasks A and BSanders used a Naive Bayes classifier and token-based feature extraction to create a corpus (Sanders,2011) for SA on Twitter.
We were able to dischargeonly part of the corpus, from which we selected250 positive tweets and the same number of neg-ative ones.
In unconstrained mode, senti.ue-enhas the same configuration, but uses extra instancesfrom these two corpus for training.Task A is treated with the same mechanism.
Thesystem classifies the sentiment for the text inside thegiven boundaries.
Because many of these cases havea single word, our system uses a third extra corpusfor training in unconstrained mode.
Each word onAFINN lexicon is added to training set, with pos-itive or negative class, depending on its sentimentvalue.4 ResultsWe submitted our system?s result for each of theeight expected runs.
Each run was a combinationof subtask (A or B), dataset (Twitter or SMS) andtraining mode (constrained or unconstrained).
Afterthe deadline for submission, the organization evalu-ated the results.
The precision in our system?s outputis indicated in Table 1.
The use of more training in-stances in unconstrained mode leads to an improve-ment of precision in Task A, for all classes.
In TaskB we notice the opposite effect, with a slight drop inprecision for positive and negative classes, and about1% improvement in neutral class precision.
We alsonote that precision has lower values in neutral classfor Task A, whereas in Task B it is the class negativethat has the lowest precision.Table 2 shows the recall obtained for the same re-sults.
This metric also shows a gain in Task A,for positive and negative classes using unconstrainedmode.
For subtask B, the constrained mode had bet-510T Data Mode Positive Negative NeutralAsmsC 0.5341 0.5453 0.6792U 0.6471 0.6196 0.6730twitterC 0.4898 0.4958 0.7500U 0.6203 0.5704 0.7000BsmsC 0.5711 0.3350 0.7061U 0.5386 0.4594 0.6556twitterC 0.5515 0.3245 0.6555U 0.5280 0.4359 0.5854Table 2: senti.ue-en recall in Tasks A and BT Data Mode Positive Negative NeutralAsmsC 0.6431 0.6787 0.1937U 0.7420 0.7407 0.2246twitterC 0.6390 0.6169 0.1090U 0.7478 0.6798 0.1281BsmsC 0.5142 0.3788 0.7114U 0.4977 0.4367 0.6905twitterC 0.5866 0.3581 0.6004U 0.5633 0.3937 0.5735Table 3: senti.ue-en F-measure in Tasks A and Bter recall for positive and neutral classes.
But recallvaries in the opposite direction in the negative classwhen using our extra training instances.Using the F-measure metric to evaluate our results,we get the values in Table 3.
This balanced assess-ment between precision and recall confirms the im-provement of results in Task A when using the un-constrained mode.
We note, for Task B, a small lossin unconstrained mode on positive class, but that isoutweighed by the gain on the negative class.In SemEval-2013 Task 2, the participating systemsare ranked by their score.
This corresponds to theaverage F-measure in positive and negative classes.Table 4 shows the score obtained by our system.
Thescore is in line with our forecasts in the Task A, butbelow what we wanted in Task B.
Looking at Table 3we see that positive and negative classes?
F-measurevalues are substantially lower than the values forneutral class, in Task B and in both constrained andunconstrained mode.
For Task B, most correct re-sults were in the class less relevant for the score.5 ConclusionsWith our participation in SemEval-2013 Task 2 weintended to build a real-time SA system for the En-glish used nowadays in social media content.
Thisgoal was achieved and we experienced the use of im-T Data Mode ScoreAsmsC 0.6609U 0.7413twitterC 0.6279U 0.7138BsmsC 0.4465U 0.4672twitterC 0.4724U 0.4785Table 4: senti.ue-en scoreportant English linguistic resources to support thistask, such as corpora and sentiment lexicons.We had some problems detected only after the closeof submission.
Lemmatization did not always workwell.
In ?last index of polarized term?
feature, wenoticed a problem that ironically came precisely atthe version used to submit, where the last indexwas counted from the start of text, and it should becounted from the end.We think that the difference in system performancebetween Task A and Task B has to do with theamount of noise present in the text.
Because manyof the texts to classify in Task A had a single wordor a short phrase, the system was more likely to suc-ceed.
Another reason is the fact that our system hasnot been tuned to maximize the score (F-measure inpositive and negative classes).
During developmentwe took into account only the overall accuracy seenin NLTK classifier result.
Perhaps the overall systemperformance may have been affected by our deci-sion of merge the training and the development cor-pus as training set.
We used a class balanced set fordevelopment-time evaluation, smaller than the givendevelopment set, and the final test set had a differentclass distribution (Wilson et al 2013).By reviewing the system, we feel that the classifica-tion algorithms in the pipeline system should swap.Now we would use first the Decision Tree classi-fier, and after, receiving an extra feature, the NaiveBayes classifier, which as mentioned in section 3,suggested slightly better results for positive and neg-ative classes.
For the future, we intend to evolve thesystem in order to become more precise and target-aware.
For the first part we need to review and evalu-ate the actual contribution of the current features.
Asfor the second, we intend to introduce named entityrecognition, so that each sentiment can be associatedwith its target entity.511ReferencesAndrew L. Maas, Raymond Daly, Peter Pham, DanHuang, Andrew Ng and Christopher Potts.
2011.Learning Word Vectors for Sentiment Analysis.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pp: 142-150.
ACL.
Portland, USA.Bing Liu, Minqing Hu and Junsheng Cheng.
2005.
Opin-ion Observer: Analyzing and Comparing Opinionson the Web.
In Proceedings of the 14th InternationalWorld Wide Web conference (WWW-2005).
Chiba,Japan.Bo Pang, Lillian Lee, Shivakumar Vaithyanathan.
2002.Thumbs up?
Sentiment Classification using MachineLearning Techniques.
In Proceedings of EMNLP.
pp:79?86.Finn A?rup Nielsen.
2011.
A New ANEW: Evalua-tion of a Word List for Sentiment Analysis in Mi-croblogs.
In Proceedings, 1st Workshop on Mak-ing Sense of Microposts (#MSM2011): Big thingscome in small packages.
pp: 93-98.
Greece.http://arxiv.org/abs/1103.2903Gebrekirstos Gebremeskel.
2011.
Sentiment Analysis ofTwitter Posts About news.
Master?s thesis.
Universityof Malta.Hila?rio Fernandes.
2013.
Sentiment Detection andClassification in Non Structured Information Sources.Master?s thesis, ECT - Universidade de E?vora.Jose?
Saias.
2010.
Contextualizac?a?o e Activac?a?oSema?ntica na Selecc?a?o de Resultados em Sistemasde Pergunta-Resposta.
PhD thesis, Universidade deE?vora.Jose?
Saias and Paulo Quaresma.
2012.
Di@ue inclef2012: question answering approach to the multi-ple choice qa4mre challenge.
In Proceedings of CLEF2012 Evaluation Labs and Workshop - Working NotesPapers, Rome, Italy.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and TiejunZhao.
2011.
Target-dependent twitter sentiment clas-sification.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies - Volume 1, HLT ?11,pages 151?160.
Association for Computational Lin-guistics.
USA.Luciano Barbosa and Junlan Feng.
2010.
Robust Sen-timent Detection on Twitter from Biased and NoisyData.
Coling 2010. pages 36?44.
Beijing.Niek J. Sanders.
2011.
Sanders-Twitter Sentiment Cor-pus.
Sanders Analytics LLCPrinceton University.
2010.
?About WordNet.?
WordNet.http://wordnet.princeton.eduRoy de Groot.
2012.
Data mining for tweet sentimentclassification.
Master?s thesis, Faculty of Science -Utrecht University.Stefano Baccianella, Andrea Esuli and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An Enhanced Lex-ical Resource for Sentiment Analysis and OpinionMining.
In Proceedings of the Seventh conferenceon International Language Resources and Evaluation- LREC?10.
European Language Resources Associa-tion.
Malta.Steven Bird.
2006.
NLTK: the natural language toolkit.In Proceedings of the COLING?06/ACL on Interactivepresentation sessions.
Australia.
http://nltk.orgTetsuya Nasukawa, Jeonghee Yi.
2003.
Sentiment analy-sis: capturing favorability using natural language pro-cessing.
In Proceedings of the 2nd International Con-ference on Knowledge Capture(K-CAP).
USA.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, AlanRitter, Sara Rosenthal and Veselin Stoyanov.
2013.SemEval-2013 task 2: Sentiment analysis in twitter.
InProceedings of the 7th International Workshop on Se-mantic Evaluation.
Association for Computation Lin-guistics.512
