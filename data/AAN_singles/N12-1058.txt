2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 523?527,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsCo-reference via Pointing and Haptics in Multi-Modal DialoguesLin Chen, Barbara Di EugenioDepartment of Computer ScienceUniversity of Illinois at Chicago851 S Morgan ST, Chicago, IL 60607, USA{lchen43,bdieugen}@uic.eduAbstractThis paper describes our ongoing work onresolving third person pronouns and deicticwords in a multi-modal corpus.
We show thatabout two thirds of these referring expressionshave antecedents that are introduced by point-ing gestures or by haptic-ostensive actions(actions that involve manipulating an object).After describing our annotation scheme, wediscuss the co-reference models we learn frommulti-modal features.
The usage of haptic-ostensive actions in a co-reference model is anovel contribution of our work.1 IntroductionCo-reference resolution has received a lot of atten-tion.
However, as Eisenstein and Davis (2006)noted, most research on co-reference resolution hasfocused on written text.
This task is much moredifficult in dialogue, especially in multi-modal di-alogue contexts.
First, utterances are informal, un-grammatical and disfluent.
Second, people sponta-neously use gestures and other body language.
Asnoticed by Kehler (2000), Goldin-Meadow (2003),and Chen et al (2011), in a multi-modal corpus,the antecedents of referring expressions are often in-troduced via gestures.
Whereas the role played bypointing gestures in referring has been studied, thesame is not true for other types of gestures.
In thispaper, alongside pointing gestures, we will discussthe role played by Haptic-Ostensive (H-O) actions,i.e., referring to an object by manipulating it in theworld (Landragin et al, 2002; Foster et al, 2008).As far as we know, no computational models of co-reference have been developed that include H-O ac-tions: (Landragin et al, 2002) focused on percep-tual salience and (Foster et al, 2008) on generationrather than interpretation.
We should point out thatat the time of writing we only focus on resolvingthird person pronouns and deictics.The rest of this paper is organized as follows.
InSection 2 we describe our multi-modal annotationscheme.
In Section 3 we present the pronoun/deicticresolution system.
In Section 4, we discuss experi-ments and results.2 The Data SetThe dataset we use in this paper is a subset of theELDERLY-AT-HOME corpus (Di Eugenio et al,2010), a multi-modal corpus in the domain of elderlycare.
It contains 20 human-human dialogues.
Ineach dialogue, a helper (HEL) and an elderly person(ELD) performed Activities of Daily Living (Krapp,2002), such as getting up from chairs, finding pots,cooking pastas, in a realistic setting, a studio apart-ment used for teaching and research.
The corpuscontains videos and voice data in avi format, hapticsdata collected via instrumented gloves in csv format,and the transcribed utterances in xml format.We focused on specific subdialogues in this cor-pus, that we call Find tasks: a Find task is a con-tinuous time span during which the two subjectswere collaborating on finding objects.
Find tasksarise naturally while helping perform ADLs such aspreparing dinner.
An excerpt from a Find taskis shown below, including annotations for pointinggestures and for H-O actions (annotations are per-523formed via the Anvil tool (Kipp, 2001)).ELD : Can you get me a pot?HEL: (opens cabinet, takes out pot, without saying a word)[Open(HEL,Cabinet1),Take-Out(HEL,Pot1)]ELD: Not that one, try over there.
[Point(ELD,Cabinet5)]Because the targets of pointing gestures and H-O actions are real life objects, we designed a refer-ring index system to annotate them.
The referringindex system consists of compile time indices andrun time indices.
We give pre-defined indices to tar-gets which cannot be moved, like cabinets, draw-ers, fridge.
We assign run time indices to targetswhich can be moved, and exist in multiple copies,like cups, glasses.
A referring index consists of atype and an index; the index increases according tothe order of appearance in the dialogue.
For exam-ple, ?Pot#1?
means the first pot referred to in thedialogue.
If a pointing gesture or H-O action in-volved multiple objects, we used JSON (JavaScriptObject Notation)1 Array to mark it.
For example,[C#1, C#2] means Cabinet#1 and Cabinet#2.We define a pointing gesture as a hand gesturewithout physical contact with the target, whereasgestures that involve physical contact with an ob-ject are haptic-obstensive (H-O).2 We use four tracksin Anvil to mark these gestures, two for pointinggestures, and two for H-O actions.
In each pair oftracks, one track is used for HEL, one for ELD.
Forboth types of gestures, we mark the start time, endtime and the target(s) of the gesture using the re-ferring index system we introduced above.
Addi-tionally we mark the type of an H-O action: Touch,Hold, Take-Out (as in taking out an object from acabinet or the fridge), Close, Open.3Our co-reference annotation follows an approachsimilar to (Eisenstein and Davis, 2006).
We markthe pronouns and deictics which need to be resolved,their antecedents, and the co-reference links be-tween them.
To mark pronouns, deictics and tex-tual antecedents, we use the shallow parser from1http://www.json.org/2Whereas not all haptic actions are ostensive, in our dia-logues they all potentially perform an ostensive function.3Our subjects occasionally hold objects together, e.g.
to filla pot with water: these actions are not included among the H-Oactions, and are annotated separately.Find Subtasks 142Length (Seconds) 5009Speech Turns 1746Words 8213Pointing Gestures 362H-O Actions 629Pronouns and Deictics 827Resolved Ref.
Expr.
757Textual Antecedent 218Pointing Gesture Antecedent 266H-O Antecedent 273Table 1: Annotation StatisticsApache OpenNLP Tools 4 to chunk the utterances ineach turn.
We use heuristics rules to automaticallymark potential textual antecedents and the phraseswe need to resolve.
Afterwards we use Anvil to editthe results of automatic processing.
To annotate co-reference links, we first assign each of the textualantecedents, the pointing gestures and H-O actionsa unique markable index.
Finally, we link referringexpressions to their closest antecedent (if applicable)using the markable indices.Table 1 shows corpus and annotation statistics.We annotated 142 Find subtasks, whose total lengthis about 1 hour and 24 minutes.
This sub-corpuscomprises 1746 spoken turns, which include 8213words.
10% of the 8213 words (827 words) are pro-nouns or deictics.
Note that for only 757/827 (92%)were the annotators able to determine an antecedent.Interestingly, 71% of those 757 pronouns or deicticsrefer to specific antecedents that are introduced ex-clusively by gestures, either pointing or H-O actions.In the earlier example, only the type for the referentof that in No, not that one had been introduced textu-ally, but not its specific antecedent pot1.
Clearly, tobe effective on such data any model of co-referencemust include the targets of pointing gestures and H-O actions.
Our current model does not take into ac-count the type provided by the de dicto interpretationof indefinites such as a pot above, but we intend toaddress this issue in future work.In order to verify the reliability of our annotations,we double coded 15% of the data for pointing ges-tures and H-O actions, namely the dialogues from3 pairs of subjects, or 22 Find subtasks.
We ob-4http://incubator.apache.org/opennlp/524tained reasonable ?
values: for pointing gestures,?=0.751, for H-O actions, ?=0.703, and for co-reference, ?=0.70.3 The Co-reference ModelIn this paper we focus on how to use gesture infor-mation (pointing or H-O) to solve the referring ex-pressions of interest.
Given a pronoun or deictic, webuild co-reference pairs by pairing it with the targetsof pointing gestures and H-O actions in a given timewindow.
We mark the correct pairs as ?True?
andthen we train a classification model to judge if a co-reference pair is a true pair.
The main componentof the resolution system is the co-reference classifi-cation model.
Since our antecedents are not textual,most of the traditional features for co-reference res-olution do not apply.
Rather, we use the followingmulti-modal features - U is the utterance containingthe pronoun / deictic to be solved:?
Time distance between the spans of U and ofthe pointing/H-O action.
If the two spans over-lap, the distance is 0.?
Speaker agreement: If the speaker of U and theactor of the pointing/H-O action are the same.?
Markable type agreement: If the markable typeof the pronoun/deictic and of the targets ofpointing gesture/H-O action are compatible.?
Number agreement: If the number of the pro-noun/deictic is the same as that of the targets ofthe pointing gesture/H-O action.?
Object agreement: If the deictic is containedin a phrase, such as ?this big blue bowl?,we will check if the additional object descrip-tion ?bowl?
matches the targets of pointinggesture/H-O action.?
H-O Action type: for co-reference pairs withantecedents from H-O actions.For markable type agreement, we defined twotypes of markables: PLC (place) and OBJ (object).PLC includes all the targets which cannot easilybe moved, OBJ includes all the targets like cups,pots.
We use heuristics rules to assign markabletypes to pronouns/deictics and the targets of point-ing gestures/H-O actions.
To determine the numberof the targets, we extract information from the an-notations; if the target is a JSON array, it means itis plural.
To extract additional object description forthe object agreement feature, we use the StanfordTyped Dependency parser (De Marneffe and Man-ning., 2008).
We check if the pronoun/deictic is in-volved in ?det?
and ?nsubj?
relations, if so, we ex-tract the ?gov?
element of that relation as the objectto compare with the target of gestures/H-O actions.4 Experiments and DiscussionsWe have experimented with 3 types of classificationmodels: Maximum Entropy (MaxEnt), DecisionTree and Support Vector Machine (SVM), respec-tively implemented via the following three pack-ages: MaxEnt, J48 from Weka (Hall et al, 2009),and LibSVM (Chang and Lin, 2011).
All of theresults reported below are calculated using 10 foldcross validation.We have run a series of experiments changing thehistory length from 0 to 10 seconds for generatingco-reference pairs (history changes in increments of1 second, hence, there are 11 sets of experiments).For each history length, we build the 3 models men-tioned above.
An additional baseline model treats aco-reference pair as ?True?
if speaker agreement istrue for the pair, and the time distance is 0.
Besidethe specified baseline, J48 can be seen as a more so-phisticated baseline as well.
When we ran the 10fold experiment with J48 algorithm, 5 out of 10 gen-erated decision trees only used 3 attributes.We use two metrics to measure the performanceof the models.
One are the standard precision, re-call and F-Score with respect to the generated co-reference pairs; the other is the number of pro-nouns and deictics that are correctly resolved.
Givena pronoun/deictic pi, if the classifier returns morethan one positive co-reference pair for pi, we use aheuristic resolver to choose the target.
We dividethose positive pairs into two subsets, those wherethe speaker of pi is the same as the performer of thegesture (SAME), and those with the other speaker(OTHER).
If SOME is not empty, we will chooseSOME, otherwise OTHER.
If the chosen set con-tains more than one pair, we will choose the target525Model Hist.
Prec.
Rec.
F.NumberResolvedBaseline 2 .707 .526 .603 359J48 1 .801 .534 .641 371SVM 2 .683 .598 .637 369MaxEnt 0 .738 .756 .747 374MaxEnt 2 .723 .671 .696 384Table 2: Gesture&Haptics Co-reference Model Resultsof the gesture/H-O action in the most recent pair.Given the space limit, Table 2 only shows theresults for each model which resolved most pro-nouns/deictics, and the model which produced thebest F-score.
In Table 2, with the change of Historywindow setting, the gold standard of co-referencepairs change.
When the history window is larger,there are more co-reference candidate pairs, whichhelp resolve more pronouns and deictics.Given we work on a new corpus, it is hard tocompare our results to previous work, additionallyour models currently do not deal with textual an-tecedents.
For example Strube and Mu?ller (2003)reports their best F-Measure as .4742, while oursis .747.
As concerns accuracy, whereas 384/827(46%) may appear low, note the task we are per-forming is harder since we are trying to solve all pro-nouns/deictics via gestures, not only the ones whichhave an antecedent introduced by a pointing or H-Oaction (see Table 1).
Even if our feature set is lim-ited, all the classification models perform better thanbaseline in all the experiments; the biggest improve-ment is 14.4% in F-score, and solving 25 more pro-nouns and deictics.
There are no significant differ-ences in the performances of the 3 different classifi-cation models.
Table 2 shows that the history lengthof the best models is less than or equal to 2 seconds,which is within the standard error range of annota-tions when we marked the time spans for events.5 ConclusionsThis paper introduced our multi-modal co-referenceannotation scheme that includes pointing gesturesand H-O actions in the corpus ELDERLY-AT-HOME.
Our data shows that 2/3 of antecedents ofpronouns/deictics are introduced by pointing ges-tures or H-O actions, and not in speech.
A co-reference resolution system has been built to resolvepronouns and deictics to the antecedents introducedby pointing gestures and H-O actions.
The classi-fication models show better performance than thebaseline model.
In the near future, we will integratea module which can resolve pronouns and deictics totextual antecedents, including type information pro-vided by indefinite descriptions.
This will make thesystem fully multi-modal.
Additionally we intendto study issues of timing.
Preliminary studies of ourcorpus show that the average distance between a pro-noun/deictic and its antecedent is 8.26?
for textualantecedents, but only 0.66?
for gesture antecedents,consistent with our results that show the best modelsinclude very short histories, at most 2?
long.AcknowledgmentsThis work is supported by award IIS 0905593 fromthe National Science Foundation.
Thanks to theother members of the RoboHelper project, espe-cially to Anruo Wang, for their many contributions,especially to the data collection effort.
Additionally,we thank the anonymous reviewers for their valuablecomments.ReferencesChih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.Lin Chen, Anruo Wang, and Barbara Di Eugenio.
2011.Improving pronominal and deictic co-reference resolu-tion with multi-modal features.
In Proceedings of theSIGDIAL 2011 Conference, pages 307?311, Portland,Oregon, June.
Association for Computational Linguis-tics.Marie-Catherine De Marneffe and Christopher D. Man-ning.
2008.
The stanford typed dependencies repre-sentation.
In Coling 2008: Proceedings of the work-shop on Cross-Framework and Cross-Domain ParserEvaluation, pages 1?8.
Association for ComputationalLinguistics.Barbara Di Eugenio, Milos?
Z?efran, Jezekiel Ben-Arie, Mark Foreman, Lin Chen, Simone Franzini,Shankaranand Jagadeesan, Maria Javaid, and KaiMa.
2010.
Towards Effective Communication withRobotic Assistants for the Elderly: Integrating Speech,Vision and Haptics.
In Dialog with Robots, AAAI 2010Fall Symposium, Arlington, VA, USA, November.526Jacob Eisenstein and Randall Davis.
2006.
GestureImproves Coreference Resolution.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers, pages 37?40.Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,Robin L. Hill, Jon Oberlander, and Alois Knoll.
2008.The roles of haptic-ostensive referring expressions incooperative, task-based human-robot dialogue.
InProceedings of the 3rd ACM/IEEE international con-ference on Human Robot Interaction, HRI ?08, pages295?302.
ACM.S.
Goldin-Meadow.
2003.
Hearing gesture: How ourhands help us think.
Harvard University Press.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Explorations, 11(1).Andrew Kehler.
2000.
Cognitive Status and Form ofReference in Multimodal Human-Computer Interac-tion.
In AAAI 00, The 15th Annual Conference of theAmerican Association for Artificial Intelligence, pages685?689.Michael Kipp.
2001.
Anvil-a generic annotation toolfor multimodal dialogue.
In Proceedings of the 7thEuropean Conference on Speech Communication andTechnology, pages 1367?1370.Kristine M. Krapp.
2002.
The Gale Encyclopedia ofNursing & Allied Health.
Gale Group, Inc. ChapterActivities of Daily Living Evaluation.F.
Landragin, N. Bellalem, and L. Romary.
2002.
Refer-ring to objects with spoken and haptic modalities.
InProceedings of the Fourth IEEE International Confer-ence on Multimodal Interfaces (ICMI?02), pages 99?104, Pittsburgh, PA.Michael Strube and Christoph Mu?ller.
2003.
A machinelearning approach to pronoun resolution in spoken di-alogue.
In Proceedings of the 41st Annual Meeting onAssociation for Computational Linguistics-Volume 1.527
