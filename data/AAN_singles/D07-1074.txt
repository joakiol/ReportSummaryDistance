Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
708?716, Prague, June 2007. c?2007 Association for Computational LinguisticsLarge-Scale Named Entity DisambiguationBased on Wikipedia DataSilviu CucerzanMicrosoft ResearchOne Microsoft Way, Redmond, WA 98052, USAsilviu@microsoft.comAbstractThis paper presents a large-scale system for therecognition and semantic disambiguation ofnamed entities based on information extractedfrom a large encyclopedic collection and Websearch results.
It describes in detail the disam-biguation paradigm employed and the informationextraction process from Wikipedia.
Through aprocess of maximizing the agreement between thecontextual information extracted from Wikipediaand the context of a document, as well as theagreement among the category tags associatedwith the candidate entities, the implemented sys-tem shows high disambiguation accuracy on bothnews stories and Wikipedia articles.1 Introduction and Related WorkThe ability to identify the named entities (such aspeople and locations) has been established as animportant task in several areas, including topic de-tection and tracking, machine translation, and in-formation retrieval.
Its goal is the identification ofmentions of entities in text (also referred to as sur-face forms henceforth), and their labeling with oneof several entity type labels.
Note that an entity(such as George W. Bush, the current president ofthe U.S.) can be referred to by multiple surfaceforms (e.g., ?George Bush?
and ?Bush?)
and a sur-face form (e.g., ?Bush?)
can refer to multiple enti-ties (e.g., two U.S. presidents, the football playerReggie Bush, and the rock band called Bush).When it was introduced, in the 6th Message Un-derstanding Conference (Grishman and Sundheim,1996), the named entity recognition task comprisedthree entity identification and labeling subtasks:ENAMEX (proper names and acronyms designat-ing persons, locations, and organizations), TIMEX(absolute temporal terms) and NUMEX (numericexpressions, monetary expressions, and percent-ages).
Since 1995, other similar named entity rec-ognition tasks have been defined, among whichCoNLL (e.g., Tjong Kim Sang and De Meulder,2003) and ACE (Doddington et al, 2004).
In addi-tion to structural disambiguation (e.g., does ?theAlliance for Democracy in Mali?
mention one,two, or three entities?)
and entity labeling (e.g.,does ?Washington went ahead?
mention a person,a place, or an organization?
), MUC and ACE alsoincluded a within document coreference task, ofgrouping all the mentions of an entity in a docu-ment together (Hirschman and Chinchor, 1997).When breaking the document boundary and scal-ing entity tracking to a large document collectionor the Web, resolving semantic ambiguity becomesof central importance, as many surface forms turnout to be ambiguous.
For example, the surfaceform ?Texas?
is used to refer to more than twentydifferent named entities in Wikipedia.
In the con-text ?former Texas quarterback James Street?,Texas refers to the University of Texas at Austin;in the context ?in 2000, Texas released a greatesthits album?, Texas refers to the British pop band;in the context ?Texas borders Oklahoma on thenorth?, it refers to the U.S. state; while in the con-text ?the characters in Texas include both real andfictional explorers?, the same surface form refersto the novel written by James A. Michener.Bagga and Baldwin (1998) tackled the problemof cross-document coreference by comparing, forany pair of entities in two documents, the wordvectors built from all the sentences containingmentions of the targeted entities.
Ravin and Kazi(1999) further refined the method of solving co-reference through measuring context similarity andintegrated it into Nominator (Wacholder et al,1997), which was one of the first successful sys-tems for named entity recognition and co-referenceresolution.
However, both studies targeted the clus-tering of all mentions of an entity across a givendocument collection rather than the mapping ofthese mentions to a given reference list of entities.A body of work that did employ reference entitylists targeted the resolution of geographic names in708text.
Woodruff and Plaunt (1994) used a list of 80kgeographic entities and achieved a disambiguationprecision of 75%.
Kanada (1999) employed a listof 96k entities and reported 96% precision for geo-graphic name disambiguation in Japanese text.Smith and Crane (2002) used the Cruchley?s andthe Getty thesauri, in conjunction with heuristicsinspired from the Nominator work, and obtainedbetween 74% and 93% precision at recall levels of89-99% on five different history text corpora.Overell and R?ger (2006) also employed the Gettythesaurus as reference and used Wikipedia to developa co-occurrence model and to test their system.In many respects, the problem of resolving am-biguous surface forms based on a reference list ofentities is similar to the lexical sample task in wordsense disambiguation (WSD).
This task, which hassupported large-scale evaluations ?
SENSEVAL 1-3(Kilgarriff and Rosenzweig, 2000; Edmonds andCotton, 2001; Mihalcea et al, 2004) ?
aims to as-sign dictionary meanings to all the instances of apredetermined set of polysemous words in a corpus(for example, choose whether the word ?church?refers to a building or an institution in a given con-text).
However, these evaluations did not includeproper noun disambiguation and omitted namedentity meanings from the targeted semantic labelsand the development and test contexts (e.g.,?Church and Gale showed that the frequency [..]?
).The problem of resolving ambiguous names alsoarises naturally in Web search.
For queries such as?Jim Clark?
or ?Michael Jordan?, search enginesreturn blended sets of results referring to manydifferent people.
Mann and Yarowsky (2003) ad-dressed the task of clustering the Web search re-sults for a set of ambiguous personal names byemploying a rich feature space of biographic factsobtained via bootstrapped extraction patterns.
Theyreported 88% precision and 73% recall in a three-wayclassification (most common, secondary, and other uses).Raghavan et al (2004) explored the use of entitylanguage models for tasks such as clustering enti-ties by profession and classifying politicians asliberal or conservative.
To build the models, theyrecognized the named entities in the TREC-8 cor-pus and computed the probability distributionsover words occurring within a certain distance ofany instance labeled as Person of the canonicalsurface form of 162 famous people.Our aim has been to build a named entity recog-nition and disambiguation system that employs acomprehensive list of entities and a vast amount ofworld knowledge.
Thus, we turned our attention tothe Wikipedia collection, the largest organizedknowledge repository on the Web (Remy, 2002).Wikipedia was successfully employed previouslyby Strube and Ponzetto (2006) and Gabrilovich andMarkovitch (2007) to devise methods for computingsemantic relatedness of documents, WikiRelate!and Explicit Semantic Analysis (ESA), respec-tively.
For any pair of words, WikiRelate!
attemptsto find a pair of articles with titles that containthose words and then computes their relatednessfrom the word-based similarity of the articles andthe distance between the articles?
categories in theWikipedia category tree.
ESA works by first build-ing an inverted index from words to all Wikipediaarticles that contain them.
Then, it estimates a re-latedness score for any two documents by using theinverted index to build a vector over Wikipediaarticles for each document and by computing thecosine similarity between the two vectors.The most similar work to date was published byBunescu and Paca (2006).
They employed severalof the disambiguation resources discussed in thispaper (Wikipedia entity pages, redirection pages,categories, and hyperlinks) and built a context-article cosine similarity model and an SVM basedon a taxonomy kernel.
They evaluated their modelsfor person name disambiguation over 110, 540,and 2,847 categories, reporting accuracies between55.4% and 84.8% on (55-word context, entity)pairs extracted from Wikipedia, depending on themodel and the development/test data employed.The system discussed in this paper performs bothnamed entity identification and disambiguation.The entity identification and in-document corefer-ence components resemble the Nominator system(Wacholder et al, 1997).
However, while Nomina-tor made heavy use of heuristics and lexical cluesto solve the structural ambiguity of entity men-tions, we employ statistics extracted from Wikipe-dia and Web search results.
The disambiguationcomponent, which constitutes the main focus of thepaper, employs a vast amount of contextual andcategory information automatically extracted fromWikipedia over a space of 1.4 million distinct enti-ties/concepts, making extensive use of the highlyinterlinked structure of this collection.
We aug-ment the Wikipedia category information with in-formation automatically extracted from Wikipedialist pages and use it in conjunction with the contextinformation in a vectorial model that employs anovel disambiguation method.7092 The Disambiguation ParadigmWe present in this section an overview of the pro-posed disambiguation model and the world knowl-edge data employed in the instantiation of themodel discussed in this paper.
The formal model isdiscussed in detailed in Section 5.The world knowledge used includes the knownentities (most articles in Wikipedia are associatedto an entity/concept), their entity class when avail-able (Person, Location, Organization, and Miscel-laneous), their known surface forms (terms that areused to mention the entities in text), contextualevidence (words or other entities that describe orco-occur with an entity), and category tags (whichdescribe topics to which an entity belongs to).For example, Figure 1 shows nine of the over 70different entities that are referred to as ?Columbia?in Wikipedia and some of the category and contex-tual information associated with one of these enti-ties, the Space Shuttle Columbia.The disambiguation process uses the data associ-ated with the known surface forms identified in adocument and all their possible entity disambigua-tions to maximize the agreement between the con-text data stored for the candidate entities and thecontextual information in the document, and also,the agreement among the category tags of the can-didate entities.
For example, a document that con-tains the surface forms ?Columbia?
and?Discovery?
is likely to refer to the Space ShuttleColumbia and the Space Shuttle Discovery becausethese candidate entities share the category tagsLIST_astronomical_topics, CAT_Manned_space-craft, CAT_Space_Shuttles (the extraction of suchtags is presented in Section 3.2), while other entitydisambiguations, such as Columbia Pictures andSpace Shuttle Discovery, do not share any com-mon category tags.
The agreement maximizationprocess is discussed in depth in Section 5.This process is based on the assumption thattypically, all instances of a surface form in adocument have the same meaning.
Nonetheless,there are a non-negligible number of cases inwhich the one sense per discourse assumption(Gale et al, 1992) does not hold.
To address thisproblem, we employ an iterative approach, ofshrinking the context size used to disambiguatesurface forms for which there is no dominatingentity disambiguation at document level, perform-ing the disambiguation at the paragraph level andthen at the sentence level if necessary.Figure 1.
The model of storing the information ex-tracted from Wikipedia into two databases.3 Information Extraction from WikipediaWe discuss now the extraction of entities and thethree main types of disambiguation clues (entitysurface forms, category tags, and contexts) used bythe implemented system.
While this informationextraction was performed on the English version ofthe Wikipedia collection, versions in other lan-guages or other collections, such as Encarta orWebMD, could be targeted in a similar manner.When processing the Wikipedia collection, wedistinguish among four types of articles: entitypages, redirecting pages, disambiguation pages,and list pages.
The characteristics of these articlesand the processing applied to each type to extractthe three sets of clues employed by the disam-biguation model are discussed in the next threesubsections.3.1 Surface Form to Entity MappingsThere are four sources that we use to extract entitysurface forms: the titles of entity pages, the titles ofredirecting pages, the disambiguation pages, andthe references to entity pages in other Wikipediaarticles.
An entity page is an article that containsinformation focused on one single entity, such as aperson, a place, or a work of art.
For example,Wikipedia contains a page titled ?Texas (TV se-ries)?, which offers information about the soapopera that aired on NBC from 1980 until 1982.
Aredirecting page typically contains only a refer-ence to an entity page.
For example, the articletitled ?Another World in Texas?
contains a redirec-ColumbiaColombiaColumbia UniversityColumbia RiverColumbia PicturesColumbia BicyclesSpace Shuttle ColumbiaUSS ColumbiaColumbia, MarylandColumbia, California...Space Shuttle ColumbiaTags:Manned spacecraftSpace program fatalitiesSpace ShuttlesContexts:NASAKennedy Space CenterEileen CollinsEntities Surface Forms710tion to the article titled ?Texas (TV series)?.
Fromthese two articles, we extract the entity Texas (TVseries) and its surface forms Texas (TV series),Texas and Another World in Texas.
As shown inthis example, we store not only the exact articletitles but also the corresponding forms from whichwe eliminate appositives (either within parenthesesor following a comma).We also extract surface form to entity mappingsfrom Wikipedia disambiguation pages, which arespecially marked articles having as title a surfaceform, typically followed by the word ?disambigua-tion?
(e.g., ?Texas (disambiguation)?
), and con-taining a list of references to pages for entities thatare typically mentioned using that surface form.Additionally, we extract all the surface formsused at least in two articles to refer to a Wikipediaentity page.
Illustratively, the article for Pam Longcontains the following Wikitext, which uses thesurface form ?Texas?
to refer to Texas (TV series):After graduation, she went to [[New York City]] andplayed Ashley Linden on [[Texas (TV series)|Texas]]from [[1981]] to [[1982]].In Wikitext, the references to other Wikipedia ar-ticles are within pairs of double square brackets.
Ifa reference contains a vertical bar then the text atthe left of the bar is the name of the referred article(e.g.
?Texas (TV Series)?
), while the text at theright of the bar (e.g., ?Texas?)
is the surface formthat is displayed (also referred to as the anchor textof the link).
Otherwise, the surface form shown inthe text is identical to the title of the Wikipediaarticle referred (e.g., ?New York City?
).Using these four sources, we extracted more than1.4 million entities, with an average of 2.4 surfaceforms per entity.
We obtained 377k entities withone surface form, 166k entities with two surfaceforms, and 79k entities with three surface forms.At the other extreme, we extracted one entity withno less than 99 surface forms.3.2 Category InformationAll articles that are titled ?List of [?]?
or ?Tableof [?]?
are treated separately as list pages.
Theywere built by Wikipedia contributors to group enti-ties of the same type together (e.g., ?List of an-thropologists?, ?List of animated television series?,etc.)
and are used by our system to extract categorytags for the entities listed in these articles.
The tagsare named after the title of the Wikipedia list page.For example, from the article ?List of band nameetymologies?, the system extracts the category tagLIST_band_name_etymologies and labels all theentities referenced in the list, including Texas(band), with this tag.
This process resulted in theextraction of more than 1 million (entity, tag) pairs.After a post-processing phase that discards tempo-ral tags, as well as several types of non-useful tagssuch as ?people by name?
and ?places by name?,we obtained a filtered list of 540 thousand pairs.We also exploit the fact that Wikipedia enablescontributors to assign categories to each article,which are defined as ?major topics that are likelyto be useful to someone reading the article?.
Be-cause any Wikipedia contributor can add a cate-gory to any article and the work of filtering outbogus assignments is tedious, these categoriesseem to be noisier than the lists, but they can stillprovide a tremendous amount of information.
Weextracted the categories of each entity page andassigned them as tags to the corresponding entity.Again, we employed some basic filtering to discardmeta-categories (e.g., ?Articles with unsourcedstatements?)
and categories not useful for the proc-ess of disambiguation through tag agreement (e.g.,?Living people?, ?1929 births?).
This extractionprocess resulted in 2.65 million (entity, tag) pairsover a space of 139,029 category tags.We also attempted to extract category tags basedon lexicosyntactic patterns, more specifically fromenumerations of entities.
For example, the para-graph titled ?Music of Scotland?
(shown below inWikitext) in the Wikipedia article on Scotland con-tains an enumeration of entities, which can be la-beled ENUM_Scotland_PAR_Music_of_Scotland:Modern Scottish [[pop music]] has produced manyinternational bands including the [[Bay City Rollers]],[[Primal Scream]], [[Simple Minds]], [[The Proclaim-ers]], [[Deacon Blue]], [[Texas (band)|Texas]], [[FranzFerdinand]], [[Belle and Sebastian]], and [[Travis(band)|Travis]], as well as individual artists such as[[Gerry Rafferty]], [[Lulu]], [[Annie Lennox]] and [[LloydCole]], and world-famous Gaelic groups such as[[Runrig]] and [[Capercaillie (band)|Capercaillie]].Lexicosyntactic patterns have been employedsuccessfully in the past (e.g., Hearst, 1992; Roarkand Charniak, 1998; Cederberg and Widdows,2003), and this type of tag extraction is still apromising direction for the future.
However, thebrute force approach we tried ?
of indiscriminatelytagging the entities of enumerations of four ormore entities ?
was found to introduce a largeamount of noise into the system in our develop-ment experiments.7113.3 ContextsTo extract contextual clues for an entity, we usethe information present in that entity?s page and inthe other articles that explicitly refer to that entity.First, the appositives in the titles of entity pages,which are eliminated to derive entity surface forms(as discussed in Section 3.1) are saved as contex-tual clues.
For example, ?TV series?
becomes acontext for the entity Texas (TV series).We then extract all the entity references in theentity page.
For example, from the article on Texas(band), for which a snippet in Wikitext is shownbelow, we extract as contexts the references popmusic, Glasgow, Scotland, and so on:'''Texas''' is a [[pop music]] band from [[Glasgow]],[[Scotland]], [[United Kingdom]].
They were foundedby [[Johnny McElhone]] in [[1986 in music|1986]] andhad their performing debut in [[March]] [[1988]] at [?
]Reciprocally, we also extract from the same ar-ticle that the entity Texas (band) is a good contextfor pop music, Glasgow, Scotland, etc.The number of contexts extracted in this manneris overwhelming and had to be reduced to a man-ageable size.
In our development experiments, weexplored various ways of reducing the context in-formation, for example, by extracting only entitieswith a certain number of mentions in an article, orby discarding mentions with low TF*IDF scores(Salton, 1989).
In the end, we chose a strategy inwhich we employ as contexts for an entity twocategory of references: those mentioned in the firstparagraph of the targeted entity page, and those forwhich the corresponding pages refer back to thetargeted entity.
For example, Pam Long and Texas(TV series) are extracted as relevant contexts foreach other because their corresponding Wikipediaarticles reference one another ?
a relevant snippetfrom the Pam Long article is cited in Section 3.1and a snippet from the article for Texas (TV se-ries) that references Pam Long is shown below:In 1982 [[Gail Kobe]] became executive producer and[[Pam Long]] became headwriter.In this manner, we extracted approximately 38million (entity, context) pairs.4 Document AnalysisIn this section, we describe concisely the main textprocessing and entity identification components ofthe implemented system.
We will then focus on thenovel entity disambiguation component, which wepropose and evaluate in this paper, in Section 5.Figure 2.
An overview of the processes employed bythe proposed system.Figure 2 outlines the processes and the resourcesthat are employed by the implemented system inthe analysis of text documents.
First, the systemsplits a document into sentences and truecases thebeginning of each sentence, hypothesizing whetherthe first word is part of an entity or it is capitalizedbecause of orthographic conventions.
It also identi-fies titles and hypothesizes the correct case for allwords in the titles.
This is done based on statisticsextracted from a one-billion-word corpus, withback-off to Web statistics.In a second stage, a hybrid named-entity recog-nizer based on capitalization rules, Web statistics,and statistics extracted from the CoNLL 2003shared task data (Tjong Kim Sang and DeMeulder, 2003) identifies the  boundaries of  theentity  mentions in the text and assigns each set ofmentions sharing the same surface form a probabil-ity distribution over four labels: Person, Location,Organization, and Miscellaneous.1 The named en-tity recognition component resolves the structuralambiguity with regard to conjunctions (e.g., ?Bar-nes and Noble?, ?Lewis and Clark?
), possessives(e.g., ?Alice's Adventures in Wonderland?, ?Brit-ain's Tony Blair?
), and prepositional attachment(e.g., ?Whitney Museum of American Art?,?Whitney Museum in New York?)
by using thesurface form information extracted from Wikipe-dia, when available, with back-off to co-occurrencecounts on the Web, in a similar way to Lapata andKeller (2004).
Recursively, for each ambiguousterm T0 of the form T1 Particle T2, where Particleis one of a possessive pronoun, a coordinative con-junction, and a preposition, optionally followed bya determiner, and the terms T1 and T2 are se-1While the named entity labels are used only to solve in-document coreferences by the current system, as describedfurther in this section, preliminary experiments of probabilisti-cally labeling the Wikipedia pages show that the these labelscould also be used successfully in the disambiguation process.Truecaser andSentenceBreakerEntityRecognizerEntityDisambiguator	!
"#$%712quences of capitalized words and particles, wesend to a search engine the query ??T1?
?T2?
?,which forces the engine to return only documentsin which the whole terms T1 and T2 appear.
Wethen count the number of times the snippets of thetop N = 200 search results returned contain the termT0 and compare it with an empirically obtainedthreshold to hypothesize whether T0 is the mentionof one entity or encompasses the mentions of twoentities, T1 and T2.As Wacholder et al (1997) noted, it is fairlycommon for one of the mentions of an entity in adocument to be a long, typical surface form of thatentity (e.g., ?George W.
Bush?
), while the othermentions are shorter surface forms (e.g., ?Bush?
).Therefore, before attempting to solve the semanticambiguity, the system hypothesizes in-documentcoreferences and maps short surface forms tolonger surface forms with the same dominant label(for example, ?Brown?/PERSON can be mapped to?Michael Brown?/PERSON).
Acronyms are also re-solved in a similar manner when possible.In the third stage, the contextual and category in-formation extracted from Wikipedia is used to dis-ambiguate the entities in the text.
This stage isdiscussed formally in Section 5 and evaluated inSection 6.
Note that the performance of the disam-biguation component is meaningful only whenmost named entity mentions are accurately identi-fied in text.
Thus, we first measured the perform-ance of the named entity recognition component onthe CoNLL 2003 test set and obtained a competi-tive F-measure of 0.835 (82.2% precision  and84.8% recall).Finally, the implemented system creates hyper-links to the appropriate pages in Wikipedia.
Figure3 shows the output of the implemented system on asample news story, in which the identified and dis-ambiguated surface forms are hyperlinked toWikipedia articles.5 The Disambiguation ComponentThe disambiguation process employs a vectorspace model, in which a vectorial representation ofthe processed document is compared with the vec-torial representations of the Wikipedia entities.Once the named entity surface forms were identi-fied and the in-document coreferences hypothe-sized, the system retrieves all possible entitydisambiguations of each surface form.
TheirWikipedia contexts that occur in the document andtheir category tags are aggregated into a documentvector, which is subsequently compared with theWikipedia entity vector (of categories and con-texts) of each possible entity disambiguation.
Wethen choose the assignment of entities to surfaceforms that maximizes the similarity between thedocument vector and the entity vectors, as we ex-plain further.Formally, let  = {c1,?,cM} be the set of knowncontexts from Wikipedia and  = {t1,?,tN} the setof known category tags.
An entity e can then berepresented as a vector ?e?
{0,1}M+N, with twocomponents, ?e|?
{0,1}M and ?e|?
{0,1}N, corre-sponding to the context and category information,respectively:1, if ci is a context for entity e?ei =    { 0, otherwise1, if tj is a category tag for e?eM+j ={ 0, otherwise.Figure 3.
Screenshot of the implemented system showing an example of analyzed text.
The superimposed tooltipsshow how several of the surface forms were disambiguated based on the context and category agreement method.713Let ?
(s) denote the set of entities that are knownto have a surface form s. For example, recallingFigure 1, Colombia (the country) and ColumbiaUniversity are entities that are known to have thesurface form ?Columbia?.
Let D be the analyzeddocument and S(D) = {s1,?,sn} the set of surfaceforms identified in D. We build its context vectord = {d1,?,dM}?M, where di is the number of oc-currences of context ci in D. To account for allpossible disambiguations of the surface forms in D,we also build an extended vector ?d M+N so thatdd C =|  and  ?
?=)( )(||DSs seTeTd??
.
2Our goal is to find the assignment of entities tosurface forms si ei, i?1..n, that maximizes theagreement between ?ei| and d, as well as theagreement between the categories of any two enti-ties Tei?
and Te j?
.
This can be written as:= ==???
?><+><ninjTeTeniCeeeijjiinssnd1 11),..,(,,maxarg)(..)1(1????
?, (1)where >?
?< ,  denotes the scalar product of vectors.Note that the quality of an assignment of an entityto a surface form depends on all the other assign-ments made, which makes this a difficult optimiza-tion problem.
An arguably more robust strategy toaccount for category agreement, which also provesto be computationally efficient, is to maximize theagreement between the categories of the assignedentity to each surface form and all possible disam-biguations of the other surface forms in D. We willshow that this is equivalent to computing:=???>?<niTeesseeiinnd1)(..)(),..,(,maxarg11????
(2)Indeed, using the definition of d and partitioningthe context and category components, we can re-write the sum in equation (2) as( )(q.e.d.)
,,,,,,1 1 )(11 1 )(111   = = ?== = ?===?><+><=>?<+><=>?<+><ninjTseeTeniCeniTenj seTeTeniCeniTeTTeniCeijjiiijiiiiidddd???????????
?2We use the notation d to emphasize that this vector containsinformation that was not present in the original document D.Note now that the maximization of the sum in (2)is equivalent to the maximization of each of itsterms, which means that the computation reducesto nid Teeseiiii..1,,maxarg)(?>?<???
?., or  equivalently,nid Teeseiiii..1,||||,maxarg 2)(??><????
(3)Our disambiguation process therefore employs twosteps: first, it builds the extended document vectorand second, it maximizes the scalar products inequation (3).
In practice, it is not necessary to buildthe document vector over all contexts , but onlyover the contexts of the possible entity disam-biguations of the surface forms in the document.Also note that we are not normalizing the scalarproducts by the norms of the vectors (which wouldlead to the computation of cosine similarity).
Inthis manner, we implicitly account for the fre-quency with which a surface form is used to men-tion various entities and for the importance of theseentities (important entities have longer Wikipediaarticles, are mentioned more frequently in otherarticles, and also tend to have more category tags).While rarely, one surface form can be used tomention two or more different entities in a docu-ment (e.g., ?Supreme Court?
may refer to the fed-eral institution in one paragraph and to a state?sjudicial institution in another paragraph).
To ac-count for such cases, the described disambiguationprocess is performed iteratively for the instances ofthe surface forms with multiple disambiguationswith similarity scores higher than an empiricallydetermined threshold, by shrinking the contextused for the disambiguation of each instance fromdocument level to paragraph level, and if neces-sary, to sentence level.6 EvaluationWe used as development data for building the de-scribed system the Wikipedia collection as of April2, 2006 and a set of 100 news stories on a diverserange of topics.
For the final evaluation, we per-formed data extraction from the September 11,2006 version of the Wikipedia collection.We evaluated the system in two ways: on a set ofWikipedia articles, by comparing the system out-put with the references created by human contribu-tors, and on a set of news stories, by doing a post-hoc evaluation of the system output.
The evalua-tion data can be downloaded from http://research.microsoft.com/users/silviu/WebAssistant/TestData.714In both settings, we computed a disambiguationbaseline in the following manner: for each surfaceform, if there was an entity page or redirect pagewhose title matches exactly the surface form thenwe chose the corresponding entity as the baselinedisambiguation; otherwise, we chose the entitymost frequently mentioned in Wikipedia using thatsurface form.6.1 Wikipedia ArticlesWe selected at random 350 Wikipedia entity pagesand we discarded their content during the informa-tion extraction phase.
We then performed an auto-matic evaluation, in which we compared thehyperlinks created by our system with the linkscreated by the Wikipedia contributors.
In an at-tempt to discard most of the non-named entities,we only kept for evaluation the surface forms thatstarted with an uppercase letter.
The test articlescontained 5,812 such surface forms.
551 of themreferenced non-existing articles (for example, thefilmography section of a director contained linkedmentions of all his movies although many of themdid not have an associated Wikipedia page).
Also,130 of the surface forms were not used in otherWikipedia articles and therefore both the baselineand the proposed system could not hypothesize adisambiguation for them.
The accuracy on the re-maining 5,131 surface forms was 86.2% for thebaseline system and 88.3% for the proposed sys-tem.
A McNemar test showed that the difference isnot significant, the main cause being that the ma-jority of the test surface forms were unambiguous.When restricting the test set only to the 1,668 am-biguous surface forms, the difference in accuracybetween the two systems is significant at p = 0.01.An error analysis showed that the Wikipedia setused as gold standard contained relatively manysurface forms with erroneous or out-of-date links,many of them being correctly disambiguated bythe proposed system (thus, counted as errors).
Forexample, the test page ?The Gods (band)?
links toPaul Newton, the painter, and Uriah Heep, which isa disambiguation page, probably because the origi-nal pages changed over time, while the proposedsystem correctly hypothesizes links to Paul New-ton (musician) and Uriah Heep (band).6.2 News StoriesWe downloaded the top two stories in the tenMSNBC news categories (Business, U.S. Politics,Entertainment, Health, Sports, Tech & Science,Travel, TV News, U.S. News, and World News) asof January 2, 2007 and we used them as input toour system.
We then performed a post-hoc evalua-tion of the disambiguations hypothesized for thesurface forms correctly identified by the system(i.e.
if the boundaries of a surface form were notidentified correctly then we disregarded it).We defined a disambiguation to be correct if itrepresented the best possible Wikipedia article thatwould satisfy a user?s need for information andincorrect otherwise.
For example, the article Vikingprogram is judged as correct for ?Viking Landers?,for which there is no separate article in the Wi-kipedia collection.
Linking a surface form to awrong Wikipedia article was counted as an errorregardless whether or not an appropriate Wikipediaarticle existed.
When the system could not disam-biguate a surface form (e.g.
?N?
Sync?, ?
?Bama?,and ?Harris County Jail?
), we performed a searchin Wikipedia for the appropriate entity.
If an articlefor that entity existed (e.g., ?N Sync and Alabama)then we counted that instance as an error.
Other-wise, we counted it separately as non-recallable(e.g.
there is no Wikipedia article for the HarrisCounty Jail entity and the article for Harris County,Texas does not discuss the jail system).The test set contained 756 surface forms, ofwhich 127 were non-recallable.
The proposed sys-tem obtained an accuracy of 91.4%, versus a51.7% baseline (significant at p = 0.01).
An analy-sis of these data showed not only that the mostcommon surface forms used in news are highlyambiguous but also that a large number of Wikipe-dia pages with titles that are popular surface formsin news discuss subjects different from those withcommon news usage (e.g., the page titled ?China?discusses the Chinese civilization and is not thecorrect assignment for the People's Republic ofChina entity; similarly, the default page for?Blackberry?
talks about the fruit rather than thewireless company with the same name).7 Conclusions and Potential ImpactWe presented a large scale named entity disam-biguation system that employs a huge amount ofinformation automatically extracted from Wikipe-dia over a space of more than 1.4 million entities.In tests on both real news data and Wikipedia text,the system obtained accuracies exceeding 91% and88%.
Because the entity recognition and disam-715biguation processes employed use very little lan-guage-dependent resources additional to Wikipe-dia, the system can be easily adapted to languagesother than English.The system described in this paper has been fullyimplemented as a Web browser (Figure 3), whichcan analyze any Web page or client text document.The application on a large scale of such an entityextraction and disambiguation system could resultin a move from the current space of words to aspace of concepts, which enables several paradigmshifts and opens new research directions, which weare currently investigating, from entity-based in-dexing and searching of document collections topersonalized views of the Web through entity-based user bookmarks.AcknowledgmentsThe author would like to gratefully thank MikeSchultz and Robert Ragno for their help in buildingthe system infrastructure, Microsoft Live Searchfor providing access to their search engine, and theanonymous reviewers for their useful comments.ReferencesBagga, A. and B. Baldwin.
1998.
Entity-based cross-document coreferencing using the vector spacemodel.
In Proceedings of COLING-ACL, 79-85.Bunescu, R. and M. Paca.
2006.
Using EncyclopedicKnowledge for Named Entity Disambiguation.
InProceedings of EACL, 9-16.Cederberg, S. and D. Widdows.
2003.
Using LSA andnoun coordination information to improve the preci-sion and recall of hyponymy extraction.
In Proceed-ings of CoNLL, 111-118.Doddington, G., A. Mitchell, M. Przybocki, L. Ram-shaw, S. Strassel, and R. Weischedel.
2004.
ACEprogram ?
task definitions and performance measures.In Proceedings of LREC, 837-840.Edmonds, P. and S. Cotton.
2001.
Senseval-2 overview.In Proceedings of SENSEVAL-2, 1-6.Gabrilovich, E. and S. Markovitch.
2007.
Computingsemantic relatedness using Wikipedia-based explicitsemantic analysis.
Proceedings of IJCAI, 1606-1611.Gale, W., K. Church, and D. Yarowsky.
1992.
Onesense per discourse.
In Proceedings of the 4thDARPA SNL Workshop, 233-237.Grishman, R. and B. Sundheim.
1996.
Message Under-standing Conference - 6: A brief history.
In Proceed-ings of COLING, 466-471.Hearst, M. 1992.
Automatic Acquisition of Hyponymsfrom Large Text Corpora.
In Proc.
COLING, 539-545.Hirschman, L. and N. Chinchor.
1997.
MUC-7 Corefer-ence Task Definition.
In Proceedings of MUC-7.Kanada, Y.
1999.
A method of geographical name ex-traction from Japanese text.
In Proceedings of CIKM,46-54.Kilgarriff, A. and J. Rosenzweig.
2000.
Framework andresults for English Senseval.
Computers and Humani-ties, Special Issue on SENSEVAL, 15-48.Lapata, M. and F. Keller.
2004.
The Web as a Baseline:Evaluating the Performance of Unsupervised Web-based Models for a Range of NLP Tasks.
In Proceed-ings of HLT, 121-128.Mann, G. S. and D. Yarowsky.
2003.
UnsupervisedPersonal Name Disambiguation.
In Proceedings ofCoNLL, 33-40.Mihalcea, R., T. Chklovski, and A. Kilgarriff.
The Sen-seval-3 English lexical sample task.
In Proceedings ofSENSEVAL-3, 25-28.Overell, S., and S. R?ger.
2006 Identifying and ground-ing descriptions of places.
In SIGIR Workshop onGeographic Information Retrieval.Raghavan, H., J. Allan, and A. McCallum.
2004.
Anexploration of entity models, collective classificationand relation description.
In KDD Workshop on LinkAnalysis and Group Detection.Ravin, Y. and Z. Kazi.
1999.
Is Hillary Rodham Clintonthe President?
In ACL Workshop on Coreference andit's Applications.Remy, M. 2002.
Wikipedia: The free encyclopedia.
InOnline Information Review, 26(6): 434.Roark, B. and E. Charniak.
1998.
Noun-phrase co-occurrence statistics for semi-automatic semanticlexicon construction.
In Proceedings of COLING-ACL, 1110-1116.Salton, G. 1989.
Automatic Text Processing.
Addison-Wesley.Smith, D. A. and G. Crane.
2002.
Disambiguating geo-graphic names in a historic digital library.
In Pro-ceedings of ECDL, 127-136.Strube, M. and S. P. Ponzeto.
2006.
WikiRelate!
Com-puting semantic relatedness using Wikipedia.
In Pro-ceedings of AAAI, 1419-1424.Tjong Kim Sang, E. F. and F. De Meulder.
2003.
Intro-duction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.
In Proceed-ings of CoNLL, 142-147.Wacholder, N., Y. Ravin, and M. Choi.
1997.
Disam-biguation of proper names in text.
In Proceedings ofANLP, 202-208.Woodruff, A. G. and C. Paunt.
GIPSY:Automatic geo-graphic indexing of documents.
Journal of the Ameri-can Society for Information Science and Technology,45(9):645-655.716
