Proceedings of the 12th Conference of the European Chapter of the ACL, pages 273?281,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsWho is ?You??
Combining Linguistic and Gaze Features to ResolveSecond-Person References in Dialogue?Matthew Frampton1, Raquel Ferna?ndez1, Patrick Ehlen1, Mario Christoudias2,Trevor Darrell2 and Stanley Peters11Center for the Study of Language and Information, Stanford University{frampton, raquelfr, ehlen, peters}@stanford.edu2International Computer Science Institute, University of California at Berkeleycmch@icsi.berkeley.edu, trevor@eecs.berkeley.eduAbstractWe explore the problem of resolving thesecond person English pronoun you inmulti-party dialogue, using a combinationof linguistic and visual features.
First, wedistinguish generic and referential uses,then we classify the referential uses as ei-ther plural or singular, and finally, for thelatter cases, we identify the addressee.
Inour first set of experiments, the linguisticand visual features are derived from man-ual transcriptions and annotations, but inthe second set, they are generated throughentirely automatic means.
Results showthat a multimodal system is often prefer-able to a unimodal one.1 IntroductionThe English pronoun you is the second most fre-quent word in unrestricted conversation (after Iand right before it).1 Despite this, with the ex-ception of Gupta et al (2007b; 2007a), its re-solution has received very little attention in the lit-erature.
This is perhaps not surprising since thevast amount of work on anaphora and referenceresolution has focused on text or discourse - medi-ums where second-person deixis is perhaps notas prominent as it is in dialogue.
For spoken di-alogue pronoun resolution modules however, re-solving you is an essential task that has an impor-tant impact on the capabilities of dialogue summa-rization systems.
?We thank the anonymous EACL reviewers, and SurabhiGupta, John Niekrasz and David Demirdjian for their com-ments and technical assistance.
This work was supported bythe CALO project (DARPA grant NBCH-D-03-0010).1See e.g.
http://www.kilgarriff.co.uk/BNC_lists/Besides being important for computational im-plementations, resolving you is also an interestingand challenging research problem.
As for thirdperson pronouns such as it, some uses of you arenot strictly referential.
These include discoursemarker uses such as you know in example (1), andgeneric uses like (2), where you does not refer tothe addressee as it does in (3).
(1) It?s not just, you know, noises like somethinghitting.
(2) Often, you need to know specific button se-quences to get certain functionalities done.
(3) I think it?s good.
You?ve done a good review.However, unlike it, you is ambiguous between sin-gular and plural interpretations - an issue that isparticularly problematic in multi-party conversa-tions.
While you clearly has a plural referent in(4), in (3) the number of its referent is ambigu-ous.2(4) I don?t know if you guys have any questions.When an utterance contains a singular referen-tial you, resolving the you amounts to identifyingthe individual to whom the utterance is addressed.This is trivial in two-person dialogue since the cur-rent listener is always the addressee, but in conver-sations with multiple participants, it is a complexproblem where different kinds of linguistic and vi-sual information play important roles (Jovanovic,2007).
One of the issues we investigate here is2In contrast, the referential use of the pronoun it (as wellas that of some demonstratives) is ambiguous between NPinterpretations and discourse-deictic ones (Webber, 1991).273how this applies to the more concrete problem ofresolving the second person pronoun you.We approach this issue as a three-step prob-lem.
Using the AMI Meeting Corpus (McCowanet al, 2005) of multi-party dialogues, we first dis-criminate between referential and generic uses ofyou.
Then, within the referential uses, we dis-tinguish between singular and plural, and finally,we resolve the singular referential instances byidentifying the intended addressee.
We use multi-modal features: initially, we extract discourse fea-tures from manual transcriptions and use visual in-formation derived from manual annotations, butthen we move to a fully automatic approach, us-ing 1-best transcriptions produced by an automaticspeech recognizer (ASR) and visual features auto-matically extracted from raw video.In the next section of this paper, we give a briefoverview of related work.
We describe our data inSection 3, and explain how we extract visual andlinguistic features in Sections 4 and 5 respectively.Section 6 then presents our experiments with man-ual transcriptions and annotations, while Section7, those with automatically extracted information.We end with conclusions in Section 8.2 Related Work2.1 Reference Resolution in DialogueAlthough the vast majority of work on referenceresolution has been with monologic text, some re-cent research has dealt with the more complexscenario of spoken dialogue (Strube and Mu?ller,2003; Byron, 2004; Arstein and Poesio, 2006;Mu?ller, 2007).
There has been work on the iden-tification of non-referential uses of the pronoun it:Mu?ller (2006) uses a set of shallow features au-tomatically extracted from manual transcripts oftwo-party dialogue in order to train a rule-basedclassifier, and achieves an F-score of 69%.The only existing work on the resolution of youthat we are aware of is Gupta et al (2007b; 2007a).In line with our approach, the authors first disam-biguate between generic and referential you, andthen attempt to resolve the reference of the ref-erential cases.
Generic uses of you account for47% of their data set, and for the generic vs. ref-erential disambiguation, they achieve an accuracyof 84% on two-party conversations and 75% onmulti-party dialogue.
For the reference resolutiontask, they achieve 47%, which is 10 points overa baseline that always classifies the next speakeras the addressee.
These results are achieved with-out visual information, using manual transcripts,and a combination of surface features and manu-ally tagged dialogue acts.2.2 Addressee DetectionResolving the referential instances of you amountsto determining the addressee(s) of the utterancecontaining the pronoun.
Recent years have seenan increasing amount of research on automaticaddressee detection.
Much of this work focuseson communication between humans and computa-tional agents (such as robots or ubiquitous com-puting systems) that interact with users who maybe engaged in other activities, including interac-tion with other humans.
In these situations, itis important for a system to be able to recognizewhen it is being addressed by a user.
Bakx etal.
(2003) and Turnhout et al (2005) studied thisissue in the context of mixed human-human andhuman-computer interaction using facial orienta-tion and utterance length as clues for addresseedetection, while Katzenmaier et al (2004) inves-tigated whether the degree to which a user utter-ance fits the language model of a conversationalrobot can be useful in detecting system-addressedutterances.
This research exploits the fact that hu-mans tend to speak differently to systems than toother humans.Our research is closer to that of Jovanovicet al (2006a; 2007), who studied addressing inhuman-human multi-party dialogue.
Jovanovicand colleagues focus on addressee identification inface-to-face meetings with four participants.
Theyuse a Bayesian Network classifier trained on sev-eral multimodal features (including visual featuressuch as gaze direction, discourse features such asthe speaker and dialogue act of preceding utter-ances, and utterance features such as lexical cluesand utterance duration).
Using a combination offeatures from various resources was found to im-prove performance (the best system achieves anaccuracy of 77% on a portion of the AMI MeetingCorpus).
Although this result is very encouraging,it is achieved with the use of manually producedinformation - in particular, manual transcriptions,dialogue acts and annotations of visual focus of at-tention.
One of the issues we aim to investigatehere is how automatically extracted multimodalinformation can help in detecting the addressee(s)of you-utterances.274Generic Referential Ref Sing.
Ref Pl.49.14% 50.86% 67.92% 32.08%Table 1: Distribution of you interpretations3 DataOur experiments are performed using the AMIMeeting Corpus (McCowan et al, 2005), a collec-tion of scenario-driven meetings among four par-ticipants, manually transcribed and annotated withseveral different types of information (includingdialogue acts, topics, visual focus of attention, andaddressee).
We use a sub-corpus of 948 utterancescontaining you, and these were extracted from 10different meetings.
The you-utterances are anno-tated as either discourse marker, generic or refer-ential.We excluded the discourse marker cases, whichaccount for only 8% of the data, and of the refer-ential cases, selected those with an AMI addresseeannotation.3 The addressee of a dialogue act canbe unknown, a single meeting participant, twoparticipants, or the whole audience (three partici-pants in the AMI corpus).
Since there are very fewinstances of two-participant addressee, we distin-guish only between singular and plural addressees.The resulting distribution of classes is shown inTable 1.4We approach the reference resolution task as atwo-step process, first discriminating between plu-ral and singular references, and then resolving thereference of the singular cases.
The latter task re-quires a classification scheme for distinguishingbetween the three potential addressees (listeners)for the given you-utterance.In their four-way classification scheme,Gupta et al (2007a) label potential addressees interms of the order in which they speak after theyou-utterance.
That is, for a given you-utterance,the potential addressee who speaks next is labeled1, the potential addressee who speaks after that is2, and the remaining participant is 3.
Label 4 isused for group addressing.
However, this resultsin a very skewed class distribution because thenext speaker is the intended addressee 41% ofthe time, and 38% of instances are plural - the3Addressee annotations are not provided for some dia-logue act types - see (Jovanovic et al, 2006b).4Note that the percentages of the referential singular andreferential plural are relative to the total of referential in-stances.L1 L2 L335.17% 30.34% 34.49%Table 2: Distribution of addressees for singular youremaining two classes therefore make up a smallpercentage of the data.We were able to obtain a much less skewed classdistribution by identifying the potential addresseesin terms of their position in relation to the currentspeaker.
The meeting setting includes a rectangu-lar table with two participants seated at each ofits opposite longer sides.
Thus, for a given you-utterance, we label listeners as either L1, L2 orL3 depending on whether they are sitting opposite,diagonally or laterally from the speaker.
Table 2shows the resulting class distribution for our data-set.
Such a labelling scheme is more similar to Jo-vanovic (2007), where participants are identifiedby their seating position.4 Visual Information4.1 Features from Manual AnnotationsWe derived per-utterance visual features from theFocus Of Attention (FOA) annotations providedby the AMI corpus.
These annotations track meet-ing participants?
head orientation and eye gazeduring a meeting.5 Our first step was to use theFOA annotations in order to compute what we re-fer to as Gaze Duration Proportion (GDP) valuesfor each of the utterances of interest - a measuresimilar to the ?Degree of Mean Duration of Gaze?described by (Takemae et al, 2004).
Here a GDPvalue denotes the proportion of time in utterance ufor which subject i is looking at target j:GDPu(i, j) =?jT (i, j)/Tuwere Tu is the length of utterance u in millisec-onds, and T (i, j), the amount of that time that ispends looking at j.
The gazer i can only refer toone of the four meeting participants, but the tar-get j can also refer to the white-board/projectorscreen present in the meeting room.
For each utter-ance then, all of the possible values of i and j areused to construct a matrix of GDP values.
Fromthis matrix, we then construct ?Highest GDP?
fea-tures for each of the meeting participants: such5A description of the FOA labeling scheme is avail-able from the AMI Meeting Corpus website http://corpus.amiproject.org/documentations/guidelines-1/275For each participant Pi?
target for whole utterance?
target for first third of utterance?
target for second third of utterance?
target for third third of utterance?
target for -/+ 2 secs from you start time?
ratio 2nd hyp.
target / 1st hyp.
target?
ratio 3rd hyp.
target / 1st hyp.
target?
participant in mutual gaze with speakerTable 3: Visual Featuresfeatures record the target with the highest GDPvalue and so indicate whom/what the meeting par-ticipant spent most time looking at during the ut-terance.We also generated a number of additional fea-tures for each individual.
These include firstly,three features which record the candidate ?gazee?with the highest GDP during each third of the ut-terance, and which therefore account for gaze tran-sitions.
So as to focus more closely on where par-ticipants are looking around the time when youis uttered, another feature records the candidatewith the highest GDP -/+ 2 seconds from the starttime of the you.
Two further features give someindication of the amount of looking around thatthe speaker does during an utterance - we hypoth-esized that participants (especially the speaker)might look around more in utterances with plu-ral addressees.
The first is the ratio of the sec-ond highest GDP to the highest, and the secondis the ratio of the third highest to the highest.
Fi-nally, there is a highest GDP mutual gaze featurefor the speaker, indicating with which other indi-vidual, the speaker spent most time engaged in amutual gaze.Hence this gives a total of 29 features: sevenfeatures for each of the four participants, plus onemutual gaze feature.
They are summarized in Ta-ble 3.
These visual features are different to thoseused by Jovanovic (2007) (see Section 2).
Jo-vanovic?s features record the number of times thateach participant looks at each other participantduring the utterance, and in addition, the gaze di-rection of the current speaker.
Hence, they are nothighest GDP values, they do not include a mutualgaze feature and they do not record whether par-ticipants look at the white-board/projector screen.4.2 Automatic Features from Raw VideoTo perform automatic visual feature extraction, asix degree-of-freedom head tracker was run overeach subject?s video sequence for the utterancescontaining you.
For each utterance, this gave 4 se-quences, one per subject, of the subject?s 3D headorientation and location at each video frame alongwith 3D head rotational velocities.
From thesemeasurements we computed two types of visualinformation: participant gaze and mutual gaze.The 3D head orientation and location of eachsubject along with camera calibration informationwas used to compute participant gaze informationfor each video frame of each sequence in the formof a gaze probability matrix.
More precisely, cam-era calibration is first used to estimate the 3D headorientation and location of all subjects in the sameworld coordinate system.The gaze probability matrix is a 4 ?
5 matrixwhere entry i, j stores the probability that subjecti is looking at subject j for each of the four sub-jects and the last column corresponds to the white-board/projector screen (i.e., entry i, j where j = 5is the probability that subject i is looking at thescreen).
Gaze probability G(i, j) is defined asG(i, j) = G0e?
?i,j2/?2where ?i,j is the angular difference between thegaze of subject i and the direction defined by thelocation of subjects i and j. G0 is a normalizationfactor such that?j G(i, j) = 1 and ?
is a user-defined constant (in our experiments, we chose?
= 15 degrees).Using the gaze probability matrix, a 4 ?
1 per-frame mutual gaze vector was computed that forentry i stores the probability that the speaker andsubject i are looking at one another.In order to create features equivalent to thosedescribed in Section 4.1, we first collapse theframe-level probability matrix into a matrix of bi-nary values.
We convert the probability for eachframe into a binary judgement of whether subjecti is looking at target j:H(i, j) = ?G(i, j)?
is a binary value to evaluate G(i, j) > ?, where?
is a high-pass thresholding value - or ?gaze prob-ability threshold?
(GPT) - between 0 and 1.Once we have a frame-level matrix of binaryvalues, for each subject i, we compute GDP val-ues for the time periods of interest, and in eachcase, choose the target with the highest GDP as thecandidate.
Hence, we compute a candidate targetfor the utterance overall, for each third of the ut-terance, and for the period -/+ 2 seconds from the276you start time, and in addition, we compute a can-didate participant for mutual gaze with the speakerfor the utterance overall.We sought to use the GPT threshold which pro-duces automatic visual features that agree bestwith the features derived from the FOA annota-tions.
Hence we experimented with different GPTvalues in increments of 0.1, and compared the re-sulting features to the manual features using thekappa statistic.
A threshold of 0.6 gave the bestkappa scores, which ranged from 20% to 44%.65 Linguistic InformationOur set of discourse features is a simplified ver-sion of those employed by Galley et al (2004) andGupta et al (2007a).
It contains three main types(summarized in Table 4):?
Sentential features (1 to 13) encode structural,durational, lexical and shallow syntactic patternsof the you-utterance.
Feature 13 is extracted us-ing the AMI ?Named Entity?
annotations and in-dicates whether a particular participant is men-tioned in the you-utterance.
Apart from this fea-ture, all other sentential features are automaticallyextracted, and besides 1, 8, 9, and 10, they are allbinary.?
Backward Looking (BL)/Forward Looking (FL)features (14 to 22) are mostly extracted from ut-terance pairs, namely the you-utterance and theBL/FL (previous/next) utterance by each listenerLi (potential addressee).
We also include a fewextra features which are not computed in terms ofutterance pairs.
These indicate the number of par-ticipants that speak during the previous and next 5utterances, and the BL and FL speaker order.
Allof these features are computed automatically.?
Dialogue Act (DA) features (23 to 24) use themanual AMI dialogue act annotations to representthe conversational function of the you-utteranceand the BL/FL utterance by each potential ad-dressee.
Along with the sentential feature basedon the AMI Named Entity annotations, these arethe only discourse features which are not com-puted automatically.
76The fact that our gaze estimator is getting any usefulagreement with respect to these annotations is encouragingand suggests that an improved tracker and/or one that adaptsto the user more effectively could work very well.7Since we use the manual transcripts of the meetings, thetranscribed words and the segmentation into utterances or di-alogue acts are of course not given automatically.
A fullyautomatic approach would involve using ASR output insteadof manual transcriptions?
something which we attempt in(1) # of you pronouns(2) you (say|said|tell|told| mention(ed)|mean(t)|sound(ed))(3) auxiliary you(4) wh-word you(5) you guys(6) if you(7) you know(8) # of words in you-utterance(9) duration of you-utterance(10) speech rate of you-utterance(11) 1st person(12) general case(13) person Named Entity tag(14) # of utterances between you- and BL/FL utt.
(15) # of speakers between you- and BL/FL utt.
(16) overlap between you- and BL/FL utt.
(binary)(17) duration of overlap between you- and BL/FL utt.
(18) time separation between you- and BL/FL utt.
(19) ratio of words in you- that are in BL/FL utt.
(20) # of participants that speak during prev.
5 utt.
(21) # of participants that speak during next 5 utt.
(22) speaker order BL/FL(23) dialogue act of the you-utterance(24) dialogue act of the BL/FL utteranceTable 4: Discourse Features6 First Set of Experiments & ResultsIn this section we report our experiments and re-sults when using manual transcriptions and anno-tations.
In Section 7 we will present the resultsobtained using ASR output and automatically ex-tracted visual information.
All experiments (hereand in the next section) are performed using aBayesian Network classifier with 10-fold cross-validation.8 In each task, we give raw overall ac-curacy results and then F-scores for each of theclasses.
We computed measures of informationgain in order to assess the predictive power of thevarious features, and did some experimentationwith Correlation-based Feature Selection (CFS)(Hall, 2000).6.1 Generic vs. Referential Uses of YouWe first address the task of distinguishing betweengeneric and referential uses of you.Baseline.
A majority class baseline that classi-fies all instances of you as referential yields an ac-curacy of 50.86% (see Table 1).Results.
A summary of the results is given in Ta-ble 5.
Using discourse features only we achievean accuracy of 77.77%, while using multimodalSection 7.8We use the the BayesNet classifier implemented in theWeka toolkit http://www.cs.waikato.ac.nz/ml/weka/.277Features Acc F1-Gen F1-RefBaseline 50.86 0 67.4Discourse 77.77 78.8 76.6Visual 60.32 64.2 55.5MM 79.02 80.2 77.7Dis w/o FL 78.34 79.1 77.5MM w/o FL 78.22 79.0 77.4Dis w/o DA 69.44 71.5 67.0MM w/o DA 72.75 74.4 70.9Table 5: Generic vs. referential uses(MM) yields 79.02%, but this increase is not sta-tistically significant.In spite of this, visual features do help to dis-tinguish between generic and referential uses -note that the visual features alone are able to beatthe baseline (p < .005).
The listeners?
gaze ismore predictive than the speaker?s: if listenerslook mostly at the white-board/projector screen in-stead of another participant, then the you is morelikely to be referential.
More will be said on thisin Section 6.2.1 in the analysis of the results forthe singular vs. plural referential task.We found sentential features of the you-utterance to be amongst the best predictors, es-pecially those that refer to surface lexical proper-ties, such as features 1, 11, 12 and 13 in Table 4.Dialogue act features provide useful informationas well.
As pointed out by Gupta et al (2007b;2007a), a you pronoun within a question (e.g.an utterance tagged as elicit-assess orelicit-inform) is more likely to be referen-tial.
Eliminating information about dialogue acts(w/o DA) brings down performance (p < .005),although accuracy remains well above the baseline(p < .001).
Note that the small changes in perfor-mance when FL information is taken out (w/o FL)are not statistically significant.6.2 Reference ResolutionWe now turn to the referential instances of you,which can be resolved by determining the ad-dressee(s) of the given utterance.6.2.1 Singular vs. Plural ReferenceWe start by trying to discriminate singular vs. plu-ral interpretations.
For this, we use a two-wayclassification scheme that distinguishes betweenindividual and group addressing.
To our knowl-edge, this is the first attempt at this task using lin-guistic information.99But see e.g.
(Takemae et al, 2004) for an approach thatuses manually extracted visual-only clues with similar aims.Baseline.
A majority class baseline that consid-ers all instances of you as referring to an individualaddressee gives 67.92% accuracy (see Table 1).Results.
A summary of the results is shown inTable 6.
There is no statistically significant differ-ence between the baseline and the results obtainedwhen visual features are used alone (67.92% vs.66.28%).
However, we found that visual informa-tion did contribute to identifying some instances ofplural addressing, as shown by the F-score for thatclass.
Furthermore, the visual features helped toimprove results when combined with discourse in-formation: using multimodal (MM) features pro-duces higher results than the discourse-only fea-ture set (p < .005), and increases from 74.24% to77.05% with CFS.As in the generic vs. referential task, the white-board/projector screen value for the listeners?
gazefeatures seems to have discriminative power -when listeners?
gaze features take this value, it isoften indicative of a plural rather than a singularyou.
It seems then, that in our data-set, the speakeroften uses the white-board/projector screen whenaddressing the group, and hence draws the listen-ers?
gaze in this direction.
We should also notethat the ratio features which we thought might beuseful here (see Section 4.1) did not prove so.Amongst the most useful discourse featuresare those that encode similarity relations betweenthe you-utterance and an utterance by a potentialaddressee.
Utterances by individual addresseestend to be more lexically cohesive with the you-utterance and so if features such as feature 19 inTable 4 indicate a low level of lexical similarity,then this increases the likelihood of plural address-ing.
Sentential features that refer to surface lexicalpatterns (features 6, 7, 11 and 12) also contributeto improved results, as does feature 21 (number ofspeakers during the next five utterances) - fewerspeaker changes correlates with plural addressing.Information about dialogue acts also plays arole in distinguishing between singular and plu-ral interpretations.
Questions tend to be addressedto individual participants, while statements show astronger correlation with plural addressees.
Whenno DA features are used (w/o DA), the drop in per-formance for the multimodal classifier to 71.19%is statistically significant (p < .05).
As for thegeneric vs. referential task, FL information doesnot have a significant effect on performance.278Features Acc F1-Sing.
F1-Pl.Baseline 67.92 80.9 0Discourse 71.19 78.9 54.6Visual 66.28 74.8 48.9MM* 77.05 83.3 63.2Dis w/o FL 72.13 80.1 53.7MM w/o FL 72.60 79.7 58.1Dis w/o DA 68.38 78.5 40.5MM w/o DA 71.19 78.8 55.3Table 6: Singular vs. plural reference; * = with Correlation-based Feature Selection (CFS).6.2.2 Detection of Individual AddresseesWe now turn to resolving the singular referentialuses of you.
Here we must detect the individualaddressee of the utterance that contains the pro-noun.Baselines.
Given the distribution shown in Ta-ble 2, a majority class baseline yields an accu-racy of 35.17%.
An off-line system that has accessto future context could implement a next-speakerbaseline that always considers the next speaker tobe the intended addressee, so yielding a high rawaccuracy of 71.03%.
A previous-speaker base-line that does not require access to future contextachieves 35% raw accuracy.Results.
Table 7 shows a summary of the re-sults, and these all outperform the majority class(MC) and previous-speaker baselines.
When alldiscourse features are available, adding visual in-formation does improve performance (74.48% vs.60.69%, p < .005), and with CFS, this increasesfurther to 80.34% (p < .005).
Using discourse orvisual features alone gives scores that are belowthe next-speaker baseline (60.69% and 65.52% vs.71.03%).
Taking all forward-looking (FL) infor-mation away reduces performance (p < .05), butthe small increase in accuracy caused by takingaway dialogue act information is not statisticallysignificant.When we investigated individual feature contri-bution, we found that the most predictive featureswere the FL and backward-looking (BL) speakerorder, and the speaker?s visual features (includingmutual gaze).
Whomever the speaker spent mosttime looking at or engaged in a mutual gaze withwas more likely to be the addressee.
All of the vi-sual features had some degree of predictive powerapart from the ratio features.
Of the other BL/FLdiscourse features, features 14, 18 and 19 (see Ta-ble 4) were more predictive.
These indicate thatutterances spoken by the intended addressee areFeatures Acc F1-L1 F1-L2 F1-L3MC baseline 35.17 52.0 0 0Discourse 60.69 59.1 60.0 62.7Visual 65.52 69.1 63.5 64.0MM* 80.34 80.0 82.4 79.0Dis w/o FL 52.41 50.7 51.8 54.5MM w/o FL 66.55 68.7 62.7 67.6Dis w/o DA 61.03 58.5 59.9 64.2MM w/o DA 73.10 72.4 69.5 72.0Table 7: Addressee detection for singular references; * =with Correlation-based Feature Selection (CFS).often adjacent to the you-utterance and lexicallysimilar.7 A Fully Automatic ApproachIn this section we describe experiments whichuse features derived from ASR transcriptions andautomatically-extracted visual information.
Weused SRI?s Decipher (Stolcke et al, 2008)10 in or-der to generate ASR transcriptions, and appliedthe head-tracker described in Section 4.2 to therelevant portions of video in order to extract thevisual information.
Recall that the Named Entityfeatures (feature 13) and the DA features used inour previous experiments had been manually an-notated, and hence are not used here.
We againdivide the problem into the same three separatetasks: we first discriminate between generic andreferential uses of you, then singular vs. pluralreferential uses, and finally we resolve the ad-dressee for singular uses.
As before, all exper-iments are performed using a Bayesian Networkclassifier and 10-fold cross validation.7.1 ResultsFor each of the three tasks, Figure 7 comparesthe accuracy results obtained using the fully-automatic approach with those reported in Section6.
The figure shows results for the majority classbaselines (MCBs), and with discourse-only (Dis),and multimodal (MM) feature sets.
Note that thedata set for the automatic approach is smaller,and that the majority class baselines have changedslightly.
This is because of differences in the ut-terance segmentation, and also because not all ofthe video sections around the you utterances wereprocessed by the head-tracker.In all three tasks we are able to significantlyoutperform the majority class baseline, but the vi-sual features only produce a significant improve-10Stolcke et al (2008) report a word error rate of 26.9% onAMI meetings.279Figure 1: Results for the manual and automatic systems; MCB = majority class baseline, Dis = discourse features, MM =multimodal, * = with Correlation-based Feature Selection (CFS), FL = forward-looking, man = manual, auto = automatic.ment in the individual addressee resolution task.For the generic vs. referential task, the discourseand multimodal classifiers both outperform themajority class baseline (p < .001), achievingaccuracy scores of 68.71% and 68.48% respec-tively.
In contrast to when using manual transcrip-tions and annotations (see Section 6.1), removingforward-looking (FL) information reduces perfor-mance (p < .05).
For the referential singularvs.
plural task, the discourse and multimodal withCFS classifier improve over the majority classbaseline (p < .05).
Multimodal with CFS doesnot improve over the discourse classifier - indeedwithout feature selection, the addition of visualfeatures causes a drop in performance (p < .05).Here, taking away FL information does not causea significant reduction in performance.
Finally,in the individual addressee resolution task, thediscourse, visual (60.78%) and multimodal clas-sifiers all outperform the majority class baseline(p < .005, p < .001 and p < .001 respec-tively).
Here the addition of visual features causesthe multimodal classifier to outperform the dis-course classifier in raw accuracy by nearly ten per-centage points (67.32% vs. 58.17%, p < .05), andwith CFS, the score increases further to 74.51%(p < .05).
Taking away FL information doescause a significant drop in performance (p < .05).8 ConclusionsWe have investigated the automatic resolution ofthe second person English pronoun you in multi-party dialogue, using a combination of linguisticand visual features.
We conducted a first set ofexperiments where our features were derived frommanual transcriptions and annotations, and then asecond set where they were generated by entirelyautomatic means.
To our knowledge, this is thefirst attempt at tackling this problem using auto-matically extracted multimodal information.Our experiments showed that visual informa-tion can be highly predictive in resolving the ad-dressee of singular referential uses of you.
Visualfeatures significantly improved the performance ofboth our manual and automatic systems, and thelatter achieved an encouraging 75% accuracy.
Wealso found that our visual features had predictivepower for distinguishing between generic and ref-erential uses of you, and between referential sin-gulars and plurals.
Indeed, for the latter task,they significantly improved the manual system?sperformance.
The listeners?
gaze features wereuseful here: in our data set it was apparently thecase that the speaker would often use the white-board/projector screen when addressing the group,thus drawing the listeners?
gaze in this direction.Future work will involve expanding our data-set, and investigating new potentially predictivefeatures.
In the slightly longer term, we plan tointegrate the resulting system into a meeting as-sistant whose purpose is to automatically extractuseful information from multi-party meetings.280ReferencesRon Arstein and Massimo Poesio.
2006.
Identifyingreference to abstract objects in dialogue.
In Pro-ceedings of the 10th Workshop on the Semantics andPragmatics of Dialogue (Brandial?06), pages 56?63, Potsdam, Germany.Ilse Bakx, Koen van Turnhout, and Jacques Terken.2003.
Facial orientation during multi-party inter-action with information kiosks.
In Proceedings ofINTERACT, Zurich, Switzerland.Donna Byron.
2004.
Resolving pronominal refer-ence to abstract entities.
Ph.D. thesis, Universityof Rochester, Department of Computer Science.Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agree-ment and disagreement in conversational speech:Use of Bayesian networks to model pragmatic de-pendencies.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Lin-guistics (ACL).Surabhi Gupta, John Niekrasz, Matthew Purver, andDaniel Jurafsky.
2007a.
Resolving ?you?
in multi-party dialog.
In Proceedings of the 8th SIGdialWorkshop on Discourse and Dialogue, Antwerp,Belgium, September.Surabhi Gupta, Matthew Purver, and Daniel Jurafsky.2007b.
Disambiguating between generic and refer-ential ?you?
in dialog.
In Proceedings of the 45thAnnual Meeting of the Association for Computa-tional Linguistics (ACL).Mark Hall.
2000.
Correlation-based Feature Selectionfor Machine Learning.
Ph.D. thesis, University ofWaikato.Natasa Jovanovic, Rieks op den Akker, and Anton Ni-jholt.
2006a.
Addressee identification in face-to-face meetings.
In Proceedings of the 11th Confer-ence of the European Chapter of the ACL (EACL),pages 169?176, Trento, Italy.Natasa Jovanovic, Rieks op den Akker, and Anton Ni-jholt.
2006b.
A corpus for studying addressingbehaviour in multi-party dialogues.
Language Re-sources and Evaluation, 40(1):5?23.
ISSN=1574-020X.Natasa Jovanovic.
2007.
To Whom It May Concern -Addressee Identification in Face-to-Face Meetings.Ph.D.
thesis, University of Twente, Enschede, TheNetherlands.Michael Katzenmaier, Rainer Stiefelhagen, and TanjaSchultz.
2004.
Identifying the addressee in human-human-robot interactions based on head pose andspeech.
In Proceedings of the 6th InternationalConference on Multimodal Interfaces, pages 144?151, State College, Pennsylvania.Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,S.
Bourban, M. Flynn, M. Guillemot, T. Hain,J.
Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,M.
Lincoln, A. Lisowska, W. Post, D. Reidsma, andP.
Wellner.
2005.
The AMI Meeting Corpus.
InProceedings of Measuring Behavior, the 5th Inter-national Conference on Methods and Techniques inBehavioral Research, Wageningen, Netherlands.Christoph Mu?ller.
2006.
Automatic detection of non-referential It in spoken multi-party dialog.
In Pro-ceedings of the 11th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL), pages 49?56, Trento, Italy.Christoph Mu?ller.
2007.
Resolving it, this, and thatin unrestricted multi-party dialog.
In Proceedingsof the 45th Annual Meeting of the Association forComputational Linguistics, pages 816?823, Prague,Czech Republic.Andreas Stolcke, Xavier Anguera, Kofi Boakye, O?zgu?rC?etin, Adam Janin, Matthew Magimai-Doss, ChuckWooters, and Jing Zheng.
2008.
The icsi-sri spring2007 meeting and lecture recognition system.
InProceedings of CLEAR 2007 and RT2007.
SpringerLecture Notes on Computer Science.Michael Strube and Christoph Mu?ller.
2003.
A ma-chine learning approach to pronoun resolution inspoken dialogue.
In Proceedings of ACL?03, pages168?175.Yoshinao Takemae, Kazuhiro Otsuka, and NaokiMukawa.
2004.
An analysis of speakers?
gazebehaviour for automatic addressee identification inmultiparty conversation and its application to videoediting.
In Proceedings of IEEE Workshop on Robotand Human Interactive Communication, pages 581?586.Koen van Turnhout, Jacques Terken, Ilse Bakx, andBerry Eggen.
2005.
Identifying the intendedaddressee in mixed human-humand and human-computer interaction from non-verbal features.
InProceedings of ICMI, Trento, Italy.Bonnie Webber.
1991.
Structure and ostension inthe interpretation of discourse deixi.
Language andCognitive Processes, 6(2):107?135.281
