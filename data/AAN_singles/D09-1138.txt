Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1328?1337,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPSupervised Learning of a Probabilistic Lexicon of Verb Semantic ClassesYusuke MiyaoUniversity of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo, Japanyusuke@is.s.u-tokyo.ac.jpJun?ichi TsujiiUniversity of TokyoUniversity of ManchesterNational Center for Text MiningHongo 7-3-1, Bunkyo-ku, Tokyo, Japantsujii@is.s.u-tokyo.ac.jpAbstractThe work presented in this paper exploresa supervised method for learning a prob-abilistic model of a lexicon of VerbNetclasses.
We intend for the probabilis-tic model to provide a probability dis-tribution of verb-class associations, overknown and unknown verbs, including pol-ysemous words.
In our approach, train-ing instances are obtained from an ex-isting lexicon and/or from an annotatedcorpus, while the features, which repre-sent syntactic frames, semantic similarity,and selectional preferences, are extractedfrom unannotated corpora.
Our modelis evaluated in type-level verb classifica-tion tasks: we measure the prediction ac-curacy of VerbNet classes for unknownverbs, and also measure the dissimilaritybetween the learned and observed proba-bility distributions.
We empirically com-pare several settings for model learning,while we vary the use of features, sourcecorpora for feature extraction, and disam-biguated corpora.
In the task of verb clas-sification into all VerbNet classes, our bestmodel achieved a 10.69% error reductionin the classification accuracy, over the pre-viously proposed model.1 IntroductionLexicons are invaluable resources for semanticprocessing.
In many cases, lexicons are neces-sary to restrict a set of semantic classes to be as-signed to a word.
In fact, a considerable number ofworks on semantic processing implicitly or explic-itly presupposes the availability of a lexicon, suchas in word sense disambiguation (WSD) (Mc-Carthy et al, 2004), and in token-level verb classdisambiguation (Lapata and Brew, 2004; Girju etal., 2005; Li and Brew, 2007; Abend et al, 2008).In other words, those methods are heavily de-pendent on the availability of a semantic lexicon.Therefore, recent research efforts have invested indeveloping semantic resources, such as WordNet(Fellbaum, 1998), FrameNet (Baker et al, 1998),and VerbNet (Kipper et al, 2000; Kipper-Schuler,2005), which greatly advanced research in seman-tic processing.
However, the construction of suchresources is expensive, and it is unrealistic to pre-suppose the availability of full-coverage lexicons;this is the case because unknown words always ap-pear in real texts, and word-semantics associationsmay vary (Abend et al, 2008).This paper explores a method for the supervisedlearning of a probabilistic model for the VerbNetlexicon.
We target the automatic classification ofarbitrary verbs, including polysemous verbs, intoall VerbNet classes; further, we target the esti-mation of a probabilistic model, which representsthe saliences of verb-class associations for polyse-mous verbs.
In our approach, an existing lexiconand/or an annotated corpus are used as the trainingdata.
Since VerbNet classes are designed to rep-resent the distinctions in the syntactic frames thatverbs can take, features, representing the statisticsof syntactic frames, are extracted from the unan-notated corpora.
Additionally, as the classes rep-resent semantic commonalities, semantically in-spired features, like distributionally similar words,are used.
These features can be considered as ageneralized representation of verbs, and we ex-pect that the obtained probabilistic model predictsVerbNet classes of the unknown words.Our model is evaluated in two tasks of type-level verb classification: one is the classificationof monosemous verbs into a small subset of theclasses, which was studied in some previous works(Joanis and Stevenson, 2003; Joanis et al, 2008).The other task is the classification of all verbs intothe full set of VerbNet classes, which has not yet1328been attempted.
In the experiments, training in-stances are obtained from VerbNet and/or Sem-Link (Loper et al, 2007), while features are ex-tracted from the British National Corpus or fromWall Street Journal.
We empirically compare sev-eral settings for model learning by varying theset of features, the source domain and the sizeof a corpus for feature extraction, and the use ofthe token-level statistics obtained from a manuallydisambiguated corpus.
We also provide the anal-ysis of the remaining errors, which will lead us tofurther improve the supervised learning of a prob-abilistic semantic lexicon.Supervised methods for automatic verb classifi-cation have been extensively investigated (Steven-son et al, 1999; Stevenson and Merlo, 1999;Merlo and Stevenson, 2001; Stevenson and Joa-nis, 2003; Joanis and Stevenson, 2003; Joanis etal., 2008).
However, their focus has been lim-ited to a small subset of verb classes, and a lim-ited number of monosemous verbs.
The main con-tributions of the present work are: i) to provideempirical results for the automatic classificationof all verbs, including polysemous ones, into allVerbNet classes, and ii) to empirically explore theeffective settings for the supervised learning of aprobabilistic lexicon of verb semantic classes.2 Background2.1 Verb lexiconLevin?s (1993) work on verb classification hasbroadened the field of computational research thatconcerns the relationships between the syntacticand semantic structures of verbs.
The principalidea behind the work is that the meanings of verbscan be identified by observing possible syntacticframes that the verbs can take.
In other words,with the knowledge of syntactic frames, verbs canbe semantically classified.
This idea provided thecomputational linguistics community with crite-ria for the definition and the classification of verbsemantics; it has subsequently resulted in the re-search of the induction of verb classes (Korhonenand Briscoe, 2004), and the construction of a verblexicon based on Levin?s criteria.VerbNet (Kipper et al, 2000; Kipper-Schuler,2005) is a lexicon of verbs organized into classesthat share the same syntactic behaviors and seman-tics.
The design of classes originates from Levin(1993), though the design has been considerablyreorganized and extends beyond the original clas-43 Emission43.1 Light Emissionbeam, glow, sparkle, .
.
.43.2 Sound Emissionblare, chime, jangle, .
.
.. .
.44 Destroyannihilate, destroy, ravage, .
.
.45 Change of State.
.
.47 Existence47.1 Existexist, persist, remain, .
.
.47.2 Entity-Specific Modes Beingbloom, breathe, foam, .
.
.47.3 Modes of Being with Motionjiggle, sway, waft, .
.
.. .
.Figure 1: VerbNet classes43.2 Sound EmissionTheme VTheme V P:loc LocationP:loc Location V Themethere V Theme P:loc LocationAgent V ThemeTheme V ObliqueLocation V with Theme47.3 Modes of Being with MotionTheme VTheme V P:loc LocationP:loc Location V Themethere V ThemeAgent V ThemeFigure 2: Syntactic frames for VerbNet classessification.
The classes therefore cover more En-glish verbs, and the classification should be moreconsistent (Korhonen and Briscoe, 2004; Kipperet al, 2006).The current version of VerbNet includes 270classes.1 Figure 1 shows a part of the classes ofVerbNet.
The top-level categories, e.g.
Emis-sion and Destroy, represent a coarse classifica-tion of verb semantics.
They are further classi-fied into verb classes, each of which expressesa group of verbs sharing syntactic frames.
Fig-ure 2 shows an excerpt from VerbNet, which rep-resents the possible syntactic frames for the SoundEmission class, including ?chime?
and ?jangle,?and the Modes of Being with Motion class, in-cluding ?jiggle?
and ?waft.?
In this figure, eachline represents a syntactic frame, where Agent,1Throughout this paper, we refer to VerbNet 2.3.
Sub-classes are ignored in this work, following the setting ofAbend et al (2008).1329. .
.
the walls still shook;VN=47.3 and an evacuationalarm blared;VN=43.2 outside.Suddenly the woman begins;VN=55.1 swaying;VN=47.3 and then .
.
.Figure 3: An excerpt from SemLinkTheme, and Location indicate the thematicroles, V denotes a verb, and P specifies a prepo-sition.
P:loc defines locative prepositions suchas: ?in?
and ?at.?
For example, the second syn-tactic frame of Sound Emission, i.e., Theme VP:loc Location, corresponds to the follow-ing sentence:1.
The coins jangled in my pocket.Theme corresponds to ?the coins,?
V to ?jangled,?P:loc to ?in,?
and Location to ?my pocket.
?While VerbNet provides associations betweenverbs and semantic classes, SemLink (Loper etal., 2007) additionally provides mappings amongVerbNet, FrameNet (Baker et al, 1998), PropBank(Palmer et al, 2005), and WordNet (Fellbaum,1998).
Since FrameNet and PropBank include an-notated instances of sentences, SemLink can beconsidered as a corpus annotated with VerbNetclasses.
Figure 3 presents some annotated sen-tences obtained from SemLink.
For example, theannotation ?blared;VN=43.2?
indicates that theoccurrence of ?blare?
in this context is classifiedas Sound Emission.2.2 Related workThere has been much research effort invested inthe automatic classification of verbs into lexicalsemantic classes, in a supervised or unsupervisedway.
The present work inherits the spirit of the su-pervised approaches to verb classification (Steven-son et al, 1999; Stevenson and Merlo, 1999;Merlo and Stevenson, 2001; Stevenson and Joanis,2003; Joanis and Stevenson, 2003; Joanis et al,2008).
Our learning framework basically followsthe above listed works: features are obtained froman unannotated (automatically parsed) corpus, andgold verb-class associations are used as traininginstances for machine learning classifiers, such asdecision trees and support vector machines.
How-ever, those works targeted a small subset of Levinclasses, and a limited number of monosemousverbs; for example, Merlo and Stevenson (2001)studied three classes and 59 verbs, and Joanis et al(2008) focused on 14 classes and 835 verbs.
Al-though these works provided a theoretical frame-work for supervised verb classification, their re-sults were not readily available for practical ap-plications, because of the limitation in the cover-age of the targeted classes/verbs on real texts.
Onthe contrary, we target the classification of arbi-trary verbs, including polysemous verbs, into allVerbNet classes (270 in total).
In this realistic sit-uation, we will empirically compare settings formodel learning, in order to explore effective con-ditions to obtain better models.Another difference from the aforementionedworks is that we aim at obtaining a probabilis-tic model, which represents saliences of classesof polysemous verbs.
Lapata and Brew (2004)and Li and Brew (2007) focused on this issue,and described methods for inducing probabilitiesof verb-class associations.
The obtained proba-bilistic model was intended to be incorporated intoa token-level disambiguation model.
Their meth-ods claimed to be unsupervised, meaning that theinduction of a probabilistic lexicon did not re-quire any hand-annotated corpora.
In fact, how-ever, their methods relied on the existence of afull-coverage lexicon, both in training and runningtime.
In their methods, a lexicon was necessaryfor restricting possible classes to which each wordbelongs.
Since most verbs are associated withonly a couple of classes, such a restriction signif-icantly reduces the search space, and the problembecomes much easier to solve.
This presupposi-tion is implicitly or explicitly used in other seman-tic disambiguation tasks (McCarthy et al, 2004),but it is unrealistic for practical applications.Clustering methods have also been extensivelyresearched for verb classification (Stevenson andMerlo, 1999; Schulte im Walde, 2000; McCarthy,2001; Korhonen, 2002; Korhonen et al, 2003;Schulte im Walde, 2003).
The extensive researchis in large part due to the intuition that the set ofclasses could not be fixed beforehand.
In partic-ular, it is often problematic to define a static setof semantic classes.
However, it is reasonable toassume that the set of VerbNet classes is fixed, be-cause Levin-type classes are more static than on-tological classes, like in WordNet synsets.
There-fore, we can apply supervised classification meth-ods to our task.
It is true that the current VerbNetclasses are imperfect and require revisions, but inthis work we adopt them as they are, because as1330time advances, more stable classifications will be-come available.The problem focused in this work has a close re-lationship with automatic thesaurus/ontology ex-pansion.
In fact, we evaluate our method in thetask of automatic verb classification, which canbe considered as lexicon expansion.
The mostprominent difference of the present work from the-saurus/ontology expansion is that the number ofclasses is much smaller in our problem, and the setof verb classes can be assumed to be fixed.
Thesecharacteristics indicate that our problem is easierand more well-defined than is the case for auto-matic thesaurus/ontology expansion.Supervised approaches to token-level verb classdisambiguation have recently been addressed(Girju et al, 2005; Abend et al, 2008), largely ow-ing to SemLink.
Their approaches fundamentallyfollow traditional supervised WSD methods: ex-tracting features representing the context in whichthe target word appears, and training a classifica-tion model with an annotated corpus.
While thoseworks achieved an impressive accuracy (more than95%), the results may not necessarily indicate themethod?s effectiveness; rather, it may imply theimportance of a lexicon.
In fact, these works re-strict their target to verb tokens, in which the cor-rect class exists in a given lexicon, and they onlyconsider candidate classes that are registered in thelexicon.
This setting reduces the ambiguity signif-icantly, and the problem becomes much easier tohandle; for example, approximately half of verbtokens are monosemous in their setting.
Thus, asimple baseline achieves very high accuracy fig-ures.
However, in our preliminary experimenton token-level verb classification with unknownverbs, we found that the accuracy for unknownverbs (i.e., lemmas not included in the VerbNetlexicon) is catastrophically low.
This indicatesthat VerbNet and SemLink are insufficient for un-known verbs, and that we cannot expect the avail-ability of a full-coverage lexicon in the real world.Instead of a static lexicon, our probabilistic modelis intended to be used as a prior distribution for thetoken-level disambiguation, as in Lapata and Brew(2004)?s model.3 A probabilistic model for verbsemantic classesIn this work, supervised learning is applied to theprobabilistic modeling of a lexicon of verb seman-tic classes.
We do not presuppose the existence ofa full-coverage lexicon; instead, we use an existinglexicon for the training data.
Combined with fea-tures extracted from unannotated corpora, a proba-bilistic model is learned from the existing lexicon.Like other supervised learning applications, ourprobabilistic lexicon can predict classes for wordsthat are not included in the original lexicon.Our model is defined in the following way.
Weassume that the set, C, of verb classes is fixed,while a set of verbs is unfixed.
With this assump-tion, probabilistic modeling can be reduced to aclassification problem.
Specifically, the goal is toobtain a probability distribution, p(c|v), of verbclass c ?
C for a given verb (lemma) v. Wecan therefore apply well-known supervised learn-ing methods to estimate p(c|v).This probability is modeled in the form of a log-linear model.p(c|v) =1Zexp(?i?ifi(c, v)),where fi(c, v) are features that represent charac-teristics of c and v, and ?iare model parametersthat express weights of the corresponding features.Model parameters can be estimated when train-ing instances, i.e., pairs ?c, v?, and features,fi(c, v), for each instance are given.
Therefore,what we have to do is to prepare the training in-stances ?c, v?, and effective features fi(c, v) thatcontribute to the better estimation of probabili-ties.
In token tagging tasks, both training instancesand features are extracted from annotated corpora.However, since our goal is the probabilistic mod-eling of a lexicon, we have to determine how toderive the training instances and features for lexi-con entries, to be discussed in the next section.For the parameter estimation of log-linear mod-els, we applied the stochastic gradient descentmethod.
A hyperparameter for l2-regularizationwas tuned to minimize the KL-divergence (seeSection 4.4) for the development set.4 Experiment designIn this work, we empirically compare several set-tings for the learning of the above probabilisticmodel, in the two tasks of automatic verb classi-fication.
In what follows, we explain the train-ing/test data, corpora for extracting features, andthe design of the features and evaluation tasks.The measures for evaluation are also introduced.13311 sound_emission-43.2 chime0.5 sound_emission-43.2 blare0.5 manner_speaking-37.3 blare0.5 modes_of_being_with_motion-47.3 sway0.5 urge-58.1 sway1 sound_emission-43.2 chime0.7 sound_emission-43.2 blare0.3 manner_speaking-37.3 blare0.6 modes_of_being_with_motion-47.3 sway0.4 urge-58.1 swayFigure 4: Training instances obtained from Verb-Net (upper) and VerbNet+SemLink (lower)4.1 DataAs our goal is the supervised learning of a lexiconof verb semantic classes, VerbNet is used as thetraining/test data.
In addition, since we aim at rep-resenting the saliences of verb-class associationswith probabilities, the gold probabilities are nec-essary.
For this purpose, we count the occurrencesof each verb-class association in the VerbNet-PropBank token mappings in the subset of theSemLink corresponding to sections 2 through 21of Penn Treebank (Marcus et al, 1994).
Fre-quency counts are normalized for each lemma,with the Laplace smoothing (the parameter is 0.5).In this work, we compare the two settings forcreating training instances.
By comparing the re-sults of these settings, we evaluate the necessityof an annotated corpus for learning a probabilisticlexicon of verb semantic classes.VerbNet We collect all ?c, v?
pairs registered inVerbNet.
For each v, all of the associatedclasses are assigned equal weights (see theupper part of Figure 4).VerbNet+SemLink Each pair ?c, v?
in VerbNetis weighted by the normalized frequency ob-tained from SemLink (see the lower part ofFigure 4).Because VerbNet classes represent groups ofsyntactic frames, and it is impossible to guess theverb class by referring to only one occurrence ina text, it is necessary to have statistics over a suf-ficient amount of a corpus.
Hence, features areextracted from a large unannotated corpus.
In thispaper, we use the following two corpora:WSJ Wall Street Journal newspaper articles(around 40 million words).BNC British National Corpus, which is a bal-anced corpus of around 100 million words.In addition to the variance of the corpus domains,we vary the size of the corpus to observe the ef-fect of increasing the corpus size.
These corporaare automatically parsed by Enju 2.3.1 (Miyao andTsujii, 2008), and the features are extracted fromthe parsing results.4.2 FeaturesLevin-like classes, including VerbNet, are de-signed to represent distinctions in syntactic framesand alternations.
Hence, if we were given the per-fect knowledge of the possible syntactic frames,verbs can be classified into the correct classes al-most perfectly (Dorr and Jones, 1996).
Previ-ous works thus proposed features that express thecorpus statistics of syntactic frames.
However,class boundaries are subtle in some cases; severalclasses share syntactic frames with each other to alarge extent.For example, the classes shown in Figure 2 havevery similar syntactic frames.
The difference is in-dicated in the last two frames of Sound Emission,although they appear much less frequently in realtexts.
Therefore, it is difficult to accurately capturethe distinctions between these classes, if we areonly provided with the statistics of the syntacticframes that appear in real texts.
In this case, how-ever, it is easy to observe that the verbs of theseclasses have different selectional preferences; thatis, the Theme of Sound Emission verbs wouldbe objects that make sounds, while the Theme ofModes of Being with Motion is likely to be ob-jects that move.2 Although Levin?s classificationinitially focused on syntactic alternations, the re-sulting classes represent some semantic common-alities.
Hence, it would be reasonable to designfeatures that capture such semantic characteristics.In this work, we re-implemented the followingfeatures proposed by Joanis et al (2008) as thestarting point.Syntactic slot Features to count the occurrencesof each syntactic slot, such as subject, ob-ject, and prepositional phrases.
For the sub-ject slot, we also count its transitive and in-transitive usages separately.
Additionally, wecount the appearances of reflexive pronounsand semantically empty constituents (it and2Syntactic frames in VerbNet include specifications of se-lectional preferences, such as animate and place, althoughwe do not explicitly use them, because it is not apparent todetermine the members of these semantic classes.1332Syntactic slot subj:0.885intrans-subj:0.578Slot overlap overlap-subj-obj:0.299overlap-obj-in:0.074Tense, voice, aspect pos-VBG:0.307pos-VBD:0.290Animacy anim-subj:0.244anim-obj:0.057Slot POS subj-PRP:0.270subj-NN:0.270Syntactic frame NP_V:0.326NP_V_NP:0.307Similar word sim-rock:0.090sim-swing:0.083Slot class subj-C82:0.219obj-C12:0.081Figure 5: Example of features for ?sway?there).
Differently from Joanis et al (2008),we consider non-nominal arguments, such assentential and adjectival complements.Slot overlap Features to measure the overlap inwords (lemmas) between two syntactic slotsof the verb.
They are intended to approxi-mate argument alternations, such as the erga-tive alternation.
For example, for the alter-nation ?The sky cleared?/?The clouds clearedfrom the sky,?
a feature to indicate the overlapbetween the subject slot and the from slot isadded (Joanis et al, 2008).
The value of thisfeature is computed by the method of Merloand Stevenson (2001).Tense, voice, aspect Features to approximate thetendency of the tense, voice, and aspect ofthe target verb.
The Penn Treebank POS tagsfor verbs (VB, VBP, VBZ, VBG, VBD, andVBN) are counted.
In addition, included arethe frequency of the co-occurrences with anadverb or an auxiliary verb, and the count ofusages as a noun or an adjective.Animacy Features to measure the frequency ofanimate arguments for each syntactic slot.Personal pronouns except it are counted asanimate, following Joanis et al (2008), whilenamed entity recognition was not used.Examples of these features are shown in Figure 5.For details, refer to Joanis et al (2008).The above features mainly represent syntacticbehaviors of target verbs.
Since our target classesare broader than in the previous works, we furtherenhance the syntactic features.
Additionally, asdiscussed above, semantically motivated featuresmay present strong clues to distinguish amongsyntactically similar classes.
We therefore includethe following four types of feature; the first twoare syntactic, while the other two are intended tocapture semantic characteristics:Slot POS In addition to the syntactic slot fea-tures, we add features that represent a com-bination of a syntactic slot and the POS ofits head word.
Since VerbNet includes ex-tended classes that take verbal and adjecti-val arguments, the POSs of arguments wouldprovide a strong clue to discriminate amongthese syntactic frames.Syntactic frame The number of arguments andtheir syntactic categories.
This feature wasmentioned as a baseline in Joanis et al(2008), but we include it in our model.Similar word Similar words (lemmas) to the tar-get verb.
Similar words are automaticallyobtained from a corpus (the same corpus asused for feature extraction) by Lin (1998)?smethod.
This feature is motivated by thehypothesis that distributionally similar wordstend to be classified into the same class.
Be-cause Lin?s method is based on the similar-ity of words in syntactic slots, the obtainedsimilar words are expected to represent a verbclass that share selectional preferences.Slot class Semantic classes of the head words ofthe arguments.
This feature is also intendedto approximate selectional preferences.
Thesemantic classes are obtained by clusteringnouns, verbs, and adjectives into 200, 100,and 50 classes respectively, by using the k-medoid method with Lin (1998)?s similarity.Figure 5 shows an example of the features for?sway,?
extracted from the BNC corpus.3 Featurevalues are defined as relative frequencies for eachlemma; while, for similar word features, featurevalues are weighted by Lin?s similarity measure.4.3 TasksWe evaluate our model in the tasks of auto-matic verb classification (a.k.a.
lexicon expan-sion): given gold verb-class associations for someset of verbs, we predict the classes for unknown3?C82?
and ?C12?
are automatically assigned clusternames.1333Verb class Levin class numberRecipient 13.1, 13.3Admire 31.2Amuse 31.1Run 51.3.2Sound Emission 43.2Light and Substance Emission 43.1, 43.4Cheat 10.6Steal and Remove 10.5, 10.1Wipe 10.4.1, 10.4.2Spray/Load 9.7Fill 9.8Other Verbs of Putting 9.1?6Change of State 45.1?4Object Drop 26.1, 26.3, 26.7Table 1: 14 classes used in Joanis et al (2008) andtheir corresponding Levin class numbersverbs.
While our main target is the full set of Verb-Net classes, we also show results for the task stud-ied in the previous work.14-class task The task to classify (almost)monosemous verbs into 14 classes.
Refer toTable 1 for the definition of the 14 classes.Following Joanis et al (2008)?s task def-inition, we removed verbs that belong tomultiple classes in these 14 classes, and alsoremoved overly polysemous verbs (in ourexperiment, verb-class associations that havethe relative frequency that is less than 0.5in SemLink are removed).
For each class,member verbs are randomly split into 50%(training), 25% (development), and 25%(final test) sets.All-class task The task to classify all target verbsinto 268 classes.4 Any verbs that did notoccur at least 100 times in the BNC cor-pus were removed.5 The remaining verbs(2517 words) are randomly split into 80%(training), 10% (development), and 10% (fi-nal test) sets, under the constraint that at leastone instance for each class is included in thetraining set.64.4 Evaluation measuresFor the 14-class task, we simply measure the clas-sification accuracy.
However, the evaluation in the4Two classes (Being Dressed and Debone) are not used inthe experiments because no lemmas belonged to these classesafter filtering by the frequency in BNC.5This is the same preprocessing as Joanis et al (2008),although we use VerbNet, while Joanis et al (2008) used theoriginal Levin classifications.6Because polysemous verbs belong to multiple classes,the class-wise data split was not adopted for the all-class task.all-class task is not trivial, because verbs may beassigned multiple classes.Since our purpose is to obtain a probabilisticmodel rather than to classify monosemous verbs,the evaluation criterion should be sensitive to theprobabilistic distribution on the test data.
In thispaper, we adopt two evaluation measures.
Oneis the top-N weighted accuracy; we count thenumber of correct pairs ?c, v?
in the N -best out-puts from the model (where N is the number ofgold classes for each lemma), where each count isweighted by the relative frequency (i.e., the countsin SemLink) of the pair in the test set.
For exam-ple, in the case for ?blare?
in Figure 4, if the modelstates that Sound Emission has the largest prob-ability, we get 0.7 points.
If Manner Speakinghas the largest probability, we instead obtain 0.3points.
Intuitively, the score is higher when themodel presents larger probabilities to classes withhigher relative frequencies.
This measure is simi-lar to the top-N precision in information retrieval;it evaluates the ranked output by the model.
Itis intuitively interpretable, but is insufficient forevaluating the quality of probability distributions.The other measure is KL-divergence, which ispopularly used for measuring the dissimilarity be-tween two probability distributions.
This is de-fined as follows:KL(p||q) =?xp(x) log(p(x))?
p(x) log(q(x)).In the experiments, this measure is applied, withthe assumption that p is the relative frequencyof ?c, v?
in the test set, and that q is the esti-mated probability distribution.
Although the KL-divergence is not a true distance metric, it is suf-ficient for measuring the fitting of the estimatedmodel to the true distribution.
We report theKL-divergence averaged over all verbs in the testset.
Since this measure indicates a dissimilarity, asmaller value is better.
When p and q are equiva-lent, KL(p||q) = 0.5 Experimental resultsTable 2 shows the accuracy obtained for the 14-class task.
The first column denotes the incorpo-rated features (?Joanis et al?s features?
or ?All fea-tures?
), and the sources of the features (?WSJ?
or?BNC?).
The two baseline results are also given:?Baseline (random)?
indicates that classes are ran-domly output, and ?Baseline (majority)?
indicates1334AccuracyBaseline (random) 7.14Baseline (majority) 26.47Joanis et al?s features/WSJ 56.86Joanis et al?s features/BNC 64.22All features/WSJ 60.29All features/BNC 68.14Table 2: Accuracy for the 14-class taskAccuracy KLBaseline (random) 0.37 ?Baseline (majority) 8.69 ?Joanis et al?s features/WSJ 30.26 3.65Joanis et al?s features/BNC 35.66 3.32All features/WSJ 34.07 3.37All features/BNC 42.54 2.99Table 3: Accuracy and KL-divergence for the all-class task (the VerbNet+SemLink setting)that the majority class (i.e., the class that has thelargest number of member verbs) is output to everylemma.
While these figures cannot be compareddirectly to the previous works due to the differencein the preprocessing, Joanis et al (2008) achieved58.4% accuracy for the 14-class task.
Table 3 and4 present the results for the all-class task.
Table 3gives the accuracy and KL-divergence achievedby the model trained with the VerbNet+SemLinktraining instances, while Table 4 presents the samemeasures by the training instances created fromVerbNet only.Our models performed substantially better onboth tasks than the baseline models.
The resultsalso proved that the features we proposed in thispaper contributed to the further improvement ofthe model from Joanis et al (2008).
In the all-classtask with the VerbNet+SemLink setting, our fea-tures achieved 10.69% error reduction in the accu-racy over Joanis et al (2008)?s features.
Anotherinteresting fact is that the model with BNC con-sistently outperformed the model with WSJ.
Thisoutcome is somewhat surprising, provided that therelative frequencies in the training/test sets are cre-ated from the WSJ portion of SemLink.
The rea-son for this is independent of the corpus size, aswill be shown below.
When comparing Table 3and 4, we can see that using SemLink statisticsresulted in a slightly better model.
This resultis predictable, because the evaluation measuresare sensitive to the relative frequencies estimatedfrom SemLink.
However, the difference remainedsmall.
In both of the tasks and the evaluation mea-sures, the best model was achieved when we useAccuracy KLBaseline (random) 0.37 ?Baseline (majority) 8.69 ?Joanis et al?s features/WSJ 29.65 3.67Joanis et al?s features/BNC 35.78 3.34All features/WSJ 34.53 3.40All features/BNC 42.38 3.02Table 4: Accuracy and KL-divergence for the all-class task (the VerbNet only setting)010203040500  20  40  60  80  100AccuracyCorpus size (M words)Accuracy (Joanis et al?s features, WSJ)Accuracy (Joanis et al?s features, BNC)Accuracy (all features, WSJ)Accuracy (all features, BNC)Figure 6: Corpus size vs. accuracyall the features extracted from BNC, and createtraining instances from VerbNet+SemLink.Figure 6 and 7 plot the accuracy and KL-divergence against the size of the unannotated cor-pus used for feature extraction.
The result clearlyindicates that the learning curve still grows at thecorpus size with 100 million words (especially forthe all features + BNC setting), which indicatesthat better models are obtained by increasing thesize of the unannotated corpora.Therefore, we can claim that the differences be-tween the domains and the size of the unannotatedcorpora are more influential than the availability ofthe annotated corpora.
This indicates that learningonly from a lexicon would be a viable solution,when a token-disambiguated corpus like SemLinkis unavailable.Table 5 shows the contribution of each featuregroup.
BNC is used for feature extraction, andVerbNet+SemLink is used for the creation of train-ing instances.
The results demonstrated the effec-tiveness of the slot POS features, and in particular,for the all-class task, most likely because Verb-Net covers verbs that take non-nominal arguments.Additionally, the similar word features contributedequally or more in both of the tasks.
This resultsuggests that we were reasonable in hypothesizingthat distributionally similar words tend to be clas-13352.533.544.50  20  40  60  80  100KL-divergenceCorpus size (M words)KL (Joanis et al?s features, WSJ)KL (Joanis et al?s features, BNC)KL (all features, WSJ)KL (all features, BNC)Figure 7: Corpus size vs. KL-divergence14-classes All classesAccuracy Accuracy KLBaseline (random) 7.14 0.37 ?Baseline (majority) 26.47 8.69 ?Joanis et al?s features 64.22 35.66 3.32+ Slot POS 66.67 38.77 3.18+ Syntactic frame 64.71 35.99 3.29+ Similar word 68.14 37.88 3.10+ Slot class 64.71 36.51 3.26All features 68.14 42.54 2.99Table 5: Contribution of featuressified into the same class.
Slot classes also con-tributed to a slight improvement, indicating thatselectional preferences are effective clues for pre-dicting VerbNet classes.
The result of the ?All fea-tures?
model for the all-class task attests that thesefeatures worked collaboratively, and using themall resulted in a considerably better model.From the analysis of the confusion matrix forthe outputs by our best model, we identified sev-eral reasons for the remaining misclassification er-rors.
A major portion of the errors were caused byconfusing the classes that take the same preposi-tions.
Examples of these errors include:?
Other Change of State verbs were misclas-sified into the Butter class: ?embalm,?
?lam-inate.?
(they take ?with?
phrases)?
Judgement verbs were misclassified into theCharacterize class: ?acclaim,?
?hail.?
(theytake ?as?
phrases)Since prepositions are strong features for auto-matic verb classification (Joanis et al, 2008), theclasses that take the same prepositions remainedconfusing.
The discovery of the features to dis-criminate among these classes would be crucial forfurther improvement.Another major error is in classifying verbs intoOther Change of State.
Examples include:?
Amuse verbs: ?impair,?
?recharge.??
Herd verbs: ?aggregate,?
?mass.
?Because Other Change of State is one of thebiggest classes, supervised learning tends to placea high probability to this class.
Therefore, whenstrong clues do not exist, verbs tend to be mis-classified into this class.
In addition, this class isnot syntactically/semantically homogeneous, andis likely to introduce noise in the machine learn-ing classifier.
A possible solution to this problemwould be to exclude this class from the classifica-tion, and to process the class separately.6 ConclusionsWe presented a method for the supervised learn-ing of a probabilistic model for a lexicon of Verb-Net classes.
By combining verb-class associa-tions from VerbNet and SemLink, and features ex-tracted from a large unannotated corpus, we couldsuccessfully train a log-linear model in a super-vised way.
The experimental results attested toour success that features proposed in this paperworked effectively in obtaining a better probabil-ity distribution.
Not only syntactic features, butalso semantic features were shown to be effective.While each of these features could increase the ac-curacy, they collaboratively contributed to a largeimprovement.
In the all-class task, we obtained10.69% error reduction in the classification accu-racy over Joanis et al (2008)?s model.
We also ob-served the trend that a larger corpus for feature ex-traction led to a better model, indicating that a bet-ter model will be obtained by increasing the size ofan unannotated corpus.We could identify the effective features and set-tings for this problem, but the classification intoall VerbNet classes remained challenging.
Onepossible direction for this research topic would beto use our model for the semi-automatic construc-tion of verb lexicons, with the help of human cura-tion.
However, there is also a demand for explor-ing other types of features that can discriminateamong confusing classes.AcknowledgmentsThis work was partially supported by Grant-in-Aid for Specially Promoted Research and Grant-in-Aid for Young Scientists (MEXT, Japan).1336ReferencesOmri Abend, Roi Reichart, and Ari Rappoport.
2008.A supervised algorithm for verb disambiguation intoVerbNet classes.
In Proceedings of COLING 2008,pages 9?16.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of COLING-ACL 1998.Bonnie J. Dorr and Doug Jones.
1996.
Role of wordsense disambiguation in lexical acquisition: Predict-ing semantics from syntactic cues.
In Proceedingsof COLING-96, pages 322?327.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,Massachusetts.Roxana Girju, Dan Roth, and Mark Sammons.
2005.Token-level disambiguation of VerbNet classes.
InThe Interdisciplinary Workshop on Verb Featuresand Verb Classes.Eric Joanis and Suzanne Stevenson.
2003.
A generalfeature space for automatic verb classification.
InProceedings of EACL 2003, pages 163?170.Eric Joanis, Suzanne Stevenson, and David James.2008.
A general feature space for automaticverb classification.
Natural Language Engineering,14(3):337?367.Karin Kipper, Hoa Trang Dang, and Martha Palmer.2000.
Class-based construction of a verb lexicon.In Proceedings of 17th National Conference on Ar-tificial Intelligence.Karin Kipper, Anna Korhonen, Neville Ryant, andMartha Palmer.
2006.
Extending VerbNet withnovel verb classes.
In Proceedings of LREC 2006.Karin Kipper-Schuler.
2005.
VerbNet: A broad-coverage, comprehensive verb lexicon.
Ph.D. the-sis, Computer and Information Science Department,University of Pennsylvania.Anna Korhonen and Ted Briscoe.
2004.
Extendedlexical-semantic classification of English verbs.
InProceedings of the HLT/NAACL Workshop on Com-putational Lexical Semantics.Anna Korhonen, Yuval Krymolowski, and ZvikaMarx.
2003.
Clustering polysemic subcategoriza-tion frame distributions semantically.
In Proceed-ings of ACL 2003.Anna Korhonen.
2002.
Semantically motivatedsubcategorization acquisition.
In Proceedings ofthe Workshop on Unsupervised Lexical Acquisition,pages 51?58.Mirella Lapata and Chris Brew.
2004.
Verb classdisambiguation using informative priors.
Computa-tional Linguistics, 30(1):45?75.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press, Chicago.Juanguo Li and Chris Brew.
2007.
DisambiguatingLevin verbs using untagged data.
In Proceedings ofRANLP 2007.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING-ACL1998.Edward Loper, Szu ting Yi, and Martha Palmer.
2007.Combining lexical resources: Mapping betweenPropBank and VerbNet.
In Proceedings of the 7thInternational Workshop on Computational Linguis-tics, Tilburg, the Netherlands.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant senses in un-tagged text.
In Proceedings of ACL 2004.Diana McCarthy.
2001.
Lexical Acquisition at theSyntax-Semantics Interface: Diathesis Alternations,Subcategorization Frames and Selectional Prefer-ences.
Ph.D. thesis, University of Sussex.Paola Merlo and Suzanne Stevenson.
2001.
Auto-matic verb-classification based on statistical distri-bution of argument structure.
Computational Lin-guistics, 27(3):373?408.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature for-est models for probabilistic HPSG parsing.
Compu-tational Linguistics, 34(1):35?80.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpusof semantic roles.
Computational Linguistics, 31(1).Sabine Schulte im Walde.
2000.
Clustering verbs se-mantically according to their alternation behavior.In Proceedings of COLING 2000, pages 747?753.Sabine Schulte im Walde.
2003.
Experiments on thechoice of features for learning verb classes.
In Pro-ceedings of EACL 2003, pages 315?322.Suzanne Stevenson and Eric Joanis.
2003.
Semi-supervised verb class discovery using noisy features.In Proceedings of CoNLL 2003, pages 71?78.Suzanne Stevenson and Paola Merlo.
1999.
Automaticverb classification using grammatical features.
InProceedings of EACL 1999, pages 45?52.Suzanne Stevenson, Paola Merlo, Natalia Kariaeva,and Kamin Whitehouse.
1999.
Supervised learningof lexical semantic verb classes using frequency dis-tributions.
In Proceedings of SigLex99: Standardiz-ing Lexical Resources, pages 15?22.1337
