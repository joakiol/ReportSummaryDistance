Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 296?303,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsRegression for Sentence-Level MT Evaluation with Pseudo ReferencesJoshua S. Albrecht and Rebecca HwaDepartment of Computer ScienceUniversity of Pittsburgh{jsa8,hwa}@cs.pitt.eduAbstractMany automatic evaluation metrics for ma-chine translation (MT) rely on making com-parisons to human translations, a resourcethat may not always be available.
We presenta method for developing sentence-level MTevaluation metrics that do not directly relyon human reference translations.
Our met-rics are developed using regression learn-ing and are based on a set of weaker indi-cators of fluency and adequacy (pseudo ref-erences).
Experimental results suggest thatthey rival standard reference-based metricsin terms of correlations with human judg-ments on new test instances.1 IntroductionAutomatic assessment of translation quality is achallenging problem because the evaluation task, atits core, is based on subjective human judgments.Reference-based metrics such as BLEU (Papineniet al, 2002) have rephrased this subjective task asa somewhat more objective question: how closelydoes the translation resemble sentences that areknown to be good translations for the same source?This approach requires the participation of humantranslators, who provide the ?gold standard?
refer-ence sentences.
However, keeping humans in theevaluation loop represents a significant expenditureboth in terms of time and resources; therefore it isworthwhile to explore ways of reducing the degreeof human involvement.To this end, Gamon et al (2005) proposed alearning-based evaluation metric that does not com-pare against reference translations.
Under a learn-ing framework, the input (i.e., the sentence to beevaluated) is represented as a set of features.
Theseare measurements that can be extracted from the in-put sentence (and may be individual metrics them-selves).
The learning algorithm combines the fea-tures to form a model (a composite evaluation met-ric) that produces the final score for the input.
With-out human references, the features in the model pro-posed by Gamon et al were primarily languagemodel features and linguistic indicators that could bedirectly derived from the input sentence alone.
Al-though their initial results were not competitive withstandard reference-based metrics, their studies sug-gested that a referenceless metric may still provideuseful information about translation fluency.
How-ever, a potential pitfall is that systems might ?gamethe metric?
by producing fluent outputs that are notadequate translations of the source.This paper proposes an alternative approach toevaluate MT outputs without comparing against hu-man references.
While our metrics are also trained,our model consists of different features and istrained under a different learning regime.
Crucially,our model includes features that capture some no-tions of adequacy by comparing the input againstpseudo references: sentences from other MT sys-tems (such as commercial off-the-shelf systems oropen sourced research systems).
To improve flu-ency judgments, the model also includes featuresthat compare the input against target-language ?ref-erences?
such as large text corpora and treebanks.Unlike human translations used by standardreference-based metrics, pseudo references are not296?gold standards?
and can be worse than the sen-tences being evaluated; therefore, these ?references?in-and-of themselves are not necessarily informativeenough for MT evaluation.
The main insight of ourapproach is that through regression, the trained met-rics can make more nuanced comparisons betweenthe input and pseudo references.
More specifically,our regression objective is to infer a function thatmaps a feature vector (which measures an input?ssimilarity to the pseudo references) to a score thatindicates the quality of the input.
This is achieved byoptimizing the model?s output to correlate against aset of training examples, which are translation sen-tences labeled with quantitative assessments of theirquality by human judges.
Although this approachdoes incur some human effort, it is primarily for thedevelopment of training data, which, ideally, can beamortized over a long period of time.To determine the feasibility of the proposed ap-proach, we conducted empirical studies that com-pare our trained metrics against standard reference-based metrics.
We report three main findings.First, pseudo references are informative compar-ison points.
Experimental results suggest thata regression-trained metric that compares againstpseudo references can have higher correlations withhuman judgments than applying standard metricswith multiple human references.
Second, the learn-ing model that uses both adequacy and fluency fea-tures performed the best, with adequacy being themore important factor.
Third, when the pseudo ref-erences are multiple MT systems, the regression-trained metric is predictive even when the input isfrom a better MT system than those providing thereferences.
We conjecture that comparing MT out-puts against other imperfect translations allows for amore nuanced discrimination of quality.2 Background and Related WorkFor a formally organized event, such as the annualMT Evaluation sponsored by National Institute ofStandard and Technology (NIST MT Eval), it maybe worthwhile to recruit multiple human translatorsto translate a few hundred sentences for evaluationreferences.
However, there are situations in whichmultiple human references are not practically avail-able (e.g., the source may be of a large quantity, andno human translation exists).
One such instance istranslation quality assurance, in which one wishesto identify poor outputs in a large body of machinetranslated text automatically for human to post-edit.Another instance is in day-to-day MT research anddevelopment, where new test set with multiple ref-erences are also hard to come by.
One could workwith previous datasets from events such as the NISTMT Evals, but there is a danger of over-fitting.
Onealso could extract a single reference from parallelcorpora, although it is known that automatic metricsare more reliable when comparing against multiplereferences.The aim of this work is to develop a trainable au-tomatic metric for evaluation without human refer-ences.
This can be seen as a form of confidence esti-mation on MT outputs (Blatz et al, 2003; Ueffing etal., 2003; Quirk, 2004).
The main distinction is thatconfidence estimation is typically performed with aparticular system in mind, and may rely on system-internal information in estimation.
In this study, wedraw on only system-independent indicators so thatthe resulting metric may be more generally applied.This allows us to have a clearer picture of the con-tributing factors as they interact with different typesof MT systems.Also relevant is previous work that applied ma-chine learning approaches to MT evaluation, bothwith human references (Corston-Oliver et al, 2001;Kulesza and Shieber, 2004; Albrecht and Hwa,2007; Liu and Gildea, 2007) and without (Gamon etal., 2005).
One motivation for the learning approachis the ease of combining multiple criteria.
Literaturein translation evaluation reports a myriad of criteriathat people use in their judgments, but it is not clearhow these factors should be combined mathemati-cally.
Machine learning offers a principled and uni-fied framework to induce a computational model ofhuman?s decision process.
Disparate indicators canbe encoded as one or more input features, and thelearning algorithm tries to find a mapping from inputfeatures to a score that quantifies the input?s qualityby optimizing the model to match human judgmentson training examples.
The framework is attractivebecause its objective directly captures the goal ofMT evaluation: how would a user rate the qualityof these translations?This work differs from previous approaches in297two aspects.
One is the representation of the model;our model treats the metric as a distance measureeven though there are no human references.
An-other is the training of the model.
More so thanwhen human references are available, regression iscentral to the success of the approach, as it deter-mines how much we can trust the distance measuresagainst each pseudo reference system.While our model does not use human referencesdirectly, its features are adapted from the followingdistance-based metrics.
The well-known BLEU (Pa-pineni et al, 2002) is based on the number of com-mon n-grams between the translation hypothesis andhuman reference translations of the same sentence.Metrics such as ROUGE, Head Word Chain (HWC),METEOR, and other recently proposed methods alloffer different ways of comparing machine and hu-man translations.
ROUGE utilizes ?skip n-grams?,which allow for matches of sequences of words thatare not necessarily adjacent (Lin and Och, 2004a).METEOR uses the Porter stemmer and synonym-matching via WordNet to calculate recall and pre-cision more accurately (Banerjee and Lavie, 2005).The HWC metrics compare dependency and con-stituency trees for both reference and machine trans-lations (Liu and Gildea, 2005).3 MT Evaluation with Pseudo Referencesusing RegressionReference-based metrics are typically thought of asmeasurements of ?similarity to good translations?because human translations are used as references,but in more general terms, they are distance mea-surements between two sentences.
The distance be-tween a translation hypothesis and an imperfect ref-erence is still somewhat informative.
As a toy ex-ample, consider a one-dimensional line segment.
Adistance from the end-point uniquely determines theposition of a point.
When the reference location isanywhere else on the line segment, a relative dis-tance to the reference does not uniquely specify alocation on the line segment.
However, the positionof a point can be uniquely determined if we are givenits relative distances to two reference locations.The problem space for MT evaluation, thoughmore complex, is not dissimilar to the toy scenario.There are two main differences.
First, we do notknow the actual distance function ?
this is what weare trying to learn.
The distance functions we haveat our disposal are all heuristic approximations to thetrue translational distance function.
Second, unlikehuman references, whose quality value is assumed tobe maximum, the quality of a pseudo reference sen-tence is not known.
In fact, prior to training, we donot even know the quality of the reference systems.Although the direct way to calibrate a reference sys-tem is to evaluate its outputs, this is not practicallyideal, since human judgments would be needed eachtime we wish to incorporate a new reference system.Our proposed alternative is to calibrate the referencesystems against an existing set of human judgmentsfor a range of outputs from different MT systems.That is, if many of the reference system?s outputsare similar to those MT outputs that received lowassessments, we conclude this reference system maynot be of high quality.
Thus, if a new translation isfound to be similar with this reference system?s out-put, it is more likely for the new translation to alsobe bad.Both issues of combining evidences from heuris-tic distances and calibrating the quality of pseudoreference systems can be addressed by a probabilis-tic learning model.
In particular, we use regressionbecause its problem formulation fits naturally withthe objective of MT evaluations.
In regression learn-ing, we are interested in approximating a function fthat maps a multi-dimensional input vector, x, to acontinuous real value, y, such that the error over a setof m training examples, {(x1, y1), .
.
.
, (xm, ym)},is minimized according to a loss function.In the context of MT evaluation, y is the ?true?quantitative measure of translation quality for an in-put sentence1.
The function f represents a mathe-matical model of human judgments of translations;an input sentence is represented as a feature vector,x, which contains the information that can be ex-tracted from the input sentence (possibly includingcomparisons against some reference sentences) thatare relevant to computing y.
Determining the set ofrelevant features for this modeling is on-going re-1Perhaps even more so than grammaticality judgments, thereis variability in people?s judgments of translation quality.
How-ever, like grammaticality judgments, people do share some sim-ilarities in their judgments at a coarse-grained level.
Ideally,what we refer to as the true value of translational quality shouldreflect the consensus judgments of all people.298search.
In this work, we consider some of the morewidely used metrics as features.
Our full featurevector consists of r ?
18 adequacy features, wherer is the number of reference systems used, and 26fluency features:Adequacy features: These include features de-rived from BLEU (e.g., n-gram precision, where1 ?
n ?
5, length ratios), PER, WER, fea-tures derived from METEOR (precision, recall,fragmentation), and ROUGE-related features (non-consecutive bigrams with a gap size of g, where1 ?
g ?
5 and longest common subsequence).Fluency features: We consider both string-levelfeatures such as computing n-gram precision againsta target-language corpus as well as several syntax-based features.
We parse each input sentence into adependency tree and compared aspects of it against alarge target-language dependency treebank.
In addi-tion to adapting the idea of Head Word Chains (Liuand Gildea, 2005), we also compared the input sen-tence?s argument structures against the treebank forcertain syntactic categories.Due to the large feature space to explore, wechose to work with support vector regression as thelearning algorithm.
As its loss function, support vec-tor regression uses an ?-insensitive error function,which allows for errors within a margin of a smallpositive value, ?, to be considered as having zero er-ror (cf.
Bishop (2006), pp.339-344).
Like its classi-fication counterpart, this is a kernel-based algorithmthat finds sparse solutions so that scores for new testinstances are efficiently computed based on a subsetof the most informative training examples.
In thiswork, Gaussian kernels are used.The cost of regression learning is that it requirestraining examples that are manually assessed by hu-man judges.
However, compared to the cost of cre-ating new references whenever new (test) sentencesare evaluated, the effort of creating human assess-ment training data is a limited (ideally, one-time)cost.
Moreover, there is already a sizable collectionof human assessed data for a range of MT systemsthrough multiple years of the NIST MT Eval efforts.Our experiments suggest that there is enough as-sessed data to train the proposed regression model.Aside from reducing the cost of developing hu-man reference translations, the proposed metric alsoprovides an alternative perspective on automatic MTevaluation that may be informative in its own right.We conjecture that a metric that compares inputsagainst a diverse population of differently imperfectsentences may be more discriminative in judgingtranslation systems than solely comparing againstgold standards.
That is, two sentences may beconsidered equally bad from the perspective of agold standard, but subtle differences between themmay become more prominent if they are comparedagainst sentences in their peer group.4 ExperimentsWe conducted experiments to determine the feasibil-ity of the proposed approach and to address the fol-lowing questions: (1) How informative are pseudoreferences in-and-of themselves?
Does varying thenumber and/or the quality of the references have animpact on the metrics?
(2) What are the contribu-tions of the adequacy features versus the fluency fea-tures to the learning-based metric?
(3) How do thequality and distribution of the training examples, to-gether with the quality of the pseudo references, im-pact the metric training?
(4) Do these factors impactthe metric?s ability in assessing sentences producedwithin a single MT system?
How does that system?squality affect metric performance?4.1 Data preparation and Experimental SetupThe implementation of support vector regressionused for these experiments is SVM-Light (Joachims,1999).
We performed all experiments using the 2004NIST Chinese MT Eval dataset.
It consists of 447source sentences that were translated by four hu-man translators as well as ten MT systems.
Eachmachine translated sentence was evaluated by twohuman judges for their fluency and adequacy on a5-point scale2.
To remove the bias in the distribu-tions of scores between different judges, we followthe normalization procedure described by Blatz etal.
(2003).
The two judge?s total scores (i.e., sumof the normalized fluency and adequacy scores) arethen averaged.2The NIST human judges use human reference translationswhen making assessments; however, our approach is generallyapplicable when the judges are bilingual speakers who comparesource sentences with translation outputs.299We chose to work with this NIST dataset becauseit contains numerous systems that span over a rangeof performance levels (see Table 1 for a ranking ofthe systems and their averaged human assessmentscores).
This allows us to have control over the vari-ability of the experiments while answering the ques-tions we posed above (such as the quality of the sys-tems providing the pseudo references, the quality ofMT systems being evaluated, and the diversity overthe distribution of training examples).Specifically, we reserved four systems (MT2,MT5, MT6, and MT9) for the role of pseudo ref-erences.
Sentences produced by the remaining sixsystems are used as evaluative data.
This set in-cludes the best and worst systems so that we can seehow well the metrics performs on sentences that arebetter (or worse) than the pseudo references.
Met-rics that require no learning are directly applied ontoall sentences of the evaluative set.
For the learning-based metrics, we perform six-fold cross validationon the evaluative dataset.
Each fold consists of sen-tences from one MT system.
In a round robin fash-ion, each fold serves as the test set while the otherfive are used for training and heldout.
Thus, thetrained models have seen neither the test instancesnor other instances from the MT system that pro-duced them.A metric is evaluated based on its Spearman rankcorrelation coefficient between the scores it gave tothe evaluative dataset and human assessments forthe same data.
The correlation coefficient is a realnumber between -1, indicating perfect negative cor-relations, and +1, indicating perfect positive cor-relations.
To compare the relative quality of dif-ferent metrics, we apply bootstrapping re-samplingon the data, and then use paired t-test to deter-mine the statistical significance of the correlationdifferences (Koehn, 2004).
For the results we re-port, unless explicitly mentioned, all stated compar-isons are statistically significant with 99.8% con-fidence.
We include two standard reference-basedmetrics, BLEU and METEOR, as baseline compar-isons.
BLEU is smoothed (Lin and Och, 2004b), andit considers only matching up to bigrams becausethis has higher correlations with human judgmentsthan when higher-ordered n-grams are included.SysID Human-assessment scoreMT1 0.661MT2 0.626MT3 0.586MT4 0.578MT5 0.537MT6 0.530MT7 0.530MT8 0.375MT9 0.332MT10 0.243Table 1: The human-judged quality of ten partici-pating systems in the NIST 2004 Chinese MT Eval-uation.
We used four systems as references (high-lighted in boldface) and the data from the remainingsix for training and evaluation.4.2 Pseudo Reference Variations vs. MetricsWe first compare different metrics?
performanceon the six-system evaluative dataset under differentconfigurations of human and/or pseudo references.For the case when only one human reference is used,the reference was chosen at random from the 2004NIST Eval dataset3.
The correlation results on theevaluative dataset are summarized in Table 2.Some trends are as expected: comparing within ametric, having four references is better than havingjust one; having human references is better than anequal number of system references; having a highquality system as reference is better than one withlow quality.
Perhaps more surprising is the consis-tent trend that metrics do significantly better withfour MT references than with one human reference,and they do almost as well as using four human ref-erences.
The results show that pseudo references areinformative, as standard metrics were able to makeuse of the pseudo references and achieve higher cor-relations than judging from fluency alone.
How-ever, higher correlations are achieved when learningwith regression, suggesting that the trained metricsare better at interpreting comparisons against pseudoreferences.Comparing within each reference configuration,the regression-trained metric that includes both ad-3One reviewer asked about the quality this human?s trans-lations.
Although we were not given official rankings of thehuman references, we compared each person against the otherthree using MT evaluation metrics and found this particulartranslator to rank third, though the quality of all four are sig-nificantly higher than even the best MT systems.300equacy and fluency features always has the highestcorrelations.
If the metric consists of only adequacyfeatures, its performance degrades with the decreas-ing quality of the references.
At another extreme, ametric based only on fluency features has an over-all correlation rate of 0.459, which is lower thanmost correlations reported in Table 2.
This confirmsthe importance of modeling adequacy; even a sin-gle mid-quality MT system may be an informativepseudo reference.
Finally, we note that a regression-trained metric with the full features set that com-pares against 4 pseudo references has a higher cor-relation than BLEU with four human references.These results suggest that the feedback from the hu-man assessed training examples was able to help thelearning algorithm to combine different features toform a better composite metric.4.3 Sentence-Level Evaluation on SingleSystemsTo explore the interaction between the quality ofthe reference MT systems and that of the test MTsystems, we further study the following pseudo ref-erence configurations: all four systems, a high-quality system with a medium quality system, twosystems of medium-quality, one medium with onepoor system, and only the high-quality system.
Foreach pseudo reference configuration, we considerthree metrics: BLEU, METEOR, and the regression-trained metric (using the full feature set).
Eachmetric evaluates sentences from four test systemsof varying quality: the best system in the dataset(MT1), the worst in the set (MT10), and two mid-ranged systems (MT4 and MT7).
The correlationcoefficients are summarized in Table 3.
Each rowspecifies a metric/reference-type combination; eachcolumn specifies an MT system being evaluated (us-ing sentences from all other systems as training ex-amples).
The fluency-only metric and standard met-rics using four human references are baselines.The overall trends at the dataset level generallyalso hold for the per-system comparisons.
With theexception of the evaluation of MT10, regression-based metrics always has higher correlations thanstandard metrics that use the same reference con-figuration (comparing correlation coefficients withineach cell).
When the best MT reference system(MT2) is included as pseudo references, regression-based metrics are typically better than or not statisti-cally different from standard applications of BLEUand METEOR with 4 human references.
Using thetwo mid-quality MT systems as references (MT5and MT6), regression metrics yield correlations thatare only slightly lower than standard metrics withhuman references.
These results support our con-jecture that comparing against multiple systems isinformative.The poorer performances of the regression-basedmetrics on MT10 point out an asymmetry in thelearning approach.
The regression model aims tolearn a function that approximates human judgmentsof translated sentences through training examples.In the space of all possible MT outputs, the neigh-borhood of good translations is much smaller thanthat of bad translations.
Thus, as long as the regres-sion models sees some examples of sentences withhigh assessment scores during training, it shouldhave a much better estimation of the characteristicsof good translations.
This idea is supported by theexperimental data.
Consider the scenario of eval-uating MT1 while using two mid-quality MT sys-tems as references.
Although the reference systemsare not as high quality as the system under evalu-ation, and although the training examples shown tothe regression model were also generated by systemswhose overall quality was rated lower, the trainedmetric was reasonably good at ranking sentencesproduced by MT1.
In contrast, the task of evaluatingsentences from MT10 is more difficult for the learn-ing approach, perhaps because it is sufficiently dif-ferent from all training and reference systems.
Cor-relations might be improved with additional refer-ence systems.4.4 DiscussionsThe design of these experiments aims to simulatepractical situations to use our proposed metrics.
Forthe more frequently encountered language pairs, itshould be possible to find at least two mid-quality(or better) MT systems to serve as pseudo refer-ences.
For example, one might use commercial off-the-shelf systems, some of which are free over theweb.
For less commonly used languages, one mightuse open source research systems (Al-Onaizan et al,1999; Burbank et al, 2005).Datasets from formal evaluation events such as301Ref type and # Ref Sys.
BLEU-S(2) METEOR Regr (adq.
only) Regr (full)4 Humans all humans 0.628 0.591 0.588 0.6441 Human HRef #3 0.536 0.512 0.487 0.5974 Systems all MTRefs 0.614 0.583 0.584 0.6322 Systems Best 2 MTRefs 0.603 0.577 0.573 0.620Mid 2 MTRefs 0.579 0.555 0.528 0.608Worst 2 MTRefs 0.541 0.508 0.467 0.5811 System Best MTRef 0.576 0.559 0.534 0.596Mid MTRef (MT5) 0.538 0.528 0.474 0.577Worst MTRef 0.371 0.329 0.151 0.495Table 2: Comparisons of metrics (columns) using different types of references (rows).
The full regression-trained metric has the highest correlation (shown in boldface) when four human references are used; it hasthe second highest correlation rate (shown in italic) when four MT system references are used instead.
Aregression-trained metric with only fluency features has a correlation coefficient of 0.459.Ref Type Metric MT-1 MT-4 MT-7 MT-10No ref Regr.
0.367 0.316 0.301 -0.0454 human refs Regr.
0.538* 0.473* 0.459* 0.247BLEU-S(2) 0.466 0.419 0.397 0.321*METEOR 0.464 0.418 0.410 0.3124 MTRefs Regr.
0.498 0.429 0.421 0.243BLEU-S(2) 0.386 0.349 0.404 0.240METEOR 0.445 0.354 0.333 0.243Best 2 MTRefs Regr.
0.492 0.418 0.403 0.201BLEU-S(2) 0.391 0.330 0.394 0.268METEOR 0.430 0.333 0.327 0.267Mid 2 MTRefs Regr.
0.450 0.413 0.388 0.219BLEU-S(2) 0.362 0.314 0.310 0.282METEOR 0.391 0.315 0.284 0.274Worst 2 MTRefs Regr.
0.430 0.386 0.365 0.158BLEU-S(2) 0.320 0.298 0.316 0.223METEOR 0.351 0.306 0.302 0.228Best MTRef Regr.
0.461 0.401 0.414 0.122BLEU-S(2) 0.371 0.330 0.380 0.242METEOR 0.375 0.318 0.392 0.283Table 3: Correlation comparisons of metrics by test systems.
For each test system (columns) the overallhighest correlations is distinguished by an asterisk (*); correlations higher than standard metrics usinghuman-references are highlighted in boldface; those that are statistically comparable to them are italicized.NIST MT Evals, which contains human assessedMT outputs for a variety of systems, can be usedfor training examples.
Alternatively, one might di-rectly recruit human judges to assess sample sen-tences from the system(s) to be evaluated.
Thisshould result in better correlations than what we re-ported here, since the human assessed training ex-amples will be more similar to the test instances thanthe setup in our experiments.In developing new MT systems, pseudo refer-ences may supplement the single human referencetranslations that could be extracted from a paralleltext.
Using the same setup as Exp.
1 (see Table 2),adding pseudo references does improve correlations.Adding four pseudo references to the single humanreference raises the correlation coefficient to 0.650(from 0.597) for the regression metric.
Adding themto four human references results in a correlation co-efficient of 0.660 (from 0.644)4.5 ConclusionIn this paper, we have presented a method for de-veloping sentence-level MT evaluation metrics with-out using human references.
We showed that bylearning from human assessed training examples,4BLEU with four human references has a correlation of0.628.
Adding four pseudo references increases BLEU to 0.650.302the regression-trained metric can evaluate an inputsentence by comparing it against multiple machine-generated pseudo references and other target lan-guage resources.
Our experimental results suggestthat the resulting metrics are robust even when thesentences under evaluation are from a system ofhigher quality than the systems serving as refer-ences.
We observe that regression metrics that usemultiple pseudo references often have comparableor higher correlation rates with human judgmentsthan standard reference-based metrics.
Our studysuggests that in conjunction with regression training,multiple imperfect references may be as informativeas gold-standard references.AcknowledgmentsThis work has been supported by NSF Grants IIS-0612791 andIIS-0710695.
We would like to thank Ric Crabbe, Dan Gildea,Alon Lavie, Stuart Shieber, and Noah Smith and the anonymousreviewers for their suggestions.
We are also grateful to NIST formaking their assessment data available to us.ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight,John Lafferty, I. Dan Melamed, Franz-Josef Och, DavidPurdy, Noah A. Smith, and David Yarowsky.
1999.Statistical machine translation.
Technical report, JHU.citeseer.nj.nec.com/al-onaizan99statistical.html.Joshua S. Albrecht and Rebecca Hwa.
2007.
A re-examinationof machine learning approaches for sentence-level MT eval-uation.
In Proceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics (ACL-2007).Satanjeev Banerjee and Alon Lavie.
2005.
Meteor: An auto-matic metric for MT evaluation with improved correlationwith human judgments.
In ACL 2005 Workshop on Intrinsicand Extrinsic Evaluation Measures for Machine Translationand/or Summarization, June.Christopher M. Bishop.
2006.
Pattern Recognition and Ma-chine Learning.
Springer Verlag.John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,Cyril Goutte, Alex Kulesza, Alberto Sanchis, and NicolaUeffing.
2003.
Confidence estimation for machine trans-lation.
Technical Report Natural Language EngineeringWorkshop Final Report, Johns Hopkins University.Andrea Burbank, Marine Carpuat, Stephen Clark, MarkusDreyer, Declan Groves Pamela.
Fox, Keith Hall, MaryHearne, I. Dan Melamed, Yihai Shen, Andy Way, BenWellington, and Dekai Wu.
2005.
Final report of the 2005language engineering workshop on statistical machine trans-lation by parsing.
Technical Report Natural Language Engi-neering Workshop Final Report, ?JHU?.Simon Corston-Oliver, Michael Gamon, and Chris Brockett.2001.
A machine learning approach to the automatic eval-uation of machine translation.
In Proceedings of the 39thAnnual Meeting of the Association for Computational Lin-guistics, July.Michael Gamon, Anthony Aue, and Martine Smets.
2005.Sentence-level MT evaluation without reference translations:Beyond language modeling.
In European Association forMachine Translation (EAMT), May.Thorsten Joachims.
1999.
Making large-scale SVM learningpractical.
In Bernhard Scho?elkopf, Christopher Burges, andAlexander Smola, editors, Advances in Kernel Methods -Support Vector Learning.
MIT Press.Philipp Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In Proceedings of the 2004 Confer-ence on Empirical Methods in Natural Language Processing(EMNLP-04).Alex Kulesza and Stuart M. Shieber.
2004.
A learning ap-proach to improving sentence-level MT evaluation.
In Pro-ceedings of the 10th International Conference on Theoreticaland Methodological Issues in Machine Translation (TMI),Baltimore, MD, October.Chin-Yew Lin and Franz Josef Och.
2004a.
Automatic evalu-ation of machine translation quality using longest commonsubsequence and skip-bigram statistics.
In Proceedings ofthe 42nd Annual Meeting of the Association for Computa-tional Linguistics, July.Chin-Yew Lin and Franz Josef Och.
2004b.
Orange: amethod for evaluating automatic evaluation metrics for ma-chine translation.
In Proceedings of the 20th InternationalConference on Computational Linguistics (COLING 2004),August.Ding Liu and Daniel Gildea.
2005.
Syntactic features forevaluation of machine translation.
In ACL 2005 Workshopon Intrinsic and Extrinsic Evaluation Measures for MachineTranslation and/or Summarization, June.Ding Liu and Daniel Gildea.
2007.
Source-language featuresand maximum correlation training for machine translationevaluation.
In Proceedings of the HLT/NAACL-2007, April.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
Bleu: a method for automatic evaluation of ma-chine translation.
In Proceedings of the 40th Annual Meetingof the Association for Computational Linguistics, Philadel-phia, PA.Christopher Quirk.
2004.
Training a sentence-level machinetranslation confidence measure.
In Proceedings of LREC2004.Nicola Ueffing, Klaus Macherey, and Hermann Ney.
2003.Confidence measures for statistical machine translation.
InMachine Translation Summit IX, pages 394?401, September.303
