Learning the Optimal use of Dependency-parsing Information for FindingTranslations with Comparable CorporaDaniel Andrade?, Takuya Matsuzaki?, Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo{daniel.andrade, matuzaki}@is.s.u-tokyo.ac.jp?Microsoft Research Asia, Beijingjtsujii@microsoft.comAbstractUsing comparable corpora to find new wordtranslations is a promising approach for ex-tending bilingual dictionaries (semi-) auto-matically.
The basic idea is based on theassumption that similar words have similarcontexts across languages.
The context ofa word is often summarized by using thebag-of-words in the sentence, or by usingthe words which are in a certain dependencyposition, e.g.
the predecessors and succes-sors.
These different context positions arethen combined into one context vector andcompared across languages.
However, previ-ous research makes the (implicit) assumptionthat these different context positions should beweighted as equally important.
Furthermore,only the same context positions are comparedwith each other, for example the successor po-sition in Spanish is compared with the suc-cessor position in English.
However, this isnot necessarily always appropriate for lan-guages like Japanese and English.
To over-come these limitations, we suggest to performa linear transformation of the context vec-tors, which is defined by a matrix.
We de-fine the optimal transformation matrix by us-ing a Bayesian probabilistic model, and showthat it is feasible to find an approximate solu-tion using Markov chain Monte Carlo meth-ods.
Our experiments demonstrate that ourproposed method constantly improves transla-tion accuracy.1 IntroductionUsing comparable corpora to automatically extendbilingual dictionaries is becoming increasingly pop-ular (Laroche and Langlais, 2010; Andrade et al,2010; Ismail and Manandhar, 2010; Laws et al,2010; Garera et al, 2009).
The general idea isbased on the assumption that similar words havesimilar contexts across languages.
The context ofa word can be described by the sentence in whichit occurs (Laroche and Langlais, 2010) or a sur-rounding word-window (Rapp, 1999; Haghighi etal., 2008).
A few previous studies, like (Garera etal., 2009), suggested to use the predecessor and suc-cessors from the dependency-parse tree, instead of aword window.
In (Andrade et al, 2011), we showedthat including dependency-parse tree context posi-tions together with a sentence bag-of-words contextcan improve word translation accuracy.
Howeverprevious works do not make an attempt to find anoptimal combination of these different context posi-tions.Our study tries to find an optimal weighting andaggregation of these context positions by learninga linear transformation of the context vectors.
Themotivation is that different context positions mightbe of different importance, e.g.
the direct predeces-sors and successors from the dependency tree mightbe more important than the larger context from thewhole sentence.
Another motivation is that depen-dency positions cannot be always compared acrossdifferent languages, e.g.
a word which tends to oc-cur as a modifier in English, can tend to occur inJapanese in a different dependency position.As a solution, we propose to learn the optimalcombination of dependency and bag-of-words sen-tence information.
Our approach uses a linear trans-formation of the context vectors, before comparing10Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 10?18,49th Annual Meeting of the Association for Computational Linguistics,Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguisticsthem using the cosine similarity.
This can be con-sidered as a generalization of the cosine similarity.We define the optimal transformation matrix by themaximum-a-posterior (MAP) solution of a Bayesianprobabilistic model.
The likelihood function for atranslation matrix is defined by considering the ex-pected achieved translation accuracy.
As a prior, weuse a Dirichlet distribution over the diagonal ele-ments in the matrix and a uniform distribution overits non-diagonal elements.
We show that it is fea-sible to find an approximation of the optimal so-lution using Markov chain Monte Carlo (MCMC)methods.
In our experiments, we compare the pro-posed method, which uses this approximation, withthe baseline method which uses the cosine similaritywithout any linear transformation.
Our experimentsshow that the translation accuracy is constantly im-proved by the proposed method.In the next section, we briefly summarize the mostrelevant previous work.
In Section 3, we then ex-plain the baseline method which is based on previ-ous research.
Section 4 explains in detail our pro-posed method, followed by Section 5 which pro-vides an empirical comparison to the baseline, andanalysis.
We summarize our findings in Section 6.2 Previous WorkUsing comparable corpora to find new translationswas pioneered in (Rapp, 1999; Fung, 1998).
The ba-sic idea for finding a translation for a word q (query),is to measure the context of q and then to comparethe context with each possible translation candidate,using an existing dictionary.
We will call wordsfor which we have a translation in the given dic-tionary, pivot words.
First, using the source cor-pus, they calculate the degree of association of aquery word q with all pivot words.
The degree ofassociation is a measure which is based on the co-occurrence frequency of q and the pivot word in acertain context position.
A context (position) can bea word-window (Rapp, 1999), sentence (Utsuro etal., 2003), or a certain position in the dependency-parse tree (Garera et al, 2009; Andrade et al, 2011).In this way, they get a context vector for q, whichcontains the degree of association to the pivot wordsin different context positions.
Using the target cor-pus, they then calculate a context vector for eachpossible translation candidate x, in the same way.Finally, they compare the context vector of q withthe context vector of each candidate x, and retrievea ranked list of possible translation candidates.
Inthe next section, we explain the baseline which isbased on that previous research.The general idea of learning an appropriatemethod to compare high-dimensional vectors is notnew.
Related research is often called ?metric-learning?, see for example (Xing et al, 2003; Basuet al, 2004).
However, for our objective function itis difficult to find an analytic solution.
To our knowl-edge, the idea of parameterizing the transformationmatrix, in the way we suggest in Section 4, and tolearn an approximate solution with a fast samplingstrategy is new.3 BaselineOur baseline measures the degree of association be-tween the query word q and each pivot word withrespect to several context positions.
As a contextposition we consider the predecessors, successors,siblings with respect to the dependency parse tree,and the whole sentence (bag-of-words).
The depen-dency information which is used is also illustrated inFigure 1.
As a measure of the degree of associationwe use the Log-odds-ratio as proposed in (Larocheand Langlais, 2010).Figure 1: Example of the dependency information usedby our approach.
Here, from the perspective of ?door?.Next, we define the context vector which containsthe degree of association between the query and eachpivot in several context positions.
First, for each11context position i we define a vector qi which con-tains the degree of association with each pivot wordin the context position i.
If we number the pivotwords from 1 to n, then this vector can be writ-ten as qi = (q1i , .
.
.
, qni ).
Note that in our case iranges from 1 to 4, representing the context posi-tions predecessors (1), successors (2), siblings (3),and the sentence bag-of-words (4).
Finally, the com-plete context vector for the query q is a long vectorq which appends each qi, i.e.
: q = (q1, .
.
.
,q4).Next, in the same way as before, we create a con-text vector x for each translation candidate x in thetarget language.
For simplicity, we assume that eachpivot word in the source language has only one cor-responding translation in the target language.
Asa consequence, the dimensions of q and x are thesame.
Finally we can score each translation candi-date by using the cosine similarity between q andx.We claim that all of the context positions (1 to 4)can contain information which is helpful to identifytranslation candidates.
However, we do not knowabout their relative importance, neither do we knowwhether these dependency positions can be com-pared across language pairs as different as Japaneseand English.
The cosine similarity simply weightsall dependency position equally important and ig-nores problems which might occur when comparingdependency positions across languages.4 Proposed MethodOur proposed method tries to overcome the short-comings of the cosine-similarity by using the fol-lowing generalization:sim(q,x) = qAxT?qAqT?xAxT , (1)where A is a positive-definite matrix in Rdn?dn, andT is the transpose of a vector.
This can also be con-sidered as linear transformation of the vectors using?A before using the normal cosine similarity, seealso (Basu et al, 2004).1The challenge is to find an appropriate matrix Awhich is expected to take the correlations between1Therefore, exactly speaking A is not the transformationmatrix, however it defines uniquely the transformation matrix?A.the different dimensions into account, and which op-timally weights the different dimensions.
Note that,if we set A to the identity matrix, we recover thenormal cosine similarity, which is our baseline.Clearly, finding an optimal matrix in Rdn?dn isinfeasible due to the high dimensionality.
We willtherefore restrict the structure of A.Let I be the identity matrix in Rn?n , then wedefine the matrix A, as follows:A =???
?d1I z1,2I z1,3I z1,4Iz1,2I d2I z2,3I z2,4Iz1,3I z2,3I d3I z3,4Iz1,4I z2,4I z3,4I d4I???
?It is clear from this definition that d1, .
.
.
, d4 weightsthe context positions 1 to 4.
Furthermore, zi,j canbe interpreted as a the confusion coefficient betweencontext position i and j.
For example, a high valuefor z2,3 means that a pivot word which occurs inthe sibling position in Japanese (source language),might not necessarily occur in the sibling position inEnglish (target language), but instead in the succes-sor position.
However, in order to reduce the dimen-sionality of the parameter space further, we assumethat each such zi,j has the same value z. Therefore,matrix A becomesA =???
?d1I zI zI zIzI d2I zI zIzI zI d3I zIzI zI zI d4I????
.In the next subsection we will explain how we de-fine an optimal solution for A.4.1 Optimal solution for AWe use a Bayesian probabilistic model in order todefine the optimal solution for A.
Formally we tryto find the maximum-a-posterior (MAP) solution ofA, i.e.
:argmaxAp(A|data, ?).
(2)The posterior probability is defined byp(A|data, ?)
?
fauc(data|A) ?
p(A|?)
.
(3)fauc(data|A) is the (unnormalized) likelihood func-tion.
p(A|?)
is the prior that captures our prior be-liefs about A, and which is parameterized by a hy-perparameter ?.124.1.1 The likelihood function fauc(data|A)As a likelihood function we use a modificationof the area under the curve (AUC) of the accuracy-vs-rank graph.
The accuracy-vs-rank graph showsthe translation accuracy at different ranks.
datarefers to the part of the gold-standard which is usedfor training.
Our complete gold-standard contains443 domain-specific Japanese nouns (query words).Each Japanese noun in the gold standard corre-sponds to one pair of the form <Japanese noun(query), English translations (answers)>.
We de-note the accuracy at rank r, by accr.
The accuracyaccr is determined by counting how often the cor-rect answer is listed in the top r translation candi-dates suggested for a query, divided by the numberof all queries in data.
The likelihood function isnow defined as follows:fauc(data|A) =20?r=1accr ?
(21 ?
r) .
(4)That means fauc(data|A) accumulates the accura-cies at the ranks from 1 to 20, where we weight ac-curacies at top ranks higher.4.1.2 The prior p(A|?
)The prior over the transformation matrix is factor-ized in the following manner:p(A|?)
= p(z|d1, .
.
.
, d4) ?
p(d1, .
.
.
, d4|?)
.The prior over the diagonal is defined as a Dirichletdistribution:p(d1, .
.
.
, d4|?)
= 1B(?)4?i=1d?
?1iwhere ?
is the concentration parameter of the sym-metric Dirichlet, and B(?)
is the normalization con-stant.
The prior over the non-diagonal value a is de-fined as:p(z|d1, .
.
.
, d4) = 1?
?
1[0,?
](z) (5)where ?
= min{d1, .
.
.
, d4}.First, note that our prior limits the possible matri-ces A to matrices which have diagonal entries whichare between 0 and 1.
This is not a restriction sincethe ranking of the translation candidates induced bythe parameterized cosine similarity will not changeif A is multiplied by a constant c > 0 .
To see this,note thatsim(q,x) = q(c ?A)x?q(c ?A)q?x(c ?A)x= qAx?qAq?xAx .Second, note that our prior limits A further, by re-quiring, in Equation (5), that every non-diagonal el-ement is smaller or equal than any diagonal element.That requirement is sensible since we do not expectthat a optimal similarity measure between Englishand Japanese will prefer context which is similar indifferent dependency positions, over context whichis similar in the same context positions.
To see this,imagine the extreme case where for example d1 is 0,and instead z12 is 1.
In that case the similarity mea-sure would ignore any similarity in the predecessorposition, but would instead compare the predeces-sors in Japanese with the successors in English.Finally, note that our prior puts probability massover a subset of the positive-definite matrices inR4?4, and puts no probability mass on matriceswhich are not positive-definite.
As a consequence,the similarity measure in Equation (1) is ensured tobe well-defined.4.2 TrainingIn the following we explain how we use the trainingdata in order to find a good solution for the matrixA.4.2.1 Setting hyperparameter ?Recall, that ?
weights our prior belief about howstrong we think that the different context positionsshould be weighted equally.
From a practical point-of-view, we do not know how strong we shouldweight that prior belief.
We therefore use empiricalBayes to estimate ?, that is we use part of the train-ing data to set ?.
First, using half of the trainingset, we find the A which maximizes p(A|data, ?
)for several ?.
Then, the remaining half of the train-ing set is used to evaluate fauc(data|A) to find thebest ?.
Note that the prior p(A|?)
can also be con-sidered as a regularization to prevent overfitting.
Inthe next sub-section we will explain how to find anapproximation ofAwhich maximizes p(A|data, ?
).134.2.2 Finding a MAP solution for ARecall that matrix A is defined by using only fiveparameters.
Since the problem is low-dimensional,we can therefore expect to find a reasonable solutionusing sampling methods.
For finding an approxima-tion of the maximum-a-posteriori (MAP) solution ofp(A|data, ?
), we use the following Markov chainMonte Carlo procedure:1.
Initialize d1, .
.
.
, d4 and z.2.
Leave z constant, and run Simulated-Annealing to find the d1, .
.
.
, d4 whichmaximize p(A|data, ?).3.
Given d1, .
.
.
, d4, sample from the uniform dis-tribution [1,min(d1, .
.
.
d4)] in order to find thez which maximizes p(A|data, ?
).The steps 2. and 3. are repeated till the convergenceof the parameters.Concerning step 2., we use Simulated-Annealing for finding a (local) maximum ofp(d1, .
.
.
, d4|data, ?)
with the following settings:As a jumping distribution we use a Dirichlet distri-bution which we update every 1000 iterations.
Thecooling rate is set to 1iteration .For step 2. and 3. it is of utmost importance tobe able to evaluate p(A|data, ?)
fast.
The com-putationally expensive part of p(A|data, ?)
is toevaluate fauc(data|A).
In order to quickly evalu-ate fauc(data|A), we need to pre-calculate part ofsim(q, x) for all queries q and all translation can-didates x.
To illustrate the basic idea, considersim(q, x) without the normalization of q and xwithrespect to A, i.e.
:sim(q, x) = qAxT = (q1, .
.
.
,q4)A(x1, .
.
.
,x4)T .Let us denote I?dn a block matrix in Rdn?dn whichcontains in each n ?
n block the identity matrix ex-cept in its diagonal; the diagonal of I?dn contains then ?
n matrix which is zero in all entries.
We cannow rewrite matrix A as:A =???
?d1I 0 0 00 d2I 0 00 0 d3I 00 0 0 d4I???
?+ z ?
I?dn .And finally we can factor out the parameters(d1, .
.
.
d4) and z in the following way:sim(q, x) = (d1, .
.
.
, d4)????q1xT1...q4xT4???+z?
(qI?dnxT )By pre-calculating???q1xT1...q4xT4???
and qI?dnxT , we canmake the evaluation of each sample, in steps 2.
and3., computationally feasible.5 ExperimentsIn the experiments of the present study, we useda collection of complaints concerning automobilescompiled by the Japanese Ministry of Land, Infras-tructure, Transport and Tourism (MLIT)2 and an-other collection of complaints concerning automo-biles compiled by the USA National Highway Traf-fic Safety Administration (NHTSA)3.
Both corporaare publicly available.
The corpora are non-parallel,but are comparable in terms of content.
The partof MLIT and NHTSA which we used for our ex-periments, contains 24090 and 47613 sentences, re-spectively.
The Japanese MLIT corpus was mor-phologically analyzed and dependency parsed usingJuman and KNP4.
The English corpus NHTSA wasPOS-tagged and stemmed with Stepp Tagger (Tsu-ruoka et al, 2005; Okazaki et al, 2008) and depen-dency parsed using the MST parser (McDonald etal., 2005).
Using the Japanese-English dictionaryJMDic5, we found 1796 content words in Japanesewhich have a translation which is in the English cor-pus.
These content words and their translations cor-respond to our pivot words in Japanese and English,respectively.62http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html3http://www-odi.nhtsa.dot.gov/downloads/index.cfm4http://www-lab25.kuee.kyoto-u.ac.jp/nl-resource/juman.html and http://www-lab25.kuee.kyoto-u.ac.jp/nl-resource/knp.html5http://www.csse.monash.edu.au/ jwb/edict doc.html6Recall that we assume a one-to-one correspondence be-tween a pivot in Japanese and English.
If a Japanese pivot wordas more than one English translation, we select the translationfor which the relative frequency in the target corpus is closestto the pivot in the source corpus.145.1 EvaluationFor the evaluation we extract a gold-standard whichcontains Japanese and English noun pairs that ac-tually occur in both corpora.7 The gold-standardis created with the help of the JMDic dictionary,whereas we correct apparently inappropriate trans-lations, and remove general nouns such as ???
(possibility) and ambiguous words such as?
(rice,America).
In this way, we obtain a final list of 443domain-specific Japanese nouns.Each Japanese noun in the gold-standard corre-sponds to one pair of the form <Japanese noun(query), English translations (answers)>.
We dividethe gold-standard into two halves.
The first half isused for for learning the matrix A, the second partis used for the evaluation.
In general, we expect thatthe optimal transformation matrixA depends mainlyon the languages (Japanese and English) and on thecorpora (MLIT and NHTSA).
However, in practice,the optimal matrix can also vary depending on thepart of the gold-standard which is used for training.These random variations are especially large, if thepart of the gold-standard which is used for trainingor testing is small.In order to take these random effects into ac-count, we perform repeated subsampling of thegold-standard.
In detail, we randomly split the gold-standard into equally-sized training and test set.
Thisis repeated five times, leading to five training andfive test sets.
The performance on each test set isshown in Table 1.
OPTIMIZED-ALL marks the re-sult of our proposed method, where matrix A is opti-mized using the training set.
The optimization of thediagonal elements d1, .
.
.
, d4, and the non-diagonalvalue z is as described in Section 4.2.
Finally, thebaseline method, as described in 3, corresponds toOPTIMIZED-ALL where d1, .
.
.
, d4 are set to 1,and z is set to 0.
This baseline is denoted as NOR-MAL.
We can see that the overall translation accu-racy varies across the test sets.
However, we see thatin all test sets our proposed method OPTIMIZED-ALL performs better than the baseline NORMAL.7Note that if the current query (Japanese noun) is a pivotword, then the word is not considered as a pivot word.5.2 AnalysisIn the previous section, we showed that the cosine-similarity is sub-optimal for comparing context vec-tors which contain information from different con-text positions.
We showed that it is possible to findan approximation of a matrix A which optimallyweights, and combines the different context posi-tions.
Recall, that the matrix A is described by theparameters d1 .
.
.
d4 and z, which can interpreted ascontext position weights and a confusion coefficient,respectively.
Therefore, by looking at these parame-ters which we learned using each training set, we canget some interesting insights.
Table 2 shows thesesparameters learned for each training set.We can see that the parameters, across the train-ing sets, are not as stable as we wish.
For examplethe weight for the predecessor position ranges from0.27 to 0.44.
As a consequence, the average values,shown in the last row of Table 2, have to be inter-preted with care.
We expect that the variance is dueto the limited size of the training set, 220 <query,answers> pairs.Nevertheless, we can draw some conclusions withconfidence.
For example, we see that the prede-cessor and successor positions are the most impor-tant contexts, since the weights for both are al-ways higher than for the other context positions.Furthermore, we clearly see that the sibling andsentence (bag-of-words) contexts, although not ashighly weighted as the former two, can be consid-ered to be relevant, since each has a weight of around0.20.
Finally, we see that z, the confusion coeffi-cient, is around 0.03, which is small.8 Therefore,we verify z?s usefulness with another experiment.We additionally define the method OPTIMIZED-DIAG which uses the same matrix as OPTIMIZED-ALL except that the confusion coefficient z is setto zero.
In Table 1, we can see that the accu-racy of OPTIMIZED-DIAG is constantly lower thanOPTIMIZED-ALL.Furthermore, we are interested in the role of thewhole sentence (bag-of-words) information which isin the context vector (in position d4 of the block vec-tor).
Therefore, we excluded the sentence informa-8In other words, z is around 17% of its maximal possiblevalue.
The maximal possible value is around 0.18, since, recallthat z is, by definition, smaller or equal to min{d1 .
.
.
d4}.15Test Set Method Top-1 Top-5 Top-10 Top-15 Top-20Accuracy Accuracy Accuracy Accuracy Accuracy1OPTIMIZED-ALL 0.20 0.37 0.47 0.50 0.54OPTIMIZED-DIAG 0.20 0.34 0.43 0.48 0.51NORMAL 0.18 0.32 0.43 0.47 0.502OPTIMIZED-ALL 0.20 0.35 0.43 0.48 0.52OPTIMIZED-DIAG 0.19 0.33 0.42 0.46 0.52NORMAL 0.18 0.34 0.42 0.47 0.493OPTIMIZED-ALL 0.17 0.31 0.37 0.44 0.48OPTIMIZED-DIAG 0.17 0.27 0.36 0.41 0.45NORMAL 0.16 0.27 0.36 0.41 0.444OPTIMIZED-ALL 0.14 0.30 0.38 0.43 0.46OPTIMIZED-DIAG 0.14 0.26 0.34 0.4 0.43NORMAL 0.15 0.29 0.37 0.41 0.445OPTIMIZED-ALL 0.18 0.34 0.42 0.46 0.51OPTIMIZED-DIAG 0.17 0.30 0.38 0.43 0.48NORMAL 0.19 0.31 0.40 0.44 0.48averageOPTIMIZED-ALL 0.18 0.33 0.41 0.46 0.50OPTIMIZED-DIAG 0.17 0.30 0.39 0.44 0.48NORMAL 0.17 0.31 0.40 0.44 0.47Table 1: Shows the accuracy at different ranks for all test sets, and, in the last column, the average over all test sets.The proposed method OPTIMIZED-ALL is compared to the baseline NORMAL.
Furthermore, for analysis, the resultswhen optimizing only the diagonal are marked as OPTIMIZED-DIAG.Training Set d1 d2 d3 d4 zpredecessor successor sibling sentence confusion coefficient1 0.35 0.26 0.19 0.20 0.032 0.27 0.29 0.21 0.23 0.033 0.35 0.31 0.16 0.18 0.024 0.44 0.24 0.17 0.16 0.045 0.39 0.28 0.20 0.13 0.03average 0.36 0.28 0.19 0.18 0.03Table 2: Shows the parameters which were learned using each training set.
d1 .
.
.
d4 are the weights of the contextpositions, which sum up to 1. z marks the degree to which it is useful to compare context across different positions.tion from the context vector.
The accuracy results,averaged over the same test sets as before, are shownin Table 3.
We can see that the accuracies are clearlylower than before (compare to Table 1).
This clearlyjustifies to include additionally sentence informationinto the context vector.
It is also interesting to notethat the average z value is now 0.14.9 This is consid-erable higher than before, and shows that a bag-of-words model can partly make the use of z redundant.However, note that the sentence bag-of-words modelcovers a broader context, beyond the direct prede-cessors, successor and siblings, which explains why9That is 48% of its maximal possible value.
Since for thedependency positions predecessor, successor and sibling we getthe average weights 0.38, 0.33 and 0.29, respectively.a small z value is still relevant in the situation wherewe include sentence bag-of-words into the contextvector.Finally, to see why it can be helpful to comparedifferent dependency positions from the context vec-tors of Japanese and English, we looked at concreteexamples.
We found, for example, that the trans-lation accuracy of the query word ????
(disc)improved when using OPTIMIZED-ALL instead ofOPTIMIZED-DIAG.
The pivot word ??
(wrap)tends together with both the Japanese query ????
(disc), and with the correct translation ?disc?in English.
However, that pivot word occurs inJapanese and English in different context positions.In the Japanese corpus ??
(wrap) tends to occur16Method Top-1 Top-5 Top-10 Top-15 Top-20OPT-DEP 0.13 0.25 0.34 0.38 0.41NOR-DEP 0.12 0.23 0.29 0.33 0.38Table 3: The proposed method, but without the sentenceinformation in the context vector, is denoted OPT-DEP.The baseline method, but without the sentence informa-tion in the context vector, is denoted NOR-DEP.together with the query????
(disc) in sentenceslike for example the following:?????
(break)????
(disc)???(wrap)????
(occured)?
?That Japanese sentence can be literally translated as?A wrap occured in the brake disc.
?, where ?wrap?is the sibling of ?disc?
in the dependency tree.
How-ever, in English, considered out of the perspectiveof ?disc?, the pivot word ?wrap?
tends to occur in adifferent dependency position.
For example, the fol-lowing sentence can be found in the English corpus:?Front disc wraps.
?In English ?wrap?
tends to occur as a successor of?disc?.
A non-zero confusion coefficient allows usto account some degree of similarity to situationswhere the query (here ??????
(disc)) and thetranslation candidate (here ?disc?)
tend to occur withthe same pivot word (here ?wrap?
), but in differentdependency positions.6 ConclusionsFinding new translations of single words using com-parable corpora is a promising method, for exam-ple, to assist the creation and extension of bilin-gual dictionaries.
The basic idea is to first createcontext vectors of the query word, and all the can-didate translations, and then, in the second step,to compare these context vectors.
Previous work(Laroche and Langlais, 2010; Fung, 1998; Gareraet al, 2009) suggests that for this task the cosine-similarity is a good choice to compare context vec-tors.
For example, Garera et al (2009) include theinformation of various context positions from thedependency-parse tree in one context vector, and, af-terwards, compares these context vectors using thecosine-similarity.
However, this makes the implicitassumption that all context positions are equally im-portant, and, furthermore, that context from differ-ent context positions does not need to be comparedwith each other.
To overcome these limitations, wesuggested to use a generalization of the cosine simi-larity which performs a linear transformation of thecontext vectors, before applying the cosine similar-ity.
The linear transformation can be described by apositive-definite matrix A.
We defined the optimalmatrix A by using a Bayesian probabilistic model.We demonstrated that it is feasible to approximatethe optimal matrix A by using MCMC-methods.Our experimental results suggest that it is bene-ficial to weight context positions individually.
Forexample, we found that predecessor and successorshould be stronger weighted than sibling, and sen-tence information.
Whereas, the latter two are alsoimportant, having a total weight of around 40%.Furthermore, we showed that for languages as dif-ferent as Japanese and English it can be helpful tocompare also different context positions across bothlanguages.
The proposed method constantly outper-formed the baseline method.
Top 1 accuracy in-creased by up to 2% percent points and Top 20 byup to 4% percent points.For future work, we consider to use different pa-rameterizations of the matrix A which could lead toeven higher improvement in accuracy.
Furthermore,we consider to include, and weight additional fea-tures like transliteration similarity.AcknowledgmentWe would like to thank the anonymous reviewersfor their helpful comments.
This work was partiallysupported by Grant-in-Aid for Specially PromotedResearch (MEXT, Japan).
The first author is sup-ported by the MEXT Scholarship and by an IBMPhD Scholarship Award.ReferencesD.
Andrade, T. Nasukawa, and J. Tsujii.
2010.
Robustmeasurement and comparison of context similarity forfinding translation pairs.
In Proceedings of the In-ternational Conference on Computational Linguistics,pages 19?27.D.
Andrade, T. Matsuzaki, and J. Tsujii.
2011.
Effec-tive use of dependency structure for bilingual lexicon17creation.
In Proceedings of the International Confer-ence on Computational Linguistics and Intelligent TextProcessing, Lecture Notes in Computer Science, pages80?92.
Springer Verlag.S.
Basu, M. Bilenko, and R.J. Mooney.
2004.
A prob-abilistic framework for semi-supervised clustering.
InProceedings of the ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining,pages 59?68.P.
Fung.
1998.
A statistical view on bilingual lexicon ex-traction: from parallel corpora to non-parallel corpora.Lecture Notes in Computer Science, 1529:1?17.N.
Garera, C. Callison-Burch, and D. Yarowsky.
2009.Improving translation lexicon induction from mono-lingual corpora via dependency contexts and part-of-speech equivalences.
In Proceedings of the Confer-ence on Computational Natural Language Learning,pages 129?137.
Association for Computational Lin-guistics.A.
Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.2008.
Learning bilingual lexicons from monolingualcorpora.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics, pages771?779.
Association for Computational Linguistics.A.
Ismail and S. Manandhar.
2010.
Bilingual lexiconextraction from comparable corpora using in-domainterms.
In Proceedings of the International Conferenceon Computational Linguistics, pages 481 ?
489.A.
Laroche and P. Langlais.
2010.
Revisiting context-based projection methods for term-translation spottingin comparable corpora.
In Proceedings of the In-ternational Conference on Computational Linguistics,pages 617 ?
625.F.
Laws, L. Michelbacher, B. Dorow, C. Scheible,U.
Heid, and H. Schu?tze.
2010.
A linguisticallygrounded graph model for bilingual lexicon extrac-tion.
In Proceedings of the International Conferenceon Computational Linguistics, pages 614?622.
Inter-national Committee on Computational Linguistics.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Pro-ceedings of the Annual Meeting of the Association forComputational Linguistics, pages 91?98.
Associationfor Computational Linguistics.N.
Okazaki, Y. Tsuruoka, S. Ananiadou, and J. Tsujii.2008.
A discriminative candidate generator for stringtransformations.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 447?456.
Association for Computational Lin-guistics.R.
Rapp.
1999.
Automatic identification of word transla-tions from unrelated English and German corpora.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics, pages 519?526.
Asso-ciation for Computational Linguistics.Y.
Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,S.
Ananiadou, and J. Tsujii.
2005.
Developing a ro-bust part-of-speech tagger for biomedical text.
LectureNotes in Computer Science, 3746:382?392.T.
Utsuro, T. Horiuchi, K. Hino, T. Hamamoto, andT.
Nakayama.
2003.
Effect of cross-language IRin bilingual lexicon acquisition from comparable cor-pora.
In Proceedings of the conference on Europeanchapter of the Association for Computational Linguis-tics, pages 355?362.
Association for ComputationalLinguistics.E.P.
Xing, A.Y.
Ng, M.I.
Jordan, and S. Russell.
2003.Distance metric learning with application to clusteringwith side-information.
Advances in Neural Informa-tion Processing Systems, pages 521?528.18
