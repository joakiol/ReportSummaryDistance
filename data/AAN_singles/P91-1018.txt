LEARNING PERCEPTUALLY-GROUNDED SEMANTICS  INTHE L0 PROJECTTer ry  Reg ier*In ternat iona l  Computer  Science Ins t i tu te1947 Center  Street,  Berkeley, CA,  94704(415) 642-4274 x 184regier@cogsci.Berkeley.ED U?
TR"Above"Figure 1: Learning to Associate Scenes with SpatialTermsABSTRACTA method is presented for acquiring perceptually-grounded semantics for spatial terms in a simple visualdomain, as a part of the L0 miniature language acquisi-tion project.
Two central problems in this learning taskare (a) ensuring that the terms learned generalize well,so that they can be accurately applied to new scenes,and (b) learning in the absence of explicit negative v-idence.
Solutions to these two problems are presented,and the results discussed.1 In t roduct ionThe L0 language learning project at the InternationalComputer Science Institute \[Feldman et al, 1990; We-ber and Stolcke, 1990\] seeks to provide an account of lan-guage acquisition in the semantic domain of spatial rela-tions between geometrical objects.
Within this domain,the work reported here addresses the subtask of learn-ing to associate scenes, containing several simple objects,with terms to describe the spatial relations among theobjects in the scenes.
This is illustrated in Figure 1.For each scene, the learning system is supplied with anindication of which object is the reference object (we callthis object the landmark, or LM), and which object is theone being located relative to the reference object (this isthe trajector, or TR).
The system is also supplied witha single spatial term that describes the spatial relation*Supported through the International Computer ScienceInstitute.portrayed in the scene.
It is to learn to associate allapplicable terms to novel scenes.The TR is restricted to be a single point for the timebeing; current work is directed at addressing the moregeneral case of an arbitrarily shaped TR.Another aspect of the task is that learning must takeplace in the absence of explicit negative instances.
Thiscondition is imposed so that the conditions under whichlearning takes place will be similar in this respect tothose under which children learn.Given this, there are two central problems in the sub-task as stated:?
Ensuring that the learning will generalize to sceneswhich were not a part of the training set.
Thismeans that the region in which a TR will be consid-ered "above" a LM may have to change size, shape,and position when a novel LM is presented.?
Learning without explicit negative vidence.This paper presents olutions to both of these prob-lems.
It begins with a general discussion of each of thetwo problems and their solutions.
Results of trainingare then presented.
Then, implementation details arediscussed.
And finally, some conclusions are presented.2 Genera l i za t ion  and  Parameter i zedReg ions2.1 The Prob lemThe problem of learning whether a particular point lies ina given region of space is a foundational one, with sev-eral widely-known "classic" solutions \[Minsky and Pa-pert, 1988; Rumelhart and McClelland, 1986\].
The taskat hand is very similar to this problem, since learningwhen "above" is an appropriate description of the spatialrelation between a LM and a point TR really amountsto learning what the extent of the region "above" a LMis.However, there is an important difference from theclassic problem.
We are interested here in learningwhether or not a given point (the TR) lies in a region(say "above", "in") which is itself located relative to aLM.
Thus, the shape, size, and position of the region aredependent on the shape, size, and position of the currentLM.
For example, the area "above" a small triangle to-ward the top of the visual field will differ in shape, size,138and position from the area "above" a large circle in themiddle of the visual field.2.2 Parameter ized RegionsPart of the solution to this problem lies in the use of pa-rameterized regions.
Rather than learn a fixed region ofspace, the system learns a region which is parameterizedby several features of the LM, and is thus dependent onthem.The LM features used are the location of the center ofmass, and the locations of the four corners of the smallestrectangle nclosing the LM (the LM's "bounding-box").Learning takes place relative to these five "key points".Consider Figure 2.
The figure in (a) shows a regionin 2-space learned using the intersection of three half-planes, as might be done using an ordinary perceptron.In (b), we see the same region, but learned relative tothe five key points of an LM.
This means imply that thelines which define the half-planes have been constrainedto pass through the key points of the LM.
The methodby which this is done is covered in Section 5.
Furtherdetails can be found in \[Re#eL 1990\].The critical point here is that now that this region hasbeen learned relative to the LM key points, it will changeposition and size when the LM key points change.
Thisis illustrated in (c).
Thus, the region is parameterizedby the LM key points.2.3 Combining RepresentationsWhile the use of parameterized regions solves much ofthe problem of generalizability across LMs, it is not suf-ficient by itself.
Two objects could have identical keypoints, and yet differ in actual shape.
Since part of thedefinition of "above" is that the TR is not in the inte-rior of the LM, and since the shape of the interior ofthe LM cannot be derived from the key points alone, thekey points are an underspecification of the LM for ourpurposes.The complete LM specification includes a bitmap ofthe interior of the LM, the "LM interior map".
This issimply a bitmap representation f the LM, with thosebits set which fall in the interior of the object.
As weshall see in greater detail in Section 5, this representa-tion is used together with parameterized regions in learn-ing the perceptual grounding for spatial term semantics.This bitmap representation helps in the case mentionedabove, since although the triangle and square will haveidentical key points, their LM interior maps will differ.In particular, since part of the learned "definition" of apoint being above a LM should be that it may not be inthe interior of the LM, that would account for the dif-ference in shape of the regions located above the squareand above the triangle.Parameterized regions and the bitmap representation,when used together, provide the system with the abilityto generalize across LMs.
We shall see examples of thisafter a presentation of the second major problem to betackled.
(a)omoel~lm~ w w m w ~ w w l  nououooooono~n~\ :/ \(b)" :%.
/ , :  .
.
.
.
.
.
.
.
.
.
.
.
.
.
(c)Figure 2: Parameterized Regions139Figure 3: Learning "Above" Without Negative Instances3 Learning Without Explicit NegativeEvidence3.1 The  Prob lemResearchers in child language acquisition have often ob-served that the child learns language apparently with-out the benefit of negative evidence \[Braine, 1971;Bowerman, 1983; Pinker, 1989\].
While these researchershave focused on the "no negative vidence" problem asit relates to the acquisition of grammar, the problem isa general one, and appears in several different aspectsof language acquisition.
In particular, it surfaces in thecontext of the learning of the semantics of lexemes forspatial relations.
The methods used to solve the prob-lem here are of general applicability, however, and arenot restricted to this particular domain.The problem is best illustrated by example.
ConsiderFigure 3.
Given the landmark (labeled "LM"), the taskis to learn the concept "above".
We have been givenfour positive instances, marked as small dotted circles inthe figure, and no negative instances.
The problem isthat we want to generalize so that we can recognize newinstances of "above" when they are presented, but sincethere are no negative instances, it is not clear where theboundaries of the region "above" the LM should be.
Onepossible generalization is the white region containing thefour instances.
Another possibility is the union of thatwhite region with the dark region surrounding the LM.Yet another is the union of the light and dark regionswith the interior of the LM.
And yet another is the cor-rect one, which is not closed at the top.
In the absence ofnegative xamples, we have no obvious reason to preferone of these generalizations over the others.One possible approach would be to take the smallestregion that encompasses all the positive instances.
Itshould be clear, however, that this will always lead toclosed regions, which are incorrect characterizations ofsuch spatial concepts as "above" and "outside".
Thus,this cannot be the answer.And yet, humans do learn these concepts, apparentlyin the absence of negative instances.
The following sec-tions indicate how that learning might take place.3.2 A Poss ib le  So lut ion  and  its DrawbacksOne solution to the "no negative evidence" problemwhich suggests itself is to take every positive instancefor one concept to be an implicit negative instance forall other spatial concepts being learned.
There are prob-lems with this approach, as we shall see, but they aresurmountable.There are related ideas present in the child lan-guage literature, which support he work presented here.\[Markman, 1987\] posits a "principle of mutual exclusiv-ity" for object naming, whereby a child assumes thateach object may only have one name.
This is to beviewed more as a learning strategy than as a hard-and-fast rule: clearly, a given object may have many names(an office chair, a chair, a piece of furniture, etc.).
Themethod being suggested really amounts to a principle ofmutual exclusivity for spatial relation terms: since eachspatial relation can only have one name, we take a pos-itive instance of one to be an implicit negative instancefor all others.In a related vein, \[Johnston and Slobin, 1979\] notethat in a study of children learning locative terms in En-glish, Italian, Serbo-Croatian, and qMrkish, terms werelearned more quickly when there was little or no syn-onymy among terms.
They point out that children seemto prefer a one-to-one meaning-to-morpheme apping;this is similar to, although not quite the same as, themutual exclusivity notion put forth here.
1In linguistics, the notion that the meaning of a givenword is partly defined by the meanings of other words inthe language is a central idea of structuralism.
This hasbeen recently reiterated by \[MacWhinney, 1989\]: "thesemantic range of words is determined by the particularcontrasts in which they are involved".
This is consonantwith the view taken here, in that contrasting words willserve as implicit negative instances to help define theboundaries of applicability of a given spatial term.There is a problem with mutual exclusivity, however.Using it as a method for generating implicit negative in-stances can yield many false negatives in the training set,i.e.
implicit negatives which really should be positives.Consider the following set of terms, which are the oneslearned by the system described here:?
above?
below?
O i l?
off1 They are not quite the same since a difference in meaningneed not correspond to a difference in actual reference.
Whenwe call a given object both a "chair" and a "throne", these aredifferent meanings, and this would thus be consistent with aone-to-one meaning-to-morpheme mapping.
It would not beconsistent with the principle of mutual exclusivity, however.140?
inside?
outside?
to the l e f t  of?
to the right ofIf we apply mutual exclusivity here, the problem of falsenegatives arises.
For example, not all positive instancesof "outside" are accurate negative instances for "above",and indeed all positive instances of "above" should infact be positive instances of "outside", and are insteadtaken as negatives, under mutual exclusivity.
"Outside" is a term that is particularly badly affectedby this problem of false implicit negatives: all of thespatial terms listed above except for "in" (and "outside"itself, of course) will supply false negatives to the trainingset for "outside".The severity of this problem is illustrated in Figure 4.In these figures, which represent training data for thespatial concept "outside", we have tall, rectangular land-marks, and training points 2 relative to the landmarks.Positive training points (instances) are marked with cir-cles, while negative instances are marked with X's.
In(a), the negative instances were placed there by theteacher, showing exactly where the region not outsidethe landmark is.
This gives us a "clean" training set, butthe use of teacher-supplied xplicit negative instances iprecisely what we are trying to get away from.
In (b), thenegative instances hown were derived from positive in-stances for the other spatial terms listed above, throughthe principle of mutual exclusivity.
Thus, this is the sortof training data we are going to have to use.
Note thatin (b) there are many false negative instances among thepositives, to say nothing of the positions which have beenmarked as both positive and negative.This issue of false implicit negatives is the centralproblem with mutual exclusivity.3.3 Salvaging Mutual ExclusivityThe basic idea used here, in salvaging the idea of mu-tual exclusivity, isto treat positive instances and implicitnegative instances differently during training:Implicit negatives are viewed as supplying onlyweak negative vidence.The intuition behind this is as follows: since the im-plicit negatives are arrived at through the application ofa fallible heuristic rule (mutual exclusivity), they shouldcount for less than the positive instances, which are allassumed to be correct.
Clearly, the implicit negativesshould not be seen as supplying excessively weak neg-ative evidence, or we revert to the original problem oflearning in the (virtual) absence of negative instances.But equally clearly, the training set noise supplied byfalse negatives i quite severe, as seen in the figure above.So this approach is to be seen as a compromise, so thatwe can use implicit negative vidence without being over-whelmed by the noise it introduces in the training setsfor the various patial concepts.The details of this method, and its implementation u -der back-propagation, are covered in Section 5.
However,2I.e.
trajectors consisting of a single point each(a)OOOQ X X - M .
Oe o m o oX X - - - OG .
.
.
.
.
OX - - - XO = , X o XI ~ m m lo Lx  ?
-~OQOO0O?
?X XX x x Q x xX x x x xx ~- .
- x - I  xx  ?X X O - X ?
?
- 0 X?
- - - X X X0 X X .
.
.
.
.
0X X Q - - x - ?
0X X ?
?
?
- ?
XX - ?
* X XX X Q - X o - * X0 ?
o - X .
XX X " " " ? "
0 X 0x O ~-  x -.-~ ?0 G0 X X X XX X (b)Figure 4: Ideal and Realistic Training Sets for "Outside"141this is a very general solution to the "no negative vi-dence" problem, and can be understood independently ofthe actual implementation details.
Any learning methodwhich allows for weakening of evidence should be able tomake use of it.
In addition, it could serve as a means foraddressing the "no negative vidence" problem in otherdomains.
For example, a method analogous to the onesuggested here could be used for object naming, the do-main for which Markman suggested mutual exclusivity.This would be necessary if the problem of false implicitnegatives i as serious in that domain as it is in this one.4 Resu l tsThis section presents the results of training.Figure 5 shows the results of learning the spatial term"outside", first without negative instances, then usingimplicit negatives obtained through mutual exclusivity,but without weakening the evidence given by these, andfinally with the negative vidence weakened.The landmark in each of these figures is a triangle.The system was trained using only rectangular land-marks.The size of the black circles indicates the appropri-ateness, as judged by the trained system, of using theterm "outside" to refer to a particular position, relativeto the LM shown.
Clearly, the concept is learned bestwhen implicit negative vidence is weakened, as in (c).When no negatives at all are used, the system overgen-eralizes, and considers even the interior of the LM to be"outside" (as in (a)).
When mutual exclusivity is used,but the evidence from implicit negatives i not weakened,the concept is learned very poorly, as the noise from thefalse implicit negatives hinders the learning of the con-cept (as in (b)).
Having all implicit negatives upplyonly weak negative vidence greatly alleviates the prob-lem of false implicit negatives in the training set, whilestill enabling us to learn without using explicit, teacher-supplied negative instances.It should be noted that in general, when using mutualexclusivity without weakening the evidence given by im-plicit negatives, the results are not always identical withthose shown in Figure 5(b), but are always of approxi-mately the same quality.Regarding the issue of generalizability across LMs, twopoints of interest are that:?
The system had not been trained on an LM in ex-actly this position.?
The system had never been trained on a triangle ofany sort.Thus, the system generalizes well to new LMs, andlearns in the absence of explicit negative instances, asdesired.
All eight concepts were learned successfully, andexhibited similar generalization to new LMs.5 Deta i l sThe system described in this section learns perceptually-grounded semantics for spatial terms using the(a)O000000000O000@0000@O000000000000000000eO000000000000O00000@OOO0000000000000000@00O0000@OOO00000000@O000OO0@O00OOO000O0~O00O00O@OO00OO000OO@00O000OOO0000000000@0000000@000000~0000@00000O000000~0000@OOOOOOOOO0~OOO0@oooooooo~M~OOOOeIoooooo~M~OOOOelooooo~M~~Ooooe Ioooo~l l~M~J~ooooe loooooooooooooooooooe l00OOOOO0OOOOOOOO0OO@l000O0OO0OO0OO000000~IOOO0000000000000000@I(b)"I 6oo0000@000- .
ooo ooe0000@000. ,  .oooe  ooo0000@0000*  .oooe  oooOOO000000 ?
.OOOe ?
eooO00@OOOO,  ooeoe  oooe000@0000-  oooee  ?
ooo000@@O000 ooooe  ooe000@00000,  -ooooeooo00O@00000-~ooooe@oooo00@0000~\ [~Jooooeooooo00@00~ooooeooooo000~~ooooaooooo0W~m~~ooooeoooooE l~~oeeoeoooo l 'd~l ;~ J~J J J J J~ooooqeooo- .oooooooooooooaeooo- .oooooooooooooaoooo-~gOOOOOO00ooo l lVOID-  QOQgOQDOOOOO|!I ~ o o e o l m ~ M ~ m A ~ d(c)o@ooooo@@oooooooooo@ooooooooooooooooooo@@oooooo@ooooooooooo@@oooooooooooooooooo@@oooooooooooooooooo@oooooooooooooooooooeooooooooooooooooooo@ooooooo@ooooooooooo@oooooooo@ooooo~ooooeooooooooooooE I I~\ ]oooo@oooooooo00E I I~!~00oooq looooooo0130131~J l~00oOOelooooo0~131~D~E~l~0oooe loooO~\[3 \ [Z I I~E JOOOO@looooooooooooo0ooooo@looooooooooooooooooo| Joooooooooooooooo000@ I0000000000000o0000011Figure 5: "Outside" without Negatives, and with Strongand Weak Implicit Negatives142quiekprop 3 algorithm \[Fahlman, 1988\], a variant onback-propagation \[Rumelhart and McClelland, 1986\].This presentation begins with an exposition of the rep-resentation used, and then moves on to the specific net-work architecture, and the basic ideas embodied in it.The weakening of evidence from implicit negative in-stances is then discussed.5.1 Representat ion  o f  the LM and TRAs mentioned above, the representation scheme for theLM comprises the following:?
A bitmap in which those pixels corresponding to theinterior of the LM are the only ones set.?
The z, y coordinates of several "key points" of theLM, where z and y each vary between 0.0 and 1.0,and indicate the location of the point in questionas a fraction of the width or height of the image.The key points currently being used are the centerof mass (CoM) of the LM, and the four corners ofthe LM's bounding box (UL: upper left, UR: upperright, LL: lower left, LR: lower right).The (punctate) TR is specified by the z, V coordinatesof the point.The activation of an output node of the system, oncetrained for a particular spatial concept, represents theappropriateness of using the spatial term in describingthe TR's location, relative to the LM.5.2 Arch i tec tureFigure 6 presents the architecture of the system.
Theeight spatial terms mentioned above are learned simul-taneously, and they share hidden-layer representations.5.2.1 Recept ive  FieldsConsider the right-hand part of the network, whichreceives input from the LM interior map.
Each of thethree nodes in the cluster labeled "I" (for interior) has areceptive field of five pixels.When a TR location is specified, the values of thefive neighboring locations hown in the LM interior map,centered on the current TR location, are copied up to thefive input nodes.
The weights on the links between thesefive nodes and the three nodes labeled "I" in the layerabove define the receptive fields learned.
When the TRposition changes, five new LM interior map pixels will be"viewed" by the receptive fields formed.
This allows thesystem to detect the LM interior (or a border betweeninterior and exterior) at a given point and to bring thatto bear if that is a relevant semantic feature for the setof spatial terms being learned.5.2.2 Parameter i zed  RegionsThe remainder of the network is dedicated to com-puting parameterized regions.
Recall that a parameter-ized region is much the same as any other region whichmight be learned by a perceptron, except hat the lines3Quickprop gets its name from its ability to quickly con-verge on a solution.
In most cases, it exhibits faster conver-gence than that obtained using conjugate gradient methods\[Fahlman, 1990\].which define the relevant half-planes are constrained togo through specific points.
In this case, these are the keypoints of the LM.A simple two-input perceptron unit defines a line inthe z, tt plane, and selects a half-plane on one side of it.Let wffi and w v refer to the weights on the links fromthe z and y inputs to the pereeptron unit.
In general,if the unit's function is a simple threshold, the equationfor such a line will bezw~ + wy = O, (1)i.e.
the net input to the perceptron unit will beherin = actor.
+ yltO~.
(2)Note that this line always passes through the origin:(0,0).If we want to force the line to pass through a particularpoint (zt,yt) in the plane, we simply shift the entirecoordinate system so that the origin is now at (zt, yt).This is trivially done by adjusting the input values suchthat the net input to the unit is now,,et,,, = (x - x , )w ,  + (V - V, )w, .
(3)Given this, we can easily force lines to pass throughthe key points of an LM, as discussed above, by setting(zt, V~) appropriately for each key point.
Once the sys-tem has learned, the regions will be parameterized bythe coordinates of the key points, so that the spatialconcepts will be independent of the size and position ofany particular LM.Now consider the left-hand part of the network.
Thisaccepts as input the z, y coordinates of the TR locationand the LM key points, and the layer above the inputlayer performs the appropriate subtractions, in line withequation 3.
Now each of the nodes in the layer abovethat is viewing the TR in a different coordinate system,shifted by the amount specified by the LM key points.Note that in the BB cluster there is one node for eachcorner of the LM's bounding-box, while the CoM clus-ter has three nodes dedicated to the LM's center of mass(and thus three lines passing through the center of mass).This results in the computation, and through weight up-dates, the learning, of a parameterized region.Of course, the hidden nodes (labeled 'T') that receiveinput from the LM interior map are also in this hiddenlayer.
Thus, receptive fields and parameterized regionsare learned together, and both may contribute to thelearned semantics of each spatial term.
Further detailscan be found in \[Regier, 1990\].5.3 Imp lement ing  "Weakened"  Mutua lExc lus iv i tyNow that the basic architecture and representations havebeen covered, we present he means by which the evi-dence from implicit negative instances is weakened.
Itis assumed that training sets have been constructed us-ing mutual exclusivity as a guiding principle, such thateach negative instance in the training set for a given spa-tial term results from a positive instance for some otherterm.143above below on0 0 0off in out left0 0 0 0rightBBI  i CoMz yUL(LM)z yUR(LM)z y(TR)z y z yLL LR(LM) (LM)ZTRz yCoM(LM)!rFigure 6: Network Architecture144?
Evidence from implicit negative instances is weak-ened simply by attenuating the error caused bythese implicit negatives.?
Thus, an implicit negative instance which yields anerror of a given magnitude will contribute l ss to theweight changes in the network than will a positiveinstance of the same error magnitude.This is done as follows:Referring back to Figure 6, note that output nodeshave been allocated for each of the spatial terms to belearned.
For a network such as this, the usual error termin back-propagation is1E = ~ ~_,(t~,p - oj,p) 2 (4)J,Pwhere j indexes over output nodes, and p indexes overinput patterns.We modify this by dividing the error at each outputnode by some number/~j,p, dependent on both the nodeand the current input pattern.1 V .
( t i ,p  - oj,p E = ~ ~ ~;  )2 (5)$,PThe general idea is that for positive instances of somespatial term, f~j,p will be 1.0, so that the error is not at-tenuated.
For an implicit negative instance of a term,however, flj,p will be some value Atten, which corre-sponds to the amount by which the error signals fromimplicit negatives are to be attenuated.Assume that we are currently viewing input patternp, a positive instance of "above".
'then the target valuefor the "above" node will be 1.0, while the target valuesfor all others will be 0.0, as they are implicit negatives.Here, flabove,p = 1.0, and fll,p = Atten, Vi ~ above.The value Atten = 32.0 was used successfully in theexperiments reported here.6 ConclusionThe system presented here learns perceptually-groundedsemantics for the core senses of eight English preposi-tions, successfully generalizing to scenes involving land-marks to which the system had not been previously ex-posed.
Moreover, the principle of mutual exclusivity issuccessfully used to allow learning without explicit nega-tive instances, despite the false negatives in the resultingtraining sets.Current research is directed at extending this work tothe case of arbitrarily shaped trajectors, and to handlingpolysemy.
Work is also being directed toward the learn-ing of non-English spatial systems.References\[Bowerman, 1983\] Melissa Bowerman, "How Do Chil-dren Avoid Constructing an Overly General Grammarin the Absence of Feedback about What is Not a Sen-tence?," In Papers and Reports on Child LanguageDevelopment.
Stanford University, 1983.\[Braine, 1971\] M. Braine, "On Two Types of Modelsof the Internalization of Grammars," In D. Slobin,editor, The Ontogenesis of Grammar.
Academic Press,1971.\[Fahlman, 1988\] Scott Fahlman, "Faster-Learning Vari-ations on Back Propagation: An Empirical Study," InProceedings of the 1988 Connectionist Models SummerSchool.
Morgan Kaufmann, 1988.\[Fahlman, 1990\] Scott Fahlman, (personal communica-tion), 1990.\[Feldman et al, 1990\] J. Feldman, G. Lakoff, A. Stolcke,and S. Weber, "Miniature Language Acquisition: ATouchstone for Cognitive Science," Technical ReportTR-90-009, International Computer Science Institute,Berkeley, CA, 1990, also in the Proceedings of the 12thAnnual Conference of the Cognitive Science Society,pp.
686-693.\[~lohnston a d Slobin, 1979\] Judith Johnston and DanSlobin, "The Development of Locative Expressions inEnglish, Italian, Serbo-Croatian and Turkish," Jour-nal of Child Language, 6:529-545, 1979.\[MacWhinney, 1989\] Brian MacWhinney, "Competitionand Lexical Categorization," In Linguistic Categoriza-tion, number 61 in Current Issues in Linguistic The-ory.
John Benjamins Publishing Co., Amsterdam andPhiladelphia, 1989.\[Markman, 1987\] Ellen M. Markman, "How ChildrenConstrain the Possible Meanings of Words," In Con-cepts and conceptual development: Ecological and in-tellectual factors in categorization.
Cambridge Univer-sity Press, 1987.\[Minsky and Papert, 1988\] Marvin Minsky and Sey-mour Papert, Perceptrons (Expanded Edition), MITPress, 1988.\[Pinker, 1989\] Steven Pinker, Learuability and Cogni-tion: The Acquisition of Argument Structure, MITPress, 1989.\[Regier, 1990\] Terry Regier, "Learning Spatial TermsWithout Explicit Negative Evidence," Technical Re-port 57, International Computer Science Institute,Berkeley, California, November 1990.\[Rumelhart and McClelland, 1986\] David Rumelhartand James McClelland, Parallel Distributed Proccess-ing: Ezplorations in the microstructure of cognition,MIT Press, 1980.\[Weber and Stolcke, 1990\] Susan Hollbach Weber andAndreas Stolcke, "L0: A Testbed for Miniature Lan-guage Acquisition," Technical Report TR-90-010, In-ternational Computer Science Institute, Berkeley, CA,1990.145
