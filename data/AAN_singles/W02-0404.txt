Revisions that Improve Cohesion in Multi-document Summaries:A Preliminary StudyJahna C. Otterbacher, Dragomir R. Radev and Airong LuoSchool of InformationUniversity of MichiganAnn Arbor, MI 48109-1092{clear, radev, airongl}@umich.eduAbstractExtractive summaries produced from multiplesource documents suffer from an array of prob-lems with respect to text cohesion.
In this pre-liminary study, we seek to understand whatproblems occur in such summaries and how of-ten.
We present an analysis of a small corpusof manually revised summaries and discuss thefeasibility of making such repairs automati-cally.
Additionally, we present a taxonomy ofthe problems that occur in the corpus, as wellas the operators which, when applied to thesummaries, can address these concerns.
Thisstudy represents a first step toward identifyingand automating revision operators that couldwork with current summarization systems inorder to repair cohesion problems in multi-document summaries.1 IntroductionWith the increasing availability of online newssources, interest in automatic summarization hascontinued to grow in recent years.
Many systemshave been developed for this purpose, includingthose that can produce summaries based on severaldocuments (Multi-document summarization, orMDS).
Generally speaking, most of these systemswork by extracting sentences from the originaltexts.
Although significant improvements continueto be made to such summarizers, they still cannotproduce summaries that resemble those writtenmanually by humans.
One area in particular inwhich the automatically produced summaries dif-fer markedly is text cohesion.Whether a summary is produced from one ormore documents, important context may be ex-cluded from the summary that disrupts its readabil-ity.
A text is not a random collection of sentences,but rather, each sentence plays a role in conveyingthe ideas the author wants to express.
Selectingsentences from multiple texts one at a time disre-gards this interdependence between sentences.
Asa result, summaries often suffer from problems ofcohesion.1.1 Text cohesion and coherence[Halliday & Hasan, 1976] offer a clear definitionfor text cohesion:[The concept of cohesion] refers to relations ofmeaning that exist within the text, and that define itas a text.
Cohesion occurs where the interpretationof some element in the discourse is dependent onthat of another (p.2).It is this property of cohesion that allows thereader to comprehend the overall meaning of atext, and to understand the author?s intentions.Therefore, in automatically produced summaries,cohesion problems should be resolved.
Otherwise,the resulting text may be unintelligible, or worse,misleading.1.2 Problems of cohesion in automatically pro-duced summariesThe following is an example of a summary pro-duced automatically from one source document.
[1] More than 130 bodies are reported to havebeen recovered after a Gulf Air jet carrying 143people crashed into the Gulf off Bahrain onWednesday.
[2] Distraught relatives also gatheredat Cairo airport, demanding information.
[3] Healso declared three days of national mourning.
[4]He said the jet fell ?sharply, like an arrow.
?The most obvious problem with this summary isthat in the last two sentences, the pronouns have noPhiladelphia, July 2002, pp.
27-36.
Association for Computational Linguistics.Proceedings of the Workshop on Automatic Summarization (including DUC 2002),antecedents; as a result, the reader does not knowwho the subjects of the sentences are.
In addition,the adverb ?also,?
used in both the second and thirdsentences, makes reference to previous events notdescribed in the summary.
Another concern is thatthere seems to be no transition between the sen-tences.
The context from the source article neces-sary to make the text cohesive is missing from thesummary.
As a result, the summary is unintelligi-ble.1.3 Text cohesion in MDSUsing multiple documents to generate a summaryfurther complicates the situation.
As contended by[Goldstein et al 2000] a multi-document summarymay contain redundant messages, since a cluster ofnews articles tends to cover the same main pointand shared background.
In addition, articles fromvarious sources could contradict one another, as tohow or when an event developed.
Finally, sincethe source articles are not all written simultane-ously, they may describe different stages of thedevelopment of an event.
Not only do news storiescome to different conclusions at various stages inan event, but also the attitudes of writers maychange.Multi-document summaries may suffer furtherfrom problems of cohesion since their source arti-cles may be written by different authors.
Not onlydo writers have their own styles, they have theoverarching structure of the article in mind whenproducing it.
As a result, in MDS we are morelikely to encounter text that is not cohesive.Previous research has addressed revision insingle-document summaries [Jing & McKeown,2000] [Mani et al 1999] and has suggested thatrevising summaries can make them more informa-tive and correct errors.
We believe that a generate-and-revise strategy might also be used in creatingbetter multiple-document summaries, within theframework of current extractive summarizationsystems.
However, as mentioned previously, thereis reason to believe that multi-document summa-ries suffer from many different coherence prob-lems and that such problems occur more often thanin single-document summaries.
Therefore, an im-portant preliminary step in determining how wemight revise such summaries is to closely examinethe cohesion problems that occur in multi-document summaries.In the current paper we analyze a small corpusof manually revised multi-document summaries.We present a taxonomy of pragmatic concernswith respect to cohesion in the summaries, as wellas the operators that can address them.
Finally, wewill discuss the feasibility of implementing suchrevisions automatically, which we hope to addressin our future work.2 Background and previous work2.1 Theories on discourse structureRhetorical Structure Theory (RST) [Mann &Thompson, 1988] has contributed a great deal tothe understanding of the discourse of writtendocuments.
RST describes the coherence nature ofa text and is based on the assumption that the ele-mentary textual units are non-overlapping textspans.
The central concept of RST is the rhetoricalrelation, which indicates the relationship betweentwo spans.RST can be used in sentence selection for sin-gle document summarization [Marcu, 1997].However, it cannot be applied to MDS.
In RST,text coherence is achieved because the writer in-tentionally establishes relationships between thephrases in the text.
This is not the case in MDS,where sentences are extracted from differentsource articles, written by various authors.Inspired by RST, [Radev, 2000] endeavored toestablish a Cross-document Structure Theory(CST) that is more appropriate for MDS.
CST fo-cuses on the relationships between sentences thatcome from multiple documents, which vary sub-stantially from those between sentences in thesame text.
Such relationships include identity,paraphrase and subsumption (one sentence con-tains more information than the other).2.2 Computational models of text coherenceBased on RST, [Marcu, 2000] established aRhetorical Parser.
The parser exploits cue phrasesin an algorithm that discovers discourserelationships between phrases in a text.
Thisparser can be used to extract sentences in single-document summarization.
To contrast,[Harabagiu, 1999] concentrated on the derivationof a model that can establish coherence relations ina text without relying on cue phrases.
She madeuse of large lexical databases, such as Wordnet,and of path finding algorithms that generate thealgorithms that generate the cohesion structure oftexts represented by a lexical path.
[Hovy, 1993] summarized previous work thatfocused on the automated planning and generationof multi-sentence texts using discourse relation-ships.
Text generation is relevant to MDS, as wecan view MDS as an attempt to generate a new textby reusing sentences from different sources.
Thesystems discussed in [Hovy, 1993] relied on aknowledge base and a representation of discoursestructure.
The dependency of text generation onknowledge of discourse structure was emphasized.2.3 Revision of single-document summaries[Mani et al 1999] focused on the revision of sin-gle-document summaries in order to improve theirinformativeness.
They noted that such revisionmight also fix ?coherence errors.?
Three types ofrevision operators were identified: sentence com-paction, sentence aggregation and sentencesmoothing.
To contrast, [Jing & McKeown, 2000]concentrated on analyzing human-written summa-ries in order to determine how professionals con-struct summaries.
They found that most sentencescould be traced back to specific cut-and-paste op-erations applied to the source document.
Theyidentified six operations and used them to imple-ment an automatic revision module.2.4 Temporal ordering of events[Filatova & Hovy, 2001] addressed the issue ofresolving temporal references in news stories.
Al-though events in articles are not always presentedin chronological order, readers must be able to re-construct the timeline of events in order to com-prehend the story.
They endeavored to develop amodule that could automatically assign a timestamp to each clause in a document.
Using a syn-tactic parser, patterns were discovered as to whichsyntactic phrases tend to indicate the occurrence ofa new event.
In MDS, the correct temporal rela-tionships between events described in the extractedsentences often needs to be reestablished, sincethey may be incorrect or unclear.
[Barzilay et al 2001] evaluated three algo-rithms for sentence ordering in multi-documentsummaries.
One algorithm implemented was theChronological Ordering algorithm.
However, theresulting summaries often suffered from abruptchanges in topic.
After conducting an experimentin which they studied how humans manually or-dered sentences in a summary, they concluded thattopically related sentences should be grouped to-gether.
The Chronological Ordering algorithm wasaugmented by introducing a cohesion constraint.The evaluation of the output summaries demon-strated a significant improvement in quality.3 Revision-based system architectureThe proposed architecture of our system, whichwould implement the generate-and-revise approachto summarization, is depicted in Figure 1.
Input tothis system is a cluster of source documents relatedto the same topic.
Next, sentence extraction takesplace, in which important sentences in the articlesare identified.
The output of this module is an ex-tract, which lists the sentences to be included in thesummary.In the next stage, Cross-document StructureTheory (CST) relationships are established.
Spe-cific relationships between sentences are identified.Here, a CST-enhancement procedure [Zhang et al2002] may take place, ensuring that interdependentsentences appear together in a summary.
Sen-tences may also be reordered in the summary withrespect to their temporal relations, topic, or othercriteria.The next stage in the process is the revisionmodule.
First, high level revision operators arechosen, with respect to the cohesion problems thatneed repair.
Afterwards, the specific lexical itemsto be added, deleted or modified are chosen.
Theoutput of this module is the revised, enhancedsummary.3.1 The MEAD summarizerThe MEAD summarizer [Radev et al 2000][Radev et al2002] is based on sentence extractionand uses a linear combination of three features torank the sentences in the source documents.
Thefirst of the three features is the centroid score,which quantifies the centrality of a sentence to theoverall cluster of documents.
The second is theposition score, which assigns higher scores to sen-tences that are closer to the beginning of the docu-ment.
The third feature, length, gives a higherscore to longer sentences.
Using a linear combina-tion of the three features, sentences are ranked byscore and added to the summary until the desiredlength is attained.SummarizationCST IdentificationC:1 - B:2B:13D:5 - A:1Sentence ReorderingTemporal, TopicalRevisionHigh-level OperatorsLexical ChoiceA:1 - 1.
<Delete> time exp2.
ThursdayC:1B:2D:5 -1.
<Add> adverb2.
MeanwhileMulti-DocumentExtractCST EnhancedSummaryCST EnhancedRevised SummarySourceDocumentsA B C DC:1B:13D:5Figure 1: Revision-based MDS architecture:Letters denote documents; numbers denote sen-tence numbers (within documents)4 Data and procedureWe generated a corpus of summaries using theMEAD summarizer.
The original documents comefrom three sources ?
DUC 2001, the Hong KongNews corpus, and the GA3-11 data set.
One clus-ter of related news articles was chosen from eachsource.
The DUC 2001 articles describe the 1991eruption of Mount Pinatubo in the Philippines.This cluster, which is not typical of the DUC data,focuses on this single event and its subevents overa 2-week time period.
Those taken from the HKcorpus are about government initiatives surround-ing the problem of drug rehabilitation.
Due to theexpense and labor involved in the generation andrevision of multi-document summaries, we haveused a subset of 15 summaries from our corpus inorder to develop our revision taxonomy and to pre-sent some initial findings.
Our future revisionstudies will employ a much larger set of data.The summaries were revised manually by thefirst author.
This was a three-step process that in-volved identifying each problem, choosing anoperator that could address the problem and thenselecting the lexical items to which the operatorshould be applied.
It is important to note that mul-tiple lexical choices are possible in some cases.Since we were interested in identifying alltypes of cohesion problems as well as consideringall possibilities for addressing these problems, thereviser was permitted to make any revision neces-sary in order to correct problems in the summaries.Obviously, a module that makes revisions auto-matically would be much more restricted in its setof revision operators.
However, since a major goalfor this paper was to establish a taxonomy of prob-lems specific to multi-document summarizationand to consider the complexities involved in mak-ing repairs in MDS, we did not place such restric-tions on the reviser.
Rather, she appliedcorrections to the summaries as to make them asintelligible as possible, given the sentences chosenby the summarizer.Source Length(sentences)#SourcedocumentsDUC 2001 3 3DUC 2001 3 3DUC 2001 5 3DUC 2001 6 5DUC 2001 9 5GA3-11 3 3GA3-11 3 5GA3-11 6 5GA3-11 8 3GA3-11 7 3HK-125 3 3HK-125 5 3HK-125 6 5HK-125 5 5HK-125 8 3Table 1: Summaries from training data4.1 Revision example<DELETE-place stamp> Cairo, Egypt ?
</DELETE> Thecrash of a Gulf Air flight that killed 143 people in Bahrain<ADD-time exp-day>Wednesday </ADD> is a disturbingd?j?
vu for Egyptians: It is the second plane crash within ayear to devastate this Arab country.
Egypt, which lacks theoil wealth of the Gulf and has an economy struggling to re-vive from decades of socialist stagnation, has a long tradi-tion of sending workers to the Gulf to fill everything fromskilled to menial jobs.
<DELETE-place stamp> Manama,Bahrain (AP) ?
</DELETE> <ADD-time exp-day> On Fri-day, </ADD> three bodies wrapped in cloth, one the size ofa small child, were lain before the faithful in the GrandMosque during a special prayer for the dead in honor of the<DELETE-redundancy> 143 </DELETE> victims of the<DELETE-overspecified entity> Gulf Air </DELETE>crash.
Bahrain?s Prime Minister Sheik Khalifa bin SalmanAl Khalifa and other top officials stood side-by-side with2,000 Muslins reciting funeral prayers before the bodies,<DELETE-redundancy> which were among the 107 adultsand 36 children killed in Wednesday?s air disaster,</DELETE> said Information Ministry spokesman Syed el-Bably.Figure 2: Revised multi-document summaryThe above figure shows an example of a revisedsummary that was produced from three source arti-cles from the GA3-11 corpus.
The news storieswere collected live from the web, and come fromtwo different sources www.foxnews.com andwww.abcnews.com.
The revision operator usedand the corresponding pragmatic concern precedethe modified text in pointed brackets.
This type ofmarkup scheme was used because it enables us touse simple Perl scripts to move between the origi-nal and revised versions of the summaries.5 Taxonomy of revision strategiesBased on our corpus of revised summaries, wehave identified five major categories of pragmaticconcerns related to text cohesion in multi-document summaries:1) Discourse ?
Concerns the relationships be-tween the sentences in a summary, as well asthose between individual sentences and theoverall summary.2) Identification of entities ?
Involves the reso-lution of referential expressions such thateach entity mentioned in a summary can eas-ily be identified by the reader.3) Temporal ?
Concerns the establishment ofthe correct temporal relationships betweenevents.4) Grammar ?
Concerns the correction ofgrammatical problems, which may be the re-sult of juxtaposing sentences from differentsources, or due to the previous revisions thatwere made.5) Location/setting ?
Involves establishingwhere each event in a summary takes place.Explanations of the specific pragmatic concerns ineach category, as well as their corresponding op-erator(s), are detailed in the appendix.
Overall,160 revisions were made across the 15 summaries.Pragmaticcategory # of revisions% of totalrevisionsDiscourse 54 34%Entities 41 26%Temporal 35 22%Grammar 20 12%Place/setting 10 6%Table 2: Revisions by pragmatic categoryThe majority (82%) of the revisions fall intothe first three categories.
This is not surprising, asin MDS, we expect to find many problems relatingto discourse ?
such as abrupt topic shifts or redun-dant messages.
Additionally, concerns relating tothe identification of entities in the text are likely tooccur when the sentence from the original docu-ment that introduced an entity is not included inthe resulting summary, but sentences that makereference to the entity are included.
Finally, it maynot be clear when events described in a summaryoccurred.
This could be because sentences whichstated when the event occurred were left out of thesummary or because the sentences include relativetime expressions such as ?today?
even though thestories were written at different times or on differ-ent days.Revisions relating to grammar or to establish-ing where an event occurred were less frequentlyused, accounting for only 12% and 6% of the totalrepairs, respectively.
Sentences extracted from theoriginal news stories are usually grammatical.However, problems related to grammar may arisefrom previous revisions.
In our corpus, the placeor setting of an event was typically obvious in thesummary and rarely required repair.Next, we present the analysis of revisionswithin each of the five categories.
We are inter-ested in revising our summaries to be as coherentas possible, without having to implement compli-cated and knowledge-intensive discourse models.Therefore, we will discuss the feasibility of im-plementing the revisions in our taxonomy auto-matically.5.1 Discourse-related concerns in MDSIt is intuitive that problems relating to discourseare abundant in our summaries and, at the sametime, that such repairs would be the most difficultto make.
The first obstacle is the detection of eachof these concerns, which requires knowledge of therhetorical relations of the sentences in the sum-mary.Problem Number (%)1) Topic shift 24  (45%)2) Purpose 18  (33%)3) Contrast   6  (11%)4) Redundancy   6  (11%)5) Conditional   0Total 54Table 3: Discourse-related revisionsIn all the instances of topic shift and lack of pur-pose in our corpus, a phrase or an entire sentencewas added to provide a transition or motivation forthe troublesome sentence.
Therefore, our modulewould require the ability to generate text, in orderto repair these problems, which occur often in oursummaries.5.2 Identification of entities in MDSNine specific problems were found that concernthe reader?s ability to identify each entity men-tioned in a summary.
Most of these revisionscould be made using rewrite rules.
For example, ifit can be determined that a definite article is usedwhen a (non-proper noun) entity is mentioned forthe first time, the misused definite article could bereplaced with the corresponding indefinite article.The most frequent problem, underspecified en-tity, is the most difficult one to correct.
This dis-fluency typically occurs where an entity is referredto by a proper noun or other noun phrase, such asthe name of a person or organization, but has notitle or further description.
In such cases, the miss-ing information may be found in the source docu-ment only.Problem Number (%)1) Underspecified entity 15  (38%)2) Misused quantifier   6  (15%)3) Overspecified entity   5  (12%)4) Repeated entity   5  (12%)5) Bare anaphor   4  (10%)6) Misused definite article   3  (  7%)7) Misused indef.
Article   1  (  2%)8) Missing article   1  (  2%)9) Missing entity   1  (  2%)Total 41Table 4: Revisions concerning entity identificationTherefore, to correct the underspecified entityproblem, a revision module might require a knowl-edge source for the profiles of entities mentionedin a summary.
When an entity is introduced forthe first time in a summary, it should be associatedwith its description (such as a title and full namefor a person).Discourse information would be useful forsolving problems such as a bare anaphor or miss-ing subject.
In revising single-document summa-ries, [Mani et al 1999] employed rules such as thereferencing of pronouns with the most recentlymentioned noun phrase.
However, this might beinappropriate in MDS, where the use of multipledocuments increases the number of possible enti-ties with which an anaphor could be referenced.5.3 Temporal relationships in MDSAn important aspect of revision in MDS is the es-tablishment of the correct temporal relationshipsbetween the events described in a summary.
Wehave identified five types of problems that fall intothis category.Problem Number (%)1) Temporal ordering 31 (89%)2) Time of event 2    (6%)3) Event repetition 1   (2.5%)4) Synchrony 1   (2.5%)5) Anachronism 0Total 35Table 5: Temporal relationships revisionsThe most frequent revision in this category forour multi-document summaries was temporal or-dering.
This is an important consideration for thesummarization of news articles, which typicallydescribe several events or a series of events in agiven news story.A revision module might use metadata, includ-ing the time stamps of source documents, in addi-tion to surface properties of sentences inaddressing this problem.
Temporal relations weretypically established by adding a time expressionto one or more sentences in a summary.
Therefore,our module will require a dictionary of such ex-pressions as well as a set of rules for assigning anappropriate expression to a given sentence.
Forexample, if the time stamps of two source docu-ments from which two adjacent summary sen-tences come indicate that they were written oneday apart, an appropriate way to order them mightbe: add a time expression indicating the day to thefirst sentence, and a relative time expression suchas ?the following day?
to the second sentence.
Ourdictionary will require both relative and absolutetime expressions at different levels of granularity(hour, day, etc.
).Most of the temporal revisions in our corpuswere made at points where sentences from differ-ent sources followed one another or when sen-tences from the same source were far apart in theoriginal document.
By using such clues, it ishoped that temporal relations problems in summa-ries can be corrected without knowledge of thediscourse.5.4 Grammatical concerns in MDSThe majority of grammatical problems in our cor-pus resulted from previous revisions performed onthe text.
For example, the addition of informationto a sentence can result in it becoming too long.Such concerns can also occur because the grammarof one sentence, such as verb tense, does not matchthat of the next sentence.Problem Number  (%)1) Run-on sentence 7  (35%)2) Mismatched verb 3  (15%)3) Missing punctuation 3  (15%)4) Awkward syntax 3  (15%)5) Parenthetical 2  (10%)6) Subheading/titles 1  (  5%)7) Misused adverb 1  (  5%)Total 20Table 6: Grammatical revisionsA revision module should be able to correctthe above concerns using rules applied after otherrevisions are made and without any discourseknowledge.5.5 Location/setting concernsThe least frequent type of revision made in ourcorpus related to establishing the correct locationsof events in a summary.
Occasionally, a sentencein a summary retains the place/source stamp thatappears at the beginning of a news article.
Thisappears ungrammatical unless the sentence is thefirst in the summary.Problem Number (%)1) Place/source stamp 6  (60%)2) Place of event 4  (40%)3) Collocation 04) Change of location 0Total 10Table 7: Location/setting concernsIn addition, such stamps might be inappropriate fora summary, since not all the sentences may sharethe same location.
In order to promote cohesion inthe summary, our module could move the stampinformation into the body of the summary.Sentences could be missing location informa-tion altogether.
In such cases, the revision modulemight require information from the source docu-ments in order to repair this problem.
Overall, therevisions related to establishing the location ofevents should not require knowledge of discoursein the summary.
Adding location information canusually be performed with the addition of a prepo-sitional phrase, usually at the beginning of the sen-tence.6 Conclusions and future workThis paper represents preliminary work in our ef-forts to address problems of text cohesion and co-herence in multi-document summaries via revision.As a first step, we need to identify the specificproblems that occur in MDS and consider how wemight address such concerns.
To this end, we haveinvestigated the optimal revisions that were per-formed on a small set of summaries.
From thisanalysis, we have formulated a taxonomy of prag-matic concerns and their operators for repairingmulti-document summaries.Knowledge ofdiscourseand text generation<ADD>transitional phrase or sentence;<ADD>motivational phrase or sentence;Knowledgeof discourseKnowledge source(entitydescriptions)Meta dataand dictionaryof expressionsSentencesurfacestructure<DELETE>Redundant information<ADD/MODIFY>discourse markers<ADD/MODIFY>description of entitymentioned for first time<ADD/MODIFY>Time expressionGrammar corrections;<MODIFY/ADD/DELETE>definite or indefinite articles;<MODIFY/ADD/DELETE>location of eventOperationComplexityFigure 3: Continuum of revision operationsThere is a scale of revision operations that can beperformed (as shown in Figure 3), ranging fromconcrete repairs that require only knowledge of thesurface structures of sentences, to knowledge-intensive repairs that cannot be implemented with-out a discourse model.
In the future, we plan toformalize our framework so that we might be ableto implement such revision strategies automati-cally.
Of course, such an automatic process will bemuch more constrained in the revisions it can ap-ply, unlike the human reviser in our current study.For example, in automating the repair process wewill be restricted to using only material from thesource documents.
In addition, we may expandour taxonomy as necessary in exploring additionaldata.
We will need to relate revision in MDS toCST since revision required in a given summarydepends on the relationships between sentences.Finally, we would like use the corpus of data wehave collected to learn revision automatically.AcknowledgmentsThe authors would like to thank Naomi Daniel,Hong Qi, Adam Winkel, Zhu Zhang and threeanonymous reviewers for their helpful commentsand feedback on this paper.
This work was par-tially supported by the National Science Founda-tion?s Information Technology Research program(ITR) under grant IIS-0082884.The version of MEAD that we used was de-veloped at the Johns Hopkins summer workshop in2001 under the direction of Dragomir Radev.
Wewant to thank the following individuals for theirwork on MEAD: Sasha Blair-Goldensohn, JohnBlitzer, Arda Celebi, Elliott Drabek, Wai Lam,Danyu Liu, Hong Qi, Horacio Saggion and SimoneTeufel.References[Barzilay et al 2001]  Regina Barzilay, NoemieElhadad, and Kathleen R. McKeown.
Sentenceordering in multi-document summarization.
InProceedings of HLT, San Diego, CA, 2001.
[Filatova & Hovy, 2001]  Elena Filatova and Edu-ard Hovy.
Assigning time-stamps to event-clauses.In Proceedings, ACL Workshop on Temporal andSpatial Information Processing, Toulouse, France,July 2001.
[Goldstein et al 2000]  Jade Goldstein, Mark Kan-trowitz, Vibhu Mittal, and Jamie Carbonell.
Sum-marizing text documents: sentence selection andevaluation metrics.
In Proceedings of the 22ndACM SIGIR Conference on Research and Devel-opment in Information Retrieval, Berkeley, CA,1999.
[Halliday & Hasan, 1976]  M. Halliday and R.Hasan.
Cohesion in English.
London: Longman,1976.
[Harabagiu, 1999]  Sanda M. Harabagiu.
Fromlexical cohesion to textual coherence: a data drivenperspective.
Journal of Pattern Recognition andArtificial Intelligence, 13(2): 247-265, 1999.
[Hovy, 1993]  Eduard Hovy.
Automated discoursegeneration using discourse structure relations.
Ar-tificial Intelligence 63, Special Issue on NaturalLanguage Processing, 1993.
[Jing & McKeown, 2000]  Hongyan Jing andKathleen R. McKeown.
Cut and paste based textsummarization.
In Proceedings of the 1st Confer-ence of the North American Chapter of the Asso-ciation for Computational Linguistics(NAACL'00), Seattle, WA, May 2000.
[Mani et al 1999]  Inderjeet Mani, Barbara Gates,and Eric Bloedorn.
Improving summaries by re-vising them.
In Proceedings of the 37th AnnualMeeting of the ACL ?99, pages 558-565, Mary-land, 1999.
[Mann & Thompson, 1988]  William C. Mann andSandra A. Thompson.
Rhetorical structure theory:toward a functional theory of text organization.Text, 8(3), 1988.
[Marcu, 1997]  Daniel Marcu.
From discoursestructure to text summaries.
In Proceedings of theACL ?97 EACL ?97 Workshop on Intelligent Scal-able Text Summarization, pages 82-88, Madrid,Spain, July 1997.
[Marcu, 2000]  Daniel Marcu.
The theory andpractice of discourse parsing and summarization,The MIT Press, November 2000.
[Radev, 2000]  Dragomir Radev.
A common the-ory of information fusion from multiple textsources, step one: cross-document structure.
InProceedings, 1st ACL SIGDIAL Workshop onDiscourse and Dialogue, Hong Kong, October2000.
[Radev et al 2000]  Dragomir R. Radev, HongyanJing and Malgorzata Budzikowska.
Centroid-basedsummarization of multiple documents: sentence,extraction, utility-based evaluation, and user stud-ies.
In ANLP/NAACL Workshop on Summariza-tion, Seattle, WA, April 2000.
[Radev et al 2002]  Dragomir Radev, Simone Teu-fel, Horacio Saggion, Wai Lam, John Blitzer, ArdaCelebi, Hong Qi, Daniu Liu and Elliot Drabek.Evaluation challenges in large-scale multi-document summarization: the MEAD project.Submitted to SIGIR 2002, Tampere, Finland, Au-gust 2002.
[Zhang et al, 2002]  Zhu Zhang, Sasha Blair-Goldensohn, and Dragomir Radev.
Towards CST-enhanced summarization.
To appear in AAAI2002, August 2002.Appendix - Taxonomy of revisions in MDSDescription Operator(s) ExampleI.
Discourse1) Topic shift In moving from onesentence to another, thetopic shifts suddenlyADD transitional sen-tence or phraseIn a related story, the government ofHong Kong announced a proposal torequire all drug rehabilitation centers....2) Purpose Sentence lacks purposein the context of thesummaryADD a sentence orphrase that motivatesthe problematic seg-mentIn order to assist the ongoing investiga-tion as to the cause of the crash, theU.S.
team from the National Transporta-tion Safety Board will join experts?3) Contrast Information in a givensentence contrasts withthat in one or moreprevious sentencesADD a discoursemarker such as ?how-ever?
or ?to contrast?MODIFY existing dis-course markerHowever, according to reports on CNN,the control tower was concerned withthe velocity and altitude of the plane,and had discussed these concerns withthe pilot.4) Redundancy Sentence contains in-formation that was pre-viously reportedDELETE the redundantconstituent (non-headelement of NP, PP or anentire relative clause orphrase)The crash of flight 072 that killed 143people?The plane, which was carryingthe 143 victims, was headed to Bahrainfrom Egypt.5) Conditional Events in a given sen-tence are conditionedon events in anothersentenceMODIFY the two sen-tences: IF (sentenceone), (sentence two).Change verb tenses toconditional.If the proposed measure were imple-mented, it would ensure broadly thesame registration standard to be appliedto all drug treatment centers.II.
Entities1) UnderspecifiedentityA newly mentionedentity has no descrip-tion or title; acronym isused with no nameADD full name, de-scription or title for newentity; MODIFY acro-nym by expandingMrs.
Clarie Lo, the Commissioner ofNarcotics, said the proposal would beintroduced for non-medical drug treat-ment centers.2) OverspecifiedentityA noun phrase referringto an entity containsredundant information(full name and title,etc.
)DELETE the redundantnon-head elements ofthe NP; MODIFY aliasa nameScientists around the world have beenmonitoring Mount Pinatubo?DavidHarlow, a ?guerrilla seismologist,?
madeaccurate predictions of the eruptions ofthe volcano.3) Repeated entity A noun phrase describ-ing an entity occurs toooften in a given context.MODIFY replace NPwith a pronoun;MODIFY use acronymIn April 2000, Mrs.
Lo announced thatthe number of young people abusingdrugs fell in 1999.
She said, ?The num-ber of drug abusers aged below 21?
?4) Missing entity Sentence is missingsubject/agent (perhapsas result of previousrevision)ADD noun phrase orpronoun?the 28,000 Americans, who work atnearby naval bases.
They crowded intoSubic Bay Naval Base as a bizarre tropi-cal blizzard?5) Misused indefi-nite articleAn indefinite article isused with a previouslyintroduced entityMODIFY change in-definite article to defi-nite.The government of announced a pro-posal?One year later, it announced thatit intends to implement the proposedscheme.6) Misused definitearticleA definite article is usedwith a new entityMODIFY change defi-nite article to indefinitearticle if entity is new.On Thursday, a second eruption ap-peared to be smaller than anticipate.7) Missing article Entity is missing anarticleADD definite article ifentity has already beenmentioned; ADD in-definite article if entityis newThe newspapers of Bahrain include: Al-Ayam; Akhbar al-Khaleej (daily in Ara-bic); Bahrain Tribune?8) Bare anaphor An anaphor has no an-tecedentMODIFY change ana-phor to its referentialnoun phraseIf Pinatubo does have a massive erup-tion, its primary means of causingdeath?Description Operator(s) Example9) Misused quanti-fierQuantifier used with anentity is inappropriateMODIFY quantifier tomatch with its antece-dent; ?these?
and ?those?must have plural ante-cedent; ?such?
can havea singular antecedentMount Pinatubo erupted Satur-day?Such volcanoes arise where one ofthe earth?s crust plates is slowly divingbeneath another?III.
Temporal relations concerns1) Temporal order-ingEstablish correct tempo-ral relationships be-tween events (orrelative to a previousevent)ADD time expression;ADD ordinal number;DELETE inappropriatetime expression;MODIFY existing timeexpressionTwo days later, a second eruption ap-peared to be smaller than scientists hadanticipated.2) Absolute time ofan eventIndicate when an indi-vidual event occursADD time expression(time, day, date, month,year)Lt. Col. Ron Rand announced at 5 a.m.Monday that the base should be evacu-ated.3) Event repetition Indicate the repetitionof an eventADD an adverb such as?again?Mount Pinatubo is likely to explodeagain in the next few days or weeks.4) Synchrony Two (or more) eventsoccur at the same timeADD an adverb such as?meanwhile?
or ?as?
;MODIFY an existingadverb?all non-essential personnel shouldbegin evacuating the base.
Meanwhile,dawn skies over central Luzon werefilled with gray ash and steam?5) Anachronism Indicate that an eventhappened in the past(?flashback?
)ADD a time expression  Pinatubo?s last eruption, over six hun-dred years ago, yielded as much moltenrock as the eruption of Mt.
St. Helens...IV.
Grammar concerns1) Run-on sentence Sentence is too long MODIFY split longsentence into two sepa-rate sentences;DELETE conjunctionLt.
Col. Ron Rand announced at 5 a.m.Monday that all personnel should beginevacuating the base.
Meanwhile, dawnskies over central Luzon were filled?2) Mismatched verb Verb tenses in the sen-tences do not matchMODIFY change verbtense; ADD aux verbThe scheme would also impose uniformcontrol on drug treatment centers.3) Missing punctua-tionPunctuation is missing ADD appropriate punc-tuation markThe ?guerrilla seismologist?
from MenloPark, who helped save thousands oflives in the Philippines, is right where?4) Awkward syntax Sentence is unclear dueto its awkward syntaxMODIFY syntactictransformationSince 1999, the ruling Emir has beenSheikh Hamad Bin-Isa Al-Khalifah ,who was born on 28 January 1950.5) Parenthetical A parenthetical is inap-propriateDELETE entire paren-thetical; DELETE pa-rentheses[ ( ]Volcanoes such as Pinatubo arisewhere one of the earth?s crust plates isslowly diving beneath another.
[ ) ]6) Misused adverb An adverb is inappro-priateDELETE adverb The scheme will [also] impose uniformcontrol on drug treatment?7) Subhead-ings/subtitlesSubheadings or subtitlesappear in summary andare not sentencesDELETE subhead-ings/subtitles; MODIFYto be grammatical[Smaller than anticipated;] On Thurs-day a second eruption appeared to besmaller than anticipated by scientists?V.
Location/setting concerns1) Location of event Establish where anevent takes placeADD ?
prepositionalphrase indicating place(city, state, country)Three bodies were lain before the faith-ful in the Grand Mosque in Manama,Bahrain during a special prayer?2) Collocation Two (or more) eventsoccur in the same placeADD ?
prepositionalphrase or adverb thatindicates collocationMeanwhile, in the same area, searchteams sifted through the wreckage.3) Change of loca-tionSummary moves fromone event to another ina different locationADD ?
prepositionalphrase indicating placefor both eventsThree bodies were lain before the faith-ful in the Grand Mosque in Manama,Bahrain during a prayer?Meanwhile inCairo, relatives of passengers waited...4) Place/sourcestampPlace/source stampfrom original articleends up in summaryDELETE ?
stamp (butcache information forlater use)[Cairo, Egypt (AP)] The crash of a GulfAir flight that killed 143 people in Bah-rain is a disturbing d?j?
vu?
