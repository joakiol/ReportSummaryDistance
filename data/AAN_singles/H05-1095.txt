Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 755?762, Vancouver, October 2005. c?2005 Association for Computational LinguisticsTranslating with non-contiguous phrasesMichel Simard, Nicola Cancedda, Bruno Cavestro, Marc Dymetman,Eric Gaussier, Cyril Goutte, Kenji YamadaXerox Research Centre EuropeFirstName.FamilyName@xrce.xerox.comPhilippe LanglaisRALI/DIRO Universite?
de Montre?alfelipe@iro.umontreal.caArne MauserRWTH Aachen Universityarne.mauser@rwth-aachen.deAbstractThis paper presents a phrase-based statis-tical machine translation method, basedon non-contiguous phrases, i.e.
phraseswith gaps.
A method for producing suchphrases from a word-aligned corpora isproposed.
A statistical translation modelis also presented that deals such phrases,as well as a training method based on themaximization of translation accuracy, asmeasured with the NIST evaluation met-ric.
Translations are produced by means ofa beam-search decoder.
Experimental re-sults are presented, that demonstrate howthe proposed method allows to better gen-eralize from the training data.1 IntroductionPossibly the most remarkable evolution of recentyears in statistical machine translation is the stepfrom word-based models to phrase-based models(Och et al, 1999; Marcu and Wong, 2002; Yamadaand Knight, 2002; Tillmann and Xia, 2003).
Whilein traditional word-based statistical models (Brownet al, 1993) the atomic unit that translation operateson is the word, phrase-based methods acknowledgethe significant role played in language by multi-word expressions, thus incorporating in a statisticalframework the insight behind Example-Based Ma-chine Translation (Somers, 1999).However, Phrase-based models proposed so faronly deal with multi-word units that are sequencesof contiguous words on both the source and the tar-get side.
We propose here a model designed to dealwith multi-word expressions that need not be con-tiguous in either or both the source and the targetside.The rest of this paper is organised as follows.
Sec-tion 2 provides motivations, definition and extrac-tion procedure for non-contiguous phrases.
The log-linear conditional translation model we adopted isthe object of Section 3; the method used to trainits parameters is described in Section 4.
Section 5briefly describes the decoder.
The experiments weconducted to asses the effectiveness of using non-contiguous phrases are presented in Section 6.2 Non-contiguous phrasesWhy should it be a good thing to use phrasescomposed of possibly non-contiguous sequences ofwords?
In doing so we expect to improve trans-lation quality by better accounting for additionallinguistic phenomena as well as by extending theeffect of contextual semantic disambiguation andexample-based translation inherent in phrase-basedMT.
An example of a phenomenon best describedusing non-contiguous units is provided by Englishphrasal verbs.
Consider the sentence ?Mary switchesher table lamp off?.
Word-based statistical mod-els would be at odds when selecting the appropri-ate translation of the verb.
If French were the targetlanguage, for instance, corpus evidence would comefrom both examples in which ?switch?
is translatedas ?allumer?
(to switch on) and as ?e?teindre?
(toswitch off).
If many-to-one word alignments are notallowed from English to French, as it is usually the7552 31PierrePierrene mange pasdoes not eatFigure 1: An example of a complex alignment asso-ciated with different syntax for negation in Englishand French.case, then the best thing a word-based model coulddo in this case would be to align ?off?
to the emptyword and hope to select the correct translation from?switch?
only, basically a 50-50 bet.
While han-dling inseparable phrasal verbs such as ?to run out?correctly, previously proposed phrase-based modelswould be helpless in this case.
A comparable behav-ior is displayed by German separable verbs.
More-over, non-contiguous linguistic units are not limitedto verbs.
Negation is formed, in French, by insertingthe words ?ne?
and ?pas?
before and after a verb re-spectively.
So, the sentence ?Pierre ne mange pas?and its English translation display a complex word-level alignment (Figure 1) current models cannot ac-count for.Flexible idioms, allowing for the insertion of lin-guistic material, are other phenomena best modeledwith non-contiguous units.2.1 Definition and library constructionWe define a bi-phrase as a pair comprising a sourcephrase and a target phrase: b = ?s?, t??.
Each of thesource and target phrases is a sequence of words andgaps (indicated by the symbol ?
); each gap acts asa placeholder for exactly one unspecified word.
Forexample, w?
= w1w2?w3??
w4 is a phrase of length7, made up of two contiguous words w1 and w2, afirst gap, a third word w3, two consecutive gaps anda final word w4.
To avoid redundancy, phrases maynot begin or end with a gap.
If a phrase does notcontain any gaps, we say it is contiguous; otherwiseit is non-contiguous.
Likewise, a bi-phrase is said tobe contiguous if both its phrases are contiguous.The translation of a source sentence s is producedby combining together bi-phrases so as to cover thesource sentence, and produce a well-formed target-language sentence (i.e.
without gaps).
A completetranslation for s can be described as an ordered se-quence of bi-phrases b1...bK .
When piecing togetherthe final translation, the target-language portion t?1of the first bi-phrase b1 is first layed down, then eachsubsequent t?k is positioned on the first ?free?
posi-tion in the target language sentence, i.e.
either theleftmost gap, or the right end of the sequence.
Fig-ure 2 illustrates this process with an example.To produce translations, our approach thereforerelies on a collection of bi-phrases, what we call abi-phrase library.
Such a library is constructed froma corpus of existing translations, aligned at the wordlevel.Two strategies come to mind to produce non-contiguous bi-phrases for these libraries.
The first isto align the words using a ?standard?
word aligne-ment technique, such as the Refined Method de-scribed in (Och and Ney, 2003) (the intersection oftwo IBM Viterbi alignments, forward and reverse,enriched with alignments from the union) and thengenerate bi-phrases by combining together individ-ual alignments that co-occur in the same pair of sen-tences.
This is the strategy that is usually adopted inother phrase-based MT approaches (Zens and Ney,2003; Och and Ney, 2004).
Here, the difference isthat we are not restricted to combinations that pro-duce strictly contiguous bi-phrases.The second strategy is to rely on a word-alignment method that naturally produces many-to-many alignments between non-contiguous words,such as the method described in (Goutte et al,2004).
By means of a matrix factorization, thismethod produces a parallel partition of the two texts,seen as sets of word tokens.
Each token thereforebelongs to one, and only one, subset within this par-tition, and corresponding subsets in the source andtarget make up what are called cepts.
For example,in Figure 1, these cepts are represented by the circlesnumbered 1, 2 and 3; each cept thus connects wordtokens in the source and the target, regardless of po-sition or contiguity.
These cepts naturally constitutebi-phrases, and can be used directly to produce a bi-phrase library.Obviously, the two strategies can be combined,and it is always possible to produce increasinglylarge and complex bi-phrases by combining togetherco-occurring bi-phrases, contiguous or not.
Oneproblem with this approach, however, is that the re-sulting libraries can become very large.
With con-756danser le tangoto tangoI do not want to tango anymoreI do not want anymoredoI wantJe ne veux plus danser le tangoJeIne plusveuxwantdonot anymoreIsource =bi?phrase 1 =bi?phrase 2 =bi?phrase 3 =bi?phrase 4 =target =Figure 2: Combining bi-phrases to produce a translation.tiguous phrases, the number of bi-phrases that canbe extracted from a single pair of sentences typicallygrows quadratically with the size of the sentences;with non-contiguous phrases, however, this growthis exponential.
As it turns out, the number of avail-able bi-phrases for the translation of a sentence hasa direct impact on the time required to compute thetranslation; we will therefore typically rely on vari-ous filtering techniques, aimed at keeping only thosebi-phrases that are more likely to be useful.
For ex-ample, we may retain only the most frequently ob-served bi-phrases, or impose limits on the number ofcepts, the size of gaps, etc.3 The ModelIn statistical machine translation, we are given asource language input sJ1 = s1...sJ , and seek thetarget-language sentence tI1 = t1...tI that is its mostlikely translation:t?I1 = argmaxtI1Pr(tI1|sJ1 ) (1)Our approach is based on a direct approximationof the posterior probability Pr(tI1|sJ1 ), using a log-linear model:Pr(tI1|sJ1 ) =1ZsJ1exp(M?m=1?mhm(tI1, sJ1 ))In such a model, the contribution of each featurefunction hm is determined by the correspondingmodel parameter ?m; ZsJ1 denotes a normalizationconstant.
This type of model is now quite widelyused for machine translation (Tillmann and Xia,2003; Zens and Ney, 2003)1.Additional variables can be introduced in such amodel, so as to account for hidden characteristics,and the feature functions can be extended accord-ingly.
For example, our model must take into ac-count the actual set of bi-phrases that was used toproduce this translation:Pr(tI1, bK1 |sJ1 ) =1ZsJ1exp(M?m=1?mhm(tI1, sJ1 , bK1 ))Our model currently relies on seven feature func-tions, which we describe here.?
The bi-phrase feature function hbp: it rep-resents the probability of producing tI1 usingsome set of bi-phrases, under the assump-tion that each source phrase produces a targetphrase independently of the others:hbp(tI1, sJ1 , bK1 ) =K?k=1logPr(t?k|s?k) (2)Individual bi-phrase probabilities Pr(t?k|s?k)are estimated based on occurrence counts in theword-aligned training corpus.?
The compositional bi-phrase feature functionhcomp: this is introduced to compensate for1Recent work from Chiang (Chiang, 2005) addresses simi-lar concerns to those motivating our work by introducing a Syn-chronous CFG for bi-phrases.
If on one hand SCFGs allow tobetter control the order of the material inserted in the gaps, onthe other gap size does not seem to be taken into account, andphrase dovetailing such as the one involving ?do ?want?
and?not ???anymore?
in Fig.
2 is disallowed.757hbp?s strong tendency to overestimate the prob-ability of rare bi-phrases; it is computed as inequation (2), except that bi-phrase probabilitiesare computed based on individual word transla-tion probabilities, somewhat as in IBM model1 (Brown et al, 1993):Pr(t?|s?)
=1|s?||t?|?t?t??s?s?Pr(t|s)?
The target language feature function htl: thisis based on a N -gram language model of thetarget language.
As such, it ignores the sourcelanguage sentence and the decomposition ofthe target into bi-phrases, to focus on the actualsequence of target-language words producedby the combination of bi-phrases:htl(tI1, sJ1 , bK1 ) =I?i=1logPr(ti|ti?1i?N+1)?
The word-count and bi-phrase count featurefunctions hwc and hbc: these control the lengthof the translation and the number of bi-phrasesused to produce it:hwc(tI1, sJ1 , bK1 ) = I hbc(tI1, sJ1 , bK1 ) = K?
The reordering feature functionhreord(tI1, sJ1 , bK1 ): it measures the amount ofreordering between bi-phrases of the sourceand target sentences.?
the gap count feature function hgc: It takes asvalue the total number of gaps (source and tar-get) within the bi-phrases of bK1 , thus allowingthe model some control over the nature of thebi-phrases it uses, in terms of the discontigui-ties they contain.4 Parameter EstimationThe values of the ?
parameters of the log-linearmodel can be set so as to optimize a given crite-rion.
For instance, one can maximize the likely-hood of some set of training sentences.
Instead, andas suggested by Och (2003), we chose to maximizedirectly the quality of the translations produced bythe system, as measured with a machine translationevaluation metric.Say we have a set of source-language sentencesS.
For a given value of ?, we can compute the set ofcorresponding target-language translations T .
Givena set of reference (?gold-standard?)
translations Rfor S and a function E(T,R) which measures the?error?
in T relative to R, then we can formulate theparameter estimation problem as2:??
= argmin?E(T,R)As pointed out by Och, one notable difficulty withthis approach is that, because the computation of Tis based on an argmax operation (see eq.
1), it is notcontinuous with regard to ?, and standard gradient-descent methods cannot be used to solve the opti-mization.
Och proposes two workarounds to thisproblem: the first one relies on a direct optimiza-tion method derived from Powell?s algorithm; thesecond introduces a smoothed (continuous) versionof the error function E(T,R) and then relies on agradient-based optimization method.We have opted for this last approach.
Och showshow to implement it when the error function can becomputed as the sum of errors on individual sen-tences.
Unfortunately, this is not the case for suchwidely used MT evaluation metrics as BLEU (Pa-pineni et al, 2002) and NIST (Doddington, 2002).We show here how it can be done for NIST; a simi-lar derivation is possible for BLEU.The NIST evaluation metric computes a weightedn-gram precision between T and R, multiplied bya factor B(S, T,R) that penalizes short translations.It can be formulated as:B(S, T,R) ?N?n=1?s?S In(ts, rs)?s?S Cn(ts)(3)where N is the largest n-gram considered (usuallyN = 4), In(ts, rs) is a weighted count of commonn-grams between the target (ts) and reference (rs)translations of sentence s, and Cn(ts) is the totalnumber of n-grams in ts.To derive a version of this formula that is a con-tinuous function of ?, we will need multiple trans-lations ts,1, ..., ts,K for each source sentence s. Thegeneral idea is to weight each of these translations2For the sake of simplicity, we consider a single referencetranslation per source sentence, but the argument can easily beextended to multiple references.758by a factor w(?, s, k), proportional to the scorem?
(ts,k|s) that ts,k is assigned by the log-linearmodel for a given ?
:w(?, s, k) =[m?(ts,k|s)?k?
m?(ts,k?
|s)]?where ?
is the smoothing factor.
Thus, inthe smoothed version of the NIST function, theterm In(ts, rs) in equation (3) is replaced by?k w(?, s, k)In(ts,k, rs), and the term Cn(ts) isreplaced by?k w(?, s, k)Cn(ts,k).
As for thebrevity penalty factor B(S, T,R), it depends onthe total length of translation T , i.e.
?s |ts|.
Inthe smoothed version, this term is replaced by?s?k w(?, s, k)|ts,k|.
Note that, when ?
?
?,then w(?, s, k) ?
0 for all translations of s, exceptthe one for which the model gives the highest score,and so the smooth and normal NIST functions pro-duce the same value.
In practice, we determine some?good?
value for ?
by trial and error (5 works fine).We thus obtain a scoring function for which wecan compute a derivative relative to ?, and which canbe optimized using gradient-based methods.
In prac-tice, we use the OPT++ implementation of a quasi-Newton optimization (Meza, 1994).
As observed byOch, the smoothed error function is not convex, andtherefore this sort of minimum-error rate training isquite sensitive to the initialization values for the ?parameters.
Our approach is to use a random set ofinitializations for the parameters, perform the opti-mization for each initialization, and select the modelwhich gives the overall best performance.Globally, parameter estimation proceeds alongthese steps:1.
Initialize the training set: using random pa-rameter values ?0, for each source sentence ofsome given set of sentences S, we computemultiple translations.
(In practice, we use theM -best translations produced by our decoder;see Section 5).2.
Optimize the parameters: using the method de-scribed above, we find ?
that produces the bestsmoothed NIST score on the training set.3.
Iterate: we then re-translate the sentences of Swith this new ?, combine the resulting multipletranslations with those already in the trainingset, and go back to step 2.Steps 2 and 3 can be repeated until the smooothedNIST score does not increase anymore3.5 DecoderWe implemented a version of the beam-search stackdecoder described in (Koehn, 2003), extended tocope with non-contiguous phrases.
Each transla-tion is the result of a sequence of decisions, each ofwhich involves the selection of a bi-phrase and of atarget position.
The final result is obtained by com-bining decisions, as in Figure 2.
Hypotheses, cor-responding to partial translations, are organised in asequence of priority stacks, one for each number ofsource words covered.
Hypotheses are extended byfilling the first available uncovered position in thetarget sentence; each extended hypotheses is theninserted in the stack corresponding to the updatednumber of covered source words.
Each hypothesis isassigned a score which is obtained as a combinationof the actual feature function values and of admissi-ble heuristics, adapted to deal with gaps in phrases,estimating the future cost for completing a transla-tion.
Each stack undergoes both threshold and his-togram pruning.
Whenever two hypotheses are in-distinguishable as far as the potential for further ex-tension is concerned, they are merged and only thehighest-scoring is further extended.
Complete trans-lations are eventually recovered in the ?last?
prioritystack, i.e.
the one corresponding to the total num-ber of source words: the best translation is the onewith the highest score, and that does not have anyremaining gaps in the target.6 EvaluationWe have conducted a number of experiments to eval-uate the potential of our approach.
We were par-ticularly interested in assessing the impact of non-contiguous bi-phrases on translation quality, as wellas comparing the different bi-phrase library contruc-tion strategies evoked in Section 2.1.3It can be seen that, as the set of possible translations forS stabilizes, we eventually reach a point where the procedureconverges to a maximum.
In practice, however, we can usuallystop much earlier.7596.1 Experimental SettingAll our experiments focused exclusively on Frenchto English translation, and were conducted using theAligned Hansards of the 36th Parliament of Canada,provided by the Natural Language Group of the USCInformation Sciences Institute, and edited by UlrichGermann.
From this data, we extracted three dis-tinct subcorpora, which we refer to as the bi-phrase-building set, the training set and the test set.
Thesewere extracted from the so-called training, test-1and test-2 portions of the Aligned Hansard, respec-tively.
Because of efficiency issues, we limited our-selves to source-language sentences of 30 words orless.
More details on the evaluation data is presentedin Table 14.6.2 Bi-phrase LibrariesFrom the bi-phrase-building set, we built a numberof libraries.
A first family of libraries was based ona word alignment ?A?, produced using the Refinedmethod described in (Och and Ney, 2003) (com-bination of two IBM-Viterbi alignments): we callthese the A libraries.
A second family of librarieswas built using alignments ?B?
produced with themethod in (Goutte et al, 2004): these are the B li-braries.
The most notable difference between thesetwo alignments is that B contains ?native?
non-contiguous bi-phrases, while A doesn?t.Some libraries were built by simply extracting thecepts from the alignments of the bi-phrase-buildingcorpus: these are the A1 and B1 libraries, and vari-ants.
Other libraries were obtained by combiningcepts that co-occur within the same pair of sen-tences, to produce ?composite?
bi-phrases.
For in-stance, the A2 libraries contain combinations of 1or 2 cepts from alignment A; B3 contains combina-tions of 1, 2 or 3 cepts, etc.Some libraries were built using a ?gap-size?
filter.For instance library A2-g3 contains those bi-phrasesobtained by combining 1 or 2 cepts from alignmentA, and in which neither the source nor the targetphrase contains more than 3 gaps.
In particular, li-brary B1-g0 does not contain any non-contiguousbi-phrases.4Preliminary experiments on different data sets allowed usto establish that 800 sentences constituted an acceptable sizefor estimating model parameters.
With such a corpus, the esti-mation procedure converges after just 2 or 3 iterations.Finally, all libraries were subjected to the sametwo filtering procedures: the first excludes all bi-phrases that occur only once in the training corpus;the second, for any given source-language phrase,retains only the 20 most frequent target-languageequivalents.
While the first of these filters typicallyeliminates a large number of entries, the second onlyaffects the most frequent source phrases, as mostphrases have less than 20 translations.6.3 ExperimentsThe parameters of the model were optimized inde-pendantly for each bi-phrase library.
In all cases,we performed only 2 iterations of the training proce-dure, then measured the performance of the systemon the test set in terms of the NIST and BLEU scoresagainst one reference translation.
As a point of com-parison, we also trained an IBM-4 translation modelwith the GIZA++ toolkit (Och and Ney, 2000), usingthe combined bi-phrase building and training sets,and translated the test set using the ReWrite decoder(Germann et al, 2001)5.Table 2 describes the various libraries that wereused for our experiments, and the results obtainedfor each.System/library bi-phrases NIST BLEUReWrite 6.6838 0.3324A1 238 K 6.6695 0.3310A2-g0 642 K 6.7675 0.3363A2-g3 4.1 M 6.7068 0.3283B1-g0 193 K 6.7898 0.3369B1 267 K 6.9172 0.3407B2-g0 499 K 6.7290 0.3391B2-g3 3.3 M 6.9707 0.3552B1-g1 206 K 6.8979 0.3441B1-g2 213 K 6.9406 0.3454B1-g3 218 K 6.9546 0.3518B1-g4 222 K 6.9527 0.3423Table 2: Bi-phrase libraries and resultsThe top part of the table presents the results forthe A libraries.
As can be seen, library A1 achievesapproximately the same score as the baseline sys-tem; this is expected, since this library is essentially5Both the ReWrite and our own system relied on a trigramlanguage model trained on the English half of the bi-phrasebuilding set.760Subset sentences source words target wordsbi-phrase-building set 931,000 17.2M 15.2Mtraining set 800 11,667 10,601test set 500 6726 6041Table 1: Data sets.made up of one-to-one alignments computed usingIBM-4 translation models.
Adding contiguous bi-phrases obtained by combining pairs of alignmentsdoes gain us some mileage (+0.1 NIST)6.
Again, thisis consistent with results observed with other sys-tems (Tillmann and Xia, 2003).
However, the addi-tion of non-contiguous bi-phrases (A2-g3) does notseem to help.The middle part of Table 2 presents analogous re-sults for the corresponding B libraries, plus the B1-g0 library, which contains only those cepts from theB alignment that are contiguous.
Interestingly, inthe experiments reported in (Goutte et al, 2004),alignment method B did not compare favorably to Aunder the widely used Alignment Error Rate (AER)metric.
Yet, the B1-g0 library performs better thanthe analogous A1 library on the translation task.This suggests that AER may not be an appropriatemetric to measure the potential of an alignment forphrase-based translation.Adding non-contiguous bi-phrases allows anothersmall gain.
Again, this is interesting, as it sug-gests that ?native?
non-contiguous bi-phrases are in-deed useful for the translation task, i.e.
those non-contiguous bi-phrases obtained directly as cepts inthe B alignment.Surprisingly, however, combining cepts from theB alignment to produce contiguous bi-phrases (B2-G0) does not turn out to be fruitful.
Why thisis so is not obvious and, certainly, more experi-ments would be required to establish whether thistendency continues with larger combinations (B3-g0, B4-g0...).
Composite non-contiguous bi-phrasesproduced with the B alignments (B2-g3) seemto bring improvements with regard to ?basic?
bi-phrases (B1), but it is not clear whether these aresignificant.6While the differences in scores in these and other experi-ments are relatively small, we believe them to be significant, asthey have been confirmed systematically in other experimentsand, in our experience, by visual inspection of the translations.Visual examination of the B1 library revealsthat many non-contiguous bi-phrases contain long-spanning phrases (i.e.
phrases containing long se-quences of gaps).
To verify whether or not thesewere really useful, we tested a series of B1 librarieswith different gap-size filters.
It must be noted that,because of the final histogram filtering we apply onlibraries (retain only the 20 most frequent transla-tions of any source phrase), library B1-g1 is nota strict subset of B1-g2.
Therefore, filtering ongap-size usually represents a tradeoff between morefrequent long-spanning bi-phrases and less frequentshort-spanning ones.The results of these experiments appear in thelower part of Table 2.
While the differences in scoreare small, it seems that concentrating on bi-phraseswith 3 gaps or less affords the best compromise.For small libraries such as those under considerationhere, this sort of filtering may not be very important.However, for higher-order libraries (B2, B3, etc.)
itbecomes crucial, because it allows to control the ex-ponential growth of the libraries.7 ConclusionsIn this paper, we have proposed a phrase-based sta-tistical machine translation method based on non-contiguous phrases.
We have also presented a esti-mation procedure for the parameters of a log-lineartranslation model, that maximizes a smooth versionof the NIST scoring function, and therefore lendsitself to standard gradient-based optimization tech-niques.From our experiments with these new methods,we essentially draw two conclusions.
The first andmost obvious is that non-contiguous bi-phrases canindeed be fruitful in phrase-based statistical machinetranslation.
While we are not yet able to character-ize which bi-phrases are most helpful, some of thosethat we are currently capable of extracting are wellsuited to cover some short-distance phenomena.761The second conclusion is that alignment quality iscrucial in producing good translations with phrase-based methods.
While this may sound obvious, ourexperiments shed some light on two specific aspectsof this question.
The first is that the alignmentmethod that produces the most useful bi-phrasesneed not be the one with the best alignment errorrate (AER).
The second is that, depending on thealignments one starts with, constructing increasinglylarge bi-phrases does not necessarily lead to bettertranslations.
Some of our best results were obtainedwith relatively small libraries (just over 200,000 en-tries) of short bi-phrases.
In other words, it?s nothow many bi-phrases you have, it?s how good theyare.
This is the line of research that we intend topursue in the near future.AcknowledgmentsThe authors are grateful to the anonymous reviewersfor their useful suggestions.
7ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameter es-timation.
Computational Linguistics, 19(2):263?311.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the ACL, pages 263?270,Ann Arbor, Michigan.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proc.
ARPA Workshop on Human Lan-guage Technology.U.
Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-mada.
2001.
Fast Decoding and Optimal Decodingfor Machine Translation.
In Proceedings of ACL 2001,Toulouse, France.Cyril Goutte, Kenji Yamada, and Eric Gaussier.
2004.Aligning words using matrix factorisation.
In Proc.ACL?04, pages 503?510.Philipp Koehn.
2003.
Noun Phrase Translation.
Ph.D.thesis, University of Southern California.7This work was supported in part by the IST Programmeof the European Community, under the PASCAL Network ofExcellence, IST-2002-506778.
This publication only reflectsthe authors?
views.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proc.
of the Conf.
on Empirical Methods inNatural Language Processing (EMNLP 02), Philadel-phia, PA.J.
C. Meza.
1994.
OPT++: An Object-Oriented ClassLibrary for Nonlinear Optimization.
Technical ReportSAND94-8225, Sandia National Laboratories, Albu-querque, USA, March.F.
J. Och and H. Ney.
2000.
Improved Statistical Align-ment Models.
In Proceedings of ACL 2000, pages440?447, Hongkong, China, October.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51, March.Franz Josef Och and Hermann Ney.
2004.
The Align-ment Template Approach to Statistical Machine Trans-lation.
Computational Linguistics, 30(4):417?449.Franz Josef Och, Christoph Tillmann, and Hermann Ney.1999.
Improved alignment models for statistical ma-chine translation.
In Proc.
of the Joint Conf.
on Em-pirical Methods in Natural Language Processing andVery Large Corpora (EMNLP/VCL 99), College Park,MD.Franz Och.
2003.
Minimum error rate training in statis-tical machine translation.
In ACL?03: 41st Ann.
Meet.of the Assoc.
for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalu-tion of machine translation.
In Proceedings of the 40thAnnual Meeting of the ACL, pages 311?318, Philadel-phia, USA.Harold Somers.
1999. Review Article: Example-basedMachine Translation.
Machine Translation, 14:113?157.Christoph Tillmann and Fei Xia.
2003.
A phrase-basedunigram model for statistical machine translation.
InProc.
of the HLT-NAACL 2003 Conference, Edmonton,Canada.Kenji Yamada and Kevin Knight.
2002.
A decoder forsyntax-based statistical MT.
In Proc.
of the 40th An-nual Conf.
of the Association for Computational Lin-guistics (ACL 02), Philadelphia, PA.Richard Zens and Hermann Ney.
2003.
Improvementsin Phrase-Based Statistical Machine Translation.
InProc.
of the HLT-NAACL 2003 Conference, Edmonton,Canada.762
