BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 104?105,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAn Approach to Reducing Annotation Costs for BioNLPMichael BloodgoodComputer and Information SciencesUniversity of DelawareNewark, DE 19716bloodgoo@cis.udel.eduK.
Vijay-ShankerComputer and Information SciencesUniversity of DelawareNewark, DE 19716vijay@cis.udel.edu1 IntroductionThere is a broad range of BioNLP tasks for whichactive learning (AL) can significantly reduce anno-tation costs and a specific AL algorithm we havedeveloped is particularly effective in reducing an-notation costs for these tasks.
We have previouslydeveloped an AL algorithm called ClosestInitPAthat works best with tasks that have the followingcharacteristics: redundancy in training material,burdensome annotation costs, Support Vector Ma-chines (SVMs) work well for the task, and imbal-anced datasets (i.e.
when set up as a binaryclassification problem, one class is substantiallyrarer than the other).
Many BioNLP tasks havethese characteristics and thus our AL algorithm is anatural approach to apply to BioNLP tasks.2 Active Learning AlgorithmClosestInitPA uses SVMs as its base learner.
Thisfits well with many BioNLP tasks where SVMsdeliver high performance (Giuliano et al, 2006;Lee et al, 2004).
ClosestInitPA is based on thestrategy of selecting the points which are closest tothe current model?s hyperplane (Tong and Koller,2002) for human annotation.
ClosestInitPA worksbest in situations with imbalanced data, which isoften the case for BioNLP tasks.
For example, inthe AIMed dataset annotated with protein-proteininteractions, the percentage of pairs of proteins inthe same sentence that are annotated as interactingis only 17.6%.SVMs (Vapnik, 1998) are learning systems thatlearn linear functions for classification.
A state-ment of the optimization problem solved by soft-margin SVMs that enables the use of asymmetriccost factors is the following:Minimize: ??
?=?+=+++1:1:2||||21yywjijjii CC ?
?r  (1)Subject to: kkk bxwyk ???+??
1][: rr          (2)where ),( bwr  represents the hyperplane that islearned, kxris the feature vector for example k, ykin {+1,-1} is the label for example k,])[1,0max( bxwy kkk +?
?=rr?is the slack vari-able for example k, and C+ and C- are user-definedcost factors that trade off separating the data with alarge margin and misclassifying training examples.Let PA=C+/C-.
PA stands for ?positive amplifi-cation.?
We use this term because as the PA is in-creased, the importance of positive examples isamplified.
ClosestInitPA is described in Figure 3.We have previously shown that setting PA basedon a small initial set of data outperforms the moreobvious approach of using the current labeled datato estimate PA.Figure 3.
ClosestInitPA algorithm.We have previously developed a stopping crite-rion called staticPredictions that is based on stop-ping when we detect that the predictions of ourmodels on some unlabeled data have stabilized.
Allof the automatic stopping points in our results aredetermined using staticPredictions.Initialization:?
L = small initial set of labeled data?
U = large pool of unlabeled dataLLPAin  examples pos #in  examples neg #=Loop until stopping criterion is met:1.
Train an SVM with parameters C+and C-set such that C+/C- = PA.2.
batch = select k points from U thatare closest to the hyperplane learnedin step 1.U = U ?
batchL = L U batch1043 ExperimentsProtein-Protein Interaction Extraction: We usedthe AImed corpus, which was previously used fortraining protein interaction extraction systems in(Giuliano et al, 2006).
We cast RE as a binaryclassification task as in (Giuliano et al, 2006).We do 10-fold cross validation and use what isreferred to in (Giuliano et al, 2006) as the KGCkernel with SVMlight (Joachims, 1999) in our ex-periments.
Table 1 reports the results.F Measure StoppingPoint Average # LabelsRandom AL20% 1012 48.33 54.3430% 1516 49.76 54.5240% 2022 53.11 56.39100% 5060 57.54 57.54AutoStopPoint 1562 51.25 55.34Table 1.
AImed Stopping Point Performance.
?AutoS-topPoint?
is when the stopping criterion says to stop.Medline Text Classification: We use the Oh-sumed corpus (Hersh, 1994) and a linear kernelwith SVMlight with binary features for each wordthat occurs in the training data at least three times.Results for the five largest categories for one ver-sus the rest classification are in Table 2.F Measure StoppingPoint Average # LabelsRandom AL20% 1260 49.99 61.4930% 1880 54.18 62.7240% 2500 57.46 63.75100% 6260 65.75 65.75AutoStopPoint 1204 47.06 60.73Table 2.
Ohsumed stopping point performance.
?AutoS-topPoint?
is when the stopping criterion says to stop.GENIA NER: We assume a two-phase model(Lee et al, 2004) where boundary identification ofnamed entities is performed in the first phase andthe entities are classified in the second phase.
As inthe semantic classification evaluation of (Lee et al,2004), we assume that boundary identification hasbeen performed.
We use features based on thosefrom (Lee et al, 2004), a one versus the rest setupand 10-fold cross validation.
Tables 3-5 show theresults for the three most common types inGENIA.F Measure StoppingPoint Average # LabelsRandom AL20% 13440 86.78 90.1630% 20120 87.81 90.2740% 26900 88.55 90.32100% 67220 90.28 90.28AutoStopPoint 8720 85.41 89.24Table 3.
Protein stopping points performance.
?AutoS-topPoint?
is when the stopping criterion says to stop.F Measure StoppingPoint Average # LabelsRandom AL20% 13440 79.85 82.0630% 20120 80.40 81.9840% 26900 80.85 81.84100% 67220 81.68 81.68AutoStopPoint 7060 78.35 82.29Table 4.
DNA stopping points performance.
?AutoS-topPoint?
is when the stopping criterion says to stop.F Measure StoppingPoint Average # LabelsRandom AL20% 13440 84.01 86.7630% 20120 84.62 86.6340% 26900 85.25 86.45100% 67220 86.08 86.08AutoStopPoint 4200 81.32 86.31Table 5.
Cell Type stopping points performance.
?Au-toStopPoint?
is when the stopping criterion says to stop.4 ConclusionsClosestInitPA is well suited to many BioNLPtasks.
In experiments, the annotation savings arepractically significant for extracting protein-proteininteractions, classifying Medline text, and perform-ing biomedical named entity recognition.ReferencesClaudio Giuliano, Alberto Lavelli, and Lorenza Roma-no.
2006.
Exploiting Shallow Linguistic Informationfor Relation Extraction from Biomedical Literature.In Proceedings of the EACL, 401-408.William Hersh, Buckley, C., Leone, T.J., and Hickman,D.
(1994).
Ohsumed: an interactive retrieval evalua-tion and new large text collection for research.
ACMSIGIR.Thorsten Joachims.
1999.
Making large-Scale SVMLearning Practical.
Advances in Kernel Methods -Support Vector Learning, MIT-Press, 169-184.Ki-Joong Lee, Young-Sook Hwang, Seonho Kim, andHae-Chang Rim.
2004.
Biomedical named entityrecognition using two-phase model based on SVMs.Journal of Biomedical Informatics, Vol 37, 436?447.Simon Tong and Daphne Koller.
2002.
Support vectormachine active learning with applications to textclassification.
JMLR 2: 45-66.Vladimir Vapnik.
1998.
Statistical Learning Theory.John Wiley & Sons, New York, NY, USA.105
