Meaning Representation and Text Planning.Christine DEFRISE Sergei NIRENBURGIR ID IA  Center  for  Mach ine  Translat ionUnivers i t6 L ibre  de Bruxel les  Carnegie  Mel lon  Univers i tyThe data flow in natural anguage generation (NLG)starts with a 'world' state, represented by structures ofan application program (e.g., an expert system) that hastext generation eeds and an impetus to produce a natu-ral language text.
The output of generation is a naturallanguage text.
The generation process involves the tasksof a) delimiting the content of the eventual text, b) planoning its structure, c) selecting lexieal, syntactic and wordorder me,'ms of realizing this structure and d) actually re-alizing the textusing the latter.
In advanced generationsystems these processes are treated not in a monolithicway, but rather as components of a large, modular gener-ator.
NLG researchers experiment with various ways ofdelimiting the modules of the generation process and con-trol architectures to drive these modules (see, for instance,McKeown, 1985, Hovy, 1987 or Meteer, 1989).
But re-gardless of the decisions about general (intermodular) orlocal (intramodular) control flow, knowledge structureshave to be defined to support processing and facilitatecommunication among the modules.The natural language generator DIOGENES(e.g., Niren-burg et al, 1989) has been originally designed for usein machine translation.
This means that the content de-limitation stage is unnecessary, as the set of meaningsto be realized by the generator is obtained in machinetranslationas  result of source text analysis.
The firstprocessing component in DIOGENES is, therefore, its textplanner which, takes as input a text meaning representa-tion (TMR) and a set of static pragmatic factors (similar toHovy's (1987) rhetorical goals) and produces a text plan(TP), a structure containing information about the orderand boundaries of target language sentences; the deci-sions about reference realization and lexical selection, tAt the next stage, a set of semantics-to-syntax mappingrules are used to produce a set of target-language syn-tactic structures (we are using the f-structures of LFG - -see, e.g., Nirenburg and Levin, 1989).
Finally, a syntacticrealizer produces a target language text from the set off-structures.To produce texts of adequate quality, natural languagegeneration needs a sufficiently expressive input language.In this paper we discuss several important aspects of theknowledge and the processing atthe text planning stage ofa generation system.
First, we describe acomprehensivelanguage processing paradigm which underlies work onboth generation and analysis of natural language in our en-vironment.
Next, we illustrate the features of our meaningrepresentation la guages, the text meaning representationlanguage TAMERLAN and the text plan representation la -guage TPL.
Finally, we describe the mechanism of textplanning in DIOGENES and illustrate the formalism and thestrategy for acquiring text planning rules.~Text phmning in DIOGF3qES is described in detail Defriseand Nirenburg, 1989; the language for writing planning rules, inNirenburg et al, in preparation; the lexical selection i DIOGF.NF'^ Sis described, e.g., in Nirenburg and Nirenburg, 1988.1 Text Meaning in Analysis & Generation.Understanding a text involves determining its proposi-tional content as well as detecting pragmatic meaning el-ements, those pertaining to the speech situation and to thenature of the discourse producer's goals.
~ A high-qualitynatural language generator must be capable of expressingthe entire set of meaning types in a natural language.
Theinput to a generator must, therelore, express all of thekinds of meanings to be rendered in the output ext.
Thisrequires a convenient knowledge representation schemefor the input, as well as actual knowledge (in the form ofheuristic selection rules) about interactions between theinput and elements of the output.The acquisition of this knowledge must be informedby systematic field work on such linguistic phenomena aslocus, topic and comment, speech acts, speaker attitudes,or reference-related phenomena ( naphora, deixis, ellip-sis, definite description, etc.).
When these phenomenaare studied in computational linguistics, they are usuallyapproached from the standpoint of language understand-ing.
The types of activities in generation often differ fromthose in analysis (see, e.g., Nirenburg and Raskin, 1987).~Ib highlight he requirements of natural anguage gener-ation, we will describe ageneration-oriented approach tospeech acts and attitudinal knowledge.1.1 Speech Acts.We model a text producer/consumer as having, in additionto knowledge about the grammar and lexis of a naturallanguage and knowledge about he world, an inventory ofworld-related and communication-related goals and plansthat are known to lead to the achievement of these goals.Producing text, then, is understood as a planning processto achieve a goal.
Being able to achieve goals throughcommunication can be seen as broadening the range oftools an agent has for attaining its goals.
It is important todistinguish clearly between the producer's goal and planinventory and a set of active goal and plan ins tances whichconstitute the text producer's agenda.
When the decisionis made to try to achieve a go~d by rhetorical means, oneof the available rhetorical plans corresponding to this goalis selected for processing.
Based on knowledge recordedin this plan structure, the agent produces a new, language-dependent, structure, a text plan, as an intermediate stepin producing a text.The process of natural anguage understanding in ourapproach will be modeled as a plan recognition process ina similar conceptual rchitecture.
In understanding, it isnecessary to determine not only the propositional contentbut also the illocutionary force of each utterance.
Thisinvolves reconstructing (on the basis of the propositionalcontent and the knowledge of the speech situation) the text2We decided to use the term 'producer' for the author of atext and the speaker in a dialog; and the term 'consumer' toindicate the reader of a text or a hearer in a dialog.1 219producer's goals, in order to decide if an utterance con-stitutes a direct or an indirect speech act, calculating anyadded non-propositional, implicit elements of meaning.In generation, aswe have seen, the task is to decide, onthe basis of the communication situation and knowledgeof the producer's intentions, whether to achieve a givengoal via rhetorical means.
If so, it is also necessary todecide whether itwill be realized in the target text directlyor indirectly.
If a direct speech act is chosen, one mustfurther determine whether to lexicalize the speech actthrough the use of a performative verb (e.g., promise,order, etc.
).The treatment of speech acts is thus always accom-plished using links between the set of producer's currentgoals and the propositional content.
The order of oper-ations, however, differs depending on whether this treat-ment is a part of analysis or generation.
To illustrate,suppose an agent is cold and has a goal to be warm.
Toachieve this goal, we construct a plan tree, with the goalbe-warm as the root, and various plans of action as sub-trees (see Figure 1).BE WARMself-action delegationgo close put on ... physical actionwindow swctae~ ~ \[reqt~mt .~tion\]J I \point ~ art point o a direct indirectopen window sweater speech act Inymch actlexieal synUtctie(perforrnati~x) (mood, tense, ...)Figure 1: The Plan Tree for the Goal 'Be-Warm.
'In analysis, the tree is traversed bottom-up: the con-sumer has direct access to the input utterance and uses itas a clue to reconstruct the producer's goals.
If completeextraction of meaning is to be achieved, it is crucial toknow whether an utterance realizes a direct or an indi-rect speech act, In generation, the tree will be traversedtop-down.
If the producer's choice is to achieve his goalthrough rhetorical means, he can generate any of the fol-lowing three utterances:(1) I order you to close the window.
(2) Close the window, please.
(3) It's cold in here.Now, even though (3) differs in propositional meaningfrom (1) and (2), the distinction between direct speech act(1) or (2) and indirect speech act (3) does not matter fromthe point of view of the producer's goals.
In TAMERLANthe representation f both Close the window and It is coldin here will contain a pointer (in TAMERLAN the filler oftheproducer-intention slot of every clause frame) to thesame plan node, in this case, 'request-action.'
The factthat in the above utterances, the request-action is realizedas a command or a statement pertains to text, not domainplanning.1.2 Producer Attitude.Our reasons for introducing attitudes as an explicit part ofthe representation f the meaning of a natural anguageclause are manifold.
In what follows we will reviewthree (partially interconnected) reasons.
Representing at-titudes helps to a) support reasoning about producer goals;b) highlight he argumentative structure of a discourse;c) provide a convenient vehicle for representing m(xlalmeanings.Almost all spoken and written discourse involves theparticipants' opinions, so much so that producing a per-fectly objective text is an almost impossible task.
Withinthe set of possible goals relating to generating text, the in-troduction (explicit or implicit, lexicalized or not) of theproducer's opinions and points of view serves two goals:?
modifying the consumer's model of the producerby stating facts (including opinions) about he latterwhich are not in principle observable by the con-sumer?
modifying the consumer's opinions by stating pro-ducer's opinions about facts of the world (the lattercan in principle be observed by the consumer)The above distinctions only become visible if one de-cides to represent attitudes overtly.
Once this decisionis made, it becomes clear that it brings about better de-scription possibilities for additional linguistic phenom-ena, such as the argumentative structure of discourse.
Ithas been observed (e.g., Anscombre and Ducrot, 1983)that texts have a well-defined argumentative structurewhich a) reflects the producer's current goals and b) influ-ences such processes as the ordering of text componentsand lexical selection in generation.
The argumentativestructure of a text is realized (or, in text understand-ing, detected) through linguistic means such as the useof scalar adverbs ('only', 'even', 'almost', etc.
), connec-tives ('but', 'since'), adjectives ('unbearable', 'fascinat-ing', etc.).
Sets of such lexical items may have to be con-sidered equivalent from a purely semantic point of view,but different in a facet of their pragmatic effect knownas argumentative orientation.
For example, to illustratethe interplay between semantic ontent and argumenta-tive orientation (i.e.
the producer's attitude towards anevent), compare (4) and (5), which have opposite truthconditions, but the same pragmatic value - -  from both(4) and (5) the consumer will infer that the producer re-gards Burma as an inefficient sleuth.
In this example it issufficient to retain pragmatic information concerning theproducer's judgment of Burma while the semantic differ-ences (induced by the use of 'few' versus 'none at all')can be disregarded.
However, in other contexts the se-mantics will matter much more - -  consider, for instance,(6) for which there can be no paraphrase With 'no clues atall.
'(4) Nestor Burma found few clues.
Nobodywas surprised.
(5) Nestor Burma found no clues at all.
Nobodywas surprised.
(6) Nestor Burma found few clues.
But it wasstill better than having none at all.The difference between (7) and (8), whose truth condi-tions are similar, is purely argumentative (or attitudinal)- -  (7) expresses a positive (optimistic!)
attitude, (8) theopposite point of view.
This example shows how crucialthe extraction of the argumentative structure is, since it isthe only clue for the (in)acceptability of (9).
(7) Nestor has a little money.
(8) Nestor has little money.
(9) *Nestor has little money.
He wouldn't mindspending some on chocolate.220  21Tinnily, we use the attitude markers as a means ofexgressing modality.
Traditionally, formal semanticistshave extended first order logic to modal ogic in order toaccount lbr mc~lals.
This places the modals at a purelysemantic level, as a result of which the it is difficult todistinguish between what is observable for both producerand consumer and what is not (opinions, beliefs, etc.).
Weconsider that expressions like 'perhaps,' 'possibly,' 'it isalmost certain that' are clues as to what the producer's be-lieg\]s and attitudes are towards facts of the world and helpthe consumer modify or update his model of the producer.It is for the above reasons that we decided to include ade, ailed specification of producer attitudes into the inputsp(>cilication for generation.2 The  Structure of TAMERLAN,TAMERLAN is a frame-based representation language thathat; the following basic entity types: text, clause, relation,proposition, attitude and pointer (to the producer's cur-rent plan).
A text frame indexes a set of clauses and a setof ):elations comprising an input text.
TAMERLAN clausesdelimit the propositional and pragmatic ontent of Utr-get language utterances.
Relations represent links amongew;:nts, objects, or textual objects.
In what follows, wewill illustrate the structure of TAMERLAN concentrating onthe', representation f attitudes and agenda pointers (cor-responding to speech acts).
An exhaustive definition andde.~cription of TAMERLAN is given in Defrise and Niren-bu~g (in preparation).2.1 Representation of Attitudes.Each TAMERLAN clause contains one or several attitudeslots.
Each is composed of four lacets: the type facet, thevalue facet, the scope facet, and the 'attributed-to' facet.The possible types of attitudes are :,t, epistemic (with values taken from the {0,1 } intervalto account for expressions like perhaps; the end-points of the interval intuitively correspond to thevalues of impossible and necessary);,~, evaluative (with values taken from a similar scale,with the endpoints interpreted as, roughly, 'the best''the worst,' the midpoint as 'neutral,' and other inter-mediate points used to account for expressions like'fairly interesting');,~ deontic (ranging from 'unfair' to 'fair');expectation (ranging from 'expected' to 'surprise').The organization of the above types is similar - -  theirvalue ranges are all one type of scale.
The differences~unong them are semantic.
The above classification isan enhancement of Reichman's treatment of "contextspaces" (1985: 56).
We use the terminology (if not ex-actly the spirit) of her distinction among the epistemic,evaluative and dex)ntic issue-type context spaces.
Contextspace is Reichman's term for a discourse segment.
Theiss~,e context space roughly COIXesponds to our attitudecomponent, while the non-issue context space providesa sh~low taxonomy for discourse segment types (Reich-ma~ lists comment, narrative support, and nonnarrativesupport as the non-issue type values).Every attitude type has a scale of values associatedwith it.
The value component specifies a region on theappropriate scale.
Though the semantics of all scales isdifferent, the set of values is the same - -  we representall attitude scales as {0,1 } intervals.
Thus, the value (>0.8) on the scale of evaluative-saliency will be lexicallyrealizable through the modifier important(ly), while thesame value on the deontic scale will end up being realizedas should or ought to.qt~e attributed-to component of the attitude simply bindsthe attitude to a particular cognitive agent (which may bethe producer of the utterance or some other known or un-known agent), who is responsible for the content of theutterance.
This is important for understanding reportedspeech, and more generally the polyphony phenomena, inthe sense of Ducrot (1984).
Ducrot's theory of polyphony,an approach to extended reported speech treatment, pro-vides a framework for dealing with the interpretation f anumber of semantic and pragmatic phenomena, e.g., thedifference in meaning and use between 'since' and 'bedcause,' certain particularities of negative sentences, etc.The scope of the attitude representation pinpoints the en-tity to which this attitude is expressed.
The values ofthe scope can be the whole proposition, a part of it oranother attitude value, with its scope.
In understandingthe text the consumer notes the attitudes of the producerto the content.
The attitudes can be expressed towardevents, objects, properties or other attitudes (see 10 - -13, respectively).
(10) The train, unfortunately, left at 5 p.m.(11) This book is interesting.
(12) The meeting was reprehensibly short.
(13) Unfortunately, I ought to leave.McKeown and Elhadad (1989) "also treat argumentativescales aid attitudinals ina generation environment.
They,however, consider these phenomena aspart of syntax, thusavoiding the need to add a special pragmatic component totheir system.
This decision is appropriate from the pointof view of minimizing the changes in an existing eneratordue to the inclusion of attitude information.
However,if compatibility is an overriding concern, introducing aseparate component is a more appropriate choice.2.2 Representation of Producer's Goals.We overtly refer to the producer's goals in each TAMER-LAN clause, using the 'producer-intention' slot.
The fillerof this slot is a pointer to an action or a plan in the pro-ducer agenda.
The producer agenda, which is a necessarybackground component of text planning, contains repre-sentations of active goal and plan instances.
Since we areinterested in discourse situations, we take into accountonly the goals and plans thata) presuppose the situationwith at least two cognitive agents and b) relate to rhetoriocal (and not physical-action) realizations of goals.To illustrate our use of the 'producer-intention' slot intext generation, consider the task of generating from aTMR which we will gloss as 'The speaker promises toreturn to his current location at 10 o'clock.'
Dependingon the context and other parameters, the producer maydecide to generate 1 will return at 10 or l promise to returnat 10.
In the latter case the decision is made to realize themeaning of the speech act lexically.
The mechanism forthis is as follows: traversing the producer-intention slotthe producer gets to the relevant point in the agenda, whichis the (primitive) plan PROMISE.
3 Since the realization3or "I'IIRFakT, etc., as the case may be3 22 i .rules for speech acts prescribe their realization as first-person-singular clauses with the lexical realizations ofthe nmnes of appropriate speech plans (acts), the naturallanguage clause I promise X is produced, and, eventually,X is expanded into the subordinate natttral language clauseto return at 10.
The central point is that the former naturallanguage clause is the realization of a pointer in the input,not an entire text representation clause.
43 The  Text  P lan  Language (TPL) .The text plan is an intermediate data structure which isthe obtained through the application of the text planningrules to TMRs.
TPs serve as input to the semantics-to-syntax mapping rules that produce f-structures which inturn serve as input to the syntactic realization module.While TMR does not specify the boundaries and orderof sentences in the output ext, the ways of treating co-reference or the selection of the appropriate l xical units(both open-class items and such closed-class ones as re-alizations of producer attitudes and discourse cohesionmarkers, connectives) for the target ext, TP contains thisinformation.
Additionally, the text planning stage alsoproduces values for such features as definiteness, tenseand mood.A text plan is an hierarchically organized set of frames.The root of this hierarchy is the text structure frame which,notably, contains the slot "has-as-part" whose value is anordered set of plan sentence frames, S_i.
A plan sen-tence frame contains a slot "subtype" whose values in-clude "simple," complex," and "and."
Another slot con-tains an ordered list of plan clause frames comprising thesentence.
A plan clause frame contains the realization,through lexical choice and feature values, of both propo-sitional and pragmatic meanings in the input.
It lists therealizations of the head of the proposition and pointersto plan-role frames that contain realizations of the caseroles of this proposition.
(Sometimes the filler of a caserole slot in a plan clause will be a pointer to another planclause.)
Realizations of producer attitudes pertaining tothe clause and of cohesion markers which realize some ofTAMERLAN relations are also included, as is an indicationof whether agiven clause is a subordinate clause or a ma-trix clause in a complex sentence.
The plan role framesare composed in a similar fashion.4 The  Mechan ism o f  Text  P lann ing .Text planning can be understood as a mapping problembetween sets of expressions in TAMERLAN and TPL.
Themapping is achieved through the application of heuristicrules of the situation-action kind.
The rules take as inputelements of the input representation a d mapped them intoelements of a text plan based the various meaning com-ponents in the input and their combinations.
In addition,the heuristic rules take into account he state of affairs inthe communication situation, modelled in our system as astatic set of pragmatic factors that determine the stylisticslant of a text.
Our pragmatic factors are a subset of therhetorical goals suggested by Hovy (1987) and include4In natural language understanding, one will expect to obtainas input some full-fledged natural language clauses whose realmeaning is purely pragmatic.
Consider, for instance, the sen-tence"I hereby inform you that X."
The meaning of"I hereby in-form you" will represented asa filler of the "producer-intention"slot or as an attitude.such factors as simplicity, formality, colorfulness, etc.Finally, the heuristic rules can take into account elementsof the text plan under construction - - those elements thatwere produced by previously triggered rules.4.1 Control.In our generator, we adopt a blackboard model of control.This means that the work in the system is performed bya set of semi-autonomous knowledge sources which ap-ply various heuristic rules or run specialized algorithmsin a distributed fashion.
There is no centralized sequen-tial model of control.
The activated knowledge som'ceinstances are collected in the processing agenda(s).
Af-ter a knowledge source instantiation "fires," its resultsare listed on one of several public data structures, black-boards, supported by our system.
The balckboards arepublic in the sense that subsequent knowledge sourceinstantiations can draw knowledge needed for their ap-plication from the blackboard spaces in which outputs ofother knwoledge source instances is recorded.
Elementsof TMR and'IT' are recorded on one of the blackboards, asare various intermediate processing results, such as thoseproduced in the process of lexical selection.Efficiency of a computational architecture can becontrolled and improved by introducing special controlknowledge sources which perform manipulations of thecontents of the processor agendas, including such opera-tions as obviation and retraction.
See Nirenburg, Nybergand Defrise, 1989 for a sketch of the text plan controllerin our generation system.5 F rom Mean ing  Representat ions  to P lans .In this section we show how decisions concerning thetreatment of agenda pointers are reflected in the text planand how decisions about realization of attitudinals aremade.
We will illustrate the treatment of attitudinals byshowing the TMRs and their corresponding TPs for a setof examples.
The treatment of attitudinals will be illus-trated by listing the attitude-related text plauning rules (amuch more comprehensive list of text planning rules isincluded in Defrise and Nirenburg, 1989).
In this fashionwe will be able to illustrateboth knowledge representationlanguages and the planning rule language.5.1 Planning the Realization of Agenda Pointers.Consider (14) - -  (16) as the desired target for generation.
(14) I will definitely be back by 10.
(15) I will be back by 10.
(16) I promise to be back by 10.The corresponding set of (somewhat simplified)TAMERLAN structures i in (17) and (18), since the struc-tures for (15) and (16) will be identical - -  indeed, the sen-tences are synonymous and differ only in what is knownas register characteristics, in this case, the level of formal-ity.
Taking the above sets as input, the text planner willproduce the text plans in (19) - -  (21).
The differencesbetween the realizations of the input (18) as either (15) or(16) will be reflected in the text plans, see (20) and (21).
(17)(make-frame text(clauses (value clausel) )(relations(value re\]ationl ))(make-frame clausel222 4(proposition (va\].ue #returnl))(attitude (value attitude\].
))(producer-intention (value #statementl)))(make-frame #returnl(is-token-of (value *return))(phase (value begin))(iteration (value i))(duration (value I))(time (value (< I0))(agent (value *producer*))(destination (value #statementl.space)))(make-frame attitudel(epistemic (value I)(scope #returnl)(attributed-to *producer*))(make-frame #statementl(is-token-of (value *statement))(time (value #statement\].time))(space (value #statementl.space)))(make-frame relationl(type (value intention-before) }(arguments(first {value #statementl.time))~second (value #returnl.time))))(18)(make-frame text(clauses (value clausel))(relations(value relationl)))(make-frame clausel(proposition (value #returnl))(producer-intention (value #promisel)))make-frame #returnl(is-token-of (value *return))(phase (value begin))(iteration (value I))(duration (value I))(time (value (< i0))(agent (value *producer*))(destination (value #statement\].space)))make-frame #promisel(is-token-of (value *promise))(time (value #promisel.time))(space (value #promisel.space)))make-frame relationl(type (value intention-before))(arguments(first (value #promisel.time))(second (value #return\].time))))(19)(TSF(has-as-part Sl))(Sl(type simple)(clauses CI))(Cl(head be back)(time(head i0)(features(tlme-relation before)))(modifier(head definitely))(features(tense future)(mood declarative))(agent rl)(destination r2))(rl (head PRO)(features(r2(2O)(number singular)(person first)))(head *ellided*))(TSF(has-as-part SI))(Sl(type simple)(clauses CI))(Cl(head be back)(features(tense future)(mood declarative))(time(head i0)(features(time-relation before)))(agent rl)(destination r2))(rl (head PRO)(features(number singular)(person first)))(r2 (head *ellided*))(2~)(TSF(has-as-part SI))(Sl(type complex)(clauses (CI C2)){Cl(head promisel)(features(tense presen%)(mood declarative))(agent rl)(theme C2))(rl (head PRO)(features(number singular)(person first)))(C2(head be back)(time(head i0)(features(tlme-relation before)))(features(tense future)(mood declarative))(agent r2)(destination r3))(r2 (head PRO){features(number singular)(person first)))(r3 (head *ellided*))5.2 Text Planning Rules for Attitudes.Text planning rules in DIOGF.NES deal with a variety ofphenomena.
Some are devoted to text structure proper--the number and order of sentences and clauses to expressthe meanings of input; clause dependency structures, etc.Others deal with treatment of reference --- pronominal-ization, ellipsis, etc.
Still others take care of lexical selec-tion, determine tense and mood features of the target ext,etc.
A set of text planning rules devoted to realization ofproducer attitudes is presented in Figure 2.5 223A\].
IF (and (= clause i.attitude.type evaluative)(= clause i.attitude.value low)(:: clause-i .att itude.~cope clause i.proposition))'\]?HEN (add-unit-fille-r C i 'attitude 'unfort~nately)A2.
\]IF (and (- clause i.attit.ude.type epistemic)(-: clause i.attitude.va\[ue I)(= clause i.attitude.scope clause i?proposition))THEN (add-unit-fac-et-fil\]er C i ~features ~moed ~declarative)A3.
IF (and (= clause i.attitude.type epistemic)(=: clause--i.att\]tude.value O)(= clause i.attitude.scope clause i.proposition))THEN (add-unit-facet-fi l ler C i 'features 'mood ~negative)A4.
I\]:' (and (: clause i.attitude.type epistemic)(=: clause--i.attitude.value 0.5)(:: clause i.attitude.scope clause \].proposition))TIIEN (add-unit-fi l ler C i 'attitude 'perhaps)Figure 2: Text planning rules for 'producer attitudes'.Rule A1 deals with an attitude of the evaluative type;rules A2 through A4 with attitudes of the epistemic type.In the rules, the if clauses check the values in TMR and,depending onthe match, either add features to TP or add alexical realization for the attitudinal meaning (as in RuleA4).6 Statusand Future Work?In the DK)GENES project we adopt the methodologicalattitude of developing the generator functionalities in abreadth-first fashion.
In other words, unlike many otherprojects, DIOGENES do not tend to describe xhaustively aspecific linguistic phenomenon (e.g., negation, attaphora,aspect, scope of quantifiers) or type of processing (e.g.,text planning, lexical selection, syntactic realiz~ttiou) be-fore proceeding to the next one.
We prefer instead to gofor a complete functioning system which contains all (or,in practice, most) of the above components and coversall (or most) of the above phenomena.
It is clear that,at the beginning, each (or many) of these componentsis somewhat incomplete, and not every phenomenon isdescribed in sufficient detail.
However, this methodol-ogy allows us to benefit from a complete xperimentationenvironment and an open-ended architecture that facili-tates the addition of knowledge to the system as well astesting and debugging.
At present we have a working pro-totype text planning and generation system with narrowcoverage.
We are working on expanding the knowledgeneeded for achieving adeeper level of analysis of each ofthe linguistic phenomena covered in the system.Acknowledgements.Many thanks to the members of the DIOGENES project,especially Eric Nyberg, and to Ken Goodman for usefuldiscussions about the material and its presentation.
Thefirst author was supported in conducting the research re-ported in this document by the Belgian National IncentiveProgram for Fundamental Research in Artificial Intelli-gence initiated by the Belgian state - -  Prime Minister'sOffice - -  Science Policy Programming.
The scientificresponsibility is assumed by the authors.Bibliography,Anscombre, J.-C. andO.
Ducrot.
1983.
\[,'argumentation dansla iangue.
Brussels: Mardaga.Defrise, C. and S. Nirenburg.
1989.
Aspects of Text Planning.CMU-CMT Memorandum.
August.Defrise, C. and S. Nirenburg (in preparation).
Aspects of TextMeaning.
Center for Machine Translation, Carnegie MellonUniversity.Ducrot, O.
1984.
Polyphonic.
In Lalies, 4.Hovy, E. 1987.
Generating Natural Language under PragmaticConstraints.
Yale University Ph.D. Dissertation.McKeown, K. 1985.
Text Generatkm.
Cambridge: CambridgeUniversity Press.McKeown, K. and M. Elhadad.
1989.
A Comparison ofSurfaceLanguage Generators: A Case Study in Choice of Connectives.MS.
Columbia University.Meteer.
M. 1989.
The Spokesman Natural Language Genera-tion System.
TechnicalReport7090.
BoltBeranekandNewmanInc.
July.Nirenburg, S. and V. Raskin.
1987.
The Subworld ConceptLexicon and the Lexicon Management System.
ComputationalLinguistics, Volume 13, Issue 3-4.Nirenburg, S., E. Nyberg, R. McCardell, S. Huffman, E. Ken-schaft and I. Nirenburg.
1988.
Diogenes-88.
Technical ReportCMU-CMT-88-107.
Carnegie Mellon University.
June.Nirenburg, S. and L. Levin.
1989.
Knowledge RepresentationSupport.
Machine Translation, 4, pp.
25 - 52.Nirenburg, S. and I. Nirenburg.
1988.
A Framework tbr Lexi-cal Selection in Natural Language Generation..Proceedings ofCOLING-88.Nirenburg, S., E. Nyberg and C. Defrise.
1989.
Text Planningwith Opportunistic Control.
Technical Report CMU-CMT-88-113.
Carnegie-Mellon University.
June.Nirenburg, S., E. Nyb~'rg and C. Defrise.
(In preparation) TheD1OG\[~F2~ Natural Language Generation System.Reichman, R. 1985.
Getting Computers to Talk Like You andMe.
Cambridge, MA: MIT Press.224 6
