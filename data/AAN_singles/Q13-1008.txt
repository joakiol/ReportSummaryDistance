Transactions of the Association for Computational Linguistics, 1 (2013) 89?98.
Action Editor: Noah Smith.Submitted 12/2012; Published 5/2013.
c?2013 Association for Computational Linguistics.A Novel Feature-based Bayesian Model for Query Focused Multi-documentSummarizationJiwei LiSchool of Computer ScienceCarnegie Mellon Universitybdlijiwei@gmail.comSujian LiLaboratory of Computational LinguisticsPeking Universitylisujian@pku.edu.cnAbstractSupervised learning methods and LDA based topicmodel have been successfully applied in the field ofmulti-document summarization.
In this paper, wepropose a novel supervised approach that can in-corporate rich sentence features into Bayesian topicmodels in a principled way, thus taking advantages ofboth topic model and feature based supervised learn-ing methods.
Experimental results on DUC2007,TAC2008 and TAC2009 demonstrate the effective-ness of our approach.1 IntroductionQuery-focused multi-document summarization(Nenkova et al 2006; Wan et al 2007; Ouyang etal., 2010) can facilitate users to grasp the main ideaof documents.
In query-focused summarization, aspecific topic description, such as a query, whichexpresses the most important topic information isproposed before the document collection, and asummary would be generated according to the giventopic.Supervised models have been widely used in sum-marization (Li, et al 2009, Shen et al 2007,Ouyang et al 2010).
Supervised models usually re-gard summarization as a classification or regressionproblem and use various sentence features to build aclassifier based on labeled negative or positive sam-ples.
However, existing supervised approaches sel-dom exploit the intrinsic structure among sentences.This disadvantage usually gives rise to serious prob-lems such as unbalance and low recall in summaries.Recently, LDA-based (Blei et al 2003) Bayesiantopic models have widely been applied in multi-document summarization in that Bayesian ap-proaches can offer clear and rigorous probabilis-tic interpretations for summaries(Daume and Marcu,2006; Haghighi and Vanderwende, 2009; Jin et al2010; Mason and Charniak, 2011; Delort and Alfon-seca, 2012).
Exiting Bayesian approaches label sen-tences or words with topics and sentences which areclosely related with query or can highly generalizedocuments are selected into summaries.
However,LDA topic model suffers from the intrinsic disad-vantages that it only uses word frequency for topicmodeling and can not use useful text features such asposition, word order etc (Zhu and Xing, 2010).
Forexample, the first sentence in a document may bemore important for summary since it is more likelyto give a global generalization about the document.It is hard for LDA model to consider such informa-tion, making useful information lost.It naturally comes to our minds that we can im-prove summarization performance by making fulluse of both useful text features and the latent seman-tic structures from by LDA topic model.
One relatedwork is from Celikyilmaz and Hakkani-Tur (2010).They built a hierarchical topic model called Hybh-sum based on LDA for topic discovery and assumedthis model can produce appropriate scores for sen-tence evaluation.
Then the scores are used for tun-ing the weights of various features that helpful forsummary generation.
Their work made a good stepof combining topic model with feature based super-vised learning.
However, what their approach con-fuses us is that whether a topic model only basedon word frequency is good enough to generate anappropriate sentence score for regression.
Actually,how to incorporate features into LDA topic modelhas been a open problem.
Supervised topic modelssuch as sLDA(Blei and MacAuliffe 2007) give ussome inspiration.
In sLDA, each document is asso-ciated with a labeled feature and sLDA can integratesuch feature into LDA for topic modeling in a prin-89cipled way.With reference to the work of supervised LDAmodels, in this paper, we propose a novel sentencefeature based Bayesian model S-sLDA for multi-document summarization.
Our approach can natu-rally combine feature based supervised methods andtopic models.
The most important and challeng-ing problem in our model is the tuning of featureweights.
To solve this problem, we transform theproblem of finding optimum feature weights into anoptimization algorithm and learn these weights ina supervised way.
A set of experiments are con-ducted based on the benchmark data of DUC2007,TAC2008 and TAC2009, and experimental resultsshow the effectiveness of our model.The rest of the paper is organized as follows.
Sec-tion 2 describes some background and related works.Section 3 describes our details of S-sLDA model.Section 4 demonstrates details of our approaches,including learning, inference and summary gener-ation.
Section 5 provides experiments results andSection 6 concludes the paper.2 Related WorkA variety of approaches have been proposedfor query-focused multi-document summarizationssuch as unsupervised (semi-supervised) approaches,supervised approaches, and Bayesian approaches.Unsupervised (semi-supervised) approaches suchas Lexrank (Erkan and Radex, 2004), manifold(Wan et al 2007) treat summarization as a graph-based ranking problem.
The relatedness betweenthe query and each sentence is achieved by impos-ing querys influence on each sentence along withthe propagation of graph.
Most supervised ap-proaches regard summarization task as a sentencelevel two class classification problem.
Supervisedmachine learning methods such as Support VectorMachine(SVM) (Li, et al 2009), Maximum En-tropy (Osborne, 2002) , Conditional Random Field(Shen et al 2007) and regression models (Ouyanget al 2010) have been adopted to leverage the richsentence features for summarization.Recently, Bayesian topic models have shown theirpower in summarization for its clear probabilisticinterpretation.
Daume and Marcu (2006) proposedBayesum model for sentence extraction based onquery expansion concept in information retrieval.Haghighi and Vanderwende (2009) proposed topic-sum and hiersum which use a LDA-like topic modeland assign each sentence a distribution over back-ground topic, doc-specific topic and content topics.Celikyilmaz and Hakkani-Tur (2010) made a goodstep in combining topic model with supervised fea-ture based regression for sentence scoring in sum-marization.
In their model, the score of trainingsentences are firstly got through a novel hierarchi-cal topic model.
Then a featured based support vec-tor regression (SVR) is used for sentence score pre-diction.
The problem of Celikyilmaz and Hakkani-Turs model is that topic model and feature based re-gression are two separate processes and the score oftraining sentences may be biased because their topicmodel only consider word frequency and fail to con-sider other important features.
Supervised featurebased topic models have been proposed in recentyears to incorporate different kinds of features intoLDA model.
Blei (2007) proposed sLDA for doc-ument response pairs and Daniel et al(2009) pro-posed Labeled LDA by defining a one to one corre-spondence between latent topic and user tags.
Zhuand Xing (2010) proposed conditional topic randomfield (CTRF) which addresses feature and indepen-dent limitation in LDA.3 Model description3.1 LDA and sLDAThe hierarchical Bayesian LDA (Blei et al 2003)models the probability of a corpus on hidden topicsas shown in Figure 1(a).
Let K be the number oftopics , M be the number of documents in the cor-pus and V be vocabulary size.
The topic distributionof each document ?m is drawn from a prior Dirichletdistribution Dir(?
), and each document word wmnis sampled from a topic-word distribution ?z spec-ified by a drawn from the topic-document distribu-tion ?m.
?
is a K ?M dimensional matrix and each?k is a distribution over the V terms.
The generat-ing procedure of LDA is illustrated in Figure 2.
?mis a mixture proportion over topics of document mand zmn is a K dimensional variable that presentsthe topic assignment distribution of different words.Supervised LDA (sLDA) (Blei and McAuliffe2007) is a document feature based model and intro-90Figure 1: Graphical models for (a) LDA model and (b)sLDA model.1.
Draw a document proportion vector ?m|?
?
Dir(?)2.
For each word in m(a)draw topic assignment zmn|?
?Multi(?zmn)(b)draw word wmn|zmn, ?
?Multi(?zmn)Figure 2: Generation process for LDAduces a response variable to each document for topicdiscovering, as shown in Figure 1(b).
In the gener-ative procedure of sLDA, the document pairwise la-bel is draw from y|?
?zm, ?, ?2 ?
p(y|?
?zm, ?, ?2), where?
?zm = 1N?Nn=1 zm,n.3.2 Problem FormulationHere we firstly give a standard formulation of thetask.
Let K be the number of topics, V be the vo-cabulary size and M be the number of documents.Each document Dm is represented with a collectionof sentence Dm = {Ss}s=Nms=1 where Nm denotesthe number of sentences in mth document.
Eachsentence is represented with a collection of words{wmsn}n=Nmsn=1 where Nms denotes the number ofwords in current sentence.
??
?Yms denotes the featurevector of current sentence and we assume that thesefeatures are independent.3.3 S-sLDAzms is the hidden variable indicating the topic ofcurrent sentence.
In S-sLDA, we make an assump-tion that words in the same sentence are generatedfrom the same topic which was proposed by Gruber(2007).
zmsn denotes the topic assignment of cur-rent word.
According to our assumption, zmsn =Figure 3: Graph model for S-sLDA model1.
Draw a document proportion vector ?m|?
?
Dir(?)2.
For each sentence in m(a)draw topic assignment zms|?
?Multi(?zmn)(b)draw feature vector ??
?Yms|zms, ?
?
p(??
?Yms|zms, ?
)(c)for each word wmsn in current sentencedraw wmsn|zms, ?
?Multi(?zms)Figure 4: generation process for S-sLDAzms for any n ?
[1, Nms].
The generative approachof S-sLDA is shown in Figure 3 and Figure 4.
Wecan see that the generative process involves not onlythe words within current sentence, but also a seriesof sentence features.
The mixture weights over fea-tures in S-sLDA are defined with a generalized lin-ear model (GLM).p(??
?Yms|zms, ?)
=exp(zTms?)??
?Yms?zms exp(zTms?)??
?Yms(1)Here we assume that each sentence has T featuresand ??
?Yms is a T ?
1 dimensional vector.
?
is aK ?
T weight matrix of each feature upon topics,which largely controls the feature generation proce-dure.
Unlike s-LDA where ?
is a latent variable esti-mated from the maximum likelihood estimation al-gorithm, in S-sLDA the value of ?
is trained througha supervised algorithm which will be illustrated indetail in Section 3.3.4 Posterior Inference and EstimationGiven a document and labels for each sentence, theposterior distribution of the latent variables is:p(?, z1:N |w1:N , Y, ?, ?1:K , ?)
=?m p(?m|?)?s[p(zms|?m)p(??
?Yms|zms, ?
)?n p(wmsn|zmsn, ?zmsn ]?d?p(?m|?)?z?s[p(zms|?m)p(??
?Yms|zms, ?
)?n p(wmsn|?zmsn)](2)Eqn.
(2) cannot be efficiently computed.
Byapplying the Jensens inequality, we obtain alower bound of the log likelihood of documentp(?, z1:N |w1:N ,??
?Yms, ?, ?1:K , ?)
?
L, whereL =?msE[logP (zms|?)]
+?msE[logP (??
?Yms|zms, ?
)]+?mE[logP (?|?)]
+?msnE[logP (wmsn|zms, ?)]
+H(q)(3)91where H(q) = ?E[logq] and it is the entropy ofvariational distribution q is defined asq(?, z|?, ?)
=?mkq(?m|?
)?snq(zmsn|?ms) (4)here ?
a K-dimensional Dirichlet parameter vectorand multinomial parameters.
The first, third andforth terms of Eqn.
(3) are identical to the corre-sponding terms for unsupervised LDA (Blei et al2003).
The second term is the expectation of logprobability of features given the latent topic assign-ments.E[logP (??
?Yms|zms, ?)]
=E(zms)T ???
?Yms ?
log?zmsexp(zTms???
?Yms)(5)where E(zms)T is a 1 ?
K dimensional vector[?msk]k=Kk=1 .
The Bayes estimation for S-sLDAmodel can be got via a variational EM algorithm.
InEM procedure, the lower bound is firstly minimizedwith respect to ?
and ?, and then minimized with ?and ?
by fixing ?
and ?.E-step:The updating of Dirichlet parameter ?
is identicalto that of unsupervised LDA, and does not involvefeature vector ???Yms.
?newm ?
?+?s?m?s (6)?newsk ?
exp{E[log?m|?]
+Nms?n=1E[log(wmsn|?1:K)]+T?t=1?ktYst} = exp[?(?mk)??
(K?k=1?mk) +T?t=1?ktYst](7)where ?(?)
denotes the log ?
function.
ms denotesthe document that current sentence comes from andYst denotes the tth feature of sentence s.M-step:The M-step for updating ?
is the same as the pro-cedure in unsupervised LDA, where the probabilityof a word generated from a topic is proportional tothe number of times this word assigned to the topic.
?newkw =M?m=1Nm?s=1Nms?n=11(wmsn = w)?kms (8)4 Our Approach4.1 LearningIn this subsection, we describe how we learn the fea-ture weight ?
in a supervised way.
The learning pro-cess of ?
is a supervised algorithm combined withvariational inference of S-sLDA.
Given a topic de-scription Q1 and a collection of training sentences Sfrom related documents, human assessors assign ascore v(v = ?2,?1, 0, 1, 1) to each sentence in S.The score is an integer between?2 (the least desiredsummary sentences) and +2 (the most desired sum-mary sentences), and score 0 denotes neutral atti-tude.
Ov = {ov1, ov2, ..., vvk}(v = ?2,?1, 0, 1, 2)is the set containing sentences with score v. Let ?Qkdenote the probability that query is generated fromtopic k. Since query does not belong to any docu-ment, we use the following strategy to leverage ?Qk?Qk =?w?Q?kw?1MM?m=1exp[?(?mk)??
(K?k=1?mk)](9)In Equ.
(9), ?w?Q ?kw denotes the probability thatall terms in query are generated from topic kand 1M?Mm=1 exp[?(?mk)??
(?Kk=1 ?mk)] can beseen as the average probability that all documents inthe corpus are talking about topic k. Eqn.
(9) isbased on the assumption that query topic is relevantto the main topic discussed by the document corpus.This is a reasonable assumption and most previousLDA summarization models are based on similar as-sumptions.Next, we define ?Ov ,k for sentence set Ov, whichcan be interpreted as the probability that all sen-tences in collection Ov are generated from topic k.?Ov,k =1|Ov|?s?Ov?sk, k ?
[1,K], v ?
[?2, 2] (10)|Ov| denotes the number of sentences in set Ov.
In-spired by the idea that desired summary sentenceswould be more semantically related with the query,we transform problem of finding optimum ?
to thefollowing optimization problem:min?L(?)
=v=2?v=?2v ?KL(Ov||Q);T?t=1?kt = 1 (11)1We select multiple queries and their related sentences fortraining92where KL(Ov||Q) is the Kullback-Leibler diver-gence between the topic and sentence set Ov asshown in Eqn.
(12).KL(Ov||Q) =K?k=1?Ovklog?Ovk?Qk(12)In Eqn.
(11), we can see that O2, which contain de-sirable sentences, would be given the largest penaltyfor its KL divergence from Query.
The case is justopposite for undesired set.Our idea is to incorporate the minimization pro-cess of Eqn.
(11) into variational inference processof S-sLDA model.
Here we perform gradient basedoptimization method to minimize Eqn.(11).
Firstly,we derive the gradient of L(?)
with respect to ?.?L(?
)?xy=v=2?v=?2v ?
?KL(Qv||Q)??xy(13)?KL(Qv||Q)?
?xy=K?k=11|Qv|(1 + log?s?Qv|Qv|)?s?Qv??sk??xy?K?k=11|Qv|?s?Qv?Qsk?xy?K?k=11Qv?s?Qv?sk?Qk??sk?
?xy(14)For simplification, we regard ?
and ?
as constantduring updating process of ?, so ??Qk?
?xy = 0.2 We canfurther get first derivative for each labeled sentence.??sk?xy????????????????Ysyexp[?(?msi)??
(K?k=1?msk) +T?t=1?ktYsy]?
?w?s?kw if k = x0 if k 6= x(15)4.2 Feature SpaceLots of features have been proven to be useful forsummarization (Louis et al 2010).
Here we dis-cuss several types of features which are adopted inS-sLDA model.
The feature values are either binaryor normalized to the interval [0,1].
The followingfeatures are used in S-sLDA:Cosine Similarity with query: Cosine similarity isbased on the tf-idf value of terms.2This is reasonable because the influence of ?
and ?
havebeen embodied in ?
during each iteration.Local Inner-document Degree Order: Local Innerdocument Degree Order is a binary feature whichindicates whether Inner-document Degree (IDD) ofsentence s is the largest among its neighbors.
IDDmeans the edge number between s and other sen-tences in the same document.Document Specific Word: 1 if a sentence containsdocument specific word, 0 otherwise.Average Unigram Probability (Nenkova and Van-derwende, 2005; Celikyilmaz and Hakkani-Tur2010): As for sentence s, p(s) = ?w?s 1|s|pD(w),where pD(w) is the observed unigram probability indocument collection.In addition, we also use the commonly used fea-tures including sentence position, paragraph po-sition, sentence length and sentence bigram fre-quency.E-stepinitialize ?0sk := 1/K for all i and s.initialize ?mi := ?mi +N)m/K for all i.initialize ?kt = 0 for all k and t.while not convergencefor m = 1 : Mupdate ?t+1m according to Eqn.
(6)for s = 1 : Nmfor k = 1 : Kupdate ?t+1sk according to Eqn.
(7)normalize the sum of ?t+1sk to 1.Minimize L(?)
according to Eqn.
(11)-(15).M-step:update ?
according to Eqn.
(8)Figure 5: Learning process of ?
in S-sLDA4.3 Sentence Selection StrategyNext we explain our sentence selection strategy.
Ac-cording to our intuition that the desired summaryshould have a small KL divergence with query, wepropose a function to score a set of sentences Sum.We use a decreasing logistic function ?
(x) = 1/(1+ex) to refine the score to the range of (0,1).Score(Sum) = ?
(KL(sum||Q)) (16)Let Sum?
denote the optimum update summary.
Wecan get Sum?
by maximizing the scoring function.Sum?
= arg maxSum?S&&words(Sum)?LScore(Sum)(17)931.
Learning: Given labeled set Ov, learn the featureweight vector ?
using algorithm in Figure 5.2.
Given new data set and ?, use algorithm in section3.3 for inference.
(The only difference betweenthis step and step (1) is that in this step we do notneed minimize L(?).3.
Select sentences for summarization from algo-rithm in Figure 6.Figure 6: Summarization Generation by S-sLDA.A greedy algorithm is applied by adding sentenceone by one to obtain Sum?.
We use G to denotethe sentence set containing selected sentences.
Thealgorithm first initializes G to ?
and X to SU .
Dur-ing each iteration, we select one sentence from Xwhich maximize Score(sm ?G).
To avoid topic re-dundancy in the summary, we also revise the MMRstrategy (Goldstein et al 1999; Ouyang et al 2007)in the process of sentence selection.
For each sm,we compute the semantic similarity between sm andeach sentence st in set Y in Eqn.
(18).cos?sem(sm, st) =?k ?smk?stk?
?k ?2smk?
?k ?2stk(18)We need to assure that the value of semantic similar-ity between two sentences is less than Thsem.
Thewhole procedure for summarization using S-sLDAmodel is illustrated in Figure 6.
Thsem is set to 0.5in the experiments.5 Experiments5.1 Experiments Set-upThe query-focused multi-document summarizationtask defined in DUC3(Document UnderstandingConference) and TAC4(Text Analysis Conference)evaluations requires generating a concise and wellorganized summary for a collection of related newsdocuments according to a given query which de-scribes the users information need.
The queryusually consists of a title and one or more narra-tive/question sentences.
The system-generated sum-maries for DUC and TAC are respectively limited to3http://duc.nist.gov/.4http://www.nist.gov/tac/.250 words and 100 words.
Our experiment data iscomposed of DUC 2007, TAC5 2008 and TAC 2009data which have 45, 48 and 44 collections respec-tively.
In our experiments, DUC 2007 data is usedas training data and TAC (2008-2009) data is usedas the test data.Stop-words in both documents and queries areremoved using a stop-word list of 598 words, andthe remaining words are stemmed by Porter Stem-mer6.
As for the automatic evaluation of summa-rization, ROUGE (Recall-Oriented Understudy forGisting Evaluation) measures, including ROUGE-1, ROUGE-2, and ROUGE-SU47 and their corre-sponding 95% confidence intervals, are used to eval-uate the performance of the summaries.
In order toobtain a more comprehensive measure of summaryquality, we also conduct manual evaluation on TACdata with reference to (Haghighi and Vanderwende,2009; Celikyilmaz and Hakkani-Tur, 2011; Delortand Alfonseca, 2011).5.2 Comparison with other Bayesian modelsIn this subsection, we compare our model with thefollowing Bayesian baselines:KL-sum: It is developed by Haghighi andVanderwende (Lin et al 2006) by using a KL-divergence based sentence selection strategy.KL(Ps||Qd) =?wP (w)logP (w)Q(w) (19)where Ps is the unigram distribution of candidatesummary andQd denotes the unigram distribution ofdocument collection.
Sentences with higher rankingscore is selected into the summary.HierSum: A LDA based approach proposed byHaghighi and Vanderwende (2009), where unigramdistribution is calculated from LDA topic model inEqu.
(14).Hybhsum: A supervised approach developed byCelikyilmaz and Hakkani-Tur (2010).For fair comparison, baselines use the same pro-precessing methods with our model and all sum-5Here, we only use the docset-A data in TAC, since TACdata is composed of docset-A and docset-B data, and the docset-B data is mainly for the update summarization task.6http://tartarus.org/ martin/PorterStemmer/.7Jackknife scoring for ROUGE is used in order to comparewith the human summaries.94maries are truncated to the same length of 100words.
From Table 1 and Table 2, we canMethods ROUGE-1 ROUGE-2 ROUGE-SU4Our 0.3724 0.1030 0.1342approach (0.3660-0.3788) (0.0999-0.1061) (0.1290-0.1394)Hybhsum 0.3703 0.1007 0.1314(0.3600-0.3806) (0.0952-0.1059) (0.1241-0.1387)HierSum 0.3613 0.0948 0.1278(0.3374-0.3752) (0.0899-0.0998) (0.1197-0.1359)KLsum 0.3504 0.0917 0.1234(0.3411-0.3597) (0.0842-0.0992) (0.1155-0.1315)StandLDA 0.3368 0.0797 0.1156(0.3252-0.3386) (0.0758-0.0836) (0.1072-0.1240)Table 1: Comparison of Bayesian models on TAC2008Methods ROUGE-1 ROUGE-2 ROUGE-SU4Our 0.3903 0.1223 0.1488approach (0.3819-0.3987) (0.1167-0.1279) (0.1446-0.1530)Hybhsum 0.3824 0.1173 0.1436(0.3686-0.3952) (0.1132-0.1214) (0.1358-0.1514)HierSum 0.3706 0.1088 0.1386(0.3624-0.3788) (0.0950-0.1144) (0.1312-0.1464)KLsum 0.3619 0.0972 0.1299(0.3510-0.3728) (0.0917-0.1047) (0.1213-0.1385)StandLDA 0.3552 0.0847 0.1214(0.3447-0.3657) (0.0813-0.0881) (0.1141-0.1286)Table 2: Comparison of Bayesian models on TAC2009see that among all the Bayesian baselines, Hybh-sum achieves the best result.
This further illus-trates the advantages of combining topic model withsupervised method.
In Table 1, we can see thatour S-sLDA model performs better than Hybhsumand the improvements are 3.4% and 3.7% with re-spect to ROUGE-2 and ROUGE-SU4 on TAC2008data.
The comparison can be extended to TAC2009data as shown in Table 2: the performance of S-sLDA is above Hybhsum by 4.3% in ROUGE-2and 5.1% in ROUGE-SU4.
It is worth explainingthat these achievements are significant, because inthe TAC2008 evaluation, the performance of the topranking systems are very close, i.e.
the best systemis only 4.2% above the 4th best system on ROUGE-2 and 1.2% on ROUGE-SU4.5.3 Comparison with other baselines.In this subsection, we compare our model with somewidely used models in summarization.Manifold: It is the one-layer graph based semi-supervised summarization approach developed byWan et al2008).
The graph is constructed only con-sidering sentence relations using tf-idf and neglectstopic information.LexRank: Graph based summarization approach(Erkan and Radev, 2004), which is a revised versionof famous web ranking algorithm PageRank.
It isan unsupervised ranking algorithms compared withManifold.SVM: A supervised method - Support Vector Ma-chine (SVM) (Vapnik 1995) which uses the samefeatures as our approach.MEAD: A centroid based summary algorithm byRadev et al(2004).
Cluster centroids in MEADconsists of words which are central not only to onearticle in a cluster, but to all the articles.
Similarityis measure using tf-idf.At the same time, we also present the top threeparticipating systems with regard to ROUGE-2 onTAC2008 and TAC2009 for comparison, denoted as(denoted as SysRank 1st, 2nd and 3rd)(Gillick et al2008; Zhang et al 2008; Gillick et al 2009; Varmaet al 2009).
The ROUGE scores of the top TACsystem are directly provided by the TAC evaluation.From Table 3 and Table 4, we can see thatour approach outperforms the baselines in terms ofROUGE metrics consistently.
When compared withthe standard supervised method SVM, the relativeimprovements over the ROUGE-1, ROUGE-2 andROUGE-SU4 scores are 4.3%, 13.1%, 8.3% respec-tively on TAC2008 and 7.2%, 14.9%, 14.3% onTAC2009.
Our model is not as good as top par-ticipating systems on TAC2008 and TAC2009.
Butconsidering the fact that our model neither uses sen-tence compression algorithm nor leverage domainknowledge bases like Wikipedia or training data,such small difference in ROUGE scores is reason-able.5.4 Manual EvaluationsIn order to obtain a more accurate measure of sum-mary quality for our S-sLDA model and Hybhsum,we performed a simple user study concerning thefollowing aspects: (1) Overall quality: Which sum-mary is better overall?
(2) Focus: Which summarycontains less irrelevant content?
(3)Responsiveness:Which summary is more responsive to the query.
(4) Non-Redundancy: Which summary is less re-dundant?
8 judges who specialize in NLP partic-ipated in the blind evaluation task.
Evaluators arepresented with two summaries generated by S-sLDA95Methods ROUGE-1 ROUGE-2 ROUGE-SU4Our 0.3724 0.1030 0.1342approach (0.3660-0.3788) (0.0999-0.1061) (0.1290-0.1394)SysRank 1st 0.3742 0.1039 0.1364(0.3639-0.3845) (0.0974-0.1104) (0.1285-0.1443)SysRank 2nd 0.3717 0.0990 0.1326(0.3610-0.3824 (0.0944-0.1038) (0.1269-0.1385)SysRank 3rd 0.3710 0.0977 0.1329(0.3550-0.3849) (0.0920-0.1034) (0.1267-0.1391)PageRank 0.3597 0.0879 0.1221(0.3499-0.3695) (0.0809-0.0950) (0.1173-0.1269)Manifold 0.3621 0.0931 0.1243(0.3506-0.3736) (0.0868-0.0994) (0.1206-0.1280)SVM 0.3588 0.0921 0.1258(0.3489-0.3687) (0.0882-0.0960) (0.1204-0.1302)MEAD 0.3558 0.0917 0.1226(0.3489-0.3627) (0.0882-0.0952) (0.1174-0.1278)Table 3: Comparison with baselines on TAC2008Methods ROUGE-1 ROUGE-2 ROUGE-SU4Our 0.3903 0.1223 0.1488approach (0.3819-0.3987) (0.1167-0.1279) (0.1446-0.1530)SysRank 1st 0.3917 0.1218 0.1505(0.3778-0.4057) (0.1122-0.1314) (0.1414-0.1596)SysRank 2nd 0.3914 0.1212 0.1513(0.3808-0.4020) (0.1147-0.1277) (0.1455-0.1571)SysRank 3rd 0.3851 0.1084 0.1447(0.3762-0.3932) (0.1025-0.1144) (0.1398-0.1496)PageRank 0.3616 0.0849 0.1249(0.3532-0.3700) (0.0802-0.0896) (0.1221-0.1277)Manifold 0.3713 0.1014 0.1342(0.3586-0.3841) (0.0950-0.1178) (0.1299-0.1385)SVM 0.3649 0.1028 0.1319(0.3536-0.3762) (0.0957-0.1099) (0.1258-0.1380)MEAD 0.3601 0.1001 0.1287(0.3536-0.3666) (0.0953-0.1049) (0.1228-0.1346)Table 4: Comparison with baselines on TAC2009and Hybhsum, as well as the four questions above.Then they need to answer which summary is better(tie).
We randomly select 20 document collectionsfrom TAC 2008 data and randomly assign two sum-maries for each collection to three different evalua-tors to judge which model is better in each aspect.As we can see from Table 5, the two models al-most tie with respect to Non-redundancy, mainlybecause both models have used appropriate MMRstrategies.
But as for Overall quality, Focus andOur(win) Hybhsum(win) TieOverall 37 14 9Focus 32 18 10Responsiveness 33 13 14Non-redundancy 13 11 36Table 5: Comparison with baselines on TAC2009Responsiveness, S-sLDA model outputs Hybhsumbased on t-test on 95% confidence level.
Ta-ble 6 shows the example summaries generated re-spectively by two models for document collectionD0803A-A in TAC2008, whose query is ?Describethe coal mine accidents in China and actions taken?.From table 6, we can see that each sentence in thesetwo summaries is somewhat related to topics of coalmines in China.
We also observe that the summaryin Table 6(a) is better than that in Table 6(b), tend-ing to select shorter sentences and provide more in-formation.
This is because, in S-sLDA model, topicmodeling is determined simultaneously by variousfeatures including terms and other ones such as sen-tence length, sentence position and so on, whichcan contribute to summary quality.
As we can see,in Table 6(b), sentences (3) and (5) provide someunimportant information such as ?somebody said?,though they contain some words which are relatedto topics about coal mines.
(1)China to close at least 4,000 coal mines this year:official (2)By Oct. 10 this year there had been 43 coalmine accidents that killed 10 or more people, (3)Offi-cials had stakes in coal mines.
(4)All the coal mineswill be closed down this year.
(5) In the first eightmonths, the death toll of coal mine accidents rose8.5 percent last year.
(6) The government has issueda series of regulations and measures to improve thecoun.try?s coal mine safety situation.
(7)The miningsafety technology and equipments have been sold tocountries.
(8)More than 6,000 miners died in accidentsin China(1) In the first eight months, the death toll of coal mineaccidents across China rose 8.5 percent from the sameperiod last year.
(2)China will close down a number ofill-operated coal mines at the end of this month, saida work safety official here Monday.
(3) Li Yizhong,director of the National Bureau of Production SafetySupervision and Administration, has said the collusionbetween mine owners and officials is to be condemned.
(4)from January to September this year, 4,228 peoplewere killed in 2,337 coal mine accidents.
(5) Chensaid officials who refused to register their stakes incoal mines within the required timeTable 6: Example summary text generated by systems(a)S-sLDA and (b) Hybhsum.
(D0803A-A, TAC2008)966 ConclusionIn this paper, we propose a novel supervised ap-proach based on revised supervised topic model forquery-focused multi document summarization.
Ourapproach naturally combines Bayesian topic modelwith supervised method and enjoy the advantages ofboth models.
Experiments on benchmark demon-strate good performance of our model.AcknowledgmentsThis research work has been supported byNSFC grants (No.90920011 and No.61273278),National Key Technology R&D Program(No:2011BAH1B0403), and National High Tech-nology R&D Program (No.2012AA011101).
Wealso thank the three anonymous reviewers for theirhelpful comments.
Corresponding author: SujianLi.ReferencesDavid Blei and Jon McAuliffe.
Supervised topic models.2007.
In Neural Information Processing SystemsDavid Blei, Andrew Ng and Micheal Jordan.
Latentdirichlet alcation.
In The Journal of Machine Learn-ing Research, page: 993-1022.Charles Broyden.
1965.
A class of methods for solv-ing nonlinear simultaneous equations.
In Math.
Comp.volume 19, page 577-593.Jaime Carbonell and Jade Goldstein.
1998.
The use ofMMR, diversity-based reranking for reordering doc-uments and producing summaries.
In Proceedings ofthe 21st annual international ACM SIGIR conferenceon Research and development in information retrieval.Asli Celikyilmaz and Dilek Hakkani-Tur.
2010.
A Hy-brid hierarchical model for multi-document summa-rization.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics.
page:815-825Jade Goldstein, Mark Kantrowitz, Vibhu Mittal andJaime Carbonell.
1999.
Summarizing Text Docu-ments: Sentence Selection and Evaluation Metrics.
InProceedings of the 22nd annual international ACM SI-GIR conference on Research and development in infor-mation retrieval, page: 121-128.Amit Grubber, Micheal Rosen-zvi and Yair Weiss.
2007.Hidden Topic Markov Model.
In Artificial Intelligenceand Statistics.Hal Daume and Daniel Marcu H. 2006.
Bayesian Query-Focused Summarization.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th annual meeting of the Association forComputational Linguistics, page 305-312.Gune Erkan and Dragomir Radev.
2004.
Lexrank: graph-based lexical centrality as salience in text summariza-tion.
In J. Artif.
Intell.
Res.
(JAIR), page 457-479.Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, The ICSISummarization System at TAC, TAC 2008.Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur,Berndt Bohnet, Yang Liu, Shasha Xie.
The ICSI/UTDSummarization System at TAC 2009.
TAC 2009Aria Haghighi and Lucy Vanderwende.
2009.
Exploringcontent models for multi-document summarization.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 362370.Feng Jin, Minlie Huang, and Xiaoyan Zhu.
2010.
Thesummarization systems at tac 2010.
In Proceedings ofthe third Text Analysis Conference, TAC-2010.Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha andYong Yu.
2009.
Enhancing diversity, coverage and bal-ance for summarization through structure learning.
InProceedings of the 18th international conference onWorld wide web, page 71-80.Chin-Yew Lin, Guihong Gao, Jianfeng Gao and Jian-YunNie.
2006.
An information-theoretic approach to au-tomatic evaluation of summaries.
In Proceedings ofthe main conference on Human Language TechnologyConference of the North American Chapter of the As-sociation of Computational Linguistics, page:462-470.Annie Louis, Aravind Joshi, Ani Nenkova.
2010.
Dis-course indicators for content selection in summariza-tion.
In Proceedings of the 11th Annual Meeting ofthe Special Interest Group on Discourse and Dialogue,page:147-156.Tengfei Ma, Xiaojun Wan.
2010.
Multi-document sum-marization using minimum distortion, in Proceedingsof International Conference of Data Mining.
page354363.Rebecca Mason and Eugene Charniak.
2011.
Extractivemulti-document summaries should explicitly not con-tain document-specific content.
In proceedings of ACLHLT, page:49-54.Ani Nenkova and Lucy Vanderwende.
The impact of fre-quency on summarization.
In Tech.
Report MSR-TR-2005-101, Microsoft Research, Redwood, Washing-ton, 2005.Ani Nenkova, Lucy Vanderwende and Kathleen McKe-own.
2006.
A compositional context sensitive multi-document summarizer: exploring the factors that inu-ence summarization.
In Proceedings of the 29th an-nual International ACM SIGIR Conference on Re-97search and Development in Information Retrieval,page 573-580.Miles Osborne.
2002.
Using maximum entropy for sen-tence extraction.
In Proceedings of the ACL-02 Work-shop on Automatic Summarization, Volume 4 page:1-8.Jahna Otterbacher, Gunes Erkan and Dragomir Radev.2005.
Using random walks for question-focused sen-tence retrieval.
In Proceedings of the Conference onHuman Language Technology and Empirical Methodsin Natural Language Processing, page 915-922You Ouyang, Wenjie Li, Sujian Li and Qin Lua.
2011.Applying regression models to query-focused multi-document summarization.
In Information Processingand Management, page 227-237.You Ouyang, Sujian.
Li, and Wenjie.
Li.
2007, Develop-ing learning strategies for topic-based summarization.In Proceedings of the sixteenth ACM conference onConference on information and knowledge manage-ment, page: 7986.Daniel Ramage, David Hall, Ramesh Nallapati andChristopher Manning.
2009.
Labeled LDA: A super-vised topic model for credit attribution in multi-labeledcorpora.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,Vol 1, page 248-256.Dou She, Jian-Tao Sun, Hua Li, Qiang Yang andZheng Chen.
2007.
Document summarization usingconditional random elds.
In Proceedings of Inter-national Joint Conference on Artificial Intelligence,page: 28622867.V.
Varma, V. Bharat, S. Kovelamudi, P. Bysani, S. GSK,K.
Kumar N, K. Reddy, N. Maganti , IIIT Hyderabadat TAC 2009.
TAC2009Xiaojun Wan and Jianwu Yang.
2008.
Multi-documentSummarization using cluster-based link analysis.
InProceedings of the 31st annual international ACM SI-GIR conference on Research and development in in-formation retrieval, page: 299-306.Xiaojun Wan, Jianwu Yang and Jianguo Xiao.
2007.Manifold-ranking based topic-focused multi-document summarization.
In Proceedings of In-ternational Joint Conference on Artificial Intelligence,page 2903-2908.Furu Wei, Wenjie Li, Qin Lu and Yanxiang He.
2008.
Ex-ploiting Query-Sensitive Similarity for Graph-BasedQuery-Oriented Summarization.
In Proceedings of the31st annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,page 283-290.Jin Zhang, Xueqi Cheng, Hongbo Xu, Xiaolei Wang, Yil-ing Zeng.
ICTCAS?s ICTGrasper at TAC 2008: Sum-marizing Dynamic Information with Signature TermsBased Content Filtering, TAC 2008.Dengzhong Zhou, Jason Weston, Arthur Gretton, OlivierBousquet and Bernhard Schlkopf.
2003.
Ranking onData Manifolds.
In Proceedings of the Conference onAdvances in Neural Information Processing Systems,page 169-176.Jun Zhu and Eric Xing.
2010.
Conditional Topic RandomFields.
In Proceedings of the 27th International Con-ference on Machine Learning.Xiaojin Zhu, Zoubin Ghahramani and John Laf-ferty.
2003.
Semi-supervised Learning using GaussianFields and Harmonic Functions.
In Proceedings of In-ternational Conference of Machine Learning, page:912-919.98
