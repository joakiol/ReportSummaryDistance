Probabilistic Models for PP-attachment Resolution and NP AnalysisEric GaussierXRCE6, Chemin de Maupertuis38240 Meylan, Francefname.lname@xrce.xerox.comNicola CanceddaXRCE6, Chemin de Maupertuis38240 Meylan, Francefname.lname@xrce.xerox.comAbstractWe present a general model for PP attach-ment resolution and NP analysis in French.We make explicit the different assumptionsour model relies on, and show how it gener-alizes previously proposed models.
We thenpresent a series of experiments conductedon a corpus of newspaper articles, and as-sess the various components of the model,as well as the different information sourcesused.1 IntroductionPrepositional phrase attachment resolution and nounphrase analysis are known to be two difficult tasks.Traditional context-free rules for example do not helpat all in selecting the good parse for a noun phrase,since all valid parses are a priori correct.
Subcatego-rization information can help solve the problem, butthe amount of information necessary to be encodedin such lexicons is huge, since in addition to subcat-egorization frames, one should encode the differentsenses of words and rules on how these senses canbe combined, as well as the different units (single ormulti word expressions) a language makes use of.
Ithas been recognized in different works that part ofthis knowledge can be (semi-)automatically acquiredfrom corpora and used in the decision process.
Severalmodels have been proposed for PP-attachment resolu-tion and NP analysis, built on various building blocksand making use of diverse information.
Most of thesemodels fit within a probabilistic framework.
Sucha framework allows one to estimate various quanti-ties and to perform inference with incomplete infor-mation, two necessary steps for the problem at hand.We present in this paper a model generalizing severalmodels already proposed and integrating several infor-mation sources.We focus here on analysing strings correspondingto a combination of traditional VERB NOUN PREPsequences and noun phrases.
More precisely, the NPswe consider are arbitrarily complex noun phrases withno embedded subordinate clause.
The PP attachmentproblems we tackle are those involved in analysingsuch NPs for languages relying on composition of Ro-mance type (as French or Italian), as well as thosepresent in verbal configurations for languages relyingon composition of Germanic type (as German or En-glish).
However, our model can easily be extended todeal with other cases as well.
The problem raised inanalysing such sequences is of the utmost relevancefor at least two reasons: 1) NPs constitute the vast ma-jority of terms.
Correctly identifying the boundariesand the internal structure of noun phrases is crucial tothe automatic discovery of domain-specific termino-logical databases; 2) PP-attachment ambiguity is oneof the main sources of syntactic ambiguity in general.In the present work, we focus on the French lan-guage since we have various lexical information at ourdisposal for this language that we would like to as-sess in the given context.
Furthermore, French dis-plays interesting propoerties (like gender and num-ber agreement for adjectives) which makes it an in-teresting language to test on.
The remainder of thepaper is organized as follows: we first describe howwe preprocessed our corpus, and which part we re-tained for probability estimation.
We then present thegeneral model we designed, and show how it com-pares with previous ones.
We then describe the exper-iments we performed and discuss the results obtained.We also describe, in the experiments section, howwe integrated different types of information sources(e.g.
prior knowledge encoded as subcategorizationframes), and how we weighted multiple sources of ev-idence according to their reliability.2 Nuclei and sequences of nucleiWe first take the general view that our problem canbe formulated as one of finding dependency relationsbetween nuclei.
Without loss of generality, we definea nucleus to be a unit that contains both the syntacticand semantic head and that exhibits only unambiguousinternal syntactic structure.
For example, the base NP?the white horse?
is a nucleus, since the attachmentsof both the determiner and the adjective to the nounare straightforward.
The segmentation into nuclei re-lies on a manually built chunker, similar to the onedescribed in (Ait-Mokhtar and Chanod, 1997), and re-sembles the one proposed in (Samuelsson, 2000).
Themotivation for this assumption is twofold.
First, theamount of grammatical information carried by indi-vidual words varies greatly across language families.Grammatical information carried by function wordsin non-agglutinative languages, for instance, is real-ized morphologically in agglutinative languages.
Amodel manipulating dependencies at the word levelonly would be constrained to the specific amount ofgrammatical and lexical information associated withwords in a given language.
Nuclei, on the other hand,tend to correspond to phrases of the same type acrosslanguages, so that relying on the notion of nucleusmakes the approach more portable.
A second moti-vation for considering nuclei as elementary unit is thattheir internal structure is by definition unambiguous,so that there is no point in applying any algorithmwhatsoever to disambiguate them.We view each nucleus as being composed of severallinguistic layers with different information, namelya semantic layer comprising the possible semanticclasses for the word under consideration, a syntacticlayer made of the POS category of the word and itsgender and number information, and a lexical layerconsisting of the word itself (referred to as the lexemein the following), and the preposition, for prepositionalphrases.
For nuclei comprising more than two non-empty words (as ?the white horse?
), we retain only onelexeme, the one associated with the last word which isconsidered to be the head word in the sequence.
Ex-cept for the semantic information, all the necessary in-formation is present in the output of the chunker.
Thesemantic lexicon we used was encoded as a finite-statetransducer, which was looked up for injecting seman-tic classes in each nucleus.
When no semantic infor-mation is available for a given word, we use its part-of-speech category as its semantic class1.
For example,starting with the sentence:1The semantic resource we used can be purchased fromwww.lexiquest.com.
This resource contains approximately90 different sementic classes organized into a hierarchy.
Wehave not made use of this hierarchy in our experiments.Il doit rencontrer le pre?sident de la fe?de?ration franc?aise.
(He has to meet the president of the French federation.
)we obtain the following sequence of nuclei:ilCAT=?PRON?, GN=?Masc-Sg?, PREP=?
?,SEM=?PRON?rencontrerCAT=?VERB?, GN=?
?, PREP=?
?, SEM=?VERB?pre?sidentCAT=?NOUN?, GN=?Masc-Sg?, PREP=?
?,SEM=?FONCTION?fe?de?rationCAT=?NOUN?, GN=?Fem-Sg?, PREP=?de?,SEM=?HUMAIN?franc?aiseCAT=?ADJ?, GN=?Fem-Sg?, PREP=?
?, SEM=?GEO?As we see in this example, the semantic resource weuse is incomplete and partly questionable.
The at-tribute HUMAN for federation can be understood ifone views a federation as a collection of human be-ings, which we believe is the rationale behind this an-notation.
However, a federation also is an institution,a sense which is missing in the resource we use.In the preceding example, the preposition de canbe attached to the verb rencontrer or to the nounpre?sident.
It cannot be attached to the pronoun il.As far as terminology extraction is our final objective,pre?sident de la fe?de?ration franc?aise can be deemeda good candidate term.
However, in order to accu-rately identify this unit, a high confidence in the factthat the preposition de attaches to the noun pre?sidentmust be achieved.
Sentences can be conveniently seg-mented into smaller self-contained units according tosome heuristics to reduce the combinatorics of attach-ments ambiguities.
We define safe chains as beingsequences of nuclei in which all the items but the firstare attached to other nuclei within the chain itself.
Inthe preceding example, for instance, only the nucleusassociated with rencontrer is not attached to a nucleuswithin the chain rencontrer ... franc?aise.
This chainis thus a safe chain.
To keep the number of alterna-tive (combinations of) attachments as low as possible,we are interested in isolating as short safe chains aspossible given the information available at this point,i.e.
words and their parts-of-speech (the knowledge ofsemantic classes is of little help in this task).In French, and except for few cases involving em-bedded clauses and coordination, the following heuris-tics can be used to identify ?minimal?
safe chains: ex-tract the longest sequences beginning with a nominal,verbal, prepositional or adjectival nucleus, containingonly nominal, prepositional, adjectival, adverbial orverbal nuclei in indefinite moods.There is a tension in parameter estimation of prob-abilistic models between relying on accurate infor-mation and relying on enough data.
In an unsuper-vised approach to PP-attachment resolution and NPanalysis, accurate information in the form of depen-dency relations between words is not directly acces-sible.
However, specific configurations can be identi-fied from which accurate information can be extracted.Safe chains provide such configurations.
Indeed ifthere is only one possible attachment site to the leftof a nucleus, then its attachment is unambiguous.
Dueto the possible ambiguities the French language dis-plays (e.g.
a preposition can be attached to a noun,a verb or an adjective), only the first two nuclei of asafe chain provide reliable information (we skip ad-verbs, the attachment of which obeys specific and sim-ple rules).
From the preceding example, for instance,we can infer a direct relation between rencontrer andpre?sident, but this is the only attachment we can besure of.
The use of less reliable information sourcesfor model parameters whose estimation would other-wise require manual supervision is the object of an ex-periment described in Section 6.3 Attachment ModelLet us denote the   nucleus in a chain by    , and thethe nucleus to which it is attached by 	   (for eachchain, we introduce an additional empty nucleus towhich the head of the chain is attached).
Given a chainof nuclei, we denote bythe set of dependency re-lations covering the chain of nuclei   .
Weare interested in the set such that fffi  is maxi-mal.
Assuming that the dependencies are built by pro-cessing the chain in linear order, We have:flffi!
#"	$&%fffi	$&%'ffifl&ffi$(%)*"%,+-+-+"	$&%./10%flfi/ffi/$(%2) (1)/ differs from  / $(% only in that it additionally speci-fies a particular attachment site    for fi3  such that nocycle nor crossing dependencies are produced.
In or-der to avoid sparse data problems, we make the simpli-fying assumption (similar to the one presented in (Eis-ner, 1996)) that the attachment of nucleus fi3  to nu-cleus    depends only on the set of indices of the pre-ceding dependency relations (in order to avoid cyclesand crossing dependencies) and on the three nuclei fi3  , and 4 /  , where 5 /  denotes the last nucleus be-ing attached to    .
5 /  is thus the closest sibling offi3.
Conditioning attachment on it the attachment offi3allows capturing the fact that the object of a verbmay depend on its subject, that the indirect object maydepend on the direct object, and other similar indirectdependencies.
In order to focus on the probabilities ofinterest, we use the following simplified notation:fffi/ffi/$&%1)!6fl78fi/9:fl;fi3ffi  <5/;78fi/; (2)where 78fi /  represents the graph produced by thedependencies generated so far.
If this graph containscycles or crossing links, the associated probability is0.
Making explicit the different elements of a nucleus,we obtain:ff93ffi  =4/978/9>"?
@A?CBD/Eff5F ffi  <5/; (3):ffGIH/ffi F!= =4/; (4):fl4J=AK/ffi LH/)F!= =4/; (5):flMON/ffiJ=AK/IH/)F!= =4/; (6):flPQ1R/ffiMON/SJ=AK/IH/)F!= =4/; (7)since the graph 78fi /  provides the index of the nu-cleus    to which fi3  is attached to.
Obviously, mostof the above probabilities cannot be directly estimated.A number of simplifying assumptions preserving sig-nificant conditional dependencies were adopted.Assumption 1: except for graphs with cycles andcrossing links, for which the associated probability is0, we assume a uniform distribution on the set of pos-sible graphs.A prior probability fl78 / ; could be used to modelcertain corpus-specific preferences such as privilegingattachments to the immediatly preceding nucleus (inFrench or English for example).
However, we decidednot to make use of this possibility for the moment.Assumption 2: the semantic class of a nucleus de-pends only on the semantic class of its regent.This assumption, also used in (Lauer and Dras,1994), amounts to considering a 1st-order Markovchain on the semantic classes of nuclei, and representsa good trade-off between model accuracy and practicalestimation of the probabilities in (3).
It leads to:fl4Fffi =4/;>fl5FffiFT 9 (8)Assumption 3: the preposition of a nucleus dependsonly on its semantic class and on the lexeme and POScategory of its regent, thus leading to:fl-IH/ffi =4/;>ffGLH/ffiF!SJ<UK9PQR (9)The nucleus 4 /  does not provide any information onthe generation of the preposition, and is thus not re-tained.
As far as the regent nucleus    is concerned,the dependence on the POS category controls the factthat adjectives are less likely to subcategorize prepo-sitions than verbs.
For arguments, the preposition iscontrolled by subcategorization frames, which directlydepend on the lexeme under consideration, and to aless extent to its semantic class (even though this de-pendence does exist, as for movement verbs whichtend to subcategorize prepositions associated with lo-cation and motion).
In the absence of subcategoriza-tion frame information, the conditioning is placed onthe lexeme, which also controls prepositional phrasescorresponding to adjuncts.
Lastly, the semantic classof the nucleus under consideration may also play a rolein the selection of the preposition, and is thus retainedin our model.Assumption 4: the POS category of a nucleus de-pends only on its semantic class.This assumption reflects the fact that our lexical re-sources assign semantic classes from disjoint sets fornouns, adjectives and adverbs (except for the TOPclass, identical for adjectives and adverbs).
This as-sumption leads to:fl4J=AK/ffiIH/)F!= =4/;>ff5J=AK/ffiF (10)Since any dependence on    and 5 /  is lost, this fac-tor has no impact on the choice of the most probableattachment for fi3  .
However, it is important to notethat this assumption relies on the specific semantic re-source we have at our disposal, and could be replaced,in other situations, with a 1st-order Markov assump-tion.Assumption 5: the gender and number of a nucleusdepend on its POS category, the POS category of itsregent, and the gender and number of its regent.In French, the language under study, gender andnumber agreements take place between the subjectand the verb, and between adjectives, or past par-ticiples, and the noun they modify/qualify.
All, andonly, these dependencies are captured in assumption 5which leads to:fffiMN/ffiJ=AK/IH/SF!< <5/9>fffiMN/ffiJ=AK43A)J=AK944MNL (11)Assumption 6: the lexeme of a nucleus depends onlyon the POS category and the semantic class of the nu-cleus itself, the lexeme, POS category and semanticclass of its regent lexeme, and the lexeme and POScategory of its closest preceding sibling.This assumption allows us to take bigram frequenciesfor lexemes into account, as well as the dependenciesa given lexeme may have on its closest sibling.
In fact,it accounts for more than just bigram frequencies sinceit leads to:ffPQR/ffi MN/SJ=AK/IH/)F!< <5/9!flPQ1R/ffi J=AK/SF!)J=AK99;PQ1R	)SFT )J=AK9=VU;PQ1R	=V (12)Assumptions 1 to 6 lead to a set of probabilitieswhich, except for the last one, can be confidently es-timated from training data.
However, we still need tosimplify equation (12) if we want to derive practicalestimations of lexical affinities.
This is the aim of thefollowing assumption.Assumption 7:    and 5 /  are independent given3.Let us first see with an example what this assumptionamounts to.
Consider the sequence eat a fish with afork.
Assumption 7 says that given with a fork, eatand a fish are independent, that is, once we knowwith a fork, the additional observation of a fish doesn?tchange our expectation of observing eat as well, andvice-versa.
This does not entail that with a fork and eatare independent given a fish, nor that a fish and with afork are independent given eat, this last dependencebeing the one we try to account for.
However, this in-dependence assumption is violated as soon as nucleus4/ brings more or different constraints on the distri-bution of nucleus    than nucleus 3  does, i.e.
whenwith a fork imposes constraints on the possible formsthe verb of nucleus    (eat in our example) can take,and so does a fish.
With assumption 7, we claim thatthe constraints imposed by with a fork suffice to deter-mine eat, and that a fish brings no additional informa-tion.Assumption 7 allows us to rewrite equation (12) as:flPQ1R/ffiMON/)J=AK/IH/)F!< <5/9>"?CW-@'?CBEfl4F(XffiPQ1R	9)J=AK9flPQ1R/ffiJ=AK/SF:ffPQR/ffiJ<UK/)F!SJ<UK9;9PQR	S)FX:ffPQR/ffiJ<UK9<VY;PQR<V (13)4 Comparison with other modelsIt is interesting to compare the proposed models toothers previously studied.
The probabilistic model de-scribed in (Lauer and Dras, 1994), addresses the prob-lem of parsing English nominal compounds.
A com-parison with this model is of interest to us since thesequences we are interested in contain both verbal andnominal phrases in French.
A second model relevantto our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learn-ing for PP attachment resolution in VERB NOUN PPsequences.
Lastly, the third model, even though usedin a supervised setting, addresses the more complexproblem of probabilistic dependency parsing on com-plete sentences 2.In the model proposed in (Lauer and Dras, 1994),that we will refer to as model L, the quantity denotedas Z[5\ /^] \ 1ffi _`bac` ] \ is the same as the quan-tity defined by our equation (8).
The quantity fl4d  inmodel L is the same as our quantity fl78fi / 9 .
Thereis no equivalent for probabilities involved in equations(9) to (11) in model L, since there is no need for themin analysing English nominal compounds.
Lastly, ourprobability to generate PQ1R / depends only on F inmodel L (the dependency on the POS category is ob-vious since only nouns are considered).
For the rest,i.e.
the way these core quantities are combined to pro-duce a probability for a parse as well as the decisionrule (selection of the most probable parse), there is nodifference between the two models.
We thus can viewour model as a generalization of model L since we canhandle PP attachment and take into account indirectindependencies.The model proposed in (Ratnaparkhi, 1998) is sim-ilar to a version of our model based solely on equa-tion (9), with no semantic information.
This is not sur-prising since the goal of this work is to disambiguatebetween prepositional attachment to the noun or to theverb in V N P sequences.
In fact, by adding to the setof prepositions an empty preposition, e , the counts ofwhich are estimated from unsafe configurations (that isJ5fgQ1HUh jilkmSnkJ4f'QHUhUIHUQ;&oJ5fgQ1HUhUpe ), equa-tion (9) captures both the contribution from the ran-dom variable used in (Ratnaparkhi, 1998) to denote thepresence or absence of any preposition that is unam-biguously attached to the noun or the verb in question,and the contribution from the conditional probabilitythat a particular preposition will occur as unambigu-ous attachment to the verb or to the noun.
We presentbelow the results we obtained with this model.From the models proposed in (Eisner, 1996), we re-tain only the model referred to as model C in this work,since the best results were obtained with it.
Model Cdoes not make use of semantic information, nor doesit rely on nuclei.
So the sequence with a fork, whichcorresponds to only one nucleus is treated as a threeword sequence in model C. Apart from this difference,model C directly relies on a combination of equations(10) and (12), namely conditioning by PQ1R	 , J=AK9 andJ=AK9=V , both the probability of generating J=AK / and theone of generating PQ1R / .
Thus, model C uses a reducedversion of equation (12) and an extended version of2Other models, as (Collins and Brooks, 1995; Merlo etal., 1998) for PP-attachment resolution, or (Collins, 1997;Samuelsson, 2000) for probabilistic parsing, are somewhatrelated, but their supervised nature makes any direct com-parison impossible.equation (10).
This extension could be used in our casetoo, but, since the input to our processing chain con-sists of tagged words (unless the input of the stochasticdependency parser of (Eisner, 1996)), we do not thinkit necessary.Furthermore, by marginalizing the counts for the es-timates of our general model, we can derive the proba-bilities used in other models.
We thus view our modelas a generalization of the previous ones.5 Estimation of probabilitiesWe followed a maximum likelihood approach to esti-mate the different probabilities our model relies on, bydirectly computing relative frequencies from our train-ing data.
We then used Laplace smoothing to smooththe obtained probabilities and deal with unobservedevents.As mentioned before, we focus on safe configu-rations to extract counts for probability estimation,which implies that, except for particular configurationsinvolving adverbs, we use only the first nuclei of thechains we arrived at.
In most cases, only the first twonuclei of each chain are not ambiguous with respectto attachment.
However, since equation (12) relies on4/ in addition to    , we consider the first three nucleiof each chain (but we skip adverbs since their attach-ment quite often obeys precise and simple rules), buttreat the third nucleus as being ambiguous with respectto which nucleus it should be attached to, the two pos-sibilities being a priori equi-probable.
Thus, from thesequence:[implante?e, VERB] (a)de?partement, NOUN, Masc-Sg, PREP = dans(b)He?rault, NOUN, Masc-Sg, PREP = de(c)(located in the county of He?rault) (En.
)we increment the counts between nuclei (a) and (b) by1, then consider that nucleus (c) is attached to nucleus(a) and increment the respective counts (in particularthe counts associated with equation 12) by 0.5, andfinally consider that nucleus (c) is attached to nucleus(b) (which is wrong in this case) and increment thecorresponding counts by 0.5.6 ExperimentsWe made two series of experiments, the first one toassess whether relying on a subset of our training cor-pus to derive probability estimates was a good strategy,and the second one to assess the different informationsources and probabilities our general model is basedon.
For all our experiments, we used articles fromthe French newspaper Le Monde consisting of 300000sentences, split into training and test data.6.1 Accurate vs. less accurate informationWe conducted a first experiment to check whether theaccurate information extracted from safe chains wassufficient to estimate probabilities.
We focused, forthis purpose, on the task of preposition attachment on200 VERB NP PP sequences randomly extracted andmanually annotated.
Furthermore, we restricted our-selves to a reduced version of the model, based on areduced version of equation (9), so as to have a com-parison point with previous models for PP-attachment.In addition to the accurate information, we used a win-dowing approach in order to extract less accurate infor-mation and assess the estimates derived from accurateinformation only.
Each time a preposition is encoun-tered with a verb or a noun in a window of k (k=3 inour experiment) words, the corresponding counts areincremented.The French lexicons we used for tagging, lemma-tization and chunking contain subcategorization infor-mation for verbs and nouns.
This information was en-coded by several linguists over several years.
Here?sfor example two entries, one for a verb and one for anoun, containing subcategorization information:que?ter - en faveur de, pourto raise funds - in favor of, forconstance - dans, en, de- constancy - in, ofSubcategorization frames only contain part of the in-formation we try to acquire from our training data,since they are designed to capture possible arguments,and not adjuncts, of a verb or a noun.
In our approach,like in other ones, we do not make such a distinctionand try to learn parameters for attaching prepositionalphrases independently of their status, adjuncts or ar-guments.
We used the following decision rule to testa method solely based on subcategorization informa-tion:if the noun subcategorizes the preposition,then attachment to the nounelse if the verb subcategorizes the preposition,then attachment to the verbelse attachment according to the default ruleand two default rules, one for attachment to nouns, theother to verbs, in order to which of these two alterna-tives is the best.
Furthermore, since subcategorizationframes aim at capturing information for specific prepo-sitional phrases (namely the ones that might consti-tute arguments of a given word), we also evaluated theabove decision rule on a subset of our test examples inwhich either the noun or the verb subcategorizes thepreposition.
The results we obtained are summarizedin table 1.Precisiondefault: noun 0.68default: verb 0.56subset 0.75Table 1: Using subcategorization framesWe then mixed the accurate and less accurate informa-tion with a weighting factor q to estimate the proba-bility we are interested in, and let q vary from 0 to1 in order to see what are the respective impacts ofaccurate and less accurate information.
By using JU /(resp.
J=r / ) to denote the number of times IH / occurswith PQ1R	)SJ=AK9  in accurate (resp.
less accurate) con-figurations, and by using J2 to denote the number ofoccurrences of PQR)SJ<UK9  , the estimation we used issummarized in the following formula:fl-IH/ffi J=AK  9PQ1R>JU/oqJ<r/oJ)oN(GLHUQ9 (14)where N(GLHUQ9  is the number of different prepositionsintroduced by our smoothing procedure.
The resultsobtained are summarized in table 2, where an incre-ment step of 0.2 is used.q 0 0.2 0.4 0.6 0.8 1precision 0.83 0.85 0.83 0.81 0.8 0.78Table 2: Influence of qThese results first show that the accurate information issufficient to derive good estimates.
Furthermore, dis-counting part of the less accurate information seems tobe essential, since the worst results are obtained whenq .
We can also notice that the best results arewell above the baseline obtained by relying only oninformation present in our lexicon, thus justifying amachine learning approach to the problem of PP at-tachment resolution.
Lastly, the results we obtainedare similar to the ones obtained by different authorson a similar task, as (Ratnaparkhi, 1998; Hindle andRooth, 1993; Brill and Resnik, 1994) for example.6.2 Evaluation of our general modelThe model described in Section 3 was tested against900 manually annotated sequences of nuclei from thenewspaper ?Le Monde?, randomly selected from aportion of the corpus which was held out from training.The average length of sequences was of 3.33 nuclei.The trivial method consisting in linking every nucleusto the preceding one achieves an accuracy of 72.08%.The proposed model was used to assign probabil-ity estimates to dependency links between nuclei inour own implementation of the parser described in(Eisner, 1996).
The latter is a ?bare-bones?
depen-dency parser which operates in a way very similar tothe CKY parser for context-free grammars, in whichthe notion of a subtree is replaced by that of a span.A span consists of two or more adjacent nuclei to-gether with the dependency links among them.
Nocycles, multiple parents, or crossing dependencies areallowed, and each nucleus not on the edge of the spanmust have a parent (i.e.
: a regent) within the span it-self.
The parser proceeds by creating spans of increas-ing size by combining together smaller spans.
Spansare combined using the ?covered concatenation?
oper-ator, which connects two spans sharing a nucleus andpossibly adds a dependency link between the leftmostand the rightmost nucleus, or vice-versa.
The proba-bility of a span is the product of the probabilities of thedependency links it contains.
A span is pruned fromthe parse table every time that there is another spancovering the same nuclei and having the same signa-ture but a higher probability.
The signature of a spanconsists of three things:s A flag indicating whether the span is minimal ornot.
A span is minimal if it is not the simple con-catenation of other legal spans;s A flag indicating whether the leftmost nucleus inthe span already has a regent within the span;s A flag indicating whether the rightmost nucleusin the span already has a regent within the span.Two spans covering the same nuclei and with thesame signiture are interchangeable in terms of thecomplete parses they can appear in, and so the onewith the lower probability can be dropped, assumingthat we are only interested in the analysis having theoverall highest probability.
For more details concern-ing the parser, see (Eisner, 1996).A number of tests using different variants of theproposed models were done.
For some of thosetests, we decided to make use of the subcategoriza-tion frame information contained in our lexicon, byextending Laplace smoothing for the probability in-volved in equation (9) by considering Dirichlet priorsover multinomial distributions of the observed data.We use three different variables to describe the dif-ferent experiments we made: se, being 1 or 0 depend-ing on whether or not we used semantic information,sb, which indicates the equivalent sample size for pri-ors that we used in our smoothing procedure for equa-tion (9) (when \1h   , the subcategorization informa-tion contained in our lexicon is not used), Kj, whichis 1 if the variables associated with the closest sisterare used in equation (12), and 0 if not.
The resultsobtained with the different experiments we conductedwere evaluated in terms of accurracy of attachmentsagainst the manually annotated reference.
We did nottake into account the attachment of the second nucleusin a chain to the first one (since this attachment is ob-vious).
Results are summarized in the following table:Exp.
name se sb kj Accuracybase - - - 0.72exp1 0 1 0 0.662exp2 0 1 1 0.701exp3 0 100 1 0.706exp4 0 200 1 0.705exp5 1 1 1 0.731exp6 1 100 1 0.731exp7 1 200 1 0.735exp8 1 500 1 0.737exp9 1 1000 1 0.735Table 3: General results7 DiscussionThere are two main conclusions we can draw from thepreceding results.
The first one is that the results aredisappointing, in so far as we were not able to reallyoutperform our baseline.
The second one is that thebest results are achieved with the complete model in-tegrating subcategorization information.With respect to our model, the difference betweenexperiment 1 and experiment 2 shows that the closestsister brings valuable information to establish the bestparse of the chain of nuclei.
Even though this infor-mation was derived from ambiguous configurations,the extraction heuristics we used does capture ac-tual dependencies, which validates our assumptions 6and 7.
The integration of subcategorization frame in-formation in experiments 3 and 4 does not improvethe results, indicating that most of information is al-ready carried in the corresponding version of the gen-eral model by bigram lexical statistics.
Furthermore,the results obtained with sucategorization informationonly for parsing V N P sequences do not comparewell with an approach solely based on bigram statis-tics, thus validating the hypothesis behind most workin probabilistic parsing that world knowledge can beapproximated, up to a certain extent, by bigram statis-tics.The main jump in performance is achieved with theuse of semantic classes.
All the experiments involv-ing semantic classes yield results over the baseline,thus indicating the well-foundedness of models mak-ing use of them.
Even though our semantic resource isincomplete (out of 70000 different tokens our corpuscomprises3, only 20000 have an entry in our seman-tic lexicon), its coverage is still sufficient to constrainword distributions and partly solve the data sparsenessproblem.
The results obtained in previous works re-lying on semantic classes are above ours (around 0.823This huge number of tokens can be explained by the factthat the lexicon used for tokenization and tagging integratesmany multi-word expressions which are not part of the se-mantic lexiconfor (Brill and Resnik, 1994) and 0.77 for (Lauer andDras, 1994)), but a direct comparison is difficult inas-much as only three-word sequences (V N P, for (Brilland Resnik, 1994) and N N N for (Lauer and Dras,1994)) were used for evaluation in those works, andthe language studied is English.
However, it may wellbe the case that the semantic resource we use does notcompare well, in terms of coverage and homogeneity,with WordNet, the semantic resource usually used.Several choices we made in the course of develop-ing our model and estimating its parameters need nowto be more carefully assessed in light of these first re-sults.
First of all, our choice to stick with (almost)accurate information, if it leads to good results for theestimation of the probability of generating the prepo-sition of a nucleus given its parent nucleus, may welllead us to rely too often on the smoothing parametersonly when estimating other probabilities.
This maywell be the case for the probability in (12) where bi-gram statistics extracted with a windowing approachmay prove to be more suited to the task.
Further-more, the Laplace smoothing, even though appealingfrom a theoretical point of view since it can be for-malized as assuming a prior over our distributions,may not be fully adequate in the case where the de-nominator is always low compared to the normalizingcontraint, a situation we encounter for equation (12).This may result in over-smoothing and thus preventour model from accurately discriminating between al-ternate parses.
Lastly, (Lauer and Dras, 1994) uses aprior over the graphs defined by parse trees to scorethe different parses.
We have assumed a uniform priorover graphs, but the results obtained with our baselineclearly indicate that we should weigh them differently.8 ConclusionWe have presented a general model for PP attachmentresolution and NP analysis in French, which gener-alizes previously proposed models.
We have shownhow to integrate several different information sourcesin our model, and how we could use it in an incremen-tal way, starting with simple versions to more complexand accurate ones.
We have also presented a series ofexperiments conducted on a corpus of newspaper ar-ticles, and tried to assess the various components ofthe model, as well as the different information sourcesused.
Our results show that the complete model, mak-ing use of all the available information yields the bestresults.
However, these results are still low, and westill need to precisely identify how to improve them.ReferencesS.
Ait-Mokhtar and J.-P. Chanod.
1997.
Incremen-tal Finite-State Parsing.
Proceedings of the Inter-national Conference on Applied Natural LanguageProcessingE.
Brill and P. Resnik.
1994.
A Rule Based Ap-proach to Prepositional Phrase Attachment Disam-biguation.
Proceedings of the 15th InternationalConference on Computational Linguistics.M.
Collins and J. Brooks.
1995.
Prepositional PhraseAttachment through a Backed-Off Model.
Proceed-ings of the Third Workshop on Very Large Corpora.M.
Collins.
1997.
Three Generative, LexicalisedModels for Statistical Parsing.
Proceedings of the35th Annual Meeting of the Association for Compu-tational Linguistics.J.
Eisner.
1996.
Three New Probabilistic Models forDependency Parsing: An Exploration.
Proceedingsof the 16th International Conference on Computa-tional Linguistics.D.
Hindle and M. Rooth.
1993.
Structural Ambiguityand Lexical Relations.
Journal of the Associationfor Computational Linguistics.
19(1).M.
Lauer and M. Dras.
1994.
A Probabilistic Modelof Compound nouns.
Proceedings of the SeventhJoint Australian Conference on Artificial Intelli-gence.P.
Merlo, M.W.
Croker and C. Berthouzoz.
1997.Attaching Multiple Prepositional Phrases: Gener-alized Backed-off Estimation.
Proceedings of theSecond Conference on Empirical Methods in Natu-ral Language Processing.A.
Ratnaparkhi.
1998.
Statistical Models for Un-supervised Prepositional Phrase Attachment.
Pro-ceedings of the joint COLING-ACL conference.C.
Samuelsson.
2000.
A Statistical Theory ofDe-pendency Syntax.
Proceedings of the 18th Inter-national Conference on Computational Linguistics.A.
Yeh and M. Vilain.
1998.
Some Properties ofPreposition and Subordinate Conjunction Attach-ment.
Proceedings of the 17th International Con-ference on Computational Linguistics and 36th An-nual Meeting of the Association for ComputationalLinguistics?.
