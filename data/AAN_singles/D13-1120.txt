Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1201?1212,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsUbertagging: Joint segmentation and supertagging for EnglishRebecca DridanInstitutt for InformatikkUniversitetet i Oslordridan@ifi.uio.noAbstractA precise syntacto-semantic analysis of En-glish requires a large detailed lexicon with thepossibility of treating multiple tokens as a sin-gle meaning-bearing unit, a word-with-spaces.However parsing with such a lexicon, as in-cluded in the English Resource Grammar, canbe very slow.
We show that we can applysupertagging techniques over an ambiguoustoken lattice without resorting to previouslyused heuristics, a process we call ubertagging.Our model achieves an ubertagging accuracythat can lead to a four to eight fold speed upwhile improving parser accuracy.1 Introduction and MotivationOver the last decade or so, supertagging has becomea standard method for increasing parser efficiencyfor heavily lexicalised grammar formalisms such asLTAG (Bangalore and Joshi, 1999), CCG (Clark andCurran, 2007) and HPSG (Matsuzaki et al 2007).In each of these systems, fine-grained lexical cate-gories, known as supertags, are used to prune theparser search space prior to full syntactic parsing,leading to faster parsing at the risk of removing nec-essary lexical items.
Various methods are used toconfigure the degree of pruning in order to balancethis trade-off.The English Resource Grammar (ERG;Flickinger (2000)) is a large hand-written HPSG-based grammar of English that produces fine-grained syntacto-semantic analyses.
Given the highlevel of lexical ambiguity in its lexicon, parsingwith the ERG should therefore also benefit fromsupertagging, but while various attempts haveshown possibilities (Blunsom, 2007; Dridan etal., 2008; Dridan, 2009), supertagging is still nota standard element in the ERG parsing pipeline.There are two main reasons for this.
The first isthat the ERG lexicon does not assign simple atomiccategories to words, but instead builds complexstructured signs from information about lemmas andlexical rules, and hence the shape and integrationof the supertags is not straightforward.
Bangaloreand Joshi (2010) define a supertag as a primitivestructure that contains all the information abouta lexical item, including argument structure, andwhere the arguments should be found.
Withinthe ERG, that information is not all contained inthe lexicon, but comes from different places.
Thechoice, therefore, of what information may bepredicted prior to parsing and how it should beintegrated into parsing is an open question.The second reason that supertagging is not stan-dard with ERG processing is one that is rarely con-sidered when processing English, namely ambigu-ous segmentation.
In most mainstream English pars-ing, the segmentation of parser input into tokens thatwill become the leaves of the parse tree is consid-ered a fixed, unambiguous process.
While recentwork (Dridan and Oepen, 2012) has shown that pro-ducing even these tokens is not a solved problem,the issue we focus on here is the ambiguous map-ping from these tokens to meaning-bearing units thatwe might call words.
Within the ERG lexicon aremany multi-token lexical entries that are sometimesreferred to as words-with-spaces.
These multi-tokenentries are added to the lexicon where the grammar-ian finds that the semantics of a fixed expression isnon-compositional and has the distributional prop-erties of other single word entries.
Some examplesinclude an adverb-like all of a sudden, a preposition-like for example and an adjective-like over and donewith.
Each of these entries create an segmentationambiguity between treating the whole expression asa single unit, or allowing analyses comprising en-1201tries triggered by the individual tokens.
Previous su-pertagging research using the ERG has either usedthe gold standard tokenisation, hence making thetask artificially easier, or else tagged the individualtokens, using various heuristics to apply multi-tokentags to single tokens.
Neither approach has beenwholly satisfactory.In this work we avoid the heuristic approachesand learn a sequential classification model that cansimultaneously determine the most likely segmen-tation and supertag sequences, a process we dububertagging.
We also experiment with more fine-grained tag sets than have been previously used, andfind that it is possible to achieve a level of ubertag-ging accuracy that can improve both parser speedand accuracy for a precise semantic parser.2 Previous WorkAs stated above, supertagging has become a stan-dard tool for particular parsing paradigms, but thedefinitions of a supertag, the methods used to learnthem, and the way they are used in parsing variesacross formalisms.
The original supertags were 300LTAG elementary trees, predicted using a fairly sim-ple trigram tagger that provided a configurable num-ber of tags per token, since the tagger was not ac-curate enough to make assigning a single tree vi-able parser input (Bangalore and Joshi, 1999).
TheC&C CCG parser uses a more complex MaximumEntropy tagger to assign tags from a set of 425 CCGlexical categories (Clark and Curran, 2007).
Theyalso found it necessary to supply more than one tagper token, and hence assign all tags that have a prob-ability within a percentage ?
of the most likely tagfor each token.
Their standard parser configura-tion uses a very restrictive ?
value initially, relax-ing it when no parse can be found.
Matsuzaki et al(2007) use a supertagger similar to the C&C taggeralongside a CFG filter to improve the speed of theirHPSG parser, feeding sequences of single tags to theparser until a parse is possible.
As in the ERG, cate-gory and inflectional information are separate in theautomatically-extracted ENJU grammar: their su-pertag set consists of 1361 tags constructed by com-bining lexical categories and lexical rules.
Figure 1shows examples of supertags from these three tagsets, all describing the simple transitive use of lends.SNP0?
VPVlendsNP1?
(a) LTAG(S[dcl]\NP)/NP(b) CCG[NP.nom<V.bse>NP.acc]-singular3rd verb rule(c) ENJU HPSGFigure 1: Examples of supertags from LTAG, CCGand ENJU HPSG, for the word lends.The ALPINO system for parsing Dutch is theclosest in spirit to our ERG parsing setup, sinceit also uses a hand-written HPSG-based grammar,including multi-token entries in its lexicon.
Prinsand van Noord (2003) use a trigram HMM taggerto calculate the likelihood of up to 2392 supertags,and discard those that are not within ?
of the mostlikely tag.
For their multi-token entries, they as-sign a constructed category to each token, so thatinstead of assigning preposition to the expres-sion met betrekking tot (?with respect to?
), theyuse (1,preposition), (2,preposition),(3,preposition).
Without these constructedcategories, they would only have 1365 supertags.Most previous supertagging attempts with theERG have used the grammar?s lexical types, whichdescribe the coarse-grained part of speech, and thesubcategorisation of a word, but not the inflection.Hence both lends and lent have a possible lexicaltype v np* pp* to le, which indicates a verb,with optional noun phrase and prepositional phrasearguments, where the preposition has the form to.The number of lexical types changes as the gram-mar grows, and is currently just over 1000.
Dridan(2009) and Fares (2013) experimented with other tagtypes, but both found lexical types to be the opti-mal balance between predictability and efficiency.Both used a multi-tagging approach dubbed selec-tive tagging to integrate the supertags into the parser.This involved only applying the supertag filter whenthe tag probability is above a configurable threshold,and not pruning otherwise.For multi-token entries, both Blunsom (2007) and1202adverb adverb adverbadverb ditto ditto1,adverb 2,adverb 3,adverball in allFigure 2: Options for tagging parts of the multi-token adverb all in all separately.Dridan (2009) assigned separate tags to each token,with Blunsom (2007) assigning a special ditto tagall but the initial token of a multi-token entry, whileDridan (2009) just assigned the same tag to each to-ken (leading to example in the expression for exam-ple receiving p np i le, a preposition-type cate-gory).
Both of these solutions (demonstrated in Fig-ure 2), as well as that of Prins and van Noord (2003),in some ways defeat one of the purposes of treatingthese expressions as fixed units.
The grammarian,by assigning the same category to, for example, allof a sudden and suddenly, is declaring that these twoexpressions have the same distributional properties,the properties that a sequential classifier is trying toexploit.
Separating the tokens loses that informa-tion, and introduces extra noise into the sequencemodel.Ytrest?l (2012) and Fares (2013) treat the multi-entry tokens as single expressions for tagging, butwith no ambiguity.
Ytrest?l (2012) manages thisby using gold standard tokenisation, which is, as hestates, the standard practice for statistical parsing,but is an artificially simplified setup.
Fares (2013) isthe only work we know about that has tried to predictthe final segmentation that the ERG produces.
Wecompare segmentation accuracy between our jointmodel and his stand-alone tokeniser in Section 6.Looking at other instances of joint segmentationand tagging leads to work in non-whitespace sepa-rated languages such as Chinese (Zhang and Clark,2010) and Japanese (Kudo et al 2004).
While at ahigh level, this work is solving the same problem,the shape of the problems are quite different froma data point of view.
Regular joint morphologicalanalysis and segmentation has much greater ambi-guity in terms of possible segmentations but, in mostcases, less ambiguity in terms of labelling than oursituation.
This also holds for other lemmatisationand morphological research, such as Toutanova andCherry (2009).
While we drew inspiration from thisaj - i leForeignv nger-tr dlrv prp olrv np*-pp* to lelendingv pst olrv - unacc leincreasedw period plrav - s-vp-po leas well.p vp i leasw period plrav - dg-v lewell.Figure 3: A selection from the 70 lexitems instanti-ated for Foreign lending increased as well.related area, as well as from the speech recognitionfield, differences in the relative frequency of obser-vations and labels, as well as in segmentation ambi-guity mean that conclusions found in these areas didnot always hold true in our problem space.3 The ParserThe parsing environment we work with is the PETparser (Callmeier, 2000), a unification-based chartparser that has been engineered for efficiency withprecision grammars, and incorporates subsumption-based ambiguity packing (Oepen and Carroll, 2000)and statistical model driven selective unpacking(Zhang et al 2007).
Parsing in PET is divided intwo stages.
The first stage, lexical parsing, coverseverything from tokenising the raw input string topopulating the base of the parse chart with the ap-propriate lexical items, ready for the second ?
syn-tactic parsing ?
stage.
In this work, we embed ourubertagging model between the two stages.
By thispoint, the input has been segmented into what wecall internal tokens, which broadly meanssplitting at whitespace and hyphens, and making?s a separate token.
These tokens are subject to amorphological analysis component which proposespossible inflectional and derivational rules based onword form, and then are used in retrieving possiblelexical entries from the lexicon.
The results of ap-plying the appropriate lexical rules, plus affixationrules triggered by punctuation, to the lexical entriesform a lexical item object, that for this work we duba lexitem.Figure 3 shows some examples of lexitemsinstantiated after the lexical parsing stage when1203analysing Foreign lending increased as well.
Thepre-terminal labels on these subtrees are the lexicaltypes that have previously been used as supertagsfor the ERG.
For uninflected words, with no punctu-ation affixed, the lexical type is the only element inthe lexitem, other than the word form (e.g.
Foreign,as).
In this example, we also see lexitems with in-flectional rules (v prp olr, v pst olr), deriva-tional rules (v nger-tr dlr) and punctuation af-fixation rules (w period plr).These lexitems are put in to a chart, forming alexical lattice, and it is over this lattice that we applyour ubertagging model, removing unlikely lexitemsbefore they are seen by the syntactic parsing stage.4 The DataThe primary data sets we use in these experimentsare from the 1.0 version of DeepBank (Flickinger etal., 2012), an HPSG annotation of the Wall StreetJournal text used for the Penn Treebank (PTB; Mar-cus et al(1993)).
The current version has gold stan-dard annotations for approximately 85% of the first22 sections.
We follow the recommendations of theDeepBank developers in using Sections 00?19 fortraining, Section 20 (WSJ20) for development andSection 21 (WSJ21) as test data.In addition, we use two further sources of trainingdata: the training portions of the LinGO RedwoodsTreebank (Oepen et al 2004), a steadily growingcollection of gold standard HPSG annotations in avariety of domains; and the Wall Street Journal sec-tion of the North American News Corpus (NANC),which has been parsed, but not manually annotated.This builds on observations by Prins and van Noord(2003), Dridan (2009) and Ytrest?l (2012) that evenuncorrected parser output makes very good train-ing data for a supertagger, since the constraints inthe parser lead to viable, if not entirely correct se-quences.
This allows us to use much larger trainingsets than would be possible if we required manuallyannotated data.In final testing, we also include two further datasets to observe how domain affects the contributionof the ubertagging.
These are both from the testportion of the Redwoods Treebank: CatB, an es-say about open-source software;1 and WeScience13,1http://catb.org/esr/writings/text from Wikipedia articles about Natural LanguageProcessing from the WeScience project (Ytrest?l etal., 2009).
Table 1 summarises the vital statistics ofthe data we use.With the focus on multi-token lexitems, it is in-structive to see just how frequent they are.
In termsof type frequency, almost 10% of the approximately38500 lexical entries in the current ERG lexiconhave more than one token in their canonical form.2However, while this is a significant percentage of thelexicon, they do not account for the same percentageof tokens during parsing.
An analysis of WSJ00:19shows that approximately one third of the sentenceshad at least one multi-token lexitem in the unprunedlexical lattice, and in just under half of those, thegold standard analysis included a multi-word entry.That gives the multi-token lexitems the awkwardproperty of being rare enough to be difficult for astatistical classifier to accurately detect (just under1% of the leaves of gold parse trees contain multi-ple tokens), but too frequent to ignore.
In addition,since these multi-token expressions have often beendistinguished because they are non-compositional,failing to detect the multi-word usage can lead toa disproportionately adverse effect on the semanticanalysis of the text.5 Ubertagging ModelOur ubertagging model is very similar to a standardtrigram Hidden Markov Model (HMM), except thatthe states are not all of the same length.
Our statesare based on the lexitems in the lexical lattice pro-duced by the lexical parsing stage of PET, and assuch, can be partially overlapping.
We formalise thisbe defining each state by its start position, end po-sition, and tag.
This turns out to make our modelequivalent to a type of Hidden semi-Markov Modelcalled a segmental HMM in Murphy (2002).
In asegmental HMM, the states are segments with a tag(t) and a length in frames (l).
In our setup, theframes are the ERG internal tokens and the segmentsare the lexitems, which are the potential candidatescathedral-bazaar/ by Eric S. Raymond2While the parser has mechanisms for handling words un-known to the lexicon, with the current grammar these mecha-nisms will never propose a multi-token lexitem, and so only themulti-token entries explicitly in the lexicon will be recognisedas such.1204LexitemsData Set Source Use Gold?
Trees All M-TWSJ00:19 DeepBank 1.0 ?00?19 train yes 33783 661451 6309Redwoods Redwoods Treebank train yes 39478 432873 6568NANC LDC2008T15 train no 2185323 42376523 399936WSJ20 DeepBank 1.0 ?20 dev yes 1721 34063 312WSJ21 DeepBank 1.0 ?21 test yes 1414 27515 253WeScience13 Redwoods Treebank test yes 802 11844 153CatB Redwoods Treebank test yes 608 11653 115Table 1: Test, development and training data used in these experiments.
The final two columns show thetotal number of lexitems used for training (All), as well as how many of those were multi-token lexitems(M-T).to become leaves of the parse tree.
As indicatedabove, the majority of segments (over 99%) will beone frame long, but segments of up to four framesare regularly seen in the training data.A standard trigram HMM has a transition proba-bility matrix A, where the elements Aijk representthe probability P (k|ij), and an emission probabilitymatrix B whose elements Bjo record the probabili-ties P (o|j).
Given these matrices and a vector of ob-served frames, O, the posterior probabilities of eachstate at frame v are calculated as:3P (qv = qy|O) =?v(qy)?v(qy)P (O)(1)where ?v(qy) is the forward probability at frame v,given a current state qy (i.e.
the probability of theobservation up to v, given the state):?v(qy) ?
P (O0:v|qv = qy) (2)=?qx?v(qxqy) (3)?v(qxqy) = Bqyov?qw?v?1(qwqx)Aqwqxqy (4)?v(qy) is the backwards probability at frame v, givena current state qy (the probability of the observation3Since we will require per-state probabilities for integrationto the parser, we focus on the calculation of posterior probabil-ities, rather than determing the single best path.from v, given the state):?v(qy) ?
P (Ov+1:V |qv = qy) (5)=?qx?v(qxqy) (6)?v(qxqy) =?qz?v+1(qyqz)AqxqyqzBqzov+1 (7)and the probability of the full observation sequenceis equal to the forward probability at the end of thesequence, or the backwards probability at the startof the sequence:P (O) = ?V (?E?)
= ?0(?S?)
(8)In implementation, our model varies only in whatwe consider the previous or next states.
While v stillindexes frames, qv now indicates a state that endswith frame v, and we look forwards and backwardsto adjacent states, not frames, formally designated interms of l, the length of the state.
Hence, we modifyequation (4):?v(qxqy) = BqyOv?l+1:v?qw?v?l(qwqx)Aqwqxqy(9)where v?l indexes the frame before the current statestarts, and hence we are summing over all statesthat lead directly to our current state.
An equivalentmodification to equation (7) gives:?v(qxqy) =?qz?Qn?l(qz)?v+l(qyqz)AqxqyqzBqzOv+1:v+l(10)1205Type Example #TagsLTYPE v np-pp* to le 1028INFL v np-pp* to le:v pas odlr 3626FULL v np-pp* to le:v pas odlr:w period plr 21866w period plrv pas odlrv np-pp* to lerecommended.Figure 4: Possible tag types and their tag set size, with examples derived from the lexitem on the right.where Qn is the set of states that start at v + 1 (i.e.,the states immediately following the current state),and l(qz) is the length of state qz .We construct the transition and emission prob-ability matrices using relative frequencies directlyobserved from the training data, where we makethe simplifying assumption that P (qk|qiqj) ?P (t(qk)|t(qi)t(qk)).
Which is to say, while lex-items with the same tag, but different length willtrigger distinct states with distinct emission proba-bilities, they will have the same transition probabili-ties, given the same proceeding tag.4 Even with ourlarge training set, some tag trigrams are rare or un-seen.
To smooth these probabilities, we use deletedinterpolation to calculate a weighted sum of the tri-gram, bigram and unigram probabilities, since it hasbeen successfully used in effective PoS taggers likethe TnT tagger (Brants, 2000).
Future work willlook more closely at the effects of different smooth-ing methods.6 Intrinsic Ubertag EvaluationIn order to develop and tune the ubertagging model,we first looked at segmentation and tagging per-formance in isolation over the development set.We looked at three tag granularities: lexical types(LTYPE) which have previously been shown to be theoptimal granularity for supertagging with the ERG,inflected types (INFL) which encompass inflectionaland derivational rules applied to the lexical type, andthe full lexical item (FULL), which also includes af-fixation rules used for punctuation handling.
Exam-ples of each tag type are shown in Figure 4, alongwith the number of tags of each type seen in thetraining data.4Since the multi-token lexical entries are defined becausethey have the same properties as the single token variants, thereis no reason to think the length of a state should influence thetag sequence probability.Segmentation TaggingTag Type F1 Sent.
F1 Sent.FULL 99.55 94.48 93.92 42.13INFL 99.45 93.55 93.74 41.49LTYPE 99.40 93.03 93.27 38.12Table 2: Segmentation and tagging performance ofthe best path found for each model, measured persegment in terms of F1, and also as complete sen-tence accuracy.Single sequence results Table 2 shows the resultswhen considering the best path through the lattice.In terms of segmentation, our sentence accuracy iscomparable to that of the stand-alone segmentationperformance reported by Fares et al(2013) oversimilar data.5 In that work, the authors used a bi-nary CRF classifier to label points between objectsthey called micro-tokens as either SPLIT or NOS-PLIT.
The CRF classifier used a less informed in-put (since it was external to the parser), but a muchmore complex model, to produce a best single pathsentence accuracy of 94.06%.
Encouragingly, thislevel of segmentation performance was shown inlater work to produce a viable parser input (Fares,2013).Switching to the tagging results, we see that theF1 numbers are quite good for tag sets of this size.6The best tag accuracy seen for ERG LTYPE-styletags was 95.55 in Ytrest?l (2012), using gold stan-dard segmentation on a different data set.
Dridan(2009) experimented with a tag granularity similarto our INFL (letype+morph) and saw a tag ac-curacy of 91.51, but with much less training data.From other formalisms, Kummerfeld et al(2010)5Fares et al(2013) used a different section of an earlier ver-sion of DeepBank, but with the same style of annotation.6We need to measure F1 rather than tag accuracy here, sincethe number of tokens tagged will vary according to the segmen-tation.1206report a single tag accuracy of 95.91, with thesmaller CCG supertag set.
Despite the promisingtag F1 numbers however, the sentence level accu-racy still indicates a performance level unacceptablefor parser input.
Comparing between tag types, wesee that, possibly surprisingly, the more fine-grainedtags are more accurately assigned, although the dif-ferences are small.
While instinctively a larger tagset should present a more difficult problem, we findthat this is mitigated both by the sparse lexical latticeprovided by the parser, and by the extra constraintsprovided by the more informative tags.Multi-tagging results The multi-tagging methodsfrom previous supertagging work becomes morecomplicated when dealing with ambiguous tokeni-sation.
Where, in other setups, one can comparetag probabilities for all tags for a particular token,that no longer holds directly when tokens can par-tially overlap.
Since ultimately, the parser uses lex-items which encompass segmentation and tagginginformation, we decided to use a simple integrationmethod, where we remove any lexitem which ourmodel assigns a probability below a certain thresh-old (?).
The effect of the different tag granular-ities is now mediated by the relationship betweenthe states in the ubertagging lattice and the lexitemsin the parser?s lattice: for the FULL model, this isa one-to-one relationship, but states from the mod-els that use coarser-grained tags may affect multiplelexitems.
To illustrate this point, Figure 5 showssome lexitems for the token forecast,, where thereare multiple possible analyses for the comma.
AFULL tag of v cp le:v pst olr:w comma plrwill select only lexitem (b), whereas an INFL tagv cp le:v pst olr will select (b) and (c) andthe LTYPE tag v cp le picks out (a), (b) and (c).On the other hand, where there is no ambiguity ininflection or affixation, an LTYPE tag of n - mc lemay relate to only a single lexitem ((f) in this case).Since we are using an absolute, rather than rel-ative, threshold, the number needs to be tuned foreach model7 and comparisons between models canonly be made based on the effects (accuracy or prun-ing power) of the threshold.
Table 3 shows howa selection of threshold values affect the accuracy7A tag set size of 1028 will lead to higher probabilities ingeneral than a tag set size of 21866.w comma-nf plrv cp leforecast,(a)w comma-nf plrv pst olrv cp leforecast,(b)w comma plrv pst olrv cp leforecast,(c)w comma plrv pst olrv np leforecast,(d)w comma plrv pas olrv np leforecast,(e)w comma plrn - mc leforecast,(f)Figure 5: Some of the lexitems triggered by fore-cast, in Despite the gloomy forecast, profits were up.Tag LexitemsType ?
Acc.
Kept Ave.FULL 0.00001 99.71 41.6 3.34FULL 0.0001 99.44 33.1 2.66FULL 0.001 98.92 25.5 2.05FULL 0.01 97.75 19.4 1.56INFL 0.0001 99.67 37.9 3.04INFL 0.001 99.25 29.0 2.33INFL 0.01 98.21 21.6 1.73INFL 0.02 97.68 19.7 1.58LTYPE 0.0002 99.75 66.3 5.33LTYPE 0.002 99.43 55.0 4.42LTYPE 0.02 98.41 43.5 3.50LTYPE 0.05 97.54 39.4 3.17Table 3: Accuracy and ambiguity after pruning lex-items in WSJ20, at a selection of thresholds ?
foreach model.
Accuracy is measured as the percent-age of gold lexitems remaining after pruning, whileambiguity is presented both as a percentage of lex-items kept, and the average number of lexitems perinitial token still remaining.12070.9750.980.9850.990.99511  2  3  4  5  6  7  8  9AccuracyAverage lexitems per initial tokenTag accuracy versus ambiguityFULLINFLLTYPEFigure 6: Accuracy over gold lexitems versus aver-age lexitems per initial token over the developmentset, for each of the different ubertagging models.and pruning impact of our different disambiguationmodels, where the accuracy is measured in termsof percentage of gold lexitems retained.
The prun-ing effect is given both as percentage of lexitemsretained after pruning, and average number of lex-items per initial token.8 Comparison between thedifferent models can be more easily made by exam-ining Figure 6.
Here we see clearly that the LTYPEmodel provides much less pruning for any givenlevel of lexitem accuracy, while the performance ofthe other models is almost indistinguishable.Analysis The current state-of-the-art POS taggingaccuracy (using the 45 tags in the PTB) is approx-imately 97.5%.
The most restrictive ?
value wereport for each model was selected to demonstratethat level of accuracy, which we can see would leadto pruning over 80% of lexitems when using FULLtags, an average of 1.56 tags per token.
Whilethis level of accuracy has been sufficient for statisti-cal treebank parsing, previous work (Dridan, 2009)has shown that tag accuracy cannot directly predictparser performance, since errors of different typescan have very different effects.
This is hard toquantify without parsing, but we made a qualitativeanalysis at the lexitems that were incorrectly being8The average number of lexitems per token for the unre-stricted parser is 8.03, although the actual assignment is far fromuniform, with up to 70 lexitems per token seen for the very am-biguous tokens.pruned.
For all models, the most difficult lexitemsto get correct were proper nouns, particular thosethat are also used as common nouns (e.g.
Bank, Air-line, Report).
While capitalisation provides a cluehere, it is not always deterministic, particularly sincethe treebank incorporates detailed decisions regard-ing the distinction between a name and a capitalisedcommon noun that require real world knowledge,and are not necessarily always consistent.
Almosttwo thirds of the errors made by the FULL and INFLmodels are related to these decisions, but only about40% for the LTYPE model.
The other errors are pre-dominately over noun and verb type lexitems, as theopen classes, with the only difference between mod-els being that the FULL model seems marginally bet-ter at classifying verbs.
The next section describesthe end-to-end setup and results when parsing thedevelopment set.7 ParsingWith encouraging ubertagging results, we now takethe next step and evaluate the effect on end-to-endparsing.
Apart from the issue of different error typeshaving unpredictable effects, there are two otherfactors that make the isolated ubertagging resultsonly an approximate indication of parsing perfor-mance.
The first confounding factor is the statisti-cal parsing disambiguation model.
To show the ef-fect of ubertagging in a realistic configuration, weonly evaluate the first analysis that the parser returns.That means that when the unrestricted parser doesnot rank the gold analysis first, errors made by ourmodel may not be visible, because we would neversee the gold analysis in any case.
On the other hand,it is possible to improve parser accuracy by pruningincorrect lexitems that were in a top ranked, non-gold analysis.The second new factor that parser integrationbrings to the picture is the effect of resource limi-tations.
For reasons of tractability, PET is run withper sentence time and memory limits.
For treebankcreation, these limits are quite high (up to four min-utes), but for these experiments, we set the time-out to a more practical 60 seconds and the memorylimit to 2048Mb.
Without lexical pruning, this leadsto approximately 3% of sentences not receiving ananalysis.
Since the main aim of ubertagging is to in-1208Tag F1Type ?
Lexitem Bracket TimeNo Pruning 94.06 88.58 6.58FULL 0.00001 95.62 89.84 3.99FULL 0.0001 95.95 90.09 2.69FULL 0.001 95.81 89.88 1.34FULL 0.01 94.19 88.29 0.64INFL 0.0001 96.10 90.37 3.45INFL 0.001 96.14 90.33 1.78INFL 0.01 95.07 89.27 0.84INFL 0.02 94.32 88.49 0.64LTYPE 0.0002 95.37 89.63 4.73LTYPE 0.002 96.03 90.20 2.89LTYPE 0.02 95.04 89.04 1.23LTYPE 0.05 93.36 87.26 0.88Table 4: Lexitem and bracket F1over WSJ20, withaverage per sentence parsing time in seconds.crease efficiency, we would expect to regain at leastsome of these unanalysed sentences, even when alexitem needed for the gold analysis has been re-moved.Table 4 shows the parsing results at the samethreshold values used in Table 3.
Accuracy is cal-culated in terms of F1 both over lexitems, and PAR-SEVAL-style labelled brackets (Black et al 1991),while efficiency is represented by average parsingtime per sentence.
We can see here that an ubertag-ging F1 of below 98 (cf.
Table 3) leads to a dropin parser accuracy, but that an ubertagging perfor-mance of between 98 and 99 can improve parser F1while also achieving speed increases up to 8-fold.From the table we confirm that, contrary to ear-lier pipeline supertagging configurations, tags of afiner granularity than LTYPE can deliver better per-formance, both in terms of accuracy and efficiency.Again, comparing graphically in Figure 7 gives aclearer picture.
Here we have graphed labelledbracket F1 against parsing time for the full range ofthreshold values explored, with the unpruned pars-ing results indicated by a cross.From this figure, we see that the INFL model, de-spite being marginally less accurate when measuredin isolation, leads to slightly more accurate parse re-sults than the FULL model at all levels of efficiency.Looking at the same graph for different samplesof the development set (not shown) shows some86878889900  1  2  3  4  5  6  7F1Time per sentenceParser accuracy versus efficiencyFULLINFLLTYPEUnrestrictedSelected configurationFigure 7: Labelled bracket F1 versus parsing timeper sentence over the development set, for each ofthe different ubertagging models.
The cross indi-cates unpruned performance, while the circle pin-points the configuration we chose for the final testruns.variance in which threshold value gives the best F1,but the relative differences and basic curve shape re-mains the same.
From these different views, usingthe guideline of maximum efficiency without harm-ing accuracy we selected our final configuration: theINFL model with a threshold value of 0.001 (markedwith a circle in Figure 7).
On the development set,this configuration leads to a 1.75 point improvementin F1 in 27% of the parsing time.8 Final ResultsTable 5 shows the results obtained when parsing us-ing the configuration selected on the developmentset, over our three test sets.
The first, WSJ21 is fromthe same domain as the development set.
Here wesee that the effect over the WSJ21 set fairly closelymirrored that of the development set, with an F1 in-crease of 1.81 in 29% of the parsing time.The Wikipedia domain of our WeScience13 testset, while very different to the newswire domain ofthe development set could still be considered in do-main for the parsing and ubertagging models, sincethere is Wikipedia data in the training sets.
Withan average sentence length of 15.18 (compared to18.86 in WSJ21), the baseline parsing time is fasterthan for WSJ21, and the speedup is not quite as large1209Baseline PrunedData Set F1 Time F1 TimeWSJ21 88.12 6.06 89.93 1.77WeScience13 86.25 4.09 87.14 1.48CatB 86.31 5.00 87.11 1.78Table 5: Parsing accuracy in terms of labelledbracket F1 and average time per sentence when pars-ing the test sets, without pruning, and then with lex-ical pruning using the INFL model with a thresholdof 0.001.but still welcome, at 36% of the baseline time.
Theincrease is accuracy is likewise smaller (due to lessissues with resource exhaustion in the baseline), butas our primary goal is to not harm accuracy, the re-sults are pleasing.The CatB test set is the standard out-of-domaintest for the parser, and is also out of domain forthe ubertagging model.
The average sentence lengthis not much below that of WSJ21, at 18.61, butthe baseline parsing speed is still noticeably faster,which appears to be a reflection of greater structuralambiguity in the newswire text.
We still achieve a re-duction in parsing time to 35% of the baseline, againwith a small improvement in accuracy.The across-the-board performance improvementon all our test sets suggests that, while tuning thepruning threshold could help, it is a robust parame-ter that can provide good performance across a va-riety of domains.
This means that we finally have arobust supertagging setup for use with the ERG thatdoesn?t require heuristic shortcuts and can be reli-ably applied in general parsing.9 Conclusions and OutlookIn this work we have demonstrated a lexical disam-biguation process dubbed ubertagging that can as-sign fine-grained supertags over an ambiguous to-ken lattice, a setup previously ignored for English.
Itis the first completely integrated supertagging setupfor use with the English Resource Grammar, whichavoids the previously necessary heuristics for deal-ing with ambiguous tokenisation, and can be ro-bustly configured for improved performance withoutloss of accuracy.
Indeed, by learning a joint segmen-tation and supertagging model, we have been ableto achieve usefully high tagging accuracies for veryfine-grained tags, which leads to potential parserspeedups of between 4 and 8 fold.Analysis of the tagging errors still being madehave suggested some possibly avoidable inconsis-tencies in the grammar and treebank, which havebeen fed back to the developers, hopefully leadingto even better results in the future.In future work, we will investigate more advancedsmoothing methods to try and boost the ubertaggingaccuracy.
We also intend to more fully explore thedomain adaptation potentials of the lexical modelthat have been seen in other parsing setups (seeRimell and Clark (2008) for example), as well as ex-amine the limits on the effects of more training data.Finally, we would like to explore just how much thestatistic properties of our data dictate the success ofthe model by looking at related problems like mor-phological analysis of unsegmented languages suchas Japanese.AcknowledgementsI am grateful to my colleagues from the Oslo Lan-guage Technology Group and the DELPH-IN con-sortium for many discussions on the issues involvedin this work, and particularly to Stephan Oepen whoinspired the initial lattice tagging idea.
Thanks alsoto three anonymous reviewers for their very con-structive feedback which improved the final ver-sion.
Large-scale experimentation and engineeringis made possible though access to the TITAN high-performance computing facilities at the Universityof Oslo, and I am grateful to the Scientific Com-putating staff at UiO, as well as to the NorwegianMetacenter for Computational Science and the Nor-wegian tax payer.1210ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: an approach to almost parsing.
Compu-tational Linguistics, 25(2):237 ?
265.Srinavas Bangalore and Aravind Joshi, editors.
2010.Supertagging: Using Complex Lexical Descriptions inNatural Language Processing.
The MIT Press, Cam-bridge, US.Ezra Black, Steve Abney, Dan Flickinger, ClaudiaGdaniec, Ralph Grishman, Phil Harrison, Don Hin-dle, Robert Ingria, Fred Jelinek, Judith Klavans, MarkLiberman, Mitch Marcus, S. Roukos, Beatrice San-torini, and Tomek Strzalkowski.
1991.
A procedurefor quantitatively comparing the syntactic coverage ofEnglish grammars.
In Proceedings of the Workshop onSpeech and Natural Language, page 306 ?
311, PacificGrove, USA.Philip Blunsom.
2007.
Structured Classification forMultilingual Natural Language Processing.
Ph.D.thesis, Department of Computer Science and SoftwareEngineering, University of Melbourne.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
In Proceedings of the Sixth Conferenceon Applied Natural Language Processing ANLP-2000,page 224 ?
231, Seattle, USA.Ulrich Callmeier.
2000.
PET.
A platform for experi-mentation with efficient HPSG processing techniques.Natural Language Engineering, 6(1):99 ?
108, March.Stephen Clark and James R. Curran.
2007.
Formalism-independent parser evaluation with CCG and Dep-Bank.
In Proceedings of the 45th Meeting of the Asso-ciation for Computational Linguistics, page 248 ?
255,Prague, Czech Republic.Rebecca Dridan and Stephan Oepen.
2012.
Tokeniza-tion.
Returning to a long solved problem.
A survey,contrastive experiment, recommendations, and toolkit.In Proceedings of the 50th Meeting of the Associationfor Computational Linguistics, page 378 ?
382, Jeju,Republic of Korea, July.Rebecca Dridan, Valia Kordoni, and Jeremy Nicholson.2008.
Enhancing performance of lexicalised gram-mars.
page 613 ?
621.Rebecca Dridan.
2009.
Using lexical statistics to im-prove HPSG parsing.
Ph.D. thesis, Department ofComputational Linguistics, Saarland University.Murhaf Fares, Stephan Oepen, and Yi Zhang.
2013.
Ma-chine learning for high-quality tokenization.
Replicat-ing variable tokenization schemes.
In ComputationalLinguistics and Intelligent Text Processing, page 231 ?244.
Springer.Murhaf Fares.
2013.
ERG tokenization and lexical cat-egorization: a sequence labeling approach.
Master?sthesis, Department of Informatics, University of Oslo.Dan Flickinger, Yi Zhang, and Valia Kordoni.
2012.DeepBank.
A dynamically annotated treebank of theWall Street Journal.
In Proceedings of the 11th Inter-national Workshop on Treebanks and Linguistic Theo-ries, page 85 ?
96, Lisbon, Portugal.
Edic?o?es Colibri.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural Language En-gineering, 6 (1):15 ?
28.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields to japanesemorphological analysis.
In Proceedings of the 2004Conference on Empirical Methods in Natural Lan-guage Processing, page 230 ?
237.Jonathan K. Kummerfeld, Jessika Roesner, Tim Daw-born, James Haggerty, James R. Curran, and StephenClark.
2010.
Faster parsing by supertagger adapta-tion.
In Proceedings of the 48th Meeting of the Asso-ciation for Computational Linguistics, page 345 ?
355,Uppsala, Sweden.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pora of English: The Penn Treebank.
ComputationalLinguistics, 19:313 ?
330.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-jii.
2007.
Efficient HPSG parsing with supertag-ging and CFG-filtering.
In Proceedings of the Interna-tional Joint Conference on Artificial Intelligence (IJ-CAI 2007), page 1671 ?
1676, Hyderabad, India.Kevin P. Murphy.
2002.
Hidden semi-Markov models(HSMMs).Stephan Oepen and John Carroll.
2000.
Ambiguity pack-ing in constraint-based parsing.
Practical results.
InProceedings of the 1st Meeting of the North AmericanChapter of the Association for Computational Linguis-tics, page 162 ?
169, Seattle, WA, USA.Stephan Oepen, Daniel Flickinger, Kristina Toutanova,and Christopher D. Manning.
2004.
LinGO Red-woods.
A rich and dynamic treebank for HPSG.
Re-search on Language and Computation, 2(4):575 ?
596.Robbert Prins and Gertjan van Noord.
2003.
Reinforcingparser preferences through tagging.
Traitement Au-tomatique des Langues, 44(3):121 ?
139.Laura Rimell and Stephen Clark.
2008.
Adaptinga lexicalized-grammar parser to contrasting domains.page 475 ?
484.Kristina Toutanova and Colin Cherry.
2009.
A globalmodel for joint lemmatization and part-of-speech pre-diction.
In Proceedings of the 47th Meeting of theAssociation for Computational Linguistics, page 486 ?494, Singapore.Gisle Ytrest?l.
2012.
Transition-based Parsing forLarge-scale Head-Driven Phrase Structure Gram-mars.
Ph.D. thesis, Department of Informatics, Uni-versity of Oslo.1211Gisle Ytrest?l, Stephan Oepen, and Dan Flickinger.2009.
Extracting and annotating Wikipedia sub-domains.
In Proceedings of the 7th InternationalWorkshop on Treebanks and Linguistic Theories, page185 ?
197, Groningen, The Netherlands.Yue Zhang and Stephen Clark.
2010.
A fast decoder forjoint word segmentation and POS-tagging using a sin-gle discriminative model.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, page 843 ?
852, Cambridge, MA,USA.Yi Zhang, Stephan Oepen, and John Carroll.
2007.
Ef-ficiency in unification-based n-best parsing.
In Pro-ceedings of the 10th International Conference on Pars-ing Technologies, page 48 ?
59, Prague, Czech Repub-lic, July.1212
