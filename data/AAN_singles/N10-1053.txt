Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 353?356,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsThe Effect of Ambiguity on the Automated Acquisition of WSD ExamplesMark Stevenson and Yikun GuoDepartment of Computer Science,University of Sheffield,Regent Court, 211 Portobello,Sheffield, S1 4DPUnited Kingdomm.stevenson@dcs.shef.ac.uk and g.yikun@dcs.shef.ac.ukAbstractSeveral methods for automatically gen-erating labeled examples that can beused as training data for WSD systemshave been proposed, including a semi-supervised approach based on relevancefeedback (Stevenson et al, 2008a).
Thisapproach was shown to generate examplesthat improved the performance of a WSDsystem for a set of ambiguous terms fromthe biomedical domain.
However, we findthat this approach does not perform as wellon other data sets.
The levels of ambigu-ity in these data sets are analysed and wesuggest this is the reason for this negativeresult.1 IntroductionSeveral studies, for example (Mihalcea et al,2004; Pradhan et al, 2007), have shown that su-pervised approaches to Word Sense Disambigua-tion (WSD) outperform unsupervised ones.
Butthese rely on labeled training data which is diffi-cult to create and not always available (e.g.
(Wee-ber et al, 2001)).
Various techniques for creatinglabeled training data automatically have been sug-gested in the literature.
Stevenson et al (2008a)describe a semi-supervised approach that used rel-evance feedback (Rocchio, 1971) to analyse ex-isting labeled examples and use the informationproduced to generate further ones.
The approachwas tested on the biomedical domain and the addi-tional examples found to improve performance ofa WSD system.
However, biomedical documentsrepresent a restricted domain.
In this paper thesame approach is tested against two data sets thatare not limited to a single domain.2 Application to a Range of Data SetsIn this paper the relevance feedback approach de-scribed by Stevenson et al (2008a) is evaluated us-ing three data sets: the NLM-WSD corpus (Wee-ber et al, 2001) which Stevenson et al (2008a)used for their experiments, the Senseval-3 lexicalsample task (Mihalcea et al, 2004) and the coarse-grained version of the SemEval English lexicalsample task (Pradhan et al, 2007).2.1 Generating ExamplesTo generate examples for a particular sense of anambiguous term all of the examples where theterm is used in that sense are considered to be?relevant documents?
while the examples in whichany other sense of the term is used are consideredto be ?irrelevant documents?.
Relevance feed-back (Rocchio, 1971) is used to generate a set ofquery terms designed to identify relevant docu-ments, and therefore instances of the sense.
Thetop five query terms are used to retrieve docu-ments and these are used as labeled examples ofthe sense.
Further details of this process are de-scribed by Stevenson et al (2008a).This process requires a collection of documentsthat can be queried to generate the additionalexamples.
For the NLM-WSD data set weused PubMed, a database of biomedical journalabstracts queried using the Entrez retrieval sys-tem (http://www.ncbi.nlm.nih.gov/sites/gquery).
The British National Corpus(BNC) was used for Senseval-3 and SemEval.1Lucene (http://lucene.apache.org) wasused to index the BNC and retrieve examples.1We also experimented with the English WaCky corpus(Baroni et al, 2009) which contains nearly 2 billion wordsautomatically retrieved from the web.
However, results werenot as good as when the BNC was used.3532.2 WSD SystemWe use a WSD system that has been shown toperform well when evaluated against ambiguitiesfound in both general text and the biomedical do-main (Stevenson et al, 2008b).
Medical SubjectHeadings (MeSH), a controlled vocabulary usedfor document indexing, are obtained from PubMedand used as additional features for the NLM-WSDdata set since they have been shown to improveperformance.
The features are combined usingthe Vector Space Model, a simple memory-basedlearning algorithm.2.3 ExperimentExperiments were carried out comparing perfor-mance when the WSD system was trained usingeither the examples in the original data set (orig-inal), the examples generated from these usingthe relevance feedback approach (additional) or acombination of these (combined).
The Senseval-3 and SemEval corpora are split into training andtest portions so the training portion is used as theoriginal data set and the WSD system evaluatedagainst the held-back data.
As there is no suchrecognised standard split for the NLM-WSD cor-pus, 10-fold cross-validation was used.
For eachfold the training portion is used as the original dataset and automatically generated examples createdby examining just that part of the data.
Evaluationis carried out against the fold?s test data and theaverage result across the 10 folds reported.Table 1 shows the results of this experiment.2Examples generated using the relevance feedbackapproach only improve results for one data set, theNLM-WSD corpus.
In this case there is a sig-nificant improvement (Mann-Whitney, p < 0.01)when the original and automatically generated ex-amples are combined.
There is no such improve-ment for the other two data sets: WSD results us-ing the additional data are noticeably worse thanwhen the original data is used alone and, althoughperformance improves when these examples arecombined with the original data, results are stilllower than using the original data.
When exam-ples are combined there is a drop in performanceof 1.2% and 2.9% for SemEval and Senseval-3 re-2Results reported here for the NLM-WSD corpus areslightly different from those reported by (Stevenson et al,2008a).
We used an additional feature (MeSH headings),which improved the baseline performance, and more queryterms which improved the quality of the additional examplesfor all three data sets.spectively.Corpus Original Additional CombinedNLM-WSD 87.9 87.6 89.2SemEval 83.7 74.6 82.5Senseval-3 68.8 56.3 65.9Table 1: Results of relevance feedback approachapplied to three data setsThese results indicate that the relevance feed-back approach described by Stevenson et al(2008a) is not able to generate useful examples forthe Senseval-3 and SemEval data sets, although itcan for the NLM-WSD data set.
We hypothesisethat these corpora contain different levels of ambi-guity which effect suitability of the approach.3 Analysis of AmbiguitiesThe three data sets are compared using measuresdesigned to determine the level of ambiguity theycontain.
Section 3.1 reports results using variouswidely used measures based on the distribution ofsenses.
Section 3.2 introduces a measure basedon the semantic similarity between the possiblesenses of ambiguous terms.3.1 Sense DistributionsThree measures for characterising the difficulty ofWSD data sets based on their sense distributionwere used.
The first is the widely applied mostfrequent sense (MFS) baseline (McCarthy et al,2004), i.e.
the proportion of examples for an am-biguous term that are labeled with the commonestsense.
The second is number of senses per am-biguous term.
The final measure, the entropy ofthe sense distribution, has been shown to be a goodindication of disambiguation difficulty (Kilgarriffand Rosenzweig, 2000).
For two of these mea-sures (number of senses and entropy) a higher fig-ure indicates greater ambiguity while for the MFSmeasure a lower figure indicates a more difficultdata set.Table 2 shows the results of computing thesemeasures averaged across all terms in the cor-pus.
For two measures (number of senses and en-tropy) the NLM-WSD corpus is least ambiguous,Senseval-3 the most ambiguous with SemEval be-tween them.
The MFS scores are very similar fortwo data sets (NLM-WSD and SemEval), both ofwhich are much higher than for Senseval-3.354These measures suggest that the NLM-WSDcorpus is less ambiguous than the other two andalso that the Senseval-3 corpus is the most am-biguous of the three.Corpus MFS Senses EntropyNLM-WSD 78.0 2.63 0.73SemEval 78.4 3.60 0.91Senseval-3 53.8 6.43 1.75Table 2: Properties of Data Sets using sense distri-bution measures3.2 Semantic SimilarityWe also developed a measure that takes into ac-count the similarity in meaning between the possi-ble senses for an ambiguous term.
This measure issimilar to the one used by Passoneau et al (2009)to analyse levels of inter-annotator agreement inword sense annotation.
Our measure is shown inequation 1 where Senses is the set of possiblesenses for an ambiguous term, |Senses| = n and(Senses2)is the set of all subsets of Senses contain-ing two of its members (i.e the set of unorderedpairs).
The similarity between a pair of senses,sim(x, y), can be computed using any lexical sim-ilarity measure, see Pedersen et al (2004).
Essen-tially this measure computes the mean of the sim-ilarities between each pair of senses for the term.sim measure =?
{x,y}(Senses2 )sim(x, y)(n2) (1)One problem with comparing the data sets usedhere is that they use a range of sense invento-ries.
Although lexical similarity measures havebeen applied to WordNet (Pedersen et al, 2004)and UMLS (Pedersen et al, 2007), it is not clearthat the scores they produce can be meaningfullycompared.
To avoid this problem we mapped thesense inventories onto a single resource: WordNetversion 3.0.The mapping was most straightforward forSenseval-3 which uses WordNet 1.7.1 and couldbe automatically mapped onto WordNet 3.0 sensesusing publicly available mappings (Daude?
et al,2000).
The SemEval data contains a mappingfrom the OntoNotes senses to groups of WordNet2.1 senses.
The first sense from this group wasmapped to WordNet 3.0 using the same mappings.Mapping the NLM-WSD corpus was moreproblematic and had to be carried out manually bycomparing sense definitions in UMLS and Word-Net 3.0.
We had expected this process to be diffi-cult but found clear mappings for the majority ofsenses.
There were even found cases in which thesense definitions were identical in both resources.
(The most likely reason for this is that some ofthe resources that are included in the UMLS wereused to compile WordNet.)
Another, more serious,problem is related to the annotation scheme usedin the NLM-WSD corpus.
If none of the possi-ble senses in UMLS were judged to be appropri-ate the annotators could label the sense as ?None?.We did not map these senses since it would requireexamining each instance to determine the most ap-propriate sense or senses in WordNet and we ex-pected this to be error prone.
In addition, there isno guarantee that all of the instances of a particularterm labeled with ?None?
refer to the same mean-ing.
All of the ?None?
senses were removed fromthe NLM-WSD data set and any terms where therewere more than ten instances marked as ?None?were also rejected from the similarity analysis.This allowed us to compute the similarity scorefor just 20 examples (40% of the total) althoughwe felt that this was a large enough sample to pro-vide insight into the data set.The WordNet::Similarity package (Ped-ersen et al, 2004) was used to compute similar-ity scores.
Results are reported for three of themeasures in this package.
(Other measures pro-duced similar results.)
The simple path measurecomputes the similarity between a pair of nodes inWordNet as the reciprocal of the number of edgesin the shortest path between them, the LCh mea-sure (Leacock et al, 1998) also uses informationabout the length of the shortest path between a pairof nodes and combines this with information aboutthe maximum depth in WordNet and the JCn mea-sure (Jaing and Conrath, 1997) makes use of in-formation theory to assign probabilities to each ofthe nodes in the WordNet hierarchy and computessimilarity based on these scores.Table 3 shows the values of equation 1 forthe three similarity measures with scores averagedacross terms.
These results indicate that for allmeasures the Senseval-3 data set contains the mostambiguity and NLM-WSD the least.
This analysisis consistent with the one carried out using mea-sures based on sense distributions (Section 3.1)355MeasureCorpusPath JCn LChNLM-WSD 0.074 0.032 1.027SemEval 0.136 0.061 1.292Senseval-3 0.159 0.063 1.500Table 3: Semantic similarity for each data set us-ing a variety of measuresand suggest that the senses in the NLM-WSD dataset are more clearly distinguished than the othertwo.4 ConclusionThis paper has explored a semi-supervised ap-proach to the generation of labeled training datafor WSD that is based on relevance feedback(Stevenson et al, 2008a).
It was tested on threedata sets but was only found to generate examplesthat were accurate enough to improve WSD per-formance for one of these.
The data set in whicha performance improvement was observed repre-sented a limited domain (biomedicine) while theother two were not restricted in this way.
Measuresdesigned to quantify the level of ambiguity wereapplied to these data sets including ones based onthe distribution of senses and another designed toquantify similarities between senses.
These mea-sures provided evidence that the corpus for whichthe relevance feedback approach was successfulcontained less ambiguity than the other two andthis suggests that the relevance feedback approachis most appropriate when the level of ambiguity islow.The experiments described in this paper high-light the importance of the level of ambiguity onthe relevance feedback approach?s ability to gen-erate useful labeled examples.
Since it is semi-supervised the ambiguity level can be checked us-ing the measures used in this paper (Section 3)and the performance of any automatically gener-ated examples can be compared with the manu-ally labeled ones (see Section 2.3) before decidingwhether or not they should be applied.ReferencesM.
Baroni, S. Bernardini, A. Ferraresi, andE.
Zanchetta.
2009.
The wacky wide web: acollection of very large linguistically processedweb-crawled corpora.
Language Resources andEvaluation, 43(3):209?226.J.
Daude?, L.
Padro?, and G. Rigau.
2000.
Mappingwordnets using structural information.
In Proceed-ings of ACL ?00, pages 504?511, Hong Kong.J.
Jaing and D. Conrath.
1997.
Semantic similar-ity based on corpus statistics and lexical taxonomy.In Proceedings of International Conference on Re-search in Computational Linguistics, Taiwan.A.
Kilgarriff and J. Rosenzweig.
2000.
Frameworkand results for English SENSEVAL.
Computers andthe Humanities, 34(1-2):15?48.C.
Leacock, M. Chodorow, and G. Miller.
1998.Using corpus statistics and WordNet relations forsense identification.
Computational Linguistics,24(1):147?165.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.2004.
Finding predominant word senses in untaggedtext.
In Proceedings of ACL?04, pages 279?286,Barcelona, Spain.R.
Mihalcea, T. Chklovski, and A. Kilgarriff.
2004.The Senseval-3 English lexical sample task.
InProceedings of Senseval-3, pages 25?28, Barcelona,Spain.R.
Passoneau, A. Salleb-Aouissi, and N. Ide.
2009.Making sense of word sense variation.
In Proceed-ings of SEW-2009, pages 2?9, Boulder, Colorado.T.
Pedersen, S. Patwardhan, and Michelizzi.
2004.Wordnet::similarity - measuring the relatedness ofconcepts.
In Proceedings of AAAI-04, pages 1024?1025, San Jose, CA.T.
Pedersen, S. Pakhomov, S. Patwardhan, andC.
Chute.
2007.
Measures of semantic similarityand relateness in the biomedical domain.
Journal ofBiomedical Informatics, 40(3):288?299.S.
Pradhan, E. Loper, D. Dligach, and M. Palmer.2007.
SemEval-2007 Task-17: English LexicalSample, SRL and All Words.
In Proceedings ofSemEval-2007, pages 87?92, Prague, Czech Repub-lic.J.
Rocchio.
1971.
Relevance feedback in Informa-tion Retrieval.
In G. Salton, editor, The SMARTRetrieval System ?
Experiments in Automatic Doc-ument Processing.
Prentice Hall, Englewood Cliffs,NJ.M.
Stevenson, Y. Guo, and R. Gaizauskas.
2008a.Acquiring Sense Tagged Examples using RelevanceFeedback.
In Proceedings of the Coling 2008, pages809?816, Manchester, UK, August.M.
Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.2008b.
Disambiguation of biomedical text using di-verse sources of information.
BMC Bioinformatics,9(Suppl 11):S7.M.
Weeber, J. Mork, and A. Aronson.
2001.
Devel-oping a Test Collection for Biomedical Word SenseDisambiguation.
In Proceedings of AMIA Sympo-sium, pages 746?50, Washington, DC.356
