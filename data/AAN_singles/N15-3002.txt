Proceedings of NAACL-HLT 2015, pages 6?10,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsAnalyzing and Visualizing Coreference Resolution ErrorsSebastian Martschat1, Thierry G?ockel2and Michael Strube11Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany(sebastian.martschat|michael.strube)@h-its.org2iQser GmbH, Walldorf, Germanythierry.goeckel@iqser.comAbstractWe present a toolkit for coreference resolutionerror analysis.
It implements a recently pro-posed analysis framework and contains richcomponents for analyzing and visualizing re-call and precision errors.1 IntroductionCoreference resolution is the task of determiningwhich mentions in a text refer to the same en-tity.
Both the natural language processing engineer(who needs a coreference resolution system for theproblem at hand) and the coreference resolution re-searcher need tools to facilitate and support systemdevelopment, comparison and analysis.In Martschat and Strube (2014), we propose aframework for error analysis for coreference resolu-tion.
In this paper, we present cort1, an implementa-tion of this framework, and show how it can be use-ful for engineers and researchers.
cort is released asopen source and is available for download2.2 Error Analysis FrameworkDue to the set-based nature of coreference resolu-tion, it is not clear how to extract errors when anentity is not correctly identified.
The idea underly-ing the analysis framework of Martschat and Strube(2014) is to employ spanning trees in a graph-basedentity representation.1Short for coreference resolution toolkit.2http://smartschat.de/softwareFigure 1 summarizes their approach.
They repre-sent reference and system entities as complete one-directional graphs (Figures 1a and 1b).
To extractrecall errors, they compute a spanning tree of thereference entity (Figure 1a).
All edges in the span-ning tree which do not appear in the system outputare extracted as recall errors (Figure 1c).
For ex-tracting precision errors, the roles of reference andsystem entities are switched.The analysis algorithm is parametrized only bythe spanning tree algorithm employed: different al-gorithms lead to different notions of errors.
InMartschat and Strube (2014), we propose an algo-rithm based on Ariel?s accessibility theory (Ariel,1990) for reference entities.
For system entity span-ning trees, we take each output pair as an edge.3 ArchitectureOur toolkit is available as a Python library.
Itconsists of three modules: the core module pro-vides mention extraction and preprocessing, thecoreference module implements features forand approaches to coreference resolution, and theanalysis module implements the error analysisframework described above and ships with otheranalysis and visualization utilities.3.1 coreAll input and output must conform to the format ofthe CoNLL-2012 shared task on coreference resolu-tion (Pradhan et al, 2012).
We employ a rule-basedmention extractor, which also computes a rich set ofmention attributes, including tokens, head, part-of-speech tags, named entity tags, gender, number, se-6(a)m1Obamam2hem3the presidentm4his(b)m1m2m3the presidentm4n1n2n3(c)m1m3the presidentFigure 1: (a) a reference entity r (represented as a complete one-directional graph) and its spanning tree Tr, (b) a setS of three system entities, (c) the errors: all edges in Trwhich are not in S.mantic class, grammatical function, coarse mentiontype and fine-grained mention type.3.2 coreferencecort ships with two coreference resolution ap-proaches.
First, it includes multigraph, which is adeterministic approach using a few strong features(Martschat and Strube, 2014).
Second, it includesa mention-pair approach (Soon et al, 2001) witha large feature set, trained via a perceptron on theCoNLL?12 English training data.System MUC B3CEAFeAverageStanfordSieve 64.96 54.49 51.24 56.90BerkeleyCoref 70.27 59.29 56.11 61.89multigraph 69.13 58.61 56.06 61.28mention-pair 69.09 57.84 53.56 60.16Table 1: Comparison of systems on CoNLL?12 Englishdevelopment data.In Table 1, we compare both approaches withStanfordSieve (Lee et al, 2013), the winner of theCoNLL-2011 shared task, and BerkeleyCoref (Dur-rett and Klein, 2013), a state-of-the-art structuredmachine learning approach.
The systems are eval-uated via the CoNLL scorer (Pradhan et al, 2014).Both implemented approaches achieve competi-tive performance.
Due to their modular implemen-tation, both approaches are easily extensible withnew features and with training or inference schemes.They therefore can serve as a good starting point forsystem development and analysis.3.3 analysisThe core of this module is the ErrorAnalysisclass, which extracts and manages errors extractedfrom one or more systems.
The user can defineown spanning tree algorithms to extract errors.
Wealready implemented the algorithms discussed inMartschat and Strube (2014).
Furthermore, thismodule provides functionality to?
categorize and filter sets of errors,?
visualize these sets,?
compare errors of different systems, and?
display errors in document context.Which of these features is interesting to the user de-pends on the use case.
In the following, we willdescribe the popular use case of improving a coref-erence system in detail.
Our system also supportsother use cases, such as the cross-system analysisdescribed in Martschat and Strube (2014).4 Use Case: Improving a CoreferenceResolution SystemA natural language processing engineer might be in-terested in improving the performance of a corefer-ence resolution system since it is necessary for an-other task.
The needs may differ depending on thetask at hand: for some tasks proper name corefer-ence may be of utmost importance, while other tasksneed mostly pronoun coreference.
Through modeland feature redesign, the engineer wants to improvethe system with respect to a certain error class.The user will start with a baseline system, whichcan be one of the implemented systems in our toolkitor a third-party system.
We now describe how cortfacilitates improving the system.74.1 Initial AnalysisTo get an initial assessment, the user can extract allerrors made by the system and then make use of theplotting component to compare these errors with themaximum possible number of errors3.For a meaningful analysis, we have to find a suit-able error categorization.
Suppose the user is inter-ested in improving recall for non-pronominal coref-erence.
Hence, following Martschat and Strube(2014), we categorize all errors by coarse mentiontype of anaphor and antecedent (proper name, noun,pronoun, demonstrative pronoun or verb)4.Both name Noun-Name Name-Noun Both nounCategory05001000150020002500300035004000Number of errorsRecall Errors maximummultigraphFigure 2: Recall errors of the multigraph baseline.Figure 2 compares the recall error numbers ofthe multigraph system with the maximum possiblenumber of errors for the categories of interest tothe engineer.
The plot was created by our toolkitvia matplotlib (Hunter, 2007).
We can see that themodel performs very well for proper name pairs.Relative to the maximum number of errors, there aremuch more recall errors in the other categories.
Aplot for precision errors shows that the system makesonly relatively few precision errors, especially forproper name pairs.After studying these plots the user decides to im-prove recall for pairs where the anaphor is a nounand the antecedent is a name.
This is a frequent cat-egory which is handled poorly by the system.3For recall, the maximum number of errors are the errorsmade by a system which puts each mention in its own cluster.For precision, we take all pairwise decisions of a model.4For a pair of mentions constituting an error, we call themention appearing later in the text the anaphor, the other men-tion antecedent.4.2 Detailed AnalysisIn order to determine how to improve the system,the user needs to perform a detailed analysis ofthe noun-name errors.
Our toolkit provides sev-eral methods to do so.
First of all, one can browsethrough the pairwise error representations.
This sug-gests further subcategorization (for example by thepresence of token overlap).
An iteration of this pro-cess leads to a fine-grained categorization of errors.However, this approach does not provide any doc-ument context, which is necessary to understandsome errors.
Maybe context features can help in re-solving the error, or the error results from multiplecompeting antecedents.
We therefore include a visu-alization component, which also allows to study theinterplay between recall and precision.Figure 3 shows a screenshot of this visualiza-tion component, which runs in a web browser usingJavaScript.
The header displays the identifier of thedocument in focus.
The left bar contains the naviga-tion panel, which includes?
a list of all documents in the corpus,?
a summary of all errors for the document in fo-cus, and?
lists of reference and system entities for thedocument in focus.To the right of the navigation panel, the document infocus is shown.
When the user picks a reference orsystem entity from the corresponding list, cort dis-plays all recall and precision errors for all mentionswhich are contained in the entity (as labeled red ar-rows between mentions).
Alternatively, the user canchoose an error category from the error summary.
Inthat case, all errors of that category are displayed.We use color to distinguish between entities:mentions in different entities have different back-ground colors.
Additionally mentions in referenceentities have a yellow border, while mentions in sys-tem entities have a blue border (for example, themention the U.S.-backed rebels is in a reference en-tity and in a system entity).
The user can choose tocolor the background of mentions either dependingon their gold entity or depending on their system en-tity.These visualization capabilities allow for a de-tailed analysis of errors and enable the user to takeall document information into account.8Figure 3: Screenshot of the visualization component.The result of the analysis is that almost all errorsare missed is-a relations, such as in the examples inFigure 3 (the U.S.-backed rebels and the Contras).4.3 Error ComparisonMotivated by this, the user can add features to thesystem, for example incorporating world knowledgefrom Wikipedia.
The output of the changed modelcan be loaded into the ErrorAnalysis objectwhich already manages the errors made by the base-line system.To compare the errors, cort implements variousfunctions.
In particular, the user can access com-mon errors and errors which are unique to one ormore systems.
This allows for an assessment of thequalitative usefulness of the new feature.
Depend-ing on the results of the comparison, the user candecide between discarding, retaining and improvingthe feature.5 Related WorkCompared to our original implementation of theerror analysis framework (Martschat and Strube,2014), we made the analysis interface more user-friendly and provide more analysis functionality.Furthermore, while our original implementation didnot include any visualization capabilities, we nowallow for both data visualization and document vi-sualization.We are aware of two other software packages forcoreference resolution error analysis.
Our toolkit9complements these.
Kummerfeld and Klein (2013)present a toolkit which extracts errors from transfor-mation of reference to system entities.
Hence, theirdefinition of what an error is not rooted in a pairwiserepresentation, and is therefore conceptually differ-ent from our definition.
They do not provide anyvisualization components.ICE (G?artner et al, 2014) is a toolkit for corefer-ence visualization and corpus analysis.
In particu-lar, the toolkit visualizes recall and precision errorsin a tree-based visualization of coreference clusters.Compared to ICE, we provide more extensive func-tionality for error analysis and can accommodate fordifferent notions of errors.6 Conclusions and Future WorkWe presented cort, a toolkit for coreference reso-lution error analysis.
It implements a graph-basedanalysis framework, ships with two strong coref-erence resolution baselines and provides extensivefunctionality for analysis and visualization.We are currently investigating whether the analy-sis framework can also be applied to structurally re-lated tasks, such as cross-document coreference res-olution (Singh et al, 2011) or entity linking.AcknowledgementsThis work has been funded by the Klaus TschiraFoundation, Heidelberg, Germany.
The first authorhas been supported by a HITS PhD scholarship.ReferencesMira Ariel.
1990.
Accessing Noun Phrase Antecedents.Routledge, London, U.K.; New York, N.Y.Greg Durrett and Dan Klein.
2013.
Easy victories anduphill battles in coreference resolution.
In Proceed-ings of the 2013 Conference on Empirical Methods inNatural Language Processing, Seattle, Wash., 18?21October 2013, pages 1971?1982.Markus G?artner, Anders Bj?orkelund, Gregor Thiele,Wolfgang Seeker, and Jonas Kuhn.
2014.
Visualiza-tion, search, and error analysis for coreference anno-tations.
In Proceedings of 52nd Annual Meeting ofthe Association for Computational Linguistics: SystemDemonstrations, Baltimore, Md., 22?27 June 2014,pages 7?12.John D. Hunter.
2007.
Matplotlib: A 2D graphicsenvironment.
Computing in Science & Engineering,9(3):90?95.Jonathan K. Kummerfeld and Dan Klein.
2013.
Error-driven analysis of challenges in coreference resolution.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, Seattle,Wash., 18?21 October 2013, pages 265?277.Heeyoung Lee, Angel Chang, Yves Peirsman, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2013.Deterministic coreference resolution based on entity-centric, precision-ranked rules.
Computational Lin-guistics, 39(4):885?916.Sebastian Martschat and Michael Strube.
2014.
Recallerror analysis for coreference resolution.
In Proceed-ings of the 2014 Conference on Empirical Methodsin Natural Language Processing, Doha, Qatar, 25?29October 2014, pages 2070?2081.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 Shared Task: Modeling multilingual unrestrictedcoreference in OntoNotes.
In Proceedings of theShared Task of the 16th Conference on ComputationalNatural Language Learning, Jeju Island, Korea, 12?14July 2012, pages 1?40.Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-uard Hovy, Vincent Ng, and Michael Strube.
2014.Scoring coreference partitions of predicted mentions:A reference implementation.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 2: Short Papers), Balti-more, Md., 22?27 June 2014, pages 30?35.Sameer Singh, Amarnag Subramanya, Fernando Pereira,and Andrew McCallum.
2011.
Large-scale cross-document coreference using distributed inference andhierarchical models.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), Portland, Oreg.,19?24 June 2011, pages 793?803.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.10
