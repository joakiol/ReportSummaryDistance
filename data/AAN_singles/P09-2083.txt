Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 329?332,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPDo Automatic Annotation Techniques Have Any Impact on SupervisedComplex Question Answering?Yllias ChaliUniversity of LethbridgeLethbridge, AB, Canadachali@cs.uleth.caSadid A. HasanUniversity of LethbridgeLethbridge, AB, Canadahasan@cs.uleth.caShafiq R. JotyUniversity of British ColumbiaVancouver, BC, Canadarjoty@cs.ubc.caAbstractIn this paper, we analyze the impact ofdifferent automatic annotation methods onthe performance of supervised approachesto the complex question answering prob-lem (defined in the DUC-2007 main task).Huge amount of annotated or labeleddata is a prerequisite for supervised train-ing.
The task of labeling can be ac-complished either by humans or by com-puter programs.
When humans are em-ployed, the whole process becomes timeconsuming and expensive.
So, in orderto produce a large set of labeled data weprefer the automatic annotation strategy.We apply five different automatic anno-tation techniques to produce labeled datausing ROUGE similarity measure, Ba-sic Element (BE) overlap, syntactic sim-ilarity measure, semantic similarity mea-sure, and Extended String SubsequenceKernel (ESSK).
The representative super-vised methods we use are Support Vec-tor Machines (SVM), Conditional Ran-dom Fields (CRF), Hidden Markov Mod-els (HMM), and Maximum Entropy (Max-Ent).
Evaluation results are presented toshow the impact.1 IntroductionIn this paper, we consider the complex questionanswering problem defined in the DUC-2007 maintask1.
We focus on an extractive approach of sum-marization to answer complex questions where asubset of the sentences in the original documentsare chosen.
For supervised learning methods,huge amount of annotated or labeled data sets areobviously required as a precondition.
The deci-sion as to whether a sentence is important enough1http://www-nlpir.nist.gov/projects/duc/duc2007/to be annotated can be taken either by humans orby computer programs.
When humans are em-ployed in the process, producing such a large la-beled corpora becomes time consuming and ex-pensive.
There comes the necessity of using au-tomatic methods to align sentences with the in-tention to build extracts from abstracts.
In thispaper, we use ROUGE similarity measure, BasicElement (BE) overlap, syntactic similarity mea-sure, semantic similarity measure, and ExtendedString Subsequence Kernel (ESSK) to automati-cally label the corpora of sentences (DUC-2006data) into extract summary or non-summary cat-egories in correspondence with the document ab-stracts.
We feed these 5 types of labeled data intothe learners of each of the supervised approaches:SVM, CRF, HMM, and MaxEnt.
Then we exten-sively investigate the performance of the classi-fiers to label unseen sentences (from 25 topics ofDUC-2007 data set) as summary or non-summarysentence.
The experimental results clearly showthe impact of different automatic annotation meth-ods on the performance of the candidate super-vised techniques.2 Automatic Annotation SchemesUsing ROUGE Similarity Measures ROUGE(Recall-Oriented Understudy for Gisting Evalua-tion) is an automatic tool to determine the qual-ity of a summary using a collection of measuresROUGE-N (N=1,2,3,4), ROUGE-L, ROUGE-Wand ROUGE-S which count the number of over-lapping units such as n-gram, word-sequences,and word-pairs between the extract and the ab-stract summaries (Lin, 2004).
We assume eachindividual document sentence as the extract sum-mary and calculate its ROUGE similarity scoreswith the corresponding abstract summaries.
Thusan average ROUGE score is assigned to each sen-tence in the document.
We choose the top N sen-tences based on ROUGE scores to have the label329+1 (summary sentences) and the rest to have thelabel ?1 (non-summary sentences).Basic Element (BE) Overlap Measure We ex-tract BEs, the ?head-modifier-relation?
triples forthe sentences in the document collection using BEpackage 1.0 distributed by ISI2.
The ranked listof BEs sorted according to their Likelihood Ra-tio (LR) scores contains important BEs at the topwhich may or may not be relevant to the abstractsummary sentences.
We filter those BEs by check-ing possible matches with an abstract sentenceword or a related word.
For each abstract sen-tence, we assign a score to every document sen-tence as the sum of its filtered BE scores dividedby the number of BEs in the sentence.
Thus, ev-ery abstract sentence contributes to the BE scoreof each document sentence and we select the topN sentences based on average BE scores to havethe label +1 and the rest to have the label ?1.Syntactic Similarity Measure In order to cal-culate the syntactic similarity between the abstractsentence and the document sentence, we first parsethe corresponding sentences into syntactic treesusing Charniak parser3(Charniak, 1999) and thenwe calculate the similarity between the two treesusing the tree kernel (Collins and Duffy, 2001).We convert each parenthesis representation gener-ated by Charniak parser to its corresponding treeand give the trees as input to the tree kernel func-tions for measuring the syntactic similarity.
Thetree kernel of two syntactic trees T1and T2is ac-tually the inner product of the two m-dimensionalvectors, v(T1) and v(T2):TK(T1, T2) = v(T1).v(T2)The TK (tree kernel) function gives the simi-larity score between the abstract sentence and thedocument sentence based on the syntactic struc-ture.
Each abstract sentence contributes a score tothe document sentences and the top N sentencesare selected to be annotated as +1 and the rest as?1 based on the average of similarity scores.Semantic Similarity Measure Shallow seman-tic representations, bearing a more compact infor-mation, can prevent the sparseness of deep struc-tural approaches and the weakness of BOW mod-els (Moschitti et al, 2007).
To experiment withsemantic structures, we parse the corresponding2BE website:http://www.isi.edu/ cyl/BE3available at ftp://ftp.cs.brown.edu/pub/nlparser/sentences semantically using a Semantic Role La-beling (SRL) system like ASSERT4.
ASSERT isan automatic statistical semantic role tagger, thatcan annotate naturally occuring text with semanticarguments.
We represent the annotated sentencesusing tree structures called semantic trees (ST).Thus, by calculating the similarity between STs,each document sentence gets a semantic similarityscore corresponding to each abstract sentence andthen the topN sentences are selected to be labeledas +1 and the rest as ?1 on the basis of averagesimilarity scores.Extended String Subsequence Kernel (ESSK)Formally, ESSK is defined as follows (Hirao et al,2004):Kessk(T,U) =d?m=1?ti?T?uj?UKm(ti, uj)Km(ti, uj) ={val(ti, uj) if m = 1K?m?1(ti, uj) ?
val(ti, uj)Here, K?m(ti, uj) is defined below.
tiand ujare the nodes of T and U , respectively.
Each nodeincludes a word and its disambiguated sense.
Thefunction val(t, u) returns the number of attributescommon to the given nodes t and u.K?m(ti, uj) ={0 if j = 1?K?m(ti, uj?1) +K?
?m(ti, uj?1)Here ?
is the decay parameter for the numberof skipped words.
We choose ?
= 0.5 for thisresearch.
K?
?m(ti, uj) is defined as:K?
?m(ti, uj) ={0 if i = 1?K?
?m(ti?1, uj) +Km(ti?1, uj)Finally, the similarity measure is defined afternormalization as below:simessk(T,U) =Kessk(T,U)?Kessk(T, T )Kessk(U,U)Indeed, this is the similarity score we assign toeach document sentence for each abstract sentenceand in the end, top N sentences are selected tobe annotated as +1 and the rest as ?1 based onaverage similarity scores.3 ExperimentsTask Description The problem definition atDUC-2007 was: ?Given a complex question (topicdescription) and a collection of relevant docu-ments, the task is to synthesize a fluent, well-organized 250-word summary of the documents4available at http://cemantix.org/assert330that answers the question(s) in the topic?.
We con-sider this task and use the five automatic annota-tion methods to label each sentence of the 50 doc-ument sets of DUC-2006 to produce five differ-ent versions of training data for feeding the SVM,HMM, CRF and MaxEnt learners.
We choose thetop 30% sentences (based on the scores assignedby an annotation scheme) of a document set tohave the label +1 and the rest to have ?1.
Unla-beled sentences of 25 document sets of DUC-2007data are used for the testing purpose.Feature Space We represent each of thedocument-sentences as a vector of feature-values.We extract several query-related features andsome other important features from each sen-tence.
We use the features: n-gram overlap,Longest Common Subsequence (LCS), WeightedLCS (WLCS), skip-bigram, exact word overlap,synonym overlap, hypernym/hyponym overlap,gloss overlap, Basic Element (BE) overlap, syn-tactic tree similarity measure, position of sen-tences, length of sentences, Named Entity (NE),cue word match, and title match (Edmundson,1969).Supervised Systems For SVM we use secondorder polynomial kernel for the ROUGE andESSK labeled training.
For the BE, syntactic, andsemantic labeled training third order polynomialkernel is used.
The use of kernel is based on theaccuracy we achieved during training.
We apply3-fold cross validation with randomized local-gridsearch for estimating the value of the trade-off pa-rameter C. We try the value of C in 2ifollowingheuristics, where i ?
{?5,?4, ?
?
?
, 4, 5} and setC as the best performed value 0.125 for secondorder polynomial kernel and default value is usedfor third order kernel.
We use SVMlight 5pack-age for training and testing in this research.
In caseof HMM, we apply the Maximum Likelihood Esti-mation (MLE) technique by frequency counts withadd-one smoothing to estimate the three HMMparameters: initial state probabilities, transitionprobabilities and emission probabilities.
We useDr.
Dekang Lin?s HMM package6to generatethe most probable label sequence given the modelparameters and the observation sequence (unla-beled DUC-2007 test data).
We use MALLET-0.4NLP toolkit7to implement the CRF.
We formu-5http://svmlight.joachims.org/6http://www.cs.ualberta.ca/?lindek/hmm.htm7http://mallet.cs.umass.edu/late our problem in terms of MALLET?s Simple-Tagger class which is a command line interface tothe MALLET CRF class.
We modify the Simple-Tagger class in order to include the provision forproducing corresponding posterior probabilities ofthe predicted labels which are used later for rank-ing sentences.
We build the MaxEnt system usingDr.
Dekang Lin?s MaxEnt package8.
To define theexponential prior of the ?
values in MaxEnt mod-els, an extra parameter ?
is used in the packageduring training.
We keep the value of ?
as default.Sentence Selection The proportion of importantsentences in the training data will differ from theone in the test data.
A simple strategy is to rankthe sentences in a document, then select the top Nsentences.
In SVM systems, we use the normal-ized distance from the hyperplane to each sampleto rank the sentences.
Then, we choose N sen-tences until the summary length (250 words forDUC-2007) is reached.
For HMM systems, weuse Maximal Marginal Relevance (MMR) basedmethod to rank the sentences (Carbonell et al,1997).
In CRF systems, we generate posteriorprobabilities corresponding to each predicted labelin the label sequence to measure the confidence ofeach sentence for summary inclusion.
Similarlyfor MaxEnt, the corresponding probability valuesof the predicted labels are used to rank the sen-tences.Evaluation Results The multiple ?referencesummaries?
given by DUC-2007 are used in theevaluation of our summary content.
We evalu-ate the system generated summaries using the au-tomatic evaluation toolkit ROUGE (Lin, 2004).We report the three widely adopted importantROUGE metrics in the results: ROUGE-1 (uni-gram), ROUGE-2 (bigram) and ROUGE-SU (skipbi-gram).
Figure 1 shows the ROUGE F-measuresfor SVM, HMM, CRF and MaxEnt systems.
TheX-axis containing ROUGE, BE, Synt (Syntactic),Sem (Semantic), and ESSK stands for the annota-tion scheme used.
The Y-axis shows the ROUGE-1 scores at the top, ROUGE-2 scores at the bottomand ROUGE-SU scores in the middle.
The super-vised systems are distinguished by the line styleused in the figure.From the figure, we can see that the ESSK la-beled SVM system is having the poorest ROUGE -1 score whereas the Sem labeled system performs8http://www.cs.ualberta.ca/?lindek/downloads.htm331Figure 1: ROUGE F-scores for different supervised systemsbest.
The other annotation methods?
impact is al-most similar here in terms of ROUGE-1.
Ana-lyzing ROUGE-2 scores, we find that the BE per-forms the best for SVM, on the other hand, Semachieves top ROUGE-SU score.
As for the twomeasures Sem annotation is performing the best,we can typically conclude that Sem annotation isthe most suitable method for the SVM system.ESSK works as the best for HMM and Sem la-beling performs the worst for all ROUGE scores.Synt and BE labeled HMMs perform almost simi-lar whereas ROUGE labeled system is pretty closeto that of ESSK.
Again, we see that the CRF per-forms best with the ESSK annotated data in termsof ROUGE -1 and ROUGE-SU scores and Semhas the highest ROUGE-2 score.
But BE and Syntlabeling work bad for CRF whereas the ROUGElabeling performs decently.
So, we can typicallyconclude that ESSK annotation is the best methodfor the CRF system.
Analyzing further, we findthat ESSK works best for MaxEnt and BE label-ing is the worst for all ROUGE scores.
We canalso see that ROUGE, Synt and Sem labeled Max-Ent systems perform almost similar.
So, from thisdiscussion we can come to a conclusion that SVMsystem performs best if the training data uses se-mantic annotation scheme and ESSK works bestfor HMM, CRF and MaxEnt systems.4 Conclusion and Future WorkIn the work reported in this paper, we have per-formed an extensive experimental evaluation toshow the impact of five automatic annotationmethods on the performance of different super-vised machine learning techniques in confrontingthe complex question answering problem.
Experi-mental results show that Sem annotation is the bestfor SVM whereas ESSK works well for HMM,CRF and MaxEnt systems.
In the near future,we plan to work on finding more sophisticated ap-proaches to effective automatic labeling so that wecan experiment with different supervised methods.ReferencesJaime Carbonell, Yibing Geng, and Jade Goldstein.1997.
Automated query-relevant summarization anddiversity-based reranking.
In IJCAI-97 Workshop onAI in Digital Libraries, pages 12?19, Japan.Eugene Charniak.
1999.
A Maximum-Entropy-Inspired Parser.
In Technical Report CS-99-12,Brown University, Computer Science Department.Michael Collins and Nigel Duffy.
2001.
ConvolutionKernels for Natural Language.
In Proceedings ofNeural Information Processing Systems, pages 625?632, Vancouver, Canada.Harold P. Edmundson.
1969.
New methods in auto-matic extracting.
Journal of the ACM, 16(2):264?285.Tsutomu Hirao, Jun Suzuki, Hideki Isozaki, and EisakuMaeda.
2004.
Dependency-based sentence align-ment for multiple document summarization.
In Pro-ceedings of the 20th International Conference onComputational Linguistics, pages 446?452.Chin-Yew Lin.
2004.
ROUGE: A Package for Au-tomatic Evaluation of Summaries.
In Proceed-ings of Workshop on Text Summarization BranchesOut, Post-Conference Workshop of Association forComputational Linguistics, pages 74?81, Barcelona,Spain.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
ExploitingSyntactic and Shallow Semantic Kernels for Ques-tion/Answer Classificaion.
In Proceedings of the45th Annual Meeting of the Association of Compu-tational Linguistics, pages 776?783, Prague, CzechRepublic.
ACL.332
